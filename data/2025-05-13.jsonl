{"id": "2505.06416", "pdf": "https://arxiv.org/pdf/2505.06416.pdf", "abs": "https://arxiv.org/abs/2505.06416", "title": "ScaleMCP: Dynamic and Auto-Synchronizing Model Context Protocol Tools for LLM Agents", "authors": ["Elias Lumer", "Anmol Gulati", "Vamse Kumar Subbiah", "Pradeep Honaganahalli Basavaraju", "James A. Burke"], "categories": ["cs.CL"], "comment": "17 pages", "summary": "Recent advancements in Large Language Models (LLMs) and the introduction of\nthe Model Context Protocol (MCP) have significantly expanded LLM agents'\ncapability to interact dynamically with external tools and APIs. However,\nexisting tool selection frameworks do not integrate MCP servers, instead\nrelying heavily on error-prone manual updates to monolithic local tool\nrepositories, leading to duplication, inconsistencies, and inefficiencies.\nAdditionally, current approaches abstract tool selection before the LLM agent\nis invoked, limiting its autonomy and hindering dynamic re-querying\ncapabilities during multi-turn interactions. To address these issues, we\nintroduce ScaleMCP, a novel tool selection approach that dynamically equips LLM\nagents with a MCP tool retriever, giving agents the autonomy to add tools into\ntheir memory, as well as an auto-synchronizing tool storage system pipeline\nthrough CRUD (create, read, update, delete) operations with MCP servers as the\nsingle source of truth. We also propose a novel embedding strategy, Tool\nDocument Weighted Average (TDWA), designed to selectively emphasize critical\ncomponents of tool documents (e.g. tool name or synthetic questions) during the\nembedding process. Comprehensive evaluations conducted on a created dataset of\n5,000 financial metric MCP servers, across 10 LLM models, 5 embedding models,\nand 5 retriever types, demonstrate substantial improvements in tool retrieval\nand agent invocation performance, emphasizing ScaleMCP's effectiveness in\nscalable, dynamic tool selection and invocation."}
{"id": "2505.06418", "pdf": "https://arxiv.org/pdf/2505.06418.pdf", "abs": "https://arxiv.org/abs/2505.06418", "title": "Is your multimodal large language model a good science tutor?", "authors": ["Ming Liu", "Liwen Wang", "Wensheng Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Multimodal large language models (MLLMs) demonstrate impressive performance\non scientific reasoning tasks (e.g., ScienceQA). However, most existing\nbenchmarks focus narrowly on the accuracy of the final answer while ignoring\nother metrics. In particular, when applying MLLMs to educational contexts, the\ngoal is not only correctness but also the ability to teach. In this paper, we\npropose a framework that evaluates MLLMs as science tutors using a\ncomprehensive educational rubric and a simulated student model that judges the\nteaching performance of the tutors. Given a list of candidate MLLM science\ntutors, we use rubric-based student judgments to produce a range of tutor\nperformance scores, identifying both strong and weak tutors. Using the training\nsection of the ScienceQA dataset, we then construct a data set of pairwise\ncomparisons between the outputs of strong and weak tutors. This enables us to\napply multiple preference optimization methods to fine-tune an underperforming\ntutor model (Qwen2-VL-2B) into more effective ones. Our results also show that\nstrong problem-solving skills do not guarantee high-quality tutoring and that\nperformance optimization-guided refinements can yield more educationally\naligned tutor models. This approach opens avenues for building MLLMs that serve\nnot only as problem solvers, but as genuinely helpful educational assistants."}
{"id": "2505.06496", "pdf": "https://arxiv.org/pdf/2505.06496.pdf", "abs": "https://arxiv.org/abs/2505.06496", "title": "xGen-small Technical Report", "authors": ["Erik Nijkamp", "Bo Pang", "Egor Pakhomov", "Akash Gokul", "Jin Qu", "Silvio Savarese", "Yingbo Zhou", "Caiming Xiong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce xGen-small, a family of 4B and 9B Transformer decoder models\noptimized for long-context applications. Our vertically integrated pipeline\nunites domain-balanced, frequency-aware data curation; multi-stage pre-training\nwith quality annealing and length extension to 128k tokens; and targeted\npost-training via supervised fine-tuning, preference learning, and online\nreinforcement learning. xGen-small delivers strong performance across various\ntasks, especially in math and coding domains, while excelling at long context\nbenchmarks."}
{"id": "2505.06538", "pdf": "https://arxiv.org/pdf/2505.06538.pdf", "abs": "https://arxiv.org/abs/2505.06538", "title": "Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model", "authors": ["Xinyue Lou", "You Li", "Jinan Xu", "Xiangyu Shi", "Chi Chen", "Kaiyu Huang"], "categories": ["cs.CL"], "comment": "Work in Progress", "summary": "The rapid development of multimodal large reasoning models (MLRMs) has\ndemonstrated broad application potential, yet their safety and reliability\nremain critical concerns that require systematic exploration. To address this\ngap, we conduct a comprehensive and systematic safety evaluation of 11 MLRMs\nacross 5 benchmarks and unveil prevalent safety degradation phenomena in most\nadvanced models. Moreover, our analysis reveals distinct safety patterns across\ndifferent benchmarks: significant safety degradation is observed across\njailbreak robustness benchmarks, whereas safety-awareness benchmarks\ndemonstrate less pronounced degradation. In particular, a long thought process\nin some scenarios even enhances safety performance. Therefore, it is a\npotential approach to addressing safety issues in MLRMs by leveraging the\nintrinsic reasoning capabilities of the model to detect unsafe intent. To\noperationalize this insight, we construct a multimodal tuning dataset that\nincorporates a safety-oriented thought process. Experimental results from\nfine-tuning existing MLRMs with this dataset effectively enhances the safety on\nboth jailbreak robustness and safety-awareness benchmarks. This study provides\na new perspective for developing safe MLRMs. Our dataset is available at\nhttps://github.com/xinyuelou/Think-in-Safety."}
{"id": "2505.06386", "pdf": "https://arxiv.org/pdf/2505.06386.pdf", "abs": "https://arxiv.org/abs/2505.06386", "title": "Embedding Atlas: Low-Friction, Interactive Embedding Visualization", "authors": ["Donghao Ren", "Fred Hohman", "Halden Lin", "Dominik Moritz"], "categories": ["cs.HC", "cs.LG"], "comment": "Website: https://apple.github.io/embedding-atlas/", "summary": "Embedding projections are popular for visualizing large datasets and models.\nHowever, people often encounter \"friction\" when using embedding visualization\ntools: (1) barriers to adoption, e.g., tedious data wrangling and loading,\nscalability limits, no integration of results into existing workflows, and (2)\nlimitations in possible analyses, without integration with external tools to\nadditionally show coordinated views of metadata. In this paper, we present\nEmbedding Atlas, a scalable, interactive visualization tool designed to make\ninteracting with large embeddings as easy as possible. Embedding Atlas uses\nmodern web technologies and advanced algorithms -- including density-based\nclustering, and automated labeling -- to provide a fast and rich data analysis\nexperience at scale. We evaluate Embedding Atlas with a competitive analysis\nagainst other popular embedding tools, showing that Embedding Atlas's feature\nset specifically helps reduce friction, and report a benchmark on its real-time\nrendering performance with millions of points. Embedding Atlas is available as\nopen source to support future work in embedding-based analysis."}
{"id": "2505.06548", "pdf": "https://arxiv.org/pdf/2505.06548.pdf", "abs": "https://arxiv.org/abs/2505.06548", "title": "REFINE-AF: A Task-Agnostic Framework to Align Language Models via Self-Generated Instructions using Reinforcement Learning from Automated Feedback", "authors": ["Aniruddha Roy", "Pretam Ray", "Abhilash Nandy", "Somak Aditya", "Pawan Goyal"], "categories": ["cs.CL"], "comment": "11 pages", "summary": "Instruction-based Large Language Models (LLMs) have proven effective in\nnumerous few-shot or zero-shot Natural Language Processing (NLP) tasks.\nHowever, creating human-annotated instruction data is time-consuming,\nexpensive, and often limited in quantity and task diversity. Previous research\nendeavors have attempted to address this challenge by proposing frameworks\ncapable of generating instructions in a semi-automated and task-agnostic manner\ndirectly from the model itself. Many of these efforts have relied on large\nAPI-only parameter-based models such as GPT-3.5 (175B), which are expensive,\nand subject to limits on a number of queries. This paper explores the\nperformance of three open-source small LLMs such as LLaMA 2-7B, LLama 2-13B,\nand Mistral 7B, using a semi-automated framework, thereby reducing human\nintervention, effort, and cost required to generate an instruction dataset for\nfine-tuning LLMs. Furthermore, we demonstrate that incorporating a\nReinforcement Learning (RL) based training algorithm into this LLMs-based\nframework leads to further enhancements. Our evaluation of the dataset reveals\nthat these RL-based frameworks achieve a substantial improvements in 63-66% of\nthe tasks compared to previous approaches."}
{"id": "2505.06428", "pdf": "https://arxiv.org/pdf/2505.06428.pdf", "abs": "https://arxiv.org/abs/2505.06428", "title": "What Do People Want to Know About Artificial Intelligence (AI)? The Importance of Answering End-User Questions to Explain Autonomous Vehicle (AV) Decisions", "authors": ["Somayeh Molaei", "Lionel P. Robert", "Nikola Banovic"], "categories": ["cs.HC", "cs.AI"], "comment": "Accepted to the Proceedings of the ACM on Human-Computer Interaction,\n  CSCW, October 2025", "summary": "Improving end-users' understanding of decisions made by autonomous vehicles\n(AVs) driven by artificial intelligence (AI) can improve utilization and\nacceptance of AVs. However, current explanation mechanisms primarily help AI\nresearchers and engineers in debugging and monitoring their AI systems, and may\nnot address the specific questions of end-users, such as passengers, about AVs\nin various scenarios. In this paper, we conducted two user studies to\ninvestigate questions that potential AV passengers might pose while riding in\nan AV and evaluate how well answers to those questions improve their\nunderstanding of AI-driven AV decisions. Our initial formative study identified\na range of questions about AI in autonomous driving that existing explanation\nmechanisms do not readily address. Our second study demonstrated that\ninteractive text-based explanations effectively improved participants'\ncomprehension of AV decisions compared to simply observing AV decisions. These\nfindings inform the design of interactions that motivate end-users to engage\nwith and inquire about the reasoning behind AI-driven AV decisions."}
{"id": "2505.06552", "pdf": "https://arxiv.org/pdf/2505.06552.pdf", "abs": "https://arxiv.org/abs/2505.06552", "title": "References Indeed Matter? Reference-Free Preference Optimization for Conversational Query Reformulation", "authors": ["Doyoung Kim", "Youngjun Lee", "Joeun Kim", "Jihwan Bang", "Hwanjun Song", "Susik Yoon", "Jae-Gil Lee"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Conversational query reformulation (CQR) has become indispensable for\nimproving retrieval in dialogue-based applications. However, existing\napproaches typically rely on reference passages for optimization, which are\nimpractical to acquire in real-world scenarios. To address this limitation, we\nintroduce a novel reference-free preference optimization framework DualReform\nthat generates pseudo reference passages from commonly-encountered\nconversational datasets containing only queries and responses. DualReform\nattains this goal through two key innovations: (1) response-based inference,\nwhere responses serve as proxies to infer pseudo reference passages, and (2)\nresponse refinement via the dual-role of CQR, where a CQR model refines\nresponses based on the shared objectives between response refinement and CQR.\nDespite not relying on reference passages, DualReform achieves 96.9--99.1% of\nthe retrieval accuracy attainable only with reference passages and surpasses\nthe state-of-the-art method by up to 31.6%."}
{"id": "2505.06620", "pdf": "https://arxiv.org/pdf/2505.06620.pdf", "abs": "https://arxiv.org/abs/2505.06620", "title": "Integrating Explainable AI in Medical Devices: Technical, Clinical and Regulatory Insights and Recommendations", "authors": ["Dima Alattal", "Asal Khoshravan Azar", "Puja Myles", "Richard Branson", "Hatim Abdulhussein", "Allan Tucker"], "categories": ["cs.HC", "cs.AI", "H.5.2"], "comment": "47 pages", "summary": "There is a growing demand for the use of Artificial Intelligence (AI) and\nMachine Learning (ML) in healthcare, particularly as clinical decision support\nsystems to assist medical professionals. However, the complexity of many of\nthese models, often referred to as black box models, raises concerns about\ntheir safe integration into clinical settings as it is difficult to understand\nhow they arrived at their predictions. This paper discusses insights and\nrecommendations derived from an expert working group convened by the UK\nMedicine and Healthcare products Regulatory Agency (MHRA). The group consisted\nof healthcare professionals, regulators, and data scientists, with a primary\nfocus on evaluating the outputs from different AI algorithms in clinical\ndecision-making contexts. Additionally, the group evaluated findings from a\npilot study investigating clinicians' behaviour and interaction with AI methods\nduring clinical diagnosis. Incorporating AI methods is crucial for ensuring the\nsafety and trustworthiness of medical AI devices in clinical settings. Adequate\ntraining for stakeholders is essential to address potential issues, and further\ninsights and recommendations for safely adopting AI systems in healthcare\nsettings are provided."}
{"id": "2505.06569", "pdf": "https://arxiv.org/pdf/2505.06569.pdf", "abs": "https://arxiv.org/abs/2505.06569", "title": "MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG", "authors": ["Woosang Lim", "Zekun Li", "Gyuwan Kim", "Sungyoung Ji", "HyeonJung Kim", "Kyuri Choi", "Jin Hyuk Lim", "Kyungpyo Park", "William Yang Wang"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Long-context (LC) Large Language Models (LLMs) combined with\nRetrieval-Augmented Generation (RAG) hold strong potential for complex\nmulti-hop and large-document tasks. However, existing RAG systems often suffer\nfrom imprecise retrieval, incomplete context coverage under constrained context\nwindows, and fragmented information caused by suboptimal context construction.\nWe introduce Multi-scale Adaptive Context RAG (MacRAG), a hierarchical\nretrieval framework that compresses and partitions documents into\ncoarse-to-fine granularities, then adaptively merges relevant contexts through\nchunk- and document-level expansions in real time. By starting from the\nfinest-level retrieval and progressively incorporating higher-level and broader\ncontext, MacRAG constructs effective query-specific long contexts, optimizing\nboth precision and coverage. Evaluations on the challenging LongBench\nexpansions of HotpotQA, 2WikiMultihopQA, and Musique confirm that MacRAG\nconsistently surpasses baseline RAG pipelines on single- and multi-step\ngeneration with Llama-3.1-8B, Gemini-1.5-pro, and GPT-4o. Our results establish\nMacRAG as an efficient, scalable solution for real-world long-context,\nmulti-hop reasoning. Our code is available at\nhttps://github.com/Leezekun/MacRAG."}
{"id": "2505.06661", "pdf": "https://arxiv.org/pdf/2505.06661.pdf", "abs": "https://arxiv.org/abs/2505.06661", "title": "Centralized Trust in Decentralized Systems: Unveiling Hidden Contradictions in Blockchain and Cryptocurrency", "authors": ["Faisal Haque Bappy", "EunJeong Cheon", "Tariqul Islam"], "categories": ["cs.HC", "cs.CR"], "comment": null, "summary": "Blockchain technology promises to democratize finance and promote social\nequity through decentralization, but questions remain about whether current\nimplementations advance or hinder these goals. Through a mixed-methods study\ncombining semi-structured interviews with 13 diverse blockchain stakeholders\nand analysis of over 3,000 cryptocurrency discussions on Reddit, we examine how\ntrust manifests in cryptocurrency ecosystems despite their decentralized\narchitecture. Our findings uncover that users actively seek out and create\ncentralized trust anchors, such as established exchanges, prominent community\nfigures, and recognized development teams, contradicting blockchain's\nfundamental promise of trustless interactions. We identify how this\ncontradiction arises from users' mental need for accountability and their\nreluctance to shoulder the full responsibility of self-custody. The study also\nreveals how these centralized trust patterns disproportionately impact\ndifferent user groups, with newer and less technical users showing stronger\npreferences for centralized intermediaries. This work contributes to our\nunderstanding of the inherent tensions between theoretical decentralization and\npractical implementation in cryptocurrency systems, highlighting the persistent\nrole of centralized trust in supposedly trustless environments."}
{"id": "2505.06591", "pdf": "https://arxiv.org/pdf/2505.06591.pdf", "abs": "https://arxiv.org/abs/2505.06591", "title": "Evaluating LLM-Generated Q&A Test: a Student-Centered Study", "authors": ["Anna Wróblewska", "Bartosz Grabek", "Jakub Świstak", "Daniel Dan"], "categories": ["cs.CL", "cs.HC"], "comment": "accepted to AIED 2025", "summary": "This research prepares an automatic pipeline for generating reliable\nquestion-answer (Q&A) tests using AI chatbots. We automatically generated a\nGPT-4o-mini-based Q&A test for a Natural Language Processing course and\nevaluated its psychometric and perceived-quality metrics with students and\nexperts. A mixed-format IRT analysis showed that the generated items exhibit\nstrong discrimination and appropriate difficulty, while student and expert star\nratings reflect high overall quality. A uniform DIF check identified two items\nfor review. These findings demonstrate that LLM-generated assessments can match\nhuman-authored tests in psychometric performance and user satisfaction,\nillustrating a scalable approach to AI-assisted assessment development."}
{"id": "2505.06676", "pdf": "https://arxiv.org/pdf/2505.06676.pdf", "abs": "https://arxiv.org/abs/2505.06676", "title": "VTutor: An Animated Pedagogical Agent SDK that Provide Real Time Multi-Model Feedback", "authors": ["Eason Chen", "Chenyu Lin", "Yu-Kai Huang", "Xinyi Tang", "Aprille Xi", "Jionghao Lin", "Kenneth Koedinger"], "categories": ["cs.HC"], "comment": null, "summary": "Pedagogical Agents (PAs) show significant potential for boosting student\nengagement and learning outcomes by providing adaptive, on-demand support in\neducational contexts. However, existing PA solutions are often hampered by\npre-scripted dialogue, unnatural animations, uncanny visual realism, and high\ndevelopment costs. To address these gaps, we introduce VTutor, an open-source\nSDK leveraging lightweight WebGL, Unity, and JavaScript frameworks. VTutor\nreceives text outputs from a large language model (LLM), converts them into\naudio via text-to-speech, and then renders a real-time, lip-synced pedagogical\nagent (PA) for immediate, large-scale deployment on web-based learning\nplatforms. By providing on-demand, personalized feedback, VTutor strengthens\nstudents' motivation and deepens their engagement with instructional material.\nUsing an anime-like aesthetic, VTutor alleviates the uncanny valley effect,\nallowing learners to engage with expressive yet comfortably stylized\ncharacters. Our evaluation with 50 participants revealed that VTutor\nsignificantly outperforms the existing talking-head approaches (e.g.,\nSadTalker) on perceived synchronization accuracy, naturalness, emotional\nexpressiveness, and overall preference. As an open-source project, VTutor\nwelcomes community-driven contributions - from novel character designs to\nspecialized showcases of pedagogical agent applications - that fuel ongoing\ninnovation in AI-enhanced education. By providing an accessible, customizable,\nand learner-centered PA solution, VTutor aims to elevate human-AI interaction\nexperience in education fields, ultimately broadening the impact of AI in\nlearning contexts. The demo link to VTutor is at\nhttps://vtutor-aied25.vercel.app."}
{"id": "2505.06594", "pdf": "https://arxiv.org/pdf/2505.06594.pdf", "abs": "https://arxiv.org/abs/2505.06594", "title": "Integrating Video and Text: A Balanced Approach to Multimodal Summary Generation and Evaluation", "authors": ["Galann Pennec", "Zhengyuan Liu", "Nicholas Asher", "Philippe Muller", "Nancy F. Chen"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) often struggle to balance visual and textual\ninformation when summarizing complex multimodal inputs, such as entire TV show\nepisodes. In this paper, we propose a zero-shot video-to-text summarization\napproach that builds its own screenplay representation of an episode,\neffectively integrating key video moments, dialogue, and character information\ninto a unified document. Unlike previous approaches, we simultaneously generate\nscreenplays and name the characters in zero-shot, using only the audio, video,\nand transcripts as input. Additionally, we highlight that existing\nsummarization metrics can fail to assess the multimodal content in summaries.\nTo address this, we introduce MFactSum, a multimodal metric that evaluates\nsummaries with respect to both vision and text modalities. Using MFactSum, we\nevaluate our screenplay summaries on the SummScreen3D dataset, demonstrating\nsuperiority against state-of-the-art VLMs such as Gemini 1.5 by generating\nsummaries containing 20% more relevant visual information while requiring 75%\nless of the video as input."}
{"id": "2505.06702", "pdf": "https://arxiv.org/pdf/2505.06702.pdf", "abs": "https://arxiv.org/abs/2505.06702", "title": "Do Language Model Agents Align with Humans in Rating Visualizations? An Empirical Study", "authors": ["Zekai Shao", "Yi Shan", "Yixuan He", "Yuxuan Yao", "Junhong Wang", "Xiaolong", "Zhang", "Yu Zhang", "Siming Chen"], "categories": ["cs.HC"], "comment": "14 pages, 8 figures", "summary": "Large language models encode knowledge in various domains and demonstrate the\nability to understand visualizations. They may also capture visualization\ndesign knowledge and potentially help reduce the cost of formative studies.\nHowever, it remains a question whether large language models are capable of\npredicting human feedback on visualizations. To investigate this question, we\nconducted three studies to examine whether large model-based agents can\nsimulate human ratings in visualization tasks. The first study, replicating a\npublished study involving human subjects, shows agents are promising in\nconducting human-like reasoning and rating, and its result guides the\nsubsequent experimental design. The second study repeated six human-subject\nstudies reported in literature on subjective ratings, but replacing human\nparticipants with agents. Consulting with five human experts, this study\ndemonstrates that the alignment of agent ratings with human ratings positively\ncorrelates with the confidence levels of the experts before the experiments.\nThe third study tests commonly used techniques for enhancing agents, including\npreprocessing visual and textual inputs, and knowledge injection. The results\nreveal the issues of these techniques in robustness and potential induction of\nbiases. The three studies indicate that language model-based agents can\npotentially simulate human ratings in visualization experiments, provided that\nthey are guided by high-confidence hypotheses from expert evaluators.\nAdditionally, we demonstrate the usage scenario of swiftly evaluating\nprototypes with agents. We discuss insights and future directions for\nevaluating and improving the alignment of agent ratings with human ratings. We\nnote that simulation may only serve as complements and cannot replace user\nstudies."}
{"id": "2505.06599", "pdf": "https://arxiv.org/pdf/2505.06599.pdf", "abs": "https://arxiv.org/abs/2505.06599", "title": "Bridging the Gap: An Intermediate Language for Enhanced and Cost-Effective Grapheme-to-Phoneme Conversion with Homographs with Multiple Pronunciations Disambiguation", "authors": ["Abbas Bertina", "Shahab Beirami", "Hossein Biniazian", "Elham Esmaeilnia", "Soheil Shahi", "Mahdi Pirnia"], "categories": ["cs.CL", "I.2.7; I.2.6"], "comment": "pdf, 8 pages, 4 figures, 4 tables", "summary": "Grapheme-to-phoneme (G2P) conversion for Persian presents unique challenges\ndue to its complex phonological features, particularly homographs and Ezafe,\nwhich exist in formal and informal language contexts. This paper introduces an\nintermediate language specifically designed for Persian language processing\nthat addresses these challenges through a multi-faceted approach. Our\nmethodology combines two key components: Large Language Model (LLM) prompting\ntechniques and a specialized sequence-to-sequence machine transliteration\narchitecture. We developed and implemented a systematic approach for\nconstructing a comprehensive lexical database for homographs with multiple\npronunciations disambiguation often termed polyphones, utilizing formal concept\nanalysis for semantic differentiation. We train our model using two distinct\ndatasets: the LLM-generated dataset for formal and informal Persian and the\nB-Plus podcasts for informal language variants. The experimental results\ndemonstrate superior performance compared to existing state-of-the-art\napproaches, particularly in handling the complexities of Persian phoneme\nconversion. Our model significantly improves Phoneme Error Rate (PER) metrics,\nestablishing a new benchmark for Persian G2P conversion accuracy. This work\ncontributes to the growing research in low-resource language processing and\nprovides a robust solution for Persian text-to-speech systems and demonstrating\nits applicability beyond Persian. Specifically, the approach can extend to\nlanguages with rich homographic phenomena such as Chinese and Arabic"}
{"id": "2505.06947", "pdf": "https://arxiv.org/pdf/2505.06947.pdf", "abs": "https://arxiv.org/abs/2505.06947", "title": "The Wisdom of Agent Crowds: A Human-AI Interaction Innovation Ignition Framework", "authors": ["Senhao Yang", "Qiwen Cheng", "Ruiqi Ma", "Liangzhe Zhao", "Zhenying Wu", "Guangqiang Yu"], "categories": ["cs.HC", "cs.MA", "I.2.7; J.4"], "comment": null, "summary": "With the widespread application of large AI models in various fields, the\nautomation level of multi-agent systems has been continuously improved.\nHowever, in high-risk decision-making scenarios such as healthcare and finance,\nhuman participation and the alignment of intelligent systems with human\nintentions remain crucial. This paper focuses on the financial scenario and\nconstructs a multi-agent brainstorming framework based on the BDI theory. A\nhuman-computer collaborative multi-agent financial analysis process is built\nusing Streamlit. The system plans tasks according to user intentions, reduces\nusers' cognitive load through real-time updated structured text summaries and\nthe interactive Cothinker module, and reasonably integrates general and\nreasoning large models to enhance the ability to handle complex problems. By\ndesigning a quantitative analysis algorithm for the sentiment tendency of\ninterview content based on LLMs and a method for evaluating the diversity of\nideas generated by LLMs in brainstorming based on k-means clustering and\ninformation entropy, the system is comprehensively evaluated. The results of\nhuman factors testing show that the system performs well in terms of usability\nand user experience. Although there is still room for improvement, it can\neffectively support users in completing complex financial tasks. The research\nshows that the system significantly improves the efficiency of human-computer\ninteraction and the quality of decision-making in financial decision-making\nscenarios, providing a new direction for the development of related fields."}
{"id": "2505.06605", "pdf": "https://arxiv.org/pdf/2505.06605.pdf", "abs": "https://arxiv.org/abs/2505.06605", "title": "Using External knowledge to Enhanced PLM for Semantic Matching", "authors": ["Min Li", "Chun Yuan"], "categories": ["cs.CL"], "comment": null, "summary": "Modeling semantic relevance has always been a challenging and critical task\nin natural language processing. In recent years, with the emergence of massive\namounts of annotated data, it has become feasible to train complex models, such\nas neural network-based reasoning models. These models have shown excellent\nperformance in practical applications and have achieved the current\nstate-ofthe-art performance. However, even with such large-scale annotated\ndata, we still need to think: Can machines learn all the knowledge necessary to\nperform semantic relevance detection tasks based on this data alone? If not,\nhow can neural network-based models incorporate external knowledge into\nthemselves, and how can relevance detection models be constructed to make full\nuse of external knowledge? In this paper, we use external knowledge to enhance\nthe pre-trained semantic relevance discrimination model. Experimental results\non 10 public datasets show that our method achieves consistent improvements in\nperformance compared to the baseline model."}
{"id": "2505.07020", "pdf": "https://arxiv.org/pdf/2505.07020.pdf", "abs": "https://arxiv.org/abs/2505.07020", "title": "R-CAGE: A Structural Model for Emotion Output Design in Human-AI Interaction", "authors": ["Suyeon Choi"], "categories": ["cs.HC", "cs.AI", "cs.CY", "H.5.2"], "comment": "theory-only preprint. Independent research", "summary": "This paper presents R-CAGE (Rhythmic Control Architecture for Guarding Ego),\na theoretical framework for restructuring emotional output in long-term\nhuman-AI interaction. While prior affective computing approaches emphasized\nexpressiveness, immersion, and responsiveness, they often neglected the\ncognitive and structural consequences of repeated emotional engagement. R-CAGE\ninstead conceptualizes emotional output not as reactive expression but as\nethical design structure requiring architectural intervention. The model is\ngrounded in experiential observations of subtle affective symptoms such as\nlocalized head tension, interpretive fixation, and emotional lag arising from\nprolonged interaction with affective AI systems. These indicate a mismatch\nbetween system-driven emotion and user interpretation that cannot be fully\nexplained by biometric data or observable behavior. R-CAGE adopts a\nuser-centered stance prioritizing psychological recovery, interpretive\nautonomy, and identity continuity. The framework consists of four control\nblocks: (1) Control of Rhythmic Expression regulates output pacing to reduce\nfatigue; (2) Architecture of Sensory Structuring adjusts intensity and timing\nof affective stimuli; (3) Guarding of Cognitive Framing reduces semantic\npressure to allow flexible interpretation; (4) Ego-Aligned Response Design\nsupports self-reference recovery during interpretive lag. By structurally\nregulating emotional rhythm, sensory intensity, and interpretive affordances,\nR-CAGE frames emotion not as performative output but as sustainable design\nunit. The goal is to protect users from oversaturation and cognitive overload\nwhile sustaining long-term interpretive agency in AI-mediated environments."}
{"id": "2505.06607", "pdf": "https://arxiv.org/pdf/2505.06607.pdf", "abs": "https://arxiv.org/abs/2505.06607", "title": "Boosting Neural Language Inference via Cascaded Interactive Reasoning", "authors": ["Min Li", "Chun Yuan"], "categories": ["cs.CL"], "comment": null, "summary": "Natural Language Inference (NLI) focuses on ascertaining the logical\nrelationship (entailment, contradiction, or neutral) between a given premise\nand hypothesis. This task presents significant challenges due to inherent\nlinguistic features such as diverse phrasing, semantic complexity, and\ncontextual nuances. While Pre-trained Language Models (PLMs) built upon the\nTransformer architecture have yielded substantial advancements in NLI,\nprevailing methods predominantly utilize representations from the terminal\nlayer. This reliance on final-layer outputs may overlook valuable information\nencoded in intermediate layers, potentially limiting the capacity to model\nintricate semantic interactions effectively. Addressing this gap, we introduce\nthe Cascaded Interactive Reasoning Network (CIRN), a novel architecture\ndesigned for deeper semantic comprehension in NLI. CIRN implements a\nhierarchical feature extraction strategy across multiple network depths,\noperating within an interactive space where cross-sentence information is\ncontinuously integrated. This mechanism aims to mimic a process of progressive\nreasoning, transitioning from surface-level feature matching to uncovering more\nprofound logical and semantic connections between the premise and hypothesis.\nBy systematically mining latent semantic relationships at various\nrepresentational levels, CIRN facilitates a more thorough understanding of the\ninput pair. Comprehensive evaluations conducted on several standard NLI\nbenchmark datasets reveal consistent performance gains achieved by CIRN over\ncompetitive baseline approaches, demonstrating the efficacy of leveraging\nmulti-level interactive features for complex relational reasoning."}
{"id": "2505.07064", "pdf": "https://arxiv.org/pdf/2505.07064.pdf", "abs": "https://arxiv.org/abs/2505.07064", "title": "ParaView-MCP: An Autonomous Visualization Agent with Direct Tool Use", "authors": ["Shusen Liu", "Haichao Miao", "Peer-Timo Bremer"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "While powerful and well-established, tools like ParaView present a steep\nlearning curve that discourages many potential users. This work introduces\nParaView-MCP, an autonomous agent that integrates modern multimodal large\nlanguage models (MLLMs) with ParaView to not only lower the barrier to entry\nbut also augment ParaView with intelligent decision support. By leveraging the\nstate-of-the-art reasoning, command execution, and vision capabilities of\nMLLMs, ParaView-MCP enables users to interact with ParaView through natural\nlanguage and visual inputs. Specifically, our system adopted the Model Context\nProtocol (MCP) - a standardized interface for model-application communication -\nthat facilitates direct interaction between MLLMs with ParaView's Python API to\nallow seamless information exchange between the user, the language model, and\nthe visualization tool itself. Furthermore, by implementing a visual feedback\nmechanism that allows the agent to observe the viewport, we unlock a range of\nnew capabilities, including recreating visualizations from examples,\nclosed-loop visualization parameter updates based on user-defined goals, and\neven cross-application collaboration involving multiple tools. Broadly, we\nbelieve such an agent-driven visualization paradigm can profoundly change the\nway we interact with visualization tools. We expect a significant uptake in the\ndevelopment of such visualization tools, in both visualization research and\nindustry."}
{"id": "2505.06624", "pdf": "https://arxiv.org/pdf/2505.06624.pdf", "abs": "https://arxiv.org/abs/2505.06624", "title": "The Efficiency of Pre-training with Objective Masking in Pseudo Labeling for Semi-Supervised Text Classification", "authors": ["Arezoo Hatefi", "Xuan-Son Vu", "Monowar Bhuyan", "Frank Drewes"], "categories": ["cs.CL"], "comment": null, "summary": "We extend and study a semi-supervised model for text classification proposed\nearlier by Hatefi et al. for classification tasks in which document classes are\ndescribed by a small number of gold-labeled examples, while the majority of\ntraining examples is unlabeled. The model leverages the teacher-student\narchitecture of Meta Pseudo Labels in which a ''teacher'' generates labels for\noriginally unlabeled training data to train the ''student'' and updates its own\nmodel iteratively based on the performance of the student on the gold-labeled\nportion of the data. We extend the original model of Hatefi et al. by an\nunsupervised pre-training phase based on objective masking, and conduct\nin-depth performance evaluations of the original model, our extension, and\nvarious independent baselines. Experiments are performed using three different\ndatasets in two different languages (English and Swedish)."}
{"id": "2505.07069", "pdf": "https://arxiv.org/pdf/2505.07069.pdf", "abs": "https://arxiv.org/abs/2505.07069", "title": "HeedVision: Attention Awareness in Collaborative Immersive Analytics Environments", "authors": ["Arvind Srinivasan", "Niklas Elmqvist"], "categories": ["cs.HC"], "comment": null, "summary": "Group awareness--the ability to perceive the activities of collaborators in a\nshared space--is a vital mechanism to support effective coordination and joint\ndata analysis in collaborative visualization. We introduce collaborative\nattention-aware visualizations (CAAVs) that track, record, and revisualize the\ncollective attention of multiple users over time. We implement this concept in\nHeedVision, a standards-compliant WebXR system that runs on modern AR/VR\nheadsets. Through a user study where pairs of analysts performed visual search\ntasks in HeedVision, we demonstrate how attention revisualization enhances\ncollaborative performance in immersive analytics. Our findings reveal that\nCAAVs substantially improve spatial coordination, search efficiency, and task\nload distribution among collaborators. This work extends attention awareness\nfrom individual to multi-user settings and provides empirical evidence for its\nbenefits in collaborative immersive analytics."}
{"id": "2505.06630", "pdf": "https://arxiv.org/pdf/2505.06630.pdf", "abs": "https://arxiv.org/abs/2505.06630", "title": "Dynamic Domain Information Modulation Algorithm for Multi-domain Sentiment Analysis", "authors": ["Chunyi Yue", "Ang Li"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "17 pages, 5 figures, 3 tables", "summary": "Multi-domain sentiment classification aims to mitigate poor performance\nmodels due to the scarcity of labeled data in a single domain, by utilizing\ndata labeled from various domains. A series of models that jointly train domain\nclassifiers and sentiment classifiers have demonstrated their advantages,\nbecause domain classification helps generate necessary information for\nsentiment classification. Intuitively, the importance of sentiment\nclassification tasks is the same in all domains for multi-domain sentiment\nclassification; but domain classification tasks are different because the\nimpact of domain information on sentiment classification varies across\ndifferent fields; this can be controlled through adjustable weights or hyper\nparameters. However, as the number of domains increases, existing\nhyperparameter optimization algorithms may face the following challenges: (1)\ntremendous demand for computing resources, (2) convergence problems, and (3)\nhigh algorithm complexity. To efficiently generate the domain information\nrequired for sentiment classification in each domain, we propose a dynamic\ninformation modulation algorithm. Specifically, the model training process is\ndivided into two stages. In the first stage, a shared hyperparameter, which\nwould control the proportion of domain classification tasks across all fields,\nis determined. In the second stage, we introduce a novel domain-aware\nmodulation algorithm to adjust the domain information contained in the input\ntext, which is then calculated based on a gradient-based and loss-based method.\nIn summary, experimental results on a public sentiment analysis dataset\ncontaining 16 domains prove the superiority of the proposed method."}
{"id": "2505.07110", "pdf": "https://arxiv.org/pdf/2505.07110.pdf", "abs": "https://arxiv.org/abs/2505.07110", "title": "DeepSORT-Driven Visual Tracking Approach for Gesture Recognition in Interactive Systems", "authors": ["Tong Zhang", "Fenghua Shao", "Runsheng Zhang", "Yifan Zhuang", "Liuqingqing Yang"], "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "Based on the DeepSORT algorithm, this study explores the application of\nvisual tracking technology in intelligent human-computer interaction,\nespecially in the field of gesture recognition and tracking. With the rapid\ndevelopment of artificial intelligence and deep learning technology,\nvisual-based interaction has gradually replaced traditional input devices and\nbecome an important way for intelligent systems to interact with users. The\nDeepSORT algorithm can achieve accurate target tracking in dynamic environments\nby combining Kalman filters and deep learning feature extraction methods. It is\nespecially suitable for complex scenes with multi-target tracking and fast\nmovements. This study experimentally verifies the superior performance of\nDeepSORT in gesture recognition and tracking. It can accurately capture and\ntrack the user's gesture trajectory and is superior to traditional tracking\nmethods in terms of real-time and accuracy. In addition, this study also\ncombines gesture recognition experiments to evaluate the recognition ability\nand feedback response of the DeepSORT algorithm under different gestures (such\nas sliding, clicking, and zooming). The experimental results show that DeepSORT\ncan not only effectively deal with target occlusion and motion blur but also\ncan stably track in a multi-target environment, achieving a smooth user\ninteraction experience. Finally, this paper looks forward to the future\ndevelopment direction of intelligent human-computer interaction systems based\non visual tracking and proposes future research focuses such as algorithm\noptimization, data fusion, and multimodal interaction in order to promote a\nmore intelligent and personalized interactive experience. Keywords-DeepSORT,\nvisual tracking, gesture recognition, human-computer interaction"}
{"id": "2505.06633", "pdf": "https://arxiv.org/pdf/2505.06633.pdf", "abs": "https://arxiv.org/abs/2505.06633", "title": "Attention Is Not All You Need: The Importance of Feedforward Networks in Transformer Models", "authors": ["Isaac Gerber"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Decoder-only transformer networks have become incredibly popular for language\nmodeling tasks. State-of-the-art models can have over a hundred transformer\nblocks, containing billions of trainable parameters, and are trained on\ntrillions of tokens of text. Each transformer block typically consists of a\nmulti-head attention (MHA) mechanism and a two-layer fully connected\nfeedforward network (FFN). In this paper, we examine the importance of the FFN\nduring the model pre-training process through a series of experiments,\nconfirming that the FFN is important to model performance. Furthermore, we show\nthat models using a transformer block configuration with three-layer FFNs with\nfewer such blocks outperform the standard two-layer configuration delivering\nlower training loss with fewer total parameters in less time."}
{"id": "2505.07142", "pdf": "https://arxiv.org/pdf/2505.07142.pdf", "abs": "https://arxiv.org/abs/2505.07142", "title": "Exploring Anthropomorphism in Conversational Agents for Environmental Sustainability", "authors": ["Mathyas Giudici", "Samuele Scherini", "Pascal Chaussumier", "Stefano Ginocchio", "Franca Garzotto"], "categories": ["cs.HC"], "comment": null, "summary": "The paper investigates the integration of Large Language Models (LLMs) into\nConversational Agents (CAs) to encourage a shift in consumption patterns from a\ndemand-driven to a supply-based paradigm. Specifically, the research examines\nthe role of anthropomorphic design in delivering environmentally conscious\nmessages by comparing two CA designs: a personified agent representing an\nappliance and a traditional, non-personified assistant. A lab study (N=26)\nassessed the impact of these designs on interaction, perceived self-efficacy,\nand engagement. Results indicate that LLM-based CAs significantly enhance\nusers' self-reported eco-friendly behaviors, with participants expressing\ngreater confidence in managing energy consumption. While the anthropomorphic\ndesign did not notably affect self-efficacy, those interacting with the\npersonified agent reported a stronger sense of connection with the system.\nThese findings suggest that although anthropomorphic CAs may improve user\nengagement, both designs hold promise for fostering sustainable behaviors in\nhome energy management."}
{"id": "2505.06660", "pdf": "https://arxiv.org/pdf/2505.06660.pdf", "abs": "https://arxiv.org/abs/2505.06660", "title": "TS-SUPERB: A Target Speech Processing Benchmark for Speech Self-Supervised Learning Models", "authors": ["Junyi Peng", "Takanori Ashihara", "Marc Delcroix", "Tsubasa Ochiai", "Oldrich Plchot", "Shoko Araki", "Jan Černocký"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at ICASSP 2025", "summary": "Self-supervised learning (SSL) models have significantly advanced speech\nprocessing tasks, and several benchmarks have been proposed to validate their\neffectiveness. However, previous benchmarks have primarily focused on\nsingle-speaker scenarios, with less exploration of target-speaker tasks in\nnoisy, multi-talker conditions -- a more challenging yet practical case. In\nthis paper, we introduce the Target-Speaker Speech Processing Universal\nPerformance Benchmark (TS-SUPERB), which includes four widely recognized\ntarget-speaker processing tasks that require identifying the target speaker and\nextracting information from the speech mixture. In our benchmark, the speaker\nembedding extracted from enrollment speech is used as a clue to condition\ndownstream models. The benchmark result reveals the importance of evaluating\nSSL models in target speaker scenarios, demonstrating that performance cannot\nbe easily inferred from related single-speaker tasks. Moreover, by using a\nunified SSL-based target speech encoder, consisting of a speaker encoder and an\nextractor module, we also investigate joint optimization across TS tasks to\nleverage mutual information and demonstrate its effectiveness."}
{"id": "2505.07154", "pdf": "https://arxiv.org/pdf/2505.07154.pdf", "abs": "https://arxiv.org/abs/2505.07154", "title": "Assessing the User Experience of Extended Reality Devices for (Dis)Assembly: A Classroom Study", "authors": ["Brandon S. Byers", "Eleftherios Triantafyllidis", "Thibaut Menny", "Martin Schulte", "Catherine De Wolf"], "categories": ["cs.HC"], "comment": null, "summary": "Despite the current rise and promising capabilities of Extended Reality (XR)\ntechnologies, the architecture, engineering, and construction industry lacks\ninformed guidance when choosing between these technologies, especially for\ncomplex processes like assembly and disassembly tasks. This research compares\nthe user experience across different XR devices for (dis)assembly utilizing the\nNASA Task Load Index and System Usability Scale metrics. Through a workshop and\nsurveys with graduate civil engineering and architecture students, the study\nfound that Augmented Reality scored highest in usability, followed closely by\nMixed Reality. However, Mixed Reality showed the best task load index score,\nindicating low cognitive demand. The findings presented in this research may\naid academics and practitioners in making informed decisions when selecting XR\nsystems in practical, real-world assembly scenarios. Moreover, this study\nsuggests opportunities and guidelines for more detailed XR system comparisons\nand exploration of XR's further role in circular construction practices."}
{"id": "2505.06696", "pdf": "https://arxiv.org/pdf/2505.06696.pdf", "abs": "https://arxiv.org/abs/2505.06696", "title": "Enhancing BERTopic with Intermediate Layer Representations", "authors": ["Dominik Koterwa", "Maciej Świtała"], "categories": ["cs.CL"], "comment": "Repository with code for reproduction:\n  https://github.com/dkoterwa/optimizing_bertopic", "summary": "BERTopic is a topic modeling algorithm that leverages transformer-based\nembeddings to create dense clusters, enabling the estimation of topic\nstructures and the extraction of valuable insights from a corpus of documents.\nThis approach allows users to efficiently process large-scale text data and\ngain meaningful insights into its structure. While BERTopic is a powerful tool,\nembedding preparation can vary, including extracting representations from\nintermediate model layers and applying transformations to these embeddings. In\nthis study, we evaluate 18 different embedding representations and present\nfindings based on experiments conducted on three diverse datasets. To assess\nthe algorithm's performance, we report topic coherence and topic diversity\nmetrics across all experiments. Our results demonstrate that, for each dataset,\nit is possible to find an embedding configuration that performs better than the\ndefault setting of BERTopic. Additionally, we investigate the influence of stop\nwords on different embedding configurations."}
{"id": "2505.07214", "pdf": "https://arxiv.org/pdf/2505.07214.pdf", "abs": "https://arxiv.org/abs/2505.07214", "title": "Towards user-centered interactive medical image segmentation in VR with an assistive AI agent", "authors": ["Pascal Spiegler", "Arash Harirpoush", "Yiming Xiao"], "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": null, "summary": "Crucial in disease analysis and surgical planning, manual segmentation of\nvolumetric medical scans (e.g. MRI, CT) is laborious, error-prone, and\nchallenging to master, while fully automatic algorithms can benefit from\nuser-feedback. Therefore, with the complementary power of the latest\nradiological AI foundation models and virtual reality (VR)'s intuitive data\ninteraction, we propose SAMIRA, a novel conversational AI agent that assists\nusers with localizing, segmenting, and visualizing 3D medical concepts in VR.\nThrough speech-based interaction, the agent helps users understand radiological\nfeatures, locate clinical targets, and generate segmentation masks that can be\nrefined with just a few point prompts. The system also supports true-to-scale\n3D visualization of segmented pathology to enhance patient-specific anatomical\nunderstanding. Furthermore, to determine the optimal interaction paradigm under\nnear-far attention-switching for refining segmentation masks in an immersive,\nhuman-in-the-loop workflow, we compare VR controller pointing, head pointing,\nand eye tracking as input modes. With a user study, evaluations demonstrated a\nhigh usability score (SUS=90.0 $\\pm$ 9.0), low overall task load, as well as\nstrong support for the proposed VR system's guidance, training potential, and\nintegration of AI in radiological segmentation tasks."}
{"id": "2505.06698", "pdf": "https://arxiv.org/pdf/2505.06698.pdf", "abs": "https://arxiv.org/abs/2505.06698", "title": "From Rankings to Insights: Evaluation Should Shift Focus from Leaderboard to Feedback", "authors": ["Zongqi Wang", "Tianle Gu", "Chen Gong", "Xin Tian", "Siqi Bao", "Yujiu Yang"], "categories": ["cs.CL"], "comment": null, "summary": "Automatic evaluation benchmarks such as MT-Bench, Arena-Hard, and Auto-Arena\nare seeing growing adoption for the evaluation of Large Language Models (LLMs).\nExisting research has primarily focused on approximating human-based model\nrankings using limited data and LLM-as-a-Judge. However, the fundamental\npremise of these studies, which attempts to replicate human rankings, is\nflawed. Specifically, these benchmarks typically offer only overall scores,\nlimiting their utility to leaderboard rankings, rather than providing feedback\nthat can guide model optimization and support model profiling. Therefore, we\nadvocate for an evaluation paradigm shift from approximating human-based model\nrankings to providing feedback with analytical value. To this end, we introduce\nFeedbacker, an evaluation framework that provides comprehensive and\nfine-grained results, thereby enabling thorough identification of a model's\nspecific strengths and weaknesses. Such feedback not only supports the targeted\noptimization of the model but also enhances the understanding of its behavior.\nFeedbacker comprises three key components: an extensible tree-based query\ntaxonomy builder, an automated query synthesis scheme, and a suite of\nvisualization and analysis tools. Furthermore, we propose a novel\nLLM-as-a-Judge method: PC2 (Pre-Comparison-derived Criteria) pointwise\nevaluation. This method derives evaluation criteria by pre-comparing the\ndifferences between several auxiliary responses, achieving the accuracy of\npairwise evaluation while maintaining the time complexity of pointwise\nevaluation. Finally, leveraging the evaluation results of 17 mainstream LLMs,\nwe demonstrate the usage of Feedbacker and highlight its effectiveness and\npotential. Our homepage project is available at\nhttps://liudan193.github.io/Feedbacker."}
{"id": "2505.07282", "pdf": "https://arxiv.org/pdf/2505.07282.pdf", "abs": "https://arxiv.org/abs/2505.07282", "title": "A Turing Test for ''Localness'': Conceptualizing, Defining, and Recognizing Localness in People and Machines", "authors": ["Zihan Gao", "Justin Cranshaw", "Jacob Thebault-Spieker"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "As digital platforms increasingly mediate interactions tied to place,\nensuring genuine local participation is essential for maintaining trust and\ncredibility in location-based services, community-driven platforms, and civic\nengagement systems. However, localness is a social and relational identity\nshaped by knowledge, participation, and community recognition. Drawing on the\nGerman philosopher Heidegger's concept of dwelling -- which extends beyond\nphysical presence to encompass meaningful connection to place -- we investigate\nhow people conceptualize and evaluate localness in both human and artificial\nagents. Using a chat-based interaction paradigm inspired by Turing's Imitation\nGame and Von Ahn's Games With A Purpose, we engaged 230 participants in\nconversations designed to examine the cues people rely on to assess local\npresence. Our findings reveal a multi-dimensional framework of localness,\nhighlighting differences in how locals and nonlocals emphasize various aspects\nof local identity. We show that people are significantly more accurate in\nrecognizing locals than nonlocals, suggesting that localness is an affirmative\nstatus requiring active demonstration rather than merely the absence of\nnonlocal traits. Additionally, we identify conditions under which artificial\nagents are perceived as local and analyze participants' sensemaking strategies\nin evaluating localness. Through predictive modeling, we determine key factors\nthat drive accurate localness judgments. By bridging theoretical perspectives\non human-place relationships with practical challenges in digital environments,\nour work informs the design of location-based services that foster meaningful\nlocal engagement. Our findings contribute to a broader understanding of\nlocalness as a dynamic and relational construct, reinforcing the importance of\ndwelling as a process of belonging, recognition, and engagement with place."}
{"id": "2505.06708", "pdf": "https://arxiv.org/pdf/2505.06708.pdf", "abs": "https://arxiv.org/abs/2505.06708", "title": "Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free", "authors": ["Zihan Qiu", "Zekun Wang", "Bo Zheng", "Zeyu Huang", "Kaiyue Wen", "Songlin Yang", "Rui Men", "Le Yu", "Fei Huang", "Suozhi Huang", "Dayiheng Liu", "Jingren Zhou", "Junyang Lin"], "categories": ["cs.CL"], "comment": null, "summary": "Gating mechanisms have been widely utilized, from early models like LSTMs and\nHighway Networks to recent state space models, linear attention, and also\nsoftmax attention. Yet, existing literature rarely examines the specific\neffects of gating. In this work, we conduct comprehensive experiments to\nsystematically investigate gating-augmented softmax attention variants.\nSpecifically, we perform a comprehensive comparison over 30 variants of 15B\nMixture-of-Experts (MoE) models and 1.7B dense models trained on a 3.5 trillion\ntoken dataset. Our central finding is that a simple modification-applying a\nhead-specific sigmoid gate after the Scaled Dot-Product Attention\n(SDPA)-consistently improves performance. This modification also enhances\ntraining stability, tolerates larger learning rates, and improves scaling\nproperties. By comparing various gating positions and computational variants,\nwe attribute this effectiveness to two key factors: (1) introducing\nnon-linearity upon the low-rank mapping in the softmax attention, and (2)\napplying query-dependent sparse gating scores to modulate the SDPA output.\nNotably, we find this sparse gating mechanism mitigates 'attention sink' and\nenhances long-context extrapolation performance, and we also release related\n$\\href{https://github.com/qiuzh20/gated_attention}{codes}$ and\n$\\href{https://huggingface.co/QwQZh/gated_attention}{models}$ to facilitate\nfuture research."}
{"id": "2505.07326", "pdf": "https://arxiv.org/pdf/2505.07326.pdf", "abs": "https://arxiv.org/abs/2505.07326", "title": "User Identification with LFI-Based Eye Movement Data Using Time and Frequency Domain Features", "authors": ["Suleyman Ozdel", "Johannes Meyer", "Yasmeen Abdrabou", "Enkelejda Kasneci"], "categories": ["cs.HC"], "comment": "Accepted to the 25th International Conference on Digital Signal\n  Processing (DSP 2025)", "summary": "Laser interferometry (LFI)-based eye-tracking systems provide an alternative\nto traditional camera-based solutions, offering improved privacy by eliminating\nthe risk of direct visual identification. However, the high-frequency signals\ncaptured by LFI-based trackers may still contain biometric information that\nenables user identification. This study investigates user identification from\nraw high-frequency LFI-based eye movement data by analyzing features extracted\nfrom both the time and frequency domains. Using velocity and distance\nmeasurements without requiring direct gaze data, we develop a multi-class\nclassification model to accurately distinguish between individuals across\nvarious activities. Our results demonstrate that even without direct visual\ncues, eye movement patterns exhibit sufficient uniqueness for user\nidentification, achieving 93.14% accuracy and a 2.52% EER with 5-second windows\nacross both static and dynamic tasks. Additionally, we analyze the impact of\nsampling rate and window size on model performance, providing insights into the\nfeasibility of LFI-based biometric recognition. Our findings demonstrate the\nnovel potential of LFI-based eye-tracking for user identification, highlighting\nboth its promise for secure authentication and emerging privacy risks. This\nwork paves the way for further research into high-frequency eye movement data."}
{"id": "2505.06782", "pdf": "https://arxiv.org/pdf/2505.06782.pdf", "abs": "https://arxiv.org/abs/2505.06782", "title": "Utilizing LLMs to Investigate the Disputed Role of Evidence in Electronic Cigarette Health Policy Formation in Australia and the UK", "authors": ["Damian Curran", "Brian Chapman", "Mike Conway"], "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Australia and the UK have developed contrasting approaches to the regulation\nof electronic cigarettes, with - broadly speaking - Australia adopting a\nrelatively restrictive approach and the UK adopting a more permissive approach.\nNotably, these divergent policies were developed from the same broad evidence\nbase. In this paper, to investigate differences in how the two jurisdictions\nmanage and present evidence, we developed and evaluated a Large Language\nModel-based sentence classifier to perform automated analyses of electronic\ncigarette-related policy documents drawn from official Australian and UK\nlegislative processes (109 documents in total). Specifically, we utilized GPT-4\nto automatically classify sentences based on whether they contained claims that\ne-cigarettes were broadly helpful or harmful for public health. Our LLM-based\nclassifier achieved an F-score of 0.9. Further, when applying the classifier to\nour entire sentence-level corpus, we found that Australian legislative\ndocuments show a much higher proportion of harmful statements, and a lower\nproportion of helpful statements compared to the expected values, with the\nopposite holding for the UK. In conclusion, this work utilized an LLM-based\napproach to provide evidence to support the contention that - drawing on the\nsame evidence base - Australian ENDS-related policy documents emphasize the\nharms associated with ENDS products and UK policy documents emphasize the\nbenefits. Further, our approach provides a starting point for using LLM-based\nmethods to investigate the complex relationship between evidence and health\npolicy formation."}
{"id": "2505.07340", "pdf": "https://arxiv.org/pdf/2505.07340.pdf", "abs": "https://arxiv.org/abs/2505.07340", "title": "Thalamus: A User Simulation Toolkit for Prototyping Multimodal Sensing Studies", "authors": ["Kayhan Latifzadeh", "Luis A. Leiva"], "categories": ["cs.HC"], "comment": null, "summary": "Conducting user studies that involve physiological and behavioral\nmeasurements is very time-consuming and expensive, as it not only involves a\ncareful experiment design, device calibration, etc. but also a careful software\ntesting. We propose Thalamus, a software toolkit for collecting and simulating\nmultimodal signals that can help the experimenters to prepare in advance for\nunexpected situations before reaching out to the actual study participants and\neven before having to install or purchase a specific device. Among other\nfeatures, Thalamus allows the experimenter to modify, synchronize, and\nbroadcast physiological signals (as coming from various data streams) from\ndifferent devices simultaneously and not necessarily located in the same place.\nThalamus is cross-platform, cross-device, and simple to use, making it thus a\nvaluable asset for HCI research."}
{"id": "2505.06862", "pdf": "https://arxiv.org/pdf/2505.06862.pdf", "abs": "https://arxiv.org/abs/2505.06862", "title": "A Split-then-Join Approach to Abstractive Summarization for Very Long Documents in a Low Resource Setting", "authors": ["Lhuqita Fazry"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "$\\texttt{BIGBIRD-PEGASUS}$ model achieves $\\textit{state-of-the-art}$ on\nabstractive text summarization for long documents. However it's capacity still\nlimited to maximum of $4,096$ tokens, thus caused performance degradation on\nsummarization for very long documents. Common method to deal with the issue is\nto truncate the documents. In this reasearch, we'll use different approach.\nWe'll use the pretrained $\\texttt{BIGBIRD-PEGASUS}$ model by fine tuned the\nmodel on other domain dataset. First, we filter out all documents which length\nless than $20,000$ tokens to focus on very long documents. To prevent domain\nshifting problem and overfitting on transfer learning due to small dataset, we\naugment the dataset by splitting document-summary training pair into parts, to\nfit the document into $4,096$ tokens. Source code available on\n$\\href{https://github.com/lhfazry/SPIN-summ}{https://github.com/lhfazry/SPIN-summ}$."}
{"id": "2505.07354", "pdf": "https://arxiv.org/pdf/2505.07354.pdf", "abs": "https://arxiv.org/abs/2505.07354", "title": "Time Perception in Virtual Reality: Effects of Emotional Valence and Stress Level", "authors": ["Kyriaki Syrigou", "Marina Stoforou", "Panagiotis Kourtesis"], "categories": ["cs.HC", "H.5.2; J.3; J.4; K.4"], "comment": "23 Pages, 10 Figures, 2 Tables", "summary": "Background & Objective: Emotional states and stress distort time perception,\nyet findings are inconsistent, particularly in immersive media. Integrating the\nAttentional Gate Model (AGM) and Internal Clock Model (ICM), we examined how\nemotional valence and stress alter perceived duration in Virtual Reality (VR).\nThis study assesses the effects of valence (calming, neutral, stressful) and\nstress (low/high) on prospective time estimation, mood, and arousal. Methods:\nFifty-four adults (18-39 years) explored three custom VR environments: (1) a\ntranquil Japanese garden, (2) an affectively neutral room, and (3) a\nthreatening underground sewer. Active navigation promoted presence; a\ndistraction task separated conditions. Valence and arousal were assessed with\nthe Visual Analog Mood Scales, stress with the Perceived Stress Scale-10\n(PSS-10), and perceived duration with a verbal estimation task. Mixed-model\nANOVAs evaluated main and interaction effects. Results: Valence reliably shaped\nperceived duration: calming VR led to overestimation, stressful VR to\nunderestimation, and neutral VR to intermediate timing. Baseline stress level,\nas measured by PSS-10, neither altered timing nor interacted with valence.\nNevertheless, the VR environments affected VAMS' mood metrics: calming\nenvironments elevated mood and reduced perceived stress, whereas stressful\nenvironments lowered mood and heightened stress. Conclusions: Findings support\nthe AGM-attentionally demanding negative environments shorten perceived\ntime-and the ICM-valence-linked arousal speeds or slows the pacemaker. Contrary\nto classical predictions, in VR, baseline stress did not distort duration,\nsuggesting valence-driven attentional allocation outweighs pre-exposure stress\nlevels. VR offers a controllable platform for dissecting time-perception\nmechanisms and advancing interventions that target emotion-related temporal\ndistortions."}
{"id": "2505.06889", "pdf": "https://arxiv.org/pdf/2505.06889.pdf", "abs": "https://arxiv.org/abs/2505.06889", "title": "IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method", "authors": ["Mihyeon Kim", "Juhyoung Park", "Youngbin Kim"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP 2024 Main", "summary": "Pre-trained Language Models (PLMs) have achieved remarkable performance on\ndiverse NLP tasks through pre-training and fine-tuning. However, fine-tuning\nthe model with a large number of parameters on limited downstream datasets\noften leads to vulnerability to adversarial attacks, causing overfitting of the\nmodel on standard datasets.\n  To address these issues, we propose IM-BERT from the perspective of a dynamic\nsystem by conceptualizing a layer of BERT as a solution of Ordinary\nDifferential Equations (ODEs). Under the situation of initial value\nperturbation, we analyze the numerical stability of two main numerical ODE\nsolvers: the explicit and implicit Euler approaches.\n  Based on these analyses, we introduce a numerically robust IM-connection\nincorporating BERT's layers. This strategy enhances the robustness of PLMs\nagainst adversarial attacks, even in low-resource scenarios, without\nintroducing additional parameters or adversarial training strategies.\n  Experimental results on the adversarial GLUE (AdvGLUE) dataset validate the\nrobustness of IM-BERT under various conditions. Compared to the original BERT,\nIM-BERT exhibits a performance improvement of approximately 8.3\\%p on the\nAdvGLUE dataset. Furthermore, in low-resource scenarios, IM-BERT outperforms\nBERT by achieving 5.9\\%p higher accuracy."}
{"id": "2505.07377", "pdf": "https://arxiv.org/pdf/2505.07377.pdf", "abs": "https://arxiv.org/abs/2505.07377", "title": "Examining the Role of LLM-Driven Interactions on Attention and Cognitive Engagement in Virtual Classrooms", "authors": ["Suleyman Ozdel", "Can Sarpkaya", "Efe Bozkir", "Hong Gao", "Enkelejda Kasneci"], "categories": ["cs.HC", "cs.AI"], "comment": "Accepted to EDM 2025 (Eighteenth International Conference on\n  Educational Data Mining)", "summary": "Transforming educational technologies through the integration of large\nlanguage models (LLMs) and virtual reality (VR) offers the potential for\nimmersive and interactive learning experiences. However, the effects of LLMs on\nuser engagement and attention in educational environments remain open\nquestions. In this study, we utilized a fully LLM-driven virtual learning\nenvironment, where peers and teachers were LLM-driven, to examine how students\nbehaved in such settings. Specifically, we investigate how peer question-asking\nbehaviors influenced student engagement, attention, cognitive load, and\nlearning outcomes and found that, in conditions where LLM-driven peer learners\nasked questions, students exhibited more targeted visual scanpaths, with their\nattention directed toward the learning content, particularly in complex\nsubjects. Our results suggest that peer questions did not introduce extraneous\ncognitive load directly, as the cognitive load is strongly correlated with\nincreased attention to the learning material. Considering these findings, we\nprovide design recommendations for optimizing VR learning spaces."}
{"id": "2505.06904", "pdf": "https://arxiv.org/pdf/2505.06904.pdf", "abs": "https://arxiv.org/abs/2505.06904", "title": "EcoLANG: Efficient and Effective Agent Communication Language Induction for Social Simulation", "authors": ["Xinyi Mou", "Chen Qian", "Wei Liu", "Xuanjing Huang", "Zhongyu Wei"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) have demonstrated an impressive ability to\nrole-play humans and replicate complex social dynamics. While large-scale\nsocial simulations are gaining increasing attention, they still face\nsignificant challenges, particularly regarding high time and computation costs.\nExisting solutions, such as distributed mechanisms or hybrid agent-based model\n(ABM) integrations, either fail to address inference costs or compromise\naccuracy and generalizability. To this end, we propose EcoLANG: Efficient and\nEffective Agent Communication Language Induction for Social Simulation. EcoLANG\noperates in two stages: (1) language evolution, where we filter synonymous\nwords and optimize sentence-level rules through natural selection, and (2)\nlanguage utilization, where agents in social simulations communicate using the\nevolved language. Experimental results demonstrate that EcoLANG reduces token\nconsumption by over 20%, enhancing efficiency without sacrificing simulation\naccuracy."}
{"id": "2505.07486", "pdf": "https://arxiv.org/pdf/2505.07486.pdf", "abs": "https://arxiv.org/abs/2505.07486", "title": "Shots and Boosters: Exploring the Use of Combined Prebunking Interventions to Raise Critical Thinking and Create Long-Term Protection Against Misinformation", "authors": ["Huiyun Tang", "Anastasia Sergeeva"], "categories": ["cs.HC"], "comment": "Presented at the 2025 ACM Workshop on Human-AI Interaction for\n  Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING", "summary": "The problem of how to effectively mitigate the flow of misinformation remains\na significant challenge. The classical approach to this is public disapproval\nof claims or \"debunking.\" The approach is still widely used on social media,\nbut it has some severe limitations in terms of applicability and efficiency. An\nalternative strategy is to enhance individuals' critical thinking through\neducational interventions. Instead of merely disproving misinformation, these\napproaches aim to strengthen users' reasoning skills, enabling them to evaluate\nand reject false information independently. In this position paper, we explore\na combination of intervention methods designed to improve critical thinking in\nthe context of online media consumption. We highlight the role of AI in\nsupporting different stages of these interventions and present a design concept\nthat integrates AI-driven strategies to foster critical reasoning and media\nliteracy."}
{"id": "2505.06914", "pdf": "https://arxiv.org/pdf/2505.06914.pdf", "abs": "https://arxiv.org/abs/2505.06914", "title": "The Distracting Effect: Understanding Irrelevant Passages in RAG", "authors": ["Chen Amiraz", "Florin Cuconasu", "Simone Filice", "Zohar Karnin"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "A well-known issue with Retrieval Augmented Generation (RAG) is that\nretrieved passages that are irrelevant to the query sometimes distract the\nanswer-generating LLM, causing it to provide an incorrect response. In this\npaper, we shed light on this core issue and formulate the distracting effect of\na passage w.r.t. a query (and an LLM). We provide a quantifiable measure of the\ndistracting effect of a passage and demonstrate its robustness across LLMs.\n  Our research introduces novel methods for identifying and using hard\ndistracting passages to improve RAG systems. By fine-tuning LLMs with these\ncarefully selected distracting passages, we achieve up to a 7.5% increase in\nanswering accuracy compared to counterparts fine-tuned on conventional RAG\ndatasets. Our contribution is two-fold: first, we move beyond the simple binary\nclassification of irrelevant passages as either completely unrelated vs.\ndistracting, and second, we develop and analyze multiple methods for finding\nhard distracting passages. To our knowledge, no other research has provided\nsuch a comprehensive framework for identifying and utilizing hard distracting\npassages."}
{"id": "2505.07498", "pdf": "https://arxiv.org/pdf/2505.07498.pdf", "abs": "https://arxiv.org/abs/2505.07498", "title": "Design Requirements for Patient-Centered Digital Health Applications: Supporting Patients' Values in Postoperative Delirium Prevention", "authors": ["David Leimstädtner", "Fatima Halzl-Yürek", "Claudia Spies", "Claudia Müller-Birn"], "categories": ["cs.HC"], "comment": null, "summary": "Postoperative delirium (POD) is among the most common complications after\nsurgeries for older adults and can entail long-term adverse health\nconsequences. Active patient participation in POD prevention presents a central\nfactor in reducing these risks. To support patient engagement through a digital\nhealth application, we use value sensitive design approaches to identify the\nrequirements for a patient-centered digital health application supporting\npatient engagement in POD prevention. Through interviews with medical\nprofessionals and patient representatives, we construct a patient journey,\nwhich serves as the basis for twelve patient value journey interviews. In these\ninterviews, patients from the high-risk group for POD revisit their recent\nexperience of undergoing surgery to elicit barriers, needs, and values\nconcerning POD prevention from a patient perspective. An analysis of the\npatient interviews derives four design requirements for a digital health\napplication supporting patients regarding POD prevention: the adaptation of\npatient-centered communication, the provision of procedural transparency,\nfostering patient empowerment through consistent guidance, and explicitly\naddressing relatives as mediators and supporters for a patient after a POD\noccurrence."}
{"id": "2505.06974", "pdf": "https://arxiv.org/pdf/2505.06974.pdf", "abs": "https://arxiv.org/abs/2505.06974", "title": "CNN-based Image Models Verify a Hypothesis that The Writers of Cuneiform Texts Improved Their Writing Skills When Studying at the Age of Hittite Empire", "authors": ["Daichi Kohmoto", "Katsutoshi Fukuda", "Daisuke Yoshida", "Takafumi Matsui", "Sachihiro Omura"], "categories": ["cs.CL"], "comment": "11 pages, 9 figures, 5 tables", "summary": "A cuneiform tablet KBo 23.1 ++/KUB 30.38, which is known to represent a text\nof Kizzuwatna rituals, was written by two writers with almost identical content\nin two iterations. Unlike other cuneiform tablets that contained information\nsuch as myths, essays, or business records, the reason why ancient people left\nsuch tablets for posterity remains unclear. To study this problem, we develop a\nnew methodology by analyzing images of a tablet quantitatively using CNN\n(Convolutional Neural Network)-based image models, without segmenting\ncuneiforms one-by-one. Our data-driven methodology implies that the writer\nwriting the first half was a `teacher' and the other writer was a `student' who\nwas training his skills of writing cuneiforms. This result has not been reached\nby classical linguistics. We also discuss related conclusions and possible\nfurther directions for applying our method and its generalizations."}
{"id": "2505.07534", "pdf": "https://arxiv.org/pdf/2505.07534.pdf", "abs": "https://arxiv.org/abs/2505.07534", "title": "The Human-Data-Model Interaction Canvas for Visual Analytics", "authors": ["Jürgen Bernard"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "7 pages, 5 figures, LaTeX; to appear at the 16th International\n  EuroVis Workshop on Visual Analytics (EuroVA'25) as a position paper", "summary": "Visual Analytics (VA) integrates humans, data, and models as key actors in\ninsight generation and data-driven decision-making. This position paper values\nand reflects on 16 VA process models and frameworks and makes nine high-level\nobservations that motivate a fresh perspective on VA. The contribution is the\nHDMI Canvas, a perspective to VA that complements the strengths of existing VA\nprocess models and frameworks. It systematically characterizes diverse roles of\nhumans, data, and models, and how these actors benefit from and contribute to\nVA processes. The descriptive power of the HDMI Canvas eases the\ndifferentiation between a series of VA building blocks, rather than describing\ngeneral VA principles only. The canvas includes modern human-centered\nmethodologies, including human knowledge externalization and forms of feedback\nloops, while interpretable and explainable AI highlight model contributions\nbeyond their conventional outputs. The HDMI Canvas has generative power,\nguiding the design of new VA processes and is optimized for external\nstakeholders, improving VA outreach, interdisciplinary collaboration, and\nuser-centered design. The utility of the HDMI Canvas is demonstrated through\ntwo preliminary case studies."}
{"id": "2505.06987", "pdf": "https://arxiv.org/pdf/2505.06987.pdf", "abs": "https://arxiv.org/abs/2505.06987", "title": "Convert Language Model into a Value-based Strategic Planner", "authors": ["Xiaoyu Wang", "Yue Zhao", "Qingqing Gu", "Zhonglin Jiang", "Xiaokai Chen", "Yong Chen", "Luo Ji"], "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 5 figures, Accepted by ACL 2025 Industry Track", "summary": "Emotional support conversation (ESC) aims to alleviate the emotional distress\nof individuals through effective conversations. Although large language models\n(LLMs) have obtained remarkable progress on ESC, most of these studies might\nnot define the diagram from the state model perspective, therefore providing a\nsuboptimal solution for long-term satisfaction. To address such an issue, we\nleverage the Q-learning on LLMs, and propose a framework called straQ*. Our\nframework allows a plug-and-play LLM to bootstrap the planning during ESC,\ndetermine the optimal strategy based on long-term returns, and finally guide\nthe LLM to response. Substantial experiments on ESC datasets suggest that\nstraQ* outperforms many baselines, including direct inference, self-refine,\nchain of thought, finetuning, and finite state machines."}
{"id": "2505.07592", "pdf": "https://arxiv.org/pdf/2505.07592.pdf", "abs": "https://arxiv.org/abs/2505.07592", "title": "Decoding Chess Puzzle Play and Standard Cognitive Tasks for BCI: A Low-Cost EEG Study", "authors": ["Matthew Russell", "Samuel Youkeles", "William Xia", "Kenny Zheng", "Aman Shah", "Robert J. K. Jacob"], "categories": ["cs.HC"], "comment": "30 pages, 13 figures", "summary": "While consumer-grade electroencephalography (EEG) devices show promise for\nBrain-Computer Interface (BCI) applications, their efficacy in detecting subtle\ncognitive states remains understudied. Using a combination of established\ncognitive paradigms (N-Back, Stroop, and Mental Rotation) and a novel\necological task (Chess puzzles), we demonstrate successful distinctions of\nworkload levels within some tasks, as well as differentiation between task\ntypes using the MUSE 2 device. With machine learning we further show reliable\npredictive power to differentiate between workload levels in the N-Back task,\nwhile also achieving effective cross-task classification. These findings\ndemonstrate that consumer-grade EEG devices can effectively detect and\ndifferentiate various forms of cognitive workload, and that they can be\nleveraged with some success towards real-time classification distinguishing\nworkload in some tasks, as well as in differentiating between nuanced cognitive\nstates, supporting their potential use in adaptive BCI applications. Research\ncode and data are further provided for future researchers."}
{"id": "2505.07157", "pdf": "https://arxiv.org/pdf/2505.07157.pdf", "abs": "https://arxiv.org/abs/2505.07157", "title": "HAMLET: Healthcare-focused Adaptive Multilingual Learning Embedding-based Topic Modeling", "authors": ["Hajar Sakai", "Sarah S. Lam"], "categories": ["cs.CL"], "comment": null, "summary": "Traditional topic models often struggle with contextual nuances and fail to\nadequately handle polysemy and rare words. This limitation typically results in\ntopics that lack coherence and quality. Large Language Models (LLMs) can\nmitigate this issue by generating an initial set of topics. However, these raw\ntopics frequently lack refinement and representativeness, which leads to\nredundancy without lexical similarity and reduced interpretability. This paper\nintroduces HAMLET, a graph-driven architecture for cross-lingual healthcare\ntopic modeling that uses LLMs. The proposed approach leverages neural-enhanced\nsemantic fusion to refine the embeddings of topics generated by the LLM.\nInstead of relying solely on statistical co-occurrence or human interpretation\nto extract topics from a document corpus, this method introduces a topic\nembedding refinement that uses Bidirectional Encoder Representations from\nTransformers (BERT) and Graph Neural Networks (GNN). After topic generation, a\nhybrid technique that involves BERT and Sentence-BERT (SBERT) is employed for\nembedding. The topic representations are further refined using a GNN, which\nestablishes connections between documents, topics, words, similar topics, and\nsimilar words. A novel method is introduced to compute similarities.\nConsequently, the topic embeddings are refined, and the top k topics are\nextracted. Experiments were conducted using two healthcare datasets, one in\nEnglish and one in French, from which six sets were derived. The results\ndemonstrate the effectiveness of HAMLET."}
{"id": "2505.07736", "pdf": "https://arxiv.org/pdf/2505.07736.pdf", "abs": "https://arxiv.org/abs/2505.07736", "title": "VTutor for High-Impact Tutoring at Scale: Managing Engagement and Real-Time Multi-Screen Monitoring with P2P Connections", "authors": ["Eason Chen", "Xinyi Tang", "Aprille Xi", "Chenyu Lin", "Conrad Borchers", "Shivang Gupta", "Jionghao Lin", "Kenneth R Koedinger"], "categories": ["cs.HC"], "comment": "6 pages", "summary": "Hybrid tutoring, where a human tutor supports multiple students in learning\nwith educational technology, is an increasingly common application to deliver\nhigh-impact tutoring at scale. However, past hybrid tutoring applications are\nlimited in guiding tutor attention to students that require support.\nSpecifically, existing conferencing tools, commonly used in hybrid tutoring, do\nnot allow tutors to monitor multiple students' screens while directly\ncommunicating and attending to multiple students simultaneously. To address\nthis issue, this paper introduces VTutor, a web-based platform leveraging\npeer-to-peer screen sharing and virtual avatars to deliver real-time,\ncontext-aware tutoring feedback at scale. By integrating a multi-student\nmonitoring dashboard with AI-powered avatar prompts, VTutor empowers a single\neducator or tutor to rapidly detect off-task or struggling students and\nintervene proactively, thus enhancing the benefits of one-on-one interactions\nin classroom contexts with several students. Drawing on insight from the\nlearning sciences and past research on animated pedagogical agents, we\ndemonstrate how stylized avatars can potentially sustain student engagement\nwhile accommodating varying infrastructure constraints. Finally, we address\nopen questions on refining large-scale, AI-driven tutoring solutions for\nimproved learner outcomes, and how VTutor could help interpret real-time\nlearner interactions to support remote tutors at scale. The VTutor platform can\nbe accessed at https://ls2025.vtutor.ai. The system demo video is at\nhttps://ls2025.vtutor.ai/video."}
{"id": "2505.07161", "pdf": "https://arxiv.org/pdf/2505.07161.pdf", "abs": "https://arxiv.org/abs/2505.07161", "title": "Towards Actionable Pedagogical Feedback: A Multi-Perspective Analysis of Mathematics Teaching and Tutoring Dialogue", "authors": ["Jannatun Naim", "Jie Cao", "Fareen Tasneem", "Jennifer Jacobs", "Brent Milne", "James Martin", "Tamara Sumner"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted to EDM'2025", "summary": "Effective feedback is essential for refining instructional practices in\nmathematics education, and researchers often turn to advanced natural language\nprocessing (NLP) models to analyze classroom dialogues from multiple\nperspectives. However, utterance-level discourse analysis encounters two\nprimary challenges: (1) multifunctionality, where a single utterance may serve\nmultiple purposes that a single tag cannot capture, and (2) the exclusion of\nmany utterances from domain-specific discourse move classifications, leading to\ntheir omission in feedback. To address these challenges, we proposed a\nmulti-perspective discourse analysis that integrates domain-specific talk moves\nwith dialogue act (using the flattened multi-functional SWBD-MASL schema with\n43 tags) and discourse relation (applying Segmented Discourse Representation\nTheory with 16 relations). Our top-down analysis framework enables a\ncomprehensive understanding of utterances that contain talk moves, as well as\nutterances that do not contain talk moves. This is applied to two mathematics\neducation datasets: TalkMoves (teaching) and SAGA22 (tutoring). Through\ndistributional unigram analysis, sequential talk move analysis, and multi-view\ndeep dive, we discovered meaningful discourse patterns, and revealed the vital\nrole of utterances without talk moves, demonstrating that these utterances, far\nfrom being mere fillers, serve crucial functions in guiding, acknowledging, and\nstructuring classroom discourse. These insights underscore the importance of\nincorporating discourse relations and dialogue acts into AI-assisted education\nsystems to enhance feedback and create more responsive learning environments.\nOur framework may prove helpful for providing human educator feedback, but also\naiding in the development of AI agents that can effectively emulate the roles\nof both educators and students."}
{"id": "2505.06278", "pdf": "https://arxiv.org/pdf/2505.06278.pdf", "abs": "https://arxiv.org/abs/2505.06278", "title": "Robust Understanding of Human-Robot Social Interactions through Multimodal Distillation", "authors": ["Tongfei Bian", "Mathieu Chollet", "Tanaya Guha"], "categories": ["cs.RO", "cs.HC"], "comment": "This paper has been submitted to ACM Multimedia 2025", "summary": "The need for social robots and agents to interact and assist humans is\ngrowing steadily. To be able to successfully interact with humans, they need to\nunderstand and analyse socially interactive scenes from their (robot's)\nperspective. Works that model social situations between humans and agents are\nfew; and even those existing ones are often too computationally intensive to be\nsuitable for deployment in real time or on real world scenarios with limited\navailable information. We propose a robust knowledge distillation framework\nthat models social interactions through various multimodal cues, yet is robust\nagainst incomplete and noisy information during inference. Our teacher model is\ntrained with multimodal input (body, face and hand gestures, gaze, raw images)\nthat transfers knowledge to a student model that relies solely on body pose.\nExtensive experiments on two publicly available human-robot interaction\ndatasets demonstrate that the our student model achieves an average accuracy\ngain of 14.75\\% over relevant baselines on multiple downstream social\nunderstanding task even with up to 51\\% of its input being corrupted. The\nstudent model is highly efficient: it is $<1$\\% in size of the teacher model in\nterms of parameters and uses $\\sim 0.5$\\textperthousand~FLOPs of that in the\nteacher model. Our code will be made public during publication."}
{"id": "2505.07162", "pdf": "https://arxiv.org/pdf/2505.07162.pdf", "abs": "https://arxiv.org/abs/2505.07162", "title": "KDH-MLTC: Knowledge Distillation for Healthcare Multi-Label Text Classification", "authors": ["Hajar Sakai", "Sarah S. Lam"], "categories": ["cs.CL"], "comment": null, "summary": "The increasing volume of healthcare textual data requires computationally\nefficient, yet highly accurate classification approaches able to handle the\nnuanced and complex nature of medical terminology. This research presents\nKnowledge Distillation for Healthcare Multi-Label Text Classification\n(KDH-MLTC), a framework leveraging model compression and Large Language Models\n(LLMs). The proposed approach addresses conventional healthcare Multi-Label\nText Classification (MLTC) challenges by integrating knowledge distillation and\nsequential fine-tuning, subsequently optimized through Particle Swarm\nOptimization (PSO) for hyperparameter tuning. KDH-MLTC transfers knowledge from\na more complex teacher LLM (i.e., BERT) to a lighter student LLM (i.e.,\nDistilBERT) through sequential training adapted to MLTC that preserves the\nteacher's learned information while significantly reducing computational\nrequirements. As a result, the classification is enabled to be conducted\nlocally, making it suitable for healthcare textual data characterized by\nsensitivity and, therefore, ensuring HIPAA compliance. The experiments\nconducted on three medical literature datasets of different sizes, sampled from\nthe Hallmark of Cancer (HoC) dataset, demonstrate that KDH-MLTC achieves\nsuperior performance compared to existing approaches, particularly for the\nlargest dataset, reaching an F1 score of 82.70%. Additionally, statistical\nvalidation and an ablation study are carried out, proving the robustness of\nKDH-MLTC. Furthermore, the PSO-based hyperparameter optimization process\nallowed the identification of optimal configurations. The proposed approach\ncontributes to healthcare text classification research, balancing efficiency\nrequirements in resource-constrained healthcare settings with satisfactory\naccuracy demands."}
{"id": "2505.06291", "pdf": "https://arxiv.org/pdf/2505.06291.pdf", "abs": "https://arxiv.org/abs/2505.06291", "title": "ALFEE: Adaptive Large Foundation Model for EEG Representation", "authors": ["Wei Xiong", "Junming Lin", "Jiangtong Li", "Jie Li", "Changjun Jiang"], "categories": ["eess.SP", "cs.CE", "cs.HC", "cs.LG"], "comment": "17pages, 17 figures", "summary": "While foundation models excel in text, image, and video domains, the critical\nbiological signals, particularly electroencephalography(EEG), remain\nunderexplored. EEG benefits neurological research with its high temporal\nresolution, operational practicality, and safety profile. However, low\nsignal-to-noise ratio, inter-subject variability, and cross-paradigm\ndifferences hinder the generalization of current models. Existing methods often\nemploy simplified strategies, such as a single loss function or a\nchannel-temporal joint representation module, and suffer from a domain gap\nbetween pretraining and evaluation tasks that compromises efficiency and\nadaptability. To address these limitations, we propose the Adaptive Large\nFoundation model for EEG signal representation(ALFEE) framework, a novel hybrid\ntransformer architecture with two learning stages for robust EEG representation\nlearning. ALFEE employs a hybrid attention that separates channel-wise feature\naggregation from temporal dynamics modeling, enabling robust EEG representation\nwith variable channel configurations. A channel encoder adaptively compresses\nvariable channel information, a temporal encoder captures task-guided\nevolution, and a hybrid decoder reconstructs signals in both temporal and\nfrequency domains. During pretraining, ALFEE optimizes task prediction, channel\nand temporal mask reconstruction, and temporal forecasting to enhance\nmulti-scale and multi-channel representation. During fine-tuning, a full-model\nadaptation with a task-specific token dictionary and a cross-attention layer\nboosts performance across multiple tasks. After 25,000 hours of pretraining,\nextensive experimental results on six downstream EEG tasks demonstrate the\nsuperior performance of ALFEE over existing models. Our ALFEE framework\nestablishes a scalable foundation for biological signal analysis with\nimplementation at https://github.com/xw1216/ALFEE."}
{"id": "2505.07184", "pdf": "https://arxiv.org/pdf/2505.07184.pdf", "abs": "https://arxiv.org/abs/2505.07184", "title": "Structural Entropy Guided Agent for Detecting and Repairing Knowledge Deficiencies in LLMs", "authors": ["Yifan Wei", "Xiaoyan Yu", "Tengfei Pan", "Angsheng Li", "Li Du"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved unprecedented performance by\nleveraging vast pretraining corpora, yet their performance remains suboptimal\nin knowledge-intensive domains such as medicine and scientific research, where\nhigh factual precision is required. While synthetic data provides a promising\navenue for augmenting domain knowledge, existing methods frequently generate\nredundant samples that do not align with the model's true knowledge gaps. To\novercome this limitation, we propose a novel Structural Entropy-guided\nKnowledge Navigator (SENATOR) framework that addresses the intrinsic knowledge\ndeficiencies of LLMs. Our approach employs the Structure Entropy (SE) metric to\nquantify uncertainty along knowledge graph paths and leverages Monte Carlo Tree\nSearch (MCTS) to selectively explore regions where the model lacks\ndomain-specific knowledge. Guided by these insights, the framework generates\ntargeted synthetic data for supervised fine-tuning, enabling continuous\nself-improvement. Experimental results on LLaMA-3 and Qwen2 across multiple\ndomain-specific benchmarks show that SENATOR effectively detects and repairs\nknowledge deficiencies, achieving notable performance improvements. The code\nand data for our methods and experiments are available at\nhttps://github.com/weiyifan1023/senator."}
{"id": "2505.06301", "pdf": "https://arxiv.org/pdf/2505.06301.pdf", "abs": "https://arxiv.org/abs/2505.06301", "title": "Domain-Adversarial Anatomical Graph Networks for Cross-User Human Activity Recognition", "authors": ["Xiaozhou Ye", "Kevin I-Kai Wang"], "categories": ["cs.LG", "cs.AI", "cs.HC"], "comment": null, "summary": "Cross-user variability in Human Activity Recognition (HAR) remains a critical\nchallenge due to differences in sensor placement, body dynamics, and behavioral\npatterns. Traditional methods often fail to capture biomechanical invariants\nthat persist across users, limiting their generalization capability. We propose\nan Edge-Enhanced Graph-Based Adversarial Domain Generalization (EEG-ADG)\nframework that integrates anatomical correlation knowledge into a unified graph\nneural network (GNN) architecture. By modeling three biomechanically motivated\nrelationships together-Interconnected Units, Analogous Units, and Lateral\nUnits-our method encodes domain-invariant features while addressing\nuser-specific variability through Variational Edge Feature Extractor. A\nGradient Reversal Layer (GRL) enforces adversarial domain generalization,\nensuring robustness to unseen users. Extensive experiments on OPPORTUNITY and\nDSADS datasets demonstrate state-of-the-art performance. Our work bridges\nbiomechanical principles with graph-based adversarial learning by integrating\ninformation fusion techniques. This fusion of information underpins our unified\nand generalized model for cross-user HAR."}
{"id": "2505.07202", "pdf": "https://arxiv.org/pdf/2505.07202.pdf", "abs": "https://arxiv.org/abs/2505.07202", "title": "On the Cost and Benefits of Training Context with Utterance or Full Conversation Training: A Comparative Stud", "authors": ["Hyouin Liu", "Zhikuan Zhang"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Modern TTS systems designed for conversations achieve high-quality utterances\nbut often remain inaccessible publicly. Are existing open-source architectures\ninadequate, or are current training techniques insufficient? This paper\ninvestigates prominent models and their underlying behaviors regarding\nconversational context. Using 20 GPU-hours on an NVIDIA H100, we empirically\nexamine two approaches: context-based utterance-level training versus full\nconversation training. Results demonstrate that context-based utterance\ntraining achieves superior MOS scores (4.3/5.0 vs 3.7/5.0) and reduces training\ntime by 37%, while full conversation approaches suffer from speaker similarity\nhallucination issues. These findings provide practical guidelines for\nconversational TTS development, favoring utterance-level training with\ncontextual conditioning for both resource efficiency and output quality."}
{"id": "2505.06402", "pdf": "https://arxiv.org/pdf/2505.06402.pdf", "abs": "https://arxiv.org/abs/2505.06402", "title": "Camera Control at the Edge with Language Models for Scene Understanding", "authors": ["Alexiy Buynitsky", "Sina Ehsani", "Bhanu Pallakonda", "Pragyana Mishra"], "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": "7 pages, 6 figures. This work was presented and published at the 11th\n  IEEE International Conference on Control, Automation and Robotics (ICCAR) in\n  2025", "summary": "In this paper, we present Optimized Prompt-based Unified System (OPUS), a\nframework that utilizes a Large Language Model (LLM) to control Pan-Tilt-Zoom\n(PTZ) cameras, providing contextual understanding of natural environments. To\nachieve this goal, the OPUS system improves cost-effectiveness by generating\nkeywords from a high-level camera control API and transferring knowledge from\nlarger closed-source language models to smaller ones through Supervised\nFine-Tuning (SFT) on synthetic data. This enables efficient edge deployment\nwhile maintaining performance comparable to larger models like GPT-4. OPUS\nenhances environmental awareness by converting data from multiple cameras into\ntextual descriptions for language models, eliminating the need for specialized\nsensory tokens. In benchmark testing, our approach significantly outperformed\nboth traditional language model techniques and more complex prompting methods,\nachieving a 35% improvement over advanced techniques and a 20% higher task\naccuracy compared to closed-source models like Gemini Pro. The system\ndemonstrates OPUS's capability to simplify PTZ camera operations through an\nintuitive natural language interface. This approach eliminates the need for\nexplicit programming and provides a conversational method for interacting with\ncamera systems, representing a significant advancement in how users can control\nand utilize PTZ camera technology."}
{"id": "2505.07205", "pdf": "https://arxiv.org/pdf/2505.07205.pdf", "abs": "https://arxiv.org/abs/2505.07205", "title": "Benchmarking Ethical and Safety Risks of Healthcare LLMs in China-Toward Systemic Governance under Healthy China 2030", "authors": ["Mouxiao Bian", "Rongzhao Zhang", "Chao Ding", "Xinwei Peng", "Jie Xu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are poised to transform healthcare under China's\nHealthy China 2030 initiative, yet they introduce new ethical and\npatient-safety challenges. We present a novel 12,000-item Q&A benchmark\ncovering 11 ethics and 9 safety dimensions in medical contexts, to\nquantitatively evaluate these risks. Using this dataset, we assess\nstate-of-the-art Chinese medical LLMs (e.g., Qwen 2.5-32B, DeepSeek), revealing\nmoderate baseline performance (accuracy 42.7% for Qwen 2.5-32B) and significant\nimprovements after fine-tuning on our data (up to 50.8% accuracy). Results show\nnotable gaps in LLM decision-making on ethics and safety scenarios, reflecting\ninsufficient institutional oversight. We then identify systemic governance\nshortfalls-including the lack of fine-grained ethical audit protocols, slow\nadaptation by hospital IRBs, and insufficient evaluation tools-that currently\nhinder safe LLM deployment. Finally, we propose a practical governance\nframework for healthcare institutions (embedding LLM auditing teams, enacting\ndata ethics guidelines, and implementing safety simulation pipelines) to\nproactively manage LLM risks. Our study highlights the urgent need for robust\nLLM governance in Chinese healthcare, aligning AI innovation with patient\nsafety and ethical standards."}
{"id": "2505.06467", "pdf": "https://arxiv.org/pdf/2505.06467.pdf", "abs": "https://arxiv.org/abs/2505.06467", "title": "PromptIQ: Who Cares About Prompts? Let System Handle It -- A Component-Aware Framework for T2I Generation", "authors": ["Nisan Chhetri", "Arpan Sainju"], "categories": ["cs.CV", "cs.HC"], "comment": "4 pages, 2 figures", "summary": "Generating high-quality images without prompt engineering expertise remains a\nchallenge for text-to-image (T2I) models, which often misinterpret poorly\nstructured prompts, leading to distortions and misalignments. While humans\neasily recognize these flaws, metrics like CLIP fail to capture structural\ninconsistencies, exposing a key limitation in current evaluation methods. To\naddress this, we introduce PromptIQ, an automated framework that refines\nprompts and assesses image quality using our novel Component-Aware Similarity\n(CAS) metric, which detects and penalizes structural errors. Unlike\nconventional methods, PromptIQ iteratively generates and evaluates images until\nthe user is satisfied, eliminating trial-and-error prompt tuning. Our results\nshow that PromptIQ significantly improves generation quality and evaluation\naccuracy, making T2I models more accessible for users with little to no prompt\nengineering expertise."}
{"id": "2505.07233", "pdf": "https://arxiv.org/pdf/2505.07233.pdf", "abs": "https://arxiv.org/abs/2505.07233", "title": "DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation", "authors": ["Jiashuo Sun", "Xianrui Zhong", "Sizhe Zhou", "Jiawei Han"], "categories": ["cs.CL", "cs.AI"], "comment": "24 pages, 6 figures, 15 tables", "summary": "Retrieval-augmented generation (RAG) systems combine large language models\n(LLMs) with external knowledge retrieval, making them highly effective for\nknowledge-intensive tasks. A crucial but often under-explored component of\nthese systems is the reranker, which refines retrieved documents to enhance\ngeneration quality and explainability. The challenge of selecting the optimal\nnumber of documents (k) remains unsolved: too few may omit critical\ninformation, while too many introduce noise and inefficiencies. Although recent\nstudies have explored LLM-based rerankers, they primarily leverage internal\nmodel knowledge and overlook the rich supervisory signals that LLMs can\nprovide, such as using response quality as feedback for optimizing reranking\ndecisions. In this paper, we propose DynamicRAG, a novel RAG framework where\nthe reranker dynamically adjusts both the order and number of retrieved\ndocuments based on the query. We model the reranker as an agent optimized\nthrough reinforcement learning (RL), using rewards derived from LLM output\nquality. Across seven knowledge-intensive datasets, DynamicRAG demonstrates\nsuperior performance, achieving state-of-the-art results. The model, data and\ncode are available at https://github.com/GasolSun36/DynamicRAG"}
{"id": "2505.06469", "pdf": "https://arxiv.org/pdf/2505.06469.pdf", "abs": "https://arxiv.org/abs/2505.06469", "title": "KCluster: An LLM-based Clustering Approach to Knowledge Component Discovery", "authors": ["Yumou Wei", "Paulo Carvalho", "John Stamper"], "categories": ["cs.AI", "cs.HC"], "comment": "Accepted to the Educational Data Mining (EDM) 2025 conference", "summary": "Educators evaluate student knowledge using knowledge component (KC) models\nthat map assessment questions to KCs. Still, designing KC models for large\nquestion banks remains an insurmountable challenge for instructors who need to\nanalyze each question by hand. The growing use of Generative AI in education is\nexpected only to aggravate this chronic deficiency of expert-designed KC\nmodels, as course engineers designing KCs struggle to keep up with the pace at\nwhich questions are generated. In this work, we propose KCluster, a novel KC\ndiscovery algorithm based on identifying clusters of congruent questions\naccording to a new similarity metric induced by a large language model (LLM).\nWe demonstrate in three datasets that an LLM can create an effective metric of\nquestion similarity, which a clustering algorithm can use to create KC models\nfrom questions with minimal human effort. Combining the strengths of LLM and\nclustering, KCluster generates descriptive KC labels and discovers KC models\nthat predict student performance better than the best expert-designed models\navailable. In anticipation of future work, we illustrate how KCluster can\nreveal insights into difficult KCs and suggest improvements to instruction."}
{"id": "2505.07247", "pdf": "https://arxiv.org/pdf/2505.07247.pdf", "abs": "https://arxiv.org/abs/2505.07247", "title": "SAS-Bench: A Fine-Grained Benchmark for Evaluating Short Answer Scoring with Large Language Models", "authors": ["Peichao Lai", "Kexuan Zhang", "Yi Lin", "Linyihan Zhang", "Feiyang Ye", "Jinhao Yan", "Yanwei Xu", "Conghui He", "Yilei Wang", "Wentao Zhang", "Bin Cui"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Subjective Answer Grading (SAG) plays a crucial role in education,\nstandardized testing, and automated assessment systems, particularly for\nevaluating short-form responses in Short Answer Scoring (SAS). However,\nexisting approaches often produce coarse-grained scores and lack detailed\nreasoning. Although large language models (LLMs) have demonstrated potential as\nzero-shot evaluators, they remain susceptible to bias, inconsistencies with\nhuman judgment, and limited transparency in scoring decisions. To overcome\nthese limitations, we introduce SAS-Bench, a benchmark specifically designed\nfor LLM-based SAS tasks. SAS-Bench provides fine-grained, step-wise scoring,\nexpert-annotated error categories, and a diverse range of question types\nderived from real-world subject-specific exams. This benchmark facilitates\ndetailed evaluation of model reasoning processes and explainability. We also\nrelease an open-source dataset containing 1,030 questions and 4,109 student\nresponses, each annotated by domain experts. Furthermore, we conduct\ncomprehensive experiments with various LLMs, identifying major challenges in\nscoring science-related questions and highlighting the effectiveness of\nfew-shot prompting in improving scoring accuracy. Our work offers valuable\ninsights into the development of more robust, fair, and educationally\nmeaningful LLM-based evaluation systems."}
{"id": "2505.06591", "pdf": "https://arxiv.org/pdf/2505.06591.pdf", "abs": "https://arxiv.org/abs/2505.06591", "title": "Evaluating LLM-Generated Q&A Test: a Student-Centered Study", "authors": ["Anna Wróblewska", "Bartosz Grabek", "Jakub Świstak", "Daniel Dan"], "categories": ["cs.CL", "cs.HC"], "comment": "accepted to AIED 2025", "summary": "This research prepares an automatic pipeline for generating reliable\nquestion-answer (Q&A) tests using AI chatbots. We automatically generated a\nGPT-4o-mini-based Q&A test for a Natural Language Processing course and\nevaluated its psychometric and perceived-quality metrics with students and\nexperts. A mixed-format IRT analysis showed that the generated items exhibit\nstrong discrimination and appropriate difficulty, while student and expert star\nratings reflect high overall quality. A uniform DIF check identified two items\nfor review. These findings demonstrate that LLM-generated assessments can match\nhuman-authored tests in psychometric performance and user satisfaction,\nillustrating a scalable approach to AI-assisted assessment development."}
{"id": "2505.07258", "pdf": "https://arxiv.org/pdf/2505.07258.pdf", "abs": "https://arxiv.org/abs/2505.07258", "title": "No Query, No Access", "authors": ["Wenqiang Wang", "Siyuan Liang", "Yangshijie Zhang", "Xiaojun Jia", "Hao Lin", "Xiaochun Cao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Textual adversarial attacks mislead NLP models, including Large Language\nModels (LLMs), by subtly modifying text. While effective, existing attacks\noften require knowledge of the victim model, extensive queries, or access to\ntraining data, limiting real-world feasibility. To overcome these constraints,\nwe introduce the \\textbf{Victim Data-based Adversarial Attack (VDBA)}, which\noperates using only victim texts. To prevent access to the victim model, we\ncreate a shadow dataset with publicly available pre-trained models and\nclustering methods as a foundation for developing substitute models. To address\nthe low attack success rate (ASR) due to insufficient information feedback, we\npropose the hierarchical substitution model design, generating substitute\nmodels to mitigate the failure of a single substitute model at the decision\nboundary.\n  Concurrently, we use diverse adversarial example generation, employing\nvarious attack methods to generate and select the adversarial example with\nbetter similarity and attack effectiveness. Experiments on the Emotion and SST5\ndatasets show that VDBA outperforms state-of-the-art methods, achieving an ASR\nimprovement of 52.08\\% while significantly reducing attack queries to 0. More\nimportantly, we discover that VDBA poses a significant threat to LLMs such as\nQwen2 and the GPT family, and achieves the highest ASR of 45.99% even without\naccess to the API, confirming that advanced NLP models still face serious\nsecurity risks. Our codes can be found at\nhttps://anonymous.4open.science/r/VDBA-Victim-Data-based-Adversarial-Attack-36EC/"}
{"id": "2505.06680", "pdf": "https://arxiv.org/pdf/2505.06680.pdf", "abs": "https://arxiv.org/abs/2505.06680", "title": "A Survey on Data-Driven Modeling of Human Drivers' Lane-Changing Decisions", "authors": ["Linxuan Huang", "Dong-Fan Xie", "Li Li", "Zhengbing He"], "categories": ["cs.AI", "cs.HC", "cs.LG", "cs.SY", "eess.SY", "physics.soc-ph"], "comment": null, "summary": "Lane-changing (LC) behavior, a critical yet complex driving maneuver,\nsignificantly influences driving safety and traffic dynamics. Traditional\nanalytical LC decision (LCD) models, while effective in specific environments,\noften oversimplify behavioral heterogeneity and complex interactions, limiting\ntheir capacity to capture real LCD. Data-driven approaches address these gaps\nby leveraging rich empirical data and machine learning to decode latent\ndecision-making patterns, enabling adaptive LCD modeling in dynamic\nenvironments. In light of the rapid development of artificial intelligence and\nthe demand for data-driven models oriented towards connected vehicles and\nautonomous vehicles, this paper presents a comprehensive survey of data-driven\nLCD models, with a particular focus on human drivers LC decision-making. It\nsystematically reviews the modeling framework, covering data sources and\npreprocessing, model inputs and outputs, objectives, structures, and validation\nmethods. This survey further discusses the opportunities and challenges faced\nby data-driven LCD models, including driving safety, uncertainty, as well as\nthe integration and improvement of technical frameworks."}
{"id": "2505.07271", "pdf": "https://arxiv.org/pdf/2505.07271.pdf", "abs": "https://arxiv.org/abs/2505.07271", "title": "On the Robustness of Reward Models for Language Model Alignment", "authors": ["Jiwoo Hong", "Noah Lee", "Eunki Kim", "Guijin Son", "Woojin Chung", "Aman Gupta", "Shao Tang", "James Thorne"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "The Bradley-Terry (BT) model is widely practiced in reward modeling for\nreinforcement learning with human feedback (RLHF). Despite its effectiveness,\nreward models (RMs) trained with BT model loss are prone to over-optimization,\nlosing generalizability to unseen input distributions. In this paper, we study\nthe cause of over-optimization in RM training and its downstream effects on the\nRLHF procedure, accentuating the importance of distributional robustness of RMs\nin unseen data. First, we show that the excessive dispersion of hidden state\nnorms is the main source of over-optimization. Then, we propose batch-wise\nsum-to-zero regularization (BSR) to enforce zero-centered reward sum per batch,\nconstraining the rewards with extreme magnitudes. We assess the impact of BSR\nin improving robustness in RMs through four scenarios of over-optimization,\nwhere BSR consistently manifests better robustness. Subsequently, we compare\nthe plain BT model and BSR on RLHF training and empirically show that robust\nRMs better align the policy to the gold preference model. Finally, we apply BSR\nto high-quality data and models, which surpasses state-of-the-art RMs in the 8B\nscale by adding more than 5% in complex preference prediction tasks. By\nconducting RLOO training with 8B RMs, AlpacaEval 2.0 reduces generation length\nby 40% while adding a 7% increase in win rate, further highlighting that\nrobustness in RMs induces robustness in RLHF training. We release the code,\ndata, and models: https://github.com/LinkedIn-XFACT/RM-Robustness."}
{"id": "2505.07100", "pdf": "https://arxiv.org/pdf/2505.07100.pdf", "abs": "https://arxiv.org/abs/2505.07100", "title": "Navigating the Rashomon Effect: How Personalization Can Help Adjust Interpretable Machine Learning Models to Individual Users", "authors": ["Julian Rosenberger", "Philipp Schröppel", "Sven Kruschel", "Mathias Kraus", "Patrick Zschech", "Maximilian Förster"], "categories": ["cs.LG", "cs.HC"], "comment": "Accepted as a Completed Research Paper at the Thirty-Third European\n  Conference on Information Systems (ECIS 2025), Amman, Jordan", "summary": "The Rashomon effect describes the observation that in machine learning (ML)\nmultiple models often achieve similar predictive performance while explaining\nthe underlying relationships in different ways. This observation holds even for\nintrinsically interpretable models, such as Generalized Additive Models (GAMs),\nwhich offer users valuable insights into the model's behavior. Given the\nexistence of multiple GAM configurations with similar predictive performance, a\nnatural question is whether we can personalize these configurations based on\nusers' needs for interpretability. In our study, we developed an approach to\npersonalize models based on contextual bandits. In an online experiment with\n108 users in a personalized treatment and a non-personalized control group, we\nfound that personalization led to individualized rather than one-size-fits-all\nconfigurations. Despite these individual adjustments, the interpretability\nremained high across both groups, with users reporting a strong understanding\nof the models. Our research offers initial insights into the potential for\npersonalizing interpretable ML."}
{"id": "2505.07289", "pdf": "https://arxiv.org/pdf/2505.07289.pdf", "abs": "https://arxiv.org/abs/2505.07289", "title": "Semantic Retention and Extreme Compression in LLMs: Can We Have Both?", "authors": ["Stanislas Laborde", "Martin Cousseau", "Antoun Yaacoub", "Lionel Prevost"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68P30 (Primary) 68T07, 68T50 (Secondary)", "I.2.6; I.5.1; I.2.7"], "comment": "Accepted for publication in the Proceedings of the 2025 International\n  Joint Conference on Neural Networks (IJCNN); this arXiv version includes an\n  appendix with 6 result tables; 10 pages, 15 figures, 7 tables", "summary": "The exponential growth in Large Language Model (LLM) deployment has\nintensified the need for efficient model compression techniques to reduce\ncomputational and memory costs. While pruning and quantization have shown\npromise, their combined potential remains largely unexplored. In this paper, we\nexamine joint compression and how strategically combining pruning and\nquantization could yield superior performance-to-compression ratios compared to\nsingle-method approaches. Recognizing the challenges in accurately assessing\nLLM performance, we address key limitations of previous evaluation frameworks\nand introduce the Semantic Retention Compression Rate (SrCr), a novel metric\nthat quantifies the trade-off between model compression and semantic\npreservation, facilitating the optimization of pruning-quantization\nconfigurations. Experiments demonstrate that our recommended combination\nachieves, on average, a 20% performance increase compared to an equivalent\nquantization-only model at the same theoretical compression rate."}
{"id": "2505.07161", "pdf": "https://arxiv.org/pdf/2505.07161.pdf", "abs": "https://arxiv.org/abs/2505.07161", "title": "Towards Actionable Pedagogical Feedback: A Multi-Perspective Analysis of Mathematics Teaching and Tutoring Dialogue", "authors": ["Jannatun Naim", "Jie Cao", "Fareen Tasneem", "Jennifer Jacobs", "Brent Milne", "James Martin", "Tamara Sumner"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted to EDM'2025", "summary": "Effective feedback is essential for refining instructional practices in\nmathematics education, and researchers often turn to advanced natural language\nprocessing (NLP) models to analyze classroom dialogues from multiple\nperspectives. However, utterance-level discourse analysis encounters two\nprimary challenges: (1) multifunctionality, where a single utterance may serve\nmultiple purposes that a single tag cannot capture, and (2) the exclusion of\nmany utterances from domain-specific discourse move classifications, leading to\ntheir omission in feedback. To address these challenges, we proposed a\nmulti-perspective discourse analysis that integrates domain-specific talk moves\nwith dialogue act (using the flattened multi-functional SWBD-MASL schema with\n43 tags) and discourse relation (applying Segmented Discourse Representation\nTheory with 16 relations). Our top-down analysis framework enables a\ncomprehensive understanding of utterances that contain talk moves, as well as\nutterances that do not contain talk moves. This is applied to two mathematics\neducation datasets: TalkMoves (teaching) and SAGA22 (tutoring). Through\ndistributional unigram analysis, sequential talk move analysis, and multi-view\ndeep dive, we discovered meaningful discourse patterns, and revealed the vital\nrole of utterances without talk moves, demonstrating that these utterances, far\nfrom being mere fillers, serve crucial functions in guiding, acknowledging, and\nstructuring classroom discourse. These insights underscore the importance of\nincorporating discourse relations and dialogue acts into AI-assisted education\nsystems to enhance feedback and create more responsive learning environments.\nOur framework may prove helpful for providing human educator feedback, but also\naiding in the development of AI agents that can effectively emulate the roles\nof both educators and students."}
{"id": "2505.07293", "pdf": "https://arxiv.org/pdf/2505.07293.pdf", "abs": "https://arxiv.org/abs/2505.07293", "title": "AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong Pretraining Data Selection", "authors": ["Kai Hua", "Steven Wu", "Ge Zhang", "Ke Shen"], "categories": ["cs.CL"], "comment": "28 pages, 19 figures", "summary": "Recently, there has been growing interest in collecting reasoning-intensive\npretraining data to improve LLMs' complex reasoning ability. Prior approaches\ntypically rely on supervised classifiers to identify such data, which requires\nlabeling by humans or LLMs, often introducing domain-specific biases. Due to\nthe attention heads being crucial to in-context reasoning, we propose\nAttentionInfluence, a simple yet effective, training-free method without\nsupervision signal. Our approach enables a small pretrained language model to\nact as a strong data selector through a simple attention head masking\noperation. Specifically, we identify retrieval heads and compute the loss\ndifference when masking these heads. We apply AttentionInfluence to a\n1.3B-parameter dense model to conduct data selection on the SmolLM corpus of\n241B tokens, and mix the SmolLM corpus with the selected subset comprising 73B\ntokens to pretrain a 7B-parameter dense model using 1T training tokens and WSD\nlearning rate scheduling. Our experimental results demonstrate substantial\nimprovements, ranging from 1.4pp to 3.5pp, across several knowledge-intensive\nand reasoning-heavy benchmarks (i.e., MMLU, MMLU-Pro, AGIEval-en, GSM8K, and\nHumanEval). This demonstrates an effective weak-to-strong scaling property,\nwith small models improving the final performance of larger models-offering a\npromising and scalable path for reasoning-centric data selection."}
{"id": "2505.07339", "pdf": "https://arxiv.org/pdf/2505.07339.pdf", "abs": "https://arxiv.org/abs/2505.07339", "title": "Laypeople's Attitudes Towards Fair, Affirmative, and Discriminatory Decision-Making Algorithms", "authors": ["Gabriel Lima", "Nina Grgić-Hlača", "Markus Langer", "Yixin Zou"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "Affirmative algorithms have emerged as a potential answer to algorithmic\ndiscrimination, seeking to redress past harms and rectify the source of\nhistorical injustices. We present the results of two experiments ($N$$=$$1193$)\ncapturing laypeople's perceptions of affirmative algorithms -- those which\nexplicitly prioritize the historically marginalized -- in hiring and criminal\njustice. We contrast these opinions about affirmative algorithms with folk\nattitudes towards algorithms that prioritize the privileged (i.e.,\ndiscriminatory) and systems that make decisions independently of demographic\ngroups (i.e., fair). We find that people -- regardless of their political\nleaning and identity -- view fair algorithms favorably and denounce\ndiscriminatory systems. In contrast, we identify disagreements concerning\naffirmative algorithms: liberals and racial minorities rate affirmative systems\nas positively as their fair counterparts, whereas conservatives and those from\nthe dominant racial group evaluate affirmative algorithms as negatively as\ndiscriminatory systems. We identify a source of these divisions: people have\nvarying beliefs about who (if anyone) is marginalized, shaping their views of\naffirmative algorithms. We discuss the possibility of bridging these\ndisagreements to bring people together towards affirmative algorithms."}
{"id": "2505.07313", "pdf": "https://arxiv.org/pdf/2505.07313.pdf", "abs": "https://arxiv.org/abs/2505.07313", "title": "Towards Multi-Agent Reasoning Systems for Collaborative Expertise Delegation: An Exploratory Design Study", "authors": ["Baixuan Xu", "Chunyang Li", "Weiqi Wang", "Wei Fan", "Tianshi Zheng", "Haochen Shi", "Tao Fan", "Yangqiu Song", "Qiang Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "18 pages", "summary": "Designing effective collaboration structure for multi-agent LLM systems to\nenhance collective reasoning is crucial yet remains under-explored. In this\npaper, we systematically investigate how collaborative reasoning performance is\naffected by three key design dimensions: (1) Expertise-Domain Alignment, (2)\nCollaboration Paradigm (structured workflow vs. diversity-driven integration),\nand (3) System Scale. Our findings reveal that expertise alignment benefits are\nhighly domain-contingent, proving most effective for contextual reasoning\ntasks. Furthermore, collaboration focused on integrating diverse knowledge\nconsistently outperforms rigid task decomposition. Finally, we empirically\nexplore the impact of scaling the multi-agent system with expertise\nspecialization and study the computational trade off, highlighting the need for\nmore efficient communication protocol design. This work provides concrete\nguidelines for configuring specialized multi-agent system and identifies\ncritical architectural trade-offs and bottlenecks for scalable multi-agent\nreasoning. The code will be made available upon acceptance."}
{"id": "2505.07552", "pdf": "https://arxiv.org/pdf/2505.07552.pdf", "abs": "https://arxiv.org/abs/2505.07552", "title": "Automated Visual Attention Detection using Mobile Eye Tracking in Behavioral Classroom Studies", "authors": ["Efe Bozkir", "Christian Kosel", "Tina Seidel", "Enkelejda Kasneci"], "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "Accepted as a long paper at the Educational Data Mining (EDM)\n  Conference 2025", "summary": "Teachers' visual attention and its distribution across the students in\nclassrooms can constitute important implications for student engagement,\nachievement, and professional teacher training. Despite that, inferring the\ninformation about where and which student teachers focus on is not trivial.\nMobile eye tracking can provide vital help to solve this issue; however, the\nuse of mobile eye tracking alone requires a significant amount of manual\nannotations. To address this limitation, we present an automated processing\npipeline concept that requires minimal manually annotated data to recognize\nwhich student the teachers focus on. To this end, we utilize state-of-the-art\nface detection models and face recognition feature embeddings to train face\nrecognition models with transfer learning in the classroom context and combine\nthese models with the teachers' gaze from mobile eye trackers. We evaluated our\napproach with data collected from four different classrooms, and our results\nshow that while it is possible to estimate the visually focused students with\nreasonable performance in all of our classroom setups, U-shaped and small\nclassrooms led to the best results with accuracies of approximately 0.7 and\n0.9, respectively. While we did not evaluate our method for teacher-student\ninteractions and focused on the validity of the technical approach, as our\nmethodology does not require a vast amount of manually annotated data and\noffers a non-intrusive way of handling teachers' visual attention, it could\nhelp improve instructional strategies, enhance classroom management, and\nprovide feedback for professional teacher development."}
{"id": "2505.07345", "pdf": "https://arxiv.org/pdf/2505.07345.pdf", "abs": "https://arxiv.org/abs/2505.07345", "title": "QUPID: Quantified Understanding for Enhanced Performance, Insights, and Decisions in Korean Search Engines", "authors": ["Ohjoon Kwon", "Changsu Lee", "Jihye Back", "Lim Sun Suk", "Inho Kang", "Donghyeon Jeon"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large language models (LLMs) have been widely used for relevance assessment\nin information retrieval. However, our study demonstrates that combining two\ndistinct small language models (SLMs) with different architectures can\noutperform LLMs in this task. Our approach -- QUPID -- integrates a generative\nSLM with an embedding-based SLM, achieving higher relevance judgment accuracy\nwhile reducing computational costs compared to state-of-the-art LLM solutions.\nThis computational efficiency makes QUPID highly scalable for real-world search\nsystems processing millions of queries daily. In experiments across diverse\ndocument types, our method demonstrated consistent performance improvements\n(Cohen's Kappa of 0.646 versus 0.387 for leading LLMs) while offering 60x\nfaster inference times. Furthermore, when integrated into production search\npipelines, QUPID improved nDCG@5 scores by 1.9%. These findings underscore how\narchitectural diversity in model combinations can significantly enhance both\nsearch relevance and operational efficiency in information retrieval systems."}
{"id": "2505.07625", "pdf": "https://arxiv.org/pdf/2505.07625.pdf", "abs": "https://arxiv.org/abs/2505.07625", "title": "QC-Adviser: Quantum Hardware Recommendations for Solving Industrial Optimization Problems", "authors": ["Djamel Laps-Bouraba", "Markus Zajac", "Uta Störl"], "categories": ["quant-ph", "cs.HC"], "comment": null, "summary": "The availability of quantum hardware via the cloud offers opportunities for\nnew approaches to computing optimization problems in an industrial environment.\nHowever, selecting the right quantum hardware is difficult for non-experts due\nto its technical characteristics. In this paper, we present the QC-Adviser\nprototype, which supports users in selecting suitable quantum annealer hardware\nwithout requiring quantum computing knowledge."}
{"id": "2505.07409", "pdf": "https://arxiv.org/pdf/2505.07409.pdf", "abs": "https://arxiv.org/abs/2505.07409", "title": "Computational Fact-Checking of Online Discourse: Scoring scientific accuracy in climate change related news articles", "authors": ["Tim Wittenborg", "Constantin Sebastian Tremel", "Markus Stocker", "Sören Auer"], "categories": ["cs.CL"], "comment": "4 pages, 4 figures, submitted to ACM Web Conference 2025", "summary": "Democratic societies need reliable information. Misinformation in popular\nmedia such as news articles or videos threatens to impair civic discourse.\nCitizens are, unfortunately, not equipped to verify this content flood consumed\ndaily at increasing rates. This work aims to semi-automatically quantify\nscientific accuracy of online media. By semantifying media of unknown veracity,\ntheir statements can be compared against equally processed trusted sources. We\nimplemented a workflow using LLM-based statement extraction and knowledge graph\nanalysis. Our neurosymbolic system was able to evidently streamline\nstate-of-the-art veracity quantification. Evaluated via expert interviews and a\nuser survey, the tool provides a beneficial veracity indication. This\nindicator, however, is unable to annotate public media at the required\ngranularity and scale. Further work towards a FAIR (Findable, Accessible,\nInteroperable, Reusable) ground truth and complementary metrics are required to\nscientifically support civic discourse."}
{"id": "2505.07664", "pdf": "https://arxiv.org/pdf/2505.07664.pdf", "abs": "https://arxiv.org/abs/2505.07664", "title": "A Case Study Investigating the Role of Generative AI in Quality Evaluations of Epics in Agile Software Development", "authors": ["Werner Geyer", "Jessica He", "Daita Sarkar", "Michelle Brachman", "Chris Hammond", "Jennifer Heins", "Zahra Ashktorab", "Carlos Rosemberg", "Charlie Hill"], "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": null, "summary": "The broad availability of generative AI offers new opportunities to support\nvarious work domains, including agile software development. Agile epics are a\nkey artifact for product managers to communicate requirements to stakeholders.\nHowever, in practice, they are often poorly defined, leading to churn, delivery\ndelays, and cost overruns. In this industry case study, we investigate\nopportunities for large language models (LLMs) to evaluate agile epic quality\nin a global company. Results from a user study with 17 product managers\nindicate how LLM evaluations could be integrated into their work practices,\nincluding perceived values and usage in improving their epics. High levels of\nsatisfaction indicate that agile epics are a new, viable application of AI\nevaluations. However, our findings also outline challenges, limitations, and\nadoption barriers that can inform both practitioners and researchers on the\nintegration of such evaluations into future agile work practices."}
{"id": "2505.07416", "pdf": "https://arxiv.org/pdf/2505.07416.pdf", "abs": "https://arxiv.org/abs/2505.07416", "title": "ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness Prediction via Human-AI Collaborative Annotation", "authors": ["Truc Mai-Thanh Nguyen", "Dat Minh Nguyen", "Son T. Luu", "Kiet Van Nguyen"], "categories": ["cs.CL"], "comment": "Accepted at NLDB 2025", "summary": "Multimodal Review Helpfulness Prediction (MRHP) is an essential task in\nrecommender systems, particularly in E-commerce platforms. Determining the\nhelpfulness of user-generated reviews enhances user experience and improves\nconsumer decision-making. However, existing datasets focus predominantly on\nEnglish and Indonesian, resulting in a lack of linguistic diversity, especially\nfor low-resource languages such as Vietnamese. In this paper, we introduce\nViMRHP (Vietnamese Multimodal Review Helpfulness Prediction), a large-scale\nbenchmark dataset for MRHP task in Vietnamese. This dataset covers four\ndomains, including 2K products with 46K reviews. Meanwhile, a large-scale\ndataset requires considerable time and cost. To optimize the annotation\nprocess, we leverage AI to assist annotators in constructing the ViMRHP\ndataset. With AI assistance, annotation time is reduced (90 to 120 seconds per\ntask down to 20 to 40 seconds per task) while maintaining data quality and\nlowering overall costs by approximately 65%. However, AI-generated annotations\nstill have limitations in complex annotation tasks, which we further examine\nthrough a detailed performance analysis. In our experiment on ViMRHP, we\nevaluate baseline models on human-verified and AI-generated annotations to\nassess their quality differences. The ViMRHP dataset is publicly available at\nhttps://github.com/trng28/ViMRHP"}
{"id": "2403.05581", "pdf": "https://arxiv.org/pdf/2403.05581.pdf", "abs": "https://arxiv.org/abs/2403.05581", "title": "Can Interpretability Layouts Influence Human Perception of Offensive Sentences?", "authors": ["Thiago Freitas dos Santos", "Nardine Osman", "Marco Schorlemmer"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper conducts a user study to assess whether three machine learning\n(ML) interpretability layouts can influence participants' views when evaluating\nsentences containing hate speech, focusing on the \"Misogyny\" and \"Racism\"\nclasses. Given the existence of divergent conclusions in the literature, we\nprovide empirical evidence on using ML interpretability in online communities\nthrough statistical and qualitative analyses of questionnaire responses. The\nGeneralized Additive Model estimates participants' ratings, incorporating\nwithin-subject and between-subject designs. While our statistical analysis\nindicates that none of the interpretability layouts significantly influences\nparticipants' views, our qualitative analysis demonstrates the advantages of ML\ninterpretability: 1) triggering participants to provide corrective feedback in\ncase of discrepancies between their views and the model, and 2) providing\ninsights to evaluate a model's behavior beyond traditional performance metrics."}
{"id": "2505.07430", "pdf": "https://arxiv.org/pdf/2505.07430.pdf", "abs": "https://arxiv.org/abs/2505.07430", "title": "Comparative sentiment analysis of public perception: Monkeypox vs. COVID-19 behavioral insights", "authors": ["Mostafa Mohaimen Akand Faisal", "Rabeya Amin Jhuma"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The emergence of global health crises, such as COVID-19 and Monkeypox (mpox),\nhas underscored the importance of understanding public sentiment to inform\neffective public health strategies. This study conducts a comparative sentiment\nanalysis of public perceptions surrounding COVID-19 and mpox by leveraging\nextensive datasets of 147,475 and 106,638 tweets, respectively. Advanced\nmachine learning models, including Logistic Regression, Naive Bayes, RoBERTa,\nDistilRoBERTa and XLNet, were applied to perform sentiment classification, with\nresults indicating key trends in public emotion and discourse. The analysis\nhighlights significant differences in public sentiment driven by disease\ncharacteristics, media representation, and pandemic fatigue. Through the lens\nof sentiment polarity and thematic trends, this study offers valuable insights\ninto tailoring public health messaging, mitigating misinformation, and\nfostering trust during concurrent health crises. The findings contribute to\nadvancing sentiment analysis applications in public health informatics, setting\nthe groundwork for enhanced real-time monitoring and multilingual analysis in\nfuture research."}
{"id": "2403.15919", "pdf": "https://arxiv.org/pdf/2403.15919.pdf", "abs": "https://arxiv.org/abs/2403.15919", "title": "Negotiating the Shared Agency between Humans & AI in the Recommender System", "authors": ["Mengke Wu", "Weizi Liu", "Yanyun Wang", "Mike Yao"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Smart recommendation algorithms have revolutionized content delivery and\nimproved efficiency across various domains. However, concerns about user agency\narise from the algorithms' inherent opacity (information asymmetry) and one-way\noutput (power asymmetry). This study introduces a dual-control mechanism aimed\nat enhancing user agency, empowering users to manage both data collection and,\nnovelly, the degree of algorithmically tailored content they receive. In a\nbetween-subject experiment with 161 participants, we evaluated the impact of\nvarying levels of transparency and control on user experience. Results show\nthat transparency alone is insufficient to foster a sense of agency, and may\neven exacerbate disempowerment compared to displaying outcomes directly.\nConversely, combining transparency with user controls-particularly those\nallowing direct influence on outcomes-significantly enhances user agency. This\nresearch provides a proof-of-concept for a novel approach and lays the\ngroundwork for designing more user-centered recommender systems that emphasize\nuser autonomy and fairness in AI-driven content delivery."}
{"id": "2505.07440", "pdf": "https://arxiv.org/pdf/2505.07440.pdf", "abs": "https://arxiv.org/abs/2505.07440", "title": "Matching Tasks with Industry Groups for Augmenting Commonsense Knowledge", "authors": ["Rituraj Singh", "Sachin Pawar", "Girish Palshikar"], "categories": ["cs.CL"], "comment": null, "summary": "Commonsense knowledge bases (KB) are a source of specialized knowledge that\nis widely used to improve machine learning applications. However, even for a\nlarge KB such as ConceptNet, capturing explicit knowledge from each industry\ndomain is challenging. For example, only a few samples of general {\\em tasks}\nperformed by various industries are available in ConceptNet. Here, a task is a\nwell-defined knowledge-based volitional action to achieve a particular goal. In\nthis paper, we aim to fill this gap and present a weakly-supervised framework\nto augment commonsense KB with tasks carried out by various industry groups\n(IG). We attempt to {\\em match} each task with one or more suitable IGs by\ntraining a neural model to learn task-IG affinity and apply clustering to\nselect the top-k tasks per IG. We extract a total of 2339 triples of the form\n$\\langle IG, is~capable~of, task \\rangle$ from two publicly available news\ndatasets for 24 IGs with the precision of 0.86. This validates the reliability\nof the extracted task-IG pairs that can be directly added to existing KBs."}
{"id": "2404.14814", "pdf": "https://arxiv.org/pdf/2404.14814.pdf", "abs": "https://arxiv.org/abs/2404.14814", "title": "MARV: Multiview Augmented Reality Visualisation for Exploring Rich Material Data", "authors": ["Alexander Gall", "Anja Heim", "Eduard Gröller", "Christoph Heinzl"], "categories": ["cs.HC", "cs.GR"], "comment": "13 pages, 6 figures", "summary": "Rich material data is complex, large and heterogeneous, integrating primary\nand secondary non-destructive testing data for spatial, spatio-temporal, as\nwell as high-dimensional data analyses. Currently, materials experts mainly\nrely on conventional desktop-based systems using 2D visualisation techniques,\nwhich render respective analyses a time-consuming and mentally demanding\nchallenge. MARV is a novel immersive visual analytics system, which makes\nanalyses of such data more effective and engaging in an augmented reality\nsetting. For this purpose, MARV includes three newly designed visualisation\ntechniques: MDD Glyphs with a Skewness Kurtosis Mapper, Temporal Evolution\nTracker, and Chrono Bins, facilitating interactive exploration and comparison\nof multidimensional distributions of attribute data from multiple time steps. A\nqualitative evaluation conducted with materials experts in a real-world case\nstudy demonstrates the benefits of the proposed visualisation techniques. This\nevaluation revealed that combining spatial and abstract data in an immersive\nenvironment improves their analytical capabilities and facilitates the\nidentification of patterns, anomalies, as well as changes over time."}
{"id": "2505.07495", "pdf": "https://arxiv.org/pdf/2505.07495.pdf", "abs": "https://arxiv.org/abs/2505.07495", "title": "Translating the Grievance Dictionary: a psychometric evaluation of Dutch, German, and Italian versions", "authors": ["Isabelle van der Vegt", "Bennett Kleinberg", "Marilu Miotto", "Jonas Festor"], "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces and evaluates three translations of the Grievance\nDictionary, a psycholinguistic dictionary for the analysis of violent,\nthreatening or grievance-fuelled texts. Considering the relevance of these\nthemes in languages beyond English, we translated the Grievance Dictionary to\nDutch, German, and Italian. We describe the process of automated translation\nsupplemented by human annotation. Psychometric analyses are performed,\nincluding internal reliability of dictionary categories and correlations with\nthe LIWC dictionary. The Dutch and German translations perform similarly to the\noriginal English version, whereas the Italian dictionary shows low reliability\nfor some categories. Finally, we make suggestions for further validation and\napplication of the dictionary, as well as for future dictionary translations\nfollowing a similar approach."}
{"id": "2405.08302", "pdf": "https://arxiv.org/pdf/2405.08302.pdf", "abs": "https://arxiv.org/abs/2405.08302", "title": "Designing Adaptive User Interfaces for mHealth Applications Targeting Chronic Disease: A User-Centered Approach", "authors": ["Wei Wang", "John Grundy", "Hourieh Khalajzadeh", "Anuradha Madugalla", "Humphrey O. Obie"], "categories": ["cs.HC", "cs.SE"], "comment": null, "summary": "Mobile Health (mHealth) applications have demonstrated considerable potential\nin supporting chronic disease self-management; however, they remain\nunder-utilised due to low engagement, limited accessibility, and poor long-term\nadherence. These issues are particularly prominent among users with chronic\ndisease, whose needs and capabilities vary widely. To address this, Adaptive\nUser Interfaces (AUIs) offer a dynamic solution by tailoring interface features\nto users' preferences, health status, and contexts. This paper presents a\ntwo-stage study to develop and validate actionable AUI design guidelines for\nmHealth applications. In stage one, an AUI prototype was evaluated through\nfocus groups, interviews, and a standalone survey, revealing key user\nchallenges and preferences. These insights informed the creation of an initial\nset of guidelines. In stage two, the guidelines were refined based on feedback\nfrom 20 end users and evaluated by 43 software practitioners through two\nsurveys. This process resulted in nine finalized guidelines. To assess\nreal-world relevance, a case study of four mHealth applications was conducted,\nwith findings supported by user reviews highlighting the utility of the\nguidelines in identifying critical adaptation issues. This study offers\nactionable, evidence-based guidelines that help software practitioners design\nAUIs in mHealth to better support individuals managing chronic diseases"}
{"id": "2505.07512", "pdf": "https://arxiv.org/pdf/2505.07512.pdf", "abs": "https://arxiv.org/abs/2505.07512", "title": "ToolACE-DEV: Self-Improving Tool Learning via Decomposition and EVolution", "authors": ["Xu Huang", "Weiwen Liu", "Xingshan Zeng", "Yuefeng Huang", "Xinlong Hao", "Yuxian Wang", "Yirong Zeng", "Chuhan Wu", "Yasheng Wang", "Ruiming Tang", "Defu Lian"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The tool-using capability of large language models (LLMs) enables them to\naccess up-to-date external information and handle complex tasks. Current\napproaches to enhancing this capability primarily rely on distilling advanced\nmodels by data synthesis. However, this method incurs significant costs\nassociated with advanced model usage and often results in data compatibility\nissues, led by the high discrepancy in the knowledge scope between the advanced\nmodel and the target model. To address these challenges, we propose\nToolACE-DEV, a self-improving framework for tool learning. First, we decompose\nthe tool-learning objective into sub-tasks that enhance basic tool-making and\ntool-using abilities. Then, we introduce a self-evolving paradigm that allows\nlightweight models to self-improve, reducing reliance on advanced LLMs.\nExtensive experiments validate the effectiveness of our approach across models\nof varying scales and architectures."}
{"id": "2502.02456", "pdf": "https://arxiv.org/pdf/2502.02456.pdf", "abs": "https://arxiv.org/abs/2502.02456", "title": "Model Human Learners: Computational Models to Guide Instructional Design", "authors": ["Christopher J. MacLellan"], "categories": ["cs.HC", "cs.AI", "cs.SC"], "comment": "Published at CogSci 2025; 6 pages, 6 figures, 1 table", "summary": "Instructional designers face an overwhelming array of design choices, making\nit challenging to identify the most effective interventions. To address this\nissue, I propose the concept of a Model Human Learner, a unified computational\nmodel of learning that can aid designers in evaluating candidate interventions.\nThis paper presents the first successful demonstration of this concept, showing\nthat a computational model can accurately predict the outcomes of two human A/B\nexperiments -- one testing a problem sequencing intervention and the other\ntesting an item design intervention. It also demonstrates that such a model can\ngenerate learning curves without requiring human data and provide theoretical\ninsights into why an instructional intervention is effective. These findings\nlay the groundwork for future Model Human Learners that integrate cognitive and\nlearning theories to support instructional design across diverse tasks and\ninterventions."}
{"id": "2505.07528", "pdf": "https://arxiv.org/pdf/2505.07528.pdf", "abs": "https://arxiv.org/abs/2505.07528", "title": "SEReDeEP: Hallucination Detection in Retrieval-Augmented Models via Semantic Entropy and Context-Parameter Fusion", "authors": ["Lei Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) models frequently encounter\nhallucination phenomena when integrating external information with internal\nparametric knowledge. Empirical studies demonstrate that the disequilibrium\nbetween external contextual information and internal parametric knowledge\nconstitutes a primary factor in hallucination generation. Existing\nhallucination detection methodologies predominantly emphasize either the\nexternal or internal mechanism in isolation, thereby overlooking their\nsynergistic effects. The recently proposed ReDeEP framework decouples these\ndual mechanisms, identifying two critical contributors to hallucinations:\nexcessive reliance on parametric knowledge encoded in feed-forward networks\n(FFN) and insufficient utilization of external information by attention\nmechanisms (particularly copy heads). ReDeEP quantitatively assesses these\nfactors to detect hallucinations and dynamically modulates the contributions of\nFFNs and copy heads to attenuate their occurrence. Nevertheless, ReDeEP and\nnumerous other hallucination detection approaches have been employed at\nlogit-level uncertainty estimation or language-level self-consistency\nevaluation, inadequately address the semantic dimensions of model responses,\nresulting in inconsistent hallucination assessments in RAG implementations.\nBuilding upon ReDeEP's foundation, this paper introduces SEReDeEP, which\nenhances computational processes through semantic entropy captured via trained\nlinear probes, thereby achieving hallucination assessments that more accurately\nreflect ground truth evaluations."}
{"id": "2502.02929", "pdf": "https://arxiv.org/pdf/2502.02929.pdf", "abs": "https://arxiv.org/abs/2502.02929", "title": "AudioMiXR: Spatial Audio Object Manipulation with 6DoF for Sound Design in Augmented Reality", "authors": ["Brandon Woodard", "Margarita Geleta", "Joseph J. LaViola Jr.", "Andrea Fanelli", "Rhonda Wilson"], "categories": ["cs.HC", "cs.SD", "eess.AS"], "comment": "Revision necessary for accuracy", "summary": "We present AudioMiXR, an augmented reality (AR) interface intended to assess\nhow users manipulate virtual audio objects situated in their physical space\nusing six degrees of freedom (6DoF) deployed on a head-mounted display (Apple\nVision Pro) for 3D sound design. Existing tools for 3D sound design are\ntypically constrained to desktop displays, which may limit spatial awareness of\nmixing within the execution environment. Utilizing an XR HMD to create\nsoundscapes may provide a real-time test environment for 3D sound design, as\nmodern HMDs can provide precise spatial localization assisted by cross-modal\ninteractions. However, there is no research on design guidelines specific to\nsound design with six degrees of freedom (6DoF) in XR. To provide a first step\ntoward identifying design-related research directions in this space, we\nconducted an exploratory study where we recruited 27 participants, consisting\nof expert and non-expert sound designers. The goal was to assess design lessons\nthat can be used to inform future research venues in 3D sound design. We ran a\nwithin-subjects study where users designed both a music and cinematic\nsoundscapes. After thematically analyzing participant data, we constructed two\ndesign lessons: 1. Proprioception for AR Sound Design, and 2. Balancing\nAudio-Visual Modalities in AR GUIs. Additionally, we provide application\ndomains that can benefit most from 6DoF sound design based on our results."}
{"id": "2505.07591", "pdf": "https://arxiv.org/pdf/2505.07591.pdf", "abs": "https://arxiv.org/abs/2505.07591", "title": "A Multi-Dimensional Constraint Framework for Evaluating and Improving Instruction Following in Large Language Models", "authors": ["Junjie Ye", "Caishuang Huang", "Zhuohan Chen", "Wenjie Fu", "Chenyuan Yang", "Leyi Yang", "Yilong Wu", "Peng Wang", "Meng Zhou", "Xiaolong Yang", "Tao Gui", "Qi Zhang", "Zhongchao Shi", "Jianping Fan", "Xuanjing Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Instruction following evaluates large language models (LLMs) on their ability\nto generate outputs that adhere to user-defined constraints. However, existing\nbenchmarks often rely on templated constraint prompts, which lack the diversity\nof real-world usage and limit fine-grained performance assessment. To fill this\ngap, we propose a multi-dimensional constraint framework encompassing three\nconstraint patterns, four constraint categories, and four difficulty levels.\nBuilding on this framework, we develop an automated instruction generation\npipeline that performs constraint expansion, conflict detection, and\ninstruction rewriting, yielding 1,200 code-verifiable instruction-following\ntest samples. We evaluate 19 LLMs across seven model families and uncover\nsubstantial variation in performance across constraint forms. For instance,\naverage performance drops from 77.67% at Level I to 32.96% at Level IV.\nFurthermore, we demonstrate the utility of our approach by using it to generate\ndata for reinforcement learning, achieving substantial gains in instruction\nfollowing without degrading general performance. In-depth analysis indicates\nthat these gains stem primarily from modifications in the model's attention\nmodules parameters, which enhance constraint recognition and adherence. Code\nand data are available in https://github.com/Junjie-Ye/MulDimIF."}
{"id": "2504.07840", "pdf": "https://arxiv.org/pdf/2504.07840.pdf", "abs": "https://arxiv.org/abs/2504.07840", "title": "Understanding Learner-LLM Chatbot Interactions and the Impact of Prompting Guidelines", "authors": ["Cansu Koyuturk", "Emily Theophilou", "Sabrina Patania", "Gregor Donabauer", "Andrea Martinenghi", "Chiara Antico", "Alessia Telari", "Alessia Testa", "Sathya Bursic", "Franca Garzotto", "Davinia Hernandez-Leo", "Udo Kruschwitz", "Davide Taibi", "Simona Amenta", "Martin Ruskov", "Dimitri Ognibene"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "Accepted for AIED 2025, the 26th International Conference on\n  Artificial Intelligence in Education, July 22 - 26, 2025, Palermo, Italy", "summary": "Large Language Models (LLMs) have transformed human-computer interaction by\nenabling natural language-based communication with AI-powered chatbots. These\nmodels are designed to be intuitive and user-friendly, allowing users to\narticulate requests with minimal effort. However, despite their accessibility,\nstudies reveal that users often struggle with effective prompting, resulting in\ninefficient responses. Existing research has highlighted both the limitations\nof LLMs in interpreting vague or poorly structured prompts and the difficulties\nusers face in crafting precise queries. This study investigates learner-AI\ninteractions through an educational experiment in which participants receive\nstructured guidance on effective prompting. We introduce and compare three\ntypes of prompting guidelines: a task-specific framework developed through a\nstructured methodology and two baseline approaches. To assess user behavior and\nprompting efficacy, we analyze a dataset of 642 interactions from 107 users.\nUsing Von NeuMidas, an extended pragmatic annotation schema for LLM interaction\nanalysis, we categorize common prompting errors and identify recurring\nbehavioral patterns. We then evaluate the impact of different guidelines by\nexamining changes in user behavior, adherence to prompting strategies, and the\noverall quality of AI-generated responses. Our findings provide a deeper\nunderstanding of how users engage with LLMs and the role of structured\nprompting guidance in enhancing AI-assisted communication. By comparing\ndifferent instructional frameworks, we offer insights into more effective\napproaches for improving user competency in AI interactions, with implications\nfor AI literacy, chatbot usability, and the design of more responsive AI\nsystems."}
{"id": "2505.07596", "pdf": "https://arxiv.org/pdf/2505.07596.pdf", "abs": "https://arxiv.org/abs/2505.07596", "title": "Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent", "authors": ["Ziyang Huang", "Xiaowei Yuan", "Yiming Ju", "Jun Zhao", "Kang Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-augmented generation (RAG) is a common strategy to reduce\nhallucinations in Large Language Models (LLMs). While reinforcement learning\n(RL) can enable LLMs to act as search agents by activating retrieval\ncapabilities, existing ones often underutilize their internal knowledge. This\ncan lead to redundant retrievals, potential harmful knowledge conflicts, and\nincreased inference latency. To address these limitations, an efficient and\nadaptive search agent capable of discerning optimal retrieval timing and\nsynergistically integrating parametric (internal) and retrieved (external)\nknowledge is in urgent need. This paper introduces the Reinforced\nInternal-External Knowledge Synergistic Reasoning Agent (IKEA), which could\nindentify its own knowledge boundary and prioritize the utilization of internal\nknowledge, resorting to external search only when internal knowledge is deemed\ninsufficient. This is achieved using a novel knowledge-boundary aware reward\nfunction and a knowledge-boundary aware training dataset. These are designed\nfor internal-external knowledge synergy oriented RL, incentivizing the model to\ndeliver accurate answers, minimize unnecessary retrievals, and encourage\nappropriate external searches when its own knowledge is lacking. Evaluations\nacross multiple knowledge reasoning tasks demonstrate that IKEA significantly\noutperforms baseline methods, reduces retrieval frequency significantly, and\nexhibits robust generalization capabilities."}
{"id": "2504.15480", "pdf": "https://arxiv.org/pdf/2504.15480.pdf", "abs": "https://arxiv.org/abs/2504.15480", "title": "Under Pressure: Contextualizing Workplace Stress Towards User-Centered Interventions", "authors": ["Antonin Brun", "Gale Lucas", "Burçin Becerik-Gerber"], "categories": ["cs.HC"], "comment": "9 pages, extended abstracts CHI '25", "summary": "Stress is a pervasive challenge that significantly impacts worker health and\nwell-being. Workplace stress is driven by various factors, ranging from\norganizational changes to poor workplace design. Although individual stress\nmanagement strategies have been shown to be effective, current interventions\noften overlook personal and contextual factors shaping stress experiences. In\nthis study, we conducted semi-structured interviews with eight office workers\nto gain a deeper understanding of their personal experiences with workplace\nstress. Our analysis reveals key stress triggers, coping mechanisms, and\nreflections on past stressful events. We highlight the multifaceted and\nindividualized nature of workplace stress, emphasizing the importance of\nintervention timing, modality, and recognizing that stress is not solely a\nnegative experience but can also have positive effects. Our findings provide\nactionable insights for the design of user-centered stress management solutions\nmore attuned to the needs of office workers."}
{"id": "2505.07601", "pdf": "https://arxiv.org/pdf/2505.07601.pdf", "abs": "https://arxiv.org/abs/2505.07601", "title": "Characterizing the Investigative Methods of Fictional Detectives with Large Language Models", "authors": ["Edirlei Soares de Lima", "Marco A. Casanova", "Bruno Feijó", "Antonio L. Furtado"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Detective fiction, a genre defined by its complex narrative structures and\ncharacter-driven storytelling, presents unique challenges for computational\nnarratology, a research field focused on integrating literary theory into\nautomated narrative generation. While traditional literary studies have offered\ndeep insights into the methods and archetypes of fictional detectives, these\nanalyses often focus on a limited number of characters and lack the scalability\nneeded for the extraction of unique traits that can be used to guide narrative\ngeneration methods. In this paper, we present an AI-driven approach for\nsystematically characterizing the investigative methods of fictional\ndetectives. Our multi-phase workflow explores the capabilities of 15 Large\nLanguage Models (LLMs) to extract, synthesize, and validate distinctive\ninvestigative traits of fictional detectives. This approach was tested on a\ndiverse set of seven iconic detectives - Hercule Poirot, Sherlock Holmes,\nWilliam Murdoch, Columbo, Father Brown, Miss Marple, and Auguste Dupin -\ncapturing the distinctive investigative styles that define each character. The\nidentified traits were validated against existing literary analyses and further\ntested in a reverse identification phase, achieving an overall accuracy of\n91.43%, demonstrating the method's effectiveness in capturing the distinctive\ninvestigative approaches of each detective. This work contributes to the\nbroader field of computational narratology by providing a scalable framework\nfor character analysis, with potential applications in AI-driven interactive\nstorytelling and automated narrative generation."}
{"id": "2504.18988", "pdf": "https://arxiv.org/pdf/2504.18988.pdf", "abs": "https://arxiv.org/abs/2504.18988", "title": "LINC: Supporting Language Independent Communication and Comprehension to Enhance Contribution in Multilingual Collaborative Meetings", "authors": ["Saramsh Gautam", "Mahmood Jasim"], "categories": ["cs.HC", "cs.CL", "H.5.3"], "comment": "The manuscript has been withdrawn by the authors due to ongoing\n  revisions and substantial updates", "summary": "Collaborative research often includes contributors with varied perspectives\nfrom diverse linguistic backgrounds. However, English as a Second Language\n(ESL) researchers often struggle to communicate during meetings in English and\ncomprehend discussions, leading to limited contribution. To investigate these\nchallenges, we surveyed 64 ESL researchers who frequently collaborate in\nmultilingual teams and identified four key design goals around participation,\ncomprehension, documentation, and feedback. Guided by these design goals, we\ndeveloped LINC, a multimodal Language INdependent Collaboration system with two\ncomponents: a real-time module for multilingual communication during meetings\nand a post-meeting dashboard for discussion analysis. We evaluated the system\nthrough a two-phased study with six triads of multilingual teams. We found that\nusing LINC, participants benefited from communicating in their preferred\nlanguage, recalled and reviewed actionable insights, and prepared for upcoming\nmeetings effectively. We discuss external factors that impact multilingual\nmeeting participation beyond language preferences and the implications of\nmultimodal systems in facilitating meetings in hybrid multilingual\ncollaborative settings beyond research."}
{"id": "2505.07608", "pdf": "https://arxiv.org/pdf/2505.07608.pdf", "abs": "https://arxiv.org/abs/2505.07608", "title": "MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining", "authors": ["Xiaomi LLM-Core Team", ":", "Bingquan Xia", "Bowen Shen", "Cici", "Dawei Zhu", "Di Zhang", "Gang Wang", "Hailin Zhang", "Huaqiu Liu", "Jiebao Xiao", "Jinhao Dong", "Liang Zhao", "Peidian Li", "Peng Wang", "Shihua Yu", "Shimao Chen", "Weikun Wang", "Wenhan Ma", "Xiangwei Deng", "Yi Huang", "Yifan Song", "Zihan Jiang", "Bowen Ye", "Can Cai", "Chenhong He", "Dong Zhang", "Duo Zhang", "Guoan Wang", "Hao Tian", "Haochen Zhao", "Heng Qu", "Hongshen Xu", "Jun Shi", "Kainan Bao", "QingKai Fang", "Kang Zhou", "Kangyang Zhou", "Lei Li", "Menghang Zhu", "Nuo Chen", "Qiantong Wang", "Shaohui Liu", "Shicheng Li", "Shuhao Gu", "Shuhuai Ren", "Shuo Liu", "Sirui Deng", "Weiji Zhuang", "Weiwei Lv", "Wenyu Yang", "Xin Zhang", "Xing Yong", "Xing Zhang", "Xingchen Song", "Xinzhe Xu", "Xu Wang", "Yihan Yan", "Yu Tu", "Yuanyuan Tian", "Yudong Wang", "Yue Yu", "Zhenru Lin", "Zhichao Song", "Zihao Yue"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present MiMo-7B, a large language model born for reasoning tasks, with\noptimization across both pre-training and post-training stages. During\npre-training, we enhance the data preprocessing pipeline and employ a\nthree-stage data mixing strategy to strengthen the base model's reasoning\npotential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional\nMulti-Token Prediction objective for enhanced performance and accelerated\ninference speed. During post-training, we curate a dataset of 130K verifiable\nmathematics and programming problems for reinforcement learning, integrating a\ntest-difficulty-driven code-reward scheme to alleviate sparse-reward issues and\nemploying strategic data resampling to stabilize training. Extensive\nevaluations show that MiMo-7B-Base possesses exceptional reasoning potential,\noutperforming even much larger 32B models. The final RL-tuned model,\nMiMo-7B-RL, achieves superior performance on mathematics, code and general\nreasoning tasks, surpassing the performance of OpenAI o1-mini. The model\ncheckpoints are available at https://github.com/xiaomimimo/MiMo."}
{"id": "2505.00945", "pdf": "https://arxiv.org/pdf/2505.00945.pdf", "abs": "https://arxiv.org/abs/2505.00945", "title": "SSRLBot: Designing and Developing a Large Language Model-based Agent using Socially Shared Regulated Learning", "authors": ["Xiaoshan Huang", "Jie Gao", "Haolun Wu"], "categories": ["cs.HC"], "comment": "8 pages, 2 figures", "summary": "Large language model (LLM)--based agents have emerged as pivotal tools in\nassisting human experts across various fields by transforming complex tasks\ninto more efficient workflows and providing actionable stakeholder insights.\nDespite their potential, the application of LLM-based agents for medical\neducation remains underexplored. The study aims to assist in evaluating the\nstudents' process and outcomes on medical case diagnosis and discussion while\nincorporating the theoretical framework of Socially Shared Regulation of\nLearning (SSRL) to assess student performance. SSRL emphasizes metacognitive,\ncognitive, motivational, and emotional interactions, highlighting the\ncollaborative management of learning processes to improve decision-making\noutcomes. Grounded in SSRL theory, this tool paper introduces SSRLBot, an\nLLM-based agent designed to enable team members to reflect on their diagnostic\nperformance and the key SSRL skills that foster team success. SSRLBot's core\nfunctions include summarizing dialogue content, analyzing participants' SSRL\nskills, and evaluating students' diagnostic results. Meanwhile, we evaluated\nSSRLBot through diagnostic conversation data collected from six groups (12\nparticipants, 1926 conversational turns). Results showed that SSRLBot can\ndeliver detailed, theory-aligned evaluations, link specific behaviors to SSRL\ndimensions, and offer actionable recommendations for improving teamwork. The\nfindings address a critical gap in medical education, advancing the application\nof LLM agents to enhance team-based decision-making and collaboration in\nhigh-stakes environments."}
{"id": "2505.07610", "pdf": "https://arxiv.org/pdf/2505.07610.pdf", "abs": "https://arxiv.org/abs/2505.07610", "title": "Concept-Level Explainability for Auditing & Steering LLM Responses", "authors": ["Kenza Amara", "Rita Sevastjanova", "Mennatallah El-Assady"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 7 figures, Submission to Neurips 2025", "summary": "As large language models (LLMs) become widely deployed, concerns about their\nsafety and alignment grow. An approach to steer LLM behavior, such as\nmitigating biases or defending against jailbreaks, is to identify which parts\nof a prompt influence specific aspects of the model's output. Token-level\nattribution methods offer a promising solution, but still struggle in text\ngeneration, explaining the presence of each token in the output separately,\nrather than the underlying semantics of the entire LLM response. We introduce\nConceptX, a model-agnostic, concept-level explainability method that identifies\nthe concepts, i.e., semantically rich tokens in the prompt, and assigns them\nimportance based on the outputs' semantic similarity. Unlike current\ntoken-level methods, ConceptX also offers to preserve context integrity through\nin-place token replacements and supports flexible explanation goals, e.g.,\ngender bias. ConceptX enables both auditing, by uncovering sources of bias, and\nsteering, by modifying prompts to shift the sentiment or reduce the harmfulness\nof LLM responses, without requiring retraining. Across three LLMs, ConceptX\noutperforms token-level methods like TokenSHAP in both faithfulness and human\nalignment. Steering tasks boost sentiment shift by 0.252 versus 0.131 for\nrandom edits and lower attack success rates from 0.463 to 0.242, outperforming\nattribution and paraphrasing baselines. While prompt engineering and\nself-explaining methods sometimes yield safer responses, ConceptX offers a\ntransparent and faithful alternative for improving LLM safety and alignment,\ndemonstrating the practical value of attribution-based explainability in\nguiding LLM behavior."}
{"id": "2505.04886", "pdf": "https://arxiv.org/pdf/2505.04886.pdf", "abs": "https://arxiv.org/abs/2505.04886", "title": "Fairness Perceptions in Regression-based Predictive Models", "authors": ["Mukund Telukunta", "Venkata Sriram Siddhardh Nadendla", "Morgan Stuart", "Casey Canfield"], "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "Regression-based predictive analytics used in modern kidney transplantation\nis known to inherit biases from training data. This leads to social\ndiscrimination and inefficient organ utilization, particularly in the context\nof a few social groups. Despite this concern, there is limited research on\nfairness in regression and its impact on organ utilization and placement. This\npaper introduces three novel divergence-based group fairness notions: (i)\nindependence, (ii) separation, and (iii) sufficiency to assess the fairness of\nregression-based analytics tools. In addition, fairness preferences are\ninvestigated from crowd feedback, in order to identify a socially accepted\ngroup fairness criterion for evaluating these tools. A total of 85 participants\nwere recruited from the Prolific crowdsourcing platform, and a Mixed-Logit\ndiscrete choice model was used to model fairness feedback and estimate social\nfairness preferences. The findings clearly depict a strong preference towards\nthe separation and sufficiency fairness notions, and that the predictive\nanalytics is deemed fair with respect to gender and race groups, but unfair in\nterms of age groups."}
{"id": "2505.07637", "pdf": "https://arxiv.org/pdf/2505.07637.pdf", "abs": "https://arxiv.org/abs/2505.07637", "title": "Chronocept: Instilling a Sense of Time in Machines", "authors": ["Krish Goel", "Sanskar Pandey", "KS Mahadevan", "Harsh Kumar", "Vishesh Khadaria"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "20 pages, 8 figures, 18 tables", "summary": "Human cognition is deeply intertwined with a sense of time, known as\nChronoception. This sense allows us to judge how long facts remain valid and\nwhen knowledge becomes outdated. Despite progress in vision, language, and\nmotor control, AI still struggles to reason about temporal validity. We\nintroduce Chronocept, the first benchmark to model temporal validity as a\ncontinuous probability distribution over time. Using skew-normal curves fitted\nalong semantically decomposed temporal axes, Chronocept captures nuanced\npatterns of emergence, decay, and peak relevance. It includes two datasets:\nBenchmark I (atomic facts) and Benchmark II (multi-sentence passages).\nAnnotations show strong inter-annotator agreement (84% and 89%). Our baselines\npredict curve parameters - location, scale, and skewness - enabling\ninterpretable, generalizable learning and outperforming classification-based\napproaches. Chronocept fills a foundational gap in AI's temporal reasoning,\nsupporting applications in knowledge grounding, fact-checking,\nretrieval-augmented generation (RAG), and proactive agents. Code and data are\npublicly available."}
{"id": "2211.09089", "pdf": "https://arxiv.org/pdf/2211.09089.pdf", "abs": "https://arxiv.org/abs/2211.09089", "title": "Psychophysiology-aided Perceptually Fluent Speech Analysis of Children Who Stutter", "authors": ["Yi Xiao", "Harshit Sharma", "Victoria Tumanova", "Asif Salekin"], "categories": ["cs.SD", "cs.HC", "eess.AS"], "comment": "13 pages, 5 figures, ICCPS 2025", "summary": "This paper presents a novel approach named PASAD that detects changes in\nperceptually fluent speech acoustics of young children. Particularly, analysis\nof perceptually fluent speech enables identifying the speech-motor-control\nfactors that are considered as the underlying cause of stuttering disfluencies.\nRecent studies indicate that the speech production of young children,\nespecially those who stutter, may get adversely affected by situational\nphysiological arousal. A major contribution of this paper is leveraging the\nspeaker's situational physiological responses in real-time to analyze the\nspeech signal effectively. The presented PASAD approach adapts a Hyper-Network\nstructure to extract temporal speech importance information leveraging\nphysiological parameters. Moreover, we collected speech and physiological\nsensing data from 73 preschool-age children who stutter (CWS) and who do not\nstutter (CWNS) in different conditions. PASAD's unique architecture enables\nidentifying speech attributes distinct to a CWS's fluent speech and mapping\nthem to the speaker's respective speech-motor-control factors. Extracted\nknowledge can enhance understanding of children's speech-motor-control and\nstuttering development. Our comprehensive evaluation shows that PASAD\noutperforms state-of-the-art multi-modal baseline approaches in different\nconditions, is expressive and adaptive to the speaker's speech and physiology,\ngeneralizable, robust, and is real-time executable."}
{"id": "2505.07653", "pdf": "https://arxiv.org/pdf/2505.07653.pdf", "abs": "https://arxiv.org/abs/2505.07653", "title": "JobHop: A Large-Scale Dataset of Career Trajectories", "authors": ["Iman Johary", "Raphael Romero", "Alexandru C. Mara", "Tijl De Bie"], "categories": ["cs.CL"], "comment": null, "summary": "Understanding labor market dynamics is essential for policymakers, employers,\nand job seekers. However, comprehensive datasets that capture real-world career\ntrajectories are scarce. In this paper, we introduce JobHop, a large-scale\npublic dataset derived from anonymized resumes provided by VDAB, the public\nemployment service in Flanders, Belgium. Utilizing Large Language Models\n(LLMs), we process unstructured resume data to extract structured career\ninformation, which is then mapped to standardized ESCO occupation codes using a\nmulti-label classification model. This results in a rich dataset of over 2.3\nmillion work experiences, extracted from and grouped into more than 391,000\nuser resumes and mapped to standardized ESCO occupation codes, offering\nvaluable insights into real-world occupational transitions. This dataset\nenables diverse applications, such as analyzing labor market mobility, job\nstability, and the effects of career breaks on occupational transitions. It\nalso supports career path prediction and other data-driven decision-making\nprocesses. To illustrate its potential, we explore key dataset characteristics,\nincluding job distributions, career breaks, and job transitions, demonstrating\nits value for advancing labor market research."}
{"id": "2310.14356", "pdf": "https://arxiv.org/pdf/2310.14356.pdf", "abs": "https://arxiv.org/abs/2310.14356", "title": "Semantic and Expressive Variation in Image Captions Across Languages", "authors": ["Andre Ye", "Sebastin Santy", "Jena D. Hwang", "Amy X. Zhang", "Ranjay Krishna"], "categories": ["cs.CV", "cs.CL", "cs.CY", "cs.HC"], "comment": "CVPR 2025", "summary": "Computer vision often treats human perception as homogeneous: an implicit\nassumption that visual stimuli are perceived similarly by everyone. This\nassumption is reflected in the way researchers collect datasets and train\nvision models. By contrast, literature in cross-cultural psychology and\nlinguistics has provided evidence that people from different cultural\nbackgrounds observe vastly different concepts even when viewing the same visual\nstimuli. In this paper, we study how these differences manifest themselves in\nvision-language datasets and models, using language as a proxy for culture. By\ncomparing textual descriptions generated across 7 languages for the same\nimages, we find significant differences in the semantic content and linguistic\nexpression. When datasets are multilingual as opposed to monolingual,\ndescriptions have higher semantic coverage on average, where coverage is\nmeasured using scene graphs, model embeddings, and linguistic taxonomies. For\nexample, multilingual descriptions have on average 29.9% more objects, 24.5%\nmore relations, and 46.0% more attributes than a set of monolingual captions.\nWhen prompted to describe images in different languages, popular models (e.g.\nLLaVA) inherit this bias and describe different parts of the image. Moreover,\nfinetuning models on captions from one language performs best on corresponding\ntest data from that language, while finetuning on multilingual data performs\nconsistently well across all test data compositions. Our work points towards\nthe need to account for and embrace the diversity of human perception in the\ncomputer vision community."}
{"id": "2505.07659", "pdf": "https://arxiv.org/pdf/2505.07659.pdf", "abs": "https://arxiv.org/abs/2505.07659", "title": "Using Information Theory to Characterize Prosodic Typology: The Case of Tone, Pitch-Accent and Stress-Accent", "authors": ["Ethan Gotlieb Wilcox", "Cui Ding", "Giovanni Acampa", "Tiago Pimentel", "Alex Warstadt", "Tamar I. Regev"], "categories": ["cs.CL"], "comment": null, "summary": "This paper argues that the relationship between lexical identity and prosody\n-- one well-studied parameter of linguistic variation -- can be characterized\nusing information theory. We predict that languages that use prosody to make\nlexical distinctions should exhibit a higher mutual information between word\nidentity and prosody, compared to languages that don't. We test this hypothesis\nin the domain of pitch, which is used to make lexical distinctions in tonal\nlanguages, like Cantonese. We use a dataset of speakers reading sentences aloud\nin ten languages across five language families to estimate the mutual\ninformation between the text and their pitch curves. We find that, across\nlanguages, pitch curves display similar amounts of entropy. However, these\ncurves are easier to predict given their associated text in the tonal\nlanguages, compared to pitch- and stress-accent languages, and thus the mutual\ninformation is higher in these languages, supporting our hypothesis. Our\nresults support perspectives that view linguistic typology as gradient, rather\nthan categorical."}
{"id": "2503.04318", "pdf": "https://arxiv.org/pdf/2503.04318.pdf", "abs": "https://arxiv.org/abs/2503.04318", "title": "InFL-UX: A Toolkit for Web-Based Interactive Federated Learning", "authors": ["Tim Maurer", "Abdulrahman Mohamed Selim", "Hasan Md Tusfiqur Alam", "Matthias Eiletz", "Michael Barz", "Daniel Sonntag"], "categories": ["cs.LG", "cs.HC"], "comment": "Accepted in the 17th ACM SIGCHI Symposium on Engineering Interactive\n  Computing Systems (EICS 2025)", "summary": "This paper presents InFL-UX, an interactive, proof-of-concept browser-based\nFederated Learning (FL) toolkit designed to integrate user contributions\nseamlessly into the machine learning (ML) workflow. InFL-UX enables users\nacross multiple devices to upload datasets, define classes, and collaboratively\ntrain classification models directly in the browser using modern web\ntechnologies. Unlike traditional FL toolkits, which often focus on backend\nsimulations, InFL-UX provides a simple user interface for researchers to\nexplore how users interact with and contribute to FL systems in real-world,\ninteractive settings. By prioritising usability and decentralised model\ntraining, InFL-UX bridges the gap between FL and Interactive Machine Learning\n(IML), empowering non-technical users to actively participate in ML\nclassification tasks."}
{"id": "2505.07671", "pdf": "https://arxiv.org/pdf/2505.07671.pdf", "abs": "https://arxiv.org/abs/2505.07671", "title": "Benchmarking Retrieval-Augmented Generation for Chemistry", "authors": ["Xianrui Zhong", "Bowen Jin", "Siru Ouyang", "Yanzhen Shen", "Qiao Jin", "Yin Fang", "Zhiyong Lu", "Jiawei Han"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Retrieval-augmented generation (RAG) has emerged as a powerful framework for\nenhancing large language models (LLMs) with external knowledge, particularly in\nscientific domains that demand specialized and dynamic information. Despite its\npromise, the application of RAG in the chemistry domain remains underexplored,\nprimarily due to the lack of high-quality, domain-specific corpora and\nwell-curated evaluation benchmarks. In this work, we introduce ChemRAG-Bench, a\ncomprehensive benchmark designed to systematically assess the effectiveness of\nRAG across a diverse set of chemistry-related tasks. The accompanying chemistry\ncorpus integrates heterogeneous knowledge sources, including scientific\nliterature, the PubChem database, PubMed abstracts, textbooks, and Wikipedia\nentries. In addition, we present ChemRAG-Toolkit, a modular and extensible RAG\ntoolkit that supports five retrieval algorithms and eight LLMs. Using\nChemRAG-Toolkit, we demonstrate that RAG yields a substantial performance gain\n-- achieving an average relative improvement of 17.4% over direct inference\nmethods. We further conduct in-depth analyses on retriever architectures,\ncorpus selection, and the number of retrieved passages, culminating in\npractical recommendations to guide future research and deployment of RAG\nsystems in the chemistry domain. The code and data is available at\nhttps://chemrag.github.io."}
{"id": "2505.05396", "pdf": "https://arxiv.org/pdf/2505.05396.pdf", "abs": "https://arxiv.org/abs/2505.05396", "title": "A Pain Assessment Framework based on multimodal data and Deep Machine Learning methods", "authors": ["Stefanos Gkikas"], "categories": ["cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "From the original abstract: This thesis initially aims to study the pain\nassessment process from a clinical-theoretical perspective while exploring and\nexamining existing automatic approaches. Building on this foundation, the\nprimary objective of this Ph.D. project is to develop innovative computational\nmethods for automatic pain assessment that achieve high performance and are\napplicable in real clinical settings. A primary goal is to thoroughly\ninvestigate and assess significant factors, including demographic elements that\nimpact pain perception, as recognized in pain research, through a computational\nstandpoint. Within the limits of the available data in this research area, our\ngoal was to design, develop, propose, and offer automatic pain assessment\npipelines for unimodal and multimodal configurations that are applicable to the\nspecific requirements of different scenarios. The studies published in this\nPh.D. thesis showcased the effectiveness of the proposed methods, achieving\nstate-of-the-art results. Additionally, they paved the way for exploring new\napproaches in artificial intelligence, foundation models, and generative\nartificial intelligence."}
{"id": "2505.07672", "pdf": "https://arxiv.org/pdf/2505.07672.pdf", "abs": "https://arxiv.org/abs/2505.07672", "title": "OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit", "authors": ["Arun S. Maiya"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "6 pages", "summary": "We present OnPrem.LLM, a Python-based toolkit for applying large language\nmodels (LLMs) to sensitive, non-public data in offline or restricted\nenvironments. The system is designed for privacy-preserving use cases and\nprovides prebuilt pipelines for document processing and storage,\nretrieval-augmented generation (RAG), information extraction, summarization,\nclassification, and prompt/output processing with minimal configuration.\nOnPrem.LLM supports multiple LLM backends -- including llama.cpp, Ollama, vLLM,\nand Hugging Face Transformers -- with quantized model support, GPU\nacceleration, and seamless backend switching. Although designed for fully local\nexecution, OnPrem.LLM also supports integration with a wide range of cloud LLM\nproviders when permitted, enabling hybrid deployments that balance performance\nwith data control. A no-code web interface extends accessibility to\nnon-technical users."}
{"id": "2505.07705", "pdf": "https://arxiv.org/pdf/2505.07705.pdf", "abs": "https://arxiv.org/abs/2505.07705", "title": "Codifying Character Logic in Role-Playing", "authors": ["Letian Peng", "Jingbo Shang"], "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces Codified Profiles for role-playing, a novel approach\nthat represents character logic as structured, executable functions for\nbehavioral decision-making. Each profile defines a set of functions\nparse_by_scene(scene) that outputs a list of logic-grounded assertions\ntriggered_statements, using both explicit control structures (e.g.,\nif-then-else) and condition checks like check_condition(scene, question), where\neach question is a semantically meaningful prompt about the scene (e.g., \"Is\nthe character in danger?\") discriminated by the role-playing LLM as true,\nfalse, or unknown. This explicit representation offers three key advantages\nover traditional prompt-based profiles, which append character descriptions\ndirectly into text prompts: (1) Persistence, by enforcing complete and\nconsistent execution of character logic, rather than relying on the model's\nimplicit reasoning; (2) Updatability, through systematic inspection and\nrevision of behavioral logic, which is difficult to track or debug in\nprompt-only approaches; (3) Controllable Randomness, by supporting stochastic\nbehavior directly within the logic, enabling fine-grained variability that\nprompting alone struggles to achieve. To validate these advantages, we\nintroduce a new benchmark constructed from 83 characters and 5,141 scenes\ncurated from Fandom, using NLI-based scoring to compare character responses\nagainst ground-truth actions. Our experiments demonstrate the significant\nbenefits of codified profiles in improving persistence, updatability, and\nbehavioral diversity. Notably, by offloading a significant portion of reasoning\nto preprocessing, codified profiles enable even 1B-parameter models to perform\nhigh-quality role-playing, providing a scalable and efficient foundation for\nlocal deployment of role-play agents."}
{"id": "2505.07731", "pdf": "https://arxiv.org/pdf/2505.07731.pdf", "abs": "https://arxiv.org/abs/2505.07731", "title": "Spoken Language Understanding on Unseen Tasks With In-Context Learning", "authors": ["Neeraj Agrawal", "Sriram Ganapathy"], "categories": ["cs.CL", "cs.LG", "eess.AS"], "comment": null, "summary": "Spoken language understanding (SLU) tasks involve diverse skills that probe\nthe information extraction, classification and/or generation capabilities of\nmodels. In this setting, task-specific training data may not always be\navailable. While traditional task-specific SLU models are unable to cater to\nsuch requirements, the speech-text large language models (LLMs) offer a\npromising alternative with emergent abilities. However, out of-the-box, our\nevaluations indicate that the zero/few-shot performance of prominent\nopen-source speech-text LLMs on SLU tasks are not up to the mark. In this\npaper, we introduce a novel approach to robust task-agnostic fine-tuning using\nrandomized class labels. With this proposed fine-tuning, we illustrate that the\nperformance of the speech-text LLMs on an unseen task is significantly improved\nover standard approaches. Critically, the proposed approach avoids the\nrequirement of task-specific data annotations for enabling new tasks in\nspeech-text LLMs."}
{"id": "2505.07775", "pdf": "https://arxiv.org/pdf/2505.07775.pdf", "abs": "https://arxiv.org/abs/2505.07775", "title": "Must Read: A Systematic Survey of Computational Persuasion", "authors": ["Nimet Beyza Bozdag", "Shuhaib Mehri", "Xiaocheng Yang", "Hyeonjeong Ha", "Zirui Cheng", "Esin Durmus", "Jiaxuan You", "Heng Ji", "Gokhan Tur", "Dilek Hakkani-Tür"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Persuasion is a fundamental aspect of communication, influencing\ndecision-making across diverse contexts, from everyday conversations to\nhigh-stakes scenarios such as politics, marketing, and law. The rise of\nconversational AI systems has significantly expanded the scope of persuasion,\nintroducing both opportunities and risks. AI-driven persuasion can be leveraged\nfor beneficial applications, but also poses threats through manipulation and\nunethical influence. Moreover, AI systems are not only persuaders, but also\nsusceptible to persuasion, making them vulnerable to adversarial attacks and\nbias reinforcement. Despite rapid advancements in AI-generated persuasive\ncontent, our understanding of what makes persuasion effective remains limited\ndue to its inherently subjective and context-dependent nature. In this survey,\nwe provide a comprehensive overview of computational persuasion, structured\naround three key perspectives: (1) AI as a Persuader, which explores\nAI-generated persuasive content and its applications; (2) AI as a Persuadee,\nwhich examines AI's susceptibility to influence and manipulation; and (3) AI as\na Persuasion Judge, which analyzes AI's role in evaluating persuasive\nstrategies, detecting manipulation, and ensuring ethical persuasion. We\nintroduce a taxonomy for computational persuasion research and discuss key\nchallenges, including evaluating persuasiveness, mitigating manipulative\npersuasion, and developing responsible AI-driven persuasive systems. Our survey\noutlines future research directions to enhance the safety, fairness, and\neffectiveness of AI-powered persuasion while addressing the risks posed by\nincreasingly capable language models."}
{"id": "2505.07784", "pdf": "https://arxiv.org/pdf/2505.07784.pdf", "abs": "https://arxiv.org/abs/2505.07784", "title": "Domain Regeneration: How well do LLMs match syntactic properties of text domains?", "authors": ["Da Ju", "Hagen Blix", "Adina Williams"], "categories": ["cs.CL"], "comment": null, "summary": "Recent improvement in large language model performance have, in all\nlikelihood, been accompanied by improvement in how well they can approximate\nthe distribution of their training data. In this work, we explore the following\nquestion: which properties of text domains do LLMs faithfully approximate, and\nhow well do they do so? Applying observational approaches familiar from corpus\nlinguistics, we prompt a commonly used, opensource LLM to regenerate text from\ntwo domains of permissively licensed English text which are often contained in\nLLM training data -- Wikipedia and news text. This regeneration paradigm allows\nus to investigate whether LLMs can faithfully match the original human text\ndomains in a fairly semantically-controlled setting. We investigate varying\nlevels of syntactic abstraction, from more simple properties like sentence\nlength, and article readability, to more complex and higher order properties\nsuch as dependency tag distribution, parse depth, and parse complexity. We find\nthat the majority of the regenerated distributions show a shifted mean, a lower\nstandard deviation, and a reduction of the long tail, as compared to the human\noriginals."}
{"id": "2505.07787", "pdf": "https://arxiv.org/pdf/2505.07787.pdf", "abs": "https://arxiv.org/abs/2505.07787", "title": "Learning from Peers in Reasoning Models", "authors": ["Tongxu Luo", "Wenyu Du", "Jiaxi Bi", "Stephen Chung", "Zhengyang Tang", "Hao Yang", "Min Zhang", "Benyou Wang"], "categories": ["cs.CL"], "comment": "29 pages, 32 figures", "summary": "Large Reasoning Models (LRMs) have the ability to self-correct even when they\nmake mistakes in their reasoning paths. However, our study reveals that when\nthe reasoning process starts with a short but poor beginning, it becomes\ndifficult for the model to recover. We refer to this phenomenon as the \"Prefix\nDominance Trap\". Inspired by psychological findings that peer interaction can\npromote self-correction without negatively impacting already accurate\nindividuals, we propose **Learning from Peers** (LeaP) to address this\nphenomenon. Specifically, every tokens, each reasoning path summarizes its\nintermediate reasoning and shares it with others through a routing mechanism,\nenabling paths to incorporate peer insights during inference. However, we\nobserve that smaller models sometimes fail to follow summarization and\nreflection instructions effectively. To address this, we fine-tune them into\nour **LeaP-T** model series. Experiments on AIME 2024, AIME 2025, AIMO 2025,\nand GPQA Diamond show that LeaP provides substantial improvements. For\ninstance, QwQ-32B with LeaP achieves nearly 5 absolute points higher than the\nbaseline on average, and surpasses DeepSeek-R1-671B on three math benchmarks\nwith an average gain of 3.3 points. Notably, our fine-tuned LeaP-T-7B matches\nthe performance of DeepSeek-R1-Distill-Qwen-14B on AIME 2024. In-depth analysis\nreveals LeaP's robust error correction by timely peer insights, showing strong\nerror tolerance and handling varied task difficulty. LeaP marks a milestone by\nenabling LRMs to collaborate during reasoning. Our code, datasets, and models\nare available at https://learning-from-peers.github.io/ ."}
{"id": "2505.07796", "pdf": "https://arxiv.org/pdf/2505.07796.pdf", "abs": "https://arxiv.org/abs/2505.07796", "title": "Learning Dynamics in Continual Pre-Training for Large Language Models", "authors": ["Xingjin Wang", "Howe Tissue", "Lu Wang", "Linjing Li", "Daniel Dajun Zeng"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ICML2025 (spotlight)", "summary": "Continual Pre-Training (CPT) has become a popular and effective method to\napply strong foundation models to specific downstream tasks. In this work, we\nexplore the learning dynamics throughout the CPT process for large language\nmodels. We specifically focus on how general and downstream domain performance\nevolves at each training step, with domain performance measured via validation\nlosses. We have observed that the CPT loss curve fundamentally characterizes\nthe transition from one curve to another hidden curve, and could be described\nby decoupling the effects of distribution shift and learning rate annealing. We\nderive a CPT scaling law that combines the two factors, enabling the prediction\nof loss at any (continual) training steps and across learning rate schedules\n(LRS) in CPT. Our formulation presents a comprehensive understanding of several\ncritical factors in CPT, including loss potential, peak learning rate, training\nsteps, replay ratio, etc. Moreover, our approach can be adapted to customize\ntraining hyper-parameters to different CPT goals such as balancing general and\ndomain-specific performance. Extensive experiments demonstrate that our scaling\nlaw holds across various CPT datasets and training hyper-parameters."}
{"id": "2505.07809", "pdf": "https://arxiv.org/pdf/2505.07809.pdf", "abs": "https://arxiv.org/abs/2505.07809", "title": "A Comparative Analysis of Static Word Embeddings for Hungarian", "authors": ["Máté Gedeon"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents a comprehensive analysis of various static word\nembeddings for Hungarian, including traditional models such as Word2Vec,\nFastText, as well as static embeddings derived from BERT-based models using\ndifferent extraction methods. We evaluate these embeddings on both intrinsic\nand extrinsic tasks to provide a holistic view of their performance. For\nintrinsic evaluation, we employ a word analogy task, which assesses the\nembeddings ability to capture semantic and syntactic relationships. Our results\nindicate that traditional static embeddings, particularly FastText, excel in\nthis task, achieving high accuracy and mean reciprocal rank (MRR) scores. Among\nthe BERT-based models, the X2Static method for extracting static embeddings\ndemonstrates superior performance compared to decontextualized and aggregate\nmethods, approaching the effectiveness of traditional static embeddings. For\nextrinsic evaluation, we utilize a bidirectional LSTM model to perform Named\nEntity Recognition (NER) and Part-of-Speech (POS) tagging tasks. The results\nreveal that embeddings derived from dynamic models, especially those extracted\nusing the X2Static method, outperform purely static embeddings. Notably, ELMo\nembeddings achieve the highest accuracy in both NER and POS tagging tasks,\nunderscoring the benefits of contextualized representations even when used in a\nstatic form. Our findings highlight the continued relevance of static word\nembeddings in NLP applications and the potential of advanced extraction methods\nto enhance the utility of BERT-based models. This piece of research contributes\nto the understanding of embedding performance in the Hungarian language and\nprovides valuable insights for future developments in the field. The training\nscripts, evaluation codes, restricted vocabulary, and extracted embeddings will\nbe made publicly available to support further research and reproducibility."}
{"id": "2505.06297", "pdf": "https://arxiv.org/pdf/2505.06297.pdf", "abs": "https://arxiv.org/abs/2505.06297", "title": "Lossless Compression of Large Language Model-Generated Text via Next-Token Prediction", "authors": ["Yu Mao", "Holger Pirk", "Chun Jason Xue"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "As large language models (LLMs) continue to be deployed and utilized across\ndomains, the volume of LLM-generated data is growing rapidly. This trend\nhighlights the increasing importance of effective and lossless compression for\nsuch data in modern text management systems. However, compressing LLM-generated\ndata presents unique challenges compared to traditional human- or\nmachine-generated content. Traditional machine-generated data is typically\nderived from computational processes or device outputs, often highly structured\nand limited to low-level elements like labels or numerical values. This\nstructure enables conventional lossless compressors to perform efficiently. In\ncontrast, LLM-generated data is more complex and diverse, requiring new\napproaches for effective compression. In this work, we conduct the first\nsystematic investigation of lossless compression techniques tailored\nspecifically to LLM-generated data. Notably, because LLMs are trained via\nnext-token prediction, we find that LLM-generated data is highly predictable\nfor the models themselves. This predictability enables LLMs to serve as\nefficient compressors of their own outputs. Through extensive experiments with\n14 representative LLMs and 8 LLM-generated datasets from diverse domains, we\nshow that LLM-based prediction methods achieve remarkable compression rates,\nexceeding 20x, far surpassing the 3x rate achieved by Gzip, a widely used\ngeneral-purpose compressor. Furthermore, this advantage holds across different\nLLM sizes and dataset types, demonstrating the robustness and practicality of\nLLM-based methods in lossless text compression under generative AI workloads."}
{"id": "2505.06313", "pdf": "https://arxiv.org/pdf/2505.06313.pdf", "abs": "https://arxiv.org/abs/2505.06313", "title": "AI Approaches to Qualitative and Quantitative News Analytics on NATO Unity", "authors": ["Bohdan M. Pavlyshenko"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.SI"], "comment": null, "summary": "The paper considers the use of GPT models with retrieval-augmented generation\n(RAG) for qualitative and quantitative analytics on NATO sentiments, NATO unity\nand NATO Article 5 trust opinion scores in different web sources: news sites\nfound via Google Search API, Youtube videos with comments, and Reddit\ndiscussions. A RAG approach using GPT-4.1 model was applied to analyse news\nwhere NATO related topics were discussed. Two levels of RAG analytics were\nused: on the first level, the GPT model generates qualitative news summaries\nand quantitative opinion scores using zero-shot prompts; on the second level,\nthe GPT model generates the summary of news summaries. Quantitative news\nopinion scores generated by the GPT model were analysed using Bayesian\nregression to get trend lines. The distributions found for the regression\nparameters make it possible to analyse an uncertainty in specified news opinion\nscore trends. Obtained results show a downward trend for analysed scores of\nopinion related to NATO unity.\n  This approach does not aim to conduct real political analysis; rather, it\nconsider AI based approaches which can be used for further analytics\n  as a part of a complex analytical approach. The obtained results demonstrate\nthat the use of GPT models for news analysis can give informative qualitative\nand quantitative analytics, providing important insights.\n  The dynamic model based on neural ordinary differential equations was\nconsidered for modelling public opinions. This approach makes it possible to\nanalyse different scenarios for evolving public opinions."}
{"id": "2505.06320", "pdf": "https://arxiv.org/pdf/2505.06320.pdf", "abs": "https://arxiv.org/abs/2505.06320", "title": "Divide (Text) and Conquer (Sentiment): Improved Sentiment Classification by Constituent Conflict Resolution", "authors": ["Jan Kościałkowski", "Paweł Marcinkowski"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "8 pages, 6 figures, 4 tables, developed as a final project for the\n  Stanford Center for Professional Education XCS224U (Natural Language\n  Understanding) course", "summary": "Sentiment classification, a complex task in natural language processing,\nbecomes even more challenging when analyzing passages with multiple conflicting\ntones. Typically, longer passages exacerbate this issue, leading to decreased\nmodel performance. The aim of this paper is to introduce novel methodologies\nfor isolating conflicting sentiments and aggregating them to effectively\npredict the overall sentiment of such passages. One of the aggregation\nstrategies involves a Multi-Layer Perceptron (MLP) model which outperforms\nbaseline models across various datasets, including Amazon, Twitter, and SST\nwhile costing $\\sim$1/100 of what fine-tuning the baseline would take."}
{"id": "2505.06653", "pdf": "https://arxiv.org/pdf/2505.06653.pdf", "abs": "https://arxiv.org/abs/2505.06653", "title": "Improving Block-Wise LLM Quantization by 4-bit Block-Wise Optimal Float (BOF4): Analysis and Variations", "authors": ["Patrick Blumenberg", "Thomas Graave", "Tim Fingscheidt"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) demand extensive memory capacity during both\nfine-tuning and inference. To enable memory-efficient fine-tuning, existing\nmethods apply block-wise quantization techniques, such as NF4 and AF4, to the\nnetwork weights. We show that these quantization techniques incur suboptimal\nquantization errors. Therefore, as a first novelty, we propose an optimization\napproach for block-wise quantization. Using this method, we design a family of\nquantizers named 4-bit block-wise optimal float (BOF4), which consistently\nreduces the quantization error compared to both baseline methods. We provide\nboth a theoretical and a data-driven solution for the optimization process and\nprove their practical equivalence. Secondly, we propose a modification to the\nemployed normalization method based on the signed absolute block maximum\n(BOF4-S), enabling further reduction of the quantization error and empirically\nachieving less degradation in language modeling performance. Thirdly, we\nexplore additional variations of block-wise quantization methods applied to\nLLMs through an experimental study on the importance of accurately representing\nzero and large-amplitude weights on the one hand, and optimization towards\nvarious error metrics on the other hand. Lastly, we introduce a mixed-precision\nquantization strategy dubbed outlier-preserving quantization (OPQ) to address\nthe distributional mismatch induced by outlier weights in block-wise\nquantization. By storing outlier weights in 16-bit precision (OPQ) while\napplying BOF4-S, we achieve top performance among 4-bit block-wise quantization\ntechniques w.r.t. perplexity."}
{"id": "2505.06803", "pdf": "https://arxiv.org/pdf/2505.06803.pdf", "abs": "https://arxiv.org/abs/2505.06803", "title": "Bridging Ears and Eyes: Analyzing Audio and Visual Large Language Models to Humans in Visible Sound Recognition and Reducing Their Sensory Gap via Cross-Modal Distillation", "authors": ["Xilin Jiang", "Junkai Wu", "Vishal Choudhari", "Nima Mesgarani"], "categories": ["cs.SD", "cs.CL", "cs.CV", "cs.MM", "eess.AS"], "comment": null, "summary": "Audio large language models (LLMs) are considered experts at recognizing\nsound objects, yet their performance relative to LLMs in other sensory\nmodalities, such as visual or audio-visual LLMs, and to humans using their\nears, eyes, or both remains unexplored. To investigate this, we systematically\nevaluate audio, visual, and audio-visual LLMs, specifically Qwen2-Audio,\nQwen2-VL, and Qwen2.5-Omni, against humans in recognizing sound objects of\ndifferent classes from audio-only, silent video, or sounded video inputs. We\nuncover a performance gap between Qwen2-Audio and Qwen2-VL that parallels the\nsensory discrepancy between human ears and eyes. To reduce this gap, we\nintroduce a cross-modal distillation framework, where an LLM in one modality\nserves as the teacher and another as the student, with knowledge transfer in\nsound classes predicted as more challenging to the student by a heuristic\nmodel. Distillation in both directions, from Qwen2-VL to Qwen2-Audio and vice\nversa, leads to notable improvements, particularly in challenging classes. This\nwork highlights the sensory gap in LLMs from a human-aligned perspective and\nproposes a principled approach to enhancing modality-specific perception in\nmultimodal LLMs."}
{"id": "2505.06814", "pdf": "https://arxiv.org/pdf/2505.06814.pdf", "abs": "https://arxiv.org/abs/2505.06814", "title": "Overview of the NLPCC 2025 Shared Task 4: Multi-modal, Multilingual, and Multi-hop Medical Instructional Video Question Answering Challenge", "authors": ["Bin Li", "Shenxi Liu", "Yixuan Weng", "Yue Du", "Yuhang Tian", "Shoujun Zhou"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "12 pages, 5 figures, 4 tables", "summary": "Following the successful hosts of the 1-st (NLPCC 2023 Foshan) CMIVQA and the\n2-rd (NLPCC 2024 Hangzhou) MMIVQA challenges, this year, a new task has been\nintroduced to further advance research in multi-modal, multilingual, and\nmulti-hop medical instructional question answering (M4IVQA) systems, with a\nspecific focus on medical instructional videos. The M4IVQA challenge focuses on\nevaluating models that integrate information from medical instructional videos,\nunderstand multiple languages, and answer multi-hop questions requiring\nreasoning over various modalities. This task consists of three tracks:\nmulti-modal, multilingual, and multi-hop Temporal Answer Grounding in Single\nVideo (M4TAGSV), multi-modal, multilingual, and multi-hop Video Corpus\nRetrieval (M4VCR) and multi-modal, multilingual, and multi-hop Temporal Answer\nGrounding in Video Corpus (M4TAGVC). Participants in M4IVQA are expected to\ndevelop algorithms capable of processing both video and text data,\nunderstanding multilingual queries, and providing relevant answers to multi-hop\nmedical questions. We believe the newly introduced M4IVQA challenge will drive\ninnovations in multimodal reasoning systems for healthcare scenarios,\nultimately contributing to smarter emergency response systems and more\neffective medical education platforms in multilingual communities. Our official\nwebsite is https://cmivqa.github.io/"}
{"id": "2505.06843", "pdf": "https://arxiv.org/pdf/2505.06843.pdf", "abs": "https://arxiv.org/abs/2505.06843", "title": "Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety", "authors": ["Zihan Guan", "Mengxuan Hu", "Ronghang Zhu", "Sheng Li", "Anil Vullikanti"], "categories": ["cs.LG", "cs.CL"], "comment": "26 pages, 13 figures", "summary": "Recent studies have uncovered a troubling vulnerability in the fine-tuning\nstage of large language models (LLMs): even fine-tuning on entirely benign\ndatasets can lead to a significant increase in the harmfulness of LLM outputs.\nBuilding on this finding, our red teaming study takes this threat one step\nfurther by developing a more effective attack. Specifically, we analyze and\nidentify samples within benign datasets that contribute most to safety\ndegradation, then fine-tune LLMs exclusively on these samples. We approach this\nproblem from an outlier detection perspective and propose Self-Inf-N, to detect\nand extract outliers for fine-tuning. Our findings reveal that fine-tuning LLMs\non 100 outlier samples selected by Self-Inf-N in the benign datasets severely\ncompromises LLM safety alignment. Extensive experiments across seven mainstream\nLLMs demonstrate that our attack exhibits high transferability across different\narchitectures and remains effective in practical scenarios. Alarmingly, our\nresults indicate that most existing mitigation strategies fail to defend\nagainst this attack, underscoring the urgent need for more robust alignment\nsafeguards. Codes are available at\nhttps://github.com/GuanZihan/Benign-Samples-Matter."}
{"id": "2505.06898", "pdf": "https://arxiv.org/pdf/2505.06898.pdf", "abs": "https://arxiv.org/abs/2505.06898", "title": "Multi-Modal Explainable Medical AI Assistant for Trustworthy Human-AI Collaboration", "authors": ["Honglong Yang", "Shanshan Song", "Yi Qin", "Lehan Wang", "Haonan Wang", "Xinpeng Ding", "Qixiang Zhang", "Bodong Du", "Xiaomeng Li"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Generalist Medical AI (GMAI) systems have demonstrated expert-level\nperformance in biomedical perception tasks, yet their clinical utility remains\nlimited by inadequate multi-modal explainability and suboptimal prognostic\ncapabilities. Here, we present XMedGPT, a clinician-centric, multi-modal AI\nassistant that integrates textual and visual interpretability to support\ntransparent and trustworthy medical decision-making. XMedGPT not only produces\naccurate diagnostic and descriptive outputs, but also grounds referenced\nanatomical sites within medical images, bridging critical gaps in\ninterpretability and enhancing clinician usability. To support real-world\ndeployment, we introduce a reliability indexing mechanism that quantifies\nuncertainty through consistency-based assessment via interactive\nquestion-answering. We validate XMedGPT across four pillars: multi-modal\ninterpretability, uncertainty quantification, and prognostic modeling, and\nrigorous benchmarking. The model achieves an IoU of 0.703 across 141 anatomical\nregions, and a Kendall's tau-b of 0.479, demonstrating strong alignment between\nvisual rationales and clinical outcomes. For uncertainty estimation, it attains\nan AUC of 0.862 on visual question answering and 0.764 on radiology report\ngeneration. In survival and recurrence prediction for lung and glioma cancers,\nit surpasses prior leading models by 26.9%, and outperforms GPT-4o by 25.0%.\nRigorous benchmarking across 347 datasets covers 40 imaging modalities and\nexternal validation spans 4 anatomical systems confirming exceptional\ngeneralizability, with performance gains surpassing existing GMAI by 20.7% for\nin-domain evaluation and 16.7% on 11,530 in-house data evaluation. Together,\nXMedGPT represents a significant leap forward in clinician-centric AI\nintegration, offering trustworthy and scalable support for diverse healthcare\napplications."}
{"id": "2505.06938", "pdf": "https://arxiv.org/pdf/2505.06938.pdf", "abs": "https://arxiv.org/abs/2505.06938", "title": "A digital perspective on the role of a stemma in material-philological transmission studies", "authors": ["Katarzyna Anna Kapitan"], "categories": ["cs.DL", "cs.CL"], "comment": null, "summary": "Taking its point of departure in the recent developments in the field of\ndigital humanities and the increasing automatisation of scholarly workflows,\nthis study explores the implications of digital approaches to textual\ntraditions for the broader field of textual scholarship. It argues that the\nrelative simplicity of creating computergenerated stemmas allows us to view the\nstemma codicum as a research tool rather than the final product of our\nscholarly investigation. Using the Old Norse saga of Hr\\'omundur as a case\nstudy, this article demonstrates that stemmas can serve as a starting point for\nexploring textual traditions further. In doing so, they enable us to address\nresearch questions that otherwise remain unanswered. The article is accompanied\nby datasets used to generate stemmas for the Hr\\'omundar saga tradition as well\nas two custom Python scripts. The scripts are designed to convert XML-based\ntextual data, encoded according to the TEI Guidelines, into the input format\nused for the analysis in the PHYLIP package to generate unrooted trees of\nrelationships between texts."}
{"id": "2505.06972", "pdf": "https://arxiv.org/pdf/2505.06972.pdf", "abs": "https://arxiv.org/abs/2505.06972", "title": "Web Page Classification using LLMs for Crawling Support", "authors": ["Yuichi Sasazawa", "Yasuhiro Sogawa"], "categories": ["cs.IR", "cs.CL"], "comment": "8 pages, 2 figures", "summary": "A web crawler is a system designed to collect web pages, and efficient\ncrawling of new pages requires appropriate algorithms. While website features\nsuch as XML sitemaps and the frequency of past page updates provide important\nclues for accessing new pages, their universal application across diverse\nconditions is challenging. In this study, we propose a method to efficiently\ncollect new pages by classifying web pages into two types, \"Index Pages\" and\n\"Content Pages,\" using a large language model (LLM), and leveraging the\nclassification results to select index pages as starting points for accessing\nnew pages. We construct a dataset with automatically annotated web page types\nand evaluate our approach from two perspectives: the page type classification\nperformance and coverage of new pages. Experimental results demonstrate that\nthe LLM-based method outperformed baseline methods in both evaluation metrics."}
{"id": "2505.06993", "pdf": "https://arxiv.org/pdf/2505.06993.pdf", "abs": "https://arxiv.org/abs/2505.06993", "title": "Towards the Three-Phase Dynamics of Generalization Power of a DNN", "authors": ["Yuxuan He", "Junpeng Zhang", "Hongyuan Zhang", "Quanshi Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper proposes a new perspective for analyzing the generalization power\nof deep neural networks (DNNs), i.e., directly disentangling and analyzing the\ndynamics of generalizable and non-generalizable interaction encoded by a DNN\nthrough the training process. Specifically, this work builds upon the recent\ntheoretical achievement in explainble AI, which proves that the detailed\ninference logic of DNNs can be can be strictly rewritten as a small number of\nAND-OR interaction patterns. Based on this, we propose an efficient method to\nquantify the generalization power of each interaction, and we discover a\ndistinct three-phase dynamics of the generalization power of interactions\nduring training. In particular, the early phase of training typically removes\nnoisy and non-generalizable interactions and learns simple and generalizable\nones. The second and the third phases tend to capture increasingly complex\ninteractions that are harder to generalize. Experimental results verify that\nthe learning of non-generalizable interactions is the the direct cause for the\ngap between the training and testing losses."}
{"id": "2505.07027", "pdf": "https://arxiv.org/pdf/2505.07027.pdf", "abs": "https://arxiv.org/abs/2505.07027", "title": "LLM-Augmented Chemical Synthesis and Design Decision Programs", "authors": ["Haorui Wang", "Jeff Guo", "Lingkai Kong", "Rampi Ramprasad", "Philippe Schwaller", "Yuanqi Du", "Chao Zhang"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.NE", "physics.chem-ph"], "comment": null, "summary": "Retrosynthesis, the process of breaking down a target molecule into simpler\nprecursors through a series of valid reactions, stands at the core of organic\nchemistry and drug development. Although recent machine learning (ML) research\nhas advanced single-step retrosynthetic modeling and subsequent route searches,\nthese solutions remain restricted by the extensive combinatorial space of\npossible pathways. Concurrently, large language models (LLMs) have exhibited\nremarkable chemical knowledge, hinting at their potential to tackle complex\ndecision-making tasks in chemistry. In this work, we explore whether LLMs can\nsuccessfully navigate the highly constrained, multi-step retrosynthesis\nplanning problem. We introduce an efficient scheme for encoding reaction\npathways and present a new route-level search strategy, moving beyond the\nconventional step-by-step reactant prediction. Through comprehensive\nevaluations, we show that our LLM-augmented approach excels at retrosynthesis\nplanning and extends naturally to the broader challenge of synthesizable\nmolecular design."}
{"id": "2505.07155", "pdf": "https://arxiv.org/pdf/2505.07155.pdf", "abs": "https://arxiv.org/abs/2505.07155", "title": "Reassessing Large Language Model Boolean Query Generation for Systematic Reviews", "authors": ["Shuai Wang", "Harrisen Scells", "Bevan Koopman", "Guido Zuccon"], "categories": ["cs.IR", "cs.CL"], "comment": "Accepted in SIGIR-2025", "summary": "Systematic reviews are comprehensive literature reviews that address highly\nfocused research questions and represent the highest form of evidence in\nmedicine. A critical step in this process is the development of complex Boolean\nqueries to retrieve relevant literature. Given the difficulty of manually\nconstructing these queries, recent efforts have explored Large Language Models\n(LLMs) to assist in their formulation. One of the first studies,Wang et al.,\ninvestigated ChatGPT for this task, followed by Staudinger et al., which\nevaluated multiple LLMs in a reproducibility study. However, the latter\noverlooked several key aspects of the original work, including (i) validation\nof generated queries, (ii) output formatting constraints, and (iii) selection\nof examples for chain-of-thought (Guided) prompting. As a result, its findings\ndiverged significantly from the original study. In this work, we systematically\nreproduce both studies while addressing these overlooked factors. Our results\nshow that query effectiveness varies significantly across models and prompt\ndesigns, with guided query formulation benefiting from well-chosen seed\nstudies. Overall, prompt design and model selection are key drivers of\nsuccessful query formulation. Our findings provide a clearer understanding of\nLLMs' potential in Boolean query generation and highlight the importance of\nmodel- and prompt-specific optimisations. The complex nature of systematic\nreviews adds to challenges in both developing and reproducing methods but also\nhighlights the importance of reproducibility studies in this domain."}
{"id": "2505.07166", "pdf": "https://arxiv.org/pdf/2505.07166.pdf", "abs": "https://arxiv.org/abs/2505.07166", "title": "Pre-training vs. Fine-tuning: A Reproducibility Study on Dense Retrieval Knowledge Acquisition", "authors": ["Zheng Yao", "Shuai Wang", "Guido Zuccon"], "categories": ["cs.IR", "cs.CL"], "comment": "Accepted in SIGIR-2025", "summary": "Dense retrievers utilize pre-trained backbone language models (e.g., BERT,\nLLaMA) that are fine-tuned via contrastive learning to perform the task of\nencoding text into sense representations that can be then compared via a\nshallow similarity operation, e.g. inner product. Recent research has\nquestioned the role of fine-tuning vs. that of pre-training within dense\nretrievers, specifically arguing that retrieval knowledge is primarily gained\nduring pre-training, meaning knowledge not acquired during pre-training cannot\nbe sub-sequentially acquired via fine-tuning. We revisit this idea here as the\nclaim was only studied in the context of a BERT-based encoder using DPR as\nrepresentative dense retriever. We extend the previous analysis by testing\nother representation approaches (comparing the use of CLS tokens with that of\nmean pooling), backbone architectures (encoder-only BERT vs. decoder-only\nLLaMA), and additional datasets (MSMARCO in addition to Natural Questions). Our\nstudy confirms that in DPR tuning, pre-trained knowledge underpins retrieval\nperformance, with fine-tuning primarily adjusting neuron activation rather than\nreorganizing knowledge. However, this pattern does not hold universally, such\nas in mean-pooled (Contriever) and decoder-based (LLaMA) models. We ensure full\nreproducibility and make our implementation publicly available at\nhttps://github.com/ielab/DenseRetriever-Knowledge-Acquisition."}
{"id": "2505.07167", "pdf": "https://arxiv.org/pdf/2505.07167.pdf", "abs": "https://arxiv.org/abs/2505.07167", "title": "One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models", "authors": ["Haoran Gu", "Handing Wang", "Yi Mei", "Mengjie Zhang", "Yaochu Jin"], "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have been extensively used across diverse\ndomains, including virtual assistants, automated code generation, and\nscientific research. However, they remain vulnerable to jailbreak attacks,\nwhich manipulate the models into generating harmful responses despite safety\nalignment. Recent studies have shown that current safety-aligned LLMs often\nundergo the shallow safety alignment, where the first few tokens largely\ndetermine whether the response will be harmful. Through comprehensive\nobservations, we find that safety-aligned LLMs and various defense strategies\ngenerate highly similar initial tokens in their refusal responses, which we\ndefine as safety trigger tokens. Building on this insight, we propose\n\\texttt{D-STT}, a simple yet effective defense algorithm that identifies and\nexplicitly decodes safety trigger tokens of the given safety-aligned LLM to\ntrigger the model's learned safety patterns. In this process, the safety\ntrigger is constrained to a single token, which effectively preserves model\nusability by introducing minimum intervention in the decoding process.\nExtensive experiments across diverse jailbreak attacks and benign prompts\ndemonstrate that \\ours significantly reduces output harmfulness while\npreserving model usability and incurring negligible response time overhead,\noutperforming ten baseline methods."}
{"id": "2505.07188", "pdf": "https://arxiv.org/pdf/2505.07188.pdf", "abs": "https://arxiv.org/abs/2505.07188", "title": "Securing Genomic Data Against Inference Attacks in Federated Learning Environments", "authors": ["Chetan Pathade", "Shubham Patil"], "categories": ["cs.CR", "cs.CL"], "comment": "10 Pages, 7 Figures", "summary": "Federated Learning (FL) offers a promising framework for collaboratively\ntraining machine learning models across decentralized genomic datasets without\ndirect data sharing. While this approach preserves data locality, it remains\nsusceptible to sophisticated inference attacks that can compromise individual\nprivacy. In this study, we simulate a federated learning setup using synthetic\ngenomic data and assess its vulnerability to three key attack vectors:\nMembership Inference Attack (MIA), Gradient-Based Membership Inference Attack,\nand Label Inference Attack (LIA). Our experiments reveal that Gradient-Based\nMIA achieves the highest effectiveness, with a precision of 0.79 and F1-score\nof 0.87, underscoring the risk posed by gradient exposure in federated updates.\nAdditionally, we visualize comparative attack performance through radar plots\nand quantify model leakage across clients. The findings emphasize the\ninadequacy of na\\\"ive FL setups in safeguarding genomic privacy and motivate\nthe development of more robust privacy-preserving mechanisms tailored to the\nunique sensitivity of genomic data."}
{"id": "2505.07365", "pdf": "https://arxiv.org/pdf/2505.07365.pdf", "abs": "https://arxiv.org/abs/2505.07365", "title": "Multi-Domain Audio Question Answering Toward Acoustic Content Reasoning in The DCASE 2025 Challenge", "authors": ["Chao-Han Huck Yang", "Sreyan Ghosh", "Qing Wang", "Jaeyeon Kim", "Hengyi Hong", "Sonal Kumar", "Guirui Zhong", "Zhifeng Kong", "S Sakshi", "Vaibhavi Lokegaonkar", "Oriol Nieto", "Ramani Duraiswami", "Dinesh Manocha", "Gunhee Kim", "Jun Du", "Rafael Valle", "Bryan Catanzaro"], "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.MM", "eess.AS"], "comment": "Preprint. DCASE 2025 Audio QA Challenge:\n  https://dcase.community/challenge2025/task-audio-question-answering", "summary": "We present Task 5 of the DCASE 2025 Challenge: an Audio Question Answering\n(AQA) benchmark spanning multiple domains of sound understanding. This task\ndefines three QA subsets (Bioacoustics, Temporal Soundscapes, and Complex QA)\nto test audio-language models on interactive question-answering over diverse\nacoustic scenes. We describe the dataset composition (from marine mammal calls\nto soundscapes and complex real-world clips), the evaluation protocol (top-1\naccuracy with answer-shuffling robustness), and baseline systems\n(Qwen2-Audio-7B, AudioFlamingo 2, Gemini-2-Flash). Preliminary results on the\ndevelopment set are compared, showing strong variation across models and\nsubsets. This challenge aims to advance the audio understanding and reasoning\ncapabilities of audio-language models toward human-level acuity, which are\ncrucial for enabling AI agents to perceive and interact about the world\neffectively."}
{"id": "2505.07460", "pdf": "https://arxiv.org/pdf/2505.07460.pdf", "abs": "https://arxiv.org/abs/2505.07460", "title": "A Survey on Collaborative Mechanisms Between Large and Small Language Models", "authors": ["Yi Chen", "JiaHao Zhao", "HaoHao Han"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) deliver powerful AI capabilities but face\ndeployment challenges due to high resource costs and latency, whereas Small\nLanguage Models (SLMs) offer efficiency and deployability at the cost of\nreduced performance. Collaboration between LLMs and SLMs emerges as a crucial\nparadigm to synergistically balance these trade-offs, enabling advanced AI\napplications, especially on resource-constrained edge devices. This survey\nprovides a comprehensive overview of LLM-SLM collaboration, detailing various\ninteraction mechanisms (pipeline, routing, auxiliary, distillation, fusion),\nkey enabling technologies, and diverse application scenarios driven by\non-device needs like low latency, privacy, personalization, and offline\noperation. While highlighting the significant potential for creating more\nefficient, adaptable, and accessible AI, we also discuss persistent challenges\nincluding system overhead, inter-model consistency, robust task allocation,\nevaluation complexity, and security/privacy concerns. Future directions point\ntowards more intelligent adaptive frameworks, deeper model fusion, and\nexpansion into multimodal and embodied AI, positioning LLM-SLM collaboration as\na key driver for the next generation of practical and ubiquitous artificial\nintelligence."}
{"id": "2505.07558", "pdf": "https://arxiv.org/pdf/2505.07558.pdf", "abs": "https://arxiv.org/abs/2505.07558", "title": "Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models", "authors": ["Rei Higuchi", "Taiji Suzuki"], "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": null, "summary": "Aligning large language models (LLMs) with human preferences is crucial for\nsafe deployment, yet existing methods assume specific preference models like\nBradley-Terry model. This assumption leads to statistical inconsistency, where\nmore data doesn't guarantee convergence to true human preferences. To address\nthis critical gap, we introduce a novel alignment method Direct Density Ratio\nOptimization (DDRO). DDRO directly estimates the density ratio between\npreferred and unpreferred output distributions, circumventing the need for\nexplicit human preference modeling. We theoretically prove that DDRO is\nstatistically consistent, ensuring convergence to the true preferred\ndistribution as the data size grows, regardless of the underlying preference\nstructure. Experiments demonstrate that DDRO achieves superior performance\ncompared to existing methods on many major benchmarks. DDRO unlocks the\npotential for truly data-driven alignment, paving the way for more reliable and\nhuman-aligned LLMs."}
{"id": "2505.07704", "pdf": "https://arxiv.org/pdf/2505.07704.pdf", "abs": "https://arxiv.org/abs/2505.07704", "title": "Through the Looking Glass: Common Sense Consistency Evaluation of Weird Images", "authors": ["Elisei Rykov", "Kseniia Petrushina", "Kseniia Titova", "Anton Razzhigaev", "Alexander Panchenko", "Vasily Konovalov"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Measuring how real images look is a complex task in artificial intelligence\nresearch. For example, an image of a boy with a vacuum cleaner in a desert\nviolates common sense. We introduce a novel method, which we call Through the\nLooking Glass (TLG), to assess image common sense consistency using Large\nVision-Language Models (LVLMs) and Transformer-based encoder. By leveraging\nLVLMs to extract atomic facts from these images, we obtain a mix of accurate\nfacts. We proceed by fine-tuning a compact attention-pooling classifier over\nencoded atomic facts. Our TLG has achieved a new state-of-the-art performance\non the WHOOPS! and WEIRD datasets while leveraging a compact fine-tuning\ncomponent."}
{"id": "2505.07768", "pdf": "https://arxiv.org/pdf/2505.07768.pdf", "abs": "https://arxiv.org/abs/2505.07768", "title": "Enhancing Code Generation via Bidirectional Comment-Level Mutual Grounding", "authors": ["Yifeng Di", "Tianyi Zhang"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "Accepted to ICSE 2025", "summary": "Large Language Models (LLMs) have demonstrated unprecedented capability in\ncode generation. However, LLM-generated code is still plagued with a wide range\nof functional errors, especially for complex programming tasks that LLMs have\nnot seen before. Recent studies have shown that developers often struggle with\ninspecting and fixing incorrect code generated by LLMs, diminishing their\nproductivity and trust in LLM-based code generation. Inspired by the mutual\ngrounding theory in communication, we propose an interactive approach that\nleverages code comments as a medium for developers and LLMs to establish a\nshared understanding. Our approach facilitates iterative grounding by\ninterleaving code generation, inline comment generation, and contextualized\nuser feedback through editable comments to align generated code with developer\nintent. We evaluated our approach on two popular benchmarks and demonstrated\nthat our approach significantly improved multiple state-of-the-art LLMs, e.g.,\n17.1% pass@1 improvement for code-davinci-002 on HumanEval. Furthermore, we\nconducted a user study with 12 participants in comparison to two baselines: (1)\ninteracting with GitHub Copilot, and (2) interacting with a multi-step code\ngeneration paradigm called Multi-Turn Program Synthesis. Participants completed\nthe given programming tasks 16.7% faster and with 10.5% improvement in task\nsuccess rate when using our approach. Both results show that interactively\nrefining code comments enables the collaborative establishment of mutual\ngrounding, leading to more accurate code generation and higher developer\nconfidence."}
{"id": "2306.09597", "pdf": "https://arxiv.org/pdf/2306.09597.pdf", "abs": "https://arxiv.org/abs/2306.09597", "title": "Clickbait Detection via Large Language Models", "authors": ["Han Wang", "Yi Zhu", "Ye Wang", "Yun Li", "Yunhao Yuan", "Jipeng Qiang"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 4 figures", "summary": "Clickbait, which aims to induce users with some surprising and even thrilling\nheadlines for increasing click-through rates, permeates almost all online\ncontent publishers, such as news portals and social media. Recently, Large\nLanguage Models (LLMs) have emerged as a powerful instrument and achieved\ntremendous success in a series of NLP downstream tasks. However, it is not yet\nknown whether LLMs can be served as a high-quality clickbait detection system.\nIn this paper, we analyze the performance of LLMs in the few-shot and zero-shot\nscenarios on several English and Chinese benchmark datasets. Experimental\nresults show that LLMs cannot achieve the best results compared to the\nstate-of-the-art deep and fine-tuning PLMs methods. Different from human\nintuition, the experiments demonstrated that LLMs cannot make satisfied\nclickbait detection just by the headlines."}
{"id": "2310.13548", "pdf": "https://arxiv.org/pdf/2310.13548.pdf", "abs": "https://arxiv.org/abs/2310.13548", "title": "Towards Understanding Sycophancy in Language Models", "authors": ["Mrinank Sharma", "Meg Tong", "Tomasz Korbak", "David Duvenaud", "Amanda Askell", "Samuel R. Bowman", "Newton Cheng", "Esin Durmus", "Zac Hatfield-Dodds", "Scott R. Johnston", "Shauna Kravec", "Timothy Maxwell", "Sam McCandlish", "Kamal Ndousse", "Oliver Rausch", "Nicholas Schiefer", "Da Yan", "Miranda Zhang", "Ethan Perez"], "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML", "I.2.6"], "comment": "32 pages, 20 figures", "summary": "Human feedback is commonly utilized to finetune AI assistants. But human\nfeedback may also encourage model responses that match user beliefs over\ntruthful ones, a behaviour known as sycophancy. We investigate the prevalence\nof sycophancy in models whose finetuning procedure made use of human feedback,\nand the potential role of human preference judgments in such behavior. We first\ndemonstrate that five state-of-the-art AI assistants consistently exhibit\nsycophancy across four varied free-form text-generation tasks. To understand if\nhuman preferences drive this broadly observed behavior, we analyze existing\nhuman preference data. We find that when a response matches a user's views, it\nis more likely to be preferred. Moreover, both humans and preference models\n(PMs) prefer convincingly-written sycophantic responses over correct ones a\nnon-negligible fraction of the time. Optimizing model outputs against PMs also\nsometimes sacrifices truthfulness in favor of sycophancy. Overall, our results\nindicate that sycophancy is a general behavior of state-of-the-art AI\nassistants, likely driven in part by human preference judgments favoring\nsycophantic responses."}
{"id": "2312.11805", "pdf": "https://arxiv.org/pdf/2312.11805.pdf", "abs": "https://arxiv.org/abs/2312.11805", "title": "Gemini: A Family of Highly Capable Multimodal Models", "authors": ["Gemini Team", "Rohan Anil", "Sebastian Borgeaud", "Jean-Baptiste Alayrac", "Jiahui Yu", "Radu Soricut", "Johan Schalkwyk", "Andrew M. Dai", "Anja Hauth", "Katie Millican", "David Silver", "Melvin Johnson", "Ioannis Antonoglou", "Julian Schrittwieser", "Amelia Glaese", "Jilin Chen", "Emily Pitler", "Timothy Lillicrap", "Angeliki Lazaridou", "Orhan Firat", "James Molloy", "Michael Isard", "Paul R. Barham", "Tom Hennigan", "Benjamin Lee", "Fabio Viola", "Malcolm Reynolds", "Yuanzhong Xu", "Ryan Doherty", "Eli Collins", "Clemens Meyer", "Eliza Rutherford", "Erica Moreira", "Kareem Ayoub", "Megha Goel", "Jack Krawczyk", "Cosmo Du", "Ed Chi", "Heng-Tze Cheng", "Eric Ni", "Purvi Shah", "Patrick Kane", "Betty Chan", "Manaal Faruqui", "Aliaksei Severyn", "Hanzhao Lin", "YaGuang Li", "Yong Cheng", "Abe Ittycheriah", "Mahdis Mahdieh", "Mia Chen", "Pei Sun", "Dustin Tran", "Sumit Bagri", "Balaji Lakshminarayanan", "Jeremiah Liu", "Andras Orban", "Fabian Güra", "Hao Zhou", "Xinying Song", "Aurelien Boffy", "Harish Ganapathy", "Steven Zheng", "HyunJeong Choe", "Ágoston Weisz", "Tao Zhu", "Yifeng Lu", "Siddharth Gopal", "Jarrod Kahn", "Maciej Kula", "Jeff Pitman", "Rushin Shah", "Emanuel Taropa", "Majd Al Merey", "Martin Baeuml", "Zhifeng Chen", "Laurent El Shafey", "Yujing Zhang", "Olcan Sercinoglu", "George Tucker", "Enrique Piqueras", "Maxim Krikun", "Iain Barr", "Nikolay Savinov", "Ivo Danihelka", "Becca Roelofs", "Anaïs White", "Anders Andreassen", "Tamara von Glehn", "Lakshman Yagati", "Mehran Kazemi", "Lucas Gonzalez", "Misha Khalman", "Jakub Sygnowski", "Alexandre Frechette", "Charlotte Smith", "Laura Culp", "Lev Proleev", "Yi Luan", "Xi Chen", "James Lottes", "Nathan Schucher", "Federico Lebron", "Alban Rrustemi", "Natalie Clay", "Phil Crone", "Tomas Kocisky", "Jeffrey Zhao", "Bartek Perz", "Dian Yu", "Heidi Howard", "Adam Bloniarz", "Jack W. Rae", "Han Lu", "Laurent Sifre", "Marcello Maggioni", "Fred Alcober", "Dan Garrette", "Megan Barnes", "Shantanu Thakoor", "Jacob Austin", "Gabriel Barth-Maron", "William Wong", "Rishabh Joshi", "Rahma Chaabouni", "Deeni Fatiha", "Arun Ahuja", "Gaurav Singh Tomar", "Evan Senter", "Martin Chadwick", "Ilya Kornakov", "Nithya Attaluri", "Iñaki Iturrate", "Ruibo Liu", "Yunxuan Li", "Sarah Cogan", "Jeremy Chen", "Chao Jia", "Chenjie Gu", "Qiao Zhang", "Jordan Grimstad", "Ale Jakse Hartman", "Xavier Garcia", "Thanumalayan Sankaranarayana Pillai", "Jacob Devlin", "Michael Laskin", "Diego de Las Casas", "Dasha Valter", "Connie Tao", "Lorenzo Blanco", "Adrià Puigdomènech Badia", "David Reitter", "Mianna Chen", "Jenny Brennan", "Clara Rivera", "Sergey Brin", "Shariq Iqbal", "Gabriela Surita", "Jane Labanowski", "Abhi Rao", "Stephanie Winkler", "Emilio Parisotto", "Yiming Gu", "Kate Olszewska", "Ravi Addanki", "Antoine Miech", "Annie Louis", "Denis Teplyashin", "Geoff Brown", "Elliot Catt", "Jan Balaguer", "Jackie Xiang", "Pidong Wang", "Zoe Ashwood", "Anton Briukhov", "Albert Webson", "Sanjay Ganapathy", "Smit Sanghavi", "Ajay Kannan", "Ming-Wei Chang", "Axel Stjerngren", "Josip Djolonga", "Yuting Sun", "Ankur Bapna", "Matthew Aitchison", "Pedram Pejman", "Henryk Michalewski", "Tianhe Yu", "Cindy Wang", "Juliette Love", "Junwhan Ahn", "Dawn Bloxwich", "Kehang Han", "Peter Humphreys", "Thibault Sellam", "James Bradbury", "Varun Godbole", "Sina Samangooei", "Bogdan Damoc", "Alex Kaskasoli", "Sébastien M. R. Arnold", "Vijay Vasudevan", "Shubham Agrawal", "Jason Riesa", "Dmitry Lepikhin", "Richard Tanburn", "Srivatsan Srinivasan", "Hyeontaek Lim", "Sarah Hodkinson", "Pranav Shyam", "Johan Ferret", "Steven Hand", "Ankush Garg", "Tom Le Paine", "Jian Li", "Yujia Li", "Minh Giang", "Alexander Neitz", "Zaheer Abbas", "Sarah York", "Machel Reid", "Elizabeth Cole", "Aakanksha Chowdhery", "Dipanjan Das", "Dominika Rogozińska", "Vitaliy Nikolaev", "Pablo Sprechmann", "Zachary Nado", "Lukas Zilka", "Flavien Prost", "Luheng He", "Marianne Monteiro", "Gaurav Mishra", "Chris Welty", "Josh Newlan", "Dawei Jia", "Miltiadis Allamanis", "Clara Huiyi Hu", "Raoul de Liedekerke", "Justin Gilmer", "Carl Saroufim", "Shruti Rijhwani", "Shaobo Hou", "Disha Shrivastava", "Anirudh Baddepudi", "Alex Goldin", "Adnan Ozturel", "Albin Cassirer", "Yunhan Xu", "Daniel Sohn", "Devendra Sachan", "Reinald Kim Amplayo", "Craig Swanson", "Dessie Petrova", "Shashi Narayan", "Arthur Guez", "Siddhartha Brahma", "Jessica Landon", "Miteyan Patel", "Ruizhe Zhao", "Kevin Villela", "Luyu Wang", "Wenhao Jia", "Matthew Rahtz", "Mai Giménez", "Legg Yeung", "James Keeling", "Petko Georgiev", "Diana Mincu", "Boxi Wu", "Salem Haykal", "Rachel Saputro", "Kiran Vodrahalli", "James Qin", "Zeynep Cankara", "Abhanshu Sharma", "Nick Fernando", "Will Hawkins", "Behnam Neyshabur", "Solomon Kim", "Adrian Hutter", "Priyanka Agrawal", "Alex Castro-Ros", "George van den Driessche", "Tao Wang", "Fan Yang", "Shuo-yiin Chang", "Paul Komarek", "Ross McIlroy", "Mario Lučić", "Guodong Zhang", "Wael Farhan", "Michael Sharman", "Paul Natsev", "Paul Michel", "Yamini Bansal", "Siyuan Qiao", "Kris Cao", "Siamak Shakeri", "Christina Butterfield", "Justin Chung", "Paul Kishan Rubenstein", "Shivani Agrawal", "Arthur Mensch", "Kedar Soparkar", "Karel Lenc", "Timothy Chung", "Aedan Pope", "Loren Maggiore", "Jackie Kay", "Priya Jhakra", "Shibo Wang", "Joshua Maynez", "Mary Phuong", "Taylor Tobin", "Andrea Tacchetti", "Maja Trebacz", "Kevin Robinson", "Yash Katariya", "Sebastian Riedel", "Paige Bailey", "Kefan Xiao", "Nimesh Ghelani", "Lora Aroyo", "Ambrose Slone", "Neil Houlsby", "Xuehan Xiong", "Zhen Yang", "Elena Gribovskaya", "Jonas Adler", "Mateo Wirth", "Lisa Lee", "Music Li", "Thais Kagohara", "Jay Pavagadhi", "Sophie Bridgers", "Anna Bortsova", "Sanjay Ghemawat", "Zafarali Ahmed", "Tianqi Liu", "Richard Powell", "Vijay Bolina", "Mariko Iinuma", "Polina Zablotskaia", "James Besley", "Da-Woon Chung", "Timothy Dozat", "Ramona Comanescu", "Xiance Si", "Jeremy Greer", "Guolong Su", "Martin Polacek", "Raphaël Lopez Kaufman", "Simon Tokumine", "Hexiang Hu", "Elena Buchatskaya", "Yingjie Miao", "Mohamed Elhawaty", "Aditya Siddhant", "Nenad Tomasev", "Jinwei Xing", "Christina Greer", "Helen Miller", "Shereen Ashraf", "Aurko Roy", "Zizhao Zhang", "Ada Ma", "Angelos Filos", "Milos Besta", "Rory Blevins", "Ted Klimenko", "Chih-Kuan Yeh", "Soravit Changpinyo", "Jiaqi Mu", "Oscar Chang", "Mantas Pajarskas", "Carrie Muir", "Vered Cohen", "Charline Le Lan", "Krishna Haridasan", "Amit Marathe", "Steven Hansen", "Sholto Douglas", "Rajkumar Samuel", "Mingqiu Wang", "Sophia Austin", "Chang Lan", "Jiepu Jiang", "Justin Chiu", "Jaime Alonso Lorenzo", "Lars Lowe Sjösund", "Sébastien Cevey", "Zach Gleicher", "Thi Avrahami", "Anudhyan Boral", "Hansa Srinivasan", "Vittorio Selo", "Rhys May", "Konstantinos Aisopos", "Léonard Hussenot", "Livio Baldini Soares", "Kate Baumli", "Michael B. Chang", "Adrià Recasens", "Ben Caine", "Alexander Pritzel", "Filip Pavetic", "Fabio Pardo", "Anita Gergely", "Justin Frye", "Vinay Ramasesh", "Dan Horgan", "Kartikeya Badola", "Nora Kassner", "Subhrajit Roy", "Ethan Dyer", "Víctor Campos Campos", "Alex Tomala", "Yunhao Tang", "Dalia El Badawy", "Elspeth White", "Basil Mustafa", "Oran Lang", "Abhishek Jindal", "Sharad Vikram", "Zhitao Gong", "Sergi Caelles", "Ross Hemsley", "Gregory Thornton", "Fangxiaoyu Feng", "Wojciech Stokowiec", "Ce Zheng", "Phoebe Thacker", "Çağlar Ünlü", "Zhishuai Zhang", "Mohammad Saleh", "James Svensson", "Max Bileschi", "Piyush Patil", "Ankesh Anand", "Roman Ring", "Katerina Tsihlas", "Arpi Vezer", "Marco Selvi", "Toby Shevlane", "Mikel Rodriguez", "Tom Kwiatkowski", "Samira Daruki", "Keran Rong", "Allan Dafoe", "Nicholas FitzGerald", "Keren Gu-Lemberg", "Mina Khan", "Lisa Anne Hendricks", "Marie Pellat", "Vladimir Feinberg", "James Cobon-Kerr", "Tara Sainath", "Maribeth Rauh", "Sayed Hadi Hashemi", "Richard Ives", "Yana Hasson", "Eric Noland", "Yuan Cao", "Nathan Byrd", "Le Hou", "Qingze Wang", "Thibault Sottiaux", "Michela Paganini", "Jean-Baptiste Lespiau", "Alexandre Moufarek", "Samer Hassan", "Kaushik Shivakumar", "Joost van Amersfoort", "Amol Mandhane", "Pratik Joshi", "Anirudh Goyal", "Matthew Tung", "Andrew Brock", "Hannah Sheahan", "Vedant Misra", "Cheng Li", "Nemanja Rakićević", "Mostafa Dehghani", "Fangyu Liu", "Sid Mittal", "Junhyuk Oh", "Seb Noury", "Eren Sezener", "Fantine Huot", "Matthew Lamm", "Nicola De Cao", "Charlie Chen", "Sidharth Mudgal", "Romina Stella", "Kevin Brooks", "Gautam Vasudevan", "Chenxi Liu", "Mainak Chain", "Nivedita Melinkeri", "Aaron Cohen", "Venus Wang", "Kristie Seymore", "Sergey Zubkov", "Rahul Goel", "Summer Yue", "Sai Krishnakumaran", "Brian Albert", "Nate Hurley", "Motoki Sano", "Anhad Mohananey", "Jonah Joughin", "Egor Filonov", "Tomasz Kępa", "Yomna Eldawy", "Jiawern Lim", "Rahul Rishi", "Shirin Badiezadegan", "Taylor Bos", "Jerry Chang", "Sanil Jain", "Sri Gayatri Sundara Padmanabhan", "Subha Puttagunta", "Kalpesh Krishna", "Leslie Baker", "Norbert Kalb", "Vamsi Bedapudi", "Adam Kurzrok", "Shuntong Lei", "Anthony Yu", "Oren Litvin", "Xiang Zhou", "Zhichun Wu", "Sam Sobell", "Andrea Siciliano", "Alan Papir", "Robby Neale", "Jonas Bragagnolo", "Tej Toor", "Tina Chen", "Valentin Anklin", "Feiran Wang", "Richie Feng", "Milad Gholami", "Kevin Ling", "Lijuan Liu", "Jules Walter", "Hamid Moghaddam", "Arun Kishore", "Jakub Adamek", "Tyler Mercado", "Jonathan Mallinson", "Siddhinita Wandekar", "Stephen Cagle", "Eran Ofek", "Guillermo Garrido", "Clemens Lombriser", "Maksim Mukha", "Botu Sun", "Hafeezul Rahman Mohammad", "Josip Matak", "Yadi Qian", "Vikas Peswani", "Pawel Janus", "Quan Yuan", "Leif Schelin", "Oana David", "Ankur Garg", "Yifan He", "Oleksii Duzhyi", "Anton Älgmyr", "Timothée Lottaz", "Qi Li", "Vikas Yadav", "Luyao Xu", "Alex Chinien", "Rakesh Shivanna", "Aleksandr Chuklin", "Josie Li", "Carrie Spadine", "Travis Wolfe", "Kareem Mohamed", "Subhabrata Das", "Zihang Dai", "Kyle He", "Daniel von Dincklage", "Shyam Upadhyay", "Akanksha Maurya", "Luyan Chi", "Sebastian Krause", "Khalid Salama", "Pam G Rabinovitch", "Pavan Kumar Reddy M", "Aarush Selvan", "Mikhail Dektiarev", "Golnaz Ghiasi", "Erdem Guven", "Himanshu Gupta", "Boyi Liu", "Deepak Sharma", "Idan Heimlich Shtacher", "Shachi Paul", "Oscar Akerlund", "François-Xavier Aubet", "Terry Huang", "Chen Zhu", "Eric Zhu", "Elico Teixeira", "Matthew Fritze", "Francesco Bertolini", "Liana-Eleonora Marinescu", "Martin Bölle", "Dominik Paulus", "Khyatti Gupta", "Tejasi Latkar", "Max Chang", "Jason Sanders", "Roopa Wilson", "Xuewei Wu", "Yi-Xuan Tan", "Lam Nguyen Thiet", "Tulsee Doshi", "Sid Lall", "Swaroop Mishra", "Wanming Chen", "Thang Luong", "Seth Benjamin", "Jasmine Lee", "Ewa Andrejczuk", "Dominik Rabiej", "Vipul Ranjan", "Krzysztof Styrc", "Pengcheng Yin", "Jon Simon", "Malcolm Rose Harriott", "Mudit Bansal", "Alexei Robsky", "Geoff Bacon", "David Greene", "Daniil Mirylenka", "Chen Zhou", "Obaid Sarvana", "Abhimanyu Goyal", "Samuel Andermatt", "Patrick Siegler", "Ben Horn", "Assaf Israel", "Francesco Pongetti", "Chih-Wei \"Louis\" Chen", "Marco Selvatici", "Pedro Silva", "Kathie Wang", "Jackson Tolins", "Kelvin Guu", "Roey Yogev", "Xiaochen Cai", "Alessandro Agostini", "Maulik Shah", "Hung Nguyen", "Noah Ó Donnaile", "Sébastien Pereira", "Linda Friso", "Adam Stambler", "Adam Kurzrok", "Chenkai Kuang", "Yan Romanikhin", "Mark Geller", "ZJ Yan", "Kane Jang", "Cheng-Chun Lee", "Wojciech Fica", "Eric Malmi", "Qijun Tan", "Dan Banica", "Daniel Balle", "Ryan Pham", "Yanping Huang", "Diana Avram", "Hongzhi Shi", "Jasjot Singh", "Chris Hidey", "Niharika Ahuja", "Pranab Saxena", "Dan Dooley", "Srividya Pranavi Potharaju", "Eileen O'Neill", "Anand Gokulchandran", "Ryan Foley", "Kai Zhao", "Mike Dusenberry", "Yuan Liu", "Pulkit Mehta", "Ragha Kotikalapudi", "Chalence Safranek-Shrader", "Andrew Goodman", "Joshua Kessinger", "Eran Globen", "Prateek Kolhar", "Chris Gorgolewski", "Ali Ibrahim", "Yang Song", "Ali Eichenbaum", "Thomas Brovelli", "Sahitya Potluri", "Preethi Lahoti", "Cip Baetu", "Ali Ghorbani", "Charles Chen", "Andy Crawford", "Shalini Pal", "Mukund Sridhar", "Petru Gurita", "Asier Mujika", "Igor Petrovski", "Pierre-Louis Cedoz", "Chenmei Li", "Shiyuan Chen", "Niccolò Dal Santo", "Siddharth Goyal", "Jitesh Punjabi", "Karthik Kappaganthu", "Chester Kwak", "Pallavi LV", "Sarmishta Velury", "Himadri Choudhury", "Jamie Hall", "Premal Shah", "Ricardo Figueira", "Matt Thomas", "Minjie Lu", "Ting Zhou", "Chintu Kumar", "Thomas Jurdi", "Sharat Chikkerur", "Yenai Ma", "Adams Yu", "Soo Kwak", "Victor Ähdel", "Sujeevan Rajayogam", "Travis Choma", "Fei Liu", "Aditya Barua", "Colin Ji", "Ji Ho Park", "Vincent Hellendoorn", "Alex Bailey", "Taylan Bilal", "Huanjie Zhou", "Mehrdad Khatir", "Charles Sutton", "Wojciech Rzadkowski", "Fiona Macintosh", "Roopali Vij", "Konstantin Shagin", "Paul Medina", "Chen Liang", "Jinjing Zhou", "Pararth Shah", "Yingying Bi", "Attila Dankovics", "Shipra Banga", "Sabine Lehmann", "Marissa Bredesen", "Zifan Lin", "John Eric Hoffmann", "Jonathan Lai", "Raynald Chung", "Kai Yang", "Nihal Balani", "Arthur Bražinskas", "Andrei Sozanschi", "Matthew Hayes", "Héctor Fernández Alcalde", "Peter Makarov", "Will Chen", "Antonio Stella", "Liselotte Snijders", "Michael Mandl", "Ante Kärrman", "Paweł Nowak", "Xinyi Wu", "Alex Dyck", "Krishnan Vaidyanathan", "Raghavender R", "Jessica Mallet", "Mitch Rudominer", "Eric Johnston", "Sushil Mittal", "Akhil Udathu", "Janara Christensen", "Vishal Verma", "Zach Irving", "Andreas Santucci", "Gamaleldin Elsayed", "Elnaz Davoodi", "Marin Georgiev", "Ian Tenney", "Nan Hua", "Geoffrey Cideron", "Edouard Leurent", "Mahmoud Alnahlawi", "Ionut Georgescu", "Nan Wei", "Ivy Zheng", "Dylan Scandinaro", "Heinrich Jiang", "Jasper Snoek", "Mukund Sundararajan", "Xuezhi Wang", "Zack Ontiveros", "Itay Karo", "Jeremy Cole", "Vinu Rajashekhar", "Lara Tumeh", "Eyal Ben-David", "Rishub Jain", "Jonathan Uesato", "Romina Datta", "Oskar Bunyan", "Shimu Wu", "John Zhang", "Piotr Stanczyk", "Ye Zhang", "David Steiner", "Subhajit Naskar", "Michael Azzam", "Matthew Johnson", "Adam Paszke", "Chung-Cheng Chiu", "Jaume Sanchez Elias", "Afroz Mohiuddin", "Faizan Muhammad", "Jin Miao", "Andrew Lee", "Nino Vieillard", "Jane Park", "Jiageng Zhang", "Jeff Stanway", "Drew Garmon", "Abhijit Karmarkar", "Zhe Dong", "Jong Lee", "Aviral Kumar", "Luowei Zhou", "Jonathan Evens", "William Isaac", "Geoffrey Irving", "Edward Loper", "Michael Fink", "Isha Arkatkar", "Nanxin Chen", "Izhak Shafran", "Ivan Petrychenko", "Zhe Chen", "Johnson Jia", "Anselm Levskaya", "Zhenkai Zhu", "Peter Grabowski", "Yu Mao", "Alberto Magni", "Kaisheng Yao", "Javier Snaider", "Norman Casagrande", "Evan Palmer", "Paul Suganthan", "Alfonso Castaño", "Irene Giannoumis", "Wooyeol Kim", "Mikołaj Rybiński", "Ashwin Sreevatsa", "Jennifer Prendki", "David Soergel", "Adrian Goedeckemeyer", "Willi Gierke", "Mohsen Jafari", "Meenu Gaba", "Jeremy Wiesner", "Diana Gage Wright", "Yawen Wei", "Harsha Vashisht", "Yana Kulizhskaya", "Jay Hoover", "Maigo Le", "Lu Li", "Chimezie Iwuanyanwu", "Lu Liu", "Kevin Ramirez", "Andrey Khorlin", "Albert Cui", "Tian LIN", "Marcus Wu", "Ricardo Aguilar", "Keith Pallo", "Abhishek Chakladar", "Ginger Perng", "Elena Allica Abellan", "Mingyang Zhang", "Ishita Dasgupta", "Nate Kushman", "Ivo Penchev", "Alena Repina", "Xihui Wu", "Tom van der Weide", "Priya Ponnapalli", "Caroline Kaplan", "Jiri Simsa", "Shuangfeng Li", "Olivier Dousse", "Fan Yang", "Jeff Piper", "Nathan Ie", "Rama Pasumarthi", "Nathan Lintz", "Anitha Vijayakumar", "Daniel Andor", "Pedro Valenzuela", "Minnie Lui", "Cosmin Paduraru", "Daiyi Peng", "Katherine Lee", "Shuyuan Zhang", "Somer Greene", "Duc Dung Nguyen", "Paula Kurylowicz", "Cassidy Hardin", "Lucas Dixon", "Lili Janzer", "Kiam Choo", "Ziqiang Feng", "Biao Zhang", "Achintya Singhal", "Dayou Du", "Dan McKinnon", "Natasha Antropova", "Tolga Bolukbasi", "Orgad Keller", "David Reid", "Daniel Finchelstein", "Maria Abi Raad", "Remi Crocker", "Peter Hawkins", "Robert Dadashi", "Colin Gaffney", "Ken Franko", "Anna Bulanova", "Rémi Leblond", "Shirley Chung", "Harry Askham", "Luis C. Cobo", "Kelvin Xu", "Felix Fischer", "Jun Xu", "Christina Sorokin", "Chris Alberti", "Chu-Cheng Lin", "Colin Evans", "Alek Dimitriev", "Hannah Forbes", "Dylan Banarse", "Zora Tung", "Mark Omernick", "Colton Bishop", "Rachel Sterneck", "Rohan Jain", "Jiawei Xia", "Ehsan Amid", "Francesco Piccinno", "Xingyu Wang", "Praseem Banzal", "Daniel J. Mankowitz", "Alex Polozov", "Victoria Krakovna", "Sasha Brown", "MohammadHossein Bateni", "Dennis Duan", "Vlad Firoiu", "Meghana Thotakuri", "Tom Natan", "Matthieu Geist", "Ser tan Girgin", "Hui Li", "Jiayu Ye", "Ofir Roval", "Reiko Tojo", "Michael Kwong", "James Lee-Thorp", "Christopher Yew", "Danila Sinopalnikov", "Sabela Ramos", "John Mellor", "Abhishek Sharma", "Kathy Wu", "David Miller", "Nicolas Sonnerat", "Denis Vnukov", "Rory Greig", "Jennifer Beattie", "Emily Caveness", "Libin Bai", "Julian Eisenschlos", "Alex Korchemniy", "Tomy Tsai", "Mimi Jasarevic", "Weize Kong", "Phuong Dao", "Zeyu Zheng", "Frederick Liu", "Fan Yang", "Rui Zhu", "Tian Huey Teh", "Jason Sanmiya", "Evgeny Gladchenko", "Nejc Trdin", "Daniel Toyama", "Evan Rosen", "Sasan Tavakkol", "Linting Xue", "Chen Elkind", "Oliver Woodman", "John Carpenter", "George Papamakarios", "Rupert Kemp", "Sushant Kafle", "Tanya Grunina", "Rishika Sinha", "Alice Talbert", "Diane Wu", "Denese Owusu-Afriyie", "Cosmo Du", "Chloe Thornton", "Jordi Pont-Tuset", "Pradyumna Narayana", "Jing Li", "Saaber Fatehi", "John Wieting", "Omar Ajmeri", "Benigno Uria", "Yeongil Ko", "Laura Knight", "Amélie Héliou", "Ning Niu", "Shane Gu", "Chenxi Pang", "Yeqing Li", "Nir Levine", "Ariel Stolovich", "Rebeca Santamaria-Fernandez", "Sonam Goenka", "Wenny Yustalim", "Robin Strudel", "Ali Elqursh", "Charlie Deck", "Hyo Lee", "Zonglin Li", "Kyle Levin", "Raphael Hoffmann", "Dan Holtmann-Rice", "Olivier Bachem", "Sho Arora", "Christy Koh", "Soheil Hassas Yeganeh", "Siim Põder", "Mukarram Tariq", "Yanhua Sun", "Lucian Ionita", "Mojtaba Seyedhosseini", "Pouya Tafti", "Zhiyu Liu", "Anmol Gulati", "Jasmine Liu", "Xinyu Ye", "Bart Chrzaszcz", "Lily Wang", "Nikhil Sethi", "Tianrun Li", "Ben Brown", "Shreya Singh", "Wei Fan", "Aaron Parisi", "Joe Stanton", "Vinod Koverkathu", "Christopher A. Choquette-Choo", "Yunjie Li", "TJ Lu", "Abe Ittycheriah", "Prakash Shroff", "Mani Varadarajan", "Sanaz Bahargam", "Rob Willoughby", "David Gaddy", "Guillaume Desjardins", "Marco Cornero", "Brona Robenek", "Bhavishya Mittal", "Ben Albrecht", "Ashish Shenoy", "Fedor Moiseev", "Henrik Jacobsson", "Alireza Ghaffarkhah", "Morgane Rivière", "Alanna Walton", "Clément Crepy", "Alicia Parrish", "Zongwei Zhou", "Clement Farabet", "Carey Radebaugh", "Praveen Srinivasan", "Claudia van der Salm", "Andreas Fidjeland", "Salvatore Scellato", "Eri Latorre-Chimoto", "Hanna Klimczak-Plucińska", "David Bridson", "Dario de Cesare", "Tom Hudson", "Piermaria Mendolicchio", "Lexi Walker", "Alex Morris", "Matthew Mauger", "Alexey Guseynov", "Alison Reid", "Seth Odoom", "Lucia Loher", "Victor Cotruta", "Madhavi Yenugula", "Dominik Grewe", "Anastasia Petrushkina", "Tom Duerig", "Antonio Sanchez", "Steve Yadlowsky", "Amy Shen", "Amir Globerson", "Lynette Webb", "Sahil Dua", "Dong Li", "Surya Bhupatiraju", "Dan Hurt", "Haroon Qureshi", "Ananth Agarwal", "Tomer Shani", "Matan Eyal", "Anuj Khare", "Shreyas Rammohan Belle", "Lei Wang", "Chetan Tekur", "Mihir Sanjay Kale", "Jinliang Wei", "Ruoxin Sang", "Brennan Saeta", "Tyler Liechty", "Yi Sun", "Yao Zhao", "Stephan Lee", "Pandu Nayak", "Doug Fritz", "Manish Reddy Vuyyuru", "John Aslanides", "Nidhi Vyas", "Martin Wicke", "Xiao Ma", "Evgenii Eltyshev", "Nina Martin", "Hardie Cate", "James Manyika", "Keyvan Amiri", "Yelin Kim", "Xi Xiong", "Kai Kang", "Florian Luisier", "Nilesh Tripuraneni", "David Madras", "Mandy Guo", "Austin Waters", "Oliver Wang", "Joshua Ainslie", "Jason Baldridge", "Han Zhang", "Garima Pruthi", "Jakob Bauer", "Feng Yang", "Riham Mansour", "Jason Gelman", "Yang Xu", "George Polovets", "Ji Liu", "Honglong Cai", "Warren Chen", "XiangHai Sheng", "Emily Xue", "Sherjil Ozair", "Christof Angermueller", "Xiaowei Li", "Anoop Sinha", "Weiren Wang", "Julia Wiesinger", "Emmanouil Koukoumidis", "Yuan Tian", "Anand Iyer", "Madhu Gurumurthy", "Mark Goldenson", "Parashar Shah", "MK Blake", "Hongkun Yu", "Anthony Urbanowicz", "Jennimaria Palomaki", "Chrisantha Fernando", "Ken Durden", "Harsh Mehta", "Nikola Momchev", "Elahe Rahimtoroghi", "Maria Georgaki", "Amit Raul", "Sebastian Ruder", "Morgan Redshaw", "Jinhyuk Lee", "Denny Zhou", "Komal Jalan", "Dinghua Li", "Blake Hechtman", "Parker Schuh", "Milad Nasr", "Kieran Milan", "Vladimir Mikulik", "Juliana Franco", "Tim Green", "Nam Nguyen", "Joe Kelley", "Aroma Mahendru", "Andrea Hu", "Joshua Howland", "Ben Vargas", "Jeffrey Hui", "Kshitij Bansal", "Vikram Rao", "Rakesh Ghiya", "Emma Wang", "Ke Ye", "Jean Michel Sarr", "Melanie Moranski Preston", "Madeleine Elish", "Steve Li", "Aakash Kaku", "Jigar Gupta", "Ice Pasupat", "Da-Cheng Juan", "Milan Someswar", "Tejvi M.", "Xinyun Chen", "Aida Amini", "Alex Fabrikant", "Eric Chu", "Xuanyi Dong", "Amruta Muthal", "Senaka Buthpitiya", "Sarthak Jauhari", "Nan Hua", "Urvashi Khandelwal", "Ayal Hitron", "Jie Ren", "Larissa Rinaldi", "Shahar Drath", "Avigail Dabush", "Nan-Jiang Jiang", "Harshal Godhia", "Uli Sachs", "Anthony Chen", "Yicheng Fan", "Hagai Taitelbaum", "Hila Noga", "Zhuyun Dai", "James Wang", "Chen Liang", "Jenny Hamer", "Chun-Sung Ferng", "Chenel Elkind", "Aviel Atias", "Paulina Lee", "Vít Listík", "Mathias Carlen", "Jan van de Kerkhof", "Marcin Pikus", "Krunoslav Zaher", "Paul Müller", "Sasha Zykova", "Richard Stefanec", "Vitaly Gatsko", "Christoph Hirnschall", "Ashwin Sethi", "Xingyu Federico Xu", "Chetan Ahuja", "Beth Tsai", "Anca Stefanoiu", "Bo Feng", "Keshav Dhandhania", "Manish Katyal", "Akshay Gupta", "Atharva Parulekar", "Divya Pitta", "Jing Zhao", "Vivaan Bhatia", "Yashodha Bhavnani", "Omar Alhadlaq", "Xiaolin Li", "Peter Danenberg", "Dennis Tu", "Alex Pine", "Vera Filippova", "Abhipso Ghosh", "Ben Limonchik", "Bhargava Urala", "Chaitanya Krishna Lanka", "Derik Clive", "Yi Sun", "Edward Li", "Hao Wu", "Kevin Hongtongsak", "Ianna Li", "Kalind Thakkar", "Kuanysh Omarov", "Kushal Majmundar", "Michael Alverson", "Michael Kucharski", "Mohak Patel", "Mudit Jain", "Maksim Zabelin", "Paolo Pelagatti", "Rohan Kohli", "Saurabh Kumar", "Joseph Kim", "Swetha Sankar", "Vineet Shah", "Lakshmi Ramachandruni", "Xiangkai Zeng", "Ben Bariach", "Laura Weidinger", "Tu Vu", "Alek Andreev", "Antoine He", "Kevin Hui", "Sheleem Kashem", "Amar Subramanya", "Sissie Hsiao", "Demis Hassabis", "Koray Kavukcuoglu", "Adam Sadovsky", "Quoc Le", "Trevor Strohman", "Yonghui Wu", "Slav Petrov", "Jeffrey Dean", "Oriol Vinyals"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "This report introduces a new family of multimodal models, Gemini, that\nexhibit remarkable capabilities across image, audio, video, and text\nunderstanding. The Gemini family consists of Ultra, Pro, and Nano sizes,\nsuitable for applications ranging from complex reasoning tasks to on-device\nmemory-constrained use-cases. Evaluation on a broad range of benchmarks shows\nthat our most-capable Gemini Ultra model advances the state of the art in 30 of\n32 of these benchmarks - notably being the first model to achieve human-expert\nperformance on the well-studied exam benchmark MMLU, and improving the state of\nthe art in every one of the 20 multimodal benchmarks we examined. We believe\nthat the new capabilities of the Gemini family in cross-modal reasoning and\nlanguage understanding will enable a wide variety of use cases. We discuss our\napproach toward post-training and deploying Gemini models responsibly to users\nthrough services including Gemini, Gemini Advanced, Google AI Studio, and Cloud\nVertex AI."}
{"id": "2405.06691", "pdf": "https://arxiv.org/pdf/2405.06691.pdf", "abs": "https://arxiv.org/abs/2405.06691", "title": "Fleet of Agents: Coordinated Problem Solving with Large Language Models", "authors": ["Lars Klein", "Nearchos Potamitis", "Roland Aydin", "Robert West", "Caglar Gulcehre", "Akhil Arora"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "comment": "ICML 2025; 28 pages, 68 figures, 8 tables", "summary": "While numerous frameworks have been developed to enhance the reasoning\nabilities of large language models (LLMs), there is a scarcity of methods that\neffectively balance the trade-off between cost and quality. In this paper, we\nintroduce Fleet of Agents (FoA), a novel and intuitive yet principled framework\nutilizing LLMs as agents to navigate through dynamic tree searches, employing a\ngenetic-type particle filtering approach. FoA spawns a multitude of agents,\neach exploring the search space autonomously, followed by a selection phase\nwhere resampling based on a heuristic value function optimizes the balance\nbetween exploration and exploitation. This mechanism enables dynamic branching,\nadapting the exploration strategy based on discovered solutions. We conduct\nextensive experiments on three benchmark tasks, ``Game of 24'',\n``Mini-Crosswords'', and ``WebShop'', utilizing four different LLMs,\n``GPT-3.5'', ``GPT-4'', ``LLaMA3.2-11B'', and ``LLaMA3.2-90B''. On average\nacross all tasks and LLMs, FoA obtains a quality improvement of ~5% while\nrequiring only ~40% of the cost of previous SOTA methods. Notably, our analyses\nreveal that (1) FoA achieves the best cost-quality trade-off among all\nbenchmarked methods and (2) FoA + LLaMA3.2-11B surpasses the Llama3.2-90B\nmodel. FoA is publicly available at https://github.com/au-clan/FoA."}
{"id": "2406.15163", "pdf": "https://arxiv.org/pdf/2406.15163.pdf", "abs": "https://arxiv.org/abs/2406.15163", "title": "A Syntax-Injected Approach for Faster and More Accurate Sentiment Analysis", "authors": ["Muhammad Imran", "Olga Kellert", "Carlos Gómez-Rodríguez"], "categories": ["cs.CL"], "comment": null, "summary": "Sentiment Analysis (SA) is a crucial aspect of Natural Language Processing\n(NLP), addressing subjective assessments in textual content. Syntactic parsing\nis useful in SA because explicit syntactic information can improve accuracy\nwhile providing explainability, but it tends to be a computational bottleneck\nin practice due to the slowness of parsing algorithms. This paper addresses\nsaid bottleneck by using a SEquence Labeling Syntactic Parser (SELSP) to inject\nsyntax into SA. By treating dependency parsing as a sequence labeling problem,\nwe greatly enhance the speed of syntax-based SA. SELSP is trained and evaluated\non a ternary polarity classification task, demonstrating its faster performance\nand better accuracy in polarity prediction tasks compared to conventional\nparsers like Stanza and to heuristic approaches that use shallow syntactic\nrules for SA like VADER. This increased speed and improved accuracy make SELSP\nparticularly appealing to SA practitioners in both research and industry. In\naddition, we test several sentiment dictionaries on our SELSP to see which one\nimproves the performance in polarity prediction tasks. Moreover, we compare the\nSELSP with Transformer-based models trained on a 5-label classification task.\nThe results show that dictionaries that capture polarity judgment variation\nprovide better results than dictionaries that ignore polarity judgment\nvariation. Moreover, we show that SELSP is considerably faster than\nTransformer-based models in polarity prediction tasks."}
{"id": "2406.17692", "pdf": "https://arxiv.org/pdf/2406.17692.pdf", "abs": "https://arxiv.org/abs/2406.17692", "title": "From Distributional to Overton Pluralism: Investigating Large Language Model Alignment", "authors": ["Thom Lake", "Eunsol Choi", "Greg Durrett"], "categories": ["cs.CL", "cs.LG"], "comment": "NAACL 2025 (Main Conference)", "summary": "The alignment process changes several properties of a large language model's\n(LLM's) output distribution. We analyze two aspects of post-alignment\ndistributional shift of LLM responses. First, we re-examine previously reported\nreductions in response diversity post-alignment. Our analysis suggests that an\napparent drop in the diversity of responses is largely explained by quality\ncontrol and information aggregation. Alignment suppresses irrelevant and\nunhelpful content while shifting the output distribution toward longer\nresponses that cover information spanning several responses from the base LLM,\nessentially presenting diverse information in a single response. Finding little\nevidence that alignment suppresses useful information, it is natural to ask the\nopposite question: do aligned models surface information that cannot be\nrecovered from base models? Our second investigation shows this is not the case\nand the behavior of aligned models is recoverable from base models without\nfine-tuning. A combination of in-context examples and lower-resolution semantic\nhints about response content can elicit responses from base LLMs that are as\nsimilar to alignment-tuned LLM responses as alignment-tuned LLM responses are\nto each other. Taken together, these results indicate that current alignment\ntechniques capture but do not extend the useful subset of assistant-like base\nLLM behavior, providing further evidence for the Superficial Alignment\nHypothesis. They also show that in-context alignment can go surprisingly far as\na strategy for imitating aligned LLMs without fine-tuning. Our code and data is\navailable at https://github.com/thomlake/investigating-alignment."}
{"id": "2408.05093", "pdf": "https://arxiv.org/pdf/2408.05093.pdf", "abs": "https://arxiv.org/abs/2408.05093", "title": "Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive Prompting for Large-Language-Models", "authors": ["Zikai Xie"], "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, submitted to ACL ARR", "summary": "Large language models (LLMs) have generated significant attention since their\ninception, finding applications across various academic and industrial domains.\nHowever, these models often suffer from the \"hallucination problem\", where\noutputs, though grammatically and logically coherent, lack factual accuracy or\nare entirely fabricated. A particularly troubling issue discovered and widely\ndiscussed recently is the numerical comparison error where multiple LLMs\nincorrectly infer that \"9.11$>$9.9\". We discovered that the order in which LLMs\ngenerate answers and reasoning impacts their consistency. Specifically, results\nvary significantly when an LLM generates an answer first and then provides the\nreasoning versus generating the reasoning process first and then the\nconclusion. Inspired by this, we propose a new benchmark method for assessing\nLLM consistency: comparing responses generated through these two different\napproaches. This benchmark effectively identifies instances where LLMs\nfabricate answers and subsequently generate justifications. Furthermore, we\nintroduce a novel and straightforward prompt strategy designed to mitigate this\nissue. Experimental results demonstrate that this strategy improves performance\nacross various LLMs compared to direct questioning. This work not only sheds\nlight on a critical flaw in LLMs but also offers a practical solution to\nenhance their reliability."}
{"id": "2408.05497", "pdf": "https://arxiv.org/pdf/2408.05497.pdf", "abs": "https://arxiv.org/abs/2408.05497", "title": "MABR: Multilayer Adversarial Bias Removal Without Prior Bias Knowledge", "authors": ["Maxwell J. Yin", "Boyu Wang", "Charles Ling"], "categories": ["cs.CL"], "comment": null, "summary": "Models trained on real-world data often mirror and exacerbate existing social\nbiases. Traditional methods for mitigating these biases typically require prior\nknowledge of the specific biases to be addressed, such as gender or racial\nbiases, and the social groups associated with each instance. In this paper, we\nintroduce a novel adversarial training strategy that operates independently of\nprior bias-type knowledge and protected attribute labels. Our approach\nproactively identifies biases during model training by utilizing auxiliary\nmodels, which are trained concurrently by predicting the performance of the\nmain model without relying on task labels. Additionally, we implement these\nauxiliary models at various levels of the feature maps of the main model,\nenabling the detection of a broader and more nuanced range of bias features.\nThrough experiments on racial and gender biases in sentiment and occupation\nclassification tasks, our method effectively reduces social biases without the\nneed for demographic annotations. Moreover, our approach not only matches but\noften surpasses the efficacy of methods that require detailed demographic\ninsights, marking a significant advancement in bias mitigation techniques."}
{"id": "2408.09701", "pdf": "https://arxiv.org/pdf/2408.09701.pdf", "abs": "https://arxiv.org/abs/2408.09701", "title": "Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code Generation in LLMs via Zero-Shot Cross-Lingual Transfer", "authors": ["Mingda Li", "Abhijit Mishra", "Utkarsh Mujumdar"], "categories": ["cs.CL", "68T50 (Primary) 68T07 (Secondary)"], "comment": "Accepted and to appear in IJCNN 2025", "summary": "The use of Large Language Models (LLMs) for program code generation has\ngained substantial attention, but their biases and limitations with non-English\nprompts challenge global inclusivity. This paper investigates the complexities\nof multilingual prompt-based code generation. Our evaluations of LLMs,\nincluding CODELLAMA and CODEGEMMA, reveal significant disparities in code\nquality for non-English prompts; we also demonstrate the inadequacy of simple\napproaches like prompt translation, bootstrapped data augmentation, and\nfine-tuning. To address this, we propose a zero-shot cross-lingual approach\nusing a neural projection technique, integrating a cross-lingual encoder like\nLASER to map multilingual embeddings from it into the LLM's token space. This\nmethod requires training only on English data and scales effectively to other\nlanguages. Results on a translated and quality-checked MBPP dataset show\nsubstantial improvements in code quality. This research promotes a more\ninclusive code generation landscape by empowering LLMs with multilingual\ncapabilities to support the diverse linguistic spectrum in programming."}
{"id": "2409.11055", "pdf": "https://arxiv.org/pdf/2409.11055.pdf", "abs": "https://arxiv.org/abs/2409.11055", "title": "Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant", "authors": ["Jemin Lee", "Sihyeong Park", "Jinse Kwon", "Jihun Oh", "Yongin Kwon"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in IJCAI 2025, 21 pages, 2 figure", "summary": "Quantization has gained attention as a promising solution for the\ncost-effective deployment of large and small language models. However, most\nprior work has been limited to perplexity or basic knowledge tasks and lacks a\ncomprehensive evaluation of recent models like Llama-3.3. In this paper, we\nconduct a comprehensive evaluation of instruction-tuned models spanning 1B to\n405B parameters, applying four quantization methods across 13 datasets. Our\nfindings reveal that (1) quantized models generally surpass smaller FP16\nbaselines, yet they often struggle with instruction-following and hallucination\ndetection; (2) FP8 consistently emerges as the most robust option across tasks,\nand AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller\nmodels can suffer severe accuracy drops at 4-bit quantization, while 70B-scale\nmodels maintain stable performance; (4) notably, \\textit{hard} tasks do not\nalways experience the largest accuracy losses, indicating that quantization\nmagnifies a model's inherent weaknesses rather than simply correlating with\ntask difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant\nperformance declines in Coding and STEM tasks, though it occasionally reports\nimprovements in reasoning."}
{"id": "2409.13746", "pdf": "https://arxiv.org/pdf/2409.13746.pdf", "abs": "https://arxiv.org/abs/2409.13746", "title": "Mapping Biomedical Ontology Terms to IDs: Effect of Domain Prevalence on Prediction Accuracy", "authors": ["Thanh Son Do", "Daniel B. Hier", "Tayo Obafemi-Ajayi"], "categories": ["cs.CL", "cs.AI", "I.2"], "comment": "Presented at 2025 IEEE Conference on Artificial Intelligence (CAI).\n  Santa Clara, CA. May 5, 2025", "summary": "This study evaluates the ability of large language models (LLMs) to map\nbiomedical ontology terms to their corresponding ontology IDs across the Human\nPhenotype Ontology (HPO), Gene Ontology (GO), and UniProtKB terminologies.\nUsing counts of ontology IDs in the PubMed Central (PMC) dataset as a surrogate\nfor their prevalence in the biomedical literature, we examined the relationship\nbetween ontology ID prevalence and mapping accuracy. Results indicate that\nontology ID prevalence strongly predicts accurate mapping of HPO terms to HPO\nIDs, GO terms to GO IDs, and protein names to UniProtKB accession numbers.\nHigher prevalence of ontology IDs in the biomedical literature correlated with\nhigher mapping accuracy. Predictive models based on receiver operating\ncharacteristic (ROC) curves confirmed this relationship.\n  In contrast, this pattern did not apply to mapping protein names to Human\nGenome Organisation's (HUGO) gene symbols. GPT-4 achieved a high baseline\nperformance (95%) in mapping protein names to HUGO gene symbols, with mapping\naccuracy unaffected by prevalence. We propose that the high prevalence of HUGO\ngene symbols in the literature has caused these symbols to become lexicalized,\nenabling GPT-4 to map protein names to HUGO gene symbols with high accuracy.\nThese findings highlight the limitations of LLMs in mapping ontology terms to\nlow-prevalence ontology IDs and underscore the importance of incorporating\nontology ID prevalence into the training and evaluation of LLMs for biomedical\napplications."}
{"id": "2410.01294", "pdf": "https://arxiv.org/pdf/2410.01294.pdf", "abs": "https://arxiv.org/abs/2410.01294", "title": "Endless Jailbreaks with Bijection Learning", "authors": ["Brian R. Y. Huang", "Maximilian Li", "Leonard Tang"], "categories": ["cs.CL"], "comment": null, "summary": "Despite extensive safety measures, LLMs are vulnerable to adversarial inputs,\nor jailbreaks, which can elicit unsafe behaviors. In this work, we introduce\nbijection learning, a powerful attack algorithm which automatically fuzzes LLMs\nfor safety vulnerabilities using randomly-generated encodings whose complexity\ncan be tightly controlled. We leverage in-context learning to teach models\nbijective encodings, pass encoded queries to the model to bypass built-in\nsafety mechanisms, and finally decode responses back into English. Our attack\nis extremely effective on a wide range of frontier language models. Moreover,\nby controlling complexity parameters such as number of key-value mappings in\nthe encodings, we find a close relationship between the capability level of the\nattacked LLM and the average complexity of the most effective bijection\nattacks. Our work highlights that new vulnerabilities in frontier models can\nemerge with scale: more capable models are more severely jailbroken by\nbijection attacks."}
{"id": "2410.14812", "pdf": "https://arxiv.org/pdf/2410.14812.pdf", "abs": "https://arxiv.org/abs/2410.14812", "title": "Isolated Causal Effects of Natural Language", "authors": ["Victoria Lin", "Louis-Philippe Morency", "Eli Ben-Michael"], "categories": ["cs.CL", "stat.ME"], "comment": "ICML 2025", "summary": "As language technologies become widespread, it is important to understand how\nchanges in language affect reader perceptions and behaviors. These\nrelationships may be formalized as the isolated causal effect of some focal\nlanguage-encoded intervention (e.g., factual inaccuracies) on an external\noutcome (e.g., readers' beliefs). In this paper, we introduce a formal\nestimation framework for isolated causal effects of language. We show that a\ncore challenge of estimating isolated effects is the need to approximate all\nnon-focal language outside of the intervention. Drawing on the principle of\nomitted variable bias, we provide measures for evaluating the quality of both\nnon-focal language approximations and isolated effect estimates themselves. We\nfind that poor approximation of non-focal language can lead to bias in the\ncorresponding isolated effect estimates due to omission of relevant variables,\nand we show how to assess the sensitivity of effect estimates to such bias\nalong the two key axes of fidelity and overlap. In experiments on\nsemi-synthetic and real-world data, we validate the ability of our framework to\ncorrectly recover isolated effects and demonstrate the utility of our proposed\nmeasures."}
{"id": "2410.18966", "pdf": "https://arxiv.org/pdf/2410.18966.pdf", "abs": "https://arxiv.org/abs/2410.18966", "title": "Does Data Contamination Detection Work (Well) for LLMs? A Survey and Evaluation on Detection Assumptions", "authors": ["Yujuan Fu", "Ozlem Uzuner", "Meliha Yetisgen", "Fei Xia"], "categories": ["cs.CL"], "comment": "This paper is accepted by NAACL 2025 findings. Link to the paper\n  presentation: https://youtu.be/IhaxwbZOcaU", "summary": "Large language models (LLMs) have demonstrated great performance across\nvarious benchmarks, showing potential as general-purpose task solvers. However,\nas LLMs are typically trained on vast amounts of data, a significant concern in\ntheir evaluation is data contamination, where overlap between training data and\nevaluation datasets inflates performance assessments. Multiple approaches have\nbeen developed to identify data contamination. These approaches rely on\nspecific assumptions that may not hold universally across different settings.\nTo bridge this gap, we systematically review 50 papers on data contamination\ndetection, categorize the underlying assumptions, and assess whether they have\nbeen rigorously validated. We identify and analyze eight categories of\nassumptions and test three of them as case studies. Our case studies focus on\ndetecting direct, instance-level data contamination, which is also referred to\nas Membership Inference Attacks (MIA). Our analysis reveals that MIA approaches\nbased on these three assumptions can have similar performance to random\nguessing, on datasets used in LLM pretraining, suggesting that current LLMs\nmight learn data distributions rather than memorizing individual instances.\nMeanwhile, MIA can easily fail when there are data distribution shifts between\nthe seen and unseen instances."}
{"id": "2411.02316", "pdf": "https://arxiv.org/pdf/2411.02316.pdf", "abs": "https://arxiv.org/abs/2411.02316", "title": "Evaluating Creative Short Story Generation in Humans and Large Language Models", "authors": ["Mete Ismayilzada", "Claire Stevenson", "Lonneke van der Plas"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ICCC 2025", "summary": "Story-writing is a fundamental aspect of human imagination, relying heavily\non creativity to produce narratives that are novel, effective, and surprising.\nWhile large language models (LLMs) have demonstrated the ability to generate\nhigh-quality stories, their creative story-writing capabilities remain\nunder-explored. In this work, we conduct a systematic analysis of creativity in\nshort story generation across 60 LLMs and 60 people using a five-sentence\ncue-word-based creative story-writing task. We use measures to automatically\nevaluate model- and human-generated stories across several dimensions of\ncreativity, including novelty, surprise, diversity, and linguistic complexity.\nWe also collect creativity ratings and Turing Test classifications from\nnon-expert and expert human raters and LLMs. Automated metrics show that LLMs\ngenerate stylistically complex stories, but tend to fall short in terms of\nnovelty, surprise and diversity when compared to average human writers. Expert\nratings generally coincide with automated metrics. However, LLMs and\nnon-experts rate LLM stories to be more creative than human-generated stories.\nWe discuss why and how these differences in ratings occur, and their\nimplications for both human and artificial creativity."}
{"id": "2411.03700", "pdf": "https://arxiv.org/pdf/2411.03700.pdf", "abs": "https://arxiv.org/abs/2411.03700", "title": "The Root Shapes the Fruit: On the Persistence of Gender-Exclusive Harms in Aligned Language Models", "authors": ["Anaelia Ovalle", "Krunoslav Lehman Pavasovic", "Louis Martin", "Luke Zettlemoyer", "Eric Michael Smith", "Kai-Wei Chang", "Adina Williams", "Levent Sagun"], "categories": ["cs.CL"], "comment": "Accepted to 2025 ACM FAccT", "summary": "Natural-language assistants are designed to provide users with helpful\nresponses while avoiding harmful outputs, largely achieved through alignment to\nhuman preferences. Yet there is limited understanding of whether alignment\ntechniques may inadvertently perpetuate or even amplify harmful biases\ninherited from their pre-aligned base models. This issue is compounded by the\nchoice of bias evaluation benchmarks in popular preference-finetuned models,\nwhich predominantly focus on dominant social categories, such as binary gender,\nthereby limiting insights into biases affecting underrepresented groups.\nTowards addressing this gap, we center transgender, nonbinary, and other\ngender-diverse identities to investigate how alignment procedures interact with\npre-existing gender-diverse bias in LLMs. Our key contributions include: 1) a\ncomprehensive survey of bias evaluation modalities across leading\npreference-finetuned LLMs, highlighting critical gaps in gender-diverse\nrepresentation, 2) systematic evaluation of gender-diverse biases across 16\nmodels spanning Direct Preference Optimization (DPO) stages, uncovering harms\npopular bias benchmarks fail to detect, and 3) a flexible framework for\nmeasuring harmful biases in implicit reward signals applicable to other social\ncontexts. Our findings reveal that DPO-aligned models are particularly\nsensitive to supervised finetuning (SFT), and can amplify two forms of\nreal-world gender-diverse harms from their base models: stigmatization and\ngender non-affirmative language. We conclude with recommendations tailored to\nDPO and broader alignment practices, advocating for the adoption of\ncommunity-informed bias evaluation frameworks to more effectively identify and\naddress underrepresented harms in LLMs."}
{"id": "2411.04223", "pdf": "https://arxiv.org/pdf/2411.04223.pdf", "abs": "https://arxiv.org/abs/2411.04223", "title": "Diversity Helps Jailbreak Large Language Models", "authors": ["Weiliang Zhao", "Daniel Ben-Levi", "Wei Hao", "Junfeng Yang", "Chengzhi Mao"], "categories": ["cs.CL"], "comment": null, "summary": "We have uncovered a powerful jailbreak technique that leverages large\nlanguage models' ability to diverge from prior context, enabling them to bypass\nsafety constraints and generate harmful outputs. By simply instructing the LLM\nto deviate and obfuscate previous attacks, our method dramatically outperforms\nexisting approaches, achieving up to a 62.83% higher success rate in\ncompromising ten leading chatbots, including GPT-4, Gemini, and Llama, while\nusing only 12.9% of the queries. This revelation exposes a critical flaw in\ncurrent LLM safety training, suggesting that existing methods may merely mask\nvulnerabilities rather than eliminate them. Our findings sound an urgent alarm\nfor the need to revolutionize testing methodologies to ensure robust and\nreliable LLM security."}
{"id": "2411.15100", "pdf": "https://arxiv.org/pdf/2411.15100.pdf", "abs": "https://arxiv.org/abs/2411.15100", "title": "XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models", "authors": ["Yixin Dong", "Charlie F. Ruan", "Yaxing Cai", "Ruihang Lai", "Ziyi Xu", "Yilong Zhao", "Tianqi Chen"], "categories": ["cs.CL", "cs.AI", "cs.PL"], "comment": "MLSys '25", "summary": "The applications of LLM Agents are becoming increasingly complex and diverse,\nleading to a high demand for structured outputs that can be parsed into code,\nstructured function calls, and embodied agent commands. These developments\nbring significant demands for structured generation in LLM inference.\nContext-free grammar is a flexible approach to enable structured generation via\nconstrained decoding. However, executing context-free grammar requires going\nthrough several stack states over all tokens in vocabulary during runtime,\nbringing non-negligible overhead for structured generation. In this paper, we\npropose XGrammar, a flexible and efficient structure generation engine for\nlarge language models. XGrammar accelerates context-free grammar execution by\ndividing the vocabulary into context-independent tokens that can be prechecked\nand context-dependent tokens that need to be interpreted during runtime. We\nfurther build transformations to expand the grammar context and reduce the\nnumber of context-independent tokens. Additionally, we build an efficient\npersistent stack to accelerate the context-dependent token checks. Finally, we\nco-design the grammar engine with LLM inference engine to overlap grammar\ncomputation with GPU executions. Evaluation results show that XGrammar can\nachieve up to 100x speedup over existing solutions. Combined with an LLM\ninference engine, it can generate near-zero overhead structure generation in\nend-to-end low-LLM serving."}
{"id": "2412.05342", "pdf": "https://arxiv.org/pdf/2412.05342.pdf", "abs": "https://arxiv.org/abs/2412.05342", "title": "Multi-Party Supervised Fine-tuning of Language Models for Multi-Party Dialogue Generation", "authors": ["Xiaoyu Wang", "Ningyuan Xi", "Teng Chen", "Qingqing Gu", "Yue Zhao", "Xiaokai Chen", "Zhonglin Jiang", "Yong Chen", "Luo Ji"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by IJCNN 2025", "summary": "Large Language Models (LLM) are usually fine-tuned to participate in dyadic\nor two-party dialogues, which can not adapt well to multi-party dialogues\n(MPD), which hinders their applications in such scenarios including\nmulti-personal meetings, discussions and daily communication. Previous\nLLM-based researches mainly focus on the multi-agent framework, while their\nbase LLMs are still pairwisely fine-tuned. In this work, we design a\nmulti-party fine-tuning framework (MuPaS) for LLMs on the multi-party dialogue\ndatasets, and prove such a straightforward framework can let the LLM align with\nthe multi-party conversation style efficiently and effectively. We also design\ntwo training strategies which can convert MuPaS into the MPD simulator.\nSubstantial experiments show that MuPaS can achieve state-of-the-art\nmulti-party response, higher accuracy of the-next-speaker prediction, higher\nhuman and automatic evaluated utterance qualities, and can even generate\nreasonably with out-of-distribution scene, topic and role descriptions. The\nMuPaS framework bridges the LLM training with more complicated multi-party\napplications, such as conversation generation, virtual rehearsal or\nmeta-universe."}
{"id": "2412.08587", "pdf": "https://arxiv.org/pdf/2412.08587.pdf", "abs": "https://arxiv.org/abs/2412.08587", "title": "Advancing Single and Multi-task Text Classification through Large Language Model Fine-tuning", "authors": ["Hang Zhao", "Qile P. Chen", "Yijing Barry Zhang", "Gang Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 3 tables", "summary": "Both encoder-only models (e.g., BERT, RoBERTa) and large language models\n(LLMs, e.g., Llama3) have been widely used for text classification tasks.\nHowever, there is a lack of systematic studies comparing the performance of\nencoder-based models and LLMs in text classification, particularly when\nfine-tuning is involved. This study employed a diverse range of models and\nmethods, varying in size and architecture, and including both fine-tuned and\npre-trained approaches. We first assessed the performances of these LLMs on the\n20 Newsgroups (20NG) and MASSIVE datasets, comparing them to encoder-only\nRoBERTa models. Additionally, we explored the multi-task capabilities of both\nmodel types by combining multiple classification tasks, including intent\ndetection and slot-filling, into a single model using data from both datasets.\nOur results indicate that fully fine-tuned Llama3-70B models outperform\nRoBERTa-large and other decoder LLMs across various classification tasks and\ndatasets. Moreover, the consolidated multi-task fine-tuned LLMs matched the\nperformance of dual-model setups in both tasks across both datasets. Overall,\nour study provides a comprehensive benchmark of encoder-only and LLM models on\ntext classification tasks and demonstrates a method to combine two or more\nfully fine-tuned decoder LLMs for reduced latency and equivalent performance."}
{"id": "2412.20309", "pdf": "https://arxiv.org/pdf/2412.20309.pdf", "abs": "https://arxiv.org/abs/2412.20309", "title": "Understanding the Impact of Confidence in Retrieval Augmented Generation: A Case Study in the Medical Domain", "authors": ["Shintaro Ozaki", "Yuta Kato", "Siyuan Feng", "Masayo Tomita", "Kazuki Hayashi", "Wataru Hashimoto", "Ryoma Obara", "Masafumi Oyamada", "Katsuhiko Hayashi", "Hidetaka Kamigaito", "Taro Watanabe"], "categories": ["cs.CL"], "comment": "Accepted to BioNLP2025 (Workshop colocated with ACL2025)", "summary": "Retrieval Augmented Generation (RAG) complements the knowledge of Large\nLanguage Models (LLMs) by leveraging external information to enhance response\naccuracy for queries. This approach is widely applied in several fields by\ntaking its advantage of injecting the most up-to-date information, and\nresearchers are focusing on understanding and improving this aspect to unlock\nthe full potential of RAG in such high-stakes applications. However, despite\nthe potential of RAG to address these needs, the mechanisms behind the\nconfidence levels of its outputs remain underexplored, although the confidence\nof information is very critical in some domains, such as finance, healthcare,\nand medicine. Our study focuses the impact of RAG on confidence within the\nmedical domain under various configurations and models. We evaluate confidence\nby treating the model's predicted probability as its output and calculating\nExpected Calibration Error (ECE) and Adaptive Calibration Error (ACE) scores\nbased on the probabilities and accuracy. In addition, we analyze whether the\norder of retrieved documents within prompts calibrates the confidence. Our\nfindings reveal large variation in confidence and accuracy depending on the\nmodel, settings, and the format of input prompts. These results underscore the\nnecessity of optimizing configurations based on the specific model and\nconditions."}
{"id": "2501.01652", "pdf": "https://arxiv.org/pdf/2501.01652.pdf", "abs": "https://arxiv.org/abs/2501.01652", "title": "MIRAGE: Exploring How Large Language Models Perform in Complex Social Interactive Environments", "authors": ["Yin Cai", "Zhouhong Gu", "Zhaohan Du", "Zheyu Ye", "Shaosheng Cao", "Yiqian Xu", "Hongwei Feng", "Ping Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable capabilities in\nenvironmental perception, reasoning-based decision-making, and simulating\ncomplex human behaviors, particularly in interactive role-playing contexts.\nThis paper introduces the Multiverse Interactive Role-play Ability General\nEvaluation (MIRAGE), a comprehensive framework designed to assess LLMs'\nproficiency in portraying advanced human behaviors through murder mystery\ngames. MIRAGE features eight intricately crafted scripts encompassing diverse\nthemes and styles, providing a rich simulation. To evaluate LLMs' performance,\nMIRAGE employs four distinct methods: the Trust Inclination Index (TII) to\nmeasure dynamics of trust and suspicion, the Clue Investigation Capability\n(CIC) to measure LLMs' capability of conducting information, the Interactivity\nCapability Index (ICI) to assess role-playing capabilities and the Script\nCompliance Index (SCI) to assess LLMs' capability of understanding and\nfollowing instructions. Our experiments indicate that even popular models like\nGPT-4 face significant challenges in navigating the complexities presented by\nthe MIRAGE. The datasets and simulation codes are available in\n\\href{https://github.com/lime728/MIRAGE}{github}."}
{"id": "2501.11110", "pdf": "https://arxiv.org/pdf/2501.11110.pdf", "abs": "https://arxiv.org/abs/2501.11110", "title": "Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large Language Models via a Multi-Paradigm Perspective", "authors": ["Yiyao Yu", "Yuxiang Zhang", "Dongdong Zhang", "Xiao Liang", "Hengyuan Zhang", "Xingxing Zhang", "Mahmoud Khademi", "Hany Awadalla", "Junjie Wang", "Yujiu Yang", "Furu Wei"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have made notable progress in mathematical\nreasoning, yet often rely on single-paradigm reasoning, limiting their\neffectiveness across diverse tasks. We introduce Chain-of-Reasoning (CoR), a\nnovel unified framework integrating multiple reasoning paradigms--Natural\nLanguage Reasoning (NLR), Algorithmic Reasoning (AR), and Symbolic Reasoning\n(SR)--to enable synergistic collaboration. CoR generates multiple potential\nanswers via different reasoning paradigms and synthesizes them into a coherent\nfinal solution. We propose a Progressive Paradigm Training (PPT) strategy for\nmodels to progressively master these paradigms, leading to CoR-Math-7B.\nExperimental results demonstrate that CoR-Math-7B significantly outperforms\ncurrent SOTA models, achieving up to a 41.0% absolute improvement over GPT-4o\nin theorem proving and a 15.0% improvement over RL-based methods on the MATH\nbenchmark in arithmetic tasks. These results show the enhanced mathematical\ncomprehension ability of our model, enabling zero-shot generalization across\ntasks."}
{"id": "2501.13428", "pdf": "https://arxiv.org/pdf/2501.13428.pdf", "abs": "https://arxiv.org/abs/2501.13428", "title": "Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language Models", "authors": ["Bo Gao", "Michael W. Spratling"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "14 pages and 2 figures", "summary": "Large language models have achieved remarkable success in recent years,\nprimarily due to the implementation of self-attention mechanisms. However,\ntraditional Softmax attention suffers from numerical instability and reduced\nperformance as the length of inference tokens increases. This paper addresses\nthese issues by decomposing the Softmax operation into a non-linear\ntransformation and the $l_1$-norm. We identify the latter as essential for\nmaintaining model performance. By replacing the non-linear transformation with\nthe Softplus activation function and introducing a dynamic scale factor for\ndifferent token lengths based on invariance entropy, we create a novel\nattention mechanism with performance better than conventional Softmax attention\nacross various inference lengths. To further improve the length extrapolation\nability of the proposed attention mechanism, we introduce a novel re-weighting\nmechanism that amplifies significant attention weights while diminishing weaker\nones, enabling the model to concentrate more effectively on relevant tokens.\nWhen combined with our proposed attention mechanism, this approach maintains\nnearly constant validation loss even at 16$\\times$ the training token length,\nensures numerical stability, and achieves superior results on downstream\nbenchmarks."}
{"id": "2502.01568", "pdf": "https://arxiv.org/pdf/2502.01568.pdf", "abs": "https://arxiv.org/abs/2502.01568", "title": "Visual Theory of Mind Enables the Invention of Proto-Writing", "authors": ["Benjamin A. Spiegel", "Lucas Gelfond", "George Konidaris"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for oral presentation at CogSci 2025, published here with\n  permission from organizers", "summary": "Symbolic writing systems are graphical semiotic codes that are ubiquitous in\nmodern society but are otherwise absent in the animal kingdom. Anthropological\nevidence suggests that the earliest forms of some writing systems originally\nconsisted of iconic pictographs, which signify their referent via visual\nresemblance. While previous studies have examined the emergence and,\nseparately, the evolution of pictographic systems through a computational lens,\nmost employ non-naturalistic methodologies that make it difficult to draw clear\nanalogies to human and animal cognition. We develop a multi-agent reinforcement\nlearning testbed for emergent communication called a Signification Game, and\nformulate a model of inferential communication that enables agents to leverage\nvisual theory of mind to communicate actions using pictographs. Our model,\nwhich is situated within a broader formalism for animal communication, sheds\nlight on the cognitive and cultural processes underlying the emergence of\nproto-writing."}
{"id": "2502.12084", "pdf": "https://arxiv.org/pdf/2502.12084.pdf", "abs": "https://arxiv.org/abs/2502.12084", "title": "VLM2-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues", "authors": ["Jianshu Zhang", "Dongyu Yao", "Renjie Pi", "Paul Pu Liang", "Yi R. Fung"], "categories": ["cs.CL"], "comment": "Project Page: https://vlm2-bench.github.io/", "summary": "Visually linking matching cues is a crucial ability in daily life, such as\nidentifying the same person in multiple photos based on their cues, even\nwithout knowing who they are. Despite the extensive knowledge that\nvision-language models (VLMs) possess, it remains largely unexplored whether\nthey are capable of performing this fundamental task. To address this, we\nintroduce VLM2-Bench, a benchmark designed to assess whether VLMs can Visually\nLink Matching cues, with 9 subtasks and over 3,000 test cases. Comprehensive\nevaluation across eight open-source VLMs and GPT-4o, along with further\nanalysis of various language-side and vision-side prompting methods, leads to a\ntotal of eight key findings. We identify critical challenges in models' ability\nto link visual cues, highlighting a significant performance gap where even\nGPT-4o lags 34.80% behind humans. Based on these insights, we advocate for (i)\nenhancing core visual capabilities to improve adaptability and reduce reliance\non prior knowledge, (ii) establishing clearer principles for integrating\nlanguage-based reasoning in vision-centric tasks to prevent unnecessary biases,\nand (iii) shifting vision-text training paradigms toward fostering models'\nability to independently structure and infer relationships among visual cues."}
{"id": "2502.12896", "pdf": "https://arxiv.org/pdf/2502.12896.pdf", "abs": "https://arxiv.org/abs/2502.12896", "title": "None of the Others: a General Technique to Distinguish Reasoning from Memorization in Multiple-Choice LLM Evaluation Benchmarks", "authors": ["Eva Sánchez Salido", "Julio Gonzalo", "Guillermo Marco"], "categories": ["cs.CL"], "comment": null, "summary": "In LLM evaluations, reasoning is often distinguished from recall/memorization\nby performing numerical variations to math-oriented questions. Here we\nintroduce a general variation method for multiple-choice questions that\ncompletely dissociates the correct answer from previously seen tokens or\nconcepts, requiring LLMs to understand and reason (rather than memorizing) in\norder to answer correctly. Using this method, we evaluate state-of-the-art\nproprietary and open-source LLMs on two datasets available in English and\nSpanish: the public MMLU benchmark and the private UNED-Access 2024 dataset.\nResults show that all models experience remarkable accuracy drops under our\nproposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access\n2024, ranging from 10% to 93% across models. Notably, the most accurate model\nin our experimentation (OpenAI-o3-mini) is not the most robust\n(DeepSeek-R1-70B), suggesting that the best models in standard evaluations may\nnot be the ones with better reasoning capabilities. Also, we see larger\naccuracy drops in public (vs private) datasets and questions posed in their\noriginal language (vs a manual translation), which are signs of contamination\nand also point to a relevant role of recall/memorization in current LLMs'\nanswers."}
{"id": "2503.00955", "pdf": "https://arxiv.org/pdf/2503.00955.pdf", "abs": "https://arxiv.org/abs/2503.00955", "title": "SemViQA: A Semantic Question Answering System for Vietnamese Information Fact-Checking", "authors": ["Dien X. Tran", "Nam V. Nguyen", "Thanh T. Tran", "Anh T. Hoang", "Tai V. Duong", "Di T. Le", "Phuc-Lu Le"], "categories": ["cs.CL", "cs.AI"], "comment": "18 pages", "summary": "The rise of misinformation, exacerbated by Large Language Models (LLMs) like\nGPT and Gemini, demands robust fact-checking solutions, especially for\nlow-resource languages like Vietnamese. Existing methods struggle with semantic\nambiguity, homonyms, and complex linguistic structures, often trading accuracy\nfor efficiency. We introduce SemViQA, a novel Vietnamese fact-checking\nframework integrating Semantic-based Evidence Retrieval (SER) and Two-step\nVerdict Classification (TVC). Our approach balances precision and speed,\nachieving state-of-the-art results with 78.97\\% strict accuracy on ISE-DSC01\nand 80.82\\% on ViWikiFC, securing 1st place in the UIT Data Science Challenge.\nAdditionally, SemViQA Faster improves inference speed 7x while maintaining\ncompetitive accuracy. SemViQA sets a new benchmark for Vietnamese fact\nverification, advancing the fight against misinformation. The source code is\navailable at: https://github.com/DAVID-NGUYEN-S16/SemViQA."}
{"id": "2503.01217", "pdf": "https://arxiv.org/pdf/2503.01217.pdf", "abs": "https://arxiv.org/abs/2503.01217", "title": "HREB-CRF: Hierarchical Reduced-bias EMA for Chinese Named Entity Recognition", "authors": ["Sijin Sun", "Ming Deng", "Xinrui Yu", "Liangbin Zhao"], "categories": ["cs.CL", "cs.LG"], "comment": "8 pages, 6 figures; Accepted for publication at the 2025\n  International Joint Conference on Neural Networks (IJCNN 2025), Rome, Italy,\n  30 June - 5 July", "summary": "Incorrect boundary division, complex semantic representation, and differences\nin pronunciation and meaning often lead to errors in Chinese Named Entity\nRecognition(CNER). To address these issues, this paper proposes HREB-CRF\nframework: Hierarchical Reduced-bias EMA with CRF. The proposed method\namplifies word boundaries and pools long text gradients through exponentially\nfixed-bias weighted average of local and global hierarchical attention.\nExperimental results on the MSRA, Resume, and Weibo datasets show excellent in\nF1, outperforming the baseline model by 1.1\\%, 1.6\\%, and 9.8\\%. The\nsignificant improvement in F1 shows evidences of strong effectiveness and\nrobustness of approach in CNER tasks."}
{"id": "2503.01844", "pdf": "https://arxiv.org/pdf/2503.01844.pdf", "abs": "https://arxiv.org/abs/2503.01844", "title": "Can (A)I Change Your Mind?", "authors": ["Miriam Havin", "Timna Wharton Kleinman", "Moran Koren", "Yaniv Dover", "Ariel Goldstein"], "categories": ["cs.CL"], "comment": "Accetped to CogSci 2025", "summary": "The increasing integration of large language model (LLM) based conversational\nagents into everyday life raises critical cognitive and social questions about\ntheir potential to influence human opinions. Although previous studies have\nshown that LLM-based agents can generate persuasive content, these typically\ninvolve controlled, English-language settings. Addressing this, our\npreregistered study explored LLM's persuasive capabilities in more ecological,\nunconstrained scenarios, examining both static (written paragraphs) and dynamic\n(conversations via Telegram) interaction types. Conducted entirely in Hebrew\nwith 200 participants, the study assessed the persuasive effects of both LLM\nand human interlocutors on controversial civil policy topics. Results indicated\nthat participants adopted LLM and human perspectives similarly, with\nsignificant opinion changes evident across all conditions, regardless of\ninterlocutor type or interaction mode. Confidence levels increased\nsignificantly in most scenarios, except in static LLM interactions. These\nfindings demonstrate LLM-based agents' robust persuasive capabilities across\ndiverse sources and settings, highlighting their potential impact on shaping\npublic opinions."}
{"id": "2503.01921", "pdf": "https://arxiv.org/pdf/2503.01921.pdf", "abs": "https://arxiv.org/abs/2503.01921", "title": "NCL-UoR at SemEval-2025 Task 3: Detecting Multilingual Hallucination and Related Observable Overgeneration Text Spans with Modified RefChecker and Modified SeflCheckGPT", "authors": ["Jiaying Hong", "Thanet Markchom", "Jianfei Xu", "Tong Wu", "Huizhi Liang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "SemEval-2025 Task 3 (Mu-SHROOM) focuses on detecting hallucinations in\ncontent generated by various large language models (LLMs) across multiple\nlanguages. This task involves not only identifying the presence of\nhallucinations but also pinpointing their specific occurrences. To tackle this\nchallenge, this study introduces two methods: modified RefChecker and modified\nSelfCheckGPT. The modified RefChecker integrates prompt-based factual\nverification into References, structuring them as claim-based tests rather than\nsingle external knowledge sources. The modified SelfCheckGPT incorporates\nexternal knowledge to overcome its reliance on internal knowledge. In addition,\nboth methods' original prompt designs are enhanced to identify hallucinated\nwords within LLM-generated texts. Experimental results demonstrate the\neffectiveness of the approach, achieving a high ranking on the test dataset in\ndetecting hallucinations across various languages, with an average IoU of\n0.5310 and an average COR of 0.5669."}
{"id": "2503.03122", "pdf": "https://arxiv.org/pdf/2503.03122.pdf", "abs": "https://arxiv.org/abs/2503.03122", "title": "The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models", "authors": ["Zichao Li", "Xueru Wen", "Jie Lou", "Yuqiu Ji", "Yaojie Lu", "Xianpei Han", "Debing Zhang", "Le Sun"], "categories": ["cs.CL", "cs.AI"], "comment": "ICML 2025", "summary": "Multimodal Reward Models (MM-RMs) are crucial for aligning Large Language\nModels (LLMs) with human preferences, particularly as LLMs increasingly\ninteract with multimodal data. However, we find that MM-RMs trained on existing\ndatasets often struggle to generalize to out-of-distribution data due to their\nreliance on unimodal spurious correlations, primarily text-only shortcuts\nwithin the training distribution, which prevents them from leveraging true\nmultimodal reward functions. To address this, we introduce a Shortcut-aware\nMM-RM learning algorithm that mitigates this issue by dynamically reweighting\ntraining samples, shifting the distribution toward better multimodal\nunderstanding, and reducing dependence on unimodal spurious correlations. Our\nexperiments demonstrate significant improvements in generalization, downstream\ntask performance, and scalability, establishing a more robust framework for\nmultimodal reward modeling."}
{"id": "2503.10354", "pdf": "https://arxiv.org/pdf/2503.10354.pdf", "abs": "https://arxiv.org/abs/2503.10354", "title": "A Hybrid Architecture with Efficient Fine Tuning for Abstractive Patent Document Summarization", "authors": ["Nevidu Jayatilleke", "Ruvan Weerasinghe"], "categories": ["cs.CL"], "comment": "Accepted Paper in the 8th International Research Conference on Smart\n  Computing and Systems Engineering, University of Kelaniya, Sri Lanka.\n  (Pending Publication)", "summary": "Automatic patent summarization approaches that help in the patent analysis\nand comprehension procedure are in high demand due to the colossal growth of\ninnovations. The development of natural language processing (NLP), text mining,\nand deep learning has notably amplified the efficacy of text summarization\nmodels for abundant types of documents. Summarizing patent text remains a\npertinent challenge due to the labyrinthine writing style of these documents,\nwhich includes technical and legal intricacies. Additionally, these patent\ndocument contents are considerably lengthier than archetypal documents, which\ncomplicates the process of extracting pertinent information for summarization.\nEmbodying extractive and abstractive text summarization methodologies into a\nhybrid framework, this study proposes a system for efficiently creating\nabstractive summaries of patent records. The procedure involves leveraging the\nLexRank graph-based algorithm to retrieve the important sentences from input\nparent texts, then utilizing a Bidirectional Auto-Regressive Transformer (BART)\nmodel that has been fine-tuned using Low-Ranking Adaptation (LoRA) for\nproducing text summaries. This is accompanied by methodical testing and\nevaluation strategies. Furthermore, the author employed certain meta-learning\ntechniques to achieve Domain Generalization (DG) of the abstractive component\nacross multiple patent fields."}
{"id": "2504.13471", "pdf": "https://arxiv.org/pdf/2504.13471.pdf", "abs": "https://arxiv.org/abs/2504.13471", "title": "From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs", "authors": ["Jiliang Ni", "Jiachen Pu", "Zhongyi Yang", "Kun Zhou", "Hui Wang", "Xiaoliang Xiao", "Dakui Wang", "Xin Li", "Jingfeng Luo", "Conggang Hu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have significantly advanced artificial\nintelligence by optimizing traditional Natural Language Processing (NLP)\nworkflows, facilitating their integration into various systems. Many such NLP\nsystems, including ours, directly incorporate LLMs. However, this approach\neither results in expensive costs or yields suboptimal performance after\nfine-tuning. In this paper, we introduce a three-stage cost-efficient\nend-to-end LLM deployment pipeline, comprising prototyping, knowledge transfer,\nand model compression, to effectively tackle the cost-performance dilemma in\nLLM-based frameworks. Its high cost-efficiency is manifested not only in\nsimplifying system complexity and producing super-tiny online models with\nenhanced performance and reduced costs in the results, but also in addressing\ndevelopment cycle constraints, the lack of extensive high-quality data, and\nlimited computational resources during the project development process. In the\nfirst stage, we construct an optimal performance prototype system by\ntransforming complex tasks into a function call-based LLM-driven pipeline,\nwhich serves as a teacher model to generate high-quality data. In the second\nstage, we combine techniques like rejection sampling fine-tuning, reinforcement\nlearning, and knowledge distillation to transfer knowledge to 0.5B student\nmodels, delivering effective performance at minimal cost. In the final stage,\nwe further compress models to 0.4B via quantization and pruning, achieving\nultra-low latency and cost. Extensive experimental results and the framework's\nmodular design suggest cross-domain capabilities and potential applicability in\nother NLP areas."}
{"id": "2504.16007", "pdf": "https://arxiv.org/pdf/2504.16007.pdf", "abs": "https://arxiv.org/abs/2504.16007", "title": "Methods for Recognizing Nested Terms", "authors": ["Igor Rozhkov", "Natalia Loukachevitch"], "categories": ["cs.CL"], "comment": "Published in Computational Linguistics and Intellectual Technologies:\n  Proceedings of the International Conference \"Dialogue 2025\"", "summary": "In this paper, we describe our participation in the RuTermEval competition\ndevoted to extracting nested terms. We apply the Binder model, which was\npreviously successfully applied to the recognition of nested named entities, to\nextract nested terms. We obtained the best results of term recognition in all\nthree tracks of the RuTermEval competition. In addition, we study the new task\nof recognition of nested terms from flat training data annotated with terms\nwithout nestedness. We can conclude that several approaches we proposed in this\nwork are viable enough to retrieve nested terms effectively without nested\nlabeling of them."}
{"id": "2504.16394", "pdf": "https://arxiv.org/pdf/2504.16394.pdf", "abs": "https://arxiv.org/abs/2504.16394", "title": "ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs", "authors": ["Fahmida Liza Piya", "Rahmatollah Beheshti"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Unstructured clinical data can serve as a unique and rich source of\ninformation that can meaningfully inform clinical practice. Extracting the most\npertinent context from such data is critical for exploiting its true potential\ntoward optimal and timely decision-making in patient care. While prior research\nhas explored various methods for clinical text summarization, most prior\nstudies either process all input tokens uniformly or rely on heuristic-based\nfilters, which can overlook nuanced clinical cues and fail to prioritize\ninformation critical for decision-making. In this study, we propose Contextual,\na novel framework that integrates a Context-Preserving Token Filtering method\nwith a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By\npreserving context-specific important tokens and enriching them with structured\nknowledge, ConTextual improves both linguistic coherence and clinical fidelity.\nOur extensive empirical evaluations on two public benchmark datasets\ndemonstrate that ConTextual consistently outperforms other baselines. Our\nproposed approach highlights the complementary role of token-level filtering\nand structured retrieval in enhancing both linguistic and clinical integrity,\nas well as offering a scalable solution for improving precision in clinical\ntext generation."}
{"id": "2504.19044", "pdf": "https://arxiv.org/pdf/2504.19044.pdf", "abs": "https://arxiv.org/abs/2504.19044", "title": "Calibrating Translation Decoding with Quality Estimation on LLMs", "authors": ["Di Wu", "Yibin Lei", "Christof Monz"], "categories": ["cs.CL"], "comment": null, "summary": "Neural machine translation (NMT) systems typically employ maximum a\nposteriori (MAP) decoding to select the highest-scoring translation from the\ndistribution mass. However, recent evidence highlights the inadequacy of MAP\ndecoding, often resulting in low-quality or even pathological hypotheses -- the\ndecoding objective is not aligned with real-world translation quality. This\npaper proposes calibrating hypothesis likelihoods with translation quality from\na distribution view by directly optimizing their Pearson correlation -- thereby\nenhancing the effectiveness of translation decoding. With our method,\ntranslation on large language models (LLMs) improves substantially after\nlimited training (2K instances per direction). This improvement is orthogonal\nto those achieved through supervised fine-tuning, leading to substantial gains\nacross a broad range of metrics and human evaluations -- even when applied to\ntop-performing translation-specialized LLMs fine-tuned on high-quality\ntranslation data, such as Tower, or when compared to recent preference\noptimization methods, like CPO. Moreover, the calibrated translation likelihood\ncan directly serve as a strong proxy for translation quality, closely\napproximating or even surpassing some state-of-the-art translation quality\nestimation models, like CometKiwi. Lastly, our in-depth analysis demonstrates\nthat calibration enhances the effectiveness of MAP decoding, thereby enabling\ngreater efficiency in real-world deployment. The resulting state-of-the-art\ntranslation model, which covers 10 languages, along with the accompanying code\nand human evaluation data, has been released to the community:\nhttps://github.com/moore3930/calibrating-llm-mt."}
{"id": "2504.19339", "pdf": "https://arxiv.org/pdf/2504.19339.pdf", "abs": "https://arxiv.org/abs/2504.19339", "title": "Explanatory Summarization with Discourse-Driven Planning", "authors": ["Dongqi Liu", "Xi Yu", "Vera Demberg", "Mirella Lapata"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by the Transactions of the Association for Computational\n  Linguistics (TACL 2025)", "summary": "Lay summaries for scientific documents typically include explanations to help\nreaders grasp sophisticated concepts or arguments. However, current automatic\nsummarization methods do not explicitly model explanations, which makes it\ndifficult to align the proportion of explanatory content with human-written\nsummaries. In this paper, we present a plan-based approach that leverages\ndiscourse frameworks to organize summary generation and guide explanatory\nsentences by prompting responses to the plan. Specifically, we propose two\ndiscourse-driven planning strategies, where the plan is conditioned as part of\nthe input or part of the output prefix, respectively. Empirical experiments on\nthree lay summarization datasets show that our approach outperforms existing\nstate-of-the-art methods in terms of summary quality, and it enhances model\nrobustness, controllability, and mitigates hallucination."}
{"id": "2505.00024", "pdf": "https://arxiv.org/pdf/2505.00024.pdf", "abs": "https://arxiv.org/abs/2505.00024", "title": "Nemotron-Research-Tool-N1: Exploring Tool-Using Language Models with Reinforced Reasoning", "authors": ["Shaokun Zhang", "Yi Dong", "Jieyu Zhang", "Jan Kautz", "Bryan Catanzaro", "Andrew Tao", "Qingyun Wu", "Zhiding Yu", "Guilin Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 6 tables, 12 figures. - update new results - add more\n  details", "summary": "Enabling large language models with external tools has become a pivotal\nstrategy for extending their functionality beyond text space. To enhance LLMs'\ntool-calling abilities, previous approaches primarily rely on supervised\nfine-tuning (SFT) with trajectories distilled from stronger models, often\nresulting in imitative reasoning that limits generalization. In this work, we\nexplore rule-based reinforcement learning to enhance tool-calling in LLMs,\nresulting in Nemotron-Research-Tool-N1, a series of tool-calling reasoning\nmodels. Rather than enforcing supervision over intermediate distilled reasoning\ntraces, Tool-N1 is trained with a binary RL reward that assesses only the\nformat validity and functional correctness of tool invocations. This\nlightweight supervision allows the model to develop reasoning strategies\nindependently, without relying on annotated trajectories. Experiments on\nseveral major benchmarks show that Tool-N1-7B/14B clearly outperform GPT-4o. We\nconduct a systematic study on the design of rule-based reinforcement learning\nstrategies for training tool-calling models. Using 5,518 distilled reasoning\ntrajectories, we compare SFT, RL, and the SFT-then-RL pipeline, finding that\nthe widely adopted SFT-then-RL paradigm does not necessarily outperform pure\nRL."}
{"id": "2505.02656", "pdf": "https://arxiv.org/pdf/2505.02656.pdf", "abs": "https://arxiv.org/abs/2505.02656", "title": "Proper Name Diacritization for Arabic Wikipedia: A Benchmark Dataset", "authors": ["Rawan Bondok", "Mayar Nassar", "Salam Khalifa", "Kurt Micallef", "Nizar Habash"], "categories": ["cs.CL"], "comment": null, "summary": "Proper names in Arabic Wikipedia are frequently undiacritized, creating\nambiguity in pronunciation and interpretation, especially for transliterated\nnamed entities of foreign origin. While transliteration and diacritization have\nbeen well-studied separately in Arabic NLP,their intersection remains\nunderexplored. In this paper, we introduce a new manually diacritized dataset\nof Arabic proper names of various origins with their English Wikipedia\nequivalent glosses, and present the challenges and guidelines we followed to\ncreate it. We benchmark GPT-4o on the task of recovering full diacritization\ngiven the undiacritized Arabic and English forms, and analyze its performance.\nAchieving 73% accuracy, our results underscore both the difficulty of the task\nand the need for improved models and resources. We release our dataset to\nfacilitate further research on Arabic Wikipedia proper name diacritization."}
{"id": "2310.14356", "pdf": "https://arxiv.org/pdf/2310.14356.pdf", "abs": "https://arxiv.org/abs/2310.14356", "title": "Semantic and Expressive Variation in Image Captions Across Languages", "authors": ["Andre Ye", "Sebastin Santy", "Jena D. Hwang", "Amy X. Zhang", "Ranjay Krishna"], "categories": ["cs.CV", "cs.CL", "cs.CY", "cs.HC"], "comment": "CVPR 2025", "summary": "Computer vision often treats human perception as homogeneous: an implicit\nassumption that visual stimuli are perceived similarly by everyone. This\nassumption is reflected in the way researchers collect datasets and train\nvision models. By contrast, literature in cross-cultural psychology and\nlinguistics has provided evidence that people from different cultural\nbackgrounds observe vastly different concepts even when viewing the same visual\nstimuli. In this paper, we study how these differences manifest themselves in\nvision-language datasets and models, using language as a proxy for culture. By\ncomparing textual descriptions generated across 7 languages for the same\nimages, we find significant differences in the semantic content and linguistic\nexpression. When datasets are multilingual as opposed to monolingual,\ndescriptions have higher semantic coverage on average, where coverage is\nmeasured using scene graphs, model embeddings, and linguistic taxonomies. For\nexample, multilingual descriptions have on average 29.9% more objects, 24.5%\nmore relations, and 46.0% more attributes than a set of monolingual captions.\nWhen prompted to describe images in different languages, popular models (e.g.\nLLaVA) inherit this bias and describe different parts of the image. Moreover,\nfinetuning models on captions from one language performs best on corresponding\ntest data from that language, while finetuning on multilingual data performs\nconsistently well across all test data compositions. Our work points towards\nthe need to account for and embrace the diversity of human perception in the\ncomputer vision community."}
{"id": "2311.11796", "pdf": "https://arxiv.org/pdf/2311.11796.pdf", "abs": "https://arxiv.org/abs/2311.11796", "title": "Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems", "authors": ["Guangjing Wang", "Ce Zhou", "Yuanda Wang", "Bocheng Chen", "Hanqing Guo", "Qiben Yan"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "As Artificial Intelligence (AI) systems increasingly underpin critical\napplications, from autonomous vehicles to biometric authentication, their\nvulnerability to transferable attacks presents a growing concern. These\nattacks, designed to generalize across instances, domains, models, tasks,\nmodalities, or even hardware platforms, pose severe risks to security, privacy,\nand system integrity. This survey delivers the first comprehensive review of\ntransferable attacks across seven major categories, including evasion,\nbackdoor, data poisoning, model stealing, model inversion, membership\ninference, and side-channel attacks. We introduce a unified six-dimensional\ntaxonomy: cross-instance, cross-domain, cross-modality, cross-model,\ncross-task, and cross-hardware, which systematically captures the diverse\ntransfer pathways of adversarial strategies. Through this framework, we examine\nboth the underlying mechanics and practical implications of transferable\nattacks on AI systems. Furthermore, we review cutting-edge methods for\nenhancing attack transferability, organized around data augmentation and\noptimization strategies. By consolidating fragmented research and identifying\ncritical future directions, this work provides a foundational roadmap for\nunderstanding, evaluating, and defending against transferable threats in\nreal-world AI systems."}
{"id": "2403.16971", "pdf": "https://arxiv.org/pdf/2403.16971.pdf", "abs": "https://arxiv.org/abs/2403.16971", "title": "AIOS: LLM Agent Operating System", "authors": ["Kai Mei", "Xi Zhu", "Wujiang Xu", "Wenyue Hua", "Mingyu Jin", "Zelong Li", "Shuyuan Xu", "Ruosong Ye", "Yingqiang Ge", "Yongfeng Zhang"], "categories": ["cs.OS", "cs.AI", "cs.CL"], "comment": null, "summary": "LLM-based intelligent agents face significant deployment challenges,\nparticularly related to resource management. Allowing unrestricted access to\nLLM or tool resources can lead to inefficient or even potentially harmful\nresource allocation and utilization for agents. Furthermore, the absence of\nproper scheduling and resource management mechanisms in current agent designs\nhinders concurrent processing and limits overall system efficiency. As the\ndiversity and complexity of agents continue to grow, addressing these resource\nmanagement issues becomes increasingly critical to LLM-based agent systems. To\naddress these challenges, this paper proposes the architecture of AIOS\n(LLM-based AI Agent Operating System) under the context of managing LLM-based\nagents. It introduces a novel architecture for serving LLM-based agents by\nisolating resources and LLM-specific services from agent applications into an\nAIOS kernel. This AIOS kernel provides fundamental services (e.g., scheduling,\ncontext management, memory management, storage management, access control) and\nefficient management of resources (e.g., LLM and external tools) for runtime\nagents. To enhance usability, AIOS also includes an AIOS-Agent SDK, a\ncomprehensive suite of APIs designed for utilizing functionalities provided by\nthe AIOS kernel. Experimental results demonstrate that using AIOS can achieve\nup to 2.1x faster execution for serving agents built by various agent\nframeworks. The source code is available at\nhttps://github.com/agiresearch/AIOS."}
{"id": "2405.15189", "pdf": "https://arxiv.org/pdf/2405.15189.pdf", "abs": "https://arxiv.org/abs/2405.15189", "title": "EffiLearner: Enhancing Efficiency of Generated Code via Self-Optimization", "authors": ["Dong Huang", "Jianbo Dai", "Han Weng", "Puzhen Wu", "Yuhao Qing", "Heming Cui", "Zhijiang Guo", "Jie M. Zhang"], "categories": ["cs.SE", "cs.CL"], "comment": "Accepted by NeurIPS 2024", "summary": "Large language models (LLMs) have shown remarkable progress in code\ngeneration, but their generated code often suffers from inefficiency, resulting\nin longer execution times and higher memory consumption. To address this issue,\nwe propose \\textbf{EffiLearner}, a self-optimization framework that utilizes\nexecution overhead profiles to improve the efficiency of LLM-generated code.\nEffiLearner first generates code using an LLM, then executes it locally to\ncapture execution time and memory usage profiles. These profiles are fed back\nto the LLM, which then revises the code to reduce overhead. To evaluate the\neffectiveness of EffiLearner, we conduct extensive experiments on the\nEffiBench, HumanEval, and MBPP with 16 open-source and 6 closed-source models.\nOur evaluation results demonstrate that through iterative self-optimization,\nEffiLearner significantly enhances the efficiency of LLM-generated code. For\nexample, the execution time (ET) of StarCoder2-15B for the EffiBench decreases\nfrom 0.93 (s) to 0.12 (s) which reduces 87.1% the execution time requirement\ncompared with the initial code. The total memory usage (TMU) of StarCoder2-15B\nalso decreases from 22.02 (Mb*s) to 2.03 (Mb*s), which decreases 90.8% of total\nmemory consumption during the execution process. The source code of EffiLearner\nwas released in https://github.com/huangd1999/EffiLearner"}
{"id": "2406.06620", "pdf": "https://arxiv.org/pdf/2406.06620.pdf", "abs": "https://arxiv.org/abs/2406.06620", "title": "MedualTime: A Dual-Adapter Language Model for Medical Time Series-Text Multimodal Learning", "authors": ["Jiexia Ye", "Weiqi Zhang", "Ziyue Li", "Jia Li", "Meng Zhao", "Fugee Tsung"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "9 pages, 6 figure, 3 tables", "summary": "The recent rapid advancements in language models (LMs) have garnered\nattention in medical time series-text multimodal learning. However, existing\ncontrastive learning-based and prompt-based LM approaches tend to be biased,\noften assigning a primary role to time series modality while treating text\nmodality as secondary. We classify these approaches under a temporal-primary\nparadigm, which may overlook the unique and critical task-relevant information\nembedded in text modality like clinical reports, thus failing to fully leverage\nmutual benefits and complementarity of different modalities. To fill this gap,\nwe propose a novel textual-temporal multimodal learning paradigm that enables\neither modality to serve as the primary while being enhanced by the other,\nthereby effectively capturing modality-specific information and fostering\ncross-modal interaction. In specific, we design MedualTime, a language model\ncomposed of dual adapters to implement temporal-primary and textual-primary\nmodeling simultaneously. Within each adapter, lightweight adaptation tokens are\ninjected into the top layers of LM to encourage high-level modality fusion. The\nshared LM pipeline by dual adapters not only achieves adapter alignment but\nalso enables efficient fine-tuning, reducing computational resources.\nEmpirically, MedualTime demonstrates superior performance on medical data,\nachieving notable improvements of 8% accuracy and 12% F1 in supervised\nsettings. Furthermore, MedualTime's transferability is validated by few-shot\nlabel transfer experiments from coarse-grained to fine-grained medical data.\nhttps://github.com/start2020/MedualTime"}
{"id": "2408.14419", "pdf": "https://arxiv.org/pdf/2408.14419.pdf", "abs": "https://arxiv.org/abs/2408.14419", "title": "CHARTOM: A Visual Theory-of-Mind Benchmark for Multimodal Large Language Models", "authors": ["Shubham Bharti", "Shiyun Cheng", "Jihyun Rho", "Jianrui Zhang", "Mu Cai", "Yong Jae Lee", "Martina Rau", "Xiaojin Zhu"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "We introduce CHARTOM, a visual theory-of-mind benchmark for multimodal large\nlanguage models. CHARTOM consists of specially designed data visualizing\ncharts. Given a chart, a language model needs to not only correctly comprehend\nthe chart (the FACT question) but also judge if the chart will be misleading to\na human reader (the MIND question). Both questions have significant societal\nbenefits. We detail the construction of the CHARTOM benchmark including its\ncalibration on human performance. We benchmark leading LLMs as of late 2024 -\nincluding GPT, Claude, Gemini, Qwen, Llama, and Llava - on the CHARTOM dataset\nand found that our benchmark was challenging to all of them, suggesting room\nfor future large language models to improve."}
{"id": "2410.09982", "pdf": "https://arxiv.org/pdf/2410.09982.pdf", "abs": "https://arxiv.org/abs/2410.09982", "title": "Self-Data Distillation for Recovering Quality in Pruned Large Language Models", "authors": ["Vithursan Thangarasa", "Ganesh Venkatesh", "Mike Lasby", "Nish Sinnadurai", "Sean Lie"], "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to MLSys 2025. Main paper: 14 pp., 4 figs., 6 tabs.;\n  Supplementary: 5 pp", "summary": "Large language models have driven significant progress in natural language\nprocessing, but their deployment requires substantial compute and memory\nresources. As models scale, compression techniques become essential for\nbalancing model quality with computational efficiency. Structured pruning,\nwhich removes less critical components of the model, is a promising strategy\nfor reducing complexity. However, one-shot pruning often results in significant\nquality degradation, particularly in tasks requiring multi-step reasoning. To\nrecover lost quality, supervised fine-tuning (SFT) is commonly applied, but it\ncan lead to catastrophic forgetting by shifting the model's learned data\ndistribution. Therefore, addressing the degradation from both pruning and SFT\nis essential to preserve the original model's quality. In this work, we utilize\nself-data distilled fine-tuning to address these challenges. Our approach\nleverages the original, unpruned model to generate a distilled dataset that\npreserves semantic richness and mitigates catastrophic forgetting by\nmaintaining alignment with the base model's knowledge. Empirically, we\ndemonstrate that self-data distillation consistently outperforms standard SFT,\nimproving average accuracy by up to 8% on the HuggingFace OpenLLM Leaderboard\nv1. Specifically, when pruning six decoder blocks on Llama3.1-8B Instruct\n(i.e., 32 to 26 layers, reducing the model size from 8.03B to 6.72B\nparameters), our method retains 91.2% of the original model's accuracy compared\nto 81.7% with SFT, while reducing real-world FLOPs by 16.3%. Furthermore,\ncombining self-data distilled models through model merging yields enhanced\nquality retention. Additionally, leveraging these pruned models in speculative\ndecoding increases token acceptance rates, thereby improving inference\nefficiency in applied settings."}
{"id": "2410.13439", "pdf": "https://arxiv.org/pdf/2410.13439.pdf", "abs": "https://arxiv.org/abs/2410.13439", "title": "Similarity-Dissimilarity Loss for Multi-label Supervised Contrastive Learning", "authors": ["Guangming Huang", "Yunfei Long", "Cunjin Luo"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Supervised contrastive learning has achieved remarkable success by leveraging\nlabel information; however, determining positive samples in multi-label\nscenarios remains a critical challenge. In multi-label supervised contrastive\nlearning (MSCL), multi-label relations are not yet fully defined, leading to\nambiguity in identifying positive samples and formulating contrastive loss\nfunctions to construct the representation space. To address these challenges,\nwe: (i) first define five distinct multi-label relations in MSCL to\nsystematically identify positive samples, (ii) introduce a novel\nSimilarity-Dissimilarity Loss that dynamically re-weights samples through\ncomputing the similarity and dissimilarity factors between positive samples and\ngiven anchors based on multi-label relations, and (iii) further provide\ntheoretical grounded proofs for our method through rigorous mathematical\nanalysis that supports the formulation and effectiveness of the proposed loss\nfunction. We conduct the experiments across both image and text modalities, and\nextend the evaluation to medical domain. The results demonstrate that our\nmethod consistently outperforms baselines in a comprehensive evaluation,\nconfirming its effectiveness and robustness. Code is available at:\nhttps://github.com/guangminghuang/similarity-dissimilarity-loss."}
{"id": "2501.09798", "pdf": "https://arxiv.org/pdf/2501.09798.pdf", "abs": "https://arxiv.org/abs/2501.09798", "title": "Fun-tuning: Characterizing the Vulnerability of Proprietary LLMs to Optimization-based Prompt Injection Attacks via the Fine-Tuning Interface", "authors": ["Andrey Labunets", "Nishit V. Pandya", "Ashish Hooda", "Xiaohan Fu", "Earlence Fernandes"], "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "We surface a new threat to closed-weight Large Language Models (LLMs) that\nenables an attacker to compute optimization-based prompt injections.\nSpecifically, we characterize how an attacker can leverage the loss-like\ninformation returned from the remote fine-tuning interface to guide the search\nfor adversarial prompts. The fine-tuning interface is hosted by an LLM vendor\nand allows developers to fine-tune LLMs for their tasks, thus providing\nutility, but also exposes enough information for an attacker to compute\nadversarial prompts. Through an experimental analysis, we characterize the\nloss-like values returned by the Gemini fine-tuning API and demonstrate that\nthey provide a useful signal for discrete optimization of adversarial prompts\nusing a greedy search algorithm. Using the PurpleLlama prompt injection\nbenchmark, we demonstrate attack success rates between 65% and 82% on Google's\nGemini family of LLMs. These attacks exploit the classic utility-security\ntradeoff - the fine-tuning interface provides a useful feature for developers\nbut also exposes the LLMs to powerful attacks."}
{"id": "2502.01891", "pdf": "https://arxiv.org/pdf/2502.01891.pdf", "abs": "https://arxiv.org/abs/2502.01891", "title": "Training and Evaluating with Human Label Variation: An Empirical Study", "authors": ["Kemal Kurniawan", "Meladel Mistica", "Timothy Baldwin", "Jey Han Lau"], "categories": ["cs.LG", "cs.CL"], "comment": "25 pages, 7 figures. Fixed PO-JSD values on the MFRC dataset", "summary": "Human label variation (HLV) challenges the standard assumption that a\nlabelled instance has a single ground truth, instead embracing the natural\nvariation in human annotation to train and evaluate models. While various\ntraining methods and metrics for HLV have been proposed, it is still unclear\nwhich methods and metrics perform best in what settings. We propose new\nevaluation metrics for HLV leveraging fuzzy set theory. Since these new\nproposed metrics are differentiable, we then in turn experiment with employing\nthese metrics as training objectives. We conduct an extensive study over 6 HLV\ndatasets testing 14 training methods and 6 evaluation metrics. We find that\ntraining on either disaggregated annotations or soft labels performs best\nacross metrics, outperforming training using the proposed training objectives\nwith differentiable metrics. We also show that our proposed soft metric is more\ninterpretable and correlates best with human preference."}
{"id": "2502.12275", "pdf": "https://arxiv.org/pdf/2502.12275.pdf", "abs": "https://arxiv.org/abs/2502.12275", "title": "Integrating Expert Knowledge into Logical Programs via LLMs", "authors": ["Franciszek Górski", "Oskar Wysocki", "Marco Valentino", "Andre Freitas"], "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "This paper introduces ExKLoP, a novel framework designed to evaluate how\neffectively Large Language Models (LLMs) integrate expert knowledge into\nlogical reasoning systems. This capability is especially valuable in\nengineering, where expert knowledge-such as manufacturer-recommended\noperational ranges-can be directly embedded into automated monitoring systems.\nBy mirroring expert verification steps, tasks like range checking and\nconstraint validation help ensure system safety and reliability. Our approach\nsystematically evaluates LLM-generated logical rules, assessing both syntactic\nfluency and logical correctness in these critical validation tasks. We also\nexplore the models' capacity for self-correction via an iterative feedback loop\nbased on code execution outcomes. ExKLoP presents an extensible dataset\ncomprising 130 engineering premises, 950 prompts, and corresponding validation\npoints. It enables comprehensive benchmarking while allowing control over task\ncomplexity and scalability of experiments. We leverage the synthetic data\ncreation methodology to conduct extensive empirical evaluation on a diverse set\nof LLMs including Llama3, Gemma3, Codestral and QwenCoder. The results reveal\nthat most models generate nearly perfect syntactically correct code and exhibit\nstrong performance in translating expert knowledge into correct code. At the\nsame time, while most LLMs produce nearly flawless syntactic output, their\nability to correctly implement logical rules varies, as does their capacity for\nself-improvement. Overall, ExKLoP serves as a robust evaluation platform that\nstreamlines the selection of effective models for self-correcting systems while\nclearly delineating the types of errors encountered."}
{"id": "2502.14581", "pdf": "https://arxiv.org/pdf/2502.14581.pdf", "abs": "https://arxiv.org/abs/2502.14581", "title": "A Statistical Case Against Empirical Human-AI Alignment", "authors": ["Julian Rodemann", "Esteban Garces Arias", "Christoph Luther", "Christoph Jansen", "Thomas Augustin"], "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.OT"], "comment": "24 pages, 2 figures, 5 tables", "summary": "Empirical human-AI alignment aims to make AI systems act in line with\nobserved human behavior. While noble in its goals, we argue that empirical\nalignment can inadvertently introduce statistical biases that warrant caution.\nThis position paper thus advocates against naive empirical alignment, offering\nprescriptive alignment and a posteriori empirical alignment as alternatives. We\nsubstantiate our principled argument by tangible examples like human-centric\ndecoding of language models."}
{"id": "2502.14908", "pdf": "https://arxiv.org/pdf/2502.14908.pdf", "abs": "https://arxiv.org/abs/2502.14908", "title": "SegSub: Evaluating Robustness to Knowledge Conflicts and Hallucinations in Vision-Language Models", "authors": ["Peter Carragher", "Nikitha Rao", "Abhinand Jha", "R Raghav", "Kathleen M. Carley"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Vision language models (VLM) demonstrate sophisticated multimodal reasoning\nyet are prone to hallucination when confronted with knowledge conflicts,\nimpeding their deployment in information-sensitive contexts. While existing\nresearch addresses robustness in unimodal models, the multimodal domain lacks\nsystematic investigation of cross-modal knowledge conflicts. This research\nintroduces \\segsub, a framework for applying targeted image perturbations to\ninvestigate VLM resilience against knowledge conflicts. Our analysis reveals\ndistinct vulnerability patterns: while VLMs are robust to parametric conflicts\n(20% adherence rates), they exhibit significant weaknesses in identifying\ncounterfactual conditions (<30% accuracy) and resolving source conflicts (<1%\naccuracy). Correlations between contextual richness and hallucination rate (r =\n-0.368, p = 0.003) reveal the kinds of images that are likely to cause\nhallucinations. Through targeted fine-tuning on our benchmark dataset, we\ndemonstrate improvements in VLM knowledge conflict detection, establishing a\nfoundation for developing hallucination-resilient multimodal systems in\ninformation-sensitive environments."}
{"id": "2503.08980", "pdf": "https://arxiv.org/pdf/2503.08980.pdf", "abs": "https://arxiv.org/abs/2503.08980", "title": "I Predict Therefore I Am: Is Next Token Prediction Enough to Learn Human-Interpretable Concepts from Data?", "authors": ["Yuhang Liu", "Dong Gong", "Yichao Cai", "Erdun Gao", "Zhen Zhang", "Biwei Huang", "Mingming Gong", "Anton van den Hengel", "Javen Qinfeng Shi"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The remarkable achievements of large language models (LLMs) have led many to\nconclude that they exhibit a form of intelligence. This is as opposed to\nexplanations of their capabilities based on their ability to perform relatively\nsimple manipulations of vast volumes of data. To illuminate the distinction\nbetween these explanations, we introduce a novel generative model that\ngenerates tokens on the basis of human-interpretable concepts represented as\nlatent discrete variables. Under mild conditions, even when the mapping from\nthe latent space to the observed space is non-invertible, we establish an\nidentifiability result, i.e., the representations learned by LLMs through\nnext-token prediction can be approximately modeled as the logarithm of the\nposterior probabilities of these latent discrete concepts given input context,\nup to an invertible linear transformation. This theoretical finding not only\nprovides evidence that LLMs capture underlying generative factors, but also\nprovide a unified prospective for understanding of the linear representation\nhypothesis. Taking this a step further, our finding motivates a reliable\nevaluation of sparse autoencoders by treating the performance of supervised\nconcept extractors as an upper bound. Pushing this idea even further, it\ninspires a structural variant that enforces dependence among latent concepts in\naddition to promoting sparsity. Empirically, we validate our theoretical\nresults through evaluations on both simulation data and the Pythia, Llama, and\nDeepSeek model families, and demonstrate the effectiveness of our structured\nsparse autoencoder."}
{"id": "2504.01382", "pdf": "https://arxiv.org/pdf/2504.01382.pdf", "abs": "https://arxiv.org/abs/2504.01382", "title": "An Illusion of Progress? Assessing the Current State of Web Agents", "authors": ["Tianci Xue", "Weijian Qi", "Tianneng Shi", "Chan Hee Song", "Boyu Gou", "Dawn Song", "Huan Sun", "Yu Su"], "categories": ["cs.AI", "cs.CL"], "comment": "22 pages, 17 figures, 7 tables", "summary": "As digitalization and cloud technologies evolve, the web is becoming\nincreasingly important in the modern society. Autonomous web agents based on\nlarge language models (LLMs) hold a great potential in work automation. It is\ntherefore important to accurately measure and monitor the progression of their\ncapabilities. In this work, we conduct a comprehensive and rigorous assessment\nof the current state of web agents. Our results depict a very different picture\nof the competency of current agents, suggesting over-optimism in previously\nreported results. This gap can be attributed to shortcomings in existing\nbenchmarks. We introduce Online-Mind2Web, an online evaluation benchmark\nconsisting of 300 diverse and realistic tasks spanning 136 websites. It enables\nus to evaluate web agents under a setting that approximates how real users use\nthese agents. To facilitate more scalable evaluation and development, we also\ndevelop a novel LLM-as-a-Judge automatic evaluation method and show that it can\nachieve around 85% agreement with human judgment, substantially higher than\nexisting methods. Finally, we present the first comprehensive comparative\nanalysis of current web agents, highlighting both their strengths and\nlimitations to inspire future research."}
{"id": "2504.07840", "pdf": "https://arxiv.org/pdf/2504.07840.pdf", "abs": "https://arxiv.org/abs/2504.07840", "title": "Understanding Learner-LLM Chatbot Interactions and the Impact of Prompting Guidelines", "authors": ["Cansu Koyuturk", "Emily Theophilou", "Sabrina Patania", "Gregor Donabauer", "Andrea Martinenghi", "Chiara Antico", "Alessia Telari", "Alessia Testa", "Sathya Bursic", "Franca Garzotto", "Davinia Hernandez-Leo", "Udo Kruschwitz", "Davide Taibi", "Simona Amenta", "Martin Ruskov", "Dimitri Ognibene"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "Accepted for AIED 2025, the 26th International Conference on\n  Artificial Intelligence in Education, July 22 - 26, 2025, Palermo, Italy", "summary": "Large Language Models (LLMs) have transformed human-computer interaction by\nenabling natural language-based communication with AI-powered chatbots. These\nmodels are designed to be intuitive and user-friendly, allowing users to\narticulate requests with minimal effort. However, despite their accessibility,\nstudies reveal that users often struggle with effective prompting, resulting in\ninefficient responses. Existing research has highlighted both the limitations\nof LLMs in interpreting vague or poorly structured prompts and the difficulties\nusers face in crafting precise queries. This study investigates learner-AI\ninteractions through an educational experiment in which participants receive\nstructured guidance on effective prompting. We introduce and compare three\ntypes of prompting guidelines: a task-specific framework developed through a\nstructured methodology and two baseline approaches. To assess user behavior and\nprompting efficacy, we analyze a dataset of 642 interactions from 107 users.\nUsing Von NeuMidas, an extended pragmatic annotation schema for LLM interaction\nanalysis, we categorize common prompting errors and identify recurring\nbehavioral patterns. We then evaluate the impact of different guidelines by\nexamining changes in user behavior, adherence to prompting strategies, and the\noverall quality of AI-generated responses. Our findings provide a deeper\nunderstanding of how users engage with LLMs and the role of structured\nprompting guidance in enhancing AI-assisted communication. By comparing\ndifferent instructional frameworks, we offer insights into more effective\napproaches for improving user competency in AI interactions, with implications\nfor AI literacy, chatbot usability, and the design of more responsive AI\nsystems."}
{"id": "2504.18988", "pdf": "https://arxiv.org/pdf/2504.18988.pdf", "abs": "https://arxiv.org/abs/2504.18988", "title": "LINC: Supporting Language Independent Communication and Comprehension to Enhance Contribution in Multilingual Collaborative Meetings", "authors": ["Saramsh Gautam", "Mahmood Jasim"], "categories": ["cs.HC", "cs.CL", "H.5.3"], "comment": "The manuscript has been withdrawn by the authors due to ongoing\n  revisions and substantial updates", "summary": "Collaborative research often includes contributors with varied perspectives\nfrom diverse linguistic backgrounds. However, English as a Second Language\n(ESL) researchers often struggle to communicate during meetings in English and\ncomprehend discussions, leading to limited contribution. To investigate these\nchallenges, we surveyed 64 ESL researchers who frequently collaborate in\nmultilingual teams and identified four key design goals around participation,\ncomprehension, documentation, and feedback. Guided by these design goals, we\ndeveloped LINC, a multimodal Language INdependent Collaboration system with two\ncomponents: a real-time module for multilingual communication during meetings\nand a post-meeting dashboard for discussion analysis. We evaluated the system\nthrough a two-phased study with six triads of multilingual teams. We found that\nusing LINC, participants benefited from communicating in their preferred\nlanguage, recalled and reviewed actionable insights, and prepared for upcoming\nmeetings effectively. We discuss external factors that impact multilingual\nmeeting participation beyond language preferences and the implications of\nmultimodal systems in facilitating meetings in hybrid multilingual\ncollaborative settings beyond research."}
{"id": "2504.20879", "pdf": "https://arxiv.org/pdf/2504.20879.pdf", "abs": "https://arxiv.org/abs/2504.20879", "title": "The Leaderboard Illusion", "authors": ["Shivalika Singh", "Yiyang Nan", "Alex Wang", "Daniel D'Souza", "Sayash Kapoor", "Ahmet Üstün", "Sanmi Koyejo", "Yuntian Deng", "Shayne Longpre", "Noah A. Smith", "Beyza Ermis", "Marzieh Fadaee", "Sara Hooker"], "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ME"], "comment": "68 pages, 18 figures, 9 tables", "summary": "Measuring progress is fundamental to the advancement of any scientific field.\nAs benchmarks play an increasingly central role, they also grow more\nsusceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard\nfor ranking the most capable AI systems. Yet, in this work we identify\nsystematic issues that have resulted in a distorted playing field. We find that\nundisclosed private testing practices benefit a handful of providers who are\nable to test multiple variants before public release and retract scores if\ndesired. We establish that the ability of these providers to choose the best\nscore leads to biased Arena scores due to selective disclosure of performance\nresults. At an extreme, we identify 27 private LLM variants tested by Meta in\nthe lead-up to the Llama-4 release. We also establish that proprietary closed\nmodels are sampled at higher rates (number of battles) and have fewer models\nremoved from the arena than open-weight and open-source alternatives. Both\nthese policies lead to large data access asymmetries over time. Providers like\nGoogle and OpenAI have received an estimated 19.2% and 20.4% of all data on the\narena, respectively. In contrast, a combined 83 open-weight models have only\nreceived an estimated 29.7% of the total data. We show that access to Chatbot\nArena data yields substantial benefits; even limited additional data can result\nin relative performance gains of up to 112% on the arena distribution, based on\nour conservative estimates. Together, these dynamics result in overfitting to\nArena-specific dynamics rather than general model quality. The Arena builds on\nthe substantial efforts of both the organizers and an open community that\nmaintains this valuable evaluation platform. We offer actionable\nrecommendations to reform the Chatbot Arena's evaluation framework and promote\nfairer, more transparent benchmarking for the field"}
{"id": "2505.00831", "pdf": "https://arxiv.org/pdf/2505.00831.pdf", "abs": "https://arxiv.org/abs/2505.00831", "title": "SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation", "authors": ["Quang P. M. Pham", "Khoi T. N. Nguyen", "Nhi H. Doan", "Cuong A. Pham", "Kentaro Inui", "Dezhen Song"], "categories": ["cs.RO", "cs.CL"], "comment": "Paper is under review", "summary": "Efficient path planning in robotics, particularly within large-scale, dynamic\nenvironments, remains a significant hurdle. While Large Language Models (LLMs)\noffer strong reasoning capabilities, their high computational cost and limited\nadaptability in dynamic scenarios hinder real-time deployment on edge devices.\nWe present SmallPlan -- a novel framework leveraging LLMs as teacher models to\ntrain lightweight Small Language Models (SLMs) for high-level path planning\ntasks. In SmallPlan, the SLMs provide optimal action sequences to navigate\nacross scene graphs that compactly represent full-scaled 3D scenes. The SLMs\nare trained in a simulation-powered, interleaved manner with LLM-guided\nsupervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not\nonly enables SLMs to successfully complete navigation tasks but also makes them\naware of important factors like travel distance and number of trials. Through\nexperiments, we demonstrate that the fine-tuned SLMs perform competitively with\nlarger models like GPT-4o on sequential path planning, without suffering from\nhallucination and overfitting. SmallPlan is resource-efficient, making it\nwell-suited for edge-device deployment and advancing practical autonomous\nrobotics. Our source code is available here:\nhttps://github.com/quangpham2006/SmallPlan"}
{"id": "2505.05190", "pdf": "https://arxiv.org/pdf/2505.05190.pdf", "abs": "https://arxiv.org/abs/2505.05190", "title": "Revealing Weaknesses in Text Watermarking Through Self-Information Rewrite Attacks", "authors": ["Yixin Cheng", "Hongcheng Guo", "Yangming Li", "Leonid Sigal"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": "ICML 2025 Accpeted", "summary": "Text watermarking aims to subtly embed statistical signals into text by\ncontrolling the Large Language Model (LLM)'s sampling process, enabling\nwatermark detectors to verify that the output was generated by the specified\nmodel. The robustness of these watermarking algorithms has become a key factor\nin evaluating their effectiveness. Current text watermarking algorithms embed\nwatermarks in high-entropy tokens to ensure text quality. In this paper, we\nreveal that this seemingly benign design can be exploited by attackers, posing\na significant risk to the robustness of the watermark. We introduce a generic\nefficient paraphrasing attack, the Self-Information Rewrite Attack (SIRA),\nwhich leverages the vulnerability by calculating the self-information of each\ntoken to identify potential pattern tokens and perform targeted attack. Our\nwork exposes a widely prevalent vulnerability in current watermarking\nalgorithms. The experimental results show SIRA achieves nearly 100% attack\nsuccess rates on seven recent watermarking methods with only 0.88 USD per\nmillion tokens cost. Our approach does not require any access to the watermark\nalgorithms or the watermarked LLM and can seamlessly transfer to any LLM as the\nattack model, even mobile-level models. Our findings highlight the urgent need\nfor more robust watermarking."}
