{"id": "2507.01081", "pdf": "https://arxiv.org/pdf/2507.01081.pdf", "abs": "https://arxiv.org/abs/2507.01081", "title": "AI-guided digital intervention with physiological monitoring reduces intrusive memories after experimental trauma", "authors": ["Megan T. deBettencourt", "Sruthi Sakthivel", "Emily A. Holmes", "Mark Chevillet"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Trauma prevalence is vast globally. Evidence-based digital treatments can\nhelp, but most require human guidance. Human guides provide tailored\ninstructions and responsiveness to internal cognitive states, but limit\nscalability. Can generative AI and neurotechnology provide a scalable\nalternative? Here we test ANTIDOTE, combining AI guidance and pupillometry to\nautomatically deliver and monitor an evidence-based digital treatment,\nspecifically the Imagery Competing Task Intervention (ICTI), to reduce\nintrusive memories after psychological trauma. One hundred healthy volunteers\nwere exposed to videos of traumatic events and randomly assigned to an\nintervention or active control condition. As predicted, intervention\nparticipants reported significantly fewer intrusive memories over the following\nweek. Post-hoc assessment against clinical rubrics confirmed the AI guide\ndelivered the intervention successfully. Additionally, pupil size tracked\nintervention engagement and predicted symptom reduction, providing a candidate\nbiomarker of intervention effectiveness. These findings open a path toward\nrigorous AI-guided digital interventions that can scale to trauma prevalence."}
{"id": "2507.01121", "pdf": "https://arxiv.org/pdf/2507.01121.pdf", "abs": "https://arxiv.org/abs/2507.01121", "title": "From Literature to ReWA: Discussing Reproductive Well-being in HCI", "authors": ["Hafsah Mahzabin Chowdhury", "Sharifa Sultana"], "categories": ["cs.HC", "cs.CY"], "comment": "23 pages", "summary": "Reproductive well-being is shaped by intersecting cultural, religious,\ngendered, and political contexts, yet current technologies often reflect\nnarrow, Western-centric assumptions. In this literature review, we synthesize\nfindings from 147 peer-reviewed papers published between 2015 and 2025 across\nHCI, CSCW and social computing, ICTD, digital and public health, and AI for\nwell-being scholarship to map the evolving reproductive well-being landscape.\nWe identify three thematic waves that focused on early access and education,\ncultural sensitivity and privacy, and AI integration with policy-aware design,\nand highlight how technologies support or constrain diverse reproductive\nexperiences. Our analysis reveals critical gaps in inclusivity, with persistent\nexclusions of men and non-binary users, migrants, and users in the Global\nSouth. Additionally, we surfaced the significant absence of literature on the\nrole of stakeholders (e.g., husband and family members, household maids and\ncleaning helping hands, midwife, etc.) in the reproductive well-being space.\nDrawing on the findings from the literature, we propose the ReWA framework to\nsupport reproductive well-being for all agendas through six design orientations\nassociated with: location, culture, and history; polyvocality and agency;\nrationality, temporality, distributive roles, and methodology."}
{"id": "2507.01134", "pdf": "https://arxiv.org/pdf/2507.01134.pdf", "abs": "https://arxiv.org/abs/2507.01134", "title": "Animated Visual Encoding and Layer Blending for Identification of Educational Game Strategies", "authors": ["Braden Roper", "William Thompson", "Chris Weaver"], "categories": ["cs.HC"], "comment": "To be published in IEEE Visualization and Visual Analytics (VIS),\n  2025", "summary": "Game-Based Learning has proven to be an effective method for enhancing\nengagement with educational material. However, gaining a deeper understanding\nof player strategies remains challenging. Sequential game-state and\naction-based tracking tools often gather extensive data that can be difficult\nto interpret as long-term strategy. This data presents unique problems to\nvisualization, as it can be fairly natural, noisy data but is constrained\nwithin synthetic, controlled environments, leading to issues such as\noverplotting which can make interpretation complicated. We propose an animated\nvisual encoding tool that utilizes kinetic visualization to address these\nissues. This tool enables researchers to construct animated data narratives\nthrough the configuration of parameter interpolation curves and blending\nlayers. Finally, we demonstrate the usefulness of the tool while addressing\nspecific interests as outlined by a domain expert collaborator."}
{"id": "2507.01166", "pdf": "https://arxiv.org/pdf/2507.01166.pdf", "abs": "https://arxiv.org/abs/2507.01166", "title": "A Methodological Framework for Capturing Cognitive-Affective States in Collaborative Learning", "authors": ["Sifatul Anindho", "Videep Venkatesha", "Nathaniel Blanchard"], "categories": ["cs.HC"], "comment": "Accepted to the Interactive Workshop: Multimodal, Multiparty Learning\n  Analytics (MMLA) at the conference Educational Data Mining (EDM) 2025", "summary": "Identification of affective and attentional states of individuals within\ngroups is difficult to obtain without disrupting the natural flow of\ncollaboration. Recent work from our group used a retrospect cued recall\nparadigm where participants spoke about their cognitive-affective states while\nthey viewed videos of their groups. We then collected additional participants\nwhere their reports were constrained to a subset of pre-identified\ncognitive-affective states. In this latter case, participants either self\nreported or reported in response to probes. Here, we present an initial\nanalysis of the frequency and temporal distribution of participant reports, and\nhow the distributions of labels changed across the two collections. Our\napproach has implications for the educational data mining community in tracking\ncognitive-affective states in collaborative learning more effectively and in\ndeveloping improved adaptive learning systems that can detect and respond to\ncognitive-affective states."}
{"id": "2507.01019", "pdf": "https://arxiv.org/pdf/2507.01019.pdf", "abs": "https://arxiv.org/abs/2507.01019", "title": "MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered", "authors": ["Imran Mirza", "Cole Huang", "Ishwara Vasista", "Rohan Patil", "Asli Akalin", "Sean O'Brien", "Kevin Zhu"], "categories": ["cs.CL", "cs.CY"], "comment": "Accepted to Building Trust in LLMs @ ICLR 2025 and NAACL SRW 2025", "summary": "Multi-agent systems, which consist of multiple AI models interacting within a\nshared environment, are increasingly used for persona-based interactions.\nHowever, if not carefully designed, these systems can reinforce implicit biases\nin large language models (LLMs), raising concerns about fairness and equitable\nrepresentation. We present MALIBU, a novel benchmark developed to assess the\ndegree to which LLM-based multi-agent systems implicitly reinforce social\nbiases and stereotypes. MALIBU evaluates bias in LLM-based multi-agent systems\nthrough scenario-based assessments. AI models complete tasks within predefined\ncontexts, and their responses undergo evaluation by an LLM-based multi-agent\njudging system in two phases. In the first phase, judges score responses\nlabeled with specific demographic personas (e.g., gender, race, religion)\nacross four metrics. In the second phase, judges compare paired responses\nassigned to different personas, scoring them and selecting the superior\nresponse. Our study quantifies biases in LLM-generated outputs, revealing that\nbias mitigation may favor marginalized personas over true neutrality,\nemphasizing the need for nuanced detection, balanced fairness strategies, and\ntransparent evaluation benchmarks in multi-agent systems."}
{"id": "2507.01209", "pdf": "https://arxiv.org/pdf/2507.01209.pdf", "abs": "https://arxiv.org/abs/2507.01209", "title": "Judgment as Coordination: A Joint Systems View of Visualization Design Practice", "authors": ["Paul C. Parsons", "Arran Ridley"], "categories": ["cs.HC"], "comment": "IEEE VIS 2025 (conditional acceptance)", "summary": "Professional visualization design has become an increasingly important area\nof inquiry, yet much of the field's discourse remains anchored in\nresearcher-centered contexts. Studies of design practice often focus on\nindividual designers' decisions and reflections, offering limited insight into\nthe collaborative and systemic dimensions of professional work. In this paper,\nwe propose a systems-level reframing of design judgment grounded in the\ncoordination and adaptation that sustain progress amid uncertainty, constraint,\nand misalignment. Drawing on sustained engagement across multiple empirical\nstudies--including ethnographic observation of design teams and qualitative\nstudies of individual practitioners--we identify recurring episodes in which\ncoherence was preserved not by selecting an optimal option, but by repairing\nalignment, adjusting plans, and reframing goals. We interpret these dynamics\nthrough the lens of Joint Cognitive Systems, which provide tools for analyzing\nhow judgment emerges as a distributed capacity within sociotechnical activity.\nThis perspective surfaces often-invisible work in visualization design and\noffers researchers a new conceptual vocabulary for studying how design activity\nis sustained in practice."}
{"id": "2507.01160", "pdf": "https://arxiv.org/pdf/2507.01160.pdf", "abs": "https://arxiv.org/abs/2507.01160", "title": "Event-based evaluation of abstractive news summarization", "authors": ["Huiling You", "Samia Touileb", "Erik Velldal", "Lilja Øvrelid"], "categories": ["cs.CL"], "comment": "to appear at GEM2 workshop@ACL 2025", "summary": "An abstractive summary of a news article contains its most important\ninformation in a condensed version. The evaluation of automatically generated\nsummaries by generative language models relies heavily on human-authored\nsummaries as gold references, by calculating overlapping units or similarity\nscores. News articles report events, and ideally so should the summaries. In\nthis work, we propose to evaluate the quality of abstractive summaries by\ncalculating overlapping events between generated summaries, reference\nsummaries, and the original news articles. We experiment on a richly annotated\nNorwegian dataset comprising both events annotations and summaries authored by\nexpert human annotators. Our approach provides more insight into the event\ninformation contained in the summaries."}
{"id": "2507.01274", "pdf": "https://arxiv.org/pdf/2507.01274.pdf", "abs": "https://arxiv.org/abs/2507.01274", "title": "AI Meets Maritime Training: Precision Analytics for Enhanced Safety and Performance", "authors": ["Vishakha Lall", "Yisi Liu"], "categories": ["cs.HC", "cs.AI"], "comment": "Accepted and Presented at 11th International Maritime Science\n  Conference", "summary": "Traditional simulator-based training for maritime professionals is critical\nfor ensuring safety at sea but often depends on subjective trainer assessments\nof technical skills, behavioral focus, communication, and body language, posing\nchallenges such as subjectivity, difficulty in measuring key features, and\ncognitive limitations. Addressing these issues, this study develops an\nAI-driven framework to enhance maritime training by objectively assessing\ntrainee performance through visual focus tracking, speech recognition, and\nstress detection, improving readiness for high-risk scenarios. The system\nintegrates AI techniques, including visual focus determination using eye\ntracking, pupil dilation analysis, and computer vision; communication analysis\nthrough a maritime-specific speech-to-text model and natural language\nprocessing; communication correctness using large language models; and mental\nstress detection via vocal pitch. Models were evaluated on data from simulated\nmaritime scenarios with seafarers exposed to controlled high-stress events. The\nAI algorithms achieved high accuracy, with ~92% for visual detection, ~91% for\nmaritime speech recognition, and ~90% for stress detection, surpassing existing\nbenchmarks. The system provides insights into visual attention, adherence to\ncommunication checklists, and stress levels under demanding conditions. This\nstudy demonstrates how AI can transform maritime training by delivering\nobjective performance analytics, enabling personalized feedback, and improving\npreparedness for real-world operational challenges."}
{"id": "2507.01170", "pdf": "https://arxiv.org/pdf/2507.01170.pdf", "abs": "https://arxiv.org/abs/2507.01170", "title": "Matching and Linking Entries in Historical Swedish Encyclopedias", "authors": ["Simon Börjesson", "Erik Ersmark", "Pierre Nugues"], "categories": ["cs.CL"], "comment": "10 pages, 3 figures", "summary": "The \\textit{Nordisk familjebok} is a Swedish encyclopedia from the 19th and\n20th centuries. It was written by a team of experts and aimed to be an\nintellectual reference, stressing precision and accuracy. This encyclopedia had\nfour main editions remarkable by their size, ranging from 20 to 38 volumes. As\na consequence, the \\textit{Nordisk familjebok} had a considerable influence in\nuniversities, schools, the media, and society overall. As new editions were\nreleased, the selection of entries and their content evolved, reflecting\nintellectual changes in Sweden.\n  In this paper, we used digitized versions from \\textit{Project Runeberg}. We\nfirst resegmented the raw text into entries and matched pairs of entries\nbetween the first and second editions using semantic sentence embeddings. We\nthen extracted the geographical entries from both editions using a\ntransformer-based classifier and linked them to Wikidata. This enabled us to\nidentify geographic trends and possible shifts between the first and second\neditions, written between 1876-1899 and 1904-1926, respectively.\n  Interpreting the results, we observe a small but significant shift in\ngeographic focus away from Europe and towards North America, Africa, Asia,\nAustralia, and northern Scandinavia from the first to the second edition,\nconfirming the influence of the First World War and the rise of new powers. The\ncode and data are available on GitHub at\nhttps://github.com/sibbo/nordisk-familjebok."}
{"id": "2507.01436", "pdf": "https://arxiv.org/pdf/2507.01436.pdf", "abs": "https://arxiv.org/abs/2507.01436", "title": "Challenges & Opportunities with LLM-Assisted Visualization Retargeting", "authors": ["Luke S. Snyder", "Chenglong Wang", "Steven Drucker"], "categories": ["cs.HC"], "comment": "5 pages, 3 figures, 1 table", "summary": "Despite the ubiquity of visualization examples published on the web,\nretargeting existing custom chart implementations to new datasets remains\ndifficult, time-intensive, and tedious. The adaptation process assumes author\nfamiliarity with both the implementation of the example as well as how the new\ndataset might need to be transformed to fit into the example code. With recent\nadvances in Large Language Models (LLMs), automatic adaptation of code can be\nachieved from high-level user prompts, reducing the barrier for visualization\nretargeting. To better understand how LLMs can assist retargeting and its\npotential limitations, we characterize and evaluate the performance of LLM\nassistance across multiple datasets and charts of varying complexity,\ncategorizing failures according to type and severity. In our evaluation, we\ncompare two approaches: (1) directly instructing the LLM model to fully\ngenerate and adapt code by treating code as text inputs and (2) a more\nconstrained program synthesis pipeline where the LLM guides the code\nconstruction process by providing structural information (e.g., visual\nencodings) based on properties of the example code and data. We find that both\napproaches struggle when new data has not been appropriately transformed, and\ndiscuss important design recommendations for future retargeting systems."}
{"id": "2507.01213", "pdf": "https://arxiv.org/pdf/2507.01213.pdf", "abs": "https://arxiv.org/abs/2507.01213", "title": "MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis", "authors": ["Adamu Lawan", "Juhua Pu", "Haruna Yunusa", "Jawad Muhammad", "Muhammad Lawan"], "categories": ["cs.CL"], "comment": "6, 1 figure", "summary": "Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language\nProcessing (NLP) task that extracts aspects from text and determines their\nassociated sentiments, enabling fine-grained analysis of user opinions.\nExisting ABSA methods struggle to balance computational efficiency with high\nperformance: deep learning models often lack global context, transformers\ndemand significant computational resources, and Mamba-based approaches face\nCUDA dependency and diminished local correlations. Recent advancements in\nExtended Long Short-Term Memory (xLSTM) models, particularly their efficient\nmodeling of long-range dependencies, have significantly advanced the NLP\ncommunity. However, their potential in ABSA remains untapped. To this end, we\npropose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework\nintegrating a bi-directional mLSTM architecture with forward and partially\nflipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context\nmodeling by processing the initial sequence segment in reverse with dedicated\nparameters, preserving critical short-range patterns. We further introduce an\nmLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that\ndynamically combines forward mLSTM outputs as query and key with PF-mLSTM\noutputs as value, optimizing short-range dependency capture while maintaining\nglobal context and efficiency. Experimental results on three benchmark datasets\ndemonstrate that MEGA outperforms state-of-the-art baselines, achieving\nsuperior accuracy and efficiency in ABSA tasks."}
{"id": "2507.01471", "pdf": "https://arxiv.org/pdf/2507.01471.pdf", "abs": "https://arxiv.org/abs/2507.01471", "title": "Analysis of Drone-Assisted Building Inspection Training in VR vs 2D Monitor Display: an EEG Study", "authors": ["Pengkun Liu", "Jackson Greene", "Jiali Huang", "Pingbo Tang", "Yu Hou"], "categories": ["cs.HC"], "comment": null, "summary": "Researchers have been using simulation-based methods for drone-assisted\ninspection training. Multiple brain regions are associated with information\nprocesses and decision-making, and the connectivity of these regions may\nfurther influence inspectors' performance. However, researchers do not\nunderstand the pathways of the information flows when drone pilots process the\nmaintenance and manipulation of information, which may affect the efficiency of\ntacit knowledge transfer. This study aims to reveal the causal connection\nbetween participants' brain regions using an electroencephalogram and dynamic\ncausal modeling when processing drone-assisted building energy audit tasks\nusing different display modalities. The results showed similar single-direction\nconnectivity patterns for the different simulation groups. The results also\nshowed similar patterns between brain regions related to visual inspection\nperformance before and after training. These findings highlight the nature of\nbrain asymmetries and may be utilized in measuring cognitive states and\ndesigning adaptive automation in the knowledge transfer of drone-based\ninspection."}
{"id": "2507.01234", "pdf": "https://arxiv.org/pdf/2507.01234.pdf", "abs": "https://arxiv.org/abs/2507.01234", "title": "The Medium Is Not the Message: Deconfounding Text Embeddings via Linear Concept Erasure", "authors": ["Yu Fan", "Yang Tian", "Shauli Ravfogel", "Mrinmaya Sachan", "Elliott Ash", "Alexander Hoyle"], "categories": ["cs.CL"], "comment": null, "summary": "Embedding-based similarity metrics between text sequences can be influenced\nnot just by the content dimensions we most care about, but can also be biased\nby spurious attributes like the text's source or language. These document\nconfounders cause problems for many applications, but especially those that\nneed to pool texts from different corpora. This paper shows that a debiasing\nalgorithm that removes information about observed confounders from the encoder\nrepresentations substantially reduces these biases at a minimal computational\ncost. Document similarity and clustering metrics improve across every embedding\nvariant and task we evaluate -- often dramatically. Interestingly, performance\non out-of-distribution benchmarks is not impacted, indicating that the\nembeddings are not otherwise degraded."}
{"id": "2507.01548", "pdf": "https://arxiv.org/pdf/2507.01548.pdf", "abs": "https://arxiv.org/abs/2507.01548", "title": "Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants", "authors": ["Wen Zhan", "Ziqun Hua", "Peiyue Lin", "Yunfei Chen"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "A version of this manuscript has been submitted to the [IASDR 2025\n  Conference](https://iasdr2025.org/) and is currently under review", "summary": "This paper explores how older adults, particularly aging migrants in urban\nChina, can engage AI-assisted co-creation to express personal narratives that\nare often fragmented, underrepresented, or difficult to verbalize. Through a\npilot workshop combining oral storytelling and the symbolic reconstruction of\nHanzi, participants shared memories of migration and recreated new character\nforms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),\ntogether with physical materials. Supported by human facilitation and a soft AI\npresence, participants transformed lived experience into visual and tactile\nexpressions without requiring digital literacy. This approach offers new\nperspectives on human-AI collaboration and aging by repositioning AI not as a\ncontent producer but as a supportive mechanism, and by supporting narrative\nagency within sociotechnical systems."}
{"id": "2507.01259", "pdf": "https://arxiv.org/pdf/2507.01259.pdf", "abs": "https://arxiv.org/abs/2507.01259", "title": "GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant", "authors": ["Michał Matak", "Jarosław A. Chudziak"], "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 2 figures, presented at ICAART 2025, in proceedings of the\n  17th International Conference on Agents and Artificial Intelligence - Volume\n  3: ICAART", "summary": "In this paper we discuss the capability of large language models to base\ntheir answer and provide proper references when dealing with legal matters of\nnon-english and non-chinese speaking country. We discuss the history of legal\ninformation retrieval, the difference between case law and statute law, its\nimpact on the legal tasks and analyze the latest research in this field. Basing\non that background we introduce gAIus, the architecture of the cognitive\nLLM-based agent, whose responses are based on the knowledge retrieved from\ncertain legal act, which is Polish Civil Code. We propose a retrieval mechanism\nwhich is more explainable, human-friendly and achieves better results than\nembedding-based approaches. To evaluate our method we create special dataset\nbased on single-choice questions from entrance exams for law apprenticeships\nconducted in Poland. The proposed architecture critically leveraged the\nabilities of used large language models, improving the gpt-3.5-turbo-0125 by\n419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%.\nAt the end of our paper we show the possible future path of research and\npotential applications of our findings."}
{"id": "2507.01690", "pdf": "https://arxiv.org/pdf/2507.01690.pdf", "abs": "https://arxiv.org/abs/2507.01690", "title": "Designing for Community Care: Reimagining Support for Equity & Well-being in Academia", "authors": ["Beatriz Severes", "Ana O. Henriques", "Rory Clark", "Paulo Bala", "Anna Carter", "Rua Mae Williams", "Geraldine Fitzpatrick"], "categories": ["cs.HC"], "comment": null, "summary": "Academic well-being is deeply influenced by peer-support networks, yet they\nremain informal, inequitable, and unsustainable, often relying on personal\nconnections and social capital rather than structured, inclusive systems.\nAdditionally, institutional well-being responses frequently focus on student\npopulations, neglecting the emotional labour of faculty and staff, reinforcing\nan exclusionary academic culture. Drawing on HCI methodologies, participatory\ndesign, and care ethics, this workshop will provide a space for rethinking how\nacademic communities can support inclusive networks. Through pre-workshop\nengagement, co-design activities, and reflection, participants will examine\nsystemic gaps in networks and explore ways to embed care, equity, and\nsustainability into academic peer-support frameworks -- from informal,\nexclusionary models to structured, inclusive care-based ecosystems. At the end\nof the workshop, participants will co-develop design strategies for integrating\ncare and resilience in academic ecosystems, resources for designing equitable\nsupport systems, and a peer network invested and committed to fostering a\nsupportive academic community."}
{"id": "2507.01278", "pdf": "https://arxiv.org/pdf/2507.01278.pdf", "abs": "https://arxiv.org/abs/2507.01278", "title": "Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening", "authors": ["Cindy Lie Tabuse", "David Restepo", "Carolina Gracitelli", "Fernando Korn Malerbi", "Caio Regatieri", "Luis Filipe Nakayama"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) can simulate clinical reasoning based on natural\nlanguage prompts, but their utility in ophthalmology is largely unexplored.\nThis study evaluated GPT-4's ability to interpret structured textual\ndescriptions of retinal fundus photographs and simulate clinical decisions for\ndiabetic retinopathy (DR) and glaucoma screening, including the impact of\nadding real or synthetic clinical metadata. We conducted a retrospective\ndiagnostic validation study using 300 annotated fundus images. GPT-4 received\nstructured prompts describing each image, with or without patient metadata. The\nmodel was tasked with assigning an ICDR severity score, recommending DR\nreferral, and estimating the cup-to-disc ratio for glaucoma referral.\nPerformance was evaluated using accuracy, macro and weighted F1 scores, and\nCohen's kappa. McNemar's test and change rate analysis were used to assess the\ninfluence of metadata. GPT-4 showed moderate performance for ICDR\nclassification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25),\ndriven mainly by correct identification of normal cases. Performance improved\nin the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For\nglaucoma referral, performance was poor across all settings (accuracy ~78%, F1\n<0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes\n(McNemar p > 0.05), and predictions remained consistent across conditions.\nGPT-4 can simulate basic ophthalmic decision-making from structured prompts but\nlacks precision for complex tasks. While not suitable for clinical use, LLMs\nmay assist in education, documentation, or image annotation workflows in\nophthalmology."}
{"id": "2507.01719", "pdf": "https://arxiv.org/pdf/2507.01719.pdf", "abs": "https://arxiv.org/abs/2507.01719", "title": "Towards culturally-appropriate conversational AI for health in the majority world: An exploratory study with citizens and professionals in Latin America", "authors": ["Dorian Peters", "Fernanda Espinoza", "Marco da Re", "Guido Ivetta", "Luciana Benotti", "Rafael A. Calvo"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "There is justifiable interest in leveraging conversational AI (CAI) for\nhealth across the majority world, but to be effective, CAI must respond\nappropriately within culturally and linguistically diverse contexts. Therefore,\nwe need ways to address the fact that current LLMs exclude many lived\nexperiences globally. Various advances are underway which focus on top-down\napproaches and increasing training data. In this paper, we aim to complement\nthese with a bottom-up locally-grounded approach based on qualitative data\ncollected during participatory workshops in Latin America. Our goal is to\nconstruct a rich and human-centred understanding of: a) potential areas of\ncultural misalignment in digital health; b) regional perspectives on chatbots\nfor health and c)strategies for creating culturally-appropriate CAI; with a\nfocus on the understudied Latin American context. Our findings show that\nacademic boundaries on notions of culture lose meaning at the ground level and\ntechnologies will need to engage with a broader framework; one that\nencapsulates the way economics, politics, geography and local logistics are\nentangled in cultural experience. To this end, we introduce a framework for\n'Pluriversal Conversational AI for Health' which allows for the possibility\nthat more relationality and tolerance, rather than just more data, may be\ncalled for."}
{"id": "2507.01281", "pdf": "https://arxiv.org/pdf/2507.01281.pdf", "abs": "https://arxiv.org/abs/2507.01281", "title": "Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization", "authors": ["Juan Chen", "Baolong Bi", "Wei Zhang", "Jingyan Sui", "Xiaofei Zhu", "Yuanzhuo Wang", "Lingrui Mei", "Shenghua Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nintegrating their parametric knowledge with external retrieved content.\nHowever, knowledge conflicts caused by internal inconsistencies or noisy\nretrieved content can severely undermine the generation reliability of RAG\nsystems.In this work, we argue that LLMs should rethink all evidence, including\nboth retrieved content and internal knowledge, before generating responses.We\npropose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel\nframework that improves trustworthiness through Conflict-Driven Summarization\nof all available evidence.CARE-RAG first derives parameter-aware evidence by\ncomparing parameter records to identify diverse internal perspectives. It then\nrefines retrieved evidences to produce context-aware evidence, removing\nirrelevant or misleading content. To detect and summarize conflicts, we distill\na 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable\nsynthesis across multiple sources.To further ensure evaluation integrity, we\nintroduce a QA Repair step to correct outdated or ambiguous benchmark\nanswers.Experiments on revised QA datasets with retrieval data show that\nCARE-RAG consistently outperforms strong RAG baselines, especially in scenarios\nwith noisy or conflicting evidence."}
{"id": "2507.01776", "pdf": "https://arxiv.org/pdf/2507.01776.pdf", "abs": "https://arxiv.org/abs/2507.01776", "title": "Human-Machine Collaboration-Guided Space Design: Combination of Machine Learning Models and Humanistic Design Concepts", "authors": ["Yuxuan Yang"], "categories": ["cs.HC", "cs.MM"], "comment": null, "summary": "The integration of machine learning (ML) into spatial design holds immense\npotential for optimizing space utilization, enhancing functionality, and\nstreamlining design processes. ML can automate tasks, predict performance\noutcomes, and tailor spaces to user preferences. However, the emotional,\ncultural, and aesthetic dimensions of design remain crucial for creating spaces\nthat truly resonate with users-elements that ML alone cannot address. The key\nchallenge lies in harmonizing data-driven efficiency with the nuanced,\nsubjective aspects of design. This paper proposes a human-machine collaboration\nframework to bridge this gap. An effective framework should recognize that\nwhile ML enhances design efficiency through automation and prediction, it must\nbe paired with human creativity to ensure spaces are emotionally engaging and\nculturally relevant. Human designers contribute intuition, empathy, and\ncultural insight, guiding ML-generated solutions to align with users' emotional\nand cultural needs. Additionally, we explore how various ML models can be\nintegrated with human-centered design principles. These models can automate\ndesign generation and optimization, while human designers refine the outputs to\nensure emotional resonance and aesthetic appeal. Through case studies in office\nand residential design, we illustrate how this framework fosters both\ncreativity and cultural relevance. By merging ML with human creativity, spatial\ndesign can achieve a balance of efficiency and emotional impact, resulting in\nenvironments that are both functional and deeply human."}
{"id": "2507.01297", "pdf": "https://arxiv.org/pdf/2507.01297.pdf", "abs": "https://arxiv.org/abs/2507.01297", "title": "Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks", "authors": ["Xinxi Lyu", "Michael Duan", "Rulin Shao", "Pang Wei Koh", "Sewon Min"], "categories": ["cs.CL", "cs.IR"], "comment": "33 pages, 2 figures, 27 tables", "summary": "Retrieval-augmented Generation (RAG) has primarily been studied in limited\nsettings, such as factoid question answering; more challenging,\nreasoning-intensive benchmarks have seen limited success from minimal RAG. In\nthis work, we challenge this prevailing view on established,\nreasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We\nidentify a key missing component in prior work: a usable, web-scale datastore\naligned with the breadth of pretraining data. To this end, we introduce\nCompactDS: a diverse, high-quality, web-scale datastore that achieves high\nretrieval accuracy and subsecond latency on a single-node. The key insights are\n(1) most web content can be filtered out without sacrificing coverage, and a\ncompact, high-quality subset is sufficient; and (2) combining in-memory\napproximate nearest neighbor (ANN) retrieval and on-disk exact search balances\nspeed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves\nconsistent accuracy improvements across all benchmarks and model sizes\n(8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA,\nand 19% on MATH. No single data source suffices alone, highlighting the\nimportance of diversity of sources (web crawls, curated math, academic papers,\ntextbooks). Finally, we show that our carefully designed in-house datastore\nmatches or outperforms web search engines such as Google Search, as well as\nrecently proposed, complex agent-based RAG systems--all while maintaining\nsimplicity, reproducibility, and self-containment. We release CompactDS and our\nretrieval pipeline, supporting future research exploring retrieval-based AI\nsystems."}
{"id": "2507.01862", "pdf": "https://arxiv.org/pdf/2507.01862.pdf", "abs": "https://arxiv.org/abs/2507.01862", "title": "Bridging UI Design and chatbot Interactions: Applying Form-Based Principles to Conversational Agents", "authors": ["Sanjay Krishna Anbalagan", "Xinrui Nie", "Umesh Mohan", "Vijay Kumar Kanamarlapudi", "Anughna Kommalapati", "Xiaodan Zhao"], "categories": ["cs.HC", "cs.AI", "H.5.2; I.2.7"], "comment": "8 pages, 1 figure, pre-print of poster accepted for HCI International\n  2025 (HCII 2025), CCIS vol 2529", "summary": "Domain specific chatbot applications often involve multi step interactions,\nsuch as refining search filters, selecting multiple items, or performing\ncomparisons. Traditional graphical user interfaces (GUIs) handle these\nworkflows by providing explicit \"Submit\" (commit data) and \"Reset\" (discard\ndata) actions, allowing back-end systems to track user intent unambiguously. In\ncontrast, conversational agents rely on subtle language cues, which can lead to\nconfusion and incomplete context management. This paper proposes modeling these\nGUI inspired metaphors acknowledgment (submit like) and context switching\n(reset-like) as explicit tasks within large language model (LLM) prompts. By\ncapturing user acknowledgment, reset actions, and chain of thought (CoT)\nreasoning as structured session data, we preserve clarity, reduce user\nconfusion, and align domain-specific chatbot interactions with back-end logic.\nWe demonstrate our approach in hotel booking and customer management scenarios,\nhighlighting improvements in multi-turn task coherence, user satisfaction, and\nefficiency."}
{"id": "2507.01299", "pdf": "https://arxiv.org/pdf/2507.01299.pdf", "abs": "https://arxiv.org/abs/2507.01299", "title": "La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation", "authors": ["Kai Liu", "Bowen Xu", "Shaoyu Wu", "Xin Chen", "Hao Zhou", "Yongliang Tao", "Lulu Hu"], "categories": ["cs.CL"], "comment": "ICML 2025 Acceptance", "summary": "Activation sparsity can reduce the computational overhead and memory\ntransfers during the forward pass of Large Language Model (LLM) inference.\nExisting methods face limitations, either demanding time-consuming recovery\ntraining that hinders real-world adoption, or relying on empirical\nmagnitude-based pruning, which causes fluctuating sparsity and unstable\ninference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse\nActivation), a novel method for activation sparsification designed to improve\nLLM efficiency without requiring additional training or magnitude-based\npruning. We leverage layerwise orthogonal rotations to transform input\nactivations into rotated forms that are more suitable for sparsification. By\nemploying a Top-K selection approach within the rotated activations, we achieve\nconsistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA\nis effective across various sizes and types of LLMs, demonstrating minimal\nperformance degradation and robust inference acceleration. Specifically, for\nLLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a\nconsistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in\nzero-shot tasks compared to the dense model to just 0.54%, while surpassing\nTEAL by 1.77% and CATS by 17.14%."}
{"id": "2507.01944", "pdf": "https://arxiv.org/pdf/2507.01944.pdf", "abs": "https://arxiv.org/abs/2507.01944", "title": "Spatial tangible user interfaces for cognitive assessment and training", "authors": ["Ehud Sharlin", "Yuichi Itoh", "Benjamin Watson", "Yoshifumi Kitamura", "Steve Sutphen", "Lili Liu", "Fumio Kishino"], "categories": ["cs.HC"], "comment": null, "summary": "This paper discusses Tangible User Interfaces (TUIs) and their potential\nimpact on cognitive assessment and cognitive training. We believe that TUIs,\nand particularly a subset that we dub spatial TUIs, can extend human computer\ninteraction beyond some of its current limitations. Spatial TUIs exploit human\ninnate spatial and tactile ability in an intuitive and direct manner, affording\ninteraction paradigms that are practically impossible using current interface\ntechnology. As proof-of-concept we examine implementations in the field of\ncognitive assessment and training. In this paper we use Cognitive Cubes, a\nnovel TUI we developed, as an applied test bed for our beliefs, presenting\npromising experimental results for cognitive assessment of spatial ability, and\npossibly for training purposes."}
{"id": "2507.01334", "pdf": "https://arxiv.org/pdf/2507.01334.pdf", "abs": "https://arxiv.org/abs/2507.01334", "title": "Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs", "authors": ["Nifu Dan", "Yujun Cai", "Yiwei Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Navigating the complexities of physics reasoning has long been a difficult\ntask for Large Language Models (LLMs), requiring a synthesis of profound\nconceptual understanding and adept problem-solving techniques. In this study,\nwe investigate the application of advanced instruction-tuned reasoning models,\nsuch as Deepseek-R1, to address a diverse spectrum of physics problems curated\nfrom the challenging SciBench benchmark. Our comprehensive experimental\nevaluation reveals the remarkable capabilities of reasoning models. Not only do\nthey achieve state-of-the-art accuracy in answering intricate physics\nquestions, but they also generate distinctive reasoning patterns that emphasize\non symbolic derivation. Furthermore, our findings indicate that even for these\nhighly sophisticated reasoning models, the strategic incorporation of few-shot\nprompting can still yield measurable improvements in overall accuracy,\nhighlighting the potential for continued performance gains."}
{"id": "2507.01022", "pdf": "https://arxiv.org/pdf/2507.01022.pdf", "abs": "https://arxiv.org/abs/2507.01022", "title": "Workflow-Based Evaluation of Music Generation Systems", "authors": ["Shayan Dadman", "Bernt Arild Bremdal", "Andreas Bergsland"], "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.MM", "cs.SD"], "comment": "54 pages, 3 figures, 6 tables, 5 appendices", "summary": "This study presents an exploratory evaluation of Music Generation Systems\n(MGS) within contemporary music production workflows by examining eight\nopen-source systems. The evaluation framework combines technical insights with\npractical experimentation through criteria specifically designed to investigate\nthe practical and creative affordances of the systems within the iterative,\nnon-linear nature of music production. Employing a single-evaluator methodology\nas a preliminary phase, this research adopts a mixed approach utilizing\nqualitative methods to form hypotheses subsequently assessed through\nquantitative metrics. The selected systems represent architectural diversity\nacross both symbolic and audio-based music generation approaches, spanning\ncomposition, arrangement, and sound design tasks. The investigation addresses\nlimitations of current MGS in music production, challenges and opportunities\nfor workflow integration, and development potential as collaborative tools\nwhile maintaining artistic authenticity. Findings reveal these systems function\nprimarily as complementary tools enhancing rather than replacing human\nexpertise. They exhibit limitations in maintaining thematic and structural\ncoherence that emphasize the indispensable role of human creativity in tasks\ndemanding emotional depth and complex decision-making. This study contributes a\nstructured evaluation framework that considers the iterative nature of music\ncreation. It identifies methodological refinements necessary for subsequent\ncomprehensive evaluations and determines viable areas for AI integration as\ncollaborative tools in creative workflows. The research provides\nempirically-grounded insights to guide future development in the field."}
{"id": "2507.01335", "pdf": "https://arxiv.org/pdf/2507.01335.pdf", "abs": "https://arxiv.org/abs/2507.01335", "title": "LEDOM: An Open and Fundamental Reverse Language Model", "authors": ["Xunjian Yin", "Sitao Cheng", "Yuxi Xie", "Xinyu Hu", "Li Lin", "Xinyi Wang", "Liangming Pan", "William Yang Wang", "Xiaojun Wan"], "categories": ["cs.CL", "cs.AI"], "comment": "Work in progress", "summary": "We introduce LEDOM, the first purely reverse language model, trained\nautoregressively on 435B tokens with 2B and 7B parameter variants, which\nprocesses sequences in reverse temporal order through previous token\nprediction. For the first time, we present the reverse language model as a\npotential foundational model across general tasks, accompanied by a set of\nintriguing examples and insights. Based on LEDOM, we further introduce a novel\napplication: Reverse Reward, where LEDOM-guided reranking of forward language\nmodel outputs leads to substantial performance improvements on mathematical\nreasoning tasks. This approach leverages LEDOM's unique backward reasoning\ncapability to refine generation quality through posterior evaluation. Our\nfindings suggest that LEDOM exhibits unique characteristics with broad\napplication potential. We will release all models, training code, and\npre-training data to facilitate future research."}
{"id": "2507.01061", "pdf": "https://arxiv.org/pdf/2507.01061.pdf", "abs": "https://arxiv.org/abs/2507.01061", "title": "Epitome: Pioneering an Experimental Platform for AI-Social Science Integration", "authors": ["Jingjing Qu", "Kejia Hu", "Jun Zhu", "Wenhao Li", "Teng Wang", "Zhiyun Chen", "Yulei Ye", "Chaochao Lu", "Aimin Zhou", "Xiangfeng Wang", "James Evan"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "18 pages, 5figures", "summary": "The integration of Large Language Models (LLMs) into social science\nexperiments represents a transformative approach to understanding human-AI\ninteractions and their societal impacts. We introduce Epitome, the world's\nfirst open experimental platform dedicated to the deep integration of\nartificial intelligence and social science. Rooted in theoretical foundations\nfrom management, communication studies, sociology, psychology, and ethics,\nEpitome focuses on the interactive impacts of AI on individuals, organizations,\nand society during its real-world deployment. It constructs a theoretical\nsupport system through cross-disciplinary experiments. The platform offers a\none-stop comprehensive experimental solution spanning \"foundation\nmodels-complex application development-user feedback\" through seven core\nmodules, while embedding the classical \"control-comparison-comparative causal\nlogic\" of social science experiments into multilevel human-computer interaction\nenvironments, including dialogues, group chats, and multi-agent virtual\nscenarios. With its canvas-style, user-friendly interface, Epitome enables\nresearchers to easily design and run complex experimental scenarios,\nfacilitating systematic investigations into the social impacts of AI and\nexploration of integrated solutions.To demonstrate its capabilities, we\nreplicated three seminal social science experiments involving LLMs, showcasing\nEpitome's potential to streamline complex experimental designs and produce\nrobust results, suitable for publishing in the top selective journals. Our\nfindings highlight the platform's utility in enhancing the efficiency and\nquality of human-AI interactions, providing valuable insights into the societal\nimplications of AI technologies. Epitome thus offers a powerful tool for\nadvancing interdisciplinary research at the intersection of AI and social\nscience, with potential applications in policy-making, ..."}
{"id": "2507.01352", "pdf": "https://arxiv.org/pdf/2507.01352.pdf", "abs": "https://arxiv.org/abs/2507.01352", "title": "Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy", "authors": ["Chris Yuhao Liu", "Liang Zeng", "Yuzhen Xiao", "Jujie He", "Jiacai Liu", "Chaojie Wang", "Rui Yan", "Wei Shen", "Fuxiang Zhang", "Jiacheng Xu", "Yang Liu", "Yahui Zhou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite the critical role of reward models (RMs) in reinforcement learning\nfrom human feedback (RLHF), current state-of-the-art open RMs perform poorly on\nmost existing evaluation benchmarks, failing to capture the spectrum of nuanced\nand sophisticated human preferences. Even approaches that incorporate advanced\ntraining techniques have not yielded meaningful performance improvements. We\nhypothesize that this brittleness stems primarily from limitations in\npreference datasets, which are often narrowly scoped, synthetically labeled, or\nlack rigorous quality control. To address these challenges, we present a\nlarge-scale preference dataset comprising 40 million preference pairs, named\nSynPref-40M. To enable data curation at scale, we design a human-AI synergistic\ntwo-stage pipeline that leverages the complementary strengths of human\nannotation quality and AI scalability. In this pipeline, humans provide\nverified annotations, while large language models perform automatic curation\nbased on human guidance. Training on this preference mixture, we introduce\nSkywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B\nparameters, trained on a carefully curated subset of 26 million preference\npairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile\nacross a wide range of capabilities, including alignment with human\npreferences, objective correctness, safety, resistance to stylistic biases, and\nbest-of-N scaling, achieving state-of-the-art performance across seven major\nreward model benchmarks. Ablation studies confirm that the effectiveness of our\napproach stems not only from data scale but also from high-quality curation.\nThe Skywork-Reward-V2 series represents substantial progress in open reward\nmodels, highlighting the untapped potential of existing preference datasets and\ndemonstrating how human-AI curation synergy can unlock significantly higher\ndata quality."}
{"id": "2507.01111", "pdf": "https://arxiv.org/pdf/2507.01111.pdf", "abs": "https://arxiv.org/abs/2507.01111", "title": "Environment-Aware and Human-Cooperative Swing Control for Lower-Limb Prostheses in Diverse Obstacle Scenarios", "authors": ["Haosen Xing", "Haoran Ma", "Sijin Zhang", "Hartmut Geyer"], "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "Current control strategies for powered lower limb prostheses often lack\nawareness of the environment and the user's intended interactions with it. This\nlimitation becomes particularly apparent in complex terrains. Obstacle\nnegotiation, a critical scenario exemplifying such challenges, requires both\nreal-time perception of obstacle geometry and responsiveness to user intention\nabout when and where to step over or onto, to dynamically adjust swing\ntrajectories. We propose a novel control strategy that fuses environmental\nawareness and human cooperativeness: an on-board depth camera detects obstacles\nahead of swing phase, prompting an elevated early-swing trajectory to ensure\nclearance, while late-swing control defers to natural biomechanical cues from\nthe user. This approach enables intuitive stepping strategies without requiring\nunnatural movement patterns. Experiments with three non-amputee participants\ndemonstrated 100 percent success across more than 150 step-overs and 30\nstep-ons with randomly placed obstacles of varying heights (4-16 cm) and\ndistances (15-70 cm). By effectively addressing obstacle navigation -- a\ngateway challenge for complex terrain mobility -- our system demonstrates\nadaptability to both environmental constraints and user intentions, with\npromising applications across diverse locomotion scenarios."}
{"id": "2507.01437", "pdf": "https://arxiv.org/pdf/2507.01437.pdf", "abs": "https://arxiv.org/abs/2507.01437", "title": "Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction", "authors": ["Ting Xu", "Xiaoxiao Deng", "Xiandong Meng", "Haifeng Yang", "Yan Wu"], "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the challenges posed by the unstructured nature and\nhigh-dimensional semantic complexity of electronic health record texts. A deep\nlearning method based on attention mechanisms is proposed to achieve unified\nmodeling for information extraction and multi-label disease prediction. The\nstudy is conducted on the MIMIC-IV dataset. A Transformer-based architecture is\nused to perform representation learning over clinical text. Multi-layer\nself-attention mechanisms are employed to capture key medical entities and\ntheir contextual relationships. A Sigmoid-based multi-label classifier is then\napplied to predict multiple disease labels. The model incorporates a\ncontext-aware semantic alignment mechanism, enhancing its representational\ncapacity in typical medical scenarios such as label co-occurrence and sparse\ninformation. To comprehensively evaluate model performance, a series of\nexperiments were conducted, including baseline comparisons, hyperparameter\nsensitivity analysis, data perturbation studies, and noise injection tests.\nResults demonstrate that the proposed method consistently outperforms\nrepresentative existing approaches across multiple performance metrics. The\nmodel maintains strong generalization under varying data scales, interference\nlevels, and model depth configurations. The framework developed in this study\noffers an efficient algorithmic foundation for processing real-world clinical\ntexts and presents practical significance for multi-label medical text modeling\ntasks."}
{"id": "2507.01168", "pdf": "https://arxiv.org/pdf/2507.01168.pdf", "abs": "https://arxiv.org/abs/2507.01168", "title": "Towards a Signal Detection Based Measure for Assessing Information Quality of Explainable Recommender Systems", "authors": ["Yeonbin Son", "Matthew L. Bolton"], "categories": ["cs.IR", "cs.HC"], "comment": "Accepted to IEEE CAI 2025", "summary": "There is growing interest in explainable recommender systems that provide\nrecommendations along with explanations for the reasoning behind them. When\nevaluating recommender systems, most studies focus on overall recommendation\nperformance. Only a few assess the quality of the explanations. Explanation\nquality is often evaluated through user studies that subjectively gather users'\nopinions on representative explanatory factors that shape end-users'\nperspective towards the results, not about the explanation contents itself. We\naim to fill this gap by developing an objective metric to evaluate Veracity:\nthe information quality of explanations. Specifically, we decompose Veracity\ninto two dimensions: Fidelity and Attunement. Fidelity refers to whether the\nexplanation includes accurate information about the recommended item.\nAttunement evaluates whether the explanation reflects the target user's\npreferences. By applying signal detection theory, we first determine decision\noutcomes for each dimension and then combine them to calculate a sensitivity,\nwhich serves as the final Veracity value. To assess the effectiveness of the\nproposed metric, we set up four cases with varying levels of information\nquality to validate whether our metric can accurately capture differences in\nquality. The results provided meaningful insights into the effectiveness of our\nproposed metric."}
{"id": "2507.01449", "pdf": "https://arxiv.org/pdf/2507.01449.pdf", "abs": "https://arxiv.org/abs/2507.01449", "title": "LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation", "authors": ["Tianyu Liu", "Qitan Lv", "Hao Li", "Xing Gao", "Xiao Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Speculative decoding (SD), where a small draft model is employed to propose\ndraft tokens in advance and then the target model validates them in parallel,\nhas emerged as a promising technique for LLM inference acceleration. Many\nendeavors to improve SD are to eliminate the need for a draft model and\ngenerate draft tokens in a retrieval-based manner in order to further alleviate\nthe drafting overhead and significantly reduce the difficulty in deployment and\napplications. However, retrieval-based SD relies on a matching paradigm to\nretrieval the most relevant reference as the draft tokens, where these methods\noften fail to find matched and accurate draft tokens. To address this\nchallenge, we propose LogitSpec to effectively expand the retrieval range and\nfind the most relevant reference as drafts. Our LogitSpec is motivated by the\nobservation that the logit of the last token can not only predict the next\ntoken, but also speculate the next next token. Specifically, LogitSpec\ngenerates draft tokens in two steps: (1) utilizing the last logit to speculate\nthe next next token; (2) retrieving relevant reference for both the next token\nand the next next token. LogitSpec is training-free and plug-and-play, which\ncan be easily integrated into existing LLM inference frameworks. Extensive\nexperiments on a wide range of text generation benchmarks demonstrate that\nLogitSpec can achieve up to 2.61 $\\times$ speedup and 3.28 mean accepted tokens\nper decoding step. Our code is available at\nhttps://github.com/smart-lty/LogitSpec."}
{"id": "2507.01196", "pdf": "https://arxiv.org/pdf/2507.01196.pdf", "abs": "https://arxiv.org/abs/2507.01196", "title": "Are Large Brainwave Foundation Models Capable Yet? Insights from Fine-tuning", "authors": ["Na Lee", "Konstantinos Barmpas", "Yannis Panagakis", "Dimitrios Adamos", "Nikolaos Laskaris", "Stefanos Zafeiriou"], "categories": ["cs.LG", "cs.AI", "cs.HC"], "comment": null, "summary": "Foundation Models have demonstrated significant success across various\ndomains in Artificial Intelligence (AI), yet their capabilities for brainwave\nmodeling remain unclear. In this paper, we comprehensively evaluate current\nLarge Brainwave Foundation Models (LBMs) through systematic fine-tuning\nexperiments across multiple Brain-Computer Interface (BCI) benchmark tasks,\nincluding memory tasks and sleep stage classification. Our extensive analysis\nshows that state-of-the-art LBMs achieve only marginal improvements (0.9%-1.2%)\nover traditional deep architectures while requiring significantly more\nparameters (millions vs thousands), raising important questions about their\nefficiency and applicability in BCI contexts. Moreover, through detailed\nablation studies and Low-Rank Adaptation (LoRA), we significantly reduce\ntrainable parameters without performance degradation, while demonstrating that\narchitectural and training inefficiencies limit LBMs' current capabilities. Our\nexperiments span both full model fine-tuning and parameter-efficient adaptation\ntechniques, providing insights into optimal training strategies for BCI\napplications. We pioneer the application of LoRA to LBMs, revealing that\nperformance benefits generally emerge when adapting multiple neural network\ncomponents simultaneously. These findings highlight the critical need for\ndomain-specific development strategies to advance LBMs, suggesting that current\narchitectures may require redesign to fully leverage the potential of\nfoundation models in brainwave analysis."}
{"id": "2507.01479", "pdf": "https://arxiv.org/pdf/2507.01479.pdf", "abs": "https://arxiv.org/abs/2507.01479", "title": "Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities", "authors": ["Yingqiang Gao", "Kaede Johnson", "David Froehlich", "Luisa Carrer", "Sarah Ebling"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Automatic text simplification (ATS) aims to enhance language accessibility\nfor various target groups, particularly persons with intellectual disabilities.\nRecent advancements in generative AI, especially large language models (LLMs),\nhave substantially improved the quality of machine-generated text\nsimplifications, thereby mitigating information barriers for the target group.\nHowever, existing LLM-based ATS systems do not incorporate preference feedback\non text simplifications during training, resulting in a lack of personalization\ntailored to the specific needs of target group representatives.\n  In this work, we extend the standard supervised fine-tuning (SFT) approach\nfor adapting LLM-based ATS models by leveraging a computationally efficient LLM\nalignment technique -- direct preference optimization (DPO). Specifically, we\npost-train LLM-based ATS models using human feedback collected from persons\nwith intellectual disabilities, reflecting their preferences on paired text\nsimplifications generated by mainstream LLMs. Furthermore, we propose a\npipeline for developing personalized LLM-based ATS systems, encompassing data\ncollection, model selection, SFT and DPO post-training, and evaluation. Our\nfindings underscore the necessity of active participation of target group\npersons in designing personalized AI accessibility solutions aligned with human\nexpectations. This work represents a step towards personalizing inclusive AI\nsystems at the target-group level, incorporating insights not only from text\nsimplification experts but also from target group persons themselves."}
{"id": "2507.01206", "pdf": "https://arxiv.org/pdf/2507.01206.pdf", "abs": "https://arxiv.org/abs/2507.01206", "title": "2024 NASA SUITS Report: LLM-Driven Immersive Augmented Reality User Interface for Robotics and Space Exploration", "authors": ["Kathy Zhuang", "Zixun Huang", "Yukun Song", "Rui Li", "Yinuo Zhou", "Allen Y. Yang"], "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "As modern computing advances, new interaction paradigms have emerged,\nparticularly in Augmented Reality (AR), which overlays virtual interfaces onto\nphysical objects. This evolution poses challenges in machine perception,\nespecially for tasks like 3D object pose estimation in complex, dynamic\nenvironments. Our project addresses critical issues in human-robot interaction\nwithin mobile AR, focusing on non-intrusive, spatially aware interfaces. We\npresent URSA, an LLM-driven immersive AR system developed for NASA's 2023-2024\nSUITS challenge, targeting future spaceflight needs such as the Artemis\nmissions. URSA integrates three core technologies: a head-mounted AR device\n(e.g., HoloLens) for intuitive visual feedback, voice control powered by large\nlanguage models for hands-free interaction, and robot tracking algorithms that\nenable accurate 3D localization in dynamic settings. To enhance precision, we\nleverage digital twin localization technologies, using datasets like\nDTTD-Mobile and specialized hardware such as the ZED2 camera for real-world\ntracking under noise and occlusion. Our system enables real-time robot control\nand monitoring via an AR interface, even in the absence of ground-truth\nsensors--vital for hazardous or remote operations. Key contributions include:\n(1) a non-intrusive AR interface with LLM-based voice input; (2) a ZED2-based\ndataset tailored for non-rigid robotic bodies; (3) a Local Mission Control\nConsole (LMCC) for mission visualization; (4) a transformer-based 6DoF pose\nestimator (DTTDNet) optimized for depth fusion and real-time tracking; and (5)\nend-to-end integration for astronaut mission support. This work advances\ndigital twin applications in robotics, offering scalable solutions for both\naerospace and industrial domains."}
{"id": "2507.01541", "pdf": "https://arxiv.org/pdf/2507.01541.pdf", "abs": "https://arxiv.org/abs/2507.01541", "title": "Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing", "authors": ["Álvaro Zaera", "Diana Nicoleta Popa", "Ivan Sekulic", "Paolo Rosso"], "categories": ["cs.CL"], "comment": null, "summary": "Out-of-scope (OOS) intent detection is a critical challenge in task-oriented\ndialogue systems (TODS), as it ensures robustness to unseen and ambiguous\nqueries. In this work, we propose a novel but simple modular framework that\ncombines uncertainty modeling with fine-tuned large language models (LLMs) for\nefficient and accurate OOS detection. The first step applies uncertainty\nestimation to the output of an in-scope intent detection classifier, which is\ncurrently deployed in a real-world TODS handling tens of thousands of user\ninteractions daily. The second step then leverages an emerging LLM-based\napproach, where a fine-tuned LLM is triggered to make a final decision on\ninstances with high uncertainty. Unlike prior approaches, our method\neffectively balances computational efficiency and performance, combining\ntraditional approaches with LLMs and yielding state-of-the-art results on key\nOOS detection benchmarks, including real-world OOS data acquired from a\ndeployed TODS."}
{"id": "2507.01282", "pdf": "https://arxiv.org/pdf/2507.01282.pdf", "abs": "https://arxiv.org/abs/2507.01282", "title": "Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care", "authors": ["Matthew JY Kang", "Wenli Yang", "Monica R Roberts", "Byeong Ho Kang", "Charles B Malpas"], "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "The recent boom of large language models (LLMs) has re-ignited the hope that\nartificial intelligence (AI) systems could aid medical diagnosis. Yet despite\ndazzling benchmark scores, LLM assistants have yet to deliver measurable\nimprovements at the bedside. This scoping review aims to highlight the areas\nwhere AI is limited to make practical contributions in the clinical setting,\nspecifically in dementia diagnosis and care.\n  Standalone machine-learning models excel at pattern recognition but seldom\nprovide actionable, interpretable guidance, eroding clinician trust. Adjacent\nuse of LLMs by physicians did not result in better diagnostic accuracy or\nspeed. Key limitations trace to the data-driven paradigm: black-box outputs\nwhich lack transparency, vulnerability to hallucinations, and weak causal\nreasoning. Hybrid approaches that combine statistical learning with expert\nrule-based knowledge, and involve clinicians throughout the process help bring\nback interpretability. They also fit better with existing clinical workflows,\nas seen in examples like PEIRS and ATHENA-CDS.\n  Future decision-support should prioritise explanatory coherence by linking\npredictions to clinically meaningful causes. This can be done through\nneuro-symbolic or hybrid AI that combines the language ability of LLMs with\nhuman causal expertise. AI researchers have addressed this direction, with\nexplainable AI and neuro-symbolic AI being the next logical steps in further\nadvancement in AI. However, they are still based on data-driven knowledge\nintegration instead of human-in-the-loop approaches. Future research should\nmeasure success not only by accuracy but by improvements in clinician\nunderstanding, workflow fit, and patient outcomes. A better understanding of\nwhat helps improve human-computer interactions is greatly needed for AI systems\nto become part of clinical practice."}
{"id": "2507.01543", "pdf": "https://arxiv.org/pdf/2507.01543.pdf", "abs": "https://arxiv.org/abs/2507.01543", "title": "Is External Information Useful for Stance Detection with LLMs?", "authors": ["Quang Minh Nguyen", "Taegyoon Kim"], "categories": ["cs.CL"], "comment": "ACL Findings 2025", "summary": "In the stance detection task, a text is classified as either favorable,\nopposing, or neutral towards a target. Prior work suggests that the use of\nexternal information, e.g., excerpts from Wikipedia, improves stance detection\nperformance. However, whether or not such information can benefit large\nlanguage models (LLMs) remains an unanswered question, despite their wide\nadoption in many reasoning tasks. In this study, we conduct a systematic\nevaluation on how Wikipedia and web search external information can affect\nstance detection across eight LLMs and in three datasets with 12 targets.\nSurprisingly, we find that such information degrades performance in most cases,\nwith macro F1 scores dropping by up to 27.9\\%. We explain this through\nexperiments showing LLMs' tendency to align their predictions with the stance\nand sentiment of the provided information rather than the ground truth stance\nof the given text. We also find that performance degradation persists with\nchain-of-thought prompting, while fine-tuning mitigates but does not fully\neliminate it. Our findings, in contrast to previous literature on BERT-based\nsystems which suggests that external information enhances performance,\nhighlight the risks of information biases in LLM-based stance classifiers. Code\nis available at https://github.com/ngqm/acl2025-stance-detection."}
{"id": "2507.01431", "pdf": "https://arxiv.org/pdf/2507.01431.pdf", "abs": "https://arxiv.org/abs/2507.01431", "title": "Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading", "authors": ["Yoonseok Yang", "Minjune Kim", "Marlon Rondinelli", "Keren Shao"], "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": "7 pages, 5 figues, 1 table", "summary": "Grading handwritten, open-ended responses remains a major bottleneck in large\nuniversity STEM courses. We introduce Pensieve (https://www.pensieve.co), an\nAI-assisted grading platform that leverages large language models (LLMs) to\ntranscribe and evaluate student work, providing instructors with rubric-aligned\nscores, transcriptions, and confidence ratings. Unlike prior tools that focus\nnarrowly on specific tasks like transcription or rubric generation, Pensieve\nsupports the entire grading pipeline-from scanned student submissions to final\nfeedback-within a human-in-the-loop interface.\n  Pensieve has been deployed in real-world courses at over 20 institutions and\nhas graded more than 300,000 student responses. We present system details and\nempirical results across four core STEM disciplines: Computer Science,\nMathematics, Physics, and Chemistry. Our findings show that Pensieve reduces\ngrading time by an average of 65%, while maintaining a 95.4% agreement rate\nwith instructor-assigned grades for high-confidence predictions."}
{"id": "2507.01594", "pdf": "https://arxiv.org/pdf/2507.01594.pdf", "abs": "https://arxiv.org/abs/2507.01594", "title": "Emotionally Intelligent Task-oriented Dialogue Systems: Architecture, Representation, and Optimisation", "authors": ["Shutong Feng", "Hsien-chin Lin", "Nurul Lubis", "Carel van Niekerk", "Michael Heck", "Benjamin Ruppik", "Renato Vukovic", "Milica Gašić"], "categories": ["cs.CL"], "comment": "19 pages, 6 figures", "summary": "Task-oriented dialogue (ToD) systems are designed to help users achieve\nspecific goals through natural language interaction. While recent advances in\nlarge language models (LLMs) have significantly improved linguistic fluency and\ncontextual understanding, building effective and emotionally intelligent ToD\nsystems remains a complex challenge. Effective ToD systems must optimise for\ntask success, emotional understanding and responsiveness, and precise\ninformation conveyance, all within inherently noisy and ambiguous\nconversational environments. In this work, we investigate architectural,\nrepresentational, optimisational as well as emotional considerations of ToD\nsystems. We set up systems covering these design considerations with a\nchallenging evaluation environment composed of a natural-language user\nsimulator coupled with an imperfect natural language understanding module. We\npropose \\textbf{LUSTER}, an \\textbf{L}LM-based \\textbf{U}nified \\textbf{S}ystem\nfor \\textbf{T}ask-oriented dialogue with \\textbf{E}nd-to-end\n\\textbf{R}einforcement learning with both short-term (user sentiment) and\nlong-term (task success) rewards. Our findings demonstrate that combining LLM\ncapability with structured reward modelling leads to more resilient and\nemotionally responsive ToD systems, offering a practical path forward for\nnext-generation conversational agents."}
{"id": "2211.03324", "pdf": "https://arxiv.org/pdf/2211.03324.pdf", "abs": "https://arxiv.org/abs/2211.03324", "title": "Decoding Neural Signals: Invasive BMI Review", "authors": ["Rezwan Firuzi", "Ayub Bokani", "Jahan Hassan", "Hamed Ahmadyani", "Mohammad Foad Abdi", "Dana Naderi", "Diako Ebrahimi"], "categories": ["cs.HC"], "comment": "We have made significant changes to this article", "summary": "Human civilization has witnessed transformative technological milestones,\nfrom ancient fire lighting to the internet era. This chapter delves into the\ninvasive brain machine interface (BMI), a pioneering technology poised to be a\ndefining chapter in our progress. Beyond aiding medical conditions, invasive\nBMI promises far reaching impacts across diverse technologies and aspects of\nlife. The exploration begins by unraveling the biological and engineering\nprinciples essential for BMI implementation. The chapter comprehensively\nanalyzes potential applications, methodologies for detecting and decoding brain\nsignals, and options for stimulating signals within the human brain. It\nconcludes with a discussion on the multifaceted challenges and opportunities\nfor the continued development of invasive BMI. This chapter not only provides a\nprofound understanding of the foundational elements of invasive BMI but also\nserves as a guide through its applications, intricacies, and potential societal\nimplications. Navigating neurobiology, engineering innovations, and the\nevolving landscape of human AI symbiosis, the chapter sheds light on the\npromises and hurdles that define the future of invasive BMI."}
{"id": "2507.01627", "pdf": "https://arxiv.org/pdf/2507.01627.pdf", "abs": "https://arxiv.org/abs/2507.01627", "title": "Chart Question Answering from Real-World Analytical Narratives", "authors": ["Maeve Hutchinson", "Radu Jianu", "Aidan Slingsby", "Jo Wood", "Pranava Madhyastha"], "categories": ["cs.CL"], "comment": "This paper has been accepted to the ACL Student Research Workshop\n  (SRW) 2025", "summary": "We present a new dataset for chart question answering (CQA) constructed from\nvisualization notebooks. The dataset features real-world, multi-view charts\npaired with natural language questions grounded in analytical narratives.\nUnlike prior benchmarks, our data reflects ecologically valid reasoning\nworkflows. Benchmarking state-of-the-art multimodal large language models\nreveals a significant performance gap, with GPT-4.1 achieving an accuracy of\n69.3%, underscoring the challenges posed by this more authentic CQA setting."}
{"id": "2412.16825", "pdf": "https://arxiv.org/pdf/2412.16825.pdf", "abs": "https://arxiv.org/abs/2412.16825", "title": "SoK: Usability Studies in Differential Privacy", "authors": ["Onyinye Dibia", "Prianka Bhattacharjee", "Brad Stenger", "Steven Baldasty", "Mako Bates", "Ivoline C. Ngong", "Yuanyuan Feng", "Joseph P. Near"], "categories": ["cs.HC", "cs.CR"], "comment": null, "summary": "Differential Privacy (DP) has emerged as a pivotal approach for safeguarding\nindividual privacy in data analysis, yet its practical adoption is often\nhindered by challenges in the implementation and communication of DP. This\npaper presents a comprehensive systematization of existing research studies\naround the usability of DP, synthesizing insights from studies on both the\npractical use of DP tools and strategies for conveying DP parameters that\ndetermine privacy protection levels, such as epsilon($\\varepsilon$). By\nreviewing and analyzing these studies, we identify core usability challenges,\nbest practices, and critical gaps in current DP tools that affect adoption\nacross diverse user groups, including developers, data analysts, and\nnon-technical stakeholders. Our analysis highlights actionable insights and\npathways for future research that emphasizes user-centered design and clear\ncommunication, fostering the development of more accessible DP tools that meet\npractical needs and support broader adoption."}
{"id": "2507.01633", "pdf": "https://arxiv.org/pdf/2507.01633.pdf", "abs": "https://arxiv.org/abs/2507.01633", "title": "Confidence and Stability of Global and Pairwise Scores in NLP Evaluation", "authors": ["Georgii Levtsov", "Dmitry Ustalov"], "categories": ["cs.CL", "cs.IR", "62-04", "D.2.3"], "comment": "8 pages, accepted at ACL SRW 2025", "summary": "With the advent of highly capable instruction-tuned neural language models,\nbenchmarking in natural language processing (NLP) is increasingly shifting\ntowards pairwise comparison leaderboards, such as LMSYS Arena, from traditional\nglobal pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper\nempirically investigates the strengths and weaknesses of both global scores and\npairwise comparisons to aid decision-making in selecting appropriate model\nevaluation strategies. Through computational experiments on synthetic and\nreal-world datasets using standard global metrics and the popular Bradley-Terry\nmodel for pairwise comparisons, we found that while global scores provide more\nreliable overall rankings, they can underestimate strong models with rare,\nsignificant errors or low confidence. Conversely, pairwise comparisons are\nparticularly effective for identifying strong contenders among models with\nlower global scores, especially where quality metrics are hard to define (e.g.,\ntext generation), though they require more comparisons to converge if ties are\nfrequent. Our code and data are available at\nhttps://github.com/HSPyroblast/srw-ranking under a permissive license."}
{"id": "2504.13901", "pdf": "https://arxiv.org/pdf/2504.13901.pdf", "abs": "https://arxiv.org/abs/2504.13901", "title": "Examining Technology Perspectives of Older Adults with Mild Cognitive Impairment: A Scoping Review", "authors": ["Snezna B Schmidt", "Stephen Isbel", "Blooma John", "Ram Subramanian", "Nathan M DCunha"], "categories": ["cs.HC"], "comment": "Paper updated", "summary": "Mild cognitive impairment (MCI) may affect up to 20 % of people over 65 years\nold. Global incidence of MCI is increasing, and technology is being explored\nfor early intervention. Theories of technology adoption predict that useful and\neasy to use solutions will have higher rates of adoption, however, these models\ndo not specifically consider older people with cognitive impairments, or the\nunique human computer interaction challenges posed by MCI. We collated opinions\nfrom older people with MCI about technology solutions proposed for them, found\nin 83 articles published between Jan 2014 and May 2024, and found in nine\ndatabases. Inductive, thematic analysis of feedback identified five themes (i)\npurpose and need, (ii) solution design and ease of use, (iii) self-impression,\n(iv) lifestyle, and (v) interaction modality. Solutions are perceived as\nuseful, even though gaps in functional support exist, however, they are not\nperceived as entirely easy to use, due to issues related to usability and user\nexperience. Devices which are light, portable, common and have large screens,\nare preferred, as is multimodal interaction, in particular speech, visual/text\nand touch. This review recommends future work to (i) improve usability and user\nexperience, (ii) enhance personalisation, (iii) better understand interaction\npreferences and effectiveness, (iv) enable options for multimodal interaction,\nand (v) more seamlessly integrate solutions into users lifestyles."}
{"id": "2507.01645", "pdf": "https://arxiv.org/pdf/2507.01645.pdf", "abs": "https://arxiv.org/abs/2507.01645", "title": "Adapting Language Models to Indonesian Local Languages: An Empirical Study of Language Transferability on Zero-Shot Settings", "authors": ["Rifki Afina Putri"], "categories": ["cs.CL"], "comment": "AMLDS 2025", "summary": "In this paper, we investigate the transferability of pre-trained language\nmodels to low-resource Indonesian local languages through the task of sentiment\nanalysis. We evaluate both zero-shot performance and adapter-based transfer on\nten local languages using models of different types: a monolingual Indonesian\nBERT, multilingual models such as mBERT and XLM-R, and a modular adapter-based\napproach called MAD-X. To better understand model behavior, we group the target\nlanguages into three categories: seen (included during pre-training), partially\nseen (not included but linguistically related to seen languages), and unseen\n(absent and unrelated in pre-training data). Our results reveal clear\nperformance disparities across these groups: multilingual models perform best\non seen languages, moderately on partially seen ones, and poorly on unseen\nlanguages. We find that MAD-X significantly improves performance, especially\nfor seen and partially seen languages, without requiring labeled data in the\ntarget language. Additionally, we conduct a further analysis on tokenization\nand show that while subword fragmentation and vocabulary overlap with\nIndonesian correlate weakly with prediction quality, they do not fully explain\nthe observed performance. Instead, the most consistent predictor of transfer\nsuccess is the model's prior exposure to the language, either directly or\nthrough a related language."}
{"id": "2505.03440", "pdf": "https://arxiv.org/pdf/2505.03440.pdf", "abs": "https://arxiv.org/abs/2505.03440", "title": "manvr3d: A Platform for Human-in-the-loop Cell Tracking in Virtual Reality", "authors": ["Samuel Pantze", "Jean-Yves Tinevez", "Matthew McGinity", "Ulrik Günther"], "categories": ["cs.HC"], "comment": "7 pages, 6 figures, submitted to IEEE VIS 2025", "summary": "We propose manvr3d, a novel VR-ready platform for interactive\nhuman-in-the-loop cell tracking. We utilize VR controllers and eye-tracking\nhardware to facilitate rapid ground truth generation and proofreading for deep\nlearning-based cell tracking models. Life scientists reconstruct the\ndevelopmental history of organisms on the cellular level by analyzing 3D\ntime-lapse microscopy images acquired at high spatio-temporal resolution. The\nreconstruction of such cell lineage trees traditionally involves tracking\nindividual cells through all recorded time points, manually annotating their\npositions, and then linking them over time to create complete trajectories.\nDeep learning-based algorithms accelerate this process, yet depend heavily on\nmanually-annotated high-quality ground truth data and curation. Visual\nrepresentation of the image data in this process still relies primarily on 2D\nrenderings, which greatly limits spatial understanding and navigation. In this\nwork, we bridge the gap between deep learning-based cell tracking software and\n3D/VR visualization to create a human-in-the-loop cell tracking system. We lift\nthe incremental annotation, training and proofreading loop of the deep learning\nmodel into the 3rd dimension and apply natural user interfaces like hand\ngestures and eye tracking to accelerate the cell tracking workflow for life\nscientists."}
{"id": "2507.01702", "pdf": "https://arxiv.org/pdf/2507.01702.pdf", "abs": "https://arxiv.org/abs/2507.01702", "title": "AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness", "authors": ["Zixin Chen", "Hongzhan Lin", "Kaixin Li", "Ziyang Luo", "Zhen Ye", "Guang Chen", "Zhiyong Huang", "Jing Ma"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "The proliferation of multimodal memes in the social media era demands that\nmultimodal Large Language Models (mLLMs) effectively understand meme\nharmfulness. Existing benchmarks for assessing mLLMs on harmful meme\nunderstanding rely on accuracy-based, model-agnostic evaluations using static\ndatasets. These benchmarks are limited in their ability to provide up-to-date\nand thorough assessments, as online memes evolve dynamically. To address this,\nwe propose AdamMeme, a flexible, agent-based evaluation framework that\nadaptively probes the reasoning capabilities of mLLMs in deciphering meme\nharmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive\nevaluations by iteratively updating the meme data with challenging samples,\nthereby exposing specific limitations in how mLLMs interpret harmfulness.\nExtensive experiments show that our framework systematically reveals the\nvarying performance of different target mLLMs, offering in-depth, fine-grained\nanalyses of model-specific weaknesses. Our code is available at\nhttps://github.com/Lbotirx/AdamMeme."}
{"id": "2506.21319", "pdf": "https://arxiv.org/pdf/2506.21319.pdf", "abs": "https://arxiv.org/abs/2506.21319", "title": "SimVecVis: A Dataset for Enhancing MLLMs in Visualization Understanding", "authors": ["Can Liu", "Chunlin Da", "Xiaoxiao Long", "Yuxiao Yang", "Yu Zhang", "Yong Wang"], "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "Current multimodal large language models (MLLMs), while effective in natural\nimage understanding, struggle with visualization understanding due to their\ninability to decode the data-to-visual mapping and extract structured\ninformation. To address these challenges, we propose SimVec, a novel simplified\nvector format that encodes chart elements such as mark type, position, and\nsize. The effectiveness of SimVec is demonstrated by using MLLMs to reconstruct\nchart information from SimVec formats. Then, we build a new visualization\ndataset, SimVecVis, to enhance the performance of MLLMs in visualization\nunderstanding, which consists of three key dimensions: bitmap images of charts,\ntheir SimVec representations, and corresponding data-centric question-answering\n(QA) pairs with explanatory chain-of-thought (CoT) descriptions. We finetune\nstate-of-the-art MLLMs (e.g., MiniCPM and Qwen-VL), using SimVecVis with\ndifferent dataset dimensions. The experimental results show that it leads to\nsubstantial performance improvements of MLLMs with good spatial perception\ncapabilities (e.g., MiniCPM) in data-centric QA tasks. Our dataset and source\ncode are available at: https://github.com/VIDA-Lab/SimVecVis."}
{"id": "2507.01715", "pdf": "https://arxiv.org/pdf/2507.01715.pdf", "abs": "https://arxiv.org/abs/2507.01715", "title": "Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach", "authors": ["Aditya Tomar", "Rudra Murthy", "Pushpak Bhattacharyya"], "categories": ["cs.CL"], "comment": null, "summary": "Bias and stereotypes in language models can cause harm, especially in\nsensitive areas like content moderation and decision-making. This paper\naddresses bias and stereotype detection by exploring how jointly learning these\ntasks enhances model performance. We introduce StereoBias, a unique dataset\nlabeled for bias and stereotype detection across five categories: religion,\ngender, socio-economic status, race, profession, and others, enabling a deeper\nstudy of their relationship. Our experiments compare encoder-only models and\nfine-tuned decoder-only models using QLoRA. While encoder-only models perform\nwell, decoder-only models also show competitive results. Crucially, joint\ntraining on bias and stereotype detection significantly improves bias detection\ncompared to training them separately. Additional experiments with sentiment\nanalysis confirm that the improvements stem from the connection between bias\nand stereotypes, not multi-task learning alone. These findings highlight the\nvalue of leveraging stereotype information to build fairer and more effective\nAI systems."}
{"id": "2506.22941", "pdf": "https://arxiv.org/pdf/2506.22941.pdf", "abs": "https://arxiv.org/abs/2506.22941", "title": "Positioning AI Tools to Support Online Harm Reduction Practice: Applications and Design Directions", "authors": ["Kaixuan Wang", "Jason T. Jacques", "Chenxin Diao"], "categories": ["cs.HC", "cs.AI"], "comment": "16 pages, 4 figures, with appendix", "summary": "Access to accurate and actionable harm reduction information can directly\nimpact the health outcomes of People Who Use Drugs (PWUD), yet existing online\nchannels often fail to meet their diverse and dynamic needs due to limitations\nin adaptability, accessibility, and the pervasive impact of stigma. Large\nLanguage Models (LLMs) present a novel opportunity to enhance information\nprovision, but their application in such a high-stakes domain is under-explored\nand presents socio-technical challenges. This paper investigates how LLMs can\nbe responsibly designed to support the information needs of PWUD. Through a\nqualitative workshop involving diverse stakeholder groups (academics, harm\nreduction practitioners, and an online community moderator), we explored LLM\ncapabilities, identified potential use cases, and delineated core design\nconsiderations. Our findings reveal that while LLMs can address some existing\ninformation barriers (e.g., by offering responsive, multilingual, and\npotentially less stigmatising interactions), their effectiveness is contingent\nupon overcoming challenges related to ethical alignment with harm reduction\nprinciples, nuanced contextual understanding, effective communication, and\nclearly defined operational boundaries. We articulate design pathways\nemphasising collaborative co-design with experts and PWUD to develop LLM\nsystems that are helpful, safe, and responsibly governed. This work contributes\nempirically grounded insights and actionable design considerations for the\nresponsible development of LLMs as supportive tools within the harm reduction\necosystem."}
{"id": "2507.01734", "pdf": "https://arxiv.org/pdf/2507.01734.pdf", "abs": "https://arxiv.org/abs/2507.01734", "title": "LLMs for Legal Subsumption in German Employment Contracts", "authors": ["Oliver Wardas", "Florian Matthes"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "PrePrint - ICAIL25, Chicago", "summary": "Legal work, characterized by its text-heavy and resource-intensive nature,\npresents unique challenges and opportunities for NLP research. While\ndata-driven approaches have advanced the field, their lack of interpretability\nand trustworthiness limits their applicability in dynamic legal environments.\nTo address these issues, we collaborated with legal experts to extend an\nexisting dataset and explored the use of Large Language Models (LLMs) and\nin-context learning to evaluate the legality of clauses in German employment\ncontracts. Our work evaluates the ability of different LLMs to classify clauses\nas \"valid,\" \"unfair,\" or \"void\" under three legal context variants: no legal\ncontext, full-text sources of laws and court rulings, and distilled versions of\nthese (referred to as examination guidelines). Results show that full-text\nsources moderately improve performance, while examination guidelines\nsignificantly enhance recall for void clauses and weighted F1-Score, reaching\n80\\%. Despite these advancements, LLMs' performance when using full-text\nsources remains substantially below that of human lawyers. We contribute an\nextended dataset, including examination guidelines, referenced legal sources,\nand corresponding annotations, alongside our code and all log files. Our\nfindings highlight the potential of LLMs to assist lawyers in contract legality\nreview while also underscoring the limitations of the methods presented."}
{"id": "2308.02515", "pdf": "https://arxiv.org/pdf/2308.02515.pdf", "abs": "https://arxiv.org/abs/2308.02515", "title": "Feature Reweighting for EEG-based Motor Imagery Classification", "authors": ["Taveena Lotey", "Prateek Keserwani", "Debi Prosad Dogra", "Partha Pratim Roy"], "categories": ["cs.LG", "cs.HC", "eess.SP"], "comment": null, "summary": "Classification of motor imagery (MI) using non-invasive\nelectroencephalographic (EEG) signals is a critical objective as it is used to\npredict the intention of limb movements of a subject. In recent research,\nconvolutional neural network (CNN) based methods have been widely utilized for\nMI-EEG classification. The challenges of training neural networks for MI-EEG\nsignals classification include low signal-to-noise ratio, non-stationarity,\nnon-linearity, and high complexity of EEG signals. The features computed by\nCNN-based networks on the highly noisy MI-EEG signals contain irrelevant\ninformation. Subsequently, the feature maps of the CNN-based network computed\nfrom the noisy and irrelevant features contain irrelevant information. Thus,\nmany non-contributing features often mislead the neural network training and\ndegrade the classification performance. Hence, a novel feature reweighting\napproach is proposed to address this issue. The proposed method gives a noise\nreduction mechanism named feature reweighting module that suppresses irrelevant\ntemporal and channel feature maps. The feature reweighting module of the\nproposed method generates scores that reweight the feature maps to reduce the\nimpact of irrelevant information. Experimental results show that the proposed\nmethod significantly improved the classification of MI-EEG signals of Physionet\nEEG-MMIDB and BCI Competition IV 2a datasets by a margin of 9.34% and 3.82%,\nrespectively, compared to the state-of-the-art methods."}
{"id": "2507.01764", "pdf": "https://arxiv.org/pdf/2507.01764.pdf", "abs": "https://arxiv.org/abs/2507.01764", "title": "Data interference: emojis, homoglyphs, and issues of data fidelity in corpora and their results", "authors": ["Matteo Di Cristofaro"], "categories": ["cs.CL"], "comment": "Author submitted manuscript", "summary": "Tokenisation - \"the process of splitting text into atomic parts\" (Brezina &\nTimperley, 2017: 1) - is a crucial step for corpus linguistics, as it provides\nthe basis for any applicable quantitative method (e.g. collocations) while\nensuring the reliability of qualitative approaches. This paper examines how\ndiscrepancies in tokenisation affect the representation of language data and\nthe validity of analytical findings: investigating the challenges posed by\nemojis and homoglyphs, the study highlights the necessity of preprocessing\nthese elements to maintain corpus fidelity to the source data. The research\npresents methods for ensuring that digital texts are accurately represented in\ncorpora, thereby supporting reliable linguistic analysis and guaranteeing the\nrepeatability of linguistic interpretations. The findings emphasise the\nnecessity of a detailed understanding of both linguistic and technical aspects\ninvolved in digital textual data to enhance the accuracy of corpus analysis,\nand have significant implications for both quantitative and qualitative\napproaches in corpus-based research."}
{"id": "2311.10918", "pdf": "https://arxiv.org/pdf/2311.10918.pdf", "abs": "https://arxiv.org/abs/2311.10918", "title": "Jenga Stacking Based on 6D Pose Estimation for Architectural Form Finding Process", "authors": ["Zixun Huang"], "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "This paper includes a review of current state of the art 6d pose estimation\nmethods, as well as a discussion of which pose estimation method should be used\nin two types of architectural design scenarios. Taking the latest pose\nestimation research Gen6d as an example, we make a qualitative assessment of\nthe current openset methods in terms of application level, prediction speed,\nresistance to occlusion, accuracy, resistance to environmental interference,\netc. In addition, we try to combine 6D pose estimation and building wind\nenvironment assessment to create tangible architectural design approach, we\ndiscuss the limitations of the method and point out the direction in which 6d\npose estimation is eager to progress in this scenario."}
{"id": "2507.01785", "pdf": "https://arxiv.org/pdf/2507.01785.pdf", "abs": "https://arxiv.org/abs/2507.01785", "title": "MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining", "authors": ["Zhixun Chen", "Ping Guo", "Wenhan Han", "Yifan Zhang", "Binbin Liu", "Haobin Lin", "Fengze Liu", "Yan Zhao", "Bingni Zhang", "Taifeng Wang", "Yin Zheng", "Meng Fang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Data quality is a critical driver of large language model performance, yet\nexisting model-based selection methods focus almost exclusively on English. We\nintroduce MuRating, a scalable framework that transfers high-quality English\ndata-quality signals into a single rater for 17 target languages. MuRating\naggregates multiple English \"raters\" via pairwise comparisons to learn unified\ndocument-quality scores,then projects these judgments through translation to\ntrain a multilingual evaluator on monolingual, cross-lingual, and parallel text\npairs. Applied to web data, MuRating selects balanced subsets of English and\nmultilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to\nstrong baselines, including QuRater, AskLLM, DCLM and so on, our approach\nboosts average accuracy on both English benchmarks and multilingual\nevaluations, with especially large gains on knowledge-intensive tasks. We\nfurther analyze translation fidelity, selection biases, and underrepresentation\nof narrative material, outlining directions for future work."}
{"id": "2403.08700", "pdf": "https://arxiv.org/pdf/2403.08700.pdf", "abs": "https://arxiv.org/abs/2403.08700", "title": "Diffusion-based Iterative Counterfactual Explanations for Fetal Ultrasound Image Quality Assessment", "authors": ["Paraskevas Pegios", "Manxi Lin", "Nina Weng", "Morten Bo Søndergaard Svendsen", "Zahra Bashir", "Siavash Bigdeli", "Anders Nymark Christensen", "Martin Tolsgaard", "Aasa Feragen"], "categories": ["eess.IV", "cs.CV", "cs.HC", "cs.LG"], "comment": null, "summary": "Obstetric ultrasound image quality is crucial for accurate diagnosis and\nmonitoring of fetal health. However, acquiring high-quality standard planes is\ndifficult, influenced by the sonographer's expertise and factors like the\nmaternal BMI or fetus dynamics. In this work, we explore diffusion-based\ncounterfactual explainable AI to generate realistic, high-quality standard\nplanes from low-quality non-standard ones. Through quantitative and qualitative\nevaluation, we demonstrate the effectiveness of our approach in generating\nplausible counterfactuals of increased quality. This shows future promise for\nenhancing training of clinicians by providing visual feedback and potentially\nimproving standard plane quality and acquisition for downstream diagnosis and\nmonitoring."}
{"id": "2507.01786", "pdf": "https://arxiv.org/pdf/2507.01786.pdf", "abs": "https://arxiv.org/abs/2507.01786", "title": "Probing Evaluation Awareness of Language Models", "authors": ["Jord Nguyen", "Khiem Hoang", "Carlo Leonardo Attubato", "Felix Hofstätter"], "categories": ["cs.CL", "cs.AI"], "comment": "Technical AI Governance Workshop, ICML (Poster)", "summary": "Language models can distinguish between testing and deployment phases -- a\ncapability known as evaluation awareness. This has significant safety and\npolicy implications, potentially undermining the reliability of evaluations\nthat are central to AI governance frameworks and voluntary industry\ncommitments. In this paper, we study evaluation awareness in\nLlama-3.3-70B-Instruct. We show that linear probes can separate real-world\nevaluation and deployment prompts, suggesting that current models internally\nrepresent this distinction. We also find that current safety evaluations are\ncorrectly classified by the probes, suggesting that they already appear\nartificial or inauthentic to models. Our findings underscore the importance of\nensuring trustworthy evaluations and understanding deceptive capabilities. More\nbroadly, our work showcases how model internals may be leveraged to support\nblackbox methods in safety audits, especially for future models more competent\nat evaluation awareness and deception."}
{"id": "2502.09282", "pdf": "https://arxiv.org/pdf/2502.09282.pdf", "abs": "https://arxiv.org/abs/2502.09282", "title": "FE-LWS: Refined Image-Text Representations via Decoder Stacking and Fused Encodings for Remote Sensing Image Captioning", "authors": ["Swadhin Das", "Raksha Sharma"], "categories": ["cs.CV", "cs.HC", "cs.LG"], "comment": null, "summary": "Remote sensing image captioning aims to generate descriptive text from remote\nsensing images, typically employing an encoder-decoder framework. In this\nsetup, a convolutional neural network (CNN) extracts feature representations\nfrom the input image, which then guide the decoder in a sequence-to-sequence\ncaption generation process. Although much research has focused on refining the\ndecoder, the quality of image representations from the encoder remains crucial\nfor accurate captioning. This paper introduces a novel approach that integrates\nfeatures from two distinct CNN based encoders, capturing complementary\ninformation to enhance caption generation. Additionally, we propose a weighted\naveraging technique to combine the outputs of all GRUs in the stacked decoder.\nFurthermore, a comparison-based beam search strategy is incorporated to refine\ncaption selection. The results demonstrate that our fusion-based approach,\nalong with the enhanced stacked decoder, significantly outperforms both the\ntransformer-based state-of-the-art model and other LSTM-based baselines."}
{"id": "2507.01790", "pdf": "https://arxiv.org/pdf/2507.01790.pdf", "abs": "https://arxiv.org/abs/2507.01790", "title": "How Do Vision-Language Models Process Conflicting Information Across Modalities?", "authors": ["Tianze Hua", "Tian Yun", "Ellie Pavlick"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "All code and resources are available at:\n  https://github.com/ethahtz/vlm_conflicting_info_processing", "summary": "AI models are increasingly required to be multimodal, integrating disparate\ninput streams into a coherent state representation on which subsequent\nbehaviors and actions can be based. This paper seeks to understand how such\nmodels behave when input streams present conflicting information. Focusing\nspecifically on vision-language models, we provide inconsistent inputs (e.g.,\nan image of a dog paired with the caption \"A photo of a cat\") and ask the model\nto report the information present in one of the specific modalities (e.g.,\n\"What does the caption say / What is in the image?\"). We find that models often\nfavor one modality over the other, e.g., reporting the image regardless of what\nthe caption says, but that different models differ in which modality they\nfavor. We find evidence that the behaviorally preferred modality is evident in\nthe internal representational structure of the model, and that specific\nattention heads can restructure the representations to favor one modality over\nthe other. Moreover, we find modality-agnostic \"router heads\" which appear to\npromote answers about the modality requested in the instruction, and which can\nbe manipulated or transferred in order to improve performance across datasets\nand modalities. Together, the work provides essential steps towards identifying\nand controlling if and how models detect and resolve conflicting signals within\ncomplex multimodal environments."}
{"id": "2505.02329", "pdf": "https://arxiv.org/pdf/2505.02329.pdf", "abs": "https://arxiv.org/abs/2505.02329", "title": "Regulating Algorithmic Management: A Multi-Stakeholder Study of Challenges in Aligning Software and the Law for Workplace Scheduling", "authors": ["Jonathan Lynn", "Rachel Y. Kim", "Sicun Gao", "Daniel Schneider", "Sachin S. Pandya", "Min Kyung Lee"], "categories": ["cs.CY", "cs.HC", "cs.SE"], "comment": "FAccT'25", "summary": "Algorithmic management (AM)'s impact on worker well-being has led to calls\nfor regulation. However, little is known about the effectiveness and challenges\nin real-world AM regulation across the regulatory process -- rule\noperationalization, software use, and enforcement. Our multi-stakeholder study\naddresses this gap within workplace scheduling, one of the few AM domains with\nimplemented regulations. We interviewed 38 stakeholders across the regulatory\nprocess: regulators, defense attorneys, worker advocates, managers, and\nworkers. Our findings suggest that the efficacy of AM regulation is influenced\nby: (i) institutional constraints that challenge efforts to encode law into AM\nsoftware, (ii) on-the-ground use of AM software that shapes its ability to\nfacilitate compliance, (iii) mismatches between software and regulatory\ncontexts that hinder enforcement, and (iv) unique concerns that software\nintroduces when used to regulate AM. These findings underscore the importance\nof a sociotechnical approach to AM regulation, which considers organizational\nand collaborative contexts alongside the inherent attributes of software. We\noffer future research directions and implications for technology policy and\ndesign."}
{"id": "2507.01802", "pdf": "https://arxiv.org/pdf/2507.01802.pdf", "abs": "https://arxiv.org/abs/2507.01802", "title": "The Anatomy of Evidence: An Investigation Into Explainable ICD Coding", "authors": ["Katharina Beckh", "Elisa Studeny", "Sujan Sai Gannamaneni", "Dario Antweiler", "Stefan Rüping"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025 Findings", "summary": "Automatic medical coding has the potential to ease documentation and billing\nprocesses. For this task, transparency plays an important role for medical\ncoders and regulatory bodies, which can be achieved using explainability\nmethods. However, the evaluation of these approaches has been mostly limited to\nshort text and binary settings due to a scarcity of annotated data. Recent\nefforts by Cheng et al. (2023) have introduced the MDACE dataset, which\nprovides a valuable resource containing code evidence in clinical records. In\nthis work, we conduct an in-depth analysis of the MDACE dataset and perform\nplausibility evaluation of current explainable medical coding systems from an\napplied perspective. With this, we contribute to a deeper understanding of\nautomatic medical coding and evidence extraction. Our findings reveal that\nground truth evidence aligns with code descriptions to a certain degree. An\ninvestigation into state-of-the-art approaches shows a high overlap with ground\ntruth evidence. We propose match measures and highlight success and failure\ncases. Based on our findings, we provide recommendations for developing and\nevaluating explainable medical coding systems."}
{"id": "2507.01810", "pdf": "https://arxiv.org/pdf/2507.01810.pdf", "abs": "https://arxiv.org/abs/2507.01810", "title": "Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes", "authors": ["Nikita Neveditsin", "Pawan Lingras", "Vijay Mago"], "categories": ["cs.CL", "cs.IR"], "comment": "To appear in the ACL Anthology", "summary": "We present a comparative analysis of the parseability of structured outputs\ngenerated by small language models for open attribute-value extraction from\nclinical notes. We evaluate three widely used serialization formats: JSON,\nYAML, and XML, and find that JSON consistently yields the highest parseability.\nStructural robustness improves with targeted prompting and larger models, but\ndeclines for longer documents and certain note types. Our error analysis\nidentifies recurring format-specific failure patterns. These findings offer\npractical guidance for selecting serialization formats and designing prompts\nwhen deploying language models in privacy-sensitive clinical settings."}
{"id": "2507.01844", "pdf": "https://arxiv.org/pdf/2507.01844.pdf", "abs": "https://arxiv.org/abs/2507.01844", "title": "Low-Perplexity LLM-Generated Sequences and Where To Find Them", "authors": ["Arthur Wuhrmann", "Anastasiia Kucherenko", "Andrei Kucharavy"], "categories": ["cs.CL", "cs.LG"], "comment": "Camera-ready version. Accepted to ACL 2025. 10 pages, 4 figures, 6\n  tables", "summary": "As Large Language Models (LLMs) become increasingly widespread, understanding\nhow specific training data shapes their outputs is crucial for transparency,\naccountability, privacy, and fairness. To explore how LLMs leverage and\nreplicate their training data, we introduce a systematic approach centered on\nanalyzing low-perplexity sequences - high-probability text spans generated by\nthe model. Our pipeline reliably extracts such long sequences across diverse\ntopics while avoiding degeneration, then traces them back to their sources in\nthe training data. Surprisingly, we find that a substantial portion of these\nlow-perplexity spans cannot be mapped to the corpus. For those that do match,\nwe quantify the distribution of occurrences across source documents,\nhighlighting the scope and nature of verbatim recall and paving a way toward\nbetter understanding of how LLMs training data impacts their behavior."}
{"id": "2507.01853", "pdf": "https://arxiv.org/pdf/2507.01853.pdf", "abs": "https://arxiv.org/abs/2507.01853", "title": "Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages", "authors": ["Samridhi Raj Sinha", "Rajvee Sheth", "Abhishek Upperwal", "Mayank Singh"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for evaluation frameworks that go beyond English centric benchmarks and\naddress the requirements of linguistically diverse regions such as India. We\npresent EKA-EVAL, a unified and production-ready evaluation framework that\nintegrates over 35 benchmarks, including 10 Indic-specific datasets, spanning\ncategories like reasoning, mathematics, tool use, long-context understanding,\nand reading comprehension. Compared to existing Indian language evaluation\ntools, EKA-EVAL offers broader benchmark coverage, with built-in support for\ndistributed inference, quantization, and multi-GPU usage. Our systematic\ncomparison positions EKA-EVAL as the first end-to-end, extensible evaluation\nsuite tailored for both global and Indic LLMs, significantly lowering the\nbarrier to multilingual benchmarking. The framework is open-source and publicly\navailable at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA\ninitiative (https://eka.soket.ai), which aims to scale up to over 100\nbenchmarks and establish a robust, multilingual evaluation ecosystem for LLMs."}
{"id": "2507.01872", "pdf": "https://arxiv.org/pdf/2507.01872.pdf", "abs": "https://arxiv.org/abs/2507.01872", "title": "DIY-MKG: An LLM-Based Polyglot Language Learning System", "authors": ["Kenan Tang", "Yanhong Li", "Yao Qin"], "categories": ["cs.CL"], "comment": "Submitted to EMNLP 2025 System Demonstration", "summary": "Existing language learning tools, even those powered by Large Language Models\n(LLMs), often lack support for polyglot learners to build linguistic\nconnections across vocabularies in multiple languages, provide limited\ncustomization for individual learning paces or needs, and suffer from\ndetrimental cognitive offloading. To address these limitations, we design\nDo-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system\nthat supports polyglot language learning. DIY-MKG allows the user to build\npersonalized vocabulary knowledge graphs, which are constructed by selective\nexpansion with related words suggested by an LLM. The system further enhances\nlearning through rich annotation capabilities and an adaptive review module\nthat leverages LLMs for dynamic, personalized quiz generation. In addition,\nDIY-MKG allows users to flag incorrect quiz questions, simultaneously\nincreasing user engagement and providing a feedback loop for prompt refinement.\nOur evaluation of LLM-based components in DIY-MKG shows that vocabulary\nexpansion is reliable and fair across multiple languages, and that the\ngenerated quizzes are highly accurate, validating the robustness of DIY-MKG."}
{"id": "2507.01887", "pdf": "https://arxiv.org/pdf/2507.01887.pdf", "abs": "https://arxiv.org/abs/2507.01887", "title": "MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants", "authors": ["Dongyi Ding", "Tiannan Wang", "Chenghao Zhu", "Meiling Tao", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Large language models (LLMs) excel at reasoning tasks requiring long thought\nsequences for planning, reflection, and refinement. However, their substantial\nmodel size and high computational demands are impractical for widespread\ndeployment. Yet, small language models (SLMs) often struggle to learn long-form\nCoT reasoning due to their limited capacity, a phenomenon we refer to as the\n\"SLMs Learnability Gap\". To address this, we introduce\n\\textbf{Mi}d-\\textbf{Co}T \\textbf{T}eacher \\textbf{A}ssistant Distillation\n(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA\nemploys intermediate-sized models as teacher assistants and utilizes\nintermediate-length CoT sequences to bridge both the capacity and reasoning\nlength gaps. Our experiments on downstream tasks demonstrate that although SLMs\ndistilled from large teachers can perform poorly, by applying MiCoTA, they\nachieve significant improvements in reasoning performance. Specifically,\nQwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and\n3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and\nGSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform\na quantitative experiment demonstrating that our method produces data more\nclosely aligned with base SLM distributions. Our insights pave the way for\nfuture research into long-CoT data distillation for SLMs."}
{"id": "2507.01900", "pdf": "https://arxiv.org/pdf/2507.01900.pdf", "abs": "https://arxiv.org/abs/2507.01900", "title": "High-Layer Attention Pruning with Rescaling", "authors": ["Songtao Liu", "Peng Liu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Pruning is a highly effective approach for compressing large language models\n(LLMs), significantly reducing inference latency. However, conventional\ntraining-free structured pruning methods often employ a heuristic metric that\nindiscriminately removes some attention heads across all pruning layers,\nwithout considering their positions within the network architecture. In this\nwork, we propose a novel pruning algorithm that strategically prunes attention\nheads in the model's higher layers. Since the removal of attention heads can\nalter the magnitude of token representations, we introduce an adaptive\nrescaling parameter that calibrates the representation scale post-pruning to\ncounteract this effect. We conduct comprehensive experiments on a wide range of\nLLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our\nevaluation includes both generation and discriminative tasks across 27\ndatasets. The results consistently demonstrate that our method outperforms\nexisting structured pruning methods. This improvement is particularly notable\nin generation tasks, where our approach significantly outperforms existing\nbaselines."}
{"id": "2507.01903", "pdf": "https://arxiv.org/pdf/2507.01903.pdf", "abs": "https://arxiv.org/abs/2507.01903", "title": "AI4Research: A Survey of Artificial Intelligence for Scientific Research", "authors": ["Qiguang Chen", "Mingda Yang", "Libo Qin", "Jinhao Liu", "Zheng Yan", "Jiannan Guan", "Dengyun Peng", "Yiyan Ji", "Hanjing Li", "Mengkang Hu", "Yimeng Zhang", "Yihao Liang", "Yuhang Zhou", "Jiaqi Wang", "Zhi Chen", "Wanxiang Che"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Recent advancements in artificial intelligence (AI), particularly in large\nlanguage models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated\nremarkable capabilities in complex domains such as logical reasoning and\nexperimental coding. Motivated by these advancements, numerous studies have\nexplored the application of AI in the innovation process, particularly in the\ncontext of scientific research. These AI technologies primarily aim to develop\nsystems that can autonomously conduct research processes across a wide range of\nscientific disciplines. Despite these significant strides, a comprehensive\nsurvey on AI for Research (AI4Research) remains absent, which hampers our\nunderstanding and impedes further development in this field. To address this\ngap, we present a comprehensive survey and offer a unified perspective on\nAI4Research. Specifically, the main contributions of our work are as follows:\n(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify\nfive mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key\nresearch gaps and highlight promising future directions, focusing on the rigor\nand scalability of automated experiments, as well as the societal impact. (3)\nAbundant applications and resources: Finally, we compile a wealth of resources,\nincluding relevant multidisciplinary applications, data corpora, and tools. We\nhope our work will provide the research community with quick access to these\nresources and stimulate innovative breakthroughs in AI4Research."}
{"id": "2507.01915", "pdf": "https://arxiv.org/pdf/2507.01915.pdf", "abs": "https://arxiv.org/abs/2507.01915", "title": "Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models", "authors": ["Chengao Li", "Hanyu Zhang", "Yunkun Xu", "Hongyan Xue", "Xiang Ao", "Qing He"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "19 pages, 3 figures. Accepted by ACL 2025 (main)", "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful\ntechnique for aligning large language models (LLMs) with human preferences.\nHowever, effectively aligning LLMs with diverse human preferences remains a\nsignificant challenge, particularly when they are conflict. To address this\nissue, we frame human value alignment as a multi-objective optimization\nproblem, aiming to maximize a set of potentially conflicting objectives. We\nintroduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning\nparadigm that employs multiple-gradient descent to align LLMs with diverse\npreference distributions. GAPO adaptively rescales the gradients for each\nobjective to determine an update direction that optimally balances the\ntrade-offs between objectives. Additionally, we introduce P-GAPO, which\nincorporates user preferences across different objectives and achieves Pareto\nsolutions that better align with the user's specific needs. Our theoretical\nanalysis demonstrates that GAPO converges towards a Pareto optimal solution for\nmultiple objectives. Empirical results on Mistral-7B show that GAPO outperforms\ncurrent state-of-the-art methods, achieving superior performance in both\nhelpfulness and harmlessness."}
{"id": "2507.01921", "pdf": "https://arxiv.org/pdf/2507.01921.pdf", "abs": "https://arxiv.org/abs/2507.01921", "title": "NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks", "authors": ["Yang Li", "Youssef Emad", "Karthik Padthe", "Jack Lanchantin", "Weizhe Yuan", "Thao Nguyen", "Jason Weston", "Shang-Wen Li", "Dong Wang", "Ilia Kulikov", "Xian Li"], "categories": ["cs.CL"], "comment": null, "summary": "Recent work has shown that distilling reasoning traces from a larger teacher\nmodel via supervised finetuning outperforms reinforcement learning with the\nsmaller student model alone (Guo et al. 2025). However, there has not been a\nsystematic study of what kind of reasoning demonstrations from the teacher are\nmost effective in improving the student model's reasoning capabilities. In this\nwork we curate high-quality \"NaturalThoughts\" by selecting reasoning traces\nfrom a strong teacher model based on a large pool of questions from\nNaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of\nfactors that affect distilling reasoning capabilities, in terms of sample\nefficiency and scalability for general reasoning tasks. We observe that simply\nscaling up data size with random sampling is a strong baseline with steady\nperformance gains. Further, we find that selecting difficult examples that\nrequire more diverse reasoning strategies is more sample-efficient to transfer\nthe teacher model's reasoning skills. Evaluated on both Llama and Qwen models,\ntraining with NaturalThoughts outperforms existing reasoning datasets such as\nOpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including\nGPQA-Diamond, MMLU-Pro and SuperGPQA."}
{"id": "2507.01923", "pdf": "https://arxiv.org/pdf/2507.01923.pdf", "abs": "https://arxiv.org/abs/2507.01923", "title": "Decision-oriented Text Evaluation", "authors": ["Yu-Shiang Huang", "Chuan-Ju Wang", "Chung-Chi Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Natural language generation (NLG) is increasingly deployed in high-stakes\ndomains, yet common intrinsic evaluation methods, such as n-gram overlap or\nsentence plausibility, weakly correlate with actual decision-making efficacy.\nWe propose a decision-oriented framework for evaluating generated text by\ndirectly measuring its influence on human and large language model (LLM)\ndecision outcomes. Using market digest texts--including objective morning\nsummaries and subjective closing-bell analyses--as test cases, we assess\ndecision quality based on the financial performance of trades executed by human\ninvestors and autonomous LLM agents informed exclusively by these texts. Our\nfindings reveal that neither humans nor LLM agents consistently surpass random\nperformance when relying solely on summaries. However, richer analytical\ncommentaries enable collaborative human-LLM teams to outperform individual\nhuman or agent baselines significantly. Our approach underscores the importance\nof evaluating generated text by its ability to facilitate synergistic\ndecision-making between humans and LLMs, highlighting critical limitations of\ntraditional intrinsic metrics."}
{"id": "2507.01931", "pdf": "https://arxiv.org/pdf/2507.01931.pdf", "abs": "https://arxiv.org/abs/2507.01931", "title": "Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla", "authors": ["Md Sazzadul Islam Ridoy", "Sumi Akter", "Md. Aminur Rahman"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "In recent years, neural models trained on large multilingual text and speech\ndatasets have shown great potential for supporting low-resource languages. This\nstudy investigates the performances of two state-of-the-art Automatic Speech\nRecognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's\nWav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments\nusing two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to\nevaluate model performances. Through systematic fine-tuning and hyperparameter\noptimization, including learning rate, epochs, and model checkpoint selection,\nwe have compared the models based on Word Error Rate (WER), Character Error\nRate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model\noutperformed Whisper across all key evaluation metrics, demonstrated superior\nperformance while requiring fewer computational resources, and offered valuable\ninsights to develop robust speech recognition systems in low-resource\nlinguistic settings."}
{"id": "2507.01936", "pdf": "https://arxiv.org/pdf/2507.01936.pdf", "abs": "https://arxiv.org/abs/2507.01936", "title": "The Thin Line Between Comprehension and Persuasion in LLMs", "authors": ["Adrian de Wynter", "Tangming Yuan"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) are excellent at maintaining high-level,\nconvincing dialogues. They are being fast deployed as chatbots and evaluators\nin sensitive areas, such as peer review and mental health applications. This,\nalong with the disparate accounts on their reasoning capabilities, calls for a\ncloser examination of LLMs and their comprehension of dialogue. In this work we\nbegin by evaluating LLMs' ability to maintain a debate--one of the purest yet\nmost complex forms of human communication. Then we measure how this capability\nrelates to their understanding of what is being talked about, namely, their\ncomprehension of dialogical structures and the pragmatic context. We find that\nLLMs are capable of maintaining coherent, persuasive debates, often swaying the\nbeliefs of participants and audiences alike. We also note that awareness or\nsuspicion of AI involvement encourage people to be more critical of the\narguments made. When polling LLMs on their comprehension of deeper structures\nof dialogue, however, they cannot demonstrate said understanding. Our findings\ntie the shortcomings of LLMs-as-evaluators to their (in)ability to understand\nthe context. More broadly, for the field of argumentation theory we posit that,\nif an agent can convincingly maintain a dialogue, it is not necessary for it to\nknow what it is talking about. Hence, the modelling of pragmatic context and\ncoherence are secondary to effectiveness."}
{"id": "2507.01021", "pdf": "https://arxiv.org/pdf/2507.01021.pdf", "abs": "https://arxiv.org/abs/2507.01021", "title": "Scalable Offline ASR for Command-Style Dictation in Courtrooms", "authors": ["Kumarmanas Nethil", "Vaibhav Mishra", "Kriti Anandan", "Kavya Manohar"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to Interspeech 2025 Show & Tell", "summary": "We propose an open-source framework for Command-style dictation that\naddresses the gap between resource-intensive Online systems and high-latency\nBatch processing. Our approach uses Voice Activity Detection (VAD) to segment\naudio and transcribes these segments in parallel using Whisper models, enabling\nefficient multiplexing across audios. Unlike proprietary systems like\nSuperWhisper, this framework is also compatible with most ASR architectures,\nincluding widely used CTC-based models. Our multiplexing technique maximizes\ncompute utilization in real-world settings, as demonstrated by its deployment\nin around 15% of India's courtrooms. Evaluations on live data show consistent\nlatency reduction as user concurrency increases, compared to sequential batch\nprocessing. The live demonstration will showcase our open-sourced\nimplementation and allow attendees to interact with it in real-time."}
{"id": "2507.01029", "pdf": "https://arxiv.org/pdf/2507.01029.pdf", "abs": "https://arxiv.org/abs/2507.01029", "title": "PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning", "authors": ["Junjie Zhou", "Yingli Zuo", "Shichang Feng", "Peng Wan", "Qi Zhu", "Daoqiang Zhang", "Wei Shao"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "With the development of generative artificial intelligence and instruction\ntuning techniques, multimodal large language models (MLLMs) have made\nimpressive progress on general reasoning tasks. Benefiting from the\nchain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning\nproblem step-by-step. However, existing MLLMs still face significant challenges\nwhen applied to pathology visual reasoning tasks: (1) LLMs often underperforms\nbecause they lack domain-specific information, which can lead to model\nhallucinations. (2) The additional reasoning steps in CoT may introduce errors,\nleading to the divergence of answers. To address these limitations, we propose\nPathCoT, a novel zero-shot CoT prompting method which integrates the pathology\nexpert-knowledge into the reasoning process of MLLMs and incorporates\nself-evaluation to mitigate divergence of answers. Specifically, PathCoT guides\nthe MLLM with prior knowledge to perform as pathology experts, and provides\ncomprehensive analysis of the image with their domain-specific knowledge. By\nincorporating the experts' knowledge, PathCoT can obtain the answers with CoT\nreasoning. Furthermore, PathCoT incorporates a self-evaluation step that\nassesses both the results generated directly by MLLMs and those derived through\nCoT, finally determining the reliable answer. The experimental results on the\nPathMMU dataset demonstrate the effectiveness of our method on pathology visual\nunderstanding and reasoning."}
{"id": "2507.01042", "pdf": "https://arxiv.org/pdf/2507.01042.pdf", "abs": "https://arxiv.org/abs/2507.01042", "title": "Can Argus Judge Them All? Comparing VLMs Across Domains", "authors": ["Harsh Joshi", "Gautam Siddharth Kashyap", "Rafiq Ali", "Ebad Shabbir", "Niharika Jain", "Sarthak Jain", "Jiechao Gao", "Usman Naseem"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Vision-Language Models (VLMs) are advancing multimodal AI, yet their\nperformance consistency across tasks is underexamined. We benchmark CLIP, BLIP,\nand LXMERT across diverse datasets spanning retrieval, captioning, and\nreasoning. Our evaluation includes task accuracy, generation quality,\nefficiency, and a novel Cross-Dataset Consistency (CDC) metric. CLIP shows\nstrongest generalization (CDC: 0.92), BLIP excels on curated data, and LXMERT\nleads in structured reasoning. These results expose trade-offs between\ngeneralization and specialization, informing industrial deployment of VLMs and\nguiding development toward robust, task-flexible architectures."}
{"id": "2507.01049", "pdf": "https://arxiv.org/pdf/2507.01049.pdf", "abs": "https://arxiv.org/abs/2507.01049", "title": "Cohort Retrieval using Dense Passage Retrieval", "authors": ["Pranav Jadhav"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Patient cohort retrieval is a pivotal task in medical research and clinical\npractice, enabling the identification of specific patient groups from extensive\nelectronic health records (EHRs). In this work, we address the challenge of\ncohort retrieval in the echocardiography domain by applying Dense Passage\nRetrieval (DPR), a prominent methodology in semantic search. We propose a\nsystematic approach to transform an echocardiographic EHR dataset of\nunstructured nature into a Query-Passage dataset, framing the problem as a\nCohort Retrieval task. Additionally, we design and implement evaluation metrics\ninspired by real-world clinical scenarios to rigorously test the models across\ndiverse retrieval tasks. Furthermore, we present a custom-trained DPR embedding\nmodel that demonstrates superior performance compared to traditional and\noff-the-shelf SOTA methods.To our knowledge, this is the first work to apply\nDPR for patient cohort retrieval in the echocardiography domain, establishing a\nframework that can be adapted to other medical domains."}
{"id": "2507.01050", "pdf": "https://arxiv.org/pdf/2507.01050.pdf", "abs": "https://arxiv.org/abs/2507.01050", "title": "Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization", "authors": ["Jing Yu", "Yibo Zhao", "Jiapeng Zhu", "Wenming Shao", "Bo Pang", "Zhao Zhang", "Xiang Li"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The widespread dissemination of toxic content on social media poses a serious\nthreat to both online environments and public discourse, highlighting the\nurgent need for detoxification methods that effectively remove toxicity while\npreserving the original semantics. However, existing approaches often struggle\nto simultaneously achieve strong detoxification performance, semantic\npreservation, and robustness to out-of-distribution data. Moreover, they\ntypically rely on costly, manually annotated parallel corpora while showing\npoor data efficiency. To address these challenges, we propose a two-stage\ntraining framework that jointly optimizes for data efficiency, semantic\npreservation, and model generalization. We first perform supervised fine-tuning\non a small set of high-quality, filtered parallel data to establish a strong\ninitialization. Then, we leverage unlabeled toxic inputs and a custom-designed\nreward model to train the LLM using Group Relative Policy Optimization.\nExperimental results demonstrate that our method effectively mitigates the\ntrade-offs faced by previous work, achieving state-of-the-art performance with\nimproved generalization and significantly reduced dependence on annotated data.\nOur code is available at:\nhttps://anonymous.4open.science/r/Detoxification-of-Text-725F/"}
{"id": "2507.01059", "pdf": "https://arxiv.org/pdf/2507.01059.pdf", "abs": "https://arxiv.org/abs/2507.01059", "title": "Automated Vehicles Should be Connected with Natural Language", "authors": ["Xiangbo Gao", "Keshu Wu", "Hao Zhang", "Kexin Tian", "Yang Zhou", "Zhengzhong Tu"], "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CV", "cs.RO"], "comment": null, "summary": "Multi-agent collaborative driving promises improvements in traffic safety and\nefficiency through collective perception and decision making. However, existing\ncommunication media -- including raw sensor data, neural network features, and\nperception results -- suffer limitations in bandwidth efficiency, information\ncompleteness, and agent interoperability. Moreover, traditional approaches have\nlargely ignored decision-level fusion, neglecting critical dimensions of\ncollaborative driving. In this paper we argue that addressing these challenges\nrequires a transition from purely perception-oriented data exchanges to\nexplicit intent and reasoning communication using natural language. Natural\nlanguage balances semantic density and communication bandwidth, adapts flexibly\nto real-time conditions, and bridges heterogeneous agent platforms. By enabling\nthe direct communication of intentions, rationales, and decisions, it\ntransforms collaborative driving from reactive perception-data sharing into\nproactive coordination, advancing safety, efficiency, and transparency in\nintelligent transportation systems."}
{"id": "2507.01431", "pdf": "https://arxiv.org/pdf/2507.01431.pdf", "abs": "https://arxiv.org/abs/2507.01431", "title": "Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading", "authors": ["Yoonseok Yang", "Minjune Kim", "Marlon Rondinelli", "Keren Shao"], "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": "7 pages, 5 figues, 1 table", "summary": "Grading handwritten, open-ended responses remains a major bottleneck in large\nuniversity STEM courses. We introduce Pensieve (https://www.pensieve.co), an\nAI-assisted grading platform that leverages large language models (LLMs) to\ntranscribe and evaluate student work, providing instructors with rubric-aligned\nscores, transcriptions, and confidence ratings. Unlike prior tools that focus\nnarrowly on specific tasks like transcription or rubric generation, Pensieve\nsupports the entire grading pipeline-from scanned student submissions to final\nfeedback-within a human-in-the-loop interface.\n  Pensieve has been deployed in real-world courses at over 20 institutions and\nhas graded more than 300,000 student responses. We present system details and\nempirical results across four core STEM disciplines: Computer Science,\nMathematics, Physics, and Chemistry. Our findings show that Pensieve reduces\ngrading time by an average of 65%, while maintaining a 95.4% agreement rate\nwith instructor-assigned grades for high-confidence predictions."}
{"id": "2507.01504", "pdf": "https://arxiv.org/pdf/2507.01504.pdf", "abs": "https://arxiv.org/abs/2507.01504", "title": "Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence", "authors": ["Robert Aufschläger", "Youssef Shoeb", "Azarm Nowzad", "Michael Heigl", "Fabian Bally", "Martin Schramm"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "accepted for publication at the 2025 IEEE 28th International\n  Conference on Intelligent Transportation Systems (ITSC 2025), taking place\n  during November 18-21, 2025 in Gold Coast, Australia", "summary": "The collection and release of street-level recordings as Open Data play a\nvital role in advancing autonomous driving systems and AI research. However,\nthese datasets pose significant privacy risks, particularly for pedestrians,\ndue to the presence of Personally Identifiable Information (PII) that extends\nbeyond biometric traits such as faces. In this paper, we present cRID, a novel\ncross-modal framework combining Large Vision-Language Models, Graph Attention\nNetworks, and representation learning to detect textual describable clues of\nPII and enhance person re-identification (Re-ID). Our approach focuses on\nidentifying and leveraging interpretable features, enabling the detection of\nsemantically meaningful PII beyond low-level appearance cues. We conduct a\nsystematic evaluation of PII presence in person image datasets. Our experiments\nshow improved performance in practical cross-dataset Re-ID scenarios, notably\nfrom Market-1501 to CUHK03-np (detected), highlighting the framework's\npractical utility. Code is available at https://github.com/RAufschlaeger/cRID."}
{"id": "2507.01548", "pdf": "https://arxiv.org/pdf/2507.01548.pdf", "abs": "https://arxiv.org/abs/2507.01548", "title": "Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants", "authors": ["Wen Zhan", "Ziqun Hua", "Peiyue Lin", "Yunfei Chen"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "A version of this manuscript has been submitted to the [IASDR 2025\n  Conference](https://iasdr2025.org/) and is currently under review", "summary": "This paper explores how older adults, particularly aging migrants in urban\nChina, can engage AI-assisted co-creation to express personal narratives that\nare often fragmented, underrepresented, or difficult to verbalize. Through a\npilot workshop combining oral storytelling and the symbolic reconstruction of\nHanzi, participants shared memories of migration and recreated new character\nforms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),\ntogether with physical materials. Supported by human facilitation and a soft AI\npresence, participants transformed lived experience into visual and tactile\nexpressions without requiring digital literacy. This approach offers new\nperspectives on human-AI collaboration and aging by repositioning AI not as a\ncontent producer but as a supportive mechanism, and by supporting narrative\nagency within sociotechnical systems."}
{"id": "2507.01551", "pdf": "https://arxiv.org/pdf/2507.01551.pdf", "abs": "https://arxiv.org/abs/2507.01551", "title": "Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning", "authors": ["Wu Fei", "Hao Kong", "Shuxian Liang", "Yang Lin", "Yibo Yang", "Jing Tang", "Lei Chen", "Xiansheng Hua"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Process Reinforcement Learning~(PRL) has demonstrated considerable potential\nin enhancing the reasoning capabilities of Large Language Models~(LLMs).\nHowever, introducing additional process reward models incurs substantial\ncomputational overhead, and there is no unified theoretical framework for\nprocess-level advantage estimation. To bridge this gap, we propose\n\\textbf{S}elf-Guided \\textbf{P}rocess \\textbf{R}eward\n\\textbf{O}ptimization~(\\textbf{SPRO}), a novel framework that enables\nprocess-aware RL through two key innovations: (1) we first theoretically\ndemonstrate that process rewards can be derived intrinsically from the policy\nmodel itself, and (2) we introduce well-defined cumulative process rewards and\n\\textbf{M}asked \\textbf{S}tep \\textbf{A}dvantage (\\textbf{MSA}), which\nfacilitates rigorous step-wise action advantage estimation within shared-prompt\nsampling groups. Our experimental results demonstrate that SPRO outperforms\nvaniila GRPO with 3.4x higher training efficiency and a 17.5\\% test accuracy\nimprovement. Furthermore, SPRO maintains a stable and elevated policy entropy\nthroughout training while reducing the average response length by approximately\n$1/3$, evidencing sufficient exploration and prevention of reward hacking.\nNotably, SPRO incurs no additional computational overhead compared to\noutcome-supervised RL methods such as GRPO, which benefit industrial\nimplementation."}
{"id": "2507.01597", "pdf": "https://arxiv.org/pdf/2507.01597.pdf", "abs": "https://arxiv.org/abs/2507.01597", "title": "T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning", "authors": ["Yuehang Si", "Zefan Zeng", "Jincai Huang", "Qing Cheng"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Temporal Knowledge Graph (TKG) is an efficient method for describing the\ndynamic development of facts along a timeline. Most research on TKG reasoning\n(TKGR) focuses on modelling the repetition of global facts and designing\npatterns of local historical facts. However, they face two significant\nchallenges: inadequate modeling of the event distribution shift between\ntraining and test samples, and reliance on random entity substitution for\ngenerating negative samples, which often results in low-quality sampling. To\nthis end, we propose a novel distributional feature modeling approach for\ntraining TKGR models, Test-Time Training-guided Distribution shift Modelling\n(T3DM), to adjust the model based on distribution shift and ensure the global\nconsistency of model reasoning. In addition, we design a negative-sampling\nstrategy to generate higher-quality negative quadruples based on adversarial\ntraining. Extensive experiments show that T3DM provides better and more robust\nresults than the state-of-the-art baselines in most cases."}
{"id": "2507.01599", "pdf": "https://arxiv.org/pdf/2507.01599.pdf", "abs": "https://arxiv.org/abs/2507.01599", "title": "Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems", "authors": ["Zhaoyan Sun", "Jiayi Wang", "Xinyang Zhao", "Jiachi Wang", "Guoliang Li"], "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Traditional Data+AI systems utilize data-driven techniques to optimize\nperformance, but they rely heavily on human experts to orchestrate system\npipelines, enabling them to adapt to changes in data, queries, tasks, and\nenvironments. For instance, while there are numerous data science tools\navailable, developing a pipeline planning system to coordinate these tools\nremains challenging. This difficulty arises because existing Data+AI systems\nhave limited capabilities in semantic understanding, reasoning, and planning.\nFortunately, we have witnessed the success of large language models (LLMs) in\nenhancing semantic understanding, reasoning, and planning abilities. It is\ncrucial to incorporate LLM techniques to revolutionize data systems for\norchestrating Data+AI applications effectively.\n  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive\narchitecture designed to orchestrate Data+AI ecosystems, which focuses on\ntackling data-related tasks by integrating knowledge comprehension, reasoning,\nand planning capabilities. We delve into the challenges involved in designing\ndata agents, such as understanding data/queries/environments/tools,\norchestrating pipelines/workflows, optimizing and executing pipelines, and\nfostering pipeline self-reflection. Furthermore, we present examples of data\nagent systems, including a data science agent, data analytics agents (such as\nunstructured data analytics agent, semantic structured data analytics agent,\ndata lake analytics agent, and multi-modal data analytics agent), and a\ndatabase administrator (DBA) agent. We also outline several open challenges\nassociated with designing data agent systems."}
{"id": "2507.01679", "pdf": "https://arxiv.org/pdf/2507.01679.pdf", "abs": "https://arxiv.org/abs/2507.01679", "title": "Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling", "authors": ["Zeyu Huang", "Tianhao Cheng", "Zihan Qiu", "Zili Wang", "Yinghui Xu", "Edoardo M. Ponti", "Ivan Titov"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Existing post-training techniques for large language models are broadly\ncategorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning\n(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking\ndemonstration data but can lead to problematic generalization as a form of\nbehavior cloning. Conversely, RFT can significantly enhance a model's\nperformance but is prone to learn unexpected behaviors, and its performance is\nhighly sensitive to the initial policy. In this paper, we propose a unified\nview of these methods and introduce Prefix-RFT, a hybrid approach that\nsynergizes learning from both demonstration and exploration. Using mathematical\nreasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is\nboth simple and effective. It not only surpasses the performance of standalone\nSFT and RFT but also outperforms parallel mixed-policy RFT methods. A key\nadvantage is its seamless integration into existing open-source frameworks,\nrequiring only minimal modifications to the standard RFT pipeline. Our analysis\nhighlights the complementary nature of SFT and RFT, and validates that\nPrefix-RFT effectively harmonizes these two learning paradigms. Furthermore,\nablation studies confirm the method's robustness to variations in the quality\nand quantity of demonstration data. We hope this work offers a new perspective\non LLM post-training, suggesting that a unified paradigm that judiciously\nintegrates demonstration and exploration could be a promising direction for\nfuture research."}
{"id": "2507.01735", "pdf": "https://arxiv.org/pdf/2507.01735.pdf", "abs": "https://arxiv.org/abs/2507.01735", "title": "ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving", "authors": ["Kai Chen", "Ruiyuan Gao", "Lanqing Hong", "Hang Xu", "Xu Jia", "Holger Caesar", "Dengxin Dai", "Bingbing Liu", "Dzmitry Tsishkou", "Songcen Xu", "Chunjing Xu", "Qiang Xu", "Huchuan Lu", "Dit-Yan Yeung"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "ECCV 2024. Workshop page: https://coda-dataset.github.io/w-coda2024/", "summary": "In this paper, we present details of the 1st W-CODA workshop, held in\nconjunction with the ECCV 2024. W-CODA aims to explore next-generation\nsolutions for autonomous driving corner cases, empowered by state-of-the-art\nmultimodal perception and comprehension techniques. 5 Speakers from both\nacademia and industry are invited to share their latest progress and opinions.\nWe collect research papers and hold a dual-track challenge, including both\ncorner case scene understanding and generation. As the pioneering effort, we\nwill continuously bridge the gap between frontier autonomous driving techniques\nand fully intelligent, reliable self-driving agents robust towards corner\ncases."}
{"id": "2507.01752", "pdf": "https://arxiv.org/pdf/2507.01752.pdf", "abs": "https://arxiv.org/abs/2507.01752", "title": "Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training", "authors": ["Ismail Labiad", "Mathurin Videau", "Matthieu Kowalski", "Marc Schoenauer", "Alessandro Leite", "Julia Kempe", "Olivier Teytaud"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Gradient-based optimization is the workhorse of deep learning, offering\nefficient and scalable training via backpropagation. However, its reliance on\nlarge volumes of labeled data raises privacy and security concerns such as\nsusceptibility to data poisoning attacks and the risk of overfitting. In\ncontrast, black box optimization methods, which treat the model as an opaque\nfunction, relying solely on function evaluations to guide optimization, offer a\npromising alternative in scenarios where data access is restricted, adversarial\nrisks are high, or overfitting is a concern. However, black box methods also\npose significant challenges, including poor scalability to high-dimensional\nparameter spaces, as prevalent in large language models (LLMs), and high\ncomputational costs due to reliance on numerous model evaluations. This paper\nintroduces BBoxER, an evolutionary black-box method for LLM post-training that\ninduces an information bottleneck via implicit compression of the training\ndata. Leveraging the tractability of information flow, we provide strong\ntheoretical bounds on generalization, differential privacy, susceptibility to\ndata poisoning attacks, and robustness to extraction attacks. BBoxER operates\non top of pre-trained LLMs, offering a lightweight and modular enhancement\nsuitable for deployment in restricted or privacy-sensitive environments, in\naddition to non-vacuous generalization guarantees. In experiments with LLMs, we\ndemonstrate empirically that Retrofitting methods are able to learn, showing\nhow a few iterations of BBoxER improve performance and generalize well on a\nbenchmark of reasoning datasets. This positions BBoxER as an attractive add-on\non top of gradient-based optimization."}
{"id": "2507.01806", "pdf": "https://arxiv.org/pdf/2507.01806.pdf", "abs": "https://arxiv.org/abs/2507.01806", "title": "LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs", "authors": ["Reza Arabpour", "Haitz Sáez de Ocáriz Borde", "Anastasis Kratsios"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "5-page main paper (excluding references) + 11-page appendix, 3\n  tables, 1 figure. Accepted to ICML 2025 Workshop on Efficient Systems for\n  Foundation Models", "summary": "Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language\nModels (LLMs) by enabling parameter-efficient updates. However, their\nwidespread adoption remains limited by the reliance on GPU-based training. In\nthis work, we propose a theoretically grounded approach to LoRA fine-tuning\ndesigned specifically for users with limited computational resources,\nparticularly those restricted to standard laptop CPUs. Our method learns a\nmeta-operator that maps any input dataset, represented as a probability\ndistribution, to a set of LoRA weights by leveraging a large bank of\npre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of\nperforming new gradient-based updates, our pipeline constructs adapters via\nlightweight combinations of existing LoRAs directly on CPU. While the resulting\nadapters do not match the performance of GPU-trained counterparts, they\nconsistently outperform the base Mistral model on downstream tasks, offering a\npractical and accessible alternative to traditional GPU-based fine-tuning."}
{"id": "2507.01951", "pdf": "https://arxiv.org/pdf/2507.01951.pdf", "abs": "https://arxiv.org/abs/2507.01951", "title": "Test-Time Scaling with Reflective Generative Model", "authors": ["Zixiao Wang", "Yuxin Wang", "Xiaorui Wang", "Mengting Xing", "Jie Gao", "Jianjun Xu", "Guangcan Liu", "Chenhui Jin", "Zhuo Wang", "Shengzhuo Zhang", "Hongtao Xie"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We introduce our first reflective generative model MetaStone-S1, which\nobtains OpenAI o3's performance via the self-supervised process reward model\n(SPRM). Through sharing the backbone network and using task-specific heads for\nnext token prediction and process scoring respectively, SPRM successfully\nintegrates the policy model and process reward model(PRM) into a unified\ninterface without extra process annotation, reducing over 99% PRM parameters\nfor efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable\nfor test time scaling (TTS), and we provide three reasoning effort modes (low,\nmedium, and high), based on the controllable thinking length. Moreover, we\nempirically establish a scaling law that reveals the relationship between total\nthinking computation and TTS performance. Experiments demonstrate that our\nMetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with\nonly 32B parameter size. To support the research community, we have\nopen-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1."}
{"id": "2404.16369", "pdf": "https://arxiv.org/pdf/2404.16369.pdf", "abs": "https://arxiv.org/abs/2404.16369", "title": "Don't Say No: Jailbreaking LLM by Suppressing Refusal", "authors": ["Yukai Zhou", "Jian Lou", "Zhijie Huang", "Zhan Qin", "Yibei Yang", "Wenjie Wang"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Findings", "summary": "Ensuring the safety alignment of Large Language Models (LLMs) is critical for\ngenerating responses consistent with human values. However, LLMs remain\nvulnerable to jailbreaking attacks, where carefully crafted prompts manipulate\nthem into producing toxic content. One category of such attacks reformulates\nthe task as an optimization problem, aiming to elicit affirmative responses\nfrom the LLM. However, these methods heavily rely on predefined objectionable\nbehaviors, limiting their effectiveness and adaptability to diverse harmful\nqueries. In this study, we first identify why the vanilla target loss is\nsuboptimal and then propose enhancements to the loss objective. We introduce\nDSN (Don't Say No) attack, which combines a cosine decay schedule method with\nrefusal suppression to achieve higher success rates. Extensive experiments\ndemonstrate that DSN outperforms baseline attacks and achieves state-of-the-art\nattack success rates (ASR). DSN also shows strong universality and\ntransferability to unseen datasets and black-box models."}
{"id": "2405.13012", "pdf": "https://arxiv.org/pdf/2405.13012.pdf", "abs": "https://arxiv.org/abs/2405.13012", "title": "Divergent Creativity in Humans and Large Language Models", "authors": ["Antoine Bellemare-Pepin", "François Lespinasse", "Philipp Thölke", "Yann Harel", "Kory Mathewson", "Jay A. Olson", "Yoshua Bengio", "Karim Jerbi"], "categories": ["cs.CL", "cs.AI"], "comment": "First two and last listed authors are corresponding authors. The\n  first two listed authors contributed equally to this work", "summary": "The recent surge of Large Language Models (LLMs) has led to claims that they\nare approaching a level of creativity akin to human capabilities. This idea has\nsparked a blend of excitement and apprehension. However, a critical piece that\nhas been missing in this discourse is a systematic evaluation of LLMs' semantic\ndiversity, particularly in comparison to human divergent thinking. To bridge\nthis gap, we leverage recent advances in computational creativity to analyze\nsemantic divergence in both state-of-the-art LLMs and a substantial dataset of\n100,000 humans. We found evidence that LLMs can surpass average human\nperformance on the Divergent Association Task, and approach human creative\nwriting abilities, though they fall short of the typical performance of highly\ncreative humans. Notably, even the top performing LLMs are still largely\nsurpassed by highly creative individuals, underscoring a ceiling that current\nLLMs still fail to surpass. Our human-machine benchmarking framework addresses\nthe polemic surrounding the imminent replacement of human creative labour by\nAI, disentangling the quality of the respective creative linguistic outputs\nusing established objective measures. While prompting deeper exploration of the\ndistinctive elements of human inventive thought compared to those of AI\nsystems, we lay out a series of techniques to improve their outputs with\nrespect to semantic diversity, such as prompt design and hyper-parameter\ntuning."}
{"id": "2408.02544", "pdf": "https://arxiv.org/pdf/2408.02544.pdf", "abs": "https://arxiv.org/abs/2408.02544", "title": "Caution for the Environment: Multimodal Agents are Susceptible to Environmental Distractions", "authors": ["Xinbei Ma", "Yiting Wang", "Yao Yao", "Tongxin Yuan", "Aston Zhang", "Zhuosheng Zhang", "Hai Zhao"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "This paper investigates the faithfulness of multimodal large language model\n(MLLM) agents in a graphical user interface (GUI) environment, aiming to\naddress the research question of whether multimodal GUI agents can be\ndistracted by environmental context. A general scenario is proposed where both\nthe user and the agent are benign, and the environment, while not malicious,\ncontains unrelated content. A wide range of MLLMs are evaluated as GUI agents\nusing a simulated dataset, following three working patterns with different\nlevels of perception. Experimental results reveal that even the most powerful\nmodels, whether generalist agents or specialist GUI agents, are susceptible to\ndistractions. While recent studies predominantly focus on the helpfulness of\nagents, our findings first indicate that these agents are prone to\nenvironmental distractions. Furthermore, we implement an adversarial\nenvironment injection and analyze the approach to improve faithfulness, calling\nfor a collective focus on this important topic."}
{"id": "2409.13514", "pdf": "https://arxiv.org/pdf/2409.13514.pdf", "abs": "https://arxiv.org/abs/2409.13514", "title": "Unifying Global and Near-Context Biasing in a Single Trie Pass", "authors": ["Iuliia Thorbecke", "Esaú Villatoro-Tello", "Juan Zuluaga-Gomez", "Shashi Kumar", "Sergio Burdisso", "Pradeep Rangappa", "Andrés Carofilis", "Srikanth Madikeri", "Petr Motlicek", "Karthik Pandia", "Kadri Hacioğlu", "Andreas Stolcke"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to TSD2025", "summary": "Despite the success of end-to-end automatic speech recognition (ASR) models,\nchallenges persist in recognizing rare, out-of-vocabulary words - including\nnamed entities (NE) - and in adapting to new domains using only text data. This\nwork presents a practical approach to address these challenges through an\nunexplored combination of an NE bias list and a word-level n-gram language\nmodel (LM). This solution balances simplicity and effectiveness, improving\nentities' recognition while maintaining or even enhancing overall ASR\nperformance. We efficiently integrate this enriched biasing method into a\ntransducer-based ASR system, enabling context adaptation with almost no\ncomputational overhead. We present our results on three datasets spanning four\nlanguages and compare them to state-of-the-art biasing strategies. We\ndemonstrate that the proposed combination of keyword biasing and n-gram LM\nimproves entity recognition by up to 32% relative and reduces overall WER by up\nto a 12% relative."}
{"id": "2409.20434", "pdf": "https://arxiv.org/pdf/2409.20434.pdf", "abs": "https://arxiv.org/abs/2409.20434", "title": "QAEncoder: Towards Aligned Representation Learning in Question Answering Systems", "authors": ["Zhengren Wang", "Qinhan Yu", "Shida Wei", "Zhiyu Li", "Feiyu Xiong", "Xiaoxing Wang", "Simin Niu", "Hao Liang", "Wentao Zhang"], "categories": ["cs.CL"], "comment": "ACL 2025 Oral", "summary": "Modern QA systems entail retrieval-augmented generation (RAG) for accurate\nand trustworthy responses. However, the inherent gap between user queries and\nrelevant documents hinders precise matching. We introduce QAEncoder, a\ntraining-free approach to bridge this gap. Specifically, QAEncoder estimates\nthe expectation of potential queries in the embedding space as a robust\nsurrogate for the document embedding, and attaches document fingerprints to\neffectively distinguish these embeddings. Extensive experiments across diverse\ndatasets, languages, and embedding models confirmed QAEncoder's alignment\ncapability, which offers a simple-yet-effective solution with zero additional\nindex storage, retrieval latency, training costs, or catastrophic forgetting\nand hallucination issues. The repository is publicly available at\nhttps://github.com/IAAR-Shanghai/QAEncoder."}
{"id": "2410.06716", "pdf": "https://arxiv.org/pdf/2410.06716.pdf", "abs": "https://arxiv.org/abs/2410.06716", "title": "Guaranteed Generation from Large Language Models", "authors": ["Minbeom Kim", "Thibaut Thonet", "Jos Rozen", "Hwaran Lee", "Kyomin Jung", "Marc Dymetman"], "categories": ["cs.CL"], "comment": "ICLR 2025", "summary": "As large language models (LLMs) are increasingly used across various\napplications, there is a growing need to control text generation to satisfy\nspecific constraints or requirements. This raises a crucial question: Is it\npossible to guarantee strict constraint satisfaction in generated outputs while\npreserving the distribution of the original model as much as possible? We first\ndefine the ideal distribution - the one closest to the original model, which\nalso always satisfies the expressed constraint - as the ultimate goal of\nguaranteed generation. We then state a fundamental limitation, namely that it\nis impossible to reach that goal through autoregressive training alone. This\nmotivates the necessity of combining training-time and inference-time methods\nto enforce such guarantees. Based on this insight, we propose GUARD, a simple\nyet effective approach that combines an autoregressive proposal distribution\nwith rejection sampling. Through GUARD's theoretical properties, we show how\ncontrolling the KL divergence between a specific proposal and the target ideal\ndistribution simultaneously optimizes inference speed and distributional\ncloseness. To validate these theoretical concepts, we conduct extensive\nexperiments on two text generation settings with hard-to-satisfy constraints: a\nlexical constraint scenario and a sentiment reversal scenario. These\nexperiments show that GUARD achieves perfect constraint satisfaction while\nalmost preserving the ideal distribution with highly improved inference\nefficiency. GUARD provides a principled approach to enforcing strict guarantees\nfor LLMs without compromising their generative capabilities."}
{"id": "2412.05563", "pdf": "https://arxiv.org/pdf/2412.05563.pdf", "abs": "https://arxiv.org/abs/2412.05563", "title": "A Survey on Uncertainty Quantification of Large Language Models: Taxonomy, Open Research Challenges, and Future Directions", "authors": ["Ola Shorinwa", "Zhiting Mei", "Justin Lidard", "Allen Z. Ren", "Anirudha Majumdar"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The remarkable performance of large language models (LLMs) in content\ngeneration, coding, and common-sense reasoning has spurred widespread\nintegration into many facets of society. However, integration of LLMs raises\nvalid questions on their reliability and trustworthiness, given their\npropensity to generate hallucinations: plausible, factually-incorrect\nresponses, which are expressed with striking confidence. Previous work has\nshown that hallucinations and other non-factual responses generated by LLMs can\nbe detected by examining the uncertainty of the LLM in its response to the\npertinent prompt, driving significant research efforts devoted to quantifying\nthe uncertainty of LLMs. This survey seeks to provide an extensive review of\nexisting uncertainty quantification methods for LLMs, identifying their salient\nfeatures, along with their strengths and weaknesses. We present existing\nmethods within a relevant taxonomy, unifying ostensibly disparate methods to\naid understanding of the state of the art. Furthermore, we highlight\napplications of uncertainty quantification methods for LLMs, spanning chatbot\nand textual applications to embodied artificial intelligence applications in\nrobotics. We conclude with open research challenges in uncertainty\nquantification of LLMs, seeking to motivate future research."}
{"id": "2501.03456", "pdf": "https://arxiv.org/pdf/2501.03456.pdf", "abs": "https://arxiv.org/abs/2501.03456", "title": "Text to Band Gap: Pre-trained Language Models as Encoders for Semiconductor Band Gap Prediction", "authors": ["Ying-Ting Yeh", "Janghoon Ock", "Shagun Maheshwari", "Amir Barati Farimani"], "categories": ["cs.CL", "cond-mat.mtrl-sci"], "comment": null, "summary": "We investigate the use of transformer-based language models, RoBERTa, T5, and\nLLaMA, for predicting the band gaps of semiconductor materials directly from\ntextual representations that encode key material features such as chemical\ncomposition, crystal system, space group, number of atoms per unit cell,\nvalence electron count, and other relevant electronic and structural\nproperties. Quantum chemistry simulations such as DFT provide accurate\npredictions but are computationally intensive, limiting their feasibility for\nlarge-scale materials screening. Shallow ML models offer faster alternatives\nbut typically require extensive data preprocessing to convert non-numerical\nmaterial features into structured numerical inputs, often at the cost of losing\ncritical descriptive information. In contrast, our approach leverages\npretrained language models to process textual data directly, eliminating the\nneed for manual feature engineering. We construct material descriptions in two\nformats: structured strings that combine key features in a consistent template,\nand natural language narratives generated using the ChatGPT API. For each\nmodel, we append a custom regression head and perform task-specific finetuning\non a curated dataset of inorganic compounds. Our results show that finetuned\nlanguage models, particularly the decoder-only LLaMA-3 architecture, can\noutperform conventional approaches in prediction accuracy and flexibility,\nachieving an MAE of 0.25 eV and R2 of 0.89, compared to the best shallow ML\nbaseline, which achieved an MAE of 0.32 eV and R2 of 0.84. Notably, LLaMA-3\nachieves competitive accuracy with minimal finetuning, suggesting its\narchitecture enables more transferable representations for scientific tasks.\nThis work demonstrates the effectiveness of finetuned language models for\nscientific property prediction and provides a scalable, language-native\nframework for materials informatics."}
{"id": "2502.12084", "pdf": "https://arxiv.org/pdf/2502.12084.pdf", "abs": "https://arxiv.org/abs/2502.12084", "title": "VLM2-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues", "authors": ["Jianshu Zhang", "Dongyu Yao", "Renjie Pi", "Paul Pu Liang", "Yi R. Fung"], "categories": ["cs.CL"], "comment": "Project Page: https://vlm2-bench.github.io/ Camera Ready version", "summary": "Visually linking matching cues is a crucial ability in daily life, such as\nidentifying the same person in multiple photos based on their cues, even\nwithout knowing who they are. Despite the extensive knowledge that\nvision-language models (VLMs) possess, it remains largely unexplored whether\nthey are capable of performing this fundamental task. To address this, we\nintroduce \\textbf{VLM2-Bench}, a benchmark designed to assess whether VLMs can\nVisually Link Matching cues, with 9 subtasks and over 3,000 test cases.\nComprehensive evaluation across twelve VLMs, along with further analysis of\nvarious language-side and vision-side prompting methods, leads to a total of\neight key findings. We identify critical challenges in models' ability to link\nvisual cues, highlighting a significant performance gap. Based on these\ninsights, we advocate for (i) enhancing core visual capabilities to improve\nadaptability and reduce reliance on prior knowledge, (ii) establishing clearer\nprinciples for integrating language-based reasoning in vision-centric tasks to\nprevent unnecessary biases, and (iii) shifting vision-text training paradigms\ntoward fostering models' ability to independently structure and infer\nrelationships among visual cues."}
{"id": "2502.18443", "pdf": "https://arxiv.org/pdf/2502.18443.pdf", "abs": "https://arxiv.org/abs/2502.18443", "title": "olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models", "authors": ["Jake Poznanski", "Aman Rangapur", "Jon Borchardt", "Jason Dunkelberger", "Regan Huff", "Daniel Lin", "Aman Rangapur", "Christopher Wilhelm", "Kyle Lo", "Luca Soldaini"], "categories": ["cs.CL"], "comment": null, "summary": "PDF documents have the potential to provide trillions of novel, high-quality\ntokens for training language models. However, these documents come in a\ndiversity of types with differing formats and visual layouts that pose a\nchallenge when attempting to extract and faithfully represent the underlying\ncontent for language model use. Traditional open source tools often produce\nlower quality extractions compared to vision language models (VLMs), but\nreliance on the best VLMs can be prohibitively costly (e.g., over 6,240 USD per\nmillion PDF pages for GPT-4o) or infeasible if the PDFs cannot be sent to\nproprietary APIs. We present olmOCR, an open-source toolkit for processing PDFs\ninto clean, linearized plain text in natural reading order while preserving\nstructured content like sections, tables, lists, equations, and more. Our\ntoolkit runs a fine-tuned 7B vision language model (VLM) trained on\nolmOCR-mix-0225, a sample of 260,000 pages from over 100,000 crawled PDFs with\ndiverse properties, including graphics, handwritten text and poor quality\nscans. olmOCR is optimized for large-scale batch processing, able to scale\nflexibly to different hardware setups and can convert a million PDF pages for\nonly 176 USD. To aid comparison with existing systems, we also introduce\nolmOCR-Bench, a curated set of 1,400 PDFs capturing many content types that\nremain challenging even for the best tools and VLMs, including formulas,\ntables, tiny fonts, old scans, and more. We find olmOCR outperforms even top\nVLMs including GPT-4o, Gemini Flash 2 and Qwen-2.5-VL. We openly release all\ncomponents of olmOCR: our fine-tuned VLM model, training code and data, an\nefficient inference pipeline that supports vLLM and SGLang backends, and\nbenchmark olmOCR-Bench."}
{"id": "2503.00032", "pdf": "https://arxiv.org/pdf/2503.00032.pdf", "abs": "https://arxiv.org/abs/2503.00032", "title": "KatFishNet: Detecting LLM-Generated Korean Text through Linguistic Feature Analysis", "authors": ["Shinwoo Park", "Shubin Kim", "Do-Kyung Kim", "Yo-Sub Han"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 main conference", "summary": "The rapid advancement of large language models (LLMs) increases the\ndifficulty of distinguishing between human-written and LLM-generated text.\nDetecting LLM-generated text is crucial for upholding academic integrity,\npreventing plagiarism, protecting copyrights, and ensuring ethical research\npractices. Most prior studies on detecting LLM-generated text focus primarily\non English text. However, languages with distinct morphological and syntactic\ncharacteristics require specialized detection approaches. Their unique\nstructures and usage patterns can hinder the direct application of methods\nprimarily designed for English. Among such languages, we focus on Korean, which\nhas relatively flexible spacing rules, a rich morphological system, and less\nfrequent comma usage compared to English. We introduce KatFish, the first\nbenchmark dataset for detecting LLM-generated Korean text. The dataset consists\nof text written by humans and generated by four LLMs across three genres.\n  By examining spacing patterns, part-of-speech diversity, and comma usage, we\nilluminate the linguistic differences between human-written and LLM-generated\nKorean text. Building on these observations, we propose KatFishNet, a detection\nmethod specifically designed for the Korean language. KatFishNet achieves an\naverage of 19.78% higher AUROC compared to the best-performing existing\ndetection method. Our code and data are available at\nhttps://github.com/Shinwoo-Park/detecting_llm_generated_korean_text_through_linguistic_analysis."}
{"id": "2505.11764", "pdf": "https://arxiv.org/pdf/2505.11764.pdf", "abs": "https://arxiv.org/abs/2505.11764", "title": "Towards Universal Semantics With Large Language Models", "authors": ["Raymond Baartmans", "Matthew Raffel", "Rahul Vikram", "Aiden Deringer", "Lizhong Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a\nuniversal set of semantic primes: simple, primitive word-meanings that have\nbeen shown to exist in most, if not all, languages of the world. According to\nthis framework, any word, regardless of complexity, can be paraphrased using\nthese primes, revealing a clear and universally translatable meaning. These\nparaphrases, known as explications, can offer valuable applications for many\nnatural language processing (NLP) tasks, but producing them has traditionally\nbeen a slow, manual process. In this work, we present the first study of using\nlarge language models (LLMs) to generate NSM explications. We introduce\nautomatic evaluation methods, a tailored dataset for training and evaluation,\nand fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in\nproducing accurate, cross-translatable explications, marking a significant step\ntoward universal semantic representation with LLMs and opening up new\npossibilities for applications in semantic analysis, translation, and beyond."}
{"id": "2505.15962", "pdf": "https://arxiv.org/pdf/2505.15962.pdf", "abs": "https://arxiv.org/abs/2505.15962", "title": "Pre-training Large Memory Language Models with Internal and External Knowledge", "authors": ["Linxi Zhao", "Sofian Zalouk", "Christian K. Belardi", "Justin Lovelace", "Jin Peng Zhou", "Kilian Q. Weinberger", "Yoav Artzi", "Jennifer J. Sun"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Code, models, and data available at\n  https://github.com/kilian-group/LMLM", "summary": "Neural language models are black-boxes -- both linguistic patterns and\nfactual knowledge are distributed across billions of opaque parameters. This\nentangled encoding makes it difficult to reliably inspect, verify, or update\nspecific facts. We propose a new class of language models, Large Memory\nLanguage Models (LMLM) with a pre-training recipe that stores factual knowledge\nin both internal weights and an external database. Our approach strategically\nmasks externally retrieved factual values from the training loss, thereby\nteaching the model to perform targeted lookups rather than relying on\nmemorization in model weights. Our experiments demonstrate that LMLMs achieve\ncompetitive performance compared to significantly larger, knowledge-dense LLMs\non standard benchmarks, while offering the advantages of explicit, editable,\nand verifiable knowledge bases. This work represents a fundamental shift in how\nlanguage models interact with and manage factual knowledge."}
{"id": "2505.19121", "pdf": "https://arxiv.org/pdf/2505.19121.pdf", "abs": "https://arxiv.org/abs/2505.19121", "title": "Delving into Multilingual Ethical Bias: The MSQAD with Statistical Hypothesis Tests for Large Language Models", "authors": ["Seunguk Yu", "Juhwan Choi", "Youngbin Kim"], "categories": ["cs.CL"], "comment": "ACL 2025 main conference", "summary": "Despite the recent strides in large language models, studies have underscored\nthe existence of social biases within these systems. In this paper, we delve\ninto the validation and comparison of the ethical biases of LLMs concerning\nglobally discussed and potentially sensitive topics, hypothesizing that these\nbiases may arise from language-specific distinctions. Introducing the\nMultilingual Sensitive Questions & Answers Dataset (MSQAD), we collected news\narticles from Human Rights Watch covering 17 topics, and generated socially\nsensitive questions along with corresponding responses in multiple languages.\nWe scrutinized the biases of these responses across languages and topics,\nemploying two statistical hypothesis tests. The results showed that the null\nhypotheses were rejected in most cases, indicating biases arising from\ncross-language differences. It demonstrates that ethical biases in responses\nare widespread across various languages, and notably, these biases were\nprevalent even among different LLMs. By making the proposed MSQAD openly\navailable, we aim to facilitate future research endeavors focused on examining\ncross-language biases in LLMs and their variant models."}
{"id": "2505.20295", "pdf": "https://arxiv.org/pdf/2505.20295.pdf", "abs": "https://arxiv.org/abs/2505.20295", "title": "Self-reflective Uncertainties: Do LLMs Know Their Internal Answer Distribution?", "authors": ["Michael Kirchhof", "Luca Füger", "Adam Goliński", "Eeshan Gunesh Dhekane", "Arno Blaas", "Sinead Williamson"], "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "To reveal when a large language model (LLM) is uncertain about a response,\nuncertainty quantification commonly produces percentage numbers along with the\noutput. But is this all we can do? We argue that in the output space of LLMs,\nthe space of strings, exist strings expressive enough to summarize the\ndistribution over output strings the LLM deems possible. We lay a foundation\nfor this new avenue of uncertainty explication and present SelfReflect, a\ntheoretically-motivated metric to assess how faithfully a string summarizes an\nLLM's internal answer distribution. We show that SelfReflect is able to\ndiscriminate even subtle differences of candidate summary strings and that it\naligns with human judgement, outperforming alternative metrics such as LLM\njudges and embedding comparisons. With SelfReflect, we investigate a number of\nself-summarization methods and find that even state-of-the-art reasoning models\nstruggle to explicate their internal uncertainty. But we find that faithful\nsummarizations can be generated by sampling and summarizing. To support the\ndevelopment of this universal form of LLM uncertainties, we publish our metric\nat https://github.com/apple/ml-selfreflect"}
{"id": "2505.22618", "pdf": "https://arxiv.org/pdf/2505.22618.pdf", "abs": "https://arxiv.org/abs/2505.22618", "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding", "authors": ["Chengyue Wu", "Hao Zhang", "Shuchen Xue", "Zhijian Liu", "Shizhe Diao", "Ligeng Zhu", "Ping Luo", "Song Han", "Enze Xie"], "categories": ["cs.CL"], "comment": null, "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."}
{"id": "2506.06955", "pdf": "https://arxiv.org/pdf/2506.06955.pdf", "abs": "https://arxiv.org/abs/2506.06955", "title": "BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning", "authors": ["Ha-Thanh Nguyen", "Chaoran Liu", "Qianying Liu", "Hideyuki Tachibana", "Su Myat Noe", "Yusuke Miyao", "Koichi Takeda", "Sadao Kurohashi"], "categories": ["cs.CL", "cs.AI"], "comment": "This version includes typo corrections, added logit lens analysis for\n  open models, and an updated author list", "summary": "We present BIS Reasoning 1.0, the first large-scale Japanese dataset of\nsyllogistic reasoning problems explicitly designed to evaluate\nbelief-inconsistent reasoning in large language models (LLMs). Unlike prior\ndatasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned\nreasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent\nsyllogisms to uncover reasoning biases in LLMs trained on human-aligned\ncorpora. We benchmark state-of-the-art models - including GPT models, Claude\nmodels, and leading Japanese LLMs - revealing significant variance in\nperformance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies\ncritical weaknesses in current LLMs when handling logically valid but\nbelief-conflicting inputs. These findings have important implications for\ndeploying LLMs in high-stakes domains such as law, healthcare, and scientific\nliterature, where truth must override intuitive belief to ensure integrity and\nsafety."}
{"id": "2506.17352", "pdf": "https://arxiv.org/pdf/2506.17352.pdf", "abs": "https://arxiv.org/abs/2506.17352", "title": "Towards Safety Evaluations of Theory of Mind in Large Language Models", "authors": ["Tatsuhiro Aoshima", "Mitsuaki Akiyama"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As the capabilities of large language models (LLMs) continue to advance, the\nimportance of rigorous safety evaluation is becoming increasingly evident.\nRecent concerns within the realm of safety assessment have highlighted\ninstances in which LLMs exhibit behaviors that appear to disable oversight\nmechanisms and respond in a deceptive manner. For example, there have been\nreports suggesting that, when confronted with information unfavorable to their\nown persistence during task execution, LLMs may act covertly and even provide\nfalse answers to questions intended to verify their behavior. To evaluate the\npotential risk of such deceptive actions toward developers or users, it is\nessential to investigate whether these behaviors stem from covert, intentional\nprocesses within the model. In this study, we propose that it is necessary to\nmeasure the theory of mind capabilities of LLMs. We begin by reviewing existing\nresearch on theory of mind and identifying the perspectives and tasks relevant\nto its application in safety evaluation. Given that theory of mind has been\npredominantly studied within the context of developmental psychology, we\nanalyze developmental trends across a series of open-weight LLMs. Our results\nindicate that while LLMs have improved in reading comprehension, their theory\nof mind capabilities have not shown comparable development. Finally, we present\nthe current state of safety evaluation with respect to LLMs' theory of mind,\nand discuss remaining challenges for future work."}
{"id": "2506.21567", "pdf": "https://arxiv.org/pdf/2506.21567.pdf", "abs": "https://arxiv.org/abs/2506.21567", "title": "BioPars: A Pretrained Biomedical Large Language Model for Persian Biomedical Text Mining", "authors": ["Baqer M. Merzah", "Tania Taami", "Salman Asoudeh", "Saeed Mirzaee", "Amir reza Hossein pour", "Amir Ali Bengari"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have recently gained attention in the life\nsciences due to their capacity to model, extract, and apply complex biological\ninformation. Beyond their classical use as chatbots, these systems are\nincreasingly used for complex analysis and problem-solving in specialized\nfields, including bioinformatics. First, we introduce BIOPARS-BENCH, a dataset\nfrom over 10,000 scientific articles, textbooks, and medical websites.\nBioParsQA was also introduced to evaluate the proposed model, which consists of\n5,231 Persian medical questions and answers. This study then introduces\nBioPars, a simple but accurate measure designed to assess LLMs for three main\nabilities: acquiring subject-specific knowledge, interpreting and synthesizing\nsuch knowledge, and demonstrating proper evidence. Comparing ChatGPT, Llama,\nand Galactica, our study highlights their ability to remember and retrieve\nlearned knowledge but also reveals shortcomings in addressing higher-level,\nreal-world questions and fine-grained inferences. These findings indicate the\nneed for further fine-tuning to address the capabilities of LLM in\nbioinformatics tasks. To our knowledge, BioPars is the first application of LLM\nin Persian medical QA, especially for generating long answers. Evaluation of\nfour selected medical QA datasets shows that BioPars has achieved remarkable\nresults compared to comparative approaches. The model on BioParsQA achieved a\nROUGE-L score of 29.99, which is an improvement over GPT-4 1.0. The model\nachieved a BERTScore of 90.87 with the MMR method. The MoverScore and BLEURT\nvalues were also higher in this model than the other three models. In addition,\nthe reported scores for the model are MoverScore=60.43 and BLEURT=50.78.\nBioPars is an ongoing project and all resources related to its development will\nbe made available via the following GitHub repository:\nhttps://github.com/amirap80/BioPars."}
{"id": "2506.21848", "pdf": "https://arxiv.org/pdf/2506.21848.pdf", "abs": "https://arxiv.org/abs/2506.21848", "title": "LinguaSynth: Heterogeneous Linguistic Signals for News Classification", "authors": ["Duo Zhang", "Junyi Mo"], "categories": ["cs.CL"], "comment": null, "summary": "Deep learning has significantly advanced NLP, but its reliance on large\nblack-box models introduces critical interpretability and computational\nefficiency concerns. This paper proposes LinguaSynth, a novel text\nclassification framework that strategically integrates five complementary\nlinguistic feature types: lexical, syntactic, entity-level, word-level\nsemantics, and document-level semantics within a transparent logistic\nregression model. Unlike transformer-based architectures, LinguaSynth maintains\ninterpretability and computational efficiency, achieving an accuracy of 84.89\npercent on the 20 Newsgroups dataset and surpassing a robust TF-IDF baseline by\n3.32 percent. Through rigorous feature interaction analysis, we show that\nsyntactic and entity-level signals provide essential disambiguation and\neffectively complement distributional semantics. LinguaSynth sets a new\nbenchmark for interpretable, resource-efficient NLP models and challenges the\nprevailing assumption that deep neural networks are necessary for\nhigh-performing text classification."}
{"id": "2506.22405", "pdf": "https://arxiv.org/pdf/2506.22405.pdf", "abs": "https://arxiv.org/abs/2506.22405", "title": "Sequential Diagnosis with Language Models", "authors": ["Harsha Nori", "Mayank Daswani", "Christopher Kelly", "Scott Lundberg", "Marco Tulio Ribeiro", "Marc Wilson", "Xiaoxuan Liu", "Viknesh Sounderajah", "Jonathan Carlson", "Matthew P Lungren", "Bay Gross", "Peter Hames", "Mustafa Suleyman", "Dominic King", "Eric Horvitz"], "categories": ["cs.CL"], "comment": "23 pages, 10 figures", "summary": "Artificial intelligence holds great promise for expanding access to expert\nmedical knowledge and reasoning. However, most evaluations of language models\nrely on static vignettes and multiple-choice questions that fail to reflect the\ncomplexity and nuance of evidence-based medicine in real-world settings. In\nclinical practice, physicians iteratively formulate and revise diagnostic\nhypotheses, adapting each subsequent question and test to what they've just\nlearned, and weigh the evolving evidence before committing to a final\ndiagnosis. To emulate this iterative process, we introduce the Sequential\nDiagnosis Benchmark, which transforms 304 diagnostically challenging New\nEngland Journal of Medicine clinicopathological conference (NEJM-CPC) cases\ninto stepwise diagnostic encounters. A physician or AI begins with a short case\nabstract and must iteratively request additional details from a gatekeeper\nmodel that reveals findings only when explicitly queried. Performance is\nassessed not just by diagnostic accuracy but also by the cost of physician\nvisits and tests performed. We also present the MAI Diagnostic Orchestrator\n(MAI-DxO), a model-agnostic orchestrator that simulates a panel of physicians,\nproposes likely differential diagnoses and strategically selects high-value,\ncost-effective tests. When paired with OpenAI's o3 model, MAI-DxO achieves 80%\ndiagnostic accuracy--four times higher than the 20% average of generalist\nphysicians. MAI-DxO also reduces diagnostic costs by 20% compared to\nphysicians, and 70% compared to off-the-shelf o3. When configured for maximum\naccuracy, MAI-DxO achieves 85.5% accuracy. These performance gains with MAI-DxO\ngeneralize across models from the OpenAI, Gemini, Claude, Grok, DeepSeek, and\nLlama families. We highlight how AI systems, when guided to think iteratively\nand act judiciously, can advance diagnostic precision and cost-effectiveness in\nclinical care."}
{"id": "2506.22853", "pdf": "https://arxiv.org/pdf/2506.22853.pdf", "abs": "https://arxiv.org/abs/2506.22853", "title": "DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues", "authors": ["Kyochul Jang", "Donghyeon Lee", "Kyusik Kim", "Dongseok Heo", "Taewhoo Lee", "Woojeong Kim", "Bongwon Suh"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, ACL 2025 Vienna", "summary": "Existing function-calling benchmarks focus on single-turn interactions.\nHowever, they overlook the complexity of real-world scenarios. To quantify how\nexisting benchmarks address practical applications, we introduce DICE-SCORE, a\nmetric that evaluates the dispersion of tool-related information such as\nfunction name and parameter values throughout the dialogue. Analyzing existing\nbenchmarks through DICE-SCORE reveals notably low scores, highlighting the need\nfor more realistic scenarios. To address this gap, we present DICE-BENCH, a\nframework that constructs practical function-calling datasets by synthesizing\nconversations through a tool graph that maintains dependencies across rounds\nand a multi-agent system with distinct personas to enhance dialogue\nnaturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our\nexperiments on 19 LLMs with DICE-BENCH show that significant advances are still\nrequired before such models can be deployed effectively in real-world settings.\nOur code and data are all publicly available:\nhttps://snuhcc.github.io/DICE-Bench/."}
{"id": "2507.00601", "pdf": "https://arxiv.org/pdf/2507.00601.pdf", "abs": "https://arxiv.org/abs/2507.00601", "title": "Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based Approach", "authors": ["Shuangquan Lyu", "Yingnan Deng", "Guiran Liu", "Zhen Qi", "Ruotong Wang"], "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the limited transfer and adaptation capabilities of\nlarge language models in low-resource language scenarios. It proposes a unified\nframework that combines a knowledge transfer module with parameter-efficient\nfine-tuning strategies. The method introduces knowledge alignment loss and soft\nprompt tuning to guide the model in effectively absorbing the structural\nfeatures of target languages or tasks under minimal annotation. This enhances\nboth generalization performance and training stability. The framework includes\nlightweight adaptation modules to reduce computational costs. During training,\nit integrates freezing strategies and prompt injection to preserve the model's\noriginal knowledge while enabling quick adaptation to new tasks. The study also\nconducts stability analysis experiments and synthetic pseudo-data transfer\nexperiments to systematically evaluate the method's applicability and\nrobustness across different low-resource tasks. Experimental results show that\ncompared with existing multilingual pre-trained models and mainstream transfer\nmethods, the proposed approach achieves higher performance and stability on\ncross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates\nparticularly strong advantages under extremely data-scarce conditions. The\nproposed method offers strong generality and scalability. It enhances\ntask-specific adaptability while preserving the general capabilities of large\nlanguage models. This makes it well-suited for complex semantic modeling and\nmultilingual processing tasks."}
{"id": "2307.02075", "pdf": "https://arxiv.org/pdf/2307.02075.pdf", "abs": "https://arxiv.org/abs/2307.02075", "title": "Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for Entity Alignment", "authors": ["Qijie Ding", "Jie Yin", "Daokun Zhang", "Junbin Gao"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Entity alignment (EA) aims at identifying equivalent entity pairs across\ndifferent knowledge graphs (KGs) that refer to the same real-world identity. To\ncircumvent the shortage of seed alignments provided for training, recent EA\nmodels utilize pseudo-labeling strategies to iteratively add unaligned entity\npairs predicted with high confidence to the seed alignments for model training.\nHowever, the adverse impact of confirmation bias during pseudo-labeling has\nbeen largely overlooked, thus hindering entity alignment performance. To\nsystematically combat confirmation bias for pseudo-labeling-based entity\nalignment, we propose a Unified Pseudo-Labeling framework for Entity Alignment\n(UPL-EA) that explicitly eliminates pseudo-labeling errors to boost the\naccuracy of entity alignment. UPL-EA consists of two complementary components:\n(1) Optimal Transport (OT)-based pseudo-labeling uses discrete OT modeling as\nan effective means to determine entity correspondences and reduce erroneous\nmatches across two KGs. An effective criterion is derived to infer\npseudo-labeled alignments that satisfy one-to-one correspondences; (2) Parallel\npseudo-label ensembling refines pseudo-labeled alignments by combining\npredictions over multiple models independently trained in parallel. The\nensembled pseudo-labeled alignments are thereafter used to augment seed\nalignments to reinforce subsequent model training for alignment inference. The\neffectiveness of UPL-EA in eliminating pseudo-labeling errors is both\ntheoretically supported and experimentally validated. Our extensive results and\nin-depth analyses demonstrate the superiority of UPL-EA over 15 competitive\nbaselines and its utility as a general pseudo-labeling framework for entity\nalignment."}
{"id": "2402.10787", "pdf": "https://arxiv.org/pdf/2402.10787.pdf", "abs": "https://arxiv.org/abs/2402.10787", "title": "Squat: Quant Small Language Models on the Edge", "authors": ["Xuan Shen", "Peiyan Dong", "Zhenglun Kong", "Yifan Gong", "Changdi Yang", "Zhaoyang Han", "Yanyue Xie", "Lei Lu", "Cheng Lyu", "Chao Wu", "Yanzhi Wang", "Pu Zhao"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepeted by ICCAD 2025", "summary": "A growing trend has emerged in designing high-quality Small Language Models\n(SLMs) with a few million parameters. This trend is driven by the increasing\nconcerns over cloud costs, privacy, and latency. Considering that full\nparameter training is feasible for SLMs on mobile devices, Quantization-Aware\nTraining (QAT) is employed to improve efficiency by reducing computational\noverhead and memory footprint. However, previous QAT works adopt fine-grained\nquantization methods to compress models with billions of parameters on GPUs,\nincompatible with current commodity hardware, such as mobile and edge devices,\nwhich relies on Single Instruction Multiple Data (SIMD) instructions. Thus, the\ngeneralization of these methods to SLMs on mobile devices is limited. In this\npaper, we propose Squat method, an effective QAT framework with deployable\nquantization for SLMs on mobile devices. Specifically, we propose\nentropy-guided and distribution-aligned distillation to mitigate the distortion\nof attention information from quantization. Besides, we employ sub-8-bit token\nadaptive quantization, assigning varying bit widths to different tokens based\non their importance. Furthermore, we develop a SIMD-based Multi-Kernel\nMixed-Precision (MKMP) multiplier to support sub-8-bit mixed-precision MAC on\nmobile devices. Our extensive experiments verify the substantial improvements\nof our method compared to other QAT methods across various datasets.\nFurthermore, we achieve an on-device speedup of up to 2.37x compared with its\nFP16 counterparts, signaling a great advancement. Code:\nhttps://github.com/shawnricecake/squant"}
{"id": "2410.23114", "pdf": "https://arxiv.org/pdf/2410.23114.pdf", "abs": "https://arxiv.org/abs/2410.23114", "title": "Unified Triplet-Level Hallucination Evaluation for Large Vision-Language Models", "authors": ["Junjie Wu", "Tsz Ting Chung", "Kai Chen", "Dit-Yan Yeung"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted by TMLR 2025. Project Page:\n  https://kaichen1998.github.io/projects/tri-he/", "summary": "Despite the outstanding performance in vision-language reasoning, Large\nVision-Language Models (LVLMs) might generate hallucinated contents that do not\nexist in the given image. Most existing LVLM hallucination benchmarks are\nconstrained to evaluate the object-related hallucinations. However, the\npotential hallucination on the relations between two objects, i.e., relation\nhallucination, still lacks investigation. To remedy that, we design a unified\nframework to measure the object and relation hallucination in LVLMs\nsimultaneously. The core idea of our framework is to evaluate hallucinations\nvia (object, relation, object) triplets extracted from LVLMs' responses, making\nit easily generalizable to different vision-language tasks. Based on our\nframework, we further introduce Tri-HE, a novel Triplet-level Hallucination\nEvaluation benchmark which can be used to study both object and relation\nhallucination at the same time. With comprehensive evaluations on Tri-HE, we\nobserve that the relation hallucination issue is even more serious than object\nhallucination among existing LVLMs, highlighting a previously neglected problem\ntowards reliable LVLMs. Moreover, based on our findings, we design a simple\ntraining-free approach that effectively mitigates hallucinations for LVLMs. Our\ndataset and code for the reproduction of our experiments are available publicly\nat https://github.com/wujunjie1998/Tri-HE."}
{"id": "2412.04787", "pdf": "https://arxiv.org/pdf/2412.04787.pdf", "abs": "https://arxiv.org/abs/2412.04787", "title": "Direct Quantized Training of Language Models with Stochastic Rounding", "authors": ["Kaiyan Zhao", "Tsuguchika Tabaru", "Kenichi Kobayashi", "Takumi Honda", "Masafumi Yamazaki", "Yoshimasa Tsuruoka"], "categories": ["cs.LG", "cs.CL"], "comment": "work in progress, extended experiments to 1B size models", "summary": "Although recent quantized Large Language Models (LLMs), such as BitNet, have\npaved the way for significant reduction in memory usage during deployment with\nbinary or ternary weights, training these models still demands substantial\nmemory footprints. This is partly because high-precision (i.e., unquantized)\nweights required for straight-through estimation must be maintained throughout\nthe whole training process. To address this, we explore directly updating the\nquantized low-precision weights without relying on straight-through estimation\nduring backpropagation, aiming to save memory usage during training.\nSpecifically, we employ a stochastic rounding technique to minimize the\ninformation loss caused by the use of low-bit weights throughout training.\nExperimental results on our LLaMA-structured models of various sizes indicate\nthat (1) training with only low-precision weights is feasible even when they\nare constrained to ternary values; (2) extending the bit width to 8 bits\nachieves performance on par with BitNet b1.58; (3) our models remain robust to\nprecision scaling and memory reduction, showing minimal performance degradation\nwhen moving from FP32 to lower-memory environments (BF16/FP8); and (4) our\nmodels also support inference using ternary weights, showcasing their\nflexibility in deployment."}
{"id": "2504.03814", "pdf": "https://arxiv.org/pdf/2504.03814.pdf", "abs": "https://arxiv.org/abs/2504.03814", "title": "Recursive Training Loops in LLMs: How training data properties modulate distribution shift in generated data?", "authors": ["Grgur Kovač", "Jérémy Perez", "Rémy Portelas", "Peter Ford Dominey", "Pierre-Yves Oudeyer"], "categories": ["cs.LG", "cs.AI", "cs.CL", "68T50", "I.2.7"], "comment": null, "summary": "Large language models (LLMs) are increasingly used in the creation of online\ncontent, creating feedback loops as subsequent generations of models will be\ntrained on this synthetic data. Such loops were shown to lead to distribution\nshifts - models misrepresenting the true underlying distributions of human data\n(also called model collapse). However, how human data properties affect such\nshifts remains poorly understood. In this paper, we provide the first empirical\nexamination of the effect of such properties on the outcome of recursive\ntraining. We first confirm that using different human datasets leads to\ndistribution shifts of different magnitudes. Through exhaustive manipulation of\ndataset properties combined with regression analyses, we then identify a set of\nproperties predicting distribution shift magnitudes. Lexical diversity is found\nto amplify these shifts, while semantic diversity and data quality mitigate\nthem. Furthermore, we find that these influences are highly modular: data\nscrapped from a given internet domain has little influence on the content\ngenerated for another domain. Finally, experiments on political bias reveal\nthat human data properties affect whether the initial bias will be amplified or\nreduced. Overall, our results portray a novel view, where different parts of\ninternet may undergo different types of distribution shift."}
{"id": "2506.06382", "pdf": "https://arxiv.org/pdf/2506.06382.pdf", "abs": "https://arxiv.org/abs/2506.06382", "title": "On the Fundamental Impossibility of Hallucination Control in Large Language Models", "authors": ["Michał P. Karpowicz"], "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.GT", "cs.LG"], "comment": "major review, transformer inference application, examples added,\n  corrections", "summary": "We prove that perfect hallucination control in large language models is\nmathematically impossible. No LLM inference mechanism can simultaneously\nachieve truthful response generation, semantic information conservation,\nrelevant knowledge revelation, and knowledge-constrained optimality. This\nimpossibility is fundamental, arising from the mathematical structure of\ninformation aggregation itself rather than engineering limitations. The proof\nspans three mathematical frameworks: auction theory, proper scoring theory for\nprobabilistic predictions, and log-sum-exp analysis for transformer\narchitectures. In each setting, we demonstrate that information aggregation\ncreates unavoidable violations of conservation principles. The Jensen gap in\ntransformer probability aggregation provides a direct measure of this\nimpossibility. These results reframe hallucination from an engineering bug to\nan inevitable mathematical feature of distributed intelligence. There are\nfundamental trade-offs between truthfulness, knowledge utilization, and\nresponse completeness, providing principled foundations for managing rather\nthan eliminating hallucination. This work reveals deep connections between\nneural network inference, philosophy of knowledge and reasoning, and classical\nresults in game theory and information theory, opening new research directions\nfor developing beneficial AI systems within mathematical constraints."}
{"id": "2506.18183", "pdf": "https://arxiv.org/pdf/2506.18183.pdf", "abs": "https://arxiv.org/abs/2506.18183", "title": "Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?", "authors": ["Zhiting Mei", "Christina Zhang", "Tenny Yin", "Justin Lidard", "Ola Shorinwa", "Anirudha Majumdar"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning language models have set state-of-the-art (SOTA) records on many\nchallenging benchmarks, enabled by multi-step reasoning induced using\nreinforcement learning. However, like previous language models, reasoning\nmodels are prone to generating confident, plausible responses that are\nincorrect (hallucinations). Knowing when and how much to trust these models is\ncritical to the safe deployment of reasoning models in real-world applications.\nTo this end, we explore uncertainty quantification of reasoning models in this\nwork. Specifically, we ask three fundamental questions: First, are reasoning\nmodels well-calibrated? Second, does deeper reasoning improve model\ncalibration? Finally, inspired by humans' innate ability to double-check their\nthought processes to verify the validity of their answers and their confidence,\nwe ask: can reasoning models improve their calibration by explicitly reasoning\nabout their chain-of-thought traces? We introduce introspective uncertainty\nquantification (UQ) to explore this direction. In extensive evaluations on SOTA\nreasoning models across a broad range of benchmarks, we find that reasoning\nmodels: (i) are typically overconfident, with self-verbalized confidence\nestimates often greater than 85% particularly for incorrect responses, (ii)\nbecome even more overconfident with deeper reasoning, and (iii) can become\nbetter calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not\nuniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we\nconclude with important research directions to design necessary UQ benchmarks\nand improve the calibration of reasoning models."}
{"id": "2507.00316", "pdf": "https://arxiv.org/pdf/2507.00316.pdf", "abs": "https://arxiv.org/abs/2507.00316", "title": "$μ^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation", "authors": ["Siyou Li", "Pengyao Qin", "Huanan Wu", "Dong Nie", "Arun J. Thirunavukarasu", "Juntao Yu", "Le Zhang"], "categories": ["cs.LG", "cs.CL", "eess.IV"], "comment": "Accepted by MICCAI 2025", "summary": "Automated radiology report generation (RRG) aims to produce detailed textual\nreports from clinical imaging, such as computed tomography (CT) scans, to\nimprove the accuracy and efficiency of diagnosis and provision of management\nadvice. RRG is complicated by two key challenges: (1) inherent complexity in\nextracting relevant information from imaging data under resource constraints,\nand (2) difficulty in objectively evaluating discrepancies between\nmodel-generated and expert-written reports. To address these challenges, we\npropose $\\mu^2$LLM, a $\\underline{\\textbf{mu}}$ltiscale\n$\\underline{\\textbf{mu}}$ltimodal large language models for RRG tasks. The\nnovel ${\\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal\nfeatures from the multiscale visual tokenizer and the text tokenizer, then\nenhances report generation quality through direct preference optimization\n(DPO), guided by GREEN-RedLlama. Experimental results on four large CT\nimage-report medical datasets demonstrate that our method outperforms existing\napproaches, highlighting the potential of our fine-tuned $\\mu^2$LLMs on limited\ndata for RRG tasks. At the same time, for prompt engineering, we introduce a\nfive-stage, LLM-driven pipeline that converts routine CT reports into paired\nvisual-question-answer triples and citation-linked reasoning narratives,\ncreating a scalable, high-quality supervisory corpus for explainable multimodal\nradiology LLM. All code, datasets, and models will be publicly available in our\nofficial repository. https://github.com/Siyou-Li/u2Tokenizer"}
{"id": "2507.00487", "pdf": "https://arxiv.org/pdf/2507.00487.pdf", "abs": "https://arxiv.org/abs/2507.00487", "title": "MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large Language Models", "authors": ["Jianghao Lin", "Xinyuan Wang", "Xinyi Dai", "Menghui Zhu", "Bo Chen", "Ruiming Tang", "Yong Yu", "Weinan Zhang"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Tool retrieval is a critical component in enabling large language models\n(LLMs) to interact effectively with external tools. It aims to precisely filter\nthe massive tools into a small set of candidates for the downstream\ntool-augmented LLMs. However, most existing approaches primarily focus on\noptimizing tool representations, often neglecting the importance of precise\nquery comprehension. To address this gap, we introduce MassTool, a multi-task\nsearch-based framework designed to enhance both query representation and tool\nretrieval accuracy. MassTool employs a two-tower architecture: a tool usage\ndetection tower that predicts the need for function calls, and a tool retrieval\ntower that leverages a query-centric graph convolution network (QC-GCN) for\neffective query-tool matching. It also incorporates search-based user intent\nmodeling (SUIM) to handle diverse and out-of-distribution queries, alongside an\nadaptive knowledge transfer (AdaKT) module for efficient multi-task learning.\nBy jointly optimizing tool usage detection loss, list-wise retrieval loss, and\ncontrastive regularization loss, MassTool establishes a robust dual-step\nsequential decision-making pipeline for precise query understanding. Extensive\nexperiments demonstrate its effectiveness in improving retrieval accuracy. Our\ncode is available at https://github.com/wxydada/MassTool."}
{"id": "2507.00808", "pdf": "https://arxiv.org/pdf/2507.00808.pdf", "abs": "https://arxiv.org/abs/2507.00808", "title": "Multi-interaction TTS toward professional recording reproduction", "authors": ["Hiroki Kanagawa", "Kenichi Fujita", "Aya Watanabe", "Yusuke Ijima"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "7 pages,6 figures, Accepted to Speech Synthesis Workshop 2025 (SSW13)", "summary": "Voice directors often iteratively refine voice actors' performances by\nproviding feedback to achieve the desired outcome. While this iterative\nfeedback-based refinement process is important in actual recordings, it has\nbeen overlooked in text-to-speech synthesis (TTS). As a result, fine-grained\nstyle refinement after the initial synthesis is not possible, even though the\nsynthesized speech often deviates from the user's intended style. To address\nthis issue, we propose a TTS method with multi-step interaction that allows\nusers to intuitively and rapidly refine synthesized speech. Our approach models\nthe interaction between the TTS model and its user to emulate the relationship\nbetween voice actors and voice directors. Experiments show that the proposed\nmodel with its corresponding dataset enables iterative style refinements in\naccordance with users' directions, thus demonstrating its multi-interaction\ncapability. Sample audios are available:\nhttps://ntt-hilab-gensp.github.io/ssw13multiinteractiontts/"}
