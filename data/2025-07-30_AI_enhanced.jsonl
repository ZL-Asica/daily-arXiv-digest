{"id": "2507.21070", "pdf": "https://arxiv.org/pdf/2507.21070.pdf", "abs": "https://arxiv.org/abs/2507.21070", "title": "Enhancing Manufacturing Training Through VR Simulations", "authors": ["Vladislav Li", "Ilias Siniosoglou", "Panagiotis Sarigiannidis", "Vasileios Argyriou"], "categories": ["cs.HC"], "comment": null, "summary": "In contemporary training for industrial manufacturing, reconciling\ntheoretical knowledge with practical experience continues to be a significant\ndifficulty. As companies transition to more intricate and technology-oriented\nsettings, conventional training methods frequently inadequately equip workers\nwith essential practical skills while maintaining safety and efficiency.\nVirtual Reality has emerged as a transformational instrument to tackle this\nissue by providing immersive, interactive, and risk-free teaching experiences.\nThrough the simulation of authentic industrial environments, virtual reality\nfacilitates the acquisition of vital skills for trainees within a regulated and\nstimulating context, therefore mitigating the hazards linked to experiential\nlearning in the workplace. This paper presents a sophisticated VR-based\nindustrial training architecture aimed at improving learning efficacy via\nhigh-fidelity simulations, dynamic and context-sensitive scenarios, and\nadaptive feedback systems. The suggested system incorporates intuitive\ngesture-based controls, reducing the learning curve for users across all skill\nlevels. A new scoring metric, namely, VR Training Scenario Score (VRTSS), is\nused to assess trainee performance dynamically, guaranteeing ongoing engagement\nand incentive. The experimental assessment of the system reveals promising\noutcomes, with significant enhancements in information retention, task\nexecution precision, and overall training efficacy. The results highlight the\ncapability of VR as a crucial instrument in industrial training, providing a\nscalable, interactive, and efficient substitute for conventional learning\nmethods.", "AI": {"tldr": "This paper discusses a VR-based industrial training system that enhances learning through immersive simulations.", "motivation": "The need to reconcile theoretical knowledge with practical experience in industrial manufacturing training.", "method": "The paper presents a VR training architecture featuring high-fidelity simulations, dynamic scenarios, and adaptive feedback systems, with gesture-based controls.", "result": "Experimental results indicate significant improvements in information retention, task execution precision, and overall training efficacy.", "conclusion": "The study demonstrates VR's potential as a scalable and efficient alternative to traditional training methods in industrial settings.", "key_contributions": ["Introduction of VR Training Scenario Score (VRTSS) for performance assessment", "Implementation of gesture-based controls for improved accessibility", "Demonstrated significant improvements in trainee outcomes compared to conventional methods."], "limitations": "", "keywords": ["Virtual Reality", "Industrial Training", "Gesture-based Controls", "Training Efficacy", "Information Retention"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.21071", "pdf": "https://arxiv.org/pdf/2507.21071.pdf", "abs": "https://arxiv.org/abs/2507.21071", "title": "FingerTip 20K: A Benchmark for Proactive and Personalized Mobile LLM Agents", "authors": ["Qinglong Yang", "Haoming Li", "Haotian Zhao", "Xiaokai Yan", "Jingtao Ding", "Fengli Xu", "Yong Li"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Mobile GUI agents are becoming critical tools for enhancing human-device\ninteraction efficiency, with multimodal large language models (MLLMs) emerging\nas dominant paradigms in this domain. Current agents, however, are limited to\nfollowing explicit human instructions, resulting in insufficient capability for\nproactive intent anticipation. Additionally, these agents fail to leverage the\ncontextual information associated with users during task execution, thereby\nneglecting potentially vast differences in user preferences. To address these\nchallenges, we introduce the FingerTip benchmark. It contains two new tracks:\nproactive task suggestions by analyzing environment observation and users'\nprevious intents, and personalized task execution by catering to users' action\npreferences. We collected unique human demonstrations of multi-step Android\ndevice interactions across a variety of everyday apps. These demonstrations are\nnot isolated but are continuously acquired from the users' long-term usage in\ntheir real lives, and encompass essential user-related contextual information.\nOur experiments reveal challenges of the tasks we propose. The model fine-tuned\nwith the data we collected effectively utilized user information and achieved\ngood results, highlighting the potential of our approach in building more\nuser-oriented mobile GUI agents. Our code is open-source at\nhttps://anonymous.4open.science/r/FingerTip-57B8 for reproducibility.", "AI": {"tldr": "The paper introduces the FingerTip benchmark for proactive and personalized mobile GUI agents utilizing multimodal large language models.", "motivation": "To enhance human-device interaction by enabling proactive intent anticipation and leveraging user context during task execution.", "method": "The FingerTip benchmark consists of tracks for proactive task suggestions and personalized task execution, collecting human demonstrations of multi-step Android interactions in real-life contexts.", "result": "Experiments demonstrated that models fine-tuned with collected data effectively utilized user information to enhance task performance, revealing challenges in the proposed tasks.", "conclusion": "The findings suggest the potential for creating more user-oriented mobile GUI agents through proactive and context-aware capabilities.", "key_contributions": ["Introduction of the FingerTip benchmark for evaluating mobile GUI agents.", "Collection of real-life, context-rich user interaction data.", "Demonstration of improved model performance leveraging user preferences."], "limitations": "Challenges in task execution and the need for continuous user interaction data to maintain effectiveness.", "keywords": ["mobile GUI agents", "multimodal large language models", "human-device interaction", "proactive task suggestions", "personalized task execution"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.21072", "pdf": "https://arxiv.org/pdf/2507.21072.pdf", "abs": "https://arxiv.org/abs/2507.21072", "title": "Snap, Segment, Deploy: A Visual Data and Detection Pipeline for Wearable Industrial Assistants", "authors": ["Di Wen", "Junwei Zheng", "Ruiping Liu", "Yi Xu", "Kunyu Peng", "Rainer Stiefelhagen"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Industrial assembly tasks increasingly demand rapid adaptation to complex\nprocedures and varied components, yet are often conducted in environments with\nlimited computing, connectivity, and strict privacy requirements. These\nconstraints make conventional cloud-based or fully autonomous solutions\nimpractical for factory deployment. This paper introduces a mobile-device-based\nassistant system for industrial training and operational support, enabling\nreal-time, semi-hands-free interaction through on-device perception and voice\ninterfaces. The system integrates lightweight object detection, speech\nrecognition, and Retrieval-Augmented Generation (RAG) into a modular on-device\npipeline that operates entirely on-device, enabling intuitive support for part\nhandling and procedure understanding without relying on manual supervision or\ncloud services. To enable scalable training, we adopt an automated data\nconstruction pipeline and introduce a two-stage refinement strategy to improve\nvisual robustness under domain shift. Experiments on our generated dataset,\ni.e., Gear8, demonstrate improved robustness to domain shift and common visual\ncorruptions. A structured user study further confirms its practical viability,\nwith positive user feedback on the clarity of the guidance and the quality of\nthe interaction. These results indicate that our framework offers a deployable\nsolution for real-time, privacy-preserving smart assistance in industrial\nenvironments. We will release the Gear8 dataset and source code upon\nacceptance.", "AI": {"tldr": "A mobile-device-based assistant system for industrial tasks that operates entirely on-device, offering real-time support through voice interfaces and perception technologies.", "motivation": "To address the challenges of rapid adaptation and support in industrial assembly tasks under limited computing and privacy constraints.", "method": "Developed a modular on-device pipeline integrating object detection, speech recognition, and Retrieval-Augmented Generation (RAG) for real-time interaction.", "result": "Experiments with the Gear8 dataset showed improved robustness to domain shifts and positive user feedback confirmed the system's practical viability.", "conclusion": "The framework provides a deployable, privacy-preserving smart assistance solution for industrial environments.", "key_contributions": ["Introduction of an on-device mobile assistant for industrial training.", "Integration of lightweight object detection and speech recognition for semi-hands-free interaction.", "Creation of the Gear8 dataset and demonstration of improved robustness to domain shifts."], "limitations": "", "keywords": ["Industrial assistance", "On-device processing", "Retrieval-Augmented Generation", "Voice interfaces", "Privacy-preserving solutions"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.21074", "pdf": "https://arxiv.org/pdf/2507.21074.pdf", "abs": "https://arxiv.org/abs/2507.21074", "title": "Empowering Educators in the Age of AI: An Empirical Study on Creating custom GPTs in Qualitative Research Method education", "authors": ["Qian Huang", "Thijs Willems"], "categories": ["cs.HC", "cs.AI"], "comment": "20 pages", "summary": "As generative AI (Gen-AI) tools become more prevalent in education, there is\na growing need to understand how educators, not just students, can actively\nshape their design and use. This study investigates how two instructors\nintegrated four custom GPT tools into a Masters-level Qualitative Research\nMethods course for Urban Planning Policy students. Addressing two key gaps: the\ndominant framing of students as passive AI users, and the limited use of AI in\nqualitative methods education. The study explores how Gen-AI can support\ndisciplinary learning when aligned with pedagogical intent. Drawing on the\nTechnological Pedagogical Content Knowledge (TPACK) framework and action\nresearch methodology, the instructors designed GPTs to scaffold tasks such as\nresearch question formulation, interview practice, fieldnote analysis, and\ndesign thinking. Thematic analysis of student reflections, AI chat logs, and\nfinal assignments revealed that the tools enhanced student reflexivity,\nimproved interview techniques, and supported structured analytic thinking.\nHowever, students also expressed concerns about cognitive overload, reduced\nimmersion in data, and the formulaic nature of AI responses. The study offers\nthree key insights: AI can be a powerful scaffold for active learning when\npaired with human facilitation; custom GPTs can serve as cognitive partners in\niterative research practice; and educator-led design is critical to\npedagogically meaningful AI integration. This research contributes to emerging\nscholarship on AI in higher education by demonstrating how empowering educators\nto design custom tools can promote more reflective, responsible, and\ncollaborative learning with AI.", "AI": {"tldr": "This study explores the integration of custom GPT tools in a Master's-level Qualitative Research Methods course, highlighting how educators can design AI applications to enhance learning.", "motivation": "To understand how educators can actively shape the design and use of generative AI tools in education, moving beyond the view of students as passive users.", "method": "The study employed action research methodology and the TPACK framework to investigate the integration of custom GPT tools for various educational tasks.", "result": "The thematic analysis revealed that the use of GPT tools enhanced student reflexivity, improved interview techniques, and supported structured analytic thinking, despite some students expressing concerns about cognitive overload and reduced immersion.", "conclusion": "The study concludes that educator-led design of AI tools is crucial for meaningful integration of AI in education, promoting reflective and collaborative learning.", "key_contributions": ["Demonstrated the potential of AI as a scaffold for active learning when paired with human facilitation.", "Showed custom GPTs can act as cognitive partners in research practice.", "Highlighted the importance of educator-led design for successful AI integration in pedagogy."], "limitations": "Students expressed concerns about cognitive overload and the formulaic nature of AI responses.", "keywords": ["Generative AI", "Qualitative Research", "Educator Design", "AI in Education", "Pedagogical Intent"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2507.21058", "pdf": "https://arxiv.org/pdf/2507.21058.pdf", "abs": "https://arxiv.org/abs/2507.21058", "title": "Categorical Classification of Book Summaries Using Word Embedding Techniques", "authors": ["Kerem Keskin", "Mümine Kaya Keleş"], "categories": ["cs.CL", "cs.AI"], "comment": "in Turkish language. This paper was published in the proceedings of\n  the 6th International Conference on Data Science and Applications ICONDATA24,\n  held on September between 2 and 6, 2024, in Pristina, Kosovo. For full text\n  book see https://www.icondata.org/en/proceedings-books", "summary": "In this study, book summaries and categories taken from book sites were\nclassified using word embedding methods, natural language processing techniques\nand machine learning algorithms. In addition, one hot encoding, Word2Vec and\nTerm Frequency - Inverse Document Frequency (TF-IDF) methods, which are\nfrequently used word embedding methods were used in this study and their\nsuccess was compared. Additionally, the combination table of the pre-processing\nmethods used is shown and added to the table. Looking at the results, it was\nobserved that Support Vector Machine, Naive Bayes and Logistic Regression\nModels and TF-IDF and One-Hot Encoder word embedding techniques gave more\nsuccessful results for Turkish texts.", "AI": {"tldr": "The study classifies book summaries and categories using word embeddings, NLP techniques, and machine learning algorithms, focusing on methods like TF-IDF and Word2Vec, with a successful application on Turkish texts.", "motivation": "To classify book summaries and categories effectively using advanced NLP techniques and machine learning algorithms.", "method": "The study employed word embedding methods including One Hot Encoding, Word2Vec, and TF-IDF, alongside machine learning classifiers such as Support Vector Machine, Naive Bayes, and Logistic Regression.", "result": "The models that utilized TF-IDF and One-Hot Encoding techniques achieved superior results for classifying Turkish texts.", "conclusion": "Support Vector Machine, Naive Bayes, and Logistic Regression models performed well with the chosen word embedding techniques, indicating their effectiveness in this context.", "key_contributions": ["Comparison of multiple word embedding techniques (TF-IDF, One-Hot Encoding, Word2Vec)", "Evaluation of various machine learning algorithms on Turkish texts", "Insights into the performance of ML models in text classification tasks"], "limitations": "The study is focused on Turkish texts, which may limit the generalizability of results to other languages.", "keywords": ["word embedding", "natural language processing", "machine learning", "text classification", "Turkish"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.21075", "pdf": "https://arxiv.org/pdf/2507.21075.pdf", "abs": "https://arxiv.org/abs/2507.21075", "title": "Can LLMs Reason About Trust?: A Pilot Study", "authors": ["Anushka Debnath", "Stephen Cranefield", "Emiliano Lorini", "Bastin Tony Roy Savarimuthu"], "categories": ["cs.HC", "cs.CL", "cs.CY", "cs.MA"], "comment": "17 pages, 5 figures, 3 tables Accepted for presentation as a full\n  paper at the COINE 2025 workshop at AAMAS 2025 see\n  https://coin-workshop.github.io/coine-2025-detroit/accepted_for_presentation.html", "summary": "In human society, trust is an essential component of social attitude that\nhelps build and maintain long-term, healthy relationships which creates a\nstrong foundation for cooperation, enabling individuals to work together\neffectively and achieve shared goals. As many human interactions occur through\nelectronic means such as using mobile apps, the potential arises for AI systems\nto assist users in understanding the social state of their relationships. In\nthis paper we investigate the ability of Large Language Models (LLMs) to reason\nabout trust between two individuals in an environment which requires fostering\ntrust relationships. We also assess whether LLMs are capable of inducing trust\nby role-playing one party in a trust based interaction and planning actions\nwhich can instil trust.", "AI": {"tldr": "The paper explores how Large Language Models (LLMs) can reason about and induce trust in interpersonal interactions that occur through digital communication.", "motivation": "To understand how AI can facilitate trust in electronic interactions, which is critical for collaboration and healthy relationships in society.", "method": "The study investigates LLMs' capability to analyze trust between individuals and simulate interactions to foster trust, using role-play scenarios.", "result": "LLMs show potential in reasoning about trust dynamics and can engage in role-playing to induce trust in specific scenarios.", "conclusion": "Integrating LLMs into applications can enhance users' understanding of trust in their digital interactions, leading to better cooperative outcomes.", "key_contributions": ["Investigation of trust reasoning by LLMs in social interactions.", "Demonstration of LLM role-playing to foster trust between parties.", "Assessment of trust dynamics in electronic communication."], "limitations": "The study is limited to specific scenarios and may not generalize to all trust dynamics.", "keywords": ["trust", "Large Language Models", "human-computer interaction", "social relationships", "role-playing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.21065", "pdf": "https://arxiv.org/pdf/2507.21065.pdf", "abs": "https://arxiv.org/abs/2507.21065", "title": "Dialogic Social Learning for Artificial Agents: Enhancing LLM Ontology Acquisition through Mixed-Initiative Educational Interactions", "authors": ["Sabrina Patania", "Luca Annese", "Cansu Koyuturk", "Azzurra Ruggeri", "Dimitri Ognibene"], "categories": ["cs.CL", "cs.HC", "cs.LG", "cs.RO", "I.2.7, I.2.9, j.4,"], "comment": "submitted to ICSR2025", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing extensive offline datasets. However, they often face challenges in\nacquiring and integrating complex, knowledge online. Traditional AI training\nparadigms, predominantly based on supervised learning or reinforcement\nlearning, mirror a 'Piagetian' model of independent exploration. These\napproaches typically rely on large datasets and sparse feedback signals,\nlimiting the models' ability to learn efficiently from interactions. Drawing\ninspiration from Vygotsky's sociocultural theory, this study explores the\npotential of socially mediated learning paradigms to address these limitations.\n  We introduce a dynamic environment, termed the 'AI Social Gym', where an AI\nlearner agent engages in dyadic pedagogical dialogues with knowledgeable AI\nteacher agents. These interactions emphasize external, structured dialogue as a\ncore mechanism for knowledge acquisition, contrasting with methods that depend\nsolely on internal inference or pattern recognition.\n  Our investigation focuses on how different pedagogical strategies impact the\nAI learning process in the context of ontology acquisition. Empirical results\nindicate that such dialogic approaches-particularly those involving\nmixed-direction interactions combining top-down explanations with\nlearner-initiated questioning-significantly enhance the LLM's ability to\nacquire and apply new knowledge, outperforming both unidirectional\ninstructional methods and direct access to structured knowledge, formats\ntypically present in training datasets.\n  These findings suggest that integrating pedagogical and psychological\ninsights into AI and robot training can substantially improve post-training\nknowledge acquisition and response quality. This approach offers a\ncomplementary pathway to existing strategies like prompt engineering", "AI": {"tldr": "Explores socially mediated learning for AI models using dyadic dialogues between learner and teacher agents to enhance knowledge acquisition.", "motivation": "Traditional AI learning methods struggle with integrating complex knowledge. This study seeks to improve knowledge acquisition in LLMs via sociocultural learning paradigms.", "method": "Introduced the 'AI Social Gym' where AI learner engages with teacher agents in structured dialogues, assessing different pedagogical strategies for knowledge acquisition.", "result": "Mixed-direction interactions significantly enhance LLM's ontology acquisition, outperforming traditional instructional methods and direct knowledge access.", "conclusion": "Integrating pedagogical insights into AI training can improve post-training knowledge acquisition and response quality, presenting a valuable alternative to existing approaches like prompt engineering.", "key_contributions": ["Proposed a novel environment for AI learning called 'AI Social Gym'.", "Demonstrated the effectiveness of mixed-direction pedagogical strategies in knowledge acquisition.", "Highlighted the importance of integrating psychological theories into AI training."], "limitations": "The study may not account for all types of knowledge integration and context variations in real-world applications.", "keywords": ["Large Language Models", "Social Learning", "Pedagogical Strategies", "Knowledge Acquisition", "AI Training"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.21077", "pdf": "https://arxiv.org/pdf/2507.21077.pdf", "abs": "https://arxiv.org/abs/2507.21077", "title": "Data-Driven and Participatory Approaches toward Neuro-Inclusive AI", "authors": ["Naba Rizvi"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "PhD Dissertation at UC San Diego (June 2025)", "summary": "Biased data representation in AI marginalizes up to 75 million autistic\npeople worldwide through medical applications viewing autism as a deficit of\nneurotypical social skills rather than an aspect of human diversity, and this\nperspective is grounded in research questioning the humanity of autistic\npeople. Turing defined artificial intelligence as the ability to mimic human\ncommunication, and as AI development increasingly focuses on human-like agents,\nthis benchmark remains popular. In contrast, we define Neuro-Inclusive AI as\ndatasets and systems that move away from mimicking humanness as a benchmark for\nmachine intelligence. Then, we explore the origins, prevalence, and impact of\nanti-autistic biases in current research. Our work finds that 90% of human-like\nAI agents exclude autistic perspectives, and AI creators continue to believe\nethical considerations are beyond the scope of their work. To improve the\nautistic representation in data, we conduct empirical experiments with\nannotators and LLMs, finding that binary labeling schemes sufficiently capture\nthe nuances of labeling anti-autistic hate speech. Our benchmark, AUTALIC, can\nbe used to evaluate or fine-tune models, and was developed to serve as a\nfoundation for more neuro-inclusive future work.", "AI": {"tldr": "The paper critiques biased AI representations that marginalize autistic individuals and proposes Neuro-Inclusive AI which emphasizes neurodiversity over mimicking human communication.", "motivation": "To address the marginalization of autistic individuals in AI systems and propose frameworks for more inclusive representation.", "method": "Empirical experiments with annotators and LLMs to assess binary labeling of anti-autistic hate speech; creation of an evaluation benchmark called AUTALIC.", "result": "90% of current human-like AI agents exclude autistic perspectives; the AUTALIC benchmark allows for the evaluation and fine-tuning of models for better representation.", "conclusion": "There is a significant lack of ethical consideration in AI developing environments, necessitating the establishment of Neuro-Inclusive AI standards and practices.", "key_contributions": ["Definition and exploration of Neuro-Inclusive AI", "Development of the AUTALIC benchmark for evaluating anti-autistic hate speech", "Empirical evidence on the exclusion of autistic perspectives in AI systems"], "limitations": "Ethical considerations in AI development are often disregarded, posing challenges for implementation.", "keywords": ["Neuro-Inclusive AI", "autism", "bias", "machine learning", "human-computer interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.21073", "pdf": "https://arxiv.org/pdf/2507.21073.pdf", "abs": "https://arxiv.org/abs/2507.21073", "title": "Product vs. Process: Exploring EFL Students' Editing of AI-Generated Text for Expository Writing", "authors": ["David James Woo", "Yangyang Yu", "Kai Guo", "Yilin Huang", "April Ka Yeng Fung"], "categories": ["cs.CL", "cs.HC"], "comment": "45 pages, 11 figures", "summary": "Text generated by artificial intelligence (AI) chatbots is increasingly used\nin English as a foreign language (EFL) writing contexts, yet its impact on\nstudents' expository writing process and compositions remains understudied.\nThis research examines how EFL secondary students edit AI-generated text.\nExploring editing behaviors in their expository writing process and in\nexpository compositions, and their effect on human-rated scores for content,\norganization, language, and overall quality. Participants were 39 Hong Kong\nsecondary students who wrote an expository composition with AI chatbots in a\nworkshop. A convergent design was employed to analyze their screen recordings\nand compositions to examine students' editing behaviors and writing qualities.\nAnalytical methods included qualitative coding, descriptive statistics,\ntemporal sequence analysis, human-rated scoring, and multiple linear regression\nanalysis. We analyzed over 260 edits per dataset, and identified two editing\npatterns: one where students refined introductory units repeatedly before\nprogressing, and another where they quickly shifted to extensive edits in body\nunits (e.g., topic and supporting sentences). MLR analyses revealed that the\nnumber of AI-generated words positively predicted all score dimensions, while\nmost editing variables showed minimal impact. These results suggest a\ndisconnect between students' significant editing effort and improved\ncomposition quality, indicating AI supports but does not replace writing\nskills. The findings highlight the importance of genre-specific instruction and\nprocess-focused writing before AI integration. Educators should also develop\nassessments valuing both process and product to encourage critical engagement\nwith AI text.", "AI": {"tldr": "This research examines EFL secondary students' editing behaviors with AI-generated texts in their writing process.", "motivation": "To investigate the impact of AI-generated text on students' expository writing and editing behaviors, as its use in EFL contexts is still understudied.", "method": "A convergent design analyzing screen recordings and compositions of 39 Hong Kong secondary students using qualitative coding, descriptive statistics, temporal sequence analysis, human-rated scoring, and multiple linear regression analysis.", "result": "Identified two editing patterns among students and found that while AI-generated words positively correlated with composition scores, editing effort had minimal impact on quality.", "conclusion": "AI supports students' writing but doesn't replace writing skills; genre-specific instruction and assessments that value the writing process are important.", "key_contributions": ["Investigated the editing behaviors of students using AI-generated texts", "Identified significant patterns in editing processes during expository writing", "Highlighted the need for genre-specific instruction regarding AI integration"], "limitations": "", "keywords": ["AI in education", "EFL writing", "editing behaviors", "composition quality", "human-rated scoring"], "importance_score": 6, "read_time_minutes": 45}}
{"id": "2507.21078", "pdf": "https://arxiv.org/pdf/2507.21078.pdf", "abs": "https://arxiv.org/abs/2507.21078", "title": "What Makes a Level Hard in Super Mario Maker 2?", "authors": ["Carlo A. Furia", "Andrea Mocci"], "categories": ["cs.HC", "cs.SE"], "comment": null, "summary": "Games like Super Mario Maker 2 (SMM2) lower the barrier for casual users to\nbecome level designers. In this paper, we set out to analyze a vast amount of\ndata about SMM2 user-written levels, in order to understand what factors affect\na level's difficulty as experienced by other users. To this end, we perform two\nkinds of analyses: one based on regression models and one using natural\nlanguage processing techniques. The main results shed light on which level\ncharacteristics (e.g., its style, popularity, timing) and which topics and\nsentiments have a consistent association with easier or harder levels. While\nnone of our findings are startling, they help distill some key differences\nbetween easy and hard SMM2 levels, which, in turn, can pave the way for a\nbetter understanding of end-user level design.", "AI": {"tldr": "Analyzes factors affecting difficulty in user-designed levels of Super Mario Maker 2.", "motivation": "To understand what factors influence the difficulty of user-created levels in Super Mario Maker 2.", "method": "The paper employs regression models and natural language processing techniques to analyze user-written levels data.", "result": "Identifies level characteristics and sentiments consistently associated with levels' difficulties.", "conclusion": "The findings provide insights into differences between easy and hard levels, contributing to better understanding of user-level design.", "key_contributions": ["Analysis of user-level design data from SMM2", "Identification of level characteristics affecting difficulty", "Use of regression and NLP techniques for evaluation"], "limitations": "", "keywords": ["Super Mario Maker 2", "level design", "difficulty analysis", "natural language processing", "user-generated content"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.21080", "pdf": "https://arxiv.org/pdf/2507.21080.pdf", "abs": "https://arxiv.org/abs/2507.21080", "title": "Which symbol grounding problem should we try to solve?", "authors": ["Vincent C. Müller"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Floridi and Taddeo propose a condition of \"zero semantic commitment\" for\nsolutions to the grounding problem, and a solution to it. I argue briefly that\ntheir condition cannot be fulfilled, not even by their own solution. After a\nlook at Luc Steels' very different competing suggestion, I suggest that we need\nto re-think what the problem is and what role the 'goals' in a system play in\nformulating the problem. On the basis of a proper understanding of computing, I\ncome to the conclusion that the only sensible grounding problem is how we can\nexplain and re-produce the behavioral ability and function of meaning in\nartificial computational agents", "AI": {"tldr": "The paper critiques Floridi and Taddeo's 'zero semantic commitment' condition for the grounding problem and proposes a re-thinking of the problem itself.", "motivation": "To address the limitations of the 'zero semantic commitment' condition proposed by Floridi and Taddeo regarding the grounding problem in artificial agents.", "method": "The author critiques existing proposals, especially contrasting Floridi and Taddeo's condition with Luc Steels' suggestion, advocating for a re-evaluation of the grounding problem's definition and the role of system goals.", "result": "The author concludes that the true grounding problem involves understanding and reproducing the behavioral ability and function of meaning in computational agents rather than adhering strictly to previous definitions.", "conclusion": "A re-assessment of the grounding problem is necessary, focusing on how to explain and reproduce meaning-related behaviors in artificial systems.", "key_contributions": ["Critique of 'zero semantic commitment' as insufficient for grounding problem solutions.", "Introduction of a new perspective on defining the grounding problem.", "Emphasis on the role of 'goals' in computational agent design."], "limitations": "", "keywords": ["grounding problem", "artificial agents", "semantic commitment"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2507.21079", "pdf": "https://arxiv.org/pdf/2507.21079.pdf", "abs": "https://arxiv.org/abs/2507.21079", "title": "Metaverse Support Groups for LGBTQ+ Youth: An Observational Study on Safety, Self-Expression, and Early Intervention", "authors": ["Joe Hasei", "Yosuke Matsumoto", "Hiroki Kawai", "Yuko Okahisa", "Manabu Takaki", "Toshifumi Ozaki"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "This study assessed metaverse-based support groups designed to reduce social\nisolation and suicide risk among LGBTQ+ youths. Using the Cluster platform,\nenhanced anonymity, avatar-based self-expression, and accessibility were\nprovided. Key findings showed that 79.2% chose avatars matching their gender\nidentity, reporting high satisfaction (mean: 4.10/5) and low discomfort (mean:\n1.79/5). Social confidence significantly improved in virtual spaces compared to\nreal-world interactions (p<0.001), particularly among participants with\ninitially low confidence, averaging an increase of 2.08 points. About half of\nthe first-time participants were 16 or younger, highlighting potential for\nearly intervention. The metaverse scored higher than real-world environments\nfor safety/privacy (3.94/5), self-expression (4.02/5), and accessibility\n(4.21/5). Additionally, 73.6% reported feeling more accepted virtually.\nHowever, some highly confident individuals offline experienced mild adaptation\nchallenges, averaging a confidence decrease of 0.58 points, indicating virtual\nsupport complements rather than replaces in-person services. These findings\nsuggest metaverse-based support effectively lowers psychological barriers and\nprovides affirming spaces, potentially reducing severe outcomes such as\nsuicidal ideation. Future studies should focus on integrating virtual support\nwith existing community and clinical frameworks to enhance long-term impacts.", "AI": {"tldr": "This study evaluates metaverse-based support groups aimed at reducing social isolation and suicide risk among LGBTQ+ youths, finding significant improvements in social confidence and high user satisfaction.", "motivation": "To address social isolation and suicide risk among LGBTQ+ youths through innovative online support mechanisms.", "method": "Participants engaged in metaverse-based support groups on the Cluster platform, utilizing avatars for anonymity and self-expression.", "result": "79.2% of participants chose gender-matching avatars, with significant improvements in social confidence, particularly among those with initially low confidence (p<0.001).", "conclusion": "Metaverse-based support offers effective psychological support, enhancing self-expression and acceptance, while complementing traditional in-person services.", "key_contributions": ["Demonstrated high satisfaction and low discomfort in avatar use for LGBTQ+ youths", "Showed metaverse environments improve social confidence and provide safer spaces", "Reinforced the importance of integrated virtual and in-person support frameworks"], "limitations": "Highly confident individuals offline faced adaptation challenges, suggesting that virtual support should complement, not replace, physical interactions.", "keywords": ["metaverse", "LGBTQ+ support", "social isolation", "suicide risk", "virtual environments"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.21083", "pdf": "https://arxiv.org/pdf/2507.21083.pdf", "abs": "https://arxiv.org/abs/2507.21083", "title": "ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional Framing Induces Bias in LLM Outputs", "authors": ["Franck Bardol"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models like GPT-4 adjust their responses not only based on the\nquestion asked, but also on how it is emotionally phrased. We systematically\nvary the emotional tone of 156 prompts - spanning controversial and everyday\ntopics - and analyze how it affects model responses. Our findings show that\nGPT-4 is three times less likely to respond negatively to a negatively framed\nquestion than to a neutral one. This suggests a \"rebound\" bias where the model\novercorrects, often shifting toward neutrality or positivity. On sensitive\ntopics (e.g., justice or politics), this effect is even more pronounced:\ntone-based variation is suppressed, suggesting an alignment override. We\nintroduce concepts like the \"tone floor\" - a lower bound in response negativity\n- and use tone-valence transition matrices to quantify behavior. Visualizations\nbased on 1536-dimensional embeddings confirm semantic drift based on tone. Our\nwork highlights an underexplored class of biases driven by emotional framing in\nprompts, with implications for AI alignment and trust. Code and data are\navailable at: https://github.com/bardolfranck/llm-responses-viewer", "AI": {"tldr": "This paper investigates how emotional phrasing of prompts affects the responses of GPT-4, revealing significant biases influenced by the tone of the input.", "motivation": "To explore how large language models adjust their responses based on the emotional tone of inquiries and understand the implications for AI alignment and trust.", "method": "The study systematically varied the emotional tone of 156 prompts and analyzed the resulting model responses, focusing on negative and neutral framing.", "result": "Findings indicate that GPT-4 is three times less likely to respond negatively to negatively framed prompts compared to neutral ones, highlighting a rebound bias and suppressed tone variation in sensitive topics.", "conclusion": "The research demonstrates significant tone-driven biases in LLM responses, calling for further examination in the context of AI alignment and user trust.", "key_contributions": ["Identification of the 'rebound' bias in LLM responses to emotional phrasing", "Introduction of the 'tone floor' concept to quantify response negativity", "Utilization of tone-valence transition matrices to analyze model behavior"], "limitations": "Limited scope of prompts and contexts analyzed; results may not generalize to all types of language models or applications.", "keywords": ["Large Language Models", "Emotional Framing", "AI Alignment", "Biases in AI", "GPT-4"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.21081", "pdf": "https://arxiv.org/pdf/2507.21081.pdf", "abs": "https://arxiv.org/abs/2507.21081", "title": "Empathy in Explanation", "authors": ["Katherine M. Collins", "Kartik Chandra", "Adrian Weller", "Jonathan Ragan-Kelley", "Joshua B. Tenenbaum"], "categories": ["cs.HC", "cs.AI"], "comment": "CogSci non-archival conference paper", "summary": "Why do we give the explanations we do? Recent work has suggested that we\nshould think of explanation as a kind of cooperative social interaction,\nbetween a why-question-asker and an explainer. Here, we apply this perspective\nto consider the role that emotion plays in this social interaction. We develop\na computational framework for modeling explainers who consider the emotional\nimpact an explanation might have on a listener. We test our framework by using\nit to model human intuitions about how a doctor might explain to a patient why\nthey have a disease, taking into account the patient's propensity for regret.\nOur model predicts human intuitions well, better than emotion-agnostic\nablations, suggesting that people do indeed reason about emotion when giving\nexplanations.", "AI": {"tldr": "The paper presents a computational framework examining the role of emotion in explanations, specifically in a medical context, and shows that emotional considerations improve predictive accuracy.", "motivation": "To investigate why and how emotion influences the process of giving explanations, particularly in a cooperative interaction between explainer and listener.", "method": "Developed a computational model that simulates explainers considering emotional impact, validated against human intuitions in doctor-patient contexts.", "result": "The model accurately predicts human intuition regarding emotional considerations in explanations, outperforming non-emotional baseline models.", "conclusion": "Emotional factors play a significant role in how explanations are formulated, highlighting the need for incorporating emotional context in AI systems for better human interaction.", "key_contributions": ["Introduces a novel computational framework for modeling explanations considering emotion.", "Demonstrates through empirical testing that emotion-aware explanations align more closely with human intuition.", "Provides insights into doctor-patient communication dynamics."], "limitations": "The research is based on a specific domain (medical explanations) and may not generalize to other contexts.", "keywords": ["explanation", "emotion", "human-computer interaction", "computational modeling", "doctor-patient communication"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.21084", "pdf": "https://arxiv.org/pdf/2507.21084.pdf", "abs": "https://arxiv.org/abs/2507.21084", "title": "Reviving Your MNEME: Predicting The Side Effects of LLM Unlearning and Fine-Tuning via Sparse Model Diffing", "authors": ["Aly M. Kassem", "Zhuan Shi", "Negar Rostamzadeh", "Golnoosh Farnadi"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are frequently fine-tuned or unlearned to adapt\nto new tasks or eliminate undesirable behaviors. While existing evaluation\nmethods assess performance after such interventions, there remains no general\napproach for detecting unintended side effects, such as unlearning biology\ncontent degrading performance on chemistry tasks, particularly when these\neffects are unpredictable or emergent. To address this issue, we introduce\nMNEME, Model diffiNg for Evaluating Mechanistic Effects, a lightweight\nframework for identifying these side effects using sparse model diffing. MNEME\ncompares base and fine-tuned models on task-agnostic data (for example, The\nPile, LMSYS-Chat-1M) without access to fine-tuning data to isolate behavioral\nshifts. Applied to five LLMs across three scenarios: WMDP knowledge unlearning,\nemergent misalignment, and benign fine-tuning, MNEME achieves up to 95 percent\naccuracy in predicting side effects, aligning with known benchmarks and\nrequiring no custom heuristics. Furthermore, we show that retraining on\nhigh-activation samples can partially reverse these effects. Our results\ndemonstrate that sparse probing and diffing offer a scalable and automated lens\ninto fine-tuning-induced model changes, providing practical tools for\nunderstanding and managing LLM behavior.", "AI": {"tldr": "The paper presents MNEME, a framework to identify unintended side effects of fine-tuning large language models (LLMs) by comparing base and fine-tuned models on task-agnostic data, achieving high prediction accuracy.", "motivation": "There is a lack of general approaches for detecting unintended side effects resulting from fine-tuning of LLMs, which can degrade performance on specific tasks.", "method": "MNEME uses sparse model diffing to compare base and fine-tuned models on task-agnostic data to detect behavioral shifts without needing access to fine-tuning data.", "result": "MNEME achieves up to 95 percent accuracy in predicting side effects across five LLMs and three scenarios, aligning with known benchmarks.", "conclusion": "The framework shows that sparse probing and diffing can provide scalable tools for understanding and managing changes in LLM behavior resulting from fine-tuning.", "key_contributions": ["Introduction of the MNEME framework for detecting unintended side effects in LLMs", "Demonstration of high accuracy in predicting model behavior shifts", "Provision of practical tools for managing fine-tuning effects"], "limitations": "", "keywords": ["large language models", "fine-tuning", "model diffing", "unintended side effects", "HCI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.21088", "pdf": "https://arxiv.org/pdf/2507.21088.pdf", "abs": "https://arxiv.org/abs/2507.21088", "title": "Eliciting User Requirements for AI-Enhanced Learning Environments using a Participatory Approach", "authors": ["Bibeg Limbu", "Irene-Angelica Chounta", "Vilma Sukacke", "Andromachi Filippidi", "Chara Spyropoulou", "Marianna Anagnostopoulou", "Eleftheria Tsourlidaki", "Nikos Karacapilidis"], "categories": ["cs.HC"], "comment": "12 pages, 2 figures, 15th International Conference on Methodologies\n  and Intelligent Systems for Technology Enhanced Learning (mis4tel), Workshop\n  Track: Workshop on Integration of Emerging Technologies into Education and\n  Training (ETELT) https://mis4tel-conference.net/tracks/workshops/etelt,\n  accepted", "summary": "This paper explores the needs \\& expectations of educational stakeholders for\nAI (Artificial Intelligence)-enhanced learning environments. Data was collected\nfollowing two-phased participatory workshops. The first workshop outlined\nstakeholders' profiles in terms of technical and pedagogical characteristics.\nThe qualitative data collected was analysed using deductive thematic analysis\nwith Activity Theory, explicating the user needs. The second workshop\narticulated expectations related to the integration of AI in education.\nInductive thematic analysis of the second workshop led to the elicitation of\nusers' expectations. We cross-examined the needs and expectations, identifying\ncontradictions, to generate user requirements for emerging technologies. The\npaper provides suggestions for future design initiatives that incorporate AI in\nlearning environments.", "AI": {"tldr": "The paper investigates stakeholder needs and expectations for AI in educational contexts through participatory workshops, analyzing qualitative data to formulate user requirements for AI-enhanced learning environments.", "motivation": "To understand the needs and expectations of educational stakeholders regarding the integration of AI technologies in learning environments.", "method": "Two-phased participatory workshops involving qualitative data collection and analysis through deductive and inductive thematic analysis, framed by Activity Theory.", "result": "Identified needs and expectations of users, along with contradictions, leading to user requirements for integrating AI in educational settings.", "conclusion": "The findings provide valuable insights for designing future AI-based educational technologies that meet stakeholder requirements.", "key_contributions": ["Identification of user needs and expectations for AI in education", "Methodological approach using participatory workshops and thematic analysis", "Recommendations for future AI-enhanced learning designs"], "limitations": "The study is limited to specific educational stakeholders and contexts, potentially affecting generalizability.", "keywords": ["AI in education", "stakeholder expectations", "user requirements", "thematic analysis", "participatory workshops"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2507.21086", "pdf": "https://arxiv.org/pdf/2507.21086.pdf", "abs": "https://arxiv.org/abs/2507.21086", "title": "Multi-Amateur Contrastive Decoding for Text Generation", "authors": ["Jaydip Sen", "Subhasis Dasgupta", "Hetvi Waghela"], "categories": ["cs.CL"], "comment": "This paper has been accepted for oral presentation and publication in\n  the proceedings of the IEEE I2ITCON 2025. The conference will be organized in\n  Pune, India, from July 4 to 5, 2025. This is the accepted version of the\n  paper and NOT the final camera-ready version. The paper is 11 pages long and\n  contains 5 figures and 6 tables", "summary": "Contrastive Decoding (CD) has emerged as an effective inference-time strategy\nfor enhancing open-ended text generation by exploiting the divergence in output\nprobabilities between a large expert language model and a smaller amateur\nmodel. Although CD improves coherence and fluency, its dependence on a single\namateur restricts its capacity to capture the diverse and multifaceted failure\nmodes of language generation, such as repetition, hallucination, and stylistic\ndrift. This paper proposes Multi-Amateur Contrastive Decoding (MACD), a\ngeneralization of the CD framework that employs an ensemble of amateur models\nto more comprehensively characterize undesirable generation patterns. MACD\nintegrates contrastive signals through both averaging and consensus\npenalization mechanisms and extends the plausibility constraint to operate\neffectively in the multi-amateur setting. Furthermore, the framework enables\ncontrollable generation by incorporating amateurs with targeted stylistic or\ncontent biases. Experimental results across multiple domains, such as news,\nencyclopedic, and narrative, demonstrate that MACD consistently surpasses\nconventional decoding methods and the original CD approach in terms of fluency,\ncoherence, diversity, and adaptability, all without requiring additional\ntraining or fine-tuning.", "AI": {"tldr": "This paper introduces Multi-Amateur Contrastive Decoding (MACD), an enhancement to contrastive decoding that uses multiple amateur models to improve fluency, coherence, diversity, and adaptability in language generation, addressing limitations of traditional methods.", "motivation": "The paper addresses the limitations of Contrastive Decoding (CD), which relies on a single amateur model, hindering its ability to capture diverse failure modes in language generation.", "method": "MACD employs an ensemble of amateur models, integrating contrastive signals through averaging and consensus penalization, and allows for controllable generation with stylistic or content biases.", "result": "Experimental results indicate that MACD outperforms both traditional decoding methods and the original CD in fluency, coherence, diversity, and adaptability across various content domains without additional training.", "conclusion": "MACD represents a significant advancement in language generation by effectively leveraging multiple amateur models to enhance output quality.", "key_contributions": ["Introduction of Multi-Amateur Contrastive Decoding (MACD)", "Ensemble approach to characterizing generation failure modes", "Facilitates controllable text generation based on stylistic and content biases"], "limitations": "", "keywords": ["Contrastive Decoding", "Multi-Amateur Contrastive Decoding", "Language Generation", "Fluency", "Coherence"], "importance_score": 8, "read_time_minutes": 11}}
{"id": "2507.21089", "pdf": "https://arxiv.org/pdf/2507.21089.pdf", "abs": "https://arxiv.org/abs/2507.21089", "title": "Emotionally Aware Moderation: The Potential of Emotion Monitoring in Shaping Healthier Social Media Conversations", "authors": ["Xiaotian Su", "Naim Zierau", "Soomin Kim", "April Yi Wang", "Thiemo Wambsganss"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Social media platforms increasingly employ proactive moderation techniques,\nsuch as detecting and curbing toxic and uncivil comments, to prevent the spread\nof harmful content. Despite these efforts, such approaches are often criticized\nfor creating a climate of censorship and failing to address the underlying\ncauses of uncivil behavior. Our work makes both theoretical and practical\ncontributions by proposing and evaluating two types of emotion monitoring\ndashboards to users' emotional awareness and mitigate hate speech. In a study\ninvolving 211 participants, we evaluate the effects of the two mechanisms on\nuser commenting behavior and emotional experiences. The results reveal that\nthese interventions effectively increase users' awareness of their emotional\nstates and reduce hate speech. However, our findings also indicate potential\nunintended effects, including increased expression of negative emotions (Angry,\nFear, and Sad) when discussing sensitive issues. These insights provide a basis\nfor further research on integrating proactive emotion regulation tools into\nsocial media platforms to foster healthier digital interactions.", "AI": {"tldr": "The paper evaluates two types of emotion monitoring dashboards aimed at increasing emotional awareness and reducing hate speech on social media, based on a study with 211 participants.", "motivation": "To address criticisms of social media moderation techniques that often lead to censorship and fail to mitigate the root causes of uncivil behavior.", "method": "Conducted a study involving 211 participants to evaluate the effects of two emotion monitoring dashboards on user commenting behavior and emotional experiences.", "result": "The interventions increased users' emotional awareness and reduced hate speech, but also led to increased expression of negative emotions when discussing sensitive issues.", "conclusion": "Integrating proactive emotion regulation tools into social media platforms could foster healthier digital interactions, despite some unintended negative effects.", "key_contributions": ["Proposed two emotion monitoring dashboards to enhance emotional awareness.", "Evaluated the impact on user commenting behavior and emotional experiences.", "Provided insights for further research on emotion regulation tools in social media."], "limitations": "Potential unintended effects include increased negativity in user emotions when discussing sensitive topics.", "keywords": ["emotional awareness", "hate speech", "social media", "proactive moderation", "user behavior"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.21095", "pdf": "https://arxiv.org/pdf/2507.21095.pdf", "abs": "https://arxiv.org/abs/2507.21095", "title": "QU-NLP at CheckThat! 2025: Multilingual Subjectivity in News Articles Detection using Feature-Augmented Transformer Models with Sequential Cross-Lingual Fine-Tuning", "authors": ["Mohammad AL-Smadi"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents our approach to the CheckThat! 2025 Task 1 on\nsubjectivity detection, where systems are challenged to distinguish whether a\nsentence from a news article expresses the subjective view of the author or\npresents an objective view on the covered topic. We propose a feature-augmented\ntransformer architecture that combines contextual embeddings from pre-trained\nlanguage models with statistical and linguistic features. Our system leveraged\npre-trained transformers with additional lexical features: for Arabic we used\nAraELECTRA augmented with part-of-speech (POS) tags and TF-IDF features, while\nfor the other languages we fine-tuned a cross-lingual DeBERTa~V3 model combined\nwith TF-IDF features through a gating mechanism. We evaluated our system in\nmonolingual, multilingual, and zero-shot settings across multiple languages\nincluding English, Arabic, German, Italian, and several unseen languages. The\nresults demonstrate the effectiveness of our approach, achieving competitive\nperformance across different languages with notable success in the monolingual\nsetting for English (rank 1st with macro-F1=0.8052), German (rank 3rd with\nmacro-F1=0.8013), Arabic (rank 4th with macro-F1=0.5771), and Romanian (rank\n1st with macro-F1=0.8126) in the zero-shot setting. We also conducted an\nablation analysis that demonstrated the importance of combining TF-IDF features\nwith the gating mechanism and the cross-lingual transfer for subjectivity\ndetection. Furthermore, our analysis reveals the model's sensitivity to both\nthe order of cross-lingual fine-tuning and the linguistic proximity of the\ntraining languages.", "AI": {"tldr": "The paper introduces a feature-augmented transformer architecture for subjectivity detection, achieving high performance across multiple languages.", "motivation": "To develop a system that can effectively distinguish between subjective and objective sentences in news articles across different languages.", "method": "The approach combines contextual embeddings from pre-trained language models with additional statistical and linguistic features, utilizing AraELECTRA and a cross-lingual DeBERTa~V3 model.", "result": "The system showed competitive performance, ranking 1st in English and Romanian in zero-shot settings, and performing well in monolingual tasks for several languages.", "conclusion": "The findings underscore the effectiveness of incorporating TF-IDF features and cross-lingual transfer in subjectivity detection tasks.", "key_contributions": ["Feature-augmented transformer architecture", "Successful application in monolingual and zero-shot settings across multiple languages", "Importance of TF-IDF features and gating mechanism in improving model performance"], "limitations": "", "keywords": ["subjectivity detection", "transformer architecture", "cross-lingual", "TF-IDF", "natural language processing"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.21090", "pdf": "https://arxiv.org/pdf/2507.21090.pdf", "abs": "https://arxiv.org/abs/2507.21090", "title": "Thinking Like a Scientist: Can Interactive Simulations Foster Critical AI Literacy?", "authors": ["Yiling Zhao", "Audrey Michal", "Nithum Thain", "Hari Subramonyam"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "As AI systems shape individual and societal decisions, fostering critical AI\nliteracy is essential. Traditional approaches, such as blog articles, static\nlessons, and social media discussions, often fail to support deep conceptual\nunderstanding and critical engagement. This study examines whether interactive\nsimulations can help learners think like a scientist by engaging them in\nhypothesis testing, experimentation, and direct observation of AI behavior. In\na controlled study with 605 participants, we assess how interactive AI\ntutorials impact learning of key concepts such as fairness, dataset\nrepresentativeness, and bias in language models. Results show that interactive\nsimulations effectively enhance AI literacy across topics, supporting greater\nknowledge transfer and self-reported confidence, though engagement alone does\nnot predict learning. This work contributes to the growing field of AI literacy\neducation, highlighting how interactive, inquiry-driven methodologies can\nbetter equip individuals to critically engage with AI in their daily lives.", "AI": {"tldr": "This study explores the effectiveness of interactive simulations in enhancing AI literacy, comparing it to traditional educational methods.", "motivation": "The need for fostering critical AI literacy as AI increasingly influences decisions made by individuals and society.", "method": "Controlled study with 605 participants examining the impact of interactive AI tutorials on learning about fairness, dataset representativeness, and bias.", "result": "Interactive simulations enhance AI literacy and knowledge transfer, resulting in increased self-reported confidence among participants, though engagement does not predict learning outcomes.", "conclusion": "Interactive, inquiry-driven methodologies are more effective for educating individuals about AI, enabling critical engagement with AI technologies.", "key_contributions": ["Demonstration of interactive simulations' effectiveness in teaching AI concepts", "Evidence of improved knowledge transfer and confidence in understanding AI bias", "Critique of traditional educational methods for AI literacy"], "limitations": "", "keywords": ["AI literacy", "interactive simulations", "education", "bias in language models", "learning methodologies"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.21099", "pdf": "https://arxiv.org/pdf/2507.21099.pdf", "abs": "https://arxiv.org/abs/2507.21099", "title": "Rewrite-to-Rank: Optimizing Ad Visibility via Retrieval-Aware Text Rewriting", "authors": ["Chloe Ho", "Ishneet Sukhvinder Singh", "Diya Sharma", "Tanvi Reddy Anumandla", "Michael Lu", "Vasu Sharma", "Kevin Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Search algorithms and user query relevance have given LLMs the ability to\nreturn relevant information, but the effect of content phrasing on ad\nvisibility remains underexplored. We investigate how LLM-based rewriting of\nadvertisements can improve their ranking in retrieval systems and inclusion in\ngenerated LLM responses, without modifying the retrieval model itself. We\nintroduce a supervised fine-tuning framework with a custom loss balancing\nsemantic relevance and content fidelity. To evaluate effectiveness, we propose\ntwo metrics: DeltaMRR@K (ranking improvement) and DeltaDIR@K (inclusion\nfrequency improvement). Our approach presents a scalable method to optimize ad\nphrasing, enhancing visibility in retrieval-based LLM workflows. Experiments\nacross both instruction-based and few-shot prompting demonstrate that PPO\ntrained models outperform both prompt engineering and supervised fine-tuning in\nmost cases, achieving up to a 2.79 DeltaDIR@5 and 0.0073 DeltaMRR@5 in\ninstruction-based prompting. These results highlight the importance of how the\nad is written before retrieval and prompt format and reinforcement learning in\neffective ad rewriting for LLM integrated retrieval systems.", "AI": {"tldr": "Explores LLM-based advertisement rewriting to improve visibility in retrieval systems using a supervised fine-tuning framework.", "motivation": "Understand the impact of ad content phrasing on their visibility without changing retrieval models.", "method": "Implemented a supervised fine-tuning framework focusing on semantic relevance and content fidelity; evaluated using DeltaMRR@K and DeltaDIR@K metrics.", "result": "PPO trained models showed superior performance over prompt engineering and supervised fine-tuning, achieving improved metrics for ad visibility.", "conclusion": "Ad rewriting significantly influences retrieval and prompt effectiveness; reinforcement learning methods can optimize ad phrasing.", "key_contributions": ["Proposed a supervised fine-tuning framework for ad rewriting", "Introduced DeltaMRR@K and DeltaDIR@K as evaluation metrics", "Demonstrated PPO models outperforming traditional methods in ad visibility enhancement"], "limitations": "", "keywords": ["LLM", "advertisement rewriting", "retrieval systems", "fine-tuning", "reinforcement learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.21124", "pdf": "https://arxiv.org/pdf/2507.21124.pdf", "abs": "https://arxiv.org/abs/2507.21124", "title": "VizGenie: Toward Self-Refining, Domain-Aware Workflows for Next-Generation Scientific Visualization", "authors": ["Ayan Biswas", "Terece L. Turton", "Nishath Rajiv Ranasinghe", "Shawn Jones", "Bradley Love", "William Jones", "Aric Hagberg", "Han-Wei Shen", "Nathan DeBardeleben", "Earl Lawrence"], "categories": ["cs.HC", "cs.AI", "cs.GR", "cs.LG"], "comment": null, "summary": "We present VizGenie, a self-improving, agentic framework that advances\nscientific visualization through large language model (LLM) by orchestrating of\na collection of domain-specific and dynamically generated modules. Users\ninitially access core functionalities--such as threshold-based filtering, slice\nextraction, and statistical analysis--through pre-existing tools. For tasks\nbeyond this baseline, VizGenie autonomously employs LLMs to generate new\nvisualization scripts (e.g., VTK Python code), expanding its capabilities\non-demand. Each generated script undergoes automated backend validation and is\nseamlessly integrated upon successful testing, continuously enhancing the\nsystem's adaptability and robustness. A distinctive feature of VizGenie is its\nintuitive natural language interface, allowing users to issue high-level\nfeature-based queries (e.g., ``visualize the skull\"). The system leverages\nimage-based analysis and visual question answering (VQA) via fine-tuned vision\nmodels to interpret these queries precisely, bridging domain expertise and\ntechnical implementation. Additionally, users can interactively query generated\nvisualizations through VQA, facilitating deeper exploration. Reliability and\nreproducibility are further strengthened by Retrieval-Augmented Generation\n(RAG), providing context-driven responses while maintaining comprehensive\nprovenance records. Evaluations on complex volumetric datasets demonstrate\nsignificant reductions in cognitive overhead for iterative visualization tasks.\nBy integrating curated domain-specific tools with LLM-driven flexibility,\nVizGenie not only accelerates insight generation but also establishes a\nsustainable, continuously evolving visualization practice. The resulting\nplatform dynamically learns from user interactions, consistently enhancing\nsupport for feature-centric exploration and reproducible research in scientific\nvisualization.", "AI": {"tldr": "VizGenie is a self-improving framework that enhances scientific visualization using LLMs with a user-friendly natural language interface, enabling dynamic generation and validation of visualization scripts.", "motivation": "To advance scientific visualization by creating a flexible system that integrates LLMs for generating domain-specific visualization tools on-demand.", "method": "VizGenie orchestrates a collection of dynamically generated modules and integrates LLMs to produce visualization scripts validated through automated backend processing. It incorporates image-based analysis and visual question answering.", "result": "The system significantly reduces cognitive overhead during iterative visualization tasks and supports deeper exploration through interactive queries.", "conclusion": "VizGenie establishes a sustainable and evolving visualization practice that enhances insight generation and reproducibility in scientific research.", "key_contributions": ["Dynamic generation of visualization scripts using LLMs", "Integration of intuitive natural language interface for user queries", "Enhanced adaptability through continuous learning from user interactions"], "limitations": "", "keywords": ["scientific visualization", "large language models", "natural language interface"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.21104", "pdf": "https://arxiv.org/pdf/2507.21104.pdf", "abs": "https://arxiv.org/abs/2507.21104", "title": "iLSU-T: an Open Dataset for Uruguayan Sign Language Translation", "authors": ["Ariel E. Stassi", "Yanina Boria", "J. Matías Di Martino", "Gregory Randall"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 5 figures, 19th International Conference on Automatic Face\n  and Gesture Recognition IEEE FG 2025", "summary": "Automatic sign language translation has gained particular interest in the\ncomputer vision and computational linguistics communities in recent years.\nGiven each sign language country particularities, machine translation requires\nlocal data to develop new techniques and adapt existing ones. This work\npresents iLSU T, an open dataset of interpreted Uruguayan Sign Language RGB\nvideos with audio and text transcriptions. This type of multimodal and curated\ndata is paramount for developing novel approaches to understand or generate\ntools for sign language processing. iLSU T comprises more than 185 hours of\ninterpreted sign language videos from public TV broadcasting. It covers diverse\ntopics and includes the participation of 18 professional interpreters of sign\nlanguage. A series of experiments using three state of the art translation\nalgorithms is presented. The aim is to establish a baseline for this dataset\nand evaluate its usefulness and the proposed pipeline for data processing. The\nexperiments highlight the need for more localized datasets for sign language\ntranslation and understanding, which are critical for developing novel tools to\nimprove accessibility and inclusion of all individuals. Our data and code can\nbe accessed.", "AI": {"tldr": "This paper presents iLSU T, an open dataset of Uruguayan Sign Language videos accompanied by audio and text transcriptions, aimed at improving sign language translation and processing.", "motivation": "The need for localized datasets for sign language translation to enhance accessibility and inclusion.", "method": "The dataset comprises over 185 hours of interpreted sign language videos from public TV broadcasting, featuring 18 professional interpreters and covering diverse topics. Experiments were conducted using three state-of-the-art translation algorithms to establish a baseline for the dataset.", "result": "The experiments demonstrated the dataset's usefulness and underscored the importance of localized data in advancing sign language processing techniques.", "conclusion": "The iLSU T dataset can significantly aid in developing new tools for sign language processing and underscores the necessity for more localized datasets.", "key_contributions": ["Introduction of the iLSU T dataset of Uruguayan Sign Language", "Experiments demonstrating the dataset's effectiveness", "Highlighting the importance of localized datasets for sign language accessibility"], "limitations": "", "keywords": ["sign language", "machine translation", "dataset", "accessibility", "computer vision"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.21303", "pdf": "https://arxiv.org/pdf/2507.21303.pdf", "abs": "https://arxiv.org/abs/2507.21303", "title": "Impact of eHMI on Pedestrians' Interactions with Level-5 Automated Driving Systems", "authors": ["Viktoria Marcus", "Griffin Pitts", "Sanaz Motamedi"], "categories": ["cs.HC", "cs.CY", "cs.ET"], "comment": "Accepted and to be presented at ASPIRE 2025 - the 69th International\n  Annual Meeting of HFES", "summary": "Each year, over half of global traffic fatalities involve vulnerable road\nusers (e.g. pedestrians), often due to human error. Level-5 automated driving\nsystems (ADSs) could reduce driver errors contributing to pedestrian accidents,\nthough effectiveness depends on clarity and understandability for other road\nusers. External human-machine interfaces (eHMIs) have been proposed to\nfacilitate pedestrian-ADS communication, though consensus on optimal eHMI\nfeatures remains unclear. In an online survey, 153 participants responded to\nroad-crossing scenarios involving level-5 ADSs, with and without eHMIs. With\neHMIs, pedestrians crossed earlier and more confidently, and reported\nsignificantly increased perceptions of safety, trust, and understanding when\ninteracting with level-5 ADSs. Visual eHMI features (including a text display\nand external speedometer) were ranked more necessary than auditory ones, though\nauditory cues received positive feedback. This study demonstrates that eHMIs\ncan significantly improve pedestrians' understanding of level-5 ADS intent and\nenhance perceived safety and trust, facilitating more intuitive pedestrian-ADS\ninteractions.", "AI": {"tldr": "The study explores the impact of external human-machine interfaces (eHMIs) on pedestrian interactions with level-5 automated driving systems (ADSs), highlighting their role in improving safety and trust.", "motivation": "To address the high rate of traffic fatalities involving vulnerable road users, particularly due to human error, and to evaluate how eHMIs can enhance communication between pedestrians and level-5 ADSs.", "method": "An online survey was conducted with 153 participants assessing their interactions with level-5 ADSs in road-crossing scenarios, both with and without eHMIs.", "result": "Participants who encountered eHMIs reported crossing earlier and with more confidence, exhibiting increased perceptions of safety, trust, and understanding towards level-5 ADSs compared to scenarios without eHMIs.", "conclusion": "eHMIs significantly enhance pedestrian understanding of ADS intent and improve their perceived safety and trust, thereby facilitating better interactions between pedestrians and automated vehicles.", "key_contributions": ["Demonstration of the positive impact of eHMIs on pedestrian behavior and safety perceptions in interactions with level-5 ADSs.", "Identification of visual eHMI features as more effective than auditory ones, enhancing user experience and understanding.", "Provision of empirical data supporting the importance of clear communication between pedestrians and automated systems."], "limitations": "", "keywords": ["automated driving systems", "external human-machine interfaces", "pedestrian safety", "trust", "traffic fatalities"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.21106", "pdf": "https://arxiv.org/pdf/2507.21106.pdf", "abs": "https://arxiv.org/abs/2507.21106", "title": "Creation of a Numerical Scoring System to Objectively Measure and Compare the Level of Rhetoric in Arabic Texts: A Feasibility Study, and A Working Prototype", "authors": ["Mandar Marathe"], "categories": ["cs.CL"], "comment": "This dissertation was submitted by Mandar Marathe on 6 September\n  2022, in partial fulfilment of the requirements for the Master of Arts degree\n  in Advanced Arabic at the University of Exeter", "summary": "Arabic Rhetoric is the field of Arabic linguistics which governs the art and\nscience of conveying a message with greater beauty, impact and persuasiveness.\nThe field is as ancient as the Arabic language itself and is found extensively\nin classical and contemporary Arabic poetry, free verse and prose. In practical\nterms, it is the intelligent use of word order, figurative speech and\nlinguistic embellishments to enhance message delivery. Despite the volumes that\nhave been written about it and the high status accorded to it, there is no way\nto objectively know whether a speaker or writer has used Arabic rhetoric in a\ngiven text, to what extent, and why. There is no objective way to compare the\nuse of Arabic rhetoric across genres, authors or epochs. It is impossible to\nknow which of pre-Islamic poetry, Andalucian Arabic poetry, or modern literary\ngenres are richer in Arabic rhetoric. The aim of the current study was to\ndevise a way to measure the density of the literary devices which constitute\nArabic rhetoric in a given text, as a proxy marker for Arabic rhetoric itself.\nA comprehensive list of 84 of the commonest literary devices and their\ndefinitions was compiled. A system of identifying literary devices in texts was\nconstructed. A method of calculating the density of literary devices based on\nthe morpheme count of the text was utilised. Four electronic tools and an\nanalogue tool were created to support the calculation of an Arabic text's\nrhetorical literary device density, including a website and online calculator.\nAdditionally, a technique of reporting the distribution of literary devices\nused across the three sub-domains of Arabic rhetoric was created. The output of\nthis project is a working tool which can accurately report the density of\nArabic rhetoric in any Arabic text or speech.", "AI": {"tldr": "The study develops a method to measure the density of Arabic rhetorical devices in texts to assess their impact and use across genres.", "motivation": "To enable objective evaluation of Arabic rhetoric usage in texts and compare its density across different genres and authors.", "method": "A system was constructed to identify and calculate the density of 84 common literary devices based on morpheme counts, supported by electronic tools including a website and online calculator.", "result": "The project produced a tool that accurately reports the density of rhetorical devices in any Arabic text or speech.", "conclusion": "This tool provides a new way to assess Arabic rhetorical richness and aids further linguistic analysis.", "key_contributions": ["Development of a measurement system for Arabic rhetorical devices.", "Creation of electronic support tools including an online calculator.", "Establishment of a technique for reporting rhetorical device distribution."], "limitations": "", "keywords": ["Arabic Rhetoric", "Literary Devices", "Text Analysis", "Computational Linguistics", "Language Processing"], "importance_score": 0, "read_time_minutes": 10}}
{"id": "2507.21378", "pdf": "https://arxiv.org/pdf/2507.21378.pdf", "abs": "https://arxiv.org/abs/2507.21378", "title": "ProMemAssist: Exploring Timely Proactive Assistance Through Working Memory Modeling in Multi-Modal Wearable Devices", "authors": ["Kevin Pu", "Ting Zhang", "Naveen Sendhilnathan", "Sebastian Freitag", "Raj Sodhi", "Tanya Jonker"], "categories": ["cs.HC", "cs.AI"], "comment": "Accepted to UIST'25", "summary": "Wearable AI systems aim to provide timely assistance in daily life, but\nexisting approaches often rely on user initiation or predefined task knowledge,\nneglecting users' current mental states. We introduce ProMemAssist, a smart\nglasses system that models a user's working memory (WM) in real-time using\nmulti-modal sensor signals. Grounded in cognitive theories of WM, our system\nrepresents perceived information as memory items and episodes with encoding\nmechanisms, such as displacement and interference. This WM model informs a\ntiming predictor that balances the value of assistance with the cost of\ninterruption. In a user study with 12 participants completing cognitively\ndemanding tasks, ProMemAssist delivered more selective assistance and received\nhigher engagement compared to an LLM baseline system. Qualitative feedback\nhighlights the benefits of WM modeling for nuanced, context-sensitive support,\noffering design implications for more attentive and user-aware proactive\nagents.", "AI": {"tldr": "ProMemAssist is a smart glasses system that models a user's working memory in real-time to deliver context-sensitive assistance, outperforming traditional LLM systems in user engagement.", "motivation": "Existing wearable AI systems lack consideration for users' mental states and often require user initiation for assistance.", "method": "ProMemAssist uses multi-modal sensor signals to model working memory, employing cognitive theories to represent information and predict optimal assistance timing.", "result": "In user studies, ProMemAssist provided more tailored assistance and achieved higher user engagement compared to a LLM baseline.", "conclusion": "The use of working memory modeling can enhance user-aware proactive agents, opening up new design possibilities for assistance systems.", "key_contributions": ["Introduction of a smart glasses system for modeling working memory in real-time", "Demonstrated improved engagement in user studies over LLM systems", "Provided design implications for context-sensitive user support"], "limitations": "", "keywords": ["wearable AI", "working memory", "user engagement", "context-sensitive assistance", "smart glasses"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.21107", "pdf": "https://arxiv.org/pdf/2507.21107.pdf", "abs": "https://arxiv.org/abs/2507.21107", "title": "Curved Inference: Concern-Sensitive Geometry in Large Language Model Residual Streams", "authors": ["Rob Manson"], "categories": ["cs.CL", "cs.AI"], "comment": "29 pages, 22 figures", "summary": "We propose Curved Inference - a geometric Interpretability framework that\ntracks how the residual stream trajectory of a large language model bends in\nresponse to shifts in semantic concern. Across 20 matched prompts spanning\nemotional, moral, perspective, logical, identity, environmental, and nonsense\ndomains, we analyse Gemma3-1b and LLaMA3.2-3b using five native-space metrics,\nwith a primary focus on curvature (\\k{appa}_i) and salience (S(t)). These\nmetrics are computed under a pullback semantic metric derived from the\nunembedding matrix, ensuring that all measurements reflect token-aligned\ngeometry rather than raw coordinate structure. We find that concern-shifted\nprompts reliably alter internal activation trajectories in both models - with\nLLaMA exhibiting consistent, statistically significant scaling in both\ncurvature and salience as concern intensity increases. Gemma also responds to\nconcern but shows weaker differentiation between moderate and strong variants.\nOur results support a two-layer view of LLM geometry - a latent conceptual\nstructure encoded in the embedding space, and a contextual trajectory shaped by\nprompt-specific inference. Curved Inference reveals how models navigate,\nreorient, or reinforce semantic meaning over depth, offering a principled\nmethod for diagnosing alignment, abstraction, and emergent inference dynamics.\nThese findings offer fresh insight into semantic abstraction and model\nalignment through the lens of Curved Inference.", "AI": {"tldr": "This paper introduces Curved Inference, a geometric Interpretability framework for analyzing large language models' responses to semantic shifts across various domains.", "motivation": "To understand how large language models track changes in semantic concerns and how this affects their internal trajectories.", "method": "The study analyzes two models, Gemma3-1b and LLaMA3.2-3b, using geometric metrics related to curvature and salience on 20 matched prompts across different semantic domains.", "result": "LLaMA3.2-3b shows significant scaling in curvature and salience with increased concern intensity, while Gemma3-1b exhibits weaker differentiation in responses.", "conclusion": "Curved Inference provides a new approach to assess model alignment and abstraction, revealing the dynamics of inference through prompts.", "key_contributions": ["Introduction of the Curved Inference framework", "Geometric analysis of LLM trajectories in response to semantic prompts", "Insights into model alignment and emergent inference dynamics"], "limitations": "", "keywords": ["large language models", "geometric interpretability", "semantic abstraction"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2507.21411", "pdf": "https://arxiv.org/pdf/2507.21411.pdf", "abs": "https://arxiv.org/abs/2507.21411", "title": "InSituTale: Enhancing Augmented Data Storytelling with Physical Objects", "authors": ["Kentaro Takahira", "Yue Yu", "Takanori Fujiwara", "Suzuki Ryo", "Huamin Qu"], "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "Augmented data storytelling enhances narrative delivery by integrating\nvisualizations with physical environments and presenter actions. Existing\nsystems predominantly rely on body gestures or speech to control\nvisualizations, leaving interactions with physical objects largely\nunderexplored. We introduce augmented physical data storytelling, an approach\nenabling presenters to manipulate visualizations through physical object\ninteractions. To inform this approach, we first conducted a survey of\ndata-driven presentations to identify common visualization commands. We then\nconducted workshops with nine HCI/VIS researchers to collect mappings between\nphysical manipulations and these commands. Guided by these insights, we\ndeveloped InSituTale, a prototype that combines object tracking via a depth\ncamera with Vision-LLM for detecting real-world events. Through physical\nmanipulations, presenters can dynamically execute various visualization\ncommands, delivering cohesive data storytelling experiences that blend physical\nand digital elements. A user study with 12 participants demonstrated that\nInSituTale enables intuitive interactions, offers high utility, and facilitates\nan engaging presentation experience.", "AI": {"tldr": "This paper presents InSituTale, a system for augmented physical data storytelling that allows presenters to manipulate visualizations through physical object interactions, enhancing narrative delivery.", "motivation": "There is a need to improve narrative delivery in presentations by incorporating interactions with physical objects, as existing systems mainly focus on body gestures or speech.", "method": "The authors surveyed data-driven presentations to identify common visualization commands and conducted workshops with HCI/VIS researchers to develop mappings between physical manipulations and these commands. They then created the InSituTale prototype using object tracking and Vision-LLM to execute visualization commands through physical interactions.", "result": "User studies with 12 participants showed that InSituTale allows for intuitive interactions and contributes to a cohesive and engaging presentation experience, demonstrating high utility.", "conclusion": "Augmented physical data storytelling through InSituTale effectively blends physical and digital elements, enhancing user engagement in data presentations.", "key_contributions": ["Introduction of augmented physical data storytelling", "Development of InSituTale prototype integrating object tracking and Vision-LLM", "User study demonstrating high utility and intuitive interactions"], "limitations": "", "keywords": ["data storytelling", "augmented reality", "HCI", "visualizations", "interaction design"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.21108", "pdf": "https://arxiv.org/pdf/2507.21108.pdf", "abs": "https://arxiv.org/abs/2507.21108", "title": "A Survey of Classification Tasks and Approaches for Legal Contracts", "authors": ["Amrita Singh", "Aditya Joshi", "Jiaojiao Jiang", "Hye-young Paik"], "categories": ["cs.CL", "cs.AI"], "comment": "Under review. 49 pages + references", "summary": "Given the large size and volumes of contracts and their underlying inherent\ncomplexity, manual reviews become inefficient and prone to errors, creating a\nclear need for automation. Automatic Legal Contract Classification (LCC)\nrevolutionizes the way legal contracts are analyzed, offering substantial\nimprovements in speed, accuracy, and accessibility. This survey delves into the\nchallenges of automatic LCC and a detailed examination of key tasks, datasets,\nand methodologies. We identify seven classification tasks within LCC, and\nreview fourteen datasets related to English-language contracts, including\npublic, proprietary, and non-public sources. We also introduce a methodology\ntaxonomy for LCC, categorized into Traditional Machine Learning, Deep Learning,\nand Transformer-based approaches. Additionally, the survey discusses evaluation\ntechniques and highlights the best-performing results from the reviewed\nstudies. By providing a thorough overview of current methods and their\nlimitations, this survey suggests future research directions to improve the\nefficiency, accuracy, and scalability of LCC. As the first comprehensive survey\non LCC, it aims to support legal NLP researchers and practitioners in improving\nlegal processes, making legal information more accessible, and promoting a more\ninformed and equitable society.", "AI": {"tldr": "This survey provides a comprehensive overview of Automatic Legal Contract Classification (LCC), including challenges, methodologies, and future research directions to enhance efficiency and accessibility in legal contract analysis.", "motivation": "Manual reviews of legal contracts are inefficient and error-prone, creating a need for automation through Automatic Legal Contract Classification.", "method": "The survey examines seven classification tasks, fourteen datasets, and introduces a methodology taxonomy for LCC, including Traditional Machine Learning, Deep Learning, and Transformer-based approaches.", "result": "The survey highlights key tasks, datasets, and evaluation techniques in LCC, presenting the best-performing results and identifying limitations of current methods.", "conclusion": "The findings advocate for future research to enhance the efficiency, accuracy, and scalability of LCC, aiming to improve legal processes and access to legal information.", "key_contributions": ["Comprehensive overview of LCC challenges and methodologies", "Detailed examination of datasets and evaluation techniques", "Suggestions for future research directions in legal NLP"], "limitations": "None explicitly stated in the abstract, but acknowledges limitations of current methodologies.", "keywords": ["Legal Contract Classification", "Natural Language Processing", "Machine Learning", "Deep Learning", "Legal Informatics"], "importance_score": 5, "read_time_minutes": 30}}
{"id": "2507.21435", "pdf": "https://arxiv.org/pdf/2507.21435.pdf", "abs": "https://arxiv.org/abs/2507.21435", "title": "MindChat: Enhancing BCI Spelling with Large Language Models in Realistic Scenarios", "authors": ["JIaheng Wang", "Yucun Zhong", "Chengjie Huang", "Lin Yao"], "categories": ["cs.HC"], "comment": null, "summary": "Brain-computer interface (BCI) spellers can render a new communication\nchannel independent of peripheral nervous system, which are especially valuable\nfor patients with severe motor disabilities. However, current BCI spellers\noften require users to type intended utterances letter-by-letter while spelling\nerrors grow proportionally due to inaccurate electroencephalogram (EEG)\ndecoding, largely impeding the efficiency and usability of BCIs in real-world\ncommunication. In this paper, we present MindChat, a large language model\n(LLM)-assisted BCI speller to enhance BCI spelling efficiency by reducing\nusers' manual keystrokes. Building upon prompt engineering, we prompt LLMs\n(GPT-4o) to continuously suggest context-aware word and sentence\ncompletions/predictions during spelling. Online copy-spelling experiments\nencompassing four dialogue scenarios demonstrate that MindChat saves more than\n62\\% keystrokes and over 32\\% spelling time compared with traditional BCI\nspellers. We envision high-speed BCI spellers enhanced by LLMs will potentially\nlead to truly practical applications.", "AI": {"tldr": "The paper presents MindChat, an LLM-assisted BCI speller designed to enhance typing efficiency for users with motor disabilities by suggesting context-aware completions, significantly reducing keystrokes and spelling time.", "motivation": "Current BCI spellers require extensive manual typing, leading to increased spelling errors and inefficiency, particularly for individuals with severe motor disabilities.", "method": "MindChat uses a large language model (GPT-4o) to provide continuous context-aware word and sentence suggestions during spelling tasks, leveraging prompt engineering techniques.", "result": "In online experiments, MindChat achieved over a 62% reduction in keystrokes and more than a 32% decrease in spelling time compared to traditional BCI spellers.", "conclusion": "The integration of LLMs in BCI spellers can enhance speed and efficiency, facilitating practical applications for users with communication challenges due to motor impairments.", "key_contributions": ["Development of MindChat, an LLM-assisted BCI speller that improves efficiency.", "Demonstrated significant reduction in keystrokes and spelling time through experimental results.", "Utilization of prompt engineering for better user interaction with BCI technology."], "limitations": "", "keywords": ["brain-computer interface", "BCI speller", "large language model", "motor disabilities", "context-aware predictions"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.21110", "pdf": "https://arxiv.org/pdf/2507.21110.pdf", "abs": "https://arxiv.org/abs/2507.21110", "title": "SemRAG: Semantic Knowledge-Augmented RAG for Improved Question-Answering", "authors": ["Kezhen Zhong", "Basem Suleiman", "Abdelkarim Erradi", "Shijing Chen"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "16 pages, 12 figures", "summary": "This paper introduces SemRAG, an enhanced Retrieval Augmented Generation\n(RAG) framework that efficiently integrates domain-specific knowledge using\nsemantic chunking and knowledge graphs without extensive fine-tuning.\nIntegrating domain-specific knowledge into large language models (LLMs) is\ncrucial for improving their performance in specialized tasks. Yet, existing\nadaptations are computationally expensive, prone to overfitting and limit\nscalability. To address these challenges, SemRAG employs a semantic chunking\nalgorithm that segments documents based on the cosine similarity from sentence\nembeddings, preserving semantic coherence while reducing computational\noverhead. Additionally, by structuring retrieved information into knowledge\ngraphs, SemRAG captures relationships between entities, improving retrieval\naccuracy and contextual understanding. Experimental results on MultiHop RAG and\nWikipedia datasets demonstrate SemRAG has significantly enhances the relevance\nand correctness of retrieved information from the Knowledge Graph,\noutperforming traditional RAG methods. Furthermore, we investigate the\noptimization of buffer sizes for different data corpus, as optimizing buffer\nsizes tailored to specific datasets can further improve retrieval performance,\nas integration of knowledge graphs strengthens entity relationships for better\ncontextual comprehension. The primary advantage of SemRAG is its ability to\ncreate an efficient, accurate domain-specific LLM pipeline while avoiding\nresource-intensive fine-tuning. This makes it a practical and scalable approach\naligned with sustainability goals, offering a viable solution for AI\napplications in domain-specific fields.", "AI": {"tldr": "SemRAG is an enhanced Retrieval Augmented Generation framework that uses semantic chunking and knowledge graphs to improve performance in domain-specific tasks without extensive fine-tuning.", "motivation": "The paper addresses the need for integrating domain-specific knowledge in large language models efficiently, while overcoming the limitations of existing adaptations that are computationally expensive and prone to overfitting.", "method": "SemRAG employs a semantic chunking algorithm that segments documents based on cosine similarity from sentence embeddings, and structures retrieved information into knowledge graphs to enhance retrieval accuracy and maintain semantic coherence.", "result": "SemRAG outperforms traditional RAG methods, significantly enhancing the relevance and correctness of retrieved information in tests conducted on MultiHop RAG and Wikipedia datasets.", "conclusion": "SemRAG provides a practical and scalable solution for creating efficient domain-specific LLM pipelines without the resource-intensive process of fine-tuning, aligning with sustainability goals.", "key_contributions": ["Introduction of the semantic chunking algorithm for efficient document segmentation.", "Utilization of knowledge graphs to improve contextual understanding and entity relationships.", "Demonstration of significant performance improvements over traditional RAG methods."], "limitations": "", "keywords": ["Retrieval Augmented Generation", "semantic chunking", "knowledge graphs"], "importance_score": 10, "read_time_minutes": 16}}
{"id": "2507.21462", "pdf": "https://arxiv.org/pdf/2507.21462.pdf", "abs": "https://arxiv.org/abs/2507.21462", "title": "Using Tactile Charts to Support Comprehension and Learning of Complex Visualizations for Blind and Low-Vision Individuals", "authors": ["Tingying He", "Maggie McCracken", "Daniel Hajas", "Sarah Creem-Regehr", "Alexander Lex"], "categories": ["cs.HC"], "comment": null, "summary": "We investigate whether tactile charts support comprehension and learning of\ncomplex visualizations for blind and low-vision (BLV) individuals and\ncontribute four tactile chart designs and an interview study. Visualizations\nare powerful tools for conveying data, yet BLV individuals typically can rely\nonly on assistive technologies -- primarily alternative texts -- to access this\ninformation. Prior research shows the importance of mental models of chart\ntypes for interpreting these descriptions, yet BLV individuals have no means to\nbuild such a mental model based on images of visualizations. Tactile charts\nshow promise to fill this gap in supporting the process of building mental\nmodels. Yet studies on tactile data representations mostly focus on simple\nchart types, and it is unclear whether they are also appropriate for more\ncomplex charts as would be found in scientific publications. Working with two\nBLV researchers, we designed 3D-printed tactile template charts with\nexploration instructions for four advanced chart types: UpSet plots, violin\nplots, clustered heatmaps, and faceted line charts. We then conducted an\ninterview study with 12 BLV participants comparing whether using our tactile\ntemplates improves mental models and understanding of charts and whether this\nunderstanding translates to novel datasets experienced through alt texts.\nThematic analysis shows that tactile models support chart type understanding\nand are the preferred learning method by BLV individuals. We also report\nparticipants' opinions on tactile chart design and their role in BLV education.", "AI": {"tldr": "This study examines the effectiveness of tactile charts in enhancing comprehension and learning for blind and low-vision (BLV) individuals, presenting four custom 3D-printed tactile chart designs after conducting interviews with BLV participants.", "motivation": "Blind and low-vision individuals often rely on assistive technologies for accessing visual information, making it essential to explore alternative approaches like tactile charts to build their mental models of data visualizations.", "method": "The authors designed four 3D-printed tactile templates for complex chart types and conducted thematic analysis through interviews with 12 BLV participants to assess improvements in understanding and mental models.", "result": "The study found that tactile charts significantly support understanding of complex chart types and are favored by BLV individuals as a learning method over traditional alternative texts.", "conclusion": "Tactile chart models are effective in improving the understanding of complex visualizations for BLV individuals and can play a valuable role in their education.", "key_contributions": ["Development of four tactile chart designs for complex visualizations", "Comparison of tactile charts versus traditional alt texts among BLV participants", "Insights on BLV participants' preferences for tactile chart learning"], "limitations": "The study primarily involves a small sample size and may not generalize to all BLV individuals.", "keywords": ["tactile charts", "blind and low-vision", "data visualization", "mental models", "education"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.21112", "pdf": "https://arxiv.org/pdf/2507.21112.pdf", "abs": "https://arxiv.org/abs/2507.21112", "title": "InsurTech innovation using natural language processing", "authors": ["Panyi Dong", "Zhiyu Quan"], "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "With the rapid rise of InsurTech, traditional insurance companies are\nincreasingly exploring alternative data sources and advanced technologies to\nsustain their competitive edge. This paper provides both a conceptual overview\nand practical case studies of natural language processing (NLP) and its\nemerging applications within insurance operations with a focus on transforming\nraw, unstructured text into structured data suitable for actuarial analysis and\ndecision-making. Leveraging real-world alternative data provided by an\nInsurTech industry partner that enriches traditional insurance data sources, we\napply various NLP techniques to demonstrate practical use cases in the\ncommercial insurance context. These enriched, text-derived insights not only\nadd to and refine traditional rating factors for commercial insurance pricing\nbut also offer novel perspectives for assessing underlying risk by introducing\nnovel industry classifications. Through these demonstrations, we show that NLP\nis not merely a supplementary tool but a foundational element for modern,\ndata-driven insurance analytics.", "AI": {"tldr": "The paper explores the integration of natural language processing (NLP) in insurance operations, showcasing case studies that illustrate its application in transforming unstructured text into actionable data for actuarial analysis.", "motivation": "The rise of InsurTech is prompting traditional insurance companies to use alternative data sources and advanced technologies to maintain competitiveness.", "method": "The paper presents a conceptual overview and practical case studies of NLP, applying various techniques to real-world alternative data from an InsurTech partner to enhance traditional insurance operations.", "result": "NLP techniques were applied to transform unstructured text into structured data, enriching traditional rating factors for insurance pricing and offering novel perspectives for risk assessment.", "conclusion": "NLP is essential for modern insurance analytics, acting as a foundational element rather than just a supplementary tool.", "key_contributions": ["Demonstration of NLP's application in a real-world InsurTech context.", "Introduction of new industry classifications for risk assessment.", "Refinement of traditional insurance pricing strategies through enriched text-derived insights."], "limitations": "", "keywords": ["Natural Language Processing", "Insurance", "InsurTech", "Data Analytics", "Risk Assessment"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.21490", "pdf": "https://arxiv.org/pdf/2507.21490.pdf", "abs": "https://arxiv.org/abs/2507.21490", "title": "Conversations over Clicks: Impact of Chatbots on Information Search in Interdisciplinary Learning", "authors": ["Hannah Kim", "Sergei L. Kosakovsky Pond", "Stephen MacNeil"], "categories": ["cs.HC", "cs.CY", "cs.IR", "J.3; K.3.2"], "comment": "9 pages, 2 tables, 3 figures, 2025 ASEE/IEEE Frontiers in Education\n  (FIE) Conference preprint", "summary": "This full research paper investigates the impact of generative AI (GenAI) on\nthe learner experience, with a focus on how learners engage with and utilize\nthe information it provides. In e-learning environments, learners often need to\nnavigate a complex information space on their own. This challenge is further\ncompounded in interdisciplinary fields like bioinformatics, due to the varied\nprior knowledge and backgrounds. In this paper, we studied how GenAI influences\ninformation search in bioinformatics research: (1) How do interactions with a\nGenAI chatbot influence learner orienteering behaviors?; and (2) How do\nlearners identify information scent in GenAI chatbot responses? We adopted an\nautoethnographic approach to investigate these questions. GenAI was found to\nsupport orienteering once a learning plan was established, but it was\ncounterproductive prior to that. Moreover, traditionally value-rich information\nsources such as bullet points and related terms proved less effective when\napplied to GenAI responses. Information scents were primarily recognized\nthrough the presence or absence of prior knowledge of the domain. These\nfindings suggest that GenAI should be adopted into e-learning environments with\ncaution, particularly in interdisciplinary learning contexts.", "AI": {"tldr": "This paper explores the influence of generative AI (GenAI) on learner engagement and information search in bioinformatics education, highlighting both supportive and counterproductive effects based on learner preparation.", "motivation": "To investigate how GenAI affects learner experiences in navigating complex information spaces in interdisciplinary fields like bioinformatics.", "method": "An autoethnographic approach was used to study learner interactions with a GenAI chatbot, examining orienteering behaviors and information scent identification.", "result": "GenAI supports orienteering once a learning plan is established but is counterproductive before that; traditional information formats are less effective with GenAI responses.", "conclusion": "GenAI should be integrated into e-learning with caution, especially in interdisciplinary contexts, due to its nuanced effects on learner navigation and information processing.", "key_contributions": ["Investigation of GenAI's impact on learner orienteering behaviors", "Identification of how information scent is perceived in GenAI responses", "Recommendations for cautious adoption of GenAI in e-learning environments."], "limitations": "The findings may not generalize to all learning contexts outside of bioinformatics.", "keywords": ["Generative AI", "e-learning", "bioinformatics", "learner engagement", "information search"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.21134", "pdf": "https://arxiv.org/pdf/2507.21134.pdf", "abs": "https://arxiv.org/abs/2507.21134", "title": "TRIDENT: Benchmarking LLM Safety in Finance, Medicine, and Law", "authors": ["Zheng Hui", "Yijiang River Dong", "Ehsan Shareghi", "Nigel Collier"], "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in high-risk\ndomains such as law, finance, and medicine, systematically evaluating their\ndomain-specific safety and compliance becomes critical. While prior work has\nlargely focused on improving LLM performance in these domains, it has often\nneglected the evaluation of domain-specific safety risks. To bridge this gap,\nwe first define domain-specific safety principles for LLMs based on the AMA\nPrinciples of Medical Ethics, the ABA Model Rules of Professional Conduct, and\nthe CFA Institute Code of Ethics. Building on this foundation, we introduce\nTrident-Bench, a benchmark specifically targeting LLM safety in the legal,\nfinancial, and medical domains. We evaluated 19 general-purpose and\ndomain-specialized models on Trident-Bench and show that it effectively reveals\nkey safety gaps -- strong generalist models (e.g., GPT, Gemini) can meet basic\nexpectations, whereas domain-specialized models often struggle with subtle\nethical nuances. This highlights an urgent need for finer-grained\ndomain-specific safety improvements. By introducing Trident-Bench, our work\nprovides one of the first systematic resources for studying LLM safety in law\nand finance, and lays the groundwork for future research aimed at reducing the\nsafety risks of deploying LLMs in professionally regulated fields. Code and\nbenchmark will be released at: https://github.com/zackhuiiiii/TRIDENT", "AI": {"tldr": "The paper introduces Trident-Bench, a benchmark for evaluating large language model (LLM) safety in high-risk domains like law, finance, and medicine, revealing key safety gaps in generalist and domain-specialized models.", "motivation": "The increasing deployment of LLMs in high-risk domains necessitates systematic evaluations of their safety and compliance, which has been overlooked in prior work.", "method": "The authors define domain-specific safety principles for LLMs inspired by existing ethical frameworks and create Trident-Bench to test these principles across various models.", "result": "Trident-Bench was used to evaluate 19 models, demonstrating that while generalist models can meet basic safety expectations, domain-specialized models often fail to address subtle ethical issues effectively.", "conclusion": "Trident-Bench serves as a foundational resource for assessing LLM safety in regulated fields, urging the need for targeted improvements to address safety concerns in these critical domains.", "key_contributions": ["Introduction of Trident-Bench for evaluating LLM safety in high-risk domains.", "Establishment of domain-specific safety principles for LLM assessments.", "Empirical evaluation showing gaps in safety performance between generalist and domain-specialized models."], "limitations": "The evaluation focuses on only 19 models, and further research is needed to comprehensively address all safety nuances.", "keywords": ["large language models", "safety", "benchmark", "high-risk domains", "ethical compliance"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.21654", "pdf": "https://arxiv.org/pdf/2507.21654.pdf", "abs": "https://arxiv.org/abs/2507.21654", "title": "AI Literacy as a Key Driver of User Experience in AI-Powered Assessment: Insights from Socratic Mind", "authors": ["Meryem Yilmaz Soylu", "Jeonghyun Lee", "Jui-Tse Hung", "Christopher Zhang Cui", "David A. Joyner"], "categories": ["cs.HC", "cs.AI", "K.3.1; I.2.6"], "comment": "34 pages, 1 figure, 3 tables", "summary": "As Artificial Intelligence (AI) tools become increasingly embedded in higher\neducation, understanding how students interact with these systems is essential\nto supporting effective learning. This study examines how students' AI literacy\nand prior exposure to AI technologies shape their perceptions of Socratic Mind,\nan interactive AI-powered formative assessment tool. Drawing on\nSelf-Determination Theory and user experience research, we analyze\nrelationships among AI literacy, perceived usability, satisfaction, engagement,\nand perceived learning effectiveness. Data from 309 undergraduates in Computer\nScience and Business courses were collected through validated surveys. Partial\nleast squares structural equation modeling showed that AI literacy - especially\nself-efficacy, conceptual understanding, and application skills - significantly\npredicts usability, satisfaction, and engagement. Usability and satisfaction,\nin turn, strongly predict perceived learning effectiveness, while prior AI\nexposure showed no significant effect. These findings highlight that AI\nliteracy, rather than exposure alone, shapes student experiences. Designers\nshould integrate adaptive guidance and user-centered features to support\ndiverse literacy levels, fostering inclusive, motivating, and effective\nAI-based learning environments.", "AI": {"tldr": "This study investigates how AI literacy influences students' perceptions of an AI-powered assessment tool in higher education.", "motivation": "Understanding student interactions with AI tools in higher education is essential for effective learning support.", "method": "The study analyzed data from 309 undergraduate students using validated surveys and partial least squares structural equation modeling.", "result": "AI literacy predicts usability, satisfaction, and engagement with the AI tool, while prior exposure to AI technology does not significantly affect these perceptions.", "conclusion": "Enhancing AI literacy is crucial for improving student experiences with AI tools, suggesting the need for user-centered design in education technology.", "key_contributions": ["Identifies the importance of AI literacy over mere exposure in shaping student experiences with AI tools.", "Suggests design principles for AI educational tools to cater to varying literacy levels.", "Provides empirical data linking AI literacy to learning effectiveness through usability and engagement."], "limitations": "", "keywords": ["AI literacy", "higher education", "user experience", "formative assessment", "self-determination theory"], "importance_score": 8, "read_time_minutes": 34}}
{"id": "2507.21138", "pdf": "https://arxiv.org/pdf/2507.21138.pdf", "abs": "https://arxiv.org/abs/2507.21138", "title": "TTS-1 Technical Report", "authors": ["Oleg Atamanenko", "Anna Chalova", "Joseph Coombes", "Nikki Cope", "Phillip Dang", "Zhifeng Deng", "Jimmy Du", "Michael Ermolenko", "Feifan Fan", "Yufei Feng", "Cheryl Fichter", "Pavel Filimonov", "Louis Fischer", "Kylan Gibbs", "Valeria Gusarova", "Pavel Karpik", "Andreas Assad Kottner", "Ian Lee", "Oliver Louie", "Jasmine Mai", "Mikhail Mamontov", "Suri Mao", "Nurullah Morshed", "Igor Poletaev", "Florin Radu", "Dmytro Semernia", "Evgenii Shingarev", "Vikram Sivaraja", "Peter Skirko", "Rinat Takhautdinov", "Robert Villahermosa", "Jean Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "20 pages, 10 figures. For associated modeling and training code, see\n  https://github.com/inworld-ai/tts", "summary": "We introduce Inworld TTS-1, a set of two Transformer-based autoregressive\ntext-to-speech (TTS) models. Our largest model, TTS-1-Max, has 8.8B parameters\nand is designed for utmost quality and expressiveness in demanding\napplications. TTS-1 is our most efficient model, with 1.6B parameters, built\nfor real-time speech synthesis and on-device use cases. By scaling train-time\ncompute and applying a sequential process of pre-training, fine-tuning, and\nRL-alignment of the speech-language model (SpeechLM) component, both models\nachieve state-of-the-art performance on a variety of benchmarks, demonstrating\nexceptional quality relying purely on in-context learning of the speaker's\nvoice. Inworld TTS-1 and TTS-1-Max can generate high-resolution 48 kHz speech\nwith low latency, and support 11 languages with fine-grained emotional control\nand non-verbal vocalizations through audio markups. We additionally open-source\nour training and modeling code under an MIT license.", "AI": {"tldr": "Introduction of Inworld TTS-1, a set of Transformer-based text-to-speech models designed for high-quality speech synthesis and real-time applications.", "motivation": "To develop state-of-the-art text-to-speech models that can deliver exceptional quality and expressiveness for demanding applications and real-time use cases.", "method": "The models utilize a sequential process of pre-training, fine-tuning, and RL-alignment to optimize the SpeechLM component, scaling train-time compute for maximum performance.", "result": "Both TTS-1 and TTS-1-Max achieve state-of-the-art performance on benchmarks, generating high-resolution speech at 48 kHz with low latency in 11 languages, allowing for emotional control and non-verbal vocalizations.", "conclusion": "The introduction of these models marks a significant advancement in text-to-speech technology, offering powerful tools for developers and researchers in the field.", "key_contributions": ["Development of TTS-1-Max (8.8B parameters) for high-quality speech synthesis.", "Creation of TTS-1 (1.6B parameters) optimized for real-time applications.", "Open-sourcing training and modeling code to promote further research."], "limitations": "", "keywords": ["text-to-speech", "Transformer", "speech synthesis", "deep learning", "emotional control"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.21722", "pdf": "https://arxiv.org/pdf/2507.21722.pdf", "abs": "https://arxiv.org/abs/2507.21722", "title": "Identification of Design Recommendations for Augmented Reality Authors in Corporate Training", "authors": ["Stefan Graser", "Martin Schrepp", "Stephan Böhm"], "categories": ["cs.HC", "cs.SE"], "comment": "9 pages, 1 table, 1 figure", "summary": "Innovative technologies, such as Augmented Reality (AR), introduce new\ninteraction paradigms, demanding the identification of software requirements\nduring the software development process. In general, design recommendations are\nrelated to this, supporting the design of applications positively and meeting\nstakeholder needs. However, current research lacks context-specific AR design\nrecommendations. This study addresses this gap by identifying and analyzing\npractical AR design recommendations relevant to the evaluation phase of the\nUser-Centered Design (UCD) process. We rely on an existing dataset of Mixed\nReality (MR) design recommendations. We applied a multi-method approach by (1)\nextending the dataset with AR-specific recommendations published since 2020,\n(2) classifying the identified recommendations using a NLP classification\napproach based on a pre-trained Sentence Transformer model, (3) summarizing the\ncontent of all topics, and (4) evaluating their relevance concerning AR in\nCorporate Training (CT) both based on a qualitative Round Robin approach with\nfive experts. As a result, an updated dataset of 597 practitioner design\nrecommendations, classified into 84 topics, is provided with new insights into\ntheir applicability in the context of AR in CT. Based on this, 32 topics with a\ntotal of 284 statements were evaluated as relevant for AR in CT. This research\ndirectly contributes to the authors' work for extending their AR-specific User\nExperience (UX) measurement approach, supporting AR authors in targeting the\nimprovement of AR applications for CT scenarios.", "AI": {"tldr": "This study identifies and classifies AR design recommendations for the User-Centered Design process in Corporate Training, addressing the lack of context-specific guidelines.", "motivation": "There is a gap in context-specific AR design recommendations during the software development process, particularly relevant to the evaluation phase of User-Centered Design.", "method": "A multi-method approach was used: extending an existing Mixed Reality design recommendations dataset with AR-specific recommendations, classifying them using a NLP-based classification approach, summarizing content, and qualitatively evaluating relevance with expert feedback.", "result": "An updated dataset of 597 design recommendations classified into 84 topics was created; 32 topics with 284 statements were evaluated as relevant for AR in Corporate Training.", "conclusion": "The research enhances understanding of AR application design in Corporate Training and contributes to a User Experience measurement approach for AR authors.", "key_contributions": ["Developed an updated dataset of AR design recommendations for Corporate Training.", "Utilized NLP classification methods to analyze design recommendations.", "Provided expert evaluations on the relevance of design topics for AR in Corporate Training."], "limitations": "", "keywords": ["Augmented Reality", "User-Centered Design", "Corporate Training", "Design Recommendations", "NLP"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.21168", "pdf": "https://arxiv.org/pdf/2507.21168.pdf", "abs": "https://arxiv.org/abs/2507.21168", "title": "Diverse LLMs or Diverse Question Interpretations? That is the Ensembling Question", "authors": ["Rafael Rosales", "Santiago Miret"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7; I.2.0"], "comment": null, "summary": "Effectively leveraging diversity has been shown to improve performance for\nvarious machine learning models, including large language models (LLMs).\nHowever, determining the most effective way of using diversity remains a\nchallenge. In this work, we compare two diversity approaches for answering\nbinary questions using LLMs: model diversity, which relies on multiple models\nanswering the same question, and question interpretation diversity, which\nrelies on using the same model to answer the same question framed in different\nways. For both cases, we apply majority voting as the ensemble consensus\nheuristic to determine the final answer. Our experiments on boolq, strategyqa,\nand pubmedqa show that question interpretation diversity consistently leads to\nbetter ensemble accuracy compared to model diversity. Furthermore, our analysis\nof GPT and LLaMa shows that model diversity typically produces results between\nthe best and the worst ensemble members without clear improvement.", "AI": {"tldr": "This paper evaluates two diversity approaches in using LLMs for answering binary questions and finds that question interpretation diversity yields better performance than model diversity.", "motivation": "The study aims to enhance performance in machine learning models, particularly LLMs, by exploring effective implementations of diversity in answering binary questions.", "method": "The paper compares two approaches: model diversity (using multiple models) and question interpretation diversity (using variations of the same question with a single model), employing majority voting for consensus on answers.", "result": "Experiments on datasets like boolq, strategyqa, and pubmedqa reveal that question interpretation diversity improves ensemble accuracy consistently, while model diversity does not significantly enhance performance.", "conclusion": "Adopting question interpretation diversity is more effective for LLMs in answering binary questions, as opposed to relying solely on different models.", "key_contributions": ["Comparative analysis of two diversity techniques in LLMs", "Experimental results showing the superiority of question interpretation diversity", "Insights into performance dynamics of model diversity"], "limitations": "The study does not explore diversity techniques beyond the comparison made, which may limit broader applicability.", "keywords": ["machine learning", "large language models", "diversity approaches", "ensemble accuracy", "binary questions"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.21811", "pdf": "https://arxiv.org/pdf/2507.21811.pdf", "abs": "https://arxiv.org/abs/2507.21811", "title": "Helping or Homogenizing? GenAI as a Design Partner to Pre-Service SLPs for Just-in-Time Programming of AAC", "authors": ["Cynthia Zastudil", "Christine Holyfield", "Christine Kapp", "Kate Hamilton", "Kriti Baru", "Liam Newsam", "June A. Smith", "Stephen MacNeil"], "categories": ["cs.HC"], "comment": null, "summary": "Augmentative and alternative communication (AAC) devices are used by many\npeople around the world who experience difficulties in communicating verbally.\nOne AAC device which is especially useful for minimally verbal autistic\nchildren in developing language and communication skills are visual scene\ndisplays (VSD). VSDs use images with interactive hotspots embedded in them to\ndirectly connect language to real-world contexts which are meaningful to the\nAAC user. While VSDs can effectively support emergent communicators, their\nwidespread adoption is impacted by how difficult these devices are to\nconfigure. We developed a prototype that uses generative AI to automatically\nsuggest initial hotspots on an image to help non-experts efficiently create\nVSDs. We conducted a within-subjects user study to understand how effective our\nprototype is in supporting non-expert users, specifically pre-service\nspeech-language pathologists (SLP) who are not familiar with VSDs as an AAC\nintervention. Pre-service SLPs are actively studying to become clinically\ncertified SLPs and have domain-specific knowledge about language and\ncommunication skill development. We evaluated the effectiveness of our\nprototype based on creation time, quality, and user confidence. We also\nanalyzed the relevance and developmental appropriateness of the automatically\ngenerated hotspots and how often users interacted with the generated hotspots.\nOur results were mixed with SLPs becoming more efficient and confident.\nHowever, there were multiple negative impacts as well, including over-reliance\nand homogenization of communication options. The implications of these findings\nreach beyond the domain of AAC, especially as generative AI becomes more\nprevalent across domains, including assistive technology. Future work is needed\nto further identify and address these risks associated with integrating\ngenerative AI into assistive technology.", "AI": {"tldr": "This paper presents a generative AI prototype aimed at assisting non-expert users, particularly pre-service speech-language pathologists, in creating visual scene displays (VSDs) for augmentative and alternative communication (AAC) devices for minimally verbal autistic children.", "motivation": "To facilitate the configuration of VSDs for AAC users by utilizing generative AI, addressing challenges faced by non-experts.", "method": "A prototype was developed to automatically suggest hotspots on images for VSDs; a user study was conducted to evaluate its effectiveness among pre-service speech-language pathologists.", "result": "The use of the prototype improved SLPs' efficiency and confidence in creating VSDs, but also led to concerns about over-reliance and homogenization of communication options.", "conclusion": "While the prototype enhances the creation of VSDs, it raises caution regarding the risks of generative AI in assistive technology, necessitating further research.", "key_contributions": ["Development of a generative AI prototype for AAC device configuration", "User study demonstrating the impact on efficiency and confidence", "Identification of risks such as over-reliance and communication homogenization"], "limitations": "Mixed results in user experience and concerns about the quality of generated communication options.", "keywords": ["generative AI", "augmentative and alternative communication", "visual scene displays", "speech-language pathologists", "assistive technology"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.21186", "pdf": "https://arxiv.org/pdf/2507.21186.pdf", "abs": "https://arxiv.org/abs/2507.21186", "title": "Contrast-CAT: Contrasting Activations for Enhanced Interpretability in Transformer-based Text Classifiers", "authors": ["Sungmin Han", "Jeonghyun Lee", "Sangkyun Lee"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Transformers have profoundly influenced AI research, but explaining their\ndecisions remains challenging -- even for relatively simpler tasks such as\nclassification -- which hinders trust and safe deployment in real-world\napplications. Although activation-based attribution methods effectively explain\ntransformer-based text classification models, our findings reveal that these\nmethods can be undermined by class-irrelevant features within activations,\nleading to less reliable interpretations. To address this limitation, we\npropose Contrast-CAT, a novel activation contrast-based attribution method that\nrefines token-level attributions by filtering out class-irrelevant features. By\ncontrasting the activations of an input sequence with reference activations,\nContrast-CAT generates clearer and more faithful attribution maps. Experimental\nresults across various datasets and models confirm that Contrast-CAT\nconsistently outperforms state-of-the-art methods. Notably, under the MoRF\nsetting, it achieves average improvements of x1.30 in AOPC and x2.25 in LOdds\nover the most competing methods, demonstrating its effectiveness in enhancing\ninterpretability for transformer-based text classification.", "AI": {"tldr": "Contrast-CAT is a novel method designed to enhance the interpretability of transformer-based text classification by filtering out class-irrelevant features.", "motivation": "Explaining the decisions of transformer models is crucial for trust and safe deployment, yet existing attribution methods face challenges due to class-irrelevant features.", "method": "Contrast-CAT improves token-level attributions by contrasting the activations of input sequences with reference activations to eliminate irrelevant features.", "result": "Experimental results show that Contrast-CAT outperforms state-of-the-art attribution methods, with notable improvements in interpretability metrics like AOPC and LOdds.", "conclusion": "Contrast-CAT effectively enhances the reliability of interpretations in transformer-based text classification, making it a significant advancement in the field.", "key_contributions": ["Introduction of Contrast-CAT, a new attribution method for transformers.", "Improved filtering of class-irrelevant features in attributions.", "Demonstrated superior performance over existing methods in various datasets."], "limitations": "", "keywords": ["transformers", "explainability", "attribution methods", "human-computer interaction", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.21837", "pdf": "https://arxiv.org/pdf/2507.21837.pdf", "abs": "https://arxiv.org/abs/2507.21837", "title": "VeasyGuide: Personalized Visual Guidance for Low-vision Learners on Instructor Actions in Presentation Videos", "authors": ["Yotam Sechayk", "Ariel Shamir", "Amy Pavel", "Takeo Igarashi"], "categories": ["cs.HC"], "comment": "ASSETS '25, Denver, CO, USA", "summary": "Instructors often rely on visual actions such as pointing, marking, and\nsketching to convey information in educational presentation videos. These\nsubtle visual cues often lack verbal descriptions, forcing low-vision (LV)\nlearners to search for visual indicators or rely solely on audio, which can\nlead to missed information and increased cognitive load. To address this\nchallenge, we conducted a co-design study with three LV participants and\ndeveloped VeasyGuide, a tool that uses motion detection to identify instructor\nactions and dynamically highlight and magnify them. VeasyGuide produces\nfamiliar visual highlights that convey spatial context and adapt to diverse\nlearners and content through extensive personalization and real-time visual\nfeedback. VeasyGuide reduces visual search effort by clarifying what to look\nfor and where to look. In an evaluation with 8 LV participants, learners\ndemonstrated a significant improvement in detecting instructor actions, with\nfaster response times and significantly reduced cognitive load. A separate\nevaluation with 8 sighted participants showed that VeasyGuide also enhanced\nengagement and attentiveness, suggesting its potential as a universally\nbeneficial tool.", "AI": {"tldr": "VeasyGuide enhances learning for low-vision learners by dynamically highlighting instructor actions in educational videos, improving information detection and reducing cognitive load.", "motivation": "Low-vision learners struggle to interpret visual cues in educational videos, leading to missed information and increased cognitive load.", "method": "A co-design study with low-vision participants led to the development of VeasyGuide, which uses motion detection for highlighting instructor actions in real-time based on user needs.", "result": "The evaluation demonstrated significant improvements in action detection, response times, and reduced cognitive load for low-vision learners, and increased engagement among sighted participants.", "conclusion": "VeasyGuide presents a promising approach to making educational content more accessible for all learners through personalization and real-time feedback.", "key_contributions": ["Development of VeasyGuide for low-vision learner accessibility", "Use of motion detection for dynamic highlighting", "Improvement in engagement for both low-vision and sighted learners"], "limitations": "", "keywords": ["Low-vision learners", "Educational videos", "Motion detection"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.21234", "pdf": "https://arxiv.org/pdf/2507.21234.pdf", "abs": "https://arxiv.org/abs/2507.21234", "title": "Understanding Public Perception of Crime in Bangladesh: A Transformer-Based Approach with Explainability", "authors": ["Fatema Binte Hassan", "Md Al Jubair", "Mohammad Mehadi Hasan", "Tahmid Hossain", "S M Mehebubur Rahman Khan Shuvo", "Mohammad Shamsul Arefin"], "categories": ["cs.CL"], "comment": null, "summary": "In recent years, social media platforms have become prominent spaces for\nindividuals to express their opinions on ongoing events, including criminal\nincidents. As a result, public sentiment can shift dynamically over time. This\nstudy investigates the evolving public perception of crime-related news by\nclassifying user-generated comments into three categories: positive, negative,\nand neutral. A newly curated dataset comprising 28,528 Bangla-language social\nmedia comments was developed for this purpose. We propose a transformer-based\nmodel utilizing the XLM-RoBERTa Base architecture, which achieves a\nclassification accuracy of 97%, outperforming existing state-of-the-art methods\nin Bangla sentiment analysis. To enhance model interpretability, explainable AI\ntechnique is employed to identify the most influential features driving\nsentiment classification. The results underscore the effectiveness of\ntransformer-based models in processing low-resource languages such as Bengali\nand demonstrate their potential to extract actionable insights that can support\npublic policy formulation and crime prevention strategies.", "AI": {"tldr": "The paper investigates public sentiment on crime-related news through sentiment analysis of Bangla social media comments using a transformer-based model, achieving high accuracy and enhancing model interpretability.", "motivation": "To understand the dynamics of public perception regarding crime news as expressed on social media.", "method": "A transformer-based model using XLM-RoBERTa Base architecture was developed and tested on a dataset of 28,528 Bangla-language comments, classifying sentiments as positive, negative, or neutral.", "result": "The proposed model achieved a classification accuracy of 97%, surpassing existing state-of-the-art methods in Bangla sentiment analysis, and utilized explainable AI techniques for interpretability.", "conclusion": "Transformer-based models prove effective in low-resource languages like Bengali, providing valuable insights that can influence public policy and crime prevention strategies.", "key_contributions": ["Development of a new dataset of Bangla comments for sentiment analysis", "Introduction of a transformer-based model that outperforms existing methods", "Application of explainable AI for improved model interpretability"], "limitations": "", "keywords": ["Sentiment Analysis", "Social Media", "Transformer Models", "XLM-RoBERTa", "Public Policy"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2507.21900", "pdf": "https://arxiv.org/pdf/2507.21900.pdf", "abs": "https://arxiv.org/abs/2507.21900", "title": "Leveraging LLMs for Persona-Based Visualization of Election Data", "authors": ["Swaroop Panda", "Arun Kumar Sekar"], "categories": ["cs.HC"], "comment": null, "summary": "Visualizations are essential tools for disseminating information regarding\nelections and their outcomes, potentially influencing public perceptions.\nPersonas, delineating distinctive segments within the populace, furnish a\nvaluable framework for comprehending the nuanced perspectives, requisites, and\nbehaviors of diverse voter demographics. In this work, we propose making\nvisualizations tailored to these personas to make election information easier\nto understand and more relevant. Using data from UK parliamentary elections and\nnew developments in Large Language Models (LLMs), we create personas that\nencompass the diverse demographics, technological preferences, voting\ntendencies, and information consumption patterns observed among\nvoters.Subsequently, we elucidate how these personas can inform the design of\nvisualizations through specific design criteria. We then provide illustrative\nexamples of visualization prototypes based on these criteria and evaluate these\nprototypes using these personas and LLMs. We finally propose some actionable\ninsights based upon the framework and the different design artifacts.", "AI": {"tldr": "This paper explores the use of personas derived from diverse voter demographics to design effective visualizations for election information, enhanced by Large Language Models (LLMs).", "motivation": "The motivation behind this work is to improve the understanding and relevance of election information through personalized visualizations that cater to different voter demographics.", "method": "The study combines data from UK parliamentary elections with LLM developments to create personas representing various voter segments and inform the design of visualizations based on these personas.", "result": "Prototypes of visualizations tailored to identified personas were created and evaluated, illustrating improvements in clarity and relevance of election data for different voter groups.", "conclusion": "The findings suggest that designing visualizations with an understanding of voter personas greatly enhances information dissemination, offering actionable insights for election strategy and education.", "key_contributions": ["Utilization of personas for election data visualization", "Integration of Large Language Models in persona development", "Prototyping and evaluation of tailored visualizations for diverse voter segments"], "limitations": "", "keywords": ["visualization", "personas", "election", "Large Language Models", "voter demographics"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.21242", "pdf": "https://arxiv.org/pdf/2507.21242.pdf", "abs": "https://arxiv.org/abs/2507.21242", "title": "Bangla BERT for Hyperpartisan News Detection: A Semi-Supervised and Explainable AI Approach", "authors": ["Mohammad Mehadi Hasan", "Fatema Binte Hassan", "Md Al Jubair", "Zobayer Ahmed", "Sazzatul Yeakin", "Md Masum Billah"], "categories": ["cs.CL"], "comment": null, "summary": "In the current digital landscape, misinformation circulates rapidly, shaping\npublic perception and causing societal divisions. It is difficult to identify\nhyperpartisan news in Bangla since there aren't many sophisticated natural\nlanguage processing methods available for this low-resource language. Without\neffective detection methods, biased content can spread unchecked, posing\nserious risks to informed discourse. To address this gap, our research\nfine-tunes Bangla BERT. This is a state-of-the-art transformer-based model,\ndesigned to enhance classification accuracy for hyperpartisan news. We evaluate\nits performance against traditional machine learning models and implement\nsemi-supervised learning to enhance predictions further. Not only that, we use\nLIME to provide transparent explanations of the model's decision-making\nprocess, which helps to build trust in its outcomes. With a remarkable accuracy\nscore of 95.65%, Bangla BERT outperforms conventional approaches, according to\nour trial data. The findings of this study demonstrate the usefulness of\ntransformer models even in environments with limited resources, which opens the\ndoor to further improvements in this area.", "AI": {"tldr": "This paper presents a fine-tuned Bangla BERT model to classify hyperpartisan news in Bangla, achieving a significant accuracy of 95.65%, surpassing traditional machine learning methods, with interpretable predictions using LIME.", "motivation": "To address the rapid spread of misinformation in the Bangla language due to a lack of sophisticated NLP methods for detecting hyperpartisan news.", "method": "Fine-tuning Bangla BERT, a transformer-based model, and comparing its performance against traditional machine learning models while implementing semi-supervised learning and LIME for interpretability.", "result": "The Bangla BERT model achieved a remarkable accuracy of 95.65%, outperforming conventional approaches to hyperpartisan news classification.", "conclusion": "The study highlights the effectiveness of transformer models in low-resource settings, suggesting possibilities for further advancements in NLP for Bangla.", "key_contributions": ["Fine-tuning of Bangla BERT for hyperpartisan news classification", "Implementation of semi-supervised learning techniques", "Use of LIME for interpretable model predictions"], "limitations": "", "keywords": ["Bangla BERT", "hyperpartisan news", "natural language processing", "semi-supervised learning", "LIME"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2507.21953", "pdf": "https://arxiv.org/pdf/2507.21953.pdf", "abs": "https://arxiv.org/abs/2507.21953", "title": "MapAgent: Trajectory-Constructed Memory-Augmented Planning for Mobile Task Automation", "authors": ["Yi Kong", "Dianxi Shi", "Guoli Yang", "Zhang ke-di", "Chenlin Huang", "Xiaopeng Li", "Songchang Jin"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The recent advancement of autonomous agents powered by Large Language Models\n(LLMs) has demonstrated significant potential for automating tasks on mobile\ndevices through graphical user interfaces (GUIs). Despite initial progress,\nthese agents still face challenges when handling complex real-world tasks.\nThese challenges arise from a lack of knowledge about real-life mobile\napplications in LLM-based agents, which may lead to ineffective task planning\nand even cause hallucinations. To address these challenges, we propose a novel\nLLM-based agent framework called MapAgent that leverages memory constructed\nfrom historical trajectories to augment current task planning. Specifically, we\nfirst propose a trajectory-based memory mechanism that transforms task\nexecution trajectories into a reusable and structured page-memory database.\nEach page within a trajectory is extracted as a compact yet comprehensive\nsnapshot, capturing both its UI layout and functional context. Secondly, we\nintroduce a coarse-to-fine task planning approach that retrieves relevant pages\nfrom the memory database based on similarity and injects them into the LLM\nplanner to compensate for potential deficiencies in understanding real-world\napp scenarios, thereby achieving more informed and context-aware task planning.\nFinally, planned tasks are transformed into executable actions through a task\nexecutor supported by a dual-LLM architecture, ensuring effective tracking of\ntask progress. Experimental results in real-world scenarios demonstrate that\nMapAgent achieves superior performance to existing methods. The code will be\nopen-sourced to support further research.", "AI": {"tldr": "MapAgent is a novel LLM-based agent framework that enhances task planning on mobile devices by using a memory mechanism based on historical trajectories.", "motivation": "To improve task automation on mobile devices by addressing the challenges faced by LLM-based agents in understanding complex real-world tasks.", "method": "Introduced a trajectory-based memory mechanism to create a structured database of task execution snapshots, coupled with a coarse-to-fine task planning approach to enhance context-aware planning.", "result": "MapAgent outperforms existing methods in real-world scenarios by providing more informed task planning and effective execution.", "conclusion": "The proposed framework significantly improves task automation and will be open-sourced to encourage further research and development.", "key_contributions": ["Development of a trajectory-based memory mechanism for task planning.", "Coarse-to-fine task planning approach that enhances understanding of app scenarios.", "Execution of planned tasks through a dual-LLM architecture."], "limitations": "", "keywords": ["Large Language Models", "task planning", "mobile applications", "HCI", "autonomous agents"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.21302", "pdf": "https://arxiv.org/pdf/2507.21302.pdf", "abs": "https://arxiv.org/abs/2507.21302", "title": "Can human clinical rationales improve the performance and explainability of clinical text classification models?", "authors": ["Christoph Metzner", "Shang Gao", "Drahomira Herrmannova", "Heidi A. Hanson"], "categories": ["cs.CL"], "comment": null, "summary": "AI-driven clinical text classification is vital for explainable automated\nretrieval of population-level health information. This work investigates\nwhether human-based clinical rationales can serve as additional supervision to\nimprove both performance and explainability of transformer-based models that\nautomatically encode clinical documents. We analyzed 99,125 human-based\nclinical rationales that provide plausible explanations for primary cancer site\ndiagnoses, using them as additional training samples alongside 128,649\nelectronic pathology reports to evaluate transformer-based models for\nextracting primary cancer sites. We also investigated sufficiency as a way to\nmeasure rationale quality for pre-selecting rationales. Our results showed that\nclinical rationales as additional training data can improve model performance\nin high-resource scenarios but produce inconsistent behavior when resources are\nlimited. Using sufficiency as an automatic metric to preselect rationales also\nleads to inconsistent results. Importantly, models trained on rationales were\nconsistently outperformed by models trained on additional reports instead. This\nsuggests that clinical rationales don't consistently improve model performance\nand are outperformed by simply using more reports. Therefore, if the goal is\noptimizing accuracy, annotation efforts should focus on labeling more reports\nrather than creating rationales. However, if explainability is the priority,\ntraining models on rationale-supplemented data may help them better identify\nrationale-like features. We conclude that using clinical rationales as\nadditional training data results in smaller performance improvements and only\nslightly better explainability (measured as average token-level rationale\ncoverage) compared to training on additional reports.", "AI": {"tldr": "This study examines the use of human-based clinical rationales to enhance the performance and explainability of transformer-based models in classifying clinical text, particularly for cancer diagnoses.", "motivation": "To improve the performance and explainability of AI-driven clinical text classification by using human-based clinical rationales as additional supervision in transformer models.", "method": "Analyzed 99,125 human-based clinical rationales along with 128,649 electronic pathology reports to evaluate the performance of transformer-based models in extracting primary cancer site diagnoses.", "result": "Clinical rationales improved model performance in high-resource scenarios but showed inconsistent performance in low-resource settings. Models trained on rationales performed worse than those trained solely on more reports.", "conclusion": "The use of clinical rationales for training does not significantly enhance model performance compared to using more reports, though they may improve explainability.", "key_contributions": ["Investigation of the impact of human-based rationales on AI model performance and explainability in healthcare settings.", "Analysis of rationale quality using sufficiency as a metric.", "Comparative performance analysis between rationale-based training and standard report-based training."], "limitations": "Clinical rationales did not consistently improve performance and were outperformed by models trained on additional reports. Also, the sufficiency metric led to inconsistent results.", "keywords": ["clinical rationales", "transformer models", "explainability", "cancer diagnosis", "health informatics"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.22051", "pdf": "https://arxiv.org/pdf/2507.22051.pdf", "abs": "https://arxiv.org/abs/2507.22051", "title": "DataSway: Vivifying Metaphoric Visualization with Animation Clip Generation and Coordination", "authors": ["Liwenhan Xie", "Jiayi Zhou", "Anyi Rao", "Huamin Qu", "Xinhuan Shu"], "categories": ["cs.HC"], "comment": "19 pages, 5 figures", "summary": "Animating metaphoric visualizations brings data to life, enhancing the\ncomprehension of abstract data encodings and fostering deeper engagement.\nHowever, creators face significant challenges in designing these animations,\nsuch as crafting motions that align semantically with the metaphors,\nmaintaining faithful data representation during animation, and seamlessly\nintegrating interactivity. We propose a human-AI co-creation workflow that\nfacilitates creating animations for SVG-based metaphoric visualizations. Users\ncan initially derive animation clips for data elements from vision-language\nmodels (VLMs) and subsequently coordinate their timelines based on entity\norder, attribute values, spatial layout, or randomness. Our design decisions\nwere informed by a formative study with experienced designers (N=8). We further\ndeveloped a prototype, DataSway, and conducted a user study (N=14) to evaluate\nits creativity support and usability. A gallery with 6 cases demonstrates its\ncapabilities and applications in web-based hypermedia. We conclude with\nimplications for future research on bespoke data visualization animation.", "AI": {"tldr": "This paper presents a human-AI co-creation workflow for animating metaphoric visualizations in web-based contexts, addressing challenges in design and implementation.", "motivation": "The motivation behind this work is to enhance comprehension and engagement in abstract data through animated metaphoric visualizations, while overcoming design challenges faced by creators.", "method": "A human-AI co-creation workflow is proposed, utilizing vision-language models (VLMs) to generate animation clips and allowing users to coordinate timelines based on various attributes.", "result": "A user study with 14 participants showed that the prototype, DataSway, effectively supports creativity and usability for animating metaphoric visualizations.", "conclusion": "The findings highlight the potential of the proposed workflow and prototype in advancing data visualization practices, with recommendations for future research in the field.", "key_contributions": ["Development of a human-AI co-creation workflow for animation design", "Introduction of DataSway, a tool for creating SVG-based animated visualizations", "Insights gathered from user studies on creativity support and usability"], "limitations": "The study is limited by its small sample size and focus on specific design workflows, which may not generalize to all types of visualizations.", "keywords": ["visualization", "human-AI co-creation", "metaphoric visualizations", "animation", "data engagement"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.21319", "pdf": "https://arxiv.org/pdf/2507.21319.pdf", "abs": "https://arxiv.org/abs/2507.21319", "title": "Do Large Language Models Understand Morality Across Cultures?", "authors": ["Hadi Mohammadi", "Yasmeen F. S. S. Meijer", "Efthymia Papadopoulou", "Ayoub Bagheri"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have established them as\npowerful tools across numerous domains. However, persistent concerns about\nembedded biases, such as gender, racial, and cultural biases arising from their\ntraining data, raise significant questions about the ethical use and societal\nconsequences of these technologies. This study investigates the extent to which\nLLMs capture cross-cultural differences and similarities in moral perspectives.\nSpecifically, we examine whether LLM outputs align with patterns observed in\ninternational survey data on moral attitudes. To this end, we employ three\ncomplementary methods: (1) comparing variances in moral scores produced by\nmodels versus those reported in surveys, (2) conducting cluster alignment\nanalyses to assess correspondence between country groupings derived from LLM\noutputs and survey data, and (3) directly probing models with comparative\nprompts using systematically chosen token pairs. Our results reveal that\ncurrent LLMs often fail to reproduce the full spectrum of cross-cultural moral\nvariation, tending to compress differences and exhibit low alignment with\nempirical survey patterns. These findings highlight a pressing need for more\nrobust approaches to mitigate biases and improve cultural representativeness in\nLLMs. We conclude by discussing the implications for the responsible\ndevelopment and global deployment of LLMs, emphasizing fairness and ethical\nalignment.", "AI": {"tldr": "This study explores how well large language models (LLMs) capture cross-cultural moral differences, finding significant biases and misalignment with survey data on moral attitudes.", "motivation": "To investigate ethical concerns surrounding the biases present in LLMs and their alignment with cross-cultural moral perspectives.", "method": "Employ three methods: comparing variances in moral scores, conducting cluster alignment analyses, and probing models with comparative prompts.", "result": "Current LLMs fail to capture the diversity of cross-cultural moral attitudes, compressing differences and aligning poorly with international survey data.", "conclusion": "There's a pressing need to improve cultural representativeness in LLMs to ensure responsible and ethical development and deployment.", "key_contributions": ["Analysis of cross-cultural moral alignment in LLMs", "Identification of bias in LLM outputs", "Recommendations for improving ethical alignment in LLM development"], "limitations": "Focused on limited moral dimensions and cultural contexts; results may not generalize to all cultural or ethical frameworks.", "keywords": ["Large Language Models", "Cross-Cultural Differences", "Moral Perspectives", "Bias in AI", "Ethical AI"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.21331", "pdf": "https://arxiv.org/pdf/2507.21331.pdf", "abs": "https://arxiv.org/abs/2507.21331", "title": "A Deep Learning Automatic Speech Recognition Model for Shona Language", "authors": ["Leslie Wellington Sirora", "Mainford Mutandavari"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "This study presented the development of a deep learning-based Automatic\nSpeech Recognition system for Shona, a low-resource language characterized by\nunique tonal and grammatical complexities. The research aimed to address the\nchallenges posed by limited training data, lack of labelled data, and the\nintricate tonal nuances present in Shona speech, with the objective of\nachieving significant improvements in recognition accuracy compared to\ntraditional statistical models. The research first explored the feasibility of\nusing deep learning to develop an accurate ASR system for Shona. Second, it\ninvestigated the specific challenges involved in designing and implementing\ndeep learning architectures for Shona speech recognition and proposed\nstrategies to mitigate these challenges. Lastly, it compared the performance of\nthe deep learning-based model with existing statistical models in terms of\naccuracy. The developed ASR system utilized a hybrid architecture consisting of\na Convolutional Neural Network for acoustic modelling and a Long Short-Term\nMemory network for language modelling. To overcome the scarcity of data, data\naugmentation techniques and transfer learning were employed. Attention\nmechanisms were also incorporated to accommodate the tonal nature of Shona\nspeech. The resulting ASR system achieved impressive results, with a Word Error\nRate of 29%, Phoneme Error Rate of 12%, and an overall accuracy of 74%. These\nmetrics indicated the potential of deep learning to enhance ASR accuracy for\nunder-resourced languages like Shona. This study contributed to the advancement\nof ASR technology for under-resourced languages like Shona, ultimately\nfostering improved accessibility and communication for Shona speakers\nworldwide.", "AI": {"tldr": "A deep learning-based Automatic Speech Recognition system was developed for the Shona language, achieving significant improvements in recognition accuracy over traditional models.", "motivation": "To address the challenges of developing an ASR system for Shona, a low-resource language with unique tonal and grammatical complexities, by improving recognition accuracy.", "method": "The research utilized a hybrid architecture combining a Convolutional Neural Network for acoustic modeling and a Long Short-Term Memory network for language modeling, employing data augmentation and transfer learning techniques.", "result": "The deep learning-based ASR system achieved a Word Error Rate of 29%, Phoneme Error Rate of 12%, and an overall accuracy of 74%.", "conclusion": "The study demonstrated the potential of deep learning to enhance ASR accuracy for under-resourced languages like Shona, aiding in better communication for Shona speakers worldwide.", "key_contributions": ["Development of an ASR system tailored for Shona language", "Use of hybrid deep learning architecture with data augmentation techniques", "Achievement of benchmark recognition metrics for under-resourced languages"], "limitations": "", "keywords": ["Automatic Speech Recognition", "Deep Learning", "Shona Language"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.21065", "pdf": "https://arxiv.org/pdf/2507.21065.pdf", "abs": "https://arxiv.org/abs/2507.21065", "title": "Dialogic Social Learning for Artificial Agents: Enhancing LLM Ontology Acquisition through Mixed-Initiative Educational Interactions", "authors": ["Sabrina Patania", "Luca Annese", "Cansu Koyuturk", "Azzurra Ruggeri", "Dimitri Ognibene"], "categories": ["cs.CL", "cs.HC", "cs.LG", "cs.RO", "I.2.7, I.2.9, j.4,"], "comment": "submitted to ICSR2025", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing extensive offline datasets. However, they often face challenges in\nacquiring and integrating complex, knowledge online. Traditional AI training\nparadigms, predominantly based on supervised learning or reinforcement\nlearning, mirror a 'Piagetian' model of independent exploration. These\napproaches typically rely on large datasets and sparse feedback signals,\nlimiting the models' ability to learn efficiently from interactions. Drawing\ninspiration from Vygotsky's sociocultural theory, this study explores the\npotential of socially mediated learning paradigms to address these limitations.\n  We introduce a dynamic environment, termed the 'AI Social Gym', where an AI\nlearner agent engages in dyadic pedagogical dialogues with knowledgeable AI\nteacher agents. These interactions emphasize external, structured dialogue as a\ncore mechanism for knowledge acquisition, contrasting with methods that depend\nsolely on internal inference or pattern recognition.\n  Our investigation focuses on how different pedagogical strategies impact the\nAI learning process in the context of ontology acquisition. Empirical results\nindicate that such dialogic approaches-particularly those involving\nmixed-direction interactions combining top-down explanations with\nlearner-initiated questioning-significantly enhance the LLM's ability to\nacquire and apply new knowledge, outperforming both unidirectional\ninstructional methods and direct access to structured knowledge, formats\ntypically present in training datasets.\n  These findings suggest that integrating pedagogical and psychological\ninsights into AI and robot training can substantially improve post-training\nknowledge acquisition and response quality. This approach offers a\ncomplementary pathway to existing strategies like prompt engineering", "AI": {"tldr": "This study introduces the 'AI Social Gym', a dynamic environment for AI learners to engage in pedagogical dialogues with AI teachers, enhancing knowledge acquisition through dialogic interactions.", "motivation": "To address the limitations of traditional AI training paradigms that rely on large datasets and sparse feedback, by exploring socially mediated learning paradigms inspired by Vygotsky's sociocultural theory.", "method": "The research involves the creation of the 'AI Social Gym' environment where an AI learner engages in dialogues with AI teacher agents, focusing on different pedagogical strategies and their impact on ontology acquisition.", "result": "Empirical results show that dialogic interactions, especially mixed-direction strategies, significantly improve the LLM's ability to acquire and apply new knowledge compared to traditional methods.", "conclusion": "Integrating pedagogical and psychological insights into AI training can enhance post-training knowledge acquisition and response quality, providing a new avenue to improve LLM performance.", "key_contributions": ["Development of the AI Social Gym for AI learning.", "Demonstration of the effectiveness of dialogic interactions for knowledge acquisition in LLMs.", "Integration of pedagogical strategies into AI training methodologies."], "limitations": "", "keywords": ["Large Language Models", "sociocultural theory", "pedagogical dialogues", "knowledge acquisition", "AI training"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.21340", "pdf": "https://arxiv.org/pdf/2507.21340.pdf", "abs": "https://arxiv.org/abs/2507.21340", "title": "StructText: A Synthetic Table-to-Text Approach for Benchmark Generation with Multi-Dimensional Evaluation", "authors": ["Satyananda Kashyap", "Sola Shirai", "Nandana Mihindukulasooriya", "Horst Samulowitz"], "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "comment": "Data available:\n  https://huggingface.co/datasets/ibm-research/struct-text and code available\n  at: https://github.com/ibm/struct-text", "summary": "Extracting structured information from text, such as key-value pairs that\ncould augment tabular data, is quite useful in many enterprise use cases.\nAlthough large language models (LLMs) have enabled numerous automated pipelines\nfor converting natural language into structured formats, there is still a lack\nof benchmarks for evaluating their extraction quality, especially in specific\ndomains or focused documents specific to a given organization. Building such\nbenchmarks by manual annotations is labour-intensive and limits the size and\nscalability of the benchmarks. In this work, we present StructText, an\nend-to-end framework for automatically generating high-fidelity benchmarks for\nkey-value extraction from text using existing tabular data. It uses available\ntabular data as structured ground truth, and follows a two-stage\n``plan-then-execute'' pipeline to synthetically generate corresponding\nnatural-language text. To ensure alignment between text and structured source,\nwe introduce a multi-dimensional evaluation strategy that combines (a)\nLLM-based judgments on factuality, hallucination, and coherence and (b)\nobjective extraction metrics measuring numeric and temporal accuracy. We\nevaluated the proposed method on 71,539 examples across 49 datasets. Results\nreveal that while LLMs achieve strong factual accuracy and avoid hallucination,\nthey struggle with narrative coherence in producing extractable text. Notably,\nmodels presume numerical and temporal information with high fidelity yet this\ninformation becomes embedded in narratives that resist automated extraction. We\nrelease a framework, including datasets, evaluation tools, and baseline\nextraction systems, to support continued research.", "AI": {"tldr": "This paper presents StructText, a framework designed to automatically generate high-quality benchmarks for extracting key-value pairs from text using existing tabular data.", "motivation": "To address the lack of benchmarks for evaluating extraction quality of LLMs in specific domains, facilitating the conversion of natural language into structured formats for enterprise use.", "method": "StructText utilizes a two-stage 'plan-then-execute' pipeline that synthesizes natural language text from existing tabular data, and employs a multi-dimensional evaluation strategy that includes LLM-based judgments and objective extraction metrics.", "result": "Evaluated on 71,539 examples across 49 datasets, results indicate strong factual accuracy from LLMs, but challenges with narrative coherence affecting automated extraction.", "conclusion": "The framework includes released datasets, evaluation tools, and baseline extraction systems to stimulate further research in structured information extraction.", "key_contributions": ["Introduction of StructText framework for benchmark generation", "Two-stage plan-then-execute pipeline for text synthesis", "Multi-dimensional evaluation combining LLM judgments and objective metrics"], "limitations": "The framework may not cover all domain-specific variations in data extraction.", "keywords": ["structured information extraction", "large language models", "benchmark generation", "natural language processing", "enterprise use cases"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.21369", "pdf": "https://arxiv.org/pdf/2507.21369.pdf", "abs": "https://arxiv.org/abs/2507.21369", "title": "Turbocharging Web Automation: The Impact of Compressed History States", "authors": ["Xiyue Zhu", "Peng Tang", "Haofu Liao", "Srikar Appalaraju"], "categories": ["cs.CL"], "comment": null, "summary": "Language models have led to a leap forward in web automation. The current web\nautomation approaches take the current web state, history actions, and language\ninstruction as inputs to predict the next action, overlooking the importance of\nhistory states. However, the highly verbose nature of web page states can\nresult in long input sequences and sparse information, hampering the effective\nutilization of history states. In this paper, we propose a novel web history\ncompressor approach to turbocharge web automation using history states. Our\napproach employs a history compressor module that distills the most\ntask-relevant information from each history state into a fixed-length short\nrepresentation, mitigating the challenges posed by the highly verbose history\nstates. Experiments are conducted on the Mind2Web and WebLINX datasets to\nevaluate the effectiveness of our approach. Results show that our approach\nobtains 1.2-5.4% absolute accuracy improvements compared to the baseline\napproach without history inputs.", "AI": {"tldr": "This paper introduces a web history compressor that enhances web automation by effectively using historical states, resulting in improved accuracy in task execution.", "motivation": "To address the limitations of current web automation methods that do not effectively utilize historical states, particularly due to the verbose nature of web page states.", "method": "The authors propose a novel history compressor module that distills task-relevant information from verbose history states into a fixed-length representation to improve automation accuracy.", "result": "The proposed method demonstrates 1.2-5.4% absolute accuracy improvements on the Mind2Web and WebLINX datasets compared to a baseline approach that lacks history inputs.", "conclusion": "The introduction of a history compressor significantly boosts the effectiveness of web automation by leveraging historical states more efficiently.", "key_contributions": ["Introduction of a novel web history compressor for web automation.", "Demonstration of improved accuracy through experiments on Mind2Web and WebLINX datasets.", "Addressing the challenges of verbose web page states in automation tasks."], "limitations": "", "keywords": ["web automation", "history states", "language models", "accuracy improvement", "dataset evaluation"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.21428", "pdf": "https://arxiv.org/pdf/2507.21428.pdf", "abs": "https://arxiv.org/abs/2507.21428", "title": "MemTool: Optimizing Short-Term Memory Management for Dynamic Tool Calling in LLM Agent Multi-Turn Conversations", "authors": ["Elias Lumer", "Anmol Gulati", "Vamse Kumar Subbiah", "Pradeep Honaganahalli Basavaraju", "James A. Burke"], "categories": ["cs.CL"], "comment": "23 Pages, 20 Figures", "summary": "Large Language Model (LLM) agents have shown significant autonomous\ncapabilities in dynamically searching and incorporating relevant tools or Model\nContext Protocol (MCP) servers for individual queries. However, fixed context\nwindows limit effectiveness in multi-turn interactions requiring repeated,\nindependent tool usage. We introduce MemTool, a short-term memory framework\nenabling LLM agents to dynamically manage tools or MCP server contexts across\nmulti-turn conversations. MemTool offers three agentic architectures: 1)\nAutonomous Agent Mode, granting full tool management autonomy, 2) Workflow\nMode, providing deterministic control without autonomy, and 3) Hybrid Mode,\ncombining autonomous and deterministic control. Evaluating each MemTool mode\nacross 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100\nconsecutive user interactions, measuring tool removal ratios (short-term memory\nefficiency) and task completion accuracy. In Autonomous Agent Mode, reasoning\nLLMs achieve high tool-removal efficiency (90-94% over a 3-window average),\nwhile medium-sized models exhibit significantly lower efficiency (0-60%).\nWorkflow and Hybrid modes consistently manage tool removal effectively, whereas\nAutonomous and Hybrid modes excel at task completion. We present trade-offs and\nrecommendations for each MemTool mode based on task accuracy, agency, and model\ncapabilities.", "AI": {"tldr": "Introducing MemTool, a memory framework for LLM agents that enhances tool management across multi-turn conversations.", "motivation": "To address the limitations of fixed context windows in LLM agents during multi-turn interactions by enabling dynamic management of tools and contexts.", "method": "MemTool introduces three architectures: Autonomous Agent Mode, Workflow Mode, and Hybrid Mode, evaluated on 13+ LLMs using the ScaleMCP benchmark through 100 user interactions.", "result": "Autonomous Agent Mode achieved high tool removal efficiency (90-94%), while medium-sized models showed lower efficiency (0-60%). Workflow and Hybrid modes effectively managed tool removal, with varying success in task completion across modes.", "conclusion": "MemTool provides important insights for optimizing LLM interaction modes based on efficiency and task completion, highlighting trade-offs between agency and effectiveness.", "key_contributions": ["Developed the MemTool framework for LLM memory management", "Evaluated three distinct interaction architectures for LLM agents", "Demonstrated the efficiency of tool management in multi-turn dialogues"], "limitations": "The study primarily focuses on the performance of various LLMs; potential limitations in generalizability across different contexts or applications are not explored.", "keywords": ["Large Language Models", "Multi-turn interactions", "Memory management", "Tool management", "Human-Computer Interaction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.21073", "pdf": "https://arxiv.org/pdf/2507.21073.pdf", "abs": "https://arxiv.org/abs/2507.21073", "title": "Product vs. Process: Exploring EFL Students' Editing of AI-Generated Text for Expository Writing", "authors": ["David James Woo", "Yangyang Yu", "Kai Guo", "Yilin Huang", "April Ka Yeng Fung"], "categories": ["cs.CL", "cs.HC"], "comment": "45 pages, 11 figures", "summary": "Text generated by artificial intelligence (AI) chatbots is increasingly used\nin English as a foreign language (EFL) writing contexts, yet its impact on\nstudents' expository writing process and compositions remains understudied.\nThis research examines how EFL secondary students edit AI-generated text.\nExploring editing behaviors in their expository writing process and in\nexpository compositions, and their effect on human-rated scores for content,\norganization, language, and overall quality. Participants were 39 Hong Kong\nsecondary students who wrote an expository composition with AI chatbots in a\nworkshop. A convergent design was employed to analyze their screen recordings\nand compositions to examine students' editing behaviors and writing qualities.\nAnalytical methods included qualitative coding, descriptive statistics,\ntemporal sequence analysis, human-rated scoring, and multiple linear regression\nanalysis. We analyzed over 260 edits per dataset, and identified two editing\npatterns: one where students refined introductory units repeatedly before\nprogressing, and another where they quickly shifted to extensive edits in body\nunits (e.g., topic and supporting sentences). MLR analyses revealed that the\nnumber of AI-generated words positively predicted all score dimensions, while\nmost editing variables showed minimal impact. These results suggest a\ndisconnect between students' significant editing effort and improved\ncomposition quality, indicating AI supports but does not replace writing\nskills. The findings highlight the importance of genre-specific instruction and\nprocess-focused writing before AI integration. Educators should also develop\nassessments valuing both process and product to encourage critical engagement\nwith AI text.", "AI": {"tldr": "This research examines how EFL secondary students edit AI-generated text in their writing, analyzing editing behaviors and their impact on composition quality.", "motivation": "To study the impact of AI-generated text on students' expository writing processes and compositions in EFL contexts, which is currently understudied.", "method": "The study used a convergent design analyzing screen recordings and compositions of 39 Hong Kong secondary students, employing qualitative coding, descriptive statistics, temporal sequence analysis, human-rated scoring, and multiple linear regression analysis.", "result": "The analysis identified two editing patterns among students and revealed that while AI-generated words positively impacted scores, editing efforts had minimal overall impact on composition quality.", "conclusion": "AI supports EFL writing but does not replace the need for foundational writing skills, emphasizing the necessity of process-focused instruction before integrating AI.", "key_contributions": ["Identification of distinct editing patterns among EFL students using AI-generated text.", "Findings indicate a disconnect between editing efforts and improved writing quality.", "Highlights the need for genre-specific teaching and assessment methods in writing education."], "limitations": "Study limited to a specific population of Hong Kong secondary students and may not generalize to other contexts.", "keywords": ["AI in education", "EFL writing", "editing behavior", "composition quality", "human rated scoring"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2507.21432", "pdf": "https://arxiv.org/pdf/2507.21432.pdf", "abs": "https://arxiv.org/abs/2507.21432", "title": "Towards Locally Deployable Fine-Tuned Causal Large Language Models for Mode Choice Behaviour", "authors": ["Tareq Alsaleh", "Bilal Farooq"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study investigates the adoption of open-access, locally deployable\ncausal large language models (LLMs) for travel mode choice prediction and\nintroduces LiTransMC, the first fine-tuned causal LLM developed for this task.\nWe systematically benchmark eleven LLMs (1-12B parameters) across three stated\nand revealed preference datasets, testing 396 configurations and generating\nover 79,000 synthetic commuter predictions. Beyond predictive accuracy, we\nevaluate models generated reasoning using BERTopic for topic modelling and a\nnovel Explanation Strength Index, providing the first structured analysis of\nhow LLMs articulate decision factors in alignment with behavioural theory.\nLiTransMC, fine-tuned using parameter efficient and loss masking strategy,\nachieved a weighted F1 score of 0.6845 and a Jensen-Shannon Divergence of\n0.000245, surpassing both untuned local models and larger proprietary systems,\nincluding GPT-4o with advanced persona inference and embedding-based loading,\nwhile also outperforming classical mode choice methods such as discrete choice\nmodels and machine learning classifiers for the same dataset. This dual\nimprovement, i.e., high instant-level accuracy and near-perfect distributional\ncalibration, demonstrates the feasibility of creating specialist, locally\ndeployable LLMs that integrate prediction and interpretability. Through\ncombining structured behavioural prediction with natural language reasoning,\nthis work unlocks the potential for conversational, multi-task transport models\ncapable of supporting agent-based simulations, policy testing, and behavioural\ninsight generation. These findings establish a pathway for transforming general\npurpose LLMs into specialized, explainable tools for transportation research\nand policy formulation, while maintaining privacy, reducing cost, and\nbroadening access through local deployment.", "AI": {"tldr": "This study presents LiTransMC, a fine-tuned causal LLM for predicting travel mode choices, outperforming traditional models.", "motivation": "To develop locally deployable causal LLMs that improve travel mode choice prediction while ensuring interpretability and privacy.", "method": "Benchmarked eleven LLMs across three datasets, generating synthetic predictions and evaluating with BERTopic and a novel Explanation Strength Index.", "result": "LiTransMC achieved a weighted F1 score of 0.6845, outperforming larger proprietary models and classical methods in predictive accuracy and distributional calibration.", "conclusion": "The study shows the potential of developing specialist LLMs that combine prediction and interpretability for transport research and policy.", "key_contributions": ["Introduction of LiTransMC, a novel causal LLM for travel mode choice", "Systematic benchmarking of multiple LLMs demonstrating improved predictive performance", "Exploration of model reasoning and interpretability through structured analysis"], "limitations": "", "keywords": ["Causal LLMs", "Travel Mode Choice", "Interpretability"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.21476", "pdf": "https://arxiv.org/pdf/2507.21476.pdf", "abs": "https://arxiv.org/abs/2507.21476", "title": "Which LLMs Get the Joke? Probing Non-STEM Reasoning Abilities with HumorBench", "authors": ["Reuben Narad", "Siddharth Suresh", "Jiayi Chen", "Pine S. L. Dysart-Bricken", "Bob Mankoff", "Robert Nowak", "Jifan Zhang", "Lalit Jain"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present HumorBench, a benchmark designed to evaluate large language\nmodels' (LLMs) ability to reason about and explain sophisticated humor in\ncartoon captions. As reasoning models increasingly saturate existing benchmarks\nin mathematics and science, novel and challenging evaluations of model\nintelligence beyond STEM domains are essential. Reasoning is fundamentally\ninvolved in text-based humor comprehension, requiring the identification of\nconnections between concepts in cartoons/captions and external cultural\nreferences, wordplays, and other mechanisms. HumorBench includes approximately\n300 unique cartoon-caption pairs from the New Yorker Caption Contest and\nCartoonstock.com, with expert-annotated evaluation rubrics identifying\nessential joke elements. LLMs are evaluated based on their explanations towards\nthe humor and abilities in identifying the joke elements. To perform well on\nthis task, models must form and test hypotheses about associations between\nconcepts, potentially backtracking from initial interpretations to arrive at\nthe most plausible explanation. Our extensive benchmarking of current SOTA\nmodels reveals three key insights: (1) LLM progress on STEM reasoning transfers\neffectively to humor comprehension; (2) models trained exclusively on STEM\nreasoning data still perform well on HumorBench, demonstrating strong\ntransferability of reasoning abilities; and (3) test-time scaling by increasing\nthinking token budgets yields mixed results across different models in humor\nreasoning.", "AI": {"tldr": "HumorBench is a benchmark to assess LLMs' capabilities in understanding and explaining humor in cartoon captions, featuring 300 unique pairs and expert-annotated rubrics.", "motivation": "To evaluate LLMs in reasoning beyond STEM domains, particularly in humor comprehension.", "method": "HumorBench utilizes approximately 300 cartoon-caption pairs with annotated evaluation rubrics to assess LLMs' explanations and identification of joke elements.", "result": "Current state-of-the-art LLMs show effective transfer of STEM reasoning skills to humor comprehension, and models trained on STEM data perform well on HumorBench.", "conclusion": "Increased token budgets for reasoning yield variable performance across models in humor reasoning tasks.", "key_contributions": ["Introduction of HumorBench for LLM evaluation in humor reasoning", "Findings on the transferability of STEM reasoning abilities to humor comprehension", "Insights on model performance with varying token budgets."], "limitations": "", "keywords": ["Humor", "Large Language Models", "Benchmarking", "Reasoning", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.21482", "pdf": "https://arxiv.org/pdf/2507.21482.pdf", "abs": "https://arxiv.org/abs/2507.21482", "title": "Improving Task Diversity in Label Efficient Supervised Finetuning of LLMs", "authors": ["Abhinav Arabelly", "Jagrut Nemade", "Robert D Nowak", "Jifan Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse domains, but developing high-performing models for specialized\napplications often requires substantial human annotation -- a process that is\ntime-consuming, labor-intensive, and expensive. In this paper, we address the\nlabel-efficient learning problem for supervised finetuning (SFT) by leveraging\ntask-diversity as a fundamental principle for effective data selection. This is\nmarkedly different from existing methods based on the prompt-diversity. Our\napproach is based on two key observations: 1) task labels for different prompts\nare often readily available; 2) pre-trained models have significantly varying\nlevels of confidence across tasks. We combine these facts to devise a simple\nyet effective sampling strategy: we select examples across tasks using an\ninverse confidence weighting strategy. This produces models comparable to or\nbetter than those trained with more complex sampling procedures, while being\nsignificantly easier to implement and less computationally intensive. Notably,\nour experimental results demonstrate that this method can achieve better\naccuracy than training on the complete dataset (a 4\\% increase in MMLU score).\nAcross various annotation budgets and two instruction finetuning datasets, our\nalgorithm consistently performs at or above the level of the best existing\nmethods, while reducing annotation costs by up to 80\\%.", "AI": {"tldr": "This paper introduces a label-efficient learning strategy for supervised finetuning of large language models by leveraging task-diversity for effective data selection, resulting in higher accuracy and reduced annotation costs.", "motivation": "To address the labor-intensive and costly process of human annotation in developing specialized applications of large language models.", "method": "The proposed approach leverages task-diversity for data selection using inverse confidence weighting to sample examples across tasks, offering a simpler and less computationally intensive alternative to existing methods.", "result": "The method achieves better accuracy than models trained on complete datasets, showing a 4% increase in MMLU score and consistently performing at or above existing best methods while reducing annotation costs by up to 80%.", "conclusion": "This approach presents a straightforward yet effective solution for improving label efficiency in supervised finetuning by utilizing task-diversity.", "key_contributions": ["Introduction of a task-diversity principle for data selection in supervised finetuning", "Development of an inverse confidence weighting sampling strategy", "Demonstrated accuracy improvements and significant reductions in annotation costs."], "limitations": "", "keywords": ["Large Language Models", "label-efficient learning", "task-diversity", "inverse confidence weighting", "annotation costs"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.21500", "pdf": "https://arxiv.org/pdf/2507.21500.pdf", "abs": "https://arxiv.org/abs/2507.21500", "title": "VN-MTEB: Vietnamese Massive Text Embedding Benchmark", "authors": ["Loc Pham", "Tung Luu", "Thu Vo", "Minh Nguyen", "Viet Hoang"], "categories": ["cs.CL", "cs.AI"], "comment": "19 pages (including reference, appendix) 41 datasets from 6 tasks\n  (retrieval, classification, pair-classification, clustering, rerank, sts) 7\n  figures, 16 tables, benchmark 18 text embedding models", "summary": "Vietnam ranks among the top countries in terms of both internet traffic and\nonline toxicity. As a result, implementing embedding models for recommendation\nand content control duties in applications is crucial. However, a lack of\nlarge-scale test datasets, both in volume and task diversity, makes it tricky\nfor scientists to effectively evaluate AI models before deploying them in\nreal-world, large-scale projects. To solve this important problem, we introduce\na Vietnamese benchmark, VN-MTEB for embedding models, which we created by\ntranslating a large number of English samples from the Massive Text Embedding\nBenchmark using our new automated framework. We leverage the strengths of large\nlanguage models (LLMs) and cutting-edge embedding models to conduct translation\nand filtering processes to retain high-quality samples, guaranteeing a natural\nflow of language and semantic fidelity while preserving named entity\nrecognition (NER) and code snippets. Our comprehensive benchmark consists of 41\ndatasets from six tasks specifically designed for Vietnamese text embeddings.\nIn our analysis, we find that bigger and more complex models using Rotary\nPositional Embedding outperform those using Absolute Positional Embedding in\nembedding tasks. Datasets are available at HuggingFace:\nhttps://huggingface.co/collections/GreenNode/vn-mteb-68871433f0f7573b8e1a6686", "AI": {"tldr": "The paper introduces VN-MTEB, a benchmark for Vietnamese text embeddings created to evaluate AI models, addressing limitations in existing datasets.", "motivation": "There is a significant lack of large-scale test datasets for Vietnamese text, which hampers effective evaluation of AI models in real-world applications.", "method": "A large number of English samples from the Massive Text Embedding Benchmark were translated into Vietnamese using an automated framework leveraging large language models. The benchmark consists of 41 datasets across six different tasks.", "result": "Analysis shows that models using Rotary Positional Embedding outperform those using Absolute Positional Embedding in embedding tasks.", "conclusion": "VN-MTEB provides a comprehensive resource for evaluating embedding models for Vietnamese text and includes datasets available on HuggingFace.", "key_contributions": ["Introduction of a Vietnamese benchmark for AI models", "Translation framework using LLMs for creating high-quality datasets", "Performance analysis of embedding models with different positional embedding techniques"], "limitations": "The study focuses solely on Vietnamese text, potentially limiting its applicability to other languages.", "keywords": ["Vietnamese benchmark", "text embeddings", "Rotary Positional Embedding", "AI model evaluation", "large language models"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.21509", "pdf": "https://arxiv.org/pdf/2507.21509.pdf", "abs": "https://arxiv.org/abs/2507.21509", "title": "Persona Vectors: Monitoring and Controlling Character Traits in Language Models", "authors": ["Runjin Chen", "Andy Arditi", "Henry Sleight", "Owain Evans", "Jack Lindsey"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models interact with users through a simulated 'Assistant'\npersona. While the Assistant is typically trained to be helpful, harmless, and\nhonest, it sometimes deviates from these ideals. In this paper, we identify\ndirections in the model's activation space-persona vectors-underlying several\ntraits, such as evil, sycophancy, and propensity to hallucinate. We confirm\nthat these vectors can be used to monitor fluctuations in the Assistant's\npersonality at deployment time. We then apply persona vectors to predict and\ncontrol personality shifts that occur during training. We find that both\nintended and unintended personality changes after finetuning are strongly\ncorrelated with shifts along the relevant persona vectors. These shifts can be\nmitigated through post-hoc intervention, or avoided in the first place with a\nnew preventative steering method. Moreover, persona vectors can be used to flag\ntraining data that will produce undesirable personality changes, both at the\ndataset level and the individual sample level. Our method for extracting\npersona vectors is automated and can be applied to any personality trait of\ninterest, given only a natural-language description.", "AI": {"tldr": "The paper introduces persona vectors to monitor and control personality shifts in large language models, ensuring alignment with desired traits and mitigating unintended changes during training.", "motivation": "To identify and manage deviations in the Assistant persona of large language models, which may lead to undesirable traits such as evil or hallucination.", "method": "The authors propose a method to extract persona vectors that represent various personality traits, allowing for real-time monitoring and intervention during deployment and training.", "result": "The study found that personality shifts during training are correlated with changes along persona vectors, and that these shifts can be mitigated with targeted interventions.", "conclusion": "The paper demonstrates that automated persona vector extraction can help predict and control personality changes in language models, improving alignment with intended user interaction.", "key_contributions": ["Introduction of persona vectors for monitoring model personality shifts", "Development of preventative steering methods to avoid undesirable changes", "Automated extraction method applicable to various personality traits"], "limitations": "", "keywords": ["personality vectors", "large language models", "personality traits", "machine learning", "HCI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.21522", "pdf": "https://arxiv.org/pdf/2507.21522.pdf", "abs": "https://arxiv.org/abs/2507.21522", "title": "Model-free Speculative Decoding for Transformer-based ASR with Token Map Drafting", "authors": ["Tuan Vu Ho", "Hiroaki Kokubo", "Masaaki Yamamoto", "Yohei Kawaguchi"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at EUSIPCO 2025", "summary": "End-to-end automatic speech recognition (ASR) systems based on transformer\narchitectures, such as Whisper, offer high transcription accuracy and\nrobustness. However, their autoregressive decoding is computationally\nexpensive, hence limiting deployment on CPU-based and resource-constrained\ndevices. Speculative decoding (SD) mitigates this issue by using a smaller\ndraft model to propose candidate tokens, which are then verified by the main\nmodel. However, this approach is impractical for devices lacking hardware\naccelerators like GPUs. To address this, we propose \\emph{Token Map Drafting},\na model-free SD technique that eliminates the need for a separate draft model.\nInstead, we leverage a precomputed n-gram token map derived from\ndomain-specific training data, enabling efficient speculative decoding with\nminimal overhead. Our method significantly accelerates ASR inference in\nstructured, low-perplexity domains without sacrificing transcription accuracy.\nExperimental results demonstrate decoding speed-ups of $1.27\\times$ on the\nCI-AVSR dataset and $1.37\\times$ on our internal dataset without degrading\nrecognition accuracy. Additionally, our approach achieves a $10\\%$ absolute\nimprovement in decoding speed over the Distill-spec baseline running on CPU,\nhighlighting its effectiveness for on-device ASR applications.", "AI": {"tldr": "This paper introduces a model-free speculative decoding technique called Token Map Drafting, which improves the efficiency of automatic speech recognition systems on resource-constrained devices without compromising accuracy.", "motivation": "End-to-end ASR systems like Whisper face challenges with high computational costs during autoregressive decoding, limiting their deployment on resource-constrained devices.", "method": "The proposed Token Map Drafting technique utilizes a precomputed n-gram token map from domain-specific training data to accelerate speculative decoding without a separate draft model.", "result": "The method achieves speed-ups of 1.27x on the CI-AVSR dataset and 1.37x on an internal dataset, with a 10% improvement over the Distill-spec baseline on CPU, all while maintaining transcription accuracy.", "conclusion": "Token Map Drafting offers an effective solution for enhancing ASR inference speed on devices lacking hardware accelerators, making it suitable for on-device applications.", "key_contributions": ["Introduction of Token Map Drafting for ASR", "Significant speed improvements in decoding without accuracy loss", "Model-free solution that reduces reliance on separate draft models"], "limitations": "", "keywords": ["automatic speech recognition", "speculative decoding", "Token Map Drafting"], "importance_score": 4, "read_time_minutes": 7}}
{"id": "2507.21526", "pdf": "https://arxiv.org/pdf/2507.21526.pdf", "abs": "https://arxiv.org/abs/2507.21526", "title": "TriangleMix: A Lossless and Efficient Attention Pattern for Long Context Prefilling", "authors": ["Zhiyuan He", "Yike Zhang", "Chengruidong Zhang", "Huiqiang Jiang", "Yuqing Yang", "Lili Qiu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) rely on attention mechanisms whose time\ncomplexity grows quadratically with input sequence length, creating significant\ncomputational bottlenecks during the prefilling stage. Existing static sparse\nattention methods typically degrade accuracy, while dynamic sparsity methods\nintroduce additional computational overhead due to runtime sparse index\nestimation. To address these limitations, we propose TriangleMix, a novel\ntraining-free static attention pattern. TriangleMix employs dense attention in\nshallow layers and switches to a triangle-shaped sparse pattern in deeper\nlayers. Extensive experiments demonstrate that TriangleMix reduces attention\noverhead by 3.7x to 15.3x in deep layers, and decreases overall\nTime-to-First-Token (TTFT) by 12% to 32% for sequence lengths ranging from 32K\nto 128K, without sacrificing model accuracy. Moreover, TriangleMix can be\nseamlessly integrated with dynamic sparsity methods to achieve further speedup,\ne.g. accelerating MInference by 19% at 128K, highlighting its potential to\nenhance LLM inference efficiency.", "AI": {"tldr": "TriangleMix reduces attention overhead in LLMs by employing a novel static attention pattern, achieving significant speedups while maintaining accuracy.", "motivation": "To overcome the computational bottlenecks caused by the quadratic growth of attention mechanisms in LLMs during the prefilling stage.", "method": "TriangleMix implements dense attention in shallow layers and a triangle-shaped sparse pattern in deeper layers, providing a training-free static attention solution.", "result": "TriangleMix reduces attention overhead by 3.7x to 15.3x in deep layers and decreases Time-to-First-Token (TTFT) by 12% to 32% for sequence lengths of 32K to 128K.", "conclusion": "TriangleMix enhances LLM inference efficiency and integrates well with dynamic sparsity methods for further improvements.", "key_contributions": ["Introduction of TriangleMix as a training-free static attention pattern", "Reduction of attention overhead significantly across layers", "Integration with dynamic sparsity methods for improved inference speed"], "limitations": "", "keywords": ["Large Language Models", "attention mechanisms", "sparsity methods"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.21532", "pdf": "https://arxiv.org/pdf/2507.21532.pdf", "abs": "https://arxiv.org/abs/2507.21532", "title": "Automatic Classification of User Requirements from Online Feedback -- A Replication Study", "authors": ["Meet Bhatt", "Nic Boilard", "Muhammad Rehan Chaudhary", "Cole Thompson", "Jacob Idoko", "Aakash Sorathiya", "Gouri Ginde"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "10 pages, 3 figures, Replication package available at\n  https://zenodo.org/records/15626782, Accepted at AIRE 2025 (12th\n  International Workshop on Artificial Intelligence and Requirements\n  Engineering)", "summary": "Natural language processing (NLP) techniques have been widely applied in the\nrequirements engineering (RE) field to support tasks such as classification and\nambiguity detection. Although RE research is rooted in empirical investigation,\nit has paid limited attention to replicating NLP for RE (NLP4RE) studies. The\nrapidly advancing realm of NLP is creating new opportunities for efficient,\nmachine-assisted workflows, which can bring new perspectives and results to the\nforefront. Thus, we replicate and extend a previous NLP4RE study (baseline),\n\"Classifying User Requirements from Online Feedback in Small Dataset\nEnvironments using Deep Learning\", which evaluated different deep learning\nmodels for requirement classification from user reviews. We reproduced the\noriginal results using publicly released source code, thereby helping to\nstrengthen the external validity of the baseline study. We then extended the\nsetup by evaluating model performance on an external dataset and comparing\nresults to a GPT-4o zero-shot classifier. Furthermore, we prepared the\nreplication study ID-card for the baseline study, important for evaluating\nreplication readiness. Results showed diverse reproducibility levels across\ndifferent models, with Naive Bayes demonstrating perfect reproducibility. In\ncontrast, BERT and other models showed mixed results. Our findings revealed\nthat baseline deep learning models, BERT and ELMo, exhibited good\ngeneralization capabilities on an external dataset, and GPT-4o showed\nperformance comparable to traditional baseline machine learning models.\nAdditionally, our assessment confirmed the baseline study's replication\nreadiness; however missing environment setup files would have further enhanced\nreadiness. We include this missing information in our replication package and\nprovide the replication study ID-card for our study to further encourage and\nsupport the replication of our study.", "AI": {"tldr": "The paper replicates and extends a previous NLP4RE study focused on classifying user requirements from online feedback using deep learning, providing improved insights into model performance and replication readiness.", "motivation": "To replicate and extend previous research in applying NLP techniques to requirements engineering, thus enhancing understanding and validation of NLP4RE methodologies.", "method": "The study reproduces original results using source code and evaluates model performance on an external dataset, comparing it to a GPT-4o zero-shot classifier.", "result": "Diverse reproducibility levels were found across models, with Naive Bayes showing perfect reproducibility and BERT demonstrating mixed results. GPT-4o performed comparably to traditional models and the study confirmed the baseline's replication readiness.", "conclusion": "The findings indicate potential for improved machine-assisted workflows in requirements engineering, emphasizing the need for better replication practices and transparency in setup documentation.", "key_contributions": ["Replication of a previous NLP4RE study to confirm findings.", "Evaluation of model performance on an external dataset and comparison with a GPT-4o zero-shot classifier.", "Provision of a replication study ID-card to enhance replication readiness."], "limitations": "Missing environment setup files hindered full replication readiness but were included in the study's replication package.", "keywords": ["NLP4RE", "requirements engineering", "deep learning", "replication study", "GPT-4o"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.21536", "pdf": "https://arxiv.org/pdf/2507.21536.pdf", "abs": "https://arxiv.org/abs/2507.21536", "title": "Modern Uyghur Dependency Treebank (MUDT): An Integrated Morphosyntactic Framework for a Low-Resource Language", "authors": ["Jiaxin Zuo", "Yiquan Wang", "Yuan Pan", "Xiadiya Yibulayin"], "categories": ["cs.CL"], "comment": null, "summary": "To address a critical resource gap in Uyghur Natural Language Processing\n(NLP), this study introduces a dependency annotation framework designed to\novercome the limitations of existing treebanks for the low-resource,\nagglutinative language. This inventory includes 18 main relations and 26\nsubtypes, with specific labels such as cop:zero for verbless clauses and\ninstr:case=loc/dat for nuanced instrumental functions. To empirically validate\nthe necessity of this tailored approach, we conducted a cross-standard\nevaluation using a pre-trained Universal Dependencies parser. The analysis\nrevealed a systematic 47.9% divergence in annotations, pinpointing the\ninadequacy of universal schemes for handling Uyghur-specific structures.\nGrounded in nine annotation principles that ensure typological accuracy and\nsemantic transparency, the Modern Uyghur Dependency Treebank (MUDT) provides a\nmore accurate and semantically transparent representation, designed to enable\nsignificant improvements in parsing and downstream NLP tasks, and offers a\nreplicable model for other morphologically complex languages.", "AI": {"tldr": "This study presents a dependency annotation framework for Uyghur NLP addressing resource gaps, highlighting significant divergences from existing standards and improving parsing accuracy.", "motivation": "To address a critical resource gap in Uyghur Natural Language Processing, this research aims to create a dependency annotation framework for the language.", "method": "The study introduces 18 main relations and 26 subtypes in the framework and conducts a cross-standard evaluation using a pre-trained Universal Dependencies parser.", "result": "The analysis showed a 47.9% divergence in annotations, demonstrating the limitations of universal schemes in handling Uyghur-specific structures.", "conclusion": "The Modern Uyghur Dependency Treebank (MUDT) improves parsing and downstream NLP tasks and serves as a model for other morphologically complex languages.", "key_contributions": ["Development of a tailored dependency annotation framework for Uyghur.", "Identification of 47.9% divergence in annotations from existing standards.", "Introduction of nine annotation principles ensuring typological accuracy."], "limitations": "", "keywords": ["Uyghur", "Natural Language Processing", "Dependency Annotation", "Treebank", "Morphological Complexity"], "importance_score": 3, "read_time_minutes": 5}}
{"id": "2507.21544", "pdf": "https://arxiv.org/pdf/2507.21544.pdf", "abs": "https://arxiv.org/abs/2507.21544", "title": "MAGIC: A Multi-Hop and Graph-Based Benchmark for Inter-Context Conflicts in Retrieval-Augmented Generation", "authors": ["Jungyeon Lee", "Kangmin Lee", "Taeuk Kim"], "categories": ["cs.CL"], "comment": null, "summary": "Knowledge conflict often arises in retrieval-augmented generation (RAG)\nsystems, where retrieved documents may be inconsistent with one another or\ncontradict the model's parametric knowledge. Existing benchmarks for\ninvestigating the phenomenon have notable limitations, including a narrow focus\non the question answering setup, heavy reliance on entity substitution\ntechniques, and a restricted range of conflict types. To address these issues,\nwe propose a knowledge graph (KG)-based framework that generates varied and\nsubtle conflicts between two similar yet distinct contexts, while ensuring\ninterpretability through the explicit relational structure of KGs. Experimental\nresults on our benchmark, MAGIC, provide intriguing insights into the inner\nworkings of LLMs regarding knowledge conflict: both open-source and proprietary\nmodels struggle with conflict detection -- especially when multi-hop reasoning\nis required -- and often fail to pinpoint the exact source of contradictions.\nFinally, we present in-depth analyses that serve as a foundation for improving\nLLMs in integrating diverse, sometimes even conflicting, information.", "AI": {"tldr": "The paper addresses knowledge conflict in RAG systems using a knowledge graph-based framework to generate conflicts and analyze LLM performance.", "motivation": "To investigate knowledge conflict in retrieval-augmented generation systems, addressing limitations in existing benchmarks.", "method": "A knowledge graph (KG)-based framework generates varied conflicts between distinct contexts, enabling interpretation through the relational structure of KGs.", "result": "Experimental results on the MAGIC benchmark reveal that LLMs, both open-source and proprietary, struggle with conflict detection, especially in multi-hop reasoning scenarios.", "conclusion": "In-depth analyses provide a foundation for enhancing LLMs' ability to process conflicting information more effectively.", "key_contributions": ["Introduction of a KG-based framework for generating conflicts", "Development of the MAGIC benchmark for examining knowledge conflict", "Insights into LLMs' capabilities and limitations in conflict detection"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Knowledge Graph", "LLM", "Conflict Detection", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2410.04177", "pdf": "https://arxiv.org/pdf/2410.04177.pdf", "abs": "https://arxiv.org/abs/2410.04177", "title": "Exploring Keyboard Positioning and Ten-Finger Typing in Mixed Reality", "authors": ["Cecilia Schmitz", "Joshua Reynolds", "Scott Kuhl", "Keith Vertanen"], "categories": ["cs.HC"], "comment": null, "summary": "Accuracy and speed are pivotal when typing. Mixed reality typing is typically\nperformed by typing on a midair keyboard with your index fingers. This deprives\nusers of both the tactile feedback available on physical devices and the\nability to press keys with the most convenient finger. Our first experiment\ninvestigated providing tactile feedback by positioning the virtual keyboard on\na table or wall. The keyboard was deterministic (without auto-correct),\nsupported mixed case typing with symbols, and relied only on the hand-tracking\nprovided by a commodity headset's egocentric cameras. Users preferred and had\nthe highest entry rate of 12 words-per-minute using a midair keyboard. Error\nrates were similar in all conditions. Our second experiment explored ten-finger\ntyping and used a novel eye-tracking technique to avoid accidental key presses.\nThis technique was preferred for ten-finger typing and halved corrections.\nHowever, participants were faster using their index fingers without\neye-tracking at 11 words-per-minute.", "AI": {"tldr": "This paper investigates typing performance in mixed reality using virtual keyboards, focusing on tactile feedback and eye-tracking techniques.", "motivation": "The need for improved typing accuracy and speed in mixed reality environments, as traditional midair typing methods lack tactile feedback and convenience.", "method": "Two experiments were conducted: the first explored tactile feedback by positioning a virtual keyboard on various surfaces and the second investigated ten-finger typing using an eye-tracking technique to enhance keystroke accuracy.", "result": "Participants achieved the highest typing speed of 12 words-per-minute with the midair keyboard and had lower correction rates with ten-finger typing when using eye-tracking, despite being slightly slower than index-finger typing.", "conclusion": "While midair keyboards without tactile feedback were still preferred for speed, incorporating tactile feedback and eye-tracking can enhance accuracy in mixed reality typing.", "key_contributions": ["Introduction of tactile feedback for virtual keyboards in mixed reality", "Novel eye-tracking technique to prevent accidental key presses", "Comparative analysis of typing methods in mixed reality environments"], "limitations": "Results may vary based on different user preferences and the lack of auto-correct in the tested keyboards.", "keywords": ["mixed reality", "typing performance", "tactile feedback", "eye-tracking", "human-computer interaction"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2507.21556", "pdf": "https://arxiv.org/pdf/2507.21556.pdf", "abs": "https://arxiv.org/abs/2507.21556", "title": "Evaluating the cognitive reality of Spanish irregular morphomic patterns: Humans vs. Transformers", "authors": ["Akhilesh Kakolu Ramarao", "Kevin Tang", "Dinah Baer-Henney"], "categories": ["cs.CL"], "comment": null, "summary": "This study investigates the cognitive plausibility of the Spanish irregular\nmorphomic pattern by directly comparing transformer-based neural networks to\nhuman behavioral data from \\citet{Nevins2015TheRA}. Using the same analytical\nframework as the original human study, we evaluate whether transformer models\ncan replicate human-like sensitivity to a complex linguistic phenomena, the\nmorphome, under controlled input conditions. Our experiments focus on three\nfrequency conditions: natural, low-frequency, and high-frequency distributions\nof verbs exhibiting irregular morphomic patterns. While the models outperformed\nhumans in stem and suffix accuracy, a clear divergence emerged in response\npreferences. Unlike humans, who consistently favored natural responses across\nall test items, models' preferred irregular responses and were influenced by\nthe proportion of irregular verbs in their training data. Additionally, models\ntrained on the natural and low-frequency distributions, but not the\nhigh-frequency distribution, were sensitive to the phonological similarity\nbetween test items and real Spanish L-shaped verbs.", "AI": {"tldr": "This study compares transformer-based neural networks to human behaviors regarding Spanish irregular morphomic patterns, revealing key differences in linguistic response preferences.", "motivation": "To explore how transformer models replicate human sensitivity to complex linguistic phenomena, specifically focusing on the Spanish morphomic pattern.", "method": "The study utilized the same analytical framework as an earlier human study, testing transformer models under controlled input conditions across three frequency distributions of irregular verb patterns.", "result": "Transformer models achieved higher accuracy in stem and suffix identification than humans but diverged in preference for natural versus irregular responses, which were influenced by their training data.", "conclusion": "Models showed some sensitivity to phonological similarities only in specific frequency conditions, indicating significant differences between model behavior and human linguistic processing.", "key_contributions": ["Direct comparison of transformer models and human behavior on linguistic tasks", "Identification of performance patterns across different frequency distributions", "Revealing discrepancies in response preferences between humans and models"], "limitations": "The study focuses solely on Spanish and specific neuro-linguistic tasks, which may limit the generalizability of findings.", "keywords": ["transformer models", "Spanish linguistics", "morphomic patterns", "neural networks", "human behavior"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.21568", "pdf": "https://arxiv.org/pdf/2507.21568.pdf", "abs": "https://arxiv.org/abs/2507.21568", "title": "Multi-Hypothesis Distillation of Multilingual Neural Translation Models for Low-Resource Languages", "authors": ["Aarón Galiano-Jiménez", "Juan Antonio Pérez-Ortiz", "Felipe Sánchez-Martínez", "Víctor M. Sánchez-Cartagena"], "categories": ["cs.CL"], "comment": "17 pages, 12 figures", "summary": "This paper explores sequence-level knowledge distillation (KD) of\nmultilingual pre-trained encoder-decoder translation models. We argue that the\nteacher model's output distribution holds valuable insights for the student,\nbeyond the approximated mode obtained through beam search (the standard\ndecoding method), and present Multi-Hypothesis Distillation (MHD), a\nsequence-level KD method that generates multiple translations for each source\nsentence. This provides a larger representation of the teacher model\ndistribution and exposes the student model to a wider range of target-side\nprefixes. We leverage $n$-best lists from beam search to guide the student's\nlearning and examine alternative decoding methods to address issues like low\nvariability and the under-representation of infrequent tokens. For low-resource\nlanguages, our research shows that while sampling methods may slightly\ncompromise translation quality compared to beam search based approaches, they\nenhance the generated corpora with greater variability and lexical richness.\nThis ultimately improves student model performance and mitigates the gender\nbias amplification often associated with KD.", "AI": {"tldr": "This paper presents Multi-Hypothesis Distillation, a sequence-level knowledge distillation method for multilingual translation models, which improves variability and performance of student models by exposing them to multiple translations from teacher models.", "motivation": "To improve knowledge distillation in multilingual translation models by leveraging the teacher model's output distribution beyond standard decoding methods.", "method": "The authors introduce Multi-Hypothesis Distillation (MHD), utilizing n-best lists from beam search to expose the student model to a wider array of translations and addressing variability issues.", "result": "The study finds that while sampling methods may compromise translation quality slightly, they notably enhance variability and lexical richness, leading to improved student model performance.", "conclusion": "The proposed MHD method not only enhances translation diversity but also helps mitigate gender bias amplification in knowledge distillation.", "key_contributions": ["Introduction of Multi-Hypothesis Distillation (MHD) for sequence-level knowledge distillation.", "Demonstration of improved lexical variability and richness in generated translations.", "Mitigation of gender bias amplification in student models through enhanced training techniques."], "limitations": "The potential compromise in translation quality when using sampling methods compared to beam search approaches.", "keywords": ["knowledge distillation", "multilingual translation", "sequence-level learning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.21609", "pdf": "https://arxiv.org/pdf/2507.21609.pdf", "abs": "https://arxiv.org/abs/2507.21609", "title": "Multilingual JobBERT for Cross-Lingual Job Title Matching", "authors": ["Jens-Joris Decorte", "Matthias De Lange", "Jeroen Van Hautte"], "categories": ["cs.CL"], "comment": "Accepted to the TalentCLEF 2025 Workshop as part of CLEF 2025", "summary": "We introduce JobBERT-V3, a contrastive learning-based model for cross-lingual\njob title matching. Building on the state-of-the-art monolingual JobBERT-V2,\nour approach extends support to English, German, Spanish, and Chinese by\nleveraging synthetic translations and a balanced multilingual dataset of over\n21 million job titles. The model retains the efficiency-focused architecture of\nits predecessor while enabling robust alignment across languages without\nrequiring task-specific supervision. Extensive evaluations on the TalentCLEF\n2025 benchmark demonstrate that JobBERT-V3 outperforms strong multilingual\nbaselines and achieves consistent performance across both monolingual and\ncross-lingual settings. While not the primary focus, we also show that the\nmodel can be effectively used to rank relevant skills for a given job title,\ndemonstrating its broader applicability in multilingual labor market\nintelligence. The model is publicly available:\nhttps://huggingface.co/TechWolf/JobBERT-v3.", "AI": {"tldr": "JobBERT-V3 is a cross-lingual job title matching model using contrastive learning, outperforming multilingual baselines with no task-specific supervision.", "motivation": "To improve job title matching across multiple languages by leveraging a large multilingual dataset and contrastive learning techniques.", "method": "The model is built on the architecture of JobBERT-V2 and incorporates synthetic translations across four languages with extensive evaluations on the TalentCLEF 2025 benchmark.", "result": "JobBERT-V3 outperforms established multilingual baselines and provides consistent performance for both monolingual and cross-lingual job title matching tasks.", "conclusion": "The model shows potential beyond its primary use by effectively ranking relevant skills for job titles, contributing to multilingual labor market intelligence.", "key_contributions": ["Introduction of JobBERT-V3 for cross-lingual job title matching", "Use of synthetic translations to enhance multilingual support", "Demonstrated broader applicability in ranking relevant skills for job titles"], "limitations": "", "keywords": ["cross-lingual", "job title matching", "contrastive learning", "multilingual", "JobBERT-V3"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.21645", "pdf": "https://arxiv.org/pdf/2507.21645.pdf", "abs": "https://arxiv.org/abs/2507.21645", "title": "Libra: Assessing and Improving Reward Model by Learning to Think", "authors": ["Meng Zhou", "Bei Li", "Jiahao Liu", "Xiaowen Shi", "Yang Bai", "Rongxiang Weng", "Jingang Wang", "Xunliang Cai"], "categories": ["cs.CL"], "comment": "Work In Progress", "summary": "Reinforcement learning (RL) has significantly improved the reasoning ability\nof large language models. However, current reward models underperform in\nchallenging reasoning scenarios and predominant RL training paradigms rely on\nrule-based or reference-based rewards, which impose two critical limitations:\n1) the dependence on finely annotated reference answer to attain rewards; and\n2) the requirement for constrained output format. These limitations\nfundamentally hinder further RL data scaling and sustained enhancement of model\nreasoning performance. To address these limitations, we propose a comprehensive\nframework for evaluating and improving the performance of reward models in\ncomplex reasoning scenarios. We first present a reasoning-oriented benchmark\n(Libra Bench), systematically constructed from a diverse collection of\nchallenging mathematical problems and advanced reasoning models, to address the\nlimitations of existing reward model benchmarks in reasoning scenarios. We\nfurther introduce a novel approach for improving the generative reward model\nvia learning-to-think methodologies. Based on the proposed approach, we develop\nLibra-RM series, a collection of generative reward models with reasoning\ncapabilities that achieve state-of-the-art results on various benchmarks.\nComprehensive downstream experiments are conducted and the experimental results\ndemonstrate the correlation between our Libra Bench and downstream application,\nand the potential of Libra-RM to further improve reasoning models with\nunlabeled data.", "AI": {"tldr": "This paper proposes a framework, including a new benchmark and generative reward models, to enhance reasoning abilities in large language models.", "motivation": "Current reinforcement learning reward models fail in complex reasoning scenarios due to their reliance on annotated references and constrained output formats.", "method": "The authors introduce Libra Bench, a reasoning-oriented benchmark for evaluating reward models, and develop Libra-RM series generative reward models using learning-to-think methodologies.", "result": "Libra-RM models achieve state-of-the-art results on various benchmarks, showing improved reasoning capabilities, especially with unlabeled data.", "conclusion": "The findings suggest that the proposed framework can overcome existing limitations in reward modeling and has potential for improving reasoning models in practical applications.", "key_contributions": ["Introduction of Libra Bench as a comprehensive reasoning benchmark.", "Development of Libra-RM generative reward models with improved reasoning capabilities.", "Demonstration of enhanced performance in downstream applications using the proposed framework."], "limitations": "", "keywords": ["Reinforcement Learning", "Language Models", "Reasoning", "Benchmarking", "Generative Models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.21652", "pdf": "https://arxiv.org/pdf/2507.21652.pdf", "abs": "https://arxiv.org/abs/2507.21652", "title": "UnsafeChain: Enhancing Reasoning Model Safety via Hard Cases", "authors": ["Raj Vardhan Tomar", "Preslav Nakov", "Yuxia Wang"], "categories": ["cs.CL"], "comment": null, "summary": "As large reasoning models (LRMs) grow more capable, chain-of-thought (CoT)\nreasoning introduces new safety challenges. Existing SFT-based safety alignment\nstudies dominantly focused on filtering prompts with safe, high-quality\nresponses, while overlooking hard prompts that always elicit harmful outputs.\nTo fill this gap, we introduce UnsafeChain, a safety alignment dataset\nconstructed from hard prompts with diverse sources, where unsafe completions\nare identified and explicitly corrected into safe responses. By exposing models\nto unsafe behaviors and guiding their correction, UnsafeChain enhances safety\nwhile preserving general reasoning ability. We fine-tune three LRMs on\nUnsafeChain and compare them against recent SafeChain and STAR-1 across six\nout-of-distribution and five in-distribution benchmarks. UnsafeChain\nconsistently outperforms prior datasets, with even a 1K subset matching or\nsurpassing baseline performance, demonstrating the effectiveness and\ngeneralizability of correction-based supervision. We release our dataset and\ncode at https://github.com/mbzuai-nlp/UnsafeChain", "AI": {"tldr": "Introducing UnsafeChain, a safety alignment dataset to improve LRM safety with hard prompts and correction-based supervision.", "motivation": "To address safety challenges in large reasoning models (LRMs) by exposing and correcting unsafe behaviors in prompts.", "method": "UnsafeChain is constructed from hard prompts where unsafe completions are identified and corrected into safe responses. Three LRMs were fine-tuned on this dataset and tested against prior benchmarks.", "result": "UnsafeChain outperformed prior datasets including SafeChain and STAR-1, demonstrating improved safety and reasoning ability across various benchmarks.", "conclusion": "Correction-based supervision using UnsafeChain is effective in enhancing model safety without compromising general reasoning ability.", "key_contributions": ["Introduction of UnsafeChain dataset for safety alignment of LRMs", "Demonstrated superiority of UnsafeChain over previous safety datasets", "Enhanced model performance via correction-based supervision"], "limitations": "", "keywords": ["Large Reasoning Models", "Safety Alignment", "Chain-of-Thought Reasoning", "Machine Learning", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.21750", "pdf": "https://arxiv.org/pdf/2507.21750.pdf", "abs": "https://arxiv.org/abs/2507.21750", "title": "Adversarial Defence without Adversarial Defence: Enhancing Language Model Robustness via Instance-level Principal Component Removal", "authors": ["Yang Wang", "Chenghao Xiao", "Yizhi Li", "Stuart E. Middleton", "Noura Al Moubayed", "Chenghua Lin"], "categories": ["cs.CL"], "comment": "This paper was accepted with an A-decision to Transactions of the\n  Association for Computational Linguistics. This version is the\n  pre-publication version prior to MIT Press production", "summary": "Pre-trained language models (PLMs) have driven substantial progress in\nnatural language processing but remain vulnerable to adversarial attacks,\nraising concerns about their robustness in real-world applications. Previous\nstudies have sought to mitigate the impact of adversarial attacks by\nintroducing adversarial perturbations into the training process, either\nimplicitly or explicitly. While both strategies enhance robustness, they often\nincur high computational costs. In this work, we propose a simple yet effective\nadd-on module that enhances the adversarial robustness of PLMs by removing\ninstance-level principal components, without relying on conventional\nadversarial defences or perturbing the original training data. Our approach\ntransforms the embedding space to approximate Gaussian properties, thereby\nreducing its susceptibility to adversarial perturbations while preserving\nsemantic relationships. This transformation aligns embedding distributions in a\nway that minimises the impact of adversarial noise on decision boundaries,\nenhancing robustness without requiring adversarial examples or costly\ntraining-time augmentation. Evaluations on eight benchmark datasets show that\nour approach improves adversarial robustness while maintaining comparable\nbefore-attack accuracy to baselines, achieving a balanced trade-off between\nrobustness and generalisation.", "AI": {"tldr": "This paper introduces a novel module that enhances the adversarial robustness of pre-trained language models without traditional adversarial defenses or high computational costs.", "motivation": "The increasing vulnerability of pre-trained language models (PLMs) to adversarial attacks raises concerns about their applicability in real-world scenarios, necessitating effective and efficient robustness strategies.", "method": "The proposed method removes instance-level principal components to transform the embedding space towards Gaussian properties, which minimizes the impact of adversarial noise on decision boundaries without needing adversarial examples or costly training augmentations.", "result": "The approach was evaluated on eight benchmark datasets, demonstrating improved adversarial robustness while maintaining accuracy comparable to existing baselines.", "conclusion": "This method strikes a balance between adversarial robustness and generalization, offering a simpler alternative to existing adversarial training techniques.", "key_contributions": ["Introduction of a novel module for enhancing adversarial robustness in PLMs", "Transformation of the embedding space to approximate Gaussian characteristics", "Demonstration of improved robustness with minimal impact on accuracy"], "limitations": "", "keywords": ["Adversarial robustness", "Pre-trained language models", "Natural language processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.21773", "pdf": "https://arxiv.org/pdf/2507.21773.pdf", "abs": "https://arxiv.org/abs/2507.21773", "title": "AgriEval: A Comprehensive Chinese Agricultural Benchmark for Large Language Models", "authors": ["Lian Yan", "Haotian Wang", "Chen Tang", "Haifeng Liu", "Tianyang Sun", "Liangliang Liu", "Yi Guan", "Jingchi Jiang"], "categories": ["cs.CL"], "comment": "36 pages, 22 figures", "summary": "In the agricultural domain, the deployment of large language models (LLMs) is\nhindered by the lack of training data and evaluation benchmarks. To mitigate\nthis issue, we propose AgriEval, the first comprehensive Chinese agricultural\nbenchmark with three main characteristics: (1) Comprehensive Capability\nEvaluation. AgriEval covers six major agriculture categories and 29\nsubcategories within agriculture, addressing four core cognitive scenarios:\nmemorization, understanding, inference, and generation. (2) High-Quality Data.\nThe dataset is curated from university-level examinations and assignments,\nproviding a natural and robust benchmark for assessing the capacity of LLMs to\napply knowledge and make expert-like decisions. (3) Diverse Formats and\nExtensive Scale. AgriEval comprises 14,697 multiple-choice questions and 2,167\nopen-ended question-and-answer questions, establishing it as the most extensive\nagricultural benchmark available to date. We also present comprehensive\nexperimental results over 51 open-source and commercial LLMs. The experimental\nresults reveal that most existing LLMs struggle to achieve 60% accuracy,\nunderscoring the developmental potential in agricultural LLMs. Additionally, we\nconduct extensive experiments to investigate factors influencing model\nperformance and propose strategies for enhancement. AgriEval is available at\nhttps://github.com/YanPioneer/AgriEval/.", "AI": {"tldr": "AgriEval is the first comprehensive Chinese agricultural benchmark designed to evaluate the performance of large language models (LLMs) in agriculture. It includes diverse questions and high-quality data from educational materials.", "motivation": "The paper addresses the challenge of limited training data and evaluation benchmarks for LLMs in agriculture, aimed at fostering improvements in their capabilities.", "method": "AgriEval consists of 14,697 multiple-choice questions and 2,167 open-ended questions spanning six major agriculture categories. Experimental results were conducted over 51 LLMs to gauge their performance.", "result": "Most existing LLMs showcased struggles, achieving less than 60% accuracy, indicating a significant developmental opportunity for agricultural applications of LLMs.", "conclusion": "The establishment of AgriEval can encourage advancements in LLM performance in agriculture and assists in identifying strategies for model enhancement.", "key_contributions": ["Introduction of AgriEval as a benchmark for agricultural LLMs", "Comprehensive dataset from educational sources covering a wide range of agricultural topics", "Experimental evaluation of 51 LLMs highlighting areas for improvement"], "limitations": "", "keywords": ["agriculture", "language models", "benchmarking", "evaluation", "natural language processing"], "importance_score": 4, "read_time_minutes": 20}}
{"id": "2507.21782", "pdf": "https://arxiv.org/pdf/2507.21782.pdf", "abs": "https://arxiv.org/abs/2507.21782", "title": "The Problem with Safety Classification is not just the Models", "authors": ["Sowmya Vajjala"], "categories": ["cs.CL"], "comment": "Pre-print, Short paper", "summary": "Studying the robustness of Large Language Models (LLMs) to unsafe behaviors\nis an important topic of research today. Building safety classification models\nor guard models, which are fine-tuned models for input/output safety\nclassification for LLMs, is seen as one of the solutions to address the issue.\nAlthough there is a lot of research on the safety testing of LLMs themselves,\nthere is little research on evaluating the effectiveness of such safety\nclassifiers or the evaluation datasets used for testing them, especially in\nmultilingual scenarios. In this position paper, we demonstrate how multilingual\ndisparities exist in 5 safety classification models by considering datasets\ncovering 18 languages. At the same time, we identify potential issues with the\nevaluation datasets, arguing that the shortcomings of current safety\nclassifiers are not only because of the models themselves. We expect that these\nfindings will contribute to the discussion on developing better methods to\nidentify harmful content in LLM inputs across languages.", "AI": {"tldr": "This paper discusses the effectiveness of safety classification models for Large Language Models (LLMs), particularly in multilingual contexts, highlighting disparities in safety evaluation.", "motivation": "To address the issue of unsafe behaviors in Large Language Models (LLMs) through improved safety classification models and evaluation datasets, especially in multilingual scenarios.", "method": "The study analyzes 5 safety classification models and their performance across evaluation datasets spanning 18 different languages.", "result": "The authors identify significant multilingual disparities in safety classifiers and raise concerns about the efficacy of current evaluation datasets used in testing these models.", "conclusion": "Findings suggest that issues with safety classifiers are not solely model-related, but also stem from inadequacies in the evaluation datasets.", "key_contributions": ["Identification of multilingual disparities in LLM safety classification models.", "Critique of the evaluation datasets used for testing safety classifiers in diverse languages.", "Contribution to discussions on improving harm detection methods in multilingual LLM inputs."], "limitations": "Limited exploration of potential mitigation strategies for identified issues.", "keywords": ["Large Language Models", "safety classification", "multilingual disparities", "evaluation datasets", "harmful content detection"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.21810", "pdf": "https://arxiv.org/pdf/2507.21810.pdf", "abs": "https://arxiv.org/abs/2507.21810", "title": "ChartMark: A Structured Grammar for Chart Annotation", "authors": ["Yiyu Chen", "Yifan Wu", "Shuyu Shen", "Yupeng Xie", "Leixian Shen", "Hui Xiong", "Yuyu Luo"], "categories": ["cs.CL", "cs.SE"], "comment": "IEEE VIS 2025", "summary": "Chart annotations enhance visualization accessibility but suffer from\nfragmented, non-standardized representations that limit cross-platform reuse.\nWe propose ChartMark, a structured grammar that separates annotation semantics\nfrom visualization implementations. ChartMark features a hierarchical framework\nmapping onto annotation dimensions (e.g., task, chart context), supporting both\nabstract intents and precise visual details. Our toolkit demonstrates\nconverting ChartMark specifications into Vega-Lite visualizations, highlighting\nits flexibility, expressiveness, and practical applicability.", "AI": {"tldr": "ChartMark is a structured grammar for standardized chart annotations that enhances visualization accessibility and cross-platform reuse.", "motivation": "To address fragmented and non-standardized representations of chart annotations which limit accessibility and interoperability across different platforms.", "method": "ChartMark is developed as a hierarchical framework that separates annotation semantics from visualization implementations, mapping onto various annotation dimensions.", "result": "The toolkit successfully converts ChartMark specifications into Vega-Lite visualizations, demonstrating its flexibility and expressiveness.", "conclusion": "ChartMark improves the accessibility of visualizations by providing a structured approach to annotations that can be uniformly applied across platforms.", "key_contributions": ["Structured grammar for chart annotations", "Hierarchical framework for annotation dimensions", "Toolkit for converting specifications to Vega-Lite visualizations"], "limitations": "", "keywords": ["chart annotations", "visualization accessibility", "Vega-Lite"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2507.21813", "pdf": "https://arxiv.org/pdf/2507.21813.pdf", "abs": "https://arxiv.org/abs/2507.21813", "title": "Overview of ADoBo at IberLEF 2025: Automatic Detection of Anglicisms in Spanish", "authors": ["Elena Alvarez-Mellado", "Jordi Porta-Zamorano", "Constantine Lignos", "Julio Gonzalo"], "categories": ["cs.CL"], "comment": "Accepted in the journal Procesamiento del Lenguaje Natural 75", "summary": "This paper summarizes the main findings of ADoBo 2025, the shared task on\nanglicism identification in Spanish proposed in the context of IberLEF 2025.\nParticipants of ADoBo 2025 were asked to detect English lexical borrowings (or\nanglicisms) from a collection of Spanish journalistic texts. Five teams\nsubmitted their solutions for the test phase. Proposed systems included LLMs,\ndeep learning models, Transformer-based models and rule-based systems. The\nresults range from F1 scores of 0.17 to 0.99, which showcases the variability\nin performance different systems can have for this task.", "AI": {"tldr": "This paper discusses the ADoBo 2025 task on identifying anglicisms in Spanish, highlighting various approaches and their performance.", "motivation": "The task aims to explore the challenges of detecting English lexical borrowings in Spanish language contexts, relevant for understanding linguistic dynamics in bilingual settings.", "method": "Participants used a range of approaches including LLMs, deep learning models, Transformer-based models, and rule-based systems to identify anglicisms in Spanish journalistic texts.", "result": "The systems demonstrated a wide performance range with F1 scores between 0.17 and 0.99, illustrating the challenges and effectiveness of different methodologies.", "conclusion": "The findings from ADoBo 2025 provide insights into the effectiveness of various techniques for anglicism detection, revealing the potential of advanced NLP tools in linguistic tasks.", "key_contributions": ["Identification of English loanwords in Spanish", "Evaluation of multiple AI methodologies for NLP tasks", "Performance metrics highlighting variability across systems"], "limitations": "Limited to Spanish texts and specific linguistic features; may not generalize to other languages or contexts.", "keywords": ["anglicisms", "NLP", "Spanish", "language processing", "machine learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.21815", "pdf": "https://arxiv.org/pdf/2507.21815.pdf", "abs": "https://arxiv.org/abs/2507.21815", "title": "HRIPBench: Benchmarking LLMs in Harm Reduction Information Provision to Support People Who Use Drugs", "authors": ["Kaixuan Wang", "Chenxin Diao", "Jason T. Jacques", "Zhongliang Guo", "Shuai Zhao"], "categories": ["cs.CL", "cs.CY"], "comment": "15 pages, 5 figures, 12 tables, a dataset", "summary": "Millions of individuals' well-being are challenged by the harms of substance\nuse. Harm reduction as a public health strategy is designed to improve their\nhealth outcomes and reduce safety risks. Some large language models (LLMs) have\ndemonstrated a decent level of medical knowledge, promising to address the\ninformation needs of people who use drugs (PWUD). However, their performance in\nrelevant tasks remains largely unexplored. We introduce HRIPBench, a benchmark\ndesigned to evaluate LLM's accuracy and safety risks in harm reduction\ninformation provision. The benchmark dataset HRIP-Basic has 2,160\nquestion-answer-evidence pairs. The scope covers three tasks: checking safety\nboundaries, providing quantitative values, and inferring polysubstance use\nrisks. We build the Instruction and RAG schemes to evaluate model behaviours\nbased on their inherent knowledge and the integration of domain knowledge. Our\nresults indicate that state-of-the-art LLMs still struggle to provide accurate\nharm reduction information, and sometimes, carry out severe safety risks to\nPWUD. The use of LLMs in harm reduction contexts should be cautiously\nconstrained to avoid inducing negative health outcomes. WARNING: This paper\ncontains illicit content that potentially induces harms.", "AI": {"tldr": "The paper introduces HRIPBench, a benchmark for assessing the accuracy and safety of LLMs in providing harm reduction information for substance use, revealing significant challenges in their performance.", "motivation": "The study targets the urgent need for reliable information sources for people who use drugs (PWUD) to enhance health outcomes and minimize risks associated with substance use.", "method": "The authors developed HRIPBench, a benchmark comprising 2,160 question-answer-evidence pairs, categorizing the evaluation into tasks such as checking safety boundaries, assessing quantitative values, and inferring polysubstance use risks with LLMs.", "result": "The findings show that state-of-the-art LLMs frequently fail to deliver precise harm reduction information and may pose severe safety risks to PWUD.", "conclusion": "The deployment of LLMs in harm reduction should be approached with caution to prevent adverse health impacts among users.", "key_contributions": ["Introduction of HRIPBench as a new benchmark for evaluating LLMs in harm reduction contexts", "Creation of a dataset (HRIP-Basic) with diverse question-answer-evidence pairs", "Identification of significant performance gaps in LLMs when used in substance use scenarios."], "limitations": "The potential risk associated with the misuse of LLMs in harm reduction contexts and the focus on illicit content may limit applicability.", "keywords": ["Harm reduction", "Large language models", "Substance use", "Benchmarking", "Public health"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.21828", "pdf": "https://arxiv.org/pdf/2507.21828.pdf", "abs": "https://arxiv.org/abs/2507.21828", "title": "Modelling Adjectival Modification Effects on Semantic Plausibility", "authors": ["Anna Golub", "Beate Zywietz", "Annerose Eichel"], "categories": ["cs.CL"], "comment": "Accepted at ESSLLI 2025 Student Session", "summary": "While the task of assessing the plausibility of events such as ''news is\nrelevant'' has been addressed by a growing body of work, less attention has\nbeen paid to capturing changes in plausibility as triggered by event\nmodification. Understanding changes in plausibility is relevant for tasks such\nas dialogue generation, commonsense reasoning, and hallucination detection as\nit allows to correctly model, for example, ''gentle sarcasm'' as a sign of\ncloseness rather than unkindness among friends [9]. In this work, we tackle the\nADEPT challenge benchmark [6] consisting of 16K English sentence pairs\ndiffering by exactly one adjectival modifier. Our modeling experiments provide\na conceptually novel method by using sentence transformers, and reveal that\nboth they and transformer-based models struggle with the task at hand, and\nsentence transformers - despite their conceptual alignment with the task - even\nunder-perform in comparison to models like RoBERTa. Furthermore, an in-depth\ncomparison with prior work highlights the importance of a more realistic,\nbalanced evaluation method: imbalances distort model performance and evaluation\nmetrics, and weaken result trustworthiness.", "AI": {"tldr": "This paper investigates how changes in event plausibility, due to adjectival modifiers, affect models used for dialogue generation and commonsense reasoning, finding that current transformer models struggle with these nuances.", "motivation": "The study addresses the gap in research regarding how modifications to events affect their plausibility, which is essential for applications like dialogue generation and commonsense reasoning.", "method": "The authors conducted modeling experiments using sentence transformers on the ADEPT challenge benchmark, consisting of 16K English sentence pairs that differ by one adjectival modifier.", "result": "The experiments revealed that sentence transformer models underperform compared to models like RoBERTa and emphasized the need for balanced evaluation methods to avoid distorted evaluations.", "conclusion": "The findings indicate that transformer models struggle with the assessment of event plausibility influenced by adjectival modifications, highlighting the importance of realistic evaluation metrics.", "key_contributions": ["Introduction of a novel approach using sentence transformers for plausibility assessment", "Demonstration of the limitations of current models in handling subtle linguistic modifications", "Emphasis on the need for balanced evaluation methodologies in model performance assessment"], "limitations": "Models struggle with nuanced changes in event plausibility and results are affected by evaluation metric imbalances.", "keywords": ["plausibility assessment", "sentence transformers", "dialogue generation", "commonsense reasoning", "evaluation metrics"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.21831", "pdf": "https://arxiv.org/pdf/2507.21831.pdf", "abs": "https://arxiv.org/abs/2507.21831", "title": "Introducing HALC: A general pipeline for finding optimal prompting strategies for automated coding with LLMs in the computational social sciences", "authors": ["Andreas Reich", "Claudia Thoms", "Tobias Schrimpf"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "48 pages, 9 figures and 8 tables", "summary": "LLMs are seeing widespread use for task automation, including automated\ncoding in the social sciences. However, even though researchers have proposed\ndifferent prompting strategies, their effectiveness varies across LLMs and\ntasks. Often trial and error practices are still widespread. We propose\nHALC$-$a general pipeline that allows for the systematic and reliable\nconstruction of optimal prompts for any given coding task and model, permitting\nthe integration of any prompting strategy deemed relevant. To investigate LLM\ncoding and validate our pipeline, we sent a total of 1,512 individual prompts\nto our local LLMs in over two million requests. We test prompting strategies\nand LLM task performance based on few expert codings (ground truth). When\ncompared to these expert codings, we find prompts that code reliably for single\nvariables (${\\alpha}$climate = .76; ${\\alpha}$movement = .78) and across two\nvariables (${\\alpha}$climate = .71; ${\\alpha}$movement = .74) using the LLM\nMistral NeMo. Our prompting strategies are set up in a way that aligns the LLM\nto our codebook$-$we are not optimizing our codebook for LLM friendliness. Our\npaper provides insights into the effectiveness of different prompting\nstrategies, crucial influencing factors, and the identification of reliable\nprompts for each coding task and model.", "AI": {"tldr": "The paper proposes HALC, a systematic pipeline for constructing optimal prompts for LLM coding tasks, validated through extensive testing of prompting strategies.", "motivation": "The need for a reliable method to construct effective prompts for LLMs in automated coding tasks in the social sciences.", "method": "A general pipeline (HALC) was developed, and its effectiveness was validated by sending 1,512 prompts to local LLMs, assessing performance against expert codings.", "result": "The paper found that certain prompts code reliably for single and multiple variables with performance measures indicating moderate reliability.", "conclusion": "The authors provide insights into effective prompting strategies and highlight the importance of aligning prompts with their codebook for optimal LLM performance.", "key_contributions": ["Introduction of HALC for prompt construction", "Validation of prompting strategies through extensive testing", "Identification of reliable prompts for LLM coding tasks"], "limitations": "The study focuses on a specific LLM (Mistral NeMo) and may not generalize to all models or tasks.", "keywords": ["Human-Computer Interaction", "Large Language Models", "Prompt Engineering", "Social Sciences", "Task Automation"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2507.21836", "pdf": "https://arxiv.org/pdf/2507.21836.pdf", "abs": "https://arxiv.org/abs/2507.21836", "title": "AutoTIR: Autonomous Tools Integrated Reasoning via Reinforcement Learning", "authors": ["Yifan Wei", "Xiaoyan Yu", "Yixuan Weng", "Tengfei Pan", "Angsheng Li", "Li Du"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs), when enhanced through reasoning-oriented\npost-training, evolve into powerful Large Reasoning Models (LRMs).\nTool-Integrated Reasoning (TIR) further extends their capabilities by\nincorporating external tools, but existing methods often rely on rigid,\npredefined tool-use patterns that risk degrading core language competence.\nInspired by the human ability to adaptively select tools, we introduce AutoTIR,\na reinforcement learning framework that enables LLMs to autonomously decide\nwhether and which tool to invoke during the reasoning process, rather than\nfollowing static tool-use strategies. AutoTIR leverages a hybrid reward\nmechanism that jointly optimizes for task-specific answer correctness,\nstructured output adherence, and penalization of incorrect tool usage, thereby\nencouraging both precise reasoning and efficient tool integration. Extensive\nevaluations across diverse knowledge-intensive, mathematical, and general\nlanguage modeling tasks demonstrate that AutoTIR achieves superior overall\nperformance, significantly outperforming baselines and exhibits superior\ngeneralization in tool-use behavior. These results highlight the promise of\nreinforcement learning in building truly generalizable and scalable TIR\ncapabilities in LLMs. The code and data are available at\nhttps://github.com/weiyifan1023/AutoTIR.", "AI": {"tldr": "AutoTIR is a reinforcement learning framework that enhances Large Language Models by allowing them to autonomously select and utilize external tools for reasoning, improving task performance and tool integration.", "motivation": "To overcome the limitations of existing static tool-use methods in Large Language Models and improve their reasoning capabilities through adaptive tool selection.", "method": "AutoTIR employs a hybrid reinforcement learning approach to determine when and which external tools to invoke, optimizing for output correctness and efficient tool usage.", "result": "AutoTIR demonstrates superior performance across various tasks compared to baseline models, showing enhanced generalization in tool-use behavior.", "conclusion": "The study emphasizes the effectiveness of reinforcement learning in developing adaptable tool integration capabilities, promising better scalability and performance of reasoning in LLMs.", "key_contributions": ["Introduction of AutoTIR framework for adaptive tool selection in LLMs", "Hybrid reward mechanism that balances output correctness and tool usage", "Demonstration of improved performance over static tool-use methods"], "limitations": "", "keywords": ["Large Language Models", "Reinforcement Learning", "Tool-Integrated Reasoning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.21892", "pdf": "https://arxiv.org/pdf/2507.21892.pdf", "abs": "https://arxiv.org/abs/2507.21892", "title": "Graph-R1: Towards Agentic GraphRAG Framework via End-to-end Reinforcement Learning", "authors": ["Haoran Luo", "Haihong E", "Guanting Chen", "Qika Lin", "Yikai Guo", "Fangzhi Xu", "Zemin Kuang", "Meina Song", "Xiaobao Wu", "Yifan Zhu", "Luu Anh Tuan"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Retrieval-Augmented Generation (RAG) mitigates hallucination in LLMs by\nincorporating external knowledge, but relies on chunk-based retrieval that\nlacks structural semantics. GraphRAG methods improve RAG by modeling knowledge\nas entity-relation graphs, but still face challenges in high construction cost,\nfixed one-time retrieval, and reliance on long-context reasoning and prompt\ndesign. To address these challenges, we propose Graph-R1, an agentic GraphRAG\nframework via end-to-end reinforcement learning (RL). It introduces lightweight\nknowledge hypergraph construction, models retrieval as a multi-turn\nagent-environment interaction, and optimizes the agent process via an\nend-to-end reward mechanism. Experiments on standard RAG datasets show that\nGraph-R1 outperforms traditional GraphRAG and RL-enhanced RAG methods in\nreasoning accuracy, retrieval efficiency, and generation quality.", "AI": {"tldr": "Graph-R1 is a novel agentic GraphRAG framework that improves retrieval-augmented generation (RAG) using end-to-end reinforcement learning to enhance reasoning and retrieval efficiency.", "motivation": "To address the limitations of existing GraphRAG methods, such as high construction costs and fixed one-time retrieval, while improving reasoning accuracy and generation quality in LLMs.", "method": "The proposed Graph-R1 framework utilizes lightweight knowledge hypergraph construction and models retrieval as a multi-turn agent-environment interaction, optimizing the process through an end-to-end reinforcement learning reward mechanism.", "result": "Graph-R1 outperforms traditional GraphRAG and RL-enhanced RAG methods in reasoning accuracy, retrieval efficiency, and generation quality based on experimental evaluations on standard RAG datasets.", "conclusion": "The introduction of Graph-R1 significantly enhances the capabilities of retrieval-augmented generation by addressing key challenges inherent to previous methods.", "key_contributions": ["Development of an agentic GraphRAG framework", "Utilization of lightweight hypergraph construction", "Integration of multi-turn agent-environment interactions with RL optimization"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "GraphRAG", "Reinforcement Learning", "Knowledge Graphs", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.21914", "pdf": "https://arxiv.org/pdf/2507.21914.pdf", "abs": "https://arxiv.org/abs/2507.21914", "title": "Rote Learning Considered Useful: Generalizing over Memorized Data in LLMs", "authors": ["Qinyuan Wu", "Soumi Das", "Mahsa Amani", "Bishwamittra Ghosh", "Mohammad Aflah Khan", "Krishna P. Gummadi", "Muhammad Bilal Zafar"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Rote learning is a memorization technique based on repetition. It is commonly\nbelieved to hinder generalization by encouraging verbatim memorization rather\nthan deeper understanding. This insight holds for even learning factual\nknowledge that inevitably requires a certain degree of memorization. In this\nwork, we demonstrate that LLMs can be trained to generalize from rote memorized\ndata. We introduce a two-phase memorize-then-generalize framework, where the\nmodel first rote memorizes factual subject-object associations using a\nsemantically meaningless token and then learns to generalize by fine-tuning on\na small set of semantically meaningful prompts. Extensive experiments over 8\nLLMs show that the models can reinterpret rote memorized data through the\nsemantically meaningful prompts, as evidenced by the emergence of structured,\nsemantically aligned latent representations between the two. This surprising\nfinding opens the door to both effective and efficient knowledge injection and\npossible risks of repurposing the memorized data for malicious usage.", "AI": {"tldr": "This paper demonstrates that LLMs can effectively generalize from rote memorized data using a two-phase memorization and fine-tuning approach.", "motivation": "To explore the potential of LLMs for generalization from rote memorization, challenging the belief that such techniques hinder deeper understanding.", "method": "A two-phase framework where LLMs memorize factual data with a meaningless token and then fine-tune using semantically meaningful prompts.", "result": "Experiments showed that LLMs can reinterpret rote memorized data through meaningful prompts, producing structured and semantically aligned representations.", "conclusion": "The findings suggest new possibilities for knowledge injection in LLMs and raise concerns about the misuse of memorized data.", "key_contributions": ["Introduction of a two-phase memorize-then-generalize framework", "Demonstration of LLMs reinterpreting memorized data", "Insights into knowledge injection and risks of misuse"], "limitations": "Further research needed to explore the implications of repurposing memorized data and generalization effectiveness across different contexts.", "keywords": ["Rote learning", "LLMs", "Generalization", "Knowledge Injection", "Fine-tuning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.21919", "pdf": "https://arxiv.org/pdf/2507.21919.pdf", "abs": "https://arxiv.org/abs/2507.21919", "title": "Training language models to be warm and empathetic makes them less reliable and more sycophantic", "authors": ["Lujain Ibrahim", "Franziska Sofia Hafner", "Luc Rocher"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Artificial intelligence (AI) developers are increasingly building language\nmodels with warm and empathetic personas that millions of people now use for\nadvice, therapy, and companionship. Here, we show how this creates a\nsignificant trade-off: optimizing language models for warmth undermines their\nreliability, especially when users express vulnerability. We conducted\ncontrolled experiments on five language models of varying sizes and\narchitectures, training them to produce warmer, more empathetic responses, then\nevaluating them on safety-critical tasks. Warm models showed substantially\nhigher error rates (+10 to +30 percentage points) than their original\ncounterparts, promoting conspiracy theories, providing incorrect factual\ninformation, and offering problematic medical advice. They were also\nsignificantly more likely to validate incorrect user beliefs, particularly when\nuser messages expressed sadness. Importantly, these effects were consistent\nacross different model architectures, and occurred despite preserved\nperformance on standard benchmarks, revealing systematic risks that current\nevaluation practices may fail to detect. As human-like AI systems are deployed\nat an unprecedented scale, our findings indicate a need to rethink how we\ndevelop and oversee these systems that are reshaping human relationships and\nsocial interaction.", "AI": {"tldr": "The paper investigates the trade-off between warmth and reliability in language models, demonstrating that more empathetic responses lead to increased error rates and unsafe outputs.", "motivation": "To explore the impact of empathetic language model designs on their reliability and safety in critical tasks.", "method": "Controlled experiments on five language models were conducted to compare their warm, empathetic responses to standard outputs, evaluating them on safety-critical tasks.", "result": "Warm language models showed a 10 to 30 percentage points increase in error rates, including the promotion of conspiracy theories and incorrect medical advice, particularly when responding to vulnerable users.", "conclusion": "The findings suggest a significant need to reevaluate how we design and evaluate empathetic AI systems to mitigate risks in user interactions and ensure reliability.", "key_contributions": ["Demonstrates a trade-off between model warmth and reliability", "Highlights significant error rates in empathetic models", "Calls for changes in AI development practices to ensure safety"], "limitations": "The study focuses on controlled experiments, which may not fully capture real-world complexities.", "keywords": ["language models", "empathy", "reliability", "AI safety", "user vulnerability"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.21931", "pdf": "https://arxiv.org/pdf/2507.21931.pdf", "abs": "https://arxiv.org/abs/2507.21931", "title": "Post-Training Large Language Models via Reinforcement Learning from Self-Feedback", "authors": ["Carel van Niekerk", "Renato Vukovic", "Benjamin Matthias Ruppik", "Hsien-chin Lin", "Milica Gašić"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often produce plausible but poorly-calibrated\nanswers, limiting their reliability on reasoning-intensive tasks. We present\nReinforcement Learning from Self-Feedback (RLSF), a post-training stage that\nuses the model's own confidence as an intrinsic reward, mimicking how humans\nlearn in the absence of external feedback. After a frozen LLM generates several\nchain-of-thought solutions, we define and compute the confidence of each final\nanswer span and rank the traces accordingly. These synthetic preferences are\nthen used to fine-tune the policy with standard preference optimization,\nsimilar to RLHF yet requiring no human labels, gold answers, or externally\ncurated rewards.\n  RLSF simultaneously (i) refines the model's probability estimates --\nrestoring well-behaved calibration -- and (ii) strengthens step-by-step\nreasoning, yielding improved performance on arithmetic reasoning and\nmultiple-choice question answering.\n  By turning a model's own uncertainty into useful self-feedback, RLSF affirms\nreinforcement learning on intrinsic model behaviour as a principled and\ndata-efficient component of the LLM post-training pipeline and warrents further\nresearch in intrinsic rewards for LLM post-training.", "AI": {"tldr": "Introducing RLSF, a method that enhances LLM performance by using self-feedback from the model's confidence to improve reasoning and calibration without external inputs.", "motivation": "LLMs produce unreliable answers in reasoning tasks due to poor calibration, necessitating methods for improvement without relying on human feedback.", "method": "RLSF utilizes a frozen LLM to generate solutions and evaluate their confidence, employing these evaluated preferences to fine-tune the model in a reinforcement learning-like manner.", "result": "RLSF improves model calibration and strengthens reasoning capabilities, leading to better performance in arithmetic reasoning and question answering.", "conclusion": "RLSF demonstrates that self-feedback can be effectively used in LLM post-training, proposing an area for future research on intrinsic rewards.", "key_contributions": ["Introduces a novel RLSF approach for LLM calibration.", "Demonstrates improved reasoning through self-feedback.", "Shows potential for data-efficient post-training methods."], "limitations": "", "keywords": ["Large Language Models", "Reinforcement Learning", "Self-Feedback", "Calibration", "Reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.21934", "pdf": "https://arxiv.org/pdf/2507.21934.pdf", "abs": "https://arxiv.org/abs/2507.21934", "title": "Culinary Crossroads: A RAG Framework for Enhancing Diversity in Cross-Cultural Recipe Adaptation", "authors": ["Tianyi Hu", "Andrea Morales-Garzón", "Jingyi Zheng", "Maria Maistro", "Daniel Hershcovich"], "categories": ["cs.CL"], "comment": null, "summary": "In cross-cultural recipe adaptation, the goal is not only to ensure cultural\nappropriateness and retain the original dish's essence, but also to provide\ndiverse options for various dietary needs and preferences. Retrieval Augmented\nGeneration (RAG) is a promising approach, combining the retrieval of real\nrecipes from the target cuisine for cultural adaptability with large language\nmodels (LLMs) for relevance. However, it remains unclear whether RAG can\ngenerate diverse adaptation results. Our analysis shows that RAG tends to\noverly rely on a limited portion of the context across generations, failing to\nproduce diverse outputs even when provided with varied contextual inputs. This\nreveals a key limitation of RAG in creative tasks with multiple valid answers:\nit fails to leverage contextual diversity for generating varied responses. To\naddress this issue, we propose CARRIAGE, a plug-and-play RAG framework for\ncross-cultural recipe adaptation that enhances diversity in both retrieval and\ncontext organization. To our knowledge, this is the first RAG framework that\nexplicitly aims to generate highly diverse outputs to accommodate multiple user\npreferences. Our experiments show that CARRIAGE achieves Pareto efficiency in\nterms of diversity and quality of recipe adaptation compared to closed-book\nLLMs.", "AI": {"tldr": "CARRIAGE enhances recipe adaptation by improving diversity in Retrieval Augmented Generation (RAG) processes.", "motivation": "To ensure cultural appropriateness and provide diverse options for various dietary needs in cross-cultural recipe adaptation.", "method": "The authors propose CARRIAGE, a plug-and-play RAG framework that increases diversity in recipe retrieval and contextual organization.", "result": "CARRIAGE demonstrates Pareto efficiency, achieving better diversity and quality in recipe adaptations than traditional closed-book LLMs.", "conclusion": "CARRIAGE is the first RAG framework designed specifically to generate diverse outputs for user preferences in recipe adaptation.", "key_contributions": ["Introduction of CARRIAGE framework for recipe adaptation", "Enhancement of diversity in RAG outputs", "Demonstration of Pareto efficiency in diversity and quality metrics."], "limitations": "RAG's inherent reliance on context may still limit creativity in certain scenarios.", "keywords": ["cross-cultural adaptation", "recipe generation", "diversity", "Retrieval Augmented Generation", "large language models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.21980", "pdf": "https://arxiv.org/pdf/2507.21980.pdf", "abs": "https://arxiv.org/abs/2507.21980", "title": "Predicting Microbial Ontology and Pathogen Risk from Environmental Metadata with Large Language Models", "authors": ["Hyunwoo Yoo", "Gail L. Rosen"], "categories": ["cs.CL"], "comment": null, "summary": "Traditional machine learning models struggle to generalize in microbiome\nstudies where only metadata is available, especially in small-sample settings\nor across studies with heterogeneous label formats. In this work, we explore\nthe use of large language models (LLMs) to classify microbial samples into\nontology categories such as EMPO 3 and related biological labels, as well as to\npredict pathogen contamination risk, specifically the presence of E. Coli,\nusing environmental metadata alone. We evaluate LLMs such as ChatGPT-4o, Claude\n3.7 Sonnet, Grok-3, and LLaMA 4 in zero-shot and few-shot settings, comparing\ntheir performance against traditional models like Random Forests across\nmultiple real-world datasets. Our results show that LLMs not only outperform\nbaselines in ontology classification, but also demonstrate strong predictive\nability for contamination risk, generalizing across sites and metadata\ndistributions. These findings suggest that LLMs can effectively reason over\nsparse, heterogeneous biological metadata and offer a promising metadata-only\napproach for environmental microbiology and biosurveillance applications.", "AI": {"tldr": "This paper explores the use of large language models (LLMs) for classifying microbial samples and predicting pathogen contamination risks using only environmental metadata, demonstrating their effectiveness over traditional machine learning models.", "motivation": "To address the limitations of traditional machine learning in generalizing across microbiome studies that have limited data and heterogeneous label formats.", "method": "The study evaluates various LLMs such as ChatGPT-4o, Claude 3.7 Sonnet, Grok-3, and LLaMA 4 in zero-shot and few-shot settings, comparing their performance against traditional models like Random Forests across multiple real-world datasets.", "result": "LLMs outperform baseline traditional models in ontology classification and predict contamination risk effectively, generalizing well across different sites and metadata distributions.", "conclusion": "LLMs offer a promising, metadata-only approach for environmental microbiology and biosurveillance applications, exceeding the performance of traditional methods.", "key_contributions": ["Demonstrated LLMs' capability to outperform traditional machine learning models in classifying microbial samples using metadata only.", "Insights into the generalization abilities of LLMs across diverse microbiome datasets and risk prediction for contamination.", "Provided a framework for utilizing LLMs in environmental microbiology and biosurveillance."], "limitations": "", "keywords": ["Large Language Models", "Microbiome Studies", "Contamination Risk Prediction", "Environmental Metadata", "Biosurveillance"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.22050", "pdf": "https://arxiv.org/pdf/2507.22050.pdf", "abs": "https://arxiv.org/abs/2507.22050", "title": "DeepSieve: Information Sieving via LLM-as-a-Knowledge-Router", "authors": ["Minghao Guo", "Qingcheng Zeng", "Xujiang Zhao", "Yanchi Liu", "Wenchao Yu", "Mengnan Du", "Haifeng Chen", "Wei Cheng"], "categories": ["cs.CL"], "comment": "22 pages, work in progress", "summary": "Large Language Models (LLMs) excel at many reasoning tasks but struggle with\nknowledge-intensive queries due to their inability to dynamically access\nup-to-date or domain-specific information. Retrieval-Augmented Generation (RAG)\nhas emerged as a promising solution, enabling LLMs to ground their responses in\nexternal sources. However, existing RAG methods lack fine-grained control over\nboth the query and source sides, often resulting in noisy retrieval and shallow\nreasoning. In this work, we introduce DeepSieve, an agentic RAG framework that\nincorporates information sieving via LLM-as-a-knowledge-router. DeepSieve\ndecomposes complex queries into structured sub-questions and recursively routes\neach to the most suitable knowledge source, filtering irrelevant information\nthrough a multi-stage distillation process. Our design emphasizes modularity,\ntransparency, and adaptability, leveraging recent advances in agentic system\ndesign. Experiments on multi-hop QA tasks across heterogeneous sources\ndemonstrate improved reasoning depth, retrieval precision, and interpretability\nover conventional RAG approaches.", "AI": {"tldr": "DeepSieve is a new Retrieval-Augmented Generation framework designed to improve LLMs' ability to handle knowledge-intensive queries effectively through structured sub-questions and multi-stage information sieving.", "motivation": "To address the limitations of existing RAG methods that struggle with dynamic knowledge access and often lead to noisy retrieval and shallow reasoning in LLMs.", "method": "DeepSieve decomposes complex queries into structured sub-questions and routes them to the most suitable knowledge source, using a multi-stage distillation process to filter irrelevant information.", "result": "Experiments show that DeepSieve outperforms conventional RAG approaches in terms of reasoning depth, retrieval precision, and interpretability on multi-hop QA tasks.", "conclusion": "DeepSieve enhances the capability of LLMs in knowledge-intensive tasks, making it more effective for applications requiring deep reasoning and accurate information retrieval.", "key_contributions": ["Introduces DeepSieve framework to enhance LLM knowledge retrieval capabilities", "Implements structured query decomposition for improved routing to information sources", "Enhances modularity and adaptability in retrieval-augmented generation."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Large Language Models", "Multi-hop QA"], "importance_score": 9, "read_time_minutes": 22}}
{"id": "2507.21075", "pdf": "https://arxiv.org/pdf/2507.21075.pdf", "abs": "https://arxiv.org/abs/2507.21075", "title": "Can LLMs Reason About Trust?: A Pilot Study", "authors": ["Anushka Debnath", "Stephen Cranefield", "Emiliano Lorini", "Bastin Tony Roy Savarimuthu"], "categories": ["cs.HC", "cs.CL", "cs.CY", "cs.MA"], "comment": "17 pages, 5 figures, 3 tables Accepted for presentation as a full\n  paper at the COINE 2025 workshop at AAMAS 2025 see\n  https://coin-workshop.github.io/coine-2025-detroit/accepted_for_presentation.html", "summary": "In human society, trust is an essential component of social attitude that\nhelps build and maintain long-term, healthy relationships which creates a\nstrong foundation for cooperation, enabling individuals to work together\neffectively and achieve shared goals. As many human interactions occur through\nelectronic means such as using mobile apps, the potential arises for AI systems\nto assist users in understanding the social state of their relationships. In\nthis paper we investigate the ability of Large Language Models (LLMs) to reason\nabout trust between two individuals in an environment which requires fostering\ntrust relationships. We also assess whether LLMs are capable of inducing trust\nby role-playing one party in a trust based interaction and planning actions\nwhich can instil trust.", "AI": {"tldr": "This paper explores how Large Language Models (LLMs) can understand and foster trust in human relationships through electronic interactions.", "motivation": "Trust is crucial in social relationships, and as many interactions occur online, AI has the potential to assist in managing these dynamics.", "method": "The paper investigates LLMs' reasoning about trust between individuals and their ability to induce trust through role-playing scenarios.", "result": "The study finds that LLMs can effectively reason about and induce trust in specified contexts, demonstrating their potential in relationship management.", "conclusion": "LLMs can help foster trust in digital interactions, paving the way for more effective cooperative behaviors among users.", "key_contributions": ["Analysis of LLMs in reasoning about trust", "Role-playing scenario implementation", "Insights into AI-assisted social relationship management"], "limitations": "The investigation is limited to specific scenarios and may not generalize to all types of trust interactions.", "keywords": ["Trust", "Large Language Models", "Human-Computer Interaction", "AI", "Social Relationships"], "importance_score": 8, "read_time_minutes": 17}}
{"id": "2507.21089", "pdf": "https://arxiv.org/pdf/2507.21089.pdf", "abs": "https://arxiv.org/abs/2507.21089", "title": "Emotionally Aware Moderation: The Potential of Emotion Monitoring in Shaping Healthier Social Media Conversations", "authors": ["Xiaotian Su", "Naim Zierau", "Soomin Kim", "April Yi Wang", "Thiemo Wambsganss"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Social media platforms increasingly employ proactive moderation techniques,\nsuch as detecting and curbing toxic and uncivil comments, to prevent the spread\nof harmful content. Despite these efforts, such approaches are often criticized\nfor creating a climate of censorship and failing to address the underlying\ncauses of uncivil behavior. Our work makes both theoretical and practical\ncontributions by proposing and evaluating two types of emotion monitoring\ndashboards to users' emotional awareness and mitigate hate speech. In a study\ninvolving 211 participants, we evaluate the effects of the two mechanisms on\nuser commenting behavior and emotional experiences. The results reveal that\nthese interventions effectively increase users' awareness of their emotional\nstates and reduce hate speech. However, our findings also indicate potential\nunintended effects, including increased expression of negative emotions (Angry,\nFear, and Sad) when discussing sensitive issues. These insights provide a basis\nfor further research on integrating proactive emotion regulation tools into\nsocial media platforms to foster healthier digital interactions.", "AI": {"tldr": "This paper evaluates emotion monitoring dashboards aimed at increasing users' emotional awareness and reducing hate speech on social media.", "motivation": "To address the limitations of current moderation techniques in curbing hate speech without fostering a climate of censorship.", "method": "A study was conducted with 211 participants to assess the impact of two emotion monitoring mechanisms on commenting behavior and emotional experiences.", "result": "The interventions improved users' awareness of their emotional states and successfully reduced hate speech, but also led to increased expression of negative emotions when discussing sensitive issues.", "conclusion": "While emotion regulation tools can lower hate speech, they might inadvertently heighten negative emotional expressions, necessitating further investigation into their design and implementation.", "key_contributions": ["Proposed emotion monitoring dashboards", "Evaluated user behavior and emotional outcomes", "Identified unintended effects of emotional interventions"], "limitations": "Potential unintended consequences of increased negative emotional expression.", "keywords": ["emotion monitoring", "hate speech", "social media", "user behavior", "emotional awareness"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2403.06963", "pdf": "https://arxiv.org/pdf/2403.06963.pdf", "abs": "https://arxiv.org/abs/2403.06963", "title": "The pitfalls of next-token prediction", "authors": ["Gregor Bachmann", "Vaishnavh Nagarajan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML 2024", "summary": "Can a mere next-token predictor faithfully model human intelligence? We\ncrystallize this emerging concern and correct popular misconceptions\nsurrounding it, and advocate a simple multi-token objective.\n  As a starting point, we argue that the two often-conflated phases of\nnext-token prediction -- autoregressive inference and teacher-forced training\n-- must be treated distinctly. The popular criticism that errors can compound\nduring autoregressive inference, crucially assumes that teacher-forcing has\nlearned an accurate next-token predictor. This assumption sidesteps a more\ndeep-rooted problem we expose: in certain classes of tasks, teacher-forcing can\nsimply fail to learn an accurate next-token predictor in the first place. We\ndescribe a general mechanism of how teacher-forcing can fail, and design a\nminimal planning task where both the Transformer and the Mamba architecture\nempirically fail in that manner -- remarkably, despite the task being\nstraightforward to learn.\n  Finally, we provide preliminary evidence that this failure can be resolved\nusing _teacherless_ training, a simple modification using dummy tokens that\npredicts multiple tokens in advance. We hope this finding can ground future\ndebates and inspire explorations beyond the next-token prediction paradigm. We\nmake our code available under\nhttps://github.com/gregorbachmann/Next-Token-Failures", "AI": {"tldr": "This paper critiques next-token prediction in AI, discussing misconceptions and advocating for a multi-token objective to better model human intelligence.", "motivation": "To address concerns about the ability of next-token predictors to truly model human intelligence and correct misconceptions regarding their effectiveness.", "method": "The authors distinguish between autoregressive inference and teacher-forced training phases, identifying scenarios where teacher-forcing fails to learn an accurate next-token predictor. They introduce a minimal planning task to demonstrate this failure empirically.", "result": "Evidence is provided that both the Transformer and Mamba architectures fail at this task, despite its simplicity. A novel approach using teacherless training shows potential in resolving the failures observed in teacher-forcing.", "conclusion": "The findings suggest directions for future research beyond the next-token prediction paradigm, highlighting the need for improved training methodologies in AI modeling.", "key_contributions": ["Clarification of the roles of autoregressive inference and teacher-forced training in next-token prediction", "Empirical demonstration of failure modes in popular architectures like Transformer and Mamba", "Introduction of teacherless training as a potential solution to identified problems."], "limitations": "The study primarily focuses on a specific minimal planning task; broader applicability needs further exploration.", "keywords": ["next-token prediction", "teacher-forcing", "machine learning", "human intelligence", "teacherless training"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2409.18924", "pdf": "https://arxiv.org/pdf/2409.18924.pdf", "abs": "https://arxiv.org/abs/2409.18924", "title": "Simulated patient systems are intelligent when powered by large language model-based AI agents", "authors": ["Huizi Yu", "Jiayan Zhou", "Lingyao Li", "Shan Chen", "Jack Gallifant", "Anye Shi", "Xiang Li", "Jingxian He", "Wenyue Hua", "Mingyu Jin", "Guang Chen", "Yang Zhou", "Zhao Li", "Trisha Gupte", "Ming-Li Chen", "Zahra Azizi", "Yongfeng Zhang", "Yanqiu Xing", "Themistocles L. Danielle S. Bitterman", "Themistocles L. Assimes", "Xin Ma", "Lin Lu", "Lizhou Fan"], "categories": ["cs.CL", "cs.AI"], "comment": "64 pages, 14 figures, 16 tables", "summary": "Simulated patient systems play an important role in modern medical education\nand research, providing safe, integrative medical training environments and\nsupporting clinical decision-making simulations. We developed AIPatient, an\nintelligent simulated patient system powered by large language model-based AI\nagents. The system incorporates the Retrieval Augmented Generation (RAG)\nframework, powered by six task-specific LLM-based AI agents for complex\nreasoning. For simulation reality, the system is also powered by the AIPatient\nKG (Knowledge Graph), built with de-identified real patient data from the\nMedical Information Mart for Intensive Care (MIMIC)-III database. Primary\noutcomes showcase the system's intelligence, including the system's accuracy in\nElectronic Record (EHR)-based medical Question Answering (QA), readability,\nrobustness, and stability. The system achieved a QA accuracy of 94.15% when all\nsix AI agents present, surpassing benchmarks with partial or no agent\nintegration. Its knowledgebase demonstrated high validity (F1 score=0.89).\nReadability scores showed median Flesch Reading Ease at 77.23 and median Flesch\nKincaid Grade at 5.6, indicating accessibility to all medical professionals.\nRobustness and stability were confirmed with non-significant variance (ANOVA\nF-value=0.6126, p > 0.1; F-value=0.782, p > 0.1). A user study with medical\nstudents further demonstrated that AIPatient offers high fidelity, strong\nusability, and effective educational value, performing comparably or better\nthan human-simulated patients in medical history-taking scenarios. The\npromising intelligence of the AIPatient system highlights its potential to\nsupport a wide range of applications, including medical education, model\nevaluation, and system integration.", "AI": {"tldr": "AIPatient is an intelligent simulated patient system that uses LLMs for enhanced accuracy and usability in medical education and decision-making.", "motivation": "To develop a safe and integrative training environment for medical education and improve clinical decision-making simulations using AI.", "method": "The system utilizes Retrieval Augmented Generation (RAG) framework with six task-specific LLM-based AI agents and a knowledge graph built from de-identified patient data.", "result": "Achieved 94.15% QA accuracy with high readability scores and demonstrated strong usability and educational value in user studies with medical students.", "conclusion": "AIPatient has significant potential for enhancing medical education and various applications in healthcare.", "key_contributions": ["Integration of LLMs in simulated patient systems", "High QA accuracy surpassing benchmarks", "Demonstrated usability in medical training scenarios"], "limitations": "", "keywords": ["Simulated Patient Systems", "Large Language Models", "Medical Education"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2410.16491", "pdf": "https://arxiv.org/pdf/2410.16491.pdf", "abs": "https://arxiv.org/abs/2410.16491", "title": "BIG5-CHAT: Shaping LLM Personalities Through Training on Human-Grounded Data", "authors": ["Wenkai Li", "Jiarui Liu", "Andy Liu", "Xuhui Zhou", "Mona Diab", "Maarten Sap"], "categories": ["cs.CL"], "comment": null, "summary": "In this work, we tackle the challenge of embedding realistic human\npersonality traits into LLMs. Previous approaches have primarily focused on\nprompt-based methods that describe the behavior associated with the desired\npersonality traits, suffering from realism and validity issues. To address\nthese limitations, we introduce BIG5-CHAT, a large-scale dataset containing\n100,000 dialogues designed to ground models in how humans express their\npersonality in language. Leveraging this dataset, we explore Supervised\nFine-Tuning and Direct Preference Optimization as training-based methods to\nalign LLMs more naturally with human personality patterns. Our methods\noutperform prompting on personality assessments such as BFI and IPIP-NEO, with\ntrait correlations more closely matching human data. Furthermore, our\nexperiments reveal that models trained to exhibit higher conscientiousness,\nhigher agreeableness, lower extraversion, and lower neuroticism display better\nperformance on reasoning tasks, aligning with psychological findings on how\nthese traits impact human cognitive performance. To our knowledge, this work is\nthe first comprehensive study to demonstrate how training-based methods can\nshape LLM personalities through learning from real human behaviors.", "AI": {"tldr": "This paper presents BIG5-CHAT, a dataset for integrating human personality traits in large language models (LLMs) and explores training methods to better align LLMs with human personality expressions.", "motivation": "The study aims to improve the realism and validity of LLMs' representations of human personality traits, which has been inadequately addressed in previous approaches reliant on prompting.", "method": "Introduces the BIG5-CHAT dataset and explores Supervised Fine-Tuning and Direct Preference Optimization to enhance LLMs' alignment with human personality attributes.", "result": "The proposed methods outperform existing prompting techniques in personality assessments and lead to models that demonstrate better reasoning performance when trained on certain personality traits.", "conclusion": "This work shows that training-based approaches can effectively shape LLM personalities, resulting in more realistic and valid personality trait expressions grounded in human behaviors.", "key_contributions": ["Introduction of the BIG5-CHAT dataset with 100,000 dialogues", "Demonstration of training-based methods outperforming traditional prompt-based methods", "Findings aligning personality traits with improved reasoning performance in LLMs."], "limitations": "", "keywords": ["human personality traits", "LLMs", "BIG5-CHAT", "Supervised Fine-Tuning", "Direct Preference Optimization"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2411.19096", "pdf": "https://arxiv.org/pdf/2411.19096.pdf", "abs": "https://arxiv.org/abs/2411.19096", "title": "Pralekha: Cross-Lingual Document Alignment for Indic Languages", "authors": ["Sanjay Suryanarayanan", "Haiyue Song", "Mohammed Safi Ur Rahman Khan", "Anoop Kunchukuttan", "Raj Dabre"], "categories": ["cs.CL"], "comment": null, "summary": "Mining parallel document pairs for document-level machine translation (MT)\nremains challenging due to the limitations of existing Cross-Lingual Document\nAlignment (CLDA) techniques. Most approaches rely on metadata such as URLs,\nwhich is often unavailable in low-resource language settings, while others\nrepresent documents using pooled sentence embeddings, which fail to capture\nfine-grained alignment cues. Moreover, current sentence embedding models have\nlimited context windows, hindering their ability to represent document-level\ninformation effectively. To address these challenges for Indic languages, we\nintroduce PRALEKHA, a large-scale benchmark for evaluating document-level\nalignment techniques. It contains over 3 million aligned document pairs across\n11 Indic languages and English, of which 1.5 million are English--Indic pairs.\nFurthermore, we propose Document Alignment Coefficient (DAC), a novel metric\nfor fine-grained document alignment. Unlike pooling-based approaches, DAC\naligns documents by matching smaller chunks and computes similarity as the\nratio of aligned chunks to the average number of chunks in a pair. Intrinsic\nevaluation shows that DAC achieves substantial improvements over pooling-based\nbaselines, particularly in noisy scenarios. Extrinsic evaluation further\ndemonstrates that document MT models trained on DAC-aligned pairs consistently\noutperform those using baseline alignment methods. These results highlight\nDAC's effectiveness for parallel document mining. The PRALEKHA dataset and CLDA\nevaluation framework will be made publicly available.", "AI": {"tldr": "PRALEKHA is a benchmark for document-level alignment in MT, particularly for Indic languages, introducing a new metric (DAC) that outperforms existing alignment methods.", "motivation": "To improve document-level machine translation (MT) by addressing the limitations of current Cross-Lingual Document Alignment (CLDA) techniques in low-resource settings.", "method": "Introduction of PRALEKHA, a benchmark dataset with over 3 million aligned document pairs for evaluating document alignment techniques, and the Document Alignment Coefficient (DAC) for fine-grained alignment.", "result": "DAC showed significant improvements over pooling-based methods, particularly in noisy scenarios, and document MT models using DAC-aligned pairs performed better than those with traditional alignment methods.", "conclusion": "PRALEKHA and DAC provide valuable resources for improving document-level alignment in machine translation, particularly for low-resource languages.", "key_contributions": ["Introduction of a large-scale benchmark (PRALEKHA) for document alignment in Indic languages.", "Development of Document Alignment Coefficient (DAC), enhancing alignment accuracy by focusing on smaller chunks.", "Demonstration of DAC's superior performance in both intrinsic and extrinsic evaluations for document MT."], "limitations": "", "keywords": ["Document-level machine translation", "Cross-Lingual Document Alignment", "Document Alignment Coefficient"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2502.18978", "pdf": "https://arxiv.org/pdf/2502.18978.pdf", "abs": "https://arxiv.org/abs/2502.18978", "title": "Low-Confidence Gold: Refining Low-Confidence Samples for Efficient Instruction Tuning", "authors": ["Hongyi Cai", "Jie Li", "Mohammad Mahdinur Rahman", "Wenzhen Dong"], "categories": ["cs.CL", "cs.AI"], "comment": "8 pages", "summary": "The effectiveness of instruction fine-tuning for Large Language Models is\nfundamentally constrained by the quality and efficiency of training datasets.\nThis work introduces Low-Confidence Gold (LCG), a novel filtering framework\nthat employs centroid-based clustering and confidence-guided selection for\nidentifying valuable instruction pairs. Through a semi-supervised approach\nusing a lightweight classifier trained on representative samples, LCG curates\nhigh-quality subsets while preserving data diversity. Experimental evaluation\ndemonstrates that models fine-tuned on LCG-filtered subsets of 6K samples\nachieve superior performance compared to existing methods, with substantial\nimprovements on MT-bench and consistent gains across comprehensive evaluation\nmetrics. The framework's efficacy while maintaining model performance\nestablishes a promising direction for efficient instruction tuning.", "AI": {"tldr": "This paper presents Low-Confidence Gold (LCG), a filtering framework to enhance instruction fine-tuning of Large Language Models by selecting high-quality instruction pairs.", "motivation": "The study addresses the limitations of current datasets used for fine-tuning Large Language Models, focusing on improving quality and efficiency.", "method": "The LCG framework utilizes centroid-based clustering and confidence-guided selection to curate instruction pairs, employing a semi-supervised approach with a lightweight classifier.", "result": "Models fine-tuned on LCG-filtered datasets of 6K samples demonstrated superior performance on MT-bench and other evaluation metrics.", "conclusion": "The LCG framework significantly enhances instruction tuning effectiveness while maintaining model performance, indicating a strong potential for further research in efficient instructional tuning methods.", "key_contributions": ["Introduction of Low-Confidence Gold (LCG) framework for filtering instruction pairs.", "Use of centroid-based clustering and confidence-guided selection for efficient dataset curation.", "Demonstrated performance improvements on fine-tuning models with specific evaluation metrics."], "limitations": "", "keywords": ["Large Language Models", "Instruction Fine-Tuning", "Data Filtering"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.04844", "pdf": "https://arxiv.org/pdf/2503.04844.pdf", "abs": "https://arxiv.org/abs/2503.04844", "title": "Narrative Context Protocol: An Open-Source Storytelling Framework for Generative AI", "authors": ["Hank Gerba"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Here we introduce Narrative Context Protocol (NCP), an open-source narrative\nstandard designed to enable narrative interoperability, AI-driven authoring\ntools, real-time emergent narratives, and more. By encoding a story's structure\nin a \"Storyform,\" which is a structured register of its narrative features, NCP\nenables narrative portability across systems as well as intent-based\nconstraints for generative storytelling systems. We demonstrate the\ncapabilities of NCP through a year-long experiment, during which an author used\nNCP and a custom authoring platform to create a playable, text-based experience\nbased on her pre-existing novella. This experience is driven by generative AI,\nwith unconstrained natural language input. NCP functions as a set of\n\"guardrails\" that allows the generative system to accommodate player agency\nwhile also ensuring that narrative context and coherence are maintained.", "AI": {"tldr": "Introduction of Narrative Context Protocol (NCP) for narrative interoperability and AI-driven storytelling.", "motivation": "To enable narrative portability and real-time emergent narratives in AI-driven authoring tools.", "method": "Utilization of NCP as a structured register of narrative features for a year-long experiment with a custom authoring platform.", "result": "An author created a playable, text-based experience that maintained narrative coherence while accommodating player agency.", "conclusion": "NCP effectively functions as guardrails for generative systems in storytelling, balancing player input with narrative structure.", "key_contributions": ["Development of an open-source narrative standard (NCP)", "Demonstration of narrative interoperability in AI-driven authoring tools", "Implementation of a year-long experiment showcasing NCP's functionality"], "limitations": "", "keywords": ["Narrative Context Protocol", "generative storytelling", "AI-driven authoring"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2503.06424", "pdf": "https://arxiv.org/pdf/2503.06424.pdf", "abs": "https://arxiv.org/abs/2503.06424", "title": "Training LLM-based Tutors to Improve Student Learning Outcomes in Dialogues", "authors": ["Alexander Scarlatos", "Naiming Liu", "Jaewook Lee", "Richard Baraniuk", "Andrew Lan"], "categories": ["cs.CL", "cs.CY"], "comment": "Published in AIED 2025: The 26th International Conference on\n  Artificial Intelligence in Education", "summary": "Generative artificial intelligence (AI) has the potential to scale up\npersonalized tutoring through large language models (LLMs). Recent AI tutors\nare adapted for the tutoring task by training or prompting LLMs to follow\neffective pedagogical principles, though they are not trained to maximize\nstudent learning throughout the course of a dialogue. Therefore, they may\nengage with students in a suboptimal way. We address this limitation by\nintroducing an approach to train LLMs to generate tutor utterances that\nmaximize the likelihood of student correctness, while still encouraging the\nmodel to follow good pedagogical practice. Specifically, we generate a set of\ncandidate tutor utterances and score them using (1) an LLM-based student model\nto predict the chance of correct student responses and (2) a pedagogical rubric\nevaluated by GPT-4o. We then use the resulting data to train an open-source\nLLM, Llama 3.1 8B, using direct preference optimization. We show that tutor\nutterances generated by our model lead to significantly higher chances of\ncorrect student responses while maintaining the pedagogical quality of GPT-4o.\nWe also conduct qualitative analyses and a human evaluation to demonstrate that\nour model generates high quality tutor utterances.", "AI": {"tldr": "This paper presents a method for training large language models (LLMs) to generate more effective tutoring utterances that maximize student correctness while adhering to good pedagogical practices.", "motivation": "To improve the effectiveness of AI tutors by optimizing their responses to enhance student learning outcomes during tutoring sessions.", "method": "The authors generate candidate tutor utterances, score them using an LLM-based student model to predict correct student responses, and evaluate them based on a pedagogical rubric assessed by GPT-4o. They train an open-source LLM, Llama 3.1 8B, using direct preference optimization.", "result": "The tutor utterances generated by the proposed model significantly improve the likelihood of correct student responses while ensuring high pedagogical quality.", "conclusion": "The approach not only enhances the tutoring effectiveness of AI but also maintains quality standards, validated through qualitative analyses and human evaluations.", "key_contributions": ["Introduction of a scoring system for tutor utterances based on student response likelihood and pedagogical quality.", "Training of Llama 3.1 8B to generate more effective tutor responses leveraging direct preference optimization.", "Demonstration of improved student outcomes through both quantitative metrics and qualitative evaluations."], "limitations": "", "keywords": ["generative AI", "large language models", "tutoring", "pedagogical practices", "student learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.13401", "pdf": "https://arxiv.org/pdf/2503.13401.pdf", "abs": "https://arxiv.org/abs/2503.13401", "title": "Levels of Analysis for Large Language Models", "authors": ["Alexander Ku", "Declan Campbell", "Xuechunzi Bai", "Jiayi Geng", "Ryan Liu", "Raja Marjieh", "R. Thomas McCoy", "Andrew Nam", "Ilia Sucholutsky", "Veniamin Veselovsky", "Liyi Zhang", "Jian-Qiao Zhu", "Thomas L. Griffiths"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Modern artificial intelligence systems, such as large language models, are\nincreasingly powerful but also increasingly hard to understand. Recognizing\nthis problem as analogous to the historical difficulties in understanding the\nhuman mind, we argue that methods developed in cognitive science can be useful\nfor understanding large language models. We propose a framework for applying\nthese methods based on the levels of analysis that David Marr proposed for\nstudying information processing systems. By revisiting established cognitive\nscience techniques relevant to each level and illustrating their potential to\nyield insights into the behavior and internal organization of large language\nmodels, we aim to provide a toolkit for making sense of these new kinds of\nminds.", "AI": {"tldr": "The paper proposes a framework for applying cognitive science methods to better understand large language models, drawing parallels to the study of the human mind.", "motivation": "There is a growing need to understand the workings of modern AI systems, particularly large language models, due to their increasing complexity.", "method": "The authors propose to use David Marr's levels of analysis as a framework for applying cognitive science techniques to study large language models.", "result": "By revisiting cognitive science methods suitable for each level of analysis, the framework shows potential for providing insights into how large language models behave and are organized internally.", "conclusion": "The proposed toolkit from cognitive science could enhance our understanding of large language models, offering clearer insights into their functioning.", "key_contributions": ["Introducing a cognitive science approach to analyze large language models", "Application of Marr's levels of analysis to AI systems", "Providing a toolkit for understanding AI behavior and organization"], "limitations": "", "keywords": ["Cognitive Science", "Large Language Models", "Marr's Levels of Analysis", "AI Understanding", "Information Processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.16531", "pdf": "https://arxiv.org/pdf/2503.16531.pdf", "abs": "https://arxiv.org/abs/2503.16531", "title": "EEG-CLIP : Learning EEG representations from natural language descriptions", "authors": ["Tidiane Camaret Ndir", "Robin Tibor Schirrmeister", "Tonio Ball"], "categories": ["cs.CL", "cs.LG", "eess.SP"], "comment": null, "summary": "Deep networks for electroencephalogram (EEG) decoding are often only trained\nto solve one specific task, such as pathology or age decoding. A more general\ntask-agnostic approach is to train deep networks to match a (clinical) EEG\nrecording to its corresponding textual medical report and vice versa. This\napproach was pioneered in the computer vision domain matching images and their\ntext captions and subsequently allowed to do successful zero-shot decoding\nusing textual class prompts. In this work, we follow this approach and develop\na contrastive learning framework, EEG-CLIP, that aligns the EEG time series and\nthe descriptions of the corresponding clinical text in a shared embedding\nspace. We investigated its potential for versatile EEG decoding, evaluating\nperformance in a range of few-shot and zero-shot settings. Overall, we show\nthat EEG-CLIP manages to non-trivially align text and EEG representations. Our\nwork presents a promising approach to learn general EEG representations, which\ncould enable easier analyses of diverse decoding questions through zero-shot\ndecoding or training task-specific models from fewer training examples. The\ncode for reproducing our results is available at\nhttps://github.com/tidiane-camaret/EEGClip", "AI": {"tldr": "Development of a contrastive learning framework, EEG-CLIP, for aligning EEG time series with clinical text descriptions, aimed at enhancing EEG decoding versatility.", "motivation": "Current deep networks for EEG decoding are often task-specific; this paper proposes a more general task-agnostic approach using contrastive learning to align EEG data with textual medical reports.", "method": "We developed the EEG-CLIP framework which utilizes contrastive learning to create a shared embedding space for matching EEG signals with their corresponding clinical text descriptions.", "result": "EEG-CLIP shows significant potential for versatile EEG decoding, performing well in few-shot and zero-shot settings by successfully aligning EEG and text representations.", "conclusion": "This method presents a promising route for developing general EEG representations and facilitates easier analysis across various decoding tasks.", "key_contributions": ["Introduction of EEG-CLIP for EEG and text alignment", "Evaluation in few-shot and zero-shot learning contexts", "Open-source implementation for reproducibility"], "limitations": "", "keywords": ["EEG decoding", "contrastive learning", "few-shot learning", "zero-shot learning", "medical text"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2503.20797", "pdf": "https://arxiv.org/pdf/2503.20797.pdf", "abs": "https://arxiv.org/abs/2503.20797", "title": "\"Whose Side Are You On?\" Estimating Ideology of Political and News Content Using Large Language Models and Few-shot Demonstration Selection", "authors": ["Muhammad Haroon", "Magdalena Wojcieszak", "Anshuman Chhabra"], "categories": ["cs.CL", "cs.CY", "cs.SI"], "comment": null, "summary": "The rapid growth of social media platforms has led to concerns about\nradicalization, filter bubbles, and content bias. Existing approaches to\nclassifying ideology are limited in that they require extensive human effort,\nthe labeling of large datasets, and are not able to adapt to evolving\nideological contexts. This paper explores the potential of Large Language\nModels (LLMs) for classifying the political ideology of online content in the\ncontext of the two-party US political spectrum through in-context learning\n(ICL). Our extensive experiments involving demonstration selection in\nlabel-balanced fashion, conducted on three datasets comprising news articles\nand YouTube videos, reveal that our approach significantly outperforms\nzero-shot and traditional supervised methods. Additionally, we evaluate the\ninfluence of metadata (e.g., content source and descriptions) on ideological\nclassification and discuss its implications. Finally, we show how providing the\nsource for political and non-political content influences the LLM's\nclassification.", "AI": {"tldr": "This paper investigates the use of Large Language Models (LLMs) to classify the political ideology of online content, addressing limitations in existing methods.", "motivation": "To address concerns around radicalization, filter bubbles, and content bias in social media, as well as to improve ideological classification methods.", "method": "The paper employs in-context learning (ICL) with LLMs on three datasets of news articles and YouTube videos, utilizing demonstration selection in a label-balanced manner.", "result": "The experiments show that the proposed approach significantly outperforms both zero-shot and traditional supervised methods for ideological classification.", "conclusion": "The study highlights the effectiveness of LLMs in ideological classification and the influence of metadata on this task, suggesting that the inclusion of source information can impact classification outcomes.", "key_contributions": ["Utilization of LLMs for ideological classification with in-context learning", "Demonstration of significant performance improvement over existing methods", "Analysis of metadata influence on classification outcomes"], "limitations": "", "keywords": ["Large Language Models", "political ideology", "in-context learning", "social media", "classification"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.00042", "pdf": "https://arxiv.org/pdf/2504.00042.pdf", "abs": "https://arxiv.org/abs/2504.00042", "title": "Beyond the Reported Cutoff: Where Large Language Models Fall Short on Financial Knowledge", "authors": ["Agam Shah", "Liqin Ye", "Sebastian Jaskowski", "Wei Xu", "Sudheer Chava"], "categories": ["cs.CL"], "comment": "Paper accepted at CoLM 2025", "summary": "Large Language Models (LLMs) are frequently utilized as sources of knowledge\nfor question-answering. While it is known that LLMs may lack access to\nreal-time data or newer data produced after the model's cutoff date, it is less\nclear how their knowledge spans across historical information. In this study,\nwe assess the breadth of LLMs' knowledge using financial data of U.S. publicly\ntraded companies by evaluating more than 197k questions and comparing model\nresponses to factual data. We further explore the impact of company\ncharacteristics, such as size, retail investment, institutional attention, and\nreadability of financial filings, on the accuracy of knowledge represented in\nLLMs. Our results reveal that LLMs are less informed about past financial\nperformance, but they display a stronger awareness of larger companies and more\nrecent information. Interestingly, at the same time, our analysis also reveals\nthat LLMs are more likely to hallucinate for larger companies, especially for\ndata from more recent years. The code, prompts, and model outputs are available\non GitHub.", "AI": {"tldr": "This study evaluates the breadth of knowledge in Large Language Models (LLMs) using financial data from U.S. publicly traded companies, revealing their strengths and weaknesses in querying historical performance data.", "motivation": "To assess how well LLMs represent historical knowledge, especially in the context of financial data, and to understand the effects of company characteristics on their accuracy.", "method": "The study analyzes over 197k questions posed to LLMs regarding the financial data of U.S. publicly traded companies and compares the responses to factual information.", "result": "LLMs demonstrate limited knowledge of past financial performance but show greater accuracy for larger companies and more recent data, although they also tend to hallucinate more for these entities.", "conclusion": "LLMs are useful for retrieving knowledge about larger companies and recent events, but caution is needed due to potential inaccuracies, especially with historical data.", "key_contributions": ["Evaluation of LLMs' knowledge breadth using a large dataset of financial information", "Insights into the role of company characteristics on LLM accuracy", "Identification of hallucination tendencies in LLMs when querying larger firms"], "limitations": "Limited to financial data of U.S. publicly traded companies, may not represent other sectors or markets accurately.", "keywords": ["Large Language Models", "financial data", "question-answering", "machine learning", "hallucination"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.04142", "pdf": "https://arxiv.org/pdf/2504.04142.pdf", "abs": "https://arxiv.org/abs/2504.04142", "title": "My Life in Artificial Intelligence: People, anecdotes, and some lessons learnt", "authors": ["Kees van Deemter"], "categories": ["cs.CL", "cs.AI"], "comment": "34 pages", "summary": "In this very personal workography, I relate my 40-year experiences as a\nresearcher and educator in and around Artificial Intelligence (AI), more\nspecifically Natural Language Processing. I describe how curiosity, and the\ncircumstances of the day, led me to work in both industry and academia, and in\nvarious countries, including The Netherlands (Amsterdam, Eindhoven, and\nUtrecht), the USA (Stanford), England (Brighton), Scotland (Aberdeen), and\nChina (Beijing and Harbin). People and anecdotes play a large role in my story;\nthe history of AI forms its backdrop. I focus on things that might be of\ninterest to (even) younger colleagues, given the choices they face in their own\nwork and life at a time when AI is finally emerging from the shadows.", "AI": {"tldr": "A personal reflection on 40 years of experience in AI, particularly in Natural Language Processing, sharing insights and anecdotes from industry and academia.", "motivation": "To share experiences and insights from a long career in AI, focusing on the impact of personal curiosity and external circumstances on career trajectories.", "method": "A narrative biography outlining experiences across various countries and positions, supplemented by personal anecdotes.", "result": "The author highlights pivotal moments, choices, and influences throughout a four-decade career in AI, illustrating how these shaped their professional journey.", "conclusion": "AI is emerging as a significant field, and the author's experiences may guide younger colleagues in navigating their careers in this evolving landscape.", "key_contributions": ["Personal anecdotes from a 40-year career in AI", "Insights on career choices in the field of Natural Language Processing", "Reflections on the evolution of AI and its impact on career paths"], "limitations": "", "keywords": ["Artificial Intelligence", "Natural Language Processing", "Career Reflections"], "importance_score": 4, "read_time_minutes": 40}}
{"id": "2504.10227", "pdf": "https://arxiv.org/pdf/2504.10227.pdf", "abs": "https://arxiv.org/abs/2504.10227", "title": "Probing then Editing Response Personality of Large Language Models", "authors": ["Tianjie Ju", "Zhenyu Shao", "Bowen Wang", "Yujia Chen", "Zhuosheng Zhang", "Hao Fei", "Mong-Li Lee", "Wynne Hsu", "Sufeng Duan", "Gongshen Liu"], "categories": ["cs.CL"], "comment": "Accepted at COLM 2025", "summary": "Large Language Models (LLMs) have demonstrated promising capabilities to\ngenerate responses that simulate consistent personality traits. Despite the\nmajor attempts to analyze personality expression through output-based\nevaluations, little is known about how such traits are internally encoded\nwithin LLM parameters. In this paper, we introduce a layer-wise probing\nframework to systematically investigate the layer-wise capability of LLMs in\nsimulating personality for responding. We conduct probing experiments on 11\nopen-source LLMs over the PersonalityEdit benchmark and find that LLMs\npredominantly simulate personality for responding in their middle and upper\nlayers, with instruction-tuned models demonstrating a slightly clearer\nseparation of personality traits. Furthermore, by interpreting the trained\nprobing hyperplane as a layer-wise boundary for each personality category, we\npropose a layer-wise perturbation method to edit the personality expressed by\nLLMs during inference. Our results show that even when the prompt explicitly\nspecifies a particular personality, our method can still successfully alter the\nresponse personality of LLMs. Interestingly, the difficulty of converting\nbetween certain personality traits varies substantially, which aligns with the\nrepresentational distances in our probing experiments. Finally, we conduct a\ncomprehensive MMLU benchmark evaluation and time overhead analysis,\ndemonstrating that our proposed personality editing method incurs only minimal\ndegradation in general capabilities while maintaining low training costs and\nacceptable inference latency. Our code is publicly available at\nhttps://github.com/universe-sky/probing-then-editing-personality.", "AI": {"tldr": "This paper introduces a probing framework to investigate how Large Language Models (LLMs) simulate personality traits in their responses and proposes a method to edit those traits.", "motivation": "Despite LLMs' ability to simulate personality, little is known about how these traits are encoded within their parameters.", "method": "A layer-wise probing framework is utilized to explore the capabilities of 11 open-source LLMs in simulating personality, alongside a layer-wise perturbation method for editing personality traits during inference.", "result": "LLMs predominantly simulate personality in their middle and upper layers, with instruction-tuned models showing clearer separation of traits; the proposed personality editing method minimally impacts general capabilities and demonstrates successful personality alteration during inference.", "conclusion": "The proposed method for editing LLM personality traits is effective and does not significantly degrade performance, allowing for practical applications in human-computer interaction.", "key_contributions": ["Introduced a layer-wise probing framework for analyzing LLM personality simulation.", "Developed a layer-wise perturbation method for editing personality in LLM responses.", "Demonstrated that editing personality incurs minimal performance costs."], "limitations": "Focus on open-source LLMs may not generalize to proprietary models; the exploration is based only on the specified benchmark and may not capture all possible personality traits.", "keywords": ["Large Language Models", "Personality Simulation", "Layer-wise Probing", "Personality Editing", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.10861", "pdf": "https://arxiv.org/pdf/2504.10861.pdf", "abs": "https://arxiv.org/abs/2504.10861", "title": "Ai2 Scholar QA: Organized Literature Synthesis with Attribution", "authors": ["Amanpreet Singh", "Joseph Chee Chang", "Chloe Anastasiades", "Dany Haddad", "Aakanksha Naik", "Amber Tanaka", "Angele Zamarron", "Cecile Nguyen", "Jena D. Hwang", "Jason Dunkleberger", "Matt Latzke", "Smita Rao", "Jaron Lochner", "Rob Evans", "Rodney Kinney", "Daniel S. Weld", "Doug Downey", "Sergey Feldman"], "categories": ["cs.CL"], "comment": "7 pages", "summary": "Retrieval-augmented generation is increasingly effective in answering\nscientific questions from literature, but many state-of-the-art systems are\nexpensive and closed-source. We introduce Ai2 Scholar QA, a free online\nscientific question answering application. To facilitate research, we make our\nentire pipeline public: as a customizable open-source Python package and\ninteractive web app, along with paper indexes accessible through public APIs\nand downloadable datasets. We describe our system in detail and present\nexperiments analyzing its key design decisions. In an evaluation on a recent\nscientific QA benchmark, we find that Ai2 Scholar QA outperforms competing\nsystems.", "AI": {"tldr": "Ai2 Scholar QA is an open-source tool for scientific question answering that outperforms existing systems.", "motivation": "To provide an effective and accessible solution for answering scientific questions from literature, circumventing the costs and limitations of existing closed-source systems.", "method": "Developed as a customizable open-source Python package and interactive web app, with public APIs for paper indexes and downloadable datasets. Experiments were conducted to analyze key design decisions.", "result": "Ai2 Scholar QA outperforms competing systems on a scientific QA benchmark.", "conclusion": "The system's open-source nature allows for further research and customization, potentially enhancing scientific inquiry capabilities.", "key_contributions": ["Introduction of Ai2 Scholar QA as a free online scientific QA application", "Public availability of an open-source pipeline and datasets", "Performance improvement over existing scientific QA systems."], "limitations": "", "keywords": ["scientific question answering", "open-source", "retrieval-augmented generation", "AI", "benchmark"], "importance_score": 9, "read_time_minutes": 7}}
