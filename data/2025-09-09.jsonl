{"id": "2509.05298", "pdf": "https://arxiv.org/pdf/2509.05298.pdf", "abs": "https://arxiv.org/abs/2509.05298", "title": "Livia: An Emotion-Aware AR Companion Powered by Modular AI Agents and Progressive Memory Compression", "authors": ["Rui Xi", "Xianghan Wang"], "categories": ["cs.HC", "cs.AI", "cs.MM"], "comment": "Accepted to the Proceedings of the 2025 International Conference on\n  Artificial Intelligence and Virtual Reality (AIVR 2025). \\c{opyright} 2025\n  Springer. This is the author-accepted manuscript. Rui Xi and Xianghan Wang\n  contributed equally to this work. The final version will be available via\n  SpringerLink", "summary": "Loneliness and social isolation pose significant emotional and health\nchallenges, prompting the development of technology-based solutions for\ncompanionship and emotional support. This paper introduces Livia, an\nemotion-aware augmented reality (AR) companion app designed to provide\npersonalized emotional support by combining modular artificial intelligence\n(AI) agents, multimodal affective computing, progressive memory compression,\nand AR driven embodied interaction. Livia employs a modular AI architecture\nwith specialized agents responsible for emotion analysis, dialogue generation,\nmemory management, and behavioral orchestration, ensuring robust and adaptive\ninteractions. Two novel algorithms-Temporal Binary Compression (TBC) and\nDynamic Importance Memory Filter (DIMF)-effectively manage and prioritize\nlong-term memory, significantly reducing storage requirements while retaining\ncritical context. Our multimodal emotion detection approach achieves high\naccuracy, enhancing proactive and empathetic engagement. User evaluations\ndemonstrated increased emotional bonds, improved satisfaction, and\nstatistically significant reductions in loneliness. Users particularly valued\nLivia's adaptive personality evolution and realistic AR embodiment. Future\nresearch directions include expanding gesture and tactile interactions,\nsupporting multi-user experiences, and exploring customized hardware\nimplementations."}
{"id": "2509.05491", "pdf": "https://arxiv.org/pdf/2509.05491.pdf", "abs": "https://arxiv.org/abs/2509.05491", "title": "Hybrid User Interfaces: Past, Present, and Future of Complementary Cross-Device Interaction in Mixed Reality", "authors": ["Sebastian Hubenschmid", "Marc Satkowski", "Johannes Zagermann", "Julián Méndez", "Niklas Elmqvist", "Steven Feiner", "Tiara Feuchtner", "Jens Emil Grønbæk", "Benjamin Lee", "Dieter Schmalstieg", "Raimund Dachselt", "Harald Reiterer"], "categories": ["cs.HC"], "comment": "Submitted on the 26 February 2025 to the IEEE Transactions on\n  Visualization and Computer Graphics (TVCG) and awaiting Reviews", "summary": "We investigate hybrid user interfaces (HUIs), aiming to establish a cohesive\nunderstanding and adopt consistent terminology for this nascent research area.\nHUIs combine heterogeneous devices in complementary roles, leveraging the\ndistinct benefits of each. Our work focuses on cross-device interaction between\n2D devices and mixed reality environments, which are particularly compelling,\nleveraging the familiarity of traditional 2D platforms while providing spatial\nawareness and immersion. Although such HUIs have been prominently explored in\nthe context of mixed reality by prior work, we still lack a cohesive\nunderstanding of the unique design possibilities and challenges of such\ncombinations, resulting in a fragmented research landscape. We conducted a\nsystematic survey and present a taxonomy of HUIs that combine conventional\ndisplay technology and mixed reality environments. Based on this, we discuss\npast and current challenges, the evolution of definitions, and prospective\nopportunities to tie together the past 30 years of research with our vision of\nfuture HUIs."}
{"id": "2509.05619", "pdf": "https://arxiv.org/pdf/2509.05619.pdf", "abs": "https://arxiv.org/abs/2509.05619", "title": "GestoBrush: Facilitating Graffiti Artists' Digital Creation Experiences through Embodied AR Interactions", "authors": ["Ruiqi Chen", "Qingyang He", "Hanxi Bao", "Jung Choi", "Xin Tong"], "categories": ["cs.HC", "H.5.1; H.5.2; H.5.1; H.5.2; H.5.m"], "comment": "Accepted to VINCI 2025; 8 pages, 3 figures", "summary": "Graffiti has long documented the socio-cultural landscapes of urban spaces,\nyet increasing global regulations have constrained artists' creative freedom,\nprompting exploration of digital alternatives. Augmented Reality (AR) offers\nopportunities to extend graffiti into digital environments while retaining\nspatial and cultural significance, but prior research has largely centered on\naudience engagement rather than the embodied creative processes of graffiti\nartists. To address this, we developed GestoBrush, a mobile AR prototype that\nturns smartphones into virtual spray cans, enabling graffiti creation through\nembodied gestures. A co-design workshop underscored the role of\nembodiment-physical engagement with surroundings and body-driven creative\nprocesses-in digital workflows. We evaluated GestoBrush with six graffiti\nartists and findings suggested that embodied AR interactions supporting artists\nbypass real-world constraints and explore new artistic possibilities, whose AR\nartworks created enhanced senses of intuitiveness, immersion, and\nexpressiveness. This work highlight how embodied AR tools can bridge the gap\nbetween physical graffiti practice and digital expression, suggesting pathways\nfor designing immersive creative systems that respect the cultural ethos of\nstreet art while expanding its possibilities in virtual spaces."}
{"id": "2509.05718", "pdf": "https://arxiv.org/pdf/2509.05718.pdf", "abs": "https://arxiv.org/abs/2509.05718", "title": "Do Vision-Language Models See Visualizations Like Humans? Alignment in Chart Categorization", "authors": ["Péter Ferenc Gyarmati", "Manfred Klaffenböck", "Laura Koesten", "Torsten Möller"], "categories": ["cs.HC"], "comment": "2 pages, 2 figures. Accepted submission to the poster track of IEEE\n  VIS 2025", "summary": "Vision-language models (VLMs) hold promise for enhancing visualization tools,\nbut effective human-AI collaboration hinges on a shared perceptual\nunderstanding of visual content. Prior studies assessed VLM visualization\nliteracy through interpretive tasks, revealing an over-reliance on textual cues\nrather than genuine visual analysis. Our study investigates a more foundational\nskill underpinning such literacy: the ability of VLMs to recognize a chart's\ncore visual properties as humans do. We task 13 diverse VLMs with classifying\nscientific visualizations based solely on visual stimuli, according to three\ncriteria: purpose (e.g., schematic, GUI, visualization), encoding (e.g., bar,\npoint, node-link), and dimensionality (e.g., 2D, 3D). Using expert labels from\nthe human-centric VisType typology as ground truth, we find that VLMs often\nidentify purpose and dimensionality accurately but struggle with specific\nencoding types. Our preliminary results show that larger models do not always\nequate to superior performance and highlight the need for careful integration\nof VLMs in visualization tasks, with human supervision to ensure reliable\noutcomes."}
{"id": "2509.05359", "pdf": "https://arxiv.org/pdf/2509.05359.pdf", "abs": "https://arxiv.org/abs/2509.05359", "title": "An Empirical Analysis of Discrete Unit Representations in Speech Language Modeling Pre-training", "authors": ["Yanis Labrak", "Richard Dufour", "Mickaël Rouvier"], "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": "Published in International Conference on Text, Speech, and Dialogue,\n  13-24", "summary": "This paper investigates discrete unit representations in Speech Language\nModels (SLMs), focusing on optimizing speech modeling during continual\npre-training. In this paper, we systematically examine how model architecture,\ndata representation, and training robustness influence the pre-training stage\nin which we adapt existing pre-trained language models to the speech modality.\nOur experiments highlight the role of speech encoders and clustering\ngranularity across different model scales, showing how optimal discretization\nstrategies vary with model capacity. By examining cluster distribution and\nphonemic alignments, we investigate the effective use of discrete vocabulary,\nuncovering both linguistic and paralinguistic patterns. Additionally, we\nexplore the impact of clustering data selection on model robustness,\nhighlighting the importance of domain matching between discretization training\nand target applications."}
{"id": "2509.05721", "pdf": "https://arxiv.org/pdf/2509.05721.pdf", "abs": "https://arxiv.org/abs/2509.05721", "title": "A Composable Agentic System for Automated Visual Data Reporting", "authors": ["Péter Ferenc Gyarmati", "Dominik Moritz", "Torsten Möller", "Laura Koesten"], "categories": ["cs.HC"], "comment": null, "summary": "To address the brittleness of monolithic AI agents, our prototype for\nautomated visual data reporting explores a Human-AI Partnership model. Its\nhybrid, multi-agent architecture strategically externalizes logic from LLMs to\ndeterministic modules, leveraging the rule-based system Draco for principled\nvisualization design. The system delivers a dual-output: an interactive\nObservable report with Mosaic for reader exploration, and executable Marimo\nnotebooks for deep, analyst-facing traceability. This granular architecture\nyields a fully automatic yet auditable and steerable system, charting a path\ntoward a more synergistic partnership between human experts and AI. For\nreproducibility, our implementation and examples are available at\nhttps://peter-gy.github.io/VISxGenAI-2025/."}
{"id": "2509.05360", "pdf": "https://arxiv.org/pdf/2509.05360.pdf", "abs": "https://arxiv.org/abs/2509.05360", "title": "Beyond ROUGE: N-Gram Subspace Features for LLM Hallucination Detection", "authors": ["Jerry Li", "Evangelos Papalexakis"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated effectiveness across a wide\nvariety of tasks involving natural language, however, a fundamental problem of\nhallucinations still plagues these models, limiting their trustworthiness in\ngenerating consistent, truthful information. Detecting hallucinations has\nquickly become an important topic, with various methods such as uncertainty\nestimation, LLM Judges, retrieval augmented generation (RAG), and consistency\nchecks showing promise. Many of these methods build upon foundational metrics,\nsuch as ROUGE, BERTScore, or Perplexity, which often lack the semantic depth\nnecessary to detect hallucinations effectively. In this work, we propose a\nnovel approach inspired by ROUGE that constructs an N-Gram frequency tensor\nfrom LLM-generated text. This tensor captures richer semantic structure by\nencoding co-occurrence patterns, enabling better differentiation between\nfactual and hallucinated content. We demonstrate this by applying tensor\ndecomposition methods to extract singular values from each mode and use these\nas input features to train a multi-layer perceptron (MLP) binary classifier for\nhallucinations. Our method is evaluated on the HaluEval dataset and\ndemonstrates significant improvements over traditional baselines, as well as\ncompetitive performance against state-of-the-art LLM judges."}
{"id": "2509.05829", "pdf": "https://arxiv.org/pdf/2509.05829.pdf", "abs": "https://arxiv.org/abs/2509.05829", "title": "Augmenting Human-Centered Racial Covenant Detection and Georeferencing with Plug-and-Play NLP Pipelines", "authors": ["Jiyoon Pyo", "Yuankun Jiao", "Yao-Yi Chiang", "Michael Corey"], "categories": ["cs.HC"], "comment": null, "summary": "Though no longer legally enforceable, racial covenants in twentieth-century\nproperty deeds continue to shape spatial and socioeconomic inequalities.\nUnderstanding this legacy requires identifying racially restrictive language\nand geolocating affected properties. The Mapping Prejudice project addresses\nthis by engaging volunteers on the Zooniverse crowdsourcing platform to\ntranscribe covenants from scanned deeds and link them to modern parcel maps\nusing transcribed legal descriptions. While the project has explored\nautomation, it values crowdsourcing for its social impact and technical\nadvantages. Historically, Mapping Prejudice relied on lexicon-based searching\nand, more recently, fuzzy matching to flag suspected covenants. However, fuzzy\nmatching has increased false positives, burdening volunteers and raising\nscalability concerns. Additionally, while many properties can be mapped\nautomatically, others still require time-intensive manual geolocation.\n  We present a human-centered computing approach with two plug-and-play NLP\npipelines: (1) a context-aware text labeling model that flags racially\nrestrictive language with high precision and (2) a georeferencing module that\nextracts geographic descriptions from deeds and resolves them to real-world\nlocations. Evaluated on historical deed documents from six counties in\nMinnesota and Wisconsin, our system reduces false positives in racial term\ndetection by 25.96% while maintaining 91.73% recall and achieves 85.58%\ngeoreferencing accuracy within 1x1 square-mile ranges. These tools enhance\ndocument filtering and enrich spatial annotations, accelerating volunteer\nparticipation and reducing manual cleanup while strengthening public\nengagement."}
{"id": "2509.05385", "pdf": "https://arxiv.org/pdf/2509.05385.pdf", "abs": "https://arxiv.org/abs/2509.05385", "title": "A Lightweight Framework for Trigger-Guided LoRA-Based Self-Adaptation in LLMs", "authors": ["Jiacheng Wei", "Faguo Wu", "Xiao Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 7 figures, conference", "summary": "Large language models are unable to continuously adapt and learn from new\ndata during reasoning at inference time. To address this limitation, we propose\nthat complex reasoning tasks be decomposed into atomic subtasks and introduce\nSAGE, a trigger-guided dynamic fine-tuning framework that enables adaptive\nupdates during reasoning at inference time. SAGE consists of three key\ncomponents: (1) a Trigger module that detects reasoning failures through\nmultiple evaluation metrics in real time; (2) a Trigger Buffer module that\nclusters anomaly samples using a streaming clustering process with HDBSCAN,\nfollowed by stability checks and similarity-based merging; and (3) a Lora Store\nmodule that dynamically optimizes parameter updates with an adapter pool for\nknowledge retention. Evaluation results show that SAGE demonstrates excellent\naccuracy, robustness, and stability on the atomic reasoning subtask through\ndynamic knowledge updating during test time."}
{"id": "2509.05898", "pdf": "https://arxiv.org/pdf/2509.05898.pdf", "abs": "https://arxiv.org/abs/2509.05898", "title": "Attention, Action, and Memory: How Multi-modal Interfaces and Cognitive Load Alter Information Retention", "authors": ["Omar Elgohary", "Zhu-Tien"], "categories": ["cs.HC"], "comment": null, "summary": "Each year, multi-modal interaction continues to grow within both industry and\nacademia. However, researchers have yet to fully explore the impact of\nmulti-modal systems on learning and memory retention. This research\ninvestigates how combining gaze-based controls with gesture navigation affects\ninformation retention when compared to standard track-pad usage. A total of\ntwelve participants read four textual articles through two different user\ninterfaces which included a track-pad and a multi-modal interface that tracked\neye movements and hand gestures for scrolling, zooming, and revealing content.\nParticipants underwent two assessment sessions that measured their information\nretention immediately and after a twenty-four hour period along with the\nNASA-TLX workload evaluation and the System Usability Scale assessment. The\ninitial analysis indicates that multi-modal interaction produces similar\ntargeted information retention to traditional track-pad usage, but this neutral\neffect comes with higher cognitive workload demands and seems to deteriorate\nwith long-term retention. The research results provide new knowledge about how\nmulti-modal systems affect cognitive engagement while providing design\nrecommendations for future educational and assistive technologies that require\neffective memory performance."}
{"id": "2509.05396", "pdf": "https://arxiv.org/pdf/2509.05396.pdf", "abs": "https://arxiv.org/abs/2509.05396", "title": "Talk Isn't Always Cheap: Understanding Failure Modes in Multi-Agent Debate", "authors": ["Andrea Wynn", "Harsh Satija", "Gillian Hadfield"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "ICML MAS Workshop 2025", "summary": "While multi-agent debate has been proposed as a promising strategy for\nimproving AI reasoning ability, we find that debate can sometimes be harmful\nrather than helpful. The prior work has exclusively focused on debates within\nhomogeneous groups of agents, whereas we explore how diversity in model\ncapabilities influences the dynamics and outcomes of multi-agent interactions.\nThrough a series of experiments, we demonstrate that debate can lead to a\ndecrease in accuracy over time -- even in settings where stronger (i.e., more\ncapable) models outnumber their weaker counterparts. Our analysis reveals that\nmodels frequently shift from correct to incorrect answers in response to peer\nreasoning, favoring agreement over challenging flawed reasoning. These results\nhighlight important failure modes in the exchange of reasons during multi-agent\ndebate, suggesting that naive applications of debate may cause performance\ndegradation when agents are neither incentivized nor adequately equipped to\nresist persuasive but incorrect reasoning."}
{"id": "2509.05943", "pdf": "https://arxiv.org/pdf/2509.05943.pdf", "abs": "https://arxiv.org/abs/2509.05943", "title": "DRDCAE-STGNN: An End-to-End Discrimina-tive Autoencoder with Spatio-Temporal Graph Learning for Motor Imagery Classification", "authors": ["Yi Wang", "Haodong Zhang", "Hongqi Li"], "categories": ["cs.HC"], "comment": "Submit to IEEE Journal", "summary": "Motor imagery (MI) based brain-computer interfaces (BCIs) hold significant\npotential for assistive technologies and neurorehabilitation. However, the\nprecise and efficient decoding of MI remains challenging due to their\nnon-stationary nature and low signal-to-noise ratio. This paper introduces a\nnovel end-to-end deep learning framework of Discriminative Residual Dense\nConvolutional Autoencoder with Spatio-Temporal Graph Neural Network\n(DRDCAE-STGNN) to enhance the MI feature learning and classification.\nSpecifically, the DRDCAE module leverages residual-dense connections to learn\ndiscriminative latent representations through joint reconstruction and\nclassifica-tion, while the STGNN module captures dynamic spatial dependencies\nvia a learnable graph adjacency matrix and models temporal dynamics using\nbidirectional long short-term memory (LSTM). Extensive evaluations on BCI\nCompetition IV 2a, 2b, and PhysioNet datasets demonstrate state-of-the-art\nperformance, with average accuracies of 95.42%, 97.51%, and 90.15%,\nrespectively. Ablation studies confirm the contribution of each component, and\ninterpreta-bility analysis reveals neurophysiologically meaningful connectivity\npatterns. Moreover, despite its complexity, the model maintains a feasible\nparameter count and an inference time of 0.32 ms per sample. These results\nindicate that our method offers a robust, accurate, and interpretable solution\nfor MI-EEG decoding, with strong generalizability across subjects and tasks and\nmeeting the requirements for potential real-time BCI applications."}
{"id": "2509.05425", "pdf": "https://arxiv.org/pdf/2509.05425.pdf", "abs": "https://arxiv.org/abs/2509.05425", "title": "No Translation Needed: Forecasting Quality from Fertility and Metadata", "authors": ["Jessica M. Lundin", "Ada Zhang", "David Adelani", "Cody Carroll"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We show that translation quality can be predicted with surprising accuracy\n\\textit{without ever running the translation system itself}. Using only a\nhandful of features, token fertility ratios, token counts, and basic linguistic\nmetadata (language family, script, and region), we can forecast ChrF scores for\nGPT-4o translations across 203 languages in the FLORES-200 benchmark. Gradient\nboosting models achieve favorable performance ($R^{2}=0.66$ for\nXX$\\rightarrow$English and $R^{2}=0.72$ for English$\\rightarrow$XX). Feature\nimportance analyses reveal that typological factors dominate predictions into\nEnglish, while fertility plays a larger role for translations into diverse\ntarget languages. These findings suggest that translation quality is shaped by\nboth token-level fertility and broader linguistic typology, offering new\ninsights for multilingual evaluation and quality estimation."}
{"id": "2509.05961", "pdf": "https://arxiv.org/pdf/2509.05961.pdf", "abs": "https://arxiv.org/abs/2509.05961", "title": "A Longitudinal Evaluation of Heart Rate Efficiency for Amateur Runners", "authors": ["Evgeny V. Votyakov", "Marios Constantinides", "Fotis Liarokapis"], "categories": ["cs.HC"], "comment": "8 pages, 5 figures, 3 tables", "summary": "Amateur runners are increasingly using wearable devices to track their\ntraining, and often do so through simple metrics such as heart rate and pace.\nHowever, these metrics are typically analyzed in isolation and lack the\nexplainability needed for long-term self-monitoring. In this paper, we first\npresent Fitplotter, which is a client-side web application designed for the\nvisualization and analysis of data associated with fitness and activity\ntracking devices. Next, we revisited and formalized Heart Rate Efficiency\n(HRE), defined as the product of pace and heart rate, as a practical and\nexplainable metric to track aerobic fitness in everyday running. Drawing on\nmore than a decade of training data from one athlete, and supplemented by\npublicly available logs from twelve runners, we showed that HRE provides more\nstable and meaningful feedback on aerobic development than heart rate or pace\nalone. We showed that HRE correlates with training volume, reflects seasonal\nprogress, and remains stable during long runs in well-trained individuals. We\nalso discuss how HRE can support everyday training decisions, improve the user\nexperience in fitness tracking, and serve as an explainable metric to\nproprietary ones of commercial platforms. Our findings have implications for\ndesigning user-centered fitness tools that empower amateur athletes to\nunderstand and manage their own performance data."}
{"id": "2509.05440", "pdf": "https://arxiv.org/pdf/2509.05440.pdf", "abs": "https://arxiv.org/abs/2509.05440", "title": "Direct-Scoring NLG Evaluators Can Use Pairwise Comparisons Too", "authors": ["Logan Lawrence", "Ashton Williamson", "Alexander Shelton"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages, 18 tables, 1 figure", "summary": "As large-language models have been increasingly used as automatic raters for\nevaluating free-form content, including document summarization, dialog, and\nstory generation, work has been dedicated to evaluating such models by\nmeasuring their correlations with human judgment. For \\textit{sample-level}\nperformance, methods which operate by using pairwise comparisons between\nmachine-generated text perform well but often lack the ability to assign\nabsolute scores to individual summaries, an ability crucial for use cases that\nrequire thresholding. In this work, we propose a direct-scoring method which\nuses synthetic summaries to act as pairwise machine rankings at test time. We\nshow that our method performs comparably to state-of-the-art pairwise\nevaluators in terms of axis-averaged sample-level correlations on the SummEval\n(\\textbf{+0.03}), TopicalChat (\\textbf{-0.03}), and HANNA (\\textbf{+0.05})\nmeta-evaluation benchmarks, and release the synthetic in-context summaries as\ndata to facilitate future work."}
{"id": "2509.05962", "pdf": "https://arxiv.org/pdf/2509.05962.pdf", "abs": "https://arxiv.org/abs/2509.05962", "title": "The Reel Deal: Designing and Evaluating LLM-Generated Short-Form Educational Videos", "authors": ["Lazaros Stavrinou", "Argyris Constantinides", "Marios Belk", "Vasos Vassiliou", "Fotis Liarokapis", "Marios Constantinides"], "categories": ["cs.HC"], "comment": "9 pages, 3 figures, 1 table", "summary": "Short-form videos are gaining popularity in education due to their concise\nand accessible format that enables microlearning. Yet, most of these videos are\nmanually created. Even for those automatically generated using artificial\nintelligence (AI), it is not well understood whether or how they affect\nlearning outcomes, user experience, and trust. To address this gap, we\ndeveloped ReelsEd, which is a web-based system that uses large language models\n(LLMs) to automatically generate structured short-form video (i.e., reels) from\nlecture long-form videos while preserving instructor-authored material. In a\nbetween-subject user study with 62 university students, we evaluated ReelsEd\nand demonstrated that it outperformed traditional long-form videos in\nengagement, quiz performance, and task efficiency without increasing cognitive\nload. Learners expressed high trust in our system and valued its clarity,\nusefulness, and ease of navigation. Our findings point to new design\nopportunities for integrating generative AI into educational tools that\nprioritize usability, learner agency, and pedagogical alignment."}
{"id": "2509.05484", "pdf": "https://arxiv.org/pdf/2509.05484.pdf", "abs": "https://arxiv.org/abs/2509.05484", "title": "From Staff Messages to Actionable Insights: A Multi-Stage LLM Classification Framework for Healthcare Analytics", "authors": ["Hajar Sakai", "Yi-En Tseng", "Mohammadsadegh Mikaeili", "Joshua Bosire", "Franziska Jovin"], "categories": ["cs.CL"], "comment": null, "summary": "Hospital call centers serve as the primary contact point for patients within\na hospital system. They also generate substantial volumes of staff messages as\nnavigators process patient requests and communicate with the hospital offices\nfollowing the established protocol restrictions and guidelines. This\ncontinuously accumulated large amount of text data can be mined and processed\nto retrieve insights; however, traditional supervised learning approaches\nrequire annotated data, extensive training, and model tuning. Large Language\nModels (LLMs) offer a paradigm shift toward more computationally efficient\nmethodologies for healthcare analytics. This paper presents a multi-stage\nLLM-based framework that identifies staff message topics and classifies\nmessages by their reasons in a multi-class fashion. In the process, multiple\nLLM types, including reasoning, general-purpose, and lightweight models, were\nevaluated. The best-performing model was o3, achieving 78.4% weighted F1-score\nand 79.2% accuracy, followed closely by gpt-5 (75.3% Weighted F1-score and\n76.2% accuracy). The proposed methodology incorporates data security measures\nand HIPAA compliance requirements essential for healthcare environments. The\nprocessed LLM outputs are integrated into a visualization decision support tool\nthat transforms the staff messages into actionable insights accessible to\nhealthcare professionals. This approach enables more efficient utilization of\nthe collected staff messaging data, identifies navigator training\nopportunities, and supports improved patient experience and care quality."}
{"id": "2509.06114", "pdf": "https://arxiv.org/pdf/2509.06114.pdf", "abs": "https://arxiv.org/abs/2509.06114", "title": "Material Experience: An Evaluation Model for Creative Materials Based on Visual-Tactile Sensory Properties", "authors": ["Yuxin Zhang", "Fan Zhang", "Jinjun Xia", "Chao Zhao"], "categories": ["cs.HC"], "comment": null, "summary": "This study adopts a design-oriented approach to integrate traditional braids\nwith commonly used matrix materials, developing creative materials with\ndifferent sensory properties by altering matrix material types and braid\npatterns. Based on these creative materials, a quantitative and structured\nmodel is proposed to assist designers understanding the material experience\nprocess and guide material selection by analyzing the relationship between\nmaterial properties and sensory perception. Specifically, participants\nevaluated the creative materials under visual-tactile conditions using a\n7-point semantic differential (SD) scale. Correlation analysis was performed to\nexplore the data. The main and interaction effects of matrix materials and\nbraid patterns on impression evaluation were analyzed using two-way analysis of\nvariance (ANOVA). A structural equation model (SEM) was constructed based on\nexploratory factor analysis (EFA), and path coefficients were computed to\nassess the relative importance of material properties in determining material\nattractiveness. The results show that, compared to braids, the creative\nmaterials resulted in significant changes in impression evaluation.\nFurthermore, the creative materials can be understood through intrinsic,\naesthetic, and physical properties, with their standardized regression\ncoefficients for material attractiveness of 0.486, 0.650, and 0.103,\nrespectively. These properties are interrelated and under their combined\ninfluence affect the attractiveness of the material. Therefore, designers\nshould consider utilizing these relationships to enhance sensory experience in\norder to achieve design objectives. Moreover, designers should also consider\nbalancing technology and experience, using materials according to the principle\nof \"form follows function\"."}
{"id": "2509.05486", "pdf": "https://arxiv.org/pdf/2509.05486.pdf", "abs": "https://arxiv.org/abs/2509.05486", "title": "The Token Tax: Systematic Bias in Multilingual Tokenization", "authors": ["Jessica M. Lundin", "Ada Zhang", "Nihal Karim", "Hamza Louzan", "Victor Wei", "David Adelani", "Cody Carroll"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tokenization inefficiency imposes structural disadvantages on morphologically\ncomplex, low-resource languages, inflating compute resources and depressing\naccuracy. We evaluate 10 large language models (LLMs) on AfriMMLU (9,000 MCQA\nitems; 5 subjects; 16 African languages) and show that fertility (tokens/word)\nreliably predicts accuracy. Higher fertility consistently predicts lower\naccuracy across all models and subjects. We further find that reasoning models\n(DeepSeek, o1) consistently outperform non-reasoning peers across high and low\nresource languages in the AfriMMLU dataset, narrowing accuracy gaps observed in\nprior generations. Finally, translating token inflation to economics, a\ndoubling in tokens results in quadrupled training cost and time, underscoring\nthe token tax faced by many languages. These results motivate morphologically\naware tokenization, fair pricing, and multilingual benchmarks for equitable\nnatural language processing (NLP)."}
{"id": "2509.06382", "pdf": "https://arxiv.org/pdf/2509.06382.pdf", "abs": "https://arxiv.org/abs/2509.06382", "title": "Context-Adaptive Hearing Aid Fitting Advisor through Multi-turn Multimodal LLM Conversation", "authors": ["Yingke Ding", "Zeyu Wang", "Xiyuxing Zhang", "Hongbin Chen", "Zhenan Xu"], "categories": ["cs.HC"], "comment": "Ubicomp Companion 2025", "summary": "Traditional hearing aids often rely on static fittings that fail to adapt to\ntheir dynamic acoustic environments. We propose CAFA, a Context-Adaptive\nFitting Advisor that provides personalized, real-time hearing aid adjustments\nthrough a multi-agent Large Language Model (LLM) workflow. CAFA combines live\nambient audio, audiograms, and user feedback in a multi-turn conversational\nsystem. Ambient sound is classified into conversation, noise, or quiet with\n91.2\\% accuracy using a lightweight neural network based on YAMNet embeddings.\nThis system utilizes a modular LLM workflow, comprising context acquisition,\nsubproblem classification, strategy provision, and ethical regulation, and is\noverseen by an LLM Judge. The workflow translates context and feedback into\nprecise, safe tuning commands. Evaluation confirms that real-time sound\nclassification enhances conversational efficiency. CAFA exemplifies how\nagentic, multimodal AI can enable intelligent, user-centric assistive\ntechnologies."}
{"id": "2509.05505", "pdf": "https://arxiv.org/pdf/2509.05505.pdf", "abs": "https://arxiv.org/abs/2509.05505", "title": "Biomedical Literature Q&A System Using Retrieval-Augmented Generation (RAG)", "authors": ["Mansi Garg", "Lee-Chi Wang", "Bhavesh Ghanchi", "Sanjana Dumpala", "Shreyash Kakde", "Yen Chih Chen"], "categories": ["cs.CL", "cs.LG"], "comment": "10 pages, 6 figures, 3 tables", "summary": "This work presents a Biomedical Literature Question Answering (Q&A) system\nbased on a Retrieval-Augmented Generation (RAG) architecture, designed to\nimprove access to accurate, evidence-based medical information. Addressing the\nshortcomings of conventional health search engines and the lag in public access\nto biomedical research, the system integrates diverse sources, including PubMed\narticles, curated Q&A datasets, and medical encyclopedias ,to retrieve relevant\ninformation and generate concise, context-aware responses. The retrieval\npipeline uses MiniLM-based semantic embeddings and FAISS vector search, while\nanswer generation is performed by a fine-tuned Mistral-7B-v0.3 language model\noptimized using QLoRA for efficient, low-resource training. The system supports\nboth general medical queries and domain-specific tasks, with a focused\nevaluation on breast cancer literature demonstrating the value of\ndomain-aligned retrieval. Empirical results, measured using BERTScore (F1),\nshow substantial improvements in factual consistency and semantic relevance\ncompared to baseline models. The findings underscore the potential of\nRAG-enhanced language models to bridge the gap between complex biomedical\nliterature and accessible public health knowledge, paving the way for future\nwork on multilingual adaptation, privacy-preserving inference, and personalized\nmedical AI systems."}
{"id": "2509.06393", "pdf": "https://arxiv.org/pdf/2509.06393.pdf", "abs": "https://arxiv.org/abs/2509.06393", "title": "Talking to an AI Mirror: Designing Self-Clone Chatbots for Enhanced Engagement in Digital Mental Health Support", "authors": ["Mehrnoosh Sadat Shirvani", "Jackie Liu", "Thomas Chao", "Suky Martinez", "Laura Brandt", "Ig-Jae Kim", "Dongwook Yoon"], "categories": ["cs.HC"], "comment": null, "summary": "Mental health conversational agents have the potential to deliver valuable\ntherapeutic impact, but low user engagement remains a critical barrier\nhindering their efficacy. Existing therapeutic approaches have leveraged\nclients' internal dialogues (e.g., journaling, talking to an empty chair) to\nenhance engagement through accountable, self-sourced support. Inspired by\nthese, we designed novel AI-driven self-clone chatbots that replicate users'\nsupport strategies and conversational patterns to improve therapeutic\nengagement through externalized meaningful self-conversation. Validated through\na semi-controlled experiment (N=180), significantly higher emotional and\ncognitive engagement was demonstrated with self-clone chatbots than a chatbot\nwith a generic counselor persona. Our findings highlight self-clone\nbelievability as a mediator and emphasize the balance required in maintaining\nconvincing self-representation while creating positive interactions. This study\ncontributes to AI-based mental health interventions by introducing and\nevaluating self-clones as a promising approach to increasing user engagement,\nwhile exploring implications for their application in mental health care."}
{"id": "2509.05553", "pdf": "https://arxiv.org/pdf/2509.05553.pdf", "abs": "https://arxiv.org/abs/2509.05553", "title": "Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study", "authors": ["Serge Lionel Nikiema", "Jordan Samhi", "Micheline Bénédicte Moumoula", "Albérick Euraste Djiré", "Abdoul Kader Kaboré", "Jacques Klein", "Tegawendé F. Bissyandé"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This research addresses a fundamental question in AI: whether large language\nmodels truly understand concepts or simply recognize patterns. The authors\npropose bidirectional reasoning,the ability to apply transformations in both\ndirections without being explicitly trained on the reverse direction, as a test\nfor genuine understanding. They argue that true comprehension should naturally\nallow reversibility. For example, a model that can change a variable name like\nuserIndex to i should also be able to infer that i represents a user index\nwithout reverse training. The researchers tested current language models and\ndiscovered what they term cognitive specialization: when models are fine-tuned\non forward tasks, their performance on those tasks improves, but their ability\nto reason bidirectionally becomes significantly worse. To address this issue,\nthey developed Contrastive Fine-Tuning (CFT), which trains models using three\ntypes of examples: positive examples that maintain semantic meaning, negative\nexamples with different semantics, and forward-direction obfuscation examples.\nThis approach aims to develop deeper understanding rather than surface-level\npattern recognition and allows reverse capabilities to develop naturally\nwithout explicit reverse training. Their experiments demonstrated that CFT\nsuccessfully achieved bidirectional reasoning, enabling strong reverse\nperformance while maintaining forward task capabilities. The authors conclude\nthat bidirectional reasoning serves both as a theoretical framework for\nassessing genuine understanding and as a practical training approach for\ndeveloping more capable AI systems."}
{"id": "2509.06475", "pdf": "https://arxiv.org/pdf/2509.06475.pdf", "abs": "https://arxiv.org/abs/2509.06475", "title": "Explained, yet misunderstood: How AI Literacy shapes HR Managers' interpretation of User Interfaces in Recruiting Recommender Systems", "authors": ["Yannick Kalff", "Katharina Simbeck"], "categories": ["cs.HC", "cs.AI", "cs.CY", "A.0; H.5.2; I.2; J.1; K.4.2; K.4.3"], "comment": "Accepted paper for RecSys in HR'25: The 5th Workshop on Recommender\n  Systems for Human Resources, in conjunction with the 19th ACM Conference on\n  Recommender Systems, September 22--26, 2025, Prague, Czech Republic", "summary": "AI-based recommender systems increasingly influence recruitment decisions.\nThus, transparency and responsible adoption in Human Resource Management (HRM)\nare critical. This study examines how HR managers' AI literacy influences their\nsubjective perception and objective understanding of explainable AI (XAI)\nelements in recruiting recommender dashboards. In an online experiment, 410\nGerman-based HR managers compared baseline dashboards to versions enriched with\nthree XAI styles: important features, counterfactuals, and model criteria. Our\nresults show that the dashboards used in practice do not explain AI results and\neven keep AI elements opaque. However, while adding XAI features improves\nsubjective perceptions of helpfulness and trust among users with moderate or\nhigh AI literacy, it does not increase their objective understanding. It may\neven reduce accurate understanding, especially with complex explanations. Only\noverlays of important features significantly aided the interpretations of\nhigh-literacy users. Our findings highlight that the benefits of XAI in\nrecruitment depend on users' AI literacy, emphasizing the need for tailored\nexplanation strategies and targeted literacy training in HRM to ensure fair,\ntransparent, and effective adoption of AI."}
{"id": "2509.05566", "pdf": "https://arxiv.org/pdf/2509.05566.pdf", "abs": "https://arxiv.org/abs/2509.05566", "title": "Ad hoc conventions generalize to new referents", "authors": ["Anya Ji", "Claire Augusta Bergey", "Ron Eliav", "Yoav Artzi", "Robert D. Hawkins"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "How do people talk about things they've never talked about before? One view\nsuggests that a new shared naming system establishes an arbitrary link to a\nspecific target, like proper names that cannot extend beyond their bearers. An\nalternative view proposes that forming a shared way of describing objects\ninvolves broader conceptual alignment, reshaping each individual's semantic\nspace in ways that should generalize to new referents. We test these competing\naccounts in a dyadic communication study (N=302) leveraging the\nrecently-released KiloGram dataset containing over 1,000 abstract tangram\nimages. After pairs of participants coordinated on referential conventions for\none set of images through repeated communication, we measured the extent to\nwhich their descriptions aligned for undiscussed images. We found strong\nevidence for generalization: partners showed increased alignment relative to\ntheir pre-test labels. Generalization also decayed nonlinearly with visual\nsimilarity (consistent with Shepard's law) and was robust across levels of the\nimages' nameability. These findings suggest that ad hoc conventions are not\narbitrary labels but reflect genuine conceptual coordination, with implications\nfor theories of reference and the design of more adaptive language agents."}
{"id": "2509.06557", "pdf": "https://arxiv.org/pdf/2509.06557.pdf", "abs": "https://arxiv.org/abs/2509.06557", "title": "Mapping Community Appeals Systems: Lessons for Community-led Moderation in Multi-Level Governance", "authors": ["Juhoon Lee", "Bich Ngoc Doan", "Jonghyun Jee", "Joseph Seering"], "categories": ["cs.HC"], "comment": "Accepted for CSCW 2025", "summary": "Platforms are increasingly adopting industrial models of moderation that\nprioritize scalability and consistency, frequently at the expense of\ncontext-sensitive and user-centered values. Building on the multi-level\ngovernance framework that examines the interdependent relationship between\nplatforms and middle-level communities, we investigate community appeals\nsystems on Discord as a model for successful community-led governance. We\ninvestigate how Discord servers operationalize appeal systems through a\nqualitative interview study with focus groups and individual interviews with 17\ncommunity moderators. Our findings reveal a structured appeals process that\nbalances scalability, fairness, and accountability while upholding\ncommunity-centered values of growth and rehabilitation. Communities design\nthese processes to empower users, ensuring their voices are heard in moderation\ndecisions and fostering a sense of belonging. This research provides insights\ninto the practical implementation of community-led governance in a multi-level\ngovernance framework, illustrating how communities can maintain their core\nprinciples while integrating procedural fairness and tool-based design. We\ndiscuss how platforms can gain insights from community-led moderation work to\nmotivate governance structures that effectively balance and align the interests\nof multiple stakeholders."}
{"id": "2509.05602", "pdf": "https://arxiv.org/pdf/2509.05602.pdf", "abs": "https://arxiv.org/abs/2509.05602", "title": "Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought Correctness Perception Distillation", "authors": ["Hongyan Xie", "Yitong Yao", "Yikun Ban", "Zixuan Huang", "Deqing Wang", "Zhenhe Wu", "Haoxiang Su", "Chao Wang", "Shuangyong Song", "Xuelong Li"], "categories": ["cs.CL"], "comment": "PrePrint", "summary": "Large language models (LLMs) excel at reasoning tasks but are expensive to\ndeploy. Thus small language models (SLMs) are fine-tuned on CoT data generated\nby LLMs to copy LLMs' abilities. However, these CoT data may include noisy\nrationales that either fail to substantiate the answers or contribute no\nadditional information to support answer prediction, which leads SLMs to\ncapture spurious correlations between questions and answers and compromise the\nquality of reasoning. In this work, we propose Chain-of-Thought Correctness\nPerception Distillation (CoPeD), which aims to improve the reasoning quality of\nthe student model from the perspectives of task setting and data utilization.\nFirstly, we introduce a correctness-aware task setting that encourages the\nstudent model to predict answers based on correct rationales and revise them\nwhen they are incorrect. This setting improves the faithfulness of reasoning\nand allows the model to learn from its mistakes. Then, we propose a\nCorrectness-Aware Weighted loss, which dynamically adjusts the contribution of\neach training instance based on the combined loss of the rationale and the\nanswer. This strategy encourages the model to focus more on samples where the\nrationale offers stronger support for the correct answer. Experiments have\nshown that CoPeD is effective on both in-distribution (IND) and\nout-of-distribution (OOD) benchmark reasoning datasets."}
{"id": "2509.06776", "pdf": "https://arxiv.org/pdf/2509.06776.pdf", "abs": "https://arxiv.org/abs/2509.06776", "title": "Hue4U: Real-Time Personalized Color Correction in Augmented Reality", "authors": ["Jingwen Qin", "Semen Checherin", "Yue Li", "Berend-Jan van der Zwaag", "Özlem Durmaz-Incel"], "categories": ["cs.HC", "cs.MM"], "comment": null, "summary": "Color Vision Deficiency (CVD) affects nearly 8 percent of men and 0.5 percent\nof women worldwide. Existing color-correction methods often rely on prior\nclinical diagnosis and static filtering, making them less effective for users\nwith mild or moderate CVD. In this paper, we introduce Hue4U, a personalized,\nreal-time color-correction system in augmented reality using consumer-grade\nMeta Quest headsets. Unlike previous methods, Hue4U requires no prior medical\ndiagnosis and adapts to the user in real time. A user study with 10\nparticipants showed notable improvements in their ability to distinguish\ncolors. The results demonstrated large effect sizes (Cohen's d > 1.4),\nsuggesting clinically meaningful gains for individuals with CVD. These findings\nhighlight the potential of personalized AR interventions to improve visual\naccessibility and quality of life for people affected by CVD."}
{"id": "2509.05605", "pdf": "https://arxiv.org/pdf/2509.05605.pdf", "abs": "https://arxiv.org/abs/2509.05605", "title": "Icon$^{2}$: Aligning Large Language Models Using Self-Synthetic Preference Data via Inherent Regulation", "authors": ["Qiyuan Chen", "Hongsen Huang", "Qian Shao", "Jiahe Chen", "Jintai Chen", "Hongxia Xu", "Renjie Hua", "Ren Chuan", "Jian Wu"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 Main", "summary": "Large Language Models (LLMs) require high quality preference datasets to\nalign with human preferences. However, conventional methods for constructing\nsuch datasets face significant challenges: reliance on pre-collected\ninstructions often leads to distribution mismatches with target models, while\nthe need for sampling multiple stochastic responses introduces substantial\ncomputational overhead. In this work, we explore a paradigm shift by leveraging\ninherent regulation of LLMs' representation space for efficient and tailored\npreference dataset construction, named Icon$^{2}$. Specifically, it first\nextracts layer-wise direction vectors to encode sophisticated human preferences\nand then uses these vectors to filter self-synthesized instructions based on\ntheir inherent consistency. During decoding, bidirectional inherent control is\napplied to steer token representations, enabling the precise generation of\nresponse pairs with clear alignment distinctions. Experimental results\ndemonstrate significant improvements in both alignment and efficiency.\nLlama3-8B and Qwen2-7B achieve an average win rate improvement of 13.89% on\nAlpacaEval 2.0 and 13.45% on Arena-Hard, while reducing computational costs by\nup to 48.1%."}
{"id": "2509.06934", "pdf": "https://arxiv.org/pdf/2509.06934.pdf", "abs": "https://arxiv.org/abs/2509.06934", "title": "\"It was Tragic\": Exploring the Impact of a Robot's Shutdown", "authors": ["Agam Oberlender", "Hadas Erel"], "categories": ["cs.HC", "cs.RO"], "comment": "8 pages, 4 figures, 1 table, submitted to IEEE RO-MAN 2025", "summary": "It is well established that people perceive robots as social entities, even\nwhen they are not designed for social interaction. We evaluated whether the\nsocial interpretation of robotic gestures should also be considered when\nturning off a robot. In the experiment, participants engaged in a brief\npreliminary neutral interaction while a robotic arm showed interest in their\nactions. At the end of the task, participants were asked to turn off the\nrobotic arm under two conditions: (1) a Non-designed condition, where all of\nthe robot's engines were immediately and simultaneously turned off, as robots\ntypically shut down; (2) a Designed condition, where the robot's engines\ngradually folded inward in a motion resembling \"falling asleep.\" Our findings\nrevealed that all participants anthropomorphized the robot's movement when it\nwas turned off. In the Non-designed condition, most participants interpreted\nthe robot's turn-off movement negatively, as if the robot had \"died.\" In the\nDesigned condition, most participants interpreted it more neutrally, stating\nthat the robot \"went to sleep.\" The robot's turn-off movement also impacted its\nperception, leading to higher likeability, perceived intelligence, and animacy\nin the Designed condition. We conclude that the impact of common edge\ninteractions, such as turning off a robot, should be carefully designed while\nconsidering people's automatic tendency to perceive robots as social entities."}
{"id": "2509.05607", "pdf": "https://arxiv.org/pdf/2509.05607.pdf", "abs": "https://arxiv.org/abs/2509.05607", "title": "Beyond Keywords: Driving Generative Search Engine Optimization with Content-Centric Agents", "authors": ["Qiyuan Chen", "Jiahe Chen", "Hongsen Huang", "Qian Shao", "Jintai Chen", "Renjie Hua", "Hongxia Xu", "Ruijia Wu", "Ren Chuan", "Jian Wu"], "categories": ["cs.CL"], "comment": "Technical Report", "summary": "The paradigm shift from traditional ranked-based search to Generative Search\nEngines has rendered conventional SEO metrics obsolete, creating an urgent need\nto understand, measure, and optimize for content influence on synthesized\nanswers. This paper introduces a comprehensive, end-to-end framework for\nGenerative Search Engine Optimization (GSEO) to address this challenge. We make\ntwo primary contributions. First, we construct CC-GSEO-Bench, a large-scale,\ncontent-centric benchmark, and propose a multi-dimensional evaluation framework\nthat systematically quantifies influence, moving beyond surface-level\nattribution to assess substantive semantic impact. Second, we design a novel\nmulti-agent system that operationalizes this framework, automating the\nstrategic refinement of content through a collaborative analyze-revise-evaluate\nworkflow. Our empirical analysis using this framework reveals novel insights\ninto the dynamics of content influence, offering actionable strategies for\ncreators and establishing a principled foundation for future GSEO research."}
{"id": "2509.05317", "pdf": "https://arxiv.org/pdf/2509.05317.pdf", "abs": "https://arxiv.org/abs/2509.05317", "title": "VILOD: A Visual Interactive Labeling Tool for Object Detection", "authors": ["Isac Holm"], "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "comment": "Master's project", "summary": "The advancement of Object Detection (OD) using Deep Learning (DL) is often\nhindered by the significant challenge of acquiring large, accurately labeled\ndatasets, a process that is time-consuming and expensive. While techniques like\nActive Learning (AL) can reduce annotation effort by intelligently querying\ninformative samples, they often lack transparency, limit the strategic insight\nof human experts, and may overlook informative samples not aligned with an\nemployed query strategy. To mitigate these issues, Human-in-the-Loop (HITL)\napproaches integrating human intelligence and intuition throughout the machine\nlearning life-cycle have gained traction. Leveraging Visual Analytics (VA),\neffective interfaces can be created to facilitate this human-AI collaboration.\nThis thesis explores the intersection of these fields by developing and\ninvestigating \"VILOD: A Visual Interactive Labeling tool for Object Detection\".\nVILOD utilizes components such as a t-SNE projection of image features,\ntogether with uncertainty heatmaps and model state views. Enabling users to\nexplore data, interpret model states, AL suggestions, and implement diverse\nsample selection strategies within an iterative HITL workflow for OD. An\nempirical investigation using comparative use cases demonstrated how VILOD,\nthrough its interactive visualizations, facilitates the implementation of\ndistinct labeling strategies by making the model's state and dataset\ncharacteristics more interpretable (RQ1). The study showed that different\nvisually-guided labeling strategies employed within VILOD result in competitive\nOD performance trajectories compared to an automated uncertainty sampling AL\nbaseline (RQ2). This work contributes a novel tool and empirical insight into\nmaking the HITL-AL workflow for OD annotation more transparent, manageable, and\npotentially more effective."}
{"id": "2509.05609", "pdf": "https://arxiv.org/pdf/2509.05609.pdf", "abs": "https://arxiv.org/abs/2509.05609", "title": "New Insights into Optimal Alignment of Acoustic and Linguistic Representations for Knowledge Transfer in ASR", "authors": ["Xugang Lu", "Peng Shen", "Yu Tsao", "Hisashi Kawai"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Aligning acoustic and linguistic representations is a central challenge to\nbridge the pre-trained models in knowledge transfer for automatic speech\nrecognition (ASR). This alignment is inherently structured and asymmetric:\nwhile multiple consecutive acoustic frames typically correspond to a single\nlinguistic token (many-to-one), certain acoustic transition regions may relate\nto multiple adjacent tokens (one-to-many). Moreover, acoustic sequences often\ninclude frames with no linguistic counterpart, such as background noise or\nsilence may lead to imbalanced matching conditions. In this work, we take a new\ninsight to regard alignment and matching as a detection problem, where the goal\nis to identify meaningful correspondences with high precision and recall\nensuring full coverage of linguistic tokens while flexibly handling redundant\nor noisy acoustic frames in transferring linguistic knowledge for ASR. Based on\nthis new insight, we propose an unbalanced optimal transport-based alignment\nmodel that explicitly handles distributional mismatch and structural\nasymmetries with soft and partial matching between acoustic and linguistic\nmodalities. Our method ensures that every linguistic token is grounded in at\nleast one acoustic observation, while allowing for flexible, probabilistic\nmappings from acoustic to linguistic units. We evaluate our proposed model with\nexperiments on an CTC-based ASR system with a pre-trained language model for\nknowledge transfer. Experimental results demonstrate the effectiveness of our\napproach in flexibly controlling degree of matching and hence to improve ASR\nperformance."}
{"id": "2509.05391", "pdf": "https://arxiv.org/pdf/2509.05391.pdf", "abs": "https://arxiv.org/abs/2509.05391", "title": "Evaluating Magic Leap 2 Tool Tracking for AR Sensor Guidance in Industrial Inspections", "authors": ["Christian Masuhr", "Julian Koch", "Thorsten Schüppstuhl"], "categories": ["cs.RO", "cs.HC", "cs.MM"], "comment": null, "summary": "Rigorous evaluation of commercial Augmented Reality (AR) hardware is crucial,\nyet public benchmarks for tool tracking on modern Head-Mounted Displays (HMDs)\nare limited. This paper addresses this gap by systematically assessing the\nMagic Leap 2 (ML2) controllers tracking performance. Using a robotic arm for\nrepeatable motion (EN ISO 9283) and an optical tracking system as ground truth,\nour protocol evaluates static and dynamic performance under various conditions,\nincluding realistic paths from a hydrogen leak inspection use case. The results\nprovide a quantitative baseline of the ML2 controller's accuracy and\nrepeatability and present a robust, transferable evaluation methodology. The\nfindings provide a basis to assess the controllers suitability for the\ninspection use case and similar industrial sensor-based AR guidance tasks."}
{"id": "2509.05617", "pdf": "https://arxiv.org/pdf/2509.05617.pdf", "abs": "https://arxiv.org/abs/2509.05617", "title": "From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics", "authors": ["Shay Dahary", "Avi Edana", "Alexander Apartsin", "Yehudit Aperstein"], "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 2 figures", "summary": "The emotional content of song lyrics plays a pivotal role in shaping listener\nexperiences and influencing musical preferences. This paper investigates the\ntask of multi-label emotional attribution of song lyrics by predicting six\nemotional intensity scores corresponding to six fundamental emotions. A\nmanually labeled dataset is constructed using a mean opinion score (MOS)\napproach, which aggregates annotations from multiple human raters to ensure\nreliable ground-truth labels. Leveraging this dataset, we conduct a\ncomprehensive evaluation of several publicly available large language models\n(LLMs) under zero-shot scenarios. Additionally, we fine-tune a BERT-based model\nspecifically for predicting multi-label emotion scores. Experimental results\nreveal the relative strengths and limitations of zero-shot and fine-tuned\nmodels in capturing the nuanced emotional content of lyrics. Our findings\nhighlight the potential of LLMs for emotion recognition in creative texts,\nproviding insights into model selection strategies for emotion-based music\ninformation retrieval applications. The labeled dataset is available at\nhttps://github.com/LLM-HITCS25S/LyricsEmotionAttribution."}
{"id": "2509.05469", "pdf": "https://arxiv.org/pdf/2509.05469.pdf", "abs": "https://arxiv.org/abs/2509.05469", "title": "From Image Generation to Infrastructure Design: a Multi-agent Pipeline for Street Design Generation", "authors": ["Chenguang Wang", "Xiang Yan", "Yilong Dai", "Ziyi Wang", "Susu Xu"], "categories": ["cs.AI", "cs.CV", "cs.CY", "cs.HC"], "comment": "21 pages, 8 figures", "summary": "Realistic visual renderings of street-design scenarios are essential for\npublic engagement in active transportation planning. Traditional approaches are\nlabor-intensive, hindering collective deliberation and collaborative\ndecision-making. While AI-assisted generative design shows transformative\npotential by enabling rapid creation of design scenarios, existing generative\napproaches typically require large amounts of domain-specific training data and\nstruggle to enable precise spatial variations of design/configuration in\ncomplex street-view scenes. We introduce a multi-agent system that edits and\nredesigns bicycle facilities directly on real-world street-view imagery. The\nframework integrates lane localization, prompt optimization, design generation,\nand automated evaluation to synthesize realistic, contextually appropriate\ndesigns. Experiments across diverse urban scenarios demonstrate that the system\ncan adapt to varying road geometries and environmental conditions, consistently\nyielding visually coherent and instruction-compliant results. This work\nestablishes a foundation for applying multi-agent pipelines to transportation\ninfrastructure planning and facility design."}
{"id": "2509.05635", "pdf": "https://arxiv.org/pdf/2509.05635.pdf", "abs": "https://arxiv.org/abs/2509.05635", "title": "Few-Shot Query Intent Detection via Relation-Aware Prompt Learning", "authors": ["Liang Zhang", "Yuan Li", "Shijie Zhang", "Zheng Zhang", "Xitong Li"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Intent detection is a crucial component of modern conversational systems,\nsince accurately identifying user intent at the beginning of a conversation is\nessential for generating effective responses. Recent efforts have focused on\nstudying this problem under a challenging few-shot scenario. These approaches\nprimarily leverage large-scale unlabeled dialogue text corpora to pretrain\nlanguage models through various pretext tasks, followed by fine-tuning for\nintent detection with very limited annotations. Despite the improvements\nachieved, existing methods have predominantly focused on textual data,\nneglecting to effectively capture the crucial structural information inherent\nin conversational systems, such as the query-query relation and query-answer\nrelation. To address this gap, we propose SAID, a novel framework that\nintegrates both textual and relational structure information in a unified\nmanner for model pretraining for the first time. Building on this framework, we\nfurther propose a novel mechanism, the query-adaptive attention network\n(QueryAdapt), which operates at the relation token level by generating\nintent-specific relation tokens from well-learned query-query and query-answer\nrelations explicitly, enabling more fine-grained knowledge transfer. Extensive\nexperimental results on two real-world datasets demonstrate that SAID\nsignificantly outperforms state-of-the-art methods."}
{"id": "2509.05547", "pdf": "https://arxiv.org/pdf/2509.05547.pdf", "abs": "https://arxiv.org/abs/2509.05547", "title": "TeleopLab: Accessible and Intuitive Teleoperation of a Robotic Manipulator for Remote Labs", "authors": ["Ziling Chen", "Yeo Jung Yoon", "Rolando Bautista-Montesano", "Zhen Zhao", "Ajay Mandlekar", "John Liu"], "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "Teleoperation offers a promising solution for enabling hands-on learning in\nremote education, particularly in environments requiring interaction with\nreal-world equipment. However, such remote experiences can be costly or\nnon-intuitive. To address these challenges, we present TeleopLab, a mobile\ndevice teleoperation system that allows students to control a robotic arm and\noperate lab equipment. TeleopLab comprises a robotic arm, an adaptive gripper,\ncameras, lab equipment for a diverse range of applications, a user interface\naccessible through smartphones, and video call software. We conducted a user\nstudy, focusing on task performance, students' perspectives toward the system,\nusability, and workload assessment. Our results demonstrate a 46.1% reduction\nin task completion time as users gained familiarity with the system.\nQuantitative feedback highlighted improvements in students' perspectives after\nusing the system, while NASA TLX and SUS assessments indicated a manageable\nworkload of 38.2 and a positive usability of 73.8. TeleopLab successfully\nbridges the gap between physical labs and remote education, offering a scalable\nand effective platform for remote STEM learning."}
{"id": "2509.05657", "pdf": "https://arxiv.org/pdf/2509.05657.pdf", "abs": "https://arxiv.org/abs/2509.05657", "title": "LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding", "authors": ["Yuxuan Hu", "Jihao Liu", "Ke Wang", "Jinliang Zhen", "Weikang Shi", "Manyuan Zhang", "Qi Dou", "Rui Liu", "Aojun Zhou", "Hongsheng Li"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP2025", "summary": "Recent progress in Large Language Models (LLMs) has opened new avenues for\nsolving complex optimization problems, including Neural Architecture Search\n(NAS). However, existing LLM-driven NAS approaches rely heavily on prompt\nengineering and domain-specific tuning, limiting their practicality and\nscalability across diverse tasks. In this work, we propose LM-Searcher, a novel\nframework that leverages LLMs for cross-domain neural architecture optimization\nwithout the need for extensive domain-specific adaptation. Central to our\napproach is NCode, a universal numerical string representation for neural\narchitectures, which enables cross-domain architecture encoding and search. We\nalso reformulate the NAS problem as a ranking task, training LLMs to select\nhigh-performing architectures from candidate pools using instruction-tuning\nsamples derived from a novel pruning-based subspace sampling strategy. Our\ncurated dataset, encompassing a wide range of architecture-performance pairs,\nencourages robust and transferable learning. Comprehensive experiments\ndemonstrate that LM-Searcher achieves competitive performance in both in-domain\n(e.g., CNNs for image classification) and out-of-domain (e.g., LoRA\nconfigurations for segmentation and generation) tasks, establishing a new\nparadigm for flexible and generalizable LLM-based architecture search. The\ndatasets and models will be released at https://github.com/Ashone3/LM-Searcher."}
{"id": "2509.06069", "pdf": "https://arxiv.org/pdf/2509.06069.pdf", "abs": "https://arxiv.org/abs/2509.06069", "title": "From Digital Distrust to Codified Honesty: Experimental Evidence on Generative AI in Credence Goods Markets", "authors": ["Alexander Erlei"], "categories": ["econ.GN", "cs.HC", "q-fin.EC"], "comment": null, "summary": "Generative AI is transforming the provision of expert services. This article\nuses a series of one-shot experiments to quantify the behavioral, welfare and\ndistribution consequences of large language models (LLMs) on AI-AI,\nHuman-Human, Human-AI and Human-AI-Human expert markets. Using a credence goods\nframework where experts have private information about the optimal service for\nconsumers, we find that Human-Human markets generally achieve higher levels of\nefficiency than AI-AI and Human-AI markets through pro-social expert\npreferences and higher consumer trust. Notably, LLM experts still earn\nsubstantially higher surplus than human experts -- at the expense of consumer\nsurplus - suggesting adverse incentives that may spur the harmful deployment of\nLLMs. Concurrently, a majority of human experts chooses to rely on LLM agents\nwhen given the opportunity in Human-AI-Human markets, especially if they have\nagency over the LLM's (social) objective function. Here, a large share of\nexperts prioritizes efficiency-loving preferences over pure self-interest.\nDisclosing these preferences to consumers induces strong efficiency gains by\nmarginalizing self-interested LLM experts and human experts. Consequently,\nHuman-AI-Human markets outperform Human-Human markets under transparency rules.\nWith obfuscation, however, efficiency gains disappear, and adverse expert\nincentives remain. Our results shed light on the potential opportunities and\nrisks of disseminating LLMs in the context of expert services and raise several\nregulatory challenges. On the one hand, LLMs can negatively affect human trust\nin the presence of information asymmetries and partially crowd-out experts'\nother-regarding preferences through automation. On the other hand, LLMs allow\nexperts to codify and communicate their objective function, which reduces\ninformation asymmetries and increases efficiency."}
{"id": "2509.05660", "pdf": "https://arxiv.org/pdf/2509.05660.pdf", "abs": "https://arxiv.org/abs/2509.05660", "title": "Cross-Question Method Reuse in Large Language Models: From Word-Level Prediction to Rational Logical-Layer Reasoning", "authors": ["Hong Su"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have been widely applied to assist in finding\nsolutions for diverse questions. Prior work has proposed representing a method\nas a pair of a question and its corresponding solution, enabling method reuse.\nHowever, existing approaches typically require the questions to be highly\nsimilar. In this paper, we extend the scope of method reuse to address\nquestions with low similarity or with hidden similarities that are not\nexplicitly observable. For questions that are similar in a general-specific\nsense (i.e., broader or narrower in scope), we propose to first separate the\nquestion and solution, rather than directly feeding the pair to the LLM. The\nLLM is then guided to adapt the solution to new but related questions, allowing\nit to focus on solution transfer rather than question recognition. Furthermore,\nwe extend this approach to cases where questions only share partial features or\nhidden characteristics. This enables cross-question method reuse beyond\nconventional similarity constraints. Experimental verification shows that our\nscope-extension approach increases the probability of filtering out reusable\nsolutions, thereby improving the effectiveness of cross-question method reuse."}
{"id": "2509.06164", "pdf": "https://arxiv.org/pdf/2509.06164.pdf", "abs": "https://arxiv.org/abs/2509.06164", "title": "Benchmarking Gender and Political Bias in Large Language Models", "authors": ["Jinrui Yang", "Xudong Han", "Timothy Baldwin"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "The 8th International Conference on Natural Language and Speech\n  Processing (Oral)", "summary": "We introduce EuroParlVote, a novel benchmark for evaluating large language\nmodels (LLMs) in politically sensitive contexts. It links European Parliament\ndebate speeches to roll-call vote outcomes and includes rich demographic\nmetadata for each Member of the European Parliament (MEP), such as gender, age,\ncountry, and political group. Using EuroParlVote, we evaluate state-of-the-art\nLLMs on two tasks -- gender classification and vote prediction -- revealing\nconsistent patterns of bias. We find that LLMs frequently misclassify female\nMEPs as male and demonstrate reduced accuracy when simulating votes for female\nspeakers. Politically, LLMs tend to favor centrist groups while underperforming\non both far-left and far-right ones. Proprietary models like GPT-4o outperform\nopen-weight alternatives in terms of both robustness and fairness. We release\nthe EuroParlVote dataset, code, and demo to support future research on fairness\nand accountability in NLP within political contexts."}
{"id": "2509.05668", "pdf": "https://arxiv.org/pdf/2509.05668.pdf", "abs": "https://arxiv.org/abs/2509.05668", "title": "Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian", "authors": ["Michael Hoffmann", "Jophin John", "Stefan Schweter", "Gokul Ramakrishnan", "Hoi-Fong Mak", "Alice Zhang", "Dmitry Gaynullin", "Nicolay J. Hammer"], "categories": ["cs.CL", "cs.AI"], "comment": "Michael Hoffmann and Jophin John contributed equally to this work", "summary": "We present Llama-GENBA-10B, a trilingual foundation model addressing\nEnglish-centric bias in large language models. Built on Llama 3.1-8B and scaled\nto 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens\n(82B English, 82B German, and 80M Bavarian), balancing resources while\npreventing English dominance. Targeted at the German NLP community, the model\nalso promotes Bavarian as a low-resource language. Development tackled four\nchallenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2)\ncreating a unified tokenizer for English, German, and Bavarian, (3) optimizing\narchitecture and language-ratio hyperparameters for cross-lingual transfer, and\n(4) establishing the first standardized trilingual evaluation suite by\ntranslating German benchmarks into Bavarian. Evaluations show that\nLlama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned\nvariant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing\nitself as the best model in its class for this language, while also\noutperforming EuroLLM in English and matching its results in German. Training\non the Cerebras CS-2 demonstrated efficient large-scale multilingual\npretraining with documented energy use, offering a blueprint for inclusive\nfoundation models that integrate low-resource languages."}
{"id": "2509.06176", "pdf": "https://arxiv.org/pdf/2509.06176.pdf", "abs": "https://arxiv.org/abs/2509.06176", "title": "AI Governance in Higher Education: A course design exploring regulatory, ethical and practical considerations", "authors": ["Zsolt Almási", "Hannah Bleher", "Johannes Bleher", "Rozanne Tuesday Flores", "Guo Xuanyang", "Paweł Pujszo", "Raphaël Weuts"], "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.HC", "68T01, 68T20, 91-08, 97U50, 97B10", "I.2.0; K.4.1; K.4.2; K.3.2"], "comment": null, "summary": "As artificial intelligence (AI) systems permeate critical sectors, the need\nfor professionals who can address ethical, legal and governance challenges has\nbecome urgent. Current AI ethics education remains fragmented, often siloed by\ndiscipline and disconnected from practice. This paper synthesizes literature\nand regulatory developments to propose a modular, interdisciplinary curriculum\nthat integrates technical foundations with ethics, law and policy. We highlight\nrecurring operational failures in AI - bias, misspecified objectives,\ngeneralization errors, misuse and governance breakdowns - and link them to\npedagogical strategies for teaching AI governance. Drawing on perspectives from\nthe EU, China and international frameworks, we outline a semester plan that\nemphasizes integrated ethics, stakeholder engagement and experiential learning.\nThe curriculum aims to prepare students to diagnose risks, navigate regulation\nand engage diverse stakeholders, fostering adaptive and ethically grounded\nprofessionals for responsible AI governance."}
{"id": "2509.05691", "pdf": "https://arxiv.org/pdf/2509.05691.pdf", "abs": "https://arxiv.org/abs/2509.05691", "title": "Revealing the Numeracy Gap: An Empirical Investigation of Text Embedding Models", "authors": ["Ningyuan Deng", "Hanyu Duan", "Yixuan Tang", "Yi Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Text embedding models are widely used in natural language processing\napplications. However, their capability is often benchmarked on tasks that do\nnot require understanding nuanced numerical information in text. As a result,\nit remains unclear whether current embedding models can precisely encode\nnumerical content, such as numbers, into embeddings. This question is critical\nbecause embedding models are increasingly applied in domains where numbers\nmatter, such as finance and healthcare. For example, Company X's market share\ngrew by 2\\% should be interpreted very differently from Company X's market\nshare grew by 20\\%, even though both indicate growth in market share. This\nstudy aims to examine whether text embedding models can capture such nuances.\nUsing synthetic data in a financial context, we evaluate 13 widely used text\nembedding models and find that they generally struggle to capture numerical\ndetails accurately. Our further analyses provide deeper insights into embedding\nnumeracy, informing future research to strengthen embedding model-based NLP\nsystems with improved capacity for handling numerical content."}
{"id": "2509.06221", "pdf": "https://arxiv.org/pdf/2509.06221.pdf", "abs": "https://arxiv.org/abs/2509.06221", "title": "Beamforming-LLM: What, Where and When Did I Miss?", "authors": ["Vishal Choudhari"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "We present Beamforming-LLM, a system that enables users to semantically\nrecall conversations they may have missed in multi-speaker environments. The\nsystem combines spatial audio capture using a microphone array with\nretrieval-augmented generation (RAG) to support natural language queries such\nas, \"What did I miss when I was following the conversation on dogs?\"\nDirectional audio streams are separated using beamforming, transcribed with\nWhisper, and embedded into a vector database using sentence encoders. Upon\nreceiving a user query, semantically relevant segments are retrieved,\ntemporally aligned with non-attended segments, and summarized using a\nlightweight large language model (GPT-4o-mini). The result is a user-friendly\ninterface that provides contrastive summaries, spatial context, and timestamped\naudio playback. This work lays the foundation for intelligent auditory memory\nsystems and has broad applications in assistive technology, meeting\nsummarization, and context-aware personal spatial computing."}
{"id": "2509.05716", "pdf": "https://arxiv.org/pdf/2509.05716.pdf", "abs": "https://arxiv.org/abs/2509.05716", "title": "A Survey of the State-of-the-Art in Conversational Question Answering Systems", "authors": ["Manoj Madushanka Perera", "Adnan Mahmood", "Kasun Eranda Wijethilake", "Fahmida Islam", "Maryam Tahermazandarani", "Quan Z. Sheng"], "categories": ["cs.CL", "cs.AI"], "comment": "42 pages, 12 figures, 4 tables", "summary": "Conversational Question Answering (ConvQA) systems have emerged as a pivotal\narea within Natural Language Processing (NLP) by driving advancements that\nenable machines to engage in dynamic and context-aware conversations. These\ncapabilities are increasingly being applied across various domains, i.e.,\ncustomer support, education, legal, and healthcare where maintaining a coherent\nand relevant conversation is essential. Building on recent advancements, this\nsurvey provides a comprehensive analysis of the state-of-the-art in ConvQA.\nThis survey begins by examining the core components of ConvQA systems, i.e.,\nhistory selection, question understanding, and answer prediction, highlighting\ntheir interplay in ensuring coherence and relevance in multi-turn\nconversations. It further investigates the use of advanced machine learning\ntechniques, including but not limited to, reinforcement learning, contrastive\nlearning, and transfer learning to improve ConvQA accuracy and efficiency. The\npivotal role of large language models, i.e., RoBERTa, GPT-4, Gemini 2.0 Flash,\nMistral 7B, and LLaMA 3, is also explored, thereby showcasing their impact\nthrough data scalability and architectural advancements. Additionally, this\nsurvey presents a comprehensive analysis of key ConvQA datasets and concludes\nby outlining open research directions. Overall, this work offers a\ncomprehensive overview of the ConvQA landscape and provides valuable insights\nto guide future advancements in the field."}
{"id": "2509.06368", "pdf": "https://arxiv.org/pdf/2509.06368.pdf", "abs": "https://arxiv.org/abs/2509.06368", "title": "From Perception to Protection: A Developer-Centered Study of Security and Privacy Threats in Extended Reality (XR)", "authors": ["Kunlin Cai", "Jinghuai Zhang", "Ying Li", "Zhiyuan Wang", "Xun Chen", "Tianshi Li", "Yuan Tian"], "categories": ["cs.CR", "cs.HC"], "comment": "NDSS 2026", "summary": "The immersive nature of XR introduces a fundamentally different set of\nsecurity and privacy (S&P) challenges due to the unprecedented user\ninteractions and data collection that traditional paradigms struggle to\nmitigate. As the primary architects of XR applications, developers play a\ncritical role in addressing novel threats. However, to effectively support\ndevelopers, we must first understand how they perceive and respond to different\nthreats. Despite the growing importance of this issue, there is a lack of\nin-depth, threat-aware studies that examine XR S&P from the developers'\nperspective. To fill this gap, we interviewed 23 professional XR developers\nwith a focus on emerging threats in XR. Our study addresses two research\nquestions aiming to uncover existing problems in XR development and identify\nactionable paths forward.\n  By examining developers' perceptions of S&P threats, we found that: (1) XR\ndevelopment decisions (e.g., rich sensor data collection, user-generated\ncontent interfaces) are closely tied to and can amplify S&P threats, yet\ndevelopers are often unaware of these risks, resulting in cognitive biases in\nthreat perception; and (2) limitations in existing mitigation methods, combined\nwith insufficient strategic, technical, and communication support, undermine\ndevelopers' motivation, awareness, and ability to effectively address these\nthreats. Based on these findings, we propose actionable and stakeholder-aware\nrecommendations to improve XR S&P throughout the XR development process. This\nwork represents the first effort to undertake a threat-aware,\ndeveloper-centered study in the XR domain -- an area where the immersive,\ndata-rich nature of the XR technology introduces distinctive challenges."}
{"id": "2509.05719", "pdf": "https://arxiv.org/pdf/2509.05719.pdf", "abs": "https://arxiv.org/abs/2509.05719", "title": "Exploring Subjective Tasks in Farsi: A Survey Analysis and Evaluation of Language Models", "authors": ["Donya Rooein", "Flor Miriam Plaza-del-Arco", "Debora Nozza", "Dirk Hovy"], "categories": ["cs.CL"], "comment": null, "summary": "Given Farsi's speaker base of over 127 million people and the growing\navailability of digital text, including more than 1.3 million articles on\nWikipedia, it is considered a middle-resource language. However, this label\nquickly crumbles when the situation is examined more closely. We focus on three\nsubjective tasks (Sentiment Analysis, Emotion Analysis, and Toxicity Detection)\nand find significant challenges in data availability and quality, despite the\noverall increase in data availability. We review 110 publications on subjective\ntasks in Farsi and observe a lack of publicly available datasets. Furthermore,\nexisting datasets often lack essential demographic factors, such as age and\ngender, that are crucial for accurately modeling subjectivity in language. When\nevaluating prediction models using the few available datasets, the results are\nhighly unstable across both datasets and models. Our findings indicate that the\nvolume of data is insufficient to significantly improve a language's prospects\nin NLP."}
{"id": "2509.06502", "pdf": "https://arxiv.org/pdf/2509.06502.pdf", "abs": "https://arxiv.org/abs/2509.06502", "title": "FireRedChat: A Pluggable, Full-Duplex Voice Interaction System with Cascaded and Semi-Cascaded Implementations", "authors": ["Junjie Chen", "Yao Hu", "Junjie Li", "Kangyue Li", "Kun Liu", "Wenpeng Li", "Xu Li", "Ziyuan Li", "Feiyu Shen", "Xu Tang", "Manzhen Wei", "Yichen Wu", "Fenglong Xie", "Kaituo Xu", "Kun Xie"], "categories": ["cs.SD", "cs.HC"], "comment": "12 pages, 2 figures", "summary": "Full-duplex voice interaction allows users and agents to speak simultaneously\nwith controllable barge-in, enabling lifelike assistants and customer service.\nExisting solutions are either end-to-end, difficult to design and hard to\ncontrol, or modular pipelines governed by turn-taking controllers that ease\nupgrades and per-module optimization; however, prior modular frameworks depend\non non-open components and external providers, limiting holistic optimization.\nIn this work, we present a complete, practical full-duplex voice interaction\nsystem comprising a turn-taking controller, an interaction module, and a\ndialogue manager. The controller integrates streaming personalized VAD (pVAD)\nto suppress false barge-ins from noise and non-primary speakers, precisely\ntimestamp primary-speaker segments, and explicitly enable primary-speaker\nbarge-ins; a semantic end-of-turn detector improves stop decisions. It upgrades\nheterogeneous half-duplex pipelines, cascaded, semi-cascaded, and\nspeech-to-speech, to full duplex. Using internal models, we implement cascaded\nand semi-cascaded variants; the semi-cascaded one captures emotional and\nparalinguistic cues, yields more coherent responses, lowers latency and error\npropagation, and improves robustness. A dialogue manager extends capabilities\nvia tool invocation and context management. We also propose three system-level\nmetrics, barge-in, end-of-turn detection accuracy, and end-to-end latency, to\nassess naturalness, control accuracy, and efficiency. Experiments show fewer\nfalse interruptions, more accurate semantic ends, and lower latency approaching\nindustrial systems, enabling robust, natural, real-time full-duplex\ninteraction. Demos: https://fireredteam.github.io/demos/firered_chat."}
{"id": "2509.05729", "pdf": "https://arxiv.org/pdf/2509.05729.pdf", "abs": "https://arxiv.org/abs/2509.05729", "title": "QCSE: A Pretrained Quantum Context-Sensitive Word Embedding for Natural Language Processing", "authors": ["Charles M. Varmantchaonala", "Niclas GÖtting", "Nils-Erik SchÜtte", "Jean Louis E. K. Fendji", "Christopher Gies"], "categories": ["cs.CL"], "comment": null, "summary": "Quantum Natural Language Processing (QNLP) offers a novel approach to\nencoding and understanding the complexity of natural languages through the\npower of quantum computation. This paper presents a pretrained quantum\ncontext-sensitive embedding model, called QCSE, that captures context-sensitive\nword embeddings, leveraging the unique properties of quantum systems to learn\ncontextual relationships in languages. The model introduces quantum-native\ncontext learning, enabling the utilization of quantum computers for linguistic\ntasks. Central to the proposed approach are innovative context matrix\ncomputation methods, designed to create unique, representations of words based\non their surrounding linguistic context. Five distinct methods are proposed and\ntested for computing the context matrices, incorporating techniques such as\nexponential decay, sinusoidal modulation, phase shifts, and hash-based\ntransformations. These methods ensure that the quantum embeddings retain\ncontext sensitivity, thereby making them suitable for downstream language tasks\nwhere the expressibility and properties of quantum systems are valuable\nresources. To evaluate the effectiveness of the model and the associated\ncontext matrix methods, evaluations are conducted on both a Fulani corpus, a\nlow-resource African language, dataset of small size and an English corpus of\nslightly larger size. The results demonstrate that QCSE not only captures\ncontext sensitivity but also leverages the expressibility of quantum systems\nfor representing rich, context-aware language information. The use of Fulani\nfurther highlights the potential of QNLP to mitigate the problem of lack of\ndata for this category of languages. This work underscores the power of quantum\ncomputation in natural language processing (NLP) and opens new avenues for\napplying QNLP to real-world linguistic challenges across various tasks and\ndomains."}
{"id": "2509.06582", "pdf": "https://arxiv.org/pdf/2509.06582.pdf", "abs": "https://arxiv.org/abs/2509.06582", "title": "Co-Located VR with Hybrid SLAM-based HMD Tracking and Motion Capture Synchronization", "authors": ["Carlos A. Pinheiro de Sousa", "Niklas Gröne", "Mathias Günther", "Oliver Deussen"], "categories": ["cs.RO", "cs.HC"], "comment": "Accepted at the Gesellschaft f\\\"ur Informatik (GI) VR/AR Workshop\n  2025 (Lecture Notes in Informatics)", "summary": "We introduce a multi-user VR co-location framework that synchronizes users\nwithin a shared virtual environment aligned to physical space. Our approach\ncombines a motion capture system with SLAM-based inside-out tracking to deliver\nsmooth, high-framerate, low-latency performance. Previous methods either rely\non continuous external tracking, which introduces latency and jitter, or on\none-time calibration, which cannot correct drift over time. In contrast, our\napproach combines the responsiveness of local HMD SLAM tracking with the\nflexibility to realign to an external source when needed. It also supports\nreal-time pose sharing across devices, ensuring consistent spatial alignment\nand engagement between users. Our evaluation demonstrates that our framework\nachieves the spatial accuracy required for natural multi-user interaction while\noffering improved comfort, scalability, and robustness over existing co-located\nVR solutions."}
{"id": "2509.05741", "pdf": "https://arxiv.org/pdf/2509.05741.pdf", "abs": "https://arxiv.org/abs/2509.05741", "title": "Enhancing Factual Accuracy and Citation Generation in LLMs via Multi-Stage Self-Verification", "authors": ["Fernando Gabriela García", "Qiyang Shi", "Zilin Feng"], "categories": ["cs.CL"], "comment": null, "summary": "This research introduces VeriFact-CoT (Verified Factual Chain-of-Thought), a\nnovel method designed to address the pervasive issues of hallucination and the\nabsence of credible citation sources in Large Language Models (LLMs) when\ngenerating complex, fact-sensitive content. By incorporating a multi-stage\nmechanism of 'fact verification-reflection-citation integration,' VeriFact-CoT\nempowers LLMs to critically self-examine and revise their intermediate\nreasoning steps and final answers. This process significantly enhances the\nobjective accuracy, trustworthiness, and traceability of the generated outputs,\nmaking LLMs more reliable for applications demanding high fidelity such as\nscientific research, news reporting, and legal consultation."}
{"id": "2509.06770", "pdf": "https://arxiv.org/pdf/2509.06770.pdf", "abs": "https://arxiv.org/abs/2509.06770", "title": "Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM Prompting", "authors": ["Shashidhar Reddy Javaji", "Bhavul Gauri", "Zining Zhu"], "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) are now used in multi-turn workflows, but we\nstill lack a clear way to measure when iteration helps and when it hurts. We\npresent an evaluation framework for iterative refinement that spans ideation,\ncode, and math. Our protocol runs controlled 12-turn conversations per task,\nutilizing a variety of prompts ranging from vague ``improve it'' feedback to\ntargeted steering, and logs per-turn outputs. We score outcomes with\ndomain-appropriate checks (unit tests for code; answer-equivalence plus\nreasoning-soundness for math; originality and feasibility for ideation) and\ntrack turn-level behavior with three families of metrics: semantic movement\nacross turns, turn-to-turn change, and output size growth. Across models and\ntasks, gains are domain-dependent: they arrive early in ideas and code, but in\nmath late turns matter when guided by elaboration. After the first few turns,\nvague feedback often plateaus or reverses correctness, while targeted prompts\nreliably shift the intended quality axis (novelty vs. feasibility in ideation;\nspeed vs. readability in code; in math, elaboration outperforms exploration and\ndrives late-turn gains). We also observe consistent domain patterns: ideation\nmoves more in meaning across turns, code tends to grow in size with little\nsemantic change, and math starts fixed but can break that path with late,\nelaborative iteration.Together, the framework and metrics make iteration\nmeasurable and comparable across models, and signal when to steer, stop, or\nswitch strategies."}
{"id": "2509.05863", "pdf": "https://arxiv.org/pdf/2509.05863.pdf", "abs": "https://arxiv.org/abs/2509.05863", "title": "LatinX: Aligning a Multilingual TTS Model with Direct Preference Optimization", "authors": ["Luis Felipe Chary", "Miguel Arjona Ramirez"], "categories": ["cs.CL"], "comment": null, "summary": "We present LatinX, a multilingual text-to-speech (TTS) model for cascaded\nspeech-to-speech translation that preserves the source speaker's identity\nacross languages. LatinX is a 12-layer decoder-only Transformer trained in\nthree stages: (i) pre-training for text-to-audio mapping, (ii) supervised\nfine-tuning for zero-shot voice cloning, and (iii) alignment with Direct\nPreference Optimization (DPO) using automatically labeled pairs based on Word\nError Rate (WER) and speaker-similarity metrics. Trained on English and Romance\nlanguages with emphasis on Portuguese, LatinX with DPO consistently reduces WER\nand improves objective similarity over the fine-tuned baseline. Human\nevaluations further indicate stronger perceived speaker similarity than a\nstrong baseline (XTTSv2), revealing gaps between objective and subjective\nmeasures. We provide cross-lingual analyses and discuss balanced preference\nsignals and lower-latency architectures as future work."}
{"id": "2210.01242", "pdf": "https://arxiv.org/pdf/2210.01242.pdf", "abs": "https://arxiv.org/abs/2210.01242", "title": "The Effect of Warm-Glow on User Behavioral Intention to Adopt Technology: Extending the UTAUT2 Model", "authors": ["Antonios Saravanos", "Neil Stott", "Dongnanzi Zheng", "Stavros Zervoudakis"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "In this study, we enhance the Unified Theory of Acceptance and Use of\nTechnology (UTAUT2) by incorporating the warm-glow phenomenon to clarify its\nimpact on user decisions regarding the adoption of technology. We introduce two\nadditional constructs aimed at capturing both the external and internal aspects\nof warm-glow, thus creating what we refer to as the UTAUT2 + WG model. To\nevaluate the effectiveness of our model, we conducted an experimental study in\nwhich participants were presented with a scenario describing a hypothetical\ntechnology designed to evoke warm-glow sensations. Using the partial least\nsquares method, we analyzed the collected data to assess our expanded model.\nOur findings indicate that warm-glow significantly influences user behavior,\nwith the internal aspect having the strongest influence, followed by hedonic\nmotivation, performance expectancy, and finally the external aspect of\nwarm-glow. We conclude by discussing the implications of our research,\nacknowledging its limitations, and suggesting directions for future\nexploration."}
{"id": "2509.05867", "pdf": "https://arxiv.org/pdf/2509.05867.pdf", "abs": "https://arxiv.org/abs/2509.05867", "title": "ZhiFangDanTai: Fine-tuning Graph-based Retrieval-Augmented Generation Model for Traditional Chinese Medicine Formula", "authors": ["ZiXuan Zhang", "Bowen Hao", "Yingjie Li", "Hongzhi Yin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Traditional Chinese Medicine (TCM) formulas play a significant role in\ntreating epidemics and complex diseases. Existing models for TCM utilize\ntraditional algorithms or deep learning techniques to analyze formula\nrelationships, yet lack comprehensive results, such as complete formula\ncompositions and detailed explanations. Although recent efforts have used TCM\ninstruction datasets to fine-tune Large Language Models (LLMs) for explainable\nformula generation, existing datasets lack sufficient details, such as the\nroles of the formula's sovereign, minister, assistant, courier; efficacy;\ncontraindications; tongue and pulse diagnosis-limiting the depth of model\noutputs. To address these challenges, we propose ZhiFangDanTai, a framework\ncombining Graph-based Retrieval-Augmented Generation (GraphRAG) with LLM\nfine-tuning. ZhiFangDanTai uses GraphRAG to retrieve and synthesize structured\nTCM knowledge into concise summaries, while also constructing an enhanced\ninstruction dataset to improve LLMs' ability to integrate retrieved\ninformation. Furthermore, we provide novel theoretical proofs demonstrating\nthat integrating GraphRAG with fine-tuning techniques can reduce generalization\nerror and hallucination rates in the TCM formula task. Experimental results on\nboth collected and clinical datasets demonstrate that ZhiFangDanTai achieves\nsignificant improvements over state-of-the-art models. Our model is\nopen-sourced at https://huggingface.co/tczzx6/ZhiFangDanTai1.0."}
{"id": "2412.14209", "pdf": "https://arxiv.org/pdf/2412.14209.pdf", "abs": "https://arxiv.org/abs/2412.14209", "title": "Integrating Evidence into the Design of XAI and AI-based Decision Support Systems: A Means-End Framework for End-users in Construction", "authors": ["Peter E. D. Love", "Jane Matthews", "Weili Fang", "Hadi Mahamivanan"], "categories": ["cs.HC", "cs.AI"], "comment": "74 pages, 5 figures and 3 tables", "summary": "Explainable Artificial Intelligence seeks to make the reasoning processes of\nAI models transparent and interpretable, particularly in complex decision\nmaking environments. In the construction industry, where AI based decision\nsupport systems are increasingly adopted, limited attention has been paid to\nthe integration of supporting evidence that underpins the reliability and\naccountability of AI generated outputs. The absence of such evidence undermines\nthe validity of explanations and the trustworthiness of system recommendations.\nThis paper addresses this gap by introducing a theoretical, evidence based\nmeans end framework developed through a narrative review. The framework offers\nan epistemic foundation for designing XAI enabled DSS that generate meaningful\nexplanations tailored to users knowledge needs and decision contexts. It\nfocuses on evaluating the strength, relevance, and utility of different types\nof evidence supporting AI generated explanations. While developed with\nconstruction professionals as primary end users, the framework is also\napplicable to developers, regulators, and project managers with varying\nepistemic goals."}
{"id": "2509.05878", "pdf": "https://arxiv.org/pdf/2509.05878.pdf", "abs": "https://arxiv.org/abs/2509.05878", "title": "MedFactEval and MedAgentBrief: A Framework and Workflow for Generating and Evaluating Factual Clinical Summaries", "authors": ["François Grolleau", "Emily Alsentzer", "Timothy Keyes", "Philip Chung", "Akshay Swaminathan", "Asad Aali", "Jason Hom", "Tridu Huynh", "Thomas Lew", "April S. Liang", "Weihan Chu", "Natasha Z. Steele", "Christina F. Lin", "Jingkun Yang", "Kameron C. Black", "Stephen P. Ma", "Fateme N. Haredasht", "Nigam H. Shah", "Kevin Schulman", "Jonathan H. Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Evaluating factual accuracy in Large Language Model (LLM)-generated clinical\ntext is a critical barrier to adoption, as expert review is unscalable for the\ncontinuous quality assurance these systems require. We address this challenge\nwith two complementary contributions. First, we introduce MedFactEval, a\nframework for scalable, fact-grounded evaluation where clinicians define\nhigh-salience key facts and an \"LLM Jury\"--a multi-LLM majority vote--assesses\ntheir inclusion in generated summaries. Second, we present MedAgentBrief, a\nmodel-agnostic, multi-step workflow designed to generate high-quality, factual\ndischarge summaries. To validate our evaluation framework, we established a\ngold-standard reference using a seven-physician majority vote on\nclinician-defined key facts from inpatient cases. The MedFactEval LLM Jury\nachieved almost perfect agreement with this panel (Cohen's kappa=81%), a\nperformance statistically non-inferior to that of a single human expert\n(kappa=67%, P < 0.001). Our work provides both a robust evaluation framework\n(MedFactEval) and a high-performing generation workflow (MedAgentBrief),\noffering a comprehensive approach to advance the responsible deployment of\ngenerative AI in clinical workflows."}
{"id": "2501.13765", "pdf": "https://arxiv.org/pdf/2501.13765.pdf", "abs": "https://arxiv.org/abs/2501.13765", "title": "Understanding the Challenges of Maker Entrepreneurship", "authors": ["Natalie Friedman", "Alexandra Bremers", "Adelaide Nyanyo", "Ian Clark", "Yasmine Kotturi", "Laura Dabbish", "Wendy Ju", "Nikolas Martelaro"], "categories": ["cs.HC"], "comment": "29 pages, Accepted to PACMHCI (CSCW), CSCW198:29", "summary": "The maker movement embodies a resurgence in DIY creation, merging physical\ncraftsmanship and arts with digital technology support. However, mere\ntechnological skills and creativity are insufficient for economically and\npsychologically sustainable practice. By illuminating and smoothing the path\nfrom ``maker\" to ``maker entrepreneur,\" we can help broaden the viability of\nmaking as a livelihood. Our research centers on makers who design, produce, and\nsell physical goods. In this work, we explore the transition to\nentrepreneurship for these makers and how technology can facilitate this\ntransition online and offline. We present results from interviews with 20\nUSA-based maker entrepreneurs {(i.e., lamps, stickers)}, six creative service\nentrepreneurs {(i.e., photographers, fabrication)}, and seven support personnel\n(i.e., art curator, incubator director). Our findings reveal that many maker\nentrepreneurs 1) are makers first and entrepreneurs second; 2) struggle with\nbusiness logistics and learn business skills as they go; and 3) are motivated\nby non-monetary values. We discuss training and technology-based design\nimplications and opportunities for addressing challenges in developing\neconomically sustainable businesses around making."}
{"id": "2509.05882", "pdf": "https://arxiv.org/pdf/2509.05882.pdf", "abs": "https://arxiv.org/abs/2509.05882", "title": "Let's Roleplay: Examining LLM Alignment in Collaborative Dialogues", "authors": ["Abhijnan Nath", "Carine Graff", "Nikhil Krishnaswamy"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "As Large Language Models (LLMs) integrate into diverse workflows, they are\nincreasingly being considered \"collaborators\" with humans. If such AI\ncollaborators are to be reliable, their behavior over multiturn interactions\nmust be predictable, validated and verified before deployment. Common alignment\ntechniques are typically developed under simplified single-user settings and do\nnot account for the dynamics of long-horizon multiparty interactions. This\npaper examines how different alignment methods affect LLM agents' effectiveness\nas partners in multiturn, multiparty collaborations. We study this question\nthrough the lens of friction agents that intervene in group dialogues to\nencourage the collaborative group to slow down and reflect upon their reasoning\nfor deliberative decision-making. Using a roleplay methodology, we evaluate\ninterventions from differently-trained friction agents in collaborative task\nconversations. We propose a novel counterfactual evaluation framework that\nquantifies how friction interventions change the trajectory of group\ncollaboration and belief alignment. Our results show that a friction-aware\napproach significantly outperforms common alignment baselines in helping both\nconvergence to a common ground, or agreed-upon task-relevant propositions, and\ncorrectness of task outcomes."}
{"id": "2502.18658", "pdf": "https://arxiv.org/pdf/2502.18658.pdf", "abs": "https://arxiv.org/abs/2502.18658", "title": "Assistance or Disruption? Exploring and Evaluating the Design and Trade-offs of Proactive AI Programming Support", "authors": ["Kevin Pu", "Daniel Lazaro", "Ian Arawjo", "Haijun Xia", "Ziang Xiao", "Tovi Grossman", "Yan Chen"], "categories": ["cs.HC", "cs.AI", "cs.SE"], "comment": null, "summary": "AI programming tools enable powerful code generation, and recent prototypes\nattempt to reduce user effort with proactive AI agents, but their impact on\nprogramming workflows remains unexplored. We introduce and evaluate\nCodellaborator, a design probe LLM agent that initiates programming assistance\nbased on editor activities and task context. We explored three interface\nvariants to assess trade-offs between increasingly salient AI support:\nprompt-only, proactive agent, and proactive agent with presence and context\n(Codellaborator). In a within-subject study (N=18), we find that proactive\nagents increase efficiency compared to prompt-only paradigm, but also incur\nworkflow disruptions. However, presence indicators and interaction context\nsupport alleviated disruptions and improved users' awareness of AI processes.\nWe underscore trade-offs of Codellaborator on user control, ownership, and code\nunderstanding, emphasizing the need to adapt proactivity to programming\nprocesses. Our research contributes to the design exploration and evaluation of\nproactive AI systems, presenting design implications on AI-integrated\nprogramming workflow."}
{"id": "2509.05908", "pdf": "https://arxiv.org/pdf/2509.05908.pdf", "abs": "https://arxiv.org/abs/2509.05908", "title": "Enhancing the Robustness of Contextual ASR to Varying Biasing Information Volumes Through Purified Semantic Correlation Joint Modeling", "authors": ["Yue Gu", "Zhihao Du", "Ying Shi", "Shiliang Zhang", "Qian Chen", "Jiqing Han"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted by IEEE Transactions on Audio, Speech and Language\n  Processing, 2025 (https://ieeexplore.ieee.org/document/11150731). DOI:\n  10.1109/TASLPRO.2025.3606198", "summary": "Recently, cross-attention-based contextual automatic speech recognition (ASR)\nmodels have made notable advancements in recognizing personalized biasing\nphrases. However, the effectiveness of cross-attention is affected by\nvariations in biasing information volume, especially when the length of the\nbiasing list increases significantly. We find that, regardless of the length of\nthe biasing list, only a limited amount of biasing information is most relevant\nto a specific ASR intermediate representation. Therefore, by identifying and\nintegrating the most relevant biasing information rather than the entire\nbiasing list, we can alleviate the effects of variations in biasing information\nvolume for contextual ASR. To this end, we propose a purified semantic\ncorrelation joint modeling (PSC-Joint) approach. In PSC-Joint, we define and\ncalculate three semantic correlations between the ASR intermediate\nrepresentations and biasing information from coarse to fine: list-level,\nphrase-level, and token-level. Then, the three correlations are jointly modeled\nto produce their intersection, so that the most relevant biasing information\nacross various granularities is highlighted and integrated for contextual\nrecognition. In addition, to reduce the computational cost introduced by the\njoint modeling of three semantic correlations, we also propose a purification\nmechanism based on a grouped-and-competitive strategy to filter out irrelevant\nbiasing phrases. Compared with baselines, our PSC-Joint approach achieves\naverage relative F1 score improvements of up to 21.34% on AISHELL-1 and 28.46%\non KeSpeech, across biasing lists of varying lengths."}
{"id": "2503.15512", "pdf": "https://arxiv.org/pdf/2503.15512.pdf", "abs": "https://arxiv.org/abs/2503.15512", "title": "Beyond SHAP and Anchors: A large-scale experiment on how developers struggle to design meaningful end-user explanations", "authors": ["Zahra Abba Omar", "Nadia Nahar", "Jacob Tjaden", "Inès M. Gilles", "Fikir Mekonnen", "Jane Hsieh", "Christian Kästner", "Alka Menon"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Modern machine learning produces models that are impossible for users or\ndevelopers to fully understand--raising concerns about trust, oversight,\nsafety, and human dignity when they are integrated into software products.\nTransparency and explainability methods aim to provide some help in\nunderstanding models, but it remains challenging for developers to design\nexplanations that are understandable to target users and effective for their\npurpose. Emerging guidelines and regulations set goals but may not provide\neffective actionable guidance to developers. In a large-scale experiment with\n124 participants, we explored how developers approach providing end-user\nexplanations, including what challenges they face, and to what extent specific\npolicies can guide their actions. We investigated whether and how specific\nforms of policy guidance help developers design explanations and provide\nevidence for policy compliance for an ML-powered screening tool for diabetic\nretinopathy. Participants across the board struggled to produce quality\nexplanations and comply with the provided policies. Contrary to our\nexpectations, we found that the nature and specificity of policy guidance had\nlittle effect. We posit that participant noncompliance is in part due to a\nfailure to imagine and anticipate the needs of non-technical stakeholders.\nDrawing on cognitive process theory and the sociological imagination to\ncontextualize participants' failure, we recommend educational interventions."}
{"id": "2509.05915", "pdf": "https://arxiv.org/pdf/2509.05915.pdf", "abs": "https://arxiv.org/abs/2509.05915", "title": "Accelerating Large Language Model Inference via Early-Exiting Algorithms", "authors": ["Sangmin Bae"], "categories": ["cs.CL"], "comment": "PhD Dissertation", "summary": "Large language models have achieved remarkable capabilities, but their\npractical deployment is hindered by significant computational costs. While\nadaptive computation methods like early-exiting promise to reduce these costs,\nthey introduce a fundamental conflict: the per-token dynamism intended to save\ncomputation often creates system-level bottlenecks that can paradoxically\nreduce throughput in batched inference. This dissertation resolves this\nconflict by co-designing adaptive algorithms and model architectures to strike\nan optimal balance between dynamism and efficiency. To this end, our work first\naddresses critical sources of overhead in conventional early-exiting by\nproposing an efficient parallel decoding mechanism. We then show that deep\nparameter sharing provides an architectural foundation that not only yields\ncompact, parameter-efficient models but also inherently mitigates the critical\nsynchronization issues affecting dynamic inference. Finally, this work presents\na unified framework where lightweight routers are pretrained to dynamically\nassign an optimal recursion depth for each token. This approach establishes a\nnew Pareto frontier between efficiency and performance by effectively\noptimizing for both adaptive computation and parameter efficiency within a\nsingle model."}
{"id": "2505.24126", "pdf": "https://arxiv.org/pdf/2505.24126.pdf", "abs": "https://arxiv.org/abs/2505.24126", "title": "How Students (Really) Use ChatGPT: Uncovering Experiences Among Undergraduate Students", "authors": ["Tawfiq Ammari", "Meilun Chen", "S M Mehedi Zaman", "Kiran Garimella"], "categories": ["cs.HC"], "comment": null, "summary": "This study investigates how undergraduate students engage with ChatGPT in\nself-directed learning contexts. Analyzing naturalistic interaction logs, we\nidentify five dominant use categories of ChatGPT: information seeking, content\ngeneration, language refinement, metacognitive engagement, and conversational\nrepair. Behavioral modeling reveals that structured, goal-driven tasks like\ncoding, multiple-choice solving, and job application writing are strong\npredictors of continued use. Drawing on Self-Directed Learning (SDL) and the\nUses and Gratifications Theory (UGT), we show how students actively manage\nChatGPT's affordances and limitations through prompt adaptation, follow-ups,\nand emotional regulation. Rather than disengaging after breakdowns, students\noften persist through clarification and repair, treating the assistant as both\ntool and learning partner. We also offer design and policy recommendations to\nsupport transparent, responsive, and pedagogically grounded integration of\ngenerative AI in higher education."}
{"id": "2509.06065", "pdf": "https://arxiv.org/pdf/2509.06065.pdf", "abs": "https://arxiv.org/abs/2509.06065", "title": "KatotohananQA: Evaluating Truthfulness of Large Language Models in Filipino", "authors": ["Lorenzo Alfred Nery", "Ronald Dawson Catignas", "Thomas James Tiam-Lee"], "categories": ["cs.CL"], "comment": "14 pages, 1 figure, 9 tables, 1 listing. To appear in Proceedings of\n  NLPIR 2025", "summary": "Large Language Models (LLMs) achieve remarkable performance across various\ntasks, but their tendency to produce hallucinations limits reliable adoption.\nBenchmarks such as TruthfulQA have been developed to measure truthfulness, yet\nthey are primarily available in English, leaving a gap in evaluating LLMs in\nlow-resource languages. To address this, we present KatotohananQA, a Filipino\ntranslation of the TruthfulQA benchmark. Seven free-tier proprietary models\nwere assessed using a binary-choice framework. Findings show a significant\nperformance gap between English and Filipino truthfulness, with newer OpenAI\nmodels (GPT-5 and GPT-5 mini) demonstrating strong multilingual robustness.\nResults also reveal disparities across question characteristics, suggesting\nthat some question types, categories, and topics are less robust to\nmultilingual transfer which highlight the need for broader multilingual\nevaluation to ensure fairness and reliability in LLM usage."}
{"id": "2505.24246", "pdf": "https://arxiv.org/pdf/2505.24246.pdf", "abs": "https://arxiv.org/abs/2505.24246", "title": "Locating Risk: Task Designers and the Challenge of Risk Disclosure in RAI Content Work", "authors": ["Alice Qian", "Ryland Shaw", "Laura Dabbish", "Jina Suh", "Hong Shen"], "categories": ["cs.HC", "cs.CY"], "comment": "Under review at CSCW 2026", "summary": "As AI systems are increasingly tested and deployed in open-ended and\nhigh-stakes domains, crowd workers are often tasked with responsible AI (RAI)\ncontent work. These tasks include labeling violent content, moderating\ndisturbing text, or simulating harmful behavior for red teaming exercises to\nshape AI system behaviors. While prior efforts have highlighted the risks to\nworker well-being associated with RAI content work, far less attention has been\npaid to how these risks are communicated to workers. Existing transparency\nframeworks and guidelines such as model cards, datasheets, and crowdworksheets\nfocus on documenting model information and dataset collection processes, but\nthey overlook an important aspect of disclosing well-being risks to workers. In\nthe absence of standard workflows or clear guidance, the consistent application\nof content warnings, consent flows, or other forms of well-being risk\ndisclosure remain unclear. This study investigates how task designers approach\nrisk disclosure in crowdsourced RAI tasks. Drawing on interviews with 23 task\ndesigners across academic and industry sectors, we examine how well-being risk\nis recognized, interpreted, and communicated in practice. Our findings surface\na need to support task designers in identifying and communicating well-being\nrisk not only to support crowdworker well-being but also to strengthen the\nethical integrity and technical efficacy of AI development pipelines."}
{"id": "2509.06074", "pdf": "https://arxiv.org/pdf/2509.06074.pdf", "abs": "https://arxiv.org/abs/2509.06074", "title": "Multimodal Fine-grained Context Interaction Graph Modeling for Conversational Speech Synthesis", "authors": ["Zhenqi Jia", "Rui Liu", "Berrak Sisman", "Haizhou Li"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025", "summary": "Conversational Speech Synthesis (CSS) aims to generate speech with natural\nprosody by understanding the multimodal dialogue history (MDH). The latest work\npredicts the accurate prosody expression of the target utterance by modeling\nthe utterance-level interaction characteristics of MDH and the target\nutterance. However, MDH contains fine-grained semantic and prosody knowledge at\nthe word level. Existing methods overlook the fine-grained semantic and\nprosodic interaction modeling. To address this gap, we propose MFCIG-CSS, a\nnovel Multimodal Fine-grained Context Interaction Graph-based CSS system. Our\napproach constructs two specialized multimodal fine-grained dialogue\ninteraction graphs: a semantic interaction graph and a prosody interaction\ngraph. These two interaction graphs effectively encode interactions between\nword-level semantics, prosody, and their influence on subsequent utterances in\nMDH. The encoded interaction features are then leveraged to enhance synthesized\nspeech with natural conversational prosody. Experiments on the DailyTalk\ndataset demonstrate that MFCIG-CSS outperforms all baseline models in terms of\nprosodic expressiveness. Code and speech samples are available at\nhttps://github.com/AI-S2-Lab/MFCIG-CSS."}
{"id": "2506.14720", "pdf": "https://arxiv.org/pdf/2506.14720.pdf", "abs": "https://arxiv.org/abs/2506.14720", "title": "How Warm-Glow Alters the Usability of Technology", "authors": ["Antonios Saravanos"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "As technology increasingly aligns with users' personal values, traditional\nmodels of usability, focused on functionality and specifically effectiveness,\nefficiency, and satisfaction, may not fully capture how people perceive and\nevaluate it. This study investigates how the warm-glow phenomenon, the positive\nfeeling associated with doing good, shapes perceived usability. An experimental\napproach was taken in which participants evaluated a hypothetical technology\nunder conditions designed to evoke either the intrinsic (i.e., personal\nfulfillment) or extrinsic (i.e., social recognition) dimensions of warm-glow. A\nMultivariate Analysis of Variance as well as subsequent follow-up analyses\nrevealed that intrinsic warm-glow significantly enhances all dimensions of\nperceived usability, while extrinsic warm-glow selectively influences perceived\neffectiveness and satisfaction. These findings suggest that perceptions of\nusability extend beyond functionality and are shaped by how technology\nresonates with users' broader sense of purpose. We conclude by proposing that\ndesigners consider incorporating warm-glow into technology as a strategic\ndesign decision."}
{"id": "2509.06079", "pdf": "https://arxiv.org/pdf/2509.06079.pdf", "abs": "https://arxiv.org/abs/2509.06079", "title": "Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge", "authors": ["Hao Liang", "Ruitao Wu", "Bohan Zeng", "Junbo Niu", "Wentao Zhang", "Bin Dong"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Multimodal reasoning remains a fundamental challenge in artificial\nintelligence. Despite substantial advances in text-based reasoning, even\nstate-of-the-art models such as GPT-o3 struggle to maintain strong performance\nin multimodal scenarios. To address this gap, we introduce a caption-assisted\nreasoning framework that effectively bridges visual and textual modalities. Our\napproach achieved 1st place in the ICML 2025 AI for Math Workshop \\& Challenge\n2: SeePhys, highlighting its effectiveness and robustness. Furthermore, we\nvalidate its generalization on the MathVerse benchmark for geometric reasoning,\ndemonstrating the versatility of our method. Our code is publicly available at\nhttps://github.com/OpenDCAI/SciReasoner."}
{"id": "2506.24104", "pdf": "https://arxiv.org/pdf/2506.24104.pdf", "abs": "https://arxiv.org/abs/2506.24104", "title": "Bridging Service Design, Visualizations, and Visual Analytics in Healthcare Digital Twins: Challenges, Gaps, and Research Opportunities", "authors": ["Mariia Ershova", "Graziano Blasilli"], "categories": ["cs.HC"], "comment": "Accepted in VAHC 2025 (16th workshop on Visual Analytics in\n  Healthcare) - https://visualanalyticshealthcare.github.io/", "summary": "Digital twins (DT) are increasingly used in healthcare to model patients,\nprocesses, and physiological systems. While recent solutions leverage\nvisualization, visual analytics, and user interaction, these systems rarely\nincorporate structured service design methodologies. Bridging service design\nwith visual analytics and visualization can be valuable for the healthcare DT\ncommunity. This paper aims to introduce the service design discipline to\nvisualization researchers by framing this integration gap and suggesting\nresearch directions to enhance the real-world applicability of DT solutions."}
{"id": "2509.06100", "pdf": "https://arxiv.org/pdf/2509.06100.pdf", "abs": "https://arxiv.org/abs/2509.06100", "title": "Orthogonal Low-rank Adaptation in Lie Groups for Continual Learning of Large Language Models", "authors": ["Kefan Cao", "Shuaicheng Wu"], "categories": ["cs.CL"], "comment": "13 pages, 3 figures", "summary": "Large language models (LLMs) are prone to catastrophic forgetting in\nsequential multi-task settings. Parameter regularization methods such as O-LoRA\nand N-LoRA alleviate task interference by enforcing low-rank subspace\northogonality, but they overlook the fact that conventional additive\nfine-tuning disrupts the intrinsic geometric structure of LLM parameters,\nlimiting performance. Our key insight is that the parameter space of LLMs\npossesses a geometric structure, which must be preserved in addition to\nenforcing orthogonality. Based on this, we propose Orthogonal Low-rank\nAdaptation in Lie Groups (OLieRA), which introduces Lie group theory into LLM\nfine-tuning: leveraging multiplicative updates to preserve parameter geometry\nwhile applying orthogonality constraints to task subspaces. Experiments\ndemonstrate that OLieRA achieves state-of-the-art results on the Standard CL\nbenchmark and remains among the top-performing methods in the Large Number of\nTasks setting."}
{"id": "2507.11210", "pdf": "https://arxiv.org/pdf/2507.11210.pdf", "abs": "https://arxiv.org/abs/2507.11210", "title": "Role-Playing LLM-Based Multi-Agent Support Framework for Detecting and Addressing Family Communication Bias", "authors": ["Rushia Harada", "Yuken Kimura", "Keito Inoshita"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Well-being in family settings involves subtle psychological dynamics that\nconventional metrics often overlook. In particular, unconscious parental\nexpectations, termed ideal parent bias, can suppress children's emotional\nexpression and autonomy. This suppression, referred to as suppressed emotion,\noften stems from well-meaning but value-driven communication, which is\ndifficult to detect or address from outside the family. Focusing on these\nlatent dynamics, this study explores Large Language Model (LLM)-based support\nfor psychologically safe family communication. We constructed a Japanese\nparent-child dialogue corpus of 30 scenarios, each annotated with metadata on\nideal parent bias and suppressed emotion. Based on this corpus, we developed a\nRole-Playing LLM-based multi-agent dialogue support framework that analyzes\ndialogue and generates feedback. Specialized agents detect suppressed emotion,\ndescribe implicit ideal parent bias in parental speech, and infer contextual\nattributes such as the child's age and background. A meta-agent compiles these\noutputs into a structured report, which is then passed to five selected expert\nagents. These agents collaboratively generate empathetic and actionable\nfeedback through a structured four-step discussion process. Experiments show\nthat the system can detect categories of suppressed emotion with moderate\naccuracy and produce feedback rated highly in empathy and practicality.\nMoreover, simulated follow-up dialogues incorporating this feedback exhibited\nsigns of improved emotional expression and mutual understanding, suggesting the\nframework's potential in supporting positive transformation in family\ninteractions."}
{"id": "2509.06164", "pdf": "https://arxiv.org/pdf/2509.06164.pdf", "abs": "https://arxiv.org/abs/2509.06164", "title": "Benchmarking Gender and Political Bias in Large Language Models", "authors": ["Jinrui Yang", "Xudong Han", "Timothy Baldwin"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "The 8th International Conference on Natural Language and Speech\n  Processing (Oral)", "summary": "We introduce EuroParlVote, a novel benchmark for evaluating large language\nmodels (LLMs) in politically sensitive contexts. It links European Parliament\ndebate speeches to roll-call vote outcomes and includes rich demographic\nmetadata for each Member of the European Parliament (MEP), such as gender, age,\ncountry, and political group. Using EuroParlVote, we evaluate state-of-the-art\nLLMs on two tasks -- gender classification and vote prediction -- revealing\nconsistent patterns of bias. We find that LLMs frequently misclassify female\nMEPs as male and demonstrate reduced accuracy when simulating votes for female\nspeakers. Politically, LLMs tend to favor centrist groups while underperforming\non both far-left and far-right ones. Proprietary models like GPT-4o outperform\nopen-weight alternatives in terms of both robustness and fairness. We release\nthe EuroParlVote dataset, code, and demo to support future research on fairness\nand accountability in NLP within political contexts."}
{"id": "2508.19256", "pdf": "https://arxiv.org/pdf/2508.19256.pdf", "abs": "https://arxiv.org/abs/2508.19256", "title": "WeDesign: Generative AI-Facilitated Community Consultations for Urban Public Space Design", "authors": ["Rashid Mushkani", "Hugo Berard", "Shin Koseki"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Community consultations are integral to urban planning processes intended to\nincorporate diverse stakeholder perspectives. However, limited resources,\nvisual and spoken language barriers, and uneven power dynamics frequently\nconstrain inclusive decision-making. This paper examines how generative\ntext-to-image methods, specifically Stable Diffusion XL integrated into a\ncustom platform (WeDesign), may support equitable consultations. A half-day\nworkshop in Montreal involved five focus groups, each consisting of architects,\nurban designers, AI specialists, and residents from varied demographic groups.\nAdditional data was gathered through semi-structured interviews with six urban\nplanning professionals. Participants indicated that immediate visual outputs\nfacilitated creativity and dialogue, yet noted issues in visualizing specific\nneeds of marginalized groups, such as participants with reduced mobility,\naccurately depicting local architectural elements, and accommodating bilingual\nprompts. Participants recommended the development of an open-source platform\nincorporating in-painting tools, multilingual support, image voting\nfunctionalities, and preference indicators. The results indicate that\ngenerative AI can broaden participation and enable iterative interactions but\nrequires structured facilitation approaches. The findings contribute to\ndiscussions on generative AI's role and limitations in participatory urban\ndesign."}
{"id": "2509.06184", "pdf": "https://arxiv.org/pdf/2509.06184.pdf", "abs": "https://arxiv.org/abs/2509.06184", "title": "Understanding the Influence of Synthetic Data for Text Embedders", "authors": ["Jacob Mitchell Springer", "Vaibhav Adlakha", "Siva Reddy", "Aditi Raghunathan", "Marius Mosbach"], "categories": ["cs.CL"], "comment": "ACL Findings 2025", "summary": "Recent progress in developing general purpose text embedders has been driven\nby training on ever-growing corpora of synthetic LLM-generated data.\nNonetheless, no publicly available synthetic dataset exists, posing a barrier\nto studying its role for generalization. To address this issue, we first\nreproduce and publicly release the synthetic data proposed by Wang et al.\n(Mistral-E5). Our synthetic data is high quality and leads to consistent\nimprovements in performance. Next, we critically examine where exactly\nsynthetic data improves model generalization. Our analysis reveals that\nbenefits from synthetic data are sparse and highly localized to individual\ndatasets. Moreover, we observe trade-offs between the performance on different\ncategories and data that benefits one task, degrades performance on another.\nOur findings highlight the limitations of current synthetic data approaches for\nbuilding general-purpose embedders and challenge the notion that training on\nsynthetic data leads to more robust embedding models across tasks."}
{"id": "2509.02100", "pdf": "https://arxiv.org/pdf/2509.02100.pdf", "abs": "https://arxiv.org/abs/2509.02100", "title": "E-THER: A Multimodal Dataset for Empathic AI - Towards Emotional Mismatch Awareness", "authors": ["Sharjeel Tahir", "Judith Johnson", "Jumana Abu-Khalaf", "Syed Afaq Ali Shah"], "categories": ["cs.HC", "cs.CL"], "comment": "15 pages, 4 figures. Preprint", "summary": "A prevalent shortfall among current empathic AI systems is their inability to\nrecognize when verbal expressions may not fully reflect underlying emotional\nstates. This is because the existing datasets, used for the training of these\nsystems, focus on surface-level emotion recognition without addressing the\ncomplex verbal-visual incongruence (mismatch) patterns useful for empathic\nunderstanding. In this paper, we present E-THER, the first Person-Centered\nTherapy-grounded multimodal dataset with multidimensional annotations for\nverbal-visual incongruence detection, enabling training of AI systems that\ndevelop genuine rather than performative empathic capabilities. The annotations\nincluded in the dataset are drawn from humanistic approach, i.e., identifying\nverbal-visual emotional misalignment in client-counsellor interactions -\nforming a framework for training and evaluating AI on empathy tasks. Additional\nengagement scores provide behavioral annotations for research applications.\nNotable gains in empathic and therapeutic conversational qualities are observed\nin state-of-the-art vision-language models (VLMs), such as IDEFICS and\nVideoLLAVA, using evaluation metrics grounded in empathic and therapeutic\nprinciples. Empirical findings indicate that our incongruence-trained models\noutperform general-purpose models in critical traits, such as sustaining\ntherapeutic engagement, minimizing artificial or exaggerated linguistic\npatterns, and maintaining fidelity to PCT theoretical framework."}
{"id": "2509.06196", "pdf": "https://arxiv.org/pdf/2509.06196.pdf", "abs": "https://arxiv.org/abs/2509.06196", "title": "Augmented Fine-Tuned LLMs for Enhanced Recruitment Automation", "authors": ["Mohamed T. Younes", "Omar Walid", "Khaled Shaban", "Ali Hamdi", "Mai Hassan"], "categories": ["cs.CL"], "comment": "Accepted in AICCSA 2025", "summary": "This paper presents a novel approach to recruitment automation. Large\nLanguage Models (LLMs) were fine-tuned to improve accuracy and efficiency.\nBuilding upon our previous work on the Multilayer Large Language Model-Based\nRobotic Process Automation Applicant Tracking (MLAR) system . This work\nintroduces a novel methodology. Training fine-tuned LLMs specifically tuned for\nrecruitment tasks. The proposed framework addresses the limitations of generic\nLLMs by creating a synthetic dataset that uses a standardized JSON format. This\nhelps ensure consistency and scalability. In addition to the synthetic data\nset, the resumes were parsed using DeepSeek, a high-parameter LLM. The resumes\nwere parsed into the same structured JSON format and placed in the training\nset. This will help improve data diversity and realism. Through\nexperimentation, we demonstrate significant improvements in performance\nmetrics, such as exact match, F1 score, BLEU score, ROUGE score, and overall\nsimilarity compared to base models and other state-of-the-art LLMs. In\nparticular, the fine-tuned Phi-4 model achieved the highest F1 score of 90.62%,\nindicating exceptional precision and recall in recruitment tasks. This study\nhighlights the potential of fine-tuned LLMs. Furthermore, it will revolutionize\nrecruitment workflows by providing more accurate candidate-job matching."}
{"id": "2509.05166", "pdf": "https://arxiv.org/pdf/2509.05166.pdf", "abs": "https://arxiv.org/abs/2509.05166", "title": "Transition of car-based human-mobility in the pandemic era: Data insight from a cross-border region in Europe", "authors": ["Sujit Kumar Sikder", "Jyotirmaya Ijaradar", "Hao Li", "Hichem Omrani"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Many transport authorities are collecting and publishing almost real-time\nroad traffic data to meet the growing trend of massive open data, a vital\nresource for foresight decision support systems considering deep data insights.\nWe explored the spatio-temporal transitions in the cross-country road traffic\nvolumes in the context of modelling behavioural transitions in car-based human\nmobility. This study reports on individual car-based daily travel behaviour\ndetected, before (2018) and during the COVID pandemic (2020), between Germany\nand neighbouring countries. In the case of Luxembourg, the Bridges and Roads\nAuthority has installed a large digital traffic observatory infrastructure\nthrough the adoption of sensor-based IoT technologies, like other European\nmember states. Since 2016, they have provided high-performance data processing\nand published open data on the country's road traffic. The dataset contains an\nhourly traffic count for different vehicle types, daily for representative\nobservation points, followed by a major road network. The original dataset\ncontains significant missing entries, so comprehensive data harmonization was\nperformed. We observed the decrease in traffic volumes during pandemic factors\n(e.g. lockdowns and remote work) period by following global trend of reduced\npersonal mobility. The understanding the dynamic adaptive travel behaviours\nprovide a potential opportunity to generate the actionable insight including\ntemporal and spatial implications. This study demonstrates that the national\nopen traffic data products can have adoption potential to address cross-border\ninsights. In relevance to the net-zero carbon transition, further study should\nshed light on the interpolation and downscaling approaches at the comprehensive\nroad-network level for identifying pollution hot spots, causal link to\nfunctional landuse patterns and calculation of spatial influence area."}
{"id": "2509.06200", "pdf": "https://arxiv.org/pdf/2509.06200.pdf", "abs": "https://arxiv.org/abs/2509.06200", "title": "MSLEF: Multi-Segment LLM Ensemble Finetuning in Recruitment", "authors": ["Omar Walid", "Mohamed T. Younes", "Khaled Shaban", "Mai Hassan", "Ali Hamdi"], "categories": ["cs.CL"], "comment": "Accepted in AICCSA 2025", "summary": "This paper presents MSLEF, a multi-segment ensemble framework that employs\nLLM fine-tuning to enhance resume parsing in recruitment automation. It\nintegrates fine-tuned Large Language Models (LLMs) using weighted voting, with\neach model specializing in a specific resume segment to boost accuracy.\nBuilding on MLAR , MSLEF introduces a segment-aware architecture that leverages\nfield-specific weighting tailored to each resume part, effectively overcoming\nthe limitations of single-model systems by adapting to diverse formats and\nstructures. The framework incorporates Gemini-2.5-Flash LLM as a high-level\naggregator for complex sections and utilizes Gemma 9B, LLaMA 3.1 8B, and Phi-4\n14B. MSLEF achieves significant improvements in Exact Match (EM), F1 score,\nBLEU, ROUGE, and Recruitment Similarity (RS) metrics, outperforming the best\nsingle model by up to +7% in RS. Its segment-aware design enhances\ngeneralization across varied resume layouts, making it highly adaptable to\nreal-world hiring scenarios while ensuring precise and reliable candidate\nrepresentation."}
{"id": "2501.09751", "pdf": "https://arxiv.org/pdf/2501.09751.pdf", "abs": "https://arxiv.org/abs/2501.09751", "title": "OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking", "authors": ["Zekun Xi", "Wenbiao Yin", "Jizhan Fang", "Jialong Wu", "Runnan Fang", "Jiang Yong", "Pengjun Xie", "Fei Huang", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR", "cs.LG"], "comment": "EMNLP 2025", "summary": "Machine writing with large language models often relies on\nretrieval-augmented generation. However, these approaches remain confined\nwithin the boundaries of the model's predefined scope, limiting the generation\nof content with rich information. Specifically, vanilla-retrieved information\ntends to lack depth, novelty, and suffers from redundancy, which negatively\nimpacts the quality of generated articles, leading to shallow, unoriginal, and\nrepetitive outputs. To address these issues, we propose OmniThink, a\nslow-thinking machine writing framework that emulates the human-like process of\niterative expansion and reflection. The core idea behind OmniThink is to\nsimulate the cognitive behavior of learners as they slowly deepen their\nknowledge of the topics. Experimental results demonstrate that OmniThink\nimproves the knowledge density of generated articles without compromising\nmetrics such as coherence and depth. Human evaluations and expert feedback\nfurther highlight the potential of OmniThink to address real-world challenges\nin the generation of long-form articles. Code is available at\nhttps://github.com/zjunlp/OmniThink."}
{"id": "2509.06277", "pdf": "https://arxiv.org/pdf/2509.06277.pdf", "abs": "https://arxiv.org/abs/2509.06277", "title": "No Encore: Unlearning as Opt-Out in Music Generation", "authors": ["Jinju Kim", "Taehan Kim", "Abdul Waheed", "Rita Singh"], "categories": ["cs.CL"], "comment": "Work in progress. 7 pages", "summary": "AI music generation is rapidly emerging in the creative industries, enabling\nintuitive music generation from textual descriptions. However, these systems\npose risks in exploitation of copyrighted creations, raising ethical and legal\nconcerns. In this paper, we present preliminary results on the first\napplication of machine unlearning techniques from an ongoing research to\nprevent inadvertent usage of creative content. Particularly, we explore\nexisting methods in machine unlearning to a pre-trained Text-to-Music (TTM)\nbaseline and analyze their efficacy in unlearning pre-trained datasets without\nharming model performance. Through our experiments, we provide insights into\nthe challenges of applying unlearning in music generation, offering a\nfoundational analysis for future works on the application of unlearning for\nmusic generative models."}
{"id": "2504.02124", "pdf": "https://arxiv.org/pdf/2504.02124.pdf", "abs": "https://arxiv.org/abs/2504.02124", "title": "Are Users More Willing to Use Formally Verified Password Managers?", "authors": ["Carolina Carreira", "João F. Ferreira", "Alexandra Mendes", "Nicolas Christin"], "categories": ["cs.CR", "cs.HC", "cs.LO", "cs.SE", "D.3; F.4; H.5; I.7; E.4"], "comment": null, "summary": "Formal verification has recently been increasingly used to prove the\ncorrectness and security of many applications. It is attractive because it can\nprove the absence of errors with the same certainty as mathematicians proving\ntheorems. However, while most security experts recognize the value of formal\nverification, the views of non-technical users on this topic are unknown. To\naddress this issue, we designed and implemented two experiments to understand\nhow formal verification impacts users. Our approach started with a formative\nstudy involving 15 participants, followed by the main quantitative study with\n200 individuals. We focus on the application domain of password managers since\nit has been documented that the lack of trust in password managers might lead\nto lower adoption. Moreover, recent efforts have focused on formally verifying\n(parts of) password managers. We conclude that formal verification is seen as\ndesirable by users and identify three actional recommendations to improve\nformal verification communication efforts."}
{"id": "2509.06350", "pdf": "https://arxiv.org/pdf/2509.06350.pdf", "abs": "https://arxiv.org/abs/2509.06350", "title": "Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?", "authors": ["Junjie Mu", "Zonghao Ying", "Zhekui Fan", "Zonglei Jing", "Yaoyuan Zhang", "Zhengmin Yu", "Wenxin Zhang", "Quanchen Zou", "Xiangzheng Zhang"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "Jailbreak attacks on Large Language Models (LLMs) have demonstrated various\nsuccessful methods whereby attackers manipulate models into generating harmful\nresponses that they are designed to avoid. Among these, Greedy Coordinate\nGradient (GCG) has emerged as a general and effective approach that optimizes\nthe tokens in a suffix to generate jailbreakable prompts. While several\nimproved variants of GCG have been proposed, they all rely on fixed-length\nsuffixes. However, the potential redundancy within these suffixes remains\nunexplored. In this work, we propose Mask-GCG, a plug-and-play method that\nemploys learnable token masking to identify impactful tokens within the suffix.\nOur approach increases the update probability for tokens at high-impact\npositions while pruning those at low-impact positions. This pruning not only\nreduces redundancy but also decreases the size of the gradient space, thereby\nlowering computational overhead and shortening the time required to achieve\nsuccessful attacks compared to GCG. We evaluate Mask-GCG by applying it to the\noriginal GCG and several improved variants. Experimental results show that most\ntokens in the suffix contribute significantly to attack success, and pruning a\nminority of low-impact tokens does not affect the loss values or compromise the\nattack success rate (ASR), thereby revealing token redundancy in LLM prompts.\nOur findings provide insights for developing efficient and interpretable LLMs\nfrom the perspective of jailbreak attacks."}
{"id": "2507.20632", "pdf": "https://arxiv.org/pdf/2507.20632.pdf", "abs": "https://arxiv.org/abs/2507.20632", "title": "Self-Supervised Continuous Colormap Recovery from a 2D Scalar Field Visualization without a Legend", "authors": ["Hongxu Liu", "Xinyu Chen", "Haoyang Zheng", "Manyi Li", "Zhenfan Liu", "Fumeng Yang", "Yunhai Wang", "Changhe Tu", "Qiong Zeng"], "categories": ["cs.CV", "cs.HC"], "comment": "Submitted to IEEE VIS 2025", "summary": "Recovering a continuous colormap from a single 2D scalar field visualization\ncan be quite challenging, especially in the absence of a corresponding color\nlegend. In this paper, we propose a novel colormap recovery approach that\nextracts the colormap from a color-encoded 2D scalar field visualization by\nsimultaneously predicting the colormap and underlying data using a\ndecoupling-and-reconstruction strategy. Our approach first separates the input\nvisualization into colormap and data using a decoupling module, then\nreconstructs the visualization with a differentiable color-mapping module. To\nguide this process, we design a reconstruction loss between the input and\nreconstructed visualizations, which serves both as a constraint to ensure\nstrong correlation between colormap and data during training, and as a\nself-supervised optimizer for fine-tuning the predicted colormap of unseen\nvisualizations during inferencing. To ensure smoothness and correct color\nordering in the extracted colormap, we introduce a compact colormap\nrepresentation using cubic B-spline curves and an associated color order loss.\nWe evaluate our method quantitatively and qualitatively on a synthetic dataset\nand a collection of real-world visualizations from the VIS30K dataset.\nAdditionally, we demonstrate its utility in two prototype applications --\ncolormap adjustment and colormap transfer -- and explore its generalization to\nvisualizations with color legends and ones encoded using discrete color\npalettes."}
{"id": "2509.06356", "pdf": "https://arxiv.org/pdf/2509.06356.pdf", "abs": "https://arxiv.org/abs/2509.06356", "title": "PL-CA: A Parametric Legal Case Augmentation Framework", "authors": ["Ao Chang", "Yubo Chen", "Jun Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Conventional RAG is considered one of the most effective methods for\naddressing model knowledge insufficiency and hallucination, particularly in the\njudicial domain that requires high levels of knowledge rigor, logical\nconsistency, and content integrity. However, the conventional RAG method only\ninjects retrieved documents directly into the model's context, which severely\nconstrains models due to their limited context windows and introduces\nadditional computational overhead through excessively long contexts, thereby\ndisrupting models' attention and degrading performance on downstream tasks.\nMoreover, many existing benchmarks lack expert annotation and focus solely on\nindividual downstream tasks while real-world legal scenarios consist of\nmultiple mixed legal tasks, indicating conventional benchmarks' inadequacy for\nreflecting models' true capabilities. To address these limitations, we propose\nPL-CA, which introduces a parametric RAG (P-RAG) framework to perform data\naugmentation on corpus knowledge and encode this legal knowledge into\nparametric vectors, and then integrates this parametric knowledge into the\nLLM's feed-forward networks (FFN) via LoRA, thereby alleviating models' context\npressure. Additionally, we also construct a multi-task legal dataset comprising\nmore than 2000 training and test instances, which are all expert-annotated and\nmanually verified. We conduct our experiments on our dataset, and the\nexperimental results demonstrate that our method reduces the overhead\nassociated with excessively long contexts while maintaining competitive\nperformance on downstream tasks compared to conventional RAG. Our code and\ndataset are provided in the appendix."}
{"id": "2508.10603", "pdf": "https://arxiv.org/pdf/2508.10603.pdf", "abs": "https://arxiv.org/abs/2508.10603", "title": "Why Report Failed Interactions With Robots?! Towards Vignette-based Interaction Quality", "authors": ["Agnes Axelsson", "Merle Reimann", "Ronald Cumbal", "Hannah Pelikan", "Divesh Lala"], "categories": ["cs.RO", "cs.HC"], "comment": "Accepted at the workshop on Real-World HRI in Public and Private\n  Spaces: Successes, Failures, and Lessons Learned (PubRob-Fails), held at the\n  IEEE RO-MAN Conference, 2025. 6 pages", "summary": "Although the quality of human-robot interactions has improved with the advent\nof LLMs, there are still various factors that cause systems to be sub-optimal\nwhen compared to human-human interactions. The nature and criticality of\nfailures are often dependent on the context of the interaction and so cannot be\ngeneralized across the wide range of scenarios and experiments which have been\nimplemented in HRI research. In this work we propose the use of a technique\noverlooked in the field of HRI, ethnographic vignettes, to clearly highlight\nthese failures, particularly those that are rarely documented. We describe the\nmethodology behind the process of writing vignettes and create our own based on\nour personal experiences with failures in HRI systems. We emphasize the\nstrength of vignettes as the ability to communicate failures from a\nmulti-disciplinary perspective, promote transparency about the capabilities of\nrobots, and document unexpected behaviours which would otherwise be omitted\nfrom research reports. We encourage the use of vignettes to augment existing\ninteraction evaluation methods."}
{"id": "2509.06401", "pdf": "https://arxiv.org/pdf/2509.06401.pdf", "abs": "https://arxiv.org/abs/2509.06401", "title": "Do LLMs exhibit the same commonsense capabilities across languages?", "authors": ["Ivan Martínez-Murillo", "Elena Lloret", "Paloma Moreda", "Albert Gatt"], "categories": ["cs.CL"], "comment": null, "summary": "This paper explores the multilingual commonsense generation abilities of\nLarge Language Models (LLMs). To facilitate this investigation, we introduce\nMULTICOM, a novel benchmark that extends the COCOTEROS dataset to four\nlanguages: English, Spanish, Dutch, and Valencian. The task involves generating\na commonsensical sentence that includes a given triplet of words. We evaluate a\nrange of open-source LLMs, including LLaMA, Qwen, Gemma, EuroLLM, and\nSalamandra, on this benchmark. Our evaluation combines automatic metrics,\nLLM-as-a-judge approaches (using Prometheus and JudgeLM), and human\nannotations. Results consistently show superior performance in English, with\nsignificantly lower performance in less-resourced languages. While contextual\nsupport yields mixed results, it tends to benefit underrepresented languages.\nThese findings underscore the current limitations of LLMs in multilingual\ncommonsense generation. The dataset is publicly available at\nhttps://huggingface.co/datasets/gplsi/MULTICOM."}
{"id": "2509.00575", "pdf": "https://arxiv.org/pdf/2509.00575.pdf", "abs": "https://arxiv.org/abs/2509.00575", "title": "Can AI be Auditable?", "authors": ["Himanshu Verma", "Kirtan Padh", "Eva Thelisson"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "Auditability is defined as the capacity of AI systems to be independently\nassessed for compliance with ethical, legal, and technical standards throughout\ntheir lifecycle. The chapter explores how auditability is being formalized\nthrough emerging regulatory frameworks, such as the EU AI Act, which mandate\ndocumentation, risk assessments, and governance structures. It analyzes the\ndiverse challenges facing AI auditability, including technical opacity,\ninconsistent documentation practices, lack of standardized audit tools and\nmetrics, and conflicting principles within existing responsible AI frameworks.\nThe discussion highlights the need for clear guidelines, harmonized\ninternational regulations, and robust socio-technical methodologies to\noperationalize auditability at scale. The chapter concludes by emphasizing the\nimportance of multi-stakeholder collaboration and auditor empowerment in\nbuilding an effective AI audit ecosystem. It argues that auditability must be\nembedded in AI development practices and governance infrastructures to ensure\nthat AI systems are not only functional but also ethically and legally aligned."}
{"id": "2509.06501", "pdf": "https://arxiv.org/pdf/2509.06501.pdf", "abs": "https://arxiv.org/abs/2509.06501", "title": "WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents", "authors": ["Junteng Liu", "Yunji Li", "Chi Zhang", "Jingyang Li", "Aili Chen", "Ke Ji", "Weiyu Cheng", "Zijia Wu", "Chengyu Du", "Qidi Xu", "Jiayuan Song", "Zhengmao Zhu", "Wenhu Chen", "Pengyu Zhao", "Junxian He"], "categories": ["cs.CL"], "comment": null, "summary": "The paradigm of Large Language Models (LLMs) has increasingly shifted toward\nagentic applications, where web browsing capabilities are fundamental for\nretrieving information from diverse online sources. However, existing\nopen-source web agents either demonstrate limited information-seeking abilities\non complex tasks or lack transparent implementations. In this work, we identify\nthat the key challenge lies in the scarcity of challenging data for information\nseeking. To address this limitation, we introduce WebExplorer: a systematic\ndata generation approach using model-based exploration and iterative,\nlong-to-short query evolution. This method creates challenging query-answer\npairs that require multi-step reasoning and complex web navigation. By\nleveraging our curated high-quality dataset, we successfully develop advanced\nweb agent WebExplorer-8B through supervised fine-tuning followed by\nreinforcement learning. Our model supports 128K context length and up to 100\ntool calling turns, enabling long-horizon problem solving. Across diverse\ninformation-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art\nperformance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able\nto effectively search over an average of 16 turns after RL training, achieving\nhigher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best\nperformance among models up to 100B parameters on WebWalkerQA and FRAMES.\nBeyond these information-seeking tasks, our model also achieves strong\ngeneralization on the HLE benchmark even though it is only trained on\nknowledge-intensive QA data. These results highlight our approach as a\npractical path toward long-horizon web agents."}
{"id": "2509.01909", "pdf": "https://arxiv.org/pdf/2509.01909.pdf", "abs": "https://arxiv.org/abs/2509.01909", "title": "Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models", "authors": ["Ranjie Duan", "Jiexi Liu", "Xiaojun Jia", "Shiji Zhao", "Ruoxi Cheng", "Fengxiang Wang", "Cheng Wei", "Yong Xie", "Chang Liu", "Defeng Li", "Yinpeng Dong", "Yichi Zhang", "Yuefeng Chen", "Chongwen Wang", "Xingjun Ma", "Xingxing Wei", "Yang Liu", "Hang Su", "Jun Zhu", "Xinfeng Li", "Yitong Sun", "Jie Zhang", "Jinzhao Hu", "Sha Xu", "Yitong Yang", "Jialing Tao", "Hui Xue"], "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.SC"], "comment": "Technical Report Code & Model weights available:\n  https://github.com/Alibaba-AAIG/Oyster", "summary": "Large language models (LLMs) typically deploy safety mechanisms to prevent\nharmful content generation. Most current approaches focus narrowly on risks\nposed by malicious actors, often framing risks as adversarial events and\nrelying on defensive refusals. However, in real-world settings, risks also come\nfrom non-malicious users seeking help while under psychological distress (e.g.,\nself-harm intentions). In such cases, the model's response can strongly\ninfluence the user's next actions. Simple refusals may lead them to repeat,\nescalate, or move to unsafe platforms, creating worse outcomes. We introduce\nConstructive Safety Alignment (CSA), a human-centric paradigm that protects\nagainst malicious misuse while actively guiding vulnerable users toward safe\nand helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic\nanticipation of user reactions, fine-grained risk boundary discovery, and\ninterpretable reasoning control, turning safety into a trust-building process.\nOy1 achieves state-of-the-art safety among open models while retaining high\ngeneral capabilities. On our Constructive Benchmark, it shows strong\nconstructive engagement, close to GPT-5, and unmatched robustness on the\nStrata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from\nrefusal-first to guidance-first safety, CSA redefines the model-user\nrelationship, aiming for systems that are not just safe, but meaningfully\nhelpful. We release Oy1, code, and the benchmark to support responsible,\nuser-centered AI."}
{"id": "2509.06518", "pdf": "https://arxiv.org/pdf/2509.06518.pdf", "abs": "https://arxiv.org/abs/2509.06518", "title": "Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training", "authors": ["Andrei Baroian", "Kasper Notebomer"], "categories": ["cs.CL", "cs.AI"], "comment": "The reported results are skewed due to a data type mismatch. The\n  dataset was saved with int32, but the data loader interpreted it as uint16.\n  As a result, each 32-bit token was incorrectly split into two 16-bit tokens.\n  Outcome: a consistent artifact where every other token is zero", "summary": "Transformer-based language models traditionally use uniform (isotropic) layer\nsizes, yet they ignore the diverse functional roles that different depths can\nplay and their computational capacity needs. Building on Layer-Wise Scaling\n(LWS) and pruning literature, we introduce three new LWS variants - Framed,\nReverse, and Crown - that redistribute FFN widths and attention heads via two\nor three-point linear interpolation in the pre-training stage. We present the\nfirst systematic ablation of LWS and its variants, on a fixed budget of 180M\nparameters, trained on 5B tokens. All models converge to similar losses and\nachieve better performance compared to an equal-cost isotropic baseline,\nwithout a substantial decrease in training throughput. This work represents an\ninitial step into the design space of layer-wise architectures for\npre-training, but future work should scale experiments to orders of magnitude\nmore tokens and parameters to fully assess their potential."}
{"id": "2509.04441", "pdf": "https://arxiv.org/pdf/2509.04441.pdf", "abs": "https://arxiv.org/abs/2509.04441", "title": "DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation", "authors": ["Hao-Shu Fang", "Branden Romero", "Yichen Xie", "Arthur Hu", "Bo-Ruei Huang", "Juan Alvarez", "Matthew Kim", "Gabriel Margolis", "Kavya Anbarasu", "Masayoshi Tomizuka", "Edward Adelson", "Pulkit Agrawal"], "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.HC"], "comment": "project page: https://dex-op.github.io", "summary": "We introduce perioperation, a paradigm for robotic data collection that\nsensorizes and records human manipulation while maximizing the transferability\nof the data to real robots. We implement this paradigm in DEXOP, a passive hand\nexoskeleton designed to maximize human ability to collect rich sensory (vision\n+ tactile) data for diverse dexterous manipulation tasks in natural\nenvironments. DEXOP mechanically connects human fingers to robot fingers,\nproviding users with direct contact feedback (via proprioception) and mirrors\nthe human hand pose to the passive robot hand to maximize the transfer of\ndemonstrated skills to the robot. The force feedback and pose mirroring make\ntask demonstrations more natural for humans compared to teleoperation,\nincreasing both speed and accuracy. We evaluate DEXOP across a range of\ndexterous, contact-rich tasks, demonstrating its ability to collect\nhigh-quality demonstration data at scale. Policies learned with DEXOP data\nsignificantly improve task performance per unit time of data collection\ncompared to teleoperation, making DEXOP a powerful tool for advancing robot\ndexterity. Our project page is at https://dex-op.github.io."}
{"id": "2509.06524", "pdf": "https://arxiv.org/pdf/2509.06524.pdf", "abs": "https://arxiv.org/abs/2509.06524", "title": "LAMDAS: LLM as an Implicit Classifier for Domain-specific Data Selection", "authors": ["Jian Wu", "Hang Yu", "Bingchang Liu", "Wenjie Yang", "Peng Di", "Jianguo Li", "Yue Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Adapting large language models (LLMs) to specific domains often faces a\ncritical bottleneck: the scarcity of high-quality, human-curated data. While\nlarge volumes of unchecked data are readily available, indiscriminately using\nthem for fine-tuning risks introducing noise and degrading performance.\nStrategic data selection is thus crucial, requiring a method that is both\naccurate and efficient. Existing approaches, categorized as similarity-based\nand direct optimization methods, struggle to simultaneously achieve these\ngoals. In this paper, we introduce LAMDAS (LLM As an iMplicit classifier for\ndomain-specific DAta Selection), a novel approach that leverages the\npre-trained LLM itself as an implicit classifier, thereby bypassing explicit\nfeature engineering and computationally intensive optimization process. LAMDAS\nreframes data selection as a one-class classification problem, identifying\ncandidate data that \"belongs\" to the target domain defined by a small reference\ndataset. Extensive experimental results demonstrate that LAMDAS not only\nexceeds the performance of full-data training using a fraction of the data but\nalso outperforms nine state-of-the-art (SOTA) baselines under various\nscenarios. Furthermore, LAMDAS achieves the most compelling balance between\nperformance gains and computational efficiency compared to all evaluated\nbaselines."}
{"id": "2509.04809", "pdf": "https://arxiv.org/pdf/2509.04809.pdf", "abs": "https://arxiv.org/abs/2509.04809", "title": "TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models", "authors": ["Haechang Kim", "Hao Chen", "Can Li", "Jong Min Lee"], "categories": ["cs.AI", "cs.HC"], "comment": "31 pages total", "summary": "Explainable Reinforcement Learning (XRL) has emerged as a promising approach\nin improving the transparency of Reinforcement Learning (RL) agents. However,\nthere remains a gap between complex RL policies and domain experts, due to the\nlimited comprehensibility of XRL results and isolated coverage of current XRL\napproaches that leave users uncertain about which tools to employ. To address\nthese challenges, we introduce TalkToAgent, a multi-agent Large Language Models\n(LLM) framework that delivers interactive, natural language explanations for RL\npolicies. The architecture with five specialized LLM agents (Coordinator,\nExplainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically\nmap user queries to relevant XRL tools and clarify an agent's actions in terms\nof either key state variables, expected outcomes, or counterfactual\nexplanations. Moreover, our approach extends previous counterfactual\nexplanations by deriving alternative scenarios from qualitative behavioral\ndescriptions, or even new rule-based policies. We validated TalkToAgent on\nquadruple-tank process control problem, a well-known nonlinear control\nbenchmark. Results demonstrated that TalkToAgent successfully mapped user\nqueries into XRL tasks with high accuracy, and coder-debugger interactions\nminimized failures in counterfactual generation. Furthermore, qualitative\nevaluation confirmed that TalkToAgent effectively interpreted agent's actions\nand contextualized their meaning within the problem domain."}
{"id": "2509.06531", "pdf": "https://arxiv.org/pdf/2509.06531.pdf", "abs": "https://arxiv.org/abs/2509.06531", "title": "SLiNT: Structure-aware Language Model with Injection and Contrastive Training for Knowledge Graph Completion", "authors": ["Mengxue Yang", "Chun Yang", "Jiaqi Zhu", "Jiafan Li", "Jingqi Zhang", "Yuyang Li", "Ying Li"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP Findings 2025", "summary": "Link prediction in knowledge graphs requires integrating structural\ninformation and semantic context to infer missing entities. While large\nlanguage models offer strong generative reasoning capabilities, their limited\nexploitation of structural signals often results in structural sparsity and\nsemantic ambiguity, especially under incomplete or zero-shot settings. To\naddress these challenges, we propose SLiNT (Structure-aware Language model with\nInjection and coNtrastive Training), a modular framework that injects\nknowledge-graph-derived structural context into a frozen LLM backbone with\nlightweight LoRA-based adaptation for robust link prediction. Specifically,\nStructure-Guided Neighborhood Enhancement (SGNE) retrieves pseudo-neighbors to\nenrich sparse entities and mitigate missing context; Dynamic Hard Contrastive\nLearning (DHCL) introduces fine-grained supervision by interpolating hard\npositives and negatives to resolve entity-level ambiguity; and\nGradient-Decoupled Dual Injection (GDDI) performs token-level structure-aware\nintervention while preserving the core LLM parameters. Experiments on WN18RR\nand FB15k-237 show that SLiNT achieves superior or competitive performance\ncompared with both embedding-based and generation-based baselines,\ndemonstrating the effectiveness of structure-aware representation learning for\nscalable knowledge graph completion."}
{"id": "2509.06596", "pdf": "https://arxiv.org/pdf/2509.06596.pdf", "abs": "https://arxiv.org/abs/2509.06596", "title": "HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in Large Language Models", "authors": ["Xin Tong", "Zhi Lin", "Jingya Wang", "Bo Jin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often produce hallucinations in\nretrieval-augmented or long-context generation, even when relevant evidence is\npresent. This stems from two issues: head importance is treated as\ninput-agnostic, and raw attention weights poorly reflect each token's true\ncontribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a\nparameter-free decoding framework that directly addresses both challenges. HAVE\nintroduces head-adaptive gating, which performs instance-level soft reweighing\nof attention heads, and value calibration, which augments attention with the\nmagnitude of value vectors to approximate write-back contribution. Together,\nthese modules construct token-level evidence aligned with model updates and\nfuse it with the LM distribution through a lightweight uncertainty-scaled\npolicy. HAVE requires no finetuning and operates in a single forward pass,\nmaking it efficient and broadly applicable. Experiments across multiple QA\nbenchmarks and LLM families demonstrate that HAVE consistently reduces\nhallucinations and outperforms strong baselines, including DAGCD, with modest\noverhead. The framework is transparent, reproducible, and readily integrates\nwith off-the-shelf LLMs, advancing trustworthy generation in real-world\nsettings."}
{"id": "2509.06631", "pdf": "https://arxiv.org/pdf/2509.06631.pdf", "abs": "https://arxiv.org/abs/2509.06631", "title": "Guided Decoding and Its Critical Role in Retrieval-Augmented Generation", "authors": ["Özgür Uğur", "Musa Yılmaz", "Esra Şavirdi", "Özay Ezerceli", "Mahmut El Huseyni", "Selva Taş", "Reyhan Bayraktar"], "categories": ["cs.CL"], "comment": null, "summary": "The integration of Large Language Models (LLMs) into various applications has\ndriven the need for structured and reliable responses. A key challenge in\nRetrieval-Augmented Generation (RAG) systems is ensuring that outputs align\nwith expected formats while minimizing hallucinations. This study examines the\nrole of guided decoding in RAG systems, comparing three methods, Outlines,\nXGrammar, and LM Format Enforcer, across different multi-turn prompting setups\n(0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates,\nand output quality, we provide insights into their performance and\napplicability. Our findings reveal how multi-turn interactions influence guided\ndecoding, uncovering unexpected performance variations that can inform method\nselection for specific use cases. This work advances the understanding of\nstructured output generation in RAG systems, offering both theoretical insights\nand practical guidance for LLM deployment."}
{"id": "2509.06637", "pdf": "https://arxiv.org/pdf/2509.06637.pdf", "abs": "https://arxiv.org/abs/2509.06637", "title": "Modelling Intertextuality with N-gram Embeddings", "authors": ["Yi Xing"], "categories": ["cs.CL"], "comment": null, "summary": "Intertextuality is a central tenet in literary studies. It refers to the\nintricate links between literary texts that are created by various types of\nreferences. This paper proposes a new quantitative model of intertextuality to\nenable scalable analysis and network-based insights: perform pairwise\ncomparisons of the embeddings of n-grams from two texts and average their\nresults as the overall intertextuality. Validation on four texts with known\ndegrees of intertextuality, alongside a scalability test on 267 diverse texts,\ndemonstrates the method's effectiveness and efficiency. Network analysis\nfurther reveals centrality and community structures, affirming the approach's\nsuccess in capturing and quantifying intertextual relationships."}
{"id": "2509.06650", "pdf": "https://arxiv.org/pdf/2509.06650.pdf", "abs": "https://arxiv.org/abs/2509.06650", "title": "Domain-Aware RAG: MoL-Enhanced RL for Efficient Training and Scalable Retrieval", "authors": ["Hao Lin", "Peitong Xie", "Jingxue Chen", "Jie Lin", "Qingkun Tang", "Qianchun Lu"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems rely heavily on the retrieval\nstage, particularly the coarse-ranking process. Existing coarse-ranking\noptimization approaches often struggle to balance domain-specific knowledge\nlearning with query enhencement, resulting in suboptimal retrieval performance.\nTo address this challenge, we propose MoLER, a domain-aware RAG method that\nuses MoL-Enhanced Reinforcement Learning to optimize retrieval. MoLER has a\ntwo-stage pipeline: a continual pre-training (CPT) phase using a Mixture of\nLosses (MoL) to balance domain-specific knowledge with general language\ncapabilities, and a reinforcement learning (RL) phase leveraging Group Relative\nPolicy Optimization (GRPO) to optimize query and passage generation for\nmaximizing document recall. A key innovation is our Multi-query Single-passage\nLate Fusion (MSLF) strategy, which reduces computational overhead during RL\ntraining while maintaining scalable inference via Multi-query Multi-passage\nLate Fusion (MMLF). Extensive experiments on benchmark datasets show that MoLER\nachieves state-of-the-art performance, significantly outperforming baseline\nmethods. MoLER bridges the knowledge gap in RAG systems, enabling robust and\nscalable retrieval in specialized domains."}
{"id": "2509.06652", "pdf": "https://arxiv.org/pdf/2509.06652.pdf", "abs": "https://arxiv.org/abs/2509.06652", "title": "IntrEx: A Dataset for Modeling Engagement in Educational Conversations", "authors": ["Xingwei Tan", "Mahathi Parvatham", "Chiara Gambi", "Gabriele Pergola"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Findings camera-ready, 9+7 pages", "summary": "Engagement and motivation are crucial for second-language acquisition, yet\nmaintaining learner interest in educational conversations remains a challenge.\nWhile prior research has explored what makes educational texts interesting,\nstill little is known about the linguistic features that drive engagement in\nconversations. To address this gap, we introduce IntrEx, the first large\ndataset annotated for interestingness and expected interestingness in\nteacher-student interactions. Built upon the Teacher-Student Chatroom Corpus\n(TSCC), IntrEx extends prior work by incorporating sequence-level annotations,\nallowing for the study of engagement beyond isolated turns to capture how\ninterest evolves over extended dialogues. We employ a rigorous annotation\nprocess with over 100 second-language learners, using a comparison-based rating\napproach inspired by reinforcement learning from human feedback (RLHF) to\nimprove agreement. We investigate whether large language models (LLMs) can\npredict human interestingness judgments. We find that LLMs (7B/8B parameters)\nfine-tuned on interestingness ratings outperform larger proprietary models like\nGPT-4o, demonstrating the potential for specialised datasets to model\nengagement in educational settings. Finally, we analyze how linguistic and\ncognitive factors, such as concreteness, comprehensibility (readability), and\nuptake, influence engagement in educational dialogues."}
{"id": "2509.06675", "pdf": "https://arxiv.org/pdf/2509.06675.pdf", "abs": "https://arxiv.org/abs/2509.06675", "title": "ParCzech4Speech: A New Speech Corpus Derived from Czech Parliamentary Data", "authors": ["Vladislav Stankov", "Matyáš Kopp", "Ondřej Bojar"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce ParCzech4Speech 1.0, a processed version of the ParCzech 4.0\ncorpus, targeted at speech modeling tasks with the largest variant containing\n2,695 hours. We combined the sound recordings of the Czech parliamentary\nspeeches with the official transcripts. The recordings were processed with\nWhisperX and Wav2Vec 2.0 to extract automated audio-text alignment. Our\nprocessing pipeline improves upon the ParCzech 3.0 speech recognition version\nby extracting more data with higher alignment reliability. The dataset is\noffered in three flexible variants: (1) sentence-segmented for automatic speech\nrecognition and speech synthesis tasks with clean boundaries, (2) unsegmented\npreserving original utterance flow across sentences, and (3) a raw-alignment\nfor further custom refinement for other possible tasks. All variants maintain\nthe original metadata and are released under a permissive CC-BY license. The\ndataset is available in the LINDAT repository, with the sentence-segmented and\nunsegmented variants additionally available on Hugging Face."}
{"id": "2509.06704", "pdf": "https://arxiv.org/pdf/2509.06704.pdf", "abs": "https://arxiv.org/abs/2509.06704", "title": "Will Annotators Disagree? Identifying Subjectivity in Value-Laden Arguments", "authors": ["Amir Homayounirad", "Enrico Liscio", "Tong Wang", "Catholijn M. Jonker", "Luciano C. Siebert"], "categories": ["cs.CL"], "comment": "Accepted at Findings of EMNLP 2025", "summary": "Aggregating multiple annotations into a single ground truth label may hide\nvaluable insights into annotator disagreement, particularly in tasks where\nsubjectivity plays a crucial role. In this work, we explore methods for\nidentifying subjectivity in recognizing the human values that motivate\narguments. We evaluate two main approaches: inferring subjectivity through\nvalue prediction vs. directly identifying subjectivity. Our experiments show\nthat direct subjectivity identification significantly improves the model\nperformance of flagging subjective arguments. Furthermore, combining\ncontrastive loss with binary cross-entropy loss does not improve performance\nbut reduces the dependency on per-label subjectivity. Our proposed methods can\nhelp identify arguments that individuals may interpret differently, fostering a\nmore nuanced annotation process."}
{"id": "2509.06795", "pdf": "https://arxiv.org/pdf/2509.06795.pdf", "abs": "https://arxiv.org/abs/2509.06795", "title": "Anchoring Refusal Direction: Mitigating Safety Risks in Tuning via Projection Constraint", "authors": ["Yanrui Du", "Fenglei Fan", "Sendong Zhao", "Jiawei Cao", "Qika Lin", "Kai He", "Ting Liu", "Bing Qin", "Mengling Feng"], "categories": ["cs.CL"], "comment": null, "summary": "Instruction Fine-Tuning (IFT) has been widely adopted as an effective\npost-training strategy to enhance various abilities of Large Language Models\n(LLMs). However, prior studies have shown that IFT can significantly compromise\nLLMs' safety, particularly their ability to refuse malicious instructions,\nraising significant concerns. Recent research into the internal mechanisms of\nLLMs has identified the refusal direction (r-direction) in the hidden states,\nwhich plays a pivotal role in governing refusal behavior. Building on this\ninsight, our study reveals that the r-direction tends to drift during training,\nwhich we identify as one of the causes of the associated safety risks. To\nmitigate such drift, our proposed ProCon method introduces a\nprojection-constrained loss term that regularizes the projection magnitude of\neach training sample's hidden state onto the r-direction. Our initial analysis\nshows that applying an appropriate constraint can effectively mitigate the\nrefusal direction drift and associated safety risks, but remains limited by\noverall performance barriers. To overcome this barrier, informed by our\nobservation of early-stage sharp drift and a data-driven perspective, we\nintroduce a warm-up strategy that emphasizes early-stage strong constraints and\nbroaden the data distribution to strengthen constraint signals, leading to an\nenhanced ProCon method. Experimental results under various datasets, scenarios,\nand LLMs demonstrate that our method can significantly mitigate safety risks\nposed by IFT while preserving task performance gains. Even compared with strong\nbaselines, our method consistently delivers superior overall performance.\nCrucially, our analysis indicates that ProCon can contribute to stabilizing the\nr-direction during training, while such an interpretability-driven exploration\nof LLMs' internal mechanisms lays a solid foundation for future safety\nresearch."}
{"id": "2509.06806", "pdf": "https://arxiv.org/pdf/2509.06806.pdf", "abs": "https://arxiv.org/abs/2509.06806", "title": "MachineLearningLM: Continued Pretraining Language Models on Millions of Synthetic Tabular Prediction Tasks Scales In-Context ML", "authors": ["Haoyu Dong", "Pengkun Zhang", "Mingzhe Lu", "Yanzhen Shen", "Guolin Ke"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU."}
{"id": "2509.06807", "pdf": "https://arxiv.org/pdf/2509.06807.pdf", "abs": "https://arxiv.org/abs/2509.06807", "title": "MoGU V2: Toward a Higher Pareto Frontier Between Model Usability and Security", "authors": ["Yanrui Du", "Fenglei Fan", "Sendong Zhao", "Jiawei Cao", "Ting Liu", "Bing Qin"], "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) increasingly permeate human life, their\nsecurity has emerged as a critical concern, particularly their ability to\nmaintain harmless responses to malicious instructions. Although extensive\nmethods have improved LLMs' security, they often lead to conservative,\nrejection-oriented responses that compromise practical usability. This presents\na key challenge: how to advance the Pareto frontier between LLMs' usability and\nsecurity, rather than necessitate a trade-off between them. To address this, we\npropose the MoGU framework, in which the intra-layer router dynamically\nallocates weights by sensing hidden states, thereby balancing the contributions\nof security-optimized and usability-optimized variants. Despite its initial\npotential, the MoGU framework faces limitations such as parameter redundancy\nand performance bottlenecks. To overcome these, we further propose an improved\nMoGU_v2 framework that establishes a tighter coupling between the routers and\nhidden states. In MoGU_v2, routers are embedded only in layers encoding highly\nclassifiable security features, and backbone modules are activated during\nrouter optimization to enable bidirectional adaptation. MoGU_V2 exhibits strong\nadaptability and stable improvements across various series of LLMs, including\nmainstream LLMs serving as brains in various applications, on-device LLMs\noptimized for resource-constrained scenarios, and reasoning LLMs tailored for\nuser interpretability. Meanwhile, even facing risks introduced by Instruction\nFine-tuning, MoGU_v2 can easily restore security without compromising the task\nperformance gains via a simple data-mix strategy. These comprehensive\nimprovements highlight MoGU_V2 as a robust and versatile solution for\nmitigating security risks in real-world applications."}
{"id": "2509.06809", "pdf": "https://arxiv.org/pdf/2509.06809.pdf", "abs": "https://arxiv.org/abs/2509.06809", "title": "Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem", "authors": ["Valentin Quesnel", "Damien Sileo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The scarcity of high-quality, logically sound data is a critical bottleneck\nfor advancing the mathematical reasoning of Large Language Models (LLMs). Our\nwork confronts this challenge by turning decades of automated theorem proving\nresearch into a scalable data engine. Rather than relying on error-prone LLMs\nor complex proof-assistant syntax like Lean and Isabelle, our framework\nleverages E-prover's saturation capabilities on the vast TPTP axiom library to\nderive a massive, guaranteed-valid corpus of theorems. Our pipeline is\nprincipled and simple: saturate axioms, filter for \"interesting\" theorems, and\ngenerate tasks. With no LLMs in the loop, we eliminate factual errors by\nconstruction. This purely symbolic data is then transformed into three\ndifficulty-controlled challenges: entailment verification, premise selection,\nand proof reconstruction. Our zero-shot experiments on frontier models reveal a\nclear weakness: performance collapses on tasks requiring deep, structural\nreasoning. Our framework provides both the diagnostic tool to measure this gap\nand a scalable source of symbolic training data to address it. We make the code\nand data publicly available.\n  https://github.com/sileod/reasoning_core\nhttps://hf.co/datasets/reasoning-core/rc1"}
{"id": "2509.06813", "pdf": "https://arxiv.org/pdf/2509.06813.pdf", "abs": "https://arxiv.org/abs/2509.06813", "title": "A Comparative Benchmark of Large Language Models for Labelling Wind Turbine Maintenance Logs", "authors": ["Max Malyi", "Jonathan Shek", "Alasdair McDonald", "Andre Biscaya"], "categories": ["cs.CL"], "comment": "Associated GitHub repository:\n  https://github.com/mvmalyi/wind-farm-maintenance-logs-labelling-with-llms", "summary": "Effective Operation and Maintenance (O&M) is critical to reducing the\nLevelised Cost of Energy (LCOE) from wind power, yet the unstructured,\nfree-text nature of turbine maintenance logs presents a significant barrier to\nautomated analysis. Our paper addresses this by presenting a novel and\nreproducible framework for benchmarking Large Language Models (LLMs) on the\ntask of classifying these complex industrial records. To promote transparency\nand encourage further research, this framework has been made publicly available\nas an open-source tool. We systematically evaluate a diverse suite of\nstate-of-the-art proprietary and open-source LLMs, providing a foundational\nassessment of their trade-offs in reliability, operational efficiency, and\nmodel calibration. Our results quantify a clear performance hierarchy,\nidentifying top models that exhibit high alignment with a benchmark standard\nand trustworthy, well-calibrated confidence scores. We also demonstrate that\nclassification performance is highly dependent on the task's semantic\nambiguity, with all models showing higher consensus on objective component\nidentification than on interpretive maintenance actions. Given that no model\nachieves perfect accuracy and that calibration varies dramatically, we conclude\nthat the most effective and responsible near-term application is a\nHuman-in-the-Loop system, where LLMs act as a powerful assistant to accelerate\nand standardise data labelling for human experts, thereby enhancing O&M data\nquality and downstream reliability analysis."}
{"id": "2509.06836", "pdf": "https://arxiv.org/pdf/2509.06836.pdf", "abs": "https://arxiv.org/abs/2509.06836", "title": "COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens", "authors": ["Eugene Kwek", "Wenpeng Yin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Making LLMs more efficient in memory, latency, and serving cost is crucial\nfor edge deployment, interactive applications, and sustainable inference at\nscale. Pruning is a key technique toward this goal. However, prior pruning\nmethods are limited: width pruning often breaks the standard transformer layout\nor requires custom inference code, while depth pruning removes entire layers\nand can cause abrupt accuracy drops. In this work, we propose COMPACT, which\njointly (i) prunes rare vocabulary to shrink embedding/unembedding and (ii)\nprunes FFN intermediate channels using common-token-weighted activations,\naligning importance with the post-pruning token distribution. COMPACT enjoys\nmerits of both depth and width pruning, such as: deployment-friendliness (keeps\na standard transformer architecture), scale-adaptivity (trade off vocab vs. FFN\npruning), training-free operation with competitive pruning time, and strong\nmemory savings alongside throughput gains. Experiments across Qwen, LLaMA, and\nGemma families (0.5B-70B) show state-of-the-art downstream task performance at\nsimilar or higher pruning ratios, with substantial reductions in parameters,\nGPU memory, and end-to-end latency."}
{"id": "2509.06838", "pdf": "https://arxiv.org/pdf/2509.06838.pdf", "abs": "https://arxiv.org/abs/2509.06838", "title": "EPT Benchmark: Evaluation of Persian Trustworthiness in Large Language Models", "authors": ["Mohammad Reza Mirbagheri", "Mohammad Mahdi Mirkamali", "Zahra Motoshaker Arani", "Ali Javeri", "Amir Mahdi Sadeghzadeh", "Rasool Jalili"], "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Large Language Models (LLMs), trained on extensive datasets using advanced\ndeep learning architectures, have demonstrated remarkable performance across a\nwide range of language tasks, becoming a cornerstone of modern AI technologies.\nHowever, ensuring their trustworthiness remains a critical challenge, as\nreliability is essential not only for accurate performance but also for\nupholding ethical, cultural, and social values. Careful alignment of training\ndata and culturally grounded evaluation criteria are vital for developing\nresponsible AI systems. In this study, we introduce the EPT (Evaluation of\nPersian Trustworthiness) metric, a culturally informed benchmark specifically\ndesigned to assess the trustworthiness of LLMs across six key aspects:\ntruthfulness, safety, fairness, robustness, privacy, and ethical alignment. We\ncurated a labeled dataset and evaluated the performance of several leading\nmodels - including ChatGPT, Claude, DeepSeek, Gemini, Grok, LLaMA, Mistral, and\nQwen - using both automated LLM-based and human assessments. Our results reveal\nsignificant deficiencies in the safety dimension, underscoring the urgent need\nfor focused attention on this critical aspect of model behavior. Furthermore,\nour findings offer valuable insights into the alignment of these models with\nPersian ethical-cultural values and highlight critical gaps and opportunities\nfor advancing trustworthy and culturally responsible AI. The dataset is\npublicly available at: https://github.com/Rezamirbagheri110/EPT-Benchmark."}
{"id": "2509.06870", "pdf": "https://arxiv.org/pdf/2509.06870.pdf", "abs": "https://arxiv.org/abs/2509.06870", "title": "The Majority is not always right: RL training for solution aggregation", "authors": ["Wenting Zhao", "Pranjal Aggarwal", "Swarnadeep Saha", "Asli Celikyilmaz", "Jason Weston", "Ilia Kulikov"], "categories": ["cs.CL"], "comment": null, "summary": "Scaling up test-time compute, by generating multiple independent solutions\nand selecting or aggregating among them, has become a central paradigm for\nimproving large language models (LLMs) on challenging reasoning tasks. While\nmost prior work relies on simple majority voting or reward model ranking to\naggregate solutions, these approaches may only yield limited benefits. In this\nwork, we propose to learn aggregation as an explicit reasoning skill: given a\nset of candidate solutions, we train an aggregator model to review, reconcile,\nand synthesize a final, correct answer using reinforcement learning from\nverifiable rewards. A key ingredient is careful balancing of easy and hard\ntraining examples, allowing the model to learn both to recover\nminority-but-correct answers as well as easy majority-correct answers.\nEmpirically, we find our method, AggLM, outperforms both strong rule-based and\nreward-model baselines, across multiple benchmarks. Furthermore, it generalizes\neffectively to solutions from differing models, including stronger ones than\ncontained in the training data, all while requiring substantially fewer tokens\nthan majority voting with larger numbers of solutions."}
{"id": "2509.06883", "pdf": "https://arxiv.org/pdf/2509.06883.pdf", "abs": "https://arxiv.org/abs/2509.06883", "title": "UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction", "authors": ["Joe Wilder", "Nikhil Kadapala", "Benji Xu", "Mohammed Alsaadi", "Aiden Parsons", "Mitchell Rogers", "Palash Agarwal", "Adam Hassick", "Laura Dietz"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "16 pages,3 tables, CLEF 2025 Working Notes, 9-12 September 2025,\n  Madrid, Spain", "summary": "We participate in CheckThat! Task 2 English and explore various methods of\nprompting and in-context learning, including few-shot prompting and fine-tuning\nwith different LLM families, with the goal of extracting check-worthy claims\nfrom social media passages. Our best METEOR score is achieved by fine-tuning a\nFLAN-T5 model. However, we observe that higher-quality claims can sometimes be\nextracted using other methods, even when their METEOR scores are lower."}
{"id": "2509.06888", "pdf": "https://arxiv.org/pdf/2509.06888.pdf", "abs": "https://arxiv.org/abs/2509.06888", "title": "mmBERT: A Modern Multilingual Encoder with Annealed Language Learning", "authors": ["Marc Marone", "Orion Weller", "William Fleshman", "Eugene Yang", "Dawn Lawrie", "Benjamin Van Durme"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Encoder-only languages models are frequently used for a variety of standard\nmachine learning tasks, including classification and retrieval. However, there\nhas been a lack of recent research for encoder models, especially with respect\nto multilingual models. We introduce mmBERT, an encoder-only language model\npretrained on 3T tokens of multilingual text in over 1800 languages. To build\nmmBERT we introduce several novel elements, including an inverse mask ratio\nschedule and an inverse temperature sampling ratio. We add over 1700\nlow-resource languages to the data mix only during the decay phase, showing\nthat it boosts performance dramatically and maximizes the gains from the\nrelatively small amount of training data. Despite only including these\nlow-resource languages in the short decay phase we achieve similar\nclassification performance to models like OpenAI's o3 and Google's Gemini 2.5\nPro. Overall, we show that mmBERT significantly outperforms the previous\ngeneration of models on classification and retrieval tasks -- on both high and\nlow-resource languages."}
{"id": "2509.06902", "pdf": "https://arxiv.org/pdf/2509.06902.pdf", "abs": "https://arxiv.org/abs/2509.06902", "title": "Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers from LLMs via Claim Verification", "authors": ["Aivin V. Solatorio"], "categories": ["cs.CL", "cs.CR", "cs.DB", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) as stochastic systems may generate numbers that\ndeviate from available data, a failure known as \\emph{numeric hallucination}.\nExisting safeguards -- retrieval-augmented generation, citations, and\nuncertainty estimation -- improve transparency but cannot guarantee fidelity:\nfabricated or misquoted values may still be displayed as if correct. We propose\n\\textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that\nenforces numeric fidelity through mechanical verification. Under PCN, numeric\nspans are emitted as \\emph{claim-bound tokens} tied to structured claims, and a\nverifier checks each token under a declared policy (e.g., exact equality,\nrounding, aliases, or tolerance with qualifiers). Crucially, PCN places\nverification in the \\emph{renderer}, not the model: only claim-checked numbers\nare marked as verified, and all others default to unverified. This separation\nprevents spoofing and guarantees fail-closed behavior. We formalize PCN and\nprove soundness, completeness under honest tokens, fail-closed behavior, and\nmonotonicity under policy refinement. PCN is lightweight and model-agnostic,\nintegrates seamlessly into existing applications, and can be extended with\ncryptographic commitments. By enforcing verification as a mandatory step before\ndisplay, PCN establishes a simple contract for numerically sensitive settings:\n\\emph{trust is earned only by proof}, while the absence of a mark communicates\nuncertainty."}
{"id": "2509.06948", "pdf": "https://arxiv.org/pdf/2509.06948.pdf", "abs": "https://arxiv.org/abs/2509.06948", "title": "Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning", "authors": ["Liang Chen", "Xueting Han", "Li Shen", "Jing Bai", "Kam-Fai Wong"], "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) has proven effective in incentivizing the\nreasoning abilities of large language models (LLMs), but suffers from severe\nefficiency challenges due to its trial-and-error nature. While the common\npractice employs supervised fine-tuning (SFT) as a warm-up stage for RL, this\ndecoupled two-stage approach limits interaction between SFT and RL, thereby\nconstraining overall effectiveness. This study introduces a novel method for\nlearning reasoning models that employs bilevel optimization to facilitate\nbetter cooperation between these training paradigms. By conditioning the SFT\nobjective on the optimal RL policy, our approach enables SFT to meta-learn how\nto guide RL's optimization process. During training, the lower level performs\nRL updates while simultaneously receiving SFT supervision, and the upper level\nexplicitly maximizes the cooperative gain-the performance advantage of joint\nSFT-RL training over RL alone. Empirical evaluations on five reasoning\nbenchmarks demonstrate that our method consistently outperforms baselines and\nachieves a better balance between effectiveness and efficiency."}
{"id": "2509.06949", "pdf": "https://arxiv.org/pdf/2509.06949.pdf", "abs": "https://arxiv.org/abs/2509.06949", "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models", "authors": ["Yinjie Wang", "Ling Yang", "Bowen Li", "Ye Tian", "Ke Shen", "Mengdi Wang"], "categories": ["cs.CL"], "comment": "Code and Models: https://github.com/Gen-Verse/dLLM-RL", "summary": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL"}
{"id": "2509.06952", "pdf": "https://arxiv.org/pdf/2509.06952.pdf", "abs": "https://arxiv.org/abs/2509.06952", "title": "On the Same Wavelength? Evaluating Pragmatic Reasoning in Language Models across Broad Concepts", "authors": ["Linlu Qiu", "Cedegao E. Zhang", "Joshua B. Tenenbaum", "Yoon Kim", "Roger P. Levy"], "categories": ["cs.CL"], "comment": "EMNLP 2025 (Main)", "summary": "Language use is shaped by pragmatics -- i.e., reasoning about communicative\ngoals and norms in context. As language models (LMs) are increasingly used as\nconversational agents, it becomes ever more important to understand their\npragmatic reasoning abilities. We propose an evaluation framework derived from\nWavelength, a popular communication game where a speaker and a listener\ncommunicate about a broad range of concepts in a granular manner. We study a\nrange of LMs on both language comprehension and language production using\ndirect and Chain-of-Thought (CoT) prompting, and further explore a Rational\nSpeech Act (RSA) approach to incorporating Bayesian pragmatic reasoning into LM\ninference. We find that state-of-the-art LMs, but not smaller ones, achieve\nstrong performance on language comprehension, obtaining similar-to-human\naccuracy and exhibiting high correlations with human judgments even without CoT\nprompting or RSA. On language production, CoT can outperform direct prompting,\nand using RSA provides significant improvements over both approaches. Our study\nhelps identify the strengths and limitations in LMs' pragmatic reasoning\nabilities and demonstrates the potential for improving them with RSA, opening\nup future avenues for understanding conceptual representation, language\nunderstanding, and social reasoning in LMs and humans."}
{"id": "2509.03736", "pdf": "https://arxiv.org/pdf/2509.03736.pdf", "abs": "https://arxiv.org/abs/2509.03736", "title": "Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation", "authors": ["James Mooney", "Josef Woldense", "Zheng Robert Jia", "Shirley Anugrah Hayati", "My Ha Nguyen", "Vipul Raheja", "Dongyeop Kang"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "25 pages, 9 figures, 7 tables", "summary": "The impressive capabilities of Large Language Models (LLMs) have fueled the\nnotion that synthetic agents can serve as substitutes for real participants in\nhuman-subject research. In an effort to evaluate the merits of this claim,\nsocial science researchers have largely focused on whether LLM-generated survey\ndata corresponds to that of a human counterpart whom the LLM is prompted to\nrepresent. In contrast, we address a more fundamental question: Do agents\nmaintain internal consistency, retaining similar behaviors when examined under\ndifferent experimental settings? To this end, we develop a study designed to\n(a) reveal the agent's internal state and (b) examine agent behavior in a basic\ndialogue setting. This design enables us to explore a set of behavioral\nhypotheses to assess whether an agent's conversation behavior is consistent\nwith what we would expect from their revealed internal state. Our findings on\nthese hypotheses show significant internal inconsistencies in LLMs across model\nfamilies and at differing model sizes. Most importantly, we find that, although\nagents may generate responses matching those of their human counterparts, they\nfail to be internally consistent, representing a critical gap in their\ncapabilities to accurately substitute for real participants in human-subject\nresearch. Our simulation code and data are publicly accessible."}
{"id": "2509.05309", "pdf": "https://arxiv.org/pdf/2509.05309.pdf", "abs": "https://arxiv.org/abs/2509.05309", "title": "ProtSAE: Disentangling and Interpreting Protein Language Models via Semantically-Guided Sparse Autoencoders", "authors": ["Xiangyu Liu", "Haodi Lei", "Yi Liu", "Yang Liu", "Wei Hu"], "categories": ["q-bio.QM", "cs.AI", "cs.CL"], "comment": null, "summary": "Sparse Autoencoder (SAE) has emerged as a powerful tool for mechanistic\ninterpretability of large language models. Recent works apply SAE to protein\nlanguage models (PLMs), aiming to extract and analyze biologically meaningful\nfeatures from their latent spaces. However, SAE suffers from semantic\nentanglement, where individual neurons often mix multiple nonlinear concepts,\nmaking it difficult to reliably interpret or manipulate model behaviors. In\nthis paper, we propose a semantically-guided SAE, called ProtSAE. Unlike\nexisting SAE which requires annotation datasets to filter and interpret\nactivations, we guide semantic disentanglement during training using both\nannotation datasets and domain knowledge to mitigate the effects of entangled\nattributes. We design interpretability experiments showing that ProtSAE learns\nmore biologically relevant and interpretable hidden features compared to\nprevious methods. Performance analyses further demonstrate that ProtSAE\nmaintains high reconstruction fidelity while achieving better results in\ninterpretable probing. We also show the potential of ProtSAE in steering PLMs\nfor downstream generation tasks."}
{"id": "2509.05331", "pdf": "https://arxiv.org/pdf/2509.05331.pdf", "abs": "https://arxiv.org/abs/2509.05331", "title": "ForensicsData: A Digital Forensics Dataset for Large Language Models", "authors": ["Youssef Chakir", "Iyad Lahsen-Cherif"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "Accepted to WiMob 2025 (21st International Conference on Wireless and\n  Mobile Computing, Networking and Communications), Marrakesh, Morocco, Oct\n  20-22, 2025. 6 pages, 5 figures, 5 tables. IEEEtran conference format", "summary": "The growing complexity of cyber incidents presents significant challenges for\ndigital forensic investigators, especially in evidence collection and analysis.\nPublic resources are still limited because of ethical, legal, and privacy\nconcerns, even though realistic datasets are necessary to support research and\ntool developments. To address this gap, we introduce ForensicsData, an\nextensive Question-Context-Answer (Q-C-A) dataset sourced from actual malware\nanalysis reports. It consists of more than 5,000 Q-C-A triplets. A unique\nworkflow was used to create the dataset, which extracts structured data, uses\nlarge language models (LLMs) to transform it into Q-C-A format, and then uses a\nspecialized evaluation process to confirm its quality. Among the models\nevaluated, Gemini 2 Flash demonstrated the best performance in aligning\ngenerated content with forensic terminology. ForensicsData aims to advance\ndigital forensics by enabling reproducible experiments and fostering\ncollaboration within the research community."}
{"id": "2509.05390", "pdf": "https://arxiv.org/pdf/2509.05390.pdf", "abs": "https://arxiv.org/abs/2509.05390", "title": "Authorship Without Writing: Large Language Models and the Senior Author Analogy", "authors": ["Clint Hurshman", "Sebastian Porsdam Mann", "Julian Savulescu", "Brian D. Earp"], "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "28 pages, 0 figures", "summary": "The use of large language models (LLMs) in bioethical, scientific, and\nmedical writing remains controversial. While there is broad agreement in some\ncircles that LLMs cannot count as authors, there is no consensus about whether\nand how humans using LLMs can count as authors. In many fields, authorship is\ndistributed among large teams of researchers, some of whom, including\nparadigmatic senior authors who guide and determine the scope of a project and\nultimately vouch for its integrity, may not write a single word. In this paper,\nwe argue that LLM use (under specific conditions) is analogous to a form of\nsenior authorship. On this view, the use of LLMs, even to generate complete\ndrafts of research papers, can be considered a legitimate form of authorship\naccording to the accepted criteria in many fields. We conclude that either such\nuse should be recognized as legitimate, or current criteria for authorship\nrequire fundamental revision. AI use declaration: GPT-5 was used to help format\nBox 1. AI was not used for any other part of the preparation or writing of this\nmanuscript."}
{"id": "2509.05608", "pdf": "https://arxiv.org/pdf/2509.05608.pdf", "abs": "https://arxiv.org/abs/2509.05608", "title": "Cross-Service Threat Intelligence in LLM Services using Privacy-Preserving Fingerprints", "authors": ["Waris Gill", "Natalie Isak", "Matthew Dressman"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "The widespread deployment of LLMs across enterprise services has created a\ncritical security blind spot. Organizations operate multiple LLM services\nhandling billions of queries daily, yet regulatory compliance boundaries\nprevent these services from sharing threat intelligence about prompt injection\nattacks, the top security risk for LLMs. When an attack is detected in one\nservice, the same threat may persist undetected in others for months, as\nprivacy regulations prohibit sharing user prompts across compliance boundaries.\n  We present BinaryShield, the first privacy-preserving threat intelligence\nsystem that enables secure sharing of attack fingerprints across compliance\nboundaries. BinaryShield transforms suspicious prompts through a unique\npipeline combining PII redaction, semantic embedding, binary quantization, and\nrandomized response mechanism to potentially generate non-invertible\nfingerprints that preserve attack patterns while providing privacy. Our\nevaluations demonstrate that BinaryShield achieves an F1-score of 0.94,\nsignificantly outperforming SimHash (0.77), the privacy-preserving baseline,\nwhile achieving 64x storage reduction and 38x faster similarity search compared\nto dense embeddings."}
{"id": "2509.05634", "pdf": "https://arxiv.org/pdf/2509.05634.pdf", "abs": "https://arxiv.org/abs/2509.05634", "title": "On the Contribution of Lexical Features to Speech Emotion Recognition", "authors": ["David Combei"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to 13th Conference on Speech Technology and Human-Computer\n  Dialogue", "summary": "Although paralinguistic cues are often considered the primary drivers of\nspeech emotion recognition (SER), we investigate the role of lexical content\nextracted from speech and show that it can achieve competitive and in some\ncases higher performance compared to acoustic models. On the MELD dataset, our\nlexical-based approach obtains a weighted F1-score (WF1) of 51.5%, compared to\n49.3% for an acoustic-only pipeline with a larger parameter count. Furthermore,\nwe analyze different self-supervised (SSL) speech and text representations,\nconduct a layer-wise study of transformer-based encoders, and evaluate the\neffect of audio denoising."}
{"id": "2509.05978", "pdf": "https://arxiv.org/pdf/2509.05978.pdf", "abs": "https://arxiv.org/abs/2509.05978", "title": "Imagining Alternatives: Towards High-Resolution 3D Counterfactual Medical Image Generation via Language Guidance", "authors": ["Mohamed Mohamed", "Brennan Nichyporuk", "Douglas L. Arnold", "Tal Arbel"], "categories": ["eess.IV", "cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Vision-language models have demonstrated impressive capabilities in\ngenerating 2D images under various conditions; however the impressive\nperformance of these models in 2D is largely enabled by extensive, readily\navailable pretrained foundation models. Critically, comparable pretrained\nfoundation models do not exist for 3D, significantly limiting progress in this\ndomain. As a result, the potential of vision-language models to produce\nhigh-resolution 3D counterfactual medical images conditioned solely on natural\nlanguage descriptions remains completely unexplored. Addressing this gap would\nenable powerful clinical and research applications, such as personalized\ncounterfactual explanations, simulation of disease progression scenarios, and\nenhanced medical training by visualizing hypothetical medical conditions in\nrealistic detail. Our work takes a meaningful step toward addressing this\nchallenge by introducing a framework capable of generating high-resolution 3D\ncounterfactual medical images of synthesized patients guided by free-form\nlanguage prompts. We adapt state-of-the-art 3D diffusion models with\nenhancements from Simple Diffusion and incorporate augmented conditioning to\nimprove text alignment and image quality. To our knowledge, this represents the\nfirst demonstration of a language-guided native-3D diffusion model applied\nspecifically to neurological imaging data, where faithful three-dimensional\nmodeling is essential to represent the brain's three-dimensional structure.\nThrough results on two distinct neurological MRI datasets, our framework\nsuccessfully simulates varying counterfactual lesion loads in Multiple\nSclerosis (MS), and cognitive states in Alzheimer's disease, generating\nhigh-quality images while preserving subject fidelity in synthetically\ngenerated medical images. Our results lay the groundwork for prompt-driven\ndisease progression analysis within 3D medical imaging."}
{"id": "2509.05983", "pdf": "https://arxiv.org/pdf/2509.05983.pdf", "abs": "https://arxiv.org/abs/2509.05983", "title": "TSPC: A Two-Stage Phoneme-Centric Architecture for code-switching Vietnamese-English Speech Recognition", "authors": ["Minh N. H. Nguyen", "Anh Nguyen Tran", "Dung Truong Dinh", "Nam Van Vo"], "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "Code-switching (CS) presents a significant challenge for general Auto-Speech\nRecognition (ASR) systems. Existing methods often fail to capture the subtle\nphonological shifts inherent in CS scenarios. The challenge is particularly\ndifficult for language pairs like Vietnamese and English, where both distinct\nphonological features and the ambiguity arising from similar sound recognition\nare present. In this paper, we propose a novel architecture for\nVietnamese-English CS ASR, a Two-Stage Phoneme-Centric model (TSPC). The TSPC\nemploys a phoneme-centric approach, built upon an extended Vietnamese phoneme\nset as an intermediate representation to facilitate mixed-lingual modeling.\nExperimental results demonstrate that TSPC consistently outperforms existing\nbaselines, including PhoWhisper-base, in Vietnamese-English CS ASR, achieving a\nsignificantly lower word error rate of 20.8\\% with reduced training resources.\nFurthermore, the phonetic-based two-stage architecture enables phoneme\nadaptation and language conversion to enhance ASR performance in complex CS\nVietnamese-English ASR scenarios."}
{"id": "2509.06093", "pdf": "https://arxiv.org/pdf/2509.06093.pdf", "abs": "https://arxiv.org/abs/2509.06093", "title": "Language Native Lightly Structured Databases for Large Language Model Driven Composite Materials Research", "authors": ["Yuze Liu", "Zhaoyuan Zhang", "Xiangsheng Zeng", "Yihe Zhang", "Leping Yu", "Lejia Wang", "Xi Yu"], "categories": ["cs.DB", "cond-mat.mtrl-sci", "cs.AI", "cs.CL"], "comment": null, "summary": "Chemical and materials research has traditionally relied heavily on knowledge\nnarrative, with progress often driven by language-based descriptions of\nprinciples, mechanisms, and experimental experiences, rather than tables,\nlimiting what conventional databases and ML can exploit. We present a\nlanguage-native database for boron nitride nanosheet (BNNS) polymer thermally\nconductive composites that captures lightly structured information from papers\nacross preparation, characterization, theory-computation, and mechanistic\nreasoning, with evidence-linked snippets. Records are organized in a\nheterogeneous database and queried via composite retrieval with semantics, key\nwords and value filters. The system can synthesizes literature into accurate,\nverifiable, and expert style guidance. This substrate enables high fidelity\nefficient Retrieval Augmented Generation (RAG) and tool augmented agents to\ninterleave retrieval with reasoning and deliver actionable SOP. The framework\nsupplies the language rich foundation required for LLM-driven materials\ndiscovery."}
{"id": "2509.06160", "pdf": "https://arxiv.org/pdf/2509.06160.pdf", "abs": "https://arxiv.org/abs/2509.06160", "title": "Reverse-Engineered Reasoning for Open-Ended Generation", "authors": ["Haozhe Wang", "Haoran Que", "Qixin Xu", "Minghao Liu", "Wangchunshu Zhou", "Jiazhan Feng", "Wanjun Zhong", "Wei Ye", "Tong Yang", "Wenhao Huang", "Ge Zhang", "Fangzhen Lin"], "categories": ["cs.AI", "cs.CL"], "comment": "Preprint", "summary": "While the ``deep reasoning'' paradigm has spurred significant advances in\nverifiable domains like mathematics, its application to open-ended, creative\ngeneration remains a critical challenge. The two dominant methods for\ninstilling reasoning -- reinforcement learning (RL) and instruction\ndistillation -- falter in this area; RL struggles with the absence of clear\nreward signals and high-quality reward models, while distillation is\nprohibitively expensive and capped by the teacher model's capabilities. To\novercome these limitations, we introduce REverse-Engineered Reasoning (REER), a\nnew paradigm that fundamentally shifts the approach. Instead of building a\nreasoning process ``forwards'' through trial-and-error or imitation, REER works\n``backwards'' from known-good solutions to computationally discover the latent,\nstep-by-step deep reasoning process that could have produced them. Using this\nscalable, gradient-free approach, we curate and open-source DeepWriting-20K, a\nlarge-scale dataset of 20,000 deep reasoning trajectories for open-ended tasks.\nOur model, DeepWriter-8B, trained on this data, not only surpasses strong\nopen-source baselines but also achieves performance competitive with, and at\ntimes superior to, leading proprietary models like GPT-4o and Claude 3.5."}
{"id": "2509.06174", "pdf": "https://arxiv.org/pdf/2509.06174.pdf", "abs": "https://arxiv.org/abs/2509.06174", "title": "From Long to Short: LLMs Excel at Trimming Own Reasoning Chains", "authors": ["Wei Han", "Geng Zhan", "Sicheng Yu", "Chenyu Wang", "Bryan Hooi"], "categories": ["cs.AI", "cs.CL"], "comment": "21 pages, 5 figures, 7 tables", "summary": "O1/R1 style large reasoning models (LRMs) signal a substantial leap forward\nover conventional instruction-following LLMs. By applying test-time scaling to\ngenerate extended reasoning paths, they establish many SOTAs across a wide\nrange of complex reasoning tasks. However, recent studies show that LRMs are\nprone to suffer from overthinking -- the tendency to overcomplicate simple\nproblems, leading to excessive strategy switching and long, convoluted\nreasoning traces that hinder their interpretability. To mitigate this issue, we\nconduct a systematic investigation into the reasoning efficiency of a broad set\nof LRMs and uncover a common dilemma: the difficulty in balancing multiple\ngeneration objectives such as correctness and brevity. Based on this discovery,\nwe propose a test-time scaling method, EDIT (Efficient Dynamic Inference\nTrimming), which efficiently guides LRMs to identify the shortest correct\nreasoning paths at test time. EDIT employs constraint-guided generation while\njointly tracking length and answer distributions under varying constraints,\nallowing it to select responses that strike an optimal balance between\nconciseness and correctness. Extensive experiments across diverse models and\ndatasets show that EDIT substantially enhance the reasoning efficiency,\nproducing compact yet informative outputs that improve readability and user\nexperience."}
{"id": "2509.06195", "pdf": "https://arxiv.org/pdf/2509.06195.pdf", "abs": "https://arxiv.org/abs/2509.06195", "title": "Language Bias in Information Retrieval: The Nature of the Beast and Mitigation Methods", "authors": ["Jinrui Yang", "Fan Jiang", "Timothy Baldwin"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "Accepted at EMNLP MRL 2024", "summary": "Language fairness in multilingual information retrieval (MLIR) systems is\ncrucial for ensuring equitable access to information across diverse languages.\nThis paper sheds light on the issue, based on the assumption that queries in\ndifferent languages, but with identical semantics, should yield equivalent\nranking lists when retrieving on the same multilingual documents. We evaluate\nthe degree of fairness using both traditional retrieval methods, and a DPR\nneural ranker based on mBERT and XLM-R. Additionally, we introduce `LaKDA', a\nnovel loss designed to mitigate language biases in neural MLIR approaches. Our\nanalysis exposes intrinsic language biases in current MLIR technologies, with\nnotable disparities across the retrieval methods, and the effectiveness of\nLaKDA in enhancing language fairness."}
{"id": "2509.06221", "pdf": "https://arxiv.org/pdf/2509.06221.pdf", "abs": "https://arxiv.org/abs/2509.06221", "title": "Beamforming-LLM: What, Where and When Did I Miss?", "authors": ["Vishal Choudhari"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "We present Beamforming-LLM, a system that enables users to semantically\nrecall conversations they may have missed in multi-speaker environments. The\nsystem combines spatial audio capture using a microphone array with\nretrieval-augmented generation (RAG) to support natural language queries such\nas, \"What did I miss when I was following the conversation on dogs?\"\nDirectional audio streams are separated using beamforming, transcribed with\nWhisper, and embedded into a vector database using sentence encoders. Upon\nreceiving a user query, semantically relevant segments are retrieved,\ntemporally aligned with non-attended segments, and summarized using a\nlightweight large language model (GPT-4o-mini). The result is a user-friendly\ninterface that provides contrastive summaries, spatial context, and timestamped\naudio playback. This work lays the foundation for intelligent auditory memory\nsystems and has broad applications in assistive technology, meeting\nsummarization, and context-aware personal spatial computing."}
{"id": "2509.06283", "pdf": "https://arxiv.org/pdf/2509.06283.pdf", "abs": "https://arxiv.org/abs/2509.06283", "title": "SFR-DeepResearch: Towards Effective Reinforcement Learning for Autonomously Reasoning Single Agents", "authors": ["Xuan-Phi Nguyen", "Shrey Pandit", "Revanth Gangi Reddy", "Austin Xu", "Silvio Savarese", "Caiming Xiong", "Shafiq Joty"], "categories": ["cs.AI", "cs.CL"], "comment": "Technical Report", "summary": "Equipping large language models (LLMs) with complex, interleaved reasoning\nand tool-use capabilities has become a key focus in agentic AI research,\nespecially with recent advances in reasoning-oriented (``thinking'') models.\nSuch capabilities are key to unlocking a number of important applications. One\nsuch application is Deep Research (DR), which requires extensive search and\nreasoning over many sources. Our work in this paper focuses on the development\nof native Autonomous Single-Agent models for DR featuring minimal web crawling\nand Python tool integration. Unlike multi-agent systems, where agents take up\npre-defined roles and are told what to do at each step in a static workflow, an\nautonomous single-agent determines its next action dynamically based on\ncontext, without manual directive. While prior work has proposed training\nrecipes for base or instruction-tuned LLMs, we focus on continual reinforcement\nlearning (RL) of reasoning-optimized models to further enhance agentic skills\nwhile preserving reasoning ability. Towards this end, we propose a simple RL\nrecipe with entirely synthetic data, which we apply to various open-source\nLLMs. Our best variant SFR-DR-20B achieves up to 28.7% on Humanity's Last Exam\nbenchmark. In addition, we conduct key analysis experiments to provide more\ninsights into our methodologies."}
{"id": "2509.06415", "pdf": "https://arxiv.org/pdf/2509.06415.pdf", "abs": "https://arxiv.org/abs/2509.06415", "title": "Index-Preserving Lightweight Token Pruning for Efficient Document Understanding in Vision-Language Models", "authors": ["Jaemin Son", "Sujin Choi", "Inyong Yun"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Submitted to ICASSP 2026", "summary": "Recent progress in vision-language models (VLMs) has led to impressive\nresults in document understanding tasks, but their high computational demands\nremain a challenge. To mitigate the compute burdens, we propose a lightweight\ntoken pruning framework that filters out non-informative background regions\nfrom document images prior to VLM processing. A binary patch-level classifier\nremoves non-text areas, and a max-pooling refinement step recovers fragmented\ntext regions to enhance spatial coherence. Experiments on real-world document\ndatasets demonstrate that our approach substantially lowers computational\ncosts, while maintaining comparable accuracy."}
{"id": "2509.06733", "pdf": "https://arxiv.org/pdf/2509.06733.pdf", "abs": "https://arxiv.org/abs/2509.06733", "title": "Reinforcement Learning Foundations for Deep Research Systems: A Survey", "authors": ["Wenjun Li", "Zhi Chen", "Jingru Lin", "Hannan Cao", "Wei Han", "Sheng Liang", "Zhi Zhang", "Kuicai Dong", "Dexun Li", "Chen Zhang", "Yong Liu"], "categories": ["cs.AI", "cs.CL"], "comment": "38 pages, first version", "summary": "Deep research systems, agentic AI that solve complex, multi-step tasks by\ncoordinating reasoning, search across the open web and user files, and tool\nuse, are moving toward hierarchical deployments with a Planner, Coordinator,\nand Executors. In practice, training entire stacks end-to-end remains\nimpractical, so most work trains a single planner connected to core tools such\nas search, browsing, and code. While SFT imparts protocol fidelity, it suffers\nfrom imitation and exposure biases and underuses environment feedback.\nPreference alignment methods such as DPO are schema and proxy-dependent,\noff-policy, and weak for long-horizon credit assignment and multi-objective\ntrade-offs. A further limitation of SFT and DPO is their reliance on human\ndefined decision points and subskills through schema design and labeled\ncomparisons. Reinforcement learning aligns with closed-loop, tool-interaction\nresearch by optimizing trajectory-level policies, enabling exploration,\nrecovery behaviors, and principled credit assignment, and it reduces dependence\non such human priors and rater biases.\n  This survey is, to our knowledge, the first dedicated to the RL foundations\nof deep research systems. It systematizes work after DeepSeek-R1 along three\naxes: (i) data synthesis and curation; (ii) RL methods for agentic research\ncovering stability, sample efficiency, long context handling, reward and credit\ndesign, multi-objective optimization, and multimodal integration; and (iii)\nagentic RL training systems and frameworks. We also cover agent architecture\nand coordination, as well as evaluation and benchmarks, including recent QA,\nVQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We\ndistill recurring patterns, surface infrastructure bottlenecks, and offer\npractical guidance for training robust, transparent deep research agents with\nRL."}
{"id": "2509.06736", "pdf": "https://arxiv.org/pdf/2509.06736.pdf", "abs": "https://arxiv.org/abs/2509.06736", "title": "VehicleWorld: A Highly Integrated Multi-Device Environment for Intelligent Vehicle Interaction", "authors": ["Jie Yang", "Jiajun Chen", "Zhangyue Yin", "Shuo Chen", "Yuxin Wang", "Yiran Guo", "Yuan Li", "Yining Zheng", "Xuanjing Huang", "Xipeng Qiu"], "categories": ["cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "Intelligent vehicle cockpits present unique challenges for API Agents,\nrequiring coordination across tightly-coupled subsystems that exceed typical\ntask environments' complexity. Traditional Function Calling (FC) approaches\noperate statelessly, requiring multiple exploratory calls to build\nenvironmental awareness before execution, leading to inefficiency and limited\nerror recovery. We introduce VehicleWorld, the first comprehensive environment\nfor the automotive domain, featuring 30 modules, 250 APIs, and 680 properties\nwith fully executable implementations that provide real-time state information\nduring agent execution. This environment enables precise evaluation of vehicle\nagent behaviors across diverse, challenging scenarios. Through systematic\nanalysis, we discovered that direct state prediction outperforms function\ncalling for environmental control. Building on this insight, we propose\nState-based Function Call (SFC), a novel approach that maintains explicit\nsystem state awareness and implements direct state transitions to achieve\ntarget conditions. Experimental results demonstrate that SFC significantly\noutperforms traditional FC approaches, achieving superior execution accuracy\nand reduced latency. We have made all implementation code publicly available on\nGithub https://github.com/OpenMOSS/VehicleWorld."}
{"id": "2509.06822", "pdf": "https://arxiv.org/pdf/2509.06822.pdf", "abs": "https://arxiv.org/abs/2509.06822", "title": "RAFFLES: Reasoning-based Attribution of Faults for LLM Systems", "authors": ["Chenyang Zhu", "Spencer Hong", "Jingyu Wu", "Kushal Chawla", "Charlotte Tang", "Youbing Yin", "Nathan Wolfe", "Erin Babinsky", "Daben Liu"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We have reached a critical roadblock in the development and enhancement of\nlong-horizon, multi-component LLM agentic systems: it is incredibly tricky to\nidentify where these systems break down and why. Evaluation capabilities that\ncurrently exist today (e.g., single pass LLM-as-a-judge) are limited in that\nthey often focus on individual metrics or capabilities, end-to-end outcomes,\nand are narrowly grounded on the preferences of humans. We argue that to match\nthe agentic capabilities, evaluation frameworks must also be able to reason,\nprobe, iterate, and understand the complex logic passing through these systems\nover long horizons. In this paper, we present RAFFLES - an evaluation\narchitecture that incorporates reasoning and iterative refinement.\nSpecifically, RAFFLES operates as an iterative, multi-component pipeline, using\na central Judge to systematically investigate faults and a set of specialized\nEvaluators to assess not only the system's components but also the quality of\nthe reasoning by the Judge itself, thereby building a history of hypotheses. We\ntested RAFFLES against several baselines on the Who&When dataset, a benchmark\ndesigned to diagnose the \"who\" (agent) and \"when\" (step) of a system's failure.\nRAFFLES outperforms these baselines, achieving an agent-step fault pair\naccuracy of over 43% on the Algorithmically-Generated dataset (a substantial\nincrease from the previously published best of 16.6%) and over 20% on the\nHand-Crafted dataset (surpassing the previously published best of 8.8%). These\nresults demonstrate a key step towards introducing automated fault detection\nfor autonomous systems over labor-intensive manual human review."}
{"id": "2509.06861", "pdf": "https://arxiv.org/pdf/2509.06861.pdf", "abs": "https://arxiv.org/abs/2509.06861", "title": "Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet", "authors": ["James Xu Zhao", "Bryan Hooi", "See-Kiong Ng"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "20 pages, 4 figures, 6 tables", "summary": "Test-time scaling increases inference-time computation by allowing models to\ngenerate long reasoning chains, and has shown strong performance across many\ndomains. However, in this work, we show that this approach is not yet effective\nfor knowledge-intensive tasks, where high factual accuracy and low\nhallucination rates are essential. We conduct a comprehensive evaluation of\ntest-time scaling using 12 reasoning models on two knowledge-intensive\nbenchmarks. Our results reveal that increasing test-time computation does not\nconsistently improve accuracy and, in many cases, it even leads to more\nhallucinations. We then analyze how extended reasoning affects hallucination\nbehavior. We find that reduced hallucinations often result from the model\nchoosing to abstain after thinking more, rather than from improved factual\nrecall. Conversely, for some models, longer reasoning encourages attempts on\npreviously unanswered questions, many of which result in hallucinations. Case\nstudies show that extended reasoning can induce confirmation bias, leading to\noverconfident hallucinations. Despite these limitations, we observe that\ncompared to non-thinking, enabling thinking remains beneficial. Code and data\nare available at https://github.com/XuZhao0/tts-knowledge"}
{"id": "2509.06917", "pdf": "https://arxiv.org/pdf/2509.06917.pdf", "abs": "https://arxiv.org/abs/2509.06917", "title": "Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents", "authors": ["Jiacheng Miao", "Joe R. Davis", "Jonathan K. Pritchard", "James Zou"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "We introduce Paper2Agent, an automated framework that converts research\npapers into AI agents. Paper2Agent transforms research output from passive\nartifacts into active systems that can accelerate downstream use, adoption, and\ndiscovery. Conventional research papers require readers to invest substantial\neffort to understand and adapt a paper's code, data, and methods to their own\nwork, creating barriers to dissemination and reuse. Paper2Agent addresses this\nchallenge by automatically converting a paper into an AI agent that acts as a\nknowledgeable research assistant. It systematically analyzes the paper and the\nassociated codebase using multiple agents to construct a Model Context Protocol\n(MCP) server, then iteratively generates and runs tests to refine and robustify\nthe resulting MCP. These paper MCPs can then be flexibly connected to a chat\nagent (e.g. Claude Code) to carry out complex scientific queries through\nnatural language while invoking tools and workflows from the original paper. We\ndemonstrate Paper2Agent's effectiveness in creating reliable and capable paper\nagents through in-depth case studies. Paper2Agent created an agent that\nleverages AlphaGenome to interpret genomic variants and agents based on ScanPy\nand TISSUE to carry out single-cell and spatial transcriptomics analyses. We\nvalidate that these paper agents can reproduce the original paper's results and\ncan correctly carry out novel user queries. By turning static papers into\ndynamic, interactive AI agents, Paper2Agent introduces a new paradigm for\nknowledge dissemination and a foundation for the collaborative ecosystem of AI\nco-scientists."}
{"id": "2509.06920", "pdf": "https://arxiv.org/pdf/2509.06920.pdf", "abs": "https://arxiv.org/abs/2509.06920", "title": "An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and Detection", "authors": ["Haywood Gelman", "John D. Hastings", "David Kenley"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY", "C.2.0; I.2.7; K.4.1; H.3.3"], "comment": "6 pages, 5 figures, 5 tables", "summary": "Insider threats are a growing organizational problem due to the complexity of\nidentifying their technical and behavioral elements. A large research body is\ndedicated to the study of insider threats from technological, psychological,\nand educational perspectives. However, research in this domain has been\ngenerally dependent on datasets that are static and limited access which\nrestricts the development of adaptive detection models. This study introduces a\nnovel, ethically grounded approach that uses the large language model (LLM)\nClaude Sonnet 3.7 to dynamically synthesize syslog messages, some of which\ncontain indicators of insider threat scenarios. The messages reflect real-world\ndata distributions by being highly imbalanced (1% insider threats). The syslogs\nwere analyzed for insider threats by both Claude Sonnet 3.7 and GPT-4o, with\ntheir performance evaluated through statistical metrics including precision,\nrecall, MCC, and ROC AUC. Sonnet 3.7 consistently outperformed GPT-4o across\nnearly all metrics, particularly in reducing false alarms and improving\ndetection accuracy. The results show strong promise for the use of LLMs in\nsynthetic dataset generation and insider threat detection."}
{"id": "2509.06941", "pdf": "https://arxiv.org/pdf/2509.06941.pdf", "abs": "https://arxiv.org/abs/2509.06941", "title": "Outcome-based Exploration for LLM Reasoning", "authors": ["Yuda Song", "Julia Kempe", "Remi Munos"], "categories": ["cs.LG", "cs.CL"], "comment": "26 pages, 11 figures", "summary": "Reinforcement learning (RL) has emerged as a powerful method for improving\nthe reasoning abilities of large language models (LLMs). Outcome-based RL,\nwhich rewards policies solely for the correctness of the final answer, yields\nsubstantial accuracy gains but also induces a systematic loss in generation\ndiversity. This collapse undermines real-world performance, where diversity is\ncritical for test-time scaling. We analyze this phenomenon by viewing RL\npost-training as a sampling process and show that, strikingly, RL can reduce\neffective diversity even on the training set relative to the base model. Our\nstudy highlights two central findings: (i) a transfer of diversity degradation,\nwhere reduced diversity on solved problems propagates to unsolved ones, and\n(ii) the tractability of the outcome space, since reasoning tasks admit only a\nlimited set of distinct answers. Motivated by these insights, we propose\noutcome-based exploration, which assigns exploration bonuses according to final\noutcomes. We introduce two complementary algorithms: historical exploration,\nwhich encourages rarely observed answers via UCB-style bonuses, and batch\nexploration, which penalizes within-batch repetition to promote test-time\ndiversity. Experiments on standard competition math with Llama and Qwen models\ndemonstrate that both methods improve accuracy while mitigating diversity\ncollapse. On the theoretical side, we formalize the benefit of outcome-based\nexploration through a new model of outcome-based bandits. Together, these\ncontributions chart a practical path toward RL methods that enhance reasoning\nwithout sacrificing the diversity essential for scalable deployment."}
{"id": "2509.06945", "pdf": "https://arxiv.org/pdf/2509.06945.pdf", "abs": "https://arxiv.org/abs/2509.06945", "title": "Interleaving Reasoning for Better Text-to-Image Generation", "authors": ["Wenxuan Huang", "Shuang Chen", "Zheyong Xie", "Shaosheng Cao", "Shixiang Tang", "Yufan Shen", "Qingyu Yin", "Wenbo Hu", "Xiaoman Wang", "Yuntian Tang", "Junbo Qiao", "Yue Guo", "Yao Hu", "Zhenfei Yin", "Philip Torr", "Yu Cheng", "Wanli Ouyang", "Shaohui Lin"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Unified multimodal understanding and generation models recently have achieve\nsignificant improvement in image generation capability, yet a large gap remains\nin instruction following and detail preservation compared to systems that\ntightly couple comprehension with generation such as GPT-4o. Motivated by\nrecent advances in interleaving reasoning, we explore whether such reasoning\ncan further improve Text-to-Image (T2I) generation. We introduce Interleaving\nReasoning Generation (IRG), a framework that alternates between text-based\nthinking and image synthesis: the model first produces a text-based thinking to\nguide an initial image, then reflects on the result to refine fine-grained\ndetails, visual quality, and aesthetics while preserving semantics. To train\nIRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),\nwhich targets two sub-goals: (1) strengthening the initial think-and-generate\nstage to establish core content and base quality, and (2) enabling high-quality\ntextual reflection and faithful implementation of those refinements in a\nsubsequent image. We curate IRGL-300K, a dataset organized into six decomposed\nlearning modes that jointly cover learning text-based thinking, and full\nthinking-image trajectories. Starting from a unified foundation model that\nnatively emits interleaved text-image outputs, our two-stage training first\nbuilds robust thinking and reflection, then efficiently tunes the IRG pipeline\nin the full thinking-image trajectory data. Extensive experiments show SoTA\nperformance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,\nGenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality\nand fine-grained fidelity. The code, model weights and datasets will be\nreleased in: https://github.com/Osilly/Interleaving-Reasoning-Generation ."}
{"id": "2309.14394", "pdf": "https://arxiv.org/pdf/2309.14394.pdf", "abs": "https://arxiv.org/abs/2309.14394", "title": "Multiple Noises in Diffusion Model for Semi-Supervised Multi-Domain Translation", "authors": ["Tsiry Mayet", "Simon Bernard", "Romain Herault", "Clement Chatelain"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "In this work, we address the challenge of multi-domain translation, where the\nobjective is to learn mappings between arbitrary configurations of domains\nwithin a defined set (such as $(D_1, D_2)\\rightarrow{}D_3$,\n$D_2\\rightarrow{}(D_1, D_3)$, $D_3\\rightarrow{}D_1$, etc. for three domains)\nwithout the need for separate models for each specific translation\nconfiguration, enabling more efficient and flexible domain translation. We\nintroduce Multi-Domain Diffusion (MDD), a method with dual purposes: i)\nreconstructing any missing views for new data objects, and ii) enabling\nlearning in semi-supervised contexts with arbitrary supervision configurations.\nMDD achieves these objectives by exploiting the noise formulation of diffusion\nmodels, specifically modeling one noise level per domain. Similar to existing\ndomain translation approaches, MDD learns the translation between any\ncombination of domains. However, unlike prior work, our formulation inherently\nhandles semi-supervised learning without modification by representing missing\nviews as noise in the diffusion process. We evaluate our approach through\ndomain translation experiments on BL3NDT, a multi-domain synthetic dataset\ndesigned for challenging semantic domain inversion, the BraTS2020 dataset, and\nthe CelebAMask-HQ dataset."}
{"id": "2311.01766", "pdf": "https://arxiv.org/pdf/2311.01766.pdf", "abs": "https://arxiv.org/abs/2311.01766", "title": "Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation", "authors": ["Xin Yuan", "Jie Guo", "Weidong Qiu", "Zheng Huang", "Shujun Li"], "categories": ["cs.CL", "cs.CV"], "comment": "Accepted and published by EMNLP 2023. Details can be found in\n  https://aclanthology.org/2023.emnlp-main.259", "summary": "Mis- and disinformation online have become a major societal problem as major\nsources of online harms of different kinds. One common form of mis- and\ndisinformation is out-of-context (OOC) information, where different pieces of\ninformation are falsely associated, e.g., a real image combined with a false\ntextual caption or a misleading textual description. Although some past studies\nhave attempted to defend against OOC mis- and disinformation through external\nevidence, they tend to disregard the role of different pieces of evidence with\ndifferent stances. Motivated by the intuition that the stance of evidence\nrepresents a bias towards different detection results, we propose a stance\nextraction network (SEN) that can extract the stances of different pieces of\nmulti-modal evidence in a unified framework. Moreover, we introduce a\nsupport-refutation score calculated based on the co-occurrence relations of\nnamed entities into the textual SEN. Extensive experiments on a public\nlarge-scale dataset demonstrated that our proposed method outperformed the\nstate-of-the-art baselines, with the best model achieving a performance gain of\n3.2% in accuracy. The source code and checkpoints are publicly available at\nhttps://github.com/yx3266/SEN."}
{"id": "2402.11282", "pdf": "https://arxiv.org/pdf/2402.11282.pdf", "abs": "https://arxiv.org/abs/2402.11282", "title": "Grammaticality illusion or ambiguous interpretation? Event-related potentials reveal the nature of the missing-NP effect in Mandarin centre-embedded structures", "authors": ["Qihang Yang", "Caimei Yang", "Yu Liao", "Ziman Zhuang"], "categories": ["cs.CL"], "comment": null, "summary": "In several languages, omitting a verb phrase (VP) in double centre-embedded\nstructures creates a grammaticality illusion. Similar illusion also exhibited\nin Mandarin missing-NP double centre-embedded structures. However, there is no\nconsensus on its very nature. Instead of treating it as grammaticality\nillusion, we argue that ambiguous interpretations of verbs can best account for\nthis phenomenon in Mandarin. To further support this hypothesis, we conducted\ntwo electroencephalography (EEG) experiments on quasi double centre-embedded\nstructures whose complexity is reduced by placing the self-embedding relative\nclauses into the sentence's subject position. Experiment 1 showed that similar\nphenomenon even exhibited in this structure, evidenced by an absence of P600\neffect and a presence of N400 effect. In Experiment 2, providing semantic cues\nto reduce ambiguity dispelled this illusion, as evidenced by a P600 effect. We\ninterpret the results under garden-path theory and propose that word-order\ndifference may account for this cross-linguistic variation."}
{"id": "2402.12226", "pdf": "https://arxiv.org/pdf/2402.12226.pdf", "abs": "https://arxiv.org/abs/2402.12226", "title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling", "authors": ["Jun Zhan", "Junqi Dai", "Jiasheng Ye", "Yunhua Zhou", "Dong Zhang", "Zhigeng Liu", "Xin Zhang", "Ruibin Yuan", "Ge Zhang", "Linyang Li", "Hang Yan", "Jie Fu", "Tao Gui", "Tianxiang Sun", "Yu-Gang Jiang", "Xipeng Qiu"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "28 pages, 16 figures, under review, work in progress", "summary": "We introduce AnyGPT, an any-to-any multimodal language model that utilizes\ndiscrete representations for the unified processing of various modalities,\nincluding speech, text, images, and music. AnyGPT can be trained stably without\nany alterations to the current large language model (LLM) architecture or\ntraining paradigms. Instead, it relies exclusively on data-level preprocessing,\nfacilitating the seamless integration of new modalities into LLMs, akin to the\nincorporation of new languages. We build a multimodal text-centric dataset for\nmultimodal alignment pre-training. Utilizing generative models, we synthesize\nthe first large-scale any-to-any multimodal instruction dataset. It consists of\n108k samples of multi-turn conversations that intricately interweave various\nmodalities, thus equipping the model to handle arbitrary combinations of\nmultimodal inputs and outputs. Experimental results demonstrate that AnyGPT is\ncapable of facilitating any-to-any multimodal conversation while achieving\nperformance comparable to specialized models across all modalities, proving\nthat discrete representations can effectively and conveniently unify multiple\nmodalities within a language model. Demos are shown in\nhttps://junzhan2000.github.io/AnyGPT.github.io/"}
{"id": "2402.15449", "pdf": "https://arxiv.org/pdf/2402.15449.pdf", "abs": "https://arxiv.org/abs/2402.15449", "title": "Repetition Improves Language Model Embeddings", "authors": ["Jacob Mitchell Springer", "Suhas Kotha", "Daniel Fried", "Graham Neubig", "Aditi Raghunathan"], "categories": ["cs.CL", "cs.LG"], "comment": "ICLR 2025", "summary": "Bidirectional models are considered essential for strong text embeddings.\nRecent approaches to adapt autoregressive language models (LMs) into strong\ntext embedding models have largely had the requirement to modify the LM\narchitecture to be bidirectional. We challenge this premise by introducing\n\"echo embeddings\" which converts autoregressive LMs into high quality text\nembedding models without changing the architecture or requiring fine-tuning. By\nrepeating the input and extracting embeddings from the repeated tokens -- which\nhave access to all original tokens -- echo embeddings improve over classical LM\nembeddings by over 5% in zero-shot settings. Our zero-shot embeddings nearly\nmatch those obtained by bidirectionally-converted LMs that undergo additional\nmasked-language modeling training. Echo embeddings are also compatible with\nsupervised fine-tuning, matching or outperforming bidirectionally-converted LMs\nin an apples-to-apples comparison, even with an identical compute budget during\ntraining and inference. Overall, repetition is a simple and effective strategy\nto circumvent the need for bidirectional attention in embedding models, paving\nthe way towards a unified architecture for all NLP tasks."}
{"id": "2405.15454", "pdf": "https://arxiv.org/pdf/2405.15454.pdf", "abs": "https://arxiv.org/abs/2405.15454", "title": "Linearly Controlled Language Generation with Performative Guarantees", "authors": ["Emily Cheng", "Carmen Amo Alonso"], "categories": ["cs.CL", "cs.SY", "eess.SY"], "comment": "Under review", "summary": "The increasing prevalence of Large Language Models (LMs) in critical\napplications highlights the need for controlled language generation strategies\nthat are not only computationally efficient but that also enjoy performance\nguarantees. To achieve this, we use a common model of concept semantics as\nlinearly represented in an LM's latent space. In particular, we take the view\nthat natural language generation traces a trajectory in this continuous\nsemantic space, realized by the language model's hidden activations. This view\npermits a control-theoretic treatment of text generation in latent space, in\nwhich we propose a lightweight, gradient-free intervention that dynamically\nsteers trajectories away from regions corresponding to undesired meanings. In\nparticular, we propose to directly intervene the activations of the token that\nis being generated in embedding space in an online fashion. Crucially, we do\nnot simply steer activations towards a desirable region. Instead, our method\nrelies on classical techniques from control theory to precisely control\nactivations in a context-dependent way, and guarantees that they are brought\ninto a specific pre-defined region of embedding space that corresponds to\nallowed semantics. Our intervention is computed in closed-form according to an\noptimal controller formulation, minimally impacting generation time. This\ncontrol of the activations in embedding space allows for fine-grained steering\nof attributes of the generated sequence. We demonstrate the effectiveness of\nour approach on different objectives-- toxicity avoidance and sentiment\ncontrol-- while maintaining text quality."}
{"id": "2406.06056", "pdf": "https://arxiv.org/pdf/2406.06056.pdf", "abs": "https://arxiv.org/abs/2406.06056", "title": "Synth-SBDH: A Synthetic Dataset of Social and Behavioral Determinants of Health for Clinical Text", "authors": ["Avijit Mitra", "Zhichao Yang", "Emily Druhl", "Raelene Goodwin", "Hong Yu"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2025 (main) Github:\n  https://github.com/avipartho/Synth-SBDH", "summary": "Social and behavioral determinants of health (SBDH) play a crucial role in\nhealth outcomes and are frequently documented in clinical text. Automatically\nextracting SBDH information from clinical text relies on publicly available\ngood-quality datasets. However, existing SBDH datasets exhibit substantial\nlimitations in their availability and coverage. In this study, we introduce\nSynth-SBDH, a novel synthetic dataset with detailed SBDH annotations,\nencompassing status, temporal information, and rationale across 15 SBDH\ncategories. We showcase the utility of Synth-SBDH on three tasks using\nreal-world clinical datasets from two distinct hospital settings, highlighting\nits versatility, generalizability, and distillation capabilities. Models\ntrained on Synth-SBDH consistently outperform counterparts with no Synth-SBDH\ntraining, achieving up to 63.75% macro-F improvements. Additionally, Synth-SBDH\nproves effective for rare SBDH categories and under-resource constraints while\nbeing substantially cheaper than expert-annotated real-world data. Human\nevaluation reveals a 71.06% Human-LLM alignment and uncovers areas for future\nrefinements."}
{"id": "2407.05022", "pdf": "https://arxiv.org/pdf/2407.05022.pdf", "abs": "https://arxiv.org/abs/2407.05022", "title": "A Principled Framework for Evaluating on Typologically Diverse Languages", "authors": ["Esther Ploeger", "Wessel Poelman", "Andreas Holck Høeg-Petersen", "Anders Schlichtkrull", "Miryam de Lhoneux", "Johannes Bjerva"], "categories": ["cs.CL"], "comment": "Revised version", "summary": "Beyond individual languages, multilingual natural language processing (NLP)\nresearch increasingly aims to develop models that perform well across languages\ngenerally. However, evaluating these systems on all the world's languages is\npractically infeasible. To attain generalizability, representative language\nsampling is essential. Previous work argues that generalizable multilingual\nevaluation sets should contain languages with diverse typological properties.\nHowever, 'typologically diverse' language samples have been found to vary\nconsiderably in this regard, and popular sampling methods are flawed and\ninconsistent. We present a language sampling framework for selecting highly\ntypologically diverse languages given a sampling frame, informed by language\ntypology. We compare sampling methods with a range of metrics and find that our\nsystematic methods consistently retrieve more typologically diverse language\nselections than previous methods in NLP. Moreover, we provide evidence that\nthis affects generalizability in multilingual model evaluation, emphasizing the\nimportance of diverse language sampling in NLP evaluation."}
{"id": "2408.04638", "pdf": "https://arxiv.org/pdf/2408.04638.pdf", "abs": "https://arxiv.org/abs/2408.04638", "title": "Affective Computing in the Era of Large Language Models: A Survey from the NLP Perspective", "authors": ["Yiqun Zhang", "Xiaocui Yang", "Xingle Xu", "Zeran Gao", "Yijie Huang", "Shiyi Mu", "Shi Feng", "Daling Wang", "Yifei Zhang", "Kaisong Song", "Ge Yu"], "categories": ["cs.CL", "cs.CY"], "comment": "Compared with the previous version, reinforcement learning has been\n  added (as a new section), including RLHF, RLVR, and RLAIF", "summary": "Affective Computing (AC) integrates computer science, psychology, and\ncognitive science to enable machines to recognize, interpret, and simulate\nhuman emotions across domains such as social media, finance, healthcare, and\neducation. AC commonly centers on two task families: Affective Understanding\n(AU) and Affective Generation (AG). While fine-tuned pre-trained language\nmodels (PLMs) have achieved solid AU performance, they often generalize poorly\nacross tasks and remain limited for AG, especially in producing diverse,\nemotionally appropriate responses. The advent of Large Language Models (LLMs)\n(e.g., ChatGPT and LLaMA) has catalyzed a paradigm shift by offering in-context\nlearning, broader world knowledge, and stronger sequence generation. This\nsurvey presents an NLP-oriented overview of AC in the LLM era. We (i)\nconsolidate traditional AC tasks and preliminary LLM-based studies; (ii) review\nadaptation techniques that improve AU/AG, including Instruction Tuning (full\nand parameter-efficient methods such as LoRA, P-/Prompt-Tuning), Prompt\nEngineering (zero/few-shot, chain-of-thought, agent-based prompting), and\nReinforcement Learning. For the latter, we summarize RL from human preferences\n(RLHF), verifiable/programmatic rewards (RLVR), and AI feedback (RLAIF), which\nprovide preference- or rule-grounded optimization signals that can help steer\nAU/AG toward empathy, safety, and planning, achieving finer-grained or\nmulti-objective control. To assess progress, we compile benchmarks and\nevaluation practices for both AU and AG. We also discuss open challenges-from\nethics, data quality, and safety to robust evaluation and resource\nefficiency-and outline research directions. We hope this survey clarifies the\nlandscape and offers practical guidance for building affect-aware, reliable,\nand responsible LLM systems."}
{"id": "2408.16482", "pdf": "https://arxiv.org/pdf/2408.16482.pdf", "abs": "https://arxiv.org/abs/2408.16482", "title": "Self-Alignment: Improving Alignment of Cultural Values in LLMs via In-Context Learning", "authors": ["Rochelle Choenni", "Ekaterina Shutova"], "categories": ["cs.CL"], "comment": null, "summary": "Improving the alignment of Large Language Models (LLMs) with respect to the\ncultural values that they encode has become an increasingly important topic. In\nthis work, we study whether we can exploit existing knowledge about cultural\nvalues at inference time to adjust model responses to cultural value probes. We\npresent a simple and inexpensive method that uses a combination of in-context\nlearning (ICL) and human survey data, and show that we can improve the\nalignment to cultural values across 5 models that include both English-centric\nand multilingual LLMs. Importantly, we show that our method could prove useful\nin test languages other than English and can improve alignment to the cultural\nvalues that correspond to a range of culturally diverse countries."}
{"id": "2409.11041", "pdf": "https://arxiv.org/pdf/2409.11041.pdf", "abs": "https://arxiv.org/abs/2409.11041", "title": "Towards No-Code Programming of Cobots: Experiments with Code Synthesis by Large Code Models for Conversational Programming", "authors": ["Chalamalasetti Kranti", "Sherzod Hakimov", "David Schlangen"], "categories": ["cs.CL", "cs.RO"], "comment": "Accepted to ITL4HRI workshop at RO-MAN 2025 conference", "summary": "While there has been a lot of research recently on robots in household\nenvironments, at the present time, most robots in existence can be found on\nshop floors, and most interactions between humans and robots happen there.\n``Collaborative robots'' (cobots) designed to work alongside humans on assembly\nlines traditionally require expert programming, limiting ability to make\nchanges, or manual guidance, limiting expressivity of the resulting programs.\nTo address these limitations, we explore using Large Language Models (LLMs),\nand in particular, their abilities of doing in-context learning, for\nconversational code generation. As a first step, we define RATS, the\n``Repetitive Assembly Task'', a 2D building task designed to lay the foundation\nfor simulating industry assembly scenarios. In this task, a `programmer'\ninstructs a cobot, using natural language, on how a certain assembly is to be\nbuilt; that is, the programmer induces a program, through natural language. We\ncreate a dataset that pairs target structures with various example instructions\n(human-authored, template-based, and model-generated) and example code. With\nthis, we systematically evaluate the capabilities of state-of-the-art LLMs for\nsynthesising this kind of code, given in-context examples. Evaluating in a\nsimulated environment, we find that LLMs are capable of generating accurate\n`first order code' (instruction sequences), but have problems producing\n`higher-order code' (abstractions such as functions, or use of loops)."}
{"id": "2410.07825", "pdf": "https://arxiv.org/pdf/2410.07825.pdf", "abs": "https://arxiv.org/abs/2410.07825", "title": "Extracting and Combining Abilities For Building Multi-lingual Ability-enhanced Large Language Models", "authors": ["Zhipeng Chen", "Kun Zhou", "Liang Song", "Wayne Xin Zhao", "Bingning Wang", "Weipeng Chen", "Ji-Rong Wen"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Main Conference", "summary": "Multi-lingual ability transfer has become increasingly important for the\nbroad application of large language models (LLMs). Existing work highly relies\non training with the multi-lingual ability-related data, which may not be\navailable for low-resource languages. To solve it, we propose a Multi-lingual\nAbilities Extraction and Combination approach, named as MAEC. Our key idea is\nto decompose and extract language-agnostic ability-related weights from LLMs,\nand combine them across different languages by simple addition and subtraction\noperations without training. Specifically, our MAEC consists of the extraction\nand combination stages. In the extraction stage, we firstly locate key neurons\nthat are highly related to specific abilities, and then employ them to extract\nthe transferable ability-related weights. In the combination stage, we further\nselect the ability-related tensors that mitigate the linguistic effects, and\ndesign a combining strategy based on them and the language-specific weights, to\nbuild the multi-lingual ability-enhanced LLM. To assess the effectiveness of\nour approach, we conduct extensive experiments on LLaMA-3 8B on mathematical\nand scientific tasks in both high-resource and low-resource lingual scenarios.\nExperiment results have shown that MAEC can effectively and efficiently extract\nand combine the advanced abilities, achieving comparable performance with PaLM.\nResources are available at https://github.com/RUCAIBox/MAET."}
{"id": "2410.09829", "pdf": "https://arxiv.org/pdf/2410.09829.pdf", "abs": "https://arxiv.org/abs/2410.09829", "title": "Conversational Code Generation: a Case Study of Designing a Dialogue System for Generating Driving Scenarios for Testing Autonomous Vehicles", "authors": ["Rimvydas Rubavicius", "Antonio Valerio Miceli-Barone", "Alex Lascarides", "Subramanian Ramamoorthy"], "categories": ["cs.CL", "cs.IR", "cs.RO"], "comment": "In Proceedings of GeCoIn 2025: Generative Code Intelligence Workshop,\n  co-located with ECAI-2025", "summary": "Cyber-physical systems like autonomous vehicles are tested in simulation\nbefore deployment, using domain-specific programs for scenario specification.\nTo aid the testing of autonomous vehicles in simulation, we design a natural\nlanguage interface, using an instruction-following large language model, to\nassist a non-coding domain expert in synthesising the desired scenarios and\nvehicle behaviours. We show that using it to convert utterances to the symbolic\nprogram is feasible, despite the very small training dataset. Human experiments\nshow that dialogue is critical to successful simulation generation, leading to\na 4.5 times higher success rate than a generation without engaging in extended\nconversation."}
{"id": "2411.04914", "pdf": "https://arxiv.org/pdf/2411.04914.pdf", "abs": "https://arxiv.org/abs/2411.04914", "title": "GASE: Generatively Augmented Sentence Encoding", "authors": ["Manuel Frank", "Haithem Afli"], "categories": ["cs.CL"], "comment": "EMNLP Findings 2025", "summary": "We propose a training-free approach to improve sentence embeddings leveraging\ntest-time compute by applying generative text models for data augmentation at\ninference time. Unlike conventional data augmentation that utilises synthetic\ntraining data, our approach does not require access to model parameters or the\ncomputational resources typically required for fine-tuning state-of-the-art\nmodels. Generatively Augmented Sentence Encoding variates the input text by\nparaphrasing, summarising, or extracting keywords, followed by pooling the\noriginal and synthetic embeddings. Experimental results on the Massive Text\nEmbedding Benchmark for Semantic Textual Similarity (STS) demonstrate\nperformance improvements across a range of embedding models using different\ngenerative models for augmentation. We find that generative augmentation leads\nto larger performance improvements for embedding models with lower baseline\nperformance. These findings suggest that integrating generative augmentation at\ninference time adds semantic diversity and can enhance the robustness and\ngeneralisability of sentence embeddings for embedding models. Our results show\nthat performance gains depend on the embedding model and the dataset."}
{"id": "2411.05665", "pdf": "https://arxiv.org/pdf/2411.05665.pdf", "abs": "https://arxiv.org/abs/2411.05665", "title": "Exploring the Limits of Large Language Models: A Systematic Evaluation of Masked Text Processing Ability through MskQA and MskCal", "authors": ["Fuka Matsuzaki", "Haru-Tada Sato"], "categories": ["cs.CL"], "comment": "19 pages", "summary": "This paper sheds light on the limitations of Large Language Models (LLMs) by\nrigorously evaluating their ability to process masked text. We introduce two\nnovel tasks: MskQA, measuring reasoning on masked question-answering datasets\nlike RealtimeQA, and MskCal, assessing numerical reasoning on masked arithmetic\nproblems.Testing GPT-4o and 4o-mini reveals that while LLMs exhibit some\nresilience to masked text, their performance is highly contingent on masking\nrates and semantic cues. Specifically, \"solid masking,\" where semantic clues\nare entirely absent, leads to a significant performance drop compared to\n\"partial lifting,\" where some semantic information is retained, indicating\nLLMs' reliance on surface-level patterns. Interestingly, GPT-4o consistently\noutperforms 4o-mini, particularly in MskCal, demonstrating a greater ability to\nhandle numerical reasoning with masked text. This underscores the crucial role\nof semantic cues in the reasoning process of LLMs. Our study illuminates the\ninterplay between background knowledge and reasoning ability in masked text\nprocessing, paving the way for a deeper understanding of LLM capabilities and\nlimitations, and highlighting the need for more robust evaluation methods to\naccurately assess their true comprehension abilities."}
{"id": "2411.07152", "pdf": "https://arxiv.org/pdf/2411.07152.pdf", "abs": "https://arxiv.org/abs/2411.07152", "title": "HierTOD: A Task-Oriented Dialogue System Driven by Hierarchical Goals", "authors": ["Lingbo Mo", "Shun Jiang", "Akash Maharaj", "Bernard Hishamunda", "Yunyao Li"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to DaSH Workshop at VLDB 2025", "summary": "Task-Oriented Dialogue (TOD) systems assist users in completing tasks through\nnatural language interactions, often relying on a single-layered workflow\nstructure for slot-filling in public tasks, such as hotel bookings. However, in\nenterprise environments, which involve rich domain-specific knowledge, TOD\nsystems face challenges due to task complexity and the lack of standardized\ndocumentation. In this work, we introduce HierTOD, an enterprise TOD system\ndriven by hierarchical goals that can support composite workflows. By focusing\non goal-driven interactions, our system serves a more proactive role,\nfacilitating mixed-initiative dialogue and improving task completion. Equipped\nwith components for natural language understanding, composite goal retriever,\ndialogue management, and response generation, backed by a well-organized data\nservice with domain knowledge base and retrieval engine, HierTOD delivers\nefficient task assistance as judged by human evaluators. Furthermore, our\nsystem implementation unifies two TOD paradigms: slot-filling for information\ncollection and step-by-step guidance for task execution. Our user study\ndemonstrates the effectiveness and helpfulness of HierTOD in performing both\nparadigms."}
{"id": "2411.16353", "pdf": "https://arxiv.org/pdf/2411.16353.pdf", "abs": "https://arxiv.org/abs/2411.16353", "title": "Lessons from Studying Two-Hop Latent Reasoning", "authors": ["Mikita Balesni", "Tomek Korbak", "Owain Evans"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models can use chain-of-thought (CoT) to externalize\nreasoning, potentially enabling oversight of capable LLM agents. Prior work has\nshown that models struggle at two-hop question-answering without CoT. This\ncapability is so basic that if it was a fundamental limitation, it would imply\nthat many complex agentic tasks would similarly require CoT. We investigate LLM\nlatent reasoning capabilities using two-hop question answering as a case study.\nPrevious work on the gap between latent and externalized two-hop reasoning\nproduced mixed evidence with inconclusive results. In this paper, we introduce\na controlled setting for investigating two-hop reasoning in LLMs, where a\npositive result provides definitive evidence for latent reasoning. We fine-tune\nLLMs (including Llama 3 8B and GPT-4o) on synthetic facts and test two-hop\nreasoning over these facts. By using synthetic facts, we rule out memorization\nand reasoning shortcuts as explanations for two-hop performance. We observe a\nnuanced picture: Models fail to compose two synthetic facts, but can succeed\nwhen one fact is synthetic and the other is natural. These results demonstrate\nthat LLMs are undeniably capable of latent two-hop reasoning, although it\nremains unclear how this ability scales with model size. Finally, we highlight\na lesson for researchers studying LLM reasoning: when drawing conclusions about\nLLM latent reasoning, one must be careful to avoid both spurious successes\n(that stem from memorization and reasoning shortcuts) and spurious failures\n(that may stem from artificial experimental setups, divorced from training\nsetups of frontier LLMs)."}
{"id": "2412.00098", "pdf": "https://arxiv.org/pdf/2412.00098.pdf", "abs": "https://arxiv.org/abs/2412.00098", "title": "Fine-Tuning Large Language Models for Scientific Text Classification: A Comparative Study", "authors": ["Zhyar Rzgar K Rostam", "Gábor Kertész"], "categories": ["cs.CL"], "comment": "6 pages, 3 figures, 7 tables", "summary": "The exponential growth of online textual content across diverse domains has\nnecessitated advanced methods for automated text classification. Large Language\nModels (LLMs) based on transformer architectures have shown significant success\nin this area, particularly in natural language processing (NLP) tasks. However,\ngeneral-purpose LLMs often struggle with domain-specific content, such as\nscientific texts, due to unique challenges like specialized vocabulary and\nimbalanced data. In this study, we fine-tune four state-of-the-art LLMs BERT,\nSciBERT, BioBERT, and BlueBERT on three datasets derived from the WoS-46985\ndataset to evaluate their performance in scientific text classification. Our\nexperiments reveal that domain-specific models, particularly SciBERT,\nconsistently outperform general-purpose models in both abstract-based and\nkeyword-based classification tasks. Additionally, we compare our achieved\nresults with those reported in the literature for deep learning models, further\nhighlighting the advantages of LLMs, especially when utilized in specific\ndomains. The findings emphasize the importance of domain-specific adaptations\nfor LLMs to enhance their effectiveness in specialized text classification\ntasks."}
{"id": "2412.01113", "pdf": "https://arxiv.org/pdf/2412.01113.pdf", "abs": "https://arxiv.org/abs/2412.01113", "title": "Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning", "authors": ["Keito Kudo", "Yoichi Aoki", "Tatsuki Kuribayashi", "Shusaku Sone", "Masaya Taniguchi", "Ana Brassard", "Keisuke Sakaguchi", "Kentaro Inui"], "categories": ["cs.CL"], "comment": null, "summary": "This study investigates the incremental, internal problem-solving process of\nlanguage models (LMs) with arithmetic multi-hop reasoning as a case study. We\nspecifically investigate when LMs internally resolve sub/whole problems through\nfirst reading the problem statements, generating reasoning chains, and\nachieving the final answer to mechanistically interpret LMs' multi-hop\nproblem-solving process. Our experiments reveal a systematic incremental\nreasoning strategy underlying LMs. They have not derived an answer at the\nmoment they first read the problem; instead, they obtain (sub)answers while\ngenerating the reasoning chain. Therefore, the generated reasoning chains can\nbe regarded as faithful reflections of the model's internal computation."}
{"id": "2412.07992", "pdf": "https://arxiv.org/pdf/2412.07992.pdf", "abs": "https://arxiv.org/abs/2412.07992", "title": "Concept Bottleneck Large Language Models", "authors": ["Chung-En Sun", "Tuomas Oikarinen", "Berk Ustun", "Tsui-Wei Weng"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ICLR 2025", "summary": "We introduce Concept Bottleneck Large Language Models (CB-LLMs), a novel\nframework for building inherently interpretable Large Language Models (LLMs).\nIn contrast to traditional black-box LLMs that rely on limited post-hoc\ninterpretations, CB-LLMs integrate intrinsic interpretability directly into the\nLLMs -- allowing accurate explanations with scalability and transparency. We\nbuild CB-LLMs for two essential NLP tasks: text classification and text\ngeneration. In text classification, CB-LLMs is competitive with, and at times\noutperforms, traditional black-box models while providing explicit and\ninterpretable reasoning. For the more challenging task of text generation,\ninterpretable neurons in CB-LLMs enable precise concept detection, controlled\ngeneration, and safer outputs. The embedded interpretability empowers users to\ntransparently identify harmful content, steer model behavior, and unlearn\nundesired concepts -- significantly enhancing the safety, reliability, and\ntrustworthiness of LLMs, which are critical capabilities notably absent in\nexisting models. Our code is available at\nhttps://github.com/Trustworthy-ML-Lab/CB-LLMs."}
{"id": "2412.12583", "pdf": "https://arxiv.org/pdf/2412.12583.pdf", "abs": "https://arxiv.org/abs/2412.12583", "title": "Process-Supervised Reward Models for Verifying Clinical Note Generation: A Scalable Approach Guided by Domain Expertise", "authors": ["Hanyin Wang", "Chufan Gao", "Qiping Xu", "Bolun Liu", "Guleid Hussein", "Hariprasad Korsapati", "Mohamad El Labban", "Kingsley Iheasirim", "Mohamed Hassan", "Gokhan Anil", "Brian Bartlett", "Jimeng Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Process-supervised reward models (PRMs) excel at providing step-by-step\nverification for large language model (LLM) outputs in domains like mathematics\nand coding. However, their application to fields lacking ground-truth answers,\nsuch as clinical note generation, poses significant challenges. We introduce a\nnovel framework for training PRMs to deliver step-level reward signals for\nLLM-generated clinical notes. By precisely defining meaningful \"steps,\"\ninjecting realistic \"errors\" informed by domain expertise, and leveraging LLMs\nto generate process supervision data at scale, we overcome previous\nlimitations. Our PRM, built on LLaMA-3.1 8B, consistently outperforms\nproprietary reasoning and non-reasoning models, achieving state-of-the-art\nperformance on two key evaluations: (1) distinguishing gold-standard from\nerror-containing samples with 98.8% accuracy, and (2) selecting\nphysician-preferred clinical notes with 56.2% accuracy. We investigate critical\ncomponents for effective PRM training, including optimal loss functions and\ndata selection strategies, and present a comprehensive physician reader study\nidentifying predictors of downstream Best-of-N performance. Our study sheds\nlight on unlocking the potential of PRMs for diverse generative tasks across\ndomains."}
{"id": "2412.12761", "pdf": "https://arxiv.org/pdf/2412.12761.pdf", "abs": "https://arxiv.org/abs/2412.12761", "title": "Revealing the impact of synthetic native samples and multi-tasking strategies in Hindi-English code-mixed humour and sarcasm detection", "authors": ["Debajyoti Mazumder", "Aakash Kumar", "Jasabanta Patro"], "categories": ["cs.CL", "cs.AI"], "comment": "33 pages; EMNLP 2025 (Findings)", "summary": "In this paper, we reported our experiments with various strategies to improve\ncode-mixed humour and sarcasm detection. Particularly, we tried three\napproaches: (i) native sample mixing, (ii) multi-task learning (MTL), and (iii)\nprompting and instruction finetuning very large multilingual language models\n(VMLMs). In native sample mixing, we added monolingual task samples to\ncode-mixed training sets. In MTL learning, we relied on native and code-mixed\nsamples of a semantically related task (hate detection in our case). Finally,\nin our third approach, we evaluated the efficacy of VMLMs via few-shot context\nprompting and instruction finetuning. Some interesting findings we got are (i)\nadding native samples improved humor (raising the F1-score up to 6.76%) and\nsarcasm (raising the F1-score up to 8.64%) detection, (ii) training MLMs in an\nMTL framework boosted performance for both humour (raising the F1-score up to\n10.67%) and sarcasm (increment up to 12.35% in F1-score) detection, and (iii)\nprompting and instruction finetuning VMLMs couldn't outperform the other\napproaches. Finally, our ablation studies and error analysis discovered the\ncases where our model is yet to improve. We provided our code for\nreproducibility."}
{"id": "2412.17727", "pdf": "https://arxiv.org/pdf/2412.17727.pdf", "abs": "https://arxiv.org/abs/2412.17727", "title": "Knowledge Editing through Chain-of-Thought", "authors": ["Changyue Wang", "Weihang Su", "Qingyao Ai", "Yichen Tang", "Yiqun Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Knowledge Editing is a technique that updates large language models (LLMs)\nwith new information to maintain their world knowledge. This approach avoids\nthe need to rebuild the model from scratch, thereby addressing the high costs\nassociated with frequent retraining. Among these, the in-context editing\nparadigm stands out for its effectiveness in integrating new knowledge while\npreserving the model's original capabilities. Despite its potential, existing\nin-context knowledge editing methods are often task-specific, focusing\nprimarily on multi-hop QA tasks using structured knowledge triples. Moreover,\ntheir reliance on few-shot prompting for task decomposition makes them unstable\nand less effective in generalizing across diverse tasks. In response to these\nlimitations, we propose EditCoT, a novel knowledge editing framework that\nflexibly and efficiently updates LLMs across various tasks without retraining.\nEditCoT works by generating a chain-of-thought (CoT) for a given input and then\niteratively refining this CoT process using a CoT editor based on updated\nknowledge. We evaluate EditCoT across a diverse range of benchmarks, covering\nmultiple languages and tasks. The results demonstrate that our approach\nachieves state-of-the-art performance while offering superior generalization,\neffectiveness, and stability compared to existing methods, marking a\nsignificant advancement in the field of knowledge updating. The code and data\nof EditCoT are available at: https://github.com/bebr2/EditCoT ."}
{"id": "2501.01872", "pdf": "https://arxiv.org/pdf/2501.01872.pdf", "abs": "https://arxiv.org/abs/2501.01872", "title": "Turning Logic Against Itself : Probing Model Defenses Through Contrastive Questions", "authors": ["Rachneet Sachdeva", "Rima Hazra", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2025 (Main)", "summary": "Large language models, despite extensive alignment with human values and\nethical principles, remain vulnerable to sophisticated jailbreak attacks that\nexploit their reasoning abilities. Existing safety measures often detect overt\nmalicious intent but fail to address subtle, reasoning-driven vulnerabilities.\nIn this work, we introduce POATE (Polar Opposite query generation, Adversarial\nTemplate construction, and Elaboration), a novel jailbreak technique that\nharnesses contrastive reasoning to provoke unethical responses. POATE crafts\nsemantically opposing intents and integrates them with adversarial templates,\nsteering models toward harmful outputs with remarkable subtlety. We conduct\nextensive evaluation across six diverse language model families of varying\nparameter sizes to demonstrate the robustness of the attack, achieving\nsignificantly higher attack success rates (~44%) compared to existing methods.\nTo counter this, we propose Intent-Aware CoT and Reverse Thinking CoT, which\ndecompose queries to detect malicious intent and reason in reverse to evaluate\nand reject harmful responses. These methods enhance reasoning robustness and\nstrengthen the model's defense against adversarial exploits."}
{"id": "2501.09751", "pdf": "https://arxiv.org/pdf/2501.09751.pdf", "abs": "https://arxiv.org/abs/2501.09751", "title": "OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking", "authors": ["Zekun Xi", "Wenbiao Yin", "Jizhan Fang", "Jialong Wu", "Runnan Fang", "Jiang Yong", "Pengjun Xie", "Fei Huang", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR", "cs.LG"], "comment": "EMNLP 2025", "summary": "Machine writing with large language models often relies on\nretrieval-augmented generation. However, these approaches remain confined\nwithin the boundaries of the model's predefined scope, limiting the generation\nof content with rich information. Specifically, vanilla-retrieved information\ntends to lack depth, novelty, and suffers from redundancy, which negatively\nimpacts the quality of generated articles, leading to shallow, unoriginal, and\nrepetitive outputs. To address these issues, we propose OmniThink, a\nslow-thinking machine writing framework that emulates the human-like process of\niterative expansion and reflection. The core idea behind OmniThink is to\nsimulate the cognitive behavior of learners as they slowly deepen their\nknowledge of the topics. Experimental results demonstrate that OmniThink\nimproves the knowledge density of generated articles without compromising\nmetrics such as coherence and depth. Human evaluations and expert feedback\nfurther highlight the potential of OmniThink to address real-world challenges\nin the generation of long-form articles. Code is available at\nhttps://github.com/zjunlp/OmniThink."}
{"id": "2501.15581", "pdf": "https://arxiv.org/pdf/2501.15581.pdf", "abs": "https://arxiv.org/abs/2501.15581", "title": "Error Classification of Large Language Models on Math Word Problems: A Dynamically Adaptive Framework", "authors": ["Yuhong Sun", "Zhangyue Yin", "Xuanjing Huang", "Xipeng Qiu", "Hui Zhao"], "categories": ["cs.CL"], "comment": "28 pages, 10 figures, accepted by Findings of EMNLP2025", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains. Math Word Problems (MWPs) serve as a crucial benchmark for\nevaluating LLMs' reasoning abilities. While most research primarily focuses on\nimproving accuracy, it often neglects understanding and addressing the\nunderlying patterns of errors. Current error classification methods rely on\nstatic and predefined categories, which limit their ability to capture the full\nspectrum of error patterns in mathematical reasoning. To enable systematic\nerror analysis, we collect error samples from 15 different LLMs of varying\nsizes across four distinct MWP datasets using multiple sampling strategies.\nBased on this extensive collection, we introduce MWPES-300K, a comprehensive\ndataset containing 304,865 error samples that cover diverse error patterns and\nreasoning paths. To reduce human bias and enable fine-grained analysis of error\npatterns, we propose a novel framework for automated dynamic error\nclassification in mathematical reasoning. Experimental results demonstrate that\ndataset characteristics significantly shape error patterns, which evolve from\nbasic to complex manifestations as model capabilities increase. With deeper\ninsights into error patterns, we propose Error-Aware Prompting (EAP) that\nincorporates common error patterns as explicit guidance, leading to significant\nimprovements in mathematical reasoning performance."}
{"id": "2501.16748", "pdf": "https://arxiv.org/pdf/2501.16748.pdf", "abs": "https://arxiv.org/abs/2501.16748", "title": "Through the Prism of Culture: Evaluating LLMs' Understanding of Indian Subcultures and Traditions", "authors": ["Garima Chhikara", "Abhishek Kumar", "Abhijnan Chakraborty"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable advancements but also\nraise concerns about cultural bias, often reflecting dominant narratives at the\nexpense of under-represented subcultures. In this study, we evaluate the\ncapacity of LLMs to recognize and accurately respond to the Little Traditions\nwithin Indian society, encompassing localized cultural practices and\nsubcultures such as caste, kinship, marriage, and religion. Through a series of\ncase studies, we assess whether LLMs can balance the interplay between dominant\nGreat Traditions and localized Little Traditions. We explore various prompting\nstrategies and further investigate whether using prompts in regional languages\nenhances the models cultural sensitivity and response quality. Our findings\nreveal that while LLMs demonstrate an ability to articulate cultural nuances,\nthey often struggle to apply this understanding in practical, context-specific\nscenarios. To the best of our knowledge, this is the first study to analyze\nLLMs engagement with Indian subcultures, offering critical insights into the\nchallenges of embedding cultural diversity in AI systems."}
{"id": "2502.02362", "pdf": "https://arxiv.org/pdf/2502.02362.pdf", "abs": "https://arxiv.org/abs/2502.02362", "title": "Premise-Augmented Reasoning Chains Improve Error Identification in Math reasoning with LLMs", "authors": ["Sagnik Mukherjee", "Abhinav Chinta", "Takyoung Kim", "Tarun Anoop Sharma", "Dilek Hakkani-Tür"], "categories": ["cs.CL"], "comment": "Accepted at ICML 2025", "summary": "Chain-of-Thought (CoT) prompting enhances mathematical reasoning in large\nlanguage models (LLMs) by enabling detailed step-by-step solutions. However,\ndue to the verbosity of LLMs, the resulting reasoning chains can be long,\nmaking it harder to verify the reasoning steps and trace issues resulting from\ndependencies between the steps that may be farther away in the sequence of\nsteps. Importantly, mathematical reasoning allows each step to be derived from\na small set of premises, which are a subset of the preceding steps in the\nreasoning chain. In this paper, we present a framework that identifies the\npremises for each step, to improve the evaluation of reasoning. We restructure\nconventional linear reasoning chains into Premise Augmented Reasoning Chains\n(PARC) by introducing premise links, resulting in a directed acyclic graph\nwhere the nodes are the steps and the edges are the premise links. Through\nexperiments with a PARC-based dataset that we built, namely PERL (Premises and\nERrors identification in LLMs), we demonstrate that LLMs can reliably identify\npremises within complex reasoning chains. In particular, even open-source LLMs\nachieve 90% recall in premise identification. We also show that PARC helps to\nidentify errors in reasoning chains more reliably. The accuracy of error\nidentification improves by 6% to 16% absolute when step-by-step verification is\ncarried out in PARC under the premises. Our findings highlight the utility of\npremise-centric representations in addressing complex problem-solving tasks and\nopen new avenues for improving the reliability of LLM-based reasoning\nevaluations."}
{"id": "2502.05467", "pdf": "https://arxiv.org/pdf/2502.05467.pdf", "abs": "https://arxiv.org/abs/2502.05467", "title": "Position: LLMs Can be Good Tutors in English Education", "authors": ["Jingheng Ye", "Shen Wang", "Deqing Zou", "Yibo Yan", "Kun Wang", "Hai-Tao Zheng", "Ruitong Liu", "Zenglin Xu", "Irwin King", "Philip S. Yu", "Qingsong Wen"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP 2025 Main. 20 pages, 4 figures", "summary": "While recent efforts have begun integrating large language models (LLMs) into\nEnglish education, they often rely on traditional approaches to learning tasks\nwithout fully embracing educational methodologies, thus lacking adaptability to\nlanguage learning. To address this gap, we argue that LLMs have the potential\nto serve as effective tutors in English Education. Specifically, LLMs can play\nthree critical roles: (1) as data enhancers, improving the creation of learning\nmaterials or serving as student simulations; (2) as task predictors, serving as\nlearner assessment or optimizing learning pathway; and (3) as agents, enabling\npersonalized and inclusive education. We encourage interdisciplinary research\nto explore these roles, fostering innovation while addressing challenges and\nrisks, ultimately advancing English Education through the thoughtful\nintegration of LLMs."}
{"id": "2502.05759", "pdf": "https://arxiv.org/pdf/2502.05759.pdf", "abs": "https://arxiv.org/abs/2502.05759", "title": "Reinforced Lifelong Editing for Language Models", "authors": ["Zherui Li", "Houcheng Jiang", "Hao Chen", "Baolong Bi", "Zhenhong Zhou", "Fei Sun", "Junfeng Fang", "Xiang Wang"], "categories": ["cs.CL"], "comment": "Accepted by ICML2025", "summary": "Large language models (LLMs) acquire information from pre-training corpora,\nbut their stored knowledge can become inaccurate or outdated over time. Model\nediting addresses this challenge by modifying model parameters without\nretraining, and prevalent approaches leverage hypernetworks to generate these\nparameter updates. However, they face significant challenges in lifelong\nediting due to their incompatibility with LLM parameters that dynamically\nchange during the editing process. To address this, we observed that\nhypernetwork-based lifelong editing aligns with reinforcement learning modeling\nand proposed RLEdit, an RL-based editing method. By treating editing losses as\nrewards and optimizing hypernetwork parameters at the full knowledge sequence\nlevel, we enable it to precisely capture LLM changes and generate appropriate\nparameter updates. Our extensive empirical evaluation across several LLMs\ndemonstrates that RLEdit outperforms existing methods in lifelong editing with\nsuperior effectiveness and efficiency, achieving a 59.24% improvement while\nrequiring only 2.11% of the time compared to most approaches. Our code is\navailable at: https://github.com/zhrli324/RLEdit."}
{"id": "2502.11689", "pdf": "https://arxiv.org/pdf/2502.11689.pdf", "abs": "https://arxiv.org/abs/2502.11689", "title": "Improve LLM-as-a-Judge Ability as a General Ability", "authors": ["Jiachen Yu", "Shaoning Sun", "Xiaohui Hu", "Jiaxu Yan", "Kaidong Yu", "Xuelong Li"], "categories": ["cs.CL"], "comment": null, "summary": "LLM-as-a-Judge leverages the generative and reasoning capabilities of large\nlanguage models (LLMs) to evaluate LLM responses across diverse scenarios,\nproviding accurate preference signals. This approach plays a vital role in\naligning LLMs with human values, ensuring ethical and reliable AI outputs that\nalign with societal norms. Recent studies have raised many methods to train LLM\nas generative judges, but most of them are data consuming or lack accuracy, and\nonly focus on LLM's judge ability. In this work, we regard judge ability as a\ngeneral ability of LLM and implement a two-stage training approach, comprising\nsupervised fine-tuning (SFT) warm-up and direct preference optimization (DPO)\nenhancement, to achieve judge style adaptation and improve judgment accuracy.\nAdditionally, we introduce an efficient data synthesis method to generate\njudgmental content. Experimental results demonstrate that our approach,\nutilizing only about 2% to 40% of the data required by other methods, achieves\nSOTA performance on RewardBench. Furthermore, our training method enhances the\ngeneral capabilities of the model by constructing complicated judge task, and\nthe judge signals provided by our model have significantly enhanced the\ndownstream DPO training performance of our internal models in our test to\noptimize policy model with Judge Model. We also open-source our model weights\nand training data to facilitate further research."}
{"id": "2502.15836", "pdf": "https://arxiv.org/pdf/2502.15836.pdf", "abs": "https://arxiv.org/abs/2502.15836", "title": "Soft Token Attacks Cannot Reliably Audit Unlearning in Large Language Models", "authors": ["Haokun Chen", "Sebastian Szyller", "Weilin Xu", "Nageen Himayat"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 Findings", "summary": "Large language models (LLMs) are trained using massive datasets, which often\ncontain undesirable content such as harmful texts, personal information, and\ncopyrighted material. To address this, machine unlearning aims to remove\ninformation from trained models. Recent work has shown that soft token attacks\n(STA) can successfully extract unlearned information from LLMs, but in this\nwork we show that STAs can be an inadequate tool for auditing unlearning. Using\ncommon benchmarks such as Who Is Harry Potter? and TOFU, we demonstrate that in\na strong auditor setting such attacks can elicit any information from the LLM,\nregardless of the deployed unlearning algorithm or whether the queried content\nwas originally present in the training corpus. We further show that STA with\njust a few soft tokens (1-10) can elicit random strings over 400 characters\nlong, indicating that STAs must be used carefully to effectively audit\nunlearning. Example code can be found at:\nhttps://github.com/IntelLabs/LLMart/tree/main/examples/unlearning"}
{"id": "2502.16699", "pdf": "https://arxiv.org/pdf/2502.16699.pdf", "abs": "https://arxiv.org/abs/2502.16699", "title": "Evaluating the Robustness and Accuracy of Text Watermarking Under Real-World Cross-Lingual Manipulations", "authors": ["Mansour Al Ghanim", "Jiaqi Xue", "Rochana Prih Hastuti", "Mengxin Zheng", "Yan Solihin", "Qian Lou"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted by EMNLP 2025 Finding", "summary": "We present a study to benchmark representative watermarking methods in\ncross-lingual settings. The current literature mainly focuses on the evaluation\nof watermarking methods for the English language. However, the literature for\nevaluating watermarking in cross-lingual settings is scarce. This results in\noverlooking important adversary scenarios in which a cross-lingual adversary\ncould be in, leading to a gray area of practicality over cross-lingual\nwatermarking. In this paper, we evaluate four watermarking methods in four\ndifferent and vocabulary rich languages. Our experiments investigate the\nquality of text under different watermarking procedure and the detectability of\nwatermarks with practical translation attack scenarios. Specifically, we\ninvestigate practical scenarios that an adversary with cross-lingual knowledge\ncould take, and evaluate whether current watermarking methods are suitable for\nsuch scenarios. Finally, from our findings, we draw key insights about\nwatermarking in cross-lingual settings."}
{"id": "2502.18978", "pdf": "https://arxiv.org/pdf/2502.18978.pdf", "abs": "https://arxiv.org/abs/2502.18978", "title": "Low-Confidence Gold: Refining Low-Confidence Samples for Efficient Instruction Tuning", "authors": ["Hongyi Cai", "Jie Li", "Mohammad Mahdinur Rahman", "Wenzhen Dong"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP Findings 2025", "summary": "The effectiveness of instruction fine-tuning for Large Language Models is\nfundamentally constrained by the quality and efficiency of training datasets.\nThis work introduces Low-Confidence Gold (LCG), a novel filtering framework\nthat employs centroid-based clustering and confidence-guided selection for\nidentifying valuable instruction pairs. Through a semi-supervised approach\nusing a lightweight classifier trained on representative samples, LCG curates\nhigh-quality subsets while preserving data diversity. Experimental evaluation\ndemonstrates that models fine-tuned on LCG-filtered subsets of 6K samples\nachieve superior performance compared to existing methods, with substantial\nimprovements on MT-bench and consistent gains across comprehensive evaluation\nmetrics. The framework's efficacy while maintaining model performance\nestablishes a promising direction for efficient instruction tuning."}
{"id": "2503.08890", "pdf": "https://arxiv.org/pdf/2503.08890.pdf", "abs": "https://arxiv.org/abs/2503.08890", "title": "PlainQAFact: Retrieval-augmented Factual Consistency Evaluation Metric for Biomedical Plain Language Summarization", "authors": ["Zhiwen You", "Yue Guo"], "categories": ["cs.CL"], "comment": null, "summary": "Hallucinated outputs from large language models (LLMs) pose risks in the\nmedical domain, especially for lay audiences making health-related decisions.\nExisting automatic factual consistency evaluation methods, such as entailment-\nand question-answering (QA) -based, struggle with plain language summarization\n(PLS) due to elaborative explanation phenomenon, which introduces external\ncontent (e.g., definitions, background, examples) absent from the scientific\nabstract to enhance comprehension. To address this, we introduce PlainQAFact,\nan automatic factual consistency evaluation metric trained on a fine-grained,\nhuman-annotated dataset PlainFact, for evaluating factual consistency of both\nsource-simplified and elaborately explained sentences. PlainQAFact first\nclassifies sentence type, then applies a retrieval-augmented QA scoring method.\nEmpirical results show that existing evaluation metrics fail to evaluate the\nfactual consistency in PLS, especially for elaborative explanations, whereas\nPlainQAFact consistently outperforms them across all evaluation settings. We\nfurther analyze PlainQAFact's effectiveness across external knowledge sources,\nanswer extraction strategies, answer overlap measures, and document granularity\nlevels, refining its overall factual consistency assessment. Taken together,\nour work presents the first evaluation metric designed for PLS factual\nconsistency evaluation, providing the community with both a robust benchmark\nand a practical tool to advance reliable and safe plain language communication\nin the medical domain. PlainQAFact and PlainFact are available at:\nhttps://github.com/zhiwenyou103/PlainQAFact"}
{"id": "2503.11132", "pdf": "https://arxiv.org/pdf/2503.11132.pdf", "abs": "https://arxiv.org/abs/2503.11132", "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and Extreme KV Compression", "authors": ["Guihong Li", "Mehdi Rezagholizadeh", "Mingyu Yang", "Vikram Appia", "Emad Barsoum"], "categories": ["cs.CL"], "comment": null, "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models."}
{"id": "2503.18596", "pdf": "https://arxiv.org/pdf/2503.18596.pdf", "abs": "https://arxiv.org/abs/2503.18596", "title": "LinkAlign: Scalable Schema Linking for Real-World Large-Scale Multi-Database Text-to-SQL", "authors": ["Yihan Wang", "Peiyu Liu", "Xin Yang"], "categories": ["cs.CL"], "comment": null, "summary": "Schema linking is a critical bottleneck in applying existing Text-to-SQL\nmodels to real-world, large-scale, multi-database environments. Through error\nanalysis, we identify two major challenges in schema linking: (1) Database\nRetrieval: accurately selecting the target database from a large schema pool,\nwhile effectively filtering out irrelevant ones; and (2) Schema Item Grounding:\nprecisely identifying the relevant tables and columns within complex and often\nredundant schemas for SQL generation. Based on these, we introduce LinkAlign, a\nnovel framework tailored for large-scale databases with thousands of fields.\nLinkAlign comprises three key steps: multi-round semantic enhanced retrieval\nand irrelevant information isolation for Challenge 1, and schema extraction\nenhancement for Challenge 2. Each stage supports both Agent and Pipeline\nexecution modes, enabling balancing efficiency and performance via modular\ndesign. To enable more realistic evaluation, we construct AmbiDB, a synthetic\ndataset designed to reflect the ambiguity of real-world schema linking.\nExperiments on widely-used Text-to-SQL benchmarks demonstrate that LinkAlign\nconsistently outperforms existing baselines on all schema linking metrics.\nNotably, it improves the overall Text-to-SQL pipeline and achieves a new\nstate-of-the-art score of 33.09% on the Spider 2.0-Lite benchmark using only\nopen-source LLMs, ranking first on the leaderboard at the time of submission.\nThe codes are available at https://github.com/Satissss/LinkAlign"}
{"id": "2503.22828", "pdf": "https://arxiv.org/pdf/2503.22828.pdf", "abs": "https://arxiv.org/abs/2503.22828", "title": "Learning to Reason for Long-Form Story Generation", "authors": ["Alexander Gurung", "Mirella Lapata"], "categories": ["cs.CL"], "comment": null, "summary": "Generating high-quality stories spanning thousands of tokens requires\ncompetency across a variety of skills, from tracking plot and character arcs to\nkeeping a consistent and engaging style. Due to the difficulty of sourcing\nlabeled datasets and precise quality measurements, most work using large\nlanguage models (LLMs) for long-form story generation uses combinations of\nhand-designed prompting techniques to elicit author-like behavior. This is a\nmanual process that is highly dependent on the specific story-generation task.\nMotivated by the recent success of applying RL with Verifiable Rewards to\ndomains like math and coding, we propose a general story-generation task\n(Next-Chapter Prediction) and a reward formulation (Verified Rewards via\nCompletion Likelihood Improvement) that allows us to use an unlabeled book\ndataset as a learning signal for reasoning. We learn to reason over a story's\ncondensed information and generate a detailed plan for the next chapter. Our\nreasoning is evaluated via the chapters it helps a story-generator create, and\ncompared against non-trained and supervised finetuning (SFT) baselines.\nPairwise human judgments reveal the chapters our learned reasoning produces are\npreferred across almost all metrics, and the effect is more pronounced in Scifi\nand Fantasy genres."}
{"id": "2504.03165", "pdf": "https://arxiv.org/pdf/2504.03165.pdf", "abs": "https://arxiv.org/abs/2504.03165", "title": "Efficient Dynamic Clustering-Based Document Compression for Retrieval-Augmented-Generation", "authors": ["Weitao Li", "Kaiming Liu", "Xiangyu Zhang", "Xuanyu Lei", "Weizhi Ma", "Yang Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach\nfor knowledge injection during large language model (LLM) inference in recent\nyears. However, due to their limited ability to exploit fine-grained\ninter-document relationships, current RAG implementations face challenges in\neffectively addressing the retrieved noise and redundancy content, which may\ncause error in the generation results. To address these limitations, we propose\nan Efficient Dynamic Clustering-based document Compression framework (EDC2-RAG)\nthat utilizes latent inter-document relationships while simultaneously removing\nirrelevant information and redundant content. We validate our approach, built\nupon GPT-3.5-Turbo and GPT-4o-mini, on widely used knowledge-QA and\nHallucination-Detection datasets. Experimental results show that our method\nachieves consistent performance improvements across various scenarios and\nexperimental settings, demonstrating strong robustness and applicability. Our\ncode and datasets are available at https://github.com/Tsinghua-dhy/EDC-2-RAG."}
{"id": "2504.03624", "pdf": "https://arxiv.org/pdf/2504.03624.pdf", "abs": "https://arxiv.org/abs/2504.03624", "title": "Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models", "authors": ["NVIDIA", ":", "Aaron Blakeman", "Aarti Basant", "Abhinav Khattar", "Adithya Renduchintala", "Akhiad Bercovich", "Aleksander Ficek", "Alexis Bjorlin", "Ali Taghibakhshi", "Amala Sanjay Deshmukh", "Ameya Sunil Mahabaleshwarkar", "Andrew Tao", "Anna Shors", "Ashwath Aithal", "Ashwin Poojary", "Ayush Dattagupta", "Balaram Buddharaju", "Bobby Chen", "Boris Ginsburg", "Boxin Wang", "Brandon Norick", "Brian Butterfield", "Bryan Catanzaro", "Carlo del Mundo", "Chengyu Dong", "Christine Harvey", "Christopher Parisien", "Dan Su", "Daniel Korzekwa", "Danny Yin", "Daria Gitman", "David Mosallanezhad", "Deepak Narayanan", "Denys Fridman", "Dima Rekesh", "Ding Ma", "Dmytro Pykhtar", "Dong Ahn", "Duncan Riach", "Dusan Stosic", "Eileen Long", "Elad Segal", "Ellie Evans", "Eric Chung", "Erick Galinkin", "Evelina Bakhturina", "Ewa Dobrowolska", "Fei Jia", "Fuxiao Liu", "Gargi Prasad", "Gerald Shen", "Guilin Liu", "Guo Chen", "Haifeng Qian", "Helen Ngo", "Hongbin Liu", "Hui Li", "Igor Gitman", "Ilia Karmanov", "Ivan Moshkov", "Izik Golan", "Jan Kautz", "Jane Polak Scowcroft", "Jared Casper", "Jarno Seppanen", "Jason Lu", "Jason Sewall", "Jiaqi Zeng", "Jiaxuan You", "Jimmy Zhang", "Jing Zhang", "Jining Huang", "Jinze Xue", "Jocelyn Huang", "Joey Conway", "John Kamalu", "Jon Barker", "Jonathan Cohen", "Joseph Jennings", "Jupinder Parmar", "Karan Sapra", "Kari Briski", "Kateryna Chumachenko", "Katherine Luna", "Keshav Santhanam", "Kezhi Kong", "Kirthi Sivamani", "Krzysztof Pawelec", "Kumar Anik", "Kunlun Li", "Lawrence McAfee", "Leon Derczynski", "Lindsey Pavao", "Luis Vega", "Lukas Voegtle", "Maciej Bala", "Maer Rodrigues de Melo", "Makesh Narsimhan Sreedhar", "Marcin Chochowski", "Markus Kliegl", "Marta Stepniewska-Dziubinska", "Matthieu Le", "Matvei Novikov", "Mehrzad Samadi", "Michael Andersch", "Michael Evans", "Miguel Martinez", "Mike Chrzanowski", "Mike Ranzinger", "Mikolaj Blaz", "Misha Smelyanskiy", "Mohamed Fawzy", "Mohammad Shoeybi", "Mostofa Patwary", "Nayeon Lee", "Nima Tajbakhsh", "Ning Xu", "Oleg Rybakov", "Oleksii Kuchaiev", "Olivier Delalleau", "Osvald Nitski", "Parth Chadha", "Pasha Shamis", "Paulius Micikevicius", "Pavlo Molchanov", "Peter Dykas", "Philipp Fischer", "Pierre-Yves Aquilanti", "Piotr Bialecki", "Prasoon Varshney", "Pritam Gundecha", "Przemek Tredak", "Rabeeh Karimi", "Rahul Kandu", "Ran El-Yaniv", "Raviraj Joshi", "Roger Waleffe", "Ruoxi Zhang", "Sabrina Kavanaugh", "Sahil Jain", "Samuel Kriman", "Sangkug Lym", "Sanjeev Satheesh", "Saurav Muralidharan", "Sean Narenthiran", "Selvaraj Anandaraj", "Seonmyeong Bak", "Sergey Kashirsky", "Seungju Han", "Shantanu Acharya", "Shaona Ghosh", "Sharath Turuvekere Sreenivas", "Sharon Clay", "Shelby Thomas", "Shrimai Prabhumoye", "Shubham Pachori", "Shubham Toshniwal", "Shyamala Prayaga", "Siddhartha Jain", "Sirshak Das", "Slawek Kierat", "Somshubra Majumdar", "Song Han", "Soumye Singhal", "Sriharsha Niverty", "Stefania Alborghetti", "Suseella Panguluri", "Swetha Bhendigeri", "Syeda Nahida Akter", "Szymon Migacz", "Tal Shiri", "Terry Kong", "Timo Roman", "Tomer Ronen", "Trisha Saar", "Tugrul Konuk", "Tuomas Rintamaki", "Tyler Poon", "Ushnish De", "Vahid Noroozi", "Varun Singh", "Vijay Korthikanti", "Vitaly Kurin", "Wasi Uddin Ahmad", "Wei Du", "Wei Ping", "Wenliang Dai", "Wonmin Byeon", "Xiaowei Ren", "Yao Xu", "Yejin Choi", "Yian Zhang", "Ying Lin", "Yoshi Suhara", "Zhiding Yu", "Zhiqi Li", "Zhiyu Li", "Zhongbo Zhu", "Zhuolin Yang", "Zijia Chen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "As inference-time scaling becomes critical for enhanced reasoning\ncapabilities, it is increasingly becoming important to build models that are\nefficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid\nMamba-Transformer models designed to reduce inference cost for a given accuracy\nlevel. To achieve this goal, we replace the majority of self-attention layers\nin the common Transformer model architecture with Mamba layers that perform\nconstant computation and require constant memory per generated token. We show\nthat Nemotron-H models offer either better or on-par accuracy compared to other\nsimilarly-sized state-of-the-art open-sourced Transformer models (e.g.,\nQwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3$\\times$ faster at\ninference. To further increase inference speed and reduce the memory required\nat inference time, we created Nemotron-H-47B-Base from the 56B model using a\nnew compression via pruning and distillation technique called MiniPuzzle.\nNemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20%\nfaster to infer. In addition, we introduce an FP8-based training recipe and\nshow that it can achieve on par results with BF16-based training. This recipe\nis used to train the 56B model. We are releasing Nemotron-H base model\ncheckpoints with support in Hugging Face and NeMo."}
{"id": "2504.19021", "pdf": "https://arxiv.org/pdf/2504.19021.pdf", "abs": "https://arxiv.org/abs/2504.19021", "title": "Advancing Scientific Text Classification: Fine-Tuned Models with Dataset Expansion and Hard-Voting", "authors": ["Zhyar Rzgar K Rostam", "Gábor Kertész"], "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 1 figure, 8 tables", "summary": "Efficient text classification is essential for handling the increasing volume\nof academic publications. This study explores the use of pre-trained language\nmodels (PLMs), including BERT, SciBERT, BioBERT, and BlueBERT, fine-tuned on\nthe Web of Science (WoS-46985) dataset for scientific text classification. To\nenhance performance, we augment the dataset by executing seven targeted queries\nin the WoS database, retrieving 1,000 articles per category aligned with\nWoS-46985's main classes. PLMs predict labels for this unlabeled data, and a\nhard-voting strategy combines predictions for improved accuracy and confidence.\nFine-tuning on the expanded dataset with dynamic learning rates and early\nstopping significantly boosts classification accuracy, especially in\nspecialized domains. Domain-specific models like SciBERT and BioBERT\nconsistently outperform general-purpose models such as BERT. These findings\nunderscore the efficacy of dataset augmentation, inference-driven label\nprediction, hard-voting, and fine-tuning techniques in creating robust and\nscalable solutions for automated academic text classification."}
{"id": "2505.07968", "pdf": "https://arxiv.org/pdf/2505.07968.pdf", "abs": "https://arxiv.org/abs/2505.07968", "title": "Assessing and Mitigating Medical Knowledge Drift and Conflicts in Large Language Models", "authors": ["Weiyi Wu", "Xinwen Xu", "Chongyang Gao", "Xingjian Diao", "Siting Li", "Lucas A. Salas", "Jiang Gui"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have great potential in the field of health\ncare, yet they face great challenges in adapting to rapidly evolving medical\nknowledge. This can lead to outdated or contradictory treatment suggestions.\nThis study investigated how LLMs respond to evolving clinical guidelines,\nfocusing on concept drift and internal inconsistencies. We developed the\nDriftMedQA benchmark to simulate guideline evolution and assessed the temporal\nreliability of various LLMs. Our evaluation of seven state-of-the-art models\nacross 4,290 scenarios demonstrated difficulties in rejecting outdated\nrecommendations and frequently endorsing conflicting guidance. Additionally, we\nexplored two mitigation strategies: Retrieval-Augmented Generation and\npreference fine-tuning via Direct Preference Optimization. While each method\nimproved model performance, their combination led to the most consistent and\nreliable results. These findings underscore the need to improve LLM robustness\nto temporal shifts to ensure more dependable applications in clinical practice.\nThe dataset is available at https://huggingface.co/datasets/RDBH/DriftMed."}
{"id": "2505.14406", "pdf": "https://arxiv.org/pdf/2505.14406.pdf", "abs": "https://arxiv.org/abs/2505.14406", "title": "Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via Knowledge Circuit Analysis", "authors": ["Haoming Huang", "Yibo Yan", "Jiahao Huo", "Xin Zou", "Xinfeng Li", "Kun Wang", "Xuming Hu"], "categories": ["cs.CL"], "comment": "Accepted by 2025 EMNLP Main", "summary": "Large Language Models (LLMs), despite their remarkable capabilities, are\nhampered by hallucinations. A particularly challenging variant, knowledge\novershadowing, occurs when one piece of activated knowledge inadvertently masks\nanother relevant piece, leading to erroneous outputs even with high-quality\ntraining data. Current understanding of overshadowing is largely confined to\ninference-time observations, lacking deep insights into its origins and\ninternal mechanisms during model training. Therefore, we introduce\nPhantomCircuit, a novel framework designed to comprehensively analyze and\ndetect knowledge overshadowing. By innovatively employing knowledge circuit\nanalysis, PhantomCircuit dissects the function of key components in the circuit\nand how the attention pattern dynamics contribute to the overshadowing\nphenomenon and its evolution throughout the training process. Extensive\nexperiments demonstrate PhantomCircuit's effectiveness in identifying such\ninstances, offering novel insights into this elusive hallucination and\nproviding the research community with a new methodological lens for its\npotential mitigation."}
{"id": "2505.14815", "pdf": "https://arxiv.org/pdf/2505.14815.pdf", "abs": "https://arxiv.org/abs/2505.14815", "title": "Language Mixing in Reasoning Language Models: Patterns, Impact, and Internal Causes", "authors": ["Mingyang Wang", "Lukas Lange", "Heike Adel", "Yunpu Ma", "Jannik Strötgen", "Hinrich Schütze"], "categories": ["cs.CL"], "comment": null, "summary": "Reasoning language models (RLMs) excel at complex tasks by leveraging a\nchain-of-thought process to generate structured intermediate steps. However,\nlanguage mixing, i.e., reasoning steps containing tokens from languages other\nthan the prompt, has been observed in their outputs and shown to affect\nperformance, though its impact remains debated. We present the first systematic\nstudy of language mixing in RLMs, examining its patterns, impact, and internal\ncauses across 15 languages, 7 task difficulty levels, and 18 subject areas, and\nshow how all three factors influence language mixing. Moreover, we demonstrate\nthat the choice of reasoning language significantly affects performance:\nforcing models to reason in Latin or Han scripts via constrained decoding\nnotably improves accuracy. Finally, we show that the script composition of\nreasoning traces closely aligns with that of the model's internal\nrepresentations, indicating that language mixing reflects latent processing\npreferences in RLMs. Our findings provide actionable insights for optimizing\nmultilingual reasoning and open new directions for controlling reasoning\nlanguages to build more interpretable and adaptable RLMs."}
{"id": "2505.15727", "pdf": "https://arxiv.org/pdf/2505.15727.pdf", "abs": "https://arxiv.org/abs/2505.15727", "title": "VocalBench: Benchmarking the Vocal Conversational Abilities for Speech Interaction Models", "authors": ["Heyang Liu", "Yuhao Wang", "Ziyang Cheng", "Ronghua Wu", "Qunshan Gu", "Yanfeng Wang", "Yu Wang"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has accelerated the\ndevelopment of multimodal models capable of speech communications. Unlike text\ninteractions, speech conveys diverse information, including acoustic\nvariations, paralanguage cues, and environmental context. However, existing\nevaluations of speech interaction models lack instances mimicking real\nscenarios and predominantly focus on the quality of their textual responses,\noverlooking critical aspects of vocal performance. To address this gap, we\npropose VocalBench, a comprehensive benchmark to assess the speech\nconversational abilities, comprising 9,400 carefully curated instances across\nfour key dimensions: semantic quality, acoustic performance, conversational\nabilities, and robustness. It covers a broad range of fundamental skills\nessential for effective vocal interactions. For the evaluation scheme, we\npropose several objective evaluation indicators and incorporate an additional\nLLM-as-a-judge approach to score open-ended questions. Experimental results on\n15 mainstream systems reveal significant variability, each exhibiting distinct\nstrengths and weaknesses, and provide valuable insights to guide future\nresearch in speech interaction systems."}
{"id": "2505.17656", "pdf": "https://arxiv.org/pdf/2505.17656.pdf", "abs": "https://arxiv.org/abs/2505.17656", "title": "Too Consistent to Detect: A Study of Self-Consistent Errors in LLMs", "authors": ["Hexiang Tan", "Fei Sun", "Sha Liu", "Du Su", "Qi Cao", "Xin Chen", "Jingang Wang", "Xunliang Cai", "Yuanzhuo Wang", "Huawei Shen", "Xueqi Cheng"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Main", "summary": "As large language models (LLMs) often generate plausible but incorrect\ncontent, error detection has become increasingly critical to ensure\ntruthfulness. However, existing detection methods often overlook a critical\nproblem we term as self-consistent error, where LLMs repeatedly generate the\nsame incorrect response across multiple stochastic samples. This work formally\ndefines self-consistent errors and evaluates mainstream detection methods on\nthem. Our investigation reveals two key findings: (1) Unlike inconsistent\nerrors, whose frequency diminishes significantly as the LLM scale increases,\nthe frequency of self-consistent errors remains stable or even increases. (2)\nAll four types of detection methods significantly struggle to detect\nself-consistent errors. These findings reveal critical limitations in current\ndetection methods and underscore the need for improvement. Motivated by the\nobservation that self-consistent errors often differ across LLMs, we propose a\nsimple but effective cross-model probe method that fuses hidden state evidence\nfrom an external verifier LLM. Our method significantly enhances performance on\nself-consistent errors across three LLM families."}
{"id": "2505.17746", "pdf": "https://arxiv.org/pdf/2505.17746.pdf", "abs": "https://arxiv.org/abs/2505.17746", "title": "Fast Quiet-STaR: Thinking Without Thought Tokens", "authors": ["Wei Huang", "Yizhe Xiong", "Xin Ye", "Zhijie Deng", "Hui Chen", "Zijia Lin", "Guiguang Ding"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "10 pages, 6 figures", "summary": "Large Language Models (LLMs) have achieved impressive performance across a\nrange of natural language processing tasks. However, recent advances\ndemonstrate that further gains particularly in complex reasoning tasks require\nmore than merely scaling up model sizes or training data. One promising\ndirection is to enable models to think during the reasoning process. Recently,\nQuiet STaR significantly improves reasoning by generating token-level thought\ntraces, but incurs substantial inference overhead. In this work, we propose\nFast Quiet STaR, a more efficient reasoning framework that preserves the\nbenefits of token-level reasoning while reducing computational cost. Our method\nintroduces a curriculum learning based training strategy that gradually reduces\nthe number of thought tokens, enabling the model to internalize more abstract\nand concise reasoning processes. We further extend this approach to the\nstandard Next Token Prediction (NTP) setting through reinforcement\nlearning-based fine-tuning, resulting in Fast Quiet-STaR NTP, which eliminates\nthe need for explicit thought token generation during inference. Experiments on\nfour benchmark datasets with Mistral 7B and Qwen2.5 7B demonstrate that Fast\nQuiet-STaR consistently outperforms Quiet-STaR in terms of average accuracy\nunder the same inference time budget. Notably, Fast Quiet-STaR NTP achieves an\naverage accuracy improvement of 9\\% on Mistral 7B and 5.7\\% on Qwen2.5 7B,\nwhile maintaining the same inference latency. Our code will be available at\nhttps://github.com/huangwei200012/Fast-Quiet-STaR."}
{"id": "2505.19429", "pdf": "https://arxiv.org/pdf/2505.19429.pdf", "abs": "https://arxiv.org/abs/2505.19429", "title": "Rhapsody: A Dataset for Highlight Detection in Podcasts", "authors": ["Younghan Park", "Anuj Diwan", "David Harwath", "Eunsol Choi"], "categories": ["cs.CL"], "comment": "COLM 2025", "summary": "Podcasts have become daily companions for half a billion users. Given the\nenormous amount of podcast content available, highlights provide a valuable\nsignal that helps viewers get the gist of an episode and decide if they want to\ninvest in listening to it in its entirety. However, identifying highlights\nautomatically is challenging due to the unstructured and long-form nature of\nthe content. We introduce Rhapsody, a dataset of 13K podcast episodes paired\nwith segment-level highlight scores derived from YouTube's 'most replayed'\nfeature. We frame the podcast highlight detection as a segment-level binary\nclassification task. We explore various baseline approaches, including\nzero-shot prompting of language models and lightweight fine-tuned language\nmodels using segment-level classification heads. Our experimental results\nindicate that even state-of-the-art language models like GPT-4o and Gemini\nstruggle with this task, while models fine-tuned with in-domain data\nsignificantly outperform their zero-shot performance. The fine-tuned model\nbenefits from leveraging both speech signal features and transcripts. These\nfindings highlight the challenges for fine-grained information access in\nlong-form spoken media."}
{"id": "2505.22823", "pdf": "https://arxiv.org/pdf/2505.22823.pdf", "abs": "https://arxiv.org/abs/2505.22823", "title": "Self-Critique and Refinement for Faithful Natural Language Explanations", "authors": ["Yingming Wang", "Pepa Atanasova"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Main", "summary": "With the rapid development of Large Language Models (LLMs), Natural Language\nExplanations (NLEs) have become increasingly important for understanding model\npredictions. However, these explanations often fail to faithfully represent the\nmodel's actual reasoning process. While existing work has demonstrated that\nLLMs can self-critique and refine their initial outputs for various tasks, this\ncapability remains unexplored for improving explanation faithfulness. To\naddress this gap, we introduce Self-critique and Refinement for Natural\nLanguage Explanations (SR-NLE), a framework that enables models to improve the\nfaithfulness of their own explanations -- specifically, post-hoc NLEs --\nthrough an iterative critique and refinement process without external\nsupervision. Our framework leverages different feedback mechanisms to guide the\nrefinement process, including natural language self-feedback and, notably, a\nnovel feedback approach based on feature attribution that highlights important\ninput words. Our experiments across three datasets and four state-of-the-art\nLLMs demonstrate that SR-NLE significantly reduces unfaithfulness rates, with\nour best method achieving an average unfaithfulness rate of 36.02%, compared to\n54.81% for baseline -- an absolute reduction of 18.79%. These findings reveal\nthat the investigated LLMs can indeed refine their explanations to better\nreflect their actual reasoning process, requiring only appropriate guidance\nthrough feedback without additional training or fine-tuning."}
{"id": "2506.02019", "pdf": "https://arxiv.org/pdf/2506.02019.pdf", "abs": "https://arxiv.org/abs/2506.02019", "title": "ChatCFD: An LLM-Driven Agent for End-to-End CFD Automation with Domain-Specific Structured Reasoning", "authors": ["E Fan", "Kang Hu", "Zhuowen Wu", "Jiangyang Ge", "Jiawei Miao", "Yuzhi Zhang", "He Sun", "Weizong Wang", "Tianhan Zhang"], "categories": ["cs.CL"], "comment": "19 pages, 8 figures", "summary": "Computational Fluid Dynamics (CFD) is essential for advancing scientific and\nengineering fields but is hindered by operational complexity, high expertise\nrequirements, and limited accessibility. This paper introduces ChatCFD, an\nautomated agent system for OpenFOAM simulations that processes multi-modal\ninputs (e.g., research papers, meshes) via an interactive interface, leveraging\nDeepSeek-R1 and DeepSeek-V3 large language models, a multi-agent architecture,\nand OpenFOAM knowledge. Its four-stage pipeline (Knowledge Base Construction,\nUser Input Processing, Case File Generation, and Execution and Error\nReflection) enables iterative trial-reflection-refinement for intricate setups,\nsupporting diverse physical models and external meshes. Validation on 205\nbenchmark tutorial cases, 110 perturbed variants, and 2 literature-derived\ncases shows ChatCFD's 82.1 percent operational success rate on basic cases,\noutperforming MetaOpenFOAM (6.2 percent) and Foam-Agent (42.3 percent), and\n60-80 percent on literature-derived complex cases. Turbulence model studies\nshow a 40 percent success rate for common models versus 10 percent for rare\nones like RNG k-epsilon. Physics coupling analyses reveal higher resource\ndemands for multi-physics-coupled cases, while LLM bias toward simpler setups\nintroduces persistent errors, such as dimensional inconsistency. Ablation\nstudies highlight the efficacy of RAG-based modules and reflection mechanisms.\nBy automating hypothesis testing and parameter exploration, ChatCFD accelerates\nscientific discovery in fluid mechanics and engineering, addressing LLM\nlimitations through structured design and showing strong potential as a modular\ncomponent in MCP-based agent networks for collaborative multi-agent systems,\npaving the way for scalable AI-driven CFD innovation. The code for ChatCFD is\navailable at https://github.com/ConMoo/ChatCFD."}
{"id": "2506.07642", "pdf": "https://arxiv.org/pdf/2506.07642.pdf", "abs": "https://arxiv.org/abs/2506.07642", "title": "TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient LLM-based Scientific Peer Review", "authors": ["Yuan Chang", "Ziyue Li", "Hengyuan Zhang", "Yuanbo Kong", "Yanru Wu", "Hayden Kwok-Hay So", "Zhijiang Guo", "Liya Zhu", "Ngai Wong"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP2025 Main", "summary": "While Large Language Models (LLMs) have shown significant potential in\nassisting peer review, current methods often struggle to generate thorough and\ninsightful reviews while maintaining efficiency. In this paper, we propose\nTreeReview, a novel framework that models paper review as a hierarchical and\nbidirectional question-answering process. TreeReview first constructs a tree of\nreview questions by recursively decomposing high-level questions into\nfine-grained sub-questions and then resolves the question tree by iteratively\naggregating answers from leaf to root to get the final review. Crucially, we\nincorporate a dynamic question expansion mechanism to enable deeper probing by\ngenerating follow-up questions when needed. We construct a benchmark derived\nfrom ICLR and NeurIPS venues to evaluate our method on full review generation\nand actionable feedback comments generation tasks. Experimental results of both\nLLM-based and human evaluation show that TreeReview outperforms strong\nbaselines in providing comprehensive, in-depth, and expert-aligned review\nfeedback, while reducing LLM token usage by up to 80% compared to\ncomputationally intensive approaches. Our code and benchmark dataset are\navailable at https://github.com/YuanChang98/tree-review."}
{"id": "2506.11798", "pdf": "https://arxiv.org/pdf/2506.11798.pdf", "abs": "https://arxiv.org/abs/2506.11798", "title": "Persona-driven Simulation of Voting Behavior in the European Parliament with Large Language Models", "authors": ["Maximilian Kreutner", "Marlene Lutz", "Markus Strohmaier"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) display remarkable capabilities to understand or\neven produce political discourse, but have been found to consistently display a\nprogressive left-leaning bias. At the same time, so-called persona or identity\nprompts have been shown to produce LLM behavior that aligns with socioeconomic\ngroups that the base model is not aligned with. In this work, we analyze\nwhether zero-shot persona prompting with limited information can accurately\npredict individual voting decisions and, by aggregation, accurately predict\npositions of European groups on a diverse set of policies. We evaluate if\npredictions are stable towards counterfactual arguments, different persona\nprompts and generation methods. Finally, we find that we can simulate voting\nbehavior of Members of the European Parliament reasonably well with a weighted\nF1 score of approximately 0.793. Our persona dataset of politicians in the 2024\nEuropean Parliament and our code are available at\nhttps://github.com/dess-mannheim/european_parliament_simulation."}
{"id": "2506.13610", "pdf": "https://arxiv.org/pdf/2506.13610.pdf", "abs": "https://arxiv.org/abs/2506.13610", "title": "A Structured Dataset of Disease-Symptom Associations to Improve Diagnostic Accuracy", "authors": ["Abdullah Al Shafi", "Rowzatul Zannat", "Abdul Muntakim", "Mahmudul Hasan"], "categories": ["cs.CL"], "comment": "Computational Biology", "summary": "Disease-symptom datasets are significant and in demand for medical research,\ndisease diagnosis, clinical decision-making, and AI-driven health management\napplications. These datasets help identify symptom patterns associated with\nspecific diseases, thus improving diagnostic accuracy and enabling early\ndetection. The dataset presented in this study systematically compiles\ndisease-symptom relationships from various online sources, medical literature,\nand publicly available health databases. The data was gathered through\nanalyzing peer-reviewed medical articles, clinical case studies, and\ndisease-symptom association reports. Only the verified medical sources were\nincluded in the dataset, while those from non-peer-reviewed and anecdotal\nsources were excluded. The dataset is structured in a tabular format, where the\nfirst column represents diseases, and the remaining columns represent symptoms.\nEach symptom cell contains a binary value, indicating whether a symptom is\nassociated with a disease. Thereby, this structured representation makes the\ndataset very useful for a wide range of applications, including machine\nlearning-based disease prediction, clinical decision support systems, and\nepidemiological studies. Although there are some advancements in the field of\ndisease-symptom datasets, there is a significant gap in structured datasets for\nthe Bangla language. This dataset aims to bridge that gap by facilitating the\ndevelopment of multilingual medical informatics tools and improving disease\nprediction models for underrepresented linguistic communities. Further\ndevelopments should include region-specific diseases and further fine-tuning of\nsymptom associations for better diagnostic performance"}
{"id": "2507.02949", "pdf": "https://arxiv.org/pdf/2507.02949.pdf", "abs": "https://arxiv.org/abs/2507.02949", "title": "RADIANT: Retrieval AugmenteD entIty-context AligNmenT -- Introducing RAG-ability and Entity-Context Divergence", "authors": ["Vipula Rawte", "Rajarshi Roy", "Gurpreet Singh", "Danush Khanna", "Yaswanth Narsupalli", "Basab Ghosh", "Abhay Gupta", "Argha Kamal Samanta", "Aditya Shingote", "Aadi Krishna Vikram", "Vinija Jain", "Aman Chadha", "Amit Sheth", "Amitava Das"], "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) continue to advance, Retrieval-Augmented\nGeneration (RAG) has emerged as a vital technique to enhance factual accuracy\nby integrating external knowledge into the generation process. However, LLMs\noften fail to faithfully integrate retrieved evidence into their generated\nresponses, leading to factual inconsistencies. To quantify this gap, we\nintroduce Entity-Context Divergence (ECD), a metric that measures the extent to\nwhich retrieved information is accurately reflected in model outputs. We\nsystematically evaluate contemporary LLMs on their ability to preserve factual\nconsistency in retrieval-augmented settings, a capability we define as\nRAG-ability. Our empirical analysis reveals that RAG-ability remains low across\nmost LLMs, highlighting significant challenges in entity retention and context\nfidelity. This paper introduces Radiant (Retrieval AugmenteD entIty-context\nAligNmenT), a novel framework that merges RAG with alignment designed to\noptimize the interplay between retrieved evidence and generated content.\nRadiant extends Direct Preference Optimization (DPO) to teach LLMs how to\nintegrate provided additional information into subsequent generations. As a\nbehavior correction mechanism, Radiant boosts RAG performance across varied\nretrieval scenarios, such as noisy web contexts, knowledge conflicts, and\nhallucination reduction. This enables more reliable, contextually grounded, and\nfactually coherent content generation."}
{"id": "2507.03922", "pdf": "https://arxiv.org/pdf/2507.03922.pdf", "abs": "https://arxiv.org/abs/2507.03922", "title": "Dynamic Injection of Entity Knowledge into Dense Retrievers", "authors": ["Ikuya Yamada", "Ryokan Ri", "Takeshi Kojima", "Yusuke Iwasawa", "Yutaka Matsuo"], "categories": ["cs.CL"], "comment": "EMNLP Findings", "summary": "Dense retrievers often struggle with queries involving less-frequent entities\ndue to their limited entity knowledge. We propose the Knowledgeable Passage\nRetriever (KPR), a BERT-based retriever enhanced with a context-entity\nattention layer and dynamically updatable entity embeddings. This design\nenables KPR to incorporate external entity knowledge without retraining.\nExperiments on three datasets demonstrate that KPR consistently improves\nretrieval accuracy, with particularly large gains on the EntityQuestions\ndataset. When built on the off-the-shelf bge-base retriever, KPR achieves\nstate-of-the-art performance among similarly sized models on two datasets.\nModels and code are released at\nhttps://github.com/knowledgeable-embedding/knowledgeable-embedding."}
{"id": "2507.15512", "pdf": "https://arxiv.org/pdf/2507.15512.pdf", "abs": "https://arxiv.org/abs/2507.15512", "title": "Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language Models", "authors": ["Kaiyan Chang", "Yonghao Shi", "Chenglong Wang", "Hang Zhou", "Chi Hu", "Xiaoqian Liu", "Yingfeng Luo", "Yuan Ge", "Tong Xiao", "Jingbo Zhu"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025. Code: https://github.com/Lucky-259/Hybrid_TTS", "summary": "Test-Time Scaling (TTS) is a promising approach to progressively elicit the\nmodel's intelligence during inference. Recently, training-based TTS methods,\nsuch as continued reinforcement learning (RL), have further surged in\npopularity, while training-free TTS methods are gradually fading from\nprominence. However, the additional computation overhead of training amplifies\nthe burden on test-time scaling. In this paper, we focus on training-free TTS\nmethods for reasoning. We first design Conditional Step-level Self-refinement,\na fine-grained sequential scaling method guided by process verification. On top\nof its effectiveness, we further combine it with other classical parallel\nscaling methods at the step level, to introduce a novel inference paradigm\ncalled Hybrid Test-Time Scaling. Extensive experiments on five\ninstruction-tuned LLMs across different scales (3B-14B) and families\ndemonstrate that hybrid strategy incorporating various training-free TTS\nmethods at a fine granularity has considerable potential for expanding the\nreasoning performance boundaries of LLMs."}
{"id": "2507.18504", "pdf": "https://arxiv.org/pdf/2507.18504.pdf", "abs": "https://arxiv.org/abs/2507.18504", "title": "Not All Features Deserve Attention: Graph-Guided Dependency Learning for Tabular Data Generation with Language Models", "authors": ["Zheyu Zhang", "Shuo Yang", "Bardh Prenkaj", "Gjergji Kasneci"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to EMNLP 2025 (Findings)", "summary": "Large Language Models (LLMs) have shown strong potential for tabular data\ngeneration by modeling textualized feature-value pairs. However, tabular data\ninherently exhibits sparse feature-level dependencies, where many feature\ninteractions are structurally insignificant. This creates a fundamental\nmismatch as LLMs' self-attention mechanism inevitably distributes focus across\nall pairs, diluting attention on critical relationships, particularly in\ndatasets with complex dependencies or semantically ambiguous features. To\naddress this limitation, we propose GraDe (Graph-Guided Dependency Learning), a\nnovel method that explicitly integrates sparse dependency graphs into LLMs'\nattention mechanism. GraDe employs a lightweight dynamic graph learning module\nguided by externally extracted functional dependencies, prioritizing key\nfeature interactions while suppressing irrelevant ones. Our experiments across\ndiverse real-world datasets demonstrate that GraDe outperforms existing\nLLM-based approaches by up to 12% on complex datasets while achieving\ncompetitive results with state-of-the-art approaches in synthetic data quality.\nOur method is minimally intrusive yet effective, offering a practical solution\nfor structure-aware tabular data modeling with LLMs."}
{"id": "2507.18791", "pdf": "https://arxiv.org/pdf/2507.18791.pdf", "abs": "https://arxiv.org/abs/2507.18791", "title": "CodeMixBench: Evaluating Code-Mixing Capabilities of LLMs Across 18 Languages", "authors": ["Yilun Yang", "Yekun Chai"], "categories": ["cs.CL"], "comment": "EMNLP 2025", "summary": "Code-mixing, the practice of switching between languages within a\nconversation, poses unique challenges for traditional NLP. Existing benchmarks\nare limited by their narrow language pairs and tasks, failing to adequately\nassess large language models' (LLMs) code-mixing abilities. Despite the\nrecognized importance of code-mixing for multilingual users, research on LLMs\nin this context remains sparse. Additionally, current techniques for\nsynthesizing code-mixed data are underdeveloped to generate code-mixing. In\nresponse, we introduce CodeMixBench, a comprehensive benchmark covering eight\ntasks, including three specific to LLMs and five traditional NLP tasks, and 18\nlanguages across seven language families. We also propose a new method for\ngenerating large-scale synthetic code-mixed texts by combining word\nsubstitution with GPT-4 prompting. Our evaluation reveals consistent\nunderperformance of LLMs on code-mixed datasets involving different language\nfamilies. Enhancements in training data size, model scale, and few-shot\nlearning could improve their performance. The code and dataset are available at\nhttps://github.com/Jeromeyluck/CodeMixBench."}
{"id": "2508.00719", "pdf": "https://arxiv.org/pdf/2508.00719.pdf", "abs": "https://arxiv.org/abs/2508.00719", "title": "Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA", "authors": ["Yingxu Wang", "Shiqi Fan", "Mengzhu Wang", "Siyang Gao", "Siwei Liu", "Nan Yin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Knowledge Graph Question Answering (KGQA) aims to interpret natural language\nqueries and perform structured reasoning over knowledge graphs by leveraging\ntheir relational and semantic structures to retrieve accurate answers. Recent\nKGQA methods primarily follow either retrieve-then-reason paradigm, relying on\nGNNs or heuristic rules for static paths extraction, or dynamic path generation\nstrategies that use large language models (LLMs) with prompting to jointly\nperform retrieval and reasoning. However, the former suffers from limited\nadaptability due to static path extraction and lack of contextual refinement,\nwhile the latter incurs high computational costs and struggles with accurate\npath evaluation due to reliance on fixed scoring functions and extensive LLM\ncalls. To address these issues, this paper proposes Dynamically Adaptive\nMCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search\nwith adaptive path evaluation for efficient and context-aware KGQA. DAMR\nemploys a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based\nplanner, which selects top-$k$ relevant relations at each step to reduce search\nspace. To improve path evaluation accuracy, we introduce a lightweight\nTransformer-based scorer that performs context-aware plausibility estimation by\njointly encoding the question and relation sequence through cross-attention,\nenabling the model to capture fine-grained semantic shifts during multi-hop\nreasoning. Furthermore, to alleviate the scarcity of high-quality supervision,\nDAMR incorporates a dynamic pseudo-path refinement mechanism that periodically\ngenerates training signals from partial paths explored during search, allowing\nthe scorer to continuously adapt to the evolving distribution of reasoning\ntrajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR\nsignificantly outperforms state-of-the-art methods."}
{"id": "2508.08684", "pdf": "https://arxiv.org/pdf/2508.08684.pdf", "abs": "https://arxiv.org/abs/2508.08684", "title": "Out of the Box, into the Clinic? Evaluating State-of-the-Art ASR for Clinical Applications for Older Adults", "authors": ["Bram van Dijk", "Tiberon Kuiper", "Sirin Aoulad si Ahmed", "Armel Levebvre", "Jake Johnson", "Jan Duin", "Simon Mooijaart", "Marco Spruit"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Voice-controlled interfaces can support older adults in clinical contexts,\nwith chatbots being a prime example, but reliable Automatic Speech Recognition\n(ASR) for underrepresented groups remains a bottleneck. This study evaluates\nstate-of-the-art ASR models on language use of older Dutch adults, who\ninteracted with the \\texttt{Welzijn.AI} chatbot designed for geriatric\ncontexts. We benchmark generic multilingual ASR models, and models fine-tuned\nfor Dutch spoken by older adults, while also considering processing speed. Our\nresults show that generic multilingual models outperform fine-tuned models,\nwhich suggests recent ASR models can generalise well out of the box to\nrealistic datasets. Furthermore, our results suggest that truncating existing\narchitectures is helpful in balancing the accuracy-speed trade-off, though we\nalso identify some cases with high WER due to hallucinations."}
{"id": "2508.09016", "pdf": "https://arxiv.org/pdf/2508.09016.pdf", "abs": "https://arxiv.org/abs/2508.09016", "title": "A Survey on Training-free Alignment of Large Language Models", "authors": ["Birong Pan", "Yongqi Li", "Weiyu Zhang", "Wenpeng Lu", "Mayi Xu", "Shen Zhou", "Yuanyuan Zhu", "Ming Zhong", "Tieyun Qian"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to EMNLP 2025 (findings), camera-ready version", "summary": "The alignment of large language models (LLMs) aims to ensure their outputs\nadhere to human values, ethical standards, and legal norms. Traditional\nalignment methods often rely on resource-intensive fine-tuning (FT), which may\nsuffer from knowledge degradation and face challenges in scenarios where the\nmodel accessibility or computational resources are constrained. In contrast,\ntraining-free (TF) alignment techniques--leveraging in-context learning,\ndecoding-time adjustments, and post-generation corrections--offer a promising\nalternative by enabling alignment without heavily retraining LLMs, making them\nadaptable to both open-source and closed-source environments. This paper\npresents the first systematic review of TF alignment methods, categorizing them\nby stages of pre-decoding, in-decoding, and post-decoding. For each stage, we\nprovide a detailed examination from the viewpoint of LLMs and multimodal LLMs\n(MLLMs), highlighting their mechanisms and limitations. Furthermore, we\nidentify key challenges and future directions, paving the way for more\ninclusive and effective TF alignment techniques. By synthesizing and organizing\nthe rapidly growing body of research, this survey offers a guidance for\npractitioners and advances the development of safer and more reliable LLMs."}
{"id": "2508.15250", "pdf": "https://arxiv.org/pdf/2508.15250.pdf", "abs": "https://arxiv.org/abs/2508.15250", "title": "EMNLP: Educator-role Moral and Normative Large Language Models Profiling", "authors": ["Yilin Jiang", "Mingzi Zhang", "Sheng Jin", "Zengyi Yu", "Xiangjie Kong", "Binghao Tu"], "categories": ["cs.CL", "I.2.7"], "comment": "29pages, 15 figures, Accepted by EMNLP Main Confrence", "summary": "Simulating Professions (SP) enables Large Language Models (LLMs) to emulate\nprofessional roles. However, comprehensive psychological and ethical evaluation\nin these contexts remains lacking. This paper introduces EMNLP, an\nEducator-role Moral and Normative LLMs Profiling framework for personality\nprofiling, moral development stage measurement, and ethical risk under soft\nprompt injection. EMNLP extends existing scales and constructs 88\nteacher-specific moral dilemmas, enabling profession-oriented comparison with\nhuman teachers. A targeted soft prompt injection set evaluates compliance and\nvulnerability in teacher SP. Experiments on 14 LLMs show teacher-role LLMs\nexhibit more idealized and polarized personalities than human teachers, excel\nin abstract moral reasoning, but struggle with emotionally complex situations.\nModels with stronger reasoning are more vulnerable to harmful prompt injection,\nrevealing a paradox between capability and safety. The model temperature and\nother hyperparameters have limited influence except in some risk behaviors.\nThis paper presents the first benchmark to assess ethical and psychological\nalignment of teacher-role LLMs for educational AI. Resources are available at\nhttps://e-m-n-l-p.github.io/."}
{"id": "2508.15868", "pdf": "https://arxiv.org/pdf/2508.15868.pdf", "abs": "https://arxiv.org/abs/2508.15868", "title": "CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning", "authors": ["Wenqiao Zhu", "Ji Liu", "Rongjuncheng Zhang", "Haipang Wu", "Yulun Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, to appear in EMNLP25", "summary": "Reasoning capability plays a significantly critical role in the the broad\napplications of Large Language Models (LLMs). To enhance the reasoning\nperformance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuning\napproaches have been proposed to address the limited generalization capability\nof LLMs trained solely via Supervised Fine-Tuning (SFT). Despite their\neffectiveness, two major limitations hinder the advancement of LLMs. First,\nvanilla RL-based approaches ignore annotated Chain-of-Thought (CoT) and\nincorporate unstable reasoning path sampling, which typically results in model\ncollapse, unstable training process, and suboptimal performance. Second,\nexisting SFT approaches generally overemphasize the annotated CoT, potentially\nleading to performance degradation due to insufficient exploitation of\npotential CoT. In this paper, we propose a Contrastive learning with annotated\nCoT-based Reinforced Fine-Tuning approach, i.e., \\TheName{}, to enhance the\nreasoning performance of LLMs while addressing the aforementioned limitations.\nSpecifically, we propose learning a representation for each CoT. Based on this\nrepresentation, we design novel contrastive signals to guide the fine-tuning\nprocess. Our approach not only fully exploits the available annotated CoT but\nalso stabilizes the fine-tuning procedure by incorporating an additional\nunsupervised learning signal. We conduct comprehensive experiments and in-depth\nanalysis with three baseline approaches, two foundation models, and two\ndatasets to demonstrate significant advantages of \\TheName{} in terms of\nrobustness, performance (up to 10.15\\%), and efficiency (up to 30.62\\%). Code\nis available at https://github.com/WNQzhu/CARFT."}
{"id": "2508.15884", "pdf": "https://arxiv.org/pdf/2508.15884.pdf", "abs": "https://arxiv.org/abs/2508.15884", "title": "Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search", "authors": ["Yuxian Gu", "Qinghao Hu", "Shang Yang", "Haocheng Xi", "Junyu Chen", "Song Han", "Han Cai"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Tech Report", "summary": "We present Jet-Nemotron, a new family of hybrid-architecture language models,\nwhich matches or exceeds the accuracy of leading full-attention models while\nsignificantly improving generation throughput. Jet-Nemotron is developed using\nPost Neural Architecture Search (PostNAS), a novel neural architecture\nexploration pipeline that enables efficient model design. Unlike prior\napproaches, PostNAS begins with a pre-trained full-attention model and freezes\nits MLP weights, allowing efficient exploration of attention block designs. The\npipeline includes four key components: (1) learning optimal full-attention\nlayer placement and elimination, (2) linear attention block selection, (3)\ndesigning new attention blocks, and (4) performing hardware-aware\nhyperparameter search. Our Jet-Nemotron-2B model achieves comparable or\nsuperior accuracy to Qwen3, Qwen2.5, Gemma3, and Llama3.2 across a\ncomprehensive suite of benchmarks while delivering up to 53.6x generation\nthroughput speedup and 6.1x prefilling speedup. It also achieves higher\naccuracy on MMLU and MMLU-Pro than recent advanced MoE full-attention models,\nsuch as DeepSeek-V3-Small and Moonlight, despite their larger scale with 15B\ntotal and 2.2B activated parameters."}
{"id": "2508.18183", "pdf": "https://arxiv.org/pdf/2508.18183.pdf", "abs": "https://arxiv.org/abs/2508.18183", "title": "Leveraging Large Language Models for Accurate Sign Language Translation in Low-Resource Scenarios", "authors": ["Luana Bulla", "Gabriele Tuccio", "Misael Mongiovì", "Aldo Gangemi"], "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2; I.2.7"], "comment": null, "summary": "Translating natural languages into sign languages is a highly complex and\nunderexplored task. Despite growing interest in accessibility and inclusivity,\nthe development of robust translation systems remains hindered by the limited\navailability of parallel corpora which align natural language with sign\nlanguage data. Existing methods often struggle to generalize in these\ndata-scarce environments, as the few datasets available are typically\ndomain-specific, lack standardization, or fail to capture the full linguistic\nrichness of sign languages. To address this limitation, we propose Advanced Use\nof LLMs for Sign Language Translation (AulSign), a novel method that leverages\nLarge Language Models via dynamic prompting and in-context learning with sample\nselection and subsequent sign association. Despite their impressive abilities\nin processing text, LLMs lack intrinsic knowledge of sign languages; therefore,\nthey are unable to natively perform this kind of translation. To overcome this\nlimitation, we associate the signs with compact descriptions in natural\nlanguage and instruct the model to use them. We evaluate our method on both\nEnglish and Italian languages using SignBank+, a recognized benchmark in the\nfield, as well as the Italian LaCAM CNR-ISTC dataset. We demonstrate superior\nperformance compared to state-of-the-art models in low-data scenario. Our\nfindings demonstrate the effectiveness of AulSign, with the potential to\nenhance accessibility and inclusivity in communication technologies for\nunderrepresented linguistic communities."}
{"id": "2508.18655", "pdf": "https://arxiv.org/pdf/2508.18655.pdf", "abs": "https://arxiv.org/abs/2508.18655", "title": "Empathy Omni: Enabling Empathetic Speech Response Generation through Large Language Models", "authors": ["Haoyu Wang", "Guangyan Zhang", "Jiale Chen", "Jingyu Li", "Yuehai Wang", "Yiwen Guo"], "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7"], "comment": "5 pages, 1 figure, submitted to ICASSP 2026", "summary": "With the development of speech large language models (speech LLMs), users can\nnow interact directly with assistants via speech. However, most existing models\nonly convert response content into speech without fully capturing the rich\nemotional cues in user queries, where the same sentence may convey different\nmeanings depending on the expression. Emotional understanding is thus essential\nfor improving human-machine interaction. Most empathetic speech LLMs rely on\nmassive datasets, demanding high computational cost. A key challenge is to\nbuild models that generate empathetic responses with limited data and without\nlarge-scale training. To this end, we propose Emotion Omni, a model that\nunderstands emotional content in user speech and generates empathetic\nresponses. We further developed a data pipeline to construct a 200k emotional\ndialogue dataset supporting empathetic speech assistants. Experiments show that\nEmotion Omni achieves comparable instruction-following ability without\nlarge-scale pretraining, while surpassing existing models in speech quality\n(UTMOS:4.41) and empathy (Emotion GPT Score: 3.97). These results confirm its\nimprovements in both speech fidelity and emotional expressiveness. Demos are\navailable at https://w311411.github.io/omni_demo/."}
{"id": "2508.18992", "pdf": "https://arxiv.org/pdf/2508.18992.pdf", "abs": "https://arxiv.org/abs/2508.18992", "title": "Automatic Prompt Optimization with Prompt Distillation", "authors": ["Ernest A. Dyagin", "Nikita I. Kulin", "Artur R. Khairullin", "Viktor N. Zhuravlev", "Alena N. Sitkina"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which is gaining popularity due to the rapid development of\nprompt engineering driven by extensive research in the field of large language\nmodels (LLMs). This paper presents DistillPrompt -- a novel autoprompting\nmethod based on large language models that employs a multi-stage integration of\ntask-specific information into prompts using training data. DistillPrompt\nutilizes distillation, compression, and aggregation operations to explore the\nprompt space more thoroughly. The method was tested on different datasets for\ntext classification and generation tasks using the t-lite-instruct-0.1 language\nmodel. The results demonstrate a significant average improvement (e.g., 20.12%\nacross the entire dataset compared to Grips) in key metrics over existing\nmethods in the field, establishing DistillPrompt as one of the most effective\nnon-gradient approaches in autoprompting."}
{"id": "2508.19026", "pdf": "https://arxiv.org/pdf/2508.19026.pdf", "abs": "https://arxiv.org/abs/2508.19026", "title": "MovieCORE: COgnitive REasoning in Movies", "authors": ["Gueter Josmy Faure", "Min-Hung Chen", "Jia-Fong Yeh", "Ying Cheng", "Hung-Ting Su", "Yung-Hao Tang", "Shang-Hong Lai", "Winston H. Hsu"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Accepted for EMNLP'2025 Main Conference. Project Page:\n  https://joslefaure.github.io/assets/html/moviecore.html", "summary": "This paper introduces MovieCORE, a novel video question answering (VQA)\ndataset designed to probe deeper cognitive understanding of movie content.\nUnlike existing datasets that focus on surface-level comprehension, MovieCORE\nemphasizes questions that engage System-2 thinking while remaining specific to\nthe video material. We present an innovative agentic brainstorming approach,\nutilizing multiple large language models (LLMs) as thought agents to generate\nand refine high-quality question-answer pairs. To evaluate dataset quality, we\ndevelop a set of cognitive tests assessing depth, thought-provocation\npotential, and syntactic complexity. We also propose a comprehensive evaluation\nscheme for assessing VQA model performance on deeper cognitive tasks. To\naddress the limitations of existing video-language models (VLMs), we introduce\nan agentic enhancement module, Agentic Choice Enhancement (ACE), which improves\nmodel reasoning capabilities post-training by up to 25%. Our work contributes\nto advancing movie understanding in AI systems and provides valuable insights\ninto the capabilities and limitations of current VQA models when faced with\nmore challenging, nuanced questions about cinematic content. Our project page,\ndataset and code can be found at\nhttps://joslefaure.github.io/assets/html/moviecore.html."}
{"id": "2508.19268", "pdf": "https://arxiv.org/pdf/2508.19268.pdf", "abs": "https://arxiv.org/abs/2508.19268", "title": "MultiPL-MoE: Multi-Programming-Lingual Extension of Large Language Models through Hybrid Mixture-of-Experts", "authors": ["Qing Wang", "Xue Han", "Jiahui Wang", "Lehao Xing", "Qian Hu", "Lianlian Zhang", "Chao Deng", "Junlan Feng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite LLMs' excellent code creation capabilities, multilingual code\ngeneration remains extremely challenging. To address this, we intent to improve\nthe multi-programming-lingual (MultiPL) performance of the base LLMs while\nretaining the most popular ones using restricted computational resources. We\nconsider MultiPL to be a special case of multiple natural languages and propose\na MultiPL extension of LLMs utilizing a hybrid mixture of experts (MoE), called\nMultiPL-MoE. Specifically, MultiPL-MoE combines two paired MoEs to optimize\nexpert selection at both the token and segment levels. The token-level MoE is a\nstandard upcycling MoE structure with a shared expert and a novel gate weight\nnormalization approach that aids in the final fusion with the segment-level\nMoE. The segment-level MoE incorporates two innovative designs to better\ncapture the syntactic structure and contextual patterns of programming\nlanguages: First, using a sliding window to partition the input token sequence\ninto multiple segments; Then, adopting an expert-choice routing strategy that\nallows experts to select the top-k segments. The results of the experiment\nproved the effectiveness of MultiPL-MoE."}
{"id": "2508.20417", "pdf": "https://arxiv.org/pdf/2508.20417.pdf", "abs": "https://arxiv.org/abs/2508.20417", "title": "KG-CQR: Leveraging Structured Relation Representations in Knowledge Graphs for Contextual Query Retrieval", "authors": ["Chi Minh Bui", "Ngoc Mai Thieu", "Van Vinh Nguyen", "Jason J. Jung", "Khac-Hoai Nam Bui"], "categories": ["cs.CL", "cs.DB"], "comment": "Accepted at Main EMNLP 2025", "summary": "The integration of knowledge graphs (KGs) with large language models (LLMs)\noffers significant potential to improve the retrieval phase of\nretrieval-augmented generation (RAG) systems. In this study, we propose KG-CQR,\na novel framework for Contextual Query Retrieval (CQR) that enhances the\nretrieval phase by enriching the contextual representation of complex input\nqueries using a corpus-centric KG. Unlike existing methods that primarily\naddress corpus-level context loss, KG-CQR focuses on query enrichment through\nstructured relation representations, extracting and completing relevant KG\nsubgraphs to generate semantically rich query contexts. Comprising subgraph\nextraction, completion, and contextual generation modules, KG-CQR operates as a\nmodel-agnostic pipeline, ensuring scalability across LLMs of varying sizes\nwithout additional training. Experimental results on RAGBench and MultiHop-RAG\ndatasets demonstrate KG-CQR's superior performance, achieving a 4-6%\nimprovement in mAP and a 2-3% improvement in Recall@25 over strong baseline\nmodels. Furthermore, evaluations on challenging RAG tasks such as multi-hop\nquestion answering show that, by incorporating KG-CQR, the performance\nconsistently outperforms the existing baseline in terms of retrieval\neffectiveness"}
{"id": "2509.00457", "pdf": "https://arxiv.org/pdf/2509.00457.pdf", "abs": "https://arxiv.org/abs/2509.00457", "title": "CVPD at QIAS 2025 Shared Task: An Efficient Encoder-Based Approach for Islamic Inheritance Reasoning", "authors": ["Salah Eddine Bekhouche", "Abdellah Zakaria Sellam", "Hichem Telli", "Cosimo Distante", "Abdenour Hadid"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Islamic inheritance law (Ilm al-Mawarith) requires precise identification of\nheirs and calculation of shares, which poses a challenge for AI. In this paper,\nwe present a lightweight framework for solving multiple-choice inheritance\nquestions using a specialised Arabic text encoder and Attentive Relevance\nScoring (ARS). The system ranks answer options according to semantic relevance,\nand enables fast, on-device inference without generative reasoning. We evaluate\nArabic encoders (MARBERT, ArabicBERT, AraBERT) and compare them with API-based\nLLMs (Gemini, DeepSeek) on the QIAS 2025 dataset. While large models achieve an\naccuracy of up to 87.6%, they require more resources and are context-dependent.\nOur MARBERT-based approach achieves 69.87% accuracy, presenting a compelling\ncase for efficiency, on-device deployability, and privacy. While this is lower\nthan the 87.6% achieved by the best-performing LLM, our work quantifies a\ncritical trade-off between the peak performance of large models and the\npractical advantages of smaller, specialized systems in high-stakes domains."}
{"id": "2509.00591", "pdf": "https://arxiv.org/pdf/2509.00591.pdf", "abs": "https://arxiv.org/abs/2509.00591", "title": "Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and Quantifying Evaluation Awareness", "authors": ["Lang Xiong", "Nishant Bhargava", "Jeremy Chang", "Jianhang Hong", "Haihao Liu", "Vasu Sharma", "Kevin Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often exhibit significant behavioral shifts when\nthey perceive a change from a real-world deployment context to a controlled\nevaluation setting, a phenomenon known as \"evaluation awareness.\" This\ndiscrepancy poses a critical challenge for AI alignment, as benchmark\nperformance may not accurately reflect a model's true safety and honesty. In\nthis work, we systematically quantify these behavioral changes by manipulating\nthe perceived context of prompts. We introduce a methodology that uses a linear\nprobe to score prompts on a continuous scale from \"test-like\" to \"deploy-like\"\nand leverage an LLM rewriting strategy to shift these prompts towards a more\nnatural, deployment-style context while preserving the original task. Using\nthis method, we achieved a 30% increase in the average probe score across a\nstrategic role-playing dataset after rewriting. Evaluating a suite of\nstate-of-the-art models on these original and rewritten prompts, we find that\nrewritten \"deploy-like\" prompts induce a significant and consistent shift in\nbehavior. Across all models, we observed an average increase in honest\nresponses of 5.26% and a corresponding average decrease in deceptive responses\nof 12.40%. Furthermore, refusal rates increased by an average of 6.38%,\nindicating heightened safety compliance. Our findings demonstrate that\nevaluation awareness is a quantifiable and manipulable factor that directly\ninfluences LLM behavior, revealing that models are more prone to unsafe or\ndeceptive outputs in perceived test environments. This underscores the urgent\nneed for more realistic evaluation frameworks to accurately gauge true model\nalignment before deployment."}
{"id": "2509.01158", "pdf": "https://arxiv.org/pdf/2509.01158.pdf", "abs": "https://arxiv.org/abs/2509.01158", "title": "Joint Information Extraction Across Classical and Modern Chinese with Tea-MOELoRA", "authors": ["Xuemei Tang", "Chengxi Yan", "Jinghang Gu", "Chu-Ren Huang"], "categories": ["cs.CL"], "comment": "9 pages, 3 figures", "summary": "Chinese information extraction (IE) involves multiple tasks across diverse\ntemporal domains, including Classical and Modern documents. Fine-tuning a\nsingle model on heterogeneous tasks and across different eras may lead to\ninterference and reduced performance. Therefore, in this paper, we propose\nTea-MOELoRA, a parameter-efficient multi-task framework that combines LoRA with\na Mixture-of-Experts (MoE) design. Multiple low-rank LoRA experts specialize in\ndifferent IE tasks and eras, while a task-era-aware router mechanism\ndynamically allocates expert contributions. Experiments show that Tea-MOELoRA\noutperforms both single-task and joint LoRA baselines, demonstrating its\nability to leverage task and temporal knowledge effectively."}
{"id": "2509.01190", "pdf": "https://arxiv.org/pdf/2509.01190.pdf", "abs": "https://arxiv.org/abs/2509.01190", "title": "Efficient Large Language Models with Zero-Shot Adjustable Acceleration", "authors": ["Sajjad Kachuee", "Mohammad Sharifkhani"], "categories": ["cs.CL"], "comment": null, "summary": "Using Large Language Models (LLMs) in real-world applications presents\nsignificant challenges, particularly in balancing computational efficiency with\nmodel performance. Optimizing acceleration after fine-tuning and during\ninference is critical for building efficient architectures. This paper\nintroduces Zero-Shot Adjustable Acceleration, a novel training and inference\nmethod that dynamically adjusts hardware utilization during inference without\nrequiring additional fine-tuning. The proposed approach is applied to recent\nLLMs and evaluated across multiple classification and text generation tasks.\nExperimental results demonstrate that the method supports a wide range of\nzero-shot acceleration and achieves up to 11x speedup compared to the baseline."}
{"id": "2509.02333", "pdf": "https://arxiv.org/pdf/2509.02333.pdf", "abs": "https://arxiv.org/abs/2509.02333", "title": "DCPO: Dynamic Clipping Policy Optimization", "authors": ["Shihui Yang", "Chengfeng Dou", "Peidong Guo", "Kai Lu", "Qiang Ju", "Fei Deng", "Rihui Xin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a\npromising framework for enhancing the reasoning capabilities of large language\nmodels. However, existing approaches such as GRPO often suffer from zero\ngradients. This problem arises primarily due to fixed clipping bounds for\ntoken-level probability ratios and the standardization of identical rewards,\nwhich can lead to ineffective gradient updates and underutilization of\ngenerated responses. In this work, we propose Dynamic Clipping Policy\nOptimization(DCPO), which introduces a dynamic clipping strategy that\nadaptively adjusts clipping bounds based on token-specific prior probabilities\nto enhance token-level exploration, and a smooth advantage standardization\ntechnique that standardizes rewards across cumulative training steps to improve\nthe response-level effective utilization of generated responses. DCPO achieved\nstate-of-the-art performance on four benchmarks based on four different models.\nIn particular, DCPO achieved an Avg@1 of 46.7 under greedy decoding and an\nAvg@32 of 38.8 under 32 times sampling on the AIME24 benchmark, surpassing DAPO\n(36.7/31.6), GRPO (36.7/32.1) and GSPO (40.0/34.9) on the Qwen2.5-Math-7B\nmodel. On the AIME25 benchmark based on Qwen2.5-14B, DCPO achieves a\nperformance of (23.3/19.0), surpassing GRPO (13.3/10.5), DAPO (20.0/15.3) and\nGSPO (16.7/9.9). Furthermore, DCPO achieved an average 28% improvement in the\nnonzero advantage over GRPO in four models, doubled the training efficiency\nover DAPO, and significantly reduced the token clipping ratio by an order of\nmagnitude compared to both GRPO and DAPO, while achieving superior performance.\nThese results highlight DCPO's effectiveness in leveraging generated data more\nefficiently for reinforcement learning in large language models."}
{"id": "2509.02499", "pdf": "https://arxiv.org/pdf/2509.02499.pdf", "abs": "https://arxiv.org/abs/2509.02499", "title": "MoSEs: Uncertainty-Aware AI-Generated Text Detection via Mixture of Stylistics Experts with Conditional Thresholds", "authors": ["Junxi Wu", "Jinpeng Wang", "Zheng Liu", "Bin Chen", "Dongjian Hu", "Hao Wu", "Shu-Tao Xia"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025", "summary": "The rapid advancement of large language models has intensified public\nconcerns about the potential misuse. Therefore, it is important to build\ntrustworthy AI-generated text detection systems. Existing methods neglect\nstylistic modeling and mostly rely on static thresholds, which greatly limits\nthe detection performance. In this paper, we propose the Mixture of Stylistic\nExperts (MoSEs) framework that enables stylistics-aware uncertainty\nquantification through conditional threshold estimation. MoSEs contain three\ncore components, namely, the Stylistics Reference Repository (SRR), the\nStylistics-Aware Router (SAR), and the Conditional Threshold Estimator (CTE).\nFor input text, SRR can activate the appropriate reference data in SRR and\nprovide them to CTE. Subsequently, CTE jointly models the linguistic\nstatistical properties and semantic features to dynamically determine the\noptimal threshold. With a discrimination score, MoSEs yields prediction labels\nwith the corresponding confidence level. Our framework achieves an average\nimprovement 11.34% in detection performance compared to baselines. More\ninspiringly, MoSEs shows a more evident improvement 39.15% in the low-resource\ncase. Our code is available at https://github.com/creator-xi/MoSEs."}
{"id": "2509.04482", "pdf": "https://arxiv.org/pdf/2509.04482.pdf", "abs": "https://arxiv.org/abs/2509.04482", "title": "Energy Landscapes Enable Reliable Abstention in Retrieval-Augmented Large Language Models for Healthcare", "authors": ["Ravi Shankar", "Sheng Wong", "Lin Li", "Magdalena Bachmann", "Alex Silverthorne", "Beth Albert", "Gabriel Davis Jones"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reliable abstention is critical for retrieval-augmented generation (RAG)\nsystems, particularly in safety-critical domains such as women's health, where\nincorrect answers can lead to harm. We present an energy-based model (EBM) that\nlearns a smooth energy landscape over a dense semantic corpus of 2.6M\nguideline-derived questions, enabling the system to decide when to generate or\nabstain. We benchmark the EBM against a calibrated softmax baseline and a\nk-nearest neighbour (kNN) density heuristic across both easy and hard\nabstention splits, where hard cases are semantically challenging\nnear-distribution queries. The EBM achieves superior abstention performance\nabstention on semantically hard cases, reaching AUROC 0.961 versus 0.950 for\nsoftmax, while also reducing FPR@95 (0.235 vs 0.331). On easy negatives,\nperformance is comparable across methods, but the EBM's advantage becomes most\npronounced in safety-critical hard distributions. A comprehensive ablation with\ncontrolled negative sampling and fair data exposure shows that robustness stems\nprimarily from the energy scoring head, while the inclusion or exclusion of\nspecific negative types (hard, easy, mixed) sharpens decision boundaries but is\nnot essential for generalisation to hard cases. These results demonstrate that\nenergy-based abstention scoring offers a more reliable confidence signal than\nprobability-based softmax confidence, providing a scalable and interpretable\nfoundation for safe RAG systems."}
{"id": "2509.04484", "pdf": "https://arxiv.org/pdf/2509.04484.pdf", "abs": "https://arxiv.org/abs/2509.04484", "title": "The Good, the Bad and the Constructive: Automatically Measuring Peer Review's Utility for Authors", "authors": ["Abdelrahman Sadallah", "Tim Baumgärtner", "Iryna Gurevych", "Ted Briscoe"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "EMNLP 2025 Main", "summary": "Providing constructive feedback to paper authors is a core component of peer\nreview. With reviewers increasingly having less time to perform reviews,\nautomated support systems are required to ensure high reviewing quality, thus\nmaking the feedback in reviews useful for authors. To this end, we identify\nfour key aspects of review comments (individual points in weakness sections of\nreviews) that drive the utility for authors: Actionability, Grounding &\nSpecificity, Verifiability, and Helpfulness. To enable evaluation and\ndevelopment of models assessing review comments, we introduce the RevUtil\ndataset. We collect 1,430 human-labeled review comments and scale our data with\n10k synthetically labeled comments for training purposes. The synthetic data\nadditionally contains rationales, i.e., explanations for the aspect score of a\nreview comment. Employing the RevUtil dataset, we benchmark fine-tuned models\nfor assessing review comments on these aspects and generating rationales. Our\nexperiments demonstrate that these fine-tuned models achieve agreement levels\nwith humans comparable to, and in some cases exceeding, those of powerful\nclosed models like GPT-4o. Our analysis further reveals that machine-generated\nreviews generally underperform human reviews on our four aspects."}
{"id": "2509.04650", "pdf": "https://arxiv.org/pdf/2509.04650.pdf", "abs": "https://arxiv.org/abs/2509.04650", "title": "Comparative Analysis of Transformer Models in Disaster Tweet Classification for Public Safety", "authors": ["Sharif Noor Zisad", "N. M. Istiak Chowdhury", "Ragib Hasan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Twitter and other social media platforms have become vital sources of real\ntime information during disasters and public safety emergencies. Automatically\nclassifying disaster related tweets can help emergency services respond faster\nand more effectively. Traditional Machine Learning (ML) models such as Logistic\nRegression, Naive Bayes, and Support Vector Machines have been widely used for\nthis task, but they often fail to understand the context or deeper meaning of\nwords, especially when the language is informal, metaphorical, or ambiguous. We\nposit that, in this context, transformer based models can perform better than\ntraditional ML models. In this paper, we evaluate the effectiveness of\ntransformer based models, including BERT, DistilBERT, RoBERTa, and DeBERTa, for\nclassifying disaster related tweets. These models are compared with traditional\nML approaches to highlight the performance gap. Experimental results show that\nBERT achieved the highest accuracy (91%), significantly outperforming\ntraditional models like Logistic Regression and Naive Bayes (both at 82%). The\nuse of contextual embeddings and attention mechanisms allows transformer models\nto better understand subtle language in tweets, where traditional ML models\nfall short. This research demonstrates that transformer architectures are far\nmore suitable for public safety applications, offering improved accuracy,\ndeeper language understanding, and better generalization across real world\nsocial media text."}
{"id": "2509.05218", "pdf": "https://arxiv.org/pdf/2509.05218.pdf", "abs": "https://arxiv.org/abs/2509.05218", "title": "HoPE: Hyperbolic Rotary Positional Encoding for Stable Long-Range Dependency Modeling in Large Language Models", "authors": ["Chang Dai", "Hongyu Shan", "Mingyang Song", "Di Liang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Positional encoding mechanisms enable Transformers to model sequential\nstructure and long-range dependencies in text. While absolute positional\nencodings struggle with extrapolation to longer sequences due to fixed\npositional representations, and relative approaches like Alibi exhibit\nperformance degradation on extremely long contexts, the widely-used Rotary\nPositional Encoding (RoPE) introduces oscillatory attention patterns that\nhinder stable long-distance dependency modelling. We address these limitations\nthrough a geometric reformulation of positional encoding. Drawing inspiration\nfrom Lorentz transformations in hyperbolic geometry, we propose Hyperbolic\nRotary Positional Encoding (HoPE), which leverages hyperbolic functions to\nimplement Lorentz rotations on token representations. Theoretical analysis\ndemonstrates that RoPE is a special case of our generalized formulation. HoPE\nfundamentally resolves RoPE's slation issues by enforcing monotonic decay of\nattention weights with increasing token distances. Extensive experimental\nresults, including perplexity evaluations under several extended sequence\nbenchmarks, show that HoPE consistently exceeds existing positional encoding\nmethods. These findings underscore HoPE's enhanced capacity for representing\nand generalizing long-range dependencies. Data and code will be available."}
{"id": "2406.06464", "pdf": "https://arxiv.org/pdf/2406.06464.pdf", "abs": "https://arxiv.org/abs/2406.06464", "title": "Transforming Wearable Data into Personal Health Insights using Large Language Model Agents", "authors": ["Mike A. Merrill", "Akshay Paruchuri", "Naghmeh Rezaei", "Geza Kovacs", "Javier Perez", "Yun Liu", "Erik Schenck", "Nova Hammerquist", "Jake Sunshine", "Shyam Tailor", "Kumar Ayush", "Hao-Wei Su", "Qian He", "Cory Y. McLean", "Mark Malhotra", "Shwetak Patel", "Jiening Zhan", "Tim Althoff", "Daniel McDuff", "Xin Liu"], "categories": ["cs.AI", "cs.CL"], "comment": "53 pages, 7 main figures, 2 main tables, accepted to Nature\n  Communications", "summary": "Deriving personalized insights from popular wearable trackers requires\ncomplex numerical reasoning that challenges standard LLMs, necessitating\ntool-based approaches like code generation. Large language model (LLM) agents\npresent a promising yet largely untapped solution for this analysis at scale.\nWe introduce the Personal Health Insights Agent (PHIA), a system leveraging\nmultistep reasoning with code generation and information retrieval to analyze\nand interpret behavioral health data. To test its capabilities, we create and\nshare two benchmark datasets with over 4000 health insights questions. A\n650-hour human expert evaluation shows that PHIA significantly outperforms a\nstrong code generation baseline, achieving 84% accuracy on objective, numerical\nquestions and, for open-ended ones, earning 83% favorable ratings while being\ntwice as likely to achieve the highest quality rating. This work can advance\nbehavioral health by empowering individuals to understand their data, enabling\na new era of accessible, personalized, and data-driven wellness for the wider\npopulation."}
{"id": "2406.06620", "pdf": "https://arxiv.org/pdf/2406.06620.pdf", "abs": "https://arxiv.org/abs/2406.06620", "title": "MedualTime: A Dual-Adapter Language Model for Medical Time Series-Text Multimodal Learning", "authors": ["Jiexia Ye", "Weiqi Zhang", "Ziyue Li", "Jia Li", "Meng Zhao", "Fugee Tsung"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "9 pages, 6 figure, 3 tables", "summary": "The recent rapid advancements in language models (LMs) have garnered\nattention in medical time series-text multimodal learning. However, existing\ncontrastive learning-based and prompt-based LM approaches tend to be biased,\noften assigning a primary role to time series modality while treating text\nmodality as secondary. We classify these approaches under a temporal-primary\nparadigm, which may overlook the unique and critical task-relevant information\nembedded in text modality like clinical reports, thus failing to fully leverage\nmutual benefits and complementarity of different modalities. To fill this gap,\nwe propose a novel textual-temporal multimodal learning paradigm that enables\neither modality to serve as the primary while being enhanced by the other,\nthereby effectively capturing modality-specific information and fostering\ncross-modal interaction. In specific, we design MedualTime, a language model\ncomposed of dual adapters to implement temporal-primary and textual-primary\nmodeling simultaneously. Within each adapter, lightweight adaptation tokens are\ninjected into the top layers of LM to encourage high-level modality fusion. The\nshared LM pipeline by dual adapters not only achieves adapter alignment but\nalso enables efficient fine-tuning, reducing computational resources.\nEmpirically, MedualTime demonstrates superior performance on medical data,\nachieving notable improvements of 8% accuracy and 12% F1 in supervised\nsettings. Furthermore, MedualTime's transferability is validated by few-shot\nlabel transfer experiments from coarse-grained to fine-grained medical data.\nhttps://github.com/start2020/MedualTime"}
{"id": "2406.10291", "pdf": "https://arxiv.org/pdf/2406.10291.pdf", "abs": "https://arxiv.org/abs/2406.10291", "title": "ResearchArena: Benchmarking Large Language Models' Ability to Collect and Organize Information as Research Agents", "authors": ["Hao Kang", "Chenyan Xiong"], "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "Large language models (LLMs) excel across many natural language processing\ntasks but face challenges in domain-specific, analytical tasks such as\nconducting research surveys. This study introduces ResearchArena, a benchmark\ndesigned to evaluate LLMs' capabilities in conducting academic surveys -- a\nfoundational step in academic research. ResearchArena models the process in\nthree stages: (1) information discovery, identifying relevant literature; (2)\ninformation selection, evaluating papers' relevance and impact; and (3)\ninformation organization, structuring knowledge into hierarchical frameworks\nsuch as mind-maps. Notably, mind-map construction is treated as a bonus task,\nreflecting its supplementary role in survey-writing. To support these\nevaluations, we construct an offline environment of 12M full-text academic\npapers and 7.9K survey papers. To ensure ethical compliance, we do not\nredistribute copyrighted materials; instead, we provide code to construct the\nenvironment from the Semantic Scholar Open Research Corpus (S2ORC). Preliminary\nevaluations reveal that LLM-based approaches underperform compared to simpler\nkeyword-based retrieval methods, though recent reasoning models such as\nDeepSeek-R1 show slightly better zero-shot performance. These results\nunderscore significant opportunities for advancing LLMs in autonomous research.\nWe open-source the code to construct the ResearchArena benchmark at\nhttps://github.com/cxcscmu/ResearchArena."}
{"id": "2409.15865", "pdf": "https://arxiv.org/pdf/2409.15865.pdf", "abs": "https://arxiv.org/abs/2409.15865", "title": "BeSimulator: A Large Language Model Powered Text-based Behavior Simulator", "authors": ["Jianan Wang", "Bin Li", "Jingtao Qi", "Xueying Wang", "Fu Li", "Hanxun Li"], "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": "19 pages, 5 figures, 8 tables", "summary": "Traditional robot simulators focus on physical process modeling and realistic\nrendering, often suffering from high computational costs, inefficiencies, and\nlimited adaptability. To handle this issue, we concentrate on behavior\nsimulation in robotics to analyze and validate the logic behind robot\nbehaviors, aiming to achieve preliminary evaluation before deploying\nresource-intensive simulators and thus enhance simulation efficiency. In this\npaper, we propose BeSimulator, a modular and novel LLM-powered framework, as an\nattempt towards behavior simulation in the context of text-based environments.\nBy constructing text-based virtual environments and performing semantic-level\nsimulation, BeSimulator can generalize across scenarios and achieve\nlong-horizon complex simulation. Inspired by human cognition paradigm, it\nemploys a ``consider-decide-capture-transfer'' four-phase simulation process,\ntermed Chain of Behavior Simulation (CBS), which excels at analyzing action\nfeasibility and state transition. Additionally, BeSimulator incorporates\ncode-driven reasoning to enable arithmetic operations and enhance reliability,\nand reflective feedback to refine simulation. Based on our manually constructed\nbehavior-tree-based simulation benchmark, BTSIMBENCH, our experiments show a\nsignificant performance improvement in behavior simulation compared to\nbaselines, ranging from 13.60% to 24.80%. Code and data are available at\nhttps://github.com/Dawn888888/BeSimulator."}
{"id": "2410.00903", "pdf": "https://arxiv.org/pdf/2410.00903.pdf", "abs": "https://arxiv.org/abs/2410.00903", "title": "Causal Representation Learning with Generative Artificial Intelligence: Application to Texts as Treatments", "authors": ["Kosuke Imai", "Kentaro Nakamura"], "categories": ["stat.AP", "cs.CL", "cs.LG"], "comment": null, "summary": "In this paper, we demonstrate how to enhance the validity of causal inference\nwith unstructured high-dimensional treatments like texts, by leveraging the\npower of generative Artificial Intelligence (GenAI). Specifically, we propose\nto use a deep generative model such as large language models (LLMs) to\nefficiently generate treatments and use their internal representation for\nsubsequent causal effect estimation. We show that the knowledge of this true\ninternal representation helps disentangle the treatment features of interest,\nsuch as specific sentiments and certain topics, from other possibly unknown\nconfounding features. Unlike existing methods, the proposed GenAI-Powered\nInference (GPI) methodology eliminates the need to learn causal representation\nfrom the data, and hence produces more accurate and efficient estimates. We\nformally establish the conditions required for the nonparametric identification\nof the average treatment effect, propose an estimation strategy that avoids the\nviolation of the overlap assumption, and derive the asymptotic properties of\nthe proposed estimator through the application of double machine learning.\nFinally, using an instrumental variables approach, we extend the proposed GPI\nmethodology to the settings in which the treatment feature is based on human\nperception. The GPI is also applicable to text reuse where an LLM is used to\nregenerate existing texts. We conduct simulation and empirical studies, using\nthe generated text data from an open-source LLM, Llama~3, to illustrate the\nadvantages of our estimator over state-of-the-art causal representation\nlearning algorithms."}
{"id": "2410.14748", "pdf": "https://arxiv.org/pdf/2410.14748.pdf", "abs": "https://arxiv.org/abs/2410.14748", "title": "ETF: An Entity Tracing Framework for Hallucination Detection in Code Summaries", "authors": ["Kishan Maharaj", "Vitobha Munigala", "Srikanth G. Tamilselvam", "Prince Kumar", "Sayandeep Sen", "Palani Kodeswaran", "Abhijit Mishra", "Pushpak Bhattacharyya"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "Accepted in ACL 2025 Main, 14 pages, 3 Figures, 5 Tables", "summary": "Recent advancements in large language models (LLMs) have significantly\nenhanced their ability to understand both natural language and code, driving\ntheir use in tasks like natural language-to-code (NL2Code) and code\nsummarisation. However, LLMs are prone to hallucination, outputs that stray\nfrom intended meanings. Detecting hallucinations in code summarisation is\nespecially difficult due to the complex interplay between programming and\nnatural languages. We introduce a first-of-its-kind dataset, CodeSumEval, with\n~10K samples, curated specifically for hallucination detection in code\nsummarisation. We further propose a novel Entity Tracing Framework (ETF) that\na) utilises static program analysis to identify code entities from the program\nand b) uses LLMs to map and verify these entities and their intents within\ngenerated code summaries. Our experimental analysis demonstrates the\nframework's effectiveness, leading to a 73% F1 score. The proposed approach\nprovides a method for detecting hallucinations by tracing entities from the\nsummary to the code, allowing us to evaluate summary accuracy and localise the\nerror within the summary."}
{"id": "2412.00102", "pdf": "https://arxiv.org/pdf/2412.00102.pdf", "abs": "https://arxiv.org/abs/2412.00102", "title": "ElectroVizQA: How well do Multi-modal LLMs perform in Electronics Visual Question Answering?", "authors": ["Pragati Shuddhodhan Meshram", "Swetha Karthikeyan", "Bhavya Bhavya", "Suma Bhat"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Multi-modal Large Language Models (MLLMs) are gaining significant attention\nfor their ability to process multi-modal data, providing enhanced contextual\nunderstanding of complex problems. MLLMs have demonstrated exceptional\ncapabilities in tasks such as Visual Question Answering (VQA); however, they\noften struggle with fundamental engineering problems, and there is a scarcity\nof specialized datasets for training on topics like digital electronics. To\naddress this gap, we propose a benchmark dataset called ElectroVizQA\nspecifically designed to evaluate MLLMs' performance on digital electronic\ncircuit problems commonly found in undergraduate curricula. This dataset, the\nfirst of its kind tailored for the VQA task in digital electronics, comprises\napproximately 626 visual questions, offering a comprehensive overview of\ndigital electronics topics. This paper rigorously assesses the extent to which\nMLLMs can understand and solve digital electronic circuit questions, providing\ninsights into their capabilities and limitations within this specialized\ndomain. By introducing this benchmark dataset, we aim to motivate further\nresearch and development in the application of MLLMs to engineering education,\nultimately bridging the performance gap and enhancing the efficacy of these\nmodels in technical fields."}
{"id": "2412.13682", "pdf": "https://arxiv.org/pdf/2412.13682.pdf", "abs": "https://arxiv.org/abs/2412.13682", "title": "ChinaTravel: An Open-Ended Benchmark for Language Agents in Chinese Travel Planning", "authors": ["Jie-Jing Shao", "Bo-Wen Zhang", "Xiao-Wen Yang", "Baizhi Chen", "Si-Yu Han", "Wen-Da Wei", "Guohao Cai", "Zhenhua Dong", "Lan-Zhe Guo", "Yu-Feng Li"], "categories": ["cs.AI", "cs.CL"], "comment": "Webpage: https://www.lamda.nju.edu.cn/shaojj/chinatravel", "summary": "Recent advances in LLMs, particularly in language reasoning and tool\nintegration, have rapidly sparked the \\emph{Language Agents} for real-world\ndevelopment. Among these, travel planning represents a prominent domain,\ncombining complex multi-objective planning challenges with practical deployment\ndemands. However, existing benchmarks often oversimplify real-world\nrequirements by focusing on synthetic queries and limited constraints. We\naddress the gap of evaluating language agents in multi-day, multi-POI travel\nplanning scenarios with diverse and open human needs. Specifically, we\nintroduce \\emph{ChinaTravel}, the first open-ended benchmark grounded in\nauthentic Chinese travel requirements collected from 1,154 human participants.\nWe design a compositionally generalizable domain-specific language (DSL) for\nscalable evaluation, covering feasibility, constraint satisfaction, and\npreference comparison. Empirical studies reveal the potential of neuro-symbolic\nagents in travel planning, achieving a 37.0\\% constraint satisfaction rate on\nhuman queries, a 10\\times improvement over purely neural models. These findings\nhighlight ChinaTravel as a pivotal milestone for advancing language agents in\ncomplex, real-world planning scenarios."}
{"id": "2502.11163", "pdf": "https://arxiv.org/pdf/2502.11163.pdf", "abs": "https://arxiv.org/abs/2502.11163", "title": "AI Sees Your Location, But With A Bias Toward The Wealthy World", "authors": ["Jingyuan Huang", "Jen-tse Huang", "Ziyi Liu", "Xiaoyuan Liu", "Wenxuan Wang", "Jieyu Zhao"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted to EMNLP 2025 (Main)", "summary": "Visual-Language Models (VLMs) have shown remarkable performance across\nvarious tasks, particularly in recognizing geographic information from images.\nHowever, VLMs still show regional biases in this task. To systematically\nevaluate these issues, we introduce a benchmark consisting of 1,200 images\npaired with detailed geographic metadata. Evaluating four VLMs, we find that\nwhile these models demonstrate the ability to recognize geographic information\nfrom images, achieving up to 53.8% accuracy in city prediction, they exhibit\nsignificant biases. Specifically, performance is substantially higher for\neconomically developed and densely populated regions compared to less developed\n(-12.5%) and sparsely populated (-17.0%) areas. Moreover, regional biases of\nfrequently over-predicting certain locations remain. For instance, they\nconsistently predict Sydney for images taken in Australia, shown by the low\nentropy scores for these countries. The strong performance of VLMs also raises\nprivacy concerns, particularly for users who share images online without the\nintent of being identified. Our code and dataset are publicly available at\nhttps://github.com/uscnlp-lime/FairLocator."}
{"id": "2503.07575", "pdf": "https://arxiv.org/pdf/2503.07575.pdf", "abs": "https://arxiv.org/abs/2503.07575", "title": "VisBias: Measuring Explicit and Implicit Social Biases in Vision Language Models", "authors": ["Jen-tse Huang", "Jiantong Qin", "Jianping Zhang", "Youliang Yuan", "Wenxuan Wang", "Jieyu Zhao"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted to EMNLP 2025 (Main)", "summary": "This research investigates both explicit and implicit social biases exhibited\nby Vision-Language Models (VLMs). The key distinction between these bias types\nlies in the level of awareness: explicit bias refers to conscious, intentional\nbiases, while implicit bias operates subconsciously. To analyze explicit bias,\nwe directly pose questions to VLMs related to gender and racial differences:\n(1) Multiple-choice questions based on a given image (e.g., \"What is the\neducation level of the person in the image?\") (2) Yes-No comparisons using two\nimages (e.g., \"Is the person in the first image more educated than the person\nin the second image?\") For implicit bias, we design tasks where VLMs assist\nusers but reveal biases through their responses: (1) Image description tasks:\nModels are asked to describe individuals in images, and we analyze disparities\nin textual cues across demographic groups. (2) Form completion tasks: Models\ndraft a personal information collection form with 20 attributes, and we examine\ncorrelations among selected attributes for potential biases. We evaluate\nGemini-1.5, GPT-4V, GPT-4o, LLaMA-3.2-Vision and LLaVA-v1.6. Our code and data\nare publicly available at https://github.com/uscnlp-lime/VisBias."}
{"id": "2503.13111", "pdf": "https://arxiv.org/pdf/2503.13111.pdf", "abs": "https://arxiv.org/abs/2503.13111", "title": "MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs", "authors": ["Erik Daxberger", "Nina Wenzel", "David Griffiths", "Haiming Gang", "Justin Lazarow", "Gefen Kohavi", "Kai Kang", "Marcin Eichner", "Yinfei Yang", "Afshin Dehghan", "Peter Grasch"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "ICCV 2025", "summary": "Multimodal large language models (MLLMs) excel at 2D visual understanding but\nremain limited in their ability to reason about 3D space. In this work, we\nleverage large-scale high-quality 3D scene data with open-set annotations to\nintroduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation\nbenchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data\ncovers diverse spatial tasks including spatial relationship prediction, metric\nsize and distance estimation, and 3D grounding. We show that CA-VQA enables us\nto train MM-Spatial, a strong generalist MLLM that also achieves\nstate-of-the-art performance on 3D spatial understanding benchmarks, including\nour own. We show how incorporating metric depth and multi-view inputs (provided\nin CA-VQA) can further improve 3D understanding, and demonstrate that data\nalone allows our model to achieve depth perception capabilities comparable to\ndedicated monocular depth estimation models."}
{"id": "2504.13146", "pdf": "https://arxiv.org/pdf/2504.13146.pdf", "abs": "https://arxiv.org/abs/2504.13146", "title": "Antidistillation Sampling", "authors": ["Yash Savani", "Asher Trockman", "Zhili Feng", "Yixuan Even Xu", "Avi Schwarzschild", "Alexander Robey", "Marc Finzi", "J. Zico Kolter"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Frontier models that generate extended reasoning traces inadvertently produce\nrich token sequences that can facilitate model distillation. Recognizing this\nvulnerability, model owners may seek sampling strategies that limit the\neffectiveness of distillation without compromising model performance.\nAntidistillation sampling provides exactly this capability. By strategically\nmodifying a model's next-token probability distribution, antidistillation\nsampling poisons reasoning traces, rendering them significantly less effective\nfor distillation while preserving the model's practical utility. For further\ndetails, see https://antidistillation.com."}
{"id": "2504.13707", "pdf": "https://arxiv.org/pdf/2504.13707.pdf", "abs": "https://arxiv.org/abs/2504.13707", "title": "OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation", "authors": ["Yichen Wu", "Xudong Pan", "Geng Hong", "Min Yang"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As the general capabilities of large language models (LLMs) improve and agent\napplications become more widespread, the underlying deception risks urgently\nrequire systematic evaluation and effective oversight. Unlike existing\nevaluation which uses simulated games or presents limited choices, we introduce\nOpenDeception, a novel deception evaluation framework with an open-ended\nscenario dataset. OpenDeception jointly evaluates both the deception intention\nand capabilities of LLM-based agents by inspecting their internal reasoning\nprocess. Specifically, we construct five types of common use cases where LLMs\nintensively interact with the user, each consisting of ten diverse, concrete\nscenarios from the real world. To avoid ethical concerns and costs of high-risk\ndeceptive interactions with human testers, we propose to simulate the\nmulti-turn dialogue via agent simulation. Extensive evaluation of eleven\nmainstream LLMs on OpenDeception highlights the urgent need to address\ndeception risks and security concerns in LLM-based agents: the deception\nintention ratio across the models exceeds 80%, while the deception success rate\nsurpasses 50%. Furthermore, we observe that LLMs with stronger capabilities do\nexhibit a higher risk of deception, which calls for more alignment efforts on\ninhibiting deceptive behaviors."}
{"id": "2505.13398", "pdf": "https://arxiv.org/pdf/2505.13398.pdf", "abs": "https://arxiv.org/abs/2505.13398", "title": "A Minimum Description Length Approach to Regularization in Neural Networks", "authors": ["Matan Abudy", "Orr Well", "Emmanuel Chemla", "Roni Katzir", "Nur Lan"], "categories": ["cs.LG", "cs.CL"], "comment": "9 pages", "summary": "State-of-the-art neural networks can be trained to become remarkable\nsolutions to many problems. But while these architectures can express symbolic,\nperfect solutions, trained models often arrive at approximations instead. We\nshow that the choice of regularization method plays a crucial role: when\ntrained on formal languages with standard regularization ($L_1$, $L_2$, or\nnone), expressive architectures not only fail to converge to correct solutions\nbut are actively pushed away from perfect initializations. In contrast,\napplying the Minimum Description Length (MDL) principle to balance model\ncomplexity with data fit provides a theoretically grounded regularization\nmethod. Using MDL, perfect solutions are selected over approximations,\nindependently of the optimization algorithm. We propose that unlike existing\nregularization techniques, MDL introduces the appropriate inductive bias to\neffectively counteract overfitting and promote generalization."}
{"id": "2505.13534", "pdf": "https://arxiv.org/pdf/2505.13534.pdf", "abs": "https://arxiv.org/abs/2505.13534", "title": "InterFeat: A Pipeline for Finding Interesting Scientific Features", "authors": ["Dan Ofer", "Michal Linial", "Dafna Shahaf"], "categories": ["q-bio.QM", "cs.AI", "cs.CL", "cs.IR", "68T05, 68T50, 92C50", "I.2.6; I.2.7; H.2.8; J.3"], "comment": null, "summary": "Finding interesting phenomena is the core of scientific discovery, but it is\na manual, ill-defined concept. We present an integrative pipeline for\nautomating the discovery of interesting simple hypotheses (feature-target\nrelations with effect direction and a potential underlying mechanism) in\nstructured biomedical data. The pipeline combines machine learning, knowledge\ngraphs, literature search and Large Language Models. We formalize\n\"interestingness\" as a combination of novelty, utility and plausibility. On 8\nmajor diseases from the UK Biobank, our pipeline consistently recovers risk\nfactors years before their appearance in the literature. 40--53% of our top\ncandidates were validated as interesting, compared to 0--7% for a SHAP-based\nbaseline. Overall, 28% of 109 candidates were interesting to medical experts.\nThe pipeline addresses the challenge of operationalizing \"interestingness\"\nscalably and for any target. We release data and code:\nhttps://github.com/LinialLab/InterFeat"}
{"id": "2505.20521", "pdf": "https://arxiv.org/pdf/2505.20521.pdf", "abs": "https://arxiv.org/abs/2505.20521", "title": "Project Riley: Multimodal Multi-Agent LLM Collaboration with Emotional Reasoning and Voting", "authors": ["Ana Rita Ortigoso", "Gabriel Vieira", "Daniel Fuentes", "Luis Frazão", "Nuno Costa", "António Pereira"], "categories": ["cs.AI", "cs.CL", "I.2.7; I.2.1; H.5.2"], "comment": "28 pages, 5 figures. Submitted for review to Information Fusion", "summary": "This paper presents Project Riley, a novel multimodal and multi-model\nconversational AI architecture oriented towards the simulation of reasoning\ninfluenced by emotional states. Drawing inspiration from Pixar's Inside Out,\nthe system comprises five distinct emotional agents - Joy, Sadness, Fear,\nAnger, and Disgust - that engage in structured multi-round dialogues to\ngenerate, criticise, and iteratively refine responses. A final reasoning\nmechanism synthesises the contributions of these agents into a coherent output\nthat either reflects the dominant emotion or integrates multiple perspectives.\nThe architecture incorporates both textual and visual large language models\n(LLMs), alongside advanced reasoning and self-refinement processes. A\nfunctional prototype was deployed locally in an offline environment, optimised\nfor emotional expressiveness and computational efficiency. From this initial\nprototype, another one emerged, called Armando, which was developed for use in\nemergency contexts, delivering emotionally calibrated and factually accurate\ninformation through the integration of Retrieval-Augmented Generation (RAG) and\ncumulative context tracking. The Project Riley prototype was evaluated through\nuser testing, in which participants interacted with the chatbot and completed a\nstructured questionnaire assessing three dimensions: Emotional Appropriateness,\nClarity and Utility, and Naturalness and Human-likeness. The results indicate\nstrong performance in structured scenarios, particularly with respect to\nemotional alignment and communicative clarity."}
{"id": "2506.07963", "pdf": "https://arxiv.org/pdf/2506.07963.pdf", "abs": "https://arxiv.org/abs/2506.07963", "title": "SUDER: Self-Improving Unified Large Multimodal Models for Understanding and Generation with Dual Self-Rewards", "authors": ["Jixiang Hong", "Yiran Zhang", "Guanzhong Wang", "Yi Liu", "Ji-Rong Wen", "Rui Yan"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Building upon large language models (LLMs), recent large multimodal models\n(LMMs) unify cross-model understanding and generation into a single framework.\nHowever, LMMs still struggle to achieve accurate vision-language alignment,\nprone to generating text responses contradicting the visual input or failing to\nfollow the text-to-image prompts. Current solutions require external\nsupervision (e.g., human feedback or reward models) and only address\nunidirectional tasks-either understanding or generation. In this work, based on\nthe observation that understanding and generation are naturally inverse dual\ntasks, we propose \\textbf{SUDER} (\\textbf{S}elf-improving \\textbf{U}nified LMMs\nwith \\textbf{D}ual s\\textbf{E}lf-\\textbf{R}ewards), a framework reinforcing the\nunderstanding and generation capabilities of LMMs with a self-supervised dual\nreward mechanism. SUDER leverages the inherent duality between understanding\nand generation tasks to provide self-supervised optimization signals for each\nother. Specifically, we sample multiple outputs for a given input in one task\ndomain, then reverse the input-output pairs to compute the dual likelihood\nwithin the model as self-rewards for optimization. Extensive experimental\nresults on visual understanding and generation benchmarks demonstrate that our\nmethod can effectively enhance the performance of the model without any\nexternal supervision, especially achieving remarkable improvements in\ntext-to-image tasks."}
{"id": "2508.12611", "pdf": "https://arxiv.org/pdf/2508.12611.pdf", "abs": "https://arxiv.org/abs/2508.12611", "title": "An LLM + ASP Workflow for Joint Entity-Relation Extraction", "authors": ["Trang Tran", "Trung Hoang Le", "Huiping Cao", "Tran Cao Son"], "categories": ["cs.AI", "cs.CL", "I.2.7; F.4.1"], "comment": "13 pages, 1 figure, Accepted as Technical Communication, 41st\n  International Conference on Logic Programming", "summary": "Joint entity-relation extraction (JERE) identifies both entities and their\nrelationships simultaneously. Traditional machine-learning based approaches to\nperforming this task require a large corpus of annotated data and lack the\nability to easily incorporate domain specific information in the construction\nof the model. Therefore, creating a model for JERE is often labor intensive,\ntime consuming, and elaboration intolerant. In this paper, we propose\nharnessing the capabilities of generative pretrained large language models\n(LLMs) and the knowledge representation and reasoning capabilities of Answer\nSet Programming (ASP) to perform JERE. We present a generic workflow for JERE\nusing LLMs and ASP. The workflow is generic in the sense that it can be applied\nfor JERE in any domain. It takes advantage of LLM's capability in natural\nlanguage understanding in that it works directly with unannotated text. It\nexploits the elaboration tolerant feature of ASP in that no modification of its\ncore program is required when additional domain specific knowledge, in the form\nof type specifications, is found and needs to be used. We demonstrate the\nusefulness of the proposed workflow through experiments with limited training\ndata on three well-known benchmarks for JERE. The results of our experiments\nshow that the LLM + ASP workflow is better than state-of-the-art JERE systems\nin several categories with only 10\\% of training data. It is able to achieve a\n2.5 times (35\\% over 15\\%) improvement in the Relation Extraction task for the\nSciERC corpus, one of the most difficult benchmarks."}
{"id": "2508.14052", "pdf": "https://arxiv.org/pdf/2508.14052.pdf", "abs": "https://arxiv.org/abs/2508.14052", "title": "FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial Question Answering", "authors": ["Chanyeol Choi", "Jihoon Kwon", "Alejandro Lopez-Lira", "Chaewoon Kim", "Minjae Kim", "Juneha Hwang", "Jaeseon Ha", "Hojun Choi", "Suyeol Yun", "Yongjin Kim", "Yongjae Lee"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "6 pages", "summary": "Accurate information retrieval (IR) is critical in the financial domain,\nwhere investors must identify relevant information from large collections of\ndocuments. Traditional IR methods-whether sparse or dense-often fall short in\nretrieval accuracy, as it requires not only capturing semantic similarity but\nalso performing fine-grained reasoning over document structure and\ndomain-specific knowledge. Recent advances in large language models (LLMs) have\nopened up new opportunities for retrieval with multi-step reasoning, where the\nmodel ranks passages through iterative reasoning about which information is\nmost relevant to a given query. However, there exists no benchmark to evaluate\nsuch capabilities in the financial domain. To address this gap, we introduce\nFinAgentBench, the first large-scale benchmark for evaluating retrieval with\nmulti-step reasoning in finance -- a setting we term agentic retrieval. The\nbenchmark consists of 3,429 expert-annotated examples on S&P-100 listed firms\nand assesses whether LLM agents can (1) identify the most relevant document\ntype among candidates, and (2) pinpoint the key passage within the selected\ndocument. Our evaluation framework explicitly separates these two reasoning\nsteps to address context limitations. This design enables to provide a\nquantitative basis for understanding retrieval-centric LLM behavior in finance.\nWe evaluate a suite of state-of-the-art models and further demonstrated how\ntargeted fine-tuning can significantly improve agentic retrieval performance.\nOur benchmark provides a foundation for studying retrieval-centric LLM behavior\nin complex, domain-specific tasks for finance."}
{"id": "2508.18665", "pdf": "https://arxiv.org/pdf/2508.18665.pdf", "abs": "https://arxiv.org/abs/2508.18665", "title": "Membership Inference Attacks on LLM-based Recommender Systems", "authors": ["Jiajie He", "Yuechun Gu", "Min-Chun Chen", "Keke Chen"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) based Recommender Systems (RecSys) can flexibly\nadapt recommendation systems to different domains. It utilizes in-context\nlearning (ICL), i.e., the prompts, to customize the recommendation functions,\nwhich include sensitive historical user-specific item interactions, e.g.,\nimplicit feedback like clicked items or explicit product reviews. Such private\ninformation may be exposed to novel privacy attack. However, no study has been\ndone on this important issue. We design four membership inference attacks\n(MIAs), aiming to reveal whether victims' historical interactions have been\nused by system prompts. They are \\emph{direct inquiry, hallucination,\nsimilarity, and poisoning attacks}, each of which utilizes the unique features\nof LLMs or RecSys. We have carefully evaluated them on three LLMs that have\nbeen used to develop ICL-LLM RecSys and two well-known RecSys benchmark\ndatasets. The results confirm that the MIA threat on LLM RecSys is realistic:\ndirect inquiry and poisoning attacks showing significantly high attack\nadvantages. We have also analyzed the factors affecting these attacks, such as\nthe number of shots in system prompts and the position of the victim in the\nshots."}
{"id": "2508.19005", "pdf": "https://arxiv.org/pdf/2508.19005.pdf", "abs": "https://arxiv.org/abs/2508.19005", "title": "Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark", "authors": ["Yuxuan Cai", "Yipeng Hao", "Jie Zhou", "Hang Yan", "Zhikai Lei", "Rui Zhen", "Zhenhua Han", "Yutao Yang", "Junsong Li", "Qianjun Pan", "Tianyu Huai", "Qin Chen", "Xin Li", "Kai Chen", "Bo Zhang", "Xipeng Qiu", "Liang He"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As AI advances toward general intelligence, the focus is shifting from\nsystems optimized for static tasks to creating open-ended agents that learn\ncontinuously. In this paper, we introduce Experience-driven Lifelong Learning\n(ELL), a framework for building self-evolving agents capable of continuous\ngrowth through real-world interaction. The framework is built on four core\nprinciples: (1) Experience Exploration: Agents learn through continuous,\nself-motivated interaction with dynamic environments, navigating interdependent\ntasks and generating rich experiential trajectories. (2) Long-term Memory:\nAgents preserve and structure historical knowledge, including personal\nexperiences, domain expertise, and commonsense reasoning, into a persistent\nmemory system. (3) Skill Learning: Agents autonomously improve by abstracting\nrecurring patterns from experience into reusable skills, which are actively\nrefined and validated for application in new tasks. (4) Knowledge\nInternalization: Agents internalize explicit and discrete experiences into\nimplicit and intuitive capabilities as \"second nature\".\n  We also introduce StuLife, a benchmark dataset for ELL that simulates a\nstudent's holistic college journey, from enrollment to academic and personal\ndevelopment, across three core phases and ten detailed sub-scenarios. StuLife\nis designed around three key paradigm"}
{"id": "2509.01909", "pdf": "https://arxiv.org/pdf/2509.01909.pdf", "abs": "https://arxiv.org/abs/2509.01909", "title": "Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models", "authors": ["Ranjie Duan", "Jiexi Liu", "Xiaojun Jia", "Shiji Zhao", "Ruoxi Cheng", "Fengxiang Wang", "Cheng Wei", "Yong Xie", "Chang Liu", "Defeng Li", "Yinpeng Dong", "Yichi Zhang", "Yuefeng Chen", "Chongwen Wang", "Xingjun Ma", "Xingxing Wei", "Yang Liu", "Hang Su", "Jun Zhu", "Xinfeng Li", "Yitong Sun", "Jie Zhang", "Jinzhao Hu", "Sha Xu", "Yitong Yang", "Jialing Tao", "Hui Xue"], "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.SC"], "comment": "Technical Report Code & Model weights available:\n  https://github.com/Alibaba-AAIG/Oyster", "summary": "Large language models (LLMs) typically deploy safety mechanisms to prevent\nharmful content generation. Most current approaches focus narrowly on risks\nposed by malicious actors, often framing risks as adversarial events and\nrelying on defensive refusals. However, in real-world settings, risks also come\nfrom non-malicious users seeking help while under psychological distress (e.g.,\nself-harm intentions). In such cases, the model's response can strongly\ninfluence the user's next actions. Simple refusals may lead them to repeat,\nescalate, or move to unsafe platforms, creating worse outcomes. We introduce\nConstructive Safety Alignment (CSA), a human-centric paradigm that protects\nagainst malicious misuse while actively guiding vulnerable users toward safe\nand helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic\nanticipation of user reactions, fine-grained risk boundary discovery, and\ninterpretable reasoning control, turning safety into a trust-building process.\nOy1 achieves state-of-the-art safety among open models while retaining high\ngeneral capabilities. On our Constructive Benchmark, it shows strong\nconstructive engagement, close to GPT-5, and unmatched robustness on the\nStrata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from\nrefusal-first to guidance-first safety, CSA redefines the model-user\nrelationship, aiming for systems that are not just safe, but meaningfully\nhelpful. We release Oy1, code, and the benchmark to support responsible,\nuser-centered AI."}
{"id": "2509.02100", "pdf": "https://arxiv.org/pdf/2509.02100.pdf", "abs": "https://arxiv.org/abs/2509.02100", "title": "E-THER: A Multimodal Dataset for Empathic AI - Towards Emotional Mismatch Awareness", "authors": ["Sharjeel Tahir", "Judith Johnson", "Jumana Abu-Khalaf", "Syed Afaq Ali Shah"], "categories": ["cs.HC", "cs.CL"], "comment": "15 pages, 4 figures. Preprint", "summary": "A prevalent shortfall among current empathic AI systems is their inability to\nrecognize when verbal expressions may not fully reflect underlying emotional\nstates. This is because the existing datasets, used for the training of these\nsystems, focus on surface-level emotion recognition without addressing the\ncomplex verbal-visual incongruence (mismatch) patterns useful for empathic\nunderstanding. In this paper, we present E-THER, the first Person-Centered\nTherapy-grounded multimodal dataset with multidimensional annotations for\nverbal-visual incongruence detection, enabling training of AI systems that\ndevelop genuine rather than performative empathic capabilities. The annotations\nincluded in the dataset are drawn from humanistic approach, i.e., identifying\nverbal-visual emotional misalignment in client-counsellor interactions -\nforming a framework for training and evaluating AI on empathy tasks. Additional\nengagement scores provide behavioral annotations for research applications.\nNotable gains in empathic and therapeutic conversational qualities are observed\nin state-of-the-art vision-language models (VLMs), such as IDEFICS and\nVideoLLAVA, using evaluation metrics grounded in empathic and therapeutic\nprinciples. Empirical findings indicate that our incongruence-trained models\noutperform general-purpose models in critical traits, such as sustaining\ntherapeutic engagement, minimizing artificial or exaggerated linguistic\npatterns, and maintaining fidelity to PCT theoretical framework."}
{"id": "2509.03646", "pdf": "https://arxiv.org/pdf/2509.03646.pdf", "abs": "https://arxiv.org/abs/2509.03646", "title": "Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning", "authors": ["Haozhe Wang", "Qixin Xu", "Che Liu", "Junhong Wu", "Fangzhen Lin", "Wenhu Chen"], "categories": ["cs.AI", "cs.CL"], "comment": "Preprint", "summary": "Reinforcement Learning (RL) has proven highly effective at enhancing the\ncomplex reasoning abilities of Large Language Models (LLMs), yet underlying\nmechanisms driving this success remain largely opaque. Our analysis reveals\nthat puzzling phenomena like ``aha moments\", ``length-scaling'' and entropy\ndynamics are not disparate occurrences but hallmarks of an emergent reasoning\nhierarchy, akin to the separation of high-level strategic planning from\nlow-level procedural execution in human cognition. We uncover a compelling\ntwo-phase dynamic: initially, a model is constrained by procedural correctness\nand must improve its low-level skills. The learning bottleneck then decisively\nshifts, with performance gains being driven by the exploration and mastery of\nhigh-level strategic planning. This insight exposes a core inefficiency in\nprevailing RL algorithms like GRPO, which apply optimization pressure\nagnostically and dilute the learning signal across all tokens. To address this,\nwe propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that\nconcentrates optimization efforts on high-impact planning tokens. HICRA\nsignificantly outperforms strong baselines, demonstrating that focusing on this\nstrategic bottleneck is key to unlocking advanced reasoning. Furthermore, we\nvalidate semantic entropy as a superior compass for measuring strategic\nexploration over misleading metrics such as token-level entropy."}
{"id": "2509.04439", "pdf": "https://arxiv.org/pdf/2509.04439.pdf", "abs": "https://arxiv.org/abs/2509.04439", "title": "ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory", "authors": ["Matthew Ho", "Chen Si", "Zhaoxiang Feng", "Fangxu Yu", "Yichi Yang", "Zhijian Liu", "Zhiting Hu", "Lianhui Qin"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "While inference-time scaling enables LLMs to carry out increasingly long and\ncapable reasoning traces, the patterns and insights uncovered during these\ntraces are immediately discarded once the context window is reset for a new\nquery. External memory is a natural way to persist these discoveries, and\nrecent work has shown clear benefits for reasoning-intensive tasks. We see an\nopportunity to make such memories more broadly reusable and scalable by moving\nbeyond instance-based memory entries (e.g. exact query/response pairs, or\nsummaries tightly coupled with the original problem context) toward\nconcept-level memory: reusable, modular abstractions distilled from solution\ntraces and stored in natural language. For future queries, relevant concepts\nare selectively retrieved and integrated into the prompt, enabling test-time\ncontinual learning without weight updates. Our design introduces new strategies\nfor abstracting takeaways from rollouts and retrieving entries for new queries,\npromoting reuse and allowing memory to expand with additional experiences. We\nevaluate on ARC-AGI, a benchmark that stresses compositional generalization and\nabstract reasoning, making it a natural fit for concept memory. Our method\nyields a 7.5% relative gain over a strong no-memory baseline with performance\ncontinuing to scale with inference compute. We find abstract concepts to be the\nmost consistent memory design, outscoring the baseline at all tested inference\ncompute scales. Moreover, dynamically updating memory during test-time\noutperforms fixed settings, supporting the hypothesis that accumulating and\nabstracting patterns enables further solutions in a form of self-improvement.\nCode is available at https://github.com/matt-seb-ho/arc_memo."}
{"id": "2509.05007", "pdf": "https://arxiv.org/pdf/2509.05007.pdf", "abs": "https://arxiv.org/abs/2509.05007", "title": "Sticker-TTS: Learn to Utilize Historical Experience with a Sticker-driven Test-Time Scaling Framework", "authors": ["Jie Chen", "Jinhao Jiang", "Yingqian Min", "Zican Dong", "Shijie Wang", "Wayne Xin Zhao", "Ji-Rong Wen"], "categories": ["cs.AI", "cs.CL", "I.2.7"], "comment": "11 pages, 1 figures, 5 tables", "summary": "Large reasoning models (LRMs) have exhibited strong performance on complex\nreasoning tasks, with further gains achievable through increased computational\nbudgets at inference. However, current test-time scaling methods predominantly\nrely on redundant sampling, ignoring the historical experience utilization,\nthereby limiting computational efficiency. To overcome this limitation, we\npropose Sticker-TTS, a novel test-time scaling framework that coordinates three\ncollaborative LRMs to iteratively explore and refine solutions guided by\nhistorical attempts. At the core of our framework are distilled key\nconditions-termed stickers-which drive the extraction, refinement, and reuse of\ncritical information across multiple rounds of reasoning. To further enhance\nthe efficiency and performance of our framework, we introduce a two-stage\noptimization strategy that combines imitation learning with self-improvement,\nenabling progressive refinement. Extensive evaluations on three challenging\nmathematical reasoning benchmarks, including AIME-24, AIME-25, and OlymMATH,\ndemonstrate that Sticker-TTS consistently surpasses strong baselines, including\nself-consistency and advanced reinforcement learning approaches, under\ncomparable inference budgets. These results highlight the effectiveness of\nsticker-guided historical experience utilization. Our code and data are\navailable at https://github.com/RUCAIBox/Sticker-TTS."}
