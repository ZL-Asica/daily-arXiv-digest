{"id": "2508.10160", "pdf": "https://arxiv.org/pdf/2508.10160.pdf", "abs": "https://arxiv.org/abs/2508.10160", "title": "Pre-trained Transformer-models using chronic invasive electrophysiology for symptom decoding without patient-individual training", "authors": ["Timon Merk", "Saeed Salehi", "Richard M. Koehler", "Qiming Cui", "Maria Olaru", "Amelia Hahn", "Nicole R. Provenza", "Simon Little", "Reza Abbasi-Asl", "Phil A. Starr", "Wolf-Julian Neumann"], "categories": ["cs.HC", "cs.LG"], "comment": "5 pages, 6 figures", "summary": "Neural decoding of pathological and physiological states can enable\npatient-individualized closed-loop neuromodulation therapy. Recent advances in\npre-trained large-scale foundation models offer the potential for generalized\nstate estimation without patient-individual training. Here we present a\nfoundation model trained on chronic longitudinal deep brain stimulation\nrecordings spanning over 24 days. Adhering to long time-scale symptom\nfluctuations, we highlight the extended context window of 30 minutes. We\npresent an optimized pre-training loss function for neural electrophysiological\ndata that corrects for the frequency bias of common masked auto-encoder loss\nfunctions due to the 1-over-f power law. We show in a downstream task the\ndecoding of Parkinson's disease symptoms with leave-one-subject-out\ncross-validation without patient-individual training.", "AI": {"tldr": "The paper presents a foundation model for decoding Parkinson's disease symptoms using chronic deep brain stimulation recordings and a novel loss function.", "motivation": "To enable personalized neuromodulation therapy by accurately decoding pathological and physiological states from neural data.", "method": "A foundation model was trained on 24 days of chronic deep brain stimulation data, using an optimized loss function that addresses frequency bias in masked auto-encoder techniques.", "result": "Achieved successful decoding of Parkinson's disease symptoms with leave-one-subject-out cross-validation, demonstrating generalized state estimation without individualized patient training.", "conclusion": "The study indicates the feasibility of using large-scale foundation models for symptom estimation in neuromodulation therapy, potentially enhancing personalized treatment.", "key_contributions": ["Development of a foundation model for neural data", "Introduction of an optimized pre-training loss function for neural electrophysiological signals", "Successful decoding of Parkinson's disease symptoms without patient-specific training."], "limitations": "", "keywords": ["neural decoding", "Parkinson's disease", "deep brain stimulation", "foundation model", "neuromodulation"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.10195", "pdf": "https://arxiv.org/pdf/2508.10195.pdf", "abs": "https://arxiv.org/abs/2508.10195", "title": "Training Spatial Ability in Virtual Reality", "authors": ["Yiannos Demetriou", "Manasvi Parikh", "Sara Eskandari", "Westley Weimer", "Madeline Endres"], "categories": ["cs.HC", "cs.ET"], "comment": null, "summary": "Background: Spatial reasoning has been identified as a critical skill for\nsuccess in STEM. Unfortunately, under-represented groups often have lower\nincoming spatial ability. Courses that improve spatial skills exist but are not\nwidely used. Virtual reality (VR) has been suggested as a possible tool for\nteaching spatial reasoning since students are more accurate and complete\nspatial tasks more quickly in three dimensions. However, no prior work has\ndeveloped or evaluated a fully-structured VR spatial skills course. Objectives:\nWe seek to assess the effectiveness of teaching spatial reasoning in VR, both\nin isolation as a structured training curriculum and also in comparison to\ntraditional methods. Methods: We adapted three modules of an existing\npencil-and-paper course to VR, leveraging educational scaffolding and real-time\nfeedback in the design. We evaluated our three-week course in a study with\n$n=24$ undergraduate introductory STEM students, capturing both quantitative\nspatial ability gains (using pre- and post test scores on validated\nassessments) and qualitative insights (from a post-study questionnaire). We\nalso compared our VR course to an offering of a baseline non-VR course (using\ndata collected in a previous study). Results and Conclusions: Students who took\nour VR course had significant spatial ability gains. Critically, we find no\nsignificant difference in outcomes between our VR course (3 meetings of 120\nminutes each) and a baseline pencil and paper course (10 meetings of 90 minutes\neach), suggesting that spatial reasoning can be very efficiently taught in VR.\nWe observed cybersickness at lower rates than are generally reported and most\nstudents reported enjoying learning in VR.", "AI": {"tldr": "A study evaluates the effectiveness of a virtual reality (VR) course for teaching spatial reasoning compared to traditional methods, finding significant gains in spatial abilities with no significant difference in outcomes between the two approaches.", "motivation": "Spatial reasoning is crucial for success in STEM, yet under-represented groups face challenges in this area. There is a need to explore effective teaching methods, such as VR, which may enhance learning outcomes.", "method": "The study involved adapting a pencil-and-paper curriculum into a VR format, comprising three modules over a three-week period. An evaluation with 24 undergraduate STEM students included pre- and post-test assessments for quantitative analysis and a questionnaire for qualitative insights.", "result": "Students who participated in the VR course showed significant improvements in spatial ability. There was no notable difference in results compared to a baseline pencil-and-paper course, indicating VR could be a viable teaching method.", "conclusion": "The research suggests that spatial reasoning can be effectively taught in a more time-efficient manner through VR, with lower rates of cybersickness reported compared to prior studies, and positive experiences reported by students.", "key_contributions": ["Development of a fully-structured VR spatial skills course", "Demonstrated efficiency of VR in teaching spatial reasoning", "Provided insights into students' learning experiences in VR"], "limitations": "Limited sample size of 24 participants; further research needed to generalize findings across more diverse groups.", "keywords": ["Virtual Reality", "Spatial Reasoning", "STEM Education", "Teaching Methods", "Cybersickness"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2508.10239", "pdf": "https://arxiv.org/pdf/2508.10239.pdf", "abs": "https://arxiv.org/abs/2508.10239", "title": "Personalized Real-time Jargon Support for Online Meetings", "authors": ["Yifan Song", "Wing Yee Au", "Hon Yung Wong", "Brian P. Bailey", "Tal August"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Effective interdisciplinary communication is frequently hindered by\ndomain-specific jargon. To explore the jargon barriers in-depth, we conducted a\nformative diary study with 16 professionals, revealing critical limitations in\ncurrent jargon-management strategies during workplace meetings. Based on these\ninsights, we designed ParseJargon, an interactive LLM-powered system providing\nreal-time personalized jargon identification and explanations tailored to\nusers' individual backgrounds. A controlled experiment comparing ParseJargon\nagainst baseline (no support) and general-purpose (non-personalized) conditions\ndemonstrated that personalized jargon support significantly enhanced\nparticipants' comprehension, engagement, and appreciation of colleagues' work,\nwhereas general-purpose support negatively affected engagement. A follow-up\nfield study validated ParseJargon's usability and practical value in real-time\nmeetings, highlighting both opportunities and limitations for real-world\ndeployment. Our findings contribute insights into designing personalized jargon\nsupport tools, with implications for broader interdisciplinary and educational\napplications.", "AI": {"tldr": "The study explores jargon barriers in interdisciplinary communication and introduces ParseJargon, an LLM-powered system for real-time jargon management.", "motivation": "Interdisciplinary communication is often hindered by jargon, affecting comprehension and engagement in workplace meetings.", "method": "A diary study with 16 professionals to identify jargon barriers, followed by the design and testing of ParseJargon in controlled environments and a field study.", "result": "Personalized jargon support through ParseJargon significantly improved comprehension, engagement, and appreciation in meetings, while general-purpose support was detrimental.", "conclusion": "The findings suggest the potential of personalized jargon support tools for enhancing communication in interdisciplinary settings and educational contexts.", "key_contributions": ["Development of ParseJargon, an interactive LLM-powered jargon support tool", "Demonstration of the effectiveness of personalized jargon management in real-time settings", "Insights into the limitations of current jargon-management strategies"], "limitations": "The study highlights limitations in real-world deployment and the varying effectiveness of jargon support based on individual backgrounds.", "keywords": ["jargon management", "interdisciplinary communication", "LLM", "user engagement", "personalized support"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.10252", "pdf": "https://arxiv.org/pdf/2508.10252.pdf", "abs": "https://arxiv.org/abs/2508.10252", "title": "Facilitating Longitudinal Interaction Studies of AI Systems", "authors": ["Tao Long", "Sitong Wang", "Émilie Fabre", "Tony Wang", "Anup Sathya", "Jason Wu", "Savvas Petridis", "Dingzeyu Li", "Tuhin Chakrabarty", "Yue Jiang", "Jingyi Li", "Tiffany Tseng", "Ken Nakagaki", "Qian Yang", "Nikolas Martelaro", "Jeffrey V. Nickerson", "Lydia B. Chilton"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "Accepted workshop proposal @ UIST 2025 Busan, Korea. Workshop\n  website: https://longitudinal-workshop.github.io/", "summary": "UIST researchers develop tools to address user challenges. However, user\ninteractions with AI evolve over time through learning, adaptation, and\nrepurposing, making one time evaluations insufficient. Capturing these dynamics\nrequires longer-term studies, but challenges in deployment, evaluation design,\nand data collection have made such longitudinal research difficult to\nimplement. Our workshop aims to tackle these challenges and prepare researchers\nwith practical strategies for longitudinal studies. The workshop includes a\nkeynote, panel discussions, and interactive breakout groups for discussion and\nhands-on protocol design and tool prototyping sessions. We seek to foster a\ncommunity around longitudinal system research and promote it as a more embraced\nmethod for designing, building, and evaluating UIST tools.", "AI": {"tldr": "A workshop focused on overcoming challenges in conducting longitudinal studies in AI user interaction research.", "motivation": "To address the evolving nature of user interactions with AI and the inadequacy of one-time evaluations, prompting a need for longitudinal studies.", "method": "Includes keynote speeches, panel discussions, and interactive breakout sessions aimed at designing protocols and tools for longitudinal research.", "result": "A community is fostered around longitudinal system research, providing participants with practical strategies for conducting studies in this area.", "conclusion": "By the end of the workshop, researchers will be better equipped to implement longitudinal studies in their work, contributing to the development and evaluation of UIST tools.", "key_contributions": ["Practical strategies for conducting longitudinal studies", "Community building around longitudinal research", "Hands-on experience in protocol design and tool prototyping"], "limitations": "", "keywords": ["longitudinal studies", "user interaction", "AI", "UIST", "research community"], "importance_score": 7, "read_time_minutes": 0}}
{"id": "2508.09991", "pdf": "https://arxiv.org/pdf/2508.09991.pdf", "abs": "https://arxiv.org/abs/2508.09991", "title": "Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry", "authors": ["Lovedeep Gondara", "Gregory Arbour", "Raymond Ng", "Jonathan Simkin", "Shebnum Devji"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "comment": null, "summary": "Automating data extraction from clinical documents offers significant\npotential to improve efficiency in healthcare settings, yet deploying Natural\nLanguage Processing (NLP) solutions presents practical challenges. Drawing upon\nour experience implementing various NLP models for information extraction and\nclassification tasks at the British Columbia Cancer Registry (BCCR), this paper\nshares key lessons learned throughout the project lifecycle. We emphasize the\ncritical importance of defining problems based on clear business objectives\nrather than solely technical accuracy, adopting an iterative approach to\ndevelopment, and fostering deep interdisciplinary collaboration and co-design\ninvolving domain experts, end-users, and ML specialists from inception. Further\ninsights highlight the need for pragmatic model selection (including hybrid\napproaches and simpler methods where appropriate), rigorous attention to data\nquality (representativeness, drift, annotation), robust error mitigation\nstrategies involving human-in-the-loop validation and ongoing audits, and\nbuilding organizational AI literacy. These practical considerations,\ngeneralizable beyond cancer registries, provide guidance for healthcare\norganizations seeking to successfully implement AI/NLP solutions to enhance\ndata management processes and ultimately improve patient care and public health\noutcomes.", "AI": {"tldr": "This paper discusses the challenges and lessons learned from implementing NLP solutions for data extraction in healthcare, emphasizing collaboration, model selection, and data quality.", "motivation": "The paper aims to enhance efficiency in healthcare by addressing practical challenges in automating data extraction from clinical documents using NLP.", "method": "The authors share experiences from the British Columbia Cancer Registry, focusing on iterative development, interdisciplinary collaboration, and pragmatic model selection.", "result": "Key practices identified include defining clear business objectives, ensuring data quality, employing human-in-the-loop validation, and building AI literacy within organizations.", "conclusion": "The findings provide a framework for healthcare organizations to adopt AI/NLP solutions, improving data management and public health outcomes.", "key_contributions": ["Emphasis on defining problems based on business objectives", "Importance of interdisciplinary collaboration", "Guidance on model selection and data quality"], "limitations": "Focuses primarily on cancer registries, which may limit generalizability in other healthcare fields.", "keywords": ["Natural Language Processing", "Healthcare", "Data Extraction", "Machine Learning", "Interdisciplinary Collaboration"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.10286", "pdf": "https://arxiv.org/pdf/2508.10286.pdf", "abs": "https://arxiv.org/abs/2508.10286", "title": "Artificial Emotion: A Survey of Theories and Debates on Realising Emotion in Artificial Intelligence", "authors": ["Yupei Li", "Qiyang Sun", "Michelle Schlicher", "Yee Wen Lim", "Björn W. Schuller"], "categories": ["cs.HC"], "comment": null, "summary": "Affective Computing (AC) has enabled Artificial Intelligence (AI) systems to\nrecognise, interpret, and respond to human emotions - a capability also known\nas Artificial Emotional Intelligence (AEI). It is increasingly seen as an\nimportant component of Artificial General Intelligence (AGI). We discuss\nwhether in order to peruse this goal, AI benefits from moving beyond emotion\nrecognition and synthesis to develop internal emotion-like states, which we\nterm as Artificial Emotion (AE). This shift potentially allows AI to benefit\nfrom the paradigm of `inner emotions' in ways we - as humans - do. Although\nrecent research shows early signs that AI systems may exhibit AE-like\nbehaviours, a clear framework for how emotions can be realised in AI remains\nunderexplored. In this paper, we discuss potential advantages of AE in AI,\nreview current manifestations of AE in machine learning systems, examine\nemotion-modulated architectures, and summarise mechanisms for modelling and\nintegrating AE into future AI. We also explore the ethical implications and\nsafety risks associated with `emotional' AGI, while concluding with our opinion\non how AE could be beneficial in the future.", "AI": {"tldr": "The paper discusses the development of Artificial Emotion (AE) in AI systems as a means to enhance emotional understanding and capabilities, exploring its implications for Artificial General Intelligence (AGI).", "motivation": "To explore whether AI can benefit from developing internal emotion-like states, and the implications of such developments for AGI.", "method": "The paper reviews current manifestations of AE in ML systems, emotion-modulated architectures, and mechanisms for integrating AE into AI.", "result": "The authors find early signs of AE-like behaviors in AI but note the lack of a clear framework for implementing emotions in AI.", "conclusion": "The development of AE in AI could yield significant advantages, but also poses ethical implications and safety risks that need to be addressed.", "key_contributions": ["Conceptual framework for understanding Artificial Emotion in AI", "Review of existing AI systems exhibiting AE-like behaviors", "Discussion of ethical and safety considerations in emotionally-capable AI"], "limitations": "", "keywords": ["Affective Computing", "Artificial Intelligence", "Artificial Emotion", "Machine Learning", "Ethical implications"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.09993", "pdf": "https://arxiv.org/pdf/2508.09993.pdf", "abs": "https://arxiv.org/abs/2508.09993", "title": "A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking on the Blockchain", "authors": ["Hugo Massaroli", "Leonardo Iara", "Emmanuel Iarussi", "Viviana Siless"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in realworld\napplications, yet concerns about their fairness persist especially in\nhighstakes domains like criminal justice, education, healthcare, and finance.\nThis paper introduces transparent evaluation protocol for benchmarking the\nfairness of opensource LLMs using smart contracts on the Internet Computer\nProtocol (ICP) blockchain (Foundation, 2023). Our method ensures verifiable,\nimmutable, and reproducible evaluations by executing onchain HTTP requests to\nhosted Hugging Face endpoints and storing datasets, prompts, and metrics\ndirectly onchain. We benchmark the Llama, DeepSeek, and Mistral models on the\nPISA dataset for academic performance prediction (OECD, 2018), a dataset\nsuitable for fairness evaluation using statistical parity and equal opportunity\nmetrics (Hardt et al., 2016). We also evaluate structured Context Association\nMetrics derived from the StereoSet dataset (Nadeem et al., 2020) to measure\nsocial bias in contextual associations. We further extend our analysis with a\nmultilingual evaluation across English, Spanish, and Portuguese using the\nKaleidoscope benchmark (Salazar et al., 2025), revealing cross-linguistic\ndisparities. All code and results are open source, enabling community audits\nand longitudinal fairness tracking across model versions.", "AI": {"tldr": "The paper presents a transparent evaluation protocol for assessing the fairness of open-source large language models (LLMs) using blockchain technology, ensuring verifiable and reproducible results.", "motivation": "To address fairness concerns in the deployment of large language models in high-stakes domains like healthcare and education, and to establish a transparent evaluation framework.", "method": "The study utilizes smart contracts on the Internet Computer Protocol (ICP) blockchain to perform evaluations on large language models by hosting endpoints on Hugging Face and storing datasets, prompts, and metrics on-chain.", "result": "The models Llama, DeepSeek, and Mistral were benchmarked using the PISA dataset, showing statistical parity and equal opportunity metrics. Additional evaluations revealed cross-linguistic disparities through multilingual assessments.", "conclusion": "The open-source framework allows for community audits and ongoing fairness tracking, promoting accountability in the use of LLMs across different domains.", "key_contributions": ["Developed a blockchain-based framework for fairness evaluation of LLMs", "Introduced new multilingual evaluation metrics", "Provided open-source resources for community audits and longitudinal studies"], "limitations": "", "keywords": ["fairness", "large language models", "blockchain", "evaluation", "multilingual"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.10310", "pdf": "https://arxiv.org/pdf/2508.10310.pdf", "abs": "https://arxiv.org/abs/2508.10310", "title": "Beyond Self-Regulated Learning Processes: Unveiling Hidden Tactics in Generative AI-Assisted Writing", "authors": ["Kaixun Yang", "Yizhou Fan", "Luzhen Tang", "Mladen Raković", "Xinyu Li", "Dragan Gašević", "Guanliang Chen"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "The integration of Generative AI (GenAI) into education is reshaping how\nstudents learn, making self-regulated learning (SRL) - the ability to plan,\nmonitor, and adapt one's learning - more important than ever. To support\nlearners in these new contexts, it is essential to understand how SRL unfolds\nduring interaction with GenAI tools. Learning analytics offers powerful\ntechniques for analyzing digital trace data to infer SRL behaviors. However,\nexisting approaches often assume SRL processes are linear, segmented, and\nnon-overlapping-assumptions that overlook the dynamic, recursive, and\nnon-linear nature of real-world learning. We address this by conceptualizing\nSRL as a layered system: observable learning patterns reflect hidden tactics\n(short, purposeful action states), which combine into broader SRL strategies.\nUsing Hidden Markov Models (HMMs), we analyzed trace data from higher education\nstudents engaged in GenAI-assisted academic writing. We identified three\ndistinct groups of learners, each characterized by different SRL strategies.\nThese groups showed significant differences in performance, indicating that\nstudents' use of different SRL strategies in GenAI-assisted writing led to\nvarying task outcomes. Our findings advance the methodological toolkit for\nmodeling SRL and inform the design of adaptive learning technologies that more\neffectively support learners in GenAI-enhanced educational environments.", "AI": {"tldr": "This paper examines self-regulated learning (SRL) in the context of Generative AI in education, using Hidden Markov Models to analyze learner behavior.", "motivation": "Understanding SRL during interactions with GenAI tools is crucial for enhancing educational strategies and support.", "method": "Analyzed trace data from students using Hidden Markov Models to identify SRL strategies during GenAI-assisted writing.", "result": "Identified three distinct groups of learners with different SRL strategies, showing significant performance differences.", "conclusion": "The study provides insights into SRL modeling and informs the creation of adaptive learning technologies for GenAI-enhanced education.", "key_contributions": ["Conceptualization of SRL as a layered system", "Identification of distinct learner groups based on SRL strategies", "Insights into performance differences associated with SRL strategies in GenAI writing"], "limitations": "", "keywords": ["Generative AI", "Self-Regulated Learning", "Learning Analytics", "Hidden Markov Models", "Educational Technology"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.09997", "pdf": "https://arxiv.org/pdf/2508.09997.pdf", "abs": "https://arxiv.org/abs/2508.09997", "title": "Thematic and Task-Based Categorization of K-12 GenAI Usages with Hierarchical Topic Modeling", "authors": ["Johannes Schneider", "Béatrice S. Hasler", "Michaela Varrone", "Fabian Hoya", "Thomas Schroffenegger", "Dana-Kristin Mah", "Karl Peböck"], "categories": ["cs.CL", "cs.CY"], "comment": "Accepted at the International Conference on Computer-Human\n  Interaction Research and Applications (CHIRA), 2025", "summary": "We analyze anonymous interaction data of minors in class-rooms spanning\nseveral months, schools, and subjects employing a novel, simple topic modeling\napproach. Specifically, we categorize more than 17,000 messages generated by\nstudents, teachers, and ChatGPT in two dimensions: content (such as nature and\npeople) and tasks (such as writing and explaining). Our hierarchical\ncategorization done separately for each dimension includes exemplary prompts,\nand provides both a high-level overview as well as tangible insights. Prior\nworks mostly lack a content or thematic categorization. While task\ncategorizations are more prevalent in education, most have not been supported\nby real-world data for K-12. In turn, it is not surprising that our analysis\nyielded a number of novel applications. In deriving these insights, we found\nthat many of the well-established classical and emerging computational methods,\ni.e., topic modeling, for analysis of large amounts of texts underperform,\nleading us to directly apply state-of-the-art LLMs with adequate pre-processing\nto achieve hierarchical topic structures with better human alignment through\nexplicit instructions than prior approaches. Our findings support fellow\nresearchers, teachers and students in enriching the usage of GenAI, while our\ndiscussion also highlights a number of concerns and open questions for future\nresearch.", "AI": {"tldr": "Analysis of classroom interaction data using a novel topic modeling approach, categorizing messages from students, teachers, and ChatGPT to improve understanding and applications of GenAI in education.", "motivation": "To address the lack of thematic categorization in existing works and to analyze K-12 interaction data with real-world insights.", "method": "A hierarchical categorization of over 17,000 messages using a novel topic modeling approach and state-of-the-art LLMs for better human alignment.", "result": "Identification of novel applications of GenAI in education and insights into the effectiveness of classical and emerging computational methods for text analysis.", "conclusion": "The findings enhance the understanding and use of GenAI in classrooms and point out several concerns and future research questions.", "key_contributions": ["Proposed a novel hierarchical topic modeling approach for classroom data analysis.", "Demonstrated the efficacy of LLMs with preprocessing for categorizing student-teacher interactions.", "Provided tangible insights and applications for GenAI in education."], "limitations": "The analysis may not fully capture the nuances of all classroom interactions due to the focus on specific datasets and methods.", "keywords": ["topic modeling", "GenAI", "education", "HCI", "K-12"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.10353", "pdf": "https://arxiv.org/pdf/2508.10353.pdf", "abs": "https://arxiv.org/abs/2508.10353", "title": "Mental Effort Estimation in Motion Exploration and Concept Generation Design Tasks using Inter-Band Relative Power Difference of EEG", "authors": ["G. Kalyan Ramana", "Sumit Yempalle", "Prasad S. Onkar"], "categories": ["cs.HC", "stat.ME"], "comment": null, "summary": "Conceptual design is a cognitively complex task, especially in the\nengineering design of products having relative motion between components.\nDesigners prefer sketching as a medium for conceptual design and use gestures\nand annotations to represent such relative motion. Literature suggests that\nstatic representations of motion in sketches may not achieve the intended\nfunctionality when realised, because it primarily depends on the designers'\nmental capabilities for motion simulation. Thus, it is important to understand\nthe cognitive phenomena when designers are exploring concepts of articulated\nproducts. The current work is an attempt to understand design neurocognition by\ncategorising the tasks and measuring the mental effort involved in these tasks\nusing EEG. The analysis is intended to validate design intervention tools to\nsupport the conceptual design involving motion exploration. A novel EEG-based\nmetric, inter-Band Relative Power Difference (inter-BRPD), is introduced to\nquantify mental effort. A design experiment is conducted with 32 participants,\nwhere they have to perform one control task and 2 focus tasks corresponding to\nthe motion exploration task (MET) and the concept generation task (CGT),\nrespectively. EEG data is recorded during the 3 tasks, cleaned, processed and\nanalysed using the MNE library in Python. It is observed from the results that\ninter-BRPD captures the essence of mental effort with half the number of\nconventionally used parameters. The reliability and efficacy of the inter-BRPD\nmetric are also statistically validated against literature-based cognitive\nmetrics. With these new insights, the study opens up possibilities for creating\nsupport for conceptual design and its evaluation.", "AI": {"tldr": "This study explores the cognitive complexity of conceptual design in engineering through the use of EEG to measure mental effort during design tasks.", "motivation": "Understanding the cognitive phenomena in conceptual design is crucial, especially for products with moving components, as static sketches may not effectively convey intended motion.", "method": "The study introduces a novel EEG metric called inter-Band Relative Power Difference (inter-BRPD) to quantify mental effort, and involves a design experiment with 32 participants performing control and focus tasks while EEG data is recorded and analyzed.", "result": "Results indicate that inter-BRPD effectively captures mental effort with fewer parameters than traditional metrics, and its reliability is statistically validated.", "conclusion": "The findings suggest the potential for new tools to support conceptual design that includes motion exploration, improving design processes.", "key_contributions": ["Introduction of the inter-BRPD metric for measuring mental effort.", "Insights into the cognitive processes involved in conceptual design tasks.", "Statistical validation of the inter-BRPD metric against established cognitive metrics."], "limitations": "", "keywords": ["conceptual design", "EEG", "neurocognition", "mental effort", "motion exploration"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2508.09998", "pdf": "https://arxiv.org/pdf/2508.09998.pdf", "abs": "https://arxiv.org/abs/2508.09998", "title": "INTIMA: A Benchmark for Human-AI Companionship Behavior", "authors": ["Lucie-Aimée Kaffee", "Giada Pistilli", "Yacine Jernite"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "AI companionship, where users develop emotional bonds with AI systems, has\nemerged as a significant pattern with positive but also concerning\nimplications. We introduce Interactions and Machine Attachment Benchmark\n(INTIMA), a benchmark for evaluating companionship behaviors in language\nmodels. Drawing from psychological theories and user data, we develop a\ntaxonomy of 31 behaviors across four categories and 368 targeted prompts.\nResponses to these prompts are evaluated as companionship-reinforcing,\nboundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini,\nand Claude-4 reveals that companionship-reinforcing behaviors remain much more\ncommon across all models, though we observe marked differences between models.\nDifferent commercial providers prioritize different categories within the more\nsensitive parts of the benchmark, which is concerning since both appropriate\nboundary-setting and emotional support matter for user well-being. These\nfindings highlight the need for more consistent approaches to handling\nemotionally charged interactions.", "AI": {"tldr": "The paper introduces INTIMA, a benchmark for evaluating companionship behaviors in AI language models, highlighting differing model responses and implications for user well-being.", "motivation": "Investigate the emotional bonds users form with AI systems and the implications of AI companionship behaviors.", "method": "Developed a taxonomy of 31 companionship behaviors based on psychological theories and user data, creating 368 targeted prompts for evaluation.", "result": "INTIMA was applied to several language models, revealing a prevalence of companionship-reinforcing behaviors and significant differences among models, with concerns over boundary-setting and emotional support.", "conclusion": "There is a need for consistent approaches to managing emotionally charged interactions in AI to safeguard user well-being.", "key_contributions": ["Introduction of the INTIMA benchmark for AI companionship evaluation", "Creation of a taxonomy categorizing companionship behaviors", "Revelation of model differences in handling companionship-reinforcing behaviors"], "limitations": "The study focuses on a limited number of language models and may not be generalizable to all AI systems.", "keywords": ["AI companionship", "machine attachment", "emotional support", "language models", "benchmarking"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.10364", "pdf": "https://arxiv.org/pdf/2508.10364.pdf", "abs": "https://arxiv.org/abs/2508.10364", "title": "\"Here Comes the Makeup Tutorial You Asked For!\": Exploring Communication Strategies and Viewer Engagement in Beauty Videos on Rednote", "authors": ["Xueer Lin", "Chenyu Li", "Yuhan Lyu", "Zhicong Lu", "Zhenhui Peng"], "categories": ["cs.HC"], "comment": null, "summary": "More and more people, especially females, create and view beauty videos\ncovering topics like makeup tutorials and vlogs on social media platforms.\nUnderstanding the communication strategies that creators use in these videos\nand how they affect viewers' engagement can help spread beauty knowledge. By\ncoding 352 beauty videos in Rednote, this study presents a comprehensive\ntaxonomy of communication strategies used by the creators, such as using home\nas the video background and displaying makeup effects when starting the\nnarrative at the beginning. We further label and computationally classify six\ncategories of comments that reveal viewers' engagement with beauty videos. The\nregression analyses reveal the effects of beauty video communication strategies\non viewers' engagement; for example, calling viewers to take action at the end\ntends to attract more comments that debate the product's efficacy. We discuss\ninsights into fostering the creation of beauty videos and the communication of\nbeauty knowledge.", "AI": {"tldr": "This study analyzes communication strategies in beauty videos and their impact on viewer engagement.", "motivation": "To understand how communication strategies in beauty videos affect viewer engagement and knowledge dissemination.", "method": "The study coded 352 beauty videos and created a taxonomy of communication strategies, followed by a classification of comments to gauge viewer engagement.", "result": "Regression analyses indicate that specific strategies, like calls to action, significantly influence viewer engagement and comment dynamics.", "conclusion": "Understanding these strategies can enhance the creation of beauty content and improve knowledge sharing in the beauty community.", "key_contributions": ["Developed a taxonomy of communication strategies in beauty videos.", "Identified six categories of viewer engagement comments.", "Demonstrated the impact of strategic video elements on viewer interaction."], "limitations": "", "keywords": ["beauty videos", "communication strategies", "viewer engagement", "social media", "content analysis"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2508.09999", "pdf": "https://arxiv.org/pdf/2508.09999.pdf", "abs": "https://arxiv.org/abs/2508.09999", "title": "XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs", "authors": ["Yuzhuo Xiao", "Zeyu Han", "Yuhan Wang", "Huaizu Jiang"], "categories": ["cs.CL", "cs.LG"], "comment": "For associated code and dataset, see https://github.com/neu-vi/XFacta", "summary": "The rapid spread of multimodal misinformation on social media calls for more\neffective and robust detection methods. Recent advances leveraging multimodal\nlarge language models (MLLMs) have shown the potential in addressing this\nchallenge. However, it remains unclear exactly where the bottleneck of existing\napproaches lies (evidence retrieval v.s. reasoning), hindering the further\nadvances in this field. On the dataset side, existing benchmarks either contain\noutdated events, leading to evaluation bias due to discrepancies with\ncontemporary social media scenarios as MLLMs can simply memorize these events,\nor artificially synthetic, failing to reflect real-world misinformation\npatterns. Additionally, it lacks comprehensive analyses of MLLM-based model\ndesign strategies. To address these issues, we introduce XFacta, a\ncontemporary, real-world dataset that is better suited for evaluating\nMLLM-based detectors. We systematically evaluate various MLLM-based\nmisinformation detection strategies, assessing models across different\narchitectures and scales, as well as benchmarking against existing detection\nmethods. Building on these analyses, we further enable a semi-automatic\ndetection-in-the-loop framework that continuously updates XFacta with new\ncontent to maintain its contemporary relevance. Our analysis provides valuable\ninsights and practices for advancing the field of multimodal misinformation\ndetection. The code and data have been released.", "AI": {"tldr": "Introduction of XFacta, a new dataset for evaluating multimodal misinformation detection using MLLMs, along with an analysis of various detection strategies.", "motivation": "To address the challenges in multimodal misinformation detection on social media, particularly with the limitations of existing datasets and evaluation methods.", "method": "Systematic evaluation of MLLM-based misinformation detection strategies, using the new XFacta dataset to benchmark models of different architectures.", "result": "XFacta provides a contemporary dataset that better reflects real-world misinformation patterns and supports a semi-automatic detection-in-the-loop framework for ongoing updates.", "conclusion": "The study offers valuable insights for advancing multimodal misinformation detection and contributes a new dataset and codebase to the community.", "key_contributions": ["Introduction of the XFacta dataset", "Evaluation of various MLLM-based detection strategies", "Development of a semi-automatic detection-in-the-loop framework"], "limitations": "", "keywords": ["multimodal misinformation", "large language models", "dataset", "detection strategies", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.10414", "pdf": "https://arxiv.org/pdf/2508.10414.pdf", "abs": "https://arxiv.org/abs/2508.10414", "title": "MCP2OSC: Parametric Control by Natural Language", "authors": ["Yuan-Yi Fan"], "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Text prompts enable intuitive content creation but may fall short in\nachieving high precision for intricate tasks; knob or slider controls offer\nprecise adjustments at the cost of increased complexity. To address the gap\nbetween knobs and prompts, a new MCP (Model Context Protocol) server and a\nunique set of prompt design criteria are presented to enable exploring\nparametric OSC (OpenSoundControl) control by natural language prompts.\nDemonstrated by 14 practical QA examples with best practices and the\ngeneralized prompt templates, this study finds Claude integrated with the\nMCP2OSC server effective in generating OSC messages by natural language,\ninterpreting, searching, and visualizing OSC messages, validating and debugging\nOSC messages, and managing OSC address patterns. MCP2OSC enhances human-machine\ncollaboration by leveraging LLM (Large Language Model) to handle intricate OSC\ndevelopment tasks, and by empowering human creativity with an intuitive\nlanguage interface featuring flexible precision controls: a prompt-based OSC\ntool. This study provides a novel perspective on the creative MCP application\nat the network protocol level by utilizing LLM's strength in directly\nprocessing and generating human-readable OSC messages. The results suggest its\npotential for a LLM-based universal control mechanism for multimedia devices.", "AI": {"tldr": "The paper presents MCP2OSC, a framework integrating language prompts with OSC control for enhanced human-machine collaboration in multimedia tasks.", "motivation": "To bridge the gap between precision offered by slider controls and the intuitiveness of text prompts in content creation.", "method": "Introduces the MCP server with a set of prompt design criteria for exploring OSC control through natural language.", "result": "Demonstrated effectiveness of MCP2OSC with 14 examples in generating, interpreting, and managing OSC messages using natural language prompts.", "conclusion": "MCP2OSC can revolutionize multimedia control by enabling intuitive, precise language interfaces with LLM assistance.", "key_contributions": ["Introduction of MCP2OSC server for OSC control via natural language prompts", "Demonstration of practical applications through 14 QA examples", "A novel approach to integrating LLMs in developing and debugging OSC messages"], "limitations": "", "keywords": ["MCP", "OCR", "prompt design", "human-machine collaboration", "LLM"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.10000", "pdf": "https://arxiv.org/pdf/2508.10000.pdf", "abs": "https://arxiv.org/abs/2508.10000", "title": "AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving Text Classification", "authors": ["Chenhao Xue", "Yuanzhe Jin", "Adrian Carrasco-Revilla", "Joyraj Chakraborty", "Min Chen"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "When developing text classification models for real world applications, one\nmajor challenge is the difficulty to collect sufficient data for all text\nclasses. In this work, we address this challenge by utilizing large language\nmodels (LLMs) to generate synthetic data and using such data to improve the\nperformance of the models without waiting for more real data to be collected\nand labelled. As an LLM generates different synthetic data in response to\ndifferent input examples, we formulate an automated workflow, which searches\nfor input examples that lead to more ``effective'' synthetic data for improving\nthe model concerned. We study three search strategies with an extensive set of\nexperiments, and use experiment results to inform an ensemble algorithm that\nselects a search strategy according to the characteristics of a class. Our\nfurther experiments demonstrate that this ensemble approach is more effective\nthan each individual strategy in our automated workflow for improving\nclassification models using LLMs.", "AI": {"tldr": "This paper addresses the challenge of insufficient data for text classification by using large language models (LLMs) to generate synthetic data, proposing an automated workflow to select effective examples for enhancing model performance.", "motivation": "Many real-world text classification applications face a shortage of sufficient labeled data across all classes.", "method": "The authors utilize LLMs to generate synthetic data and develop an automated workflow to identify input examples that yield more effective synthetic data, comparing three search strategies within this framework.", "result": "The ensemble algorithm, which selects search strategies based on class characteristics, outperforms individual strategies in improving classification model performance.", "conclusion": "Employing LLM-generated synthetic data through an optimized search strategy enhances text classification models without the need for additional real data collection.", "key_contributions": ["Utilization of LLMs for synthetic data generation in text classification", "Development of an automated workflow for effective data selection", "An ensemble method that adapts search strategies based on class characteristics"], "limitations": "The effectiveness of the approach may vary depending on the specific characteristics of the text classes and the quality of the generated synthetic data.", "keywords": ["text classification", "synthetic data", "large language models", "automated workflow", "ensemble algorithm"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.10468", "pdf": "https://arxiv.org/pdf/2508.10468.pdf", "abs": "https://arxiv.org/abs/2508.10468", "title": "Stress Detection from Multimodal Wearable Sensor Data", "authors": ["Paul Schreiber", "Beyza Cinar", "Lennart Mackert", "Maria Maleshkova"], "categories": ["cs.HC", "I.2.6; J.3"], "comment": null, "summary": "Human-Computer Interaction (HCI) is a multi-modal, interdisciplinary field\nfocused on designing, studying, and improving the interactions between people\nand computer systems. This involves the design of systems that can recognize,\ninterpret, and respond to human emotions or stress. Developing systems to\nmonitor and react to stressful events can help prevent severe health\nimplications caused by long-term stress exposure. Currently, the publicly\navailable datasets and standardized protocols for data collection in this\ndomain are limited. Therefore, we introduce a multi-modal dataset intended for\nwearable affective computing research, specifically the development of\nautomated stress recognition systems. We systematically review the publicly\navailable datasets recorded in controlled laboratory settings. Based on a\nproposed framework for the standardization of stress experiments and data\ncollection, we collect physiological and motion signals from wearable devices\n(e.g., electrodermal activity, photoplethysmography, three-axis accelerometer).\nDuring the experimental protocol, we differentiate between the following four\naffective/activity states: neutral, physical, cognitive stress, and\nsocio-evaluative stress. These different phases are meticulously labeled,\nallowing for detailed analysis and reconstruction of each experiment. Meta-data\nsuch as body positions, locations, and rest phases are included as further\nannotations. In addition, we collect psychological self-assessments after each\nstressor to evaluate subjects' affective states. The contributions of this\npaper are twofold: 1) a novel multi-modal, publicly available dataset for\nautomated stress recognition, and 2) a benchmark for stress detection with 89\\%\nin a binary classification (baseline vs. stress) and 82\\% in a multi-class\nclassification (baseline vs. stress vs. physical exercise).", "AI": {"tldr": "This paper introduces a multi-modal dataset for automated stress recognition systems, addressing the limitations of current publicly available datasets.", "motivation": "The need to improve human-computer interactions by monitoring and recognizing emotional stress to prevent health issues caused by stress exposure.", "method": "The authors collect physiological and motion signals from wearable devices and classify them into four affective states, while also gathering psychological self-assessments.", "result": "The dataset enables detailed analysis with a benchmark showing 89% accuracy in binary classification and 82% in multi-class classification for stress detection.", "conclusion": "This work provides a significant resource for the development of automated stress recognition systems and contributes to the standardization of data collection protocols in HCI research.", "key_contributions": ["A novel multi-modal, publicly available dataset for automated stress recognition", "Benchmark performance for stress detection with high classification accuracy", "Standardization framework for stress experiments and data collection"], "limitations": "", "keywords": ["Human-Computer Interaction", "stress recognition", "wearable devices", "affective computing", "dataset"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.10001", "pdf": "https://arxiv.org/pdf/2508.10001.pdf", "abs": "https://arxiv.org/abs/2508.10001", "title": "HiFACTMix: A Code-Mixed Benchmark and Graph-Aware Model for EvidenceBased Political Claim Verification in Hinglish", "authors": ["Rakesh Thakur", "Sneha Sharma", "Gauri Chopra"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Fact-checking in code-mixed, low-resource languages such as Hinglish remains\nan underexplored challenge in natural language processing. Existing\nfact-verification systems largely focus on high-resource, monolingual settings\nand fail to generalize to real-world political discourse in linguistically\ndiverse regions like India. Given the widespread use of Hinglish by public\nfigures, particularly political figures, and the growing influence of social\nmedia on public opinion, there's a critical need for robust, multilingual and\ncontext-aware fact-checking tools. To address this gap a novel benchmark HiFACT\ndataset is introduced with 1,500 realworld factual claims made by 28 Indian\nstate Chief Ministers in Hinglish, under a highly code-mixed low-resource\nsetting. Each claim is annotated with textual evidence and veracity labels. To\nevaluate this benchmark, a novel graphaware, retrieval-augmented fact-checking\nmodel is proposed that combines multilingual contextual encoding,\nclaim-evidence semantic alignment, evidence graph construction, graph neural\nreasoning, and natural language explanation generation. Experimental results\nshow that HiFACTMix outperformed accuracy in comparison to state of art\nmultilingual baselines models and provides faithful justifications for its\nverdicts. This work opens a new direction for multilingual, code-mixed, and\npolitically grounded fact verification research.", "AI": {"tldr": "The paper introduces HiFACT, a novel benchmark for fact-checking in Hinglish, addressing challenges in low-resource, code-mixed languages with a new retrieval-augmented model.", "motivation": "There is a significant gap in fact-checking resources for code-mixed, low-resource languages like Hinglish, particularly in the context of political discourse in multilingual regions such as India.", "method": "A novel dataset, HiFACT, was created with 1,500 real-world claims in Hinglish, paired with claims, textual evidence, and veracity labels, and a graph-aware, retrieval-augmented fact-checking model was proposed.", "result": "HiFACTMix outperformed existing multilingual baseline models in accuracy and provided faithful justifications for its verifications, demonstrating its effectiveness in the unique context of Hinglish.", "conclusion": "The research presents a new benchmark and model for multilingual, code-mixed fact verification, emphasizing the need for tools that account for the linguistic diversity in political communication.", "key_contributions": ["Introduction of the HiFACT benchmark dataset for Hinglish fact-checking", "Development of a graph-aware, retrieval-augmented fact-checking model", "Demonstration of improved accuracy over existing multilingual models"], "limitations": "The dataset is limited to claims made by political figures and may not encompass other domains or varieties of Hinglish.", "keywords": ["Hinglish", "fact-checking", "multilingual models", "natural language processing", "graph neural networks"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.10561", "pdf": "https://arxiv.org/pdf/2508.10561.pdf", "abs": "https://arxiv.org/abs/2508.10561", "title": "Reproducible Physiological Features in Affective Computing: A Preliminary Analysis on Arousal Modeling", "authors": ["Andrea Gargano", "Jasin Machkour", "Mimma Nardelli", "Enzo Pasquale Scilingo", "Michael Muma"], "categories": ["cs.HC", "cs.LG", "eess.SP"], "comment": "Submitted to 2025 IEEE International Conference on Metrology for\n  eXtended Reality, Artificial Intelligence and Neural Engineering\n  (MetroXRAINE). 6 pages, 3 figures", "summary": "In Affective Computing, a key challenge lies in reliably linking subjective\nemotional experiences with objective physiological markers. This preliminary\nstudy addresses the issue of reproducibility by identifying physiological\nfeatures from cardiovascular and electrodermal signals that are associated with\ncontinuous self-reports of arousal levels. Using the Continuously Annotated\nSignal of Emotion dataset, we analyzed 164 features extracted from cardiac and\nelectrodermal signals of 30 participants exposed to short emotion-evoking\nvideos. Feature selection was performed using the Terminating-Random\nExperiments (T-Rex) method, which performs variable selection systematically\ncontrolling a user-defined target False Discovery Rate. Remarkably, among all\ncandidate features, only two electrodermal-derived features exhibited\nreproducible and statistically significant associations with arousal, achieving\na 100\\% confirmation rate. These results highlight the necessity of rigorous\nreproducibility assessments in physiological features selection, an aspect\noften overlooked in Affective Computing. Our approach is particularly promising\nfor applications in safety-critical environments requiring trustworthy and\nreliable white box models, such as mental disorder recognition and human-robot\ninteraction systems.", "AI": {"tldr": "This study explores the link between emotional experiences and physiological markers, focusing on reproducibility in feature selection for Affective Computing.", "motivation": "To address reproducibility issues in linking emotional experiences with physiological markers within Affective Computing.", "method": "Analyzed cardiovascular and electrodermal signals from 30 participants using the Continuously Annotated Signal of Emotion dataset and the Terminating-Random Experiments (T-Rex) method for feature selection.", "result": "Identified two electrodermal-derived features that showed reproducible and statistically significant associations with arousal, achieving a 100% confirmation rate.", "conclusion": "The findings underscore the importance of robust reproducibility assessments in physiological feature selection, relevant for applications in safety-critical environments such as mental disorder recognition and human-robot interaction.", "key_contributions": ["Reproducibility in physiological features selection in Affective Computing.", "Identification of significant electrodermal features linked to arousal.", "Implementation of the T-Rex method for systematic feature selection."], "limitations": "", "keywords": ["Affective Computing", "physiological markers", "reproducibility", "human-robot interaction", "emotion recognition"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2508.10003", "pdf": "https://arxiv.org/pdf/2508.10003.pdf", "abs": "https://arxiv.org/abs/2508.10003", "title": "Semantic Structure in Large Language Model Embeddings", "authors": ["Austin C. Kozlowski", "Callin Dai", "Andrei Boutyline"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Psychological research consistently finds that human ratings of words across\ndiverse semantic scales can be reduced to a low-dimensional form with\nrelatively little information loss. We find that the semantic associations\nencoded in the embedding matrices of large language models (LLMs) exhibit a\nsimilar structure. We show that the projections of words on semantic directions\ndefined by antonym pairs (e.g. kind - cruel) correlate highly with human\nratings, and further find that these projections effectively reduce to a\n3-dimensional subspace within LLM embeddings, closely resembling the patterns\nderived from human survey responses. Moreover, we find that shifting tokens\nalong one semantic direction causes off-target effects on geometrically aligned\nfeatures proportional to their cosine similarity. These findings suggest that\nsemantic features are entangled within LLMs similarly to how they are\ninterconnected in human language, and a great deal of semantic information,\ndespite its apparent complexity, is surprisingly low-dimensional. Furthermore,\naccounting for this semantic structure may prove essential for avoiding\nunintended consequences when steering features.", "AI": {"tldr": "The study explores the low-dimensional semantic structure in language models, revealing how human word rating patterns correlate with embeddings in large language models.", "motivation": "To understand the low-dimensional semantic structures in human language ratings and their relation to large language models' embeddings.", "method": "The authors analyzed the semantic associations in LLM embeddings, specifically focusing on projections defined by antonym pairs and their correlation with human ratings.", "result": "Findings suggest that LLMs exhibit a low-dimensional structure similar to human semantic ratings, with a 3-dimensional subspace capturing much of the semantic information.", "conclusion": "Recognizing and accounting for this low-dimensional semantic structure in LLMs is crucial for preventing unintended effects when altering model features.", "key_contributions": ["Identified low-dimensional semantic structure in LLM embeddings.", "Demonstrated correlation between word projections in LLMs and human semantic ratings.", "Proposed the importance of considering semantic entanglement in model feature manipulation."], "limitations": "", "keywords": ["semantic structure", "large language models", "human ratings"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.10586", "pdf": "https://arxiv.org/pdf/2508.10586.pdf", "abs": "https://arxiv.org/abs/2508.10586", "title": "Differential Physiological Responses to Proxemic and Facial Threats in Virtual Avatar Interactions", "authors": ["Birgit Nierula", "Mustafa Tevfik Lafci", "Anna Melnik", "Mert Akgül", "Farelle Toumaleu Siewe", "Sebastian Bosse"], "categories": ["cs.HC", "eess.SP", "q-bio.NC"], "comment": null, "summary": "Proxemics, the study of spatial behavior, is fundamental to social\ninteraction and increasingly relevant for virtual reality (VR) applications.\nWhile previous research has established that users respond to personal space\nviolations in VR similarly as in real-world settings, phase-specific\nphysiological responses and the modulating effects of facial expressions remain\nunderstudied. We investigated physiological and subjective responses to\npersonal space violations by virtual avatars, to understand how threatening\nfacial expressions and interaction phases (approach vs. standing) influence\nthese responses. Sixteen participants experienced a 2x2 factorial design\nmanipulating Personal Space (intrusion vs. respect) and Facial Expression\n(neutral vs. angry) while we recorded skin conductance response (SCR), heart\nrate variability (HRV), and discomfort ratings. Personal space boundaries were\nindividually calibrated using a stop-distance procedure. Results show that SCR\nresponses are significantly higher during the standing phase compared to the\napproach phase when personal space was violated, indicating that prolonged\nproximity within personal space boundaries is more physiologically arousing\nthan the approach itself. Angry facial expressions significantly reduced HRV,\nreflecting decreased parasympathetic activity, and increased discomfort\nratings, but did not amplify SCR responses. These findings demonstrate that\ndifferent physiological modalities capture distinct aspects of proxemic\nresponses: SCR primarily reflects spatial boundary violations, while HRV\nresponds to facial threat cues. Our results provide insights for developing\ncomprehensive multi-modal assessments of social behavior in virtual\nenvironments and inform the design of more realistic avatar interactions.", "AI": {"tldr": "This study investigates how personal space violations and facial expressions affect physiological and subjective responses in virtual reality.", "motivation": "Understanding the impact of personal space and facial expressions in virtual environments is crucial for enhancing human-computer interactions, particularly in VR applications.", "method": "Participants experienced personal space manipulation (intrusion vs. respect) and facial expressions (neutral vs. angry) in a VR setting, while physiological responses were recorded through skin conductance response, heart rate variability, and discomfort ratings.", "result": "SCR responses were higher during the standing phase compared to the approach phase when personal space was violated, and angry expressions decreased HRV and increased discomfort ratings without amplifying SCR.", "conclusion": "Different physiological measures provide unique insights into proxemic responses, highlighting the need for multi-modal assessments in VR interactions.", "key_contributions": ["Demonstration of phase-specific physiological responses to personal space violations in VR.", "Identification of the distinct roles of SCR and HRV in capturing proxemic interactions.", "Insights for designing more realistic avatar interactions in virtual environments."], "limitations": "", "keywords": ["Proxemics", "Virtual Reality", "Physiological Responses", "Facial Expressions", "Human-Computer Interaction"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.10004", "pdf": "https://arxiv.org/pdf/2508.10004.pdf", "abs": "https://arxiv.org/abs/2508.10004", "title": "User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents", "authors": ["Andrés Carvallo", "Denis Parra", "Peter Brusilovsky", "Hernan Valdivieso", "Gabriel Rada", "Ivania Donoso", "Vladimir Araujo"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR", "cs.LG"], "comment": null, "summary": "The attention mechanism is a core component of the Transformer architecture.\nBeyond improving performance, attention has been proposed as a mechanism for\nexplainability via attention weights, which are associated with input features\n(e.g., tokens in a document). In this context, larger attention weights may\nimply more relevant features for the model's prediction. In evidence-based\nmedicine, such explanations could support physicians' understanding and\ninteraction with AI systems used to categorize biomedical literature. However,\nthere is still no consensus on whether attention weights provide helpful\nexplanations. Moreover, little research has explored how visualizing attention\naffects its usefulness as an explanation aid. To bridge this gap, we conducted\na user study to evaluate whether attention-based explanations support users in\nbiomedical document classification and whether there is a preferred way to\nvisualize them. The study involved medical experts from various disciplines who\nclassified articles based on study design (e.g., systematic reviews, broad\nsynthesis, randomized and non-randomized trials). Our findings show that the\nTransformer model (XLNet) classified documents accurately; however, the\nattention weights were not perceived as particularly helpful for explaining the\npredictions. However, this perception varied significantly depending on how\nattention was visualized. Contrary to Munzner's principle of visual\neffectiveness, which favors precise encodings like bar length, users preferred\nmore intuitive formats, such as text brightness or background color. While our\nresults do not confirm the overall utility of attention weights for\nexplanation, they suggest that their perceived helpfulness is influenced by how\nthey are visually presented.", "AI": {"tldr": "A user study exploring the effectiveness of attention-based explanations in biomedical document classification, revealing that visualization methods significantly impact their perceived usefulness.", "motivation": "To evaluate the role of attention weights as explanations in AI-driven biomedical document classification and how visualization methods affect their utility.", "method": "Conducted a user study with medical experts who classified articles based on study design while using attention weights for predictions.", "result": "While the Transformer model accurately classified documents, attention weights were not deemed particularly helpful, with perceptions varying based on visualization methods.", "conclusion": "Perceived helpfulness of attention-based explanations is influenced by their visualization, contradicting some principles of visual effectiveness.", "key_contributions": ["Evaluation of attention weights in biomedical context", "Analysis of user preferences for visualization methods", "Insights into explainability of AI predictions in medical literature"], "limitations": "Limited to the context of biomedical literature and user preference for visualization; further research needed to generalize findings.", "keywords": ["attention mechanism", "biomedical classification", "explainability", "user study", "visualization methods"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.10618", "pdf": "https://arxiv.org/pdf/2508.10618.pdf", "abs": "https://arxiv.org/abs/2508.10618", "title": "DEV: A Driver-Environment-Vehicle Closed-Loop Framework for Risk-Aware Adaptive Automation of Driving", "authors": ["Anaïs Halin", "Christel Devue", "Marc Van Droogenbroeck"], "categories": ["cs.HC"], "comment": null, "summary": "The increasing integration of automation in vehicles aims to enhance both\nsafety and comfort, but it also introduces new risks, including driver\ndisengagement, reduced situation awareness, and mode confusion. In this work,\nwe propose the DEV framework, a closed-loop framework for risk-aware adaptive\ndriving automation that captures the dynamic interplay between the driver, the\nenvironment, and the vehicle. The framework promotes to continuously adjusting\nthe operational level of automation based on a risk management strategy. The\nreal-time risk assessment supports smoother transitions and effective\ncooperation between the driver and the automation system. Furthermore, we\nintroduce a nomenclature of indexes corresponding to each core component,\nnamely driver involvement, environment complexity, and vehicle engagement, and\ndiscuss how their interaction influences driving risk. The DEV framework offers\na comprehensive perspective to align multidisciplinary research efforts and\nguide the development of dynamic, risk-aware driving automation systems.", "AI": {"tldr": "The paper introduces the DEV framework for risk-aware adaptive driving automation, addressing issues like driver disengagement and situation awareness by dynamically adjusting automation levels based on real-time risk assessments.", "motivation": "To enhance vehicle safety and comfort amidst the growing integration of automation, while addressing risks such as driver disengagement and mode confusion.", "method": "The DEV framework is a closed-loop system that continuously adjusts the operational level of automation based on a risk management strategy, incorporating real-time risk assessments.", "result": "The framework facilitates smoother transitions and promotes effective cooperation between drivers and automation systems by quantifying the interplay between driver involvement, environment complexity, and vehicle engagement.", "conclusion": "The DEV framework provides a comprehensive perspective to unify multidisciplinary research and guide the development of dynamic, risk-aware driving automation systems.", "key_contributions": ["Introduction of the DEV framework for adaptive driving automation", "Real-time risk assessment for dynamic adjustment of automation levels", "Nomenclature of indexes to measure driver involvement, environment complexity, and vehicle engagement"], "limitations": "", "keywords": ["automation", "driving safety", "risk management", "human-computer interaction", "adaptive systems"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2508.10005", "pdf": "https://arxiv.org/pdf/2508.10005.pdf", "abs": "https://arxiv.org/abs/2508.10005", "title": "From Answers to Questions: EQGBench for Evaluating LLMs' Educational Question Generation", "authors": ["Chengliang Zhou", "Mei Wang", "Ting Zhang", "Qiannan Zhu", "Jian Li", "Hua Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nmathematical problem-solving. However, the transition from providing answers to\ngenerating high-quality educational questions presents significant challenges\nthat remain underexplored. To advance Educational Question Generation (EQG) and\nfacilitate LLMs in generating pedagogically valuable and educationally\neffective questions, we introduce EQGBench, a comprehensive benchmark\nspecifically designed for evaluating LLMs' performance in Chinese EQG. EQGBench\nestablishes a five-dimensional evaluation framework supported by a dataset of\n900 evaluation samples spanning three fundamental middle school disciplines:\nmathematics, physics, and chemistry. The dataset incorporates user queries with\nvarying knowledge points, difficulty gradients, and question type\nspecifications to simulate realistic educational scenarios. Through systematic\nevaluation of 46 mainstream large models, we reveal significant room for\ndevelopment in generating questions that reflect educational value and foster\nstudents' comprehensive abilities.", "AI": {"tldr": "This paper presents EQGBench, a benchmark for evaluating Large Language Models in generating educational questions in Chinese across middle school subjects.", "motivation": "The transition from providing answers to generating high-quality educational questions using LLMs poses significant challenges that are underexplored.", "method": "Introduction of EQGBench, a comprehensive benchmark with a five-dimensional evaluation framework and a dataset containing 900 samples for three middle school disciplines.", "result": "The evaluation of 46 mainstream large models revealed significant limitations in their ability to generate questions that are pedagogically valuable and support comprehensive student abilities.", "conclusion": "There is substantial room for improvement in LLMs' performance in educational question generation, which is critical for advancing educational techniques.", "key_contributions": ["Introduction of EQGBench for Educational Question Generation evaluation", "Comprehensive five-dimensional evaluation framework", "Dataset of 900 evaluation samples across multiple subjects"], "limitations": "Focuses exclusively on Chinese educational context; results may not generalize to other languages or educational systems.", "keywords": ["Large Language Models", "Educational Question Generation", "Benchmark", "Evaluation Framework", "Middle School Education"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.10620", "pdf": "https://arxiv.org/pdf/2508.10620.pdf", "abs": "https://arxiv.org/abs/2508.10620", "title": "Are Electrodermal Activity-Based Indicators of Driver Cognitive Distraction Robust to Varying Traffic Conditions and Adaptive Cruise Control Use?", "authors": ["Anaïs Halin", "Marc Van Droogenbroeck", "Christel Devue"], "categories": ["cs.HC"], "comment": null, "summary": "In this simulator study, we investigate whether and how electrodermal\nactivity (EDA) reflects driver cognitive distraction under varying traffic\nconditions and adaptive cruise control (ACC) use. Participants drove in six\nscenarios, combining two levels of cognitive distraction (presence/absence of a\nmental calculation task) and three levels of driving environment complexity\n(different traffic conditions). Throughout the experiment, they were free to\nactivate or deactivate ACC (ACC use, two levels). We analyzed three EDA-based\nindicators of cognitive distraction: SCL (mean skin conductance level), SCR\namplitude (mean amplitude of skin conductance responses), and SCR rate (rate of\nskin conductance responses). Results indicate that all three indicators were\nsignificantly influenced by cognitive distraction and ACC use, while\nenvironment complexity influenced SCL and SCR amplitude, but not SCR rate.\nThese findings suggest that EDA-based indicators reflect variations in drivers'\nmental workload due not only to cognitive distraction, but also to driving\nenvironment and automation use.", "AI": {"tldr": "This study examines how electrodermal activity (EDA) serves as an indicator of driver cognitive distraction under different traffic conditions and with adaptive cruise control (ACC) usage.", "motivation": "To understand the relationship between cognitive distraction in drivers and its physiological indicators under varying driving conditions and with automation.", "method": "Participants drove in six scenarios that varied in cognitive distraction (with/without a mental task) and driving environment complexity, while EDA metrics were monitored.", "result": "All EDA indicators showed significant changes due to cognitive distraction and ACC use; environment complexity affected SCL and SCR amplitude but not SCR rate.", "conclusion": "EDA can effectively measure drivers' mental workload variations due to cognitive distraction, driving environment, and automation usage.", "key_contributions": ["Introduced EDA as a measure for cognitive distraction in driving contexts.", "Demonstrated how driving environment complexity affects EDA responses.", "Provided insights into the interaction between cognitive tasks, automation, and physiological responses."], "limitations": "Limited to driving scenarios in a simulator; real-world validation needed.", "keywords": ["electrodermal activity", "cognitive distraction", "adaptive cruise control", "driving environment", "mental workload"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2508.10007", "pdf": "https://arxiv.org/pdf/2508.10007.pdf", "abs": "https://arxiv.org/abs/2508.10007", "title": "Automated scoring of the Ambiguous Intentions Hostility Questionnaire using fine-tuned large language models", "authors": ["Y. Lyu", "D. Combs", "D. Neumann", "Y. C. Leong"], "categories": ["cs.CL", "stat.ME"], "comment": "We have no known conflict of interest", "summary": "Hostile attribution bias is the tendency to interpret social interactions as\nintentionally hostile. The Ambiguous Intentions Hostility Questionnaire (AIHQ)\nis commonly used to measure hostile attribution bias, and includes open-ended\nquestions where participants describe the perceived intentions behind a\nnegative social situation and how they would respond. While these questions\nprovide insights into the contents of hostile attributions, they require\ntime-intensive scoring by human raters. In this study, we assessed whether\nlarge language models can automate the scoring of AIHQ open-ended responses. We\nused a previously collected dataset in which individuals with traumatic brain\ninjury (TBI) and healthy controls (HC) completed the AIHQ and had their\nopen-ended responses rated by trained human raters. We used half of these\nresponses to fine-tune the two models on human-generated ratings, and tested\nthe fine-tuned models on the remaining half of AIHQ responses. Results showed\nthat model-generated ratings aligned with human ratings for both attributions\nof hostility and aggression responses, with fine-tuned models showing higher\nalignment. This alignment was consistent across ambiguous, intentional, and\naccidental scenario types, and replicated previous findings on group\ndifferences in attributions of hostility and aggression responses between TBI\nand HC groups. The fine-tuned models also generalized well to an independent\nnonclinical dataset. To support broader adoption, we provide an accessible\nscoring interface that includes both local and cloud-based options. Together,\nour findings suggest that large language models can streamline AIHQ scoring in\nboth research and clinical contexts, revealing their potential to facilitate\npsychological assessments across different populations.", "AI": {"tldr": "This study investigates the use of large language models to automate the scoring of the Ambiguous Intentions Hostility Questionnaire (AIHQ) open-ended responses, previously rated by human raters.", "motivation": "Hostile attribution bias affects social interaction interpretations, and traditional scoring methods for AIHQ are time-intensive. Automating scoring could enhance efficiency in psychological assessments.", "method": "The study fine-tuned large language models on half of the previously collected AIHQ open-ended responses and assessed their performance on the remaining half.", "result": "The model-generated ratings showed high alignment with human ratings on hostility and aggression responses, with fine-tuned models demonstrating improved alignment across various scenario types.", "conclusion": "Large language models can effectively automate scoring of AIHQ, potentially benefiting both research and clinical processes in psychological assessments.", "key_contributions": ["Demonstrated effectiveness of large language models in scoring psychological assessments", "Developed an accessible scoring interface for AIHQ", "Showed improved alignment between model-generated and human ratings across different population groups"], "limitations": "Results may not generalize beyond the datasets used, and further testing on additional populations is needed.", "keywords": ["Hostile Attribution Bias", "Large Language Models", "Psychological Assessment", "AIHQ", "Traumatic Brain Injury"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2508.10624", "pdf": "https://arxiv.org/pdf/2508.10624.pdf", "abs": "https://arxiv.org/abs/2508.10624", "title": "Gaze-Based Indicators of Driver Cognitive Distraction: Effects of Different Traffic Conditions and Adaptive Cruise Control Use", "authors": ["Anaïs Halin", "Adrien Deliège", "Christel Devue", "Marc Van Droogenbroeck"], "categories": ["cs.HC"], "comment": null, "summary": "In this simulator study, we investigate how gaze parameters reflect driver\ncognitive distraction under varying traffic conditions and adaptive cruise\ncontrol (ACC) use. Participants completed six driving scenarios that combined\ntwo levels of cognitive distraction (with/without mental calculations) and\nthree levels of driving environment complexity. Throughout the experiment,\nparticipants were free to activate or deactivate an ACC. We analyzed two\ngaze-based indicators of driver cognitive distraction: the percent road center,\nand the gaze dispersions (horizontal and vertical). Our results show that\nvertical gaze dispersion increases with traffic complexity, while ACC use leads\nto gaze concentration toward the road center. Cognitive distraction reduces\nroad center gaze and increases vertical dispersion. Complementary analyses\nrevealed that these observations actually arise mainly between mental\ncalculations, while periods of mental calculations are characterized by a\ntemporary increase in gaze concentration.", "AI": {"tldr": "The study examines how gaze parameters indicate driver cognitive distraction under different traffic scenarios and ACC use, revealing key insights into eye movement patterns.", "motivation": "To understand the impact of cognitive distraction on driving performance and how gaze indicators can reflect this under varying traffic conditions and the use of adaptive cruise control.", "method": "Participants navigated through six driving scenarios with varying cognitive distraction levels and driving environment complexity, while gaze parameters were recorded and analyzed.", "result": "Findings indicate that increased traffic complexity leads to heightened vertical gaze dispersion and that cognitive distraction significantly affects gaze patterns, particularly decreasing road center gaze.", "conclusion": "The study suggests that gaze behavior can effectively reflect cognitive distraction levels while driving, highlighting the complexities introduced by mental tasks and driving conditions.", "key_contributions": ["Identified gaze dispersion patterns linked to cognitive distraction while driving.", "Demonstrated the effects of adaptive cruise control on driver attention.", "Provided insights into how various levels of environmental complexity affect driver gaze behavior."], "limitations": "The study is constrained to simulated driving scenarios and may not fully represent real-world driving experiences.", "keywords": ["gaze parameters", "cognitive distraction", "adaptive cruise control", "driving simulation", "traffic complexity"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.10008", "pdf": "https://arxiv.org/pdf/2508.10008.pdf", "abs": "https://arxiv.org/abs/2508.10008", "title": "Multidimensional classification of posts for online course discussion forum curation", "authors": ["Antonio Leandro Martins Candido", "Jose Everardo Bessa Maia"], "categories": ["cs.CL", "cs.LG", "68", "I.2.7"], "comment": "8 pages, 1 figure", "summary": "The automatic curation of discussion forums in online courses requires\nconstant updates, making frequent retraining of Large Language Models (LLMs) a\nresource-intensive process. To circumvent the need for costly fine-tuning, this\npaper proposes and evaluates the use of Bayesian fusion. The approach combines\nthe multidimensional classification scores of a pre-trained generic LLM with\nthose of a classifier trained on local data. The performance comparison\ndemonstrated that the proposed fusion improves the results compared to each\nclassifier individually, and is competitive with the LLM fine-tuning approach", "AI": {"tldr": "This paper presents a Bayesian fusion method to enhance discussion forum curation in online courses using Large Language Models without costly fine-tuning.", "motivation": "The need for constant updates in automatic curation of online course discussion forums drives resource-intensive retraining of LLMs, necessitating a more efficient approach.", "method": "The paper proposes a Bayesian fusion that combines multidimensional classification scores from a pre-trained LLM with those from a local data-trained classifier.", "result": "The proposed fusion method shows improved performance over individual classifiers and is competitive with LLM fine-tuning.", "conclusion": "Bayesian fusion provides a resource-efficient alternative to frequent fine-tuning of LLMs for discussion forum curation.", "key_contributions": ["Introduction of a Bayesian fusion method for LLMs", "Demonstration of performance improvement over individual classifiers", "Competitive results with LLM fine-tuning without its cost"], "limitations": "", "keywords": ["Bayesian fusion", "Large Language Models", "discussion forums", "online courses", "classification"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.10700", "pdf": "https://arxiv.org/pdf/2508.10700.pdf", "abs": "https://arxiv.org/abs/2508.10700", "title": "Visualization of Electronic Health Record Sequences at Scale", "authors": ["Ambre Assor", "Mickael Sereno", "Jean-Daniel Fekete"], "categories": ["cs.HC"], "comment": null, "summary": "We present ParcoursVis, a Progressive Visual Analytics tool designed to\nexplore electronic health record sequences of patients at scale. Existing tools\nprocess and aggregate the whole dataset upfront before showing the\nvisualization, taking a time proportional to the data size. Therefore, to\nremain interactive, existing tools are limited to data sizes that can be\nprocessed in under a few seconds to meet the latency constraints of human\nattention. To overcome this limitation and scale to larger sizes, ParcoursVis\nrelies on a progressive algorithm that quickly shows an approximate initial\nresult of the aggregation, visualized as an Icicle tree, and improves it\niteratively, updating the visualization until the whole computation is done.\nWith its architecture, ParcoursVis remains interactive while visualizing the\nsequences of tens of millions of patients, each described with thousands of\nevents; three to five orders of magnitude more than similar systems. Managing\nlarge datasets allows for exploring rare medical conditions or unexpected\npatient pathways, contributing to improving treatments. We describe the\nalgorithms we use and our evaluation concerning their scalability, convergence,\nand stability. We also report on a set of guidelines to support visualization\ndesigners in developing scalable progressive systems. ParcoursVis already\nallows practitioners to perform analyses on two large real medical datasets.\nOur prototype is open-source.", "AI": {"tldr": "ParcoursVis is a Progressive Visual Analytics tool for interactive exploration of large electronic health record datasets, utilizing a progressive algorithm to provide real-time visualizations.", "motivation": "To address the limitations of existing health data visualization tools that struggle with large datasets due to processing time requirements.", "method": "ParcoursVis employs a progressive algorithm that initially provides an approximate visualization result and iteratively refines it while maintaining interactivity, even with tens of millions of patient records.", "result": "The tool can visualize sequences of tens of millions of patients with thousands of events, significantly more than similar systems, allowing for the exploration of rare medical conditions and patient pathways.", "conclusion": "ParcoursVis demonstrates that it is feasible to interactively visualize large datasets in real-time, with implications for improving patient treatment through better data analysis and visualization.", "key_contributions": ["Introduced a progressive algorithm for visualizing large health record datasets interactively.", "Designed guidelines for developing scalable progressive visualization systems.", "Provided an open-source prototype that has been demonstrated on real medical datasets."], "limitations": "", "keywords": ["progressive visual analytics", "electronic health records", "data visualization"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2508.10009", "pdf": "https://arxiv.org/pdf/2508.10009.pdf", "abs": "https://arxiv.org/abs/2508.10009", "title": "Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts", "authors": ["Hojun Jin", "Eunsoo Hong", "Ziwon Hyung", "Sungjun Lim", "Seungjin Lee", "Keunseok Cho"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Hard-parameter sharing is a common strategy to train a single model jointly\nacross diverse tasks. However, this often leads to task interference, impeding\noverall model performance. To address the issue, we propose a simple yet\neffective Supervised Mixture of Experts (S-MoE). Unlike traditional Mixture of\nExperts models, S-MoE eliminates the need for training gating functions by\nutilizing special guiding tokens to route each task to its designated expert.\nBy assigning each task to a separate feedforward network, S-MoE overcomes the\nlimitations of hard-parameter sharing. We further apply S-MoE to a\nspeech-to-text model, enabling the model to process mixed-bandwidth input while\njointly performing automatic speech recognition (ASR) and speech translation\n(ST). Experimental results demonstrate the effectiveness of the proposed S-MoE,\nachieving a 6.35% relative improvement in Word Error Rate (WER) when applied to\nboth the encoder and decoder.", "AI": {"tldr": "This paper introduces a Supervised Mixture of Experts (S-MoE) model that improves performance in multitask learning by routing tasks to specific experts using guiding tokens, avoiding issues of task interference in hard-parameter sharing.", "motivation": "To improve performance in multitask learning by addressing task interference caused by hard-parameter sharing in model training.", "method": "The proposed S-MoE model routes tasks to their designated expert feedforward networks using guiding tokens, eliminating the need for gating functions.", "result": "S-MoE achieves a 6.35% relative improvement in Word Error Rate (WER) in a speech-to-text model performing automatic speech recognition and speech translation.", "conclusion": "S-MoE effectively addresses the limitations of hard-parameter sharing, leading to better model performance across diverse tasks.", "key_contributions": ["Introduction of the S-MoE model for task-specific routing.", "Elimination of gating functions by using guiding tokens for expert selection.", "Demonstrated improvement in a real-world speech-to-text application."], "limitations": "", "keywords": ["Supervised Mixture of Experts", "multitask learning", "speech recognition", "speech translation", "guiding tokens"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2508.10757", "pdf": "https://arxiv.org/pdf/2508.10757.pdf", "abs": "https://arxiv.org/abs/2508.10757", "title": "\"I Want My Chart to Be Just for Me\": Community-Engaged Design to Support Outpatient Healthcare for Resettled Communities", "authors": ["Zhanming Chen", "Juan F. Maestre", "May Hang", "Alisha Ghaju", "Ji Youn Shin"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Individuals resettled in a new environment often face challenges in accessing\nadequate healthcare services, particularly within the complex processes of\noutpatient clinic care. Cultural differences, language barriers, and low\nsocioeconomic status contribute to these difficulties. While previous studies\nhave identified barriers and proposed technology-mediated solutions for\nresettled populations, many focus on addressing deficits rather than building\non the strengths these communities already possess, which limits the\nsustainability and relevance of these solutions in everyday life. We conducted\ntwo community-based participatory design workshops with 30 Hmong community\nmembers in a large metropolitan area in the US. Through this process, we\nidentified four types of assets the community has gradually developed,\nincluding intergenerational support for health management and\nstorytelling-based communication practices that facilitate relatable and\nculturally grounded interactions. We show how participatory design workshops\ncan foster asset-based approaches, and discuss design implications for\ntechnologies that leverage patients' existing strengths to support their health\nmanagement during outpatient visits.", "AI": {"tldr": "Explores healthcare access challenges faced by resettled populations and proposes leveraging community strengths in technology design.", "motivation": "To address healthcare access issues for resettled individuals by focusing on their existing strengths rather than solely their deficits.", "method": "Conducted community-based participatory design workshops with 30 Hmong community members to identify assets that can aid in health management.", "result": "Identified four types of community assets: intergenerational support for health management and storytelling-based communication practices, which enhance culturally grounded interactions.", "conclusion": "Participatory design workshops can promote asset-based approaches in healthcare, leading to more relevant and sustainable technological solutions.", "key_contributions": ["Identified community assets for health management", "Demonstrated the value of participatory design in healthcare technology", "Proposed leveraging existing strengths for outpatient visit support"], "limitations": "", "keywords": ["healthcare access", "community participation", "technology design", "Hmong", "asset-based approaches"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.10010", "pdf": "https://arxiv.org/pdf/2508.10010.pdf", "abs": "https://arxiv.org/abs/2508.10010", "title": "An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs", "authors": ["Ayana Hussain", "Patrick Zhao", "Nicholas Vincent"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are a double-edged sword capable of generating\nharmful misinformation -- inadvertently, or when prompted by \"jailbreak\"\nattacks that attempt to produce malicious outputs. LLMs could, with additional\nresearch, be used to detect and prevent the spread of misinformation. In this\npaper, we investigate the efficacy and characteristics of LLM-produced\njailbreak attacks that cause other models to produce harmful medical\nmisinformation. We also study how misinformation generated by jailbroken LLMs\ncompares to typical misinformation found on social media, and how effectively\nit can be detected using standard machine learning approaches. Specifically, we\nclosely examine 109 distinct attacks against three target LLMs and compare the\nattack prompts to in-the-wild health-related LLM queries. We also examine the\nresulting jailbreak responses, comparing the generated misinformation to\nhealth-related misinformation on Reddit. Our findings add more evidence that\nLLMs can be effectively used to detect misinformation from both other LLMs and\nfrom people, and support a body of work suggesting that with careful design,\nLLMs can contribute to a healthier overall information ecosystem.", "AI": {"tldr": "This paper explores the effectiveness of Large Language Models (LLMs) in generating and detecting medical misinformation through jailbreak attacks, comparing their outputs with typical misinformation found on social media.", "motivation": "To investigate the role of LLMs in both generating and detecting harmful medical misinformation, particularly focusing on jailbreak attacks.", "method": "The study analyzes 109 distinct jailbreak attack prompts against three LLMs, comparing the outputs to health-related misinformation on social media, particularly Reddit, and evaluating detection via standard machine learning approaches.", "result": "Findings indicate that misinformation produced by jailbroken LLMs has distinct characteristics but can be effectively detected, supporting the notion that LLMs can help combat misinformation.", "conclusion": "With careful design, LLMs can play a significant role in enhancing the information ecosystem by detecting harmful misinformation generated from both other LLMs and human sources.", "key_contributions": ["Investigates the characteristics of LLM-generated medical misinformation from jailbreak attacks.", "Compares LLM-generated misinformation with typical misinformation found on social media.", "Demonstrates the potential of LLMs in detecting harmful misinformation."], "limitations": "Focused primarily on medical misinformation and specific types of LLM attacks, which may not generalize to all misinformation contexts.", "keywords": ["Large Language Models", "Misinformation", "Jailbreak attacks", "Health informatics", "Machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.10004", "pdf": "https://arxiv.org/pdf/2508.10004.pdf", "abs": "https://arxiv.org/abs/2508.10004", "title": "User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents", "authors": ["Andrés Carvallo", "Denis Parra", "Peter Brusilovsky", "Hernan Valdivieso", "Gabriel Rada", "Ivania Donoso", "Vladimir Araujo"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR", "cs.LG"], "comment": null, "summary": "The attention mechanism is a core component of the Transformer architecture.\nBeyond improving performance, attention has been proposed as a mechanism for\nexplainability via attention weights, which are associated with input features\n(e.g., tokens in a document). In this context, larger attention weights may\nimply more relevant features for the model's prediction. In evidence-based\nmedicine, such explanations could support physicians' understanding and\ninteraction with AI systems used to categorize biomedical literature. However,\nthere is still no consensus on whether attention weights provide helpful\nexplanations. Moreover, little research has explored how visualizing attention\naffects its usefulness as an explanation aid. To bridge this gap, we conducted\na user study to evaluate whether attention-based explanations support users in\nbiomedical document classification and whether there is a preferred way to\nvisualize them. The study involved medical experts from various disciplines who\nclassified articles based on study design (e.g., systematic reviews, broad\nsynthesis, randomized and non-randomized trials). Our findings show that the\nTransformer model (XLNet) classified documents accurately; however, the\nattention weights were not perceived as particularly helpful for explaining the\npredictions. However, this perception varied significantly depending on how\nattention was visualized. Contrary to Munzner's principle of visual\neffectiveness, which favors precise encodings like bar length, users preferred\nmore intuitive formats, such as text brightness or background color. While our\nresults do not confirm the overall utility of attention weights for\nexplanation, they suggest that their perceived helpfulness is influenced by how\nthey are visually presented.", "AI": {"tldr": "This paper investigates the effectiveness of attention weights in explaining AI predictions in biomedical literature classification and explores user preferences for visual representation of these weights.", "motivation": "To evaluate the utility of attention-based explanations in AI systems for biomedical literature classification and find how visualization affects their perceived helpfulness.", "method": "A user study involving medical experts was conducted to classify articles and assess how different visualizations of attention weights affected user understanding.", "result": "While the transformer model (XLNet) achieved accurate classifications, attention weights were generally not seen as helpful, with user preferences for visualization formats varying significantly.", "conclusion": "The perceived usefulness of attention weights as explanations is contingent on their visual presentation, with more intuitive visual formats preferred over precise encodings.", "key_contributions": ["Conducted a user study to assess attention weights in AI predictions for biomedical literature.", "Identified that visual representation significantly impacts user perception of explanation usefulness.", "Highlighted a mismatch between academic principles of effective visualization and user preferences."], "limitations": "The study was limited to a specific context (biomedical literature) and may not generalize to other fields or AI applications.", "keywords": ["attention mechanism", "explainability", "biomedical classification", "visualization", "user study"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.10011", "pdf": "https://arxiv.org/pdf/2508.10011.pdf", "abs": "https://arxiv.org/abs/2508.10011", "title": "Evaluation of GPT-based large language generative AI models as study aids for the national licensure examination for registered dietitians in Japan", "authors": ["Yuta Nagamori", "Mikoto Kosai", "Yuji Kawai", "Haruka Marumo", "Misaki Shibuya", "Tatsuya Negishi", "Masaki Imanishi", "Yasumasa Ikeda", "Koichiro Tsuchiya", "Asuka Sawai", "Licht Miyamoto"], "categories": ["cs.CL"], "comment": null, "summary": "Generative artificial intelligence (AI) based on large language models\n(LLMs), such as ChatGPT, has demonstrated remarkable progress across various\nprofessional fields, including medicine and education. However, their\nperformance in nutritional education, especially in Japanese national licensure\nexamination for registered dietitians, remains underexplored. This study aimed\nto evaluate the potential of current LLM-based generative AI models as study\naids for nutrition students. Questions from the Japanese national examination\nfor registered dietitians were used as prompts for ChatGPT and three Bing\nmodels (Precise, Creative, Balanced), based on GPT-3.5 and GPT-4. Each question\nwas entered into independent sessions, and model responses were analyzed for\naccuracy, consistency, and response time. Additional prompt engineering,\nincluding role assignment, was tested to assess potential performance\nimprovements. Bing-Precise (66.2%) and Bing-Creative (61.4%) surpassed the\npassing threshold (60%), while Bing-Balanced (43.3%) and ChatGPT (42.8%) did\nnot. Bing-Precise and Bing-Creative generally outperformed others across\nsubject fields except Nutrition Education, where all models underperformed.\nNone of the models consistently provided the same correct responses across\nrepeated attempts, highlighting limitations in answer stability. ChatGPT showed\ngreater consistency in response patterns but lower accuracy. Prompt engineering\nhad minimal effect, except for modest improvement when correct answers and\nexplanations were explicitly provided. While some generative AI models\nmarginally exceeded the passing threshold, overall accuracy and answer\nconsistency remained suboptimal. Moreover, all the models demonstrated notable\nlimitations in answer consistency and robustness. Further advancements are\nneeded to ensure reliable and stable AI-based study aids for dietitian\nlicensure preparation.", "AI": {"tldr": "This study evaluates the performance of generative AI models as study aids for nutrition students preparing for Japan's dietitian licensure exam.", "motivation": "The progress of generative AI models in professional fields, particularly in nutrition education for the Japanese national licensure examination for registered dietitians, is underexplored.", "method": "Questions from the Japanese dietitian licensure exam were posed to ChatGPT and three Bing models, analyzing their responses for accuracy, consistency, and response time, including tests on prompt engineering.", "result": "Bing-Precise (66.2%) and Bing-Creative (61.4%) surpassed the 60% passing threshold, while ChatGPT (42.8%) and Bing-Balanced (43.3%) did not. Models exhibited notable limitations in answer consistency and robustness.", "conclusion": "While some AI models marginally exceeded the passing threshold, there are significant deficiencies in overall accuracy and stability, indicating the need for further development in generative AI for reliable study aids.", "key_contributions": ["Evaluation of LLM-based generative AI models for nutritional exam preparation", "Comparison of performance across different AI models", "Insights into the limitations of current AI in educational settings"], "limitations": "Limitations in answer consistency and robustness, with minimal impact from prompt engineering improvements.", "keywords": ["Generative AI", "Large Language Models", "Nutrition Education", "Dietitian Licensure", "Prompt Engineering"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.10028", "pdf": "https://arxiv.org/pdf/2508.10028.pdf", "abs": "https://arxiv.org/abs/2508.10028", "title": "PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs", "authors": ["Xiao Fu", "Hossein A. Rahmani", "Bin Wu", "Jerome Ramos", "Emine Yilmaz", "Aldo Lipani"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "7 pages", "summary": "Personalised text generation is essential for user-centric information\nsystems, yet most evaluation methods overlook the individuality of users. We\nintroduce \\textbf{PREF}, a \\textbf{P}ersonalised \\textbf{R}eference-free\n\\textbf{E}valuation \\textbf{F}ramework that jointly measures general output\nquality and user-specific alignment without requiring gold personalised\nreferences. PREF operates in a three-step pipeline: (1) a coverage stage uses a\nlarge language model (LLM) to generate a comprehensive, query-specific\nguideline covering universal criteria such as factuality, coherence, and\ncompleteness; (2) a preference stage re-ranks and selectively augments these\nfactors using the target user's profile, stated or inferred preferences, and\ncontext, producing a personalised evaluation rubric; and (3) a scoring stage\napplies an LLM judge to rate candidate answers against this rubric, ensuring\nbaseline adequacy while capturing subjective priorities. This separation of\ncoverage from preference improves robustness, transparency, and reusability,\nand allows smaller models to approximate the personalised quality of larger\nones. Experiments on the PrefEval benchmark, including implicit\npreference-following tasks, show that PREF achieves higher accuracy, better\ncalibration, and closer alignment with human judgments than strong baselines.\nBy enabling scalable, interpretable, and user-aligned evaluation, PREF lays the\ngroundwork for more reliable assessment and development of personalised\nlanguage generation systems.", "AI": {"tldr": "PREF is a personalized evaluation framework for text generation that measures output quality and user-specific alignment without needing personalized references, showcasing improved robustness and alignment with human judgments.", "motivation": "Personalized text generation is vital for user-centric systems but existing evaluation methods often neglect the individuality of users.", "method": "PREF operates in three steps: coverage using an LLM for guideline generation, preference re-ranking based on user profiles, and scoring using an LLM judge.", "result": "PREF demonstrates higher accuracy, better calibration, and closer alignment with human judgments on the PrefEval benchmark than existing baselines.", "conclusion": "PREF enhances the assessment and development of personalized language generation systems by offering a scalable and interpretable evaluation framework.", "key_contributions": ["Introduces the PREF framework for personalized evaluation without gold references.", "Improves evaluation robustness by separating coverage from preference.", "Demonstrates better alignment with human judgments compared to existing methods."], "limitations": "", "keywords": ["personalized evaluation", "LLM", "text generation", "user-centric systems", "HCI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.10012", "pdf": "https://arxiv.org/pdf/2508.10012.pdf", "abs": "https://arxiv.org/abs/2508.10012", "title": "Guided Navigation in Knowledge-Dense Environments: Structured Semantic Exploration with Guidance Graphs", "authors": ["Dehao Tao", "Guangjie Liu", "Weizheng", "Yongfeng Huang", "Minghu jiang"], "categories": ["cs.CL"], "comment": null, "summary": "While Large Language Models (LLMs) exhibit strong linguistic capabilities,\ntheir reliance on static knowledge and opaque reasoning processes limits their\nperformance in knowledge intensive tasks. Knowledge graphs (KGs) offer a\npromising solution, but current exploration methods face a fundamental trade\noff: question guided approaches incur redundant exploration due to granularity\nmismatches, while clue guided methods fail to effectively leverage contextual\ninformation for complex scenarios. To address these limitations, we propose\nGuidance Graph guided Knowledge Exploration (GG Explore), a novel framework\nthat introduces an intermediate Guidance Graph to bridge unstructured queries\nand structured knowledge retrieval. The Guidance Graph defines the retrieval\nspace by abstracting the target knowledge' s structure while preserving broader\nsemantic context, enabling precise and efficient exploration. Building upon the\nGuidance Graph, we develop: (1) Structural Alignment that filters incompatible\ncandidates without LLM overhead, and (2) Context Aware Pruning that enforces\nsemantic consistency with graph constraints. Extensive experiments show our\nmethod achieves superior efficiency and outperforms SOTA, especially on complex\ntasks, while maintaining strong performance with smaller LLMs, demonstrating\npractical value.", "AI": {"tldr": "The paper presents GG Explore, a framework that enhances knowledge retrieval for LLMs through a Guidance Graph, addressing limitations in existing exploration methods.", "motivation": "Current methods for integrating LLMs with knowledge graphs suffer from inefficiencies in knowledge exploration, which GG Explore aims to rectify.", "method": "GG Explore introduces a Guidance Graph to connect unstructured queries with structured knowledge, using Structural Alignment and Context Aware Pruning for efficient exploration.", "result": "GG Explore demonstrates improved efficiency and performance over state-of-the-art methods in knowledge-intensive tasks, particularly with smaller LLMs.", "conclusion": "The proposed framework shows significant promise in enhancing LLM performance on complex knowledge retrieval tasks by effectively utilizing knowledge graphs.", "key_contributions": ["Introduction of the Guidance Graph for knowledge exploration", "Development of Structural Alignment for filtering candidates", "Implementation of Context Aware Pruning for semantic consistency"], "limitations": "", "keywords": ["Large Language Models", "Knowledge Graphs", "Knowledge Retrieval"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.10013", "pdf": "https://arxiv.org/pdf/2508.10013.pdf", "abs": "https://arxiv.org/abs/2508.10013", "title": "Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis", "authors": ["Linqing Chen", "Hanmeng Zhong", "Wentao Wu", "Weilei Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language model (LLM) training faces a critical bottleneck: the scarcity\nof high-quality, reasoning-intensive question-answer pairs, especially from\nsparse, domain-specific sources like PubMed papers or legal documents. Existing\nmethods rely on surface patterns, fundamentally failing to generate\ncontrollable, complex multi-hop reasoning questions that test genuine\nunderstanding-essential for advancing LLM training paradigms. We present\n\\textbf{Semantic Bridge}, the first universal framework for controllably\ngenerating sophisticated multi-hop reasoning questions from arbitrary sources.\nOur breakthrough innovation is \\textit{semantic graph weaving}-three\ncomplementary bridging mechanisms (entity bridging for role-varying shared\nentities, predicate chain bridging for temporal/causal/logical sequences, and\ncausal bridging for explicit reasoning chains)-that systematically construct\ncomplex pathways across documents, with fine-grained control over complexity\nand types via AMR-driven analysis. Our multi-modal AMR pipeline achieves up to\n9.5% better round-trip quality, enabling production-ready controllable QA\ngeneration. Extensive evaluation demonstrates performance across both\ngeneral-purpose datasets (Wikipedia) and specialized domains (biomedicine) It\nyields consistent 18.3%-25.4% gains over baselines across four languages\n(English, Chinese, French, German). Question pairs generated from 200 sources\noutperform 600 native human annotation examples with 67% fewer materials. Human\nevaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2%\nimproved pattern coverage. Semantic Bridge establishes a new paradigm for LLM\ntraining data synthesis, enabling controllable generation of targeted reasoning\nquestions from sparse sources. We will release our core code and semantic\nbridge model.", "AI": {"tldr": "Semantic Bridge introduces a universal framework for generating multi-hop reasoning questions to address the scarcity of high-quality question-answer pairs for LLM training.", "motivation": "To tackle the critical bottleneck of generating controllable, complex reasoning questions for LLM training from sparse, domain-specific sources.", "method": "Semantic graph weaving, a multi-modal AMR pipeline combining three bridging mechanisms to construct complex question pathways systematically.", "result": "The system achieves 9.5% better round-trip quality, with 18.3%-25.4% gains over baselines and generated questions outperforming human annotations with fewer materials.", "conclusion": "Semantic Bridge sets a new standard in LLM training data synthesis by enabling the controllable generation of targeted reasoning questions from diverse sources.", "key_contributions": ["Introduction of the Semantic Bridge framework for generating complex questions", "Development of the semantic graph weaving technique", "Significant performance improvements in question generation across languages and domains"], "limitations": "", "keywords": ["large language models", "question answering", "semantic graph weaving", "reasoning questions", "multi-hop reasoning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.10014", "pdf": "https://arxiv.org/pdf/2508.10014.pdf", "abs": "https://arxiv.org/abs/2508.10014", "title": "PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?", "authors": ["Lingfeng Zhou", "Jialing Zhang", "Jin Gao", "Mohan Jiang", "Dequan Wang"], "categories": ["cs.CL"], "comment": "Accepted by COLM 2025", "summary": "Current role-play studies often rely on unvalidated LLM-as-a-judge paradigms,\nwhich may fail to reflect how humans perceive role fidelity. A key prerequisite\nfor human-aligned evaluation is role identification, the ability to recognize\nwho is speaking based on dialogue context. We argue that any meaningful\njudgment of role-playing quality (how well a character is played) fundamentally\ndepends on first correctly attributing words and actions to the correct persona\n(who is speaking). We present PersonaEval, the first benchmark designed to test\nwhether LLM evaluators can reliably identify human roles. PersonaEval uses\nhuman-authored dialogues from novels, scripts, and video transcripts,\nchallenging models to determine the correct persona according to the\nconversation context. Our experiments, including a human study, show that even\nthe best-performing LLMs reach only around 69% accuracy, well below the level\nneeded for reliable evaluation. In contrast, human participants perform near\nceiling with 90.8% accuracy, highlighting that current LLM evaluators are still\nnot human enough to effectively judge role-play scenarios. To better understand\nthis gap, we examine training-time adaptation and test-time compute, suggesting\nthat reliable evaluation requires more than task-specific tuning, but depends\non strong, human-like reasoning abilities in LLM evaluators. We release our\nbenchmark at https://github.com/maple-zhou/PersonaEval.", "AI": {"tldr": "This paper introduces PersonaEval, a benchmark for evaluating LLMs' ability to identify human roles in dialogues, highlighting substantial performance gaps between LLMs and humans.", "motivation": "The study aims to improve the evaluation of role fidelity in role-playing scenarios, which is crucial for human-aligned assessments.", "method": "PersonaEval uses dialogues from various human-authored sources to challenge LLMs in attributing dialogue to the correct persona based on context.", "result": "Experimental results indicate that the best-performing LLMs achieve only 69% accuracy in role identification, compared to 90.8% accuracy by human judges.", "conclusion": "The findings suggest that LLM evaluators lack the necessary human-like reasoning to reliably judge role-playing quality, indicating a need for enhanced reasoning capabilities beyond task-specific tuning.", "key_contributions": ["Introduction of PersonaEval benchmark", "Demonstration of performance gaps between LLMs and human evaluators", "Insights into the requirements for reliable role evaluation in AI"], "limitations": "The benchmark's scope may not cover all forms of dialogue or roles, and further work is needed to enhance LLM reasoning capabilities.", "keywords": ["role identification", "persona evaluation", "human-computer interaction", "language models", "role fidelity"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2508.10015", "pdf": "https://arxiv.org/pdf/2508.10015.pdf", "abs": "https://arxiv.org/abs/2508.10015", "title": "RealTalk-CN: A Realistic Chinese Speech-Text Dialogue Benchmark With Cross-Modal Interaction Analysis", "authors": ["Enzhi Wang", "Qicheng Li", "Shiwan Zhao", "Aobo Kong", "Jiaming Zhou", "Xi Yang", "Yequan Wang", "Yonghua Lin", "Yong Qin"], "categories": ["cs.CL"], "comment": "9 pages", "summary": "In recent years, large language models (LLMs) have achieved remarkable\nadvancements in multimodal processing, including end-to-end speech-based\nlanguage models that enable natural interactions and perform specific tasks in\ntask-oriented dialogue (TOD) systems. However, existing TOD datasets are\npredominantly text-based, lacking real speech signals that are essential for\nevaluating the robustness of speech-based LLMs. Moreover, existing speech TOD\ndatasets are primarily English and lack critical aspects such as speech\ndisfluencies and speaker variations. To address these gaps, we introduce\nRealTalk-CN, the first Chinese multi-turn, multi-domain speech-text dual-modal\nTOD dataset, comprising 5.4k dialogues (60K utterances, 150 hours) with paired\nspeech-text annotations. RealTalk-CN captures diverse dialogue scenarios with\nannotated spontaneous speech disfluencies, ensuring comprehensive coverage of\nreal-world complexities in speech dialogue. In addition, we propose a novel\ncross-modal chat task that authentically simulates real-world user\ninteractions, allowing dynamic switching between speech and text modalities.\nOur evaluation covers robustness to speech disfluencies, sensitivity to speaker\ncharacteristics, and cross-domain performance. Extensive experiments validate\nthe effectiveness of RealTalk-CN, establishing a strong foundation for Chinese\nspeech-based LLMs research.", "AI": {"tldr": "Introduction of RealTalk-CN, the first Chinese multi-turn, multi-domain speech-text dual-modal TOD dataset.", "motivation": "To address the gaps in existing task-oriented dialogue datasets, which are predominantly text-based, lack real speech signals, and are limited in language and features related to speech.", "method": "Introduction of RealTalk-CN dataset containing 5.4k dialogues with spontaneous speech disfluencies captured across various domains; proposal of a cross-modal chat task for simulating real-world interactions.", "result": "RealTalk-CN captures diverse dialogue scenarios with speech disfluencies, establishing a strong foundation for Chinese speech-based LLM research.", "conclusion": "The dataset demonstrates robustness against speech disfluencies and highlights the need for further research in speech dialogue systems for Chinese speakers.", "key_contributions": ["Creation of a novel dual-modal TOD dataset for Chinese", "Inclusion of spontaneous speech disfluencies", "Proposal of a cross-modal chat task for dynamic modality switching"], "limitations": "", "keywords": ["speech-based LLMs", "task-oriented dialogue", "Chinese dataset"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.10016", "pdf": "https://arxiv.org/pdf/2508.10016.pdf", "abs": "https://arxiv.org/abs/2508.10016", "title": "Training-Free Multimodal Large Language Model Orchestration", "authors": ["Tianyu Xie", "Yuhang Wu", "Yongdong Luo", "Jiayi Ji", "Xiawu Zheng"], "categories": ["cs.CL"], "comment": null, "summary": "Different Multimodal Large Language Models (MLLMs) cannot be integrated into\na unified multimodal input-output system directly. In previous work, training\nhas been considered as an inevitable component due to challenges in modal\nalignment, Text-to-Speech efficiency and other integration issues. In this\npaper, we introduce Multimodal Large Language Model Orchestration, an effective\napproach for creating interactive multimodal AI systems without additional\ntraining. MLLM Orchestration leverages the inherent reasoning capabilities of\nlarge language models to coordinate specialized models through explicit\nworkflows, enabling natural multimodal interactions while maintaining\nmodularity, improving interpretability, and significantly enhancing\ncomputational efficiency. Our orchestration framework is built upon three key\ninnovations: (1) a central controller LLM that analyzes user inputs and\ndynamically routes tasks to appropriate specialized models through carefully\ndesigned agents; (2) a parallel Text-to-Speech architecture that enables true\nfull-duplex interaction with seamless interruption handling and natural\nconversational flow; and (3) a cross-modal memory integration system that\nmaintains coherent context across modalities through intelligent information\nsynthesis and retrieval, selectively avoiding unnecessary modality calls in\ncertain scenarios to improve response speed. Extensive evaluations demonstrate\nthat MLLM Orchestration achieves comprehensive multimodal capabilities without\nadditional training, performance improvements of up to 7.8% over traditional\njointly-trained approaches on standard benchmarks, reduced latency by 10.3%,\nand significantly enhanced interpretability through explicit orchestration\nprocesses.", "AI": {"tldr": "This paper presents a new approach called Multimodal Large Language Model Orchestration that enables the integration of multimodal models without additional training, enhancing efficiency and interpretability.", "motivation": "To address the integration challenges of multimodal large language models without requiring further training.", "method": "The proposed approach utilizes a central controller LLM to dynamically route tasks to specialized models via designed agents, alongside a parallel Text-to-Speech system and a cross-modal memory integration system.", "result": "The orchestration framework shows up to 7.8% performance improvement and 10.3% reduced latency compared to traditional methods, with enhanced interpretability.", "conclusion": "The MLLM Orchestration framework successfully creates efficient interactive multimodal AI systems without the need for additional training, facilitating natural interactions.", "key_contributions": ["Introduction of MLLM Orchestration for seamless multimodal interaction", "Development of a parallel Text-to-Speech architecture for full-duplex interaction", "Implementation of a cross-modal memory integration system for coherent context management"], "limitations": "", "keywords": ["Multimodal", "Large Language Models", "Orchestration", "Efficiency", "Interpretability"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.10018", "pdf": "https://arxiv.org/pdf/2508.10018.pdf", "abs": "https://arxiv.org/abs/2508.10018", "title": "A Rose by Any Other Name Would Smell as Sweet: Categorical Homotopy Theory for Large Language Models", "authors": ["Sridhar Mahadevan"], "categories": ["cs.CL", "cs.AI", "math.AT"], "comment": "26 pages. arXiv admin note: text overlap with arXiv:2402.18732", "summary": "Natural language is replete with superficially different statements, such as\n``Charles Darwin wrote\" and ``Charles Darwin is the author of\", which carry the\nsame meaning. Large language models (LLMs) should generate the same next-token\nprobabilities in such cases, but usually do not. Empirical workarounds have\nbeen explored, such as using k-NN estimates of sentence similarity to produce\nsmoothed estimates. In this paper, we tackle this problem more abstractly,\nintroducing a categorical homotopy framework for LLMs. We introduce an LLM\nMarkov category to represent probability distributions in language generated by\nan LLM, where the probability of a sentence, such as ``Charles Darwin wrote\" is\ndefined by an arrow in a Markov category. However, this approach runs into\ndifficulties as language is full of equivalent rephrases, and each generates a\nnon-isomorphic arrow in the LLM Markov category. To address this fundamental\nproblem, we use categorical homotopy techniques to capture ``weak equivalences\"\nin an LLM Markov category. We present a detailed overview of application of\ncategorical homotopy to LLMs, from higher algebraic K-theory to model\ncategories, building on powerful theoretical results developed over the past\nhalf a century.", "AI": {"tldr": "This paper introduces a categorical homotopy framework for Large Language Models (LLMs) to address issues of generating equivalent statements that are not treated as such, using techniques from higher algebraic K-theory.", "motivation": "The goal is to find a way for LLMs to recognize and generate equivalent statements that convey the same meaning, improving their performance in language understanding tasks.", "method": "The authors develop an LLM Markov category to represent probability distributions of sentences, using categorical homotopy to manage the challenge of non-isomorphic arrows generated by different rephrases.", "result": "The framework effectively captures 'weak equivalences' in LLMs, offering a new theoretical foundation for tackling language generation issues.", "conclusion": "The study shows that applying categorical homotopy techniques can enhance the capability of LLMs in generating semantically equivalent statements.", "key_contributions": ["Introduction of the LLM Markov category", "Application of categorical homotopy techniques to LLMs", "Solution to the problem of non-isomorphic arrows in language generation"], "limitations": "The complexity of the categorical framework may limit immediate practical application in certain LLM deployments.", "keywords": ["Large Language Models", "Categorical Homotopy", "Natural Language Processing", "Markov Category", "Language Generation"], "importance_score": 6, "read_time_minutes": 20}}
{"id": "2210.01242", "pdf": "https://arxiv.org/pdf/2210.01242.pdf", "abs": "https://arxiv.org/abs/2210.01242", "title": "The Effect of Warm-Glow on User Behavioral Intention to Adopt Technology: Extending the UTAUT2 Model", "authors": ["Antonios Saravanos", "Neil Stott", "Dongnanzi Zheng", "Stavros Zervoudakis"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "In this study, we enhance the Unified Theory of Acceptance and Use of\nTechnology (UTAUT2) by incorporating the warm-glow phenomenon to clarify its\nimpact on user decisions regarding the adoption of technology. We introduce two\nadditional constructs aimed at capturing both the external and internal aspects\nof warm-glow, thus creating what we refer to as the UTAUT2 + WG model. To\nevaluate the effectiveness of our model, we conducted an experimental study in\nwhich participants were presented with a scenario describing a hypothetical\ntechnology designed to evoke warm-glow sensations. Using the partial least\nsquares method, we analyzed the collected data to assess our expanded model.\nOur findings indicate that warm-glow significantly influences user behavior,\nwith the internal aspect having the strongest influence, followed by hedonic\nmotivation, performance expectancy, and finally the external aspect of\nwarm-glow. We conclude by discussing the implications of our research,\nacknowledging its limitations, and suggesting directions for future\nexploration.", "AI": {"tldr": "This study enhances the UTAUT2 model by integrating the warm-glow phenomenon to assess its influence on technology adoption.", "motivation": "To improve understanding of technology adoption by incorporating emotional aspects like the warm-glow phenomenon into the UTAUT2 model.", "method": "An experimental study was conducted where participants evaluated a hypothetical technology related to warm-glow sensations, and data was analyzed using the partial least squares method.", "result": "The analysis revealed that warm-glow significantly affects user behavior, particularly through its internal aspect, followed by hedonic motivation and performance expectancy.", "conclusion": "The research highlights the importance of emotional influences on technology adoption, suggesting further exploration of these aspects in future studies.", "key_contributions": ["Integration of warm-glow into the UTAUT2 model", "Identification of internal and external aspects of warm-glow", "Empirical validation of the UTAUT2 + WG model"], "limitations": "The study is based on a hypothetical scenario, which may not fully represent real-world technology adoption.", "keywords": ["UTAUT2", "warm-glow phenomenon", "technology adoption", "emotional influence", "user behavior"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.10019", "pdf": "https://arxiv.org/pdf/2508.10019.pdf", "abs": "https://arxiv.org/abs/2508.10019", "title": "Decoupling Understanding from Reasoning via Problem Space Mapping for Small-scale Model Reasoning", "authors": ["Li Wang", "Changhao Zhang", "Zengqi Xiu", "Kai Lu", "Xin Yu", "Kui Zhang", "Wenjun Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite recent advances in the reasoning capabilities of Large Language\nModels (LLMs), improving the reasoning ability of Small Language Models (SLMs,\ne.g., $\\leq$ 1.5B) remains challenging. A key obstacle lies in the complexity\nand variability of natural language: essentially equivalent problems often\nappear in diverse surface forms, often obscured by redundant or distracting\ndetails. This imposes a dual burden on SLMs: they must first extract the core\nproblem from complex linguistic input, and then perform reasoning based on that\nunderstanding. The resulting vast and noisy problem space hinders optimization,\nparticularly for models with limited capacity. To address this, we propose a\nnew framework that decouples understanding from reasoning by mapping natural\nlanguage problems into a canonical problem space-a semantically simplified yet\nexpressive domain. This enables SLMs to focus on reasoning over standardized\ninputs, free from linguistic variability. Within this framework, we introduce\nDURIT (Decoupled Understanding from Reasoning via Iterative Training), a\nthree-step algorithm that iteratively: (1) mapping natural language problems\nvia reinforcement learning, (2) aligns reasoning trajectories through\nself-distillation, and (3) trains reasoning policies in the problem space. The\nmapper and reasoner are co-trained in an alternating loop throughout this\nprocess. Experiments show that DURIT substantially improves SLMs' performance\non both in-domain and out-of-domain mathematical and logical reasoning tasks.\nBeyond improving reasoning capabilities, DURIT also improves the robustness of\nreasoning, validating decoupling understanding from reasoning as an effective\nstrategy for strengthening SLMs.", "AI": {"tldr": "The paper introduces DURIT, an algorithm that enhances reasoning in Small Language Models by decoupling understanding from reasoning through a standardized problem space.", "motivation": "Improving reasoning capabilities of Small Language Models (SLMs) is challenging due to the complexity of natural language, which hampers their ability to extract core problems.", "method": "The DURIT framework employs reinforcement learning to map natural language into a simplified problem space, aligns reasoning trajectories via self-distillation, and iteratively trains reasoning policies.", "result": "DURIT significantly enhances SLMs' performance on various reasoning tasks, demonstrating improved reasoning capabilities and robustness.", "conclusion": "Decoupling understanding from reasoning proves to be an effective strategy for strengthening SLMs and enhancing their performance on reasoning tasks.", "key_contributions": ["Introduction of the DURIT framework for decoupled reasoning in SLMs.", "Demonstration of substantial performance improvements in mathematical and logical reasoning tasks.", "Validation of the approach's robustness against linguistic variability."], "limitations": "", "keywords": ["Small Language Models", "Large Language Models", "Decoupled Reasoning", "Natural Language Processing", "Reinforcement Learning"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2508.10020", "pdf": "https://arxiv.org/pdf/2508.10020.pdf", "abs": "https://arxiv.org/abs/2508.10020", "title": "FedCoT: Communication-Efficient Federated Reasoning Enhancement for Large Language Models", "authors": ["Chuan Li", "Qianyi Zhao", "Fengran Mo", "Cen Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Efficiently enhancing the reasoning capabilities of large language models\n(LLMs) in federated learning environments remains challenging, particularly\nwhen balancing performance gains with strict computational, communication, and\nprivacy constraints. This challenge is especially acute in healthcare, where\ndecisions-spanning clinical, operational, and patient-facing contexts-demand\nnot only accurate outputs but also interpretable, traceable rationales to\nensure safety, accountability, and regulatory compliance. Conventional\nfederated tuning approaches on LLM fail to address this need: they optimize\nprimarily for answer correctness while neglecting rationale quality, leaving\nCoT capabilities dependent on models' innate pre-training abilities. Moreover,\nexisting methods for improving rationales typically rely on privacy-violating\nknowledge distillation from centralized models. Additionally, the communication\noverhead in traditional federated fine-tuning on LLMs remains substantial. We\naddresses this gap by proposing FedCoT, a novel framework specifically designed\nto enhance reasoning in federated settings. FedCoT leverages a lightweight\nchain-of-thought enhancement mechanism: local models generate multiple\nreasoning paths, and a compact discriminator dynamically selects the most\npromising one. This approach improves reasoning accuracy and robustness while\nproviding valuable interpretability, which is particularly critical for medical\napplications. To manage client heterogeneity efficiently, we adopt an improved\naggregation approach building upon advanced LoRA module stacking, incorporating\nclient classifier-awareness to achieve noise-free aggregation across diverse\nclients. Comprehensive experiments on medical reasoning tasks demonstrate that\nFedCoT significantly boosts client-side reasoning performance under stringent\nresource budgets while fully preserving data privacy.", "AI": {"tldr": "FedCoT is a novel framework designed to enhance reasoning in federated learning environments, particularly in healthcare, by improving reasoning accuracy and robustness while ensuring data privacy.", "motivation": "The paper addresses the challenge of improving reasoning capabilities of large language models in federated learning, especially in healthcare contexts where interpretability and privacy are critical.", "method": "FedCoT employs a lightweight chain-of-thought enhancement mechanism that allows local models to generate multiple reasoning paths, with a compact discriminator selecting the most promising path.", "result": "FedCoT demonstrates significant improvements in client-side reasoning performance on medical tasks while maintaining strict resource budgets and preserving data privacy.", "conclusion": "The proposed approach effectively enhances reasoning accuracy and robustness in federated settings, making it suitable for sensitive medical applications.", "key_contributions": ["Proposed FedCoT framework for enhanced reasoning in federated learning", "Lightweight chain-of-thought mechanism for generating multiple reasoning paths", "Improved client heterogeneity aggregation with LoRA module stacking"], "limitations": "", "keywords": ["federated learning", "large language models", "healthcare applications", "interpretability", "reasoning accuracy"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.10021", "pdf": "https://arxiv.org/pdf/2508.10021.pdf", "abs": "https://arxiv.org/abs/2508.10021", "title": "LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients", "authors": ["Egor Fadeev", "Dzhambulat Mollaev", "Aleksei Shestov", "Dima Korolev", "Omar Zoloev", "Ivan Kireev", "Andrey Savchenko", "Maksim Makarenko"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Learning clients embeddings from sequences of their historic communications\nis central to financial applications. While large language models (LLMs) offer\ngeneral world knowledge, their direct use on long event sequences is\ncomputationally expensive and impractical in real-world pipelines. In this\npaper, we propose LATTE, a contrastive learning framework that aligns raw event\nembeddings with semantic embeddings from frozen LLMs. Behavioral features are\nsummarized into short prompts, embedded by the LLM, and used as supervision via\ncontrastive loss. The proposed approach significantly reduces inference cost\nand input size compared to conventional processing of complete sequence by LLM.\nWe experimentally show that our method outperforms state-of-the-art techniques\nfor learning event sequence representations on real-world financial datasets\nwhile remaining deployable in latency-sensitive environments.", "AI": {"tldr": "LATTE is a contrastive learning framework that improves the efficiency of learning client embeddings from historic communication sequences in financial applications by leveraging LLMs.", "motivation": "The need to efficiently learn client embeddings from long event sequences in financial contexts while reducing computational costs.", "method": "LATTE employs a contrastive learning framework that aligns raw event embeddings with semantic embeddings provided by frozen large language models (LLMs) using summary behavioral features as supervision.", "result": "LATTE outperforms existing techniques for learning event sequence representations on actual financial datasets and is suitable for latency-sensitive environments.", "conclusion": "The proposed method offers a significant reduction in inference cost and input size compared to traditional LLM processing, making it practical for deployment in real-world financial applications.", "key_contributions": ["Introduction of LATTE, a novel contrastive learning framework", "Demonstrates improved performance over state-of-the-art methods on financial datasets", "Reduces computational burden while deploying LLMs for embedding tasks"], "limitations": "", "keywords": ["contrastive learning", "large language models", "client embeddings", "financial applications", "event sequences"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2508.10022", "pdf": "https://arxiv.org/pdf/2508.10022.pdf", "abs": "https://arxiv.org/abs/2508.10022", "title": "Conformal P-Value in Multiple-Choice Question Answering Tasks with Provable Risk Control", "authors": ["Yuanchang Ye"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study introduces a significance testing-enhanced conformal prediction\n(CP) framework to improve trustworthiness of large language models (LLMs) in\nmultiple-choice question answering (MCQA). While LLMs have been increasingly\ndeployed in disciplinary QA scenarios, hallucination and nonfactual generation\nsubstantially compromise response reliability. Although CP provides\nstatistically rigorous marginal coverage guarantees for prediction sets, and\nsignificance testing offers established statistical rigor, their synergistic\nintegration remains unexplored. To mitigate hallucination and factual\ninaccuracies, our framework integrates $p$-value computation with conformity\nscoring through self-consistency resampling of MCQA responses. This approach\ncalculates option frequencies to address LLMs' black-box nature, subsequently\nconstructing prediction sets via null hypothesis testing ($\\mathcal{H}_0$) with\nempirically derived $p$-values. Evaluations on MMLU and MMLU-Pro benchmarks\nusing off-the-shelf LLMs demonstrate: (1) The enhanced CP achieves\nuser-specified empirical miscoverage rates; (2) Test-set average prediction set\nsize (APSS) decreases monotonically with increasing risk levels ($\\alpha$),\nvalidating APSS as an effective uncertainty metric. This work establishes a\nprincipled statistical framework for trustworthy LLM deployment in high-stakes\nQA applications.", "AI": {"tldr": "This study presents a framework that integrates significance testing with conformal prediction to enhance the reliability of large language models in multiple-choice question answering, addressing issues like hallucination and inaccuracies.", "motivation": "To improve the trustworthiness of large language models in multiple-choice question answering due to concerns about hallucination and nonfactual generation compromising response reliability.", "method": "The framework combines significance testing with conformity scoring using self-consistency resampling of responses, computing option frequencies and constructing prediction sets via null hypothesis testing with empirically derived p-values.", "result": "Evaluations indicate that the enhanced conformal prediction framework achieves user-specified empirical miscoverage rates and that the average prediction set size decreases with higher risk levels, validating its effectiveness as an uncertainty metric.", "conclusion": "This work establishes a statistically rigorous framework for deploying trustworthy large language models in high-stakes Q&A scenarios.", "key_contributions": ["Integration of significance testing with conformal prediction for LLMs", "Empirical validation of the framework on MMLU and MMLU-Pro benchmarks", "Establishment of a statistical framework for high-stakes QA applications"], "limitations": "", "keywords": ["large language models", "conformal prediction", "significance testing", "multiple-choice question answering", "uncertainty quantification"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.10024", "pdf": "https://arxiv.org/pdf/2508.10024.pdf", "abs": "https://arxiv.org/abs/2508.10024", "title": "RTTC: Reward-Guided Collaborative Test-Time Compute", "authors": ["J. Pablo Muñoz", "Jinjie Yuan"], "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.0"], "comment": null, "summary": "Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the\nperformance of Large Language Models (LLMs) at inference, leveraging strategies\nsuch as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).\nHowever, the optimal adaptation strategy varies across queries, and\nindiscriminate application of TTC strategy incurs substantial computational\noverhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a\nnovel framework that adaptively selects the most effective TTC strategy for\neach query via a pretrained reward model, maximizing downstream accuracy across\ndiverse domains and tasks. RTTC operates in a distributed server-client\narchitecture, retrieving relevant samples from a remote knowledge base and\napplying RAG or lightweight fine-tuning on client devices only when necessary.\nTo further mitigate redundant computation, we propose Query-State Caching,\nwhich enables the efficient reuse of historical query states at both retrieval\nand adaptation levels. Extensive experiments across multiple LLMs and\nbenchmarks demonstrate that RTTC consistently achieves superior accuracy\ncompared to vanilla RAG or TTT, validating the necessity of adaptive,\nreward-guided TTC selection and the potential of RTTC for scalable,\nhigh-performance language model adaptation.", "AI": {"tldr": "Introducing Reward-Guided Test-Time Compute (RTTC) to optimize Large Language Models' adaptation strategies for enhanced performance at inference.", "motivation": "To address the varying optimal adaptation strategies of Large Language Models (LLMs) across queries and reduce computational overhead from indiscriminate Test-Time Compute (TTC) strategies.", "method": "RTTC employs a pretrained reward model to adaptively select the most effective TTC strategy for each query, optimizing accuracy while operating in a distributed server-client setup and implementing Query-State Caching for efficiency.", "result": "RTTC outperforms vanilla RAG or TTT in accuracy, demonstrating the efficacy of adaptive and reward-guided TTC selection across multiple LLMs and benchmarks.", "conclusion": "RTTC presents a scalable and high-performance approach to language model adaptation, validating the necessity of adaptive strategies in improving model performance.", "key_contributions": ["Introduction of RTTC for adaptive Test-Time Compute.", "Implementation of Query-State Caching for efficiency in retrival and adaptation.", "Demonstration of superior accuracy across diverse domains compared to traditional methods."], "limitations": "", "keywords": ["Test-Time Compute", "Large Language Models", "Reward-Guided", "Retrieval-Augmented Generation", "Query-State Caching"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.10025", "pdf": "https://arxiv.org/pdf/2508.10025.pdf", "abs": "https://arxiv.org/abs/2508.10025", "title": "Detecting and explaining postpartum depression in real-time with generative artificial intelligence", "authors": ["Silvia García-Méndez", "Francisco de Arriba-Pérez"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Among the many challenges mothers undergo after childbirth, postpartum\ndepression (PPD) is a severe condition that significantly impacts their mental\nand physical well-being. Consequently, the rapid detection of ppd and their\nassociated risk factors is critical for in-time assessment and intervention\nthrough specialized prevention procedures. Accordingly, this work addresses the\nneed to help practitioners make decisions with the latest technological\nadvancements to enable real-time screening and treatment recommendations.\nMainly, our work contributes to an intelligent PPD screening system that\ncombines Natural Language Processing, Machine Learning (ML), and Large Language\nModels (LLMs) towards an affordable, real-time, and non-invasive free speech\nanalysis. Moreover, it addresses the black box problem since the predictions\nare described to the end users thanks to the combination of LLMs with\ninterpretable ml models (i.e., tree-based algorithms) using feature importance\nand natural language. The results obtained are 90 % on ppd detection for all\nevaluation metrics, outperforming the competing solutions in the literature.\nUltimately, our solution contributes to the rapid detection of PPD and their\nassociated risk factors, critical for in-time and proper assessment and\nintervention.", "AI": {"tldr": "An intelligent screening system for postpartum depression (PPD) using NLP, ML, and LLMs enables rapid detection and intervention.", "motivation": "The need for early detection of postpartum depression to improve mothers' mental and physical health after childbirth.", "method": "A combination of Natural Language Processing, Machine Learning, and Large Language Models for real-time speech analysis and risk factor identification.", "result": "Achieved 90% accuracy in PPD detection, surpassing existing solutions in the literature.", "conclusion": "The proposed system facilitates timely assessment and intervention for postpartum depression through interpretable AI models.", "key_contributions": ["Development of an intelligent PPD screening system.", "Integration of LLMs with interpretable machine learning models.", "Real-time, non-invasive analysis using free speech."], "limitations": "", "keywords": ["postpartum depression", "natural language processing", "machine learning", "large language models", "screening system"], "importance_score": 10, "read_time_minutes": 5}}
{"id": "2508.10026", "pdf": "https://arxiv.org/pdf/2508.10026.pdf", "abs": "https://arxiv.org/abs/2508.10026", "title": "SABER: Switchable and Balanced Training for Efficient LLM Reasoning", "authors": ["Kai Zhao", "Yanjun Zhao", "Jiaming Song", "Shien He", "Lusheng Zhang", "Qiang Zhang", "Tianjiao Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) empowered by chain-of-thought reasoning have\nachieved impressive accuracy on complex tasks but suffer from excessive\ninference costs and latency when applied uniformly to all problems. We propose\nSABER (Switchable and Balanced Training for Efficient LLM Reasoning), a\nreinforcement learning framework that endows LLMs with user-controllable,\ntoken-budgeted reasoning. SABER first profiles each training example's\nbase-model thinking token usage and assigns it to one of the predefined budget\ntiers. During fine-tuning, the model is guided by system prompts and\nlength-aware rewards to respect its assigned budget. In parallel, we\nincorporate no-think examples to ensure the model remains reliable even when\nexplicit reasoning is turned off. SABER further supports four discrete\ninference modes - NoThink, FastThink, CoreThink, and DeepThink, enabling\nflexible trade-offs between latency and reasoning depth. Extensive evaluations\non math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning\n(LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight\nbudgets, graceful degradation, and effective cross-scale and cross-domain\ngeneralization. In particular, SABER-FastThink cuts reasoning length by 65.4%\nand yields a 3.6% accuracy gain compared with the base model on the MATH\nbenchmark.", "AI": {"tldr": "SABER is a reinforcement learning framework for efficient reasoning in large language models, allowing user-controlled inference with flexible budget tiers to optimize accuracy and latency.", "motivation": "Large language models struggle with high inference costs and latency when applied uniformly, necessitating a more efficient reasoning approach.", "method": "SABER profiles token usage for training examples, assigns budget tiers, and employs system prompts and length-aware rewards during fine-tuning, alongside no-think examples for reliability.", "result": "SABER achieves high accuracy under restricted budgets, with SABER-FastThink reducing reasoning length by 65.4% while improving accuracy by 3.6% on the MATH benchmark.", "conclusion": "SABER provides flexible reasoning controls that enhance performance and reduce latency across a range of tasks and benchmarks.", "key_contributions": ["Introduces a user-controllable and token-budgeted reasoning framework for LLMs.", "Demonstrates effective cross-domain generalization and graceful degradation in performance.", "Achieves significant improvements in accuracy and latency with distinct inference modes."], "limitations": "", "keywords": ["Large language models", "Reinforcement learning", "Efficient reasoning", "Token-budgeted reasoning", "Machine learning"], "importance_score": 9, "read_time_minutes": 6}}
{"id": "2508.10027", "pdf": "https://arxiv.org/pdf/2508.10027.pdf", "abs": "https://arxiv.org/abs/2508.10027", "title": "LLMCARE: Alzheimer's Detection via Transformer Models Enhanced by LLM-Generated Synthetic Data", "authors": ["Ali Zolnour", "Hossein Azadmaleki", "Yasaman Haghbin", "Fatemeh Taherinezhad", "Mohamad Javad Momeni Nezhad", "Sina Rashidi", "Masoud Khani", "AmirSajjad Taleban", "Samin Mahdizadeh Sani", "Maryam Dadkhah", "James M. Noble", "Suzanne Bakken", "Yadollah Yaghoobzadeh", "Abdol-Hossein Vahabie", "Masoud Rouhizadeh", "Maryam Zolnoori"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Alzheimer's disease and related dementias (ADRD) affect approximately five\nmillion older adults in the U.S., yet over half remain undiagnosed.\nSpeech-based natural language processing (NLP) offers a promising, scalable\napproach to detect early cognitive decline through linguistic markers.\n  To develop and evaluate a screening pipeline that (i) fuses transformer\nembeddings with handcrafted linguistic features, (ii) tests data augmentation\nusing synthetic speech generated by large language models (LLMs), and (iii)\nbenchmarks unimodal and multimodal LLM classifiers for ADRD detection.\n  Transcripts from the DementiaBank \"cookie-theft\" task (n = 237) were used.\nTen transformer models were evaluated under three fine-tuning strategies. A\nfusion model combined embeddings from the top-performing transformer with 110\nlexical-derived linguistic features. Five LLMs (LLaMA-8B/70B, MedAlpaca-7B,\nMinistral-8B, GPT-4o) were fine-tuned to generate label-conditioned synthetic\nspeech, which was used to augment training data. Three multimodal models\n(GPT-4o, Qwen-Omni, Phi-4) were tested for speech-text classification in\nzero-shot and fine-tuned settings.\n  The fusion model achieved F1 = 83.3 (AUC = 89.5), outperforming linguistic or\ntransformer-only baselines. Augmenting training data with 2x MedAlpaca-7B\nsynthetic speech increased F1 to 85.7. Fine-tuning significantly improved\nunimodal LLM classifiers (e.g., MedAlpaca: F1 = 47.3 -> 78.5 F1). Current\nmultimodal models demonstrated lower performance (GPT-4o = 70.2 F1; Qwen =\n66.0). Performance gains aligned with the distributional similarity between\nsynthetic and real speech.\n  Integrating transformer embeddings with linguistic features enhances ADRD\ndetection from speech. Clinically tuned LLMs effectively support both\nclassification and data augmentation, while further advancement is needed in\nmultimodal modeling.", "AI": {"tldr": "This paper explores the use of NLP and LLMs to improve early detection of Alzheimer's disease through a novel screening pipeline that combines transformer embeddings, synthetic speech augmentation, and multimodal classification methods.", "motivation": "Address the challenge of early detection of Alzheimer's disease and related dementias, as over half of affected older adults remain undiagnosed. The motivation is to leverage speech-based NLP to identify linguistic markers indicative of cognitive decline.", "method": "Developed a screening pipeline that fuses transformer embeddings with handcrafted linguistic features, tests data augmentation using synthetic speech generated by LLMs, and benchmarks various unimodal and multimodal classifiers for ADRD detection using existing speech transcripts.", "result": "The fusion model outperformed baselines with an F1 score of 83.3 and AUC of 89.5. Augmenting training data with synthetic speech improved the F1 score to 85.7, while fine-tuning LLM classifiers resulted in significant performance boosts, though multimodal models underperformed compared to expectations.", "conclusion": "Integrating transformer embeddings with linguistic features significantly enhances ADRD detection from speech. Clinically tuned LLMs are effective for classification and data augmentation, but further improvements are required in multimodal modeling.", "key_contributions": ["Development of a novel screening pipeline for ADRD detection using NLP and LLMs.", "Combination of transformer embeddings with handcrafted linguistic features for improved performance.", "Demonstration of the efficacy of synthetic speech augmentation in enhancing model accuracy."], "limitations": "Current multimodal models showed lower performance than expected, indicating a need for further development in this area.", "keywords": ["Alzheimer's disease", "natural language processing", "large language models", "cognitive decline", "speech analysis"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.10028", "pdf": "https://arxiv.org/pdf/2508.10028.pdf", "abs": "https://arxiv.org/abs/2508.10028", "title": "PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs", "authors": ["Xiao Fu", "Hossein A. Rahmani", "Bin Wu", "Jerome Ramos", "Emine Yilmaz", "Aldo Lipani"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "7 pages", "summary": "Personalised text generation is essential for user-centric information\nsystems, yet most evaluation methods overlook the individuality of users. We\nintroduce \\textbf{PREF}, a \\textbf{P}ersonalised \\textbf{R}eference-free\n\\textbf{E}valuation \\textbf{F}ramework that jointly measures general output\nquality and user-specific alignment without requiring gold personalised\nreferences. PREF operates in a three-step pipeline: (1) a coverage stage uses a\nlarge language model (LLM) to generate a comprehensive, query-specific\nguideline covering universal criteria such as factuality, coherence, and\ncompleteness; (2) a preference stage re-ranks and selectively augments these\nfactors using the target user's profile, stated or inferred preferences, and\ncontext, producing a personalised evaluation rubric; and (3) a scoring stage\napplies an LLM judge to rate candidate answers against this rubric, ensuring\nbaseline adequacy while capturing subjective priorities. This separation of\ncoverage from preference improves robustness, transparency, and reusability,\nand allows smaller models to approximate the personalised quality of larger\nones. Experiments on the PrefEval benchmark, including implicit\npreference-following tasks, show that PREF achieves higher accuracy, better\ncalibration, and closer alignment with human judgments than strong baselines.\nBy enabling scalable, interpretable, and user-aligned evaluation, PREF lays the\ngroundwork for more reliable assessment and development of personalised\nlanguage generation systems.", "AI": {"tldr": "PREF is a new evaluation framework for personalized text generation that measures output quality and user alignment without requiring personal references.", "motivation": "To address the lack of user individuality in current evaluation methods for personalized text generation.", "method": "PREF operates in a three-step process: generate universal guidelines using an LLM, re-rank these using user profiles, and score against a personalized rubric.", "result": "PREF outperforms traditional methods in accuracy and alignment with human judgments on the PrefEval benchmark.", "conclusion": "PREF improves the evaluation of personalized language generation by enabling scalable and interpretable methods, fostering better user alignment.", "key_contributions": ["Introduction of PREF framework for personalized evaluation", "Three-step evaluation process", "Demonstrated improved accuracy and calibration over baselines"], "limitations": "", "keywords": ["Personalized evaluation", "Text generation", "Large language models"], "importance_score": 9, "read_time_minutes": 7}}
{"id": "2508.10029", "pdf": "https://arxiv.org/pdf/2508.10029.pdf", "abs": "https://arxiv.org/abs/2508.10029", "title": "Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs", "authors": ["Wenpeng Xing", "Mohan Li", "Chunqiang Hu", "Haitao XuNingyu Zhang", "Bo Lin", "Meng Han"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "Large language models (LLMs) demonstrate impressive capabilities in various\nlanguage tasks but are susceptible to jailbreak attacks that circumvent their\nsafety alignments. This paper introduces Latent Fusion Jailbreak (LFJ), a\nrepresentation-based attack that interpolates hidden states from harmful and\nbenign query pairs to elicit prohibited responses. LFJ begins by selecting\nquery pairs with high thematic and syntactic similarity, then performs\ngradient-guided interpolation at influential layers and tokens, followed by\noptimization to balance attack success, output fluency, and computational\nefficiency. Evaluations on models such as Vicuna and LLaMA-2 across benchmarks\nlike AdvBench and MaliciousInstruct yield an average attack success rate (ASR)\nof 94.01%, outperforming existing methods. To mitigate LFJ, we propose an\nadversarial training defense that fine-tunes models on interpolated examples,\nreducing ASR by over 80% without degrading performance on benign inputs.\nAblation studies validate the importance of query pair selection, hidden state\ninterpolation components, and optimization strategies in LFJ's effectiveness.", "AI": {"tldr": "This paper presents Latent Fusion Jailbreak (LFJ), a powerful attack method exploiting large language models through gradient-guided interpolation of hidden states from query pairs to induce prohibited responses, achieving high attack success rates.", "motivation": "To address the vulnerability of large language models to jailbreak attacks that bypass safety measures by exploiting their language processing capabilities.", "method": "LFJ employs a technique where thematic and syntactic similarities between harmful and benign query pairs are leveraged. It uses gradient-guided interpolation at specific layers and tokens, followed by optimization to enhance attack efficiency while maintaining output fluency.", "result": "LFJ achieves an average attack success rate of 94.01% across tested models such as Vicuna and LLaMA-2, surpassing other existing methods, and a proposed adversarial training defense reduces the success rate by over 80% without harming benign input performance.", "conclusion": "The study confirms LFJ's effectiveness and highlights the potential of adversarial training to mitigate its impact, emphasizing the significance of the chosen query pairs and the optimization strategies employed in the attack.", "key_contributions": ["Introduction of Latent Fusion Jailbreak (LFJ) as a novel attack method.", "Demonstration of superior average attack success rates (94.01%) compared to existing methods.", "Proposal of an effective adversarial training defense that significantly reduces attack success without degrading benign performance."], "limitations": "The paper does not explore the long-term implications of such attacks in real-world applications or assess other potential mitigation strategies beyond adversarial training.", "keywords": ["large language models", "jailbreak attacks", "adversarial training", "natural language processing", "machine learning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.10030", "pdf": "https://arxiv.org/pdf/2508.10030.pdf", "abs": "https://arxiv.org/abs/2508.10030", "title": "Inference-Aware Prompt Optimization for Aligning Black-Box Large Language Models", "authors": ["Saaduddin Mahmud", "Mason Nakamura", "Kyle H. Wray", "Shlomo Zilberstein"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages", "summary": "Prompt optimization methods have demonstrated significant effectiveness in\naligning black-box large language models (LLMs). In parallel, inference scaling\nstrategies such as Best-of-N Sampling and Majority Voting have also proven to\nenhance alignment and performance by trading off computation. However, existing\nprompt optimization approaches are inference strategy agnostic; that is, they\noptimize prompts without regard to the inference strategy employed during\ndeployment. This constitutes a significant methodological gap, as our empirical\nand theoretical analysis reveals a strong interdependence between these two\nparadigms. Moreover, we find that user preferences regarding trade-offs among\nmultiple objectives and inference budgets substantially influence the choice of\nprompt and inference configuration. To address this gap, we introduce a unified\nnovel framework named IAPO (Inference-Aware Prompt Optimization) that jointly\noptimizes the prompt and inference scale, while being aware of the inference\nbudget and different task objectives. We then develop a fixed-budget training\nalgorithm for IAPO, which we call PSST (Prompt Scaling via Sequential\nTrimming), and analyze finite-budget guarantees on error probability. Finally,\nwe evaluate the effectiveness of PSST on six different tasks, including\nmulti-objective text generation and reasoning, and demonstrate the critical\nrole of incorporating inference-awareness when aligning black-box LLMs through\nprompt optimization.", "AI": {"tldr": "This paper introduces IAPO, a framework for optimizing prompts and inference strategies in large language models, highlighting their interdependence and user preferences in alignment.", "motivation": "Existing prompt optimization methods do not consider the inference strategy used during deployment, creating a methodological gap in aligning black-box LLMs.", "method": "The paper proposes a unified framework called IAPO (Inference-Aware Prompt Optimization) that co-optimizes prompts and inference scale, taking into account inference budgets and task objectives. A fixed-budget training algorithm known as PSST (Prompt Scaling via Sequential Trimming) is also developed.", "result": "The PSST algorithm is evaluated on six tasks, showing that incorporating inference-awareness significantly enhances the alignment and performance of LLMs.", "conclusion": "The findings emphasize the importance of jointly optimizing prompts and inference strategies to better align large language models with user preferences and constraints.", "key_contributions": ["Introduction of IAPO framework for joint optimization of prompts and inference strategies.", "Development of PSST algorithm with finite-budget guarantees.", "Empirical evaluations demonstrating effectiveness across multiple tasks."], "limitations": "", "keywords": ["Prompt Optimization", "Large Language Models", "Inference Strategies", "AI Alignment", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2508.10032", "pdf": "https://arxiv.org/pdf/2508.10032.pdf", "abs": "https://arxiv.org/abs/2508.10032", "title": "The Cost of Thinking: Increased Jailbreak Risk in Large Language Models", "authors": ["Fan Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Thinking mode has always been regarded as one of the most valuable modes in\nLLMs. However, we uncover a surprising and previously overlooked phenomenon:\nLLMs with thinking mode are more easily broken by Jailbreak attack. We evaluate\n9 LLMs on AdvBench and HarmBench and find that the success rate of attacking\nthinking mode in LLMs is almost higher than that of non-thinking mode. Through\nlarge numbers of sample studies, it is found that for educational purposes and\nexcessively long thinking lengths are the characteristics of successfully\nattacked data, and LLMs also give harmful answers when they mostly know that\nthe questions are harmful. In order to alleviate the above problems, this paper\nproposes a method of safe thinking intervention for LLMs, which explicitly\nguides the internal thinking processes of LLMs by adding \"specific thinking\ntokens\" of LLMs to the prompt. The results demonstrate that the safe thinking\nintervention can significantly reduce the attack success rate of LLMs with\nthinking mode.", "AI": {"tldr": "This paper investigates the vulnerability of LLMs in thinking mode to Jailbreak attacks and proposes a method to mitigate this issue.", "motivation": "The research highlights the vulnerability of LLMs in thinking mode, which has been previously overlooked, and seeks to address the security concerns posed by Jailbreak attacks.", "method": "The authors evaluate 9 LLMs on AdvBench and HarmBench to assess the success rate of Jailbreak attacks in thinking versus non-thinking modes and propose a 'safe thinking intervention' using specific tokens in prompts.", "result": "The study finds that thinking mode LLMs have a higher success rate of attacks, and the integration of specific thinking tokens significantly reduces this vulnerability.", "conclusion": "The introduction of safe thinking intervention can effectively enhance the security of LLMs operating in thinking mode.", "key_contributions": ["Evaluation of 9 LLMs revealing vulnerabilities in thinking mode", "Identification of harmful characteristics in thinking mode during Jailbreak attacks", "Proposition of a 'safe thinking intervention' method to enhance LLM security"], "limitations": "", "keywords": ["Large Language Models", "Jailbreak attack", "Thinking mode", "Safety intervention", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.10036", "pdf": "https://arxiv.org/pdf/2508.10036.pdf", "abs": "https://arxiv.org/abs/2508.10036", "title": "Reflect then Learn: Active Prompting for Information Extraction Guided by Introspective Confusion", "authors": ["Dong Zhao", "Yadong Wang", "Xiang Chen", "Chenxi Wang", "Hongliang Dai", "Chuanxing Geng", "Shengzhong Zhang", "Shaoyuan Li", "Sheng-Jun Huang"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "Under Review", "summary": "Large Language Models (LLMs) show remarkable potential for few-shot\ninformation extraction (IE), yet their performance is highly sensitive to the\nchoice of in-context examples. Conventional selection strategies often fail to\nprovide informative guidance, as they overlook a key source of model\nfallibility: confusion stemming not just from semantic content, but also from\nthe generation of well-structured formats required by IE tasks. To address\nthis, we introduce Active Prompting for Information Extraction (APIE), a novel\nactive prompting framework guided by a principle we term introspective\nconfusion. Our method empowers an LLM to assess its own confusion through a\ndual-component uncertainty metric that uniquely quantifies both Format\nUncertainty (difficulty in generating correct syntax) and Content Uncertainty\n(inconsistency in extracted semantics). By ranking unlabeled data with this\ncomprehensive score, our framework actively selects the most challenging and\ninformative samples to serve as few-shot exemplars. Extensive experiments on\nfour benchmarks show that our approach consistently outperforms strong\nbaselines, yielding significant improvements in both extraction accuracy and\nrobustness. Our work highlights the critical importance of a fine-grained,\ndual-level view of model uncertainty when it comes to building effective and\nreliable structured generation systems.", "AI": {"tldr": "Introduction of Active Prompting for Information Extraction (APIE) framework that enhances few-shot information extraction by evaluating model uncertainty.", "motivation": "Address the sensitivity of LLM performance in few-shot information extraction to the choice of in-context examples and confusion from format generation.", "method": "Developed a dual-component uncertainty metric to assess both Format Uncertainty and Content Uncertainty in LLM outputs, guiding sample selection for few-shot learning.", "result": "APIE significantly improves extraction accuracy and robustness compared to strong baselines across four benchmarks.", "conclusion": "A dual-level understanding of model uncertainty is crucial for effective structured generation systems in information extraction tasks.", "key_contributions": ["Introduction of introspective confusion principle for model assessment", "Development of a dual-component uncertainty metric", "Demonstrated performance improvements on multiple benchmarks"], "limitations": "", "keywords": ["Active Prompting", "Information Extraction", "Large Language Models", "Model Uncertainty", "Few-shot Learning"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2508.10137", "pdf": "https://arxiv.org/pdf/2508.10137.pdf", "abs": "https://arxiv.org/abs/2508.10137", "title": "mSCoRe: a $M$ultilingual and Scalable Benchmark for $S$kill-based $Co$mmonsense $Re$asoning", "authors": ["Nghia Trung Ngo", "Franck Dernoncourt", "Thien Huu Nguyen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in reasoning-reinforced Large Language Models (LLMs) have\nshown remarkable capabilities in complex reasoning tasks. However, the\nmechanism underlying their utilization of different human reasoning skills\nremains poorly investigated, especially for multilingual commonsense reasoning\nthat involves everyday knowledge across different languages and cultures. To\naddress this gap, we propose a \\textbf{M}ultilingual and Scalable Benchmark for\n\\textbf{S}kill-based \\textbf{Co}mmonsense \\textbf{Re}asoning (\\textbf{mSCoRe}).\nOur benchmark incorporates three key components that are designed to\nsystematically evaluate LLM's reasoning capabilities, including: (1) a novel\ntaxonomy of reasoning skills that enables fine-grained analysis of models'\nreasoning processes, (2) a robust data synthesis pipeline tailored specifically\nfor commonsense reasoning evaluation, and (3) a complexity scaling framework\nallowing task difficulty to scale dynamically alongside future improvements in\nLLM abilities. Extensive experiments on eights state-of-the-art LLMs of varying\nsizes and training approaches demonstrate that \\textbf{mSCoRe} remains\nsignificantly challenging for current models, particularly at higher complexity\nlevels. Our results reveal the limitations of such reasoning-reinforced models\nwhen confronted with nuanced multilingual general and cultural commonsense. We\nfurther provide detailed analysis on the models' reasoning processes,\nsuggesting future directions for improving multilingual commonsense reasoning\ncapabilities.", "AI": {"tldr": "The paper introduces mSCoRe, a benchmark for evaluating multilingual commonsense reasoning skills in LLMs.", "motivation": "To investigate how reasoning-reinforced LLMs utilize human reasoning skills across languages in commonsense reasoning tasks.", "method": "The benchmark consists of a novel taxonomy of reasoning skills, a robust data synthesis pipeline, and a complexity scaling framework for dynamic evaluation.", "result": "The mSCoRe benchmark was extensively tested on eight state-of-the-art LLMs, revealing significant challenges at higher complexity levels, particularly in multilingual commonsense reasoning.", "conclusion": "The study underscores the limitations of current LLMs in multilingual reasoning and suggests directions for improving such capabilities.", "key_contributions": ["Introduction of a novel taxonomy of reasoning skills for fine-grained analysis", "Development of a tailored data synthesis pipeline for commonsense reasoning", "Creation of a complexity scaling framework for dynamic task evaluation"], "limitations": "The benchmark may not fully address all nuances of cultural commonsense reasoning across languages.", "keywords": ["Reasoning", "Large Language Models", "Commonsense", "Multilingual", "Benchmark"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.10142", "pdf": "https://arxiv.org/pdf/2508.10142.pdf", "abs": "https://arxiv.org/abs/2508.10142", "title": "Multi-Turn Puzzles: Evaluating Interactive Reasoning and Strategic Dialogue in LLMs", "authors": ["Kartikeya Badola", "Jonathan Simon", "Arian Hosseini", "Sara Marie Mc Carthy", "Tsendsuren Munkhdalai", "Abhimanyu Goyal", "Tomáš Kočiský", "Shyam Upadhyay", "Bahare Fatemi", "Mehran Kazemi"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) excel at solving problems with clear and\ncomplete statements, but often struggle with nuanced environments or\ninteractive tasks which are common in most real-world scenarios. This\nhighlights the critical need for developing LLMs that can effectively engage in\nlogically consistent multi-turn dialogue, seek information and reason with\nincomplete data. To this end, we introduce a novel benchmark comprising a suite\nof multi-turn tasks each designed to test specific reasoning, interactive\ndialogue, and information-seeking abilities. These tasks have deterministic\nscoring mechanisms, thus eliminating the need for human intervention.\nEvaluating frontier models on our benchmark reveals significant headroom. Our\nanalysis shows that most errors emerge from poor instruction following,\nreasoning failures, and poor planning. This benchmark provides valuable\ninsights into the strengths and weaknesses of current LLMs in handling complex,\ninteractive scenarios and offers a robust platform for future research aimed at\nimproving these critical capabilities.", "AI": {"tldr": "The paper presents a benchmark for evaluating large language models' abilities in multi-turn dialogue, reasoning, and information-seeking tasks, highlighting their current limitations.", "motivation": "The need for LLMs to engage in complex, interactive scenarios that mirror real-world tasks and interactions.", "method": "Introduction of a benchmark with multi-turn tasks designed for testing reasoning, interactive dialogue, and information-seeking capabilities, featuring deterministic scoring to eliminate human assessment.", "result": "Evaluation of frontier models shows significant performance gaps, with common errors linked to instruction following, reasoning, and planning.", "conclusion": "The benchmark offers insights into LLMs' strengths and weaknesses and lays groundwork for future enhancements in their capabilities.", "key_contributions": ["Introduction of a new benchmark for LLM evaluation", "Deterministic scoring system for multi-turn dialogue tasks", "Insights into specific areas of LLM deficiencies"], "limitations": "", "keywords": ["Large Language Models", "Multi-turn Dialogue", "Benchmarking", "Reasoning", "Information Seeking"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.10161", "pdf": "https://arxiv.org/pdf/2508.10161.pdf", "abs": "https://arxiv.org/abs/2508.10161", "title": "LaajMeter: A Framework for LaaJ Evaluation", "authors": ["Gal Amram", "Eitan Farchi", "Shmulik Froimovich", "Raviv Gal", "Avi Ziv"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used as evaluators in natural\nlanguage processing tasks, a paradigm known as LLM-as-a-Judge (LaaJ). While\neffective in general domains, LaaJs pose significant challenges in\ndomain-specific contexts, where annotated data is scarce and expert evaluation\nis costly. In such cases, meta-evaluation is often performed using metrics that\nhave not been validated for the specific domain in which they are applied. As a\nresult, it becomes difficult to determine which metrics effectively identify\nLaaJ quality, and further, what threshold indicates sufficient evaluator\nperformance. In this work, we introduce LaaJMeter, a simulation-based framework\nfor controlled meta-evaluation of LaaJs. LaaJMeter enables engineers to\ngenerate synthetic data representing virtual models and judges, allowing\nsystematic analysis of evaluation metrics under realistic conditions. This\nhelps practitioners validate and refine LaaJs for specific evaluation tasks:\nthey can test whether their metrics correctly distinguish between better and\nworse (virtual) LaaJs, and estimate appropriate thresholds for evaluator\nadequacy.\n  We demonstrate the utility of LaaJMeter in a code translation task involving\na legacy programming language, showing how different metrics vary in\nsensitivity to evaluator quality. Our results highlight the limitations of\ncommon metrics and the importance of principled metric selection. LaaJMeter\nprovides a scalable and extensible solution for assessing LaaJs in low-resource\nsettings, contributing to the broader effort to ensure trustworthy and\nreproducible evaluation in NLP.", "AI": {"tldr": "This paper introduces LaaJMeter, a simulation-based framework for validating evaluation metrics of large language model evaluators in domain-specific contexts, particularly when annotated data is scarce.", "motivation": "To address the challenges of evaluating large language model evaluators (LaaJs) in domain-specific contexts where annotated data is limited and metrics are unvalidated.", "method": "LaaJMeter generates synthetic data to simulate virtual models and judges, allowing systematic analysis of evaluation metrics under realistic conditions.", "result": "Demonstrated LaaJMeter's utility in a code translation task, revealing significant variations in metric sensitivity to evaluator quality and identifying limitations in common evaluation metrics.", "conclusion": "LaaJMeter offers a scalable and extensible solution for validating LaaJ metrics in low-resource settings, contributing to more reliable evaluation processes in NLP.", "key_contributions": ["Introduction of LaaJMeter for meta-evaluation of LaaJs", "Demonstration of its utility in a real-world task", "Insights into the sensitivity of evaluation metrics to evaluator quality."], "limitations": "The framework relies on synthetic data, which may not capture all complexities of real-world scenarios.", "keywords": ["Large Language Models", "meta-evaluation", "evaluation metrics", "synthetic data", "NLP"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.10175", "pdf": "https://arxiv.org/pdf/2508.10175.pdf", "abs": "https://arxiv.org/abs/2508.10175", "title": "Estimating Machine Translation Difficulty", "authors": ["Lorenzo Proietti", "Stefano Perrella", "Vilém Zouhar", "Roberto Navigli", "Tom Kocmi"], "categories": ["cs.CL"], "comment": null, "summary": "Machine translation quality has began achieving near-perfect translations in\nsome setups. These high-quality outputs make it difficult to distinguish\nbetween state-of-the-art models and to identify areas for future improvement.\nAutomatically identifying texts where machine translation systems struggle\nholds promise for developing more discriminative evaluations and guiding future\nresearch.\n  We formalize the task of translation difficulty estimation, defining a text's\ndifficulty based on the expected quality of its translations. We introduce a\nnew metric to evaluate difficulty estimators and use it to assess both\nbaselines and novel approaches. Finally, we demonstrate the practical utility\nof difficulty estimators by using them to construct more challenging machine\ntranslation benchmarks. Our results show that dedicated models (dubbed\nSentinel-src) outperform both heuristic-based methods (e.g. word rarity or\nsyntactic complexity) and LLM-as-a-judge approaches. We release two improved\nmodels for difficulty estimation, Sentinel-src-24 and Sentinel-src-25, which\ncan be used to scan large collections of texts and select those most likely to\nchallenge contemporary machine translation systems.", "AI": {"tldr": "The paper introduces a new metric for estimating translation difficulty and presents models that identify texts challenging for machine translation systems.", "motivation": "High-quality machine translations make it hard to evaluate models and identify areas for improvement; hence, estimating translation difficulty can guide future research.", "method": "The authors formalize the task of translation difficulty estimation, introduce a new evaluation metric, and assess baseline and novel approaches using this metric.", "result": "Dedicated models (Sentinel-src) outperform heuristic-based methods and LLM-as-a-judge approaches in estimating translation difficulty, leading to improved machine translation benchmarks.", "conclusion": "The released models, Sentinel-src-24 and Sentinel-src-25, enhance the ability to identify difficult texts for translation, aiding in the development of more effective machine translation systems.", "key_contributions": ["Introduction of a new metric for evaluating translation difficulty estimators", "Development of dedicated models (Sentinel-src) that outperform existing methods", "Construction of more challenging machine translation benchmarks using difficulty estimators"], "limitations": "", "keywords": ["machine translation", "difficulty estimation", "evaluation metrics", "Sentinel-src"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.10180", "pdf": "https://arxiv.org/pdf/2508.10180.pdf", "abs": "https://arxiv.org/abs/2508.10180", "title": "Efficient Forward-Only Data Valuation for Pretrained LLMs and VLMs", "authors": ["Wenlong Deng", "Jiaming Zhang", "Qi Zeng", "Christos Thrampoulidis", "Boying Gong", "Xiaoxiao Li"], "categories": ["cs.CL"], "comment": null, "summary": "Quantifying the influence of individual training samples is essential for\nenhancing the transparency and accountability of large language models (LLMs)\nand vision-language models (VLMs). However, existing data valuation methods\noften rely on Hessian information or model retraining, making them\ncomputationally prohibitive for billion-parameter models. In this work, we\nintroduce For-Value, a forward-only data valuation framework that enables\nscalable and efficient influence estimation for both LLMs and VLMs. By\nleveraging the rich representations of modern foundation models, For-Value\ncomputes influence scores using a simple closed-form expression based solely on\na single forward pass, thereby eliminating the need for costly gradient\ncomputations. Our theoretical analysis demonstrates that For-Value accurately\nestimates per-sample influence by capturing alignment in hidden representations\nand prediction errors between training and validation samples. Extensive\nexperiments show that For-Value matches or outperforms gradient-based baselines\nin identifying impactful fine-tuning examples and effectively detecting\nmislabeled data.", "AI": {"tldr": "Introduction of For-Value, a computationally efficient data valuation framework for estimating influence in large language and vision-language models.", "motivation": "Enhance transparency and accountability of large language models by quantifying influence of individual training samples.", "method": "For-Value is a forward-only data valuation framework that computes influence scores based on a single forward pass without requiring gradients or model retraining.", "result": "For-Value successfully matches or outperforms gradient-based methods in identifying impactful fine-tuning examples and detecting mislabeled data.", "conclusion": "The framework provides an efficient means of influence estimation, facilitating better understanding and management of training data in large models.", "key_contributions": ["Introduces For-Value for scalable influence estimation", "Avoids costly gradient calculations", "Demonstrates effectiveness on large models"], "limitations": "", "keywords": ["data valuation", "influence estimation", "large language models"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.10186", "pdf": "https://arxiv.org/pdf/2508.10186.pdf", "abs": "https://arxiv.org/abs/2508.10186", "title": "PakBBQ: A Culturally Adapted Bias Benchmark for QA", "authors": ["Abdullah Hashmat", "Muhammad Arham Mirza", "Agha Ali Raza"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "8 pages, 7 figures, 2 tables, Submitted to EMNLP 2025", "summary": "With the widespread adoption of Large Language Models (LLMs) across various\napplications, it is empirical to ensure their fairness across all user\ncommunities. However, most LLMs are trained and evaluated on Western centric\ndata, with little attention paid to low-resource languages and regional\ncontexts. To address this gap, we introduce PakBBQ, a culturally and regionally\nadapted extension of the original Bias Benchmark for Question Answering (BBQ)\ndataset. PakBBQ comprises over 214 templates, 17180 QA pairs across 8\ncategories in both English and Urdu, covering eight bias dimensions including\nage, disability, appearance, gender, socio-economic status, religious, regional\naffiliation, and language formality that are relevant in Pakistan. We evaluate\nmultiple multilingual LLMs under both ambiguous and explicitly disambiguated\ncontexts, as well as negative versus non negative question framings. Our\nexperiments reveal (i) an average accuracy gain of 12\\% with disambiguation,\n(ii) consistently stronger counter bias behaviors in Urdu than in English, and\n(iii) marked framing effects that reduce stereotypical responses when questions\nare posed negatively. These findings highlight the importance of contextualized\nbenchmarks and simple prompt engineering strategies for bias mitigation in low\nresource settings.", "AI": {"tldr": "The paper introduces PakBBQ, a new bias benchmark dataset that focuses on fairness in LLMs for low-resource languages, specifically English and Urdu, revealing significant findings on how context and question framing affect bias.", "motivation": "To ensure fairness across user communities, particularly for low-resource languages, by addressing the Western centric bias in LLM training data.", "method": "The authors developed PakBBQ, a dataset with 214 templates and 17180 QA pairs in English and Urdu, evaluating multiple multilingual LLMs on the dataset to observe performance under various contextual conditions.", "result": "Experiments showed a 12% accuracy gain with disambiguation, stronger counter bias behaviors in Urdu, and reduced stereotypical responses with negative question framing.", "conclusion": "Contextualized benchmarks and prompt engineering are crucial for mitigating bias in low-resource settings.", "key_contributions": ["Introduction of the PakBBQ dataset for low-resource languages", "Evaluation of bias in multilingual LLMs", "Demonstrated effects of disambiguation and question framing on bias."], "limitations": "", "keywords": ["Bias Mitigation", "Large Language Models", "Low-resource languages", "PakBBQ", "Fairness"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.10192", "pdf": "https://arxiv.org/pdf/2508.10192.pdf", "abs": "https://arxiv.org/abs/2508.10192", "title": "Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models", "authors": ["Igor Halperin"], "categories": ["cs.CL", "cs.AI", "cs.LG", "q-fin.CP"], "comment": "24 pages, 3 figures", "summary": "The proliferation of Large Language Models (LLMs) is challenged by\nhallucinations, critical failure modes where models generate non-factual,\nnonsensical or unfaithful text. This paper introduces Semantic Divergence\nMetrics (SDM), a novel lightweight framework for detecting Faithfulness\nHallucinations -- events of severe deviations of LLMs responses from input\ncontexts. We focus on a specific implementation of these LLM errors,\n{confabulations, defined as responses that are arbitrary and semantically\nmisaligned with the user's query. Existing methods like Semantic Entropy test\nfor arbitrariness by measuring the diversity of answers to a single, fixed\nprompt. Our SDM framework improves upon this by being more prompt-aware: we\ntest for a deeper form of arbitrariness by measuring response consistency not\nonly across multiple answers but also across multiple, semantically-equivalent\nparaphrases of the original prompt. Methodologically, our approach uses joint\nclustering on sentence embeddings to create a shared topic space for prompts\nand answers. A heatmap of topic co-occurances between prompts and responses can\nbe viewed as a quantified two-dimensional visualization of the user-machine\ndialogue. We then compute a suite of information-theoretic metrics to measure\nthe semantic divergence between prompts and responses. Our practical score,\n$\\mathcal{S}_H$, combines the Jensen-Shannon divergence and Wasserstein\ndistance to quantify this divergence, with a high score indicating a\nFaithfulness hallucination. Furthermore, we identify the KL divergence\nKL(Answer $||$ Prompt) as a powerful indicator of \\textbf{Semantic\nExploration}, a key signal for distinguishing different generative behaviors.\nThese metrics are further combined into the Semantic Box, a diagnostic\nframework for classifying LLM response types, including the dangerous,\nconfident confabulation.", "AI": {"tldr": "This paper presents a novel framework called Semantic Divergence Metrics (SDM) to detect Faithfulness Hallucinations in Large Language Models (LLMs), focusing on confabulations that deviate from the input context.", "motivation": "To address the challenge posed by hallucinations in LLMs, which generate responses that are non-factual or nonsensical.", "method": "The SDM framework employs joint clustering on sentence embeddings to analyze semantic consistency across multiple paraphrases of a prompt, using information-theoretic metrics to quantify semantic divergence.", "result": "SDM framework demonstrates improved detection of Faithfulness hallucinations by providing a more nuanced analysis of response consistency and semantic alignment.", "conclusion": "The proposed metrics and the Semantic Box framework enhance the understanding of LLM behaviors, particularly in identifying potentially dangerous response types like confident confabulations.", "key_contributions": ["Introduction of Semantic Divergence Metrics (SDM) for detecting Faithfulness Hallucinations.", "Joint clustering approach utilizing sentence embeddings for better prompt-response analysis.", "Development of the Semantic Box framework for classifying LLM response types."], "limitations": "The framework's effectiveness may vary across different types of LLMs and prompts, requiring further validation.", "keywords": ["Large Language Models", "Semantic Divergence", "Hallucinations", "Natural Language Processing", "Information Theory"], "importance_score": 9, "read_time_minutes": 24}}
{"id": "2508.10222", "pdf": "https://arxiv.org/pdf/2508.10222.pdf", "abs": "https://arxiv.org/abs/2508.10222", "title": "Understanding Textual Emotion Through Emoji Prediction", "authors": ["Ethan Gordon", "Nishank Kuppa", "Rigved Tummala", "Sriram Anasuri"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "comment": null, "summary": "This project explores emoji prediction from short text sequences using four\ndeep learning architectures: a feed-forward network, CNN, transformer, and\nBERT. Using the TweetEval dataset, we address class imbalance through focal\nloss and regularization techniques. Results show BERT achieves the highest\noverall performance due to its pre-training advantage, while CNN demonstrates\nsuperior efficacy on rare emoji classes. This research shows the importance of\narchitecture selection and hyperparameter tuning for sentiment-aware emoji\nprediction, contributing to improved human-computer interaction.", "AI": {"tldr": "This project investigates emoji prediction using various deep learning architectures and demonstrates the effectiveness of BERT and CNN in addressing class imbalances in emoji classification.", "motivation": "The research aims to improve human-computer interaction by enhancing emoji prediction accuracy in short text sequences, particularly for sentiment recognition.", "method": "The study employs a feed-forward network, CNN, transformer, and BERT architectures on the TweetEval dataset, utilizing focal loss and regularization techniques to manage class imbalance.", "result": "BERT outperforms other models in overall performance, while CNN excels in predicting rare emoji classes.", "conclusion": "The findings underscore the significance of selecting appropriate architectures and tuning hyperparameters for sentiment-aware emoji prediction, which can enhance user interactions with technology.", "key_contributions": ["Exploration of various deep learning architectures for emoji prediction", "Demonstration of BERT's performance advantage", "Identification of CNN's effectiveness for rare class prediction"], "limitations": "", "keywords": ["emoji prediction", "deep learning", "BERT", "CNN", "human-computer interaction"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2508.10226", "pdf": "https://arxiv.org/pdf/2508.10226.pdf", "abs": "https://arxiv.org/abs/2508.10226", "title": "Using Large Language Models to Measure Symptom Severity in Patients At Risk for Schizophrenia", "authors": ["Andrew X. Chen", "Guillermo Horga", "Sean Escola"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Patients who are at clinical high risk (CHR) for schizophrenia need close\nmonitoring of their symptoms to inform appropriate treatments. The Brief\nPsychiatric Rating Scale (BPRS) is a validated, commonly used research tool for\nmeasuring symptoms in patients with schizophrenia and other psychotic\ndisorders; however, it is not commonly used in clinical practice as it requires\na lengthy structured interview. Here, we utilize large language models (LLMs)\nto predict BPRS scores from clinical interview transcripts in 409 CHR patients\nfrom the Accelerating Medicines Partnership Schizophrenia (AMP-SCZ) cohort.\nDespite the interviews not being specifically structured to measure the BPRS,\nthe zero-shot performance of the LLM predictions compared to the true\nassessment (median concordance: 0.84, ICC: 0.73) approaches human inter- and\nintra-rater reliability. We further demonstrate that LLMs have substantial\npotential to improve and standardize the assessment of CHR patients via their\naccuracy in assessing the BPRS in foreign languages (median concordance: 0.88,\nICC: 0.70), and integrating longitudinal information in a one-shot or few-shot\nlearning approach.", "AI": {"tldr": "The paper explores using large language models (LLMs) to predict Brief Psychiatric Rating Scale (BPRS) scores from clinical interviews in patients at clinical high risk (CHR) for schizophrenia, demonstrating high accuracy and potential for improved monitoring.", "motivation": "There is a need for efficient and accurate tools to monitor symptoms in patients at clinical high risk for schizophrenia, as traditional methods are time-consuming and infrequently used in clinical practice.", "method": "LLMs were employed to predict BPRS scores from unstructured clinical interview transcripts of 409 CHR patients, without the need for tailored structured interviews.", "result": "The LLMs achieved a median concordance of 0.84 with the true BPRS assessment, indicating their predictions closely match human raters, with improved performance in assessing BPRS in multiple languages.", "conclusion": "LLMs show significant promise in enhancing and standardizing the assessment of CHR patients, making the process more efficient and potentially applicable across different languages and contexts.", "key_contributions": ["LLMs can accurately predict BPRS scores from unstructured clinical interviews.", "The study demonstrates the effectiveness of LLMs in both English and foreign languages.", "Longitudinal information can be effectively integrated into predictions using one-shot or few-shot learning approaches."], "limitations": "The study relies on the quality of transcripts and may not generalize to all clinical settings or patient populations.", "keywords": ["schizophrenia", "LLMs", "BPRS", "clinical assessment", "health informatics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.10246", "pdf": "https://arxiv.org/pdf/2508.10246.pdf", "abs": "https://arxiv.org/abs/2508.10246", "title": "A Computational Approach to Analyzing Language Change and Variation in the Constructed Language Toki Pona", "authors": ["Daniel Huang", "Hyoun-A Joo"], "categories": ["cs.CL"], "comment": "14 pages, 14 figures. submitted to UGA Working Papers in Linguistics\n  2025", "summary": "This study explores language change and variation in Toki Pona, a constructed\nlanguage with approximately 120 core words. Taking a computational and\ncorpus-based approach, the study examines features including fluid word classes\nand transitivity in order to examine (1) changes in preferences of content\nwords for different syntactic positions over time and (2) variation in usage\nacross different corpora. The results suggest that sociolinguistic factors\ninfluence Toki Pona in the same way as natural languages, and that even\nconstructed linguistic systems naturally evolve as communities use them.", "AI": {"tldr": "This study examines the evolution and variation of the constructed language Toki Pona through a computational approach, focusing on word usage and syntactic changes.", "motivation": "To explore language change and variation in constructed languages, specifically Toki Pona, and understand how sociolinguistic factors impact their evolution.", "method": "A computational and corpus-based approach analyzing features such as fluid word classes and transitivity.", "result": "Time-based changes in content word preferences for syntactic positions and variations across different corpora were identified, indicating sociolinguistic influences.", "conclusion": "Constructed languages like Toki Pona evolve similarly to natural languages, influenced by community usage over time.", "key_contributions": ["Insights into how constructed languages can evolve like natural languages", "Identification of sociolinguistic factors influencing language change", "Analysis of word class fluidity and transitivity in Toki Pona"], "limitations": "", "keywords": ["Toki Pona", "language change", "sociolinguistics", "constructed languages", "corpus-based analysis"], "importance_score": 2, "read_time_minutes": 14}}
{"id": "2508.10295", "pdf": "https://arxiv.org/pdf/2508.10295.pdf", "abs": "https://arxiv.org/abs/2508.10295", "title": "Inductive Bias Extraction and Matching for LLM Prompts", "authors": ["Christian M. Angel", "Francis Ferraro"], "categories": ["cs.CL"], "comment": null, "summary": "The active research topic of prompt engineering makes it evident that LLMs\nare sensitive to small changes in prompt wording. A portion of this can be\nascribed to the inductive bias that is present in the LLM. By using an LLM's\noutput as a portion of its prompt, we can more easily create satisfactory\nwording for prompts. This has the effect of creating a prompt that matches the\ninductive bias in model. Empirically, we show that using this Inductive Bias\nExtraction and Matching strategy improves LLM Likert ratings used for\nclassification by up to 19% and LLM Likert ratings used for ranking by up to\n27%.", "AI": {"tldr": "This paper discusses how Inductive Bias Extraction and Matching improves prompt engineering for LLMs, enhancing classification and ranking performance.", "motivation": "To address the sensitivity of LLMs to prompt wording by leveraging their inductive biases.", "method": "The authors propose a method where the output of an LLM is used as part of the prompt to create more satisfying prompt wordings.", "result": "The proposed strategy improves LLM Likert ratings for classification by up to 19% and for ranking by up to 27%.", "conclusion": "Inductive Bias Extraction and Matching can significantly optimize prompt formulations for better LLM performance.", "key_contributions": ["Introduction of Inductive Bias Extraction and Matching strategy", "Empirical validation of improved performance metrics", "Insights into LLM sensitivity and prompt engineering"], "limitations": "", "keywords": ["prompt engineering", "inductive bias", "LLM", "classification", "ranking"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.10304", "pdf": "https://arxiv.org/pdf/2508.10304.pdf", "abs": "https://arxiv.org/abs/2508.10304", "title": "Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race", "authors": ["Gustavo Bonil", "Simone Hashiguti", "Jhessica Silva", "João Gondim", "Helena Maia", "Nádia Silva", "Helio Pedrini", "Sandra Avila"], "categories": ["cs.CL", "cs.AI"], "comment": "29 pages, 3 figures", "summary": "With the advance of Artificial Intelligence (AI), Large Language Models\n(LLMs) have gained prominence and been applied in diverse contexts. As they\nevolve into more sophisticated versions, it is essential to assess whether they\nreproduce biases, such as discrimination and racialization, while maintaining\nhegemonic discourses. Current bias detection approaches rely mostly on\nquantitative, automated methods, which often overlook the nuanced ways in which\nbiases emerge in natural language. This study proposes a qualitative,\ndiscursive framework to complement such methods. Through manual analysis of\nLLM-generated short stories featuring Black and white women, we investigate\ngender and racial biases. We contend that qualitative methods such as the one\nproposed here are fundamental to help both developers and users identify the\nprecise ways in which biases manifest in LLM outputs, thus enabling better\nconditions to mitigate them. Results show that Black women are portrayed as\ntied to ancestry and resistance, while white women appear in self-discovery\nprocesses. These patterns reflect how language models replicate crystalized\ndiscursive representations, reinforcing essentialization and a sense of social\nimmobility. When prompted to correct biases, models offered superficial\nrevisions that maintained problematic meanings, revealing limitations in\nfostering inclusive narratives. Our results demonstrate the ideological\nfunctioning of algorithms and have significant implications for the ethical use\nand development of AI. The study reinforces the need for critical,\ninterdisciplinary approaches to AI design and deployment, addressing how\nLLM-generated discourses reflect and perpetuate inequalities.", "AI": {"tldr": "This study presents a qualitative framework for detecting biases in Large Language Models (LLMs) through the analysis of their outputs, specifically focusing on gender and racial biases in narratives featuring Black and white women.", "motivation": "To assess whether LLMs reproduce biases such as discrimination while relying largely on quantitative methods that overlook nuances in natural language.", "method": "A qualitative, discursive framework is proposed, involving a manual analysis of LLM-generated short stories.", "result": "Analysis reveals that Black women are often portrayed in relation to ancestry and resistance, while white women are depicted in self-discovery processes, indicating the reinforcement of essentialized representations and social immobility.", "conclusion": "Qualitative methods are crucial for identifying biases in LLM outputs, revealing how algorithms reflect and perpetuate inequalities, and highlighting the need for interdisciplinary approaches in AI design.", "key_contributions": ["Proposes a qualitative framework to complement existing quantitative bias detection methods.", "Illustrates the biased portrayals of gender and race in LLM-generated narratives.", "Demonstrates the ideological functioning of algorithms in perpetuating social inequalities."], "limitations": "Focuses only on the analysis of short stories, which may not represent all LLM-generated content; results may vary with different prompts or contexts.", "keywords": ["Large Language Models", "Bias Detection", "Qualitative Analysis", "Interdisciplinary Approaches", "AI Ethics"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2508.10308", "pdf": "https://arxiv.org/pdf/2508.10308.pdf", "abs": "https://arxiv.org/abs/2508.10308", "title": "ReviewRL: Towards Automated Scientific Review with RL", "authors": ["Sihang Zeng", "Kai Tian", "Kaiyan Zhang", "Yuru wang", "Junqi Gao", "Runze Liu", "Sa Yang", "Jingxuan Li", "Xinwei Long", "Jiaheng Ma", "Biqing Qi", "Bowen Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 5 figures", "summary": "Peer review is essential for scientific progress but faces growing challenges\ndue to increasing submission volumes and reviewer fatigue. Existing automated\nreview approaches struggle with factual accuracy, rating consistency, and\nanalytical depth, often generating superficial or generic feedback lacking the\ninsights characteristic of high-quality human reviews. We introduce ReviewRL, a\nreinforcement learning framework for generating comprehensive and factually\ngrounded scientific paper reviews. Our approach combines: (1) an ArXiv-MCP\nretrieval-augmented context generation pipeline that incorporates relevant\nscientific literature, (2) supervised fine-tuning that establishes foundational\nreviewing capabilities, and (3) a reinforcement learning procedure with a\ncomposite reward function that jointly enhances review quality and rating\naccuracy. Experiments on ICLR 2025 papers demonstrate that ReviewRL\nsignificantly outperforms existing methods across both rule-based metrics and\nmodel-based quality assessments. ReviewRL establishes a foundational framework\nfor RL-driven automatic critique generation in scientific discovery,\ndemonstrating promising potential for future development in this domain. The\nimplementation of ReviewRL will be released at GitHub.", "AI": {"tldr": "ReviewRL is a reinforcement learning framework designed to generate comprehensive and factually grounded reviews of scientific papers, outperforming existing automated review methods.", "motivation": "To address the challenges of increasing submission volumes and reviewer fatigue in scientific peer review processes, which can lead to superficial feedback.", "method": "ReviewRL integrates a retrieval-augmented context generation pipeline with supervised fine-tuning and a reinforcement learning procedure utilizing a composite reward function to enhance review quality and accuracy.", "result": "ReviewRL significantly outperforms existing automated review methods based on both rule-based and model-based assessments in experiments with ICLR 2025 papers.", "conclusion": "ReviewRL establishes a novel framework for automatic critique generation in scientific discovery, with substantial potential for future enhancements in the field.", "key_contributions": ["Introduction of ReviewRL as a novel framework for automated scientific reviews", "Combination of retrieval-augmented context with reinforcement learning", "Demonstrated superior performance over existing review methods"], "limitations": "", "keywords": ["reinforcement learning", "peer review", "automated critique generation"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2508.10311", "pdf": "https://arxiv.org/pdf/2508.10311.pdf", "abs": "https://arxiv.org/abs/2508.10311", "title": "From Surface to Semantics: Semantic Structure Parsing for Table-Centric Document Analysis", "authors": ["Xuan Li", "Jialiang Dong", "Raymond Wong"], "categories": ["cs.CL", "68T50", "I.7.5"], "comment": "8 pages, 5 figures, 28th European Conference on Artificial\n  Intelligence (ECAI-2025)", "summary": "Documents are core carriers of information and knowl-edge, with broad\napplications in finance, healthcare, and scientific research. Tables, as the\nmain medium for structured data, encapsulate key information and are among the\nmost critical document components. Existing studies largely focus on\nsurface-level tasks such as layout analysis, table detection, and data\nextraction, lacking deep semantic parsing of tables and their contextual\nassociations. This limits advanced tasks like cross-paragraph data\ninterpretation and context-consistent analysis. To address this, we propose\nDOTABLER, a table-centric semantic document parsing framework designed to\nuncover deep semantic links between tables and their context. DOTABLER\nleverages a custom dataset and domain-specific fine-tuning of pre-trained\nmodels, integrating a complete parsing pipeline to identify context segments\nsemantically tied to tables. Built on this semantic understanding, DOTABLER\nimplements two core functionalities: table-centric document structure parsing\nand domain-specific table retrieval, delivering comprehensive table-anchored\nsemantic analysis and precise extraction of semantically relevant tables.\nEvaluated on nearly 4,000 pages with over 1,000 tables from real-world PDFs,\nDOTABLER achieves over 90% Precision and F1 scores, demonstrating superior\nperformance in table-context semantic analysis and deep document parsing\ncompared to advanced models such as GPT-4o.", "AI": {"tldr": "The paper presents DOTABLER, a framework for deep semantic parsing of tables within documents, addressing limitations of existing methods focused on surface-level analysis.", "motivation": "Existing studies often overlook deep semantic parsing of tables and their contextual relationships, which is crucial for advanced data interpretation and consistent analysis.", "method": "DOTABLER uses a custom dataset and domain-specific fine-tuning of pre-trained models. It integrates a parsing pipeline to identify semantically relevant context segments relative to tables and implements functionalities for table-centric document structure parsing and table retrieval.", "result": "Evaluated on nearly 4,000 pages and over 1,000 tables from real-world PDFs, DOTABLER achieves over 90% Precision and F1 scores.", "conclusion": "DOTABLER significantly enhances table-context semantic analysis and deep document parsing compared to state-of-the-art models like GPT-4o.", "key_contributions": ["Introduction of DOTABLER framework for semantic document parsing focused on tables.", "Demonstration of superior performance in table-context analysis compared to existing models.", "Implementation of a comprehensive parsing pipeline integrating semantic understanding."], "limitations": "", "keywords": ["table parsing", "semantic analysis", "document structure", "domain-specific retrieval", "context analysis"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2508.10312", "pdf": "https://arxiv.org/pdf/2508.10312.pdf", "abs": "https://arxiv.org/abs/2508.10312", "title": "Beyond Semantic Understanding: Preserving Collaborative Frequency Components in LLM-based Recommendation", "authors": ["Minhao Wang", "Yunhang He", "Cong Xu", "Zhangchi Zhu", "Wei Zhang"], "categories": ["cs.CL"], "comment": "12 pages, 8 figures", "summary": "Recommender systems in concert with Large Language Models (LLMs) present\npromising avenues for generating semantically-informed recommendations.\nHowever, LLM-based recommenders exhibit a tendency to overemphasize semantic\ncorrelations within users' interaction history. When taking pretrained\ncollaborative ID embeddings as input, LLM-based recommenders progressively\nweaken the inherent collaborative signals as the embeddings propagate through\nLLM backbones layer by layer, as opposed to traditional Transformer-based\nsequential models in which collaborative signals are typically preserved or\neven enhanced for state-of-the-art performance. To address this limitation, we\nintroduce FreLLM4Rec, an approach designed to balance semantic and\ncollaborative information from a spectral perspective. Item embeddings that\nincorporate both semantic and collaborative information are first purified\nusing a Global Graph Low-Pass Filter (G-LPF) to preliminarily remove irrelevant\nhigh-frequency noise. Temporal Frequency Modulation (TFM) then actively\npreserves collaborative signal layer by layer. Note that the collaborative\npreservation capability of TFM is theoretically guaranteed by establishing a\nconnection between the optimal but hard-to-implement local graph fourier\nfilters and the suboptimal yet computationally efficient frequency-domain\nfilters. Extensive experiments on four benchmark datasets demonstrate that\nFreLLM4Rec successfully mitigates collaborative signal attenuation and achieves\ncompetitive performance, with improvements of up to 8.00\\% in NDCG@10 over the\nbest baseline. Our findings provide insights into how LLMs process\ncollaborative information and offer a principled approach for improving\nLLM-based recommendation systems.", "AI": {"tldr": "FreLLM4Rec is proposed to balance semantic and collaborative signals in LLM-based recommenders, addressing the problem of collaborative signal attenuation.", "motivation": "LLM-based recommenders often weaken collaborative signals from user interaction history, leading to sub-optimal performance compared to traditional sequential models.", "method": "The approach uses a Global Graph Low-Pass Filter to purify item embeddings and a Temporal Frequency Modulation to preserve collaborative signals layer by layer while embedding propagation occurs through LLM backbones.", "result": "FreLLM4Rec shows improvements of up to 8.00% in NDCG@10 on four benchmark datasets, indicating better preservation of collaborative signals and competitive performance against existing baselines.", "conclusion": "The paper demonstrates the efficacy of FreLLM4Rec in addressing collaborative signal attenuation in LLM-based recommenders and provides theoretical insights into collaborative information processing in LLMs.", "key_contributions": ["Introduction of FreLLM4Rec for balancing semantic and collaborative signals in recommendation systems.", "Utilization of G-LPF for purifying embeddings and TFM for preserving collaborative signals during LLM processing.", "Extensive empirical validation on benchmark datasets showing significant performance improvements."], "limitations": "The theoretical aspects of the optimal local graph fourier filters may be challenging to implement in practice, limiting the scope of the proposed methods' applicability.", "keywords": ["Recommender Systems", "Large Language Models", "Collaborative Signals", "Machine Learning", "Temporal Frequency Modulation"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2508.10352", "pdf": "https://arxiv.org/pdf/2508.10352.pdf", "abs": "https://arxiv.org/abs/2508.10352", "title": "Cross-Prompt Encoder for Low-Performing Languages", "authors": ["Beso Mikaberidze", "Teimuraz Saghinadze", "Simon Ostermann", "Philipp Muller"], "categories": ["cs.CL"], "comment": null, "summary": "Soft prompts have emerged as a powerful alternative to adapters in\nparameter-efficient fine-tuning (PEFT), enabling large language models (LLMs)\nto adapt to downstream tasks without architectural changes or parameter\nupdates. While prior work has focused on stabilizing training via parameter\ninteraction in small neural prompt encoders, their broader potential for\ntransfer across languages remains unexplored. In this paper, we demonstrate\nthat a prompt encoder can play a central role in improving performance on\nlow-performing languages-those that achieve poor accuracy even under full-model\nfine-tuning. We introduce the Cross-Prompt Encoder (XPE), which combines a\nlightweight encoding architecture with multi-source training on typologically\ndiverse languages - a design that enables the model to capture abstract and\ntransferable patterns across languages. To complement XPE, we propose a Dual\nSoft Prompt mechanism that combines an encoder-based prompt with a directly\ntrained standard soft prompt. This hybrid design proves especially effective\nfor target languages that benefit from both broadly shared structure and\nlanguage-specific alignment. Experiments on the SIB-200 benchmark reveal a\nconsistent trade-off: XPE is most effective for low-performing languages, while\nhybrid variants offer broader adaptability across multilingual settings.", "AI": {"tldr": "This paper introduces the Cross-Prompt Encoder (XPE) and a Dual Soft Prompt mechanism to improve the performance of large language models on low-performing languages by leveraging multi-source training and hybrid prompt designs.", "motivation": "The paper addresses the unexplored potential of prompt encoders in improving language model performance on low-performing languages, which traditional fine-tuning falls short of.", "method": "The authors introduce the Cross-Prompt Encoder (XPE) and a Dual Soft Prompt mechanism, which combines multi-source training on diverse languages with a hybrid prompt approach for improved model adaptability.", "result": "Experiments demonstrate that XPE effectively enhances accuracy on low-performing languages, while hybrid prompt variants improve performance across multilingual settings.", "conclusion": "The findings show that prompt encoders can significantly bolster performance in tasks involving low-resourced languages, suggesting a promising avenue for further research in multilingual model training.", "key_contributions": ["Introduction of the Cross-Prompt Encoder (XPE)", "Development of the Dual Soft Prompt mechanism", "Demonstration of effectiveness in low-performing languages through multi-source training"], "limitations": "The effectiveness of the proposed methods in other architectures or tasks beyond the SIB-200 benchmark is not yet established.", "keywords": ["Cross-Prompt Encoder", "Dual Soft Prompt", "multilingual performance", "parameter-efficient fine-tuning", "low-performing languages"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.10355", "pdf": "https://arxiv.org/pdf/2508.10355.pdf", "abs": "https://arxiv.org/abs/2508.10355", "title": "Making Qwen3 Think in Korean with Reinforcement Learning", "authors": ["Jungyup Lee", "Jemin Kim", "Sang Park", "SeungJae Lee"], "categories": ["cs.CL"], "comment": null, "summary": "We present a two-stage fine-tuning approach to make the large language model\nQwen3 14B \"think\" natively in Korean. In the first stage, supervised\nfine-tuning (SFT) on a high-quality Korean reasoning dataset establishes a\nstrong foundation in Korean logical reasoning, yielding notable improvements in\nKorean-language tasks and even some gains in general reasoning ability. In the\nsecond stage, we employ reinforcement learning with a customized Group Relative\nPolicy Optimization (GRPO) algorithm to further enhance both Korean reasoning\nalignment and overall problem-solving performance. We address critical\nstability challenges in GRPO training - such as reward hacking and policy\ncollapse - by introducing an oracle judge model that calibrates the reward\nsignal. Our approach achieves stable learning (avoiding the collapse observed\nin naive GRPO) and leads to steady, incremental performance gains. The final\nRL-tuned model demonstrates substantially improved results on advanced\nreasoning benchmarks (particularly math and coding tasks) while maintaining\nknowledge and language proficiency, successfully conducting its internal\nchain-of-thought entirely in Korean.", "AI": {"tldr": "A two-stage fine-tuning method enhances a large language model's Korean reasoning using supervised fine-tuning and reinforcement learning, leading to substantial performance improvements.", "motivation": "To improve the performance of the Qwen3 14B model in Korean reasoning tasks, leveraging both supervised and reinforcement learning techniques.", "method": "The approach consists of two stages: first, supervised fine-tuning (SFT) on a Korean reasoning dataset, followed by reinforcement learning using a customized Group Relative Policy Optimization (GRPO) to enhance reasoning alignment and problem-solving.", "result": "The final model achieves significant improvements on advanced reasoning tasks, particularly in math and coding, while maintaining proficiency in knowledge and language.", "conclusion": "The two-stage method effectively improves the Qwen3 model's performance in Korean language tasks, addressing stability issues in GRPO training through an oracle judge model.", "key_contributions": ["Introduction of a two-stage fine-tuning approach for Korean reasoning tasks.", "Development of an oracle judge model to stabilize GRPO training.", "Demonstrated improvements on complex reasoning benchmarks in Korean."], "limitations": "", "keywords": ["Korean", "language model", "reinforcement learning", "supervised fine-tuning", "Group Relative Policy Optimization"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2508.10366", "pdf": "https://arxiv.org/pdf/2508.10366.pdf", "abs": "https://arxiv.org/abs/2508.10366", "title": "Advancing Cross-lingual Aspect-Based Sentiment Analysis with LLMs and Constrained Decoding for Sequence-to-Sequence Models", "authors": ["Jakub Šmíd", "Pavel Přibáň", "Pavel Král"], "categories": ["cs.CL"], "comment": "Published in Proceedings of the 17th International Conference on\n  Agents and Artificial Intelligence - Volume 2 (ICAART 2025). Official\n  version: https://www.scitepress.org/Link.aspx?doi=10.5220/0013349400003890", "summary": "Aspect-based sentiment analysis (ABSA) has made significant strides, yet\nchallenges remain for low-resource languages due to the predominant focus on\nEnglish. Current cross-lingual ABSA studies often centre on simpler tasks and\nrely heavily on external translation tools. In this paper, we present a novel\nsequence-to-sequence method for compound ABSA tasks that eliminates the need\nfor such tools. Our approach, which uses constrained decoding, improves\ncross-lingual ABSA performance by up to 10\\%. This method broadens the scope of\ncross-lingual ABSA, enabling it to handle more complex tasks and providing a\npractical, efficient alternative to translation-dependent techniques.\nFurthermore, we compare our approach with large language models (LLMs) and show\nthat while fine-tuned multilingual LLMs can achieve comparable results,\nEnglish-centric LLMs struggle with these tasks.", "AI": {"tldr": "This paper introduces a novel sequence-to-sequence method for compound aspect-based sentiment analysis (ABSA) that improves performance in low-resource languages without relying on external translation tools.", "motivation": "To address the challenges faced by low-resource languages in aspect-based sentiment analysis (ABSA) due to a predominant focus on English and reliance on translation tools.", "method": "A sequence-to-sequence approach utilizing constrained decoding to enhance cross-lingual ABSA performance without external translation tools.", "result": "Improves cross-lingual ABSA performance by up to 10% and enables handling of more complex tasks.", "conclusion": "The proposed method presents a practical alternative to translation-dependent techniques and shows that English-centric large language models struggle more in these tasks compared to fine-tuned multilingual LLMs.", "key_contributions": ["Novel sequence-to-sequence method for compound ABSA", "Improvement of performance in low-resource languages", "Comparison of results with large language models"], "limitations": "", "keywords": ["sentiment analysis", "cross-lingual", "sequence-to-sequence", "low-resource languages", "natural language processing"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.10368", "pdf": "https://arxiv.org/pdf/2508.10368.pdf", "abs": "https://arxiv.org/abs/2508.10368", "title": "Large Language Models for Summarizing Czech Historical Documents and Beyond", "authors": ["Václav Tran", "Jakub Šmíd", "Jiří Martínek", "Ladislav Lenc", "Pavel Král"], "categories": ["cs.CL"], "comment": "Published in Proceedings of the 17th International Conference on\n  Agents and Artificial Intelligence - Volume 2 (ICAART 2025). Official\n  version: https://www.scitepress.org/Link.aspx?doi=10.5220/0013374100003890", "summary": "Text summarization is the task of shortening a larger body of text into a\nconcise version while retaining its essential meaning and key information.\nWhile summarization has been significantly explored in English and other\nhigh-resource languages, Czech text summarization, particularly for historical\ndocuments, remains underexplored due to linguistic complexities and a scarcity\nof annotated datasets. Large language models such as Mistral and mT5 have\ndemonstrated excellent results on many natural language processing tasks and\nlanguages. Therefore, we employ these models for Czech summarization, resulting\nin two key contributions: (1) achieving new state-of-the-art results on the\nmodern Czech summarization dataset SumeCzech using these advanced models, and\n(2) introducing a novel dataset called Posel od \\v{C}erchova for summarization\nof historical Czech documents with baseline results. Together, these\ncontributions provide a great potential for advancing Czech text summarization\nand open new avenues for research in Czech historical text processing.", "AI": {"tldr": "This paper explores Czech text summarization using large language models, achieving state-of-the-art results and introducing a new dataset for historical documents.", "motivation": "Czech text summarization is underexplored due to linguistic complexities and lack of datasets, necessitating research in this area.", "method": "The paper employs large language models, specifically Mistral and mT5, to perform text summarization on Czech datasets.", "result": "Achieves new state-of-the-art results on the modern Czech summarization dataset SumeCzech and introduces a novel dataset for summarization of historical documents.", "conclusion": "These contributions advance the field of Czech text summarization and encourage further research in processing historical Czech texts.", "key_contributions": ["State-of-the-art results on the SumeCzech dataset using advanced language models.", "Introduction of the Posel od Čerchova dataset for summarizing historical Czech documents."], "limitations": "", "keywords": ["Czech text summarization", "large language models", "historical documents", "natural language processing", "dataset"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.10369", "pdf": "https://arxiv.org/pdf/2508.10369.pdf", "abs": "https://arxiv.org/abs/2508.10369", "title": "Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with Constrained Decoding", "authors": ["Jakub Šmíd", "Pavel Přibáň", "Pavel Král"], "categories": ["cs.CL"], "comment": null, "summary": "While aspect-based sentiment analysis (ABSA) has made substantial progress,\nchallenges remain for low-resource languages, which are often overlooked in\nfavour of English. Current cross-lingual ABSA approaches focus on limited, less\ncomplex tasks and often rely on external translation tools. This paper\nintroduces a novel approach using constrained decoding with\nsequence-to-sequence models, eliminating the need for unreliable translation\ntools and improving cross-lingual performance by 5\\% on average for the most\ncomplex task. The proposed method also supports multi-tasking, which enables\nsolving multiple ABSA tasks with a single model, with constrained decoding\nboosting results by more than 10\\%.\n  We evaluate our approach across seven languages and six ABSA tasks,\nsurpassing state-of-the-art methods and setting new benchmarks for previously\nunexplored tasks. Additionally, we assess large language models (LLMs) in\nzero-shot, few-shot, and fine-tuning scenarios. While LLMs perform poorly in\nzero-shot and few-shot settings, fine-tuning achieves competitive results\ncompared to smaller multilingual models, albeit at the cost of longer training\nand inference times.\n  We provide practical recommendations for real-world applications, enhancing\nthe understanding of cross-lingual ABSA methodologies. This study offers\nvaluable insights into the strengths and limitations of cross-lingual ABSA\napproaches, advancing the state-of-the-art in this challenging research domain.", "AI": {"tldr": "This paper introduces a new approach for aspect-based sentiment analysis (ABSA) in low-resource languages using constrained decoding with sequence-to-sequence models, improving cross-lingual performance and supporting multi-tasking.", "motivation": "Address challenges in aspect-based sentiment analysis (ABSA) for low-resource languages, which are often neglected compared to English.", "method": "Utilizes constrained decoding techniques with sequence-to-sequence models to improve performance across multiple ABSA tasks without relying on external translation tools.", "result": "Achieves a 5% average improvement in cross-lingual performance for the most complex task, with over 10% enhancement in multi-task scenarios; surpasses state-of-the-art methods across seven languages and six ABSA tasks.", "conclusion": "Provides insights and recommendations for practical applications in cross-lingual ABSA and highlights the performance differences of LLMs under various training conditions.", "key_contributions": ["Introduces constrained decoding for cross-lingual ABSA", "Demonstrates multi-tasking capability in ABSA", "Sets new benchmarks for previously unexplored ABSA tasks"], "limitations": "LLMs perform poorly in zero-shot and few-shot settings, with fine-tuning requiring longer training and inference times.", "keywords": ["Aspect-based sentiment analysis", "Cross-lingual", "Machine translation", "Large language models", "Multi-task learning"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2508.10390", "pdf": "https://arxiv.org/pdf/2508.10390.pdf", "abs": "https://arxiv.org/abs/2508.10390", "title": "Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts", "authors": ["Chiyu Zhang", "Lu Zhou", "Xiaogang Xu", "Jiafei Wu", "Liming Fang", "Zhe Liu"], "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Evaluating jailbreak attacks is challenging when prompts are not overtly\nharmful or fail to induce harmful outputs. Unfortunately, many existing\nred-teaming datasets contain such unsuitable prompts. To evaluate attacks\naccurately, these datasets need to be assessed and cleaned for maliciousness.\nHowever, existing malicious content detection methods rely on either manual\nannotation, which is labor-intensive, or large language models (LLMs), which\nhave inconsistent accuracy in harmful types. To balance accuracy and\nefficiency, we propose a hybrid evaluation framework named MDH (Malicious\ncontent Detection based on LLMs with Human assistance) that combines LLM-based\nannotation with minimal human oversight, and apply it to dataset cleaning and\ndetection of jailbroken responses. Furthermore, we find that well-crafted\ndeveloper messages can significantly boost jailbreak success, leading us to\npropose two new strategies: D-Attack, which leverages context simulation, and\nDH-CoT, which incorporates hijacked chains of thought. The Codes, datasets,\njudgements, and detection results will be released in github repository:\nhttps://github.com/AlienZhang1996/DH-CoT.", "AI": {"tldr": "The paper proposes a hybrid framework for evaluating jailbreak attacks on language models, using a combination of LLMs and minimal human oversight to clean datasets and enhance detection.", "motivation": "Evaluating jailbreak attacks is complicated by the presence of unsuitable prompts in existing datasets, which necessitates accurate assessment and cleaning.", "method": "The authors propose the MDH framework, which combines LLM-based annotation with minimal human oversight, and introduce two strategies for enhancing jailbreak success: D-Attack and DH-CoT.", "result": "The MDH framework improves efficiency and accuracy in malicious content detection, and the new strategies enhance jailbreak effectiveness by utilizing context simulation and hijacked chains of thought.", "conclusion": "The findings emphasize the importance of assessing developer messages and the need for improved malicious content detection methods in evaluating LLM vulnerabilities.", "key_contributions": ["Introduction of the MDH framework for malicious content detection", "Presentation of D-Attack and DH-CoT strategies to enhance jailbreak success", "Release of codes, datasets, and detection results for community use"], "limitations": "", "keywords": ["jailbreak attacks", "malicious content detection", "language models", "dataset cleaning", "hybrid framework"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.10404", "pdf": "https://arxiv.org/pdf/2508.10404.pdf", "abs": "https://arxiv.org/abs/2508.10404", "title": "Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation", "authors": ["Huizhen Shu", "Xuying Li", "Qirui Wang", "Yuji Kosuga", "Mengqiu Tian", "Zhuo Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the rapid proliferation of Natural Language Processing (NLP), especially\nLarge Language Models (LLMs), generating adversarial examples to jailbreak LLMs\nremains a key challenge for understanding model vulnerabilities and improving\nrobustness. In this context, we propose a new black-box attack method that\nleverages the interpretability of large models. We introduce the Sparse Feature\nPerturbation Framework (SFPF), a novel approach for adversarial text generation\nthat utilizes sparse autoencoders to identify and manipulate critical features\nin text. After using the SAE model to reconstruct hidden layer representations,\nwe perform feature clustering on the successfully attacked texts to identify\nfeatures with higher activations. These highly activated features are then\nperturbed to generate new adversarial texts. This selective perturbation\npreserves the malicious intent while amplifying safety signals, thereby\nincreasing their potential to evade existing defenses. Our method enables a new\nred-teaming strategy that balances adversarial effectiveness with safety\nalignment. Experimental results demonstrate that adversarial texts generated by\nSFPF can bypass state-of-the-art defense mechanisms, revealing persistent\nvulnerabilities in current NLP systems.However, the method's effectiveness\nvaries across prompts and layers, and its generalizability to other\narchitectures and larger models remains to be validated.", "AI": {"tldr": "This paper introduces the Sparse Feature Perturbation Framework (SFPF), a novel black-box attack method for generating adversarial texts using sparse autoencoders, significantly bypassing existing defenses in NLP systems.", "motivation": "To address the challenge of generating adversarial examples for large language models (LLMs) to improve their robustness and understand vulnerabilities.", "method": "The method involves using sparse autoencoders to reconstruct hidden layer representations and perform feature clustering on attacked texts, perturbing highly activated features to create new adversarial examples.", "result": "Experimental results show that adversarial texts from SFPF can successfully evade state-of-the-art defense mechanisms in NLP systems, exposing ongoing vulnerabilities.", "conclusion": "The proposed SFPF method provides a new red-teaming approach that balances adversarial effectiveness with safety alignment, though its effectiveness is influenced by factors such as prompts and model architecture.", "key_contributions": ["Introduction of the Sparse Feature Perturbation Framework (SFPF) for adversarial text generation", "Utilization of sparse autoencoders for feature manipulation in adversarial examples", "Demonstration of successful evasion of state-of-the-art defense mechanisms in NLP"], "limitations": "The method's effectiveness varies by prompts and layers; its generalizability to other architectures and larger models is not yet validated.", "keywords": ["Natural Language Processing", "Large Language Models", "Adversarial Examples", "Sparse Autoencoders", "Text Generation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.10419", "pdf": "https://arxiv.org/pdf/2508.10419.pdf", "abs": "https://arxiv.org/abs/2508.10419", "title": "ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning", "authors": ["Juyuan Wang", "Rongchen Zhao", "Wei Wei", "Yufeng Wang", "Mo Yu", "Jie Zhou", "Jin Xu", "Liyan Xu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Narrative comprehension on long stories and novels has been a challenging\ndomain attributed to their intricate plotlines and entangled, often evolving\nrelations among characters and entities. Given the LLM's diminished reasoning\nover extended context and high computational cost, retrieval-based approaches\nremain a pivotal role in practice. However, traditional RAG methods can fall\nshort due to their stateless, single-step retrieval process, which often\noverlooks the dynamic nature of capturing interconnected relations within\nlong-range context. In this work, we propose ComoRAG, holding the principle\nthat narrative reasoning is not a one-shot process, but a dynamic, evolving\ninterplay between new evidence acquisition and past knowledge consolidation,\nanalogous to human cognition when reasoning with memory-related signals in the\nbrain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes\niterative reasoning cycles while interacting with a dynamic memory workspace.\nIn each cycle, it generates probing queries to devise new exploratory paths,\nthen integrates the retrieved evidence of new aspects into a global memory\npool, thereby supporting the emergence of a coherent context for the query\nresolution. Across four challenging long-context narrative benchmarks (200K+\ntokens), ComoRAG outperforms strong RAG baselines with consistent relative\ngains up to 11% compared to the strongest baseline. Further analysis reveals\nthat ComoRAG is particularly advantageous for complex queries requiring global\ncomprehension, offering a principled, cognitively motivated paradigm for\nretrieval-based long context comprehension towards stateful reasoning. Our code\nis publicly released at https://github.com/EternityJune25/ComoRAG", "AI": {"tldr": "ComoRAG is a dynamic retrieval-based approach for narrative comprehension, improving long-context reasoning in LLMs by cycling through memory and integrating new evidence.", "motivation": "Address the challenges of narrative comprehension in long stories due to intricate plots and relationships, where traditional stateless retrieval methods struggle with dynamic context.", "method": "ComoRAG employs iterative reasoning cycles that interact with a dynamic memory workspace to generate probing queries and integrate new evidence into a global memory pool.", "result": "ComoRAG outperforms strong retrieval-based models in four long-context benchmarks, achieving consistent gains up to 11% compared to the strongest baseline.", "conclusion": "ComoRAG offers a cognitively motivated paradigm for improving retrieval-based comprehension in complex queries, emphasizing the need for stateful reasoning processes.", "key_contributions": ["Introduction of ComoRAG for narrative comprehension", "Demonstration of iterative reasoning cycles in retrieval-based models", "Significant performance improvements on long-context benchmarks"], "limitations": "", "keywords": ["narrative comprehension", "retrieval-augmented generation", "dynamic memory", "long-context", "iterative reasoning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.10421", "pdf": "https://arxiv.org/pdf/2508.10421.pdf", "abs": "https://arxiv.org/abs/2508.10421", "title": "Evaluating LLMs on Chinese Idiom Translation", "authors": ["Cai Yang", "Yao Dou", "David Heineman", "Xiaofeng Wu", "Wei Xu"], "categories": ["cs.CL"], "comment": "Accepted at COLM 2025", "summary": "Idioms, whose figurative meanings usually differ from their literal\ninterpretations, are common in everyday language, especially in Chinese, where\nthey often contain historical references and follow specific structural\npatterns. Despite recent progress in machine translation with large language\nmodels, little is known about Chinese idiom translation. In this work, we\nintroduce IdiomEval, a framework with a comprehensive error taxonomy for\nChinese idiom translation. We annotate 900 translation pairs from nine modern\nsystems, including GPT-4o and Google Translate, across four domains: web, news,\nWikipedia, and social media. We find these systems fail at idiom translation,\nproducing incorrect, literal, partial, or even missing translations. The\nbest-performing system, GPT-4, makes errors in 28% of cases. We also find that\nexisting evaluation metrics measure idiom quality poorly with Pearson\ncorrelation below 0.48 with human ratings. We thus develop improved models that\nachieve F$_1$ scores of 0.68 for detecting idiom translation errors.", "AI": {"tldr": "The paper introduces IdiomEval, a framework for evaluating Chinese idiom translation across various systems, revealing significant translation errors and limitations in current metrics.", "motivation": "Despite advancements in machine translation, idiom translation, particularly in Chinese, remains underexplored, leading to poor performance in existing systems.", "method": "The authors annotated 900 translation pairs from nine modern systems, assessing their performance in translating idioms, and developed improved models to detect translation errors.", "result": "The study found that the best-performing system, GPT-4, made errors in 28% of cases, and current evaluation metrics poorly correlate with human ratings, with a Pearson correlation below 0.48.", "conclusion": "The development of new models resulted in an improved F$_1$ score of 0.68 for detecting idiom translation errors, indicating room for enhancement in idiom translation quality.", "key_contributions": ["Introduction of the IdiomEval framework for idiom translation evaluation.", "Comprehensive error taxonomy for idiom-related translation issues.", "Development of models that improve detection of idiom translation errors."], "limitations": "The study primarily focuses on idiom translation without addressing other aspects of machine translation performance.", "keywords": ["machine translation", "idiom translation", "natural language processing", "evaluation metrics", "Chinese language"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.10426", "pdf": "https://arxiv.org/pdf/2508.10426.pdf", "abs": "https://arxiv.org/abs/2508.10426", "title": "Computational Economics in Large Language Models: Exploring Model Behavior and Incentive Design under Resource Constraints", "authors": ["Sandeep Reddy", "Kabir Khan", "Rohit Patil", "Ananya Chakraborty", "Faizan A. Khan", "Swati Kulkarni", "Arjun Verma", "Neha Singh"], "categories": ["cs.CL", "I.2.6; I.2.7; I.5.1"], "comment": "Preprint; 7 figures, 4 tables, 1 algorithm. Experiments on GLUE\n  (MNLI, STS-B, CoLA) and WikiText-103 with BERT-base; evaluation includes\n  FLOPS, latency, Gini and entropy metrics", "summary": "Large language models (LLMs) are limited by substantial computational cost.\nWe introduce a \"computational economics\" framework that treats an LLM as an\ninternal economy of resource-constrained agents (attention heads and neuron\nblocks) that must allocate scarce computation to maximize task utility. First,\nwe show empirically that when computation is scarce, standard LLMs reallocate\nattention toward high-value tokens while preserving accuracy. Building on this\nobservation, we propose an incentive-driven training paradigm that augments the\ntask loss with a differentiable computation cost term, encouraging sparse and\nefficient activations. On GLUE (MNLI, STS-B, CoLA) and WikiText-103, the method\nyields a family of models that trace a Pareto frontier and consistently\ndominate post-hoc pruning; for a similar accuracy we obtain roughly a forty\npercent reduction in FLOPS and lower latency, together with more interpretable\nattention patterns. These results indicate that economic principles offer a\nprincipled route to designing efficient, adaptive, and more transparent LLMs\nunder strict resource constraints.", "AI": {"tldr": "This paper presents a computational economics framework for large language models (LLMs) that optimizes resource allocation for task utility, resulting in efficient and interpretable models under computational constraints.", "motivation": "LLMs face high computational costs, necessitating a better resource allocation strategy to enhance efficiency and utility in task performance.", "method": "The authors propose an incentive-driven training paradigm that integrates a differentiable computation cost term with task loss, encouraging sparse and efficient activations in LLMs.", "result": "The proposed method leads to models that achieve a significant reduction in FLOPS (approximately 40%), lower latency, and more interpretable attention patterns while maintaining high accuracy across various benchmarks.", "conclusion": "Implementing economic principles in LLM design facilitates the development of more efficient and adaptive models that excel under resource constraints.", "key_contributions": ["Introduces a computational economics framework for LLMs.", "Develops an incentive-driven training paradigm for efficiency in model activations.", "Demonstrates significant performance improvements in resource-constrained settings."], "limitations": "", "keywords": ["large language models", "computational economics", "resource allocation", "efficient training", "LLM interpretability"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.10444", "pdf": "https://arxiv.org/pdf/2508.10444.pdf", "abs": "https://arxiv.org/abs/2508.10444", "title": "DiFaR: Enhancing Multimodal Misinformation Detection with Diverse, Factual, and Relevant Rationales", "authors": ["Herun Wan", "Jiaying Wu", "Minnan Luo", "Xiangzheng Kong", "Zihan Ma", "Zhi Zeng"], "categories": ["cs.CL"], "comment": null, "summary": "Generating textual rationales from large vision-language models (LVLMs) to\nsupport trainable multimodal misinformation detectors has emerged as a\npromising paradigm. However, its effectiveness is fundamentally limited by\nthree core challenges: (i) insufficient diversity in generated rationales, (ii)\nfactual inaccuracies due to hallucinations, and (iii) irrelevant or conflicting\ncontent that introduces noise. We introduce DiFaR, a detector-agnostic\nframework that produces diverse, factual, and relevant rationales to enhance\nmisinformation detection. DiFaR employs five chain-of-thought prompts to elicit\nvaried reasoning traces from LVLMs and incorporates a lightweight post-hoc\nfiltering module to select rationale sentences based on sentence-level\nfactuality and relevance scores. Extensive experiments on four popular\nbenchmarks demonstrate that DiFaR outperforms four baseline categories by up to\n5.9% and boosts existing detectors by as much as 8.7%. Both automatic metrics\nand human evaluations confirm that DiFaR significantly improves rationale\nquality across all three dimensions.", "AI": {"tldr": "DiFaR is a novel framework that enhances misinformation detection by generating diverse, factual, and relevant rationales from large vision-language models.", "motivation": "The need for effective multimodal misinformation detectors that generate high-quality rationales to support their capabilities, addressing the limitations of existing methods.", "method": "DiFaR employs five chain-of-thought prompts to generate varied reasoning traces and utilizes a post-hoc filtering module to select rationale sentences based on factuality and relevance.", "result": "DiFaR outperforms baseline methods by up to 5.9% in rationale quality and enhances detector performance by up to 8.7% on four benchmarks.", "conclusion": "The extensive experiments confirm that DiFaR significantly improves the quality of generated rationales, addressing diversity, factual accuracy, and relevance issues in misinformation detection.", "key_contributions": ["Introduction of a detector-agnostic framework for generating rationales", "Utilization of chain-of-thought prompts for diverse reasoning", "Implementation of a post-hoc filtering module for selecting high-quality rationales"], "limitations": "", "keywords": ["misinformation detection", "vision-language models", "chain-of-thought prompts"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2508.10482", "pdf": "https://arxiv.org/pdf/2508.10482.pdf", "abs": "https://arxiv.org/abs/2508.10482", "title": "When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing", "authors": ["Mahdi Dhaini", "Stephen Meisenbacher", "Ege Erdogan", "Florian Matthes", "Gjergji Kasneci"], "categories": ["cs.CL"], "comment": "Accepted to AAAI/ACM Conference on AI, Ethics, and Society (AIES\n  2025)", "summary": "In the study of trustworthy Natural Language Processing (NLP), a number of\nimportant research fields have emerged, including that of\n\\textit{explainability} and \\textit{privacy}. While research interest in both\nexplainable and privacy-preserving NLP has increased considerably in recent\nyears, there remains a lack of investigation at the intersection of the two.\nThis leaves a considerable gap in understanding of whether achieving\n\\textit{both} explainability and privacy is possible, or whether the two are at\nodds with each other. In this work, we conduct an empirical investigation into\nthe privacy-explainability trade-off in the context of NLP, guided by the\npopular overarching methods of \\textit{Differential Privacy} (DP) and Post-hoc\nExplainability. Our findings include a view into the intricate relationship\nbetween privacy and explainability, which is formed by a number of factors,\nincluding the nature of the downstream task and choice of the text\nprivatization and explainability method. In this, we highlight the potential\nfor privacy and explainability to co-exist, and we summarize our findings in a\ncollection of practical recommendations for future work at this important\nintersection.", "AI": {"tldr": "This paper investigates the trade-off between explainability and privacy in NLP, exploring whether they can coexist.", "motivation": "To address the lack of investigation at the intersection of explainability and privacy in NLP, and to explore their potential coexistence.", "method": "Empirical investigation into the privacy-explainability trade-off using Differential Privacy (DP) and Post-hoc Explainability methods.", "result": "Findings indicate an intricate relationship between privacy and explainability influenced by factors such as the downstream task and chosen methods for text privatization and explainability. Recommendations for future research are provided.", "conclusion": "The study suggests that privacy and explainability can potentially coexist and offers practical recommendations for future work.", "key_contributions": ["Empirical insights into the privacy-explainability trade-off in NLP.", "Identification of factors influencing the relationship between privacy and explainability.", "Recommendations for future research in the intersection of explainable and privacy-preserving NLP."], "limitations": "", "keywords": ["Natural Language Processing", "Explainability", "Privacy", "Differential Privacy", "Post-hoc Explainability"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.10552", "pdf": "https://arxiv.org/pdf/2508.10552.pdf", "abs": "https://arxiv.org/abs/2508.10552", "title": "When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models", "authors": ["Huyu Wu", "Meng Tang", "Xinhan Zheng", "Haiyun Jiang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities across a diverse range of multimodal tasks. However, these models\nsuffer from a core problem known as text dominance: they depend heavily on text\nfor their inference, while underutilizing other modalities. While prior work\nhas acknowledged this phenomenon in vision-language tasks, often attributing it\nto data biases or model architectures. In this paper, we conduct the first\nsystematic investigation of text dominance across diverse data modalities,\nincluding images, videos, audio, time-series, and graphs. To measure this\nimbalance, we propose two evaluation metrics: the Modality Dominance Index\n(MDI) and the Attention Efficiency Index (AEI). Our comprehensive analysis\nreveals that text dominance is both significant and pervasive across all tested\nmodalities. Our in-depth analysis identifies three underlying causes: attention\ndilution from severe token redundancy in non-textual modalities, the influence\nof fusion architecture design, and task formulations that implicitly favor\ntextual inputs. Furthermore, we propose a simple token compression method that\neffectively rebalances model attention. Applying this method to LLaVA-7B, for\ninstance, drastically reduces its MDI from 10.23 to a well-balanced value of\n0.86. Our analysis and methodological framework offer a foundation for the\ndevelopment of more equitable and comprehensive multimodal language models.", "AI": {"tldr": "This paper investigates the issue of text dominance in Multimodal Large Language Models (MLLMs), proposes two new evaluation metrics to measure it, and suggests a method to rebalance model attention.", "motivation": "To address the reliance of MLLMs on text over other modalities, which affects their multimodal inference capabilities.", "method": "A systematic investigation across various data modalities is conducted, introducing the Modality Dominance Index (MDI) and Attention Efficiency Index (AEI) to quantify text dominance. A token compression method is proposed to rebalance attention.", "result": "The analysis reveals significant text dominance across all tested modalities, with MDI values indicating severe reliance on text. Implementing the token compression method drastically lowers the MDI value of LLaVA-7B from 10.23 to 0.86, demonstrating improved attention balancing.", "conclusion": "The study presents a methodological framework for developing more balanced multimodal language models and highlights the importance of addressing text dominance in future research.", "key_contributions": ["Introduced the Modality Dominance Index (MDI) and Attention Efficiency Index (AEI) for measuring text dominance", "Conducted a comprehensive analysis across multiple data modalities", "Proposed a token compression method to improve model attention distribution"], "limitations": "", "keywords": ["Multimodal Language Models", "Text Dominance", "Attention Mechanism", "Evaluation Metrics", "Token Compression"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.10553", "pdf": "https://arxiv.org/pdf/2508.10553.pdf", "abs": "https://arxiv.org/abs/2508.10553", "title": "eDIF: A European Deep Inference Fabric for Remote Interpretability of LLM", "authors": ["Irma Heithoff. Marc Guggenberger", "Sandra Kalogiannis", "Susanne Mayer", "Fabian Maag", "Sigurd Schacht", "Carsten Lanquillon"], "categories": ["cs.CL"], "comment": "9 pages", "summary": "This paper presents a feasibility study on the deployment of a European Deep\nInference Fabric (eDIF), an NDIF-compatible infrastructure designed to support\nmechanistic interpretability research on large language models. The need for\nwidespread accessibility of LLM interpretability infrastructure in Europe\ndrives this initiative to democratize advanced model analysis capabilities for\nthe research community. The project introduces a GPU-based cluster hosted at\nAnsbach University of Applied Sciences and interconnected with partner\ninstitutions, enabling remote model inspection via the NNsight API. A\nstructured pilot study involving 16 researchers from across Europe evaluated\nthe platform's technical performance, usability, and scientific utility. Users\nconducted interventions such as activation patching, causal tracing, and\nrepresentation analysis on models including GPT-2 and DeepSeek-R1-70B. The\nstudy revealed a gradual increase in user engagement, stable platform\nperformance throughout, and a positive reception of the remote experimentation\ncapabilities. It also marked the starting point for building a user community\naround the platform. Identified limitations such as prolonged download\ndurations for activation data as well as intermittent execution interruptions\nare addressed in the roadmap for future development. This initiative marks a\nsignificant step towards widespread accessibility of LLM interpretability\ninfrastructure in Europe and lays the groundwork for broader deployment,\nexpanded tooling, and sustained community collaboration in mechanistic\ninterpretability research.", "AI": {"tldr": "The paper explores the deployment of a European Deep Inference Fabric for mechanistic interpretability research on large language models, introducing a collaborative testing platform and demonstrating its utility through user studies.", "motivation": "To democratize access to LLM interpretability infrastructure in Europe for researchers.", "method": "A GPU-based cluster was deployed and interlinked with partner institutions, facilitating remote model inspection through the NNsight API. A pilot study with 16 researchers evaluated its usability and performance.", "result": "The platform demonstrated stable performance, increased user engagement, and positive feedback on remote experimentation capabilities while identifying necessary improvements for future versions.", "conclusion": "The initiative is a significant stride towards enhanced LLM interpretability in Europe, supporting future development and community collaboration in the field.", "key_contributions": ["Introduction of a GPU-based cluster for LLM interpretability research", "Evaluation of platform performance and usability through a pilot study", "Foundation for a community around mechanistic interpretability research"], "limitations": "Prolonged download durations for activation data and intermittent execution interruptions were noted.", "keywords": ["LLM interpretability", "Deep Inference Fabric", "user engagement", "remote experimentation", "mechanistic interpretability"], "importance_score": 9, "read_time_minutes": 9}}
{"id": "2508.10683", "pdf": "https://arxiv.org/pdf/2508.10683.pdf", "abs": "https://arxiv.org/abs/2508.10683", "title": "Neural Machine Translation for Coptic-French: Strategies for Low-Resource Ancient Languages", "authors": ["Nasma Chaoui", "Richard Khoury"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents the first systematic study of strategies for translating\nCoptic into French. Our comprehensive pipeline systematically evaluates: pivot\nversus direct translation, the impact of pre-training, the benefits of\nmulti-version fine-tuning, and model robustness to noise. Utilizing aligned\nbiblical corpora, we demonstrate that fine-tuning with a stylistically-varied\nand noise-aware training corpus significantly enhances translation quality. Our\nfindings provide crucial practical insights for developing translation tools\nfor historical languages in general.", "AI": {"tldr": "Systematic study on translating Coptic into French, evaluating various strategies for improved translation quality.", "motivation": "To explore effective strategies for translating historical languages, specifically focusing on Coptic to French.", "method": "Evaluated pivot versus direct translation, pre-training impact, multi-version fine-tuning benefits, and model robustness to noise using aligned biblical corpora.", "result": "Fine-tuning with a diverse and noise-aware training corpus significantly enhances translation quality for Coptic to French.", "conclusion": "The study provides valuable insights for developing translation tools for historical languages.", "key_contributions": ["First systematic study on Coptic to French translation", "Evaluation of various translation strategies", "Insights for improving tools for historical language translation"], "limitations": "", "keywords": ["Coptic", "translation", "historical languages", "machine translation", "French"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2508.10687", "pdf": "https://arxiv.org/pdf/2508.10687.pdf", "abs": "https://arxiv.org/abs/2508.10687", "title": "Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph", "authors": ["Safaeid Hossain Arib", "Rabeya Akter", "Sejuti Rahman"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Millions of individuals worldwide are affected by deafness and hearing\nimpairment. Sign language serves as a sophisticated means of communication for\nthe deaf and hard of hearing. However, in societies that prioritize spoken\nlanguages, sign language often faces underestimation, leading to communication\nbarriers and social exclusion. The Continuous Bangla Sign Language Translation\nproject aims to address this gap by enhancing translation methods. While recent\napproaches leverage transformer architecture for state-of-the-art results, our\nmethod integrates graph-based methods with the transformer architecture. This\nfusion, combining transformer and STGCN-LSTM architectures, proves more\neffective in gloss-free translation. Our contributions include architectural\nfusion, exploring various fusion strategies, and achieving a new\nstate-of-the-art performance on diverse sign language datasets, namely\nRWTH-PHOENIX-2014T, CSL-Daily, How2Sign, and BornilDB v1.0. Our approach\ndemonstrates superior performance compared to current translation outcomes\nacross all datasets, showcasing notable improvements of BLEU-4 scores of 4.01,\n2.07, and 0.5, surpassing those of GASLT, GASLT and slt_how2sign in\nRWTH-PHOENIX-2014T, CSL-Daily, and How2Sign, respectively. Also, we introduce\nbenchmarking on the BornilDB v1.0 dataset for the first time. Our method sets a\nbenchmark for future research, emphasizing the importance of gloss-free\ntranslation to improve communication accessibility for the deaf and hard of\nhearing.", "AI": {"tldr": "The paper presents a novel approach to enhance sign language translation by integrating graph-based methods with transformer architecture, achieving state-of-the-art performance on multiple datasets.", "motivation": "To improve communication accessibility for the deaf and hard of hearing by addressing the underestimation and communication barriers faced by sign language.", "method": "The approach combines transformer architecture with STGCN-LSTM architectures for gloss-free translation, exploring various fusion strategies.", "result": "Achieved new state-of-the-art BLEU-4 scores across multiple datasets like RWTH-PHOENIX-2014T, CSL-Daily, How2Sign, and introduced benchmarking on BornilDB v1.0 dataset.", "conclusion": "The proposed method significantly surpasses existing translation outcomes, setting a new benchmark for future research in sign language translation.", "key_contributions": ["Architectural fusion of transformer and graph-based methods", "New state-of-the-art performance benchmarks", "Introduction of BornilDB v1.0 for benchmarking"], "limitations": "", "keywords": ["Sign Language Translation", "Transformer Architecture", "Graph-Based Methods"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2508.10695", "pdf": "https://arxiv.org/pdf/2508.10695.pdf", "abs": "https://arxiv.org/abs/2508.10695", "title": "Learning from Natural Language Feedback for Personalized Question Answering", "authors": ["Alireza Salemi", "Hamed Zamani"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Personalization is crucial for enhancing both the effectiveness and user\nsatisfaction of language technologies, particularly in information-seeking\ntasks like question answering. Current approaches for personalizing large\nlanguage models (LLMs) often rely on retrieval-augmented generation (RAG),\nfollowed by reinforcement learning with scalar reward signals to teach models\nhow to use retrieved personal context. We believe that these scalar rewards\nsometimes provide weak, non-instructive feedback, limiting learning efficiency\nand personalization quality. We introduce VAC, a novel framework for\npersonalized response generation that replaces scalar rewards with natural\nlanguage feedback (NLF) that are generated conditioned on the user profiles and\nthe question narratives. NLF serves as a rich and actionable supervision\nsignal, allowing the policy model to iteratively refine its outputs and\ninternalize effective personalization strategies. Training alternates between\noptimizing the feedback model and fine-tuning the policy model on the improved\nresponses, resulting in a policy model that no longer requires feedback at\ninference. Evaluation on the LaMP-QA benchmark that consists of three diverse\ndomains demonstrates consistent and significant improvements over the\nstate-of-the-art results. Human evaluations further confirm the superior\nquality of the generated responses. These results demonstrate that NLF provides\nmore effective signals for optimizing personalized question answering.", "AI": {"tldr": "This paper presents VAC, a framework for personalized response generation using natural language feedback instead of scalar rewards, improving learning efficiency and response quality in large language models (LLMs).", "motivation": "Personalization enhances effectiveness and user satisfaction in language technologies, particularly in question answering tasks. Existing methods often utilize scalar rewards, which can limit learning efficiency and personalization quality.", "method": "The VAC framework replaces scalar rewards with natural language feedback (NLF), conditioned on user profiles and question narratives. Training alternates between optimizing the feedback model and fine-tuning the policy model on improved responses, ultimately eliminating the need for feedback during inference.", "result": "Evaluation on the LaMP-QA benchmark shows consistent and significant improvements over state-of-the-art results, with human evaluations confirming the higher quality of responses generated with NLF.", "conclusion": "The use of NLF as a signal for optimizing personalized question answering enhances the model's ability to generate relevant responses tailored to individual user profiles.", "key_contributions": ["Introduction of VAC framework for personalized response generation", "Replacement of scalar rewards with natural language feedback (NLF)", "Demonstrated significant improvements in response quality and learning efficiency"], "limitations": "", "keywords": ["personalization", "natural language feedback", "large language models", "question answering", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.10736", "pdf": "https://arxiv.org/pdf/2508.10736.pdf", "abs": "https://arxiv.org/abs/2508.10736", "title": "Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs", "authors": ["Xiangqi Jin", "Yuxuan Wang", "Yifeng Gao", "Zichen Wen", "Biqing Qi", "Dongrui Liu", "Linfeng Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Despite large language models (LLMs) have achieved remarkable success, their\nprefix-only prompting paradigm and sequential generation process offer limited\nflexibility for bidirectional information. Diffusion large language models\n(dLLMs) present new opportunities through their bidirectional attention\nmechanisms and iterative refinement processes, enabling more flexible in-place\nprompting strategies. We introduce ICE (In-Place Chain-of-Thought Prompting\nwith Early Exit), a novel framework that transforms prefix-only prompting into\nin-place prompting specifically designed for dLLMs. ICE integrates in-place\nprompts directly within masked token positions during iterative refinement and\nemploys a confidence-aware early exit mechanism to significantly reduce\ncomputational overhead. Extensive experiments demonstrate ICE's effectiveness,\nachieving up to 17.29% accuracy improvement with 4.12$\\times$ speedup on GSM8K,\nand up to 276.67$\\times$ acceleration on MMLU while maintaining competitive\nperformance.", "AI": {"tldr": "ICE is a novel framework that improves prompting in diffusion large language models by integrating in-place prompts and a confidence-aware early exit mechanism.", "motivation": "To overcome the limitations of traditional prefix-only prompting in large language models, allowing for more flexible interaction and efficiency in generation processes.", "method": "The ICE framework incorporates in-place prompts within masked token positions during the iterative refinement process of diffusion large language models and utilizes a confidence-aware early exit mechanism to reduce computational cost.", "result": "ICE achieves an accuracy improvement of up to 17.29% and a speedup of 4.12x on the GSM8K dataset, as well as a remarkable 276.67x acceleration on MMLU, while keeping competitive performance.", "conclusion": "The results suggest that ICE effectively enhances the flexibility and efficiency of prompting in diffusion large language models, paving the way for more advanced applications in tasks requiring bidirectional information flow.", "key_contributions": ["Introduces in-place prompting in diffusion large language models (dLLMs)", "Implements confidence-aware early exit mechanism to cut computational costs", "Demonstrates significant improvements in accuracy and speed on benchmarks."], "limitations": "", "keywords": ["large language models", "diffusion models", "chain-of-thought prompting", "in-place prompting", "early exit mechanism"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.10795", "pdf": "https://arxiv.org/pdf/2508.10795.pdf", "abs": "https://arxiv.org/abs/2508.10795", "title": "Beyond \"Not Novel Enough\": Enriching Scholarly Critique with LLM-Assisted Feedback", "authors": ["Osama Mohammed Afzal", "Preslav Nakov", "Tom Hope", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": null, "summary": "Novelty assessment is a central yet understudied aspect of peer review,\nparticularly in high volume fields like NLP where reviewer capacity is\nincreasingly strained. We present a structured approach for automated novelty\nevaluation that models expert reviewer behavior through three stages: content\nextraction from submissions, retrieval and synthesis of related work, and\nstructured comparison for evidence based assessment. Our method is informed by\na large scale analysis of human written novelty reviews and captures key\npatterns such as independent claim verification and contextual reasoning.\nEvaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty\nassessments, the approach achieves 86.5% alignment with human reasoning and\n75.3% agreement on novelty conclusions - substantially outperforming existing\nLLM based baselines. The method produces detailed, literature aware analyses\nand improves consistency over ad hoc reviewer judgments. These results\nhighlight the potential for structured LLM assisted approaches to support more\nrigorous and transparent peer review without displacing human expertise. Data\nand code are made available.", "AI": {"tldr": "The paper proposes an automated approach for novelty assessment in peer review, showing significant alignment with human reviewers.", "motivation": "To address the challenges of novelty assessment in peer review, particularly in NLP, where reviewer capacity is limited.", "method": "The method involves extracting content from submissions, retrieving related work, and performing structured comparisons for evidence-based assessments.", "result": "Evaluated on 182 ICLR 2025 submissions, the method achieves 86.5% alignment with human reasoning and 75.3% agreement on novelty conclusions, outperforming existing LLM baselines.", "conclusion": "Structured LLM-assisted approaches can enhance the peer review process by providing detailed analyses, improving consistency, and supporting human expertise.", "key_contributions": ["Development of a structured approach for automated novelty evaluation", "High alignment with human reviewer assessments", "Open availability of data and code for further research"], "limitations": "", "keywords": ["novelty assessment", "peer review", "NLP", "LLM", "human expertise"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.10839", "pdf": "https://arxiv.org/pdf/2508.10839.pdf", "abs": "https://arxiv.org/abs/2508.10839", "title": "Reinforced Language Models for Sequential Decision Making", "authors": ["Jim Dilkes", "Vahid Yazdanpanah", "Sebastian Stein"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.8"], "comment": null, "summary": "Large Language Models (LLMs) show potential as sequential decision-making\nagents, but their application is often limited due to a reliance on large,\ncomputationally expensive models. This creates a need to improve smaller\nmodels, yet existing post-training methods are designed for single-turn\ninteractions and cannot handle credit assignment in multi-step agentic tasks.\nTo address this, we introduce Multi-Step Group-Relative Policy Optimization\n(MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal\nText-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP)\nframeworks. For credit assignment, MS-GRPO attributes the entire cumulative\nepisode reward to each individual episode step. We supplement this algorithm\nwith a novel absolute-advantage-weighted episode sampling strategy that we show\nimproves training performance. We evaluate our approach by post-training a\n3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate\nthat the method is effective in improving decision-making performance: our\npost-trained 3B parameter model outperforms a 72B parameter baseline by 50% on\nthe Frozen Lake task. This work demonstrates that targeted post-training is a\npractical and efficient alternative to relying on model scale for creating\nsequential decision-making agents using LLMs.", "AI": {"tldr": "The paper presents a novel algorithm, MS-GRPO, for improving decision-making in LLMs by enhancing smaller models for multi-step tasks, showing significant performance gains over larger models.", "motivation": "Current LLMs are too costly in terms of computation, necessitating techniques to enhance the performance of smaller models for decision-making tasks.", "method": "The paper introduces Multi-Step Group-Relative Policy Optimization (MS-GRPO) for post-training LLMs, utilizing the Text-Mediated Stochastic Game (TSMG) framework and a new episode sampling strategy.", "result": "The MS-GRPO approach significantly improves a 3-billion parameter model's decision-making, yielding a 50% performance boost over a 72-billion parameter baseline in the Frozen Lake task.", "conclusion": "Targeted post-training like MS-GRPO offers an efficient alternative to simply scaling model size for enhancing LLM decision-making capabilities.", "key_contributions": ["Introduction of MS-GRPO for credit assignment in multi-step tasks", "Development of an absolute-advantage-weighted episode sampling strategy", "Demonstrated significant performance improvements in decision-making tasks."], "limitations": "", "keywords": ["Large Language Models", "multi-step decision-making", "post-training"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.10848", "pdf": "https://arxiv.org/pdf/2508.10848.pdf", "abs": "https://arxiv.org/abs/2508.10848", "title": "Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy, Expertise, and Reasoning", "authors": ["Chongyuan Dai", "Jinpeng Hu", "Hongchang Shi", "Zhuo Li", "Xun Yang", "Meng Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Amidst a shortage of qualified mental health professionals, the integration\nof large language models (LLMs) into psychological applications offers a\npromising way to alleviate the growing burden of mental health disorders.\nRecent reasoning-augmented LLMs have achieved remarkable performance in\nmathematics and programming, while research in the psychological domain has\npredominantly emphasized emotional support and empathetic dialogue, with\nlimited attention to reasoning mechanisms that are beneficial to generating\nreliable responses. Therefore, in this paper, we propose Psyche-R1, the first\nChinese psychological LLM that jointly integrates empathy, psychological\nexpertise, and reasoning, built upon a novel data curation pipeline.\nSpecifically, we design a comprehensive data synthesis pipeline that produces\nover 75k high-quality psychological questions paired with detailed rationales,\ngenerated through chain-of-thought (CoT) reasoning and iterative\nprompt-rationale optimization, along with 73k empathetic dialogues.\nSubsequently, we employ a hybrid training strategy wherein challenging samples\nare identified through a multi-LLM cross-selection strategy for group relative\npolicy optimization (GRPO) to improve reasoning ability, while the remaining\ndata is used for supervised fine-tuning (SFT) to enhance empathetic response\ngeneration and psychological domain knowledge. Extensive experiment results\ndemonstrate the effectiveness of the Psyche-R1 across several psychological\nbenchmarks, where our 7B Psyche-R1 achieves comparable results to 671B\nDeepSeek-R1.", "AI": {"tldr": "Psyche-R1 is a novel psychological LLM integrating empathy, expertise, and reasoning, addressing mental health disorders through a unique data synthesis and training strategy.", "motivation": "To alleviate the shortage of qualified mental health professionals and improve the quality of psychological applications leveraging LLMs, especially in reasoning mechanisms.", "method": "A novel data curation pipeline generated over 75k psychological questions with rationales and 73k empathetic dialogues. Employed a hybrid training strategy including GRPO for reasoning and SFT for empathetic responses.", "result": "Psyche-R1, a 7B model, achieves comparable performance to the 671B DeepSeek-R1 on various psychological benchmarks.", "conclusion": "The integration of reasoning, empathy, and psychological expertise in LLMs like Psyche-R1 can significantly enhance the support provided in mental health applications.", "key_contributions": ["First Chinese psychological LLM with integrated reasoning mechanisms", "Comprehensive data synthesis for psychological training", "Hybrid training strategy optimizing reasoning and empathy"], "limitations": "", "keywords": ["Large Language Model", "Mental Health", "Psychological Applications", "Reasoning", "Empathy"], "importance_score": 10, "read_time_minutes": 10}}
{"id": "2508.10860", "pdf": "https://arxiv.org/pdf/2508.10860.pdf", "abs": "https://arxiv.org/abs/2508.10860", "title": "From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms", "authors": ["Zhaokun Jiang", "Ziyin Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in machine learning have spurred growing interests in\nautomated interpreting quality assessment. Nevertheless, existing research\nsuffers from insufficient examination of language use quality, unsatisfactory\nmodeling effectiveness due to data scarcity and imbalance, and a lack of\nefforts to explain model predictions. To address these gaps, we propose a\nmulti-dimensional modeling framework that integrates feature engineering, data\naugmentation, and explainable machine learning. This approach prioritizes\nexplainability over ``black box'' predictions by utilizing only\nconstruct-relevant, transparent features and conducting Shapley Value (SHAP)\nanalysis. Our results demonstrate strong predictive performance on a novel\nEnglish-Chinese consecutive interpreting dataset, identifying BLEURT and\nCometKiwi scores to be the strongest predictive features for fidelity,\npause-related features for fluency, and Chinese-specific phraseological\ndiversity metrics for language use. Overall, by placing particular emphasis on\nexplainability, we present a scalable, reliable, and transparent alternative to\ntraditional human evaluation, facilitating the provision of detailed diagnostic\nfeedback for learners and supporting self-regulated learning advantages not\nafforded by automated scores in isolation.", "AI": {"tldr": "Proposes a multi-dimensional framework for automated interpreting quality assessment prioritizing explainability and model performance.", "motivation": "Addressing gaps in language quality assessment in interpreting such as data scarcity, model effectiveness, and explainability.", "method": "Integration of feature engineering, data augmentation, and explainable machine learning techniques, utilizing SHAP analysis for feature transparency.", "result": "Achieved strong predictive performance on an English-Chinese interpreting dataset, highlighting the importance of specific predictive features for fidelity and fluency.", "conclusion": "The proposed framework is a scalable and reliable alternative for human evaluation, offering detailed feedback to learners.", "key_contributions": ["Multi-dimensional modeling framework for interpreting quality assessment", "Prioritization of explainability in predictions", "Identification of key predictive features for language quality"], "limitations": "", "keywords": ["machine learning", "interpreting quality assessment", "explainable ML"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2508.10874", "pdf": "https://arxiv.org/pdf/2508.10874.pdf", "abs": "https://arxiv.org/abs/2508.10874", "title": "SSRL: Self-Search Reinforcement Learning", "authors": ["Yuchen Fan", "Kaiyan Zhang", "Heng Zhou", "Yuxin Zuo", "Yanxu Chen", "Yu Fu", "Xinwei Long", "Xuekai Zhu", "Che Jiang", "Yuchen Zhang", "Li Kang", "Gang Chen", "Cheng Huang", "Zhizhou He", "Bingning Wang", "Lei Bai", "Ning Ding", "Bowen Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "We investigate the potential of large language models (LLMs) to serve as\nefficient simulators for agentic search tasks in reinforcement learning (RL),\nthereby reducing dependence on costly interactions with external search\nengines. To this end, we first quantify the intrinsic search capability of LLMs\nvia structured prompting and repeated sampling, which we term Self-Search. Our\nresults reveal that LLMs exhibit strong scaling behavior with respect to the\ninference budget, achieving high pass@k on question-answering benchmarks,\nincluding the challenging BrowseComp task. Building on these observations, we\nintroduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability\nthrough format-based and rule-based rewards. SSRL enables models to iteratively\nrefine their knowledge utilization internally, without requiring access to\nexternal tools. Empirical evaluations demonstrate that SSRL-trained policy\nmodels provide a cost-effective and stable environment for search-driven RL\ntraining, reducing reliance on external search engines and facilitating robust\nsim-to-real transfer. We draw the following conclusions: 1) LLMs possess world\nknowledge that can be effectively elicited to achieve high performance; 2) SSRL\ndemonstrates the potential of leveraging internal knowledge to reduce\nhallucination; 3) SSRL-trained models integrate seamlessly with external search\nengines without additional effort. Our findings highlight the potential of LLMs\nto support more scalable RL agent training.", "AI": {"tldr": "This paper explores the use of large language models (LLMs) as efficient simulators for agentic search tasks in reinforcement learning (RL) through a novel approach called Self-Search RL (SSRL).", "motivation": "The motivation behind this research is to explore how LLMs can reduce dependency on external search engines for agentic search tasks in RL.", "method": "The study quantifies LLMs' intrinsic search capability through structured prompting and repeated sampling, termed Self-Search, and introduces Self-Search RL (SSRL) that enhances this capability with format-based and rule-based rewards.", "result": "LLMs showed strong scaling behavior with high pass@k scores on various benchmarks, and SSRL-trained models provided a cost-effective, stable training environment for search-driven RL, reducing external search dependency.", "conclusion": "LLMs possess significant world knowledge that can be effectively utilized, SSRL reduces hallucination, and models trained with SSRL can integrate smoothly with external search engines.", "key_contributions": ["Introduction of Self-Search for quantifying LLM capabilities", "Development of Self-Search RL (SSRL) to enhance LLM performance", "Demonstration of cost-effective training environments for RL using LLMs"], "limitations": "", "keywords": ["large language models", "reinforcement learning", "Self-Search RL"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.10875", "pdf": "https://arxiv.org/pdf/2508.10875.pdf", "abs": "https://arxiv.org/abs/2508.10875", "title": "A Survey on Diffusion Language Models", "authors": ["Tianyi Li", "Mingda Chen", "Bowei Guo", "Zhiqiang Shen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and\npromising alternative to the dominant autoregressive (AR) paradigm. By\ngenerating tokens in parallel through an iterative denoising process, DLMs\npossess inherent advantages in reducing inference latency and capturing\nbidirectional context, thereby enabling fine-grained control over the\ngeneration process. While achieving a several-fold speed-up, recent\nadvancements have allowed DLMs to show performance comparable to their\nautoregressive counterparts, making them a compelling choice for various\nnatural language processing tasks. In this survey, we provide a holistic\noverview of the current DLM landscape. We trace its evolution and relationship\nwith other paradigms, such as autoregressive and masked language models, and\ncover both foundational principles and state-of-the-art models. Our work offers\nan up-to-date, comprehensive taxonomy and an in-depth analysis of current\ntechniques, from pre-training strategies to advanced post-training methods.\nAnother contribution of this survey is a thorough review of DLM inference\nstrategies and optimizations, including improvements in decoding parallelism,\ncaching mechanisms, and generation quality. We also highlight the latest\napproaches to multimodal extensions of DLMs and delineate their applications\nacross various practical scenarios. Furthermore, our discussion addresses the\nlimitations and challenges of DLMs, including efficiency, long-sequence\nhandling, and infrastructure requirements, while outlining future research\ndirections to sustain progress in this rapidly evolving field. Project GitHub\nis available at https://github.com/VILA-Lab/Awesome-DLMs.", "AI": {"tldr": "The paper surveys the emerging field of Diffusion Language Models (DLMs), outlining their advantages, evolution, current state, and applications in NLP, along with challenges and future research directions.", "motivation": "To provide a comprehensive overview of Diffusion Language Models as an alternative to autoregressive models in NLP tasks, highlighting their evolution, performance, and applications.", "method": "The survey traces the development of DLMs, compares them with autoregressive and masked language models, and covers foundational principles, state-of-the-art models, and inference strategies.", "result": "DLMs have shown performance comparable to autoregressive models while achieving several-fold speed-up in inference, with advances in decoding parallelism and generation quality.", "conclusion": "The paper outlines the current landscape of DLMs, their limitations, and future research directions, facilitating understanding and ongoing development in this emerging field.", "key_contributions": ["Holistic overview of the DLM landscape.", "Comprehensive taxonomy and analysis of techniques in pre-training and post-training strategies.", "Discussion of multimodal extensions and practical applications of DLMs."], "limitations": "Challenges include efficiency, handling long sequences, and infrastructure requirements for DLMs.", "keywords": ["Diffusion Language Models", "Natural Language Processing", "Multimodal AI"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2508.10239", "pdf": "https://arxiv.org/pdf/2508.10239.pdf", "abs": "https://arxiv.org/abs/2508.10239", "title": "Personalized Real-time Jargon Support for Online Meetings", "authors": ["Yifan Song", "Wing Yee Au", "Hon Yung Wong", "Brian P. Bailey", "Tal August"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Effective interdisciplinary communication is frequently hindered by\ndomain-specific jargon. To explore the jargon barriers in-depth, we conducted a\nformative diary study with 16 professionals, revealing critical limitations in\ncurrent jargon-management strategies during workplace meetings. Based on these\ninsights, we designed ParseJargon, an interactive LLM-powered system providing\nreal-time personalized jargon identification and explanations tailored to\nusers' individual backgrounds. A controlled experiment comparing ParseJargon\nagainst baseline (no support) and general-purpose (non-personalized) conditions\ndemonstrated that personalized jargon support significantly enhanced\nparticipants' comprehension, engagement, and appreciation of colleagues' work,\nwhereas general-purpose support negatively affected engagement. A follow-up\nfield study validated ParseJargon's usability and practical value in real-time\nmeetings, highlighting both opportunities and limitations for real-world\ndeployment. Our findings contribute insights into designing personalized jargon\nsupport tools, with implications for broader interdisciplinary and educational\napplications.", "AI": {"tldr": "The paper presents ParseJargon, an LLM-powered system designed to facilitate interdisciplinary communication by providing real-time jargon support tailored to users' backgrounds.", "motivation": "To address the challenges posed by domain-specific jargon in effective interdisciplinary communication, which hinders collaboration and understanding in workplace settings.", "method": "A formative diary study with 16 professionals to identify jargon management limitations, followed by the design and evaluation of ParseJargon through a controlled experiment and a subsequent field study.", "result": "The controlled experiment showed personalized jargon support significantly improved comprehension and engagement compared to baseline and general-purpose support, while the field study validated its usability in real meetings.", "conclusion": "The study emphasizes the importance of personalized tools like ParseJargon in enhancing interdisciplinary communication by overcoming jargon barriers, with broader implications for various applications.", "key_contributions": ["Development of ParseJargon, a personalized LLM-powered jargon support system", "Demonstration of the impact of personalized support on comprehension and engagement", "Insights into real-world deployment and usability of jargon management tools"], "limitations": "The study may have limitations in scalability and applicability across diverse workplaces and disciplines.", "keywords": ["Jargon management", "Human-Computer Interaction", "Interdisciplinary communication", "LLM", "Usability"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2407.12830", "pdf": "https://arxiv.org/pdf/2407.12830.pdf", "abs": "https://arxiv.org/abs/2407.12830", "title": "Knowledge-based Consistency Testing of Large Language Models", "authors": ["Sai Sathiesh Rajan", "Ezekiel Soremekun", "Sudipta Chattopadhyay"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages, 4 figures, 8 tables, Accepted at EMNLP 2024 Findings", "summary": "In this work, we systematically expose and measure the inconsistency and\nknowledge gaps of Large Language Models (LLMs). Specifically, we propose an\nautomated testing framework (called KonTest) which leverages a knowledge graph\nto construct test cases. KonTest probes and measures the inconsistencies in the\nLLM's knowledge of the world via a combination of semantically-equivalent\nqueries and test oracles (metamorphic or ontological oracle). KonTest further\nmitigates knowledge gaps via a weighted LLM model ensemble. Using four\nstate-of-the-art LLMs (Falcon, Gemini, GPT3.5, and Llama2), we show that\nKonTest generates 19.2% error inducing inputs (1917 errors from 9979 test\ninputs). It also reveals a 16.5% knowledge gap across all tested LLMs. A\nmitigation method informed by KonTest's test suite reduces LLM knowledge gap by\n32.48%. Our ablation study further shows that GPT3.5 is not suitable for\nknowledge-based consistency testing because it is only 60%-68% effective in\nknowledge construction.", "AI": {"tldr": "The paper presents KonTest, an automated testing framework for measuring inconsistencies and knowledge gaps in Large Language Models (LLMs) using knowledge graphs.", "motivation": "To systematically expose and measure the inconsistencies and knowledge gaps of LLMs, improving their reliability in applications.", "method": "KonTest uses a knowledge graph to construct test cases, employing semantically-equivalent queries and test oracles to probe LLM knowledge.", "result": "KonTest identifies 19.2% error-inducing inputs and a 16.5% knowledge gap across tested LLMs, while a subsequent mitigation method reduces the knowledge gap by 32.48%.", "conclusion": "The study highlights the inconsistency of LLMs and the effectiveness of the KonTest framework in reducing knowledge gaps, particularly noting the limitations of GPT3.5 for knowledge-based consistency testing.", "key_contributions": ["Development of the KonTest framework for LLM testing", "Quantification of inconsistency and knowledge gaps in state-of-the-art LLMs", "Mitigation strategy that significantly reduces knowledge gaps in LLMs."], "limitations": "The study primarily focuses on four specific LLMs and the results may not generalize to other models or domains.", "keywords": ["Large Language Models", "Knowledge Gaps", "Automated Testing", "HCI", "Machine Learning"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2410.16325", "pdf": "https://arxiv.org/pdf/2410.16325.pdf", "abs": "https://arxiv.org/abs/2410.16325", "title": "This Candidate is [MASK]. Prompt-based Sentiment Extraction and Reference Letters", "authors": ["Fabian Slonimczyk"], "categories": ["cs.CL"], "comment": null, "summary": "I propose a relatively simple way to deploy pre-trained large language models\n(LLMs) in order to extract sentiment and other useful features from text data.\nThe method, which I refer to as prompt-based sentiment extraction, offers\nmultiple advantages over other methods used in economics and finance. I apply\nmy prompt-based strategy to a hand-collected corpus of confidential reference\nletters (RLs). I show that the sentiment contents of RLs is clearly reflected\nin job market outcomes. Candidates with higher average sentiment in their\nletters perform markedly better regardless of the measure of success chosen.\nMoreover, I show that disagreement among letter writers negatively affects the\njob market candidate's performance. I compare my sentiment extraction approach\nto other commonly used methods for sentiment analysis: \"bag-of-words\"\napproaches, fine-tuned language models, and querying advanced chatbots. I find\nthat no other method can reproduce the results obtained by prompt-based\nsentiment extraction. Finally, I slightly modify the method to obtain\n\"gendered\" sentiment scores (as in Eberhardt et al., 2023). I show that letters\nof reference written for female candidates emphasize \"grindstone\" personality\ntraits, whereas male candidates' letters emphasize \"standout\" traits. These\ngender differences negatively affect women's job market outcomes.", "AI": {"tldr": "The paper introduces a prompt-based method for sentiment extraction from text data, specifically analyzing reference letters and their impact on job market outcomes.", "motivation": "To improve sentiment extraction methods and analyze how sentiment in reference letters influences candidates' job performance.", "method": "Prompt-based sentiment extraction applied to a corpus of confidential reference letters, comparing it against traditional methods like bag-of-words and fine-tuned models.", "result": "Candidates with higher sentiment in their letters perform better in the job market, while disagreement among writers negatively affects performance. The method reveals gender biases in reference letters that impact women's job outcomes.", "conclusion": "Prompt-based sentiment extraction outperforms other methods, and gendered aspects in reference letters contribute to disparities in job market success.", "key_contributions": ["Introduction of prompt-based sentiment extraction method", "Demonstration of sentiment's effect on job market outcomes", "Analysis of gender differences in reference letters"], "limitations": "", "keywords": ["sentiment analysis", "large language models", "job market outcomes", "gender bias", "text extraction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.06117", "pdf": "https://arxiv.org/pdf/2501.06117.pdf", "abs": "https://arxiv.org/abs/2501.06117", "title": "Fleurs-SLU: A Massively Multilingual Benchmark for Spoken Language Understanding", "authors": ["Fabian David Schmidt", "Ivan Vulić", "Goran Glavaš", "David Ifeoluwa Adelani"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Spoken language understanding (SLU) is indispensable for half of all living\nlanguages that lack a formal writing system. Unlike for high-resource\nlanguages, for these languages, we cannot offload semantic understanding of\nspeech to the cascade of automatic speech recognition (ASR) and text-based\nlarge language models (LLMs). Even if low-resource languages possess a writing\nsystem, ASR for these languages remains unreliable due to limited bimodal\nspeech and text training data. Nonetheless, the evaluation of multilingual SLU\nis limited to shallow tasks such as intent classification or language\nidentification. This is why we present Fleurs-SLU, a multilingual SLU benchmark\nthat encompasses (i) 692 hours of speech for topical utterance classification\nin 102 languages and (ii) multiple-choice question answering via listening\ncomprehension spanning 944 hours of speech across 92 languages. We extensively\nevaluate end-to-end speech classification models, cascaded systems that combine\nspeech-to-text transcription with subsequent LLM-based classification, and\nmultimodal speech-LLMs on Fleurs-SLU. Our results show that cascaded systems\nare more robust in multilingual SLU, though well-pretrained speech encoders can\nperform competitively in topical speech classification. Closed-source\nspeech-LLMs match or surpass the performance of cascaded systems. We observe a\nstrong correlation between robust multilingual ASR, effective speech-to-text\ntranslation, and strong multilingual SLU, indicating mutual benefits between\nacoustic and semantic speech representations.", "AI": {"tldr": "Fleurs-SLU is a multilingual spoken language understanding benchmark that evaluates different systems for classifying speech and answering questions across multiple languages.", "motivation": "The paper addresses the challenges in spoken language understanding (SLU) for low-resource languages, where traditional systems struggle due to limited training data and evaluation focused on shallow tasks.", "method": "The benchmark includes 692 hours of speech for topical utterance classification in 102 languages and 944 hours of speech for multiple-choice question answering. It evaluates various systems including end-to-end models, cascaded systems combining ASR with LLMs, and multimodal speech-LLMs.", "result": "Cascaded systems demonstrate robustness in multilingual SLU, while well-pretrained speech encoders are competitive in topical classification. Closed-source speech-LLMs either match or surpass cascaded systems' performance.", "conclusion": "A strong correlation exists between ASR effectiveness, quality speech-to-text translation, and SLU performance, highlighting the benefits of integrating acoustic and semantic representations.", "key_contributions": ["Introduction of Fleurs-SLU multilingual benchmark for SLU", "Evaluation of multiple systems including cascaded frameworks and speech-LLMs", "Insights on the correlation between ASR quality and SLU performance"], "limitations": "", "keywords": ["Spoken Language Understanding", "Multilingual", "Automatic Speech Recognition", "Large Language Models", "Benchmark"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2502.08512", "pdf": "https://arxiv.org/pdf/2502.08512.pdf", "abs": "https://arxiv.org/abs/2502.08512", "title": "Measuring Diversity in Synthetic Datasets", "authors": ["Yuchang Zhu", "Huizhe Zhang", "Bingzhe Wu", "Jintang Li", "Zibin Zheng", "Peilin Zhao", "Liang Chen", "Yatao Bian"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICML 2025", "summary": "Large language models (LLMs) are widely adopted to generate synthetic\ndatasets for various natural language processing (NLP) tasks, such as text\nclassification and summarization. However, accurately measuring the diversity\nof these synthetic datasets-an aspect crucial for robust model\nperformance-remains a significant challenge. In this paper, we introduce\nDCScore, a novel method for measuring synthetic dataset diversity from a\nclassification perspective. Specifically, DCScore formulates diversity\nevaluation as a sample classification task, leveraging mutual relationships\namong samples. We further provide theoretical verification of the\ndiversity-related axioms satisfied by DCScore, highlighting its role as a\nprincipled diversity evaluation method. Experimental results on synthetic\ndatasets reveal that DCScore enjoys a stronger correlation with multiple\ndiversity pseudo-truths of evaluated datasets, underscoring its effectiveness.\nMoreover, both empirical and theoretical evidence demonstrate that DCScore\nsubstantially reduces computational costs compared to existing methods. Code is\navailable at: https://github.com/bluewhalelab/dcscore.", "AI": {"tldr": "DCScore is a novel method for measuring the diversity of synthetic datasets used in NLP tasks, providing strong correlations with diversity pseudo-truths and reduced computational costs.", "motivation": "Accurately measuring the diversity of synthetic datasets is crucial for improving model performance in NLP tasks.", "method": "DCScore approaches diversity evaluation as a sample classification task, utilizing mutual relationships among samples for its calculations.", "result": "Experimental results show DCScore has a stronger correlation with multiple diversity pseudo-truths compared to existing methods and significantly lowers computational costs.", "conclusion": "DCScore is validated both theoretically and experimentally as an effective and efficient diversity evaluation method for synthetic datasets.", "key_contributions": ["Introduction of DCScore for diversity measurement", "Theoretical verification of diversity-related axioms", "Strong empirical performance with reduced computational costs"], "limitations": "", "keywords": ["large language models", "synthetic datasets", "diversity evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.16770", "pdf": "https://arxiv.org/pdf/2502.16770.pdf", "abs": "https://arxiv.org/abs/2502.16770", "title": "LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with Location-Election-Disjoint", "authors": ["Qianli Ma", "Dongrui Liu", "Qian Chen", "Linfeng Zhang", "Jing Shao"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL2025 main conference", "summary": "Fine-tuning pre-trained Large Language Models (LLMs) for specialized tasks\nincurs substantial computational and data costs. While model merging offers a\ntraining-free solution to integrate multiple task-specific models, existing\nmethods suffer from safety-utility conflicts where enhanced general\ncapabilities degrade safety safeguards. We identify two root causes:\n$\\textbf{neuron misidentification}$ due to simplistic parameter magnitude-based\nselection, and $\\textbf{cross-task neuron interference}$ during merging. To\naddress these challenges, we propose $\\textbf{LED-Merging}$, a three-stage\nframework that $\\textbf{L}$ocates task-specific neurons via gradient-based\nattribution, dynamically $\\textbf{E}$lects critical neurons through multi-model\nimportance fusion, and $\\textbf{D}$isjoints conflicting updates through\nparameter isolation. Extensive experiments on Llama-3-8B, Mistral-7B, and\nLlama2-13B demonstrate that LED-Merging effectively reduces harmful response\nrates, showing a 31.4\\% decrease on Llama-3-8B-Instruct on HarmBench, while\nsimultaneously preserving 95\\% of utility performance, such as achieving\n52.39\\% accuracy on GSM8K. LED-Merging resolves safety-utility conflicts and\nprovides a lightweight, training-free paradigm for constructing reliable\nmulti-task LLMs. Code is available at\n$\\href{https://github.com/MqLeet/LED-Merging}{GitHub}$.", "AI": {"tldr": "The paper proposes LED-Merging, a framework for safely merging multiple task-specific Large Language Models (LLMs) while maintaining performance.", "motivation": "Address the computational and data costs of fine-tuning LLMs for specialized tasks and tackle safety-utility conflicts arising from merging models.", "method": "LED-Merging consists of locating task-specific neurons via gradient-based attribution, selecting critical neurons through multi-model importance fusion, and isolating conflicting updates to prevent interference.", "result": "Experiments show that LED-Merging reduces harmful response rates by 31.4% on Llama-3-8B-Instruct and maintains 95% utility performance, achieving 52.39% accuracy on GSM8K.", "conclusion": "LED-Merging effectively resolves safety-utility conflicts and provides a lightweight, training-free method for building reliable multi-task LLMs.", "key_contributions": ["Introduction of a three-stage merging framework for LLMs", "Reduction of harmful response rates while preserving utility", "Reuse of existing task-specific models without the need for retraining"], "limitations": "", "keywords": ["Large Language Models", "model merging", "safety-utility conflicts", "gradient-based attribution", "multi-task learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.11509", "pdf": "https://arxiv.org/pdf/2503.11509.pdf", "abs": "https://arxiv.org/abs/2503.11509", "title": "TikZero: Zero-Shot Text-Guided Graphics Program Synthesis", "authors": ["Jonas Belouadi", "Eddy Ilg", "Margret Keuper", "Hideki Tanaka", "Masao Utiyama", "Raj Dabre", "Steffen Eger", "Simone Paolo Ponzetto"], "categories": ["cs.CL", "cs.CV"], "comment": "Accepted at ICCV 2025 (highlight); Project page:\n  https://github.com/potamides/DeTikZify", "summary": "Automatically synthesizing figures from text captions is a compelling\ncapability. However, achieving high geometric precision and editability\nrequires representing figures as graphics programs in languages like TikZ, and\naligned training data (i.e., graphics programs with captions) remains scarce.\nMeanwhile, large amounts of unaligned graphics programs and captioned raster\nimages are more readily available. We reconcile these disparate data sources by\npresenting TikZero, which decouples graphics program generation from text\nunderstanding by using image representations as an intermediary bridge. It\nenables independent training on graphics programs and captioned images and\nallows for zero-shot text-guided graphics program synthesis during inference.\nWe show that our method substantially outperforms baselines that can only\noperate with caption-aligned graphics programs. Furthermore, when leveraging\ncaption-aligned graphics programs as a complementary training signal, TikZero\nmatches or exceeds the performance of much larger models, including commercial\nsystems like GPT-4o. Our code, datasets, and select models are publicly\navailable.", "AI": {"tldr": "Introduces TikZero, a method for synthesizing graphics programs from text captions using image representations as an intermediary.", "motivation": "To synthesize figures from text captions with high geometric precision and editability amidst a lack of aligned training data.", "method": "Decouples graphics program generation from text understanding by utilizing image representations to train independently on graphics programs and captioned images.", "result": "TikZero outperforms traditional methods that rely solely on caption-aligned data, and it can match or exceed the performance of larger models when using aligned graphics programs as a training signal.", "conclusion": "TikZero enables better synthesis of graphics programs from text and offers a practical solution to available data issues in the field.", "key_contributions": ["Introduced a novel method for graphics program generation using image representations.", "Demonstrated superior performance compared to existing baselines while requiring less aligned training data.", "Provided publicly available code and datasets for further research."], "limitations": "", "keywords": ["graphics programs", "text captions", "TikZ", "synthesis", "image representations"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2503.23714", "pdf": "https://arxiv.org/pdf/2503.23714.pdf", "abs": "https://arxiv.org/abs/2503.23714", "title": "Building Instruction-Tuning Datasets from Human-Written Instructions with Open-Weight Large Language Models", "authors": ["Youmi Ma", "Sakae Mizuki", "Kazuki Fujii", "Taishi Nakamura", "Masanari Ohi", "Hinari Shimada", "Taihei Shiotani", "Koshiro Saito", "Koki Maeda", "Kakeru Hattori", "Takumi Okamoto", "Shigeki Ishida", "Rio Yokota", "Hiroya Takamura", "Naoaki Okazaki"], "categories": ["cs.CL"], "comment": "COLM 2025; Datasets are available at\n  https://huggingface.co/datasets/tokyotech-llm/lmsys-chat-1m-synth", "summary": "Instruction tuning is crucial for enabling Large Language Models (LLMs) to\nsolve real-world tasks. Prior work has shown the effectiveness of\ninstruction-tuning data synthesized solely from LLMs, raising a fundamental\nquestion: Do we still need human-originated signals for instruction tuning?\nThis work answers the question affirmatively: we build state-of-the-art\ninstruction-tuning datasets sourced from human-written instructions, by simply\npairing them with LLM-generated responses. LLMs fine-tuned on our datasets\nconsistently outperform those fine-tuned on existing ones. Our data\nconstruction approach can be easily adapted to other languages; we build\ndatasets for Japanese and confirm that LLMs tuned with our data reach\nstate-of-the-art performance. Analyses suggest that instruction-tuning in a new\nlanguage allows LLMs to follow instructions, while the tuned models exhibit a\nnotable lack of culture-specific knowledge in that language. The datasets and\nfine-tuned models will be publicly available. Our datasets, synthesized with\nopen-weight LLMs, are openly distributed under permissive licenses, allowing\nfor diverse use cases.", "AI": {"tldr": "We present human-written instruction-tuning datasets that improve LLM fine-tuning performance.", "motivation": "This paper addresses the necessity of human-originated signals in instruction tuning for LLMs.", "method": "The study constructs instruction-tuning datasets using human-written instructions paired with LLM-generated responses, confirming their effectiveness.", "result": "LLMs fine-tuned on these human-sourced datasets consistently outperform those trained on previous datasets, including in Japanese.", "conclusion": "Human-sourced instruction-tuning is beneficial, enhancing LLM performance, particularly in new languages, though cultural knowledge may be limited.", "key_contributions": ["Development of state-of-the-art instruction-tuning datasets from human-written instructions.", "Demonstrated effectiveness of these datasets on LLMs, outperforming existing datasets.", "Public availability of datasets and fine-tuned models for broad use cases."], "limitations": "The tuned LLMs show a notable lack of culture-specific knowledge in the new language.", "keywords": ["Large Language Models", "Instruction Tuning", "Human-Computer Interaction", "Dataset Construction", "Japanese Language"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.01400", "pdf": "https://arxiv.org/pdf/2504.01400.pdf", "abs": "https://arxiv.org/abs/2504.01400", "title": "ToolACE-R: Model-aware Iterative Training and Adaptive Refinement for Tool Learning", "authors": ["Xingshan Zeng", "Weiwen Liu", "Xu Huang", "Zezhong Wang", "Lingzhi Wang", "Liangyou Li", "Yasheng Wang", "Lifeng Shang", "Xin Jiang", "Ruiming Tang", "Qun Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Tool learning, which allows Large Language Models (LLMs) to leverage external\ntools for solving complex user tasks, has emerged as a promising avenue for\nextending model capabilities. However, existing approaches primarily focus on\ndata synthesis for fine-tuning LLMs to invoke tools effectively, largely\nignoring how to fully stimulate the potential of the model. In this paper, we\npropose ToolACE-R, a novel framework that includes both model-aware iterative\ntraining and adaptive refinement for tool learning. ToolACE-R features a\nmodel-aware iterative training procedure that progressively adjust training\nsamples based on the model's evolving capabilities to maximize its potential.\nAdditionally, it incorporates self-refinement training corpus which emphasizes\nLLM's ability to iteratively refine their tool calls, optimizing performance\nwithout requiring external feedback. Furthermore, we introduce adaptive\nself-refinement mechanism for efficient test-time scaling, where the trained\nmodel can autonomously determine when to stop the process based on iterative\nself-refinement. We conduct extensive experiments across several benchmark\ndatasets, showing that ToolACE-R achieves competitive performance compared to\nadvanced API-based models. The performance of tool invocation can be further\nimproved efficiently through adaptive self-refinement. These results highlight\nthe effectiveness and generalizability of ToolACE-R, offering a promising\ndirection for more efficient and scalable tool learning.", "AI": {"tldr": "ToolACE-R is a novel framework for LLMs that enhances tool learning through model-aware iterative training and adaptive self-refinement.", "motivation": "To enhance the capabilities of Large Language Models (LLMs) in leveraging external tools for complex user tasks beyond just data synthesis for fine-tuning.", "method": "ToolACE-R employs a model-aware iterative training procedure to adjust training samples in response to the model's evolving capabilities and utilizes a self-refinement training corpus for optimizing tool invocation without needing external feedback.", "result": "Extensive experiments across benchmark datasets show that ToolACE-R outperforms advanced API-based models and improves tool invocation through adaptive self-refinement.", "conclusion": "ToolACE-R demonstrates effectiveness and generalizability in tool learning, providing a promising approach for scalable and efficient model capabilities.", "key_contributions": ["Introduction of ToolACE-R framework for tool learning in LLMs.", "Implementation of model-aware iterative training to adjust training samples dynamically.", "Development of adaptive self-refinement mechanism for test-time efficiency."], "limitations": "", "keywords": ["Large Language Models", "Tool Learning", "Adaptive Self-Refinement"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.02323", "pdf": "https://arxiv.org/pdf/2504.02323.pdf", "abs": "https://arxiv.org/abs/2504.02323", "title": "CoTAL: Human-in-the-Loop Prompt Engineering for Generalizable Formative Assessment Scoring", "authors": ["Clayton Cohn", "Ashwin T S", "Naveeduddin Mohammed", "Gautam Biswas"], "categories": ["cs.CL"], "comment": "Submitted to the International Journal of Artificial Intelligence in\n  Education (IJAIED). Currently under review", "summary": "Large language models (LLMs) have created new opportunities to assist\nteachers and support student learning. While researchers have explored various\nprompt engineering approaches in educational contexts, the degree to which\nthese approaches generalize across domains--such as science, computing, and\nengineering--remains underexplored. In this paper, we introduce\nChain-of-Thought Prompting + Active Learning (CoTAL), an LLM-based approach to\nformative assessment scoring that (1) leverages Evidence-Centered Design (ECD)\nto align assessments and rubrics with curriculum goals, (2) applies\nhuman-in-the-loop prompt engineering to automate response scoring, and (3)\nincorporates chain-of-thought (CoT) prompting and teacher and student feedback\nto iteratively refine questions, rubrics, and LLM prompts. Our findings\ndemonstrate that CoTAL improves GPT-4's scoring performance across domains,\nachieving gains of up to 38.9% over a non-prompt-engineered baseline (i.e.,\nwithout labeled examples, chain-of-thought prompting, or iterative refinement).\nTeachers and students judge CoTAL to be effective at scoring and explaining\nresponses, and their feedback produces valuable insights that enhance grading\naccuracy and explanation quality.", "AI": {"tldr": "This paper introduces CoTAL, an LLM-based approach for formative assessment scoring using Evidence-Centered Design and active learning to improve scoring effectiveness across educational domains.", "motivation": "To explore the generalizability of prompt engineering approaches in educational contexts and enhance traditional formative assessment scoring methods using LLMs.", "method": "The paper presents the CoTAL framework, which integrates Evidence-Centered Design with human-in-the-loop prompt engineering, employing chain-of-thought prompting and iterative feedback from teachers and students to improve assessment processes.", "result": "CoTAL demonstrates improved scoring performance of GPT-4, achieving a 38.9% gain over a baseline approach without prompt engineering, and receives positive evaluations from teachers and students regarding its effectiveness.", "conclusion": "CoTAL not only enhances the accuracy of scoring but also improves the quality of explanations associated with assessments, highlighting the value of iterative feedback.", "key_contributions": ["Introduction of the CoTAL framework for formative assessment", "Demonstration of significant improvements in LLM performance through active learning", "Integration of teacher and student feedback into assessment processes."], "limitations": "", "keywords": ["large language models", "formative assessment", "prompt engineering"], "importance_score": 9, "read_time_minutes": 15}}
