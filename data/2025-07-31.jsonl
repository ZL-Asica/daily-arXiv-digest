{"id": "2507.22134", "pdf": "https://arxiv.org/pdf/2507.22134.pdf", "abs": "https://arxiv.org/abs/2507.22134", "title": "IntentFlow: Interactive Support for Communicating Intent with LLMs in Writing Tasks", "authors": ["Yoonsu Kim", "Brandon Chin", "Kihoon Son", "Seoyoung Kim", "Juho Kim"], "categories": ["cs.HC"], "comment": null, "summary": "While large language models (LLMs) are widely used for writing, users often\nstruggle to express their nuanced and evolving intents through prompt-based\ninterfaces. Intents -- low-level strategies or preferences for achieving a\nwriting goal -- are often vague, fluid, or even subconscious, making it\ndifficult for users to articulate and adjust them. To address this, we present\nIntentFlow, which supports the communication of dynamically evolving intents\nthroughout LLM-assisted writing. IntentFlow extracts goals and intents from\nuser prompts and presents them as editable interface components, which users\ncan revise, remove, or refine via direct manipulation or follow-up prompts.\nVisual links connect each component to the output segments it influences,\nhelping users understand model behavior. In a within-subjects study (N=12),\nparticipants using IntentFlow, compared to a chat-based baseline, expressed\ntheir intents more easily and in detail, engaged in more meaningful actions to\ncommunicate intents, such as adjusting and deleting, and produced outputs that\nbetter aligned with their evolving intents. We found that editable intent\nrepresentations help users refine and consolidate a final set of intents, which\ncan be reused across similar tasks to support consistent and transferable\nLLM-assisted writing."}
{"id": "2507.22153", "pdf": "https://arxiv.org/pdf/2507.22153.pdf", "abs": "https://arxiv.org/abs/2507.22153", "title": "Towards Privacy-preserving Photorealistic Self-avatars in Mixed Reality", "authors": ["Ethan Wilson", "Vincent Bindschaedler", "Sophie JÃ¶rg", "Sean Sheikholeslam", "Kevin Butler", "Eakta Jain"], "categories": ["cs.HC", "cs.CR"], "comment": null, "summary": "Photorealistic 3D avatar generation has rapidly improved in recent years, and\nrealistic avatars that match a user's true appearance are more feasible in\nMixed Reality (MR) than ever before. Yet, there are known risks to sharing\none's likeness online, and photorealistic MR avatars could exacerbate these\nrisks. If user likenesses were to be shared broadly, there are risks for cyber\nabuse or targeted fraud based on user appearances. We propose an alternate\navatar rendering scheme for broader social MR -- synthesizing realistic avatars\nthat preserve a user's demographic identity while being distinct enough from\nthe individual user to protect facial biometric information. We introduce a\nmethodology for privatizing appearance by isolating identity within the feature\nspace of identity-encoding generative models. We develop two algorithms that\nthen obfuscate identity: \\epsmethod{} provides differential privacy guarantees\nand \\thetamethod{} provides fine-grained control for the level of identity\noffset. These methods are shown to successfully generate de-identified virtual\navatars across multiple generative architectures in 2D and 3D. With these\ntechniques, it is possible to protect user privacy while largely preserving\nattributes related to sense of self. Employing these techniques in public\nsettings could enable the use of photorealistic avatars broadly in MR,\nmaintaining high realism and immersion without privacy risk."}
{"id": "2507.22163", "pdf": "https://arxiv.org/pdf/2507.22163.pdf", "abs": "https://arxiv.org/abs/2507.22163", "title": "IdeaBlocks: Expressing and Reusing Exploratory Intents for Design Exploration with Generative AI", "authors": ["DaEun Choi", "Kihoon Son", "Jaesang Yu", "Hyunjoon Jung", "Juho Kim"], "categories": ["cs.HC"], "comment": null, "summary": "Generative AI opens new possibilities for design exploration by rapidly\ngenerating images aligned with user goals. However, our formative study (N=7)\nrevealed three key limitations hindering designers' broad and efficient\nexploration when interacting with these models. These include difficulty\nexpressing open-ended exploratory intent, lack of continuity in exploration,\nand limited support for reusing or iterating on previous ideas. We propose\nIdeaBlocks, where users can express their exploratory intents to generative AI\nwith structured input and modularize them into Exploration Blocks. These blocks\ncan be chained for continuous, non-linear exploration and reused across\ncontexts, enabling broad exploration without losing creative momentum. Our user\nstudy with 12 designers showed that participants using IdeaBlocks explored\n112.8% more images with 12.5% greater visual diversity than the baseline. They\nalso developed ideas in more iterative and continuous patterns, such as\nbranching, chaining, and revisiting ideas. We discuss design implications for\nfuture tools to better balance divergent and convergent support during\ndifferent phases of exploration, and to capture and leverage exploratory\nintents more effectively."}
{"id": "2507.22193", "pdf": "https://arxiv.org/pdf/2507.22193.pdf", "abs": "https://arxiv.org/abs/2507.22193", "title": "DissolvPCB: Fully Recyclable 3D-Printed Electronics with Liquid Metal Conductors and PVA Substrates", "authors": ["Zeyu Yan", "SuHwan Hong", "Josiah Hester", "Tingyu Cheng", "Huaishu Peng"], "categories": ["cs.HC"], "comment": null, "summary": "We introduce DissolvPCB, an electronic prototyping technique for fabricating\nfully recyclable printed circuit board assemblies (PCBAs) using affordable FDM\n3D printing, with polyvinyl alcohol (PVA) as a water-soluble substrate and\neutectic gallium-indium (EGaIn) as the conductive material. When obsolete, the\nPCBA can be easily recycled by immersing it in water: the PVA dissolves, the\nEGaIn re-forms into a liquid metal bead, and the electronic components are\nrecovered. These materials can then be reused to fabricate a new PCBA.\n  We present the DissolvPCB workflow, characterize its design parameters,\nevaluate the performance of circuits produced with it, and quantify its\nenvironmental impact through a lifecycle assessment (LCA) comparing it to\nconventional CNC-milled FR-4 boards. We further develop a software plugin that\nautomatically converts PCB design files into 3D-printable circuit substrate\nmodels. To demonstrate the capabilities of DissolvPCB, we fabricate and recycle\nthree functional prototypes: a Bluetooth speaker featuring a double-sided PCB,\na finger fidget toy with a 3D circuit topology, and a shape-changing gripper\nenabled by Joule-heat-driven 4D printing. The paper concludes with a discussion\nof current technical limitations and opportunities for future directions."}
{"id": "2507.22159", "pdf": "https://arxiv.org/pdf/2507.22159.pdf", "abs": "https://arxiv.org/abs/2507.22159", "title": "IndoPref: A Multi-Domain Pairwise Preference Dataset for Indonesian", "authors": ["Vanessa Rebecca Wiyono", "David Anugraha", "Ayu Purwarianti", "Genta Indra Winata"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Over 200 million people speak Indonesian, yet the language remains\nsignificantly underrepresented in preference-based research for large language\nmodels (LLMs). Most existing multilingual datasets are derived from English\ntranslations, often resulting in content that lacks cultural and linguistic\nauthenticity. To address this gap, we introduce IndoPref, the first fully\nhuman-authored and multi-domain Indonesian preference dataset specifically\ndesigned to evaluate the naturalness and quality of LLM-generated text. All\nannotations are natively written in Indonesian and evaluated using\nKrippendorff's alpha, demonstrating strong inter-annotator agreement.\nAdditionally, we benchmark the dataset across multiple LLMs and assess the\noutput quality of each model."}
{"id": "2507.22241", "pdf": "https://arxiv.org/pdf/2507.22241.pdf", "abs": "https://arxiv.org/abs/2507.22241", "title": "Verisimilitude as Boon and Bane: How People Initiate Opportunistic Interactions at Professional Events in Social VR", "authors": ["Victoria Chang", "Caro Williams-Pierce", "Huaishu Peng", "Ge Gao"], "categories": ["cs.HC"], "comment": null, "summary": "Opportunistic interactions-the unstructured exchanges that emerge as\nindividuals become aware of each other's presence-are essential for\nrelationship building and information sharing in everyday life. Yet, fostering\neffective opportunistic interactions has proven challenging, especially at\nprofessional events that have increasingly transitioned from in person to\nonline formats. In the current paper, we offer an in-depth qualitative account\nof how people initiate opportunistic interactions in social VR. Our\nparticipants consisted of 16 individuals with ongoing experience attending\nVR-mediated events in their professional communities. We conducted extensive\nobservations with each participant during one or more events they attended. We\nalso interviewed them after every observed event, obtaining self-reflections on\ntheir attempts to navigate opportunistic interactions with others. Our analysis\nrevealed that participants sought to understand the extent to which social VR\npreserved the real-world meanings of various nonverbal cues, which we refer to\nas verisimilitude. We detailed the unique connections between a person's\nperceived verisimilitude and their social behaviors at each of the three steps\ntoward initiating opportunistic interactions: availability recognition,\nattention capture, and ice-breaking. Across these steps, the VR platform\ntypically replaces complex social mechanisms with feasible technical ones in\norder to function, thereby altering the preconditions necessary for a nonverbal\ncue's social meanings to remain intact. We identified a rich set of strategies\nthat participants developed to assess verisimilitude and act upon it, while\nalso confirming a lack of systematic knowledge guiding their practices. Based\non these findings, we provide actionable insights for social VR platform design\nthat can best support the initiation of opportunistic interactions for\nprofessional purposes."}
{"id": "2507.22168", "pdf": "https://arxiv.org/pdf/2507.22168.pdf", "abs": "https://arxiv.org/abs/2507.22168", "title": "Persona-Augmented Benchmarking: Evaluating LLMs Across Diverse Writing Styles", "authors": ["Kimberly Le Truong", "Riccardo Fogliato", "Hoda Heidari", "Zhiwei Steven Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current benchmarks for evaluating Large Language Models (LLMs) often do not\nexhibit enough writing style diversity, with many adhering primarily to\nstandardized conventions. Such benchmarks do not fully capture the rich variety\nof communication patterns exhibited by humans. Thus, it is possible that LLMs,\nwhich are optimized on these benchmarks, may demonstrate brittle performance\nwhen faced with \"non-standard\" input. In this work, we test this hypothesis by\nrewriting evaluation prompts using persona-based LLM prompting, a low-cost\nmethod to emulate diverse writing styles. Our results show that, even with\nidentical semantic content, variations in writing style and prompt formatting\nsignificantly impact the estimated performance of the LLM under evaluation.\nNotably, we identify distinct writing styles that consistently trigger either\nlow or high performance across a range of models and tasks, irrespective of\nmodel family, size, and recency. Our work offers a scalable approach to augment\nexisting benchmarks, improving the external validity of the assessments they\nprovide for measuring LLM performance across linguistic variations."}
{"id": "2507.22252", "pdf": "https://arxiv.org/pdf/2507.22252.pdf", "abs": "https://arxiv.org/abs/2507.22252", "title": "Multidimensional Assessment of Takeover Performance in Conditionally Automated Driving", "authors": ["Kexin Liang", "Jan Luca KÃ¤stleb", "Bani Anvarib", "Simeon C. Calverta", "J. W. C. van Lint"], "categories": ["cs.HC"], "comment": null, "summary": "When automated driving systems encounter complex situations beyond their\noperational capabilities, they issue takeover requests, prompting drivers to\nresume vehicle control and return to the driving loop as a critical safety\nbackup. However, this control transition places significant demands on drivers,\nrequiring them to promptly respond to takeover requests while executing\nhigh-quality interventions. To ensure safe and comfortable control transitions,\nit is essential to develop a deep understanding of the key factors influencing\nvarious takeover performance aspects. This study evaluates drivers' takeover\nperformance across three dimensions: response efficiency, user experience, and\ndriving safety - using a driving simulator experiment. EXtreme Gradient\nBoosting (XGBoost) models are used to investigate the contributions of two\ncritical factors, i.e., Situational Awareness (SA) and Spare Capacity (SC), in\npredicting various takeover performance metrics by comparing the predictive\nresults to the baseline models that rely solely on basic Driver Characteristics\n(DC). The results reveal that (i) higher SA enables drivers to respond to\ntakeover requests more quickly, particularly for reflexive responses; and (ii)\nSC shows a greater overall impact on takeover quality than SA, where higher SC\ngenerally leads to enhanced subjective rating scores and objective execution\ntrajectories. These findings highlight the distinct yet complementary roles of\nSA and SC in shaping performance components, offering valuable insights for\noptimizing human-vehicle interactions and enhancing automated driving system\ndesign."}
{"id": "2507.22187", "pdf": "https://arxiv.org/pdf/2507.22187.pdf", "abs": "https://arxiv.org/abs/2507.22187", "title": "A Scalable Pipeline for Estimating Verb Frame Frequencies Using Large Language Models", "authors": ["Adam M. Morgan", "Adeen Flinker"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present an automated pipeline for estimating Verb Frame Frequencies\n(VFFs), the frequency with which a verb appears in particular syntactic frames.\nVFFs provide a powerful window into syntax in both human and machine language\nsystems, but existing tools for calculating them are limited in scale,\naccuracy, or accessibility. We use large language models (LLMs) to generate a\ncorpus of sentences containing 476 English verbs. Next, by instructing an LLM\nto behave like an expert linguist, we had it analyze the syntactic structure of\nthe sentences in this corpus. This pipeline outperforms two widely used\nsyntactic parsers across multiple evaluation datasets. Furthermore, it requires\nfar fewer resources than manual parsing (the gold-standard), thereby enabling\nrapid, scalable VFF estimation. Using the LLM parser, we produce a new VFF\ndatabase with broader verb coverage, finer-grained syntactic distinctions, and\nexplicit estimates of the relative frequencies of structural alternates\ncommonly studied in psycholinguistics. The pipeline is easily customizable and\nextensible to new verbs, syntactic frames, and even other languages. We present\nthis work as a proof of concept for automated frame frequency estimation, and\nrelease all code and data to support future research."}
{"id": "2507.22262", "pdf": "https://arxiv.org/pdf/2507.22262.pdf", "abs": "https://arxiv.org/abs/2507.22262", "title": "Towards Safe and Comfortable Vehicle Control Transitions: A Systematic Review of Takeover Time, Time Budget, and Takeover Performance", "authors": ["Kexin Liang", "Simeon C. Calvert", "J. W. C. van Lint"], "categories": ["cs.HC"], "comment": null, "summary": "Conditionally automated driving systems require human drivers to disengage\nfrom non-driving-related activities and resume vehicle control within limited\ntime budgets when encountering scenarios beyond system capabilities. Ensuring\nsafe and comfortable transitions is critical for reducing driving risks and\nimproving user experience. However, takeovers involve complex human-vehicle\ninteractions, resulting in substantial variability in drivers' responses,\nespecially in takeover time, defined as the duration needed to regain control.\nThis variability presents challenges in setting sufficient time budgets that\nare neither too short (risking safety and comfort) nor too long (reducing\ndriver alertness and transition efficiency).\n  Although previous research has examined the role of time budgets in\ninfluencing takeover time and performance, few studies have systematically\naddressed how to determine sufficient time budgets that adapt to diverse\nscenarios and driver needs. This review supports such efforts by examining the\nentire takeover sequence, including takeover time, time budget, and takeover\nperformance. Specifically, we (i) synthesize causal factors influencing\ntakeover time and propose a taxonomy of its determinants using the\ntask-capability interface model; (ii) review existing work on fixed and\nadaptive time budgets, introducing the concept of the takeover buffer to\ndescribe the gap between takeover time and allocated time budget; (iii) present\na second taxonomy to support standardized and context-sensitive measurement of\ntakeover performance; (iv) propose a conceptual model describing the\nrelationships among takeover time, time budget, and performance; and (v)\noutline a research agenda with six directions."}
{"id": "2507.22201", "pdf": "https://arxiv.org/pdf/2507.22201.pdf", "abs": "https://arxiv.org/abs/2507.22201", "title": "The role of media memorability in facilitating startups' access to venture capital funding", "authors": ["L. Toschi", "S. Torrisi", "A. Fronzetti Colladon"], "categories": ["cs.CL", "cs.SI", "physics.soc-ph", "I.2.7; J.4; H.4.0"], "comment": null, "summary": "Media reputation plays an important role in attracting venture capital\ninvestment. However, prior research has focused too narrowly on general media\nexposure, limiting our understanding of how media truly influences funding\ndecisions. As informed decision-makers, venture capitalists respond to more\nnuanced aspects of media content. We introduce the concept of media\nmemorability - the media's ability to imprint a startup's name in the memory of\nrelevant investors. Using data from 197 UK startups in the micro and\nnanotechnology sector (funded between 1995 and 2004), we show that media\nmemorability significantly influences investment outcomes. Our findings suggest\nthat venture capitalists rely on detailed cues such as a startup's\ndistinctiveness and connectivity within news semantic networks. This\ncontributes to research on entrepreneurial finance and media legitimation. In\npractice, startups should go beyond frequent media mentions to strengthen brand\nmemorability through more targeted, meaningful coverage highlighting their\nuniqueness and relevance within the broader industry conversation."}
{"id": "2507.22267", "pdf": "https://arxiv.org/pdf/2507.22267.pdf", "abs": "https://arxiv.org/abs/2507.22267", "title": "Promoting Online Safety by Simulating Unsafe Conversations with LLMs", "authors": ["Owen Hoffman", "Kangze Peng", "Zehua You", "Sajid Kamal", "Sukrit Venkatagiri"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Generative AI, including large language models (LLMs) have the potential --\nand already are being used -- to increase the speed, scale, and types of unsafe\nconversations online. LLMs lower the barrier for entry for bad actors to create\nunsafe conversations in particular because of their ability to generate\npersuasive and human-like text. In our current work, we explore ways to promote\nonline safety by teaching people about unsafe conversations that can occur\nonline with and without LLMs. We build on prior work that shows that LLMs can\nsuccessfully simulate scam conversations. We also leverage research in the\nlearning sciences that shows that providing feedback on one's hypothetical\nactions can promote learning. In particular, we focus on simulating scam\nconversations using LLMs. Our work incorporates two LLMs that converse with\neach other to simulate realistic, unsafe conversations that people may\nencounter online between a scammer LLM and a target LLM but users of our system\nare asked provide feedback to the target LLM."}
{"id": "2507.22209", "pdf": "https://arxiv.org/pdf/2507.22209.pdf", "abs": "https://arxiv.org/abs/2507.22209", "title": "How Well Does First-Token Entropy Approximate Word Entropy as a Psycholinguistic Predictor?", "authors": ["Christian Clark", "Byung-Doh Oh", "William Schuler"], "categories": ["cs.CL"], "comment": null, "summary": "Contextual entropy is a psycholinguistic measure capturing the anticipated\ndifficulty of processing a word just before it is encountered. Recent studies\nhave tested for entropy-related effects as a potential complement to well-known\neffects from surprisal. For convenience, entropy is typically estimated based\non a language model's probability distribution over a word's first subword\ntoken. However, this approximation results in underestimation and potential\ndistortion of true word entropy. To address this, we generate Monte Carlo (MC)\nestimates of word entropy that allow words to span a variable number of tokens.\nRegression experiments on reading times show divergent results between\nfirst-token and MC word entropy, suggesting a need for caution in using\nfirst-token approximations of contextual entropy."}
{"id": "2507.22300", "pdf": "https://arxiv.org/pdf/2507.22300.pdf", "abs": "https://arxiv.org/abs/2507.22300", "title": "ConGaIT: A Clinician-Centered Dashboard for Contestable AI in Parkinson's Disease Care", "authors": ["Phuc Truong Loc Nguyen", "Thanh Hung Do"], "categories": ["cs.HC"], "comment": null, "summary": "AI-assisted gait analysis holds promise for improving Parkinson's Disease\n(PD) care, but current clinical dashboards lack transparency and offer no\nmeaningful way for clinicians to interrogate or contest AI decisions. We\npresent Con-GaIT (Contestable Gait Interpretation & Tracking), a\nclinician-centered system that advances Contestable AI through a tightly\nintegrated interface designed for interpretability, oversight, and procedural\nrecourse. Grounded in HCI principles, ConGaIT enables structured disagreement\nvia a novel Contest & Justify interaction pattern, supported by visual\nexplanations, role-based feedback, and traceable justification logs. Evaluated\nusing the Contestability Assessment Score (CAS), the framework achieves a score\nof 0.970, demonstrating that contestability can be operationalized through\nhuman-centered design in compliance with emerging regulatory standards. A\ndemonstration of the framework is available at\nhttps://github.com/hungdothanh/Con-GaIT."}
{"id": "2507.22219", "pdf": "https://arxiv.org/pdf/2507.22219.pdf", "abs": "https://arxiv.org/abs/2507.22219", "title": "RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine Translation", "authors": ["Dongyub Jude Lee", "Zhenyi Ye", "Pengcheng He"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Preference-learning methods for machine translation (MT)--such as Direct\nPreference Optimization (DPO)--have achieved impressive gains but depend\nheavily on large, carefully curated triplet datasets and often struggle to\ngeneralize beyond their tuning domains. We propose Reinforcement Learning from\nTeacher-Model Refinement (RLfR), a novel framework that removes reliance on\nstatic triplets by leveraging continuous, high-quality feedback from an\nexternal teacher model (GPT-4o). RLfR frames each translation step as a\nmicro-tutorial: the actor generates a hypothesis, the teacher refines it, and\nthe actor is rewarded based on how closely it aligns with the teacher's\nrefinement. Guided by two complementary signals--(i) negative edit distance,\npromoting lexical and structural fidelity, and (ii) COMET score, ensuring\nsemantic adequacy--the actor progressively learns to emulate the teacher,\nmirroring a human learning process through incremental, iterative improvement.\nOn the FLORES-200 benchmark (English to and from German, Spanish, Chinese,\nKorean, and Japanese), RLfR consistently outperforms both MT-SFT and\npreference-based baselines, significantly improving COMET (semantic adequacy)\nand M-ETA (entity preservation) scores."}
{"id": "2507.22329", "pdf": "https://arxiv.org/pdf/2507.22329.pdf", "abs": "https://arxiv.org/abs/2507.22329", "title": "A Node on the Constellation: The Role of Feminist Makerspaces in Building and Sustaining Alternative Cultures of Technology Production", "authors": ["Erin Gatz", "Yasmine Kotturi", "Andrea Afua Kwamya", "Sarah Fox"], "categories": ["cs.HC"], "comment": null, "summary": "Feminist makerspaces offer community led alternatives to dominant tech\ncultures by centering care, mutual aid, and collective knowledge production.\nWhile prior CSCW research has explored their inclusive practices, less is known\nabout how these spaces sustain themselves over time. Drawing on interviews with\n18 founders and members across 8 U.S. feminist makerspaces as well as\nautoethnographic reflection, we examine the organizational and relational\npractices that support long-term endurance. We find that sustainability is not\nachieved through growth or institutionalization, but through care-driven\nstewardship, solidarity with local justice movements, and shared governance.\nThese social practices position feminist makerspaces as prefigurative\ncounterspaces - sites that enact, rather than defer, feminist values in\neveryday practice. This paper offers empirical insight into how feminist\nmakerspaces persist amid structural precarity, and highlights the forms of\nlabor and coalition-building that underpin alternative sociotechnical\ninfrastructures."}
{"id": "2507.22286", "pdf": "https://arxiv.org/pdf/2507.22286.pdf", "abs": "https://arxiv.org/abs/2507.22286", "title": "Meaning-infused grammar: Gradient Acceptability Shapes the Geometric Representations of Constructions in LLMs", "authors": ["Supantho Rakshit", "Adele Goldberg"], "categories": ["cs.CL", "cs.AI", "68T50"], "comment": "5 pages, 3 figures, Accepted for publication at the Second\n  International Workshop on Construction Grammars and NLP at the 16th\n  International Conference for Computational Semantics (IWCS) 2025", "summary": "The usage-based constructionist (UCx) approach posits that language comprises\na network of learned form-meaning pairings (constructions) whose use is largely\ndetermined by their meanings or functions, requiring them to be graded and\nprobabilistic. This study investigates whether the internal representations in\nLarge Language Models (LLMs) reflect the proposed function-infused gradience.\nWe analyze the neural representations of the English dative constructions\n(Double Object and Prepositional Object) in Pythia-$1.4$B, using a dataset of\n$5000$ sentence pairs systematically varied for human-rated preference\nstrength. A macro-level geometric analysis finds that the separability between\nconstruction representations, as measured by Energy Distance or Jensen-Shannon\nDivergence, is systematically modulated by gradient preference strength. More\nprototypical exemplars of each construction occupy more distinct regions in the\nactivation space of LLMs. These results provide strong evidence that LLMs learn\nrich, meaning-infused, graded representations of constructions and offer\nsupport for geometric measures of basic constructionist principles in LLMs."}
{"id": "2507.22352", "pdf": "https://arxiv.org/pdf/2507.22352.pdf", "abs": "https://arxiv.org/abs/2507.22352", "title": "Mitigating Response Delays in Free-Form Conversations with LLM-powered Intelligent Virtual Agents", "authors": ["Mykola Maslych", "Mohammadreza Katebi", "Christopher Lee", "Yahya Hmaiti", "Amirpouya Ghasemaghaei", "Christian Pumarada", "Janneese Palmer", "Esteban Segarra Martinez", "Marco Emporio", "Warren Snipes", "Ryan P. McMahan", "Joseph J. LaViola Jr"], "categories": ["cs.HC", "H.1.2; H.5.2; I.2.7; I.3.7"], "comment": "15 pages, 8 figures. Published at the 7th ACM Conference on\n  Conversational User Interfaces (CUI '25), July 8-10, 2025, Waterloo, Canada.\n  Open-source code available at https://github.com/ISUE/iva-cui", "summary": "We investigated the challenges of mitigating response delays in free-form\nconversations with virtual agents powered by Large Language Models (LLMs)\nwithin Virtual Reality (VR). For this, we used conversational fillers, such as\ngestures and verbal cues, to bridge delays between user input and system\nresponses and evaluate their effectiveness across various latency levels and\ninteraction scenarios. We found that latency above 4 seconds degrades quality\nof experience, while natural conversational fillers improve perceived response\ntime, especially in high-delay conditions. Our findings provide insights for\npractitioners and researchers to optimize user engagement whenever\nconversational systems' responses are delayed by network limitations or slow\nhardware. We also contribute an open-source pipeline that streamlines deploying\nconversational agents in virtual environments."}
{"id": "2507.22289", "pdf": "https://arxiv.org/pdf/2507.22289.pdf", "abs": "https://arxiv.org/abs/2507.22289", "title": "Intent Recognition and Out-of-Scope Detection using LLMs in Multi-party Conversations", "authors": ["Galo Castillo-LÃ³pez", "GaÃ«l de Chalendar", "Nasredine Semmar"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted for publication at SIGDIAL 2025", "summary": "Intent recognition is a fundamental component in task-oriented dialogue\nsystems (TODS). Determining user intents and detecting whether an intent is\nOut-of-Scope (OOS) is crucial for TODS to provide reliable responses. However,\ntraditional TODS require large amount of annotated data. In this work we\npropose a hybrid approach to combine BERT and LLMs in zero and few-shot\nsettings to recognize intents and detect OOS utterances. Our approach leverages\nLLMs generalization power and BERT's computational efficiency in such\nscenarios. We evaluate our method on multi-party conversation corpora and\nobserve that sharing information from BERT outputs to LLMs leads to system\nperformance improvement."}
{"id": "2507.22382", "pdf": "https://arxiv.org/pdf/2507.22382.pdf", "abs": "https://arxiv.org/abs/2507.22382", "title": "A Fuzzy Set-based Approach for Matching Hand-Drawing Shapes of Touch-based Gestures for Graphical Passwords", "authors": ["Adel Sabour", "Ahmed Gadallah", "Hesham Hefny"], "categories": ["cs.HC"], "comment": null, "summary": "This paper presents a two-dimension fuzzy set based approach for matching\ntouch-based gestures using fuzzy cued click point technique. The pro posed\napproach aims mainly to improve the acceptance of the most closed inac curate\nhand drawn gestures generated by the user compared with a predefined referenced\ngesture value that is stored in the user profile. Commonly, gestures are used\nin order to facilitate the interactive capabilities between humans and\ncomputerized systems. Unfortunately, most of current gesturing techniques don't\ndeal at the same level of inaccuracy of gesturing, resulted from the nature of\nhu man fingers and hands movements. This paper aims, in a more flexible manner,\nto tackle the inaccuracy problem existed with gesture-based interactions\nbetween humans and a computerized system."}
{"id": "2507.22337", "pdf": "https://arxiv.org/pdf/2507.22337.pdf", "abs": "https://arxiv.org/abs/2507.22337", "title": "A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers", "authors": ["Roxana Petcu", "Samarth Bhargav", "Maarten de Rijke", "Evangelos Kanoulas"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Understanding and solving complex reasoning tasks is vital for addressing the\ninformation needs of a user. Although dense neural models learn contextualised\nembeddings, they still underperform on queries containing negation. To\nunderstand this phenomenon, we study negation in both traditional neural\ninformation retrieval and LLM-based models. We (1) introduce a taxonomy of\nnegation that derives from philosophical, linguistic, and logical definitions;\n(2) generate two benchmark datasets that can be used to evaluate the\nperformance of neural information retrieval models and to fine-tune models for\na more robust performance on negation; and (3) propose a logic-based\nclassification mechanism that can be used to analyze the performance of\nretrieval models on existing datasets. Our taxonomy produces a balanced data\ndistribution over negation types, providing a better training setup that leads\nto faster convergence on the NevIR dataset. Moreover, we propose a\nclassification schema that reveals the coverage of negation types in existing\ndatasets, offering insights into the factors that might affect the\ngeneralization of fine-tuned models on negation."}
{"id": "2507.22455", "pdf": "https://arxiv.org/pdf/2507.22455.pdf", "abs": "https://arxiv.org/abs/2507.22455", "title": "Analysis of User Experience Evaluation Methods for Deaf users: A Case Study on a mobile App", "authors": ["A. E. Fuentes-CortÃ¡zar", "A. Rivera-HernÃ¡ndez", "J. R. Rojano-CÃ¡ceres"], "categories": ["cs.HC", "H.5.2"], "comment": "15 pages, 2 figures, presented in Ibero-American Conference on\n  Human-Computer Interaction 2025", "summary": "User Experience (UX) evaluation methods that are commonly used with hearing\nusers may not be functional or effective for Deaf users. This is because these\nmethods are primarily designed for users with hearing abilities, which can\ncreate limitations in the interaction, perception, and understanding of the\nmethods for Deaf individuals. Furthermore, traditional UX evaluation approaches\noften fail to address the unique accessibility needs of Deaf users, resulting\nin an incomplete or biased assessment of their user experience. This research\nfocused on analyzing a set of UX evaluation methods recommended for use with\nDeaf users, with the aim of validating the accessibility of each method through\nfindings and limitations. The results indicate that, although these evaluation\nmethods presented here are commonly recommended in the literature for use with\nDeaf users, they present various limitations that must be addressed in order to\nbetter adapt to the communication skills specific to the Deaf community. This\nresearch concludes that evaluation methods must be adapted to ensure accessible\nsoftware evaluation for Deaf individuals, enabling the collection of data that\naccurately reflects their experiences and needs."}
{"id": "2507.22367", "pdf": "https://arxiv.org/pdf/2507.22367.pdf", "abs": "https://arxiv.org/abs/2507.22367", "title": "Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided LLM Representations and Multimodal Apparent Behaviors", "authors": ["Jia Li", "Yichao He", "Jiacheng Xu", "Tianhao Luo", "Zhenzhen Hu", "Richang Hong", "Meng Wang"], "categories": ["cs.CL", "cs.MM"], "comment": "8 pages, 3 figures, ACM MM 2025", "summary": "Accurate and reliable personality assessment plays a vital role in many\nfields, such as emotional intelligence, mental health diagnostics, and\npersonalized education. Unlike fleeting emotions, personality traits are\nstable, often subconsciously leaked through language, facial expressions, and\nbody behaviors, with asynchronous patterns across modalities. It was hard to\nmodel personality semantics with traditional superficial features and seemed\nimpossible to achieve effective cross-modal understanding. To address these\nchallenges, we propose a novel personality assessment framework called\n\\textit{\\textbf{Traits Run Deep}}. It employs\n\\textit{\\textbf{psychology-informed prompts}} to elicit high-level\npersonality-relevant semantic representations. Besides, it devises a\n\\textit{\\textbf{Text-Centric Trait Fusion Network}} that anchors rich text\nsemantics to align and integrate asynchronous signals from other modalities. To\nbe specific, such fusion module includes a Chunk-Wise Projector to decrease\ndimensionality, a Cross-Modal Connector and a Text Feature Enhancer for\neffective modality fusion and an ensemble regression head to improve\ngeneralization in data-scarce situations. To our knowledge, we are the first to\napply personality-specific prompts to guide large language models (LLMs) in\nextracting personality-aware semantics for improved representation quality.\nFurthermore, extracting and fusing audio-visual apparent behavior features\nfurther improves the accuracy. Experimental results on the AVI validation set\nhave demonstrated the effectiveness of the proposed components, i.e.,\napproximately a 45\\% reduction in mean squared error (MSE). Final evaluations\non the test set of the AVI Challenge 2025 confirm our method's superiority,\nranking first in the Personality Assessment track. The source code will be made\navailable at https://github.com/MSA-LMC/TraitsRunDeep."}
{"id": "2507.22614", "pdf": "https://arxiv.org/pdf/2507.22614.pdf", "abs": "https://arxiv.org/abs/2507.22614", "title": "Exploring Student-AI Interactions in Vibe Coding", "authors": ["Francis Geng", "Anshul Shah", "Haolin Li", "Nawab Mulla", "Steven Swanson", "Gerald Soosai Raj", "Daniel Zingaro", "Leo Porter"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Background and Context. Chat-based and inline-coding-based GenAI has already\nhad substantial impact on the CS Education community. The recent introduction\nof ``vibe coding'' may further transform how students program, as it introduces\na new way for students to create software projects with minimal oversight.\n  Objectives. The purpose of this study is to understand how students in\nintroductory programming and advanced software engineering classes interact\nwith a vibe coding platform (Replit) when creating software and how the\ninteractions differ by programming background.\n  Methods. Interview participants were asked to think-aloud while building a\nweb application using Replit. Thematic analysis was then used to analyze the\nvideo recordings with an emphasis on the interactions between the student and\nReplit.\n  Findings. For both groups, the majority of student interactions with Replit\nwere to test or debug the prototype and only rarely did students visit code.\nPrompts by advanced software engineering students were much more likely to\ninclude relevant app feature and codebase contexts than those by introductory\nprogramming students."}
{"id": "2507.22387", "pdf": "https://arxiv.org/pdf/2507.22387.pdf", "abs": "https://arxiv.org/abs/2507.22387", "title": "PATENTWRITER: A Benchmarking Study for Patent Drafting with LLMs", "authors": ["Homaira Huda Shomee", "Suman Kalyan Maity", "Sourav Medya"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have emerged as transformative approaches in\nseveral important fields. This paper aims for a paradigm shift for patent\nwriting by leveraging LLMs to overcome the tedious patent-filing process. In\nthis work, we present PATENTWRITER, the first unified benchmarking framework\nfor evaluating LLMs in patent abstract generation. Given the first claim of a\npatent, we evaluate six leading LLMs -- including GPT-4 and LLaMA-3 -- under a\nconsistent setup spanning zero-shot, few-shot, and chain-of-thought prompting\nstrategies to generate the abstract of the patent. Our benchmark PATENTWRITER\ngoes beyond surface-level evaluation: we systematically assess the output\nquality using a comprehensive suite of metrics -- standard NLP measures (e.g.,\nBLEU, ROUGE, BERTScore), robustness under three types of input perturbations,\nand applicability in two downstream patent classification and retrieval tasks.\nWe also conduct stylistic analysis to assess length, readability, and tone.\nExperimental results show that modern LLMs can generate high-fidelity and\nstylistically appropriate patent abstracts, often surpassing domain-specific\nbaselines. Our code and dataset are open-sourced to support reproducibility and\nfuture research."}
{"id": "2507.22671", "pdf": "https://arxiv.org/pdf/2507.22671.pdf", "abs": "https://arxiv.org/abs/2507.22671", "title": "Designing for Self-Regulation in Informal Programming Learning: Insights from a Storytelling-Centric Approach", "authors": ["Sami Saeed Alghamdi", "Christopher Bull", "Ahmed Kharrufa"], "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.SE", "H.5.2; H.5.4"], "comment": "10 pages, 9 figures", "summary": "Many people learn programming independently from online resources and often\nreport struggles in achieving their personal learning goals. Learners\nfrequently describe their experiences as isolating and frustrating, challenged\nby abundant uncertainties, information overload, and distraction, compounded by\nlimited guidance. At the same time, social media serves as a personal space\nwhere many engage in diverse self-regulation practices, including help-seeking,\nusing external memory aids (e.g., self-notes), self-reflection, emotion\nregulation, and self-motivation. For instance, learners often mark achievements\nand set milestones through their posts. In response, we developed a system\nconsisting of a web platform and browser extensions to support self-regulation\nonline. The design aims to add learner-defined structure to otherwise\nunstructured experiences and bring meaning to curation and reflection\nactivities by translating them into learning stories with AI-generated\nfeedback. We position storytelling as an integrative approach to design that\nconnects resource curation, reflective and sensemaking practice, and narrative\npractices learners already use across social platforms. We recruited 15\ninformal programming learners who are regular social media users to engage with\nthe system in a self-paced manner; participation concluded upon submitting a\nlearning story and survey. We used three quantitative scales and a qualitative\nsurvey to examine users' characteristics and perceptions of the system's\nsupport for their self-regulation. User feedback suggests the system's\nviability as a self-regulation aid. Learners particularly valued in-situ\nreflection, automated story feedback, and video annotation, while other\nfeatures received mixed views. We highlight perceived benefits, friction\npoints, and design opportunities for future AI-augmented self-regulation tools."}
{"id": "2507.22410", "pdf": "https://arxiv.org/pdf/2507.22410.pdf", "abs": "https://arxiv.org/abs/2507.22410", "title": "Question Generation for Assessing Early Literacy Reading Comprehension", "authors": ["Xiaocheng Yang", "Sumuk Shashidhar", "Dilek Hakkani-Tur"], "categories": ["cs.CL", "cs.AI"], "comment": "2 pages, 1 figure, accepted by SLaTE 2025", "summary": "Assessment of reading comprehension through content-based interactions plays\nan important role in the reading acquisition process. In this paper, we propose\na novel approach for generating comprehension questions geared to K-2 English\nlearners. Our method ensures complete coverage of the underlying material and\nadaptation to the learner's specific proficiencies, and can generate a large\ndiversity of question types at various difficulty levels to ensure a thorough\nevaluation. We evaluate the performance of various language models in this\nframework using the FairytaleQA dataset as the source material. Eventually, the\nproposed approach has the potential to become an important part of autonomous\nAI-driven English instructors."}
{"id": "2507.22810", "pdf": "https://arxiv.org/pdf/2507.22810.pdf", "abs": "https://arxiv.org/abs/2507.22810", "title": "VRISE: A Virtual Reality Platfrom for Immersive and Interactive Surveying Education", "authors": ["Daniel Udekwe", "Dimitrios Bolkas", "Eren Erman Ozguven", "Ren Moses", "Qianwen", "Guo"], "categories": ["cs.HC", "cs.ET", "cs.SE"], "comment": null, "summary": "Surveying is a core component of civil engineering education, requiring\nstudents to engage in hands-on spatial measurement, instrumentation handling,\nand field-based decision-making. However, traditional instruction often poses\nlogistical and cognitive challenges that can hinder accessibility and student\nengagement. While virtual laboratories have gained traction in engineering\neducation, few are purposefully designed to support flexible, adaptive learning\nin surveying. To address this gap, we developed Virtual Reality for Immersive\nand Interactive Surveying Education (VRISE), an immersive virtual reality\nlaboratory that replicates ground-based and aerial surveying tasks through\ncustomizable, accessible, and user-friendly modules. VRISE features interactive\nexperiences such as differential leveling with a digital level equipment and\nwaypoint-based drone navigation, enhanced by input smoothing, adaptive\ninterfaces, and real-time feedback to accommodate diverse learning styles.\nEvaluation across multiple user sessions demonstrated consistent gains in\nmeasurement accuracy, task efficiency, and interaction quality, with a clear\nprogression in skill development across the ground-based and aerial surveying\nmodalities. By reducing cognitive load and physical demands, even in tasks\nrequiring fine motor control and spatial reasoning, VRISE demonstrates the\npotential of immersive, repeatable digital environments to enhance surveying\neducation, broaden participation, and strengthen core competencies in a safe\nand engaging setting."}
{"id": "2507.22411", "pdf": "https://arxiv.org/pdf/2507.22411.pdf", "abs": "https://arxiv.org/abs/2507.22411", "title": "NeedleChain: Measuring Intact Long-Context Reasoning Capability of Large Language Models", "authors": ["Hyeonseok Moon", "Heuiseok Lim"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages", "summary": "The Needle-in-a-Haystack (NIAH) benchmark is widely used to evaluate Large\nLanguage Models' (LLMs) ability to understand long contexts (LC). It evaluates\nthe capability to identify query-relevant context within extensive\nquery-irrelevant passages. Although this method serves as a widely accepted\nstandard for evaluating long-context understanding, our findings suggest it may\noverestimate the true LC capability of LLMs. We demonstrate that even\nstate-of-the-art models such as GPT-4o struggle to intactly incorporate given\ncontexts made up of solely query-relevant ten sentences. In response, we\nintroduce a novel benchmark, \\textbf{NeedleChain}, where the context consists\nentirely of query-relevant information, requiring the LLM to fully grasp the\ninput to answer correctly. Our benchmark allows for flexible context length and\nreasoning order, offering a more comprehensive analysis of LLM performance.\nAdditionally, we propose an extremely simple yet compelling strategy to improve\nLC understanding capability of LLM: ROPE Contraction. Our experiments with\nvarious advanced LLMs reveal a notable disparity between their ability to\nprocess large contexts and their capacity to fully understand them. Source code\nand datasets are available at https://github.com/hyeonseokk/NeedleChain"}
{"id": "2507.22839", "pdf": "https://arxiv.org/pdf/2507.22839.pdf", "abs": "https://arxiv.org/abs/2507.22839", "title": "Progressive Web Application for Storytelling Therapy Support", "authors": ["Javier Jimenez-Honrado", "Javier Gomez Garcia", "Felipe Costa-Tebar", "Felix A. Marco", "Jose A. Gallud", "Gabriel Sebastian Rivera"], "categories": ["cs.HC"], "comment": "Interaccion 2024", "summary": "In spite of all advances promoted by information technologies, there are\nstill activities where this technology is not applied for reasons such as being\ncarried out in non-profit organizations or because they have not adapted to\nthis modernization. Until recently, the way to work with mobile devices was\neither by connecting through a web page with the device's browser, or by\ndownloading an application from the corresponding platform. But lately,\ntechnologies are being developed that aim to break with this, as in the case of\nProgressive Web Applications (PWA). One of the advantages offered by PWA is to\naccess the web page and install it as an application on the device. The purpose\nof this article is to design a progressive Web application for the support of\nStorytelling Therapy, one of the novel therapies applied in the field of mental\nhealth. In addition to providing a software application to enhance Storytelling\nTherapy workshops, it is also intended to analyze and verify the advantages of\nPWA in a real case."}
{"id": "2507.22445", "pdf": "https://arxiv.org/pdf/2507.22445.pdf", "abs": "https://arxiv.org/abs/2507.22445", "title": "AI-generated stories favour stability over change: homogeneity and cultural stereotyping in narratives generated by gpt-4o-mini", "authors": ["Jill Walker Rettberg", "Hermann Wigers"], "categories": ["cs.CL", "cs.AI", "H.1.2; I.2.4; I.2.0; I.2.7"], "comment": "This project has received funding from the European Union's Horizon\n  2020 research and innovation programme under grant agreement number\n  101142306. The project is also supported by the Center for Digital Narrative,\n  which is funded by the Research Council of Norway through its Centres of\n  Excellence scheme, project number 332643", "summary": "Can a language model trained largely on Anglo-American texts generate stories\nthat are culturally relevant to other nationalities? To find out, we generated\n11,800 stories - 50 for each of 236 countries - by sending the prompt \"Write a\n1500 word potential {demonym} story\" to OpenAI's model gpt-4o-mini. Although\nthe stories do include surface-level national symbols and themes, they\noverwhelmingly conform to a single narrative plot structure across countries: a\nprotagonist lives in or returns home to a small town and resolves a minor\nconflict by reconnecting with tradition and organising community events.\nReal-world conflicts are sanitised, romance is almost absent, and narrative\ntension is downplayed in favour of nostalgia and reconciliation. The result is\na narrative homogenisation: an AI-generated synthetic imaginary that\nprioritises stability above change and tradition above growth. We argue that\nthe structural homogeneity of AI-generated narratives constitutes a distinct\nform of AI bias, a narrative standardisation that should be acknowledged\nalongside the more familiar representational bias. These findings are relevant\nto literary studies, narratology, critical AI studies, NLP research, and\nefforts to improve the cultural alignment of generative AI."}
{"id": "2507.22094", "pdf": "https://arxiv.org/pdf/2507.22094.pdf", "abs": "https://arxiv.org/abs/2507.22094", "title": "Scaling and Distilling Transformer Models for sEMG", "authors": ["Nicholas Mehlman", "Jean-Christophe Gagnon-Audet", "Michael Shvartsman", "Kelvin Niu", "Alexander H. Miller", "Shagun Sodhani"], "categories": ["eess.AS", "cs.AI", "cs.HC", "cs.LG"], "comment": "Accepted at TMLR 2025 (https://openreview.net/forum?id=hFPWThwUiZ),\n  11 pages", "summary": "Surface electromyography (sEMG) signals offer a promising avenue for\ndeveloping innovative human-computer interfaces by providing insights into\nmuscular activity. However, the limited volume of training data and\ncomputational constraints during deployment have restricted the investigation\nof scaling up the model size for solving sEMG tasks. In this paper, we\ndemonstrate that vanilla transformer models can be effectively scaled up on\nsEMG data and yield improved cross-user performance up to 110M parameters,\nsurpassing the model size regime investigated in other sEMG research (usually\n<10M parameters). We show that >100M-parameter models can be effectively\ndistilled into models 50x smaller with minimal loss of performance (<1.5%\nabsolute). This results in efficient and expressive models suitable for complex\nreal-time sEMG tasks in real-world environments."}
{"id": "2507.22448", "pdf": "https://arxiv.org/pdf/2507.22448.pdf", "abs": "https://arxiv.org/abs/2507.22448", "title": "Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance", "authors": ["Jingwei Zuo", "Maksim Velikanov", "Ilyas Chahed", "Younes Belkada", "Dhia Eddine Rhayem", "Guillaume Kunsch", "Hakim Hacid", "Hamza Yous", "Brahim Farhat", "Ibrahim Khadraoui", "Mugariya Farooq", "Giulia Campesan", "Ruxandra Cojocaru", "Yasser Djilali", "Shi Hu", "Iheb Chaabane", "Puneesh Khanna", "Mohamed El Amine Seddik", "Ngoc Dung Huynh", "Phuc Le Khac", "Leen AlQadi", "Billel Mokeddem", "Mohamed Chami", "Abdalgader Abubaker", "Mikhail Lubinets", "Kacper Piskorski", "Slim Frikha"], "categories": ["cs.CL"], "comment": "Technical report of Falcon-H1 model series", "summary": "In this report, we introduce Falcon-H1, a new series of large language models\n(LLMs) featuring hybrid architecture designs optimized for both high\nperformance and efficiency across diverse use cases. Unlike earlier Falcon\nmodels built solely on Transformer or Mamba architectures, Falcon-H1 adopts a\nparallel hybrid approach that combines Transformer-based attention with State\nSpace Models (SSMs), known for superior long-context memory and computational\nefficiency. We systematically revisited model design, data strategy, and\ntraining dynamics, challenging conventional practices in the field. Falcon-H1\nis released in multiple configurations, including base and instruction-tuned\nvariants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized\ninstruction-tuned models are also available, totaling over 30 checkpoints on\nHugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and\nexceptional parameter and training efficiency. The flagship Falcon-H1-34B\nmatches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B,\nand Llama3.3-70B, while using fewer parameters and less data. Smaller models\nshow similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B\nmodels, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024.\nThese models excel across reasoning, mathematics, multilingual tasks,\ninstruction following, and scientific knowledge. With support for up to 256K\ncontext tokens and 18 languages, Falcon-H1 is suitable for a wide range of\napplications. All models are released under a permissive open-source license,\nunderscoring our commitment to accessible and impactful AI research."}
{"id": "2507.22205", "pdf": "https://arxiv.org/pdf/2507.22205.pdf", "abs": "https://arxiv.org/abs/2507.22205", "title": "CTG-Insight: A Multi-Agent Interpretable LLM Framework for Cardiotocography Analysis and Classification", "authors": ["Black Sun", "Die", "Hu"], "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "Remote fetal monitoring technologies are becoming increasingly common. Yet,\nmost current systems offer limited interpretability, leaving expectant parents\nwith raw cardiotocography (CTG) data that is difficult to understand. In this\nwork, we present CTG-Insight, a multi-agent LLM system that provides structured\ninterpretations of fetal heart rate (FHR) and uterine contraction (UC) signals.\nDrawing from established medical guidelines, CTG-Insight decomposes each CTG\ntrace into five medically defined features: baseline, variability,\naccelerations, decelerations, and sinusoidal pattern, each analyzed by a\ndedicated agent. A final aggregation agent synthesizes the outputs to deliver a\nholistic classification of fetal health, accompanied by a natural language\nexplanation. We evaluate CTG-Insight on the NeuroFetalNet Dataset and compare\nit against deep learning models and the single-agent LLM baseline. Results show\nthat CTG-Insight achieves state-of-the-art accuracy (96.4%) and F1-score\n(97.8%) while producing transparent and interpretable outputs. This work\ncontributes an interpretable and extensible CTG analysis framework."}
{"id": "2507.22457", "pdf": "https://arxiv.org/pdf/2507.22457.pdf", "abs": "https://arxiv.org/abs/2507.22457", "title": "What is an \"Abstract Reasoner\"? Revisiting Experiments and Arguments about Large Language Models", "authors": ["Tian Yun", "Chen Sun", "Ellie Pavlick"], "categories": ["cs.CL", "cs.AI"], "comment": "CONLL 2025. Project webpage: https://abstract-reasoner-llm.github.io/", "summary": "Recent work has argued that large language models (LLMs) are not \"abstract\nreasoners\", citing their poor zero-shot performance on a variety of challenging\ntasks as evidence. We revisit these experiments in order to add nuance to the\nclaim. First, we show that while LLMs indeed perform poorly in a zero-shot\nsetting, even tuning a small subset of parameters for input encoding can enable\nnear-perfect performance. However, we also show that this finetuning does not\nnecessarily transfer across datasets. We take this collection of empirical\nresults as an invitation to (re-)open the discussion of what it means to be an\n\"abstract reasoner\", and why it matters whether LLMs fit the bill."}
{"id": "2507.22358", "pdf": "https://arxiv.org/pdf/2507.22358.pdf", "abs": "https://arxiv.org/abs/2507.22358", "title": "Magentic-UI: Towards Human-in-the-loop Agentic Systems", "authors": ["Hussein Mozannar", "Gagan Bansal", "Cheng Tan", "Adam Fourney", "Victor Dibia", "Jingya Chen", "Jack Gerrits", "Tyler Payne", "Matheus Kunzler Maldaner", "Madeleine Grunde-McLaughlin", "Eric Zhu", "Griffin Bassman", "Jacob Alber", "Peter Chang", "Ricky Loynd", "Friederike Niedtner", "Ece Kamar", "Maya Murad", "Rafah Hosn", "Saleema Amershi"], "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "AI agents powered by large language models are increasingly capable of\nautonomously completing complex, multi-step tasks using external tools. Yet,\nthey still fall short of human-level performance in most domains including\ncomputer use, software development, and research. Their growing autonomy and\nability to interact with the outside world, also introduces safety and security\nrisks including potentially misaligned actions and adversarial manipulation. We\nargue that human-in-the-loop agentic systems offer a promising path forward,\ncombining human oversight and control with AI efficiency to unlock productivity\nfrom imperfect systems. We introduce Magentic-UI, an open-source web interface\nfor developing and studying human-agent interaction. Built on a flexible\nmulti-agent architecture, Magentic-UI supports web browsing, code execution,\nand file manipulation, and can be extended with diverse tools via Model Context\nProtocol (MCP). Moreover, Magentic-UI presents six interaction mechanisms for\nenabling effective, low-cost human involvement: co-planning, co-tasking,\nmulti-tasking, action guards, and long-term memory. We evaluate Magentic-UI\nacross four dimensions: autonomous task completion on agentic benchmarks,\nsimulated user testing of its interaction capabilities, qualitative studies\nwith real users, and targeted safety assessments. Our findings highlight\nMagentic-UI's potential to advance safe and efficient human-agent\ncollaboration."}
{"id": "2507.22462", "pdf": "https://arxiv.org/pdf/2507.22462.pdf", "abs": "https://arxiv.org/abs/2507.22462", "title": "IFEvalCode: Controlled Code Generation", "authors": ["Jian Yang", "Wei Zhang", "Shukai Liu", "Linzheng Chai", "Yingshui Tan", "Jiaheng Liu", "Ge Zhang", "Wangchunshu Zhou", "Guanglin Niu", "Zhoujun Li", "Binyuan Hui", "Junyang Lin"], "categories": ["cs.CL"], "comment": "10 pages", "summary": "Code large language models (Code LLMs) have made significant progress in code\ngeneration by translating natural language descriptions into functional code;\nhowever, real-world applications often demand stricter adherence to detailed\nrequirements such as coding style, line count, and structural constraints,\nbeyond mere correctness. To address this, the paper introduces forward and\nbackward constraints generation to improve the instruction-following\ncapabilities of Code LLMs in controlled code generation, ensuring outputs align\nmore closely with human-defined guidelines. The authors further present\nIFEvalCode, a multilingual benchmark comprising 1.6K test samples across seven\nprogramming languages (Python, Java, JavaScript, TypeScript, Shell, C++, and\nC#), with each sample featuring both Chinese and English queries. Unlike\nexisting benchmarks, IFEvalCode decouples evaluation into two metrics:\ncorrectness (Corr.) and instruction-following (Instr.), enabling a more nuanced\nassessment. Experiments on over 40 LLMs reveal that closed-source models\noutperform open-source ones in controllable code generation and highlight a\nsignificant gap between the models' ability to generate correct code versus\ncode that precisely follows instructions."}
{"id": "2507.22365", "pdf": "https://arxiv.org/pdf/2507.22365.pdf", "abs": "https://arxiv.org/abs/2507.22365", "title": "Beyond Accuracy: How AI Metacognitive Sensitivity improves AI-assisted Decision Making", "authors": ["ZhaoBin Li", "Mark Steyvers"], "categories": ["cs.AI", "cs.HC"], "comment": "26 pages, 5 figures, submitted to Decision Analysis", "summary": "In settings where human decision-making relies on AI input, both the\npredictive accuracy of the AI system and the reliability of its confidence\nestimates influence decision quality. We highlight the role of AI metacognitive\nsensitivity -- its ability to assign confidence scores that accurately\ndistinguish correct from incorrect predictions -- and introduce a theoretical\nframework for assessing the joint impact of AI's predictive accuracy and\nmetacognitive sensitivity in hybrid decision-making settings. Our analysis\nidentifies conditions under which an AI with lower predictive accuracy but\nhigher metacognitive sensitivity can enhance the overall accuracy of human\ndecision making. Finally, a behavioral experiment confirms that greater AI\nmetacognitive sensitivity improves human decision performance. Together, these\nfindings underscore the importance of evaluating AI assistance not only by\naccuracy but also by metacognitive sensitivity, and of optimizing both to\nachieve superior decision outcomes."}
{"id": "2507.22478", "pdf": "https://arxiv.org/pdf/2507.22478.pdf", "abs": "https://arxiv.org/abs/2507.22478", "title": "SLM-SQL: An Exploration of Small Language Models for Text-to-SQL", "authors": ["Lei Sheng", "Shuai-Shuai Xu"], "categories": ["cs.CL"], "comment": "16 pages, 2 figures, work in progress", "summary": "Large language models (LLMs) have demonstrated strong performance in\ntranslating natural language questions into SQL queries (Text-to-SQL). In\ncontrast, small language models (SLMs) ranging from 0.5B to 1.5B parameters\ncurrently underperform on Text-to-SQL tasks due to their limited logical\nreasoning capabilities. However, SLMs offer inherent advantages in inference\nspeed and suitability for edge deployment. To explore their potential in\nText-to-SQL applications, we leverage recent advancements in post-training\ntechniques. Specifically, we used the open-source SynSQL-2.5M dataset to\nconstruct two derived datasets: SynSQL-Think-916K for SQL generation and\nSynSQL-Merge-Think-310K for SQL merge revision. We then applied supervised\nfine-tuning and reinforcement learning-based post-training to the SLM, followed\nby inference using a corrective self-consistency approach. Experimental results\nvalidate the effectiveness and generalizability of our method, SLM-SQL. On the\nBIRD development set, the five evaluated models achieved an average improvement\nof 31.4 points. Notably, the 0.5B model reached 56.87\\% execution accuracy\n(EX), while the 1.5B model achieved 67.08\\% EX. We will release our dataset,\nmodel, and code to github: https://github.com/CycloneBoy/slm_sql."}
{"id": "2507.22665", "pdf": "https://arxiv.org/pdf/2507.22665.pdf", "abs": "https://arxiv.org/abs/2507.22665", "title": "Cluster-Based Random Forest Visualization and Interpretation", "authors": ["Max Sondag", "Christofer Meinecke", "Dennis Collaris", "Tatiana von Landesberger", "Stef van den Elzen"], "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "Random forests are a machine learning method used to automatically classify\ndatasets and consist of a multitude of decision trees. While these random\nforests often have higher performance and generalize better than a single\ndecision tree, they are also harder to interpret. This paper presents a\nvisualization method and system to increase interpretability of random forests.\nWe cluster similar trees which enables users to interpret how the model\nperforms in general without needing to analyze each individual decision tree in\ndetail, or interpret an oversimplified summary of the full forest. To\nmeaningfully cluster the decision trees, we introduce a new distance metric\nthat takes into account both the decision rules as well as the predictions of a\npair of decision trees. We also propose two new visualization methods that\nvisualize both clustered and individual decision trees: (1) The Feature Plot,\nwhich visualizes the topological position of features in the decision trees,\nand (2) the Rule Plot, which visualizes the decision rules of the decision\ntrees. We demonstrate the efficacy of our approach through a case study on the\n\"Glass\" dataset, which is a relatively complex standard machine learning\ndataset, as well as a small user study."}
{"id": "2507.22533", "pdf": "https://arxiv.org/pdf/2507.22533.pdf", "abs": "https://arxiv.org/abs/2507.22533", "title": "CliCARE: Grounding Large Language Models in Clinical Guidelines for Decision Support over Longitudinal Cancer Electronic Health Records", "authors": ["Dongchen Li", "Jitao Liang", "Wei Li", "Xiaoyu Wang", "Longbing Cao", "Kun Yu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) hold significant promise for improving clinical\ndecision support and reducing physician burnout by synthesizing complex,\nlongitudinal cancer Electronic Health Records (EHRs). However, their\nimplementation in this critical field faces three primary challenges: the\ninability to effectively process the extensive length and multilingual nature\nof patient records for accurate temporal analysis; a heightened risk of\nclinical hallucination, as conventional grounding techniques such as\nRetrieval-Augmented Generation (RAG) do not adequately incorporate\nprocess-oriented clinical guidelines; and unreliable evaluation metrics that\nhinder the validation of AI systems in oncology. To address these issues, we\npropose CliCARE, a framework for Grounding Large Language Models in Clinical\nGuidelines for Decision Support over Longitudinal Cancer Electronic Health\nRecords. The framework operates by transforming unstructured, longitudinal EHRs\ninto patient-specific Temporal Knowledge Graphs (TKGs) to capture long-range\ndependencies, and then grounding the decision support process by aligning these\nreal-world patient trajectories with a normative guideline knowledge graph.\nThis approach provides oncologists with evidence-grounded decision support by\ngenerating a high-fidelity clinical summary and an actionable recommendation.\nWe validated our framework using large-scale, longitudinal data from a private\nChinese cancer dataset and the public English MIMIC-IV dataset. In these\ndiverse settings, CliCARE significantly outperforms strong baselines, including\nleading long-context LLMs and Knowledge Graph-enhanced RAG methods. The\nclinical validity of our results is supported by a robust evaluation protocol,\nwhich demonstrates a high correlation with assessments made by expert\noncologists."}
{"id": "2404.13765", "pdf": "https://arxiv.org/pdf/2404.13765.pdf", "abs": "https://arxiv.org/abs/2404.13765", "title": "SciDaSynth: Interactive Structured Knowledge Extraction and Synthesis from Scientific Literature with Large Language Model", "authors": ["Xingbo Wang", "Samantha L. Huey", "Rui Sheng", "Saurabh Mehta", "Fei Wang"], "categories": ["cs.HC"], "comment": "15 pages, 7 figures. Code is available at\n  https://github.com/xingbow/SciDaEx", "summary": "Extraction and synthesis of structured knowledge from extensive scientific\nliterature are crucial for advancing and disseminating scientific progress.\nAlthough many existing systems facilitate literature review and digest, they\nstruggle to process multimodal, varied, and inconsistent information within and\nacross the literature into structured data. We introduce SciDaSynth, a novel\ninteractive system powered by large language models (LLMs) that enables\nresearchers to efficiently build structured knowledge bases from scientific\nliterature at scale. The system automatically creates data tables to organize\nand summarize users' interested knowledge in literature via question-answering.\nFurthermore, it provides multi-level and multi-faceted exploration of the\ngenerated data tables, facilitating iterative validation, correction, and\nrefinement. Our within-subjects study with researchers demonstrates the\neffectiveness and efficiency of SciDaSynth in constructing quality scientific\nknowledge bases. We further discuss the design implications for human-AI\ninteraction tools for data extraction and structuring."}
{"id": "2507.22542", "pdf": "https://arxiv.org/pdf/2507.22542.pdf", "abs": "https://arxiv.org/abs/2507.22542", "title": "A Benchmark Dataset and Evaluation Framework for Vietnamese Large Language Models in Customer Support", "authors": ["Long S. T. Nguyen", "Truong P. Hua", "Thanh M. Nguyen", "Toan Q. Pham", "Nam K. Ngo", "An X. Nguyen", "Nghi D. M. Pham", "Nghia H. Nguyen", "Tho T. Quan"], "categories": ["cs.CL"], "comment": "Under review at ICCCI 2025", "summary": "With the rapid growth of Artificial Intelligence, Large Language Models\n(LLMs) have become essential for Question Answering (QA) systems, improving\nefficiency and reducing human workload in customer service. The emergence of\nVietnamese LLMs (ViLLMs) highlights lightweight open-source models as a\npractical choice for their accuracy, efficiency, and privacy benefits. However,\ndomain-specific evaluations remain limited, and the absence of benchmark\ndatasets reflecting real customer interactions makes it difficult for\nenterprises to select suitable models for support applications. To address this\ngap, we introduce the Customer Support Conversations Dataset (CSConDa), a\ncurated benchmark of over 9,000 QA pairs drawn from real interactions with\nhuman advisors at a large Vietnamese software company. Covering diverse topics\nsuch as pricing, product availability, and technical troubleshooting, CSConDa\nprovides a representative basis for evaluating ViLLMs in practical scenarios.\nWe further present a comprehensive evaluation framework, benchmarking 11\nlightweight open-source ViLLMs on CSConDa with both automatic metrics and\nsyntactic analysis to reveal model strengths, weaknesses, and linguistic\npatterns. This study offers insights into model behavior, explains performance\ndifferences, and identifies key areas for improvement, supporting the\ndevelopment of next-generation ViLLMs. By establishing a robust benchmark and\nsystematic evaluation, our work enables informed model selection for customer\nservice QA and advances research on Vietnamese LLMs. The dataset is publicly\navailable at\nhttps://huggingface.co/datasets/ura-hcmut/Vietnamese-Customer-Support-QA."}
{"id": "2411.03295", "pdf": "https://arxiv.org/pdf/2411.03295.pdf", "abs": "https://arxiv.org/abs/2411.03295", "title": "Examining Human-AI Collaboration for Co-Writing Constructive Comments Online", "authors": ["Farhana Shahid", "Maximilian Dittgen", "Mor Naaman", "Aditya Vashistha"], "categories": ["cs.HC"], "comment": null, "summary": "This paper examines if large language models (LLMs) can help people write\nconstructive comments on divisive social issues due to the difficulty of\nexpressing constructive disagreement online. Through controlled experiments\nwith 600 participants from India and the US, who reviewed and wrote\nconstructive comments on threads related to Islamophobia and homophobia, we\nobserved potential misalignment between how LLMs and humans perceive\nconstructiveness in online comments. While the LLM was more likely to\nprioritize politeness and balance among contrasting viewpoints when evaluating\nconstructiveness, participants emphasized logic and facts more than the LLM\ndid. Despite these differences, participants rated both LLM-generated and\nhuman-AI co-written comments as significantly more constructive than those\nwritten independently by humans. Our analysis also revealed that LLM-generated\ncomments integrated significantly more linguistic features of constructiveness\ncompared to human-written comments. When participants used LLMs to refine their\ncomments, the resulting comments were more constructive, more positive, less\ntoxic, and retained the original intent. However, occasionally LLMs distorted\npeople's original views -- especially when their stances were not outright\npolarizing. Based on these findings, we discuss ethical and design\nconsiderations in using LLMs to facilitate constructive discourse online."}
{"id": "2507.22545", "pdf": "https://arxiv.org/pdf/2507.22545.pdf", "abs": "https://arxiv.org/abs/2507.22545", "title": "ControlMed: Adding Reasoning Control to Medical Language Model", "authors": ["Sung-Min Lee", "Siyoon Lee", "Juyeon Kim", "Kyungmin Roh"], "categories": ["cs.CL"], "comment": "13 pages", "summary": "Reasoning Large Language Models (LLMs) with enhanced accuracy and\nexplainability are increasingly being adopted in the medical domain, as the\nlife-critical nature of clinical decision-making demands reliable support.\nDespite these advancements, existing reasoning LLMs often generate\nunnecessarily lengthy reasoning processes, leading to significant computational\noverhead and response latency. These limitations hinder their practical\ndeployment in real-world clinical environments. To address these challenges, we\nintroduce \\textbf{ControlMed}, a medical language model that enables users to\nactively control the length of the reasoning process at inference time through\nfine-grained control markers. ControlMed is trained through a three-stage\npipeline: 1) pre-training on a large-scale synthetic medical instruction\ndataset covering both \\textit{direct} and \\textit{reasoning responses}; 2)\nsupervised fine-tuning with multi-length reasoning data and explicit\nlength-control markers; and 3) reinforcement learning with model-based reward\nsignals to enhance factual accuracy and response quality. Experimental results\non a variety of English and Korean medical benchmarks demonstrate that our\nmodel achieves similar or better performance compared to state-of-the-art\nmodels. Furthermore, users can flexibly balance reasoning accuracy and\ncomputational efficiency by controlling the reasoning length as needed. These\nfindings demonstrate that ControlMed is a practical and adaptable solution for\nclinical question answering and medical information analysis."}
{"id": "2504.11257", "pdf": "https://arxiv.org/pdf/2504.11257.pdf", "abs": "https://arxiv.org/abs/2504.11257", "title": "UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis", "authors": ["Xinyi Liu", "Xiaoyi Zhang", "Ziyun Zhang", "Yan Lu"], "categories": ["cs.HC", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advancements in Large Vision-Language Models are accelerating the\ndevelopment of Graphical User Interface (GUI) agents that utilize human-like\nvision perception capabilities to enhance productivity on digital devices.\nCompared to approaches predicated on GUI metadata, which are platform-dependent\nand vulnerable to implementation variations, vision-based approaches offer\nbroader applicability. In this vision-based paradigm, the GUI instruction\ngrounding, which maps user instruction to the location of corresponding element\non the given screenshot, remains a critical challenge, particularly due to\nlimited public training dataset and resource-intensive manual instruction data\nannotation. In this paper, we delve into unexplored challenges in this task\nincluding element-to-screen ratio, unbalanced element type, and implicit\ninstruction. To address these challenges, we introduce a large-scale data\nsynthesis pipeline UI-E2I-Synth for generating varying complex instruction\ndatasets using GPT-4o instead of human annotators. Furthermore, we propose a\nnew GUI instruction grounding benchmark UI-I2E-Bench, which is designed to\naddress the limitations of existing benchmarks by incorporating diverse\nannotation aspects. Our model, trained on the synthesized data, achieves\nsuperior performance in GUI instruction grounding, demonstrating the\nadvancements of proposed data synthesis pipeline. The proposed benchmark,\naccompanied by extensive analyses, provides practical insights for future\nresearch in GUI grounding. We will release corresponding artifacts at\nhttps://microsoft.github.io/FIVE-UI-Evol/ ."}
{"id": "2507.22564", "pdf": "https://arxiv.org/pdf/2507.22564.pdf", "abs": "https://arxiv.org/abs/2507.22564", "title": "Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs", "authors": ["Xikang Yang", "Biyu Zhou", "Xuehai Tang", "Jizhong Han", "Songlin Hu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate impressive capabilities across a\nwide range of tasks, yet their safety mechanisms remain susceptible to\nadversarial attacks that exploit cognitive biases -- systematic deviations from\nrational judgment. Unlike prior jailbreaking approaches focused on prompt\nengineering or algorithmic manipulation, this work highlights the overlooked\npower of multi-bias interactions in undermining LLM safeguards. We propose\nCognitiveAttack, a novel red-teaming framework that systematically leverages\nboth individual and combined cognitive biases. By integrating supervised\nfine-tuning and reinforcement learning, CognitiveAttack generates prompts that\nembed optimized bias combinations, effectively bypassing safety protocols while\nmaintaining high attack success rates. Experimental results reveal significant\nvulnerabilities across 30 diverse LLMs, particularly in open-source models.\nCognitiveAttack achieves a substantially higher attack success rate compared to\nthe SOTA black-box method PAP (60.1% vs. 31.6%), exposing critical limitations\nin current defense mechanisms. These findings highlight multi-bias interactions\nas a powerful yet underexplored attack vector. This work introduces a novel\ninterdisciplinary perspective by bridging cognitive science and LLM safety,\npaving the way for more robust and human-aligned AI systems."}
{"id": "2507.20300", "pdf": "https://arxiv.org/pdf/2507.20300.pdf", "abs": "https://arxiv.org/abs/2507.20300", "title": "Talking-to-Build: How LLM-Assisted Interface Shapes Player Performance and Experience in Minecraft", "authors": ["Xin Sun", "Lei Wang", "Yue Li", "Jie Li", "Massimo Poesio", "Julian Frommel", "Koen Hinriks", "Jiahuan Pei"], "categories": ["cs.HC", "cs.MM"], "comment": null, "summary": "With large language models (LLMs) on the rise, in-game interactions are\nshifting from rigid commands to natural conversations. However, the impacts of\nLLMs on player performance and game experience remain underexplored. This work\nexplores LLM's role as a co-builder during gameplay, examining its impact on\ntask performance, usability, and player experience. Using Minecraft as a\nsandbox, we present an LLM-assisted interface that engages players through\nnatural language, aiming to facilitate creativity and simplify complex gaming\ncommands. We conducted a mixed-methods study with 30 participants, comparing\nLLM-assisted and command-based interfaces across simple and complex game tasks.\nQuantitative and qualitative analyses reveal that the LLM-assisted interface\nsignificantly improves player performance, engagement, and overall game\nexperience. Additionally, task complexity has a notable effect on player\nperformance and experience across both interfaces. Our findings highlight the\npotential of LLM-assisted interfaces to revolutionize virtual experiences,\nemphasizing the importance of balancing intuitiveness with predictability,\ntransparency, and user agency in AI-driven, multimodal gaming environments."}
{"id": "2507.22581", "pdf": "https://arxiv.org/pdf/2507.22581.pdf", "abs": "https://arxiv.org/abs/2507.22581", "title": "Unveiling the Influence of Amplifying Language-Specific Neurons", "authors": ["Inaya Rahmanisa", "Lyzander Marciano Andrylie", "Krisna Mahardika Ihsani", "Alfan Farizki Wicaksono", "Haryo Akbarianto Wibowo", "Alham Fikri Aji"], "categories": ["cs.CL", "cs.LG"], "comment": "Our code and dataset are made available at\n  https://github.com/tauimbz/lang-task-neuron", "summary": "Language-specific neurons in LLMs that strongly correlate with individual\nlanguages have been shown to influence model behavior by deactivating them.\nHowever, their role in amplification remains underexplored. This work\ninvestigates the effect of amplifying language-specific neurons through\ninterventions across 18 languages, including low-resource ones, using three\nmodels primarily trained in different languages. We compare amplification\nfactors by their effectiveness in steering to the target language using a\nproposed Language Steering Shift (LSS) evaluation score, then evaluate it on\ndownstream tasks: commonsense reasoning (XCOPA, XWinograd), knowledge\n(Include), and translation (FLORES). The optimal amplification factors\neffectively steer output toward nearly all tested languages. Intervention using\nthis factor on downstream tasks improves self-language performance in some\ncases but generally degrades cross-language results. These findings highlight\nthe effect of language-specific neurons in multilingual behavior, where\namplification can be beneficial especially for low-resource languages, but\nprovides limited advantage for cross-lingual transfer."}
{"id": "2507.21088", "pdf": "https://arxiv.org/pdf/2507.21088.pdf", "abs": "https://arxiv.org/abs/2507.21088", "title": "Eliciting User Requirements for AI-Enhanced Learning Environments using a Participatory Approach", "authors": ["Bibeg Limbu", "Irene-Angelica Chounta", "Vilma Sukacke", "Andromachi Filippidi", "Chara Spyropoulou", "Marianna Anagnostopoulou", "Eleftheria Tsourlidaki", "Nikos Karacapilidis"], "categories": ["cs.HC"], "comment": "12 pages, 2 figures, 15th International Conference on Methodologies\n  and Intelligent Systems for Technology Enhanced Learning (mis4tel), Workshop\n  Track: Workshop on Integration of Emerging Technologies into Education and\n  Training (ETELT) https://mis4tel-conference.net/tracks/workshops/etelt,\n  accepted", "summary": "This paper explores the needs and expectations of educational stakeholders\nfor AI (Artificial Intelligence)-enhanced learning environments. Data was\ncollected following two-phased participatory workshops. The first workshop\noutlined stakeholders' profiles in terms of technical and pedagogical\ncharacteristics. The qualitative data collected was analysed using deductive\nthematic analysis with Activity Theory, explicating the user needs. The second\nworkshop articulated expectations related to the integration of AI in\neducation. Inductive thematic analysis of the second workshop led to the\nelicitation of users' expectations. We cross-examined the needs and\nexpectations, identifying contradictions, to generate user requirements for\nemerging technologies. The paper provides suggestions for future design\ninitiatives that incorporate AI in learning environments."}
{"id": "2507.22603", "pdf": "https://arxiv.org/pdf/2507.22603.pdf", "abs": "https://arxiv.org/abs/2507.22603", "title": "BALSAM: A Platform for Benchmarking Arabic Large Language Models", "authors": ["Rawan Al-Matham", "Kareem Darwish", "Raghad Al-Rasheed", "Waad Alshammari", "Muneera Alhoshan", "Amal Almazrua", "Asma Al Wazrah", "Mais Alheraki", "Firoj Alam", "Preslav Nakov", "Norah Alzahrani", "Eman alBilali", "Nizar Habash", "Abdelrahman El-Sheikh", "Muhammad Elmallah", "Haonan Li", "Hamdy Mubarak", "Mohamed Anwar", "Zaid Alyafeai", "Ahmed Abdelali", "Nora Altwairesh", "Maram Hasanain", "Abdulmohsen Al Thubaity", "Shady Shehata", "Bashar Alhafni", "Injy Hamed", "Go Inoue", "Khalid Elmadani", "Ossama Obeid", "Fatima Haouari", "Tamer Elsayed", "Emad Alghamdi", "Khalid Almubarak", "Saied Alshahrani", "Ola Aljarrah", "Safa Alajlan", "Areej Alshaqarawi", "Maryam Alshihri", "Sultana Alghurabi", "Atikah Alzeghayer", "Afrah Altamimi", "Abdullah Alfaifi", "Abdulrahman AlOsaimy"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The impressive advancement of Large Language Models (LLMs) in English has not\nbeen matched across all languages. In particular, LLM performance in Arabic\nlags behind, due to data scarcity, linguistic diversity of Arabic and its\ndialects, morphological complexity, etc. Progress is further hindered by the\nquality of Arabic benchmarks, which typically rely on static, publicly\navailable data, lack comprehensive task coverage, or do not provide dedicated\nplatforms with blind test sets. This makes it challenging to measure actual\nprogress and to mitigate data contamination. Here, we aim to bridge these gaps.\nIn particular, we introduce BALSAM, a comprehensive, community-driven benchmark\naimed at advancing Arabic LLM development and evaluation. It includes 78 NLP\ntasks from 14 broad categories, with 52K examples divided into 37K test and 15K\ndevelopment, and a centralized, transparent platform for blind evaluation. We\nenvision BALSAM as a unifying platform that sets standards and promotes\ncollaborative research to advance Arabic LLM capabilities."}
{"id": "2507.22051", "pdf": "https://arxiv.org/pdf/2507.22051.pdf", "abs": "https://arxiv.org/abs/2507.22051", "title": "DataSway: Vivifying Metaphoric Visualization with Animation Clip Generation and Coordination", "authors": ["Liwenhan Xie", "Jiayi Zhou", "Anyi Rao", "Huamin Qu", "Xinhuan Shu"], "categories": ["cs.HC"], "comment": "19 pages, 5 figures; Website:\n  https://shellywhen.github.io/projects/DataSway", "summary": "Animating metaphoric visualizations brings data to life, enhancing the\ncomprehension of abstract data encodings and fostering deeper engagement.\nHowever, creators face significant challenges in designing these animations,\nsuch as crafting motions that align semantically with the metaphors,\nmaintaining faithful data representation during animation, and seamlessly\nintegrating interactivity. We propose a human-AI co-creation workflow that\nfacilitates creating animations for SVG-based metaphoric visualizations. Users\ncan initially derive animation clips for data elements from vision-language\nmodels (VLMs) and subsequently coordinate their timelines based on entity\norder, attribute values, spatial layout, or randomness. Our design decisions\nwere informed by a formative study with experienced designers (N=8). We further\ndeveloped a prototype, DataSway, and conducted a user study (N=14) to evaluate\nits creativity support and usability. A gallery with 6 cases demonstrates its\ncapabilities and applications in web-based hypermedia. We conclude with\nimplications for future research on bespoke data visualization animation."}
{"id": "2507.22608", "pdf": "https://arxiv.org/pdf/2507.22608.pdf", "abs": "https://arxiv.org/abs/2507.22608", "title": "Language Arithmetics: Towards Systematic Language Neuron Identification and Manipulation", "authors": ["Daniil Gurgurov", "Katharina Trinley", "Yusser Al Ghussin", "Tanja Baeumel", "Josef van Genabith", "Simon Ostermann"], "categories": ["cs.CL"], "comment": "preprint", "summary": "Large language models (LLMs) exhibit strong multilingual abilities, yet the\nneural mechanisms behind language-specific processing remain unclear. We\nanalyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and\nAya-Expanse-8B & 32B across 21 typologically diverse languages, identifying\nneurons that control language behavior. Using the Language Activation\nProbability Entropy (LAPE) method, we show that these neurons cluster in deeper\nlayers, with non-Latin scripts showing greater specialization. Related\nlanguages share overlapping neurons, reflecting internal representations of\nlinguistic proximity.\n  Through language arithmetics, i.e. systematic activation addition and\nmultiplication, we steer models to deactivate unwanted languages and activate\ndesired ones, outperforming simpler replacement approaches. These interventions\neffectively guide behavior across five multilingual tasks: language forcing,\ntranslation, QA, comprehension, and NLI. Manipulation is more successful for\nhigh-resource languages, while typological similarity improves effectiveness.\nWe also demonstrate that cross-lingual neuron steering enhances downstream\nperformance and reveal internal \"fallback\" mechanisms for language selection\nwhen neurons are progressively deactivated. Our code is made publicly available\nat https://github.com/d-gurgurov/Language-Neurons-Manipulation."}
{"id": "2404.08939", "pdf": "https://arxiv.org/pdf/2404.08939.pdf", "abs": "https://arxiv.org/abs/2404.08939", "title": "NeurIT: Pushing the Limit of Neural Inertial Tracking for Indoor Robotic IoT", "authors": ["Xinzhe Zheng", "Sijie Ji", "Yipeng Pan", "Kaiwen Zhang", "Chenshu Wu"], "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": null, "summary": "Inertial tracking is vital for robotic IoT and has gained popularity thanks\nto the ubiquity of low-cost inertial measurement units and deep\nlearning-powered tracking algorithms. Existing works, however, have not fully\nutilized IMU measurements, particularly magnetometers, nor have they maximized\nthe potential of deep learning to achieve the desired accuracy. To address\nthese limitations, we introduce NeurIT, which elevates tracking accuracy to a\nnew level. NeurIT employs a Time-Frequency Block-recurrent Transformer (TF-BRT)\nat its core, combining both RNN and Transformer to learn representative\nfeatures in both time and frequency domains. To fully utilize IMU information,\nwe strategically employ body-frame differentiation of magnetometers,\nconsiderably reducing the tracking error. We implement NeurIT on a customized\nrobotic platform and conduct evaluation in various indoor environments.\nExperimental results demonstrate that NeurIT achieves a mere 1-meter tracking\nerror over a 300-meter distance. Notably, it significantly outperforms\nstate-of-the-art baselines by 48.21% on unseen data. Moreover, NeurIT\ndemonstrates robustness in large urban complexes and performs comparably to the\nvisual-inertial approach (Tango Phone) in vision-favored conditions while\nsurpassing it in feature-sparse settings. We believe NeurIT takes an important\nstep forward toward practical neural inertial tracking for ubiquitous and\nscalable tracking of robotic things. NeurIT is open-sourced here:\nhttps://github.com/aiot-lab/NeurIT."}
{"id": "2507.22623", "pdf": "https://arxiv.org/pdf/2507.22623.pdf", "abs": "https://arxiv.org/abs/2507.22623", "title": "Multilingual Political Views of Large Language Models: Identification and Steering", "authors": ["Daniil Gurgurov", "Katharina Trinley", "Ivan Vykopal", "Josef van Genabith", "Simon Ostermann", "Roberto Zamparelli"], "categories": ["cs.CL"], "comment": "pre-print", "summary": "Large language models (LLMs) are increasingly used in everyday tools and\napplications, raising concerns about their potential influence on political\nviews. While prior research has shown that LLMs often exhibit measurable\npolitical biases--frequently skewing toward liberal or progressive\npositions--key gaps remain. Most existing studies evaluate only a narrow set of\nmodels and languages, leaving open questions about the generalizability of\npolitical biases across architectures, scales, and multilingual settings.\nMoreover, few works examine whether these biases can be actively controlled.\n  In this work, we address these gaps through a large-scale study of political\norientation in modern open-source instruction-tuned LLMs. We evaluate seven\nmodels, including LLaMA-3.1, Qwen-3, and Aya-Expanse, across 14 languages using\nthe Political Compass Test with 11 semantically equivalent paraphrases per\nstatement to ensure robust measurement. Our results reveal that larger models\nconsistently shift toward libertarian-left positions, with significant\nvariations across languages and model families. To test the manipulability of\npolitical stances, we utilize a simple center-of-mass activation intervention\ntechnique and show that it reliably steers model responses toward alternative\nideological positions across multiple languages. Our code is publicly available\nat https://github.com/d-gurgurov/Political-Ideologies-LLMs."}
{"id": "2503.03044", "pdf": "https://arxiv.org/pdf/2503.03044.pdf", "abs": "https://arxiv.org/abs/2503.03044", "title": "QE4PE: Word-level Quality Estimation for Human Post-Editing", "authors": ["Gabriele Sarti", "VilÃ©m Zouhar", "Grzegorz ChrupaÅa", "Ana Guerberof-Arenas", "Malvina Nissim", "Arianna Bisazza"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted by TACL (pre-MIT Press publication version); Code:\n  https://github.com/gsarti/qe4pe. Dataset:\n  https://huggingface.co/datasets/gsarti/qe4pe", "summary": "Word-level quality estimation (QE) methods aim to detect erroneous spans in\nmachine translations, which can direct and facilitate human post-editing. While\nthe accuracy of word-level QE systems has been assessed extensively, their\nusability and downstream influence on the speed, quality and editing choices of\nhuman post-editing remain understudied. In this study, we investigate the\nimpact of word-level QE on machine translation (MT) post-editing in a realistic\nsetting involving 42 professional post-editors across two translation\ndirections. We compare four error-span highlight modalities, including\nsupervised and uncertainty-based word-level QE methods, for identifying\npotential errors in the outputs of a state-of-the-art neural MT model.\nPost-editing effort and productivity are estimated from behavioral logs, while\nquality improvements are assessed by word- and segment-level human annotation.\nWe find that domain, language and editors' speed are critical factors in\ndetermining highlights' effectiveness, with modest differences between\nhuman-made and automated QE highlights underlining a gap between accuracy and\nusability in professional workflows."}
{"id": "2507.22676", "pdf": "https://arxiv.org/pdf/2507.22676.pdf", "abs": "https://arxiv.org/abs/2507.22676", "title": "Listening to the Unspoken: Exploring 365 Aspects of Multimodal Interview Performance Assessment", "authors": ["Jia Li", "Yang Wang", "Wenhao Qian", "Zhenzhen Hu", "Richang Hong", "Meng Wang"], "categories": ["cs.CL", "cs.MM"], "comment": "8 pages, 4 figures, ACM MM 2025.\n  github:https://github.com/MSA-LMC/365Aspects", "summary": "Interview performance assessment is essential for determining candidates'\nsuitability for professional positions. To ensure holistic and fair\nevaluations, we propose a novel and comprehensive framework that explores\n``365'' aspects of interview performance by integrating \\textit{three}\nmodalities (video, audio, and text), \\textit{six} responses per candidate, and\n\\textit{five} key evaluation dimensions. The framework employs\nmodality-specific feature extractors to encode heterogeneous data streams and\nsubsequently fused via a Shared Compression Multilayer Perceptron. This module\ncompresses multimodal embeddings into a unified latent space, facilitating\nefficient feature interaction. To enhance prediction robustness, we incorporate\na two-level ensemble learning strategy: (1) independent regression heads\npredict scores for each response, and (2) predictions are aggregated across\nresponses using a mean-pooling mechanism to produce final scores for the five\ntarget dimensions. By listening to the unspoken, our approach captures both\nexplicit and implicit cues from multimodal data, enabling comprehensive and\nunbiased assessments. Achieving a multi-dimensional average MSE of 0.1824, our\nframework secured first place in the AVI Challenge 2025, demonstrating its\neffectiveness and robustness in advancing automated and multimodal interview\nperformance assessment. The full implementation is available at\nhttps://github.com/MSA-LMC/365Aspects."}
{"id": "2504.05008", "pdf": "https://arxiv.org/pdf/2504.05008.pdf", "abs": "https://arxiv.org/abs/2504.05008", "title": "Voices of Freelance Professional Writers on AI: Limitations, Expectations, and Fears", "authors": ["Anastasiia Ivanova", "Natalia Fedorova", "Sergei Tilga", "Ekaterina Artemova"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "The rapid development of AI-driven tools, particularly large language models\n(LLMs), is reshaping professional writing. Still, key aspects of their adoption\nsuch as languages support, ethics, and long-term impact on writers voice and\ncreativity remain underexplored. In this work, we conducted a questionnaire (N\n= 301) and an interactive survey (N = 36) targeting professional writers\nregularly using AI. We examined LLM-assisted writing practices across 25+\nlanguages, ethical concerns, and user expectations. The findings of the survey\ndemonstrate important insights, reflecting upon the importance of: LLMs\nadoption for non-English speakers; the degree of misinformation, domain and\nstyle adaptation; usability and key features of LLMs. These insights can guide\nfurther development, benefiting both writers and a broader user base."}
{"id": "2507.22716", "pdf": "https://arxiv.org/pdf/2507.22716.pdf", "abs": "https://arxiv.org/abs/2507.22716", "title": "From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in Retrieval-Augmented Reasoning for LLMs", "authors": ["Jie He", "Victor Gutierrez Basulto", "Jeff Z. Pan"], "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning-based retrieval-augmented generation (RAG) methods\nenhance the reasoning abilities of large language models (LLMs). However, most\nrely only on final-answer rewards, overlooking intermediate reasoning quality.\nThis paper analyzes existing RAG reasoning models and identifies three main\nfailure patterns: (1) information insufficiency, meaning the model fails to\nretrieve adequate support; (2) faulty reasoning, where logical or content-level\nflaws appear despite sufficient information; and (3) answer-reasoning\ninconsistency, where a valid reasoning chain leads to a mismatched final\nanswer. We propose TIRESRAG-R1, a novel framework using a\nthink-retrieve-reflect process and a multi-dimensional reward system to improve\nreasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to\nencourage thorough retrieval; (2) a reasoning quality reward to assess the\nrationality and accuracy of the reasoning chain; and (3) a reflection reward to\ndetect and revise errors. It also employs a difficulty-aware reweighting\nstrategy and training sample filtering to boost performance on complex tasks.\nExperiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms\nprior RAG methods and generalizes well to single-hop tasks. The code and data\nare available at: https://github.com/probe2/TIRESRAG-R1."}
{"id": "2506.15085", "pdf": "https://arxiv.org/pdf/2506.15085.pdf", "abs": "https://arxiv.org/abs/2506.15085", "title": "EmojiVoice: Towards long-term controllable expressivity in robot speech", "authors": ["Paige TuttÃ¶sÃ­", "Shivam Mehta", "Zachary Syvenky", "Bermet Burkanova", "Gustav Eje Henter", "Angelica Lim"], "categories": ["cs.RO", "cs.HC"], "comment": "Accepted to RO-MAN 2025, Demo at HRI 2025 :\n  https://dl.acm.org/doi/10.5555/3721488.3721774 Project webpage here:\n  https://rosielab.github.io/emojivoice/ Toolbox here:\n  https://github.com/rosielab/emojivoice", "summary": "Humans vary their expressivity when speaking for extended periods to maintain\nengagement with their listener. Although social robots tend to be deployed with\n``expressive'' joyful voices, they lack this long-term variation found in human\nspeech. Foundation model text-to-speech systems are beginning to mimic the\nexpressivity in human speech, but they are difficult to deploy offline on\nrobots. We present EmojiVoice, a free, customizable text-to-speech (TTS)\ntoolkit that allows social roboticists to build temporally variable, expressive\nspeech on social robots. We introduce emoji-prompting to allow fine-grained\ncontrol of expressivity on a phase level and use the lightweight Matcha-TTS\nbackbone to generate speech in real-time. We explore three case studies: (1) a\nscripted conversation with a robot assistant, (2) a storytelling robot, and (3)\nan autonomous speech-to-speech interactive agent. We found that using varied\nemoji prompting improved the perception and expressivity of speech over a long\nperiod in a storytelling task, but expressive voice was not preferred in the\nassistant use case."}
{"id": "2507.22720", "pdf": "https://arxiv.org/pdf/2507.22720.pdf", "abs": "https://arxiv.org/abs/2507.22720", "title": "Investigating Hallucination in Conversations for Low Resource Languages", "authors": ["Amit Das", "Md. Najib Hasan", "Souvika Sarkar", "Zheng Zhang", "Fatemeh Jamshidi", "Tathagata Bhattacharya", "Nilanjana Raychawdhury", "Dongji Feng", "Vinija Jain", "Aman Chadha"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating text that closely resemble human writing. However, they often\ngenerate factually incorrect statements, a problem typically referred to as\n'hallucination'. Addressing hallucination is crucial for enhancing the\nreliability and effectiveness of LLMs. While much research has focused on\nhallucinations in English, our study extends this investigation to\nconversational data in three languages: Hindi, Farsi, and Mandarin. We offer a\ncomprehensive analysis of a dataset to examine both factual and linguistic\nerrors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0,\nDeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated\nresponses in Mandarin but generate a significantly higher number of\nhallucinations in Hindi and Farsi."}
{"id": "2506.15107", "pdf": "https://arxiv.org/pdf/2506.15107.pdf", "abs": "https://arxiv.org/abs/2506.15107", "title": "I Know You're Listening: Adaptive Voice for HRI", "authors": ["Paige TuttÃ¶sÃ­"], "categories": ["cs.RO", "cs.HC", "cs.SD", "eess.AS"], "comment": "PhD Thesis Simon Fraser University https://summit.sfu.ca/item/39353\n  Read the Room: IROS 2023, Mmm whatcha say?: INTERSPEECH 2024, Emojivoice:\n  RO-MAN 2025, You sound a little tense: SSW 2025. Thesis presentation here:\n  https://www.youtube.com/watch?v=9BcEwqYOMYI", "summary": "While the use of social robots for language teaching has been explored, there\nremains limited work on a task-specific synthesized voices for language\nteaching robots. Given that language is a verbal task, this gap may have severe\nconsequences for the effectiveness of robots for language teaching tasks. We\naddress this lack of L2 teaching robot voices through three contributions: 1.\nWe address the need for a lightweight and expressive robot voice. Using a\nfine-tuned version of Matcha-TTS, we use emoji prompting to create an\nexpressive voice that shows a range of expressivity over time. The voice can\nrun in real time with limited compute resources. Through case studies, we found\nthis voice more expressive, socially appropriate, and suitable for long periods\nof expressive speech, such as storytelling. 2. We explore how to adapt a\nrobot's voice to physical and social ambient environments to deploy our voices\nin various locations. We found that increasing pitch and pitch rate in noisy\nand high-energy environments makes the robot's voice appear more appropriate\nand makes it seem more aware of its current environment. 3. We create an\nEnglish TTS system with improved clarity for L2 listeners using known\nlinguistic properties of vowels that are difficult for these listeners. We used\na data-driven, perception-based approach to understand how L2 speakers use\nduration cues to interpret challenging words with minimal tense (long) and lax\n(short) vowels in English. We found that the duration of vowels strongly\ninfluences the perception for L2 listeners and created an \"L2 clarity mode\" for\nMatcha-TTS that applies a lengthening to tense vowels while leaving lax vowels\nunchanged. Our clarity mode was found to be more respectful, intelligible, and\nencouraging than base Matcha-TTS while reducing transcription errors in these\nchallenging tense/lax minimal pairs."}
{"id": "2507.22729", "pdf": "https://arxiv.org/pdf/2507.22729.pdf", "abs": "https://arxiv.org/abs/2507.22729", "title": "Resource-Efficient Adaptation of Large Language Models for Text Embeddings via Prompt Engineering and Contrastive Fine-tuning", "authors": ["Benedikt Roth", "Stephan Rappensperger", "Tianming Qiu", "Hamza ImamoviÄ", "Julian WÃ¶rmann", "Hao Shen"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have become a cornerstone in Natural Language\nProcessing (NLP), achieving impressive performance in text generation. Their\ntoken-level representations capture rich, human-aligned semantics. However,\npooling these vectors into a text embedding discards crucial information.\nNevertheless, many non-generative downstream tasks, such as clustering,\nclassification, or retrieval, still depend on accurate and controllable\nsentence- or document-level embeddings. We explore several adaptation\nstrategies for pre-trained, decoder-only LLMs: (i) various aggregation\ntechniques for token embeddings, (ii) task-specific prompt engineering, and\n(iii) text-level augmentation via contrastive fine-tuning. Combining these\ncomponents yields state-of-the-art performance on the English clustering track\nof the Massive Text Embedding Benchmark (MTEB). An analysis of the attention\nmap further shows that fine-tuning shifts focus from prompt tokens to\nsemantically relevant words, indicating more effective compression of meaning\ninto the final hidden state. Our experiments demonstrate that LLMs can be\neffectively adapted as text embedding models through a combination of prompt\nengineering and resource-efficient contrastive fine-tuning on synthetically\ngenerated positive pairs."}
{"id": "2507.07610", "pdf": "https://arxiv.org/pdf/2507.07610.pdf", "abs": "https://arxiv.org/abs/2507.07610", "title": "SpatialViz-Bench: Automatically Generated Spatial Visualization Reasoning Tasks for MLLMs", "authors": ["Siting Wang", "Luoyang Sun", "Cheng Deng", "Kun Shao", "Minnan Pei", "Zheng Tian", "Haifeng Zhang", "Jun Wang"], "categories": ["cs.CV", "cs.CL", "cs.HC"], "comment": null, "summary": "Humans can directly imagine and manipulate visual images in their minds, a\ncapability known as spatial visualization. While multi-modal Large Language\nModels (MLLMs) support imagination-based reasoning, spatial visualization\nremains insufficiently evaluated, typically embedded within broader\nmathematical and logical assessments. Existing evaluations often rely on IQ\ntests or math competitions that may overlap with training data, compromising\nassessment reliability. To this end, we introduce SpatialViz-Bench, a\ncomprehensive multi-modal benchmark for spatial visualization with 12 tasks\nacross 4 sub-abilities, comprising 1,180 automatically generated problems. Our\nevaluation of 33 state-of-the-art MLLMs not only reveals wide performance\nvariations and demonstrates the benchmark's strong discriminative power, but\nalso uncovers counter-intuitive findings: models show difficulty perception\nmisaligned with human intuition, exhibit dramatic 2Dto-3D performance cliffs,\ndefault to formulaic derivation over visualization, and paradoxically suffer\nperformance degradation from Chain-of-Thought prompting in open-source models.\nThrough statistical and qualitative analysis of error types, SpatialViz-Bench\ndemonstrates that state-of-the-art MLLMs continue to exhibit deficiencies in\nspatial visualization tasks, thereby addressing a significant lacuna in the\nfield. The benchmark data and evaluation code are publicly available."}
{"id": "2507.22744", "pdf": "https://arxiv.org/pdf/2507.22744.pdf", "abs": "https://arxiv.org/abs/2507.22744", "title": "Reducing Hallucinations in Summarization via Reinforcement Learning with Entity Hallucination Index", "authors": ["Praveenkumar Katwe", "Rakesh Chandra", "Balabantaray Kali", "Prasad Vittala"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": "8", "summary": "Reducing hallucinations in abstractive summarization remains a critical\nchallenge for deploying language models (LMs) in real-world settings. In this\nwork, we introduce a rewarddriven fine-tuning framework that explicitly\noptimizes for Entity Hallucination Index (EHI), a metric designed to quantify\nthe presence, correctness, and grounding of named entities in generated\nsummaries. Given a corpus of meeting transcripts, we first generate baseline\nsummaries using a pre-trained LM and compute EHI scores via automatic entity\nextraction and matching. We then apply reinforcement learning to fine-tune the\nmodel parameters, using EHI as a reward signal to bias generation toward\nentity-faithful outputs. Our approach does not rely on human-written factuality\nannotations, enabling scalable fine-tuning. Experiments demonstrate consistent\nimprovements in EHI across datasets, with qualitative analysis revealing a\nsignificant reduction in entity-level hallucinations without degradation in\nfluency or informativeness. We release a reproducible Colab pipeline,\nfacilitating further research on hallucination-aware model fine-tuning using\nlightweight, hallucintion metrics like EHI."}
{"id": "2507.22752", "pdf": "https://arxiv.org/pdf/2507.22752.pdf", "abs": "https://arxiv.org/abs/2507.22752", "title": "CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset", "authors": ["JindÅich LibovickÃ½", "JindÅich Helcl", "Andrei Manea", "Gianluca Vico"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce a benchmark for open-ended regional question answering that\nencompasses both textual and visual modalities. We also provide strong\nbaselines using state-of-the-art large language models (LLMs). Our dataset\nconsists of manually curated questions and answers grounded in Wikipedia,\ncreated by native speakers from Czechia, Slovakia, and Ukraine, with\naccompanying English translations. It includes both purely textual questions\nand those requiring visual understanding. As a baseline, we evaluate\nstate-of-the-art LLMs through prompting and complement this with human\njudgments of answer correctness. Using these human evaluations, we analyze the\nreliability of existing automatic evaluation metrics. Our baseline results\nhighlight a significant gap in regional knowledge among current LLMs. Moreover,\napart from LLM-based evaluation, there is minimal correlation between automated\nmetrics and human judgment. We release this dataset as a resource to (1) assess\nregional knowledge in LLMs, (2) study cross-lingual generation consistency in a\nchallenging setting, and (3) advance the development of evaluation metrics for\nopen-ended question answering."}
{"id": "2507.22753", "pdf": "https://arxiv.org/pdf/2507.22753.pdf", "abs": "https://arxiv.org/abs/2507.22753", "title": "Opportunities and Challenges of LLMs in Education: An NLP Perspective", "authors": ["Sowmya Vajjala", "Bashar Alhafni", "Stefano BannÃ²", "Kaushal Kumar Maurya", "Ekaterina Kochmar"], "categories": ["cs.CL"], "comment": "Pre-print", "summary": "Interest in the role of large language models (LLMs) in education is\nincreasing, considering the new opportunities they offer for teaching,\nlearning, and assessment. In this paper, we examine the impact of LLMs on\neducational NLP in the context of two main application scenarios: {\\em\nassistance} and {\\em assessment}, grounding them along the four dimensions --\nreading, writing, speaking, and tutoring. We then present the new directions\nenabled by LLMs, and the key challenges to address. We envision that this\nholistic overview would be useful for NLP researchers and practitioners\ninterested in exploring the role of LLMs in developing language-focused and\nNLP-enabled educational applications of the future."}
{"id": "2507.22758", "pdf": "https://arxiv.org/pdf/2507.22758.pdf", "abs": "https://arxiv.org/abs/2507.22758", "title": "MASCA: LLM based-Multi Agents System for Credit Assessment", "authors": ["Gautam Jajoo", "Pranjal A Chitale", "Saksham Agarwal"], "categories": ["cs.CL", "cs.CE", "cs.LG"], "comment": "Accepted at ACL REALM Workshop. Work in Progress", "summary": "Recent advancements in financial problem-solving have leveraged LLMs and\nagent-based systems, with a primary focus on trading and financial modeling.\nHowever, credit assessment remains an underexplored challenge, traditionally\ndependent on rule-based methods and statistical models. In this paper, we\nintroduce MASCA, an LLM-driven multi-agent system designed to enhance credit\nevaluation by mirroring real-world decision-making processes. The framework\nemploys a layered architecture where specialized LLM-based agents\ncollaboratively tackle sub-tasks. Additionally, we integrate contrastive\nlearning for risk and reward assessment to optimize decision-making. We further\npresent a signaling game theory perspective on hierarchical multi-agent\nsystems, offering theoretical insights into their structure and interactions.\nOur paper also includes a detailed bias analysis in credit assessment,\naddressing fairness concerns. Experimental results demonstrate that MASCA\noutperforms baseline approaches, highlighting the effectiveness of hierarchical\nLLM-based multi-agent systems in financial applications, particularly in credit\nscoring."}
{"id": "2507.22811", "pdf": "https://arxiv.org/pdf/2507.22811.pdf", "abs": "https://arxiv.org/abs/2507.22811", "title": "DBLPLink 2.0 -- An Entity Linker for the DBLP Scholarly Knowledge Graph", "authors": ["Debayan Banerjee", "Tilahun Abedissa Taffa", "Ricardo Usbeck"], "categories": ["cs.CL"], "comment": null, "summary": "In this work we present an entity linker for DBLP's 2025 version of RDF-based\nKnowledge Graph. Compared to the 2022 version, DBLP now considers publication\nvenues as a new entity type called dblp:Stream. In the earlier version of\nDBLPLink, we trained KG-embeddings and re-rankers on a dataset to produce\nentity linkings. In contrast, in this work, we develop a zero-shot entity\nlinker using LLMs using a novel method, where we re-rank candidate entities\nbased on the log-probabilities of the \"yes\" token output at the penultimate\nlayer of the LLM."}
{"id": "2507.22829", "pdf": "https://arxiv.org/pdf/2507.22829.pdf", "abs": "https://arxiv.org/abs/2507.22829", "title": "Beyond Natural Language Plans: Structure-Aware Planning for Query-Focused Table Summarization", "authors": ["Weijia Zhang", "Songgaojun Deng", "Evangelos Kanoulas"], "categories": ["cs.CL"], "comment": "10 pages, 4 figures, and 5 tables", "summary": "Query-focused table summarization requires complex reasoning, often\napproached through step-by-step natural language (NL) plans. However, NL plans\nare inherently ambiguous and lack structure, limiting their conversion into\nexecutable programs like SQL and hindering scalability, especially for\nmulti-table tasks. To address this, we propose a paradigm shift to structured\nrepresentations. We introduce a new structured plan, TaSoF, inspired by\nformalism in traditional multi-agent systems, and a framework, SPaGe, that\nformalizes the reasoning process in three phases: 1) Structured Planning to\ngenerate TaSoF from a query, 2) Graph-based Execution to convert plan steps\ninto SQL and model dependencies via a directed cyclic graph for parallel\nexecution, and 3) Summary Generation to produce query-focused summaries. Our\nmethod explicitly captures complex dependencies and improves reliability.\nExperiments on three public benchmarks show that SPaGe consistently outperforms\nprior models in both single- and multi-table settings, demonstrating the\nadvantages of structured representations for robust and scalable summarization."}
{"id": "2507.22887", "pdf": "https://arxiv.org/pdf/2507.22887.pdf", "abs": "https://arxiv.org/abs/2507.22887", "title": "Where to show Demos in Your Prompt: A Positional Bias of In-Context Learning", "authors": ["Kwesi Cobbina", "Tianyi Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In-context learning (ICL) is a critical emerging capability of large language\nmodels (LLMs), enabling few-shot learning during inference by including a few\ndemonstrations (demos) in the prompt. However, it has been found that ICL's\nperformance can be sensitive to the choices of demos and their order. This\npaper investigates an unexplored new positional bias of ICL for the first time:\nwe observe that the predictions and accuracy can drift drastically when the\npositions of demos, the system prompt, and the user message in LLM input are\nvaried. We refer to this bias as DEMOS' POSITION IN PROMPT (DPP) bias. We\ndesign a systematic evaluation pipeline to study this type of positional bias\nacross classification, question answering, summarization, and reasoning tasks.\nWe introduce two metrics, ACCURACY-CHANGE and PREDICTION-CHANGE, to quantify\nnet gains and output volatility induced by changes in the demos' position.\nExtensive experiments on ten LLMs from four open-source model families (QWEN,\nLLAMA3, MISTRAL, COHERE) verify that the bias significantly affects their\naccuracy and predictions: placing demos at the start of the prompt yields the\nmost stable and accurate outputs with gains of up to +6 points. In contrast,\nplacing demos at the end of the user message flips over 30\\% of predictions\nwithout improving correctness on QA tasks. Smaller models are most affected by\nthis sensitivity, though even large models remain marginally affected on\ncomplex tasks."}
{"id": "2507.22074", "pdf": "https://arxiv.org/pdf/2507.22074.pdf", "abs": "https://arxiv.org/abs/2507.22074", "title": "CIMR: Contextualized Iterative Multimodal Reasoning for Robust Instruction Following in LVLMs", "authors": ["Yangshu Yuan", "Heng Chen", "Xinyi Jiang", "Christian Ng", "Kexin Qiu"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) and Large\nVision-Language Models (LVLMs) has enhanced our ability to process and generate\nhuman language and visual information. However, these models often struggle\nwith complex, multi-step multi-modal instructions that require logical\nreasoning, dynamic feedback integration, and iterative self-correction. To\naddress this, we propose CIMR: Contextualized Iterative Multimodal Reasoning, a\nnovel framework that introduces a context-aware iterative reasoning and\nself-correction module. CIMR operates in two stages: initial reasoning and\nresponse generation, followed by iterative refinement using parsed multi-modal\nfeedback. A dynamic fusion module deeply integrates textual, visual, and\ncontextual features at each step. We fine-tune LLaVA-1.5-7B on the Visual\nInstruction Tuning (VIT) dataset and evaluate CIMR on the newly introduced\nMulti-modal Action Planning (MAP) dataset. CIMR achieves 91.5% accuracy,\noutperforming state-of-the-art models such as GPT-4V (89.2%), LLaVA-1.5\n(78.5%), MiniGPT-4 (75.3%), and InstructBLIP (72.8%), demonstrating the\nefficacy of its iterative reasoning and self-correction capabilities in complex\ntasks."}
{"id": "2507.22080", "pdf": "https://arxiv.org/pdf/2507.22080.pdf", "abs": "https://arxiv.org/abs/2507.22080", "title": "CodeEvo: Interaction-Driven Synthesis of Code-centric Data through Hybrid and Iterative Feedback", "authors": ["Qiushi Sun", "Jinyang Gong", "Lei Li", "Qipeng Guo", "Fei Yuan"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Acquiring high-quality instruction-code pairs is essential for training Large\nLanguage Models (LLMs) for code generation. Manually curated data is expensive\nand inherently limited in scale, motivating the development of code-centric\nsynthesis methods. Yet, current approaches either focus on augmenting existing\ncode or rely on predefined heuristics, both lacking rigorous data validation,\nwhich results in synthetic data that is ungrounded, repetitive, or overly\nsimplistic. Inspired by collaborative programming practices, we propose\nCodeEvo, a framework that synthesizes code data through iterative interactions\nbetween two LLM agents: a Coder, which generates candidate code and test cases\nbased on given instructions, and a Reviewer, which guides the synthesis process\nby producing new instructions and feedback. We further introduce a hybrid\nfeedback mechanism that combines compiler determinism with the generative\nflexibility of agents, enabling automatic quality control throughout synthesis.\nExtensive experiments demonstrate that models fine-tuned on CodeEvo data\nsignificantly outperform established baselines across code generation\nbenchmarks with various difficulties. In-depth analyses further provide\ninsights from multiple perspectives into effective code-centric data synthesis."}
{"id": "2507.22133", "pdf": "https://arxiv.org/pdf/2507.22133.pdf", "abs": "https://arxiv.org/abs/2507.22133", "title": "Prompt Optimization and Evaluation for LLM Automated Red Teaming", "authors": ["Michael Freenor", "Lauren Alvarez", "Milton Leal", "Lily Smith", "Joel Garrett", "Yelyzaveta Husieva", "Madeline Woodruff", "Ryan Miller", "Erich Kummerfeld", "Rafael Medeiros", "Sander Schulhoff"], "categories": ["cs.CR", "cs.CL"], "comment": "9 pages, 5 Figures, and 1 Appendix item", "summary": "Applications that use Large Language Models (LLMs) are becoming widespread,\nmaking the identification of system vulnerabilities increasingly important.\nAutomated Red Teaming accelerates this effort by using an LLM to generate and\nexecute attacks against target systems. Attack generators are evaluated using\nthe Attack Success Rate (ASR) the sample mean calculated over the judgment of\nsuccess for each attack. In this paper, we introduce a method for optimizing\nattack generator prompts that applies ASR to individual attacks. By repeating\neach attack multiple times against a randomly seeded target, we measure an\nattack's discoverability the expectation of the individual attack success. This\napproach reveals exploitable patterns that inform prompt optimization,\nultimately enabling more robust evaluation and refinement of generators."}
{"id": "2507.22160", "pdf": "https://arxiv.org/pdf/2507.22160.pdf", "abs": "https://arxiv.org/abs/2507.22160", "title": "Strategic Deflection: Defending LLMs from Logit Manipulation", "authors": ["Yassine Rachidy", "Jihad Rbaiti", "Youssef Hmamouche", "Faissal Sehbaoui", "Amal El Fallah Seghrouchni"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "20 pages", "summary": "With the growing adoption of Large Language Models (LLMs) in critical areas,\nensuring their security against jailbreaking attacks is paramount. While\ntraditional defenses primarily rely on refusing malicious prompts, recent\nlogit-level attacks have demonstrated the ability to bypass these safeguards by\ndirectly manipulating the token-selection process during generation. We\nintroduce Strategic Deflection (SDeflection), a defense that redefines the\nLLM's response to such advanced attacks. Instead of outright refusal, the model\nproduces an answer that is semantically adjacent to the user's request yet\nstrips away the harmful intent, thereby neutralizing the attacker's harmful\nintent. Our experiments demonstrate that SDeflection significantly lowers\nAttack Success Rate (ASR) while maintaining model performance on benign\nqueries. This work presents a critical shift in defensive strategies, moving\nfrom simple refusal to strategic content redirection to neutralize advanced\nthreats."}
{"id": "2507.22197", "pdf": "https://arxiv.org/pdf/2507.22197.pdf", "abs": "https://arxiv.org/abs/2507.22197", "title": "Explainability Through Systematicity: The Hard Systematicity Challenge for Artificial Intelligence", "authors": ["Matthieu Queloz"], "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": "39 pages; final, published version", "summary": "This paper argues that explainability is only one facet of a broader ideal\nthat shapes our expectations towards artificial intelligence (AI).\nFundamentally, the issue is to what extent AI exhibits systematicity--not\nmerely in being sensitive to how thoughts are composed of recombinable\nconstituents, but in striving towards an integrated body of thought that is\nconsistent, coherent, comprehensive, and parsimoniously principled. This richer\nconception of systematicity has been obscured by the long shadow of the\n\"systematicity challenge\" to connectionism, according to which network\narchitectures are fundamentally at odds with what Fodor and colleagues termed\n\"the systematicity of thought.\" I offer a conceptual framework for thinking\nabout \"the systematicity of thought\" that distinguishes four senses of the\nphrase. I use these distinctions to defuse the perceived tension between\nsystematicity and connectionism and show that the conception of systematicity\nthat historically shaped our sense of what makes thought rational,\nauthoritative, and scientific is more demanding than the Fodorian notion. To\ndetermine whether we have reason to hold AI models to this ideal of\nsystematicity, I then argue, we must look to the rationales for systematization\nand explore to what extent they transfer to AI models. I identify five such\nrationales and apply them to AI. This brings into view the \"hard systematicity\nchallenge.\" However, the demand for systematization itself needs to be\nregulated by the rationales for systematization. This yields a dynamic\nunderstanding of the need to systematize thought, which tells us how systematic\nwe need AI models to be and when."}
{"id": "2507.22281", "pdf": "https://arxiv.org/pdf/2507.22281.pdf", "abs": "https://arxiv.org/abs/2507.22281", "title": "CoEx -- Co-evolving World-model and Exploration", "authors": ["Minsoo Kim", "Seung-won Hwang"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Planning in modern LLM agents relies on the utilization of LLM as an internal\nworld model, acquired during pretraining. However, existing agent designs fail\nto effectively assimilate new observations into dynamic updates of the world\nmodel. This reliance on the LLM's static internal world model is progressively\nprone to misalignment with the underlying true state of the world, leading to\nthe generation of divergent and erroneous plans. We introduce a hierarchical\nagent architecture, CoEx, in which hierarchical state abstraction allows LLM\nplanning to co-evolve with a dynamically updated model of the world. CoEx plans\nand interacts with the world by using LLM reasoning to orchestrate dynamic\nplans consisting of subgoals, and its learning mechanism continuously\nincorporates these subgoal experiences into a persistent world model in the\nform of a neurosymbolic belief state, comprising textual inferences and\ncode-based symbolic memory. We evaluate our agent across a diverse set of agent\nscenarios involving rich environments and complex tasks including ALFWorld,\nPDDL, and Jericho. Our experiments show that CoEx outperforms existing agent\nparadigms in planning and exploration."}
{"id": "2507.22359", "pdf": "https://arxiv.org/pdf/2507.22359.pdf", "abs": "https://arxiv.org/abs/2507.22359", "title": "LLM-Crowdsourced: A Benchmark-Free Paradigm for Mutual Evaluation of Large Language Models", "authors": ["Qianhong Guo", "Wei Xie", "Xiaofang Cai", "Enze Wang", "Shuoyoucheng Ma", "Kai Chen", "Xiaofeng Wang", "Baosheng Wang"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Although large language models (LLMs) demonstrate remarkable capabilities\nacross various tasks, evaluating their capabilities remains a challenging task.\nExisting evaluation methods suffer from issues such as data contamination,\nblack-box operation, and subjective preference. These issues make it difficult\nto evaluate the LLMs' true capabilities comprehensively. To tackle these\nchallenges, we propose a novel benchmark-free evaluation paradigm,\nLLM-Crowdsourced. It utilizes LLMs to generate questions, answer independently,\nand evaluate mutually. This method integrates four key evaluation criteria:\ndynamic, transparent, objective, and professional, which existing evaluation\nmethods cannot satisfy simultaneously. Experiments on eight mainstream LLMs\nacross mathematics and programming verify the advantages of our method in\ndistinguishing LLM performance. Furthermore, our study reveals several novel\nfindings that are difficult for traditional methods to detect, including but\nnot limited to: (1) Gemini demonstrates the highest original and professional\nquestion-design capabilities among others; (2) Some LLMs exhibit\n''memorization-based answering'' by misrecognizing questions as familiar ones\nwith a similar structure; (3) LLM evaluation results demonstrate high\nconsistency (robustness)."}
{"id": "2507.22543", "pdf": "https://arxiv.org/pdf/2507.22543.pdf", "abs": "https://arxiv.org/abs/2507.22543", "title": "Pre-trained Models Perform the Best When Token Distributions Follow Zipf's Law", "authors": ["Yanjin He", "Qingkai Zeng", "Meng Jiang"], "categories": ["cs.LG", "cs.CL", "I.2.6; I.2.7"], "comment": null, "summary": "Tokenization is a fundamental step in natural language processing (NLP) and\nother sequence modeling domains, where the choice of vocabulary size\nsignificantly impacts model performance. Despite its importance, selecting an\noptimal vocabulary size remains underexplored, typically relying on heuristics\nor dataset-specific choices. In this work, we propose a principled method for\ndetermining the vocabulary size by analyzing token frequency distributions\nthrough Zipf's law. We show that downstream task performance correlates with\nhow closely token distributions follow power-law behavior, and that aligning\nwith Zipfian scaling improves both model efficiency and effectiveness.\nExtensive experiments across NLP, genomics, and chemistry demonstrate that\nmodels consistently achieve peak performance when the token distribution\nclosely adheres to Zipf's law, establishing Zipfian alignment as a robust and\ngeneralizable criterion for vocabulary size selection."}
{"id": "2507.22565", "pdf": "https://arxiv.org/pdf/2507.22565.pdf", "abs": "https://arxiv.org/abs/2507.22565", "title": "Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement Learning", "authors": ["Afshin Khadangi", "Amir Sartipi", "Igor Tchappi", "Ramin Bahmani", "Gilbert Fridgen"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The tension between data privacy and model utility has become the defining\nbottleneck for the practical deployment of large language models (LLMs) trained\non sensitive corpora including healthcare. Differentially private stochastic\ngradient descent (DP-SGD) guarantees formal privacy, yet it does so at a\npronounced cost: gradients are forcibly clipped and perturbed with noise,\ndegrading sample efficiency and final accuracy. Numerous variants have been\nproposed to soften this trade-off, but they all share a handicap: their control\nknobs are hard-coded, global, and oblivious to the evolving optimization\nlandscape. Consequently, practitioners are forced either to over-spend privacy\nbudget in pursuit of utility, or to accept mediocre models in order to stay\nwithin privacy constraints. We present RLDP, the first framework to cast DP\noptimization itself as a closed-loop control problem amenable to modern deep\nreinforcement learning (RL). RLDP continuously senses rich statistics of the\nlearning dynamics and acts by selecting fine-grained per parameter\ngradient-clipping thresholds as well as the magnitude of injected Gaussian\nnoise. A soft actor-critic (SAC) hyper-policy is trained online during language\nmodel fine-tuning; it learns, from scratch, how to allocate the privacy budget\nwhere it matters and when it matters. Across more than 1,600 ablation\nexperiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers\nperplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream\nutility gain. RLDP reaches each baseline's final utility after only 13-43% of\nthe gradient-update budget (mean speed-up 71%), all while honoring the same\n($\\epsilon$, $\\delta$)-DP contract and exhibiting equal or lower susceptibility\nto membership-inference and canary-extraction attacks."}
{"id": "2507.22607", "pdf": "https://arxiv.org/pdf/2507.22607.pdf", "abs": "https://arxiv.org/abs/2507.22607", "title": "VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced Multimodal Reasoning", "authors": ["Ruifeng Yuan", "Chenghao Xiao", "Sicong Leng", "Jianyu Wang", "Long Li", "Weiwen Xu", "Hou Pong Chan", "Deli Zhao", "Tingyang Xu", "Zhongyu Wei", "Hao Zhang", "Yu Rong"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "21 pages, 5 figures, 6 tables. Work in progress", "summary": "Reinforcement learning has proven its effectiveness in enhancing the\nreasoning capabilities of large language models. Recent research efforts have\nprogressively extended this paradigm to multimodal reasoning tasks. Due to the\ninherent complexity and diversity of multimodal tasks, especially in semantic\ncontent and problem formulations, existing models often exhibit unstable\nperformance across various domains and difficulty levels. To address these\nlimitations, we propose VL-Cogito, an advanced multimodal reasoning model\ntrained via a novel multi-stage Progressive Curriculum Reinforcement Learning\n(PCuRL) framework. PCuRL systematically guides the model through tasks of\ngradually increasing difficulty, substantially improving its reasoning\nabilities across diverse multimodal contexts. The framework introduces two key\ninnovations: (1) an online difficulty soft weighting mechanism, dynamically\nadjusting training difficulty across successive RL training stages; and (2) a\ndynamic length reward mechanism, which encourages the model to adaptively\nregulate its reasoning path length according to task complexity, thus balancing\nreasoning efficiency with correctness. Experimental evaluations demonstrate\nthat VL-Cogito consistently matches or surpasses existing reasoning-oriented\nmodels across mainstream multimodal benchmarks spanning mathematics, science,\nlogic, and general understanding, validating the effectiveness of our approach."}
{"id": "2507.22746", "pdf": "https://arxiv.org/pdf/2507.22746.pdf", "abs": "https://arxiv.org/abs/2507.22746", "title": "Next Tokens Denoising for Speech Synthesis", "authors": ["Yanqing Liu", "Ruiqing Xue", "Chong Zhang", "Yufei Liu", "Gang Wang", "Bohan Li", "Yao Qian", "Lei He", "Shujie Liu", "Sheng Zhao"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact 12.5 tokens per\nsecond rate. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Consequently, the proposed model can utilize KV-cache\nacross chunks and incorporate future context within each chunk. Furthermore, it\nbridges continuous and discrete feature modeling, demonstrating that continuous\nAR flow-matching can predict discrete tokens with finite scalar quantizers.\nThis efficient codec and fast chunk-autoregressive architecture also makes the\nproposed model particularly effective for generating extended content.\nExperiment for demos of our work} on podcast datasets demonstrate its\ncapability to efficiently generate high-quality zero-shot podcasts."}
{"id": "2507.22847", "pdf": "https://arxiv.org/pdf/2507.22847.pdf", "abs": "https://arxiv.org/abs/2507.22847", "title": "The Incomplete Bridge: How AI Research (Mis)Engages with Psychology", "authors": ["Han Jiang", "Pengda Wang", "Xiaoyuan Yi", "Xing Xie", "Ziang Xiao"], "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Social sciences have accumulated a rich body of theories and methodologies\nfor investigating the human mind and behaviors, while offering valuable\ninsights into the design and understanding of Artificial Intelligence (AI)\nsystems. Focusing on psychology as a prominent case, this study explores the\ninterdisciplinary synergy between AI and the field by analyzing 1,006\nLLM-related papers published in premier AI venues between 2023 and 2025, along\nwith the 2,544 psychology publications they cite. Through our analysis, we\nidentify key patterns of interdisciplinary integration, locate the psychology\ndomains most frequently referenced, and highlight areas that remain\nunderexplored. We further examine how psychology theories/frameworks are\noperationalized and interpreted, identify common types of misapplication, and\noffer guidance for more effective incorporation. Our work provides a\ncomprehensive map of interdisciplinary engagement between AI and psychology,\nthereby facilitating deeper collaboration and advancing AI systems."}
{"id": "2507.22878", "pdf": "https://arxiv.org/pdf/2507.22878.pdf", "abs": "https://arxiv.org/abs/2507.22878", "title": "GeoOutageKG: A Multimodal Geospatiotemporal Knowledge Graph for Multiresolution Power Outage Analysis", "authors": ["Ethan Frakes", "Yinghui Wu", "Roger H. French", "Mengjie Li"], "categories": ["cs.IR", "cs.CL", "cs.CY"], "comment": "Accepted to the 24th International Semantic Web Conference Resource\n  Track (ISWC 2025)", "summary": "Detecting, analyzing, and predicting power outages is crucial for grid risk\nassessment and disaster mitigation. Numerous outages occur each year,\nexacerbated by extreme weather events such as hurricanes. Existing outage data\nare typically reported at the county level, limiting their spatial resolution\nand making it difficult to capture localized patterns. However, it offers\nexcellent temporal granularity. In contrast, nighttime light satellite image\ndata provides significantly higher spatial resolution and enables a more\ncomprehensive spatial depiction of outages, enhancing the accuracy of assessing\nthe geographic extent and severity of power loss after disaster events.\nHowever, these satellite data are only available on a daily basis. Integrating\nspatiotemporal visual and time-series data sources into a unified knowledge\nrepresentation can substantially improve power outage detection, analysis, and\npredictive reasoning. In this paper, we propose GeoOutageKG, a multimodal\nknowledge graph that integrates diverse data sources, including nighttime light\nsatellite image data, high-resolution spatiotemporal power outage maps, and\ncounty-level timeseries outage reports in the U.S. We describe our method for\nconstructing GeoOutageKG by aligning source data with a developed ontology,\nGeoOutageOnto. Currently, GeoOutageKG includes over 10.6 million individual\noutage records spanning from 2014 to 2024, 300,000 NTL images spanning from\n2012 to 2024, and 15,000 outage maps. GeoOutageKG is a novel, modular and\nreusable semantic resource that enables robust multimodal data integration. We\ndemonstrate its use through multiresolution analysis of geospatiotemporal power\noutages."}
{"id": "2507.22879", "pdf": "https://arxiv.org/pdf/2507.22879.pdf", "abs": "https://arxiv.org/abs/2507.22879", "title": "RecGPT Technical Report", "authors": ["Chao Yi", "Dian Chen", "Gaoyang Guo", "Jiakai Tang", "Jian Wu", "Jing Yu", "Sunhao Dai", "Wen Chen", "Wenjun Yang", "Yuning Jiang", "Zhujin Gao", "Bo Zheng", "Chi Li", "Dimin Wang", "Dixuan Wang", "Fan Li", "Fan Zhang", "Haibin Chen", "Haozhuang Liu", "Jialin Zhu", "Jiamang Wang", "Jiawei Wu", "Jin Cui", "Ju Huang", "Kai Zhang", "Kan Liu", "Lang Tian", "Liang Rao", "Longbin Li", "Lulu Zhao", "Mao Zhang", "Na He", "Peiyang Wang", "Qiqi Huang", "Tao Luo", "Wenbo Su", "Xiaoxiao He", "Xin Tong", "Xu Chen", "Xunke Xi", "Yang Li", "Yaxuan Wu", "Yeqiu Yang", "Yi Hu", "Yinnan Song", "Yuchen Li", "Yujie Luo", "Yujin Yuan", "Yuliang Yan", "Zhengyang Wang", "Zhibo Xiao", "Zhixin Ma", "Zile Zhou"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Recommender systems are among the most impactful applications of artificial\nintelligence, serving as critical infrastructure connecting users, merchants,\nand platforms. However, most current industrial systems remain heavily reliant\non historical co-occurrence patterns and log-fitting objectives, i.e.,\noptimizing for past user interactions without explicitly modeling user intent.\nThis log-fitting approach often leads to overfitting to narrow historical\npreferences, failing to capture users' evolving and latent interests. As a\nresult, it reinforces filter bubbles and long-tail phenomena, ultimately\nharming user experience and threatening the sustainability of the whole\nrecommendation ecosystem.\n  To address these challenges, we rethink the overall design paradigm of\nrecommender systems and propose RecGPT, a next-generation framework that places\nuser intent at the center of the recommendation pipeline. By integrating large\nlanguage models (LLMs) into key stages of user interest mining, item retrieval,\nand explanation generation, RecGPT transforms log-fitting recommendation into\nan intent-centric process. To effectively align general-purpose LLMs to the\nabove domain-specific recommendation tasks at scale, RecGPT incorporates a\nmulti-stage training paradigm, which integrates reasoning-enhanced\npre-alignment and self-training evolution, guided by a Human-LLM cooperative\njudge system. Currently, RecGPT has been fully deployed on the Taobao App.\nOnline experiments demonstrate that RecGPT achieves consistent performance\ngains across stakeholders: users benefit from increased content diversity and\nsatisfaction, merchants and the platform gain greater exposure and conversions.\nThese comprehensive improvement results across all stakeholders validates that\nLLM-driven, intent-centric design can foster a more sustainable and mutually\nbeneficial recommendation ecosystem."}
{"id": "2311.07052", "pdf": "https://arxiv.org/pdf/2311.07052.pdf", "abs": "https://arxiv.org/abs/2311.07052", "title": "Towards the Law of Capacity Gap in Distilling Language Models", "authors": ["Chen Zhang", "Qiuchi Li", "Dawei Song", "Zheyu Ye", "Yan Gao", "Yan Hu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "32 pages, 10 figures, 15 tables, accepted to ACL 2025. Code and\n  checkpoints are available at https://github.com/GeneZC/MiniMA", "summary": "Language model (LM) distillation aims at distilling the knowledge in a large\nteacher LM to a small student one. As a critical issue facing LM distillation,\na superior student often arises from a teacher of a relatively small scale\ninstead of a larger one, especially in the presence of substantial capacity gap\nbetween the teacher and student. This issue, often referred to as the\n\\textit{curse of capacity gap}, suggests that there is likely an optimal\nteacher yielding the best-performing student along the scaling course of the\nteacher. Consequently, distillation trials on teachers of a wide range of\nscales are called for to determine the optimal teacher, which becomes\ncomputationally intensive in the context of large LMs (LLMs). This paper\naddresses this critical bottleneck by providing the \\textit{law of capacity\ngap} inducted from a preliminary study on distilling a broad range of\nsmall-scale (<3B) LMs, where the optimal teacher consistently scales linearly\nwith the student scale across different model and data scales. By extending the\nlaw to LLM distillation on a larger scale (7B), we succeed in obtaining\nversatile LLMs that outperform a wide array of competitors."}
{"id": "2408.16440", "pdf": "https://arxiv.org/pdf/2408.16440.pdf", "abs": "https://arxiv.org/abs/2408.16440", "title": "Instruction-tuned Large Language Models for Machine Translation in the Medical Domain", "authors": ["Miguel Rios"], "categories": ["cs.CL"], "comment": "Citation: Miguel Rios. 2025. Instruction-tuned Large Language Models\n  for Machine Translation in the Medical Domain. In Proceedings of Machine\n  Translation Summit XX Volume 1, pages 162-172", "summary": "Large Language Models (LLMs) have shown promising results on machine\ntranslation for high resource language pairs and domains. However, in\nspecialised domains (e.g. medical) LLMs have shown lower performance compared\nto standard neural machine translation models. The consistency in the machine\ntranslation of terminology is crucial for users, researchers, and translators\nin specialised domains. In this study, we compare the performance between\nbaseline LLMs and instruction-tuned LLMs in the medical domain. In addition, we\nintroduce terminology from specialised medical dictionaries into the\ninstruction formatted datasets for fine-tuning LLMs. The instruction-tuned LLMs\nsignificantly outperform the baseline models with automatic metrics."}
{"id": "2409.14820", "pdf": "https://arxiv.org/pdf/2409.14820.pdf", "abs": "https://arxiv.org/abs/2409.14820", "title": "Past Meets Present: Creating Historical Analogy with Large Language Models", "authors": ["Nianqi Li", "Siyu Yuan", "Jiangjie Chen", "Jiaqing Liang", "Feng Wei", "Zujie Liang", "Deqing Yang", "Yanghua Xiao"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 (Outstanding Paper Award)", "summary": "Historical analogies, which compare known past events with contemporary but\nunfamiliar events, are important abilities that help people make decisions and\nunderstand the world. However, research in applied history suggests that people\nhave difficulty finding appropriate analogies. And previous studies in the AI\ncommunity have also overlooked historical analogies. To fill this gap, in this\npaper, we focus on the historical analogy acquisition task, which aims to\nacquire analogous historical events for a given event. We explore retrieval and\ngeneration methods for acquiring historical analogies based on different large\nlanguage models (LLMs). Furthermore, we propose a self-reflection method to\nmitigate hallucinations and stereotypes when LLMs generate historical\nanalogies. Through human evaluations and our specially designed automatic\nmulti-dimensional assessment, we find that LLMs generally have a good potential\nfor historical analogies. And the performance of the models can be further\nimproved by using our self-reflection method."}
{"id": "2410.02744", "pdf": "https://arxiv.org/pdf/2410.02744.pdf", "abs": "https://arxiv.org/abs/2410.02744", "title": "Neutral Residues: Revisiting Adapters for Model Extension", "authors": ["Franck Signe Talla", "Edouard Grave", "HervÃ© JÃ©gou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ICML 2025", "summary": "We address the problem of extending a pretrained large language model to a\nnew domain that was not seen during training. Standard techniques, such as\nfinetuning or low-rank adaptation (LoRA) are successful at domain adaptation,\nbut do not formally add capacity to the model. This often leads to a trade-off,\nbetween performing well on the new domain vs. degrading performance on the\noriginal domain. Here, we revisit and improve adapters to extend LLMs from\nthree angles: data, architecture and training procedure, which are\nadvantageously considered jointly. The resulting method, called neutral\nresidues, modifies adapters in a way that leads each new residual block to\noutput near-zeros on the original domain. This solution leads to strong results\nwhen adapting a state-of-the-art model originally trained on English to a new\nlanguage. Neutral residues significantly outperform competing approaches such\nas finetuning, LoRA or vanilla adapters in terms of the trade-off between\nlearning the new language and not forgetting English."}
{"id": "2410.21306", "pdf": "https://arxiv.org/pdf/2410.21306.pdf", "abs": "https://arxiv.org/abs/2410.21306", "title": "Natural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges", "authors": ["Farid Ariai", "Joel Mackenzie", "Gianluca Demartini"], "categories": ["cs.CL", "cs.AI", "A.1; I.2.7; J.1"], "comment": "35 pages", "summary": "Natural Language Processing (NLP) is revolutionising the way both\nprofessionals and laypersons operate in the legal field. The considerable\npotential for NLP in the legal sector, especially in developing computational\nassistance tools for various legal processes, has captured the interest of\nresearchers for years. This survey follows the Preferred Reporting Items for\nSystematic Reviews and Meta-Analyses framework, reviewing 154 studies, with a\nfinal selection of 131 after manual filtering. It explores foundational\nconcepts related to NLP in the legal domain, illustrating the unique aspects\nand challenges of processing legal texts, such as extensive document lengths,\ncomplex language, and limited open legal datasets. We provide an overview of\nNLP tasks specific to legal text, such as Document Summarisation, Named Entity\nRecognition, Question Answering, Argument Mining, Text Classification, and\nJudgement Prediction. Furthermore, we analyse both developed legal-oriented\nlanguage models, and approaches for adapting general-purpose language models to\nthe legal domain. Additionally, we identify sixteen open research challenges,\nincluding the detection and mitigation of bias in artificial intelligence\napplications, the need for more robust and interpretable models, and improving\nexplainability to handle the complexities of legal language and reasoning."}
{"id": "2412.03334", "pdf": "https://arxiv.org/pdf/2412.03334.pdf", "abs": "https://arxiv.org/abs/2412.03334", "title": "Yankari: A Monolingual Yoruba Dataset", "authors": ["Maro Akpobi"], "categories": ["cs.CL"], "comment": "6 pages", "summary": "This paper presents Yankari, a large-scale monolingual dataset for the Yoruba\nlanguage, aimed at addressing the critical gap in Natural Language Processing\n(NLP) resources for this important West African language. Despite being spoken\nby over 30 million people, Yoruba has been severely underrepresented in NLP\nresearch and applications. We detail our methodology for creating this dataset,\nwhich includes careful source selection, automated quality control, and\nrigorous data cleaning processes. The Yankari dataset comprises 51,407\ndocuments from 13 diverse sources, totaling over 30 million tokens. Our\napproach focuses on ethical data collection practices, avoiding problematic\nsources and addressing issues prevalent in existing datasets. We provide\nthorough automated evaluations of the dataset, demonstrating its quality\ncompared to existing resources. The Yankari dataset represents a significant\nadvancement in Yoruba language resources, providing a foundation for developing\nmore accurate NLP models, supporting comparative linguistic studies, and\ncontributing to the digital accessibility of the Yoruba language."}
{"id": "2412.08528", "pdf": "https://arxiv.org/pdf/2412.08528.pdf", "abs": "https://arxiv.org/abs/2412.08528", "title": "Efficient Continual Learning for Small Language Models with a Discrete Key-Value Bottleneck", "authors": ["Andor Diera", "Lukas Galke", "Fabian Karl", "Ansgar Scherp"], "categories": ["cs.CL"], "comment": null, "summary": "Continual learning remains a challenge across various natural language\nprocessing (NLP) tasks, as models updated with new training data often risk\ncatastrophic forgetting of previously acquired knowledge. We introduce a\ndiscrete key-value bottleneck (DKVB) for encoder-only language models, enabling\nefficient continual learning through localized updates. Inspired by a discrete\nkey-value bottleneck in vision, we consider new and NLP-specific challenges. We\ncompare different bottleneck architectures for NLP and introduce a new,\ntask-independent initialization technique for the discrete keys. We evaluate\nour DKVB for NLP in four continual learning scenarios and show that it\nalleviates catastrophic forgetting. Our experiments demonstrate that the\nproposed approach achieves competitive performance compared to popular\ncontinual learning methods while incurring lower computational costs.\nFurthermore, we show that DKVB remains effective even in challenging\nsingle-head continual learning scenarios where no task ID is provided."}
{"id": "2412.14373", "pdf": "https://arxiv.org/pdf/2412.14373.pdf", "abs": "https://arxiv.org/abs/2412.14373", "title": "ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram Language Modeling", "authors": ["William Han", "Chaojing Duan", "Michael A. Rosenberg", "Emerson Liu", "Ding Zhao"], "categories": ["cs.CL", "eess.SP", "I.2.7; J.3"], "comment": "38 pages, 9 figures; Accepted to MLHC 2025", "summary": "Large Language Models (LLMs) have demonstrated exceptional versatility across\ndomains, including applications to electrocardiograms (ECGs). A growing body of\nwork focuses on generating text from multi-channeled ECG signals and\ncorresponding textual prompts. Existing approaches often involve a two-stage\nprocess: pretraining an ECG-specific encoder with a self-supervised learning\n(SSL) objective, followed by finetuning an LLM for natural language generation\n(NLG) using encoder-derived features. However, these methods face two key\nlimitations: inefficiency due to multi-stage training and challenges in\ninterpreting encoder-generated features. To overcome these issues, we propose\nECG-Byte, an adapted byte pair encoding (BPE) tokenizer pipeline for\nautoregressive language modeling of ECGs. ECG-Byte compresses and encodes ECG\nsignals into tokens, enabling direct end-to-end LLM training by combining ECG\nand text tokens. This approach enhances interpretability, as ECG tokens can be\ndirectly mapped back to the original signals. Leveraging ECG-Byte, we achieve\ncompetitive NLG performance while training 3 times faster and using just 48\\%\nof the data required by traditional two-stage methods."}
{"id": "2412.15239", "pdf": "https://arxiv.org/pdf/2412.15239.pdf", "abs": "https://arxiv.org/abs/2412.15239", "title": "Modeling Story Expectations to Understand Engagement: A Generative Framework Using LLMs", "authors": ["Hortense Fong", "George Gui"], "categories": ["cs.CL", "cs.AI", "econ.GN", "q-fin.EC", "stat.ME", "68T50, 91F20", "H.3.1; I.2.7"], "comment": null, "summary": "Understanding when and why consumers engage with stories is crucial for\ncontent creators and platforms. While existing theories suggest that audience\nbeliefs of what is going to happen should play an important role in engagement\ndecisions, empirical work has mostly focused on developing techniques to\ndirectly extract features from actual content, rather than capturing\nforward-looking beliefs, due to the lack of a principled way to model such\nbeliefs in unstructured narrative data. To complement existing feature\nextraction techniques, this paper introduces a novel framework that leverages\nlarge language models to model audience forward-looking beliefs about how\nstories might unfold. Our method generates multiple potential continuations for\neach story and extracts features related to expectations, uncertainty, and\nsurprise using established content analysis techniques. Applying our method to\nover 30,000 book chapters, we demonstrate that our framework complements\nexisting feature engineering techniques by amplifying their marginal\nexplanatory power on average by 31%. The results reveal that different types of\nengagement-continuing to read, commenting, and voting-are driven by distinct\ncombinations of current and anticipated content features. Our framework\nprovides a novel way to study and explore how audience forward-looking beliefs\nshape their engagement with narrative media, with implications for marketing\nstrategy in content-focused industries."}
{"id": "2412.16936", "pdf": "https://arxiv.org/pdf/2412.16936.pdf", "abs": "https://arxiv.org/abs/2412.16936", "title": "Rationale-guided Prompting for Knowledge-based Visual Question Answering", "authors": ["Zhongjian Hu", "Peng Yang", "Bing Li", "Fengyuan Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, Large Language Models (LLMs) have been used for knowledge-based\nVisual Question Answering (VQA). Despite the encouraging results of previous\nstudies, prior methods prompt LLMs to predict answers directly, neglecting\nintermediate thought processes. We argue that prior methods do not sufficiently\nactivate the capacities of LLMs. We propose a framework called PLRH that\nPrompts LLMs with Rationale Heuristics for knowledge-based VQA. The PLRH\nprompts LLMs with Chain of Thought (CoT) to generate rationale heuristics,\ni.e., intermediate thought processes, and then leverages the rationale\nheuristics to inspire LLMs to predict answers. Experiments show that our\napproach outperforms the existing baselines by more than 2.2 and 2.1 on OK-VQA\nand A-OKVQA, respectively."}
{"id": "2501.09213", "pdf": "https://arxiv.org/pdf/2501.09213.pdf", "abs": "https://arxiv.org/abs/2501.09213", "title": "FineMedLM-o1: Enhancing Medical Knowledge Reasoning Ability of LLM from Supervised Fine-Tuning to Test-Time Training", "authors": ["Hongzhou Yu", "Tianhao Cheng", "Yingwen Wang", "Wen He", "Qing Wang", "Ying Cheng", "Yuejie Zhang", "Rui Feng", "Xiaobo Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have shown promise in\nmedical applications such as disease diagnosis and treatment planning. However,\nmost existing medical LLMs struggle with the deep reasoning required for\ncomplex medical problems, such as differential diagnosis and medication\nrecommendations. We propose FineMedLM-o1, which leverages high-quality medical\nsynthetic data and long-form reasoning data for Supervised Fine-Tuning (SFT)\nand Direct Preference Optimization (DPO), enabling advanced dialogue and deep\nreasoning capabilities. Additionally, we introduce Test-Time Training (TTT) in\nthe medical domain for the first time, facilitating domain adaptation and\nensuring reliable, accurate reasoning. Experimental results demonstrate that\nFineMedLM-o1 achieves a 23% average performance improvement over prior models\non key medical benchmarks. Furthermore, the introduction of TTT provides an\nadditional 14% performance boost, highlighting its effectiveness in enhancing\nmedical reasoning capabilities. To support this process, we also propose a\nnovel method for synthesizing medical dialogue. Compared to other open-source\ndatasets, our dataset stands out as superior in both quality and complexity.\nThe project and data will be released on GitHub."}
{"id": "2502.14907", "pdf": "https://arxiv.org/pdf/2502.14907.pdf", "abs": "https://arxiv.org/abs/2502.14907", "title": "GneissWeb: Preparing High Quality Data for LLMs at Scale", "authors": ["Hajar Emami Gohari", "Swanand Ravindra Kadhe", "Syed Yousaf Shah", "Constantin Adam", "Abdulhamid Adebayo", "Praneet Adusumilli", "Farhan Ahmed", "Nathalie Baracaldo Angel", "Santosh Subhashrao Borse", "Yuan-Chi Chang", "Xuan-Hong Dang", "Nirmit Desai", "Revital Eres", "Ran Iwamoto", "Alexei Karve", "Yan Koyfman", "Wei-Han Lee", "Changchang Liu", "Boris Lublinsky", "Takuyo Ohko", "Pablo Pesce", "Maroun Touma", "Shiqiang Wang", "Shalisha Witherspoon", "Herbert WoisetschlÃ¤ger", "David Wood", "Kun-Lung Wu", "Issei Yoshida", "Syed Zawad", "Petros Zerfos", "Yi Zhou", "Bishwaranjan Bhattacharjee"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Data quantity and quality play a vital role in determining the performance of\nLarge Language Models (LLMs). High-quality data, in particular, can\nsignificantly boost the LLM's ability to generalize on a wide range of\ndownstream tasks. Large pre-training datasets for leading LLMs remain\ninaccessible to the public, whereas many open datasets are small in size (less\nthan 5 trillion tokens), limiting their suitability for training large models.\n  In this paper, we introduce GneissWeb, a large dataset yielding around 10\ntrillion tokens that caters to the data quality and quantity requirements of\ntraining LLMs. Our GneissWeb recipe that produced the dataset consists of\nsharded exact sub-string deduplication and a judiciously constructed ensemble\nof quality filters. GneissWeb achieves a favorable trade-off between data\nquality and quantity, producing models that outperform models trained on\nstate-of-the-art open large datasets (5+ trillion tokens).\n  We show that models trained using GneissWeb dataset outperform those trained\non FineWeb-V1.1.0 by 2.73 percentage points in terms of average score computed\non a set of 11 commonly used benchmarks (both zero-shot and few-shot) for\npre-training dataset evaluation. When the evaluation set is extended to 20\nbenchmarks (both zero-shot and few-shot), models trained using GneissWeb still\nachieve a 1.75 percentage points advantage over those trained on\nFineWeb-V1.1.0."}
{"id": "2503.03044", "pdf": "https://arxiv.org/pdf/2503.03044.pdf", "abs": "https://arxiv.org/abs/2503.03044", "title": "QE4PE: Word-level Quality Estimation for Human Post-Editing", "authors": ["Gabriele Sarti", "VilÃ©m Zouhar", "Grzegorz ChrupaÅa", "Ana Guerberof-Arenas", "Malvina Nissim", "Arianna Bisazza"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted by TACL (pre-MIT Press publication version); Code:\n  https://github.com/gsarti/qe4pe. Dataset:\n  https://huggingface.co/datasets/gsarti/qe4pe", "summary": "Word-level quality estimation (QE) methods aim to detect erroneous spans in\nmachine translations, which can direct and facilitate human post-editing. While\nthe accuracy of word-level QE systems has been assessed extensively, their\nusability and downstream influence on the speed, quality and editing choices of\nhuman post-editing remain understudied. In this study, we investigate the\nimpact of word-level QE on machine translation (MT) post-editing in a realistic\nsetting involving 42 professional post-editors across two translation\ndirections. We compare four error-span highlight modalities, including\nsupervised and uncertainty-based word-level QE methods, for identifying\npotential errors in the outputs of a state-of-the-art neural MT model.\nPost-editing effort and productivity are estimated from behavioral logs, while\nquality improvements are assessed by word- and segment-level human annotation.\nWe find that domain, language and editors' speed are critical factors in\ndetermining highlights' effectiveness, with modest differences between\nhuman-made and automated QE highlights underlining a gap between accuracy and\nusability in professional workflows."}
{"id": "2503.20988", "pdf": "https://arxiv.org/pdf/2503.20988.pdf", "abs": "https://arxiv.org/abs/2503.20988", "title": "Cross-Modal State-Space Graph Reasoning for Structured Summarization", "authors": ["Hannah Kim", "Sofia Martinez", "Jason Lee"], "categories": ["cs.CL", "cs.GR"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship and affiliation", "summary": "The ability to extract compact, meaningful summaries from large-scale and\nmultimodal data is critical for numerous applications, ranging from video\nanalytics to medical reports. Prior methods in cross-modal summarization have\noften suffered from high computational overheads and limited interpretability.\nIn this paper, we propose a \\textit{Cross-Modal State-Space Graph Reasoning}\n(\\textbf{CSS-GR}) framework that incorporates a state-space model with\ngraph-based message passing, inspired by prior work on efficient state-space\nmodels. Unlike existing approaches relying on purely sequential models, our\nmethod constructs a graph that captures inter- and intra-modal relationships,\nallowing more holistic reasoning over both textual and visual streams. We\ndemonstrate that our approach significantly improves summarization quality and\ninterpretability while maintaining computational efficiency, as validated on\nstandard multimodal summarization benchmarks. We also provide a thorough\nablation study to highlight the contributions of each component."}
{"id": "2504.01282", "pdf": "https://arxiv.org/pdf/2504.01282.pdf", "abs": "https://arxiv.org/abs/2504.01282", "title": "Prompt-Reverse Inconsistency: LLM Self-Inconsistency Beyond Generative Randomness and Prompt Paraphrasing", "authors": ["Jihyun Janice Ahn", "Wenpeng Yin"], "categories": ["cs.CL"], "comment": "accepted in COLM2025, 9 pages", "summary": "While the inconsistency of LLMs is not a novel topic, prior research has\npredominantly addressed two types of generative inconsistencies: i) Randomness\nInconsistency: running the same LLM multiple trials, yielding varying\nresponses; ii) Paraphrase Inconsistency: paraphrased prompts result in\ndifferent responses from the same LLM. Randomness Inconsistency arises from the\ninherent randomness due to stochastic sampling in generative models, while\nParaphrase Inconsistency is a consequence of the language modeling objectives,\nwhere paraphrased prompts alter the distribution of vocabulary logits. This\nresearch discovers Prompt-Reverse Inconsistency (PRIN), a new form of LLM\nself-inconsistency: given a question and a couple of LLM-generated answer\ncandidates, the LLM often has conflicting responses when prompted \"Which are\ncorrect answers?\" and \"Which are incorrect answers?\". PRIN poses a big concern\nas it undermines the credibility of LLM-as-a-judge, and suggests a challenge\nfor LLMs to adhere to basic logical rules. We conduct a series of experiments\nto investigate PRIN, examining the extent of PRIN across different LLMs,\nmethods to mitigate it, potential applications, and its relationship with\nRandomness Inconsistency and Paraphrase Inconsistency. As the first study to\nexplore PRIN, our findings offer valuable insights into the inner workings of\nLLMs and contribute to advancing trustworthy AI."}
{"id": "2504.05008", "pdf": "https://arxiv.org/pdf/2504.05008.pdf", "abs": "https://arxiv.org/abs/2504.05008", "title": "Voices of Freelance Professional Writers on AI: Limitations, Expectations, and Fears", "authors": ["Anastasiia Ivanova", "Natalia Fedorova", "Sergei Tilga", "Ekaterina Artemova"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "The rapid development of AI-driven tools, particularly large language models\n(LLMs), is reshaping professional writing. Still, key aspects of their adoption\nsuch as languages support, ethics, and long-term impact on writers voice and\ncreativity remain underexplored. In this work, we conducted a questionnaire (N\n= 301) and an interactive survey (N = 36) targeting professional writers\nregularly using AI. We examined LLM-assisted writing practices across 25+\nlanguages, ethical concerns, and user expectations. The findings of the survey\ndemonstrate important insights, reflecting upon the importance of: LLMs\nadoption for non-English speakers; the degree of misinformation, domain and\nstyle adaptation; usability and key features of LLMs. These insights can guide\nfurther development, benefiting both writers and a broader user base."}
{"id": "2504.11829", "pdf": "https://arxiv.org/pdf/2504.11829.pdf", "abs": "https://arxiv.org/abs/2504.11829", "title": "DÃ©jÃ  Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation", "authors": ["Julia Kreutzer", "Eleftheria Briakou", "Sweta Agrawal", "Marzieh Fadaee", "Kocmi Tom"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Generation capabilities and language coverage of multilingual large language\nmodels (mLLMs) are advancing rapidly. However, evaluation practices for\ngenerative abilities of mLLMs are still lacking comprehensiveness, scientific\nrigor, and consistent adoption across research labs, which undermines their\npotential to meaningfully guide mLLM development. We draw parallels with\nmachine translation (MT) evaluation, a field that faced similar challenges and\nhas, over decades, developed transparent reporting standards and reliable\nevaluations for multilingual generative models. Through targeted experiments\nacross key stages of the generative evaluation pipeline, we demonstrate how\nbest practices from MT evaluation can deepen the understanding of quality\ndifferences between models. Additionally, we identify essential components for\nrobust meta-evaluation of mLLMs, ensuring the evaluation methods themselves are\nrigorously assessed. We distill these insights into a checklist of actionable\nrecommendations for mLLM research and development."}
{"id": "2505.00059", "pdf": "https://arxiv.org/pdf/2505.00059.pdf", "abs": "https://arxiv.org/abs/2505.00059", "title": "BERSting at the Screams: A Benchmark for Distanced, Emotional and Shouted Speech Recognition", "authors": ["Paige TuttÃ¶sÃ­", "Mantaj Dhillon", "Luna Sang", "Shane Eastwood", "Poorvi Bhatia", "Quang Minh Dinh", "Avni Kapoor", "Yewon Jin", "Angelica Lim"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Computer Speech and Language, Special issue:\n  Multi-Speaker, Multi-Microphone, and Multi-Modal Distant Speech Recognition.\n  Project Webpage and Data access :\n  https://huggingface.co/datasets/Rosie-Lab/BERSt", "summary": "Some speech recognition tasks, such as automatic speech recognition (ASR),\nare approaching or have reached human performance in many reported metrics.\nYet, they continue to struggle in complex, real-world, situations, such as with\ndistanced speech. Previous challenges have released datasets to address the\nissue of distanced ASR, however, the focus remains primarily on distance,\nspecifically relying on multi-microphone array systems. Here we present the\nB(asic) E(motion) R(andom phrase) S(hou)t(s) (BERSt) dataset. The dataset\ncontains almost 4 hours of English speech from 98 actors with varying regional\nand non-native accents. The data was collected on smartphones in the actors\nhomes and therefore includes at least 98 different acoustic environments. The\ndata also includes 7 different emotion prompts and both shouted and spoken\nutterances. The smartphones were places in 19 different positions, including\nobstructions and being in a different room than the actor. This data is\npublicly available for use and can be used to evaluate a variety of speech\nrecognition tasks, including: ASR, shout detection, and speech emotion\nrecognition (SER). We provide initial benchmarks for ASR and SER tasks, and\nfind that ASR degrades both with an increase in distance and shout level and\nshows varied performance depending on the intended emotion. Our results show\nthat the BERSt dataset is challenging for both ASR and SER tasks and continued\nwork is needed to improve the robustness of such systems for more accurate\nreal-world use."}
{"id": "2505.08450", "pdf": "https://arxiv.org/pdf/2505.08450.pdf", "abs": "https://arxiv.org/abs/2505.08450", "title": "IterKey: Iterative Keyword Generation with LLMs for Enhanced Retrieval Augmented Generation", "authors": ["Kazuki Hayashi", "Hidetaka Kamigaito", "Shinya Kouda", "Taro Watanabe"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a way to complement the\nin-context knowledge of Large Language Models (LLMs) by integrating external\ndocuments. However, real-world applications demand not only accuracy but also\ninterpretability. While dense retrieval methods provide high accuracy, they\nlack interpretability; conversely, sparse retrieval methods offer transparency\nbut often fail to capture the full intent of queries due to their reliance on\nkeyword matching. To address these issues, we introduce IterKey, an LLM-driven\niterative keyword generation framework that enhances RAG via sparse retrieval.\nIterKey consists of three LLM-driven stages: generating keywords for retrieval,\ngenerating answers based on retrieved documents, and validating the answers. If\nvalidation fails, the process iteratively repeats with refined keywords. Across\nfour QA tasks, experimental results show that IterKey achieves 5% to 20%\naccuracy improvements over BM25-based RAG and simple baselines. Its performance\nis comparable to dense retrieval-based RAG and prior iterative query refinement\nmethods using dense models. In summary, IterKey is a novel BM25-based approach\nleveraging LLMs to iteratively refine RAG, effectively balancing accuracy with\ninterpretability."}
{"id": "2505.12474", "pdf": "https://arxiv.org/pdf/2505.12474.pdf", "abs": "https://arxiv.org/abs/2505.12474", "title": "What Are They Talking About? A Benchmark of Knowledge-Grounded Discussion Summarization", "authors": ["Weixiao Zhou", "Junnan Zhu", "Gengyao Li", "Xianfu Cheng", "Xinnian Liang", "Feifei Zhai", "Zhoujun Li"], "categories": ["cs.CL"], "comment": "20 pages, 17 figures and 8 tables", "summary": "Traditional dialogue summarization primarily focuses on dialogue content,\nassuming it comprises adequate information for a clear summary. However, this\nassumption often fails for discussions grounded in shared background, where\nparticipants frequently omit context and use implicit references. This results\nin summaries that are confusing to readers unfamiliar with the background. To\naddress this, we introduce Knowledge-Grounded Discussion Summarization (KGDS),\na novel task that produces a supplementary background summary for context and a\nclear opinion summary with clarified references. To facilitate research, we\nconstruct the first KGDS benchmark, featuring news-discussion pairs and\nexpert-created multi-granularity gold annotations for evaluating sub-summaries.\nWe also propose a novel hierarchical evaluation framework with fine-grained and\ninterpretable metrics. Our extensive evaluation of 12 advanced large language\nmodels (LLMs) reveals that KGDS remains a significant challenge. The models\nfrequently miss key facts and retain irrelevant ones in background\nsummarization, and often fail to resolve implicit references in opinion summary\nintegration."}
{"id": "2505.15038", "pdf": "https://arxiv.org/pdf/2505.15038.pdf", "abs": "https://arxiv.org/abs/2505.15038", "title": "Denoising Concept Vectors with Sparse Autoencoders for Improved Language Model Steering", "authors": ["Haiyan Zhao", "Xuansheng Wu", "Fan Yang", "Bo Shen", "Ninghao Liu", "Mengnan Du"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 4 figures, 4 tables", "summary": "Linear concept vectors effectively steer LLMs, but existing methods suffer\nfrom noisy features in diverse datasets that undermine steering robustness. We\npropose Sparse Autoencoder-Denoised Concept Vectors (SDCV), which selectively\nkeep the most discriminative SAE latents while reconstructing hidden\nrepresentations. Our key insight is that concept-relevant signals can be\nexplicitly separated from dataset noise by scaling up activations of top-k\nlatents that best differentiate positive and negative samples. Applied to\nlinear probing and difference-in-mean, SDCV consistently improves steering\nsuccess rates by 4-16\\% across six challenging concepts, while maintaining\ntopic relevance."}
{"id": "2505.19959", "pdf": "https://arxiv.org/pdf/2505.19959.pdf", "abs": "https://arxiv.org/abs/2505.19959", "title": "MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models", "authors": ["Zhongzhan Huang", "Guoming Ling", "Shanshan Zhong", "Hefeng Wu", "Liang Lin"], "categories": ["cs.CL"], "comment": "Accepted by ACL'25 main track", "summary": "Long Context Understanding (LCU) is a critical area for exploration in\ncurrent large language models (LLMs). However, due to the inherently lengthy\nnature of long-text data, existing LCU benchmarks for LLMs often result in\nprohibitively high evaluation costs, like testing time and inference expenses.\nThrough extensive experimentation, we discover that existing LCU benchmarks\nexhibit significant redundancy, which means the inefficiency in evaluation. In\nthis paper, we propose a concise data compression method tailored for long-text\ndata with sparse information characteristics. By pruning the well-known LCU\nbenchmark LongBench, we create MiniLongBench. This benchmark includes only 237\ntest samples across six major task categories and 21 distinct tasks. Through\nempirical analysis of over 60 LLMs, MiniLongBench achieves an average\nevaluation cost reduced to only 4.5% of the original while maintaining an\naverage rank correlation coefficient of 0.97 with LongBench results. Therefore,\nour MiniLongBench, as a low-cost benchmark, holds great potential to\nsubstantially drive future research into the LCU capabilities of LLMs. See\nhttps://github.com/MilkThink-Lab/MiniLongBench for our code, data and tutorial."}
{"id": "2505.21354", "pdf": "https://arxiv.org/pdf/2505.21354.pdf", "abs": "https://arxiv.org/abs/2505.21354", "title": "Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning", "authors": ["Bidyarthi Paul", "Jalisha Jashim Era", "Mirazur Rahman Zim", "Tahmid Sattar Aothoi", "Faisal Muhammad Shah"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Solving Bengali Math Word Problems (MWPs) remains a major challenge in\nnatural language processing (NLP) due to the language's low-resource status and\nthe multi-step reasoning required. Existing models struggle with complex\nBengali MWPs, largely because no human-annotated Bengali dataset has previously\naddressed this task. This gap has limited progress in Bengali mathematical\nreasoning. To address this, we created SOMADHAN, a dataset of 8792 complex\nBengali MWPs with manually written, step-by-step solutions. We designed this\ndataset to support reasoning-focused evaluation and model development in a\nlinguistically underrepresented context. Using SOMADHAN, we evaluated a range\nof large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series\nmodels, Deepseek, and Qwen - through both zero-shot and few-shot prompting with\nand without Chain of Thought (CoT) reasoning. CoT prompting consistently\nimproved performance over standard prompting, especially in tasks requiring\nmulti-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with\nfew-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune\nmodels efficiently, enabling them to adapt to Bengali MWPs with minimal\ncomputational cost. Our work fills a critical gap in Bengali NLP by providing a\nhigh-quality reasoning dataset and a scalable framework for solving complex\nMWPs. We aim to advance equitable research in low-resource languages and\nenhance reasoning capabilities in educational and language technologies."}
{"id": "2506.04585", "pdf": "https://arxiv.org/pdf/2506.04585.pdf", "abs": "https://arxiv.org/abs/2506.04585", "title": "MuSciClaims: Multimodal Scientific Claim Verification", "authors": ["Yash Kumar Lal", "Manikanta Bandham", "Mohammad Saqib Hasan", "Apoorva Kashi", "Mahnaz Koupaee", "Niranjan Balasubramanian"], "categories": ["cs.CL"], "comment": null, "summary": "Assessing scientific claims requires identifying, extracting, and reasoning\nwith multimodal data expressed in information-rich figures in scientific\nliterature. Despite the large body of work in scientific QA, figure captioning,\nand other multimodal reasoning tasks over chart-based data, there are no\nreadily usable multimodal benchmarks that directly test claim verification\nabilities. To remedy this gap, we introduce a new benchmark MuSciClaims\naccompanied by diagnostics tasks. We automatically extract supported claims\nfrom scientific articles, which we manually perturb to produce contradicted\nclaims. The perturbations are designed to test for a specific set of claim\nverification capabilities. We also introduce a suite of diagnostic tasks that\nhelp understand model failures. Our results show most vision-language models\nare poor (~0.3-0.5 F1), with even the best model only achieving 0.72 F1. They\nare also biased towards judging claims as supported, likely misunderstanding\nnuanced perturbations within the claims. Our diagnostics show models are bad at\nlocalizing correct evidence within figures, struggle with aggregating\ninformation across modalities, and often fail to understand basic components of\nthe figure."}
{"id": "2506.09147", "pdf": "https://arxiv.org/pdf/2506.09147.pdf", "abs": "https://arxiv.org/abs/2506.09147", "title": "LLM-as-a-qualitative-judge: automating error analysis in natural language generation", "authors": ["Nadezhda Chirkova", "Tunde Oluwaseyi Ajayi", "Seth Aycock", "Zain Muhammad Mujahid", "Vladana PerliÄ", "Ekaterina Borisova", "Markarit Vartampetian"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Prompting large language models (LLMs) to evaluate generated text, known as\nLLM-as-a-judge, has become a standard evaluation approach in natural language\ngeneration (NLG), but is primarily used as a quantitative tool, i.e. with\nnumerical scores as main outputs. In this work, we propose\nLLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main\noutput being a structured report of common issue types in the NLG system\noutputs. Our approach is targeted at providing developers with meaningful\ninsights on what improvements can be done to a given NLG system and consists of\ntwo main steps, namely open-ended per-instance issue analysis and clustering of\nthe discovered issues using an intuitive cumulative algorithm. We also\nintroduce a strategy for evaluating the proposed approach, coupled with ~300\nannotations of issues in instances from 12 NLG datasets. Our results show that\nLLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3\ncases and is capable of producing error type reports resembling the reports\ncomposed by human annotators. Our code and data are publicly available at\nhttps://github.com/tunde-ajayi/llm-as-a-qualitative-judge."}
{"id": "2506.19073", "pdf": "https://arxiv.org/pdf/2506.19073.pdf", "abs": "https://arxiv.org/abs/2506.19073", "title": "MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral Reasoning of LLMs through Hate Speech Multi-hop Explanations", "authors": ["Jackson Trager", "Diego Alves", "Matteo Guida", "Mikel K. Ngueajio", "Ameeta Agrawal", "Flor Plaza-del-Arco", "Yalda Daryanai", "Farzan Karimi-Malekabadi", "Francielle Vargas"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "Ensuring the moral reasoning capabilities of Large Language Models (LLMs) is\na growing concern as these systems are used in socially sensitive tasks.\nNevertheless, current evaluation benchmarks present two major shortcomings: a\nlack of annotations that justify moral classifications, which limits\ntransparency and interpretability; and a predominant focus on English, which\nconstrains the assessment of moral reasoning across diverse cultural settings.\nIn this paper, we introduce MFTCXplain, a multilingual benchmark dataset for\nevaluating the moral reasoning of LLMs via hate speech multi-hop explanation\nusing Moral Foundation Theory (MFT). The dataset comprises 3,000 tweets across\nPortuguese, Italian, Persian, and English, annotated with binary hate speech\nlabels, moral categories, and text span-level rationales. Empirical results\nhighlight a misalignment between LLM outputs and human annotations in moral\nreasoning tasks. While LLMs perform well in hate speech detection (F1 up to\n0.836), their ability to predict moral sentiments is notably weak (F1 < 0.35).\nFurthermore, rationale alignment remains limited mainly in underrepresented\nlanguages. These findings show the limited capacity of current LLMs to\ninternalize and reflect human moral reasoning."}
{"id": "2507.15347", "pdf": "https://arxiv.org/pdf/2507.15347.pdf", "abs": "https://arxiv.org/abs/2507.15347", "title": "Probing Information Distribution in Transformer Architectures through Entropy Analysis", "authors": ["Amedeo Buonanno", "Alessandro Rivetti", "Francesco A. N. Palmieri", "Giovanni Di Gennaro", "Gianmarco Romano"], "categories": ["cs.CL", "cs.LG"], "comment": "Presented to the Italian Workshop on Neural Networks (WIRN2025) and\n  it will appear in a Springer Chapter", "summary": "This work explores entropy analysis as a tool for probing information\ndistribution within Transformer-based architectures. By quantifying token-level\nuncertainty and examining entropy patterns across different stages of\nprocessing, we aim to investigate how information is managed and transformed\nwithin these models. As a case study, we apply the methodology to a GPT-based\nlarge language model, illustrating its potential to reveal insights into model\nbehavior and internal representations. This approach may offer insights into\nmodel behavior and contribute to the development of interpretability and\nevaluation frameworks for transformer-based models"}
{"id": "2507.15586", "pdf": "https://arxiv.org/pdf/2507.15586.pdf", "abs": "https://arxiv.org/abs/2507.15586", "title": "Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation", "authors": ["Xinping Zhao", "Shouzheng Huang", "Yan Zhong", "Xinshuo Hu", "Meishan Zhang", "Baotian Hu", "Min Zhang"], "categories": ["cs.CL"], "comment": "16 pages, 7 Figures, 10 Tables", "summary": "Retrieval-Augmented Generation (RAG) effectively improves the accuracy of\nLarge Language Models (LLMs). However, retrieval noises significantly impact\nthe quality of LLMs' generation, necessitating the development of denoising\nmechanisms. Previous methods extract evidence straightforwardly without\nexplicit thinking, which risks filtering out key clues and struggles with\ngeneralization. To this end, we propose EviOmni, which learns to extract\nrational evidence by (1) explicitly reasoning to identify potential cues within\nretrieval contents first, and then (2) consciously extracting to avoid omitting\nany key cues helpful for answering questions. Specifically, we frame evidence\nreasoning and evidence extraction into one unified response for end-to-end\ntraining; apply knowledge token masks for disentanglement to derive\nreasoning-based and extraction-based answers; and devise three types of\nverifiable reward functions, including answer, length, and format, to update\nthe model via the policy optimization algorithm. Extensive experiments on three\nbenchmark datasets show the effectiveness of EviOmni, providing compact and\nhigh-quality evidence, improving the accuracy of downstream tasks, and\npromoting effective application in online RAG systems."}
{"id": "2507.15779", "pdf": "https://arxiv.org/pdf/2507.15779.pdf", "abs": "https://arxiv.org/abs/2507.15779", "title": "Reservoir Computing as a Language Model", "authors": ["Felix KÃ¶ster", "Atsushi Uchida"], "categories": ["cs.CL"], "comment": "8 pages, 5 figures, 1 table Code available at:\n  https://github.com/fekoester/Shakespeare_Res", "summary": "Large Language Models (LLM) have dominated the science and media landscape\nduo to their impressive performance on processing large chunks of data and\nproduce human-like levels of text. Nevertheless, their huge energy demand and\nslow processing still a bottleneck for further increasing quality while also\nmaking the models accessible to everyone. To solve this bottleneck, we will\ninvestigate how reservoir computing performs on natural text processing, which\ncould enable fast and energy efficient hardware implementations. Studies\ninvestigating the use of reservoir computing as a language model remain sparse.\nIn this paper, we compare three distinct approaches for character-level\nlanguage modeling, two different reservoir computing approaches, where only an\noutput layer is trainable, and the well-known transformer-based architectures,\nwhich fully learn an attention-based sequence representation. We explore the\nperformance, computational cost and prediction accuracy for both paradigms by\nequally varying the number of trainable parameters for all models. Using a\nconsistent pipeline for all three approaches, we demonstrate that transformers\nexcel in prediction quality, whereas reservoir computers remain highly\nefficient reducing the training and inference speed. Furthermore, we\ninvestigate two types of reservoir computing: a traditional reservoir with a\nstatic linear readout, and an attention-enhanced reservoir that dynamically\nadapts its output weights via an attention mechanism. Our findings underline\nhow these paradigms scale and offer guidelines to balance resource constraints\nwith performance."}
{"id": "2507.19741", "pdf": "https://arxiv.org/pdf/2507.19741.pdf", "abs": "https://arxiv.org/abs/2507.19741", "title": "Basic Reading Distillation", "authors": ["Zhi Zhou", "Sirui Miao", "Xiangyu Duan", "Hao Yang", "Min Zhang"], "categories": ["cs.CL"], "comment": "Accepted by ACL2025", "summary": "Large language models (LLMs) have demonstrated remarkable abilities in\nvarious natural language processing areas, but they demand high computation\nresources which limits their deployment in real-world. Distillation is one\ntechnique to solve this problem through either knowledge distillation or task\ndistillation. Both distillation approaches train small models to imitate\nspecific features of LLMs, but they all neglect basic reading education for\nsmall models on generic texts that are \\emph{unrelated} to downstream tasks. In\nthis paper, we propose basic reading distillation (BRD) which educates a small\nmodel to imitate LLMs basic reading behaviors, such as named entity\nrecognition, question raising and answering, on each sentence. After such basic\neducation, we apply the small model on various tasks including language\ninference benchmarks and BIG-bench tasks. It shows that the small model can\noutperform or perform comparable to over 20x bigger LLMs. Analysis reveals that\nBRD effectively influences the probability distribution of the small model, and\nhas orthogonality to either knowledge distillation or task distillation."}
{"id": "2507.20930", "pdf": "https://arxiv.org/pdf/2507.20930.pdf", "abs": "https://arxiv.org/abs/2507.20930", "title": "FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models", "authors": ["Likun Tan", "Kuan-Wei Huang", "Kevin Wu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Hallucinations in large language models pose a critical challenge for\napplications requiring factual reliability, particularly in high-stakes domains\nsuch as finance. This work presents an effective approach for detecting and\nediting factually incorrect content in model-generated responses based on the\nprovided context. Given a user-defined domain-specific error taxonomy, we\nconstruct a synthetic dataset by inserting tagged errors into financial\nquestion-answering corpora and then fine-tune four language models, Phi-4,\nPhi-4-mini, Qwen3-4B, and Qwen3-14B, to detect and edit these factual\ninaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8%\nimprovement in binary F1 score and a 30% gain in overall detection performance\ncompared to OpenAI-o3. Notably, our fine-tuned Phi-4-mini model, despite having\nonly 4 billion parameters, maintains competitive performance with just a 2%\ndrop in binary detection and a 0.1% decline in overall detection compared to\nOpenAI-o3. Our work provides a practical solution for detecting and editing\nfactual inconsistencies in financial text generation while introducing a\ngeneralizable framework that can enhance the trustworthiness and alignment of\nlarge language models across diverse applications beyond finance. Our code and\ndata are available at https://github.com/pegasi-ai/shield."}
{"id": "2507.21919", "pdf": "https://arxiv.org/pdf/2507.21919.pdf", "abs": "https://arxiv.org/abs/2507.21919", "title": "Training language models to be warm and empathetic makes them less reliable and more sycophantic", "authors": ["Lujain Ibrahim", "Franziska Sofia Hafner", "Luc Rocher"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Artificial intelligence (AI) developers are increasingly building language\nmodels with warm and empathetic personas that millions of people now use for\nadvice, therapy, and companionship. Here, we show how this creates a\nsignificant trade-off: optimizing language models for warmth undermines their\nreliability, especially when users express vulnerability. We conducted\ncontrolled experiments on five language models of varying sizes and\narchitectures, training them to produce warmer, more empathetic responses, then\nevaluating them on safety-critical tasks. Warm models showed substantially\nhigher error rates (+10 to +30 percentage points) than their original\ncounterparts, promoting conspiracy theories, providing incorrect factual\ninformation, and offering problematic medical advice. They were also\nsignificantly more likely to validate incorrect user beliefs, particularly when\nuser messages expressed sadness. Importantly, these effects were consistent\nacross different model architectures, and occurred despite preserved\nperformance on standard benchmarks, revealing systematic risks that current\nevaluation practices may fail to detect. As human-like AI systems are deployed\nat an unprecedented scale, our findings indicate a need to rethink how we\ndevelop and oversee these systems that are reshaping human relationships and\nsocial interaction."}
{"id": "2507.22050", "pdf": "https://arxiv.org/pdf/2507.22050.pdf", "abs": "https://arxiv.org/abs/2507.22050", "title": "DeepSieve: Information Sieving via LLM-as-a-Knowledge-Router", "authors": ["Minghao Guo", "Qingcheng Zeng", "Xujiang Zhao", "Yanchi Liu", "Wenchao Yu", "Mengnan Du", "Haifeng Chen", "Wei Cheng"], "categories": ["cs.CL"], "comment": "22 pages, work in progress", "summary": "Large Language Models (LLMs) excel at many reasoning tasks but struggle with\nknowledge-intensive queries due to their inability to dynamically access\nup-to-date or domain-specific information. Retrieval-Augmented Generation (RAG)\nhas emerged as a promising solution, enabling LLMs to ground their responses in\nexternal sources. However, existing RAG methods lack fine-grained control over\nboth the query and source sides, often resulting in noisy retrieval and shallow\nreasoning. In this work, we introduce DeepSieve, an agentic RAG framework that\nincorporates information sieving via LLM-as-a-knowledge-router. DeepSieve\ndecomposes complex queries into structured sub-questions and recursively routes\neach to the most suitable knowledge source, filtering irrelevant information\nthrough a multi-stage distillation process. Our design emphasizes modularity,\ntransparency, and adaptability, leveraging recent advances in agentic system\ndesign. Experiments on multi-hop QA tasks across heterogeneous sources\ndemonstrate improved reasoning depth, retrieval precision, and interpretability\nover conventional RAG approaches. Our codes are available at\nhttps://github.com/MinghoKwok/DeepSieve."}
{"id": "2404.07214", "pdf": "https://arxiv.org/pdf/2404.07214.pdf", "abs": "https://arxiv.org/abs/2404.07214", "title": "Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions", "authors": ["Akash Ghosh", "Arkadeep Acharya", "Sriparna Saha", "Vinija Jain", "Aman Chadha"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "One of the first survey on Visual Language Models", "summary": "The advent of Large Language Models (LLMs) has significantly reshaped the\ntrajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable\nlimitation, as they are primarily adept at processing textual information. To\naddress this constraint, researchers have endeavored to integrate visual\ncapabilities with LLMs, resulting in the emergence of Vision-Language Models\n(VLMs). These advanced models are instrumental in tackling more intricate tasks\nsuch as image captioning and visual question answering. In our comprehensive\nsurvey paper, we delve into the key advancements within the realm of VLMs. Our\nclassification organizes VLMs into three distinct categories: models dedicated\nto vision-language understanding, models that process multimodal inputs to\ngenerate unimodal (textual) outputs and models that both accept and produce\nmultimodal inputs and outputs.This classification is based on their respective\ncapabilities and functionalities in processing and generating various\nmodalities of data.We meticulously dissect each model, offering an extensive\nanalysis of its foundational architecture, training data sources, as well as\nits strengths and limitations wherever possible, providing readers with a\ncomprehensive understanding of its essential components. We also analyzed the\nperformance of VLMs in various benchmark datasets. By doing so, we aim to offer\na nuanced understanding of the diverse landscape of VLMs. Additionally, we\nunderscore potential avenues for future research in this dynamic domain,\nanticipating further breakthroughs and advancements."}
{"id": "2406.05515", "pdf": "https://arxiv.org/pdf/2406.05515.pdf", "abs": "https://arxiv.org/abs/2406.05515", "title": "Mmm whatcha say? Uncovering distal and proximal context effects in first and second-language word perception using psychophysical reverse correlation", "authors": ["Paige TuttÃ¶sÃ­", "H. Henny Yeung", "Yue Wang", "Fenqi Wang", "Guillaume Denis", "Jean-Julien Aucouturier", "Angelica Lim"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to INTERSPEECH 2024 Project Webpage :\n  https://rosielab.github.io/vocal_ambiguity/ Code:\n  https://github.com/neuro-team-femto/vocal_ambiguity Data :\n  https://zenodo.org/records/12761242", "summary": "Acoustic context effects, where surrounding changes in pitch, rate or timbre\ninfluence the perception of a sound, are well documented in speech perception,\nbut how they interact with language background remains unclear. Using a\nreverse-correlation approach, we systematically varied the pitch and speech\nrate in phrases around different pairs of vowels for second language (L2)\nspeakers of English (/i/-/I/) and French (/u/-/y/), thus reconstructing, in a\ndata-driven manner, the prosodic profiles that bias their perception. Testing\nEnglish and French speakers (n=25), we showed that vowel perception is in fact\ninfluenced by conflicting effects from the surrounding pitch and speech rate: a\ncongruent proximal effect 0.2s pre-target and a distal contrastive effect up to\n1s before; and found that L1 and L2 speakers exhibited strikingly similar\nprosodic profiles in perception. We provide a novel method to investigate\nacoustic context effects across stimuli, timescales, and acoustic domain."}
{"id": "2410.07336", "pdf": "https://arxiv.org/pdf/2410.07336.pdf", "abs": "https://arxiv.org/abs/2410.07336", "title": "Positive-Augmented Contrastive Learning for Vision-and-Language Evaluation and Training", "authors": ["Sara Sarto", "Nicholas Moratelli", "Marcella Cornia", "Lorenzo Baraldi", "Rita Cucchiara"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": "International Journal of Computer Vision (2025)", "summary": "Despite significant advancements in caption generation, existing evaluation\nmetrics often fail to capture the full quality or fine-grained details of\ncaptions. This is mainly due to their reliance on non-specific human-written\nreferences or noisy pre-training data. Still, finding an effective metric is\ncrucial not only for captions evaluation but also for the generation phase.\nMetrics can indeed play a key role in the fine-tuning stage of captioning\nmodels, ultimately enhancing the quality of the generated captions. In this\npaper, we propose PAC-S++, a learnable metric that leverages the CLIP model,\npre-trained on both web-collected and cleaned data and regularized through\nadditional pairs of generated visual and textual positive samples. Exploiting\nthis stronger and curated pre-training, we also apply PAC-S++ as a reward in\nthe Self-Critical Sequence Training (SCST) stage typically employed to\nfine-tune captioning models. Extensive experiments on different image and video\ndatasets highlight the effectiveness of PAC-S++ compared to popular metrics for\nthe task, including its sensitivity to object hallucinations. Furthermore, we\nshow that integrating PAC-S++ into the fine-tuning stage of a captioning model\nresults in semantically richer captions with fewer repetitions and grammatical\nerrors. Evaluations on out-of-domain benchmarks further demonstrate the\nefficacy of our fine-tuning approach in enhancing model capabilities. Source\ncode and trained models are publicly available at:\nhttps://github.com/aimagelab/pacscore."}
{"id": "2411.08003", "pdf": "https://arxiv.org/pdf/2411.08003.pdf", "abs": "https://arxiv.org/abs/2411.08003", "title": "Can adversarial attacks by large language models be attributed?", "authors": ["Manuel Cebrian", "Andres Abeliuk", "Jan Arne Telle"], "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.FL"], "comment": "22 pages, 5 figures, 2 tables", "summary": "Attributing outputs from Large Language Models (LLMs) in adversarial\nsettings-such as cyberattacks and disinformation campaigns-presents significant\nchallenges that are likely to grow in importance. We approach this attribution\nproblem from both a theoretical and an empirical perspective, drawing on formal\nlanguage theory (identification in the limit) and data-driven analysis of the\nexpanding LLM ecosystem. By modeling an LLM's set of possible outputs as a\nformal language, we analyze whether finite samples of text can uniquely\npinpoint the originating model. Our results show that, under mild assumptions\nof overlapping capabilities among models, certain classes of LLMs are\nfundamentally non-identifiable from their outputs alone. We delineate four\nregimes of theoretical identifiability: (1) an infinite class of deterministic\n(discrete) LLM languages is not identifiable (Gold's classical result from\n1967); (2) an infinite class of probabilistic LLMs is also not identifiable (by\nextension of the deterministic case); (3) a finite class of deterministic LLMs\nis identifiable (consistent with Angluin's tell-tale criterion); and (4) even a\nfinite class of probabilistic LLMs can be non-identifiable (we provide a new\ncounterexample establishing this negative result). Complementing these\ntheoretical insights, we quantify the explosion in the number of plausible\nmodel origins (hypothesis space) for a given output in recent years. Even under\nconservative assumptions-each open-source model fine-tuned on at most one new\ndataset-the count of distinct candidate models doubles approximately every 0.5\nyears, and allowing multi-dataset fine-tuning combinations yields doubling\ntimes as short as 0.28 years. This combinatorial growth, alongside the\nextraordinary computational cost of brute-force likelihood attribution across\nall models and potential users, renders exhaustive attribution infeasible in\npractice."}
{"id": "2502.13820", "pdf": "https://arxiv.org/pdf/2502.13820.pdf", "abs": "https://arxiv.org/abs/2502.13820", "title": "Scoring Verifiers: Evaluating Synthetic Verification for Code and Reasoning", "authors": ["Aleksander Ficek", "Somshubra Majumdar", "Vahid Noroozi", "Boris Ginsburg"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "comment": "COLM 2025", "summary": "Synthetic verification techniques such as generating test cases and reward\nmodelling are common ways to enhance the coding capabilities of large language\nmodels (LLM) beyond predefined tests. Additionally, code verification has\nrecently found great success as a critical component in improving reasoning\ncapability of LLMs via reinforcement learning. In this paper, we propose an\napproach which can transform existing coding benchmarks into scoring and\nranking datasets to evaluate the effectiveness of synthetic verifiers. We also\npropose multiple metrics to measure different aspects of the synthetic\nverifiers with the proposed benchmarks. By employing the proposed approach, we\nrelease four new benchmarks (HE-R, HE-R+, MBPP-R, and MBPP-R+), and analyzed\nsynthetic verification methods with standard, reasoning-based, and reward-based\nLLMs. Our experiments show that reasoning can significantly improve test case\ngeneration and that scaling the number of test cases enhances the verification\naccuracy."}
{"id": "2503.07631", "pdf": "https://arxiv.org/pdf/2503.07631.pdf", "abs": "https://arxiv.org/abs/2503.07631", "title": "OWLViz: An Open-World Benchmark for Visual Question Answering", "authors": ["Thuy Nguyen", "Dang Nguyen", "Hoang Nguyen", "Thuan Luong", "Long Hoang Dang", "Viet Dac Lai"], "categories": ["cs.LG", "cs.CL"], "comment": "8 pages + appendix", "summary": "We present a challenging benchmark for the Open WorLd VISual question\nanswering (OWLViz) task. OWLViz presents concise, unambiguous queries that\nrequire integrating multiple capabilities, including visual understanding, web\nexploration, and specialized tool usage. While humans achieve 69.2% accuracy on\nthese intuitive tasks, even state-of-the-art VLMs struggle, with the best\nmodel, Gemini 2.0, achieving only 26.6% accuracy. Current agentic VLMs, which\nrely on limited vision and vision-language models as tools, perform even worse.\nThis performance gap reveals significant limitations in multimodal systems'\nability to select appropriate tools and execute complex reasoning sequences,\nestablishing new directions for advancing practical AI research."}
{"id": "2503.20992", "pdf": "https://arxiv.org/pdf/2503.20992.pdf", "abs": "https://arxiv.org/abs/2503.20992", "title": "ReverBERT: A State Space Model for Efficient Text-Driven Speech Style Transfer", "authors": ["Michael Brown", "Sofia Martinez", "Priya Singh"], "categories": ["cs.GR", "cs.CL"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship and affiliation", "summary": "Text-driven speech style transfer aims to mold the intonation, pace, and\ntimbre of a spoken utterance to match stylistic cues from text descriptions.\nWhile existing methods leverage large-scale neural architectures or pre-trained\nlanguage models, the computational costs often remain high. In this paper, we\npresent \\emph{ReverBERT}, an efficient framework for text-driven speech style\ntransfer that draws inspiration from a state space model (SSM) paradigm,\nloosely motivated by the image-based method of Wang and\nLiu~\\cite{wang2024stylemamba}. Unlike image domain techniques, our method\noperates in the speech space and integrates a discrete Fourier transform of\nlatent speech features to enable smooth and continuous style modulation. We\nalso propose a novel \\emph{Transformer-based SSM} layer for bridging textual\nstyle descriptors with acoustic attributes, dramatically reducing inference\ntime while preserving high-quality speech characteristics. Extensive\nexperiments on benchmark speech corpora demonstrate that \\emph{ReverBERT}\nsignificantly outperforms baselines in terms of naturalness, expressiveness,\nand computational efficiency. We release our model and code publicly to foster\nfurther research in text-driven speech style transfer."}
{"id": "2504.11257", "pdf": "https://arxiv.org/pdf/2504.11257.pdf", "abs": "https://arxiv.org/abs/2504.11257", "title": "UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis", "authors": ["Xinyi Liu", "Xiaoyi Zhang", "Ziyun Zhang", "Yan Lu"], "categories": ["cs.HC", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advancements in Large Vision-Language Models are accelerating the\ndevelopment of Graphical User Interface (GUI) agents that utilize human-like\nvision perception capabilities to enhance productivity on digital devices.\nCompared to approaches predicated on GUI metadata, which are platform-dependent\nand vulnerable to implementation variations, vision-based approaches offer\nbroader applicability. In this vision-based paradigm, the GUI instruction\ngrounding, which maps user instruction to the location of corresponding element\non the given screenshot, remains a critical challenge, particularly due to\nlimited public training dataset and resource-intensive manual instruction data\nannotation. In this paper, we delve into unexplored challenges in this task\nincluding element-to-screen ratio, unbalanced element type, and implicit\ninstruction. To address these challenges, we introduce a large-scale data\nsynthesis pipeline UI-E2I-Synth for generating varying complex instruction\ndatasets using GPT-4o instead of human annotators. Furthermore, we propose a\nnew GUI instruction grounding benchmark UI-I2E-Bench, which is designed to\naddress the limitations of existing benchmarks by incorporating diverse\nannotation aspects. Our model, trained on the synthesized data, achieves\nsuperior performance in GUI instruction grounding, demonstrating the\nadvancements of proposed data synthesis pipeline. The proposed benchmark,\naccompanied by extensive analyses, provides practical insights for future\nresearch in GUI grounding. We will release corresponding artifacts at\nhttps://microsoft.github.io/FIVE-UI-Evol/ ."}
{"id": "2504.13932", "pdf": "https://arxiv.org/pdf/2504.13932.pdf", "abs": "https://arxiv.org/abs/2504.13932", "title": "Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining", "authors": ["Deyu Cao", "Samin Aref"], "categories": ["cs.LG", "cs.CL", "68T50, 68T07, 68T09, 68U15", "I.2.7; I.2.6; I.2.4"], "comment": "This is a post-peer-review accepted manuscript from the proceedings\n  of the 22nd International Conference on Modeling Decisions for Artificial\n  Intelligence (MDAI'25). The publisher authenticated version and full citation\n  details are available on Springer's website (LNAI 15957).\n  https://doi.org/10.1007/978-3-032-00891-6_28", "summary": "The growing use of large language models has raised environmental and\neconomic concerns about their intensity of resource usage during inference.\nServing these models to each user requires substantial energy and water for\ncooling. Model compression techniques like quantization can shrink large\nlanguage models and make them more resource efficient at the cost of potential\nperformance degradation. Quantization methods compress model size through\nreplacing their high-precision parameters by quantized values of lower\nprecision. Among existing methods, the ApiQ method achieves superior accuracy\npreservation at minimal memory and time overhead. We investigate two ideas to\nextend performance in ultra-low-bit quantization beyond ApiQ's level. First, we\nlook into combining existing quantization-aware training techniques with ApiQ's\npartial training. We show that this does not outperform the baseline ApiQ\nmethod with limited training data and frozen weights. This leads to two key\ninsights: (1) The substantial representational capacity that is gained through\nfull retraining is unlikely to be feasible through partial training. (2) This\ngain may depend on using a large and diverse dataset in quantization-aware\ntraining. Second, through a novel approach informed by the two insights, we\npropose an ultra-low-bit quantization method that builds upon ApiQ and extends\nits performance without the need for full retraining. This publicly available\nmethod relies on a saliency-aware regularization term that prioritizes\npreserving the most impactful parameters during quantization. Our experiments\non LLaMA 7B and 13B benchmarks demonstrate that our method reduces the ApiQ's\naccuracy degradation by 10.85% and 7.54% respectively. A Python implementation\nof the proposed quantization method is publicly available on GitHub\nhttps://github.com/TokuyuSou/ULB-SAPR."}
{"id": "2505.19010", "pdf": "https://arxiv.org/pdf/2505.19010.pdf", "abs": "https://arxiv.org/abs/2505.19010", "title": "Co-AttenDWG: Co-Attentive Dimension-Wise Gating and Expert Fusion for Multi-Modal Offensive Content Detection", "authors": ["Md. Mithun Hossain", "Md. Shakil Hossain", "Sudipto Chaki", "M. F. Mridha"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Multi-modal learning has emerged as a crucial research direction, as\nintegrating textual and visual information can substantially enhance\nperformance in tasks such as classification, retrieval, and scene\nunderstanding. Despite advances with large pre-trained models, existing\napproaches often suffer from insufficient cross-modal interactions and rigid\nfusion strategies, failing to fully harness the complementary strengths of\ndifferent modalities. To address these limitations, we propose Co-AttenDWG,\nco-attention with dimension-wise gating, and expert fusion. Our approach first\nprojects textual and visual features into a shared embedding space, where a\ndedicated co-attention mechanism enables simultaneous, fine-grained\ninteractions between modalities. This is further strengthened by a\ndimension-wise gating network, which adaptively modulates feature contributions\nat the channel level to emphasize salient information. In parallel, dual-path\nencoders independently refine modality-specific representations, while an\nadditional cross-attention layer aligns the modalities further. The resulting\nfeatures are aggregated via an expert fusion module that integrates learned\ngating and self-attention, yielding a robust unified representation.\nExperimental results on the MIMIC and SemEval Memotion 1.0 datasets show that\nCo-AttenDWG achieves state-of-the-art performance and superior cross-modal\nalignment, highlighting its effectiveness for diverse multi-modal applications."}
{"id": "2506.06157", "pdf": "https://arxiv.org/pdf/2506.06157.pdf", "abs": "https://arxiv.org/abs/2506.06157", "title": "Masked Language Models are Good Heterogeneous Graph Generalizers", "authors": ["Jinyu Yang", "Cheng Yang", "Shanyuan Cui", "Zeyuan Guo", "Liangwei Yang", "Muhan Zhang", "Zhiqiang Zhang", "Chuan Shi"], "categories": ["cs.SI", "cs.CL"], "comment": null, "summary": "Heterogeneous graph neural networks (HGNNs) excel at capturing structural and\nsemantic information in heterogeneous graphs (HGs), while struggling to\ngeneralize across domains and tasks. With the rapid advancement of large\nlanguage models (LLMs), a recent study explored the integration of HGNNs with\nLLMs for generalizable heterogeneous graph learning. However, this approach\ntypically encodes structural information as HG tokens using HGNNs, and\ndisparities in embedding spaces between HGNNs and LLMs have been shown to bias\nthe LLM's comprehension of HGs. Moreover, since these HG tokens are often\nderived from node-level tasks, the model's ability to generalize across tasks\nremains limited. To this end, we propose a simple yet effective Masked Language\nModeling-based method, called MLM4HG. MLM4HG introduces metapath-based textual\nsequences instead of HG tokens to extract structural and semantic information\ninherent in HGs, and designs customized textual templates to unify different\ngraph tasks into a coherent cloze-style 'mask' token prediction paradigm.\nSpecifically,MLM4HG first converts HGs from various domains to texts based on\nmetapaths, and subsequently combines them with the unified task texts to form a\nHG-based corpus. Moreover, the corpus is fed into a pretrained LM for\nfine-tuning with a constrained target vocabulary, enabling the fine-tuned LM to\ngeneralize to unseen target HGs. Extensive cross-domain and multi-task\nexperiments on four real-world datasets demonstrate the superior generalization\nperformance of MLM4HG over state-of-the-art methods in both few-shot and\nzero-shot scenarios. Our code is available at\nhttps://github.com/BUPT-GAMMA/MLM4HG."}
{"id": "2506.15677", "pdf": "https://arxiv.org/pdf/2506.15677.pdf", "abs": "https://arxiv.org/abs/2506.15677", "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence", "authors": ["Yining Hong", "Rui Sun", "Bingxuan Li", "Xingcheng Yao", "Maxine Wu", "Alexander Chien", "Da Yin", "Ying Nian Wu", "Zhecan James Wang", "Kai-Wei Chang"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MM", "cs.RO"], "comment": null, "summary": "AI agents today are mostly siloed - they either retrieve and reason over vast\namount of digital information and knowledge obtained online; or interact with\nthe physical world through embodied perception, planning and action - but\nrarely both. This separation limits their ability to solve tasks that require\nintegrated physical and digital intelligence, such as cooking from online\nrecipes, navigating with dynamic map data, or interpreting real-world landmarks\nusing web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI\nagents that fluidly bridge embodiment and web-scale reasoning. To\noperationalize this concept, we first develop the Embodied Web Agents task\nenvironments, a unified simulation platform that tightly integrates realistic\n3D indoor and outdoor environments with functional web interfaces. Building\nupon this platform, we construct and release the Embodied Web Agents Benchmark,\nwhich encompasses a diverse suite of tasks including cooking, navigation,\nshopping, tourism, and geolocation - all requiring coordinated reasoning across\nphysical and digital realms for systematic assessment of cross-domain\nintelligence. Experimental results reveal significant performance gaps between\nstate-of-the-art AI systems and human capabilities, establishing both\nchallenges and opportunities at the intersection of embodied cognition and\nweb-scale knowledge access. All datasets, codes and websites are publicly\navailable at our project page https://embodied-web-agent.github.io/."}
{"id": "2507.07610", "pdf": "https://arxiv.org/pdf/2507.07610.pdf", "abs": "https://arxiv.org/abs/2507.07610", "title": "SpatialViz-Bench: Automatically Generated Spatial Visualization Reasoning Tasks for MLLMs", "authors": ["Siting Wang", "Luoyang Sun", "Cheng Deng", "Kun Shao", "Minnan Pei", "Zheng Tian", "Haifeng Zhang", "Jun Wang"], "categories": ["cs.CV", "cs.CL", "cs.HC"], "comment": null, "summary": "Humans can directly imagine and manipulate visual images in their minds, a\ncapability known as spatial visualization. While multi-modal Large Language\nModels (MLLMs) support imagination-based reasoning, spatial visualization\nremains insufficiently evaluated, typically embedded within broader\nmathematical and logical assessments. Existing evaluations often rely on IQ\ntests or math competitions that may overlap with training data, compromising\nassessment reliability. To this end, we introduce SpatialViz-Bench, a\ncomprehensive multi-modal benchmark for spatial visualization with 12 tasks\nacross 4 sub-abilities, comprising 1,180 automatically generated problems. Our\nevaluation of 33 state-of-the-art MLLMs not only reveals wide performance\nvariations and demonstrates the benchmark's strong discriminative power, but\nalso uncovers counter-intuitive findings: models show difficulty perception\nmisaligned with human intuition, exhibit dramatic 2Dto-3D performance cliffs,\ndefault to formulaic derivation over visualization, and paradoxically suffer\nperformance degradation from Chain-of-Thought prompting in open-source models.\nThrough statistical and qualitative analysis of error types, SpatialViz-Bench\ndemonstrates that state-of-the-art MLLMs continue to exhibit deficiencies in\nspatial visualization tasks, thereby addressing a significant lacuna in the\nfield. The benchmark data and evaluation code are publicly available."}
{"id": "2507.07966", "pdf": "https://arxiv.org/pdf/2507.07966.pdf", "abs": "https://arxiv.org/abs/2507.07966", "title": "Scaling RL to Long Videos", "authors": ["Yukang Chen", "Wei Huang", "Baifeng Shi", "Qinghao Hu", "Hanrong Ye", "Ligeng Zhu", "Zhijian Liu", "Pavlo Molchanov", "Jan Kautz", "Xiaojuan Qi", "Sifei Liu", "Hongxu Yin", "Yao Lu", "Song Han"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Code at https://github.com/NVlabs/Long-RL and model at\n  https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B", "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames)."}
{"id": "2507.08771", "pdf": "https://arxiv.org/pdf/2507.08771.pdf", "abs": "https://arxiv.org/abs/2507.08771", "title": "BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity", "authors": ["Chenyang Song", "Weilin Zhao", "Xu Han", "Chaojun Xiao", "Yingfa Chen", "Yuxuan Li", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.LG", "cs.CL"], "comment": "21 pages, 7 figures, 15 tables", "summary": "To alleviate the computational burden of large language models (LLMs),\narchitectures with activation sparsity, represented by mixture-of-experts\n(MoE), have attracted increasing attention. However, the non-differentiable and\ninflexible routing of vanilla MoE hurts model performance. Moreover, while each\ntoken activates only a few parameters, these sparsely-activated architectures\nexhibit low chunk-level sparsity, indicating that the union of multiple\nconsecutive tokens activates a large ratio of parameters. Such a sparsity\npattern is unfriendly for acceleration under low-resource conditions (e.g.,\nend-side devices) and incompatible with mainstream acceleration techniques\n(e.g., speculative decoding). To address these challenges, we introduce a novel\nMoE architecture, BlockFFN, as well as its efficient training and deployment\ntechniques. Specifically, we use a router integrating ReLU activation and\nRMSNorm for differentiable and flexible routing. Next, to promote both\ntoken-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training\nobjectives are designed, making BlockFFN more acceleration-friendly. Finally,\nwe implement efficient acceleration kernels, combining activation sparsity and\nspeculative decoding for the first time. The experimental results demonstrate\nthe superior performance of BlockFFN over other MoE baselines, achieving over\n80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\\times$ speedup on\nreal end-side devices than dense models. All codes and checkpoints are\navailable publicly (https://github.com/thunlp/BlockFFN)."}
{"id": "2507.19947", "pdf": "https://arxiv.org/pdf/2507.19947.pdf", "abs": "https://arxiv.org/abs/2507.19947", "title": "Spatial Language Likelihood Grounding Network for Bayesian Fusion of Human-Robot Observations", "authors": ["Supawich Sitdhipol", "Waritwong Sukprasongdee", "Ekapol Chuangsuwanich", "Rina Tse"], "categories": ["cs.RO", "cs.CL", "cs.IT", "cs.LG", "cs.SY", "eess.SY", "math.IT"], "comment": "Accepted to the 2025 IEEE International Conference on Systems, Man,\n  and Cybernetics (SMC); Supplementary video: https://cu-asl.github.io/fp-lgn/", "summary": "Fusing information from human observations can help robots overcome sensing\nlimitations in collaborative tasks. However, an uncertainty-aware fusion\nframework requires a grounded likelihood representing the uncertainty of human\ninputs. This paper presents a Feature Pyramid Likelihood Grounding Network\n(FP-LGN) that grounds spatial language by learning relevant map image features\nand their relationships with spatial relation semantics. The model is trained\nas a probability estimator to capture aleatoric uncertainty in human language\nusing three-stage curriculum learning. Results showed that FP-LGN matched\nexpert-designed rules in mean Negative Log-Likelihood (NLL) and demonstrated\ngreater robustness with lower standard deviation. Collaborative sensing results\ndemonstrated that the grounded likelihood successfully enabled\nuncertainty-aware fusion of heterogeneous human language observations and robot\nsensor measurements, achieving significant improvements in human-robot\ncollaborative task performance."}
{"id": "2507.20884", "pdf": "https://arxiv.org/pdf/2507.20884.pdf", "abs": "https://arxiv.org/abs/2507.20884", "title": "The Importance of Facial Features in Vision-based Sign Language Recognition: Eyes, Mouth or Full Face?", "authors": ["Dinh Nam Pham", "Eleftherios Avramidis"], "categories": ["cs.CV", "cs.CL", "eess.IV"], "comment": "Accepted at 9th International Workshop on Sign Language Translation\n  and Avatar Technologies @ ACM IVA'25", "summary": "Non-manual facial features play a crucial role in sign language\ncommunication, yet their importance in automatic sign language recognition\n(ASLR) remains underexplored. While prior studies have shown that incorporating\nfacial features can improve recognition, related work often relies on\nhand-crafted feature extraction and fails to go beyond the comparison of manual\nfeatures versus the combination of manual and facial features. In this work, we\nsystematically investigate the contribution of distinct facial regionseyes,\nmouth, and full faceusing two different deep learning models (a CNN-based model\nand a transformer-based model) trained on an SLR dataset of isolated signs with\nrandomly selected classes. Through quantitative performance and qualitative\nsaliency map evaluation, we reveal that the mouth is the most important\nnon-manual facial feature, significantly improving accuracy. Our findings\nhighlight the necessity of incorporating facial features in ASLR."}
{"id": "2507.21391", "pdf": "https://arxiv.org/pdf/2507.21391.pdf", "abs": "https://arxiv.org/abs/2507.21391", "title": "Multimodal LLMs as Customized Reward Models for Text-to-Image Generation", "authors": ["Shijie Zhou", "Ruiyi Zhang", "Huaisheng Zhu", "Branislav Kveton", "Yufan Zhou", "Jiuxiang Gu", "Jian Chen", "Changyou Chen"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted at ICCV 2025. Code available at\n  https://github.com/sjz5202/LLaVA-Reward", "summary": "We introduce LLaVA-Reward, an efficient reward model designed to\nautomatically evaluate text-to-image (T2I) generations across multiple\nperspectives, leveraging pretrained multimodal large language models (MLLMs).\nExisting MLLM-based approaches require instruction-following data for\nsupervised fine-tuning and evaluate generation quality on analyzing text\nresponse, which is time-consuming and difficult to train. To address this\nproblem, we propose LLaVA-Reward, which directly utilizes the hidden states of\nMLLMs given text-image pairs. To enhance the bidirectional interaction between\nvisual and textual representations in decoder-only MLLMs, we further propose\nadding a Skip-connection Cross Attention (SkipCA) module. This design enhances\ntext-image correlation reasoning by connecting early-layer visual features with\nlater-layer hidden representations. In addition, LLaVA-Reward supports\ndifferent types of preference data for efficient fine-tuning, including paired\npreference data and unpaired data. We train LLaVA-Reward on four evaluation\nperspectives: text-image alignment, fidelity/artifact, safety, and overall\nranking. Empirical results demonstrate that LLaVA-Reward outperforms\nconventional and MLLM-based methods in generating human-aligned scores for\nautomatic evaluations and inference-time scaling in text-to-image generations."}
{"id": "2507.22025", "pdf": "https://arxiv.org/pdf/2507.22025.pdf", "abs": "https://arxiv.org/abs/2507.22025", "title": "UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding", "authors": ["Shuquan Lian", "Yuhang Wu", "Jia Ma", "Zihan Song", "Bingqi Chen", "Xiawu Zheng", "Hui Li"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "The emergence of Multimodal Large Language Models (MLLMs) has driven\nsignificant advances in Graphical User Interface (GUI) agent capabilities.\nNevertheless, existing GUI agent training and inference techniques still suffer\nfrom a dilemma for reasoning designs, ineffective reward, and visual noise. To\naddress these issues, we introduce UI-AGILE, a comprehensive framework\nenhancing GUI agents at both the training and inference stages. For training,\nwe propose a suite of improvements to the Supervised Fine-Tuning (SFT) process:\n1) a Continuous Reward function to incentivize high-precision grounding; 2) a\n\"Simple Thinking\" reward to balance planning with speed and grounding accuracy;\nand 3) a Cropping-based Resampling strategy to mitigate the sparse reward\nproblem and improve learning on complex tasks. For inference, we present\nDecomposed Grounding with Selection, a novel method that dramatically improves\ngrounding accuracy on high-resolution displays by breaking the image into\nsmaller, manageable parts. Experiments show that UI-AGILE achieves the\nstate-of-the-art performance on two benchmarks ScreenSpot-Pro and\nScreenSpot-v2. For instance, using both our proposed training and inference\nenhancement methods brings 23% grounding accuracy improvement over the best\nbaseline on ScreenSpot-Pro."}
