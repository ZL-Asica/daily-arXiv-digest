{"id": "2505.00821", "pdf": "https://arxiv.org/pdf/2505.00821.pdf", "abs": "https://arxiv.org/abs/2505.00821", "title": "Should AI Mimic People? Understanding AI-Supported Writing Technology Among Black Users", "authors": ["Jeffrey Basoah", "Jay L. Cunningham", "Erica Adams", "Alisha Bose", "Aditi Jain", "Kaustubh Yadav", "Zhengyang Yang", "Katharina Reinecke", "Daniela Rosner"], "categories": ["cs.HC"], "comment": null, "summary": "AI-supported writing technologies (AISWT) that provide grammatical\nsuggestions, autocomplete sentences, or generate and rewrite text are now a\nregular feature integrated into many people's workflows. However, little is\nknown about how people perceive the suggestions these tools provide. In this\npaper, we investigate how Black American users perceive AISWT, motivated by\nprior findings in natural language processing that highlight how the underlying\nlarge language models can contain racial biases. Using interviews and\nobservational user studies with 13 Black American users of AISWT, we found a\nstrong tradeoff between the perceived benefits of using AISWT to enhance their\nwriting style and feeling like \"it wasn't built for us\". Specifically,\nparticipants reported AISWT's failure to recognize commonly used names and\nexpressions in African American Vernacular English, experiencing its\ncorrections as hurtful and alienating and fearing it might further minoritize\ntheir culture. We end with a reflection on the tension between AISWT that fail\nto include Black American culture and language, and AISWT that attempt to mimic\nit, with attention to accuracy, authenticity, and the production of social\ndifference."}
{"id": "2505.00855", "pdf": "https://arxiv.org/pdf/2505.00855.pdf", "abs": "https://arxiv.org/abs/2505.00855", "title": "Beyond the Mirror: Personal Analytics through Visual Juxtaposition with Other People's Data", "authors": ["Sungbok Shin", "Sunghyo Chung", "Hyeon Jeon", "Hyunwook Lee", "Minje Choi", "Taehun Kim", "Jaehoon Choi", "Sungahn Ko", "Jaegul Choo"], "categories": ["cs.HC"], "comment": "Submitted to IEEE VIS2025 Short Paper", "summary": "An individual's data can reveal facets of behavior and identity, but its\ninterpretation is context dependent. We can easily identify various\nself-tracking applications that help people reflect on their lives. However,\nself-tracking confined to one person's data source may fall short in terms of\nobjectiveness, and insights coming from various perspectives. To address this,\nwe examine how those interpretations about a person's data can be augmented\nwhen the data are juxtaposed with that of others using anonymized online\ncalendar logs from a schedule management app. We develop CALTREND, a visual\nanalytics system that compares an individuals anonymized online schedule logs\nwith using those from other people. Using CALTREND as a probe, we conduct a\nstudy with two domain experts, one in information technology and one in Korean\nherbal medicine. We report our observations on how comparative views help\nenrich the characterization of an individual based on the experts' comments. We\nfind that juxtaposing personal data with others' can potentially lead to\ndiverse interpretations of one dataset shaped by domain-specific mental models."}
{"id": "2505.00879", "pdf": "https://arxiv.org/pdf/2505.00879.pdf", "abs": "https://arxiv.org/abs/2505.00879", "title": "Inattentional Blindness with Augmented Reality HUDS: An On-road Study", "authors": ["Nayara de Oliveira Faria", "Joseph L. Gabbard"], "categories": ["cs.HC"], "comment": null, "summary": "As the integration of augmented reality (AR) technology in head-up displays\n(HUDs) becomes more prevalent in vehicles, it is crucial to understand how to\ndesign and evaluate AR interfaces to ensure safety. With new AR displays\ncapable of rendering images with larger field of views and at varying depths,\nthe visual and cognitive separation between graphical and real-world visual\nstimuli will be increasingly more difficult to quantify as will drivers'\nability to efficiently allocate visual attention between the two sets of\nstimuli. In this study, we present a user study that serves as a crucial first\nstep in gaining insight into inattentional blindness while using AR in surface\ntransportation, where understanding is currently limited. Our primary goal is\nto investigate how the visual demand of AR tasks influences drivers' ability to\ndetect stimuli, and whether the nature of the stimuli itself plays a role in\nthis effect. To address these questions, we designed an on-road user study\naimed at producing a more realistic and ecologically valid understanding of the\nphenomenon.\n  Our results show that drivers' ability to timely detect stimuli in the\nenvironment decreased as the AR task visual demand increased demonstrated by\nboth detection performance and inattentional blindness metrics. Further,\ninattentional blindness caused by AR displays appears to be more prevalent\nwithin drivers' central field of view. We conclude by discussing implications\ntowards a safety-centric evaluation framework for AR HUDs."}
{"id": "2505.00907", "pdf": "https://arxiv.org/pdf/2505.00907.pdf", "abs": "https://arxiv.org/abs/2505.00907", "title": "Co-Designing a Knowledge Graph Navigation Interface: A Participatory Approach", "authors": ["Stanislava Gardasevic", "Manika Lamba", "Jasmine S. Malone"], "categories": ["cs.HC", "cs.DL"], "comment": null, "summary": "Navigating and visualizing multilayered knowledge graphs remains a\nchallenging, unresolved problem in information systems design. Building on our\nearlier study, which engaged end users in both the design and population of a\ndomain-specific knowledge graph, we now focus on translating their insights\ninto actionable interface guidelines. In this paper, we synthesize\nrecommendations drawn from a participatory workshop with doctoral students. We\nthen demonstrate how these recommendations inform the design of a prototype\ninterface. Finally, we found that a participatory iterative design approach can\nhelp designers in decision making, leading to interfaces that are both\ninnovative and user-centric. By combining user-driven requirements with proven\nvisualization techniques, this paper presents a coherent framework for guiding\nfuture development of knowledge-graph navigation tools."}
{"id": "2505.00725", "pdf": "https://arxiv.org/pdf/2505.00725.pdf", "abs": "https://arxiv.org/abs/2505.00725", "title": "FinBERT-QA: Financial Question Answering with pre-trained BERT Language Models", "authors": ["Bithiah Yuan"], "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.7; I.5.1; H.3.3"], "comment": "Submitted in partial fulfillment of the requirements for the Master\n  of Science degree in Computer Science at the University of Freiburg, July 31,\n  2020", "summary": "Motivated by the emerging demand in the financial industry for the automatic\nanalysis of unstructured and structured data at scale, Question Answering (QA)\nsystems can provide lucrative and competitive advantages to companies by\nfacilitating the decision making of financial advisers. Consequently, we\npropose a novel financial QA system using the transformer-based pre-trained\nBERT language model to address the limitations of data scarcity and language\nspecificity in the financial domain. Our system focuses on financial\nnon-factoid answer selection, which retrieves a set of passage-level texts and\nselects the most relevant as the answer. To increase efficiency, we formulate\nthe answer selection task as a re-ranking problem, in which our system consists\nof an Answer Retriever using BM25, a simple information retrieval approach, to\nfirst return a list of candidate answers, and an Answer Re-ranker built with\nvariants of pre-trained BERT language models to re-rank and select the most\nrelevant answers. We investigate various learning, further pre-training, and\nfine-tuning approaches for BERT. Our experiments suggest that FinBERT-QA, a\nmodel built from applying the Transfer and Adapt further fine-tuning and\npointwise learning approach, is the most effective, improving the\nstate-of-the-art results of task 2 of the FiQA dataset by 16% on MRR, 17% on\nNDCG, and 21% on Precision@1."}
{"id": "2505.00945", "pdf": "https://arxiv.org/pdf/2505.00945.pdf", "abs": "https://arxiv.org/abs/2505.00945", "title": "SSRLBot: Designing and Developing an LLM-based Agent using Socially Shared Regulated Learning", "authors": ["Xiaoshan Huang", "Jie Gao", "Haolun Wu"], "categories": ["cs.HC"], "comment": "8 pages, 2 figures", "summary": "Large language model (LLM)-based agents are increasingly used to support\nhuman experts by streamlining complex tasks and offering actionable insights.\nHowever, their application in multi-professional decision-making, particularly\nin teamwork contexts, remains underexplored. This design-based study addresses\nthat gap by developing LLM functions to enhance collaboration, grounded in the\nSocially Shared Regulation of Learning (SSRL) framework and applied to medical\ndiagnostic teamwork. SSRL emphasizes metacognitive, cognitive, motivational,\nand emotional processes in shared learning, focusing on how teams manage these\nprocesses to improve decision-making. This paper introduces SSRLBot, a\nprototype chatbot designed to help team members reflect on both their\ndiagnostic performance and key SSRL skills. Its core functions include\nsummarizing dialogues, analyzing SSRL behaviors, evaluating diagnostic\noutcomes, annotating SSRL markers in conversation, assessing their impact on\nperformance, and identifying interpersonal regulatory dynamics. We compare\nSSRLBot's capabilities with those of Gemini-1.5, GPT-3.5, and Deepseek-R1 in a\ncase study. SSRLBot demonstrates stronger alignment with SSRL theory, offering\ndetailed evaluations that link behaviors to regulatory dimensions and\nsuggesting improvements for collaboration. By integrating SSRL theory with LLM\ncapabilities, SSRLBot contributes a novel tool for enhancing team-based\ndecision-making and collaborative learning in high-stakes environments, such as\nmedical education."}
{"id": "2505.00753", "pdf": "https://arxiv.org/pdf/2505.00753.pdf", "abs": "https://arxiv.org/abs/2505.00753", "title": "A Survey on Large Language Model based Human-Agent Systems", "authors": ["Henry Peng Zou", "Wei-Chieh Huang", "Yaozu Wu", "Yankai Chen", "Chunyu Miao", "Hoang Nguyen", "Yue Zhou", "Weizhi Zhang", "Liancheng Fang", "Langzhou He", "Yangning Li", "Yuwei Cao", "Dongyuan Li", "Renhe Jiang", "Philip S. Yu"], "categories": ["cs.CL", "cs.LG"], "comment": "Paper lists and resources are available at\n  \\url{https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers}", "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin building fully autonomous agents. However, fully autonomous LLM-based agents\nstill face significant challenges, including limited reliability due to\nhallucinations, difficulty in handling complex tasks, and substantial safety\nand ethical risks, all of which limit their feasibility and trustworthiness in\nreal-world applications. To overcome these limitations, LLM-based human-agent\nsystems (LLM-HAS) incorporate human-provided information, feedback, or control\ninto the agent system to enhance system performance, reliability and safety.\nThis paper provides the first comprehensive and structured survey of LLM-HAS.\nIt clarifies fundamental concepts, systematically presents core components\nshaping these systems, including environment & profiling, human feedback,\ninteraction types, orchestration and communication, explores emerging\napplications, and discusses unique challenges and opportunities. By\nconsolidating current knowledge and offering a structured overview, we aim to\nfoster further research and innovation in this rapidly evolving\ninterdisciplinary field. Paper lists and resources are available at\nhttps://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers."}
{"id": "2505.00948", "pdf": "https://arxiv.org/pdf/2505.00948.pdf", "abs": "https://arxiv.org/abs/2505.00948", "title": "What Makes Teamwork Work? A Multimodal Case Study on Emotions and Diagnostic Expertise in an Intelligent Tutoring System", "authors": ["Xiaoshan Huang", "Haolun Wu", "Xue Liu", "Susanne P. Lajoie"], "categories": ["cs.HC"], "comment": "8 pages, 1 figure", "summary": "Teamwork is pivotal in medical teamwork when professionals with diverse\nskills and emotional states collaborate to make critical decisions. This case\nstudy examines the interplay between emotions and professional skills in group\ndecision-making during collaborative medical diagnosis within an Intelligent\nTutoring System (ITS). By comparing verbal and physiological data between\nhigh-performing and low-performing teams of medical professionals working on a\npatient case within the ITS, alongside individuals' retrospective collaboration\nexperiences, we employ multimodal data analysis to identify patterns in team\nemotional climate and their impact on diagnostic efficiency. Specifically, we\ninvestigate how emotion-driven dialogue and professional expertise influence\nboth the information-seeking process and the final diagnostic decisions.\nGrounded in the socially shared regulation of learning framework and utilizing\nsentiment analysis, we found that social-motivational interactions are key\ndrivers of a positive team emotional climate. Furthermore, through content\nanalysis of dialogue and physiological signals to pinpoint emotional\nfluctuations, we identify episodes where knowledge exchange and skill\nacquisition are most likely to occur. Our findings offer valuable insights into\noptimizing group collaboration in medical contexts by harmonizing emotional\ndynamics with adaptive strategies for effective decision-making, ultimately\nenhancing diagnostic accuracy and teamwork effectiveness."}
{"id": "2505.00776", "pdf": "https://arxiv.org/pdf/2505.00776.pdf", "abs": "https://arxiv.org/abs/2505.00776", "title": "Reasoning Capabilities and Invariability of Large Language Models", "authors": ["Alessandro Raganato", "Rafael Peñaloza", "Marco Viviani", "Gabriella Pasi"], "categories": ["cs.CL"], "comment": "Accepted for publication in the Proceedings of the 23rd IEEE/WIC\n  International Conference on Web Intelligence and Intelligent Agent Technology\n  (WI-IAT 2024)", "summary": "Large Language Models (LLMs) have shown remarkable capabilities in\nmanipulating natural language across multiple applications, but their ability\nto handle simple reasoning tasks is often questioned. In this work, we aim to\nprovide a comprehensive analysis of LLMs' reasoning competence, specifically\nfocusing on their prompt dependency. In particular, we introduce a new\nbenchmark dataset with a series of simple reasoning questions demanding shallow\nlogical reasoning. Aligned with cognitive psychology standards, the questions\nare confined to a basic domain revolving around geometric figures, ensuring\nthat responses are independent of any pre-existing intuition about the world\nand rely solely on deduction. An empirical analysis involving zero-shot and\nfew-shot prompting across 24 LLMs of different sizes reveals that, while LLMs\nwith over 70 billion parameters perform better in the zero-shot setting, there\nis still a large room for improvement. An additional test with chain-of-thought\nprompting over 22 LLMs shows that this additional prompt can aid or damage the\nperformance of models, depending on whether the rationale is required before or\nafter the answer."}
{"id": "2505.00956", "pdf": "https://arxiv.org/pdf/2505.00956.pdf", "abs": "https://arxiv.org/abs/2505.00956", "title": "Audio Personas: Augmenting Social Perception via Body-Anchored Audio Cues", "authors": ["Yujie Tao", "Libby Ye", "Jeremy N. Bailenson", "Sean Follmer"], "categories": ["cs.HC"], "comment": null, "summary": "We introduce Audio Personas, enabling users to \"decorate\" themselves with\nbody-anchored sounds in audio augmented reality. Like outfits, makeup, and\nfragrances, audio personas offer an alternative yet dynamic channel to augment\nface-to-face interactions. For instance, one can set their audio persona as\nrain sounds to reflect a bad mood, bee sounds to establish personal boundaries,\nor a playful \"woosh\" sound to mimic passing by someone like a breeze. To\ninstantiate the concept, we implemented a headphone-based prototype with\nmulti-user tracking and audio streaming. Our formative study with designers\nrevealed that audio personas were preferred in public and semi-public-private\nspaces for managing social impressions (e.g., personality) and signaling\ncurrent states (e.g., emotions). Our preregistered in-lab study with 64\nparticipants showed that audio personas influenced how participants formed\nimpressions. Individuals with positive audio personas were rated as more\nsocially attractive, more likable, and less threatening than those with\nnegative audio personas."}
{"id": "2505.00814", "pdf": "https://arxiv.org/pdf/2505.00814.pdf", "abs": "https://arxiv.org/abs/2505.00814", "title": "Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction", "authors": ["Mario Sänger", "Ulf Leser"], "categories": ["cs.CL"], "comment": null, "summary": "Automatic relationship extraction (RE) from biomedical literature is critical\nfor managing the vast amount of scientific knowledge produced each year. In\nrecent years, utilizing pre-trained language models (PLMs) has become the\nprevalent approach in RE. Several studies report improved performance when\nincorporating additional context information while fine-tuning PLMs for RE.\nHowever, variations in the PLMs applied, the databases used for augmentation,\nhyper-parameter optimization, and evaluation methods complicate direct\ncomparisons between studies and raise questions about the generalizability of\nthese findings. Our study addresses this research gap by evaluating PLMs\nenhanced with contextual information on five datasets spanning four relation\nscenarios within a consistent evaluation framework. We evaluate three baseline\nPLMs and first conduct extensive hyperparameter optimization. After selecting\nthe top-performing model, we enhance it with additional data, including textual\nentity descriptions, relational information from knowledge graphs, and\nmolecular structure encodings. Our findings illustrate the importance of i) the\nchoice of the underlying language model and ii) a comprehensive hyperparameter\noptimization for achieving strong extraction performance. Although inclusion of\ncontext information yield only minor overall improvements, an ablation study\nreveals substantial benefits for smaller PLMs when such external data was\nincluded during fine-tuning."}
{"id": "2505.00987", "pdf": "https://arxiv.org/pdf/2505.00987.pdf", "abs": "https://arxiv.org/abs/2505.00987", "title": "Destructive Interference: Encoding Loss in the Overlap", "authors": ["Nik Aberle"], "categories": ["cs.HC"], "comment": null, "summary": "Destructive Interference is a data visualization installation that\nrepresenting the deaths and injuries caused by mass shootings in 2024 in the\nUnited States. I parametrically designed and fabricated an interlocking ring\nsculpture for each month of 2024; where the overall height corresponds to the\nlevel of violence in that month. Taller forms mark the deadliest months, while\nshorter ones reflect fewer casualties. Each inner ring encodes the number of\npeople killed or injured, and each outer ring encodes the number of shootings\nand the number of days without them. The interlocking cylinders are powered via\na motor to rotate, and lit from within. As the cylinders rotate, they cast\noverlapping shadows that represent those killed or injured by mass shootings.\nThe goal of this work is to visualize otherwise overwhelming and disparate\nstatistics in a way that is both physically present and emotionally resonant.\nBy inviting viewers to step into and engage with these shadows, the piece\ncreates space for reflection, conversation, and confrontation with the scale of\nthis ongoing crisis."}
{"id": "2505.00931", "pdf": "https://arxiv.org/pdf/2505.00931.pdf", "abs": "https://arxiv.org/abs/2505.00931", "title": "Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy in English Language Learner Writing", "authors": ["Timur Jaganov", "John Blake", "Julián Villegas", "Nicholas Carr"], "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 8 Figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "This study investigates the potential for Large Language Models (LLMs) to\nscale-up Dynamic Assessment (DA). To facilitate such an investigation, we first\ndeveloped DynaWrite-a modular, microservices-based grammatical tutoring\napplication which supports multiple LLMs to generate dynamic feedback to\nlearners of English. Initial testing of 21 LLMs, revealed GPT-4o and neural\nchat to have the most potential to scale-up DA in the language learning\nclassroom. Further testing of these two candidates found both models performed\nsimilarly in their ability to accurately identify grammatical errors in user\nsentences. However, GPT-4o consistently outperformed neural chat in the quality\nof its DA by generating clear, consistent, and progressively explicit hints.\nReal-time responsiveness and system stability were also confirmed through\ndetailed performance testing, with GPT-4o exhibiting sufficient speed and\nstability. This study shows that LLMs can be used to scale-up dynamic\nassessment and thus enable dynamic assessment to be delivered to larger groups\nthan possible in traditional teacher-learner settings."}
{"id": "2505.01000", "pdf": "https://arxiv.org/pdf/2505.01000.pdf", "abs": "https://arxiv.org/abs/2505.01000", "title": "Togedule: Scheduling Meetings with Large Language Models and Adaptive Representations of Group Availability", "authors": ["Jaeyoon Song", "Zahra Ashktorab", "Thomas W. Malone"], "categories": ["cs.HC"], "comment": "This paper has been accepted at CSCW 2025", "summary": "Scheduling is a perennial-and often challenging-problem for many groups.\nExisting tools are mostly static, showing an identical set of choices to\neveryone, regardless of the current status of attendees' inputs and\npreferences. In this paper, we propose Togedule, an adaptive scheduling tool\nthat uses large language models to dynamically adjust the pool of choices and\ntheir presentation format. With the initial prototype, we conducted a formative\nstudy (N=10) and identified the potential benefits and risks of such an\nadaptive scheduling tool. Then, after enhancing the system, we conducted two\ncontrolled experiments, one each for attendees and organizers (total N=66). For\neach experiment, we compared scheduling with verbal messages, shared calendars,\nor Togedule. Results show that Togedule significantly reduces the cognitive\nload of attendees indicating their availability and improves the speed and\nquality of the decisions made by organizers."}
{"id": "2505.00949", "pdf": "https://arxiv.org/pdf/2505.00949.pdf", "abs": "https://arxiv.org/abs/2505.00949", "title": "Llama-Nemotron: Efficient Reasoning Models", "authors": ["Akhiad Bercovich", "Itay Levy", "Izik Golan", "Mohammad Dabbah", "Ran El-Yaniv", "Omri Puny", "Ido Galil", "Zach Moshe", "Tomer Ronen", "Najeeb Nabwani", "Ido Shahaf", "Oren Tropp", "Ehud Karpas", "Ran Zilberstein", "Jiaqi Zeng", "Soumye Singhal", "Alexander Bukharin", "Yian Zhang", "Tugrul Konuk", "Gerald Shen", "Ameya Sunil Mahabaleshwarkar", "Bilal Kartal", "Yoshi Suhara", "Olivier Delalleau", "Zijia Chen", "Zhilin Wang", "David Mosallanezhad", "Adi Renduchintala", "Haifeng Qian", "Dima Rekesh", "Fei Jia", "Somshubra Majumdar", "Vahid Noroozi", "Wasi Uddin Ahmad", "Sean Narenthiran", "Aleksander Ficek", "Mehrzad Samadi", "Jocelyn Huang", "Siddhartha Jain", "Igor Gitman", "Ivan Moshkov", "Wei Du", "Shubham Toshniwal", "George Armstrong", "Branislav Kisacanin", "Matvei Novikov", "Daria Gitman", "Evelina Bakhturina", "Jane Polak Scowcroft", "John Kamalu", "Dan Su", "Kezhi Kong", "Markus Kliegl", "Rabeeh Karimi", "Ying Lin", "Sanjeev Satheesh", "Jupinder Parmar", "Pritam Gundecha", "Brandon Norick", "Joseph Jennings", "Shrimai Prabhumoye", "Syeda Nahida Akter", "Mostofa Patwary", "Abhinav Khattar", "Deepak Narayanan", "Roger Waleffe", "Jimmy Zhang", "Bor-Yiing Su", "Guyue Huang", "Terry Kong", "Parth Chadha", "Sahil Jain", "Christine Harvey", "Elad Segal", "Jining Huang", "Sergey Kashirsky", "Robert McQueen", "Izzy Putterman", "George Lam", "Arun Venkatesan", "Sherry Wu", "Vinh Nguyen", "Manoj Kilaru", "Andrew Wang", "Anna Warno", "Abhilash Somasamudramath", "Sandip Bhaskar", "Maka Dong", "Nave Assaf", "Shahar Mor", "Omer Ullman Argov", "Scot Junkin", "Oleksandr Romanenko", "Pedro Larroy", "Monika Katariya", "Marco Rovinelli", "Viji Balas", "Nicholas Edelman", "Anahita Bhiwandiwalla", "Muthu Subramaniam", "Smita Ithape", "Karthik Ramamoorthy", "Yuting Wu", "Suguna Varshini Velury", "Omri Almog", "Joyjit Daw", "Denys Fridman", "Erick Galinkin", "Michael Evans", "Katherine Luna", "Leon Derczynski", "Nikki Pope", "Eileen Long", "Seth Schneider", "Guillermo Siman", "Tomasz Grzegorzek", "Pablo Ribalta", "Monika Katariya", "Joey Conway", "Trisha Saar", "Ann Guan", "Krzysztof Pawelec", "Shyamala Prayaga", "Oleksii Kuchaiev", "Boris Ginsburg", "Oluwatobi Olabiyi", "Kari Briski", "Jonathan Cohen", "Bryan Catanzaro", "Jonah Alben", "Yonatan Geifman", "Eric Chung"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce the Llama-Nemotron series of models, an open family of\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\ninference efficiency, and an open license for enterprise use. The family comes\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\noffering superior inference throughput and memory efficiency. In this report,\nwe discuss the training procedure for these models, which entails using neural\narchitecture search from Llama 3 models for accelerated inference, knowledge\ndistillation, and continued pretraining, followed by a reasoning-focused\npost-training stage consisting of two main parts: supervised fine-tuning and\nlarge scale reinforcement learning. Llama-Nemotron models are the first\nopen-source models to support a dynamic reasoning toggle, allowing users to\nswitch between standard chat and reasoning modes during inference. To further\nsupport open research and facilitate model development, we provide the\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\nOpen Model License Agreement. 2. We release the complete post-training dataset:\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM."}
{"id": "2505.01030", "pdf": "https://arxiv.org/pdf/2505.01030.pdf", "abs": "https://arxiv.org/abs/2505.01030", "title": "Barriers to Employment: The Deaf Multimedia Authoring Tax", "authors": ["C. Vogler", "A. Glasser", "R. Kushalnagar", "M. Seita", "M. Arroyo Chavez", "K. Delk", "P. DeVries", "M. Feanny", "B. Thompson", "J. Waller"], "categories": ["cs.HC"], "comment": "5 pages", "summary": "This paper describes the challenges that deaf and hard of hearing people face\nwith creating accessible multimedia content, such as portfolios, instructional\nvideos and video presentations. Unlike content consumption, the process of\ncontent creation itself remains highly inaccessible, creating barriers to\nemployment in all stages of recruiting, hiring, and carrying out assigned job\nduties. Overcoming these barriers incurs a \"deaf content creation tax\" that\ntranslates into requiring significant additional time and resources to produce\ncontent equivalent to what a non-disabled person would produce. We highlight\nthis process and associated challenges through real-world examples experienced\nby the authors, and provide guidance and recommendations for addressing them."}
{"id": "2505.00977", "pdf": "https://arxiv.org/pdf/2505.00977.pdf", "abs": "https://arxiv.org/abs/2505.00977", "title": "A Character-based Diffusion Embedding Algorithm for Enhancing the Generation Quality of Generative Linguistic Steganographic Texts", "authors": ["Yingquan Chen", "Qianmu Li", "Xiaocong Wu", "Huifeng Li", "Qing Chang"], "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Generating high-quality steganographic text is a fundamental challenge in the\nfield of generative linguistic steganography. This challenge arises primarily\nfrom two aspects: firstly, the capabilities of existing models in text\ngeneration are limited; secondly, embedding algorithms fail to effectively\nmitigate the negative impacts of sensitive information's properties, such as\nsemantic content or randomness. Specifically, to ensure that the recipient can\naccurately extract hidden information, embedding algorithms often have to\nconsider selecting candidate words with relatively low probabilities. This\nphenomenon leads to a decrease in the number of high-probability candidate\nwords and an increase in low-probability candidate words, thereby compromising\nthe semantic coherence and logical fluency of the steganographic text and\ndiminishing the overall quality of the generated steganographic material. To\naddress this issue, this paper proposes a novel embedding algorithm,\ncharacter-based diffusion embedding algorithm (CDEA). Unlike existing embedding\nalgorithms that strive to eliminate the impact of sensitive information's\nproperties on the generation process, CDEA leverages sensitive information's\nproperties. It enhances the selection frequency of high-probability candidate\nwords in the candidate pool based on general statistical properties at the\ncharacter level and grouping methods based on power-law distributions, while\nreducing the selection frequency of low-probability candidate words in the\ncandidate pool. Furthermore, to ensure the effective transformation of\nsensitive information in long sequences, we also introduce the XLNet model.\nExperimental results demonstrate that the combination of CDEA and XLNet\nsignificantly improves the quality of generated steganographic text,\nparticularly in terms of perceptual-imperceptibility."}
{"id": "2505.01351", "pdf": "https://arxiv.org/pdf/2505.01351.pdf", "abs": "https://arxiv.org/abs/2505.01351", "title": "Closing the Loop: A Systematic Review of Experience-Driven Game Adaptation", "authors": ["Phil Lopes", "Nuno Fachada", "Maria Fonseca"], "categories": ["cs.HC"], "comment": null, "summary": "Adaptive game systems aim to enrich player experiences by dynamically\nadjusting game content in response to user data. While extensive research has\naddressed content personalization and player experience modeling, the\nintegration of these components into fully operational adaptive gameplay\nsystems remains limited. This systematic review, conducted in accordance with\nPRISMA guidelines, analyzes 17 empirical studies published between January 2015\nand May 2024, identifying and analyzing approaches that implement the complete\nexperience-driven loop -- including player sensing, modeling, and content\nadaptation. Game telemetry remains the most prevalent sensing modality,\nalthough other non-invasive methods suitable for affective modeling -- such as\nfacial expression analysis (FEA) and peripheral interaction data -- remain\nunderutilized despite their potential for real-time emotional inference.\nKnowledge-based methods, such as rule-based systems and heuristics, dominate\nmodeling and adaptation due to their interpretability and low resource demands,\nwhereas machine learning approaches face challenges related to data\navailability and transparency. Despite their relevance to immersive and\ntherapeutic experiences, affective states such as stress and anxiety remain\nlargely ignored, as systems continue to favor performance over\nemotion-sensitive adaptation. These findings highlight a crucial research\ndirection: advancing emotionally responsive game systems that move beyond\nperformance optimization by incorporating underutilized sensing modalities --\nsuch as FEA and peripheral interaction -- to enable real-time affect-driven\npersonalization. Advancing in this direction holds strong potential to increase\nimmersion, personalize gameplay, and support affect regulation across\nentertainment and therapeutic contexts."}
{"id": "2505.00979", "pdf": "https://arxiv.org/pdf/2505.00979.pdf", "abs": "https://arxiv.org/abs/2505.00979", "title": "Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models", "authors": ["Xuhui Jiang", "Shengjie Ma", "Chengjin Xu", "Cehao Yang", "Liyu Zhang", "Jian Guo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success but remain\ndata-inefficient, especially when learning from small, specialized corpora with\nlimited and proprietary data. Existing synthetic data generation methods for\ncontinue pre-training focus on intra-document content and overlook\ncross-document knowledge associations, limiting content diversity and depth. We\npropose Synthetic-on-Graph (SoG), a synthetic data generation framework that\nincorporates cross-document knowledge associations for efficient corpus\nexpansion. SoG constructs a context graph by extracting entities and concepts\nfrom the original corpus, representing cross-document associations, and\nemploying a graph walk strategy for knowledge-associated sampling. This\nenhances synthetic data diversity and coherence, enabling models to learn\ncomplex knowledge structures and handle rare knowledge. To further improve\nsynthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive\nClarifying (CC) synthetic, enhancing reasoning processes and discriminative\npower. Experiments show that SoG outperforms the state-of-the-art (SOTA) method\nin a multi-hop document Q&A dataset while performing comparably to the SOTA\nmethod on the reading comprehension task datasets, which also underscores the\nbetter generalization capability of SoG. Our work advances synthetic data\ngeneration and provides practical solutions for efficient knowledge acquisition\nin LLMs, especially in domains with limited data availability."}
{"id": "2505.01413", "pdf": "https://arxiv.org/pdf/2505.01413.pdf", "abs": "https://arxiv.org/abs/2505.01413", "title": "Group Gaze-Sharing with Projection Displays", "authors": ["Maurice Koch", "Tobias Rau", "Vladimir Mikheev", "Seyda Öney", "Michael Becher", "Xiangyu Wang", "Nelusa Pathmanathan", "Patrick Gralka", "Daniel Weiskopf", "Kuno Kurzhals"], "categories": ["cs.HC"], "comment": "2025 Symposium on Eye Tracking Research and Applications (ETRA '25)", "summary": "The eyes play an important role in human collaboration. Mutual and shared\ngaze help communicate visual attention to each other or to a specific object of\ninterest. Shared gaze was typically investigated for pair collaborations in\nremote settings and with people in virtual and augmented reality. With our\nwork, we expand this line of research by a new technique to communicate gaze\nbetween groups in tabletop workshop scenarios. To achieve this communication,\nwe use an approach based on projection mapping to unify gaze data from multiple\nparticipants into a common visualization space on a tabletop. We showcase our\napproach with a collaborative puzzle-solving task that displays shared visual\nattention on individual pieces and provides hints to solve the problem at hand."}
{"id": "2505.00985", "pdf": "https://arxiv.org/pdf/2505.00985.pdf", "abs": "https://arxiv.org/abs/2505.00985", "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling", "authors": ["Ayan Sengupta", "Yash Goel", "Tanmoy Chakraborty"], "categories": ["cs.CL"], "comment": null, "summary": "We challenge the dominant focus on neural scaling laws and advocate for a\nparadigm shift toward downscaling in the development of large language models\n(LLMs). While scaling laws have provided critical insights into performance\nimprovements through increasing model and dataset size, we emphasize the\nsignificant limitations of this approach, particularly in terms of\ncomputational inefficiency, environmental impact, and deployment constraints.\nTo address these challenges, we propose a holistic framework for downscaling\nLLMs that seeks to maintain performance while drastically reducing resource\ndemands. This paper outlines practical strategies for transitioning away from\ntraditional scaling paradigms, advocating for a more sustainable, efficient,\nand accessible approach to LLM development."}
{"id": "2505.01001", "pdf": "https://arxiv.org/pdf/2505.01001.pdf", "abs": "https://arxiv.org/abs/2505.01001", "title": "Photoshop Batch Rendering Using Actions for Stylistic Video Editing", "authors": ["Tessa De La Fuente"], "categories": ["cs.MM", "cs.GR", "cs.HC"], "comment": "11 pages, 12 figures", "summary": "My project looks at an efficient workflow for creative image/video editing\nusing Adobe Photoshop Actions tool and Batch Processing System. This innovative\napproach to video editing through Photoshop creates a fundamental shift to\ncreative workflow management through the integration of industry-leading image\nmanipulation with video editing techniques. Through systematic automation of\nActions, users can achieve a simple and consistent application of visual edits\nacross a string of images. This approach provides an alternative method to\noptimize productivity while ensuring uniform results across image collections\nthrough a post-processing pipeline."}
{"id": "2505.00989", "pdf": "https://arxiv.org/pdf/2505.00989.pdf", "abs": "https://arxiv.org/abs/2505.00989", "title": "VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel Traffic Services through Natural Language", "authors": ["Sijin Sun", "Liangbin Zhao", "Ming Deng", "Xiuju Fu"], "categories": ["cs.CL"], "comment": "8 pages, 5 figures, 7 tablels, submitted to ITSC2025", "summary": "Vessel Traffic Services (VTS) are essential for maritime safety and\nregulatory compliance through real-time traffic management. However, with\nincreasing traffic complexity and the prevalence of heterogeneous, multimodal\ndata, existing VTS systems face limitations in spatiotemporal reasoning and\nintuitive human interaction. In this work, we propose VTS-LLM Agent, the first\ndomain-adaptive large LLM agent tailored for interactive decision support in\nVTS operations. We formalize risk-prone vessel identification as a\nknowledge-augmented Text-to-SQL task, combining structured vessel databases\nwith external maritime knowledge. To support this, we construct a curated\nbenchmark dataset consisting of a custom schema, domain-specific corpus, and a\nquery-SQL test set in multiple linguistic styles. Our framework incorporates\nNER-based relational reasoning, agent-based domain knowledge injection,\nsemantic algebra intermediate representation, and query rethink mechanisms to\nenhance domain grounding and context-aware understanding. Experimental results\nshow that VTS-LLM outperforms both general-purpose and SQL-focused baselines\nunder command-style, operational-style, and formal natural language queries,\nrespectively. Moreover, our analysis provides the first empirical evidence that\nlinguistic style variation introduces systematic performance challenges in\nText-to-SQL modeling. This work lays the foundation for natural language\ninterfaces in vessel traffic services and opens new opportunities for\nproactive, LLM-driven maritime real-time traffic management."}
{"id": "2505.01192", "pdf": "https://arxiv.org/pdf/2505.01192.pdf", "abs": "https://arxiv.org/abs/2505.01192", "title": "Exploring the Impact of Explainable AI and Cognitive Capabilities on Users' Decisions", "authors": ["Federico Maria Cau", "Lucio Davide Spano"], "categories": ["cs.AI", "cs.HC"], "comment": "30 pages, 7 figures", "summary": "Artificial Intelligence (AI) systems are increasingly used for\ndecision-making across domains, raising debates over the information and\nexplanations they should provide. Most research on Explainable AI (XAI) has\nfocused on feature-based explanations, with less attention on alternative\nstyles. Personality traits like the Need for Cognition (NFC) can also lead to\ndifferent decision-making outcomes among low and high NFC individuals. We\ninvestigated how presenting AI information (prediction, confidence, and\naccuracy) and different explanation styles (example-based, feature-based,\nrule-based, and counterfactual) affect accuracy, reliance on AI, and cognitive\nload in a loan application scenario. We also examined low and high NFC\nindividuals' differences in prioritizing XAI interface elements (loan\nattributes, AI information, and explanations), accuracy, and cognitive load.\nOur findings show that high AI confidence significantly increases reliance on\nAI while reducing cognitive load. Feature-based explanations did not enhance\naccuracy compared to other conditions. Although counterfactual explanations\nwere less understandable, they enhanced overall accuracy, increasing reliance\non AI and reducing cognitive load when AI predictions were correct. Both low\nand high NFC individuals prioritized explanations after loan attributes,\nleaving AI information as the least important. However, we found no significant\ndifferences between low and high NFC groups in accuracy or cognitive load,\nraising questions about the role of personality traits in AI-assisted\ndecision-making. These findings highlight the need for user-centric\npersonalization in XAI interfaces, incorporating diverse explanation styles and\nexploring multiple personality traits and other user characteristics to\noptimize human-AI collaboration."}
{"id": "2505.01006", "pdf": "https://arxiv.org/pdf/2505.01006.pdf", "abs": "https://arxiv.org/abs/2505.01006", "title": "Token-free Models for Sarcasm Detection", "authors": ["Sumit Mamtani", "Maitreya Sonawane", "Kanika Agarwal", "Nishanth Sanjeev"], "categories": ["cs.CL"], "comment": null, "summary": "Tokenization is a foundational step in most natural language processing (NLP)\npipelines, yet it introduces challenges such as vocabulary mismatch and\nout-of-vocabulary issues. Recent work has shown that models operating directly\non raw text at the byte or character level can mitigate these limitations. In\nthis paper, we evaluate two token-free models, ByT5 and CANINE, on the task of\nsarcasm detection in both social media (Twitter) and non-social media (news\nheadlines) domains. We fine-tune and benchmark these models against token-based\nbaselines and state-of-the-art approaches. Our results show that ByT5-small and\nCANINE outperform token-based counterparts and achieve new state-of-the-art\nperformance, improving accuracy by 0.77% and 0.49% on the News Headlines and\nTwitter Sarcasm datasets, respectively. These findings underscore the potential\nof token-free models for robust NLP in noisy and informal domains such as\nsocial media."}
{"id": "2505.01219", "pdf": "https://arxiv.org/pdf/2505.01219.pdf", "abs": "https://arxiv.org/abs/2505.01219", "title": "Tell me who its founders are and I'll tell you what your online community looks like: Online community founders' personality and community attributes", "authors": ["Yaniv Dover", "Shaul Oreg"], "categories": ["cs.SI", "cs.HC"], "comment": null, "summary": "Online communities are an increasingly important stakeholder for firms, and\ndespite the growing body of research on them, much remains to be learned about\nthem and about the factors that determine their attributes and sustainability.\nWhereas most of the literature focuses on predictors such as community\nactivity, network structure, and platform interface, there is little research\nabout behavioral and psychological aspects of community members and leaders. In\nthe present study we focus on the personality traits of community founders as\npredictors of community attributes and sustainability. We develop a tool to\nestimate community members' Big Five personality traits from their social media\ntext and use it to estimate the traits of 35,164 founders in 8,625 Reddit\ncommunities. We find support for most of our predictions about the\nrelationships between founder traits and community sustainability and\nattributes, including the level of engagement within the community, aspects of\nits social network structure, and whether the founders themselves remain active\nin it."}
{"id": "2505.01015", "pdf": "https://arxiv.org/pdf/2505.01015.pdf", "abs": "https://arxiv.org/abs/2505.01015", "title": "Value Portrait: Understanding Values of LLMs with Human-aligned Benchmark", "authors": ["Jongwook Han", "Dongmin Choi", "Woojung Song", "Eun-Ju Lee", "Yohan Jo"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "32 pages, 7 figures", "summary": "The importance of benchmarks for assessing the values of language models has\nbeen pronounced due to the growing need of more authentic, human-aligned\nresponses. However, existing benchmarks rely on human or machine annotations\nthat are vulnerable to value-related biases. Furthermore, the tested scenarios\noften diverge from real-world contexts in which models are commonly used to\ngenerate text and express values. To address these issues, we propose the Value\nPortrait benchmark, a reliable framework for evaluating LLMs' value\norientations with two key characteristics. First, the benchmark consists of\nitems that capture real-life user-LLM interactions, enhancing the relevance of\nassessment results to real-world LLM usage and thus ecological validity.\nSecond, each item is rated by human subjects based on its similarity to their\nown thoughts, and correlations between these ratings and the subjects' actual\nvalue scores are derived. This psychometrically validated approach ensures that\nitems strongly correlated with specific values serve as reliable items for\nassessing those values. Through evaluating 27 LLMs with our benchmark, we find\nthat these models prioritize Benevolence, Security, and Self-Direction values\nwhile placing less emphasis on Tradition, Power, and Achievement values. Also,\nour analysis reveals biases in how LLMs perceive various demographic groups,\ndeviating from real human data."}
{"id": "2505.01372", "pdf": "https://arxiv.org/pdf/2505.01372.pdf", "abs": "https://arxiv.org/abs/2505.01372", "title": "Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability -- The Strange Science Part I.ii", "authors": ["Kola Ayonrinde", "Louis Jaburi"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "comment": "13 pages (plus appendices), 5 figures", "summary": "Mechanistic Interpretability (MI) aims to understand neural networks through\ncausal explanations. Though MI has many explanation-generating methods,\nprogress has been limited by the lack of a universal approach to evaluating\nexplanations. Here we analyse the fundamental question \"What makes a good\nexplanation?\" We introduce a pluralist Explanatory Virtues Framework drawing on\nfour perspectives from the Philosophy of Science - the Bayesian, Kuhnian,\nDeutschian, and Nomological - to systematically evaluate and improve\nexplanations in MI. We find that Compact Proofs consider many explanatory\nvirtues and are hence a promising approach. Fruitful research directions\nimplied by our framework include (1) clearly defining explanatory simplicity,\n(2) focusing on unifying explanations and (3) deriving universal principles for\nneural networks. Improved MI methods enhance our ability to monitor, predict,\nand steer AI systems."}
{"id": "2505.01035", "pdf": "https://arxiv.org/pdf/2505.01035.pdf", "abs": "https://arxiv.org/abs/2505.01035", "title": "Do We Need a Detailed Rubric for Automated Essay Scoring using Large Language Models?", "authors": ["Lui Yoshida"], "categories": ["cs.CL"], "comment": "Accepted in AIED 2025. This preprint has not undergone any\n  post-submission improvements or corrections", "summary": "This study investigates the necessity and impact of a detailed rubric in\nautomated essay scoring (AES) using large language models (LLMs). While using\nrubrics are standard in LLM-based AES, creating detailed rubrics requires\nsubstantial ef-fort and increases token usage. We examined how different levels\nof rubric detail affect scoring accuracy across multiple LLMs using the TOEFL11\ndataset. Our experiments compared three conditions: a full rubric, a simplified\nrubric, and no rubric, using four different LLMs (Claude 3.5 Haiku, Gemini 1.5\nFlash, GPT-4o-mini, and Llama 3 70B Instruct). Results showed that three out of\nfour models maintained similar scoring accuracy with the simplified rubric\ncompared to the detailed one, while significantly reducing token usage.\nHowever, one model (Gemini 1.5 Flash) showed decreased performance with more\ndetailed rubrics. The findings suggest that simplified rubrics may be\nsufficient for most LLM-based AES applications, offering a more efficient\nalternative without compromis-ing scoring accuracy. However, model-specific\nevaluation remains crucial as per-formance patterns vary across different LLMs."}
{"id": "2402.08978", "pdf": "https://arxiv.org/pdf/2402.08978.pdf", "abs": "https://arxiv.org/abs/2402.08978", "title": "Prismatic: Interactive Multi-View Cluster Analysis of Concept Stocks", "authors": ["Wong Kam-Kwai", "Yan Luo", "Xuanwu Yue", "Wei Chen", "Huamin Qu"], "categories": ["cs.HC", "cs.CE", "cs.LG"], "comment": "14 pages. A preprint version accepted to IEEE Transactions on\n  Visualization and Computer Graphics (TVCG), 2025", "summary": "Financial cluster analysis allows investors to discover investment\nalternatives and avoid undertaking excessive risks. However, this analytical\ntask faces substantial challenges arising from many pairwise comparisons, the\ndynamic correlations across time spans, and the ambiguity in deriving\nimplications from business relational knowledge. We propose Prismatic, a visual\nanalytics system that integrates quantitative analysis of historical\nperformance and qualitative analysis of business relational knowledge to\ncluster correlated businesses interactively. Prismatic features three\nclustering processes: dynamic cluster generation, knowledge-based cluster\nexploration, and correlation-based cluster validation. Utilizing a multi-view\nclustering approach, it enriches data-driven clusters with knowledge-driven\nsimilarity, providing a nuanced understanding of business correlations. Through\nwell-coordinated visual views, Prismatic facilitates a comprehensive\ninterpretation of intertwined quantitative and qualitative features,\ndemonstrating its usefulness and effectiveness via case studies on formulating\nconcept stocks and extensive interviews with domain experts."}
{"id": "2505.01068", "pdf": "https://arxiv.org/pdf/2505.01068.pdf", "abs": "https://arxiv.org/abs/2505.01068", "title": "Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs", "authors": ["Yijie Jin", "Junjie Peng", "Xuanchao Lin", "Haochen Yuan", "Lan Wang", "Cangzhi Zheng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal Sentiment Analysis (MSA) is a rapidly developing field that\nintegrates multimodal information to recognize sentiments, and existing models\nhave made significant progress in this area. The central challenge in MSA is\nmultimodal fusion, which is predominantly addressed by Multimodal Transformers\n(MulTs). Although act as the paradigm, MulTs suffer from efficiency concerns.\nIn this work, from the perspective of efficiency optimization, we propose and\nprove that MulTs are hierarchical modal-wise heterogeneous graphs (HMHGs), and\nwe introduce the graph-structured representation pattern of MulTs. Based on\nthis pattern, we propose an Interlaced Mask (IM) mechanism to design the\nGraph-Structured and Interlaced-Masked Multimodal Transformer (GsiT). It is\nformally equivalent to MulTs which achieves an efficient weight-sharing\nmechanism without information disorder through IM, enabling All-Modal-In-One\nfusion with only 1/3 of the parameters of pure MulTs. A Triton kernel called\nDecomposition is implemented to ensure avoiding additional computational\noverhead. Moreover, it achieves significantly higher performance than\ntraditional MulTs. To further validate the effectiveness of GsiT itself and the\nHMHG concept, we integrate them into multiple state-of-the-art models and\ndemonstrate notable performance improvements and parameter reduction on widely\nused MSA datasets."}
{"id": "2411.11382", "pdf": "https://arxiv.org/pdf/2411.11382.pdf", "abs": "https://arxiv.org/abs/2411.11382", "title": "Quantifying Haptic Affection of Car Door through Data-Driven Analysis of Force Profile", "authors": ["Mudassir Ibrahim Awan", "Ahsan Raza", "Waseem Hassan", "Ki-Uk Kyung", "Seokhee Jeon"], "categories": ["cs.HC"], "comment": "12 pages, 9 figures, 3 tables. Mudassir Ibrahim Awan and Ahsan Raza\n  are equally contributing authors", "summary": "Haptic affection plays a crucial role in user experience, particularly in the\nautomotive industry where the tactile quality of components can influence\ncustomer satisfaction. This study aims to accurately predict the affective\nproperty of a car door by only watching the force or torque profile of it when\nopening. To this end, a deep learning model is designed to capture the\nunderlying relationships between force profiles and user-defined adjective\nratings, providing insights into the door-opening experience. The dataset\nemployed in this research includes force profiles and user adjective ratings\ncollected from six distinct car models, reflecting a diverse set of\ndoor-opening characteristics and tactile feedback. The model's performance is\nassessed using Leave-One-Out Cross-Validation, a method that measures its\ngeneralization capability on unseen data. The results demonstrate that the\nproposed model achieves a high level of prediction accuracy, indicating its\npotential in various applications related to haptic affection and design\noptimization in the automotive industry."}
{"id": "2505.01110", "pdf": "https://arxiv.org/pdf/2505.01110.pdf", "abs": "https://arxiv.org/abs/2505.01110", "title": "MateICL: Mitigating Attention Dispersion in Large-Scale In-Context Learning", "authors": ["Murtadha Ahmed", "Wenbo", "Liu yunfeng"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nIn-Context Learning (ICL). However, the fixed position length constraints in\npre-trained models limit the number of demonstration examples. Recent efforts\nto extend context suffer from attention dispersion as the number of\ndemonstrations increases. In this paper, we introduce Mitigating Attention\nDispersion in large-scale ICL (MateICL) that enables LLMs to maintain effective\nself-attention as the context size grows. We first split the context into\nmultiple windows, each filled to the model's context capacity, which are\nprocessed separately. Then, we introduce an additional layer to recalibrate the\nattention weights, prioritizing the query tokens as the number of\ndemonstrations increases. Our empirical results show that MateICL can\neffectively leverage larger contexts to improve ICL performance. Compared to\nretrieval-based baselines, MateICL consistently achieves better performance\nwithout requiring an externally trained retrieval model. Despite recent\nadvances in inference strategies (e.g., 32k token contexts), our results\ndemonstrate that MateICL remains beneficial in computationally\nresource-constrained settings. The code is publicly available at\nhttps://github.com/amurtadha/MateICL."}
{"id": "2502.19877", "pdf": "https://arxiv.org/pdf/2502.19877.pdf", "abs": "https://arxiv.org/abs/2502.19877", "title": "Towards Multimodal Large-Language Models for Parent-Child Interaction: A Focus on Joint Attention", "authors": ["Weiyan Shi", "Viet Hai Le", "Kenny Tsu Wei Choo"], "categories": ["cs.HC"], "comment": "Accepted at CHI 2025 Late Breaking Work", "summary": "Joint attention is a critical component of early speech-language development\nand a key indicator of effective parent-child interaction. However, research on\ndetecting and analysing joint attention remains limited, particularly for\nMultimodal Large Language Models (MLLMs). This study evaluates MLLMs' ability\nto comprehend joint attention by analysing 26 parent-child interaction videos\nannotated by two speech-language pathologists. These annotations identify\nstrong and poor joint attention segments, serving as benchmarks for evaluating\nthe models' interpretive capabilities. Our findings reveal that current MLLMs\nstruggle to accurately interpret joint attention due to a lack of nuanced\nunderstanding of child-initiated eye contact, a crucial component of joint\nattention dynamics. This study highlights the importance of incorporating\ndetailed eye contact to enhance MLLMs' multimodal reasoning. Addressing these\ngaps is essential for future research to advance the use of MLLMs in analysing\nand supporting parent-child interactions."}
{"id": "2505.01162", "pdf": "https://arxiv.org/pdf/2505.01162.pdf", "abs": "https://arxiv.org/abs/2505.01162", "title": "On the Limitations of Steering in Language Model Alignment", "authors": ["Chebrolu Niranjan", "Kokil Jaidka", "Gerard Christopher Yeo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Steering vectors are a promising approach to aligning language model behavior\nat inference time. In this paper, we propose a framework to assess the\nlimitations of steering vectors as alignment mechanisms. Using a framework of\ntransformer hook interventions and antonym-based function vectors, we evaluate\nthe role of prompt structure and context complexity in steering effectiveness.\nOur findings indicate that steering vectors are promising for specific\nalignment tasks, such as value alignment, but may not provide a robust\nfoundation for general-purpose alignment in LLMs, particularly in complex\nscenarios. We establish a methodological foundation for future investigations\ninto steering capabilities of reasoning models."}
{"id": "2503.20331", "pdf": "https://arxiv.org/pdf/2503.20331.pdf", "abs": "https://arxiv.org/abs/2503.20331", "title": "WiCross: Indoor Human Zone-Crossing Detection Using Commodity WiFi Devices", "authors": ["Weiyan Shi", "Xuanzhi Wang", "Kai Niu", "Leye Wang", "Daqing Zhang"], "categories": ["cs.HC"], "comment": "Accepted at UbiComp/ISWC 2023 Poster", "summary": "Detecting whether a target crosses the given zone (e.g., a door) can enable\nvarious practical applications in smart homes, including intelligent security\nand people counting. The traditional infrared-based approach only covers a line\nand can be easily cracked. In contrast, reusing the ubiquitous WiFi devices\ndeployed in homes has the potential to cover a larger area of interest as WiFi\nsignals are scattered throughout the entire space. By detecting the walking\ndirection (i.e., approaching and moving away) with WiFi signal strength change,\nexisting work can identify the behavior of crossing between WiFi transceiver\npair. However, this method mistakenly classifies the turn-back behavior as\ncrossing behavior, resulting in a high false alarm rate. In this paper, we\npropose WiCross, which can accurately distinguish the turn-back behavior with\nthe phase statistics pattern of WiFi signals and thus robustly identify whether\nthe target crosses the area between the WiFi transceiver pair. We implement\nWiCross with commercial WiFi devices and extensive experiments demonstrate that\nWiCross can achieve an accuracy higher than 95\\% with a false alarm rate of\nless than 5%."}
{"id": "2505.01198", "pdf": "https://arxiv.org/pdf/2505.01198.pdf", "abs": "https://arxiv.org/abs/2505.01198", "title": "Gender Bias in Explainability: Investigating Performance Disparity in Post-hoc Methods", "authors": ["Mahdi Dhaini", "Ege Erdogan", "Nils Feldhus", "Gjergji Kasneci"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACM Conference on Fairness, Accountability, and\n  Transparency (FAccT) 2025", "summary": "While research on applications and evaluations of explanation methods\ncontinues to expand, fairness of the explanation methods concerning disparities\nin their performance across subgroups remains an often overlooked aspect. In\nthis paper, we address this gap by showing that, across three tasks and five\nlanguage models, widely used post-hoc feature attribution methods exhibit\nsignificant gender disparity with respect to their faithfulness, robustness,\nand complexity. These disparities persist even when the models are pre-trained\nor fine-tuned on particularly unbiased datasets, indicating that the\ndisparities we observe are not merely consequences of biased training data. Our\nresults highlight the importance of addressing disparities in explanations when\ndeveloping and applying explainability methods, as these can lead to biased\noutcomes against certain subgroups, with particularly critical implications in\nhigh-stakes contexts. Furthermore, our findings underscore the importance of\nincorporating the fairness of explanations, alongside overall model fairness\nand explainability, as a requirement in regulatory frameworks."}
{"id": "2404.17347", "pdf": "https://arxiv.org/pdf/2404.17347.pdf", "abs": "https://arxiv.org/abs/2404.17347", "title": "InspectorRAGet: An Introspection Platform for RAG Evaluation", "authors": ["Kshitij Fadnis", "Siva Sankalp Patel", "Odellia Boni", "Yannis Katsis", "Sara Rosenthal", "Benjamin Sznajder", "Marina Danilevsky"], "categories": ["cs.SE", "cs.HC"], "comment": "Published at NAACL2025 Demonstration Track", "summary": "Large Language Models (LLM) have become a popular approach for implementing\nRetrieval Augmented Generation (RAG) systems, and a significant amount of\neffort has been spent on building good models and metrics. In spite of\nincreased recognition of the need for rigorous evaluation of RAG systems, few\ntools exist that go beyond the creation of model output and automatic\ncalculation. We present InspectorRAGet, an introspection platform for\nperforming a comprehensive analysis of the quality of RAG system output.\nInspectorRAGet allows the user to analyze aggregate and instance-level\nperformance of RAG systems, using both human and algorithmic metrics as well as\nannotator quality. InspectorRAGet is suitable for multiple use cases and is\navailable publicly to the community. A live instance of the platform is\navailable at https://ibm.biz/InspectorRAGet."}
{"id": "2505.01238", "pdf": "https://arxiv.org/pdf/2505.01238.pdf", "abs": "https://arxiv.org/abs/2505.01238", "title": "EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods on NLP Models", "authors": ["Mahdi Dhaini", "Kafaite Zahra Hussain", "Efstratios Zaradoukas", "Gjergji Kasneci"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to the xAI World Conference (2025) - System Demonstration", "summary": "As Natural Language Processing (NLP) models continue to evolve and become\nintegral to high-stakes applications, ensuring their interpretability remains a\ncritical challenge. Given the growing variety of explainability methods and\ndiverse stakeholder requirements, frameworks that help stakeholders select\nappropriate explanations tailored to their specific use cases are increasingly\nimportant. To address this need, we introduce EvalxNLP, a Python framework for\nbenchmarking state-of-the-art feature attribution methods for transformer-based\nNLP models. EvalxNLP integrates eight widely recognized explainability\ntechniques from the Explainable AI (XAI) literature, enabling users to generate\nand evaluate explanations based on key properties such as faithfulness,\nplausibility, and complexity. Our framework also provides interactive,\nLLM-based textual explanations, facilitating user understanding of the\ngenerated explanations and evaluation outcomes. Human evaluation results\nindicate high user satisfaction with EvalxNLP, suggesting it is a promising\nframework for benchmarking explanation methods across diverse user groups. By\noffering a user-friendly and extensible platform, EvalxNLP aims at\ndemocratizing explainability tools and supporting the systematic comparison and\nadvancement of XAI techniques in NLP."}
{"id": "2410.17262", "pdf": "https://arxiv.org/pdf/2410.17262.pdf", "abs": "https://arxiv.org/abs/2410.17262", "title": "EmoGene: Audio-Driven Emotional 3D Talking-Head Generation", "authors": ["Wenqing Wang", "Yun Fu"], "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "comment": "Accepted by the 2025 IEEE 19th International Conference on Automatic\n  Face and Gesture Recognition (FG)", "summary": "Audio-driven talking-head generation is a crucial and useful technology for\nvirtual human interaction and film-making. While recent advances have focused\non improving image fidelity and lip synchronization, generating accurate\nemotional expressions remains underexplored. In this paper, we introduce\nEmoGene, a novel framework for synthesizing high-fidelity, audio-driven video\nportraits with accurate emotional expressions. Our approach employs a\nvariational autoencoder (VAE)-based audio-to-motion module to generate facial\nlandmarks, which are concatenated with emotional embedding in a\nmotion-to-emotion module to produce emotional landmarks. These landmarks drive\na Neural Radiance Fields (NeRF)-based emotion-to-video module to render\nrealistic emotional talking-head videos. Additionally, we propose a pose\nsampling method to generate natural idle-state (non-speaking) videos for silent\naudio inputs. Extensive experiments demonstrate that EmoGene outperforms\nprevious methods in generating high-fidelity emotional talking-head videos."}
{"id": "2505.01255", "pdf": "https://arxiv.org/pdf/2505.01255.pdf", "abs": "https://arxiv.org/abs/2505.01255", "title": "PREMISE: Matching-based Prediction for Accurate Review Recommendation", "authors": ["Wei Han", "Hui Chen", "Soujanya Poria"], "categories": ["cs.CL", "cs.IR", "cs.MM"], "comment": "19 pages, 16 figures", "summary": "We present PREMISE (PREdict with Matching ScorEs), a new architecture for the\nmatching-based learning in the multimodal fields for the multimodal review\nhelpfulness (MRHP) task. Distinct to previous fusion-based methods which\nobtains multimodal representations via cross-modal attention for downstream\ntasks, PREMISE computes the multi-scale and multi-field representations,\nfilters duplicated semantics, and then obtained a set of matching scores as\nfeature vectors for the downstream recommendation task. This new architecture\nsignificantly boosts the performance for such multimodal tasks whose context\nmatching content are highly correlated to the targets of that task, compared to\nthe state-of-the-art fusion-based methods. Experimental results on two publicly\navailable datasets show that PREMISE achieves promising performance with less\ncomputational cost."}
{"id": "2502.12354", "pdf": "https://arxiv.org/pdf/2502.12354.pdf", "abs": "https://arxiv.org/abs/2502.12354", "title": "Human-centered explanation does not fit all: The interplay of sociotechnical, cognitive, and individual factors in the effect AI explanations in algorithmic decision-making", "authors": ["Yongsu Ahn", "Yu-Ru Lin", "Malihe Alikhani", "Eunjeong Cheon"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "Recent XAI studies have investigated what constitutes a \\textit{good}\nexplanation in AI-assisted decision-making. Despite the widely accepted\nhuman-friendly properties of explanations, such as contrastive and selective,\nexisting studies have yielded inconsistent findings. To address these gaps, our\nstudy focuses on the cognitive dimensions of explanation evaluation, by\nevaluating six explanations with different contrastive strategies and\ninformation selectivity and scrutinizing factors behind their valuation\nprocess. Our analysis results find that contrastive explanations are not the\nmost preferable or understandable in general; Rather, different contrastive and\nselective explanations were appreciated to a different extent based on who they\nare, when, how, and what to explain -- with different level of cognitive load\nand engagement and sociotechnical contexts. Given these findings, we call for a\nnuanced view of explanation strategies, with implications for designing AI\ninterfaces to accommodate individual and contextual differences in AI-assisted\ndecision-making."}
{"id": "2505.01273", "pdf": "https://arxiv.org/pdf/2505.01273.pdf", "abs": "https://arxiv.org/abs/2505.01273", "title": "Anti-adversarial Learning: Desensitizing Prompts for Large Language Models", "authors": ["Xuan Li", "Zhe Yin", "Xiaodong Gu", "Beijun Shen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the widespread use of LLMs, preserving privacy in user prompts has\nbecome crucial, as prompts risk exposing privacy and sensitive data to the\ncloud LLMs. Traditional techniques like homomorphic encryption, secure\nmulti-party computation, and federated learning face challenges due to heavy\ncomputational costs and user participation requirements, limiting their\napplicability in LLM scenarios. In this paper, we propose PromptObfus, a novel\nmethod for desensitizing LLM prompts. The core idea of PromptObfus is\n\"anti-adversarial\" learning, which perturbs privacy words in the prompt to\nobscure sensitive information while retaining the stability of model\npredictions. Specifically, PromptObfus frames prompt desensitization as a\nmasked language modeling task, replacing privacy-sensitive terms with a [MASK]\ntoken. A desensitization model is trained to generate candidate replacements\nfor each masked position. These candidates are subsequently selected based on\ngradient feedback from a surrogate model, ensuring minimal disruption to the\ntask output. We demonstrate the effectiveness of our approach on three NLP\ntasks. Results show that PromptObfus effectively prevents privacy inference\nfrom remote LLMs while preserving task performance."}
{"id": "2503.04765", "pdf": "https://arxiv.org/pdf/2503.04765.pdf", "abs": "https://arxiv.org/abs/2503.04765", "title": "Generative AI in Academic Writing: A Comparison of DeepSeek, Qwen, ChatGPT, Gemini, Llama, Mistral, and Gemma", "authors": ["Omer Aydin", "Enis Karaarslan", "Fatih Safa Erenay", "Nebojsa Bacanin"], "categories": ["cs.CY", "cs.HC"], "comment": "24 pages, 4 figures, 7 tables", "summary": "DeepSeek v3, developed in China, was released in December 2024, followed by\nAlibaba's Qwen 2.5 Max in January 2025 and Qwen3 235B in April 2025. These free\nand open-source models offer significant potential for academic writing and\ncontent creation. This study evaluates their academic writing performance by\ncomparing them with ChatGPT, Gemini, Llama, Mistral, and Gemma. There is a\ncritical gap in the literature concerning how extensively these tools can be\nutilized and their potential to generate original content in terms of quality,\nreadability, and effectiveness. Using 40 papers on Digital Twin and Healthcare,\ntexts were generated through AI tools based on posed questions and paraphrased\nabstracts. The generated content was analyzed using plagiarism detection, AI\ndetection, word count comparisons, semantic similarity, and readability\nassessments. Results indicate that paraphrased abstracts showed higher\nplagiarism rates, while question-based responses also exceeded acceptable\nlevels. AI detection tools consistently identified all outputs as AI-generated.\nWord count analysis revealed that all chatbots produced a sufficient volume of\ncontent. Semantic similarity tests showed a strong overlap between generated\nand original texts. However, readability assessments indicated that the texts\nwere insufficient in terms of clarity and accessibility. This study\ncomparatively highlights the potential and limitations of popular and latest\nlarge language models for academic writing. While these models generate\nsubstantial and semantically accurate content, concerns regarding plagiarism,\nAI detection, and readability must be addressed for their effective use in\nscholarly work."}
{"id": "2505.01311", "pdf": "https://arxiv.org/pdf/2505.01311.pdf", "abs": "https://arxiv.org/abs/2505.01311", "title": "A Factorized Probabilistic Model of the Semantics of Vague Temporal Adverbials Relative to Different Event Types", "authors": ["Svenja Kenneweg", "Jörg Deigmöller", "Julian Eggert", "Philipp Cimiano"], "categories": ["cs.CL"], "comment": "7 pages, 1 figure, to be published in CogSci Proceedings 2025", "summary": "Vague temporal adverbials, such as recently, just, and a long time ago,\ndescribe the temporal distance between a past event and the utterance time but\nleave the exact duration underspecified. In this paper, we introduce a\nfactorized model that captures the semantics of these adverbials as\nprobabilistic distributions. These distributions are composed with\nevent-specific distributions to yield a contextualized meaning for an adverbial\napplied to a specific event. We fit the model's parameters using existing data\ncapturing judgments of native speakers regarding the applicability of these\nvague temporal adverbials to events that took place a given time ago. Comparing\nour approach to a non-factorized model based on a single Gaussian distribution\nfor each pair of event and temporal adverbial, we find that while both models\nhave similar predictive power, our model is preferable in terms of Occam's\nrazor, as it is simpler and has better extendability."}
{"id": "2503.14810", "pdf": "https://arxiv.org/pdf/2503.14810.pdf", "abs": "https://arxiv.org/abs/2503.14810", "title": "A Study on Human-Swarm Interaction: A Framework for Assessing Situation Awareness and Task Performance", "authors": ["Wasura D. Wattearachchi", "Erandi Lakshika", "Kathryn Kasmarik", "Michael Barlow"], "categories": ["cs.RO", "cs.HC"], "comment": "10 pages, 8 figures, 2 tables, 2 equations", "summary": "This paper introduces a framework for human swarm interaction studies that\nmeasures situation awareness in dynamic environments. A tablet-based interface\nwas developed for a user study by implementing the concepts introduced in the\nframework, where operators guided a robotic swarm in a single-target search\ntask, marking hazardous cells unknown to the swarm. Both subjective and\nobjective situation awareness measures were used, with task performance\nevaluated based on how close the robots were to the target. The framework\nenabled a structured investigation of the role of situation awareness in human\nswarm interaction, leading to key findings such as improved task performance\nacross attempts, showing the interface was learnable, centroid active robot\nposition proved to be a useful task performance metric for assessing situation\nawareness, perception and projection played a key role in task performance,\nhighlighting their importance in interface design and objective situation\nawareness influenced both subjective situation awareness and task performance,\nemphasizing the need for interfaces that emphasise objective situation\nawareness. These findings validate our framework as a structured approach for\nintegrating situation awareness concepts into human swarm interaction studies,\noffering a systematic way to assess situation awareness and task performance.\nThe framework can be applied to other swarming studies to evaluate interface\nlearnability, identify meaningful task performance metrics, and refine\ninterface designs to enhance situation awareness, ultimately improving human\nswarm interaction in dynamic environments."}
{"id": "2505.01314", "pdf": "https://arxiv.org/pdf/2505.01314.pdf", "abs": "https://arxiv.org/abs/2505.01314", "title": "A Transformer-based Neural Architecture Search Method", "authors": ["Shang Wang", "Huanrong Tang", "Jianquan Ouyang"], "categories": ["cs.CL", "cs.NE"], "comment": "GECCO 2023", "summary": "This paper presents a neural architecture search method based on Transformer\narchitecture, searching cross multihead attention computation ways for\ndifferent number of encoder and decoder combinations. In order to search for\nneural network structures with better translation results, we considered\nperplexity as an auxiliary evaluation metric for the algorithm in addition to\nBLEU scores and iteratively improved each individual neural network within the\npopulation by a multi-objective genetic algorithm. Experimental results show\nthat the neural network structures searched by the algorithm outperform all the\nbaseline models, and that the introduction of the auxiliary evaluation metric\ncan find better models than considering only the BLEU score as an evaluation\nmetric."}
{"id": "2504.08954", "pdf": "https://arxiv.org/pdf/2504.08954.pdf", "abs": "https://arxiv.org/abs/2504.08954", "title": "Should you use LLMs to simulate opinions? Quality checks for early-stage deliberation", "authors": ["Terrence Neumann", "Maria De-Arteaga", "Sina Fazelpour"], "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "The emergent capabilities of large language models (LLMs) have sparked\ninterest in assessing their ability to simulate human opinions in a variety of\ncontexts, potentially serving as surrogates for human subjects in opinion\nsurveys. However, previous evaluations of this capability have depended heavily\non costly, domain-specific human survey data, and mixed empirical results about\nLLM effectiveness create uncertainty for managers about whether investing in\nthis technology is justified in early-stage research. To address these\nchallenges, we introduce a series of quality checks to support early-stage\ndeliberation about the viability of using LLMs for simulating human opinions.\nThese checks emphasize logical constraints, model stability, and alignment with\nstakeholder expectations of model outputs, thereby reducing dependence on\nhuman-generated data in the initial stages of evaluation. We demonstrate the\nusefulness of the proposed quality control tests in the context of AI-assisted\ncontent moderation, an application that both advocates and critics of LLMs'\ncapabilities to simulate human opinion see as a desirable potential use case.\nNone of the tested models passed all quality control checks, revealing several\nfailure modes. We conclude by discussing implications of these failure modes\nand recommend how organizations can utilize our proposed tests for prompt\nengineering and in their risk management practices when considering the use of\nLLMs for opinion simulation. We make our crowdsourced dataset of claims with\nhuman and LLM annotations publicly available for future research."}
{"id": "2505.01315", "pdf": "https://arxiv.org/pdf/2505.01315.pdf", "abs": "https://arxiv.org/abs/2505.01315", "title": "Helping Big Language Models Protect Themselves: An Enhanced Filtering and Summarization System", "authors": ["Sheikh Samit Muhaimin", "Spyridon Mastorakis"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The recent growth in the use of Large Language Models has made them\nvulnerable to sophisticated adversarial assaults, manipulative prompts, and\nencoded malicious inputs. Existing countermeasures frequently necessitate\nretraining models, which is computationally costly and impracticable for\ndeployment. Without the need for retraining or fine-tuning, this study presents\na unique defense paradigm that allows LLMs to recognize, filter, and defend\nagainst adversarial or malicious inputs on their own. There are two main parts\nto the suggested framework: (1) A prompt filtering module that uses\nsophisticated Natural Language Processing (NLP) techniques, including zero-shot\nclassification, keyword analysis, and encoded content detection (e.g. base64,\nhexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and\n(2) A summarization module that processes and summarizes adversarial research\nliterature to give the LLM context-aware defense knowledge. This approach\nstrengthens LLMs' resistance to adversarial exploitation by fusing text\nextraction, summarization, and harmful prompt analysis. According to\nexperimental results, this integrated technique has a 98.71% success rate in\nidentifying harmful patterns, manipulative language structures, and encoded\nprompts. By employing a modest amount of adversarial research literature as\ncontext, the methodology also allows the model to react correctly to harmful\ninputs with a larger percentage of jailbreak resistance and refusal rate. While\nmaintaining the quality of LLM responses, the framework dramatically increases\nLLM's resistance to hostile misuse, demonstrating its efficacy as a quick and\neasy substitute for time-consuming, retraining-based defenses."}
{"id": "2505.01325", "pdf": "https://arxiv.org/pdf/2505.01325.pdf", "abs": "https://arxiv.org/abs/2505.01325", "title": "TRAVELER: A Benchmark for Evaluating Temporal Reasoning across Vague, Implicit and Explicit References", "authors": ["Svenja Kenneweg", "Jörg Deigmöller", "Philipp Cimiano", "Julian Eggert"], "categories": ["cs.CL"], "comment": "24 pages, 6 figures, submitted to Springer Nature Computer Science", "summary": "Understanding and resolving temporal references is essential in Natural\nLanguage Understanding as we often refer to the past or future in daily\ncommunication. Although existing benchmarks address a system's ability to\nreason about and resolve temporal references, systematic evaluation of specific\ntemporal references remains limited. Towards closing this gap, we introduce\nTRAVELER, a novel synthetic benchmark dataset that follows a Question Answering\nparadigm and consists of questions involving temporal references with the\ncorresponding correct answers. TRAVELER assesses models' abilities to resolve\nexplicit, implicit relative to speech time, and vague temporal references.\nBeyond investigating the performance of state-of-the-art LLMs depending on the\ntype of temporal reference, our benchmark also allows evaluation of performance\nin relation to the length of the set of events. For the category of vague\ntemporal references, ground-truth answers were established via human surveys on\nProlific, following a procedure similar to the one from Kenneweg et al. To\ndemonstrate the benchmark's applicability, we evaluate four state-of-the-art\nLLMs using a question-answering task encompassing 3,300 questions. Our findings\nshow that while the benchmarked LLMs can answer questions over event sets with\na handful of events and explicit temporal references successfully, performance\nclearly deteriorates with larger event set length and when temporal references\nget less explicit. Notably, the vague question category exhibits the lowest\nperformance across all models.\n  The benchmark is publicly available at:\nhttps://gitlab.ub.uni-bielefeld.de/s.kenneweg/TRAVELER"}
{"id": "2505.00759", "pdf": "https://arxiv.org/pdf/2505.00759.pdf", "abs": "https://arxiv.org/abs/2505.00759", "title": "Multi-Modal Language Models as Text-to-Image Model Evaluators", "authors": ["Jiahui Chen", "Candace Ross", "Reyhane Askari-Hemmat", "Koustuv Sinha", "Melissa Hall", "Michal Drozdzal", "Adriana Romero-Soriano"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The steady improvements of text-to-image (T2I) generative models lead to slow\ndeprecation of automatic evaluation benchmarks that rely on static datasets,\nmotivating researchers to seek alternative ways to evaluate the T2I progress.\nIn this paper, we explore the potential of multi-modal large language models\n(MLLMs) as evaluator agents that interact with a T2I model, with the objective\nof assessing prompt-generation consistency and image aesthetics. We present\nMultimodal Text-to-Image Eval (MT2IE), an evaluation framework that iteratively\ngenerates prompts for evaluation, scores generated images and matches T2I\nevaluation of existing benchmarks with a fraction of the prompts used in\nexisting static benchmarks. Moreover, we show that MT2IE's prompt-generation\nconsistency scores have higher correlation with human judgment than scores\npreviously introduced in the literature. MT2IE generates prompts that are\nefficient at probing T2I model performance, producing the same relative T2I\nmodel rankings as existing benchmarks while using only 1/80th the number of\nprompts for evaluation."}
{"id": "2505.00808", "pdf": "https://arxiv.org/pdf/2505.00808.pdf", "abs": "https://arxiv.org/abs/2505.00808", "title": "A Mathematical Philosophy of Explanations in Mechanistic Interpretability -- The Strange Science Part I.i", "authors": ["Kola Ayonrinde", "Louis Jaburi"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "15 pages (plus appendices), 2 figures", "summary": "Mechanistic Interpretability aims to understand neural networks through\ncausal explanations. We argue for the Explanatory View Hypothesis: that\nMechanistic Interpretability research is a principled approach to understanding\nmodels because neural networks contain implicit explanations which can be\nextracted and understood. We hence show that Explanatory Faithfulness, an\nassessment of how well an explanation fits a model, is well-defined. We propose\na definition of Mechanistic Interpretability (MI) as the practice of producing\nModel-level, Ontic, Causal-Mechanistic, and Falsifiable explanations of neural\nnetworks, allowing us to distinguish MI from other interpretability paradigms\nand detail MI's inherent limits. We formulate the Principle of Explanatory\nOptimism, a conjecture which we argue is a necessary precondition for the\nsuccess of Mechanistic Interpretability."}
{"id": "2505.00831", "pdf": "https://arxiv.org/pdf/2505.00831.pdf", "abs": "https://arxiv.org/abs/2505.00831", "title": "SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation", "authors": ["Quang P. M. Pham", "Khoi T. N. Nguyen", "Nhi H. Doan", "Cuong A. Pham", "Kentaro Inui", "Dezhen Song"], "categories": ["cs.RO", "cs.CL"], "comment": "Paper is under review", "summary": "Efficient path planning in robotics, particularly within large-scale, dynamic\nenvironments, remains a significant hurdle. While Large Language Models (LLMs)\noffer strong reasoning capabilities, their high computational cost and limited\nadaptability in dynamic scenarios hinder real-time deployment on edge devices.\nWe present SmallPlan -- a novel framework leveraging LLMs as teacher models to\ntrain lightweight Small Language Models (SLMs) for high-level path planning\ntasks. In SmallPlan, the SLMs provide optimal action sequences to navigate\nacross scene graphs that compactly represent full-scaled 3D scenes. The SLMs\nare trained in a simulation-powered, interleaved manner with LLM-guided\nsupervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not\nonly enables SLMs to successfully complete navigation tasks but also makes them\naware of important factors like travel distance and number of trials. Through\nexperiments, we demonstrate that the fine-tuned SLMs perform competitively with\nlarger models like GPT-4o on sequential path planning, without suffering from\nhallucination and overfitting. SmallPlan is resource-efficient, making it\nwell-suited for edge-device deployment and advancing practical autonomous\nrobotics."}
{"id": "2505.00903", "pdf": "https://arxiv.org/pdf/2505.00903.pdf", "abs": "https://arxiv.org/abs/2505.00903", "title": "NeMo-Inspector: A Visualization Tool for LLM Generation Analysis", "authors": ["Daria Gitman", "Igor Gitman", "Evelina Bakhturina"], "categories": ["cs.LG", "cs.CL"], "comment": "Presented at the NAACL 2025 conference", "summary": "Adapting Large Language Models (LLMs) to novel tasks and enhancing their\noverall capabilities often requires large, high-quality training datasets.\nSynthetic data, generated at scale, serves a valuable alternative when\nreal-world data is scarce or difficult to obtain. However, ensuring the quality\nof synthetic datasets is challenging, as developers must manually inspect and\nrefine numerous samples to identify errors and areas for improvement. This\nprocess is time-consuming and requires specialized tools. We introduce\nNeMo-Inspector, an open-source tool designed to simplify the analysis of\nsynthetic datasets with integrated inference capabilities. We demonstrate its\neffectiveness through two real-world cases. Analysis and cleaning of the\nsynthetically generated GSM-Plus dataset with NeMo-Inspector led to a\nsignificant decrease in low-quality samples from 46.99% to 19.51%. The tool\nalso helped identify and correct generation errors in OpenMath models,\nimproving accuracy by 1.92% on the MATH dataset and by 4.17% on the GSM8K\ndataset for a Meta-Llama-3-8B model fine-tuned on synthetic data generated from\nNemotron-4-340B."}
{"id": "2505.00926", "pdf": "https://arxiv.org/pdf/2505.00926.pdf", "abs": "https://arxiv.org/abs/2505.00926", "title": "How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias", "authors": ["Ruiquan Huang", "Yingbin Liang", "Jing Yang"], "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": "accepted by ICML 2025", "summary": "Language recognition tasks are fundamental in natural language processing\n(NLP) and have been widely used to benchmark the performance of large language\nmodels (LLMs). These tasks also play a crucial role in explaining the working\nmechanisms of transformers. In this work, we focus on two representative tasks\nin the category of regular language recognition, known as `even pairs' and\n`parity check', the aim of which is to determine whether the occurrences of\ncertain subsequences in a given sequence are even. Our goal is to explore how a\none-layer transformer, consisting of an attention layer followed by a linear\nlayer, learns to solve these tasks by theoretically analyzing its training\ndynamics under gradient descent. While even pairs can be solved directly by a\none-layer transformer, parity check need to be solved by integrating\nChain-of-Thought (CoT), either into the inference stage of a transformer\nwell-trained for the even pairs task, or into the training of a one-layer\ntransformer. For both problems, our analysis shows that the joint training of\nattention and linear layers exhibits two distinct phases. In the first phase,\nthe attention layer grows rapidly, mapping data sequences into separable\nvectors. In the second phase, the attention layer becomes stable, while the\nlinear layer grows logarithmically and approaches in direction to a max-margin\nhyperplane that correctly separates the attention layer outputs into positive\nand negative samples, and the loss decreases at a rate of $O(1/t)$. Our\nexperiments validate those theoretical results."}
{"id": "2505.00976", "pdf": "https://arxiv.org/pdf/2505.00976.pdf", "abs": "https://arxiv.org/abs/2505.00976", "title": "Attack and defense techniques in large language models: A survey and new perspectives", "authors": ["Zhiyu Liao", "Kang Chen", "Yuanguo Lin", "Kangkang Li", "Yunxuan Liu", "Hefeng Chen", "Xingwang Huang", "Yuanhui Yu"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have become central to numerous natural language\nprocessing tasks, but their vulnerabilities present significant security and\nethical challenges. This systematic survey explores the evolving landscape of\nattack and defense techniques in LLMs. We classify attacks into adversarial\nprompt attack, optimized attacks, model theft, as well as attacks on\napplication of LLMs, detailing their mechanisms and implications. Consequently,\nwe analyze defense strategies, including prevention-based and detection-based\ndefense methods. Although advances have been made, challenges remain to adapt\nto the dynamic threat landscape, balance usability with robustness, and address\nresource constraints in defense implementation. We highlight open problems,\nincluding the need for adaptive scalable defenses, explainable security\ntechniques, and standardized evaluation frameworks. This survey provides\nactionable insights and directions for developing secure and resilient LLMs,\nemphasizing the importance of interdisciplinary collaboration and ethical\nconsiderations to mitigate risks in real-world applications."}
{"id": "2505.01007", "pdf": "https://arxiv.org/pdf/2505.01007.pdf", "abs": "https://arxiv.org/abs/2505.01007", "title": "Towards the Resistance of Neural Network Watermarking to Fine-tuning", "authors": ["Ling Tang", "Yuefeng Chen", "Hui Xue", "Quanshi Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper proves a new watermarking method to embed the ownership\ninformation into a deep neural network (DNN), which is robust to fine-tuning.\nSpecifically, we prove that when the input feature of a convolutional layer\nonly contains low-frequency components, specific frequency components of the\nconvolutional filter will not be changed by gradient descent during the\nfine-tuning process, where we propose a revised Fourier transform to extract\nfrequency components from the convolutional filter. Additionally, we also prove\nthat these frequency components are equivariant to weight scaling and weight\npermutations. In this way, we design a watermark module to encode the watermark\ninformation to specific frequency components in a convolutional filter.\nPreliminary experiments demonstrate the effectiveness of our method."}
{"id": "2505.01096", "pdf": "https://arxiv.org/pdf/2505.01096.pdf", "abs": "https://arxiv.org/abs/2505.01096", "title": "Evaluating Vision Language Model Adaptations for Radiology Report Generation in Low-Resource Languages", "authors": ["Marco Salmè", "Rosa Sicilia", "Paolo Soda", "Valerio Guarrasi"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "The integration of artificial intelligence in healthcare has opened new\nhorizons for improving medical diagnostics and patient care. However,\nchallenges persist in developing systems capable of generating accurate and\ncontextually relevant radiology reports, particularly in low-resource\nlanguages. In this study, we present a comprehensive benchmark to evaluate the\nperformance of instruction-tuned Vision-Language Models (VLMs) in the\nspecialized task of radiology report generation across three low-resource\nlanguages: Italian, German, and Spanish. Employing the LLaVA architectural\nframework, we conducted a systematic evaluation of pre-trained models utilizing\ngeneral datasets, domain-specific datasets, and low-resource language-specific\ndatasets. In light of the unavailability of models that possess prior knowledge\nof both the medical domain and low-resource languages, we analyzed various\nadaptations to determine the most effective approach for these contexts. The\nresults revealed that language-specific models substantially outperformed both\ngeneral and domain-specific models in generating radiology reports, emphasizing\nthe critical role of linguistic adaptation. Additionally, models fine-tuned\nwith medical terminology exhibited enhanced performance across all languages\ncompared to models with generic knowledge, highlighting the importance of\ndomain-specific training. We also explored the influence of the temperature\nparameter on the coherence of report generation, providing insights for optimal\nmodel settings. Our findings highlight the importance of tailored language and\ndomain-specific training for improving the quality and accuracy of radiological\nreports in multilingual settings. This research not only advances our\nunderstanding of VLMs adaptability in healthcare but also points to significant\navenues for future investigations into model tuning and language-specific\nadaptations."}
{"id": "2505.01372", "pdf": "https://arxiv.org/pdf/2505.01372.pdf", "abs": "https://arxiv.org/abs/2505.01372", "title": "Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability -- The Strange Science Part I.ii", "authors": ["Kola Ayonrinde", "Louis Jaburi"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "comment": "13 pages (plus appendices), 5 figures", "summary": "Mechanistic Interpretability (MI) aims to understand neural networks through\ncausal explanations. Though MI has many explanation-generating methods,\nprogress has been limited by the lack of a universal approach to evaluating\nexplanations. Here we analyse the fundamental question \"What makes a good\nexplanation?\" We introduce a pluralist Explanatory Virtues Framework drawing on\nfour perspectives from the Philosophy of Science - the Bayesian, Kuhnian,\nDeutschian, and Nomological - to systematically evaluate and improve\nexplanations in MI. We find that Compact Proofs consider many explanatory\nvirtues and are hence a promising approach. Fruitful research directions\nimplied by our framework include (1) clearly defining explanatory simplicity,\n(2) focusing on unifying explanations and (3) deriving universal principles for\nneural networks. Improved MI methods enhance our ability to monitor, predict,\nand steer AI systems."}
{"id": "2402.14359", "pdf": "https://arxiv.org/pdf/2402.14359.pdf", "abs": "https://arxiv.org/abs/2402.14359", "title": "Rethinking Scientific Summarization Evaluation: Grounding Explainable Metrics on Facet-aware Benchmark", "authors": ["Xiuying Chen", "Tairan Wang", "Qingqing Zhu", "Taicheng Guo", "Shen Gao", "Zhiyong Lu", "Xin Gao", "Xiangliang Zhang"], "categories": ["cs.CL"], "comment": "14pages", "summary": "The summarization capabilities of pretrained and large language models (LLMs)\nhave been widely validated in general areas, but their use in scientific\ncorpus, which involves complex sentences and specialized knowledge, has been\nless assessed. This paper presents conceptual and experimental analyses of\nscientific summarization, highlighting the inadequacies of traditional\nevaluation methods, such as $n$-gram, embedding comparison, and QA,\nparticularly in providing explanations, grasping scientific concepts, or\nidentifying key content. Subsequently, we introduce the Facet-aware Metric\n(FM), employing LLMs for advanced semantic matching to evaluate summaries based\non different aspects. This facet-aware approach offers a thorough evaluation of\nabstracts by decomposing the evaluation task into simpler subtasks.Recognizing\nthe absence of an evaluation benchmark in this domain, we curate a Facet-based\nscientific summarization Dataset (FD) with facet-level annotations. Our\nfindings confirm that FM offers a more logical approach to evaluating\nscientific summaries. In addition, fine-tuned smaller models can compete with\nLLMs in scientific contexts, while LLMs have limitations in learning from\nin-context information in scientific domains. This suggests an area for future\nenhancement of LLMs."}
{"id": "2404.18624", "pdf": "https://arxiv.org/pdf/2404.18624.pdf", "abs": "https://arxiv.org/abs/2404.18624", "title": "Do Vision & Language Decoders use Images and Text equally? How Self-consistent are their Explanations?", "authors": ["Letitia Parcalabescu", "Anette Frank"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "68Txx", "I.2.7; I.2.10"], "comment": "30 pages, 8 figures, 11 tables", "summary": "Vision and language model (VLM) decoders are currently the best-performing\narchitectures on multimodal tasks. Next to answers, they are able to produce\nnatural language explanations, either in post-hoc or CoT settings. However, it\nis not clear to what extent they are using the input vision and text modalities\nwhen generating answers or explanations. In this work, we investigate if VLMs\nrely on their input modalities differently when they produce explanations as\nopposed to answers. We also evaluate the self-consistency of VLM decoders in\nboth post-hoc and CoT explanation settings, by extending existing unimodal\ntests and measures to VLM decoders. We find that most tested VLMs are less\nself-consistent than LLMs. Text contributions in all tested VL decoders are\nmore important than image contributions in all examined tasks. However, when\ncomparing explanation generation to answer generation, the contributions of\nimages are significantly stronger for generating explanations compared to\nanswers. This difference is even larger in CoT compared to post-hoc\nexplanations. Lastly, we provide an up-to-date benchmarking of state-of-the-art\nVL decoders on the VALSE benchmark, which before was restricted to VL encoders.\nWe find that the tested VL decoders still struggle with most phenomena tested\nby VALSE."}
{"id": "2409.00292", "pdf": "https://arxiv.org/pdf/2409.00292.pdf", "abs": "https://arxiv.org/abs/2409.00292", "title": "REFFLY: Melody-Constrained Lyrics Editing Model", "authors": ["Songyan Zhao", "Bingxuan Li", "Yufei Tian", "Nanyun Peng"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Automatic melody-to-lyric (M2L) generation aims to create lyrics that align\nwith a given melody. While most previous approaches generate lyrics from\nscratch, revision, editing plain text draft to fit it into the melody, offers a\nmuch more flexible and practical alternative. This enables broad applications,\nsuch as generating lyrics from flexible inputs (keywords, themes, or full text\nthat needs refining to be singable), song translation (preserving meaning\nacross languages while keeping the melody intact), or style transfer (adapting\nlyrics to different genres). This paper introduces REFFLY (REvision Framework\nFor LYrics), the first revision framework for editing and generating\nmelody-aligned lyrics. We train the lyric revision module using our curated\nsynthesized melody-aligned lyrics dataset, enabling it to transform plain text\ninto lyrics that align with a given melody. To further enhance the revision\nability, we propose training-free heuristics aimed at preserving both semantic\nmeaning and musical consistency throughout the editing process. Experimental\nresults demonstrate the effectiveness of REFFLY across various tasks (e.g.\nlyrics generation, song translation), showing that our model outperforms strong\nbaselines, including Lyra (Tian et al., 2023) and GPT-4, by 25% in both\nmusicality and text quality."}
{"id": "2412.00359", "pdf": "https://arxiv.org/pdf/2412.00359.pdf", "abs": "https://arxiv.org/abs/2412.00359", "title": "Does Self-Attention Need Separate Weights in Transformers?", "authors": ["Md Kowsher", "Nusrat Jahan Prottasha", "Chun-Nam Yu", "Ozlem Ozmen Garibay", "Niloofar Yousefi"], "categories": ["cs.CL"], "comment": "Preprint paper", "summary": "The success of self-attention lies in its ability to capture long-range\ndependencies and enhance context understanding, but it is limited by its\ncomputational complexity and challenges in handling sequential data with\ninherent directionality. This work introduces a shared weight\nself-attention-based BERT model that only learns one weight matrix for (Key,\nValue, and Query) representations instead of three individual matrices for each\nof them. Our shared weight attention reduces the training parameter size by\nmore than half and training time by around one-tenth. Furthermore, we\ndemonstrate higher prediction accuracy on small tasks of GLUE over the BERT\nbaseline and in particular a generalization power on noisy and out-of-domain\ndata. Experimental results indicate that our shared self-attention method\nachieves a parameter size reduction of 66.53% in the attention block. In the\nGLUE dataset, the shared weight self-attention-based BERT model demonstrates\naccuracy improvements of 0.38%, 5.81%, and 1.06% over the standard, symmetric,\nand pairwise attention-based BERT models, respectively. The model and source\ncode are available at Anonymous."}
{"id": "2412.06926", "pdf": "https://arxiv.org/pdf/2412.06926.pdf", "abs": "https://arxiv.org/abs/2412.06926", "title": "When Every Token Counts: Optimal Segmentation for Low-Resource Language Models", "authors": ["Bharath Raj", "Garvit Suri", "Vikrant Dewangan", "Raghav Sonavane"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "LoResLM @ COLING 2025. Project page at\n  https://vikr-182.github.io/loreslm/", "summary": "Traditional greedy tokenization methods have been a critical step in Natural\nLanguage Processing (NLP), influencing how text is converted into tokens and\ndirectly impacting model performance. While subword tokenizers like Byte-Pair\nEncoding (BPE) are widely used, questions remain about their optimality across\nmodel scales and languages. In this work, we demonstrate through extensive\nexperiments that an optimal BPE configuration significantly reduces token count\ncompared to greedy segmentation, yielding improvements in token-saving\npercentages and performance benefits, particularly for smaller models. We\nevaluate tokenization performance across various intrinsic and extrinsic tasks,\nincluding generation and classification. Our findings suggest that\ncompression-optimized tokenization strategies could provide substantial\nadvantages for multilingual and low-resource language applications,\nhighlighting a promising direction for further research and inclusive NLP."}
{"id": "2412.10422", "pdf": "https://arxiv.org/pdf/2412.10422.pdf", "abs": "https://arxiv.org/abs/2412.10422", "title": "AutoPrep: Natural Language Question-Aware Data Preparation with a Multi-Agent Framework", "authors": ["Meihao Fan", "Ju Fan", "Nan Tang", "Lei Cao", "Guoliang Li", "Xiaoyong Du"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Answering natural language (NL) questions about tables, known as Tabular\nQuestion Answering (TQA), is crucial because it allows users to quickly and\nefficiently extract meaningful insights from structured data, effectively\nbridging the gap between human language and machine-readable formats. Many of\nthese tables are derived from web sources or real-world scenarios, which\nrequire meticulous data preparation (or data prep) to ensure accurate\nresponses. However, preparing such tables for NL questions introduces new\nrequirements that extend beyond traditional data preparation. This\nquestion-aware data preparation involves specific tasks such as column\nderivation and filtering tailored to particular questions, as well as\nquestion-aware value normalization or conversion, highlighting the need for a\nmore nuanced approach in this context. Because each of the above tasks is\nunique, a single model (or agent) may not perform effectively across all\nscenarios. In this paper, we propose AutoPrep, a large language model\n(LLM)-based multi-agent framework that leverages the strengths of multiple\nagents, each specialized in a certain type of data prep, ensuring more accurate\nand contextually relevant responses. Given an NL question over a table,\nAutoPrep performs data prep through three key components. Planner: Determines a\nlogical plan, outlining a sequence of high-level operations. Programmer:\nTranslates this logical plan into a physical plan by generating the\ncorresponding low-level code. Executor: Executes the generated code to process\nthe table. To support this multi-agent framework, we design a novel\nChain-of-Clauses reasoning mechanism for high-level operation suggestion, and a\ntool-augmented method for low-level code generation..."}
{"id": "2501.00070", "pdf": "https://arxiv.org/pdf/2501.00070.pdf", "abs": "https://arxiv.org/abs/2501.00070", "title": "ICLR: In-Context Learning of Representations", "authors": ["Core Francisco Park", "Andrew Lee", "Ekdeep Singh Lubana", "Yongyi Yang", "Maya Okawa", "Kento Nishi", "Martin Wattenberg", "Hidenori Tanaka"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICLR 2025", "summary": "Recent work has demonstrated that semantics specified by pretraining data\ninfluence how representations of different concepts are organized in a large\nlanguage model (LLM). However, given the open-ended nature of LLMs, e.g., their\nability to in-context learn, we can ask whether models alter these pretraining\nsemantics to adopt alternative, context-specified ones. Specifically, if we\nprovide in-context exemplars wherein a concept plays a different role than what\nthe pretraining data suggests, do models reorganize their representations in\naccordance with these novel semantics? To answer this question, we take\ninspiration from the theory of conceptual role semantics and define a toy\n\"graph tracing\" task wherein the nodes of the graph are referenced via concepts\nseen during training (e.g., apple, bird, etc.) and the connectivity of the\ngraph is defined via some predefined structure (e.g., a square grid). Given\nexemplars that indicate traces of random walks on the graph, we analyze\nintermediate representations of the model and find that as the amount of\ncontext is scaled, there is a sudden re-organization from pretrained semantic\nrepresentations to in-context representations aligned with the graph structure.\nFurther, we find that when reference concepts have correlations in their\nsemantics (e.g., Monday, Tuesday, etc.), the context-specified graph structure\nis still present in the representations, but is unable to dominate the\npretrained structure. To explain these results, we analogize our task to energy\nminimization for a predefined graph topology, providing evidence towards an\nimplicit optimization process to infer context-specified semantics. Overall,\nour findings indicate scaling context-size can flexibly re-organize model\nrepresentations, possibly unlocking novel capabilities."}
{"id": "2501.06382", "pdf": "https://arxiv.org/pdf/2501.06382.pdf", "abs": "https://arxiv.org/abs/2501.06382", "title": "Dynamics of Spontaneous Topic Changes in Next Token Prediction with Self-Attention", "authors": ["Mumin Jia", "Jairo Diaz-Rodriguez"], "categories": ["cs.CL", "cs.AI", "stat.ML"], "comment": null, "summary": "Human cognition is punctuated by abrupt, spontaneous shifts between\ntopics-driven by emotional, contextual, or associative cues-a phenomenon known\nas spontaneous thought in neuroscience. In contrast, self-attention based\nmodels depend on structured patterns over their inputs to predict each next\ntoken, lacking spontaneity. Motivated by this distinction, we characterize\nspontaneous topic changes in self-attention architectures, revealing both their\nsimilarities and their divergences from spontaneous human thought. First, we\nestablish theoretical results under a simplified, single-layer self-attention\nmodel with suitable conditions by defining the topic as a set of Token Priority\nGraphs (TPGs). Specifically, we demonstrate that (1) the model maintains the\npriority order of tokens related to the input topic, (2) a spontaneous topic\nchange can occur only if lower-priority tokens outnumber all higher-priority\ntokens of the input topic, and (3) unlike human cognition, the longer context\nlength or the more ambiguous input topic reduces the likelihood of spontaneous\nchange. Second, we empirically validate that these dynamics persist in modern,\nstate-of-the-art LLMs, underscoring a fundamental disparity between human\ncognition and AI behaviour in the context of spontaneous topic changes. To the\nbest of our knowledge, no prior work has explored these questions with a focus\nas closely aligned to human thought."}
{"id": "2501.19378", "pdf": "https://arxiv.org/pdf/2501.19378.pdf", "abs": "https://arxiv.org/abs/2501.19378", "title": "TableMaster: A Recipe to Advance Table Understanding with Language Models", "authors": ["Lang Cao", "Hanbing Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Tables serve as a fundamental format for representing structured relational\ndata. While current language models (LMs) excel at many text-based tasks, they\nstill face challenges in table understanding due to the complex characteristics\nof tabular data, such as their structured nature. In this paper, we aim to\nenhance LMs for improved table understanding. We identify four key challenges:\n1) difficulty in locating target data, 2) deficiency in table semantics, 3)\nnumerical inaccuracies in textual reasoning, and 4) semantic inflexibility in\nsymbolic reasoning. To address these issues, we propose TableMaster, a recipe\nand comprehensive framework that integrates multiple solutions to overcome\nthese obstacles. TableMaster first extracts relevant table content and\nverbalizes it with enriched semantic context. Additionally, we introduce\nadaptive reasoning, a flexible approach that dynamically adjusts between\ntextual and symbolic reasoning, tailoring the reasoning process to each query.\nExtensive analyses and experiments demonstrate our findings and the\neffectiveness of TableMaster. On the WikiTQ dataset, TableMaster achieves an\naccuracy of 78.13% using GPT-4o-mini, surpassing existing baselines."}
{"id": "2502.01976", "pdf": "https://arxiv.org/pdf/2502.01976.pdf", "abs": "https://arxiv.org/abs/2502.01976", "title": "CITER: Collaborative Inference for Efficient Large Language Model Decoding with Token-Level Routing", "authors": ["Wenhao Zheng", "Yixiao Chen", "Weitong Zhang", "Souvik Kundu", "Yun Li", "Zhengzhong Liu", "Eric P. Xing", "Hongyi Wang", "Huaxiu Yao"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF"], "comment": null, "summary": "Large language models have achieved remarkable success in various tasks but\nsuffer from high computational costs during inference, limiting their\ndeployment in resource-constrained applications. To address this issue, we\npropose a novel Collaborative Inference with Token-lEvel Routing (CITER)\nframework that enables efficient collaboration between small and large language\nmodels (SLMs \\& LLMs) through a token-level routing strategy. Specifically,\nCITER routes non-critical tokens to an SLM for efficiency and routes critical\ntokens to an LLM for generalization quality. We formulate router training as a\npolicy optimization, where the router receives rewards based on both the\nquality of predictions and the inference costs of generation. This allows the\nrouter to learn to predict token-level routing scores and make routing\ndecisions based on both the current token and the future impact of its\ndecisions. To further accelerate the reward evaluation process, we introduce a\nshortcut which significantly reduces the costs of the reward estimation and\nimproving the practicality of our approach. Extensive experiments on five\nbenchmark datasets demonstrate that CITER reduces the inference costs while\npreserving high-quality generation, offering a promising solution for real-time\nand resource-constrained applications. Our data and code are available at\nhttps://github.com/aiming-lab/CITER."}
{"id": "2504.13068", "pdf": "https://arxiv.org/pdf/2504.13068.pdf", "abs": "https://arxiv.org/abs/2504.13068", "title": "Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models", "authors": ["Sudesh Ramesh Bhagat", "Ibne Farabi Shihab", "Anuj Sharma"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study investigates the relationship between deep learning (DL) model\naccuracy and expert agreement in classifying crash narratives. We evaluate five\nDL models -- including BERT variants, USE, and a zero-shot classifier --\nagainst expert labels and narratives, and extend the analysis to four large\nlanguage models (LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our findings reveal\nan inverse relationship: models with higher technical accuracy often show lower\nagreement with human experts, while LLMs demonstrate stronger expert alignment\ndespite lower accuracy. We use Cohen's Kappa and Principal Component Analysis\n(PCA) to quantify and visualize model-expert agreement, and employ SHAP\nanalysis to explain misclassifications. Results show that expert-aligned models\nrely more on contextual and temporal cues than location-specific keywords.\nThese findings suggest that accuracy alone is insufficient for safety-critical\nNLP tasks. We argue for incorporating expert agreement into model evaluation\nframeworks and highlight the potential of LLMs as interpretable tools in crash\nanalysis pipelines."}
{"id": "2505.00016", "pdf": "https://arxiv.org/pdf/2505.00016.pdf", "abs": "https://arxiv.org/abs/2505.00016", "title": "Sparks of Tabular Reasoning via Text2SQL Reinforcement Learning", "authors": ["Josefa Lia Stoisser", "Marc Boubnovski Martell", "Julien Fauqueur"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This work reframes the Text-to-SQL task as a pathway for teaching large\nlanguage models (LLMs) to reason over and manipulate tabular data--moving\nbeyond the traditional focus on query generation. We propose a two-stage\nframework that leverages SQL supervision to develop transferable table\nreasoning capabilities. First, we synthesize detailed chain-of-thought (CoT)\ntraces from real-world SQL queries, providing step-by-step, clause-level\nsupervision that teaches the model how to traverse, filter, and aggregate table\nfields. Second, we introduce a Group Relative Policy Optimization (GRPO)\nreinforcement learning objective that connects SQL execution accuracy to\ngeneralizable reasoning by encouraging steps that extend beyond task-specific\nsyntax and transfer across datasets. Empirically, our approach improves\nperformance on standard Text-to-SQL benchmarks and achieves substantial gains\non reasoning-intensive datasets such as BIRD and CRT-QA, demonstrating enhanced\ngeneralization and interpretability. Specifically, the distilled-quantized\nLLaMA model achieved a relative 33.9\\% increase in accuracy when trained on\nText-to-SQL tasks, while Qwen achieved a relative 14.5\\% increase. These\nresults suggest that SQL can serve not only as a target formalism but also as\nan effective scaffold for learning robust, transferable reasoning over\nstructured data."}
{"id": "2505.00056", "pdf": "https://arxiv.org/pdf/2505.00056.pdf", "abs": "https://arxiv.org/abs/2505.00056", "title": "Clustering Internet Memes Through Template Matching and Multi-Dimensional Similarity", "authors": ["Tygo Bloem", "Filip Ilievski"], "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.MM"], "comment": null, "summary": "Meme clustering is critical for toxicity detection, virality modeling, and\ntyping, but it has received little attention in previous research. Clustering\nsimilar Internet memes is challenging due to their multimodality, cultural\ncontext, and adaptability. Existing approaches rely on databases, overlook\nsemantics, and struggle to handle diverse dimensions of similarity. This paper\nintroduces a novel method that uses template-based matching with\nmulti-dimensional similarity features, thus eliminating the need for predefined\ndatabases and supporting adaptive matching. Memes are clustered using local and\nglobal features across similarity categories such as form, visual content,\ntext, and identity. Our combined approach outperforms existing clustering\nmethods, producing more consistent and coherent clusters, while\nsimilarity-based feature sets enable adaptability and align with human\nintuition. We make all supporting code publicly available to support subsequent\nresearch."}
{"id": "2505.00551", "pdf": "https://arxiv.org/pdf/2505.00551.pdf", "abs": "https://arxiv.org/abs/2505.00551", "title": "100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models", "authors": ["Chong Zhang", "Yue Deng", "Xiang Lin", "Bin Wang", "Dianwen Ng", "Hai Ye", "Xingxuan Li", "Yao Xiao", "Zhanfeng Mo", "Qi Zhang", "Lidong Bing"], "categories": ["cs.CL"], "comment": null, "summary": "The recent development of reasoning language models (RLMs) represents a novel\nevolution in large language models. In particular, the recent release of\nDeepSeek-R1 has generated widespread social impact and sparked enthusiasm in\nthe research community for exploring the explicit reasoning paradigm of\nlanguage models. However, the implementation details of the released models\nhave not been fully open-sourced by DeepSeek, including DeepSeek-R1-Zero,\nDeepSeek-R1, and the distilled small models. As a result, many replication\nstudies have emerged aiming to reproduce the strong performance achieved by\nDeepSeek-R1, reaching comparable performance through similar training\nprocedures and fully open-source data resources. These works have investigated\nfeasible strategies for supervised fine-tuning (SFT) and reinforcement learning\nfrom verifiable rewards (RLVR), focusing on data preparation and method design,\nyielding various valuable insights. In this report, we provide a summary of\nrecent replication studies to inspire future research. We primarily focus on\nSFT and RLVR as two main directions, introducing the details for data\nconstruction, method design and training procedure of current replication\nstudies. Moreover, we conclude key findings from the implementation details and\nexperimental results reported by these studies, anticipating to inspire future\nresearch. We also discuss additional techniques of enhancing RLMs, highlighting\nthe potential of expanding the application scope of these models, and\ndiscussing the challenges in development. By this survey, we aim to help\nresearchers and developers of RLMs stay updated with the latest advancements,\nand seek to inspire new ideas to further enhance RLMs."}
{"id": "2311.09830", "pdf": "https://arxiv.org/pdf/2311.09830.pdf", "abs": "https://arxiv.org/abs/2311.09830", "title": "Automating the Generation of Prompts for LLM-based Action Choice in PDDL Planning", "authors": ["Katharina Stein", "Daniel Fišer", "Jörg Hoffmann", "Alexander Koller"], "categories": ["cs.AI", "cs.CL"], "comment": "Extended version of the paper from the ICAPS'25 proceedings (same\n  main part + additional appendix)", "summary": "Large language models (LLMs) have revolutionized a large variety of NLP\ntasks. An active debate is to what extent they can do reasoning and planning.\nPrior work has assessed the latter in the specific context of PDDL planning,\nbased on manually converting three PDDL domains into natural language (NL)\nprompts. Here we automate this conversion step, showing how to leverage an LLM\nto automatically generate NL prompts from PDDL input. Our automatically\ngenerated NL prompts result in similar LLM-planning performance as the previous\nmanually generated ones. Beyond this, the automation enables us to run much\nlarger experiments, providing for the first time a broad evaluation of LLM\nplanning performance in PDDL. Our NL prompts yield better performance than PDDL\nprompts and simple template-based NL prompts. Compared to symbolic planners,\nLLM planning lags far behind; but in some domains, our best LLM configuration\nscales up further than A$^\\star$ using LM-cut."}
{"id": "2402.18789", "pdf": "https://arxiv.org/pdf/2402.18789.pdf", "abs": "https://arxiv.org/abs/2402.18789", "title": "FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning", "authors": ["Gabriele Oliaro", "Xupeng Miao", "Xinhao Cheng", "Vineeth Kada", "Ruohan Gao", "Yingyi Huang", "Remi Delacourt", "April Yang", "Yingcheng Wang", "Mengdi Wu", "Colin Unger", "Zhihao Jia"], "categories": ["cs.DC", "cs.CL", "cs.LG"], "comment": null, "summary": "Finetuning large language models (LLMs) is essential for task adaptation, yet\nserving stacks today isolate inference and finetuning on separate GPU clusters\n-- wasting resources and under-utilizing hardware. We introduce FlexLLM, the\nfirst system to co-serve LLM inference and PEFT-based finetuning on shared GPUs\nby fusing computation at the token level. The static compilation optimizations\nin FlexLLM -- dependent parallelization and graph pruning significantly shrink\nactivation memory, leading to end-to-end GPU memory savings by up to 80%. At\nruntime, a novel token-level finetuning mechanism paired with a hybrid token\nscheduler dynamically interleaves inference and training tokens within each\nco-serving iteration, meeting strict latency SLOs while maximizing utilization.\nIn end-to-end benchmarks on LLaMA-3.1-8B, Qwen-2.5-14B, and Qwen-2.5-32B,\nFlexLLM sustains the inference SLO requirements up to 20 req/s, and improves\nfinetuning throughput by 1.9-4.8x under heavy inference workloads and 2.5-6.8x\nunder light loads, preserving over 76% of peak finetuning progress even at peak\ndemand. The source code of FlexLLM is publicly available at\nhttps://github.com/flexflow/FlexFlow/."}
{"id": "2403.04577", "pdf": "https://arxiv.org/pdf/2403.04577.pdf", "abs": "https://arxiv.org/abs/2403.04577", "title": "Wiki-TabNER: Integrating Named Entity Recognition into Wikipedia Tables", "authors": ["Aneta Koleva", "Martin Ringsquandl", "Ahmed Hatem", "Thomas Runkler", "Volker Tresp"], "categories": ["cs.AI", "cs.CL"], "comment": "Accepted at SIGIR 2025 conference", "summary": "Interest in solving table interpretation tasks has grown over the years, yet\nit still relies on existing datasets that may be overly simplified. This is\npotentially reducing the effectiveness of the dataset for thorough evaluation\nand failing to accurately represent tables as they appear in the real-world. To\nenrich the existing benchmark datasets, we extract and annotate a new, more\nchallenging dataset. The proposed Wiki-TabNER dataset features complex tables\ncontaining several entities per cell, with named entities labeled using DBpedia\nclasses. This dataset is specifically designed to address named entity\nrecognition (NER) task within tables, but it can also be used as a more\nchallenging dataset for evaluating the entity linking task. In this paper we\ndescribe the distinguishing features of the Wiki-TabNER dataset and the\nlabeling process. In addition, we propose a prompting framework for evaluating\nthe new large language models on the within tables NER task. Finally, we\nperform qualitative analysis to gain insights into the challenges encountered\nby the models and to understand the limitations of the proposed~dataset."}
{"id": "2408.09632", "pdf": "https://arxiv.org/pdf/2408.09632.pdf", "abs": "https://arxiv.org/abs/2408.09632", "title": "MoDeGPT: Modular Decomposition for Large Language Model Compression", "authors": ["Chi-Heng Lin", "Shangqian Gao", "James Seale Smith", "Abhishek Patel", "Shikhar Tuli", "Yilin Shen", "Hongxia Jin", "Yen-Chang Hsu"], "categories": ["cs.LG", "cs.CL", "stat.ML", "15A23 (Primary)", "I.2.7"], "comment": "ICLR 2025 Oral", "summary": "Large Language Models (LLMs) have reshaped the landscape of artificial\nintelligence by demonstrating exceptional performance across various tasks.\nHowever, substantial computational requirements make their deployment\nchallenging on devices with limited resources. Recently, compression methods\nusing low-rank matrix techniques have shown promise, yet these often lead to\ndegraded accuracy or introduce significant overhead in parameters and inference\nlatency. This paper introduces \\textbf{Mo}dular \\textbf{De}composition\n(MoDeGPT), a novel structured compression framework that does not need recovery\nfine-tuning while resolving the above drawbacks. MoDeGPT partitions the\nTransformer block into modules comprised of matrix pairs and reduces the hidden\ndimensions via reconstructing the module-level outputs. MoDeGPT is developed\nbased on a theoretical framework that utilizes three well-established matrix\ndecomposition algorithms -- Nystr\\\"om approximation, CR decomposition, and SVD\n-- and applies them to our redefined transformer modules. Our comprehensive\nexperiments show MoDeGPT, without backward propagation, matches or surpasses\nprevious structured compression methods that rely on gradient information, and\nsaves 98% of compute costs on compressing a 13B model. On \\textsc{Llama}-2/3\nand OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30%\ncompression rates. Moreover, the compression can be done on a single GPU within\na few hours and increases the inference throughput by up to 46%."}
{"id": "2412.01003", "pdf": "https://arxiv.org/pdf/2412.01003.pdf", "abs": "https://arxiv.org/abs/2412.01003", "title": "Competition Dynamics Shape Algorithmic Phases of In-Context Learning", "authors": ["Core Francisco Park", "Ekdeep Singh Lubana", "Itamar Pres", "Hidenori Tanaka"], "categories": ["cs.LG", "cs.CL"], "comment": "ICLR 2025 Spotlight", "summary": "In-Context Learning (ICL) has significantly expanded the general-purpose\nnature of large language models, allowing them to adapt to novel tasks using\nmerely the inputted context. This has motivated a series of papers that analyze\ntractable synthetic domains and postulate precise mechanisms that may underlie\nICL. However, the use of relatively distinct setups that often lack a sequence\nmodeling nature to them makes it unclear how general the reported insights from\nsuch studies are. Motivated by this, we propose a synthetic sequence modeling\ntask that involves learning to simulate a finite mixture of Markov chains. As\nwe show, models trained on this task reproduce most well-known results on ICL,\nhence offering a unified setting for studying the concept. Building on this\nsetup, we demonstrate we can explain a model's behavior by decomposing it into\nfour broad algorithms that combine a fuzzy retrieval vs. inference approach\nwith either unigram or bigram statistics of the context. These algorithms\nengage in a competition dynamics to dominate model behavior, with the precise\nexperimental conditions dictating which algorithm ends up superseding others:\ne.g., we find merely varying context size or amount of training yields (at\ntimes sharp) transitions between which algorithm dictates the model behavior,\nrevealing a mechanism that explains the transient nature of ICL. In this sense,\nwe argue ICL is best thought of as a mixture of different algorithms, each with\nits own peculiarities, instead of a monolithic capability. This also implies\nthat making general claims about ICL that hold universally across all settings\nmay be infeasible."}
{"id": "2412.07446", "pdf": "https://arxiv.org/pdf/2412.07446.pdf", "abs": "https://arxiv.org/abs/2412.07446", "title": "A Causal World Model Underlying Next Token Prediction: Exploring GPT in a Controlled Environment", "authors": ["Raanan Y. Rohekar", "Yaniv Gurwicz", "Sungduk Yu", "Estelle Aflalo", "Vasudev Lal"], "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ML"], "comment": "International Conference on Machine Learning (ICML), 2025", "summary": "Do generative pre-trained transformer (GPT) models, trained only to predict\nthe next token, implicitly learn a world model from which a sequence is\ngenerated one token at a time? We address this question by deriving a causal\ninterpretation of the attention mechanism in GPT, and suggesting a causal world\nmodel that arises from this interpretation. Furthermore, we propose that GPT\nmodels, at inference time, can be utilized for zero-shot causal structure\nlearning for input sequences and present a confidence score. Empirical\nevaluation is conducted in a controlled environment using the setup and rules\nof the Othello and Chess strategy games. A GPT, pre-trained on real-world games\nplayed with the intention of winning, is tested on out-of-distribution\nsynthetic data consisting of sequences of random legal moves. We find that the\nGPT model is likely to generate legal next moves for out-of-distribution\nsequences for which a causal structure is encoded in the attention mechanism\nwith high confidence. In cases for which the GPT model generates illegal moves\nit also fails to capture any causal structure."}
{"id": "2501.13100", "pdf": "https://arxiv.org/pdf/2501.13100.pdf", "abs": "https://arxiv.org/abs/2501.13100", "title": "A Rate-Distortion Framework for Summarization", "authors": ["Enes Arda", "Aylin Yener"], "categories": ["cs.IT", "cs.CL", "cs.LG", "math.IT"], "comment": "Accepted to ISIT 2025. This arXiv version includes an appendix with\n  additional details", "summary": "This paper introduces an information-theoretic framework for text\nsummarization. We define the summarizer rate-distortion function and show that\nit provides a fundamental lower bound on summarizer performance. We describe an\niterative procedure, similar to Blahut-Arimoto algorithm, for computing this\nfunction. To handle real-world text datasets, we also propose a practical\nmethod that can calculate the summarizer rate-distortion function with limited\ndata. Finally, we empirically confirm our theoretical results by comparing the\nsummarizer rate-distortion function with the performances of different\nsummarizers used in practice."}
{"id": "2502.15507", "pdf": "https://arxiv.org/pdf/2502.15507.pdf", "abs": "https://arxiv.org/abs/2502.15507", "title": "Activation Steering in Neural Theorem Provers", "authors": ["Shashank Kirtania"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "incorrect explanation for a concept, need to revise and update!", "summary": "Large Language Models (LLMs) have shown promise in proving formal theorems\nusing proof assistants like Lean. However, current state of the art language\nmodels struggles to predict next step in proofs leading practitioners to use\ndifferent sampling techniques to improve LLMs capabilities. We observe that the\nLLM is capable of predicting the correct tactic; however, it faces challenges\nin ranking it appropriately within the set of candidate tactics, affecting the\noverall selection process. To overcome this hurdle, we use activation steering\nto guide LLMs responses to improve the generations at the time of inference.\nOur results suggest that activation steering offers a promising lightweight\nalternative to specialized fine-tuning for enhancing theorem proving\ncapabilities in LLMs, particularly valuable in resource-constrained\nenvironments."}
{"id": "2504.21035", "pdf": "https://arxiv.org/pdf/2504.21035.pdf", "abs": "https://arxiv.org/abs/2504.21035", "title": "A False Sense of Privacy: Evaluating Textual Data Sanitization Beyond Surface-level Privacy Leakage", "authors": ["Rui Xin", "Niloofar Mireshghallah", "Shuyue Stella Li", "Michael Duan", "Hyunwoo Kim", "Yejin Choi", "Yulia Tsvetkov", "Sewoong Oh", "Pang Wei Koh"], "categories": ["cs.CR", "cs.CL", "cs.LG"], "comment": null, "summary": "Sanitizing sensitive text data typically involves removing personally\nidentifiable information (PII) or generating synthetic data under the\nassumption that these methods adequately protect privacy; however, their\neffectiveness is often only assessed by measuring the leakage of explicit\nidentifiers but ignoring nuanced textual markers that can lead to\nre-identification. We challenge the above illusion of privacy by proposing a\nnew framework that evaluates re-identification attacks to quantify individual\nprivacy risks upon data release. Our approach shows that seemingly innocuous\nauxiliary information -- such as routine social activities -- can be used to\ninfer sensitive attributes like age or substance use history from sanitized\ndata. For instance, we demonstrate that Azure's commercial PII removal tool\nfails to protect 74\\% of information in the MedQA dataset. Although\ndifferential privacy mitigates these risks to some extent, it significantly\nreduces the utility of the sanitized text for downstream tasks. Our findings\nindicate that current sanitization techniques offer a \\textit{false sense of\nprivacy}, highlighting the need for more robust methods that protect against\nsemantic-level information leakage."}
{"id": "2505.00234", "pdf": "https://arxiv.org/pdf/2505.00234.pdf", "abs": "https://arxiv.org/abs/2505.00234", "title": "Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks", "authors": ["Vishnu Sarukkai", "Zhiqiang Xie", "Kayvon Fatahalian"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Many methods for improving Large Language Model (LLM) agents for sequential\ndecision-making tasks depend on task-specific knowledge engineering--such as\nprompt tuning, curated in-context examples, or customized observation and\naction spaces. Using these approaches, agent performance improves with the\nquality or amount of knowledge engineering invested. Instead, we investigate\nhow LLM agents can automatically improve their performance by learning\nin-context from their own successful experiences on similar tasks. Rather than\nrelying on task-specific knowledge engineering, we focus on constructing and\nrefining a database of self-generated examples. We demonstrate that even a\nnaive accumulation of successful trajectories across training tasks boosts test\nperformance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%),\nand InterCode-SQL (75% to 79%)--matching the performance the initial agent\nachieves if allowed two to three attempts per task. We then introduce two\nextensions: (1) database-level selection through population-based training to\nidentify high-performing example collections, and (2) exemplar-level selection\nthat retains individual trajectories based on their empirical utility as\nin-context examples. These extensions further enhance performance, achieving\n91% on ALFWorld--matching more complex approaches that employ task-specific\ncomponents and prompts. Our results demonstrate that automatic trajectory\ndatabase construction offers a compelling alternative to labor-intensive\nknowledge engineering."}
