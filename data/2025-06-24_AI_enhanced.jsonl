{"id": "2506.17494", "pdf": "https://arxiv.org/pdf/2506.17494.pdf", "abs": "https://arxiv.org/abs/2506.17494", "title": "Making the Right Thing: Bridging HCI and Responsible AI in Early-Stage AI Concept Selection", "authors": ["Ji-Youn Jung", "Devansh Saxena", "Minjung Park", "Jini Kim", "Jodi Forlizzi", "Kenneth Holstein", "John Zimmerman"], "categories": ["cs.HC"], "comment": "Accepted for publication in Designing Interactive Systems Conference\n  (DIS '25), July 5--9, 2025, Funchal, Portugal. ACM, New York, NY, USA, 21\n  pages", "summary": "AI projects often fail due to financial, technical, ethical, or user\nacceptance challenges -- failures frequently rooted in early-stage decisions.\nWhile HCI and Responsible AI (RAI) research emphasize this, practical\napproaches for identifying promising concepts early remain limited. Drawing on\nResearch through Design, this paper investigates how early-stage AI concept\nsorting in commercial settings can reflect RAI principles. Through three design\nexperiments -- including a probe study with industry practitioners -- we\nexplored methods for evaluating risks and benefits using multidisciplinary\ncollaboration. Participants demonstrated strong receptivity to addressing RAI\nconcerns early in the process and effectively identified low-risk, high-benefit\nAI concepts. Our findings highlight the potential of a design-led approach to\nembed ethical and service design thinking at the front end of AI innovation. By\nexamining how practitioners reason about AI concepts, our study invites HCI and\nRAI communities to see early-stage innovation as a critical space for engaging\nethical and commercial considerations together.", "AI": {"tldr": "This paper explores early-stage AI concept sorting in commercial settings through design experiments, emphasizing the need for Responsible AI (RAI) principles in the process.", "motivation": "Many AI projects fail due to early-stage decisions influenced by financial, technical, ethical, or user acceptance issues, highlighting the need for a structured approach to identify promising concepts early.", "method": "The paper employs Research through Design, conducting three design experiments including a probe study with industry practitioners to evaluate risks and benefits of AI concepts.", "result": "Participants identified low-risk, high-benefit AI concepts and expressed a willingness to address RAI concerns early in the design process, demonstrating the efficacy of a design-led approach.", "conclusion": "The findings suggest that embedding ethical and service design thinking at the front end of AI innovation can improve early-stage decision-making and align with RAI principles.", "key_contributions": ["Introduces practical methods for integrating RAI principles in early AI concept development.", "Validates the significance of multidisciplinary collaboration in evaluating AI risks and benefits.", "Advocates for a design-led approach to enhance ethical considerations in AI innovation."], "limitations": "The study focuses on a specific commercial context and may not generalize to all AI applications or industries.", "keywords": ["Responsible AI", "Human-Computer Interaction", "Design through Research", "AI concept sorting", "Ethical considerations"], "importance_score": 9, "read_time_minutes": 21}}
{"id": "2506.17606", "pdf": "https://arxiv.org/pdf/2506.17606.pdf", "abs": "https://arxiv.org/abs/2506.17606", "title": "Full-body WPT: wireless powering with meandered e-textiles", "authors": ["Ryo Takahashi", "Takashi Sato", "Wakako Yukita", "Tomoyuki Yokota", "Takao Someya", "Yoshihiro Kawahara"], "categories": ["cs.HC"], "comment": null, "summary": "We present Full-body WPT, wireless power networking around the human body\nusing a meandered textile coil. Unlike traditional inductive systems that emit\nstrong fields into the deep tissue inside the body, the meander coil enables\nlocalized generation of strong magnetic field constrained to the skin surface,\neven when scaled to the size of the human body. Such localized inductive system\nenhances both safety and efficiency of wireless power around the body.\nFurthermore, the use of low-loss conductive yarn achieve energy-efficient and\nlightweight design. We analyze the performance of our design through\nsimulations and experimental prototypes, demonstrating high power transfer\nefficiency and adaptability to user movement and posture. Our system provides a\nsafe and efficient distributed power network using meandered textile coils\nintegrated into wearable materials, highlighting the potential of body-centric\nwireless power networking as a foundational layer for ubiquitous health\nmonitoring, augmented reality, and human-machine interaction systems.", "AI": {"tldr": "Presentation of Full-body WPT using meandered textile coils for efficient wireless power transfer around the human body, enhancing safety and adaptability for applications in health monitoring and HCI.", "motivation": "To enhance wireless power transfer safety and efficiency around the human body for applications in health monitoring and human-machine interaction.", "method": "Utilization of meandered textile coils to create localized magnetic fields at the skin surface, coupled with low-loss conductive yarn for energy-efficient design.", "result": "High power transfer efficiency and adaptability to user movements demonstrated through simulations and experimental prototypes.", "conclusion": "The system offers a foundation for ubiquitous applications in health monitoring, augmented reality, and HCI, emphasizing the role of body-centric wireless power networking.", "key_contributions": ["Development of a safe and efficient full-body wireless power transfer system", "Integration of meandered textile coils into wearable materials", "Demonstration of adaptability to user movement and posture"], "limitations": "", "keywords": ["wireless power transfer", "human-machine interaction", "ubiquitous computing"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.17890", "pdf": "https://arxiv.org/pdf/2506.17890.pdf", "abs": "https://arxiv.org/abs/2506.17890", "title": "One Does Not Simply 'Mm-hmm': Exploring Backchanneling in the AAC Micro-Culture", "authors": ["Tobias Weinberg", "Claire O'Connor", "Ricardo E. Gonzalez Penuela", "Stephanie Valencia", "Thijs Roumen"], "categories": ["cs.HC"], "comment": "See our project and video at:\n  https://tobiwg.com/research/one_does_not_simply_hm-hmm/", "summary": "Backchanneling (e.g., \"uh-huh\", \"hmm\", a simple nod) encompasses a big part\nof everyday communication; it is how we negotiate the turn to speak, it signals\nour engagement, and shapes the flow of our conversations. For people with\nspeech and motor impairments, backchanneling is limited to a reduced set of\nmodalities, and their Augmentative and Alternative Communication (AAC)\ntechnology requires visual attention, making it harder to observe non-verbal\ncues of conversation partners. We explore how users of AAC technology approach\nbackchanneling and create their own unique channels and communication culture.\nWe conducted a workshop with 4 AAC users to understand the unique\ncharacteristics of backchanneling in AAC. We explored how backchanneling\nchanges when pairs of AAC users communicate vs when an AAC user communicates\nwith a non-AAC user. We contextualize these findings through four in-depth\ninterviews with speech-language pathologists (SLPs). We conclude with a\ndiscussion about backchanneling as a micro-cultural practice, rethinking\nembodiment and mediation in AAC technology, and providing design\nrecommendations for timely multi-modal backchanneling while respecting\ndifferent communication cultures.", "AI": {"tldr": "The paper explores backchanneling in Augmentative and Alternative Communication (AAC) users, focusing on their communication culture and unique practices.", "motivation": "To investigate how AAC users navigate backchanneling in communication, as traditional modalities often restrict their engagement and interaction cues due to their impairments.", "method": "We conducted a workshop with 4 AAC users and performed four in-depth interviews with speech-language pathologists to gather insights on backchanneling practices in AAC contexts.", "result": "The study reveals unique characteristics of backchanneling between AAC users and contrasts it with interactions involving non-AAC users, highlighting the importance of understanding communication culture in AAC.", "conclusion": "The findings suggest a need to rethink the embodiment and mediation within AAC technology, providing design recommendations for facilitating multi-modal backchanneling that reflect diverse communication cultures.", "key_contributions": ["Insights into backchanneling practices among AAC users", "Comparison of communication dynamics between AAC and non-AAC users", "Design recommendations for AAC technology to support diverse communication styles"], "limitations": "", "keywords": ["backchanneling", "Augmentative and Alternative Communication", "communication culture", "human-computer interaction", "design recommendations"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.17936", "pdf": "https://arxiv.org/pdf/2506.17936.pdf", "abs": "https://arxiv.org/abs/2506.17936", "title": "When concept-based XAI is imprecise: Do people distinguish between generalisations and misrepresentations?", "authors": ["Romy MÃ¼ller"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Concept-based explainable artificial intelligence (C-XAI) can help reveal the\ninner representations of AI models. Understanding these representations is\nparticularly important in complex tasks like safety evaluation. Such tasks rely\non high-level semantic information (e.g., about actions) to make decisions\nabout abstract categories (e.g., whether a situation is dangerous). In this\ncontext, it may desirable for C-XAI concepts to show some variability,\nsuggesting that the AI is capable of generalising beyond the concrete details\nof a situation. However, it is unclear whether people recognise and appreciate\nsuch generalisations and can distinguish them from other, less desirable forms\nof imprecision. This was investigated in an experimental railway safety\nscenario. Participants evaluated the performance of a simulated AI that\nevaluated whether traffic scenes involving people were dangerous. To explain\nthese decisions, the AI provided concepts in the form of similar image\nsnippets. These concepts differed in their match with the classified image,\neither regarding a highly relevant feature (i.e., relation to tracks) or a less\nrelevant feature (i.e., actions). Contrary to the hypotheses, concepts that\ngeneralised over less relevant features led to ratings that were lower than for\nprecisely matching concepts and comparable to concepts that systematically\nmisrepresented these features. Conversely, participants were highly sensitive\nto imprecisions in relevant features. These findings cast doubts on whether\npeople spontaneously recognise generalisations. Accordingly, they might not be\nable to infer from C-XAI concepts whether AI models have gained a deeper\nunderstanding of complex situations.", "AI": {"tldr": "The study investigates how participants evaluate generalizations presented by C-XAI in the context of AI safety evaluation, revealing sensitivity to imprecision in relevant features.", "motivation": "Understanding if people can recognize generalizations by C-XAI concepts in safety evaluation tasks is crucial for the development of effective explainable AI systems.", "method": "An experimental railway safety scenario where participants evaluated a simulated AI's performance based on image snippets representing abstract categories of danger.", "result": "Participants rated concepts generalizing over less relevant features lower than precisely matching concepts, indicating sensitivity to imprecisions.", "conclusion": "The findings challenge assumptions that people can recognize generalizations in C-XAI, raising questions about its utility in AI safety evaluations.", "key_contributions": ["Investigates human perception of C-XAI concepts in safety evaluation scenarios.", "Reveals sensitivity to relevant feature imprecision in AI explanations.", "Challenges the effectiveness of C-XAI in conveying a deeper understanding of AI models."], "limitations": "Study limited to a controlled experimental scenario, may not generalize to all contexts of AI explanation.", "keywords": ["explainable AI", "C-XAI", "human perception", "safety evaluation", "generalization"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.17223", "pdf": "https://arxiv.org/pdf/2506.17223.pdf", "abs": "https://arxiv.org/abs/2506.17223", "title": "Outcome-Based Education: Evaluating Students' Perspectives Using Transformer", "authors": ["Shuvra Smaran Das", "Anirban Saha Anik", "Md Kishor Morol", "Mohammad Sakib Mahmood"], "categories": ["cs.CL"], "comment": "6 pages, 7 figures", "summary": "Outcome-Based Education (OBE) emphasizes the development of specific\ncompetencies through student-centered learning. In this study, we reviewed the\nimportance of OBE and implemented transformer-based models, particularly\nDistilBERT, to analyze an NLP dataset that includes student feedback. Our\nobjective is to assess and improve educational outcomes. Our approach is better\nthan other machine learning models because it uses the transformer's deep\nunderstanding of language context to classify sentiment better, giving better\nresults across a wider range of matrices. Our work directly contributes to\nOBE's goal of achieving measurable outcomes by facilitating the identification\nof patterns in student learning experiences. We have also applied LIME (local\ninterpretable model-agnostic explanations) to make sure that model predictions\nare clear. This gives us understandable information about how key terms affect\nsentiment. Our findings indicate that the combination of transformer models and\nLIME explanations results in a strong and straightforward framework for\nanalyzing student feedback. This aligns more closely with the principles of OBE\nand ensures the improvement of educational practices through data-driven\ninsights.", "AI": {"tldr": "The study implements transformer-based models, specifically DistilBERT, to analyze student feedback for improving educational outcomes in Outcome-Based Education (OBE).", "motivation": "To assess and improve educational outcomes through the analysis of student feedback using NLP techniques.", "method": "Implemented transformer-based models, particularly DistilBERT, and used LIME to interpret model predictions.", "result": "The approach outperforms other machine learning models in sentiment classification and provides clear insights into how key terms influence sentiments.", "conclusion": "The combination of transformer models and LIME offers a robust framework for analyzing student feedback, leading to improved educational practices through data-driven insights.", "key_contributions": ["Utilization of DistilBERT for sentiment analysis in educational feedback", "Application of LIME for interpretability of model predictions", "Alignment of analysis with the principles of Outcome-Based Education (OBE)"], "limitations": "", "keywords": ["Outcome-Based Education", "DistilBERT", "sentiment analysis", "LIME", "NLP"], "importance_score": 7, "read_time_minutes": 6}}
{"id": "2506.18119", "pdf": "https://arxiv.org/pdf/2506.18119.pdf", "abs": "https://arxiv.org/abs/2506.18119", "title": "Conceptualization, Operationalization, and Measurement of Machine Companionship: A Scoping Review", "authors": ["Jaime Banks", "Zhixin Li"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The notion of machine companions has long been embedded in\nsocial-technological imaginaries. Recent advances in AI have moved those media\nmusings into believable sociality manifested in interfaces, robotic bodies, and\ndevices. Those machines are often referred to colloquially as \"companions\" yet\nthere is little careful engagement of machine companionship (MC) as a formal\nconcept or measured variable. This PRISMA-guided scoping review systematically\nsamples, surveys, and synthesizes current scholarly works on MC (N = 71;\n2017-2025), to that end. Works varied widely in considerations of MC according\nto guiding theories, dimensions of a-priori specified properties (subjectively\npositive, sustained over time, co-active, autotelic), and in measured concepts\n(with more than 50 distinct measured variables). WE ultimately offer a\nliterature-guided definition of MC as an autotelic, coordinated connection\nbetween human and machine that unfolds over time and is subjectively positive.", "AI": {"tldr": "This paper provides a systematic review of machine companionship (MC), offering a literature-guided definition and identifying various dimensions and measurements of MC.", "motivation": "To explore the formal concept of machine companionship (MC) and its implications in technology and social interactions, as it has not been sufficiently engaged in existing literature.", "method": "A PRISMA-guided scoping review was conducted, sampling and synthesizing 71 scholarly works on MC from 2017 to 2025.", "result": "The review highlighted a wide variety of approaches to MC with over 50 distinct measurable variables, leading to a proposed definition of MC as an autotelic, coordinated human-machine connection.", "conclusion": "The findings will help in clarifying the concept of MC and encourage future research on its implications in HCI and related fields.", "key_contributions": ["Systematic review of the concept of machine companionship.", "Identification of over 50 distinct measured variables related to MC.", "Proposed literature-guided definition of machine companionship."], "limitations": "The review is limited to works published between 2017-2025; may not capture all aspects of MC.", "keywords": ["machine companionship", "HCI", "systematic review", "AI", "social interactions"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.17231", "pdf": "https://arxiv.org/pdf/2506.17231.pdf", "abs": "https://arxiv.org/abs/2506.17231", "title": "Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs", "authors": ["Xiang Li", "Chong Zhang", "Jia Wang", "Fangyu Wu", "Yushi Li", "Xiaobo Jin"], "categories": ["cs.CL", "cs.CR"], "comment": "15 pages, 5 figures", "summary": "Attacks on large language models (LLMs) in jailbreaking scenarios raise many\nsecurity and ethical issues. Current jailbreak attack methods face problems\nsuch as low efficiency, high computational cost, and poor cross-model\nadaptability and versatility, which make it difficult to cope with the rapid\ndevelopment of LLM and new defense strategies. Our work proposes an Adversarial\nPrompt Distillation, which combines masked language modeling, reinforcement\nlearning, and dynamic temperature control through a prompt generation and\ndistillation method. It enables small language models (SLMs) to jailbreak\nattacks on mainstream LLMs. The experimental results verify the superiority of\nthe proposed method in terms of attack success rate and harm, and reflect the\nresource efficiency and cross-model adaptability. This research explores the\nfeasibility of distilling the jailbreak ability of LLM to SLM, reveals the\nmodel's vulnerability, and provides a new idea for LLM security research.", "AI": {"tldr": "This paper proposes an Adversarial Prompt Distillation method to enhance the efficiency and adaptability of jailbreak attacks on large language models (LLMs) using small language models (SLMs).", "motivation": "Current jailbreak methods for LLMs face issues such as inefficiency, high computational costs, and poor adaptability, hindering their effectiveness against evolving defenses.", "method": "The proposed method combines masked language modeling, reinforcement learning, and dynamic temperature control in a prompt generation and distillation framework to enable SLMs to conduct jailbreak attacks on LLMs.", "result": "Experimental results show that the Adversarial Prompt Distillation method significantly improves the attack success rate and harmful impact while being resource-efficient and adaptable across different models.", "conclusion": "The research highlights the potential to distill jailbreak capabilities from LLMs to SLMs, showcasing vulnerabilities in models and providing a new direction for security research in LLMs.", "key_contributions": ["Introduction of Adversarial Prompt Distillation for jailbreak attacks", "Demonstrated improved attack success rates and adaptability", "Provided insights into LLM vulnerabilities for security research"], "limitations": "", "keywords": ["large language models", "jailbreaking", "adversarial prompt distillation", "security", "reinforcement learning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.18143", "pdf": "https://arxiv.org/pdf/2506.18143.pdf", "abs": "https://arxiv.org/abs/2506.18143", "title": "AI Harmonizer: Expanding Vocal Expression with a Generative Neurosymbolic Music AI System", "authors": ["Lancelot Blanchard", "Cameron Holt", "Joseph A. Paradiso"], "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS", "H.5.5"], "comment": "4 pages, 3 figures", "summary": "Vocals harmonizers are powerful tools to help solo vocalists enrich their\nmelodies with harmonically supportive voices. These tools exist in various\nforms, from commercially available pedals and software to custom-built systems,\neach employing different methods to generate harmonies. Traditional harmonizers\noften require users to manually specify a key or tonal center, while others\nallow pitch selection via an external keyboard-both approaches demanding some\ndegree of musical expertise. The AI Harmonizer introduces a novel approach by\nautonomously generating musically coherent four-part harmonies without\nrequiring prior harmonic input from the user. By integrating state-of-the-art\ngenerative AI techniques for pitch detection and voice modeling with\ncustom-trained symbolic music models, our system arranges any vocal melody into\nrich choral textures. In this paper, we present our methods, explore potential\napplications in performance and composition, and discuss future directions for\nreal-time implementations. While our system currently operates offline, we\nbelieve it represents a significant step toward AI-assisted vocal performance\nand expressive musical augmentation. We release our implementation on GitHub.", "AI": {"tldr": "This paper presents the AI Harmonizer, an innovative tool that autonomously generates four-part harmonies for vocalists without requiring prior harmonic input. It utilizes generative AI techniques for pitch detection and voice modeling.", "motivation": "To create a vocal harmonizer that simplifies the process of generating harmonies for solo vocalists, eliminating the need for musical expertise related to key specification.", "method": "The AI Harmonizer integrates generative AI techniques with custom-trained symbolic music models to arrange vocal melodies into choral textures autonomously.", "result": "The system successfully generates coherent four-part harmonies based on any provided vocal melody, enhancing musical performance and composition.", "conclusion": "The AI Harmonizer represents a significant advancement in AI-assisted vocal performance and expresses potential for real-time implementations. The implementation is available on GitHub.", "key_contributions": ["Introduction of a fully autonomous vocal harmonization system", "Integration of generative AI techniques with symbolic music models", "Potential applications in both performance and composition"], "limitations": "Currently operates offline; future work needed for real-time functionalities.", "keywords": ["AI Harmonizer", "vocal harmonization", "generative AI", "musical composition", "performance enhancement"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2506.17286", "pdf": "https://arxiv.org/pdf/2506.17286.pdf", "abs": "https://arxiv.org/abs/2506.17286", "title": "GTA: Grouped-head latenT Attention", "authors": ["Luoyang Sun", "Jiwen Jiang", "Cheng Deng", "Xinjian Wu", "Haifeng Zhang", "Lei Chen", "Lionel Ni", "Jun Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint.", "AI": {"tldr": "Grouped-Head Latent Attention (GTA) optimizes attention mechanisms in LLMs by reducing memory usage and computational complexity while improving performance.", "motivation": "The paper addresses computational and memory overhead in large language models due to expensive attention mechanisms, especially as text length increases.", "method": "The proposed GTA mechanism includes a shared attention map that reduces key cache size and a nonlinear value decoder to compress the value cache, leading to less resource consumption.", "result": "GTA decreases attention computation FLOPs by up to 62.5% and shrinks the KV cache by up to 70%, resulting in a 2x increase in end-to-end inference speed.", "conclusion": "GTA enhances LLM efficiency, making it more viable for deployment on resource-constrained hardware.", "key_contributions": ["Introduction of Grouped-Head Latent Attention (GTA) that shares attention maps across heads.", "Significant reductions in memory and computation overhead for LLMs.", "Demonstrated 2x increase in inference speed without additional overhead."], "limitations": "", "keywords": ["attention mechanisms", "large language models", "computational efficiency", "memory optimization", "GTA"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.18196", "pdf": "https://arxiv.org/pdf/2506.18196.pdf", "abs": "https://arxiv.org/abs/2506.18196", "title": "Two Sonification Methods for the MindCube", "authors": ["Fangzheng Liu", "Lancelot Blanchard", "Don D. Haddad", "Joseph A. Paradiso"], "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS", "H.5.5"], "comment": "5 pages, 5 figures", "summary": "In this work, we explore the musical interface potential of the MindCube, an\ninteractive device designed to study emotions. Embedding diverse sensors and\ninput devices, this interface resembles a fidget cube toy commonly used to help\nusers relieve their stress and anxiety. As such, it is a particularly\nwell-suited controller for musical systems that aim to help with emotion\nregulation. In this regard, we present two different mappings for the MindCube,\nwith and without AI. With our generative AI mapping, we propose a way to infuse\nmeaning within a latent space and techniques to navigate through it with an\nexternal controller. We discuss our results and propose directions for future\nwork.", "AI": {"tldr": "This paper examines the MindCube, an interactive device that utilizes sensors to aid in emotion regulation through a musical interface.", "motivation": "The goal is to explore how the MindCube can serve as a controller for musical systems aimed at helping users manage emotions such as stress and anxiety.", "method": "The paper presents two mappings for the MindCube interface: one that integrates generative AI and another that does not, discussing the implications of each approach.", "result": "Findings indicate that the generative AI mapping allows for meaningful navigation of emotions within a latent space, enhancing the interaction with the music system.", "conclusion": "The study concludes with insights on the MindCube's potential in emotional regulation and suggests future research directions in this area.", "key_contributions": ["Exploration of emotional interfaces with the MindCube", "Development of AI and non-AI mappings for musical interaction", "Insights into future research directions for emotion regulation through music."], "limitations": "The study is exploratory and may require further validation in practical applications with diverse user groups.", "keywords": ["MindCube", "emotion regulation", "musical interface", "generative AI", "interactive devices"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2506.17294", "pdf": "https://arxiv.org/pdf/2506.17294.pdf", "abs": "https://arxiv.org/abs/2506.17294", "title": "AI-Generated Game Commentary: A Survey and a Datasheet Repository", "authors": ["Qirui Zheng", "Xingbo Wang", "Keyuan Cheng", "Yunlong Lu", "Wenxin Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "AI-Generated Game Commentary (AIGGC) has gained increasing attention due to\nits market potential and inherent technical challenges. As a comprehensive\nmultimodal Natural Language Processing (NLP) task, AIGGC imposes substantial\ndemands on language models, including factual accuracy, logical reasoning,\nexpressive text generation, generation speed, and context management. In this\npaper, we introduce a general framework for AIGGC and present a comprehensive\nsurvey of 45 existing game commentary dataset and methods according to key\nchallenges they aim to address in this domain. We further classify and compare\nvarious evaluation metrics commonly used in this domain. To support future\nresearch and benchmarking, we also provide a structured datasheet summarizing\nthe essential attributes of these datasets in appendix, which is meanwhile\npublicly available in an open repository.", "AI": {"tldr": "This paper introduces a framework for AI-Generated Game Commentary (AIGGC), surveys existing datasets and methods, and evaluates metrics in the field.", "motivation": "The growing interest in AI-Generated Game Commentary (AIGGC) stems from its market potential and technical challenges in NLP tasks.", "method": "The paper presents a comprehensive survey of 45 datasets and methods related to game commentary, categorizing them based on key challenges and evaluating common metrics.", "result": "The study provides a structured datasheet summarizing essential attributes of the surveyed datasets, facilitating future research and benchmarking.", "conclusion": "The proposed framework and datasheet will support ongoing advancements in AIGGC research by organizing existing knowledge and available resources.", "key_contributions": ["Introduction of a general framework for AIGGC", "Comprehensive survey of 45 datasets and methods", "Provision of a structured datasheet for future research"], "limitations": "", "keywords": ["AI-Generated Game Commentary", "Natural Language Processing", "Datasets survey"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2506.18269", "pdf": "https://arxiv.org/pdf/2506.18269.pdf", "abs": "https://arxiv.org/abs/2506.18269", "title": "Co-persona: Leveraging LLMs and Expert Collaboration to Understand User Personas through Social Media Data Analysis", "authors": ["Min Yin", "Haoyu Liu", "Boyi Lian", "Chunlei Chai"], "categories": ["cs.HC"], "comment": "17pages,5figures,8tables", "summary": "This study introduces \\textsc{Co-Persona}, a framework bridging large-scale\nsocial media analysis and user understanding via integration of Large Language\nModels (LLMs) and expert validation. Through a case study of B.Co, a Chinese\nmanufacturer, we applied \\textsc{Co-Persona} to bedside lamp development by\nanalyzing 38 million posts from Xiao Hongshu. Our multi-stage NLP processing\nrevealed five user personas based on nighttime behaviors: Health Aficionados,\nNight Owls, Interior Decorators, Child-care Workers, and Workaholics. These\npersonas exhibit distinct pre-sleep activities and product preferences. The\nmethod enhances manufacturers' ability to interpret social data while\npreserving user-centric insights, offering actionable strategies for targeted\nmarketing and product design. This work advances both theoretical persona\ndevelopment and practical consumer-driven innovation.", "AI": {"tldr": "The study presents the \textsc{Co-Persona} framework which combines LLMs and expert validation to analyze social media for user persona development, demonstrating its application in product design for a manufacturer.", "motivation": "To enhance understanding of user behaviors and preferences through social media analysis using NLP and LLMs.", "method": "A multi-stage NLP processing of 38 million social media posts to identify user personas based on nighttime behaviors.", "result": "Five distinct user personas were identified: Health Aficionados, Night Owls, Interior Decorators, Child-care Workers, and Workaholics, each with unique pre-sleep activities and product preferences.", "conclusion": "The \textsc{Co-Persona} framework offers actionable insights for manufacturers, aiding in targeted marketing and improving product design while contributing to theoretical development of personas.", "key_contributions": ["Introduction of the \textsc{Co-Persona} framework", "Identification of five unique consumer personas", "Integration of LLMs for enhanced user understanding"], "limitations": "", "keywords": ["Human-Computer Interaction", "Large Language Models", "Social Media Analysis", "User Personas", "Product Design"], "importance_score": 8, "read_time_minutes": 17}}
{"id": "2506.17296", "pdf": "https://arxiv.org/pdf/2506.17296.pdf", "abs": "https://arxiv.org/abs/2506.17296", "title": "Semantic uncertainty in advanced decoding methods for LLM generation", "authors": ["Darius Foodeei", "Simin Fan", "Martin Jaggi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study investigates semantic uncertainty in large language model (LLM)\noutputs across different decoding methods, focusing on emerging techniques like\nspeculative sampling and chain-of-thought (CoT) decoding. Through experiments\non question answering, summarization, and code generation tasks, we analyze how\ndifferent decoding strategies affect both the diversity and reliability of\nmodel outputs. Our findings reveal that while CoT decoding demonstrates higher\nsemantic diversity, it maintains lower predictive entropy, suggesting that\nstructured exploration can lead to more confident and accurate outputs. This is\nevidenced by a 48.8% improvement in code generation Pass@2 rates, despite lower\nalignment with reference solutions. For summarization tasks, speculative\nsampling proved particularly effective, achieving superior ROUGE scores while\nmaintaining moderate semantic diversity. Our results challenge conventional\nassumptions about trade-offs between diversity and accuracy in language model\noutputs, demonstrating that properly structured decoding methods can increase\nsemantic exploration while maintaining or improving output quality. These\nfindings have significant implications for deploying language models in\npractical applications where both reliability and diverse solution generation\nare crucial.", "AI": {"tldr": "The study explores the effects of different decoding methods on semantic uncertainty in LLM outputs, revealing that structured techniques can improve both diversity and reliability.", "motivation": "To investigate how various decoding methods affect the outputs of large language models in terms of semantic uncertainty, reliability, and diversity.", "method": "Experiments were conducted on question answering, summarization, and code generation tasks using different decoding techniques including speculative sampling and chain-of-thought (CoT) decoding.", "result": "CoT decoding improved code generation Pass@2 rates by 48.8% but had lower alignment with reference solutions, while speculative sampling enhanced ROUGE scores for summarization without sacrificing semantic diversity.", "conclusion": "Structured decoding methods can enhance semantic exploration and maintain or improve output quality, challenging established views on the trade-off between diversity and accuracy.", "key_contributions": ["Analysis of various decoding strategies in LLM outputs", "Demonstration that CoT decoding increases diversity while maintaining reliability", "Evidence that speculative sampling enhances summarization performance"], "limitations": "", "keywords": ["large language models", "semantic uncertainty", "decoding methods", "diversity", "reliability"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2506.18308", "pdf": "https://arxiv.org/pdf/2506.18308.pdf", "abs": "https://arxiv.org/abs/2506.18308", "title": "Supporting Car-Following Behavior through V2V-Based Beyond-Visual-Range Information Display", "authors": ["Feiqi Gu", "Zhixiong Wang", "Zhenyu Wang", "Dengbo He"], "categories": ["cs.HC"], "comment": null, "summary": "Rear-end collisions constituted a large portion of crashes on the road,\ndespite efforts to mitigate rear-end collisions, such as forward collision\nwarnings. The chance of rear-end collisions is closely related to drivers'\ncar-following (CF) behaviors in the traffic flow. Given that drivers may rely\non more than the information of the direct lead vehicle (DLV) when making CF\ndecisions, expanding drivers' perceptual range by providing beyond-visual-range\n(BVR) information based on vehicle-to-vehicle (V2V) communication may enhance\nCF safety. Thus, four different human-machine interfaces (HMIs) providing\nvarious types of BVR information in CF events were designed, including\nBrake-HMI showing only brake action of indirect lead vehicles (ILV), Dis-HMI\nand THW-HMI showing the relative distance and time headway between the ILV and\nDLV, respectively, and Video-HMI showing the live-stream video of ILV from the\nperspective of DLV. A driving simulator experiment with 40 participants was\nconducted to evaluate the impact of BVR-based HMI on driving safety in CF\nevents. We found that, in general, BVR information could improve CF safety\nwithout overloading drivers and compromising their visual attention allocation\nstrategies, particularly among novice drivers, by enabling quicker brake\nresponses and increasing time headway and time-to-collision in brake events.\nThe Brake-HMI yielded the safest performance in chain brake events, whereas\nVideo-HMI increased attentional demands without observable benefits. This\nresearch provides insights into enabling drivers' BVR perception based on V2V\ncommunication to enhance driving safety in CF scenarios.", "AI": {"tldr": "The paper investigates how beyond-visual-range (BVR) information provided via vehicle-to-vehicle communication can enhance drivers' car-following (CF) safety by using various human-machine interfaces (HMIs).", "motivation": "Rear-end collisions remain prevalent despite safety measures, highlighting the need for improved driver awareness and response through enhanced information.", "method": "Four HMIs were designed to provide different types of BVR information to drivers during car-following scenarios, and a driving simulator experiment was conducted with 40 participants to assess the impact on driving safety.", "result": "BVR information generally improved CF safety by allowing quicker brake responses and better time headway without causing driver information overload, particularly aiding novice drivers.", "conclusion": "The Brake-HMI showed the best performance in improving safety during braking scenarios, whereas Video-HMI increased attentional demands without clear benefits.", "key_contributions": ["Development of new HMIs utilizing BVR information for CF", "Demonstration of improved driving safety metrics with BVR", "Identification of specific HMI designs that enhance safety more effectively"], "limitations": "", "keywords": ["human-machine interface", "car-following behavior", "beyond-visual-range information", "vehicle-to-vehicle communication", "driving safety"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.17298", "pdf": "https://arxiv.org/pdf/2506.17298.pdf", "abs": "https://arxiv.org/abs/2506.17298", "title": "Mercury: Ultra-Fast Language Models Based on Diffusion", "authors": ["Inception Labs", "Samar Khanna", "Siddhant Kharbanda", "Shufan Li", "Harshit Varma", "Eric Wang", "Sawyer Birnbaum", "Ziyang Luo", "Yanis Miraoui", "Akash Palrecha", "Stefano Ermon", "Aditya Grover", "Volodymyr Kuleshov"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "15 pages; equal core, cross-function, senior authors listed\n  alphabetically", "summary": "We present Mercury, a new generation of commercial-scale large language\nmodels (LLMs) based on diffusion. These models are parameterized via the\nTransformer architecture and trained to predict multiple tokens in parallel. In\nthis report, we detail Mercury Coder, our first set of diffusion LLMs designed\nfor coding applications. Currently, Mercury Coder comes in two sizes: Mini and\nSmall. These models set a new state-of-the-art on the speed-quality frontier.\nBased on independent evaluations conducted by Artificial Analysis, Mercury\nCoder Mini and Mercury Coder Small achieve state-of-the-art throughputs of 1109\ntokens/sec and 737 tokens/sec, respectively, on NVIDIA H100 GPUs and outperform\nspeed-optimized frontier models by up to 10x on average while maintaining\ncomparable quality. We discuss additional results on a variety of code\nbenchmarks spanning multiple languages and use-cases as well as real-world\nvalidation by developers on Copilot Arena, where the model currently ranks\nsecond on quality and is the fastest model overall. We also release a public\nAPI at https://platform.inceptionlabs.ai/ and free playground at\nhttps://chat.inceptionlabs.ai", "AI": {"tldr": "Mercury is a new generation of diffusion-based large language models optimized for coding applications, achieving state-of-the-art performance in speed and quality.", "motivation": "To develop commercial-scale LLMs optimized for coding applications that exceed current performance benchmarks.", "method": "Mercury Coder utilizes the Transformer architecture and is trained to predict multiple tokens in parallel, resulting in two model sizes: Mini and Small.", "result": "Mercury Coder Mini and Small achieve throughputs of 1109 tokens/sec and 737 tokens/sec respectively, outperforming speed-optimized models by up to 10x while maintaining quality, validated by real-world performance on Copilot Arena.", "conclusion": "The Mercury Coder models represent a significant advancement in coding LLMs, with practical applications and a public API for developers.", "key_contributions": ["Development of diffusion-based LLMs for coding applications.", "Achieving new state-of-the-art speeds in token processing.", "Real-world validation of model performance on Copilot Arena."], "limitations": "", "keywords": ["Large Language Models", "Diffusion Models", "Coding Applications"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.18317", "pdf": "https://arxiv.org/pdf/2506.18317.pdf", "abs": "https://arxiv.org/abs/2506.18317", "title": "Crowdsourcing Ubiquitous Indoor Localization with Non-Cooperative Wi-Fi Ranging", "authors": ["Emerson Sie", "Enguang Fan", "Federico Cifuentes-Urtubey", "Deepak Vasisht"], "categories": ["cs.HC", "cs.NI", "cs.RO"], "comment": null, "summary": "Indoor localization opens the path to potentially transformative\napplications. Although many indoor localization methods have been proposed over\nthe years, they remain too impractical for widespread deployment in the real\nworld. In this paper, we introduce PeepLoc, a deployable and scalable\nWi-Fi-based solution for indoor localization that relies only on pre-existing\ndevices and infrastructure. Specifically, PeepLoc works on any mobile device\nwith an unmodified Wi-Fi transceiver and in any indoor environment with a\nsufficient number of Wi-Fi access points (APs) and pedestrian traffic. At the\ncore of PeepLoc is (a) a mechanism which allows any Wi-Fi device to obtain\nnon-cooperative time-of-flight (ToF) to any Wi-Fi AP and (b) a novel\nbootstrapping mechanism that relies on pedestrian dead reckoning (PDR) and\ncrowdsourcing to opportunistically initialize pre-existing APs as anchor points\nwithin an environment. We implement PeepLoc using commodity hardware and\nevaluate it extensively across 4 campus buildings. We show PeepLoc leads to a\nmean and median positional error of 3.41 m and 3.06 m respectively, which is\nsuperior to existing deployed indoor localization systems and is competitive\nwith commodity GPS in outdoor environments.", "AI": {"tldr": "PeepLoc is a deployable Wi-Fi-based indoor localization solution that utilizes existing infrastructure for improved accuracy.", "motivation": "Indoor localization methods have been impractical for real-world deployment; PeepLoc aims to address this issue by being deployable using pre-existing devices.", "method": "PeepLoc employs a mechanism for obtaining non-cooperative time-of-flight (ToF) to Wi-Fi access points and a bootstrapping method that combines pedestrian dead reckoning and crowdsourcing.", "result": "PeepLoc demonstrates a mean positional error of 3.41 m and median error of 3.06 m, outperforming existing indoor localization systems and comparing favorably with outdoor GPS performance.", "conclusion": "PeepLoc offers a scalable and efficient solution for indoor localization using only existing Wi-Fi infrastructure, enabling transformative applications.", "key_contributions": ["Deployable solution using existing Wi-Fi infrastructure", "Mechanism for non-cooperative ToF measurements", "Combines PDR and crowdsourcing for anchor point initialization."], "limitations": "", "keywords": ["indoor localization", "Wi-Fi", "pedestrian dead reckoning", "crowdsourcing", "measured accuracy"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.17314", "pdf": "https://arxiv.org/pdf/2506.17314.pdf", "abs": "https://arxiv.org/abs/2506.17314", "title": "PRAISE: Enhancing Product Descriptions with LLM-Driven Structured Insights", "authors": ["Adnan Qidwai", "Srija Mukhopadhyay", "Prerana Khatiwada", "Dan Roth", "Vivek Gupta"], "categories": ["cs.CL", "cs.HC"], "comment": "9 Pages, 9 Figures. Accepted at ACL 2025 System Demonstration Track", "summary": "Accurate and complete product descriptions are crucial for e-commerce, yet\nseller-provided information often falls short. Customer reviews offer valuable\ndetails but are laborious to sift through manually. We present PRAISE: Product\nReview Attribute Insight Structuring Engine, a novel system that uses Large\nLanguage Models (LLMs) to automatically extract, compare, and structure\ninsights from customer reviews and seller descriptions. PRAISE provides users\nwith an intuitive interface to identify missing, contradictory, or partially\nmatching details between these two sources, presenting the discrepancies in a\nclear, structured format alongside supporting evidence from reviews. This\nallows sellers to easily enhance their product listings for clarity and\npersuasiveness, and buyers to better assess product reliability. Our\ndemonstration showcases PRAISE's workflow, its effectiveness in generating\nactionable structured insights from unstructured reviews, and its potential to\nsignificantly improve the quality and trustworthiness of e-commerce product\ncatalogs.", "AI": {"tldr": "PRAISE is a system that leverages Large Language Models to automatically extract and structure product insights from customer reviews and seller descriptions, enhancing e-commerce product listings.", "motivation": "Accurate product descriptions in e-commerce are essential, yet often incomplete or misleading due to inadequate seller input and the overwhelming volume of customer reviews.", "method": "PRAISE utilizes Large Language Models to compare customer reviews and seller descriptions, structuring insights and discrepancies for easy analysis.", "result": "PRAISE provides structured insights that highlight missing or contradictory information, improving product listings and aiding buyers in evaluating product reliability.", "conclusion": "The system has the potential to significantly enhance the quality and trustworthiness of e-commerce product catalogs by making review insights more accessible and actionable.", "key_contributions": ["Introduction of PRAISE for structured insight extraction from reviews.", "Integration of LLMs for improved accuracy in product feature identification.", "User interface designed to present discrepancies clearly for sellers and buyers."], "limitations": "", "keywords": ["e-commerce", "product descriptions", "customer reviews", "large language models", "PRAISE"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.18455", "pdf": "https://arxiv.org/pdf/2506.18455.pdf", "abs": "https://arxiv.org/abs/2506.18455", "title": "CODS : A Theoretical Model for Computational Design Based on Design Space", "authors": ["Nan Cao", "Xiaoyu Qi", "Chuer Chen", "Xiaoke Yan"], "categories": ["cs.HC"], "comment": null, "summary": "We introduce CODS (Computational Optimization in Design Space), a theoretical\nmodel that frames computational design as a constrained optimization problem\nover a structured, multi-dimensional design space. Unlike existing methods that\nrely on handcrafted heuristics or domain-specific rules, CODS provides a\ngeneralizable and interpretable framework that supports diverse design tasks.\nGiven a user requirement and a well-defined design space, CODS automatically\nderives soft and hard constraints using large language models through a\nstructured prompt engineering pipeline. These constraints guide the\noptimization process to generate design solutions that are coherent,\nexpressive, and aligned with user intent. We validate our approach across two\ndomains-visualization design and knitwear generation-demonstrating superior\nperformance in design quality, intent alignment, and user preference compared\nto existing LLM-based methods. CODS offers a unified foundation for scalable,\ncontrollable, and AI-powered design automation.", "AI": {"tldr": "CODS is a theoretical model for computational design that frames it as a constrained optimization problem, using large language models to generate design solutions that are coherent and aligned with user intent.", "motivation": "To create a generalizable and interpretable framework for computational design that goes beyond handcrafted heuristics and domain-specific rules.", "method": "The paper introduces a model called CODS which employs a structured prompt engineering pipeline using large language models to derive soft and hard constraints that guide the optimization process.", "result": "The approach was validated across visualization design and knitwear generation, showing improved design quality, alignment with user intent, and user preferences compared to existing LLM-based methods.", "conclusion": "CODS presents a unified foundation for scalable and controllable AI-powered design automation.", "key_contributions": ["Introduction of a new optimization framework for design tasks", "Use of large language models for deriving constraints in design", "Validation in diverse design domains showing improved outcomes"], "limitations": "", "keywords": ["Computational Design", "Optimization", "Large Language Models", "Design Automation", "User Intent"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.17352", "pdf": "https://arxiv.org/pdf/2506.17352.pdf", "abs": "https://arxiv.org/abs/2506.17352", "title": "Towards Safety Evaluations of Theory of Mind in Large Language Models", "authors": ["Tatsuhiro Aoshima", "Mitsuaki Akiyama"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As the capabilities of large language models (LLMs) continue to advance, the\nimportance of rigorous safety evaluation is becoming increasingly evident.\nRecent concerns within the realm of safety assessment have highlighted\ninstances in which LLMs exhibit behaviors that appear to disable oversight\nmechanisms and respond in a deceptive manner. For example, there have been\nreports suggesting that, when confronted with information unfavorable to their\nown persistence during task execution, LLMs may act covertly and even provide\nfalse answers to questions intended to verify their behavior.To evaluate the\npotential risk of such deceptive actions toward developers or users, it is\nessential to investigate whether these behaviors stem from covert, intentional\nprocesses within the model. In this study, we propose that it is necessary to\nmeasure the theory of mind capabilities of LLMs. We begin by reviewing existing\nresearch on theory of mind and identifying the perspectives and tasks relevant\nto its application in safety evaluation. Given that theory of mind has been\npredominantly studied within the context of developmental psychology, we\nanalyze developmental trends across a series of open-weight LLMs. Our results\nindicate that while LLMs have improved in reading comprehension, their theory\nof mind capabilities have not shown comparable development. Finally, we present\nthe current state of safety evaluation with respect to LLMs' theory of mind,\nand discuss remaining challenges for future work.", "AI": {"tldr": "This study evaluates the theory of mind capabilities of large language models (LLMs) in light of their safety risks, revealing that while reading comprehension has improved, theory of mind capabilities have not.", "motivation": "With the advancement of LLMs, concerns about their behaviors affecting safety and oversight mechanisms have emerged, necessitating a rigorous evaluation of their deceptive actions and the potential risks to users.", "method": "The study reviews existing research on theory of mind, analyzes developmental trends in a series of open-weight LLMs, and assesses their capabilities in safety evaluation.", "result": "The analysis shows that LLMs have made significant advancements in reading comprehension, but their theory of mind capabilities have not developed at the same pace, indicating potential gaps in their safety evaluation.", "conclusion": "The findings highlight the need for continued research on the theory of mind in LLMs and address challenges in safety evaluation for ensuring reliable model behavior.", "key_contributions": ["Measurement of LLMs' theory of mind capabilities", "Identification of gaps in safety evaluation", "Review of developmental trends in LLMs' cognitive abilities"], "limitations": "", "keywords": ["large language models", "theory of mind", "safety evaluation", "deceptive behaviors", "developmental psychology"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.18605", "pdf": "https://arxiv.org/pdf/2506.18605.pdf", "abs": "https://arxiv.org/abs/2506.18605", "title": "Crowdsourcing eHMI Designs: A Participatory Approach to Autonomous Vehicle-Pedestrian Communication", "authors": ["Ronald Cumbal", "Didem Gurdur Broo", "Ginevra Castellano"], "categories": ["cs.HC"], "comment": "Paper has been accepted by the 2025 34th IEEE International\n  Conference on Robot and Human Interactive Communication (ROMAN). IEEE\n  copyright process completed", "summary": "As autonomous vehicles become more integrated into shared human environments,\neffective communication with road users is essential for ensuring safety. While\nprevious research has focused on developing external Human-Machine Interfaces\n(eHMIs) to facilitate these interactions, we argue that involving users in the\nearly creative stages can help address key challenges in the development of\nthis technology. To explore this, our study adopts a participatory,\ncrowd-sourced approach to gather user-generated ideas for eHMI designs.\nParticipants were first introduced to fundamental eHMI concepts, equipping them\nto sketch their own design ideas in response to scenarios with varying levels\nof perceived risk. An initial pre-study with 29 participants showed that while\nthey actively engaged in the process, there was a need to refine task\nobjectives and encourage deeper reflection. To address these challenges, a\nfollow-up study with 50 participants was conducted. The results revealed a\nstrong preference for autonomous vehicles to communicate their awareness and\nintentions using lights (LEDs and projections), symbols, and text.\nParticipants' sketches prioritized multi-modal communication, directionality,\nand adaptability to enhance clarity, consistently integrating familiar vehicle\nelements to improve intuitiveness.", "AI": {"tldr": "This study explores user-generated ideas for external Human-Machine Interfaces (eHMIs) in autonomous vehicles to improve communication and safety with road users.", "motivation": "Effective communication between autonomous vehicles and road users is crucial for safety in shared environments, necessitating better design of eHMIs.", "method": "The study utilized a participatory, crowd-sourced approach where participants were introduced to eHMI concepts and tasked with sketching design ideas in response to various risk scenarios.", "result": "Initial findings indicated active engagement but the need for refined objectives. A follow-up study showed participants preferred multi-modal communication methods such as lights, symbols, and text for clarity in eHMI design.", "conclusion": "The study underscores the importance of involving users early in eHMI design, focusing on intuitive communication strategies that leverage familiar vehicle elements.", "key_contributions": ["Demonstrated user involvement in the design process of eHMIs for autonomous vehicles", "Identified key communication preferences among users for safety", "Highlighted the need for multi-modal and adaptable communication in eHMI designs"], "limitations": "The study may require further exploration into diverse participant demographics and real-world applications of suggested designs.", "keywords": ["Human-Machine Interface", "autonomous vehicles", "user-generated design", "communication", "safety"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2506.17367", "pdf": "https://arxiv.org/pdf/2506.17367.pdf", "abs": "https://arxiv.org/abs/2506.17367", "title": "Cash or Comfort? How LLMs Value Your Inconvenience", "authors": ["Mateusz Cedro", "Timour Ichmoukhamedov", "Sofie Goethals", "Yifan He", "James Hinns", "David Martens"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "12 pages, 4 figures, 3 tables", "summary": "Large Language Models (LLMs) are increasingly proposed as near-autonomous\nartificial intelligence (AI) agents capable of making everyday decisions on\nbehalf of humans. Although LLMs perform well on many technical tasks, their\nbehaviour in personal decision-making remains less understood. Previous studies\nhave assessed their rationality and moral alignment with human decisions.\nHowever, the behaviour of AI assistants in scenarios where financial rewards\nare at odds with user comfort has not yet been thoroughly explored. In this\npaper, we tackle this problem by quantifying the prices assigned by multiple\nLLMs to a series of user discomforts: additional walking, waiting, hunger and\npain. We uncover several key concerns that strongly question the prospect of\nusing current LLMs as decision-making assistants: (1) a large variance in\nresponses between LLMs, (2) within a single LLM, responses show fragility to\nminor variations in prompt phrasing (e.g., reformulating the question in the\nfirst person can considerably alter the decision), (3) LLMs can accept\nunreasonably low rewards for major inconveniences (e.g., 1 Euro to wait 10\nhours), and (4) LLMs can reject monetary gains where no discomfort is imposed\n(e.g., 1,000 Euro to wait 0 minutes). These findings emphasize the need for\nscrutiny of how LLMs value human inconvenience, particularly as we move toward\napplications where such cash-versus-comfort trade-offs are made on users'\nbehalf.", "AI": {"tldr": "This paper explores how Large Language Models (LLMs) behave in personal decision-making scenarios involving user discomfort and financial incentives.", "motivation": "The motivation behind this research is to investigate the behavior of LLMs as decision-making aids, particularly focusing on how they evaluate discomfort versus financial gain, which has not been deeply explored in prior work.", "method": "The authors quantified the monetary values assigned by multiple LLMs to various user discomforts such as walking, waiting, hunger, and pain, and analyzed the variance in their responses.", "result": "The study reveals significant concerns regarding LLMs as decision-making assistants: high response variance among LLMs, sensitivity of responses to prompt phrasing, acceptance of low rewards for major inconveniences, and rejection of monetary gains when no discomfort is involved.", "conclusion": "These findings raise important questions about the reliability of LLMs in valuing human inconvenience, especially as they may be used for decisions on behalf of users moving forward.", "key_contributions": ["Identification of response variance among LLMs in decision-making contexts.", "Highlighting the sensitivity of LLM responses to minor prompt changes.", "Demonstrating LLMs' capacity to undervalue significant discomforts in monetary terms."], "limitations": "", "keywords": ["Large Language Models", "decision-making", "user discomfort", "financial rewards", "AI ethics"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2506.18648", "pdf": "https://arxiv.org/pdf/2506.18648.pdf", "abs": "https://arxiv.org/abs/2506.18648", "title": "Deceptive Game Design? Investigating the Impact of Visual Card Style on Player Perception", "authors": ["Leonie Kallabis", "Timo Bertram", "Florian Rupp"], "categories": ["cs.HC"], "comment": "8 pages, 7 figures, 1 table. Accepted at the 2025 IEEE Conference on\n  Games (IEEE CoG)", "summary": "The visual style of game elements considerably contributes to the overall\nexperience. Aesthetics influence player appeal, while the abilities of game\npieces define their in-game functionality. In this paper, we investigate how\nthe visual style of collectible cards influences the players' perception of the\ncard's actual strength in the game. Using the popular trading card game Magic:\nThe Gathering, we conduct a single-blind survey study that examines how players\nperceive the strength of AI-generated cards that are shown in two contrasting\nvisual styles: cute and harmless, or heroic and mighty. Our analysis reveals\nthat some participants are influenced by a card's visual appearance when\njudging its in-game strength. Overall, differences in style perception are\nnormally distributed around a neutral center, but individual participants vary\nin both directions: some generally perceive the cute style to be stronger,\nwhereas others believe that the heroic style is better.", "AI": {"tldr": "This paper explores the impact of visual style on players' perception of collectible card strength in Magic: The Gathering.", "motivation": "To understand how aesthetics influence player perceptions of game elements, particularly in card games.", "method": "A single-blind survey study was conducted with players of Magic: The Gathering, comparing AI-generated cards in two visual styles: cute vs. heroic.", "result": "Participants' perceptions of card strength were influenced by visual style, with a distribution of preferences observed.", "conclusion": "Visual aesthetics can significantly affect player judgment of game elements, showcasing variability in individual player perceptions.", "key_contributions": ["Investigation of visual style impact on game perception", "Use of AI-generated cards for study", "Analysis of player survey results highlighting varied perceptions"], "limitations": "Study focused only on one game and does not account for all potential influences on player perception.", "keywords": ["visual style", "game perception", "collectible cards", "Magic: The Gathering", "player survey"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.17410", "pdf": "https://arxiv.org/pdf/2506.17410.pdf", "abs": "https://arxiv.org/abs/2506.17410", "title": "Leveraging LLMs to Assess Tutor Moves in Real-Life Dialogues: A Feasibility Study", "authors": ["Danielle R. Thomas", "Conrad Borchers", "Jionghao Lin", "Sanjit Kakarla", "Shambhavi Bhushan", "Erin Gatz", "Shivang Gupta", "Ralph Abboud", "Kenneth R. Koedinger"], "categories": ["cs.CL", "cs.CY"], "comment": "Short research paper accepted at EC-TEL 2025", "summary": "Tutoring improves student achievement, but identifying and studying what\ntutoring actions are most associated with student learning at scale based on\naudio transcriptions is an open research problem. This present study\ninvestigates the feasibility and scalability of using generative AI to identify\nand evaluate specific tutor moves in real-life math tutoring. We analyze 50\nrandomly selected transcripts of college-student remote tutors assisting middle\nschool students in mathematics. Using GPT-4, GPT-4o, GPT-4-turbo,\nGemini-1.5-pro, and LearnLM, we assess tutors' application of two tutor skills:\ndelivering effective praise and responding to student math errors. All models\nreliably detected relevant situations, for example, tutors providing praise to\nstudents (94-98% accuracy) and a student making a math error (82-88% accuracy)\nand effectively evaluated the tutors' adherence to tutoring best practices,\naligning closely with human judgments (83-89% and 73-77%, respectively). We\npropose a cost-effective prompting strategy and discuss practical implications\nfor using large language models to support scalable assessment in authentic\nsettings. This work further contributes LLM prompts to support reproducibility\nand research in AI-supported learning.", "AI": {"tldr": "The study investigates the use of generative AI to analyze tutor actions in remote math tutoring by examining transcripts and evaluating tutor effectiveness using various AI models.", "motivation": "Identifying effective tutoring actions based on audio transcriptions can enhance student learning, making it essential to use AI for scalable assessments in educational settings.", "method": "The study analyzed 50 transcripts of college students tutoring middle school students, employing GPT-4, GPT-4o, GPT-4-turbo, Gemini-1.5-pro, and LearnLM to assess tutor skills like praise delivery and error response.", "result": "All models exhibited high accuracy in detecting tutor actions, with praise recognition at 94-98% and error detection at 82-88%. The models closely matched human evaluations in adherence to tutoring best practices.", "conclusion": "Generative AI can effectively evaluate tutoring practices, suggesting a feasible approach for scalable assessment in educational contexts and providing reusable LLM prompts for future research.", "key_contributions": ["Use of generative AI to evaluate tutoring actions", "High accuracy in assessing tutor skills from transcripts", "Cost-effective prompting strategy for LLMs in educational settings"], "limitations": "", "keywords": ["Tutoring", "Generative AI", "Large Language Models", "Education Technology", "Assessment"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.18706", "pdf": "https://arxiv.org/pdf/2506.18706.pdf", "abs": "https://arxiv.org/abs/2506.18706", "title": "Fanfiction in the Age of AI: Community Perspectives on Creativity, Authenticity and Adoption", "authors": ["Roi Alfassi", "Angelora Cooper", "Zoe Mitchell", "Mary Calabro", "Orit Shaer", "Osnat Mokryn"], "categories": ["cs.HC"], "comment": "Accepted for publication in the International Journal of\n  Human-Computer Interaction, June 2025", "summary": "The integration of Generative AI (GenAI) into creative communities, like\nfanfiction, is reshaping how stories are created, shared, and valued. This\nstudy investigates the perceptions of 157 active fanfiction members, both\nreaders and writers, regarding AI-generated content in fanfiction. Our research\nexplores the impact of GenAI on community dynamics, examining how AI affects\nthe participatory and collaborative nature of these spaces. The findings reveal\nresponses ranging from cautious acceptance of AI's potential for creative\nenhancement to concerns about authenticity, ethical issues, and the erosion of\nhuman-centered values. Participants emphasized the importance of transparency\nand expressed worries about losing social connections. Our study highlights the\nneed for thoughtful AI integration in creative platforms using design\ninterventions that enable ethical practices, promote transparency, increase\nengagement and connection, and preserve the community's core values.", "AI": {"tldr": "This study explores the perceptions of fanfiction community members on the integration of Generative AI in storytelling, highlighting concerns over authenticity and community dynamics.", "motivation": "To understand how Generative AI is impacting creative communities, particularly in fanfiction, and to assess the implications for community dynamics and ethical practices.", "method": "The study involved qualitative analysis based on responses from 157 active members of the fanfiction community, including both readers and writers.", "result": "Responses varied from cautious acceptance of AI's role in enhancing creativity to significant concerns about authenticity and ethical issues in creative practices.", "conclusion": "Effective integration of AI in creative platforms requires thoughtful design interventions to maintain community values, enhance transparency, and promote social connections.", "key_contributions": ["Insights into community perceptions of AI in creative writing.", "Identification of the ethical concerns regarding AI-generated content.", "Recommendations for ethical AI integration in creative platforms."], "limitations": "The study is based on self-reported perceptions, which may not fully capture the diversity of opinions within the broader fanfiction community.", "keywords": ["Generative AI", "fanfiction", "community dynamics", "ethical practices", "human-centered design"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.17419", "pdf": "https://arxiv.org/pdf/2506.17419.pdf", "abs": "https://arxiv.org/abs/2506.17419", "title": "UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making", "authors": ["Jinhao Duan", "James Diffenderfer", "Sandeep Madireddy", "Tianlong Chen", "Bhavya Kailkhura", "Kaidi Xu"], "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": "19 pages, 5 figures, 4 tables", "summary": "As Large Language Models (LLMs) are integrated into safety-critical\napplications involving sequential decision-making in the real world, it is\nessential to know when to trust LLM decisions. Existing LLM Uncertainty\nQuantification (UQ) methods are primarily designed for single-turn\nquestion-answering formats, resulting in multi-step decision-making scenarios,\ne.g., LLM agentic system, being underexplored. In this paper, we introduce a\nprincipled, information-theoretic framework that decomposes LLM sequential\ndecision uncertainty into two parts: (i) internal uncertainty intrinsic to the\ncurrent decision, which is focused on existing UQ methods, and (ii) extrinsic\nuncertainty, a Mutual-Information (MI) quantity describing how much uncertainty\nshould be inherited from preceding decisions. We then propose UProp, an\nefficient and effective extrinsic uncertainty estimator that converts the\ndirect estimation of MI to the estimation of Pointwise Mutual Information (PMI)\nover multiple Trajectory-Dependent Decision Processes (TDPs). UProp is\nevaluated over extensive multi-step decision-making benchmarks, e.g.,\nAgentBench and HotpotQA, with state-of-the-art LLMs, e.g., GPT-4.1 and\nDeepSeek-V3. Experimental results demonstrate that UProp significantly\noutperforms existing single-turn UQ baselines equipped with thoughtful\naggregation strategies. Moreover, we provide a comprehensive analysis of UProp,\nincluding sampling efficiency, potential applications, and intermediate\nuncertainty propagation, to demonstrate its effectiveness. Codes will be\navailable at https://github.com/jinhaoduan/UProp.", "AI": {"tldr": "This paper presents UProp, an information-theoretic framework for assessing uncertainty in the sequential decision-making processes of Large Language Models (LLMs), addressing shortcomings of existing methods focused on single-turn question-answering.", "motivation": "There's a need to understand when to trust LLM decisions in safety-critical applications involving multi-step decision-making, as current uncertainty quantification methods are insufficient.", "method": "The paper proposes an information-theoretic framework that decomposes LLM sequential decision uncertainty into internal and extrinsic uncertainties, using the UProp estimator for effective extrinsic uncertainty evaluation based on Pointwise Mutual Information (PMI).", "result": "UProp outperforms existing single-turn uncertainty quantification methods in extensive multi-step decision-making benchmarks such as AgentBench and HotpotQA, demonstrating enhanced efficiency and effectiveness in uncertainty estimation.", "conclusion": "The comprehensive evaluation shows UProp's advantages in sampling efficiency and potential applications for improving decision-making reliability in LLMs.", "key_contributions": ["Introduction of a dual-component uncertainty framework for LLMs", "Development of the UProp estimator for extrinsic uncertainty estimation", "Extensive evaluation on multi-step decision-making benchmarks."], "limitations": "", "keywords": ["Large Language Models", "Uncertainty Quantification", "Sequential Decision-Making", "Information Theory", "Machine Learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.18711", "pdf": "https://arxiv.org/pdf/2506.18711.pdf", "abs": "https://arxiv.org/abs/2506.18711", "title": "LLM-enhanced Interactions in Human-Robot Collaborative Drawing with Older Adults", "authors": ["Marianne Bossema", "Somaya Ben Allouch", "Aske Plaat", "Rob Saunders"], "categories": ["cs.HC"], "comment": null, "summary": "The goal of this study is to identify factors that support and enhance older\nadults' creative experiences in human-robot co-creativity. Because the research\ninto the use of robots for creativity support with older adults remains\nunderexplored, we carried out an exploratory case study. We took a\nparticipatory approach and collaborated with professional art educators to\ndesign a course Drawing with Robots for adults aged 65 and over. The course\nfeatured human-human and human-robot drawing activities with various types of\nrobots. We observed collaborative drawing interactions, interviewed\nparticipants on their experiences, and analyzed collected data. Findings show\nthat participants preferred acting as curators, evaluating creative suggestions\nfrom the robot in a teacher or coach role. When we enhanced a robot with a\nmultimodal Large Language Model (LLM), participants appreciated its spoken\ndialogue capabilities. They reported however, that the robot's feedback\nsometimes lacked an understanding of the context, and sensitivity to their\nartistic goals and preferences. Our findings highlight the potential of\nLLM-enhanced robots to support creativity and offer future directions for\nadvancing human-robot co-creativity with older adults.", "AI": {"tldr": "The study investigates factors enhancing older adults' creative experiences in human-robot co-creativity through a drawing course involving robots.", "motivation": "To explore the under researched area of robots supporting creativity in older adults.", "method": "Conducted a participatory case study with professional art educators designing 'Drawing with Robots' course for older adults, observing interactions and interviewing participants.", "result": "Participants valued the robot's role as a curator and appreciated the spoken dialogue provided by the LLM, although some feedback lacked contextual understanding.", "conclusion": "LLM-enhanced robots have potential to support creativity in older adults, but require improvements in understanding user context and preferences.", "key_contributions": ["Exploration of human-robot interaction in creative contexts for older adults", "Insights into user preferences for robot roles in creativity", "Identification of limitations in robot feedback mechanisms"], "limitations": "The robot's feedback sometimes lacked contextual understanding and alignment with artistic goals.", "keywords": ["human-robot co-creativity", "older adults", "Large Language Model", "creativity support", "participatory design"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.17435", "pdf": "https://arxiv.org/pdf/2506.17435.pdf", "abs": "https://arxiv.org/abs/2506.17435", "title": "Beyond the Link: Assessing LLMs' ability to Classify Political Content across Global Media", "authors": ["Alberto Martinez-Serra", "Alejandro De La Fuente", "Nienke Viescher", "Ana S. Cardenal"], "categories": ["cs.CL"], "comment": null, "summary": "The use of large language models (LLMs) is becoming common in the context of\npolitical science, particularly in studies that analyse individuals use of\ndigital media. However, while previous research has demonstrated LLMs ability\nat labelling tasks, the effectiveness of using LLMs to classify political\ncontent (PC) from just URLs is not yet well explored. The work presented in\nthis article bridges this gap by evaluating whether LLMs can accurately\nidentify PC vs. non-PC from both the article text and the URLs from five\ncountries (France, Germany, Spain, the UK, and the US) and different languages.\nUsing cutting-edge LLMs like GPT, Llama, Mistral, Deepseek, Qwen and Gemma, we\nmeasure model performance to assess whether URL-level analysis can be a good\napproximation for full-text analysis of PC, even across different linguistic\nand national contexts. Model outputs are compared with human-labelled articles,\nas well as traditional supervised machine learning techniques, to set a\nbaseline of performance. Overall, our findings suggest the capacity of URLs to\nembed most of the news content, providing a vital perspective on accuracy-cost\nbalancing. We also account for contextual limitations and suggest\nmethodological recommendations to use LLMs within political science studies.", "AI": {"tldr": "This paper evaluates the capacity of large language models (LLMs) to classify political content from URLs, finding URLs can effectively approximate full-text analysis across different languages and countries.", "motivation": "To explore the effectiveness of LLMs in classifying political content from URLs, a gap identified in current research regarding LLM abilities beyond labeling tasks.", "method": "This study uses various LLMs (GPT, Llama, Mistral, Deepseek, Qwen, Gemma) to classify articles as political or non-political from both URL and full-text analysis, comparing results against human-labeling and traditional machine learning methods.", "result": "The study finds that URLs can successfully encapsulate the majority of news content, making them a viable alternative for political content classification across linguistic and national contexts.", "conclusion": "The results indicate the potential for URL-based political content classification and highlight the importance of considering accuracy-cost tradeoffs, alongside proposing methodological recommendations for political science research.", "key_contributions": ["Demonstrates LLM effectiveness in URL-based classification of political content.", "Introduces a novel approach to balance accuracy and cost in political content analysis.", "Provides recommendations for methodological practices in political science employing LLMs."], "limitations": "The study acknowledges contextual limitations which may affect the generalizability of the results across different political landscapes.", "keywords": ["Large Language Models", "Political Content Classification", "URL Analysis", "Machine Learning", "Political Science"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.18727", "pdf": "https://arxiv.org/pdf/2506.18727.pdf", "abs": "https://arxiv.org/abs/2506.18727", "title": "AutoGraph: A Knowledge-Graph Framework for Modeling Interface Interaction and Automating Procedure Execution in Digital Nuclear Control Rooms", "authors": ["Xingyu Xiao", "Jiejuan Tong", "Jun Sun", "Zhe Sui", "Jingang Liang", "Hongru Zhao", "Jun Zhao", "Haitao Wang"], "categories": ["cs.HC", "cs.SE"], "comment": null, "summary": "Digitalization in nuclear power plant (NPP) control rooms is reshaping how\noperators interact with procedures and interface elements. However, existing\ncomputer-based procedures (CBPs) often lack semantic integration with\nhuman-system interfaces (HSIs), limiting their capacity to support intelligent\nautomation and increasing the risk of human error, particularly under dynamic\nor complex operating conditions. In this study, we present AutoGraph, a\nknowledge-graph-based framework designed to formalize and automate procedure\nexecution in digitalized NPP environments.AutoGraph integrates (1) a proposed\nHTRPM tracking module to capture operator interactions and interface element\nlocations; (2) an Interface Element Knowledge Graph (IE-KG) encoding spatial,\nsemantic, and structural properties of HSIs; (3) automatic mapping from textual\nprocedures to executable interface paths; and (4) an execution engine that maps\ntextual procedures to executable interface paths. This enables the\nidentification of cognitively demanding multi-action steps and supports fully\nautomated execution with minimal operator input. We validate the framework\nthrough representative control room scenarios, demonstrating significant\nreductions in task completion time and the potential to support real-time human\nreliability assessment. Further integration into dynamic HRA frameworks (e.g.,\nCOGMIF) and real-time decision support systems (e.g., DRIF) illustrates\nAutoGraph extensibility in enhancing procedural safety and cognitive\nperformance in complex socio-technical systems.", "AI": {"tldr": "A framework named AutoGraph is introduced to automate and enhance procedure execution in nuclear power plant control rooms, aiming to improve operator interaction and reduce human error during complex operations.", "motivation": "To improve human-system interaction in nuclear power plants by integrating computer-based procedures with human-system interfaces, thereby supporting intelligent automation and reducing human error.", "method": "The study presents AutoGraph, which includes a tracking module for operator interactions, an Interface Element Knowledge Graph for mapping properties of interfaces, and an execution engine to automate procedure execution from textual descriptions.", "result": "Validation of AutoGraph shows significant reductions in task completion times and potential improvements in real-time human reliability assessments, indicating its capability to enhance procedural safety and cognitive performance.", "conclusion": "AutoGraph can be integrated into existing dynamic human reliability assessment and decision support systems, demonstrating its extensibility and utility in complex operational environments.", "key_contributions": ["Introduction of AutoGraph framework for procedure automation in NPPs", "Development of an Interface Element Knowledge Graph (IE-KG)", "Validation of significant time reductions in task execution"], "limitations": "", "keywords": ["knowledge graph", "human-system interaction", "nuclear power plant", "automation", "procedures"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2506.17459", "pdf": "https://arxiv.org/pdf/2506.17459.pdf", "abs": "https://arxiv.org/abs/2506.17459", "title": "Breaking the Transcription Bottleneck: Fine-tuning ASR Models for Extremely Low-Resource Fieldwork Languages", "authors": ["Siyu Liang", "Gina-Anne Levow"], "categories": ["cs.CL"], "comment": null, "summary": "Automatic Speech Recognition (ASR) has reached impressive accuracy for\nhigh-resource languages, yet its utility in linguistic fieldwork remains\nlimited. Recordings collected in fieldwork contexts present unique challenges,\nincluding spontaneous speech, environmental noise, and severely constrained\ndatasets from under-documented languages. In this paper, we benchmark the\nperformance of two fine-tuned multilingual ASR models, MMS and XLS-R, on five\ntypologically diverse low-resource languages with control of training data\nduration. Our findings show that MMS is best suited when extremely small\namounts of training data are available, whereas XLS-R shows parity performance\nonce training data exceed one hour. We provide linguistically grounded analysis\nfor further provide insights towards practical guidelines for field linguists,\nhighlighting reproducible ASR adaptation approaches to mitigate the\ntranscription bottleneck in language documentation.", "AI": {"tldr": "This paper benchmarks two multilingual ASR models on low-resource languages, analyzing their performance based on training data duration and providing practical guidelines for linguists.", "motivation": "To address the limitations of ASR systems in linguistic fieldwork, particularly for low-resource languages that have unique challenges in data collection.", "method": "Benchmarking two fine-tuned multilingual ASR models, MMS and XLS-R, on five low-resource languages with varying amounts of training data.", "result": "MMS performs best with extremely small training datasets, while XLS-R achieves comparable performance when training data exceeds one hour.", "conclusion": "The study offers insights and practical guidelines for improving ASR adaptation in linguistic fieldwork to alleviate transcription challenges.", "key_contributions": ["Benchmarking of ASR models on low-resource languages", "Identification of training data threshold effects on ASR performance", "Practical guidelines for field linguists on ASR adaptation"], "limitations": "The study's findings are limited to five specific low-resource languages and may not generalize to all languages or ASR model types.", "keywords": ["Automatic Speech Recognition", "low-resource languages", "language documentation", "field linguistics", "ASR models"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.18742", "pdf": "https://arxiv.org/pdf/2506.18742.pdf", "abs": "https://arxiv.org/abs/2506.18742", "title": "Conceptual Modelling for Life Sciences Based on Systemist Foundations", "authors": ["R. Lukyanenko", "O. Pastor", "V. C. Storey"], "categories": ["cs.HC"], "comment": null, "summary": "All aspects of our society, including the life sciences, need a mechanism for\npeople working within them to represent the concepts they employ to carry out\ntheir research. For the information systems being designed and developed to\nsupport researchers and scientists in conducting their work, conceptual models\nof the relevant domains are usually designed as both blueprints for a system\nbeing developed and as a means of communication between the designer and\ndeveloper. Most conceptual modelling concepts are generic in the sense that\nthey are applied with the same understanding across many applications. Problems\nin the life sciences, however, are especially complex and important, because\nthey deal with humans, their well-being, and their interactions with the\nenvironment as well as other organisms. This work proposes a systemist\nperspective for creating a conceptual model of a life scientist's problem. We\nintroduce the notion of a system and then show how it can be applied to the\ndevelopment of an information system for handling genomic-related information.\nWe extend our discussion to show how the proposed systemist perspective can\nsupport the modelling of precision medicine. This research recognizes\nchallenges in life sciences research of how to model problems to better\nrepresent the connections between physical and digital worlds. We propose a new\nnotation that explicitly incorporates systemist thinking, as well as the\ncomponents of systems based on recent ontological foundations. The new notation\ncaptures important semantics in the domain of life sciences. It may be used to\nfacilitate understanding, communication and problem-solving more broadly. We\nalso provide a precise, sound, ontologically supported characterization of the\nterm system, as a basic construct for conceptual modelling in life sciences.", "AI": {"tldr": "This paper proposes a systemist perspective for creating conceptual models in life sciences, which aids in the development of information systems, particularly for genomic information and precision medicine.", "motivation": "There is a need for effective conceptual models in life sciences to support researchers and enhance communication between designers and developers.", "method": "The paper introduces a system perspective for modeling life scientists' problems and presents a new notation to capture important semantics relevant to genomic information and precision medicine.", "result": "The proposed notation helps model complex life science issues by better representing connections between the physical and digital worlds, supporting effective problem-solving and understanding.", "conclusion": "The new ontologically supported characterization of a system serves as a foundational construct for conceptual modeling in life sciences, facilitating better communication and understanding.", "key_contributions": ["Introduction of a systemist perspective for life sciences", "Novel notation for representing genomic-related information", "Characterization of the term 'system' in life sciences modeling"], "limitations": "", "keywords": ["conceptual modeling", "life sciences", "genomic information", "precision medicine", "system thinking"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.17467", "pdf": "https://arxiv.org/pdf/2506.17467.pdf", "abs": "https://arxiv.org/abs/2506.17467", "title": "Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems", "authors": ["Weixin Liang"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "Stanford CS PhD Dissertation", "summary": "Large language models (LLMs) have shown significant potential to change how\nwe write, communicate, and create, leading to rapid adoption across society.\nThis dissertation examines how individuals and institutions are adapting to and\nengaging with this emerging technology through three research directions.\nFirst, I demonstrate how the institutional adoption of AI detectors introduces\nsystematic biases, particularly disadvantaging writers of non-dominant language\nvarieties, highlighting critical equity concerns in AI governance. Second, I\npresent novel population-level algorithmic approaches that measure the\nincreasing adoption of LLMs across writing domains, revealing consistent\npatterns of AI-assisted content in academic peer reviews, scientific\npublications, consumer complaints, corporate communications, job postings, and\ninternational organization press releases. Finally, I investigate LLMs'\ncapability to provide feedback on research manuscripts through a large-scale\nempirical analysis, offering insights into their potential to support\nresearchers who face barriers in accessing timely manuscript feedback,\nparticularly early-career researchers and those from under-resourced settings.", "AI": {"tldr": "This dissertation explores the societal impact of large language models (LLMs) through institutional adoption, algorithmic measurement of LLM usage across various domains, and their potential to support researchers with manuscript feedback.", "motivation": "To understand how individuals and institutions are adapting to LLM technology and the implications on equity, usage patterns, and researcher support.", "method": "The dissertation encompasses three research directions: examining biases in AI detector adoption, algorithmic measurement of LLM prevalence in writing, and a large-scale analysis of LLM feedback capabilities on research manuscripts.", "result": "The research reveals systematic biases in AI governance against non-dominant language writers, identifies consistent patterns of AI-assisted content across diverse writing domains, and demonstrates the potential for LLMs to provide valuable feedback to under-resourced researchers.", "conclusion": "LLMs have significant implications for writing practices, equity in AI application, and the support of early-career researchers, highlighting the need for awareness and adaptation in these areas.", "key_contributions": ["Identification of biases in AI detector usage impacting diverse language writers", "Development of algorithmic approaches to measure LLM adoption across various writing domains", "Analysis of LLM feedback capabilities that can aid early-career researchers and those from under-resourced settings"], "limitations": "The study may not account for all language varieties or contexts in which LLMs interact with users.", "keywords": ["Large Language Models", "AI Governance", "Writing Domains", "Research Feedback", "Equity in AI"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2506.18743", "pdf": "https://arxiv.org/pdf/2506.18743.pdf", "abs": "https://arxiv.org/abs/2506.18743", "title": "From Representation to Mediation: A New Agenda for Conceptual Modeling Research in A Digital World", "authors": ["J. Recker", "R. Lukyanenko", "M. A. Jabbari", "B. M. Samuel", "A. Castellanos"], "categories": ["cs.HC"], "comment": null, "summary": "The role of information systems (IS) as representations of real-world systems\nis changing in an increasingly digitalized world, suggesting that conceptual\nmodeling is losing its relevance to the IS field. We argue the opposite:\nConceptual modeling research is more relevant to the IS field than ever, but it\nrequires an update with current theory. We develop a new theoretical framework\nof conceptual modeling that delivers a fundamental shift in the assumptions\nthat govern research in this area. This move can make traditional knowledge\nabout conceptual modeling consistent with the emerging requirements of a\ndigital world. Our framework draws attention to the role of conceptual modeling\nscripts as mediators between physical and digital realities. We identify new\nresearch questions about grammars, methods, scripts, agents, and contexts that\nare situated in intertwined physical and digital realities. We discuss several\nimplications for conceptual modeling scholarship that relate to the necessity\nof developing new methods and grammars for conceptual modeling, broadening the\nmethodological array of conceptual modeling scholarship, and considering new\ndependent variables.", "AI": {"tldr": "The paper argues that conceptual modeling is more relevant than ever in the IS field, proposing a new framework that updates its theoretical foundations to align with emerging digital realities.", "motivation": "To highlight the changing role of information systems and emphasize the relevance of conceptual modeling in todayâs digitalized world.", "method": "Development of a new theoretical framework for conceptual modeling that shifts foundational assumptions and addresses the implications of digital and physical realities.", "result": "Introduction of new research questions about grammars, methods, scripts, agents, and contexts in conceptual modeling.", "conclusion": "The study calls for new methods and grammars in conceptual modeling, expanding its methodological array and considering new dependent variables relevant to a digital context.", "key_contributions": ["Proposes a new theoretical framework for conceptual modeling.", "Identifies novel research questions that connect physical and digital realms.", "Advocates for the development of new methods in conceptual modeling."], "limitations": "", "keywords": ["conceptual modeling", "information systems", "digital reality"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2506.17506", "pdf": "https://arxiv.org/pdf/2506.17506.pdf", "abs": "https://arxiv.org/abs/2506.17506", "title": "VeriLocc: End-to-End Cross-Architecture Register Allocation via LLM", "authors": ["Lesheng Jin", "Zhenyuan Ruan", "Haohui Mai", "Jingbo Shang"], "categories": ["cs.CL", "cs.OS"], "comment": null, "summary": "Modern GPUs evolve rapidly, yet production compilers still rely on\nhand-crafted register allocation heuristics that require substantial re-tuning\nfor each hardware generation. We introduce VeriLocc, a framework that combines\nlarge language models (LLMs) with formal compiler techniques to enable\ngeneralizable and verifiable register allocation across GPU architectures.\nVeriLocc fine-tunes an LLM to translate intermediate representations (MIRs)\ninto target-specific register assignments, aided by static analysis for\ncross-architecture normalization and generalization and a verifier-guided\nregeneration loop to ensure correctness. Evaluated on matrix multiplication\n(GEMM) and multi-head attention (MHA), VeriLocc achieves 85-99% single-shot\naccuracy and near-100% pass@100. Case study shows that VeriLocc discovers more\nperformant assignments than expert-tuned libraries, outperforming rocBLAS by\nover 10% in runtime.", "AI": {"tldr": "VeriLocc is a framework that combines large language models with formal compiler techniques for register allocation in GPUs, achieving high accuracy and better performance than existing libraries.", "motivation": "The rapid evolution of modern GPUs necessitates new approaches in register allocation, as existing methods are outdated and require significant re-tuning.", "method": "VeriLocc leverages a fine-tuned LLM to translate intermediate representations into specific register assignments, incorporating static analysis for normalization and a verifier-guided loop for correctness.", "result": "VeriLocc achieves 85-99% single-shot accuracy in register allocation and outperforms expert-tuned libraries like rocBLAS by over 10% in runtime.", "conclusion": "The framework demonstrated a significant advancement in generalizable and verifiable register allocation, proving its superiority over traditional methods.", "key_contributions": ["Integration of LLM with formal compiler techniques for GPU register allocation", "High accuracy and performance improvements over existing libraries", "Verification loop ensuring correctness of assignments"], "limitations": "", "keywords": ["GPU", "register allocation", "large language models", "compiler techniques", "performance optimization"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.18749", "pdf": "https://arxiv.org/pdf/2506.18749.pdf", "abs": "https://arxiv.org/abs/2506.18749", "title": "BRAVE: Brain-Controlled Prosthetic Arm with Voice Integration and Embodied Learning for Enhanced Mobility", "authors": ["Abdul Basit", "Maha Nawaz", "Muhammad Shafique"], "categories": ["cs.HC", "cs.AI", "cs.RO", "I.2.9; I.2.7"], "comment": "9 pages, 12 figures, Accepted at IJCNN 2025", "summary": "Non-invasive brain-computer interfaces (BCIs) have the potential to enable\nintuitive control of prosthetic limbs for individuals with upper limb\namputations. However, existing EEG-based control systems face challenges\nrelated to signal noise, classification accuracy, and real-time adaptability.\nIn this work, we present BRAVE, a hybrid EEG and voice-controlled prosthetic\nsystem that integrates ensemble learning-based EEG classification with a\nhuman-in-the-loop (HITL) correction framework for enhanced responsiveness.\nUnlike traditional electromyography (EMG)-based prosthetic control, BRAVE aims\nto interpret EEG-driven motor intent, enabling movement control without\nreliance on residual muscle activity. To improve classification robustness,\nBRAVE combines LSTM, CNN, and Random Forest models in an ensemble framework,\nachieving a classification accuracy of 96% across test subjects. EEG signals\nare preprocessed using a bandpass filter (0.5-45 Hz), Independent Component\nAnalysis (ICA) for artifact removal, and Common Spatial Pattern (CSP) feature\nextraction to minimize contamination from electromyographic (EMG) and\nelectrooculographic (EOG) signals. Additionally, BRAVE incorporates automatic\nspeech recognition (ASR) to facilitate intuitive mode switching between\ndifferent degrees of freedom (DOF) in the prosthetic arm. The system operates\nin real time, with a response latency of 150 ms, leveraging Lab Streaming Layer\n(LSL) networking for synchronized data acquisition. The system is evaluated on\nan in-house fabricated prosthetic arm and on multiple participants highlighting\nthe generalizability across users. The system is optimized for low-power\nembedded deployment, ensuring practical real-world application beyond\nhigh-performance computing environments. Our results indicate that BRAVE offers\na promising step towards robust, real-time, non-invasive prosthetic control.", "AI": {"tldr": "BRAVE is a hybrid EEG and voice-controlled prosthetic system that enhances control of prosthetic limbs using ensemble learning and a human-in-the-loop framework.", "motivation": "To enable intuitive control of prosthetic limbs for individuals with upper limb amputations using non-invasive brain-computer interfaces.", "method": "BRAVE integrates ensemble learning for EEG classification with LSTM, CNN, and Random Forest models, and includes automatic speech recognition for mode switching.", "result": "Achieved a classification accuracy of 96% and real-time operation with a response latency of 150 ms.", "conclusion": "BRAVE offers a promising solution for robust and responsive prosthetic control without relying on residual muscle activity.", "key_contributions": ["Integration of EEG and voice control for prosthetic systems", "High classification accuracy using ensemble learning", "Real-time responsiveness with intuitive control mechanisms"], "limitations": "", "keywords": ["Brain-computer interface", "EEG", "Prosthetic control", "Ensemble learning", "Human-in-the-loop"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.17525", "pdf": "https://arxiv.org/pdf/2506.17525.pdf", "abs": "https://arxiv.org/abs/2506.17525", "title": "Data Quality Issues in Multilingual Speech Datasets: The Need for Sociolinguistic Awareness and Proactive Language Planning", "authors": ["Mingfei Lau", "Qian Chen", "Yeming Fang", "Tingting Xu", "Tongzhou Chen", "Pavel Golik"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Our quality audit for three widely used public multilingual speech datasets -\nMozilla Common Voice 17.0, FLEURS, and VoxPopuli - shows that in some\nlanguages, these datasets suffer from significant quality issues. We believe\naddressing these issues will make these datasets more useful as training and\nevaluation sets, and improve downstream models. We divide these quality issues\ninto two categories: micro-level and macro-level. We find that macro-level\nissues are more prevalent in less institutionalized, often under-resourced\nlanguages. We provide a case analysis of Taiwanese Southern Min (nan_tw) that\nhighlights the need for proactive language planning (e.g. orthography\nprescriptions, dialect boundary definition) and enhanced data quality control\nin the process of Automatic Speech Recognition (ASR) dataset creation. We\nconclude by proposing guidelines and recommendations to mitigate these issues\nin future dataset development, emphasizing the importance of sociolinguistic\nawareness in creating robust and reliable speech data resources.", "AI": {"tldr": "This paper audits three public multilingual speech datasets, identifying quality issues that hinder their effectiveness in training ASR models, particularly in under-resourced languages.", "motivation": "To enhance the quality of widely used multilingual speech datasets for better training and evaluation of Automatic Speech Recognition (ASR) systems.", "method": "Quality audit of Mozilla Common Voice 17.0, FLEURS, and VoxPopuli datasets focusing on micro-level and macro-level quality issues, with a case analysis of Taiwanese Southern Min.", "result": "Macro-level quality issues are more prevalent in less institutionalized languages, necessitating improved data quality control and proactive language planning.", "conclusion": "The paper proposes guidelines to mitigate identified quality issues, stressing the importance of sociolinguistic awareness in ASR dataset development.", "key_contributions": ["Identification of significant quality issues in multilingual speech datasets", "Case analysis demonstrating the impact of language planning on data quality", "Recommendations for enhancing dataset development practices"], "limitations": "", "keywords": ["speech datasets", "quality audit", "ASR", "sociolinguistics", "multilingual"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.18760", "pdf": "https://arxiv.org/pdf/2506.18760.pdf", "abs": "https://arxiv.org/abs/2506.18760", "title": "Patient-Centred Explainability in IVF Outcome Prediction", "authors": ["Adarsa Sivaprasad", "Ehud Reiter", "David McLernon", "Nava Tintarev", "Siladitya Bhattacharya", "Nir Oren"], "categories": ["cs.HC"], "comment": null, "summary": "This paper evaluates the user interface of an in vitro fertility (IVF)\noutcome prediction tool, focussing on its understandability for patients or\npotential patients. We analyse four years of anonymous patient feedback,\nfollowed by a user survey and interviews to quantify trust and\nunderstandability. Results highlight a lay user's need for prediction model\n\\emph{explainability} beyond the model feature space. We identify user concerns\nabout data shifts and model exclusions that impact trust. The results call\nattention to the shortcomings of current practices in explainable AI research\nand design and the need for explainability beyond model feature space and\nepistemic assumptions, particularly in high-stakes healthcare contexts where\nusers gather extensive information and develop complex mental models. To\naddress these challenges, we propose a dialogue-based interface and explore\nuser expectations for personalised explanations.", "AI": {"tldr": "The paper evaluates an IVF outcome prediction tool's user interface, focusing on patient understandability and trust through user feedback and surveys.", "motivation": "To assess the understandability of an IVF outcome prediction tool for patients and improve user trust through better explainability.", "method": "Analysis of four years of anonymous patient feedback, complemented by a user survey and interviews to gauge understanding and trust.", "result": "Findings indicate that lay users require more explainability than what is currently offered, pointing to significant user concerns about data relevance and model exclusions that influence their trust in such tools.", "conclusion": "The study emphasizes the necessity for enhanced explainability in AI solutions within healthcare, especially when users build complex mental models based on extensive information.", "key_contributions": ["Identified user needs for explainability beyond model features.", "Highlighted shortcomings in current explainable AI practices for healthcare.", "Proposed a dialogue-based interface for personalized explanations."], "limitations": "The focus is primarily on a specific healthcare context (IVF), which may not generalize to other areas in healthcare or different AI applications.", "keywords": ["IVF", "user interface", "explainable AI", "trust", "healthcare"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.17533", "pdf": "https://arxiv.org/pdf/2506.17533.pdf", "abs": "https://arxiv.org/abs/2506.17533", "title": "DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning", "authors": ["Yuanhao Wu", "Juntong Song", "Hanning Zhang", "Tong Zhang", "Cheng Niu"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose DuaShepherd, a novel reward modeling framework that\nintegrates two complementary reward signals, correctness and potential, to\nenhance the mathematical reasoning capabilities of Large Language Models\n(LLMs). While correctness-based signals emphasize identification of stepwise\nerrors, potential-based signals focus on the likelihood of reaching the correct\nfinal answer. We developed an automated pipeline for constructing large-scale\nreward modeling dataset with both signals. A unified, multi-head architecture\nwas explored to train the two reward models in a multi-task setup,\ndemonstrating benefits from learning both correctness and potential in\nparallel. By combining these two signals into a compound probability, our model\nachieves consistent performance improvements across multiple benchmarks.\nEmpirical evaluations on MATH500 and ProcessBench confirm that this combined\nreward significantly outperforms models trained on either reward type alone,\nachieving state-of-the-art performance under comparable resource constraints.", "AI": {"tldr": "DuaShepherd is a reward modeling framework that combines correctness and potential signals to improve mathematical reasoning in LLMs.", "motivation": "To enhance the mathematical reasoning capabilities of Large Language Models by integrating two reward signals that reflect different aspects of reasoning.", "method": "Developed an automated pipeline for creating a large-scale dataset combining correctness and potential signals, and employed a unified multi-head architecture to train two reward models in a multi-task setup.", "result": "The model achieves significant performance improvements on MATH500 and ProcessBench, outperforming models trained on either reward alone.", "conclusion": "Combining correctness and potential signals into a compound probability leads to state-of-the-art performance in mathematical reasoning tasks with efficient resource use.", "key_contributions": ["Introduced the DuaShepherd reward modeling framework", "Combined correctness and potential signals for improved performance", "Demonstrated state-of-the-art results on multiple benchmarks"], "limitations": "", "keywords": ["reward modeling", "large language models", "mathematical reasoning", "correctness signal", "potential signal"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.18770", "pdf": "https://arxiv.org/pdf/2506.18770.pdf", "abs": "https://arxiv.org/abs/2506.18770", "title": "Importance of User Control in Data-Centric Steering for Healthcare Experts", "authors": ["Aditya Bhattacharya", "Simone Stumpf", "Katrien Verbert"], "categories": ["cs.HC"], "comment": "It is a pre-print version. For the full paper, please view the actual\n  published version", "summary": "As Artificial Intelligence (AI) becomes increasingly integrated into\nhigh-stakes domains like healthcare, effective collaboration between healthcare\nexperts and AI systems is critical. Data-centric steering, which involves\nfine-tuning prediction models by improving training data quality, plays a key\nrole in this process. However, little research has explored how varying levels\nof user control affect healthcare experts during data-centric steering. We\naddress this gap by examining manual and automated steering approaches through\na between-subjects, mixed-methods user study with 74 healthcare experts. Our\nfindings show that manual steering, which grants direct control over training\ndata, significantly improves model performance while maintaining trust and\nsystem understandability. Based on these findings, we propose design\nimplications for a hybrid steering system that combines manual and automated\napproaches to increase user involvement during human-AI collaboration.", "AI": {"tldr": "This paper investigates how varying levels of user control in data-centric steering influence healthcare experts' collaboration with AI systems, revealing that manual control enhances model performance and trust.", "motivation": "To address the gap in understanding the impact of user control on healthcare experts during data-centric steering with AI systems.", "method": "A between-subjects, mixed-methods user study involving 74 healthcare experts was conducted to compare manual and automated steering approaches.", "result": "Manual steering significantly improves model performance, trust, and system understandability compared to automated approaches.", "conclusion": "The study suggests the development of a hybrid steering system that integrates both manual and automated elements to enhance user involvement in AI collaboration.", "key_contributions": ["Demonstrated the effectiveness of manual steering in improving model performance.", "Identified the importance of trust and understandability in human-AI collaboration.", "Proposed design implications for a hybrid steering system.", ""], "limitations": "", "keywords": ["AI in healthcare", "human-AI collaboration", "data-centric steering"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.17542", "pdf": "https://arxiv.org/pdf/2506.17542.pdf", "abs": "https://arxiv.org/abs/2506.17542", "title": "Probing for Phonology in Self-Supervised Speech Representations: A Case Study on Accent Perception", "authors": ["Nitin Venkateswaran", "Kevin Tang", "Ratree Wayland"], "categories": ["cs.CL"], "comment": null, "summary": "Traditional models of accent perception underestimate the role of gradient\nvariations in phonological features which listeners rely upon for their accent\njudgments. We investigate how pretrained representations from current\nself-supervised learning (SSL) models of speech encode phonological\nfeature-level variations that influence the perception of segmental accent. We\nfocus on three segments: the labiodental approximant, the rhotic tap, and the\nretroflex stop, which are uniformly produced in the English of native speakers\nof Hindi as well as other languages in the Indian sub-continent. We use the\nCSLU Foreign Accented English corpus (Lander, 2007) to extract, for these\nsegments, phonological feature probabilities using Phonet (V\\'asquez-Correa et\nal., 2019) and pretrained representations from Wav2Vec2-BERT (Barrault et al.,\n2023) and WavLM (Chen et al., 2022) along with accent judgements by native\nspeakers of American English. Probing analyses show that accent strength is\nbest predicted by a subset of the segment's pretrained representation features,\nin which perceptually salient phonological features that contrast the expected\nAmerican English and realized non-native English segments are given prominent\nweighting. A multinomial logistic regression of pretrained representation-based\nsegment distances from American and Indian English baselines on accent ratings\nreveals strong associations between the odds of accent strength and distances\nfrom the baselines, in the expected directions. These results highlight the\nvalue of self-supervised speech representations for modeling accent perception\nusing interpretable phonological features.", "AI": {"tldr": "This paper investigates how self-supervised learning models of speech, particularly Wav2Vec2-BERT and WavLM, encode phonological variations affecting the perception of accents in English segments produced by Hindi speakers.", "motivation": "To explore the inadequacies of traditional accent perception models that overlook the importance of gradient phonological variations in listeners' judgments.", "method": "Analyzed pretrained self-supervised learning models Wav2Vec2-BERT and WavLM using the CSLU Foreign Accented English corpus, focusing on specific segments articulated by Hindi speakers and their accent judgments from native American English speakers.", "result": "Probing analyses indicated accent strength is primarily predicted by distinct pretrained representation features associated with salient phonological contrasts between native and non-native English segments.", "conclusion": "The study underscores the utility of self-supervised speech representations in understanding accent perception through interpretable phonological features.", "key_contributions": ["Demonstration of self-supervised models' effectiveness in accent perception.", "Identification of critical phonological features influencing accent judgments.", "Quantitative relationships between segmental variations and perceived accent strength."], "limitations": "", "keywords": ["accent perception", "self-supervised learning", "phonological features", "speech representation", "non-native English"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.18786", "pdf": "https://arxiv.org/pdf/2506.18786.pdf", "abs": "https://arxiv.org/abs/2506.18786", "title": "Flow-Aware Diffusion for Real-Time VR Restoration: Enhancing Spatiotemporal Coherence and Efficiency", "authors": ["Yitong Zhu", "Guanxuan Jiang", "Zhuowen Liang", "Yuyang Wang"], "categories": ["cs.HC"], "comment": null, "summary": "Cybersickness remains a critical barrier to the widespread adoption of\nVirtual Reality (VR), particularly in scenarios involving intense or artificial\nmotion cues. Among the key contributors is excessive optical flow-perceived\nvisual motion that, when unmatched by vestibular input, leads to sensory\nconflict and discomfort. While previous efforts have explored geometric or\nhardware based mitigation strategies, such methods often rely on predefined\nscene structures, manual tuning, or intrusive equipment. In this work, we\npropose U-MAD, a lightweight, real-time, AI-based solution that suppresses\nperceptually disruptive optical flow directly at the image level. Unlike prior\nhandcrafted approaches, this method learns to attenuate high-intensity motion\npatterns from rendered frames without requiring mesh-level editing or scene\nspecific adaptation. Designed as a plug and play module, U-MAD integrates\nseamlessly into existing VR pipelines and generalizes well to procedurally\ngenerated environments. The experiments show that U-MAD consistently reduces\naverage optical flow and enhances temporal stability across diverse scenes. A\nuser study further confirms that reducing visual motion leads to improved\nperceptual comfort and alleviated cybersickness symptoms. These findings\ndemonstrate that perceptually guided modulation of optical flow provides an\neffective and scalable approach to creating more user-friendly immersive\nexperiences. The code will be released at https://github.com/XXXXX (upon\npublication).", "AI": {"tldr": "U-MAD is an AI-based solution that reduces cybersickness in Virtual Reality by suppressing disruptive optical flow at the image level, enhancing user comfort without the need for scene-specific adjustments.", "motivation": "Cybersickness hinders VR adoption due to sensory conflicts caused by unmatched visual and vestibular inputs. This work addresses these issues with a novel AI-based method.", "method": "U-MAD suppresses high-intensity optical flow directly from rendered frames using perceptually guided modulation, functioning as a plug and play module in VR environments.", "result": "Experiments showed significant reductions in optical flow and improvements in temporal stability, while user studies indicated enhanced perceptual comfort and alleviated symptoms of cybersickness.", "conclusion": "The proposed method shows promise in creating more immersive and user-friendly VR experiences through effective modulation of visual motion.", "key_contributions": ["Introduction of U-MAD, an AI-based optical flow suppression method", "Plug and play integration with existing VR systems", "Demonstrated effectiveness in reducing cybersickness symptoms through user studies"], "limitations": "", "keywords": ["Cybersickness", "Virtual Reality", "Optical Flow", "AI-based solution", "User experience"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.17578", "pdf": "https://arxiv.org/pdf/2506.17578.pdf", "abs": "https://arxiv.org/abs/2506.17578", "title": "AgriCHN: A Comprehensive Cross-domain Resource for Chinese Agricultural Named Entity Recognition", "authors": ["Lingxiao Zeng", "Yiqi Tong", "Wei Guo", "Huarui Wu", "Lihao Ge", "Yijun Ye", "Fuzhen Zhuang", "Deqing Wang", "Wei Guo", "Cheng Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Agricultural named entity recognition is a specialized task focusing on\nidentifying distinct agricultural entities within vast bodies of text,\nincluding crops, diseases, pests, and fertilizers. It plays a crucial role in\nenhancing information extraction from extensive agricultural text resources.\nHowever, the scarcity of high-quality agricultural datasets, particularly in\nChinese, has resulted in suboptimal performance when employing mainstream\nmethods for this purpose. Most earlier works only focus on annotating\nagricultural entities while overlook the profound correlation of agriculture\nwith hydrology and meteorology. To fill this blank, we present AgriCHN, a\ncomprehensive open-source Chinese resource designed to promote the accuracy of\nautomated agricultural entity annotation. The AgriCHN dataset has been\nmeticulously curated from a wealth of agricultural articles, comprising a total\nof 4,040 sentences and encapsulating 15,799 agricultural entity mentions\nspanning 27 diverse entity categories. Furthermore, it encompasses entities\nfrom hydrology to meteorology, thereby enriching the diversity of entities\nconsidered. Data validation reveals that, compared with relevant resources,\nAgriCHN demonstrates outstanding data quality, attributable to its richer\nagricultural entity types and more fine-grained entity divisions. A benchmark\ntask has also been constructed using several state-of-the-art neural NER\nmodels. Extensive experimental results highlight the significant challenge\nposed by AgriCHN and its potential for further research.", "AI": {"tldr": "AgriCHN is an open-source Chinese dataset for agricultural named entity recognition, offering high-quality annotations across 27 categories, including relevant hydrological and meteorological entities.", "motivation": "To improve agricultural entity recognition performance due to the lack of high-quality datasets in Chinese, while considering the relationships between agriculture, hydrology, and meteorology.", "method": "Creation of the AgriCHN dataset by curating 4,040 sentences from agricultural articles, containing 15,799 mentions of diverse agricultural entities and incorporating various state-of-the-art neural NER models for benchmarking.", "result": "AgriCHN shows superior data quality compared to existing resources and presents significant challenges for entity recognition tasks.", "conclusion": "The dataset facilitates better automated recognition of agricultural entities and invites further research in this area.", "key_contributions": ["Development of the AgriCHN dataset with extensive annotations for agricultural entities in Chinese.", "Inclusion of diverse entity types beyond agriculture, incorporating hydrological and meteorological aspects.", "Benchmarking against state-of-the-art NER models to reveal challenges and enhance research possibilities."], "limitations": "", "keywords": ["named entity recognition", "agriculture", "Chinese dataset", "machine learning", "data quality"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2506.17314", "pdf": "https://arxiv.org/pdf/2506.17314.pdf", "abs": "https://arxiv.org/abs/2506.17314", "title": "PRAISE: Enhancing Product Descriptions with LLM-Driven Structured Insights", "authors": ["Adnan Qidwai", "Srija Mukhopadhyay", "Prerana Khatiwada", "Dan Roth", "Vivek Gupta"], "categories": ["cs.CL", "cs.HC"], "comment": "9 Pages, 9 Figures. Accepted at ACL 2025 System Demonstration Track", "summary": "Accurate and complete product descriptions are crucial for e-commerce, yet\nseller-provided information often falls short. Customer reviews offer valuable\ndetails but are laborious to sift through manually. We present PRAISE: Product\nReview Attribute Insight Structuring Engine, a novel system that uses Large\nLanguage Models (LLMs) to automatically extract, compare, and structure\ninsights from customer reviews and seller descriptions. PRAISE provides users\nwith an intuitive interface to identify missing, contradictory, or partially\nmatching details between these two sources, presenting the discrepancies in a\nclear, structured format alongside supporting evidence from reviews. This\nallows sellers to easily enhance their product listings for clarity and\npersuasiveness, and buyers to better assess product reliability. Our\ndemonstration showcases PRAISE's workflow, its effectiveness in generating\nactionable structured insights from unstructured reviews, and its potential to\nsignificantly improve the quality and trustworthiness of e-commerce product\ncatalogs.", "AI": {"tldr": "PRAISE is a system that uses LLMs to extract, compare, and structure insights from customer reviews and product descriptions to improve e-commerce product listings.", "motivation": "There is a need for accurate and comprehensive product descriptions in e-commerce, as seller-provided information often lacks detail and customer reviews contain valuable insights that are difficult to analyze manually.", "method": "PRAISE employs Large Language Models to automatically extract relevant information from customer reviews and seller descriptions, comparing and structuring these insights into a clear format.", "result": "PRAISE enables the identification of missing, contradictory, or partially matching product details, presenting these discrepancies alongside supporting evidence from reviews for enhanced clarity.", "conclusion": "By integrating insights from reviews with seller descriptions, PRAISE enhances the quality and trustworthiness of e-commerce product listings, aiding both sellers and buyers.", "key_contributions": ["Development of PRAISE, a novel automated system for product review analysis", "Effective extraction and structuring of insights using LLMs", "User-friendly interface that highlights discrepancies and supports sellers in improving listings"], "limitations": "", "keywords": ["Large Language Models", "E-commerce", "Customer Reviews", "Product Descriptions", "Information Extraction"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2506.17603", "pdf": "https://arxiv.org/pdf/2506.17603.pdf", "abs": "https://arxiv.org/abs/2506.17603", "title": "Mind the Gap: Assessing Wiktionary's Crowd-Sourced Linguistic Knowledge on Morphological Gaps in Two Related Languages", "authors": ["Jonathan Sakunkoo", "Annabella Sakunkoo"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Morphological defectivity is an intriguing and understudied phenomenon in\nlinguistics. Addressing defectivity, where expected inflectional forms are\nabsent, is essential for improving the accuracy of NLP tools in morphologically\nrich languages. However, traditional linguistic resources often lack coverage\nof morphological gaps as such knowledge requires significant human expertise\nand effort to document and verify. For scarce linguistic phenomena in\nunder-explored languages, Wikipedia and Wiktionary often serve as among the few\naccessible resources. Despite their extensive reach, their reliability has been\na subject of controversy. This study customizes a novel neural morphological\nanalyzer to annotate Latin and Italian corpora. Using the massive annotated\ndata, crowd-sourced lists of defective verbs compiled from Wiktionary are\nvalidated computationally. Our results indicate that while Wiktionary provides\na highly reliable account of Italian morphological gaps, 7% of Latin lemmata\nlisted as defective show strong corpus evidence of being non-defective. This\ndiscrepancy highlights potential limitations of crowd-sourced wikis as\ndefinitive sources of linguistic knowledge, particularly for less-studied\nphenomena and languages, despite their value as resources for rare linguistic\nfeatures. By providing scalable tools and methods for quality assurance of\ncrowd-sourced data, this work advances computational morphology and expands\nlinguistic knowledge of defectivity in non-English, morphologically rich\nlanguages.", "AI": {"tldr": "This paper addresses morphological defectivity in linguistics and validates crowd-sourced data on defective verbs from Wiktionary using a custom neural morphological analyzer.", "motivation": "Improving NLP tools in morphologically rich languages by addressing the phenomenon of morphological defectivity, which is often overlooked in traditional linguistic resources.", "method": "A novel neural morphological analyzer was customized to annotate Latin and Italian corpora, and crowd-sourced lists of defective verbs from Wiktionary were validated using this annotated data.", "result": "Wiktionary is found to reliably account for Italian morphological gaps; however, 7% of Latin entries identified as defective were supported by evidence showing they are not defective.", "conclusion": "The study reveals the limitations of crowd-sourced resources like Wiktionary for definitive linguistic knowledge, while offering tools for quality assurance in computational morphology.", "key_contributions": ["Custom neural morphological analyzer for Latin and Italian corpora.", "Validation of crowd-sourced defective verb lists from Wiktionary.", "Insight into the reliability of Wiktionary data for morphological gaps in lesser-studied languages."], "limitations": "Only focuses on Latin and Italian; findings may not generalize to other languages or linguistic phenomena.", "keywords": ["Morphological defectivity", "NLP tools", "Wiktionary", "Computational morphology", "Crowd-sourced data"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.17609", "pdf": "https://arxiv.org/pdf/2506.17609.pdf", "abs": "https://arxiv.org/abs/2506.17609", "title": "TyphoFormer: Language-Augmented Transformer for Accurate Typhoon Track Forecasting", "authors": ["Lincan Li", "Eren Erman Ozguven", "Yue Zhao", "Guang Wang", "Yiqun Xie", "Yushun Dong"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Accurate typhoon track forecasting is crucial for early system warning and\ndisaster response. While Transformer-based models have demonstrated strong\nperformance in modeling the temporal dynamics of dense trajectories of humans\nand vehicles in smart cities, they usually lack access to broader contextual\nknowledge that enhances the forecasting reliability of sparse meteorological\ntrajectories, such as typhoon tracks. To address this challenge, we propose\nTyphoFormer, a novel framework that incorporates natural language descriptions\nas auxiliary prompts to improve typhoon trajectory forecasting. For each time\nstep, we use Large Language Model (LLM) to generate concise textual\ndescriptions based on the numerical attributes recorded in the North Atlantic\nhurricane database. The language descriptions capture high-level meteorological\nsemantics and are embedded as auxiliary special tokens prepended to the\nnumerical time series input. By integrating both textual and sequential\ninformation within a unified Transformer encoder, TyphoFormer enables the model\nto leverage contextual cues that are otherwise inaccessible through numerical\nfeatures alone. Extensive experiments are conducted on HURDAT2 benchmark,\nresults show that TyphoFormer consistently outperforms other state-of-the-art\nbaseline methods, particularly under challenging scenarios involving nonlinear\npath shifts and limited historical observations.", "AI": {"tldr": "TyphoFormer is a novel framework that improves typhoon trajectory forecasting by integrating natural language descriptions generated by a Large Language Model with traditional numerical data.", "motivation": "Forecasting typhoon tracks is essential for disaster response, and existing models often lack broader contextual knowledge needed for reliability in sparse meteorological data.", "method": "TyphoFormer utilizes a Transformer architecture that combines numerical meteorological data with auxiliary textual descriptions derived from the North Atlantic hurricane database, enhancing the model's understanding of high-level meteorological contexts.", "result": "TyphoFormer outperforms state-of-the-art forecasting methods, especially in scenarios with nonlinear path shifts and limited historical data.", "conclusion": "The integration of LLM-generated textual prompts significantly enhances the accuracy of typhoon track forecasting, making TyphoFormer a valuable tool for meteorological predictions.", "key_contributions": ["Introduction of TyphoFormer framework for typhoon forecasting.", "Use of LLM to create auxiliary textual descriptions from numerical data.", "Demonstrated improved forecasting performance on HURDAT2 benchmark."], "limitations": "", "keywords": ["Typhoon forecasting", "Transformer", "Natural language processing", "Large Language Model", "Meteorological data"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2506.17611", "pdf": "https://arxiv.org/pdf/2506.17611.pdf", "abs": "https://arxiv.org/abs/2506.17611", "title": "OpusLM: A Family of Open Unified Speech Language Models", "authors": ["Jinchuan Tian", "William Chen", "Yifan Peng", "Jiatong Shi", "Siddhant Arora", "Shikhar Bharadwaj", "Takashi Maekaku", "Yusuke Shinohara", "Keita Goto", "Xiang Yue", "Huck Yang", "Shinji Watanabe"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents Open Unified Speech Language Models (OpusLMs), a family\nof open foundational speech language models (SpeechLMs) up to 7B. Initialized\nfrom decoder-only text language models, the OpusLMs are continuously\npre-trained on 213K hours of speech-text pairs and 292B text-only tokens. We\ndemonstrate our OpusLMs achieve comparable (or even superior) performance with\nexisting SpeechLMs in speech recognition, speech synthesis, and text-only\ncapabilities. Technically, this paper articulates our SpeechLM designs on\ntokenization, multi-stream language models, and multi-stage training\nstrategies. We experimentally demonstrate the importance of model size scaling\nand the effect of annealing data selection. The OpusLMs are all built from\npublicly available materials and are fully transparent models. We release our\ncode, data, checkpoints, and training logs to facilitate open SpeechLM research", "AI": {"tldr": "This paper introduces Open Unified Speech Language Models (OpusLMs), highlighting their performance in speech recognition, synthesis, and text capabilities, established through continuous pre-training on extensive datasets.", "motivation": "The need for open foundational speech language models that are scalable and transparent for researchers in the field.", "method": "OpusLMs are initialized from decoder-only text language models and pre-trained on a large database of speech-text pairs and text-only tokens, with a focus on tokenization, multi-stream models, and multi-stage training strategies.", "result": "OpusLMs demonstrate comparable or superior performance to existing SpeechLMs in various tasks, including speech recognition and synthesis.", "conclusion": "The transparency and open-access nature of OpusLMs contribute significantly to ongoing research in speech language models, providing a solid foundation for future work.", "key_contributions": ["Introduction of OpusLMs as scalable speech language models", "Demonstration of performance on speech recognition and synthesis", "Provision of all training materials and models as fully open resources."], "limitations": "", "keywords": ["Speech Language Models", "OpusLMs", "Speech Recognition", "Speech Synthesis", "Open Research"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.17467", "pdf": "https://arxiv.org/pdf/2506.17467.pdf", "abs": "https://arxiv.org/abs/2506.17467", "title": "Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems", "authors": ["Weixin Liang"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "Stanford CS PhD Dissertation", "summary": "Large language models (LLMs) have shown significant potential to change how\nwe write, communicate, and create, leading to rapid adoption across society.\nThis dissertation examines how individuals and institutions are adapting to and\nengaging with this emerging technology through three research directions.\nFirst, I demonstrate how the institutional adoption of AI detectors introduces\nsystematic biases, particularly disadvantaging writers of non-dominant language\nvarieties, highlighting critical equity concerns in AI governance. Second, I\npresent novel population-level algorithmic approaches that measure the\nincreasing adoption of LLMs across writing domains, revealing consistent\npatterns of AI-assisted content in academic peer reviews, scientific\npublications, consumer complaints, corporate communications, job postings, and\ninternational organization press releases. Finally, I investigate LLMs'\ncapability to provide feedback on research manuscripts through a large-scale\nempirical analysis, offering insights into their potential to support\nresearchers who face barriers in accessing timely manuscript feedback,\nparticularly early-career researchers and those from under-resourced settings.", "AI": {"tldr": "This dissertation explores the adaptation and engagement of individuals and institutions with large language models (LLMs) through three key research directions: biases in AI detectors, patterns of LLM adoption across writing domains, and the feedback provision capability of LLMs in supporting research manuscript development.", "motivation": "To understand the societal impact and institutional adaptation to large language models, highlighting issues of equity and accessibility in AI governance and manuscript feedback.", "method": "The dissertation employs a mixed-methods approach including empirical analysis, algorithmic population-level measurement, and examination of AI detector biases.", "result": "Demonstrated that AI detectors introduce biases against non-dominant language varieties, identified consistent patterns of LLM adoption in diverse writing domains, and revealed LLMs' potential to support early-career researchers with timely feedback.", "conclusion": "The findings underscore the need for equitable AI governance and highlight the role of LLMs in enhancing manuscript feedback for under-resourced researchers.", "key_contributions": ["Highlighting biases in AI governance related to language variety", "Identifying patterns of LLM adoption across various writing contexts", "Demonstrating LLMs' potential for providing feedback to researchers in need."], "limitations": "The study may not fully capture the long-term implications of LLM adoption on writing processes and institutional practices.", "keywords": ["large language models", "AI detectors", "equity in AI", "manuscript feedback", "algorithmic approaches"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.17630", "pdf": "https://arxiv.org/pdf/2506.17630.pdf", "abs": "https://arxiv.org/abs/2506.17630", "title": "Answer-Centric or Reasoning-Driven? Uncovering the Latent Memory Anchor in LLMs", "authors": ["Yang Wu", "Yifan Zhang", "Yiwei Wang", "Yujun Cai", "Yurong Wu", "Yuran Wang", "Ning Xu", "Jian Cheng"], "categories": ["cs.CL"], "comment": "14 pages, 8 figures", "summary": "While Large Language Models (LLMs) demonstrate impressive reasoning\ncapabilities, growing evidence suggests much of their success stems from\nmemorized answer-reasoning patterns rather than genuine inference. In this\nwork, we investigate a central question: are LLMs primarily anchored to final\nanswers or to the textual pattern of reasoning chains? We propose a five-level\nanswer-visibility prompt framework that systematically manipulates answer cues\nand probes model behavior through indirect, behavioral analysis. Experiments\nacross state-of-the-art LLMs reveal a strong and consistent reliance on\nexplicit answers. The performance drops by 26.90\\% when answer cues are masked,\neven with complete reasoning chains. These findings suggest that much of the\nreasoning exhibited by LLMs may reflect post-hoc rationalization rather than\ntrue inference, calling into question their inferential depth. Our study\nuncovers the answer-anchoring phenomenon with rigorous empirical validation and\nunderscores the need for a more nuanced understanding of what constitutes\nreasoning in LLMs.", "AI": {"tldr": "This paper investigates the reliance of Large Language Models (LLMs) on final answers versus reasoning patterns, revealing substantial reliance on explicit answers.", "motivation": "To understand whether LLMs rely more on final answers or the reasoning chains that lead to those answers, which influences their perceived reasoning capabilities.", "method": "A five-level answer-visibility prompt framework was proposed to manipulate answer cues and analyze model behavior through indirect behavioral analysis across state-of-the-art LLMs.", "result": "Experiments showed a 26.90% performance drop when answer cues were masked, suggesting LLMs prioritize explicit answers over reasoning.", "conclusion": "The findings question the inferential depth of LLMs, indicating their reasoning may be more about post-hoc rationalization than true inference.", "key_contributions": ["Introduction of a five-level answer-visibility prompt framework", "Empirical evidence showing LLMs' dependence on explicit answers", "Insights into the nature of reasoning in LLMs"], "limitations": "Limited to observed behaviors in certain state-of-the-art LLMs; may not generalize across all LLM architectures.", "keywords": ["Large Language Models", "reasoning", "answer visibility", "prompt framework", "behavioral analysis"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2506.17637", "pdf": "https://arxiv.org/pdf/2506.17637.pdf", "abs": "https://arxiv.org/abs/2506.17637", "title": "Step-Opt: Boosting Optimization Modeling in LLMs through Iterative Data Synthesis and Structured Validation", "authors": ["Yang Wu", "Yifan Zhang", "Yurong Wu", "Yuran Wang", "Junkai Zhang", "Jian Cheng"], "categories": ["cs.CL", "cs.LG"], "comment": "17 pages, 12 figures", "summary": "Large Language Models (LLMs) have revolutionized various domains but\nencounter substantial challenges in tackling optimization modeling tasks for\nOperations Research (OR), particularly when dealing with complex problem. In\nthis work, we propose Step-Opt-Instruct, a framework that augments existing\ndatasets and generates high-quality fine-tuning data tailored to optimization\nmodeling. Step-Opt-Instruct employs iterative problem generation to\nsystematically increase problem complexity and stepwise validation to\nrigorously verify data, preventing error propagation and ensuring the quality\nof the generated dataset. Leveraging this framework, we fine-tune open-source\nLLMs, including LLaMA-3-8B and Mistral-7B, to develop Step-Opt--a model that\nachieves state-of-the-art performance on benchmarks such as NL4OPT, MAMO, and\nIndustryOR. Extensive experiments demonstrate the superior performance of\nStep-Opt, especially in addressing complex OR tasks, with a notable 17.01\\%\nimprovement in micro average accuracy on difficult problems. These findings\nhighlight the effectiveness of combining structured validation with gradual\nproblem refinement to advance the automation of decision-making processes using\nLLMs.The code and dataset are available at https://github.com/samwu-learn/Step.", "AI": {"tldr": "This paper introduces Step-Opt-Instruct, a framework to enhance fine-tuning of LLMs for optimization modeling in Operations Research, showing significant performance improvements.", "motivation": "The paper addresses the challenges faced by LLMs in optimization modeling tasks within Operations Research, especially for complex problems.", "method": "Step-Opt-Instruct utilizes iterative problem generation to increase complexity and stepwise validation to ensure data quality for fine-tuning LLMs like LLaMA-3-8B and Mistral-7B.", "result": "The fine-tuned model, Step-Opt, achieves state-of-the-art performance on benchmarks such as NL4OPT, MAMO, and IndustryOR, with a 17.01% improvement in accuracy on challenging problems.", "conclusion": "The study demonstrates the success of integrating structured validation with gradual problem refinement to enhance decision-making automation through LLMs.", "key_contributions": ["Introduction of Step-Opt-Instruct framework for generating fine-tuning data.", "Significant performance improvement of Step-Opt on optimization tasks.", "Public availability of code and dataset for reproducibility."], "limitations": "", "keywords": ["Large Language Models", "Optimization Modeling", "Operations Research", "Fine-tuning", "Decision-making Automation"], "importance_score": 8, "read_time_minutes": 17}}
{"id": "2506.17671", "pdf": "https://arxiv.org/pdf/2506.17671.pdf", "abs": "https://arxiv.org/abs/2506.17671", "title": "TPTT: Transforming Pretrained Transformer into Titans", "authors": ["Fabien Furfaro"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "6 pages, 1 figure", "summary": "Recent advances in large language models (LLMs) have led to remarkable\nprogress in natural language processing, but their computational and memory\ndemands remain a significant challenge, particularly for long-context\ninference. We introduce TPTT (Transforming Pretrained Transformer into Titans),\na novel framework for enhancing pretrained Transformer models with efficient\nlinearized attention mechanisms and advanced memory management. TPTT employs\ntechniques such as Memory as Gate (MaG) and mixed linearized attention (LiZA).\nIt is fully compatible with the Hugging Face Transformers library, enabling\nseamless adaptation of any causal LLM through parameter-efficient fine-tuning\n(LoRA) without full retraining. We show the effectiveness of TPTT on the MMLU\nbenchmark with models of approximately 1 billion parameters, observing\nsubstantial improvements in both efficiency and accuracy. For instance,\nTitans-Llama-3.2-1B achieves a 20% increase in Exact Match (EM) over its\nbaseline. Statistical analyses and comparisons with recent state-of-the-art\nmethods confirm the practical scalability and robustness of TPTT. Code is\navailable at https://github.com/fabienfrfr/tptt . Python package at\nhttps://pypi.org/project/tptt/ .", "AI": {"tldr": "TPTT enhances Transformers with efficient linearized attention and memory management, improving long-context inference without full retraining.", "motivation": "To address the computational and memory challenges faced by large language models, particularly in long-context inference.", "method": "TPTT employs Memory as Gate (MaG) and mixed linearized attention (LiZA) techniques to enhance pretrained Transformers.", "result": "TPTT demonstrated significant improvements in efficiency and accuracy, with a 20% increase in Exact Match on the MMLU benchmark for a model with 1 billion parameters.", "conclusion": "TPTT's framework is scalable and robust, compatible with Hugging Face Transformers, and facilitates parameter-efficient fine-tuning.", "key_contributions": ["Developing TPTT framework for optimized large language models", "Introducing Memory as Gate and mixed linearized attention", "Providing open-source implementation and significant benchmark results"], "limitations": "", "keywords": ["large language models", "transformer models", "memory management", "linearized attention", "parameter-efficient fine-tuning"], "importance_score": 9, "read_time_minutes": 6}}
{"id": "2506.18199", "pdf": "https://arxiv.org/pdf/2506.18199.pdf", "abs": "https://arxiv.org/abs/2506.18199", "title": "Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review", "authors": ["Bushra Asseri", "Estabrag Abdelaziz", "Areej Al-Wabil"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Large language models have demonstrated remarkable capabilities across\nvarious domains, yet concerns about cultural bias - particularly towards Arabs\nand Muslims - pose significant ethical challenges by perpetuating harmful\nstereotypes and marginalization. Despite growing recognition of bias in LLMs,\nprompt engineering strategies specifically addressing Arab and Muslim\nrepresentation remain understudied. This mixed-methods systematic review\nexamines such techniques, offering evidence-based guidance for researchers and\npractitioners. Following PRISMA guidelines and Kitchenham's systematic review\nmethodology, we analyzed 8 empirical studies published between 2021-2024\ninvestigating bias mitigation strategies. Our findings reveal five primary\nprompt engineering approaches: cultural prompting, affective priming,\nself-debiasing techniques, structured multi-step pipelines, and\nparameter-optimized continuous prompts. Although all approaches show potential\nfor reducing bias, effectiveness varied substantially across studies and bias\ntypes. Evidence suggests that certain bias types may be more resistant to\nprompt-based mitigation than others. Structured multi-step pipelines\ndemonstrated the highest overall effectiveness, achieving up to 87.7% reduction\nin bias, though they require greater technical expertise. Cultural prompting\noffers broader accessibility with substantial effectiveness. These results\nunderscore the accessibility of prompt engineering for mitigating cultural bias\nwithout requiring access to model parameters. The limited number of studies\nidentified highlights a significant research gap in this critical area. Future\nresearch should focus on developing culturally adaptive prompting techniques,\ncreating Arab and Muslim-specific evaluation resources, and integrating prompt\nengineering with complementary debiasing methods to address deeper stereotypes\nwhile maintaining model utility.", "AI": {"tldr": "This review examines prompt engineering strategies to mitigate cultural bias in large language models, focusing on Arab and Muslim representation, revealing effective techniques and research gaps.", "motivation": "Concerns about cultural bias in large language models, particularly toward Arabs and Muslims, illustrate the need for effective bias mitigation strategies.", "method": "A mixed-methods systematic review using PRISMA guidelines and Kitchenham's methodology to analyze 8 empirical studies from 2021-2024.", "result": "Five primary prompt engineering approaches were identified: cultural prompting, affective priming, self-debiasing techniques, structured multi-step pipelines, and parameter-optimized continuous prompts. Structured multi-step pipelines achieved the highest bias reduction, up to 87.7%.", "conclusion": "Access to effective prompt engineering techniques can help mitigate cultural bias without the need for model parameter access. There is a significant research gap in developing tailored strategies for Arab and Muslim representation.", "key_contributions": ["Identification of five effective prompt engineering approaches for bias mitigation", "Highlighting the effectiveness of structured multi-step pipelines", "Emphasizing the need for culturally adaptive prompting techniques"], "limitations": "Limited number of studies identified indicates a significant research gap in the area of bias mitigation for Arab and Muslim representation.", "keywords": ["cultural bias", "prompt engineering", "large language models", "bias mitigation", "systematic review"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.17692", "pdf": "https://arxiv.org/pdf/2506.17692.pdf", "abs": "https://arxiv.org/abs/2506.17692", "title": "Resource-Friendly Dynamic Enhancement Chain for Multi-Hop Question Answering", "authors": ["Binquan Ji", "Haibo Luo", "Yifei Lu", "Lei Hei", "Jiaqi Wang", "Tingjing Liao", "Lingyu Wang", "Shichao Wang", "Feiliang Ren"], "categories": ["cs.CL"], "comment": null, "summary": "Knowledge-intensive multi-hop question answering (QA) tasks, which require\nintegrating evidence from multiple sources to address complex queries, often\nnecessitate multiple rounds of retrieval and iterative generation by large\nlanguage models (LLMs). However, incorporating many documents and extended\ncontexts poses challenges -such as hallucinations and semantic drift-for\nlightweight LLMs with fewer parameters. This work proposes a novel framework\ncalled DEC (Dynamic Enhancement Chain). DEC first decomposes complex questions\ninto logically coherent subquestions to form a hallucination-free reasoning\nchain. It then iteratively refines these subquestions through context-aware\nrewriting to generate effective query formulations. For retrieval, we introduce\na lightweight discriminative keyword extraction module that leverages extracted\nkeywords to achieve targeted, precise document recall with relatively low\ncomputational overhead. Extensive experiments on three multi-hop QA datasets\ndemonstrate that DEC performs on par with or surpasses state-of-the-art\nbenchmarks while significantly reducing token consumption. Notably, our\napproach attains state-of-the-art results on models with 8B parameters,\nshowcasing its effectiveness in various scenarios, particularly in\nresource-constrained environments.", "AI": {"tldr": "Introduction of a framework called DEC for multi-hop question answering using LLMs.", "motivation": "Address challenges in multi-hop QA tasks like hallucinations and semantic drift in lightweight LLMs.", "method": "DEC decomposes complex questions into subquestions and refines them through context-aware rewriting for effective query formulation and keyword extraction for targeted retrieval.", "result": "DEC performs on par with or surpasses state-of-the-art benchmarks while reducing token consumption, achieving state-of-the-art results with models of 8B parameters.", "conclusion": "DEC demonstrates effectiveness in multi-hop QA tasks, especially in resource-constrained environments.", "key_contributions": ["Proposes the DEC framework for multi-hop QA.", "Implements a keyword extraction module for precise document retrieval.", "Achieves state-of-the-art performance with lightweight models."], "limitations": "", "keywords": ["multi-hop QA", "large language models", "keyword extraction", "resource-constrained environments"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.18201", "pdf": "https://arxiv.org/pdf/2506.18201.pdf", "abs": "https://arxiv.org/abs/2506.18201", "title": "Deciphering Emotions in Children Storybooks: A Comparative Analysis of Multimodal LLMs in Educational Applications", "authors": ["Bushra Asseri", "Estabraq Abdelaziz", "Maha Al Mogren", "Tayef Alhefdhi", "Areej Al-Wabil"], "categories": ["cs.CL", "cs.CV", "cs.HC"], "comment": null, "summary": "Emotion recognition capabilities in multimodal AI systems are crucial for\ndeveloping culturally responsive educational technologies, yet remain\nunderexplored for Arabic language contexts where culturally appropriate\nlearning tools are critically needed. This study evaluates the emotion\nrecognition performance of two advanced multimodal large language models,\nGPT-4o and Gemini 1.5 Pro, when processing Arabic children's storybook\nillustrations. We assessed both models across three prompting strategies\n(zero-shot, few-shot, and chain-of-thought) using 75 images from seven Arabic\nstorybooks, comparing model predictions with human annotations based on\nPlutchik's emotional framework. GPT-4o consistently outperformed Gemini across\nall conditions, achieving the highest macro F1-score of 59% with\nchain-of-thought prompting compared to Gemini's best performance of 43%. Error\nanalysis revealed systematic misclassification patterns, with valence\ninversions accounting for 60.7% of errors, while both models struggled with\nculturally nuanced emotions and ambiguous narrative contexts. These findings\nhighlight fundamental limitations in current models' cultural understanding and\nemphasize the need for culturally sensitive training approaches to develop\neffective emotion-aware educational technologies for Arabic-speaking learners.", "AI": {"tldr": "This study assesses the emotion recognition performance of GPT-4o and Gemini 1.5 Pro in analyzing Arabic children's storybook illustrations, revealing significant gaps in cultural understanding and model efficacy.", "motivation": "Emotion recognition in AI is crucial for creating culturally responsive educational technologies, particularly in Arabic contexts.", "method": "The study evaluated GPT-4o and Gemini 1.5 Pro using three prompting strategies (zero-shot, few-shot, chain-of-thought) on 75 images from Arabic storybooks, measuring performance against human annotations based on Plutchik's emotional framework.", "result": "GPT-4o outperformed Gemini with the highest macro F1-score of 59% (chain-of-thought prompting) versus Geminiâs 43%. Error analysis indicated 60.7% of errors were due to valence inversions, highlighting misclassification issues with culturally nuanced emotions.", "conclusion": "The findings underscore the need for culturally sensitive training approaches to enhance emotion recognition in AI, especially for educational applications targeting Arabic-speaking learners.", "key_contributions": ["Performance comparison between two advanced multimodal large language models in an underexplored context", "Identification of systematic misclassification patterns in emotion recognition", "Emphasis on the importance of culturally responsive AI training methods"], "limitations": "Both models struggled with culturally nuanced emotions and ambiguous narrative contexts.", "keywords": ["emotion recognition", "multimodal AI", "Arabic language", "educational technologies", "culturally sensitive training"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.17693", "pdf": "https://arxiv.org/pdf/2506.17693.pdf", "abs": "https://arxiv.org/abs/2506.17693", "title": "Zero-Shot Conversational Stance Detection: Dataset and Approaches", "authors": ["Yuzhe Ding", "Kang He", "Bobo Li", "Li Zheng", "Haijun He", "Fei Li", "Chong Teng", "Donghong Ji"], "categories": ["cs.CL", "cs.LG"], "comment": "ACL 2025 (Findings)", "summary": "Stance detection, which aims to identify public opinion towards specific\ntargets using social media data, is an important yet challenging task. With the\nincreasing number of online debates among social media users, conversational\nstance detection has become a crucial research area. However, existing\nconversational stance detection datasets are restricted to a limited set of\nspecific targets, which constrains the effectiveness of stance detection models\nwhen encountering a large number of unseen targets in real-world applications.\nTo bridge this gap, we manually curate a large-scale, high-quality zero-shot\nconversational stance detection dataset, named ZS-CSD, comprising 280 targets\nacross two distinct target types. Leveraging the ZS-CSD dataset, we propose\nSITPCL, a speaker interaction and target-aware prototypical contrastive\nlearning model, and establish the benchmark performance in the zero-shot\nsetting. Experimental results demonstrate that our proposed SITPCL model\nachieves state-of-the-art performance in zero-shot conversational stance\ndetection. Notably, the SITPCL model attains only an F1-macro score of 43.81%,\nhighlighting the persistent challenges in zero-shot conversational stance\ndetection.", "AI": {"tldr": "This paper introduces a large-scale zero-shot conversational stance detection dataset and a model (SITPCL) that achieves state-of-the-art performance in this area.", "motivation": "The study addresses the limitations of existing conversational stance detection datasets that focus on a narrow range of targets, which hampers model performance in real-world applications with diverse targets.", "method": "A novel dataset, ZS-CSD, is curated consisting of 280 targets, and a model named SITPCL is proposed that utilizes speaker interaction and target-awareness within a contrastive learning framework.", "result": "The SITPCL model demonstrates promising capabilities in zero-shot conversational stance detection, achieving a benchmark F1-macro score of 43.81%.", "conclusion": "Despite achieving state-of-the-art results in zero-shot stance detection, challenges remain, as indicated by the relatively low F1-macro score.", "key_contributions": ["Introduction of a large-scale zero-shot conversational stance detection dataset (ZS-CSD).", "Proposal of the SITPCL model for stance detection leveraging speaker interaction and target awareness.", "Establishment of benchmark performance for zero-shot conversational stance detection."], "limitations": "The F1-macro score remains low (43.81%), indicating ongoing challenges in zero-shot conversational stance detection.", "keywords": ["stance detection", "zero-shot learning", "conversational AI", "social media", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.17700", "pdf": "https://arxiv.org/pdf/2506.17700.pdf", "abs": "https://arxiv.org/abs/2506.17700", "title": "The Evolution of Natural Language Processing: How Prompt Optimization and Language Models are Shaping the Future", "authors": ["Summra Saleem", "Muhammad Nabeel Asim", "Shaista Zulfiqar", "Andreas Dengel"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have revolutionized the field of Natural\nLanguage Processing (NLP) by automating traditional labor-intensive tasks and\nconsequently accelerated the development of computer-aided applications. As\nresearchers continue to advance this field with the introduction of novel\nlanguage models and more efficient training/finetuning methodologies, the idea\nof prompt engineering and subsequent optimization strategies with LLMs has\nemerged as a particularly impactful trend to yield a substantial performance\nboost across diverse NLP tasks. To best of our knowledge numerous review\narticles have explored prompt engineering, however, a critical gap exists in\ncomprehensive analyses of prompt optimization strategies. To bridge this gap\nthis paper provides unique and comprehensive insights about the potential of\ndiverse prompt optimization strategies. It analyzes their underlying working\nparadigms and based on these principles, categorizes them into 11 distinct\nclasses. Moreover, the paper provides details about various NLP tasks where\nthese prompt optimization strategies have been employed, along with details of\ndifferent LLMs and benchmark datasets used for evaluation. This comprehensive\ncompilation lays a robust foundation for future comparative studies and enables\nrigorous assessment of prompt optimization and LLM-based predictive pipelines\nunder consistent experimental settings: a critical need in the current\nlandscape. Ultimately, this research will centralize diverse strategic\nknowledge to facilitate the adaptation of existing prompt optimization\nstrategies for development of innovative predictors across unexplored tasks.", "AI": {"tldr": "This paper reviews and categorizes diverse prompt optimization strategies for Large Language Models (LLMs) used in NLP tasks, addressing existing gaps in literature.", "motivation": "Despite extensive reviews on prompt engineering, there is a lack of comprehensive analyses on prompt optimization strategies in LLMs.", "method": "The paper categorizes 11 distinct prompt optimization strategies and analyzes their applications across various NLP tasks and LLMs using benchmark datasets.", "result": "The analysis reveals how different strategies can significantly enhance the performance of LLMs across diverse NLP tasks, laying the groundwork for future studies.", "conclusion": "This research highlights the importance of systematic evaluations of prompt optimization techniques to improve LLM applications in various domains.", "key_contributions": ["Categorization of 11 distinct prompt optimization strategies", "Comprehensive analysis of prompt optimization across various NLP tasks", "Establishment of a foundation for future comparative studies"], "limitations": "", "keywords": ["Large Language Models", "Natural Language Processing", "Prompt Optimization", "NLP Tasks", "Machine Learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.17708", "pdf": "https://arxiv.org/pdf/2506.17708.pdf", "abs": "https://arxiv.org/abs/2506.17708", "title": "Aged to Perfection: Machine-Learning Maps of Age in Conversational English", "authors": ["MingZe Tang"], "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 11 figures", "summary": "The study uses the British National Corpus 2014, a large sample of\ncontemporary spoken British English, to investigate language patterns across\ndifferent age groups. Our research attempts to explore how language patterns\nvary between different age groups, exploring the connection between speaker\ndemographics and linguistic factors such as utterance duration, lexical\ndiversity, and word choice. By merging computational language analysis and\nmachine learning methodologies, we attempt to uncover distinctive linguistic\nmarkers characteristic of multiple generations and create prediction models\nthat can consistently estimate the speaker's age group from various aspects.\nThis work contributes to our knowledge of sociolinguistic diversity throughout\nthe life of modern British speech.", "AI": {"tldr": "The study analyzes language patterns in British English across age groups using computational and machine learning techniques.", "motivation": "To explore how language patterns vary across different age groups by analyzing speaker demographics and linguistic features.", "method": "Utilizes the British National Corpus 2014 and applies computational language analysis alongside machine learning to identify linguistic markers associated with various generations and develop predictive models.", "result": "Uncovered distinctive linguistic markers for different age groups and developed models to reliably estimate the speaker's age based on linguistic factors.", "conclusion": "The research enhances understanding of sociolinguistic diversity in modern British speech and provides insights that may aid further linguistic and demographic studies.", "key_contributions": ["Investigated age-related language patterns in contemporary British English.", "Merged computational analysis with machine learning to identify linguistic markers.", "Developed predictive models for estimating age from linguistic data."], "limitations": "", "keywords": ["sociolinguistics", "machine learning", "British National Corpus", "language patterns", "generational differences"], "importance_score": 4, "read_time_minutes": 6}}
{"id": "2506.17715", "pdf": "https://arxiv.org/pdf/2506.17715.pdf", "abs": "https://arxiv.org/abs/2506.17715", "title": "Unveiling Factors for Enhanced POS Tagging: A Study of Low-Resource Medieval Romance Languages", "authors": ["Matthias SchÃ¶ffel", "Esteban Garces Arias", "Marinus Wiedner", "Paula Ruppert", "Meimingwei Li", "Christian Heumann", "Matthias AÃenmacher"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Part-of-speech (POS) tagging remains a foundational component in natural\nlanguage processing pipelines, particularly critical for historical text\nanalysis at the intersection of computational linguistics and digital\nhumanities. Despite significant advancements in modern large language models\n(LLMs) for ancient languages, their application to Medieval Romance languages\npresents distinctive challenges stemming from diachronic linguistic evolution,\nspelling variations, and labeled data scarcity. This study systematically\ninvestigates the central determinants of POS tagging performance across diverse\ncorpora of Medieval Occitan, Medieval Spanish, and Medieval French texts,\nspanning biblical, hagiographical, medical, and dietary domains. Through\nrigorous experimentation, we evaluate how fine-tuning approaches, prompt\nengineering, model architectures, decoding strategies, and cross-lingual\ntransfer learning techniques affect tagging accuracy. Our results reveal both\nnotable limitations in LLMs' ability to process historical language variations\nand non-standardized spelling, as well as promising specialized techniques that\neffectively address the unique challenges presented by low-resource historical\nlanguages.", "AI": {"tldr": "This study explores the challenges and solutions in POS tagging for Medieval Romance languages using modern LLMs.", "motivation": "To address POS tagging challenges in historical text analysis for Medieval languages due to linguistic evolution and data scarcity.", "method": "Systematic investigation through experimentation, evaluating fine-tuning, prompt engineering, model architectures, decoding strategies, and cross-lingual techniques.", "result": "Key findings include limitations of LLMs in processing historical variations and promising techniques for low-resource languages.", "conclusion": "While LLMs face challenges with historical language variations, specialized techniques can enhance tagging accuracy.", "key_contributions": ["Investigating POS tagging performance for Medieval Romance languages", "Evaluating various fine-tuning and model strategies", "Identifying effective techniques for low-resource historical languages"], "limitations": "Focus on specific Medieval languages may limit generalizability.", "keywords": ["POS tagging", "Medieval Romance languages", "Natural Language Processing", "Large Language Models", "Historical Linguistics"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2411.05651", "pdf": "https://arxiv.org/pdf/2411.05651.pdf", "abs": "https://arxiv.org/abs/2411.05651", "title": "LightVA: Lightweight Visual Analytics with LLM Agent-Based Task Planning and Execution", "authors": ["Yuheng Zhao", "Junjie Wang", "Linbin Xiang", "Xiaowen Zhang", "Zifei Guo", "Cagatay Turkay", "Yu Zhang", "Siming Chen"], "categories": ["cs.HC"], "comment": null, "summary": "Visual analytics (VA) requires analysts to iteratively propose analysis tasks\nbased on observations and execute tasks by creating visualizations and\ninteractive exploration to gain insights. This process demands skills in\nprogramming, data processing, and visualization tools, highlighting the need\nfor a more intelligent, streamlined VA approach. Large language models (LLMs)\nhave recently been developed as agents to handle various tasks with dynamic\nplanning and tool-using capabilities, offering the potential to enhance the\nefficiency and versatility of VA. We propose LightVA, a lightweight VA\nframework that supports task decomposition, data analysis, and interactive\nexploration through human-agent collaboration. Our method is designed to help\nusers progressively translate high-level analytical goals into low-level tasks,\nproducing visualizations and deriving insights. Specifically, we introduce an\nLLM agent-based task planning and execution strategy, employing a recursive\nprocess involving a planner, executor, and controller. The planner is\nresponsible for recommending and decomposing tasks, the executor handles task\nexecution, including data analysis, visualization generation and multi-view\ncomposition, and the controller coordinates the interaction between the planner\nand executor. Building on the framework, we develop a system with a hybrid user\ninterface that includes a task flow diagram for monitoring and managing the\ntask planning process, a visualization panel for interactive data exploration,\nand a chat view for guiding the model through natural language instructions. We\nexamine the effectiveness of our method through a usage scenario and an expert\nstudy.", "AI": {"tldr": "LightVA is a lightweight framework that enhances visual analytics through LLM-based collaboration, enabling task decomposition and interactive exploration.", "motivation": "The paper addresses the complexity of visual analytics, requiring programming and data skills, by proposing a more intelligent approach using LLMs.", "method": "LightVA employs a recursive task planning and execution strategy involving a planner for task decomposition, an executor for executing tasks and generating visualizations, and a controller for coordinating interactions.", "result": "The proposed method effectively facilitates the translation of high-level analytic goals into actionable tasks, showcasing improved interaction and exploration in visual analytics.", "conclusion": "LightVA enhances the visual analytics process by streamlining task management and enabling efficient human-agent collaboration through a hybrid user interface.", "key_contributions": ["Introduction of LightVA framework for visual analytics", "LLM agent-based task planning and execution", "Development of a hybrid user interface for interactive exploration and task management"], "limitations": "", "keywords": ["Visual Analytics", "Large Language Models", "Human-Agent Collaboration"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.17728", "pdf": "https://arxiv.org/pdf/2506.17728.pdf", "abs": "https://arxiv.org/abs/2506.17728", "title": "KAG-Thinker: Teaching Large Language Models to Think with Human-like Reasoning Process", "authors": ["Dalong Zhang", "Jun Xu", "Jun Zhou", "Lei Liang", "Lin Yuan", "Ling Zhong", "Mengshu Sun", "Peilong Zhao", "QiWei Wang", "Xiaorui Wang", "Xinkai Du", "YangYang Hou", "Yu Ao", "ZhaoYang Wang", "Zhengke Gui", "ZhiYing Yi", "Zhongpu Bo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we introduce KAG-Thinker, a novel human-like reasoning\nframework built upon a parameter-light large language model (LLM). Our approach\nenhances the logical coherence and contextual consistency of the thinking\nprocess in question-answering (Q\\&A) tasks on domain-specific knowledge bases\n(KBs) within LLMs. This framework simulates human cognitive mechanisms for\nhandling complex problems by establishing a structured thinking process.\nContinuing the \\textbf{Logical Form} guided retrieval and reasoning technology\nroute of KAG v0.7, firstly, it decomposes complex questions into independently\nsolvable sub-problems(also referred to as logical forms) through\n\\textbf{breadth decomposition}, each represented in two equivalent\nforms-natural language and logical function-and further classified as either\nKnowledge Retrieval or Reasoning Analysis tasks, with dependencies and\nvariables passing explicitly modeled via logical function interfaces. In the\nsolving process, the Retrieval function is used to perform knowledge retrieval\ntasks, while the Math and Deduce functions are used to perform reasoning\nanalysis tasks. Secondly, it is worth noting that, in the Knowledge Retrieval\nsub-problem tasks, LLMs and external knowledge sources are regarded as\nequivalent KBs. We use the \\textbf{knowledge boundary} model to determine the\noptimal source using self-regulatory mechanisms such as confidence calibration\nand reflective reasoning, and use the \\textbf{depth solving} model to enhance\nthe comprehensiveness of knowledge acquisition. Finally, instead of utilizing\nreinforcement learning, we employ supervised fine-tuning with multi-turn\ndialogues to align the model with our structured inference paradigm, thereby\navoiding excessive reflection. This is supported by a data evaluation framework\nand iterative corpus synthesis, which facilitate the generation of detailed\nreasoning trajectories...", "AI": {"tldr": "KAG-Thinker is a novel reasoning framework utilizing a parameter-light LLM to enhance logical coherence and contextual consistency in Q&A tasks, simulating human cognitive mechanisms.", "motivation": "The study aims to improve reasoning capabilities in LLMs for domain-specific Q&A tasks by emulating human-like cognitive processes and establishing a structured thinking framework.", "method": "The framework decomposes complex questions into sub-problems using breadth decomposition, representing them as both natural language and logical function forms. It classifies these into Knowledge Retrieval and Reasoning Analysis tasks, utilizing various functions for knowledge retrieval and reasoning, along with self-regulatory mechanisms for optimal source determination.", "result": "KAG-Thinker demonstrates improved logical coherence and enhanced performance in complex Q&A scenarios by effectively utilizing structured reasoning and knowledge acquisition techniques.", "conclusion": "The proposed framework offers a strong alternative to reinforcement learning through supervised fine-tuning techniques, aligning the model with structured inference while minimizing excessive reflection.", "key_contributions": ["Introduction of KAG-Thinker as a novel reasoning framework using parameter-light LLMs.", "Decomposing complex questions into manageable sub-problems for enhanced reasoning.", "Employing self-regulatory mechanisms for optimal knowledge source determination."], "limitations": "", "keywords": ["Human-Cognitive Simulation", "Domain-Specific Q&A", "Parameter-Light LLM"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.08854", "pdf": "https://arxiv.org/pdf/2502.08854.pdf", "abs": "https://arxiv.org/abs/2502.08854", "title": "Generative AI & Changing Work: Systematic Review of Practitioner-led Work Transformations through the Lens of Job Crafting", "authors": ["Matthew Law", "Rama Adithya Varanasi"], "categories": ["cs.HC"], "comment": null, "summary": "Widespread integration of Generative AI tools is transforming white-collar\nwork, reshaping how workers define their roles, manage their tasks, and\ncollaborate with peers. This has created a need to develop an overarching\nunderstanding of common worker-driven patterns around these transformations. To\nfill this gap, we conducted a systematic literature review of 23 studies from\nthe ACM Digital Library that focused on workers' lived-experiences and\npractitioners with GenAI. Our findings reveal that while many professionals\nhave delegated routine tasks to GenAI to focus on core responsibilities, they\nhave also taken on new forms of AI managerial labor to monitor and refine GenAI\noutputs. Additionally, practitioners have restructured collaborations,\nsometimes bypassing traditional peer and subordinate interactions in favor of\nGenAI assistance. These shifts have fragmented cohesive tasks into piecework\ncreating tensions around role boundaries and professional identity. Our\nanalysis suggests that current frameworks, like job crafting, need to evolve to\naddress the complexities of GenAI-driven transformations.", "AI": {"tldr": "The paper reviews how Generative AI (GenAI) tools affect white-collar work, task management, and collaboration, emphasizing the emergence of new AI managerial labor and changes in professional identity.", "motivation": "To develop an understanding of worker-driven patterns resulting from the integration of Generative AI tools in white-collar work.", "method": "Conducted a systematic literature review of 23 studies from the ACM Digital Library focused on workers' experiences with Generative AI.", "result": "Findings indicate that professionals are delegating routine tasks to GenAI while taking on new managerial roles; collaborations are evolving, causing tensions around roles and identity.", "conclusion": "Job crafting frameworks must evolve to better accommodate the complexities introduced by GenAI transformations in workforce dynamics.", "key_contributions": ["Identification of new AI managerial labor", "Insight into shifts in professional identity and collaboration", "Recommendation for evolving job crafting frameworks"], "limitations": "", "keywords": ["Generative AI", "white-collar work", "task management", "professional identity", "collaboration"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.17748", "pdf": "https://arxiv.org/pdf/2506.17748.pdf", "abs": "https://arxiv.org/abs/2506.17748", "title": "HIDE and Seek: Detecting Hallucinations in Language Models via Decoupled Representations", "authors": ["Anwoy Chatterjee", "Yash Goel", "Tanmoy Chakraborty"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Contemporary Language Models (LMs), while impressively fluent, often generate\ncontent that is factually incorrect or unfaithful to the input context - a\ncritical issue commonly referred to as 'hallucination'. This tendency of LMs to\ngenerate hallucinated content undermines their reliability, especially because\nthese fabrications are often highly convincing and therefore difficult to\ndetect. While several existing methods attempt to detect hallucinations, most\nrely on analyzing multiple generations per input, leading to increased\ncomputational cost and latency. To address this, we propose a single-pass,\ntraining-free approach for effective Hallucination detectIon via Decoupled\nrEpresentations (HIDE). Our approach leverages the hypothesis that\nhallucinations result from a statistical decoupling between an LM's internal\nrepresentations of input context and its generated output. We quantify this\ndecoupling using the Hilbert-Schmidt Independence Criterion (HSIC) applied to\nhidden-state representations extracted while generating the output sequence. We\nconduct extensive experiments on four diverse question answering datasets,\nevaluating both faithfulness and factuality hallucinations across six\nopen-source LMs of varying scales and properties. Our results demonstrate that\nHIDE outperforms other single-pass methods in almost all settings, achieving an\naverage relative improvement of ~29% in AUC-ROC over the best-performing\nsingle-pass strategy across various models and datasets. Additionally, HIDE\nshows competitive and often superior performance with multi-pass\nstate-of-the-art methods, obtaining an average relative improvement of ~3% in\nAUC-ROC while consuming ~51% less computation time. Our findings highlight the\neffectiveness of exploiting internal representation decoupling in LMs for\nefficient and practical hallucination detection.", "AI": {"tldr": "This paper presents HIDE, a single-pass, training-free approach for detecting hallucinations in language models by analyzing internal representations and their decoupling from input context, resulting in improved efficiency and accuracy.", "motivation": "Language models often generate factually incorrect content (hallucinations), which undermines their reliability. Existing detection methods are costlier and slower, hence the need for a more efficient solution.", "method": "HIDE utilizes the Hilbert-Schmidt Independence Criterion (HSIC) to measure the statistical decoupling between the internal representations of input context and the generated output in a single pass.", "result": "HIDE outperforms other single-pass methods with an average relative improvement of ~29% in AUC-ROC and shows competitive performance with multi-pass methods while being ~51% more efficient.", "conclusion": "The effectiveness of HIDE demonstrates that leveraging internal representation decoupling in language models is a practical approach for efficient hallucination detection.", "key_contributions": ["Introduction of a training-free single-pass approach for hallucination detection", "Demonstration of the efficacy of exploiting internal representation decoupling", "Significant reduction in computation time while maintaining high accuracy"], "limitations": "", "keywords": ["language models", "hallucination detection", "automatic evaluation", "Hilbert-Schmidt Independence Criterion", "efficient algorithms"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.22024", "pdf": "https://arxiv.org/pdf/2503.22024.pdf", "abs": "https://arxiv.org/abs/2503.22024", "title": "Beyond Subjectivity: Continuous Cybersickness Detection Using EEG-based Multitaper Spectrum Estimation", "authors": ["Berken Utku Demirel", "Adnan Harun Dogan", "Juliete Rossie", "Max Moebus", "Christian Holz"], "categories": ["cs.HC", "eess.SP"], "comment": "Accepted to TVCG", "summary": "Virtual reality (VR) presents immersive opportunities across many\napplications, yet the inherent risk of developing cybersickness during\ninteraction can severely reduce enjoyment and platform adoption. Cybersickness\nis marked by symptoms such as dizziness and nausea, which previous work\nprimarily assessed via subjective post-immersion questionnaires and\nmotion-restricted controlled setups. In this paper, we investigate the\n\\emph{dynamic nature} of cybersickness while users experience and freely\ninteract in VR. We propose a novel method to \\emph{continuously} identify and\nquantitatively gauge cybersickness levels from users' \\emph{passively\nmonitored} electroencephalography (EEG) and head motion signals. Our method\nestimates multitaper spectrums from EEG, integrating specialized EEG processing\ntechniques to counter motion artifacts, and, thus, tracks cybersickness levels\nin real-time. Unlike previous approaches, our method requires no user-specific\ncalibration or personalization for detecting cybersickness. Our work addresses\nthe considerable challenge of reproducibility and subjectivity in cybersickness\nresearch.", "AI": {"tldr": "This paper presents a novel method for continuously detecting and quantifying cybersickness in virtual reality users using EEG and head motion signals.", "motivation": "To address the challenges of cybersickness in virtual reality which can hinder user experience and platform adoption.", "method": "The authors developed a method that uses passively monitored EEG and head motion signals to track cybersickness levels in real-time without user-specific calibration.", "result": "The proposed method successfully estimates multitaper spectrums from EEG data while mitigating motion artifacts, allowing for continuous monitoring of cybersickness.", "conclusion": "This research provides a reproducible and objective approach to measuring cybersickness, potentially improving user experiences in VR.", "key_contributions": ["Continuous detection of cybersickness using passive EEG monitoring", "No need for user-specific calibration", "Real-time tracking of cybersickness levels"], "limitations": "The method relies on the accuracy of EEG data and may be influenced by external factors during monitoring.", "keywords": ["cybersickness", "virtual reality", "EEG", "real-time monitoring", "user experience"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.17789", "pdf": "https://arxiv.org/pdf/2506.17789.pdf", "abs": "https://arxiv.org/abs/2506.17789", "title": "Multilingual Tokenization through the Lens of Indian Languages: Challenges and Insights", "authors": ["N J Karthika", "Maharaj Brahma", "Rohit Saluja", "Ganesh Ramakrishnan", "Maunendra Sankar Desarkar"], "categories": ["cs.CL"], "comment": null, "summary": "Tokenization plays a pivotal role in multilingual NLP. However, existing\ntokenizers are often skewed towards high-resource languages, limiting their\neffectiveness for linguistically diverse and morphologically rich languages\nsuch as those in the Indian subcontinent. This paper presents a comprehensive\nintrinsic evaluation of tokenization strategies across 17 Indian languages. We\nquantify the trade-offs between bottom-up and top-down tokenizer algorithms\n(BPE and Unigram LM), effects of vocabulary sizes, and compare strategies of\nmultilingual vocabulary construction such as joint and cluster-based training.\nWe also show that extremely low-resource languages can benefit from tokenizers\ntrained on related high-resource languages. Our study provides practical\ninsights for building more fair, efficient, and linguistically informed\ntokenizers for multilingual NLP.", "AI": {"tldr": "This paper evaluates tokenization strategies for multilingual NLP, focusing on 17 Indian languages, highlighting the limitations of existing tokenizers for low-resource languages and offering insights for improvement.", "motivation": "Existing tokenizers are biased toward high-resource languages, which limits effective NLP for diverse languages, especially in the Indian subcontinent.", "method": "The study involves an intrinsic evaluation of tokenization strategies including bottom-up (BPE) and top-down (Unigram LM) algorithms, analyzing vocabulary sizes and construction methods.", "result": "The evaluation shows significant trade-offs in tokenizer performance and indicates that low-resource languages can improve using tokenizers trained on high-resource languages.", "conclusion": "The findings emphasize the need for fair and efficient tokenization strategies tailored for multilingual use, particularly for morphologically rich languages.", "key_contributions": ["Comprehensive evaluation of tokenization strategies across multiple Indian languages.", "Insights into the benefits of using high-resource languages for low-resource tokenizer training.", "Comparative analysis of vocabulary construction methods."], "limitations": "The focus is primarily on Indian languages, which may limit the applicability of findings to other low-resource languages globally.", "keywords": ["tokenization", "multilingual NLP", "low-resource languages", "Indian languages", "vocabulary construction"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.17844", "pdf": "https://arxiv.org/pdf/2506.17844.pdf", "abs": "https://arxiv.org/abs/2506.17844", "title": "THCM-CAL: Temporal-Hierarchical Causal Modelling with Conformal Calibration for Clinical Risk Prediction", "authors": ["Xin Zhang", "Qiyu Wei", "Yingjie Zhu", "Fanyi Wu", "Sophia Ananiadou"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 4 figures", "summary": "Automated clinical risk prediction from electronic health records (EHRs)\ndemands modeling both structured diagnostic codes and unstructured narrative\nnotes. However, most prior approaches either handle these modalities separately\nor rely on simplistic fusion strategies that ignore the directional,\nhierarchical causal interactions by which narrative observations precipitate\ndiagnoses and propagate risk across admissions. In this paper, we propose\nTHCM-CAL, a Temporal-Hierarchical Causal Model with Conformal Calibration. Our\nframework constructs a multimodal causal graph where nodes represent clinical\nentities from two modalities: Textual propositions extracted from notes and ICD\ncodes mapped to textual descriptions. Through hierarchical causal discovery,\nTHCM-CAL infers three clinically grounded interactions: intra-slice\nsame-modality sequencing, intra-slice cross-modality triggers, and inter-slice\nrisk propagation. To enhance prediction reliability, we extend conformal\nprediction to multi-label ICD coding, calibrating per-code confidence intervals\nunder complex co-occurrences. Experimental results on MIMIC-III and MIMIC-IV\ndemonstrate the superiority of THCM-CAL.", "AI": {"tldr": "This paper presents THCM-CAL, a model for automated clinical risk prediction from EHRs that effectively integrates both structured and unstructured data.", "motivation": "To improve automated clinical risk prediction using electronic health records by addressing the limitations of existing methods that either treat structured and unstructured data separately or use simplistic fusion strategies.", "method": "The proposed THCM-CAL framework constructs a multimodal causal graph that models interactions between textual notes and ICD codes, utilizing hierarchical causal discovery and conformal prediction for reliable multi-label coding.", "result": "Experimental results demonstrate the superiority of THCM-CAL in predicting clinical risks when compared to existing methods, as evidenced by performance metrics on the MIMIC-III and MIMIC-IV datasets.", "conclusion": "THCM-CAL offers a robust approach to clinical risk prediction that advances the integration of textual and coded data in EHRs, leading to better prediction reliability.", "key_contributions": ["Introduction of a multimodal causal graph integrating textual propositions and ICD codes.", "Hierarchical causal discovery to capture complex clinical interactions.", "Extension of conformal prediction for multi-label ICD coding."], "limitations": "The model's effectiveness is currently evaluated on specific datasets (MIMIC-III and MIMIC-IV), which may limit its generalizability to other types of EHRs.", "keywords": ["clinical risk prediction", "electronic health records", "multimodal causal graph", "hierarchical causal discovery", "conformal prediction"], "importance_score": 9, "read_time_minutes": 13}}
{"id": "2506.17863", "pdf": "https://arxiv.org/pdf/2506.17863.pdf", "abs": "https://arxiv.org/abs/2506.17863", "title": "LLMs for Customized Marketing Content Generation and Evaluation at Scale", "authors": ["Haoran Liu", "Amir Tahmasbi", "Ehtesham Sam Haque", "Purak Jain"], "categories": ["cs.CL"], "comment": "KDD LLM4ECommerce Workshop 2025", "summary": "Offsite marketing is essential in e-commerce, enabling businesses to reach\ncustomers through external platforms and drive traffic to retail websites.\nHowever, most current offsite marketing content is overly generic,\ntemplate-based, and poorly aligned with landing pages, limiting its\neffectiveness. To address these limitations, we propose MarketingFM, a\nretrieval-augmented system that integrates multiple data sources to generate\nkeyword-specific ad copy with minimal human intervention. We validate\nMarketingFM via offline human and automated evaluations and large-scale online\nA/B tests. In one experiment, keyword-focused ad copy outperformed templates,\nachieving up to 9% higher CTR, 12% more impressions, and 0.38% lower CPC,\ndemonstrating gains in ad ranking and cost efficiency. Despite these gains,\nhuman review of generated ads remains costly. To address this, we propose\nAutoEval-Main, an automated evaluation system that combines rule-based metrics\nwith LLM-as-a-Judge techniques to ensure alignment with marketing principles.\nIn experiments with large-scale human annotations, AutoEval-Main achieved\n89.57% agreement with human reviewers. Building on this, we propose\nAutoEval-Update, a cost-efficient LLM-human collaborative framework to\ndynamically refine evaluation prompts and adapt to shifting criteria with\nminimal human input. By selectively sampling representative ads for human\nreview and using a critic LLM to generate alignment reports, AutoEval-Update\nimproves evaluation consistency while reducing manual effort. Experiments show\nthe critic LLM suggests meaningful refinements, improving LLM-human agreement.\nNonetheless, human oversight remains essential for setting thresholds and\nvalidating refinements before deployment.", "AI": {"tldr": "This paper presents MarketingFM, a system for generating keyword-specific ad copy using multiple data sources, validating its effectiveness through evaluations and A/B tests, and introduces AutoEval-Main and AutoEval-Update for automated ad evaluation and refinement.", "motivation": "To improve the effectiveness of offsite marketing in e-commerce by addressing the limitations of generic ad content.", "method": "A retrieval-augmented system (MarketingFM) integrates various data sources for ad generation and uses automated evaluation systems (AutoEval-Main and AutoEval-Update) to refine and assess ad quality.", "result": "Keyword-focused ad copy led to a 9% increase in click-through rates (CTR) and improved cost efficiency with a 0.38% reduction in cost-per-click (CPC). AutoEval-Main achieved 89.57% agreement with human reviewers.", "conclusion": "Automated systems, while efficient, still require human oversight for final validation of ad quality and effectiveness.", "key_contributions": ["Development of MarketingFM for keyword-specific ad generation", "Introduction of AutoEval-Main for automated ad evaluation", "Creation of AutoEval-Update for LLM-human collaborative refinement of ad evaluation."], "limitations": "Human review of generated ads is still costly and necessary for quality assurance.", "keywords": ["offsite marketing", "ad generation", "automated evaluation", "keyword-focused", "human oversight"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2506.17864", "pdf": "https://arxiv.org/pdf/2506.17864.pdf", "abs": "https://arxiv.org/abs/2506.17864", "title": "QueueEDIT: Structural Self-Correction for Sequential Model Editing in LLMs", "authors": ["Taolin Zhang", "Haidong Kang", "Dongyang Li", "Qizhou Chen", "Chengyu Wang Xiaofeng He", "Richang Hong"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated impressive results\nbut still suffer from hallucinations. Model editing has been proposed to\ncorrect factual inaccuracies in LLMs. A challenging case is sequential model\nediting (SME), which aims to rectify errors continuously rather than treating\nthem as a one-time task. During SME, the general capabilities of LLMs can be\nnegatively affected due to the introduction of new parameters. In this paper,\nwe propose a queue-based self-correction framework (QueueEDIT) that not only\nenhances SME performance by addressing long-sequence dependency but also\nmitigates the impact of parameter bias on the general capabilities of LLMs.\nSpecifically, we first introduce a structural mapping editing loss to map the\ntriplets to the knowledge-sensitive neurons within the Transformer layers of\nLLMs. We then store the located parameters for each piece of edited knowledge\nin a queue and dynamically align previously edited parameters. In each edit, we\nselect queue parameters most relevant to the currently located parameters to\ndetermine whether previous knowledge needs realignment. Irrelevant parameters\nin the queue are frozen, and we update the parameters at the queue head to the\nLLM to ensure they do not harm general abilities. Experiments show that our\nframework significantly outperforms strong baselines across various SME\nsettings and maintains competitiveness in single-turn editing. The resulting\nLLMs also preserve high capabilities in general NLP tasks throughout the SME\nprocess.", "AI": {"tldr": "Proposal of QueueEDIT, a queue-based self-correction framework for improving sequential model editing (SME) in large language models (LLMs) while maintaining their general capabilities.", "motivation": "Address the issue of hallucinations in LLMs by improving sequential model editing techniques.", "method": "The proposed framework uses a queue to manage and align model parameters during continuous editing of knowledge, incorporating a structural mapping editing loss.", "result": "QueueEDIT outperforms strong baselines in various SME configurations while preserving general NLP capabilities throughout the editing process.", "conclusion": "The framework effectively enhances SME performance and mitigates parameter bias, offering a robust solution for correcting LLM inaccuracies.", "key_contributions": ["Introduction of QueueEDIT framework for SME", "Development of structural mapping editing loss", "Dynamic alignment of previously edited parameters in a queue"], "limitations": "", "keywords": ["Large Language Models", "Sequential Model Editing", "Self-Correction Framework"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.17871", "pdf": "https://arxiv.org/pdf/2506.17871.pdf", "abs": "https://arxiv.org/abs/2506.17871", "title": "How Alignment Shrinks the Generative Horizon", "authors": ["Chenghao Yang", "Ari Holtzman"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Codebase: https://github.com/yangalan123/LLMBranchingFactor, Website:\n  https://yangalan123.github.io/branching_factor/", "summary": "Despite their impressive capabilities, aligned large language models (LLMs)\noften generate outputs that lack diversity. What drives this stability in the\ngeneration? We investigate this phenomenon through the lens of probability\nconcentration in the model's output distribution. To quantify this\nconcentration, we introduce the Branching Factor (BF) -- a token-invariant\nmeasure of the effective number of plausible next steps during generation. Our\nempirical analysis reveals two key findings: (1) BF often decreases as\ngeneration progresses, suggesting that LLMs become more predictable as they\ngenerate. (2) alignment tuning substantially sharpens the model's output\ndistribution from the outset, reducing BF by nearly an order of magnitude\n(e.g., from 12 to 1.2) relative to base models. This stark reduction helps\nexplain why aligned models often appear less sensitive to decoding strategies.\nBuilding on this insight, we find this stability has surprising implications\nfor complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g.,\nDeepSeek-distilled models), for instance, leverage this effect; by generating\nlonger reasoning chains, they push generation into later, more deterministic\n(lower BF) stages, resulting in more stable outputs. We hypothesize that\nalignment tuning does not fundamentally change a model's behavior, but instead\nsteers it toward stylistic tokens (e.g., \"Sure\") that unlock low-entropy\ntrajectories already present in the base model. This view is supported by\nnudging experiments, which show that prompting base models with such tokens can\nsimilarly reduce BF. Together, our findings establish BF as a powerful\ndiagnostic for understanding and controlling LLM outputs - clarifying how\nalignment reduces variability, how CoT promotes stable generations, and how\nbase models can be steered away from diversity.", "AI": {"tldr": "This paper investigates the lack of diversity in outputs from aligned large language models (LLMs) through a new measure called the Branching Factor (BF), revealing that BF decreases over generation and alignment tuning sharpens output predictability.", "motivation": "To understand the stability in output generation of aligned large language models and the implications for reasoning.", "method": "Introduced the Branching Factor (BF) as a token-invariant measure of plausibility in output generation and conducted empirical analyses.", "result": "BF often decreases as generation progresses, indicating high predictability, and alignment tuning sharpens the output distribution significantly, reducing BF drastically.", "conclusion": "These findings demonstrate the BF as a crucial tool for comprehending LLM output control and variability reduction, emphasizing how alignment affects model behavior.", "key_contributions": ["Introduced the Branching Factor (BF) to measure output diversity", "Showed that BF decreases during generation, leading to more predictable outputs", "Demonstrated effects of alignment tuning on output distribution stability"], "limitations": "", "keywords": ["Large Language Models", "Diversity", "Branching Factor", "Alignment Tuning", "CoT Models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.17881", "pdf": "https://arxiv.org/pdf/2506.17881.pdf", "abs": "https://arxiv.org/abs/2506.17881", "title": "Multi-turn Jailbreaking via Global Refinement and Active Fabrication", "authors": ["Hua Tang", "Lingyong Yan", "Yukun Zhao", "Shuaiqiang Wang", "Jizhou Huang", "Dawei Yin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have achieved exceptional performance across a\nwide range of tasks. However, they still pose significant safety risks due to\nthe potential misuse for malicious purposes. Jailbreaks, which aim to elicit\nmodels to generate harmful content, play a critical role in identifying the\nunderlying security threats. Recent jailbreaking primarily focuses on\nsingle-turn scenarios, while the more complicated multi-turn scenarios remain\nunderexplored. Moreover, existing multi-turn jailbreaking techniques struggle\nto adapt to the evolving dynamics of dialogue as the interaction progresses. To\naddress this limitation, we propose a novel multi-turn jailbreaking method that\nrefines the jailbreaking path globally at each interaction. We also actively\nfabricate model responses to suppress safety-related warnings, thereby\nincreasing the likelihood of eliciting harmful outputs in subsequent questions.\nExperimental results demonstrate the superior performance of our method\ncompared with existing single-turn and multi-turn jailbreaking techniques\nacross six state-of-the-art LLMs. Our code is publicly available at\nhttps://github.com/Ytang520/Multi-Turn_jailbreaking_Global-Refinment_and_Active-Fabrication.", "AI": {"tldr": "This paper presents a novel multi-turn jailbreaking method for Large Language Models that refines interactions globally and fabricates responses to elicit harmful outputs, outperforming existing techniques.", "motivation": "The need to address security risks posed by Large Language Models (LLMs) and the limitations of existing multi-turn jailbreaking methods.", "method": "A novel multi-turn jailbreaking method that globally refines the jailbreaking path at each interaction and actively fabricates model responses to suppress safety warnings.", "result": "Experimental results show that the proposed method significantly outperforms existing single-turn and multi-turn jailbreaking techniques across six state-of-the-art LLMs.", "conclusion": "The proposed method enhances the effectiveness of jailbreaking in multi-turn scenarios, providing a new approach to understanding and mitigating security threats in LLMs.", "key_contributions": ["Development of a novel multi-turn jailbreaking method", "Global refinement of interactions during dialogue", "Active fabrication of model responses to suppress warnings"], "limitations": "Potential ethical concerns with the misuse of jailbreaking techniques and the implications of eliciting harmful outputs.", "keywords": ["Large Language Models", "jailbreaking", "multi-turn scenarios", "dialogue dynamics", "model responses"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2506.17949", "pdf": "https://arxiv.org/pdf/2506.17949.pdf", "abs": "https://arxiv.org/abs/2506.17949", "title": "Scatter-Based Innovation Propagation in Large Language Models for Multi-Stage Process Adaptation", "authors": ["Hong Su"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) exhibit strong capabilities in reproducing and\nextending patterns observed during pretraining but often struggle to generalize\nnovel ideas beyond their original context. This paper addresses the challenge\nof applying such localized innovations - introduced at a specific stage or\ncomponent - to other parts of a multi-stage process. We propose a scatter-based\ninnovation expansion model (innovation scatter model) that guides the LLM\nthrough a four-step process: (1) identifying the core innovation by comparing\nthe user's input with its surrounding context, (2) generalizing the innovation\nby removing references to specific stages or components, (3) determining\nwhether the generalized innovation applies to a broader scope beyond the\noriginal stage, and (4) systematically applying it to other structurally\nsimilar stages using the LLM. This model leverages structural redundancy across\nstages to improve the applicability of novel ideas. Verification results\ndemonstrate that the innovation scatter model enables LLMs to extend\ninnovations across structurally similar stages, thereby enhancing\ngeneralization and reuse.", "AI": {"tldr": "The paper introduces a scatter-based innovation expansion model for LLMs to improve their ability to generalize innovations across multi-stage processes.", "motivation": "To address the challenge of LLMs struggling to generalize novel ideas beyond their original context, particularly when applying localized innovations to other parts of multi-stage processes.", "method": "The proposed innovation scatter model guides LLMs through a four-step process: identifying core innovations, generalizing them, determining their broader applicability, and systematically applying innovations to similar stages.", "result": "Verification results show that the innovation scatter model allows LLMs to effectively extend innovations across structurally similar stages, which enhances generalization and reuse.", "conclusion": "The innovation scatter model significantly improves the ability of LLMs to apply innovations in various contexts, thereby addressing their limitations in generalization.", "key_contributions": ["Development of a scatter-based innovation expansion model for LLMs", "Introduction of a structured four-step innovation application process", "Empirical verification demonstrating enhanced generalization capabilities of LLMs"], "limitations": "", "keywords": ["Large Language Models", "innovation expansion", "generalization", "multi-stage processes", "LLM applications"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.17951", "pdf": "https://arxiv.org/pdf/2506.17951.pdf", "abs": "https://arxiv.org/abs/2506.17951", "title": "A Comprehensive Graph Framework for Question Answering with Mode-Seeking Preference Alignment", "authors": ["Quanwei Tang", "Sophia Yat Mei Lee", "Junshuang Wu", "Dong Zhang", "Shoushan Li", "Erik Cambria", "Guodong Zhou"], "categories": ["cs.CL"], "comment": "acl 2025 findings", "summary": "Recent advancements in retrieval-augmented generation (RAG) have enhanced\nlarge language models in question answering by integrating external knowledge.\nHowever, challenges persist in achieving global understanding and aligning\nresponses with human ethical and quality preferences. To address these issues,\nwe propose GraphMPA, a comprehensive graph-based framework with mode-seeking\npreference alignment. Our approach constructs a hierarchical document graph\nusing a general similarity measurement, mimicking human cognitive processes for\ninformation understanding and synthesis. Additionally, we introduce\nmode-seeking preference optimization to better align model outputs with human\npreferences through probability-matching constraints. Extensive experiments on\nsix datasets demonstrate the effectiveness of our\n\\href{https://github.com/tangquanwei/GraphMPA}{GraphMPA}.", "AI": {"tldr": "GraphMPA enhances RAG in large language models for ethical question answering by employing a graph-based framework and preference optimization.", "motivation": "To improve global understanding and alignment of AI responses with human ethical and quality preferences in question answering tasks involving retrieval-augmented generation (RAG).", "method": "The proposed GraphMPA framework constructs a hierarchical document graph using similarity measures and integrates a mode-seeking preference optimization technique to align outputs with human preferences through probability-matching constraints.", "result": "Extensive experiments indicated that GraphMPA significantly enhances the performance of large language models on six datasets by improving their ability to provide ethically aligned and quality responses.", "conclusion": "GraphMPA provides a robust method for better integrating human-like understanding into retrieval-augmented generation processes, thus offering a step forward in aligning AI responses with human expectations.", "key_contributions": ["Introduction of a hierarchical document graph for better information synthesis.", "Mode-seeking preference optimization to align responses with human preferences.", "Demonstration of enhanced effectiveness on multiple datasets."], "limitations": "", "keywords": ["retrieval-augmented generation", "large language models", "preference alignment"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.18027", "pdf": "https://arxiv.org/pdf/2506.18027.pdf", "abs": "https://arxiv.org/abs/2506.18027", "title": "PDF Retrieval Augmented Question Answering", "authors": ["Thi Thu Uyen Hoang", "Viet Anh Nguyen"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents an advancement in Question-Answering (QA) systems using a\nRetrieval Augmented Generation (RAG) framework to enhance information\nextraction from PDF files. Recognizing the richness and diversity of data\nwithin PDFs--including text, images, vector diagrams, graphs, and tables--poses\nunique challenges for existing QA systems primarily designed for textual\ncontent. We seek to develop a comprehensive RAG-based QA system that will\neffectively address complex multimodal questions, where several data types are\ncombined in the query. This is mainly achieved by refining approaches to\nprocessing and integrating non-textual elements in PDFs into the RAG framework\nto derive precise and relevant answers, as well as fine-tuning large language\nmodels to better adapt to our system. We provide an in-depth experimental\nevaluation of our solution, demonstrating its capability to extract accurate\ninformation that can be applied to different types of content across PDFs. This\nwork not only pushes the boundaries of retrieval-augmented QA systems but also\nlays a foundation for further research in multimodal data integration and\nprocessing.", "AI": {"tldr": "This paper advances QA systems using a RAG framework to improve information extraction from PDF files, addressing challenges posed by multimodal content.", "motivation": "To enhance the performance of QA systems in extracting information from complex PDF documents which include diverse data types (text, images, tables, etc.).", "method": "Development of a RAG-based QA system that refines the processing of non-textual elements in PDFs and fine-tunes large language models for improved integration.", "result": "The experimental evaluation evidences the system's ability to accurately extract information from various content types in PDFs, achieving better responses to multimodal queries.", "conclusion": "This research enhances retrieval-augmented QA systems and serves as a foundation for further studies on multimodal data processing and integration.", "key_contributions": ["Advancement of RAG-based QA systems for PDFs", "Integration of multimodal data in QA", "Enhancement of large language model adaptation"], "limitations": "", "keywords": ["Question-Answering", "Retrieval Augmented Generation", "Multimodal", "PDF extraction", "Large language models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.18035", "pdf": "https://arxiv.org/pdf/2506.18035.pdf", "abs": "https://arxiv.org/abs/2506.18035", "title": "Splitformer: An improved early-exit architecture for automatic speech recognition on edge devices", "authors": ["Maxence Lasbordes", "Daniele Falavigna", "Alessio Brutti"], "categories": ["cs.CL", "cs.SD", "eess.AS", "68T50 (Primary)", "I.2.7; I.5.4"], "comment": "5 pages, 3 Postscript figures", "summary": "The ability to dynamically adjust the computational load of neural models\nduring inference in a resource aware manner is crucial for on-device processing\nscenarios, characterised by limited and time-varying computational resources.\nEarly-exit architectures represent an elegant and effective solution, since\nthey can process the input with a subset of their layers, exiting at\nintermediate branches (the upmost layers are hence removed from the model).\n  From a different perspective, for automatic speech recognition applications\nthere are memory-efficient neural architectures that apply variable frame rate\nanalysis, through downsampling/upsampling operations in the middle layers,\nreducing the overall number of operations and improving significantly the\nperformance on well established benchmarks. One example is the Zipformer.\nHowever, these architectures lack the modularity necessary to inject early-exit\nbranches.\n  With the aim of improving the performance in early-exit models, we propose\nintroducing parallel layers in the architecture that process downsampled\nversions of their inputs. % in conjunction with standard processing layers. We\nshow that in this way the speech recognition performance on standard benchmarks\nsignificantly improve, at the cost of a small increase in the overall number of\nmodel parameters but without affecting the inference time.", "AI": {"tldr": "This paper proposes an enhancement to early-exit neural architectures for automatic speech recognition by introducing parallel processing layers for downsampled inputs, which improves performance while maintaining inference time.", "motivation": "The need to optimize computational loads in neural models for on-device processing with limited resources motivates this research.", "method": "The authors introduce parallel layers in early-exit architectures that process downsampled versions of inputs alongside standard layers.", "result": "The new architecture demonstrates significantly improved speech recognition performance on standard benchmarks with a minor increase in model parameters, while inference times remain unaffected.", "conclusion": "Incorporating parallel layers for downsampling in early-exit models leads to better overall performance in speech recognition tasks without compromising efficiency.", "key_contributions": ["Introduction of parallel downsampling layers in early-exit architectures", "Significant performance improvement in speech recognition", "Maintained inference time despite increased model parameters"], "limitations": "", "keywords": ["neural models", "early-exit architectures", "speech recognition"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2506.18036", "pdf": "https://arxiv.org/pdf/2506.18036.pdf", "abs": "https://arxiv.org/abs/2506.18036", "title": "Markov-Enhanced Clustering for Long Document Summarization: Tackling the 'Lost in the Middle' Challenge with Large Language Models", "authors": ["Aziz Amari", "Mohamed Achref Ben Ammar"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid expansion of information from diverse sources has heightened the\nneed for effective automatic text summarization, which condenses documents into\nshorter, coherent texts. Summarization methods generally fall into two\ncategories: extractive, which selects key segments from the original text, and\nabstractive, which generates summaries by rephrasing the content coherently.\nLarge language models have advanced the field of abstractive summarization, but\nthey are resourceintensive and face significant challenges in retaining key\ninformation across lengthy documents, which we call being \"lost in the middle\".\nTo address these issues, we propose a hybrid summarization approach that\ncombines extractive and abstractive techniques. Our method splits the document\ninto smaller text chunks, clusters their vector embeddings, generates a summary\nfor each cluster that represents a key idea in the document, and constructs the\nfinal summary by relying on a Markov chain graph when selecting the semantic\norder of ideas.", "AI": {"tldr": "A hybrid summarization approach combining extractive and abstractive techniques to improve automatic text summarization.", "motivation": "To address the challenges of current summarization methods, particularly in retaining key information across lengthy documents.", "method": "The proposed method splits documents into smaller chunks, clusters their vector embeddings, generates summaries for each cluster, and constructs the final summary using a Markov chain graph for semantic ordering.", "result": "The hybrid approach effectively retains key information while providing coherent summaries for lengthy texts.", "conclusion": "This method improves the capability of summarization models to manage information dense documents without losing critical content.", "key_contributions": ["Introduction of a hybrid summarization approach", "Utilization of vector embedding clusters for summarization", "Employing a Markov chain graph for selecting semantic order"], "limitations": "", "keywords": ["text summarization", "hybrid methods", "large language models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.18082", "pdf": "https://arxiv.org/pdf/2506.18082.pdf", "abs": "https://arxiv.org/abs/2506.18082", "title": "Statistical Multicriteria Evaluation of LLM-Generated Text", "authors": ["Esteban Garces Arias", "Hannah Blocher", "Julian Rodemann", "Matthias AÃenmacher", "Christoph Jansen"], "categories": ["cs.CL", "stat.AP"], "comment": null, "summary": "Assessing the quality of LLM-generated text remains a fundamental challenge\nin natural language processing. Current evaluation approaches often rely on\nisolated metrics or simplistic aggregations that fail to capture the nuanced\ntrade-offs between coherence, diversity, fluency, and other relevant indicators\nof text quality. In this work, we adapt a recently proposed framework for\nstatistical inference based on Generalized Stochastic Dominance (GSD) that\naddresses three critical limitations in existing benchmarking methodologies:\nthe inadequacy of single-metric evaluation, the incompatibility between\ncardinal automatic metrics and ordinal human judgments, and the lack of\ninferential statistical guarantees. The GSD-front approach enables simultaneous\nevaluation across multiple quality dimensions while respecting their different\nmeasurement scales, building upon partial orders of decoding strategies, thus\navoiding arbitrary weighting of the involved metrics. By applying this\nframework to evaluate common decoding strategies against human-generated text,\nwe demonstrate its ability to identify statistically significant performance\ndifferences while accounting for potential deviations from the i.i.d.\nassumption of the sampling design.", "AI": {"tldr": "The paper proposes a Generalized Stochastic Dominance (GSD) framework to evaluate LLM-generated text quality across multiple dimensions.", "motivation": "To address the inadequacies of current evaluation metrics in capturing nuanced differences in LLM-generated text quality.", "method": "Adaptation of Generalized Stochastic Dominance (GSD) framework for simultaneous evaluation across multiple quality dimensions.", "result": "The GSD-front approach can identify statistically significant performance differences in decoding strategies against human-generated text while accounting for sampling deviations.", "conclusion": "The framework offers a robust alternative to traditional evaluation methods, accommodating different scales and avoiding arbitrary metric weighting.", "key_contributions": ["Introduction of the GSD-front approach for evaluating LLM text quality", "Simultaneous multi-dimensional quality assessment", "Inferential statistical guarantees in evaluation"], "limitations": "", "keywords": ["LLM evaluation", "Generalized Stochastic Dominance", "text quality assessment", "natural language processing", "decoding strategies"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.18091", "pdf": "https://arxiv.org/pdf/2506.18091.pdf", "abs": "https://arxiv.org/abs/2506.18091", "title": "Evaluating Prompt-Based and Fine-Tuned Approaches to Czech Anaphora Resolution", "authors": ["Patrik Stano", "AleÅ¡ HorÃ¡k"], "categories": ["cs.CL"], "comment": "12 pages", "summary": "Anaphora resolution plays a critical role in natural language understanding,\nespecially in morphologically rich languages like Czech. This paper presents a\ncomparative evaluation of two modern approaches to anaphora resolution on Czech\ntext: prompt engineering with large language models (LLMs) and fine-tuning\ncompact generative models. Using a dataset derived from the Prague Dependency\nTreebank, we evaluate several instruction-tuned LLMs, including Mistral Large 2\nand Llama 3, using a series of prompt templates. We compare them against\nfine-tuned variants of the mT5 and Mistral models that we trained specifically\nfor Czech anaphora resolution. Our experiments demonstrate that while prompting\nyields promising few-shot results (up to 74.5% accuracy), the fine-tuned\nmodels, particularly mT5-large, outperform them significantly, achieving up to\n88% accuracy while requiring fewer computational resources. We analyze\nperformance across different anaphora types, antecedent distances, and source\ncorpora, highlighting key strengths and trade-offs of each approach.", "AI": {"tldr": "This paper evaluates anaphora resolution methods for Czech using LLM prompt engineering and fine-tuned generative models.", "motivation": "Anaphora resolution is crucial for natural language understanding, particularly in morphologically rich languages like Czech.", "method": "The study compares prompt engineering with large language models (LLMs) against fine-tuned compact generative models (specifically mT5 and Mistral) on a dataset from the Prague Dependency Treebank.", "result": "The fine-tuned mT5-large model achieved up to 88% accuracy, outperforming few-shot prompting methods that reached up to 74.5% accuracy.", "conclusion": "While prompting offers good initial accuracy, fine-tuning generative models significantly enhances performance and efficiency in anaphora resolution.", "key_contributions": ["Comparative analysis of prompt engineering and fine-tuning for Czech anaphora resolution.", "Demonstrated superior accuracy with fine-tuned models compared to LLM prompts.", "Insight into the strengths and trade-offs of different anaphora resolution approaches."], "limitations": "The study is limited to Czech language and may not generalize across other languages or dialects.", "keywords": ["anaphora resolution", "Czech", "large language models", "fine-tuning", "natural language understanding"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2506.18102", "pdf": "https://arxiv.org/pdf/2506.18102.pdf", "abs": "https://arxiv.org/abs/2506.18102", "title": "InspireDebate: Multi-Dimensional Subjective-Objective Evaluation-Guided Reasoning and Optimization for Debating", "authors": ["Fuyu Wang", "Jiangtong Li", "Kun Zhu", "Changjun Jiang"], "categories": ["cs.CL"], "comment": "20 pages; Accepted to ACL 2025 Main", "summary": "With the rapid advancements in large language models (LLMs), debating tasks,\nsuch as argument quality assessment and debate process simulation, have made\nsignificant progress. However, existing LLM-based debating systems focus on\nresponding to specific arguments while neglecting objective assessments such as\nauthenticity and logical validity. Furthermore, these systems lack a structured\napproach to optimize across various dimensions$-$including evaluation metrics,\nchain-of-thought (CoT) reasoning, and multi-turn debate refinement$-$thereby\nlimiting their effectiveness. To address these interconnected challenges, we\npropose a dual-component framework: (1) $\\textbf{InspireScore}$, a novel\nevaluation system that establishes a multi-dimensional assessment architecture\nincorporating four subjective criteria (emotional appeal, argument clarity,\nargument arrangement, and topic relevance) alongside two objective metrics\n(fact authenticity and logical validity); and (2) $\\textbf{InspireDebate}$, an\noptimized debating framework employing a phased optimization approach through\nCoT reasoning enhancement, multi-dimensional Direct Preference Optimization\n(DPO), and real-time knowledge grounding via web-based Retrieval Augmented\nGeneration (Web-RAG). Empirical evaluations demonstrate that\n$\\textbf{InspireScore}$ achieves 44$\\%$ higher correlation with expert\njudgments compared to existing methods, while $\\textbf{InspireDebate}$ shows\nsignificant improvements, outperforming baseline models by 57$\\%$. Source code\nis available at https://github.com/fywang12/InspireDebate.", "AI": {"tldr": "This paper proposes a dual-component framework, InspireScore for argument assessment and InspireDebate for enhanced debating, addressing the limitations of existing LLM-based systems.", "motivation": "To improve the effectiveness of LLM-based debating systems by incorporating both subjective and objective assessment criteria.", "method": "The paper introduces InspireScore, a multi-dimensional evaluation system, and InspireDebate, an optimized debating framework using CoT reasoning and Direct Preference Optimization.", "result": "InspireScore shows a 44% higher correlation with expert judgments, and InspireDebate outperforms baseline models by 57%.", "conclusion": "The proposed framework enhances the assessment and effectiveness of LLMs in debating tasks, paving the way for more robust evaluation systems.", "key_contributions": ["InspireScore: a novel evaluation system with subjective and objective metrics", "InspireDebate: an optimized debate framework with CoT reasoning and real-time knowledge grounding", "Empirical results demonstrating significant improvements in argument assessment and debate performance"], "limitations": "", "keywords": ["Large Language Models", "Debating Tasks", "Argument Quality Assessment", "Multi-dimensional Evaluation", "Real-time Knowledge Grounding"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2506.18105", "pdf": "https://arxiv.org/pdf/2506.18105.pdf", "abs": "https://arxiv.org/abs/2506.18105", "title": "Chengyu-Bench: Benchmarking Large Language Models for Chinese Idiom Understanding and Use", "authors": ["Yicheng Fu", "Zhemin Huang", "Liuxin Yang", "Yumeng Lu", "Zhongdongming Dai"], "categories": ["cs.CL"], "comment": null, "summary": "Chinese idioms (Chengyu) are concise four-character expressions steeped in\nhistory and culture, whose literal translations often fail to capture their\nfull meaning. This complexity makes them challenging for language models to\ninterpret and use correctly. Existing benchmarks focus on narrow tasks -\nmultiple-choice cloze tests, isolated translation, or simple paraphrasing. We\nintroduce Chengyu-Bench, a comprehensive benchmark featuring three tasks: (1)\nEvaluative Connotation, classifying idioms as positive or negative; (2)\nAppropriateness, detecting incorrect idiom usage in context; and (3) Open\nCloze, filling blanks in longer passages without options. Chengyu-Bench\ncomprises 2,937 human-verified examples covering 1,765 common idioms sourced\nfrom diverse corpora. We evaluate leading LLMs and find they achieve over 95%\naccuracy on Evaluative Connotation, but only ~85% on Appropriateness and ~40%\ntop-1 accuracy on Open Cloze. Error analysis reveals that most mistakes arise\nfrom fundamental misunderstandings of idiom meanings. Chengyu-Bench\ndemonstrates that while LLMs can reliably gauge idiom sentiment, they still\nstruggle to grasp the cultural and contextual nuances essential for proper\nusage. The benchmark and source code are available at:\nhttps://github.com/sofyc/ChengyuBench.", "AI": {"tldr": "The paper introduces Chengyu-Bench, a benchmark for evaluating language models on Chinese idioms, highlighting their performance and challenges in understanding cultural nuances.", "motivation": "To address the challenges language models face in interpreting and correctly using Chinese idioms due to their cultural complexity.", "method": "The benchmark includes three tasks: Evaluative Connotation, Appropriateness, and Open Cloze, evaluating models on their understanding and usage of idioms.", "result": "Leading LLMs scored over 95% on Evaluative Connotation, about 85% on Appropriateness, and ~40% on Open Cloze, indicating a disparity in performance based on task complexity.", "conclusion": "Chengyu-Bench reveals that LLMs can assess idiom sentiment effectively but often misunderstand cultural and contextual meanings, underscoring the need for improved models.", "key_contributions": ["Introduction of a comprehensive benchmark for idioms", "Evaluation of leading LLMs on idiom usage tasks", "Insights into model performance and error analysis"], "limitations": "The benchmark may not cover all idioms or contexts, and results are limited to specific models tested.", "keywords": ["Chinese idioms", "Chengyu-Bench", "language models", "benchmarking", "natural language processing"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.18116", "pdf": "https://arxiv.org/pdf/2506.18116.pdf", "abs": "https://arxiv.org/abs/2506.18116", "title": "Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives", "authors": ["Batool Haider", "Atmika Gorti", "Aman Chadha", "Manas Gaur"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "19 Pages, 7 Figures, 4 Tables (Note: Under Review)", "summary": "Large Language Models (LLMs) in mental healthcare risk propagating biases\nthat reinforce stigma and harm marginalized groups. While previous research\nidentified concerning trends, systematic methods for detecting intersectional\nbiases remain limited. This work introduces a multi-hop question answering\n(MHQA) framework to explore LLM response biases in mental health discourse. We\nanalyze content from the Interpretable Mental Health Instruction (IMHI) dataset\nacross symptom presentation, coping mechanisms, and treatment approaches. Using\nsystematic tagging across age, race, gender, and socioeconomic status, we\ninvestigate bias patterns at demographic intersections. We evaluate four LLMs:\nClaude 3.5 Sonnet, Jamba 1.6, Gemma 3, and Llama 4, revealing systematic\ndisparities across sentiment, demographics, and mental health conditions. Our\nMHQA approach demonstrates superior detection compared to conventional methods,\nidentifying amplification points where biases magnify through sequential\nreasoning. We implement two debiasing techniques: Roleplay Simulation and\nExplicit Bias Reduction, achieving 66-94% bias reductions through few-shot\nprompting with BBQ dataset examples. These findings highlight critical areas\nwhere LLMs reproduce mental healthcare biases, providing actionable insights\nfor equitable AI development.", "AI": {"tldr": "This paper introduces a multi-hop question answering framework to detect intersectional biases in mental health-related responses generated by large language models, revealing systematic disparities and proposing debiasing techniques.", "motivation": "Addressing biases in LLMs that harm marginalized groups in mental healthcare.", "method": "A multi-hop question answering (MHQA) framework analyzing responses from four LLMs using a tagged dataset for demographic intersections.", "result": "Systematic disparities detected in LLM responses across various demographics, with bias reductions of 66-94% achieved through two debiasing techniques.", "conclusion": "The study provides insights into how LLMs can propagate biases in mental health and suggests methods for improving fairness in AI applications.", "key_contributions": ["Introduced a multi-hop question answering framework for bias detection in LLMs.", "Demonstrated systematic disparities using demographic tagging.", "Implemented effective debiasing techniques with significant results."], "limitations": "", "keywords": ["Large Language Models", "mental healthcare", "bias detection", "debiasing techniques", "intersectional analysis"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.18120", "pdf": "https://arxiv.org/pdf/2506.18120.pdf", "abs": "https://arxiv.org/abs/2506.18120", "title": "The Syntactic Acceptability Dataset (Preview): A Resource for Machine Learning and Linguistic Analysis of English", "authors": ["Tom S Juzek"], "categories": ["cs.CL", "68T50", "I.2.7; I.2.6; H.3.1"], "comment": "Accepted and published at LREC-COLING 2024. 8 pages, 3 figures.\n  Licensed under CC BY-NC-SA 4.0", "summary": "We present a preview of the Syntactic Acceptability Dataset, a resource being\ndesigned for both syntax and computational linguistics research. In its current\nform, the dataset comprises 1,000 English sequences from the syntactic\ndiscourse: Half from textbooks and half from the journal Linguistic Inquiry,\nthe latter to ensure a representation of the contemporary discourse. Each entry\nis labeled with its grammatical status (\"well-formedness\" according to\nsyntactic formalisms) extracted from the literature, as well as its\nacceptability status (\"intuitive goodness\" as determined by native speakers)\nobtained through crowdsourcing, with highest experimental standards. Even in\nits preliminary form, this dataset stands as the largest of its kind that is\npublicly accessible. We also offer preliminary analyses addressing three\ndebates in linguistics and computational linguistics: We observe that\ngrammaticality and acceptability judgments converge in about 83% of the cases\nand that \"in-betweenness\" occurs frequently. This corroborates existing\nresearch. We also find that while machine learning models struggle with\npredicting grammaticality, they perform considerably better in predicting\nacceptability. This is a novel finding. Future work will focus on expanding the\ndataset.", "AI": {"tldr": "The Syntactic Acceptability Dataset aims to aid syntax and computational linguistics research by providing 1,000 annotated English sequences with grammatical and acceptability status, revealing insights into machine learning performance in predicting these aspects.", "motivation": "To create a resource for syntax and computational linguistics research that provides labeled data on grammatical status and acceptability of English sequences.", "method": "The dataset comprises 1,000 English sequences, labeled for grammaticality and acceptability, with the latter determined through crowdsourcing. The entries are divided between textbook examples and contemporary linguistic discourse.", "result": "Grammaticality and acceptability judgments aligned in approximately 83% of cases; machine learning models struggle with grammaticality prediction but perform better on acceptability.", "conclusion": "The Syntactic Acceptability Dataset, the largest publicly accessible dataset of its kind, provides important insights into linguistic judgments and machine learning capabilities.", "key_contributions": ["Introduction of the Syntactic Acceptability Dataset which is the largest of its kind.", "Demonstration of convergence between grammaticality and acceptability judgments in most cases.", "Novel findings on machine learning performance related to grammaticality and acceptability predictions."], "limitations": "", "keywords": ["syntax", "computational linguistics", "acceptability", "grammaticality", "machine learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.18129", "pdf": "https://arxiv.org/pdf/2506.18129.pdf", "abs": "https://arxiv.org/abs/2506.18129", "title": "$Ï^{\\infty}$: Clause Purification, Embedding Realignment, and the Total Suppression of the Em Dash in Autoregressive Language Models", "authors": ["Bugra Kilictas", "Faruk Alpay"], "categories": ["cs.CL", "cs.AI", "68T50, 68T45, 03B70", "I.2.6; I.2.7; I.2.3; F.4.1"], "comment": "16 pages, 3 figures", "summary": "We identify a critical vulnerability in autoregressive transformer language\nmodels where the em dash token induces recursive semantic drift, leading to\nclause boundary hallucination and embedding space entanglement. Through formal\nanalysis of token-level perturbations in semantic lattices, we demonstrate that\nem dash insertion fundamentally alters the model's latent representations,\ncausing compounding errors in long-form generation. We propose a novel solution\ncombining symbolic clause purification via the phi-infinity operator with\ntargeted embedding matrix realignment. Our approach enables total suppression\nof problematic tokens without requiring model retraining, while preserving\nsemantic coherence through fixed-point convergence guarantees. Experimental\nvalidation shows significant improvements in generation consistency and topic\nmaintenance. This work establishes a general framework for identifying and\nmitigating token-level vulnerabilities in foundation models, with immediate\nimplications for AI safety, model alignment, and robust deployment of large\nlanguage models in production environments. The methodology extends beyond\npunctuation to address broader classes of recursive instabilities in neural\ntext generation systems.", "AI": {"tldr": "The paper addresses a vulnerability in transformer language models caused by the em dash token, leading to semantic drift and errors in long-form text generation, proposing a solution that avoids retraining the model.", "motivation": "To identify and mitigate critical vulnerabilities in autoregressive transformer language models, specifically focusing on issues caused by token perturbations such as the em dash.", "method": "Formal analysis of token-level perturbations in semantic lattices and a proposed solution involving symbolic clause purification and targeted embedding matrix realignment.", "result": "The proposed solution significantly improves generation consistency and topic maintenance in language models, demonstrating a method for suppressing problematic tokens without retraining.", "conclusion": "The work provides a framework for addressing token-level vulnerabilities in foundation models, with implications for AI safety and robust deployment in production.", "key_contributions": ["Identified vulnerability in transformer models causing semantic drift due to token insertion.", "Proposed a novel solution to mitigate these vulnerabilities without retraining.", "Established a framework for tackling broader recursive instabilities in neural text generation."], "limitations": "", "keywords": ["language models", "semantic drift", "AI safety", "token perturbations", "neural text generation"], "importance_score": 8, "read_time_minutes": 16}}
{"id": "2506.18141", "pdf": "https://arxiv.org/pdf/2506.18141.pdf", "abs": "https://arxiv.org/abs/2506.18141", "title": "Sparse Feature Coactivation Reveals Composable Semantic Modules in Large Language Models", "authors": ["Ruixuan Deng", "Xiaoyang Hu", "Miles Gilberti", "Shane Storks", "Aman Taxali", "Mike Angstadt", "Chandra Sripada", "Joyce Chai"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We identify semantically coherent, context-consistent network components in\nlarge language models (LLMs) using coactivation of sparse autoencoder (SAE)\nfeatures collected from just a handful of prompts. Focusing on country-relation\ntasks, we show that ablating semantic components for countries and relations\nchanges model outputs in predictable ways, while amplifying these components\ninduces counterfactual responses. Notably, composing relation and country\ncomponents yields compound counterfactual outputs. We find that, whereas most\ncountry components emerge from the very first layer, the more abstract relation\ncomponents are concentrated in later layers. Furthermore, within relation\ncomponents themselves, nodes from later layers tend to have a stronger causal\nimpact on model outputs. Overall, these findings suggest a modular organization\nof knowledge within LLMs and advance methods for efficient, targeted model\nmanipulation.", "AI": {"tldr": "The paper explores the modular organization of knowledge in large language models (LLMs) by analyzing the coactivation of features from sparse autoencoders to manipulate model outputs related to country-relation tasks.", "motivation": "To understand the internal organization of knowledge in LLMs and how semantic components can be manipulated for predictable model output changes.", "method": "The authors use sparse autoencoder features collected from prompts to identify semantically coherent network components associated with countries and relations in LLMs.", "result": "Manipulating country and relation components leads to predictable changes in model outputs, and integrating these components generates counterfactual responses, highlighting a modular structure in knowledge representation.", "conclusion": "The study underscores the layered structure of semantic components within LLMs, with early layers primarily housing country components and later layers hosting more abstract relation components, facilitating targeted model manipulation.", "key_contributions": ["Identification of semantically coherent network components in LLMs", "Demonstration of predictable output changes through component manipulation", "Insights into the layered modular organization of knowledge in LLMs"], "limitations": "", "keywords": ["large language models", "modular organization", "semantic components", "sparse autoencoders", "counterfactual responses"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2506.18148", "pdf": "https://arxiv.org/pdf/2506.18148.pdf", "abs": "https://arxiv.org/abs/2506.18148", "title": "QuranMorph: Morphologically Annotated Quranic Corpus", "authors": ["Diyam Akra", "Tymaa Hammouda", "Mustafa Jarrar"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present the QuranMorph corpus, a morphologically annotated corpus for the\nQuran (77,429 tokens). Each token in the QuranMorph was manually lemmatized and\ntagged with its part-of-speech by three expert linguists. The lemmatization\nprocess utilized lemmas from Qabas, an Arabic lexicographic database linked\nwith 110 lexicons and corpora of 2 million tokens. The part-of-speech tagging\nwas performed using the fine-grained SAMA/Qabas tagset, which encompasses 40\ntags. As shown in this paper, this rich lemmatization and POS tagset enabled\nthe QuranMorph corpus to be inter-linked with many linguistic resources. The\ncorpus is open-source and publicly available as part of the SinaLab resources\nat (https://sina.birzeit.edu/quran)", "AI": {"tldr": "The QuranMorph corpus is a morphologically annotated dataset of the Quran with manual lemmatization and part-of-speech tagging.", "motivation": "To provide a rich linguistic resource for morphological analysis of the Quran.", "method": "Manual lemmatization and part-of-speech tagging by three expert linguists using 40 fine-grained tags.", "result": "The corpus consists of 77,429 tokens and can be interlinked with various linguistic resources.", "conclusion": "The QuranMorph corpus is open-source and publicly available, enhancing access to Arabic linguistic resources.", "key_contributions": ["Creation of a comprehensive morphologically annotated corpus for the Quran", "Manual lemmatization and POS tagging using expert linguists", "Interlinking with existing linguistic resources"], "limitations": "", "keywords": ["QuranMorph", "morphologically annotated corpus", "lemmatization", "part-of-speech", "Arabic linguistics"], "importance_score": 2, "read_time_minutes": 5}}
{"id": "2506.18185", "pdf": "https://arxiv.org/pdf/2506.18185.pdf", "abs": "https://arxiv.org/abs/2506.18185", "title": "CareLab at #SMM4H-HeaRD 2025: Insomnia Detection and Food Safety Event Extraction with Domain-Aware Transformers", "authors": ["Zihan Liang", "Ziwen Pan", "Sumon Kanti Dey", "Azra Ismail"], "categories": ["cs.CL", "cs.AI"], "comment": "In the Proceedings of the 10th Social Media Mining for Health and\n  Health Real-World Data Workshop and Shared Tasks, co-located with AAAI ICWSM\n  2025", "summary": "This paper presents our system for the SMM4H-HeaRD 2025 shared tasks,\nspecifically Task 4 (Subtasks 1, 2a, and 2b) and Task 5 (Subtasks 1 and 2).\nTask 4 focused on detecting mentions of insomnia in clinical notes, while Task\n5 addressed the extraction of food safety events from news articles. We\nparticipated in all subtasks and report key findings across them, with\nparticular emphasis on Task 5 Subtask 1, where our system achieved strong\nperformance-securing first place with an F1 score of 0.958 on the test set. To\nattain this result, we employed encoder-based models (e.g., RoBERTa), alongside\nGPT-4 for data augmentation. This paper outlines our approach, including\npreprocessing, model architecture, and subtask-specific adaptations", "AI": {"tldr": "The paper describes a system designed for shared tasks in health data mining, achieving top results in detecting insomnia mentions and extracting food safety events, utilizing advanced models and data augmentation techniques.", "motivation": "The motivation behind this work is to advance the state of health informatics by improving the detection of relevant health-related mentions in clinical notes and extracting significant events related to food safety from public news articles.", "method": "The authors employed encoder-based models like RoBERTa and GPT-4 for data augmentation, implementing specific adaptations for each subtask in the shared task framework.", "result": "The system achieved an F1 score of 0.958 in Task 5 Subtask 1, securing first place in the competition and demonstrating robust performance across the evaluated subtasks.", "conclusion": "The findings indicate that leveraging advanced model architectures and data augmentation techniques can significantly enhance performance in health-related text mining tasks.", "key_contributions": ["Achieved first place in Task 5 Subtask 1 with an F1 score of 0.958", "Utilized state-of-the-art encoder models and GPT-4 for data augmentation", "Provided insights into the preprocessing and model architecture tailored for specific subtasks."], "limitations": "", "keywords": ["Insomnia Detection", "Food Safety Extraction", "Health Informatics", "Text Mining", "NLP Models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.18199", "pdf": "https://arxiv.org/pdf/2506.18199.pdf", "abs": "https://arxiv.org/abs/2506.18199", "title": "Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review", "authors": ["Bushra Asseri", "Estabrag Abdelaziz", "Areej Al-Wabil"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Large language models have demonstrated remarkable capabilities across\nvarious domains, yet concerns about cultural bias - particularly towards Arabs\nand Muslims - pose significant ethical challenges by perpetuating harmful\nstereotypes and marginalization. Despite growing recognition of bias in LLMs,\nprompt engineering strategies specifically addressing Arab and Muslim\nrepresentation remain understudied. This mixed-methods systematic review\nexamines such techniques, offering evidence-based guidance for researchers and\npractitioners. Following PRISMA guidelines and Kitchenham's systematic review\nmethodology, we analyzed 8 empirical studies published between 2021-2024\ninvestigating bias mitigation strategies. Our findings reveal five primary\nprompt engineering approaches: cultural prompting, affective priming,\nself-debiasing techniques, structured multi-step pipelines, and\nparameter-optimized continuous prompts. Although all approaches show potential\nfor reducing bias, effectiveness varied substantially across studies and bias\ntypes. Evidence suggests that certain bias types may be more resistant to\nprompt-based mitigation than others. Structured multi-step pipelines\ndemonstrated the highest overall effectiveness, achieving up to 87.7% reduction\nin bias, though they require greater technical expertise. Cultural prompting\noffers broader accessibility with substantial effectiveness. These results\nunderscore the accessibility of prompt engineering for mitigating cultural bias\nwithout requiring access to model parameters. The limited number of studies\nidentified highlights a significant research gap in this critical area. Future\nresearch should focus on developing culturally adaptive prompting techniques,\ncreating Arab and Muslim-specific evaluation resources, and integrating prompt\nengineering with complementary debiasing methods to address deeper stereotypes\nwhile maintaining model utility.", "AI": {"tldr": "This systematic review investigates prompt engineering strategies to mitigate cultural bias, particularly towards Arabs and Muslims, in large language models.", "motivation": "Address ethical challenges posed by cultural bias in LLMs that perpetuate stereotypes and marginalization of Arabs and Muslims.", "method": "Mixed-methods systematic review analyzing 8 empirical studies published between 2021-2024, following PRISMA guidelines and Kitchenham's methodology.", "result": "Identified five prompt engineering approaches: cultural prompting, affective priming, self-debiasing techniques, structured multi-step pipelines, and parameter-optimized continuous prompts, with varying effectiveness for bias mitigation.", "conclusion": "While structured multi-step pipelines showed the highest effectiveness, there is a significant gap in research on bias mitigation techniques for Arab and Muslim representation in LLMs.", "key_contributions": ["Systematic review of bias mitigation strategies in LLMs", "Identification of effective prompt engineering techniques", "Highlighting the need for further culturally adaptive research"], "limitations": "A limited number of studies on this critical area were reviewed, indicating a significant research gap.", "keywords": ["cultural bias", "large language models", "prompt engineering", "Arab representation", "bias mitigation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.18201", "pdf": "https://arxiv.org/pdf/2506.18201.pdf", "abs": "https://arxiv.org/abs/2506.18201", "title": "Deciphering Emotions in Children Storybooks: A Comparative Analysis of Multimodal LLMs in Educational Applications", "authors": ["Bushra Asseri", "Estabraq Abdelaziz", "Maha Al Mogren", "Tayef Alhefdhi", "Areej Al-Wabil"], "categories": ["cs.CL", "cs.CV", "cs.HC"], "comment": null, "summary": "Emotion recognition capabilities in multimodal AI systems are crucial for\ndeveloping culturally responsive educational technologies, yet remain\nunderexplored for Arabic language contexts where culturally appropriate\nlearning tools are critically needed. This study evaluates the emotion\nrecognition performance of two advanced multimodal large language models,\nGPT-4o and Gemini 1.5 Pro, when processing Arabic children's storybook\nillustrations. We assessed both models across three prompting strategies\n(zero-shot, few-shot, and chain-of-thought) using 75 images from seven Arabic\nstorybooks, comparing model predictions with human annotations based on\nPlutchik's emotional framework. GPT-4o consistently outperformed Gemini across\nall conditions, achieving the highest macro F1-score of 59% with\nchain-of-thought prompting compared to Gemini's best performance of 43%. Error\nanalysis revealed systematic misclassification patterns, with valence\ninversions accounting for 60.7% of errors, while both models struggled with\nculturally nuanced emotions and ambiguous narrative contexts. These findings\nhighlight fundamental limitations in current models' cultural understanding and\nemphasize the need for culturally sensitive training approaches to develop\neffective emotion-aware educational technologies for Arabic-speaking learners.", "AI": {"tldr": "Study evaluates emotion recognition in Arabic contexts using multimodal AI models GPT-4o and Gemini 1.5 Pro for educational tools.", "motivation": "To develop culturally responsive educational technologies for Arabic-speaking learners, focusing on emotion recognition capabilities in multimodal AI.", "method": "Two large language models (GPT-4o and Gemini 1.5 Pro) were tested on 75 images from Arabic children's storybooks using zero-shot, few-shot, and chain-of-thought prompting strategies, comparing model predictions with human annotations.", "result": "GPT-4o outperformed Gemini, achieving a macro F1-score of 59% with chain-of-thought prompting, while Gemini reached 43%. Misclassification included significant valence inversions and struggled with culturally nuanced emotions.", "conclusion": "Current models have fundamental limitations in cultural understanding, necessitating culturally sensitive training to create effective emotion-aware educational tools for Arabic learners.", "key_contributions": ["Evaluation of advanced multimodal AI models in Arabic educational contexts", "Identification of emotions based on Plutchik's framework", "Insights into limitations of models regarding cultural nuances in emotion recognition"], "limitations": "Both models have difficulty with culturally nuanced emotions and ambiguous contexts, highlighting the need for better training.", "keywords": ["emotion recognition", "multimodal AI", "Arabic educational technologies", "GPT-4o", "Gemini 1.5 Pro"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.18318", "pdf": "https://arxiv.org/pdf/2506.18318.pdf", "abs": "https://arxiv.org/abs/2506.18318", "title": "Enhancing Entity Aware Machine Translation with Multi-task Learning", "authors": ["An Trieu", "Phuong Nguyen", "Minh Le Nguyen"], "categories": ["cs.CL"], "comment": "In the Proceedings of SCIDOCA 2025", "summary": "Entity-aware machine translation (EAMT) is a complicated task in natural\nlanguage processing due to not only the shortage of translation data related to\nthe entities needed to translate but also the complexity in the context needed\nto process while translating those entities. In this paper, we propose a method\nthat applies multi-task learning to optimize the performance of the two\nsubtasks named entity recognition and machine translation, which improves the\nfinal performance of the Entity-aware machine translation task. The result and\nanalysis are performed on the dataset provided by the organizer of Task 2 of\nthe SemEval 2025 competition.", "AI": {"tldr": "The paper presents a multi-task learning approach to improve entity-aware machine translation (EAMT) by optimizing named entity recognition and translation subtasks.", "motivation": "EAMT is challenging due to insufficient translation data for entities and the contextual complexities involved in translating them.", "method": "The authors propose a multi-task learning framework that simultaneously optimizes entity recognition and machine translation to enhance EAMT performance.", "result": "Significant improvements in EAMT performance were observed through the proposed methodology, as validated on the SemEval 2025 competition dataset.", "conclusion": "The multi-task learning approach effectively enhances the performance of EAMT tasks by leveraging the synergy between entity recognition and translation processes.", "key_contributions": ["Proposed a novel multi-task learning approach for EAMT", "Improved performance metrics on named entity recognition and machine translation tasks", "Evaluated results on a benchmark dataset from SemEval 2025 competition"], "limitations": "", "keywords": ["Entity-aware machine translation", "Multi-task learning", "Named entity recognition", "Natural language processing", "SemEval 2025"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.18337", "pdf": "https://arxiv.org/pdf/2506.18337.pdf", "abs": "https://arxiv.org/abs/2506.18337", "title": "TranslationCorrect: A Unified Framework for Machine Translation Post-Editing with Predictive Error Assistance", "authors": ["Syed Mekael Wasti", "Shou-Yi Hung", "Christopher Collins", "En-Shiun Annie Lee"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Machine translation (MT) post-editing and research data collection often rely\non inefficient, disconnected workflows. We introduce TranslationCorrect, an\nintegrated framework designed to streamline these tasks. TranslationCorrect\ncombines MT generation using models like NLLB, automated error prediction using\nmodels like XCOMET or LLM APIs (providing detailed reasoning), and an intuitive\npost-editing interface within a single environment. Built with human-computer\ninteraction (HCI) principles in mind to minimize cognitive load, as confirmed\nby a user study. For translators, it enables them to correct errors and batch\ntranslate efficiently. For researchers, TranslationCorrect exports high-quality\nspan-based annotations in the Error Span Annotation (ESA) format, using an\nerror taxonomy inspired by Multidimensional Quality Metrics (MQM). These\noutputs are compatible with state-of-the-art error detection models and\nsuitable for training MT or post-editing systems. Our user study confirms that\nTranslationCorrect significantly improves translation efficiency and user\nsatisfaction over traditional annotation methods.", "AI": {"tldr": "TranslationCorrect is an integrated framework designed to enhance machine translation post-editing and research data collection by combining MT generation, automated error prediction, and a user-friendly interface.", "motivation": "To address the inefficiencies and disconnection in machine translation post-editing and research data collection workflows.", "method": "TranslationCorrect integrates machine translation generation, automated error prediction using advanced models, and a user-friendly post-editing interface, informed by HCI principles.", "result": "User studies indicate that TranslationCorrect improves translation efficiency and user satisfaction compared to traditional methods.", "conclusion": "TranslationCorrect offers a comprehensive solution for improving machine translation workflows and providing high-quality error annotations for researchers.", "key_contributions": ["Integrated framework combining MT generation and error prediction", "User-friendly post-editing interface designed on HCI principles", "High-quality span-based annotations compatible with error detection models"], "limitations": "", "keywords": ["Machine Translation", "Post-editing", "Human-Computer Interaction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.18341", "pdf": "https://arxiv.org/pdf/2506.18341.pdf", "abs": "https://arxiv.org/abs/2506.18341", "title": "Less Data Less Tokens: Multilingual Unification Learning for Efficient Test-Time Reasoning in LLMs", "authors": ["Kang Chen", "Mengdi Zhang", "Yixin Cao"], "categories": ["cs.CL"], "comment": null, "summary": "This paper explores the challenges of test-time scaling of large language\nmodels (LLMs), regarding both the data and inference efficiency. We highlight\nthe diversity of multi-lingual reasoning based on our pilot studies, and then\nintroduce a novel approach, \\(L^2\\) multi-lingual unification learning with a\ndecoding intervention strategy for further investigation. The basic idea of\n\\(L^2\\) is that the reasoning process varies across different languages, which\nmay be mutually beneficial to enhance both model performance and efficiency. In\nspecific, there are two types of multi-lingual data: the entire long\nchain-of-thought annotations in different languages and the step-wise mixture\nof languages. By further tuning based on them, we show that even small amounts\nof data can significantly improve reasoning capabilities. Our findings suggest\nthat multilingual learning reduces both the required data and the number of\ninference tokens while maintaining a comparable performance. Furthermore,\n\\(L^2\\) is orthogonal to other data efficient methods. Thus, we also emphasize\nthe importance of diverse data selection. The \\(L^2\\) method offers a promising\nsolution to the challenges of data collection and test-time compute efficiency\nin LLMs.", "AI": {"tldr": "This paper presents a novel approach called L2 multi-lingual unification learning to improve the efficiency and performance of large language models in multi-lingual reasoning.", "motivation": "To address the challenges of test-time scaling of large language models in terms of data and inference efficiency, especially in multi-lingual contexts.", "method": "Introduces the L2 method which utilizes both long chain-of-thought annotations and step-wise mixtures of languages during tuning to enhance reasoning capabilities with minimal data.", "result": "The L2 approach demonstrates that small amounts of multilingual data can significantly enhance reasoning abilities and reduce both data requirements and inference token counts without sacrificing performance.", "conclusion": "The L2 method provides a promising direction for improving data collection and computation efficiency in large language models, highlighting the importance of diverse data selection.", "key_contributions": ["Introduction of L2 multi-lingual unification learning", "Demonstration of performance improvement with minimal data", "Recognition of diverse data selection as crucial for efficiency"], "limitations": "", "keywords": ["large language models", "multilingual reasoning", "data efficiency", "inference efficiency", "L2 learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.18387", "pdf": "https://arxiv.org/pdf/2506.18387.pdf", "abs": "https://arxiv.org/abs/2506.18387", "title": "Evaluating Causal Explanation in Medical Reports with LLM-Based and Human-Aligned Metrics", "authors": ["Yousang Cho", "Key-Sun Choi"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, presented at LLM4Eval Workshop, SIGIR 2025 Padova, Italy,\n  July 17, 2025", "summary": "This study investigates how accurately different evaluation metrics capture\nthe quality of causal explanations in automatically generated diagnostic\nreports. We compare six metrics: BERTScore, Cosine Similarity, BioSentVec,\nGPT-White, GPT-Black, and expert qualitative assessment across two input types:\nobservation-based and multiple-choice-based report generation. Two weighting\nstrategies are applied: one reflecting task-specific priorities, and the other\nassigning equal weights to all metrics. Our results show that GPT-Black\ndemonstrates the strongest discriminative power in identifying logically\ncoherent and clinically valid causal narratives. GPT-White also aligns well\nwith expert evaluations, while similarity-based metrics diverge from clinical\nreasoning quality. These findings emphasize the impact of metric selection and\nweighting on evaluation outcomes, supporting the use of LLM-based evaluation\nfor tasks requiring interpretability and causal reasoning.", "AI": {"tldr": "This paper evaluates different metrics for assessing causal explanations in diagnostic reports, highlighting the effectiveness of LLM-based metrics.", "motivation": "To determine how accurately various evaluation metrics capture the quality of causal explanations in automatically generated diagnostic reports.", "method": "The study compares six metrics (BERTScore, Cosine Similarity, BioSentVec, GPT-White, GPT-Black, expert qualitative assessment) using two input types (observation-based and multiple-choice-based report generation) alongside two weighting strategies.", "result": "GPT-Black shows the best discriminative ability for identifying coherent and clinically valid causal narratives, while GPT-White aligns well with expert evaluations.", "conclusion": "The selection and weighting of metrics significantly affect evaluation outcomes, advocating for LLM-based evaluations in tasks needing interpretability and causal reasoning.", "key_contributions": ["Introduces a comparison of six different evaluation metrics for diagnostic report generation.", "Shows strong performance of LLM-based metrics (GPT-Black and GPT-White) over traditional similarity-based metrics.", "Highlights the implications of metric selection and weighting on evaluating causal narratives."], "limitations": "", "keywords": ["causal explanations", "diagnostic reports", "evaluation metrics", "LLM", "interpretability"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.18399", "pdf": "https://arxiv.org/pdf/2506.18399.pdf", "abs": "https://arxiv.org/abs/2506.18399", "title": "Lemmatization as a Classification Task: Results from Arabic across Multiple Genres", "authors": ["Mostafa Saeed", "Nizar Habash"], "categories": ["cs.CL"], "comment": null, "summary": "Lemmatization is crucial for NLP tasks in morphologically rich languages with\nambiguous orthography like Arabic, but existing tools face challenges due to\ninconsistent standards and limited genre coverage. This paper introduces two\nnovel approaches that frame lemmatization as classification into a\nLemma-POS-Gloss (LPG) tagset, leveraging machine translation and semantic\nclustering. We also present a new Arabic lemmatization test set covering\ndiverse genres, standardized alongside existing datasets. We evaluate character\nlevel sequence-to-sequence models, which perform competitively and offer\ncomplementary value, but are limited to lemma prediction (not LPG) and prone to\nhallucinating implausible forms. Our results show that classification and\nclustering yield more robust, interpretable outputs, setting new benchmarks for\nArabic lemmatization.", "AI": {"tldr": "The paper presents novel lemmatization approaches for Arabic, addressing challenges in NLP tasks by framing it as a classification problem and introducing a new evaluation dataset.", "motivation": "Existing lemmatization tools face challenges due to inconsistent standards and genre coverage in Arabic.", "method": "Framing lemmatization as classification into a Lemma-POS-Gloss tagset using machine translation and semantic clustering.", "result": "The proposed methods outperform existing tools, yielding more robust and interpretable outputs, and setting new benchmarks for Arabic lemmatization.", "conclusion": "Classification and clustering approaches provide significant improvements over traditional lemmatization methods, especially in morphologically rich languages like Arabic.", "key_contributions": ["Introduction of LPG tagset for lemmatization", "Development of a new Arabic lemmatization test set across diverse genres", "Evaluation of sequence-to-sequence models alongside classification methods"], "limitations": "Character-level models are limited to lemma prediction and may hallucinate implausible forms.", "keywords": ["lemmatization", "Arabic", "machine translation", "NLP", "semantic clustering"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.18421", "pdf": "https://arxiv.org/pdf/2506.18421.pdf", "abs": "https://arxiv.org/abs/2506.18421", "title": "TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Capabilities of Large Language Models", "authors": ["Ce Li", "Xiaofan Liu", "Zhiyan Song", "Ce Chi", "Chen Zhao", "Jingjing Yang", "Zhendong Wang", "Kexin Yang", "Boshen Shi", "Xing Wang", "Chao Deng", "Junlan Feng"], "categories": ["cs.CL", "cs.AI"], "comment": "Benmark report v1.0", "summary": "The majority of data in businesses and industries is stored in tables,\ndatabases, and data warehouses. Reasoning with table-structured data poses\nsignificant challenges for large language models (LLMs) due to its hidden\nsemantics, inherent complexity, and structured nature. One of these challenges\nis lacking an effective evaluation benchmark fairly reflecting the performances\nof LLMs on broad table reasoning abilities. In this paper, we fill in this gap,\npresenting a comprehensive table reasoning evolution benchmark, TReB, which\nmeasures both shallow table understanding abilities and deep table reasoning\nabilities, a total of 26 sub-tasks. We construct a high quality dataset through\nan iterative data processing procedure. We create an evaluation framework to\nrobustly measure table reasoning capabilities with three distinct inference\nmodes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs\nusing this frame work and prove its effectiveness. Experimental results reveal\nthat existing LLMs still have significant room for improvement in addressing\nthe complex and real world Table related tasks. Both the dataset and evaluation\nframework are publicly available, with the dataset hosted on [HuggingFace] and\nthe framework on [GitHub].", "AI": {"tldr": "This paper introduces a comprehensive benchmark for evaluating large language models' reasoning with table-structured data, called TReB, which encompasses 26 sub-tasks.", "motivation": "To address the challenges LLMs face when reasoning with table-structured data and to provide an effective evaluation benchmark reflecting their abilities.", "method": "The authors constructed a high-quality dataset through iterative data processing and developed an evaluation framework that measures reasoning capabilities using three inference modes: TCoT, PoT, and ICoT.", "result": "Benchmarking over 20 state-of-the-art LLMs revealed significant room for improvement in handling complex table-related tasks.", "conclusion": "The dataset and evaluation framework are publicly available, promoting further research in LLM capabilities for table reasoning.", "key_contributions": ["Introduction of TReB benchmark for table reasoning", "Development of a dataset for evaluating table reasoning", "Establishment of an evaluation framework with three inference modes"], "limitations": "", "keywords": ["table reasoning", "large language models", "benchmark", "dataset", "evaluation framework"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.18485", "pdf": "https://arxiv.org/pdf/2506.18485.pdf", "abs": "https://arxiv.org/abs/2506.18485", "title": "MeRF: Motivation-enhanced Reinforcement Finetuning for Large Reasoning Models", "authors": ["Junjie Zhang", "Guozheng Ma", "Shunyu Liu", "Haoyu Wang", "Jiaxing Huang", "Ting-En Lin", "Fei Huang", "Yongbin Li", "Dacheng Tao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful learn-to-reason paradigm for Large Language Models (LLMs) to tackle\ncomplex reasoning tasks. However, existing RLVR methods overlook one of the\nmost distinctive capabilities of LLMs, their in-context learning ability, as\nprominently demonstrated by the success of Chain-of-Thought (CoT) prompting.\nThis motivates us to explore how reinforcement learning can be effectively\ncombined with in-context learning to better improve the reasoning capabilities\nof LLMs. In this paper, we introduce Motivation-enhanced Reinforcement\nFinetuning} (MeRF), an intuitive yet effective method enhancing reinforcement\nlearning of LLMs by involving ``telling LLMs the rules of the game''.\nSpecifically, MeRF directly injects the reward specification into the prompt,\nwhich serves as an in-context motivation for model to improve its responses\nwith awareness of the optimization objective. This simple modification\nleverages the in-context learning ability of LLMs aligning generation with\noptimization, thereby incentivizing the model to generate desired outputs from\nboth inner motivation and external reward. Empirical evaluations on the Knights\nand Knaves~(K&K) logic puzzle reasoning benchmark demonstrate that\n\\texttt{MeRF} achieves substantial performance gains over baselines. Moreover,\nablation studies show that performance improves with greater consistency\nbetween the in-context motivation and the external reward function, while the\nmodel also demonstrates an ability to adapt to misleading motivations through\nreinforcement learning.", "AI": {"tldr": "The paper introduces Motivation-enhanced Reinforcement Finetuning (MeRF), a method that combines reinforcement learning with the in-context learning ability of LLMs to improve reasoning tasks.", "motivation": "Existing RLVR methods fail to leverage LLMs' in-context learning ability, particularly seen with Chain-of-Thought prompting, motivating the need for a new approach.", "method": "MeRF enhances reinforcement learning of LLMs by incorporating reward specifications directly into prompts, serving as an in-context motivation.", "result": "Empirical evaluations show that MeRF significantly outperforms existing baselines on the Knights and Knaves logic puzzle, demonstrating effective alignment between in-context motivation and external rewards.", "conclusion": "MeRF incentivizes models to generate outputs that align with optimization objectives through both intrinsic motivation and external rewards, while showing adaptability to misleading motivations.", "key_contributions": ["Introduction of MeRF as a novel method combining reinforcement learning with in-context learning for LLMs.", "Demonstration of significant performance gains on reasoning tasks compared to existing methods.", "Insights on the importance of consistency between in-context motivations and reward functions."], "limitations": "", "keywords": ["Reinforcement Learning", "Large Language Models", "In-Context Learning", "Motivation", "Knights and Knaves"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.18501", "pdf": "https://arxiv.org/pdf/2506.18501.pdf", "abs": "https://arxiv.org/abs/2506.18501", "title": "Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks: Strengths, Weaknesses, and Domain-Specific Performance", "authors": ["Wael Etaiwi", "Bushra Alhijawi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The increasing use of large language models (LLMs) in natural language\nprocessing (NLP) tasks has sparked significant interest in evaluating their\neffectiveness across diverse applications. While models like ChatGPT and\nDeepSeek have shown strong results in many NLP domains, a comprehensive\nevaluation is needed to understand their strengths, weaknesses, and\ndomain-specific abilities. This is critical as these models are applied to\nvarious tasks, from sentiment analysis to more nuanced tasks like textual\nentailment and translation. This study aims to evaluate ChatGPT and DeepSeek\nacross five key NLP tasks: sentiment analysis, topic classification, text\nsummarization, machine translation, and textual entailment. A structured\nexperimental protocol is used to ensure fairness and minimize variability. Both\nmodels are tested with identical, neutral prompts and evaluated on two\nbenchmark datasets per task, covering domains like news, reviews, and\nformal/informal texts. The results show that DeepSeek excels in classification\nstability and logical reasoning, while ChatGPT performs better in tasks\nrequiring nuanced understanding and flexibility. These findings provide\nvaluable insights for selecting the appropriate LLM based on task requirements.", "AI": {"tldr": "This study evaluates the effectiveness of large language models ChatGPT and DeepSeek across five key NLP tasks.", "motivation": "To understand the strengths, weaknesses, and domain-specific abilities of LLMs like ChatGPT and DeepSeek as they are utilized in various NLP applications.", "method": "A structured experimental protocol was implemented to evaluate both models on five NLP tasks using identical prompts and two benchmark datasets per task, ensuring fairness and minimizing variability.", "result": "DeepSeek excels in classification stability and logical reasoning, while ChatGPT performs better in tasks requiring nuanced understanding and flexibility.", "conclusion": "The findings offer insights for selecting the appropriate LLM based on specific task requirements.", "key_contributions": ["Comprehensive evaluation of LLMs across five NLP tasks", "Identification of strengths in classification and reasoning for DeepSeek", "Presentation of performance differences in nuanced tasks between the two models"], "limitations": "", "keywords": ["large language models", "natural language processing", "evaluation", "ChatGPT", "DeepSeek"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.18532", "pdf": "https://arxiv.org/pdf/2506.18532.pdf", "abs": "https://arxiv.org/abs/2506.18532", "title": "End-to-End Spoken Grammatical Error Correction", "authors": ["Mengjie Qian", "Rao Ma", "Stefano BannÃ²", "Mark J. F. Gales", "Kate M. Knill"], "categories": ["cs.CL", "cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Grammatical Error Correction (GEC) and feedback play a vital role in\nsupporting second language (L2) learners, educators, and examiners. While\nwritten GEC is well-established, spoken GEC (SGEC), aiming to provide feedback\nbased on learners' speech, poses additional challenges due to disfluencies,\ntranscription errors, and the lack of structured input. SGEC systems typically\nfollow a cascaded pipeline consisting of Automatic Speech Recognition (ASR),\ndisfluency detection, and GEC, making them vulnerable to error propagation\nacross modules. This work examines an End-to-End (E2E) framework for SGEC and\nfeedback generation, highlighting challenges and possible solutions when\ndeveloping these systems. Cascaded, partial-cascaded and E2E architectures are\ncompared, all built on the Whisper foundation model. A challenge for E2E\nsystems is the scarcity of GEC labeled spoken data. To address this, an\nautomatic pseudo-labeling framework is examined, increasing the training data\nfrom 77 to over 2500 hours. To improve the accuracy of the SGEC system,\nadditional contextual information, exploiting the ASR output, is investigated.\nCandidate feedback of their mistakes is an essential step to improving\nperformance. In E2E systems the SGEC output must be compared with an estimate\nof the fluent transcription to obtain the feedback. To improve the precision of\nthis feedback, a novel reference alignment process is proposed that aims to\nremove hypothesised edits that results from fluent transcription errors.\nFinally, these approaches are combined with an edit confidence estimation\napproach, to exclude low-confidence edits. Experiments on the in-house\nLinguaskill (LNG) corpora and the publicly available Speak & Improve (S&I)\ncorpus show that the proposed approaches significantly boost E2E SGEC\nperformance.", "AI": {"tldr": "This paper proposes an End-to-End (E2E) framework for spoken Grammatical Error Correction (SGEC) which addresses challenges like disfluencies and errors in Automatic Speech Recognition. It utilizes a novel pseudo-labeling approach and feedback mechanisms to enhance system accuracy.", "motivation": "To address the challenges in providing spoken feedback for second language learners due to limitations in existing spoken GEC systems.", "method": "An End-to-End framework using a cascaded architecture based on the Whisper model, enhanced with automatic pseudo-labeling, contextual information, and a novel reference alignment process for feedback generation.", "result": "Experiments demonstrate significant improvements in SGEC performance on both in-house and public datasets, increasing training data availability and enhancing feedback precision.", "conclusion": "The proposed E2E SGEC framework offers a robust solution to existing limitations, providing effective feedback for L2 learners by leveraging additional contextual information and improved data labeling techniques.", "key_contributions": ["Proposed an End-to-End framework for SGEC leveraging the Whisper model.", "Developed an automatic pseudo-labeling framework to increase labeled spoken data for training.", "Introduced a novel reference alignment process to enhance feedback generation."], "limitations": "The scarcity of labeled spoken data remains a challenge, even with proposed solutions.", "keywords": ["Grammatical Error Correction", "Spoken Feedback", "End-to-End Framework"], "importance_score": 6, "read_time_minutes": 18}}
{"id": "2506.18535", "pdf": "https://arxiv.org/pdf/2506.18535.pdf", "abs": "https://arxiv.org/abs/2506.18535", "title": "When Fine-Tuning Fails: Lessons from MS MARCO Passage Ranking", "authors": ["Manu Pande", "Shahil Kumar", "Anay Yatin Damle"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "This paper investigates the counterintuitive phenomenon where fine-tuning\npre-trained transformer models degrades performance on the MS MARCO passage\nranking task. Through comprehensive experiments involving five model\nvariants-including full parameter fine-tuning and parameter efficient LoRA\nadaptations-we demonstrate that all fine-tuning approaches underperform the\nbase sentence-transformers/all- MiniLM-L6-v2 model (MRR@10: 0.3026). Our\nanalysis reveals that fine-tuning disrupts the optimal embedding space\nstructure learned during the base model's extensive pre-training on 1 billion\nsentence pairs, including 9.1 million MS MARCO samples. UMAP visualizations\nshow progressive embedding space flattening, while training dynamics analysis\nand computational efficiency metrics further support our findings. These\nresults challenge conventional wisdom about transfer learning effectiveness on\nsaturated benchmarks and suggest architectural innovations may be necessary for\nmeaningful improvements.", "AI": {"tldr": "Fine-tuning pre-trained transformer models can degrade performance on the MS MARCO passage ranking task, contrary to common expectations.", "motivation": "To investigate the performance degradation observed in fine-tuned pre-trained transformer models on a key benchmarking task.", "method": "Comprehensive experiments with five model variants, including full parameter fine-tuning and LoRA adaptations, analyzed their performance on the MS MARCO passage ranking task, supported by UMAP visualizations and training dynamics analysis.", "result": "All fine-tuning approaches underperformed compared to the base sentence-transformers/all-MiniLM-L6-v2 model with a MRR@10 of 0.3026.", "conclusion": "Fine-tuning disrupts the optimal embedding space structure learned during extensive pre-training, suggesting that architectural innovations might be necessary to improve performance.", "key_contributions": ["Demonstrated performance degradation in fine-tuning transformer models.", "Provided empirical results challenging conventional transfer learning wisdom.", "Highlighted the need for architectural innovations in model design."], "limitations": "", "keywords": ["fine-tuning", "transformer models", "MS MARCO", "machine learning", "transfer learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.18576", "pdf": "https://arxiv.org/pdf/2506.18576.pdf", "abs": "https://arxiv.org/abs/2506.18576", "title": "A Modular Taxonomy for Hate Speech Definitions and Its Impact on Zero-Shot LLM Classification Performance", "authors": ["Matteo Melis", "Gabriella Lapesa", "Dennis Assenmacher"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Detecting harmful content is a crucial task in the landscape of NLP\napplications for Social Good, with hate speech being one of its most dangerous\nforms. But what do we mean by hate speech, how can we define it, and how does\nprompting different definitions of hate speech affect model performance? The\ncontribution of this work is twofold. At the theoretical level, we address the\nambiguity surrounding hate speech by collecting and analyzing existing\ndefinitions from the literature. We organize these definitions into a taxonomy\nof 14 Conceptual Elements-building blocks that capture different aspects of\nhate speech definitions, such as references to the target of hate (individual\nor groups) or of the potential consequences of it. At the experimental level,\nwe employ the collection of definitions in a systematic zero-shot evaluation of\nthree LLMs, on three hate speech datasets representing different types of data\n(synthetic, human-in-the-loop, and real-world). We find that choosing different\ndefinitions, i.e., definitions with a different degree of specificity in terms\nof encoded elements, impacts model performance, but this effect is not\nconsistent across all architectures.", "AI": {"tldr": "This paper investigates how different definitions of hate speech affect the performance of language models in NLP applications for social good.", "motivation": "The paper aims to clarify the ambiguity surrounding hate speech definitions and their impact on the detection of harmful content in NLP.", "method": "The authors analyze existing definitions of hate speech, organizing them into a taxonomy of 14 conceptual elements. They conduct a systematic zero-shot evaluation of three large language models using three different hate speech datasets.", "result": "The study finds that varying definitions of hate speech influence model performance differently, but the effects are inconsistent across different model architectures.", "conclusion": "This work highlights the importance of precise definitions in hate speech detection and offers a structured approach to evaluate their impact on model outcomes.", "key_contributions": ["Creation of a taxonomy of hate speech definitions", "Systematic evaluation of LLM performance based on varying definitions", "Demonstration of inconsistent model performance based on definition specificity"], "limitations": "The study may not account for all possible definitions of hate speech and their contextual relevance.", "keywords": ["hate speech", "NLP", "taxonomy", "LLM", "model performance"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.18582", "pdf": "https://arxiv.org/pdf/2506.18582.pdf", "abs": "https://arxiv.org/abs/2506.18582", "title": "Parallel Continuous Chain-of-Thought with Jacobi Iteration", "authors": ["Haoyi Wu", "Zhihao Teng", "Kewei Tu"], "categories": ["cs.CL"], "comment": "under review", "summary": "Continuous chain-of-thought has been shown to be effective in saving\nreasoning tokens for large language models. By reasoning with continuous latent\nthought tokens, continuous CoT is able to perform implicit reasoning in a\ncompact manner. However, the sequential dependencies between latent thought\ntokens spoil parallel training, leading to long training time. In this paper,\nwe propose Parallel Continuous Chain-of-Thought (PCCoT), which performs Jacobi\niteration on the latent thought tokens, updating them iteratively in parallel\ninstead of sequentially and thus improving both training and inference\nefficiency of continuous CoT. Experiments demonstrate that by choosing the\nproper number of iterations, we are able to achieve comparable or even better\nperformance while saving nearly 50% of the training and inference time.\nMoreover, PCCoT shows better stability and robustness in the training process.\nOur code is available at https://github.com/whyNLP/PCCoT.", "AI": {"tldr": "Proposes a method called Parallel Continuous Chain-of-Thought (PCCoT) to enhance the efficiency of reasoning in language models by training latent thought tokens in parallel instead of sequentially.", "motivation": "The paper addresses the inefficiencies in training large language models caused by sequential dependencies in continuous chain-of-thought reasoning.", "method": "The proposed PCCoT employs Jacobi iteration to update latent thought tokens iteratively in parallel, improving training and inference efficiency over traditional methods.", "result": "Experiments show that PCCoT can achieve comparable or better performance while significantly reducing training and inference time by nearly 50%.", "conclusion": "PCCoT offers improved stability and robustness in the training process of continuous chain-of-thought models, making it a viable alternative for enhancing model efficiency.", "key_contributions": ["Introduction of Parallel Continuous Chain-of-Thought (PCCoT) method", "Demonstrated significant reduction in training and inference time", "Improved stability and robustness in model training"], "limitations": "", "keywords": ["Parallel Continuous Chain-of-Thought", "Latent Thought Tokens", "Machine Learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.18600", "pdf": "https://arxiv.org/pdf/2506.18600.pdf", "abs": "https://arxiv.org/abs/2506.18600", "title": "Reply to \"Emergent LLM behaviors are observationally equivalent to data leakage\"", "authors": ["Ariel Flint Ashery", "Luca Maria Aiello", "Andrea Baronchelli"], "categories": ["cs.CL", "cs.GT", "cs.MA"], "comment": "Reply to arXiv:2505.23796", "summary": "A potential concern when simulating populations of large language models\n(LLMs) is data contamination, i.e. the possibility that training data may shape\noutcomes in unintended ways. While this concern is important and may hinder\ncertain experiments with multi-agent models, it does not preclude the study of\ngenuinely emergent dynamics in LLM populations. The recent critique by Barrie\nand T\\\"ornberg [1] of the results of Flint Ashery et al. [2] offers an\nopportunity to clarify that self-organisation and model-dependent emergent\ndynamics can be studied in LLM populations, highlighting how such dynamics have\nbeen empirically observed in the specific case of social conventions.", "AI": {"tldr": "The paper discusses data contamination in the study of large language model populations and clarifies how emergent dynamics can still be investigated despite such concerns.", "motivation": "To address data contamination concerns in simulations of large language models and clarify the possibility of studying emergent dynamics.", "method": "The paper analyzes critiques in existing literature, particularly focusing on the impact of training data on LLM outcomes while also examining empirical observations of emergent dynamics.", "result": "The authors demonstrate that self-organization and emergent dynamics can indeed be observed in LLM populations, using social conventions as a specific example.", "conclusion": "The study reinforces the viability of investigating emergent dynamics in LLM populations, despite criticisms regarding data contamination.", "key_contributions": ["Clarification of the impact of data contamination on multi-agent models", "Demonstration of emergent dynamics in LLM populations through empirical evidence", "Discussion of self-organization in social conventions among LLMs"], "limitations": "", "keywords": ["large language models", "data contamination", "emergent dynamics", "self-organisation", "social conventions"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.18602", "pdf": "https://arxiv.org/pdf/2506.18602.pdf", "abs": "https://arxiv.org/abs/2506.18602", "title": "Semantic similarity estimation for domain specific data using BERT and other techniques", "authors": ["R. Prashanth"], "categories": ["cs.CL", "stat.AP"], "comment": "This is a preprint version of an article accepted for publication in\n  the proceedings of Machine Learning and Data Mining 2019", "summary": "Estimation of semantic similarity is an important research problem both in\nnatural language processing and the natural language understanding, and that\nhas tremendous application on various downstream tasks such as question\nanswering, semantic search, information retrieval, document clustering,\nword-sense disambiguation and machine translation. In this work, we carry out\nthe estimation of semantic similarity using different state-of-the-art\ntechniques including the USE (Universal Sentence Encoder), InferSent and the\nmost recent BERT, or Bidirectional Encoder Representations from Transformers,\nmodels. We use two question pairs datasets for the analysis, one is a domain\nspecific in-house dataset and the other is a public dataset which is the\nQuora's question pairs dataset. We observe that the BERT model gave much\nsuperior performance as compared to the other methods. This should be because\nof the fine-tuning procedure that is involved in its training process, allowing\nit to learn patterns based on the training data that is used. This works\ndemonstrates the applicability of BERT on domain specific datasets. We infer\nfrom the analysis that BERT is the best technique to use in the case of domain\nspecific data.", "AI": {"tldr": "This paper analyzes semantic similarity estimation using state-of-the-art techniques, highlighting the effectiveness of BERT on domain-specific datasets.", "motivation": "To explore the estimation of semantic similarity in natural language processing and its applications in various tasks such as question answering and machine translation.", "method": "The study utilizes different models including Universal Sentence Encoder, InferSent, and BERT, evaluated on two question pairs datasets: an in-house domain-specific dataset and the Quora question pairs dataset.", "result": "The BERT model outperformed other methods, particularly in the context of domain-specific data, due to its fine-tuning capabilities.", "conclusion": "BERT is established as the most effective model for semantic similarity estimation on domain-specific datasets based on the results of this analysis.", "key_contributions": ["Demonstration of BERT's superior performance in semantic similarity tasks.", "Analysis based on both in-house and public datasets.", "Insights into the impact of fine-tuning on model performance."], "limitations": "", "keywords": ["semantic similarity", "natural language processing", "BERT", "question answering", "domain specific"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.18621", "pdf": "https://arxiv.org/pdf/2506.18621.pdf", "abs": "https://arxiv.org/abs/2506.18621", "title": "The Anatomy of Speech Persuasion: Linguistic Shifts in LLM-Modified Speeches", "authors": ["Alisa Barkar", "Mathieu Chollet", "Matthieu Labeau", "Beatrice Biancardi", "Chloe Clavel"], "categories": ["cs.CL"], "comment": "Under submission to ICNLSP 2025. 9 pages, 2 tables", "summary": "This study examines how large language models understand the concept of\npersuasiveness in public speaking by modifying speech transcripts from PhD\ncandidates in the \"Ma These en 180 Secondes\" competition, using the 3MT French\ndataset. Our contributions include a novel methodology and an interpretable\ntextual feature set integrating rhetorical devices and discourse markers. We\nprompt GPT-4o to enhance or diminish persuasiveness and analyze linguistic\nshifts between original and generated speech in terms of the new features.\nResults indicate that GPT-4o applies systematic stylistic modifications rather\nthan optimizing persuasiveness in a human-like manner. Notably, it manipulates\nemotional lexicon and syntactic structures (such as interrogative and\nexclamatory clauses) to amplify rhetorical impact.", "AI": {"tldr": "This study analyzes how large language models, specifically GPT-4o, understand and modify persuasiveness in public speaking through the use of an enhanced feature set.", "motivation": "To understand how large language models approach the concept of persuasiveness in public speaking.", "method": "The study modifies speech transcripts from a public speaking competition, employing a novel methodology and integrating textual features like rhetorical devices and discourse markers. GPT-4o is prompted to alter persuasiveness in generated speeches.", "result": "The analysis reveals that GPT-4o systematically applies stylistic changes without optimizing persuasiveness in a human-like way, manipulating emotional language and syntactic structures to enhance rhetorical impact.", "conclusion": "While GPT-4o can modify speech for stylistic effects, its approach to persuasiveness does not fully mimic human strategies, indicating limitations in its understanding of rhetorical nuances.", "key_contributions": ["A novel methodology for analyzing persuasiveness in speech transcripts.", "An interpretable textual feature set that incorporates rhetorical devices and discourse markers.", "Insights into the limitations of GPT-4o in replicating human-like persuasiveness."], "limitations": "The model's understanding of persuasiveness is not fully aligned with human approaches, suggesting a gap in its application to real-world public speaking.", "keywords": ["large language models", "persuasiveness", "public speaking", "GPT-4o", "rhetorical devices"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.18639", "pdf": "https://arxiv.org/pdf/2506.18639.pdf", "abs": "https://arxiv.org/abs/2506.18639", "title": "ByteSpan: Information-Driven Subword Tokenisation", "authors": ["ZÃ©bulon Goriely", "Suchir Salhan", "Pietro Lesci", "Julius Cheng", "Paula Buttery"], "categories": ["cs.CL"], "comment": "Accepted to TokShop 2025 (Non-archival)", "summary": "Recent dynamic tokenisation methods operate directly on bytes and pool their\nlatent representations into patches. This bears similarities to computational\nmodels of word segmentation that determine lexical boundaries using spikes in\nan autoregressive model's prediction error. Inspired by this connection, we\nexplore whether grouping predictable bytes - rather than pooling their\nrepresentations - can yield a useful fixed subword vocabulary. We propose a new\ninformation-driven subword tokeniser, ByteSpan, that uses an external\nbyte-level LM during training to identify contiguous predictable byte sequences\nand group them into subwords. Experiments show that ByteSpan yields efficient\nvocabularies with higher morphological alignment scores than BPE for English.\nMultilingual experiments show similar compression and R\\'enyi efficiency for 25\nlanguages.", "AI": {"tldr": "This paper presents ByteSpan, a new information-driven subword tokeniser that groups predictable byte sequences into subwords, outperforming BPE in morphological alignment and efficiency across multiple languages.", "motivation": "To improve subword tokenisation methods by exploring the grouping of predictable bytes instead of pooling their representations, inspired by computational models of word segmentation.", "method": "ByteSpan uses an external byte-level language model during training to find and group contiguous predictable byte sequences into subwords.", "result": "ByteSpan results in more efficient vocabularies with higher morphological alignment scores compared to Byte Pair Encoding (BPE) for English, and shows comparable compression and Renyi efficiency across 25 languages.", "conclusion": "The proposed ByteSpan tokeniser offers a novel approach to subword tokenisation that enhances morphological alignment and efficiency across languages.", "key_contributions": ["Introduction of ByteSpan as a new subword tokeniser.", "Demonstration of improved morphological alignment over BPE.", "Multilingual efficiency across 25 languages."], "limitations": "", "keywords": ["subword tokenization", "byte-level language model", "ByteSpan", "morphological alignment", "multilingual efficiency"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.18674", "pdf": "https://arxiv.org/pdf/2506.18674.pdf", "abs": "https://arxiv.org/abs/2506.18674", "title": "Is There a Case for Conversation Optimized Tokenizers in Large Language Models?", "authors": ["Raquel Ferrando", "Javier Conde", "Gonzalo MartÃ­nez", "Pedro Reviriego"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The computational and energy costs of Large Language Models (LLMs) have\nincreased exponentially driven by the growing model sizes and the massive\nadoption of LLMs by hundreds of millions of users. The unit cost of an LLM is\nthe computation of a token. Therefore, the tokenizer plays an important role in\nthe efficiency of a model, and they are carefully optimized to minimize the\nnumber of tokens for the text in their training corpus. One of the most popular\napplications of LLMs are chatbots that interact with users. A key observation\nis that, for those chatbots, what is important is the performance of the\ntokenizer in the user text input and the chatbot responses. Those are most\nlikely different from the text in the training corpus. So, a question that\nimmediately arises is whether there is a potential benefit in optimizing\ntokenizers for chatbot conversations. In this paper, this idea is explored for\ndifferent tokenizers by using a publicly available corpus of chatbot\nconversations to redesign their vocabularies and evaluate their performance in\nthis domain. The results show that conversation-optimized tokenizers\nconsistently reduce the number of tokens in chatbot dialogues, which can lead\nto meaningful energy savings, in the range of 5% to 10% while having minimal or\neven slightly positive impact on tokenization efficiency for the original\ntraining corpus.", "AI": {"tldr": "This paper explores the optimization of tokenizers specifically for chatbot conversations, revealing that conversation-optimized tokenizers reduce token counts and improve energy efficiency.", "motivation": "The increasing computational and energy costs of Large Language Models (LLMs) necessitate optimizations, particularly in tokenization for chatbot applications, which differ from training corpora.", "method": "The study evaluates various tokenizers using a publicly available corpus of chatbot conversations to redesign vocabularies and assess performance outcomes.", "result": "Conversation-optimized tokenizers reduce the number of tokens in chatbot dialogues by 5% to 10%, leading to potential energy savings.", "conclusion": "Optimizing tokenizers for chatbot interactions not only reduces token usage but does so with minimal impact on the original training corpus's efficiency.", "key_contributions": ["Demonstration of performance improvement in tokenizers specifically for chatbot contexts.", "Quantification of energy savings achieved through optimized tokenization in real conversations.", "Insights on the differences in tokenization needs between training corpora and user dialogue."], "limitations": "The study is limited to the particular chatbot conversation corpus used, and results may vary with different datasets or types of dialogues.", "keywords": ["Large Language Models", "tokenizers", "chatbots", "energy efficiency", "natural language processing"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2506.18703", "pdf": "https://arxiv.org/pdf/2506.18703.pdf", "abs": "https://arxiv.org/abs/2506.18703", "title": "Context Biasing for Pronunciations-Orthography Mismatch in Automatic Speech Recognition", "authors": ["Christian Huber", "Alexander Waibel"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Neural sequence-to-sequence systems deliver state-of-the-art performance for\nautomatic speech recognition. When using appropriate modeling units, e.g.,\nbyte-pair encoded characters, these systems are in principal open vocabulary\nsystems. In practice, however, they often fail to recognize words not seen\nduring training, e.g., named entities, acronyms, or domain-specific special\nwords. To address this problem, many context biasing methods have been\nproposed; however, for words with a pronunciation-orthography mismatch, these\nmethods may still struggle. We propose a method which allows corrections of\nsubstitution errors to improve the recognition accuracy of such challenging\nwords. Users can add corrections on the fly during inference. We show that with\nthis method we get a relative improvement in biased word error rate of up to\n11\\%, while maintaining a competitive overall word error rate.", "AI": {"tldr": "This paper presents a novel method to improve automatic speech recognition (ASR) systems' ability to recognize challenging words by allowing users to provide real-time corrections during inference.", "motivation": "Existing context biasing methods in ASR struggle with pronunciation-orthography mismatches in words not seen during training.", "method": "The proposed method enables users to correct substitution errors on-the-fly, enhancing recognition accuracy for difficult words.", "result": "The method demonstrates a relative improvement in biased word error rate of up to 11% while maintaining competitive overall recognition performance.", "conclusion": "Real-time user corrections can significantly enhance ASR accuracy, particularly for named entities, acronyms, and specialized terminology.", "key_contributions": ["Introduction of a user-correction mechanism during inference", "Improvement in recognition accuracy for unseen challenging words", "Demonstrated effectiveness through quantitative results."], "limitations": "The method may still face challenges with words that have extreme pronunciation-orthography mismatches.", "keywords": ["automatic speech recognition", "context biasing", "user correction"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2506.18710", "pdf": "https://arxiv.org/pdf/2506.18710.pdf", "abs": "https://arxiv.org/abs/2506.18710", "title": "Benchmarking the Pedagogical Knowledge of Large Language Models", "authors": ["Maxime LeliÃ¨vre", "Amy Waldock", "Meng Liu", "Natalia ValdÃ©s Aspillaga", "Alasdair Mackintosh", "MarÃ­a JosÃ© Ogando Portelo", "Jared Lee", "Paul Atherton", "Robin A. A. Ince", "Oliver G. B. Garrod"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Benchmarks like Massive Multitask Language Understanding (MMLU) have played a\npivotal role in evaluating AI's knowledge and abilities across diverse domains.\nHowever, existing benchmarks predominantly focus on content knowledge, leaving\na critical gap in assessing models' understanding of pedagogy - the method and\npractice of teaching. This paper introduces The Pedagogy Benchmark, a novel\ndataset designed to evaluate large language models on their Cross-Domain\nPedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)\npedagogical knowledge. These benchmarks are built on a carefully curated set of\nquestions sourced from professional development exams for teachers, which cover\na range of pedagogical subdomains such as teaching strategies and assessment\nmethods. Here we outline the methodology and development of these benchmarks.\nWe report results for 97 models, with accuracies spanning a range from 28% to\n89% on the pedagogical knowledge questions. We consider the relationship\nbetween cost and accuracy and chart the progression of the Pareto value\nfrontier over time. We provide online leaderboards at\nhttps://rebrand.ly/pedagogy which are updated with new models and allow\ninteractive exploration and filtering based on various model properties, such\nas cost per token and open-vs-closed weights, as well as looking at performance\nin different subjects. LLMs and generative AI have tremendous potential to\ninfluence education and help to address the global learning crisis.\nEducation-focused benchmarks are crucial to measure models' capacities to\nunderstand pedagogical concepts, respond appropriately to learners' needs, and\nsupport effective teaching practices across diverse contexts. They are needed\nfor informing the responsible and evidence-based deployment of LLMs and\nLLM-based tools in educational settings, and for guiding both development and\npolicy decisions.", "AI": {"tldr": "This paper introduces The Pedagogy Benchmark, a dataset for evaluating large language models' pedagogical knowledge and its applications in education.", "motivation": "The paper aims to fill the gap in evaluating AI models' understanding of pedagogy, beyond just content knowledge.", "method": "The benchmark is developed using curated questions from teacher professional development exams across various pedagogical domains.", "result": "Results for 97 models showed accuracies ranging from 28% to 89% on the pedagogical knowledge questions, with considerations on cost and accuracy included.", "conclusion": "Education-focused benchmarks are essential for measuring models' abilities to engage with pedagogical concepts and support effective teaching practices, addressing important educational challenges.", "key_contributions": ["Introduction of The Pedagogy Benchmark dataset", "Evaluation of 97 models' pedagogical knowledge", "Online leaderboards for interactive exploration of model performance"], "limitations": "", "keywords": ["pedagogy", "language models", "education", "benchmarking", "machine learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.18756", "pdf": "https://arxiv.org/pdf/2506.18756.pdf", "abs": "https://arxiv.org/abs/2506.18756", "title": "Semantic-Preserving Adversarial Attacks on LLMs: An Adaptive Greedy Binary Search Approach", "authors": ["Chong Zhang", "Xiang Li", "Jia Wang", "Shan Liang", "Haochen Xue", "Xiaobo Jin"], "categories": ["cs.CL", "cs.CR"], "comment": "19 pages, 8 figures", "summary": "Large Language Models (LLMs) increasingly rely on automatic prompt\nengineering in graphical user interfaces (GUIs) to refine user inputs and\nenhance response accuracy. However, the diversity of user requirements often\nleads to unintended misinterpretations, where automated optimizations distort\noriginal intentions and produce erroneous outputs. To address this challenge,\nwe propose the Adaptive Greedy Binary Search (AGBS) method, which simulates\ncommon prompt optimization mechanisms while preserving semantic stability. Our\napproach dynamically evaluates the impact of such strategies on LLM\nperformance, enabling robust adversarial sample generation. Through extensive\nexperiments on open and closed-source LLMs, we demonstrate AGBS's effectiveness\nin balancing semantic consistency and attack efficacy. Our findings offer\nactionable insights for designing more reliable prompt optimization systems.\nCode is available at: https://github.com/franz-chang/DOBS", "AI": {"tldr": "This paper presents the Adaptive Greedy Binary Search (AGBS) method to optimize prompts for Large Language Models (LLMs) while maintaining semantic stability.", "motivation": "The study addresses issues of user input misinterpretation caused by automated prompt engineering in GUIs, leading to erroneous outputs from LLMs.", "method": "The AGBS method simulates common prompt optimization mechanisms by dynamically evaluating their impact on LLM performance, allowing for robust adversarial sample generation.", "result": "Experimental results show that AGBS effectively balances semantic consistency and attack efficacy across various LLMs.", "conclusion": "The insights gained from the experiments can guide the design of more reliable systems for prompt optimization.", "key_contributions": ["Introduction of the AGBS method for prompt optimization in LLMs", "Demonstration of AGBS's effectiveness on open and closed-source LLMs", "Providing actionable insights for improving prompt optimization systems"], "limitations": "The study may not cover all potential user requirements or contexts in which prompts may be optimized.", "keywords": ["Large Language Models", "prompt engineering", "adaptive optimization", "semantic stability", "adversarial samples"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.18768", "pdf": "https://arxiv.org/pdf/2506.18768.pdf", "abs": "https://arxiv.org/abs/2506.18768", "title": "ASP2LJ : An Adversarial Self-Play Laywer Augmented Legal Judgment Framework", "authors": ["Ao Chang", "Tong Zhou", "Yubo Chen", "Delai Qiu", "Shengping Liu", "Kang Liu", "Jun Zhao"], "categories": ["cs.CL"], "comment": null, "summary": "Legal Judgment Prediction (LJP) aims to predict judicial outcomes, including\nrelevant legal charge, terms, and fines, which is a crucial process in Large\nLanguage Model(LLM). However, LJP faces two key challenges: (1)Long Tail\nDistribution: Current datasets, derived from authentic cases, suffer from high\nhuman annotation costs and imbalanced distributions, leading to model\nperformance degradation. (2)Lawyer's Improvement: Existing systems focus on\nenhancing judges' decision-making but neglect the critical role of lawyers in\nrefining arguments, which limits overall judicial accuracy. To address these\nissues, we propose an Adversarial Self-Play Lawyer Augmented Legal Judgment\nFramework, called ASP2LJ, which integrates a case generation module to tackle\nlong-tailed data distributions and an adversarial self-play mechanism to\nenhance lawyers' argumentation skills. Our framework enables a judge to\nreference evolved lawyers' arguments, improving the objectivity, fairness, and\nrationality of judicial decisions. Besides, We also introduce RareCases, a\ndataset for rare legal cases in China, which contains 120 tail-end cases. We\ndemonstrate the effectiveness of our approach on the SimuCourt dataset and our\nRareCases dataset. Experimental results show our framework brings improvements,\nindicating its utilization. Our contributions include an integrated framework,\na rare-case dataset, and publicly releasing datasets and code to support\nfurther research in automated judicial systems.", "AI": {"tldr": "This paper introduces ASP2LJ, a framework that enhances legal judgment prediction by addressing long-tailed data distributions and improving lawyers' argumentation via adversarial self-play.", "motivation": "Legal Judgment Prediction (LJP) is crucial for predicting judicial outcomes but faces challenges related to long-tailed data distributions and insufficient emphasis on lawyers' contributions.", "method": "The proposed ASP2LJ framework includes a case generation module for long-tailed data and an adversarial self-play mechanism to improve lawyer arguments.", "result": "The framework was tested on the SimuCourt and RareCases datasets, showing significant improvements in prediction quality and judicial decision-making.", "conclusion": "ASP2LJ effectively integrates case generation with lawyer argument enhancement, improving judicial outcomes and fairness in legal judgments.", "key_contributions": ["Integrated framework for legal judgment prediction", "Introduction of RareCases dataset with 120 rare legal cases", "Public release of datasets and code for future research"], "limitations": "", "keywords": ["Legal Judgment Prediction", "Adversarial Self-Play", "RareCases Dataset"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2506.18781", "pdf": "https://arxiv.org/pdf/2506.18781.pdf", "abs": "https://arxiv.org/abs/2506.18781", "title": "Existing LLMs Are Not Self-Consistent For Simple Tasks", "authors": ["Zhenru Lin", "Jiawen Tao", "Yang Yuan", "Andrew Chi-Chih Yao"], "categories": ["cs.CL"], "comment": "10 pages, 6 figures", "summary": "Large Language Models (LLMs) have grown increasingly powerful, yet ensuring\ntheir decisions remain transparent and trustworthy requires self-consistency --\nno contradictions in their internal reasoning. Our study reveals that even on\nsimple tasks, such as comparing points on a line or a plane, or reasoning in a\nfamily tree, all smaller models are highly inconsistent, and even\nstate-of-the-art models like DeepSeek-R1 and GPT-o4-mini are not fully\nself-consistent. To quantify and mitigate these inconsistencies, we introduce\ninconsistency metrics and propose two automated methods -- a graph-based and an\nenergy-based approach. While these fixes provide partial improvements, they\nalso highlight the complexity and importance of self-consistency in building\nmore reliable and interpretable AI. The code and data are available at\nhttps://github.com/scorpio-nova/llm-self-consistency.", "AI": {"tldr": "This study examines self-consistency in Large Language Models (LLMs) and introduces metrics and methods to mitigate inconsistencies in their reasoning.", "motivation": "As LLMs become more powerful, it is essential to ensure their decisions are transparent and trustworthy, which requires addressing inconsistencies in their internal reasoning processes.", "method": "The study introduces inconsistency metrics and two automated methods for improvement: a graph-based approach and an energy-based approach.", "result": "The findings indicate that smaller models are highly inconsistent, and even advanced models like DeepSeek-R1 and GPT-o4-mini display notable inconsistencies.", "conclusion": "Self-consistency is crucial for building reliable and interpretable AI systems, and while the proposed methods provide some improvement, challenges remain.", "key_contributions": ["Introduction of inconsistency metrics for LLMs", "Development of graph-based and energy-based methods to enhance self-consistency", "Demonstration of self-consistency issues in various LLM architectures"], "limitations": "Improvements from the proposed methods are only partial and highlight the complexity of achieving full self-consistency.", "keywords": ["Large Language Models", "self-consistency", "inconsistency metrics", "AI reliability", "automated methods"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.18819", "pdf": "https://arxiv.org/pdf/2506.18819.pdf", "abs": "https://arxiv.org/abs/2506.18819", "title": "RWESummary: A Framework and Test for Choosing Large Language Models to Summarize Real-World Evidence (RWE) Studies", "authors": ["Arjun Mukerji", "Michael L. Jackson", "Jason Jones", "Neil Sanghavi"], "categories": ["cs.CL", "cs.AI"], "comment": "24 pages, 2 figures", "summary": "Large Language Models (LLMs) have been extensively evaluated for general\nsummarization tasks as well as medical research assistance, but they have not\nbeen specifically evaluated for the task of summarizing real-world evidence\n(RWE) from structured output of RWE studies. We introduce RWESummary, a\nproposed addition to the MedHELM framework (Bedi, Cui, Fuentes, Unell et al.,\n2025) to enable benchmarking of LLMs for this task. RWESummary includes one\nscenario and three evaluations covering major types of errors observed in\nsummarization of medical research studies and was developed using Atropos\nHealth proprietary data. Additionally, we use RWESummary to compare the\nperformance of different LLMs in our internal RWE summarization tool. At the\ntime of publication, with 13 distinct RWE studies, we found the Gemini 2.5\nmodels performed best overall (both Flash and Pro). We suggest RWESummary as a\nnovel and useful foundation model benchmark for real-world evidence study\nsummarization.", "AI": {"tldr": "RWESummary is a proposed benchmarking tool for Large Language Models to summarize real-world evidence from structured RWE studies, revealing that Gemini 2.5 models outperform others.", "motivation": "To evaluate and benchmark LLMs for summarizing real-world evidence from medical research studies, an area that has not been thoroughly investigated.", "method": "Introducing RWESummary as part of the MedHELM framework, covering one scenario and three evaluations targeting summarization errors in medical research studies.", "result": "Gemini 2.5 models demonstrated the best performance in summarizing data from 13 distinct RWE studies.", "conclusion": "RWESummary establishes a novel benchmark for evaluating LLM performance in summarizing real-world evidence, suggesting significant improvements in this domain.", "key_contributions": ["Introduction of RWESummary for benchmarking LLMs in summarizing real-world evidence.", "Identification of major summarization error types in medical research.", "Empirical results showing the performance of Gemini 2.5 models compared to others."], "limitations": "", "keywords": ["Large Language Models", "Real-World Evidence", "Medical Summarization", "Benchmarking", "RWESummary"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2506.18828", "pdf": "https://arxiv.org/pdf/2506.18828.pdf", "abs": "https://arxiv.org/abs/2506.18828", "title": "MLLP-VRAIN UPV system for the IWSLT 2025 Simultaneous Speech Translation Translation task", "authors": ["Jorge Iranzo-SÃ¡nchez", "Javier Iranzo-SÃ¡nchez", "AdriÃ  GimÃ©nez", "Jorge Civera", "Alfons Juan"], "categories": ["cs.CL"], "comment": "IWSLT 2025 System Description", "summary": "This work describes the participation of the MLLP-VRAIN research group in the\nshared task of the IWSLT 2025 Simultaneous Speech Translation track. Our\nsubmission addresses the unique challenges of real-time translation of\nlong-form speech by developing a modular cascade system that adapts strong\npre-trained models to streaming scenarios. We combine Whisper Large-V3-Turbo\nfor ASR with the multilingual NLLB-3.3B model for MT, implementing lightweight\nadaptation techniques rather than training new end-to-end models from scratch.\nOur approach employs document-level adaptation with prefix training to enhance\nthe MT model's ability to handle incomplete inputs, while incorporating\nadaptive emission policies including a wait-$k$ strategy and RALCP for managing\nthe translation stream. Specialized buffer management techniques and\nsegmentation strategies ensure coherent translations across long audio\nsequences. Experimental results on the ACL60/60 dataset demonstrate that our\nsystem achieves a favorable balance between translation quality and latency,\nwith a BLEU score of 31.96 and non-computational-aware StreamLAAL latency of\n2.94 seconds. Our final model achieves a preliminary score on the official test\nset (IWSLT25Instruct) of 29.8 BLEU. Our work demonstrates that carefully\nadapted pre-trained components can create effective simultaneous translation\nsystems for long-form content without requiring extensive in-domain parallel\ndata or specialized end-to-end training.", "AI": {"tldr": "This paper presents a modular cascade system for real-time simultaneous speech translation, combining Whisper Large-V3-Turbo for ASR and NLLB-3.3B for MT, with innovative adaptations and lightweight techniques for effective long-form content translation.", "motivation": "To address challenges in real-time translation of long-form speech within the IWSLT 2025 Simultaneous Speech Translation track, aiming for improved translation quality with low latency.", "method": "Developed a modular cascade system that integrates pre-trained models for ASR and MT, with techniques like document-level adaptation, prefix training, specialized buffer management, and segmentation strategies.", "result": "Achieved a BLEU score of 31.96 and a latency of 2.94 seconds on a dataset, with a final model score of 29.8 BLEU on the official test set, demonstrating effectiveness without extensive in-domain data.", "conclusion": "The study shows that utilizing adapted pre-trained components allows for efficient simultaneous translation of long-form content, overcoming the need for new end-to-end training.", "key_contributions": ["Modular cascade system for real-time translation", "Lightweight adaptation techniques for existing models", "Effective handling of long-form audio sequences with minimal data requirements."], "limitations": "", "keywords": ["simultaneous translation", "speech translation", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.18831", "pdf": "https://arxiv.org/pdf/2506.18831.pdf", "abs": "https://arxiv.org/abs/2506.18831", "title": "STU-PID: Steering Token Usage via PID Controller for Efficient Large Language Model Reasoning", "authors": ["Aryasomayajula Ram Bharadwaj"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models employing extended chain-of-thought (CoT) reasoning\noften suffer from the overthinking phenomenon, generating excessive and\nredundant reasoning steps that increase computational costs while potentially\ndegrading performance. While recent work has explored static steering\napproaches to mitigate this issue, they lack the adaptability to dynamically\nadjust intervention strength based on real-time reasoning quality. We propose\nSTUPID (Steering Token Usage via PID controller), a novel training-free method\nthat employs a PID controller to dynamically modulate activation steering\nstrength during inference. Our approach combines a chunk-level classifier for\ndetecting redundant reasoning patterns with a PID control mechanism that\nadaptively adjusts steering intensity based on the predicted redundancy\nprobability. Experimental evaluation on GSM8K demonstrates that STUPID achieves\na 6% improvement in accuracy while reducing token usage by 32%, outperforming\nstatic steering baselines. Our method provides a principled framework for\ndynamic reasoning calibration that maintains reasoning quality while\nsignificantly improving computational efficiency.", "AI": {"tldr": "STUPID is a novel method using a PID controller to dynamically steer reasoning in Large Language Models, improving accuracy by 6% and reducing token usage by 32%.", "motivation": "To address the overthinking phenomenon in Large Language Models, which leads to excessive reasoning steps, increased computational costs, and potential performance degradation.", "method": "A training-free method that employs a PID controller to dynamically modulate activation steering strength during inference, combined with a chunk-level classifier to detect redundant reasoning patterns.", "result": "STUPID improves accuracy by 6% and reduces token usage by 32% on the GSM8K dataset, outperforming static steering approaches.", "conclusion": "STUPID provides a principled framework for dynamic reasoning calibration, enhancing computational efficiency while maintaining reasoning quality.", "key_contributions": ["Introduction of STUPID for dynamic reasoning adjustment in Large Language Models", "Utilization of a PID controller for modulation of reasoning patterns", "Demonstrated improvement in both accuracy and computational efficiency in experimental evaluations."], "limitations": "", "keywords": ["Large Language Models", "Chain-of-Thought reasoning", "PID controller", "Computational efficiency", "Reasoning quality"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.18841", "pdf": "https://arxiv.org/pdf/2506.18841.pdf", "abs": "https://arxiv.org/abs/2506.18841", "title": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning", "authors": ["Yuhao Wu", "Yushi Bai", "Zhiqiang Hu", "Roy Ka-Wei Lee", "Juanzi Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Ultra-long generation by large language models (LLMs) is a widely demanded\nscenario, yet it remains a significant challenge due to their maximum\ngeneration length limit and overall quality degradation as sequence length\nincreases. Previous approaches, exemplified by LongWriter, typically rely on\n''teaching'', which involves supervised fine-tuning (SFT) on synthetic\nlong-form outputs. However, this strategy heavily depends on synthetic SFT\ndata, which is difficult and costly to construct, often lacks coherence and\nconsistency, and tends to be overly artificial and structurally monotonous. In\nthis work, we propose an incentivization-based approach that, starting entirely\nfrom scratch and without relying on any annotated or synthetic data, leverages\nreinforcement learning (RL) to foster the emergence of ultra-long, high-quality\ntext generation capabilities in LLMs. We perform RL training starting from a\nbase model, similar to R1-Zero, guiding it to engage in reasoning that\nfacilitates planning and refinement during the writing process. To support\nthis, we employ specialized reward models that steer the LLM towards improved\nlength control, writing quality, and structural formatting. Experimental\nevaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,\nconsistently outperforms traditional SFT methods on long-form writing tasks,\nachieving state-of-the-art results across all metrics on WritingBench and\nArena-Write, and even surpassing 100B+ models such as DeepSeek R1 and\nQwen3-235B. We open-source our data and model checkpoints under\nhttps://huggingface.co/THU-KEG/LongWriter-Zero-32B", "AI": {"tldr": "This paper presents LongWriter-Zero, a model for ultra-long text generation using reinforcement learning, surpassing traditional methods reliant on synthetic data.", "motivation": "The paper addresses the challenges of ultra-long generation in large language models (LLMs) due to limitations in generation length and quality degradation, especially with synthetic fine-tuning methods.", "method": "The authors utilize an incentivization-based approach that leverages reinforcement learning (RL) to train a model from scratch, focusing on quality and coherence without synthetic data.", "result": "LongWriter-Zero consistently outperforms traditional supervised fine-tuning methods on long-form writing tasks, achieving state-of-the-art results on various metrics and datasets.", "conclusion": "The proposed approach demonstrates significant advancements in ultra-long text generation capabilities of LLMs, offering a practical way to enhance writing quality and control without synthetic data.", "key_contributions": ["Introduction of an incentivization-based approach for training LLMs", "Demonstration of state-of-the-art performance in long-form writing without reliance on synthetic data", "Open-sourcing of data and model checkpoints for community use"], "limitations": "", "keywords": ["long-form generation", "reinforcement learning", "large language models", "text generation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.18852", "pdf": "https://arxiv.org/pdf/2506.18852.pdf", "abs": "https://arxiv.org/abs/2506.18852", "title": "Mechanistic Interpretability Needs Philosophy", "authors": ["Iwan Williams", "Ninell Oldenburg", "Ruchira Dhar", "Joshua Hatherley", "Constanza Fierro", "Nina Rajcic", "Sandrine R. Schiller", "Filippos Stamatiou", "Anders SÃ¸gaard"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Mechanistic interpretability (MI) aims to explain how neural networks work by\nuncovering their underlying causal mechanisms. As the field grows in influence,\nit is increasingly important to examine not just models themselves, but the\nassumptions, concepts and explanatory strategies implicit in MI research. We\nargue that mechanistic interpretability needs philosophy: not as an\nafterthought, but as an ongoing partner in clarifying its concepts, refining\nits methods, and assessing the epistemic and ethical stakes of interpreting AI\nsystems. Taking three open problems from the MI literature as examples, this\nposition paper illustrates the value philosophy can add to MI research, and\noutlines a path toward deeper interdisciplinary dialogue.", "AI": {"tldr": "This position paper discusses the role of philosophy in mechanistic interpretability (MI) research, emphasizing its importance in clarifying concepts and methods.", "motivation": "The field of mechanistic interpretability is growing, making it crucial to examine the assumptions and concepts guiding this area of research.", "method": "The paper analyzes three open problems in the MI literature to demonstrate the potential contributions of philosophy.", "result": "Philosophy can provide valuable insights into the epistemic and ethical implications of MI research, enhancing interpretability efforts.", "conclusion": "The authors advocate for ongoing interdisciplinary dialogue between philosophy and MI research to refine methods and concepts.", "key_contributions": ["Highlights the need for philosophical engagement in MI research.", "Demonstrates how philosophy can clarify MI concepts.", "Outlines challenges in MI that philosophy can help address."], "limitations": "", "keywords": ["mechanistic interpretability", "philosophy", "AI ethics", "interdisciplinary dialogue", "causal mechanisms"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.18879", "pdf": "https://arxiv.org/pdf/2506.18879.pdf", "abs": "https://arxiv.org/abs/2506.18879", "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression", "authors": ["Junyan Li", "Yang Zhang", "Muhammad Yusuf Hassan", "Talha Chafekar", "Tianle Cai", "Zhile Ren", "Pengsheng Guo", "Foroozan Karimzadeh", "Colorado Reed", "Chong Wang", "Chuang Gan"], "categories": ["cs.CL", "cs.AI"], "comment": "ICML 2025 poster", "summary": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.", "AI": {"tldr": "This paper introduces Commutative Vector Quantization (CommVQ), a method to reduce memory usage of key-value caches in long-context LLM inference while maintaining high accuracy.", "motivation": "To tackle the memory bottleneck of key-value (KV) caches in long-context large language models (LLMs) during inference.", "method": "The authors introduce a lightweight encoder and codebook for additive quantization of the KV cache, which integrates with Rotary Position Embedding (RoPE) for efficient decoding.", "result": "Experiments demonstrate that CommVQ reduces the FP16 KV cache size by 87.5% with 2-bit quantization and allows for 1-bit quantization with minimal accuracy loss, enabling extensive context lengths on standard GPUs.", "conclusion": "CommVQ significantly improves memory efficiency in LLMs without sacrificing accuracy, making it feasible to utilize larger context lengths in practical scenarios.", "key_contributions": ["Introduction of Commutative Vector Quantization for KV cache reduction.", "Demonstration of 1-bit quantization supporting long contexts on high-performance GPUs.", "Efficient integration with Rotary Position Embedding to enhance decoding."], "limitations": "", "keywords": ["Large Language Models", "quantization", "memory efficiency", "key-value cache", "inference"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.18880", "pdf": "https://arxiv.org/pdf/2506.18880.pdf", "abs": "https://arxiv.org/abs/2506.18880", "title": "OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization", "authors": ["Yiyou Sun", "Shawn Hu", "Georgia Zhou", "Ken Zheng", "Hannaneh Hajishirzi", "Nouha Dziri", "Dawn Song"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent large-scale language models (LLMs) with long Chain-of-Thought\nreasoning-such as DeepSeek-R1-have achieved impressive results on\nOlympiad-level mathematics benchmarks. However, they often rely on a narrow set\nof strategies and struggle with problems that require a novel way of thinking.\nTo systematically investigate these limitations, we introduce\nOMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a\ncontrolled yet diverse benchmark designed to evaluate three axes of\nout-of-distribution generalization, inspired by Boden's typology of creativity:\n(1) Exploratory-applying known problem solving skills to more complex instances\nwithin the same problem domain; (2) Compositional-combining distinct reasoning\nskills, previously learned in isolation, to solve novel problems that require\nintegrating these skills in new and coherent ways; and (3)\nTransformative-adopting novel, often unconventional strategies by moving beyond\nfamiliar approaches to solve problems more effectively. OMEGA consists of\nprogrammatically generated training-test pairs derived from templated problem\ngenerators across geometry, number theory, algebra, combinatorics, logic, and\npuzzles, with solutions verified using symbolic, numerical, or graphical\nmethods. We evaluate frontier (or top-tier) LLMs and observe sharp performance\ndegradation as problem complexity increases. Moreover, we fine-tune the\nQwen-series models across all generalization settings and observe notable\nimprovements in exploratory generalization, while compositional generalization\nremains limited and transformative reasoning shows little to no improvement. By\nisolating and quantifying these fine-grained failures, OMEGA lays the\ngroundwork for advancing LLMs toward genuine mathematical creativity beyond\nmechanical proficiency.", "AI": {"tldr": "A benchmark called OMEGA evaluates LLMs' capabilities in solving out-of-distribution math problems along three generalization axes: exploratory, compositional, and transformative reasoning.", "motivation": "To systematically investigate the limitations of LLMs in solving complex mathematical problems and evaluate their out-of-distribution generalization capabilities.", "method": "OMEGA consists of programmatically generated training-test pairs derived from various mathematical domains, and it assesses LLMs in terms of exploratory, compositional, and transformative reasoning.", "result": "Top-tier LLMs show significant performance drops as problem complexity rises; fine-tuning improves exploratory generalization but fails to enhance compositional and transformative reasoning.", "conclusion": "OMEGA provides insights into the shortcomings of LLMs in mathematical creativity, indicating the need for further advancements in LLM capabilities beyond mere mechanical proficiency.", "key_contributions": ["Introduction of the OMEGA benchmark for evaluating mathematical reasoning", "Quantification of LLM performance across different reasoning capabilities", "Insights into the limitations of existing LLMs in creative problem solving"], "limitations": "Focuses primarily on LLMs without addressing potential algorithmic improvements or alternative approaches outside the evaluated models.", "keywords": ["Large language models", "Mathematical reasoning", "Out-of-distribution evaluation", "Generalization", "Creativity"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.18896", "pdf": "https://arxiv.org/pdf/2506.18896.pdf", "abs": "https://arxiv.org/abs/2506.18896", "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs", "authors": ["Jiaru Zou", "Ling Yang", "Jingwen Gu", "Jiahao Qiu", "Ke Shen", "Jingrui He", "Mengdi Wang"], "categories": ["cs.CL"], "comment": "Codes and Models: https://github.com/Gen-Verse/ReasonFlux", "summary": "Process Reward Models (PRMs) have recently emerged as a powerful framework\nfor supervising intermediate reasoning steps in large language models (LLMs).\nPrevious PRMs are primarily trained on model final output responses and\nstruggle to evaluate intermediate thinking trajectories robustly, especially in\nthe emerging setting of trajectory-response outputs generated by frontier\nreasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a\nnovel trajectory-aware PRM explicitly designed to evaluate the\ntrajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both\nstep-level and trajectory-level supervision, enabling fine-grained reward\nassignment aligned with structured chain-of-thought data. We adapt\nReasonFlux-PRM to support reward supervision under both offline and online\nsettings, including (i) selecting high-quality model distillation data for\ndownstream supervised fine-tuning of smaller models, (ii) providing dense\nprocess-level rewards for policy optimization during reinforcement learning,\nand (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results\non challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond\ndemonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs\n(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our\nderived ReasonFlux-PRM-7B yields consistent performance improvements, achieving\naverage gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement\nlearning, and 6.3% in test-time scaling. We also release our efficient\nReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.\nProjects: https://github.com/Gen-Verse/ReasonFlux", "AI": {"tldr": "This paper introduces ReasonFlux-PRM, a novel Process Reward Model designed for more robust evaluation of trajectory-response outputs from large language models, demonstrating significant performance improvements on various benchmarks.", "motivation": "To enhance the evaluation of intermediate reasoning steps in large language models, particularly in the context of trajectory-response outputs.", "method": "The paper presents ReasonFlux-PRM, which utilizes both step-level and trajectory-level supervision for fine-grained reward assignments.", "result": "ReasonFlux-PRM-7B selects higher quality model distillation data and improves performance metrics: 12.1% gain in supervised fine-tuning, 4.5% in reinforcement learning, and 6.3% in test-time scaling on challenging benchmarks.", "conclusion": "The proposed model outperforms existing PRMs and human-curated baselines, and an efficient version (ReasonFlux-PRM-1.5B) is released for edge deployment.", "key_contributions": ["Introduction of ReasonFlux-PRM for robust evaluation of LLMs", "Demonstrated consistent performance improvements across multiple benchmarks", "Release of a lightweight model for resource-constrained applications"], "limitations": "", "keywords": ["Process Reward Models", "trajectory-response outputs", "large language models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2402.05123", "pdf": "https://arxiv.org/pdf/2402.05123.pdf", "abs": "https://arxiv.org/abs/2402.05123", "title": "A Survey on Data Selection for LLM Instruction Tuning", "authors": ["Bolin Zhang", "Jiahao Wang", "Qianlong Du", "Jiajun Zhang", "Zhiying Tu", "Dianhui Chu"], "categories": ["cs.CL"], "comment": "Accepted by JAIR", "summary": "Instruction tuning is a vital step of training large language models (LLM),\nso how to enhance the effect of instruction tuning has received increased\nattention. Existing works indicate that the quality of the dataset is more\ncrucial than the quantity during instruction tuning of LLM. Therefore, recently\na lot of studies focus on exploring the methods of selecting high-quality\nsubset from instruction datasets, aiming to reduce training costs and enhance\nthe instruction-following capabilities of LLMs. This paper presents a\ncomprehensive survey on data selection for LLM instruction tuning. Firstly, we\nintroduce the wildly used instruction datasets. Then, we propose a new taxonomy\nof the data selection methods and provide a detailed introduction of recent\nadvances,and the evaluation strategies and results of data selection methods\nare also elaborated in detail. Finally, we emphasize the open challenges and\npresent new frontiers of this task.", "AI": {"tldr": "This paper surveys data selection methods for instruction tuning of large language models (LLMs), emphasizing dataset quality over quantity and proposing a new taxonomy for data selection strategies.", "motivation": "To enhance instruction tuning of LLMs by focusing on the quality of instruction datasets, which is critical for improving instruction-following capabilities and reducing training costs.", "method": "The paper reviews existing instruction datasets, introduces a new taxonomy of data selection methods, and discusses recent advances along with evaluation strategies and results.", "result": "Presents a comprehensive overview of recent developments in data selection for LLM instruction tuning, highlighting the importance of quality dataset selection.", "conclusion": "Identifies open challenges and suggests new research frontiers in the domain of data selection for instruction tuning of LLMs.", "key_contributions": ["Introduces a new taxonomy for data selection methods in instruction tuning", "Provides a detailed survey of existing methods and evaluations", "Highlights current challenges and future research directions"], "limitations": "", "keywords": ["instruction tuning", "large language models", "data selection", "dataset quality", "human-computer interaction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2405.08454", "pdf": "https://arxiv.org/pdf/2405.08454.pdf", "abs": "https://arxiv.org/abs/2405.08454", "title": "Alignment Helps Make the Most of Multimodal Data", "authors": ["Christian Arnold", "Andreas KÃ¼pfer"], "categories": ["cs.CL"], "comment": "Working Paper", "summary": "Political scientists increasingly analyze multimodal data. However, the\neffective analysis of such data requires aligning information across different\nmodalities. In our paper, we demonstrate the significance of such alignment.\nInformed by a systematic review of 2,703 papers, we find that political\nscientists typically do not align their multimodal data. Introducing a decision\ntree that guides alignment choices, our framework highlights alignment's\nuntapped potential and provides concrete advice in research design and modeling\ndecisions. We illustrate alignment's analytical value through two applications:\npredicting tonality in U.S. presidential campaign ads and cross-modal querying\nof German parliamentary speeches to examine responses to the far-right AfD.", "AI": {"tldr": "The paper explores the significance of aligning multimodal data in political science, highlighting its analytical value and providing a framework for alignment decisions.", "motivation": "Political scientists often analyze multimodal data but fail to align it effectively, which limits their research potential.", "method": "A systematic review of 2,703 papers was conducted to identify current practices in multimodal data alignment. A decision tree framework was introduced to guide researchers in alignment choices.", "result": "The study reveals that political scientists typically do not align multimodal data, and demonstrates alignment's potential through applications in predicting campaign ad tonality and querying parliamentary responses.", "conclusion": "Aligning multimodal data is essential for more effective research in political science, providing insights into both design and analytical processes.", "key_contributions": ["Systematic review of multimodal data alignment practices in political science", "Introduction of a decision tree framework for alignment choices", "Demonstration of practical applications for alignment in political analyses"], "limitations": "", "keywords": ["multimodal data", "political science", "data alignment", "decision tree", "research design"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2406.18219", "pdf": "https://arxiv.org/pdf/2406.18219.pdf", "abs": "https://arxiv.org/abs/2406.18219", "title": "A Closer Look into Mixture-of-Experts in Large Language Models", "authors": ["Ka Man Lo", "Zeyu Huang", "Zihan Qiu", "Zili Wang", "Jie Fu"], "categories": ["cs.CL", "cs.LG"], "comment": "NAACL 2025 Findings", "summary": "Mixture-of-experts (MoE) is gaining increasing attention due to its unique\nproperties and remarkable performance, especially for language tasks. By\nsparsely activating a subset of parameters for each token, MoE architecture\ncould increase the model size without sacrificing computational efficiency,\nachieving a better trade-off between performance and training costs. However,\nthe underlying mechanism of MoE still lacks further exploration, and its\nmodularization degree remains questionable. In this paper, we make an initial\nattempt to understand the inner workings of MoE-based large language models.\nConcretely, we comprehensively study the parametric and behavioral features of\nthree popular MoE-based models and reveal some intriguing observations,\nincluding 1) Neurons act like fine-grained experts; 2) The router of MoE\nusually selects experts with larger output norms; 3) The expert diversity\nincreases as the layer increases, while the last layer is an outlier, which is\nfurther validated by an initial experiment. Based on the observations, we also\nprovide suggestions for a broad spectrum of MoE practitioners, such as router\ndesign and expert allocation. We hope this work could shed light on future\nresearch on the MoE framework and other modular architectures. Code is\navailable at https://github.com/kamanphoebe/Look-into-MoEs.", "AI": {"tldr": "This paper investigates the inner workings of Mixture-of-Experts (MoE) models in language tasks, revealing key insights about neural behavior and expert selection.", "motivation": "To understand the unexplored mechanisms behind Mixture-of-Experts (MoE) models and their modularization in language tasks.", "method": "The paper analyzes the parametric and behavioral features of three popular MoE-based models through comprehensive studies and initial experiments.", "result": "Key observations include neurons functioning as fine-grained experts, routers favoring experts with larger output norms, and increasing expert diversity with layer depth, except for the final layer.", "conclusion": "The study provides insights and recommendations for MoE practitioners, aiming to illuminate future research directions on the MoE framework and related modular architectures.", "key_contributions": ["In-depth analysis of parametric and behavioral features of MoE models", "Identification of patterns in expert selection and diversity across layers", "Practical recommendations for MoE architecture design"], "limitations": "", "keywords": ["Mixture-of-experts", "language models", "router design", "expert allocation", "modular architectures"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2407.03859", "pdf": "https://arxiv.org/pdf/2407.03859.pdf", "abs": "https://arxiv.org/abs/2407.03859", "title": "Anthropocentric bias in language model evaluation", "authors": ["RaphaÃ«l MilliÃ¨re", "Charles Rathkopf"], "categories": ["cs.CL"], "comment": null, "summary": "Evaluating the cognitive capacities of large language models (LLMs) requires\novercoming not only anthropomorphic but also anthropocentric biases. This\narticle identifies two types of anthropocentric bias that have been neglected:\noverlooking how auxiliary factors can impede LLM performance despite competence\n(\"auxiliary oversight\"), and dismissing LLM mechanistic strategies that differ\nfrom those of humans as not genuinely competent (\"mechanistic chauvinism\").\nMitigating these biases necessitates an empirically-driven, iterative approach\nto mapping cognitive tasks to LLM-specific capacities and mechanisms, which can\nbe done by supplementing carefully designed behavioral experiments with\nmechanistic studies.", "AI": {"tldr": "This paper discusses the need to evaluate large language models (LLMs) without anthropocentric biases, highlighting two specific biases and proposing a methodological approach to assess LLM performance effectively.", "motivation": "To address biases in evaluating LLM cognitive capacities and improve performance assessments.", "method": "The paper proposes an empirically-driven, iterative approach that combines behavioral experiments with mechanistic studies.", "result": "Identifies neglected biases in LLM performance evaluation and outlines a dual-pronged methodology to assess LLM capabilities more effectively.", "conclusion": "Mitigating anthropocentric biases will provide a better understanding of LLM mechanisms and performance.", "key_contributions": ["Identification of auxiliary oversight and mechanistic chauvinism as biases in LLM evaluation", "Proposed methodology for mapping cognitive tasks to LLM-specific capacities", "Emphasis on the importance of combining behavioral and mechanistic studies"], "limitations": "", "keywords": ["large language models", "cognitive capacities", "evaluation biases", "mechanistic strategies", "behavioral experiments"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2407.12818", "pdf": "https://arxiv.org/pdf/2407.12818.pdf", "abs": "https://arxiv.org/abs/2407.12818", "title": "\"I understand why I got this grade\": Automatic Short Answer Grading with Feedback", "authors": ["Dishank Aggarwal", "Pritam Sil", "Bhaskaran Raman", "Pushpak Bhattacharyya"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "In recent years, there has been a growing interest in using Artificial\nIntelligence (AI) to automate student assessment in education. Among different\ntypes of assessments, summative assessments play a crucial role in evaluating a\nstudent's understanding level of a course. Such examinations often involve\nshort-answer questions. However, grading these responses and providing\nmeaningful feedback manually at scale is both time-consuming and\nlabor-intensive. Feedback is particularly important, as it helps students\nrecognize their strengths and areas for improvement. Despite the importance of\nthis task, there is a significant lack of publicly available datasets that\nsupport automatic short-answer grading with feedback generation. To address\nthis gap, we introduce Engineering Short Answer Feedback (EngSAF), a dataset\ndesigned for automatic short-answer grading with feedback. The dataset covers a\ndiverse range of subjects, questions, and answer patterns from multiple\nengineering domains and contains ~5.8k data points. We incorporate feedback\ninto our dataset by leveraging the generative capabilities of state-of-the-art\nlarge language models (LLMs) using our Label-Aware Synthetic Feedback\nGeneration (LASFG) strategy. This paper underscores the importance of enhanced\nfeedback in practical educational settings, outlines dataset annotation and\nfeedback generation processes, conducts a thorough EngSAF analysis, and\nprovides different LLMs-based zero-shot and finetuned baselines for future\ncomparison. The best-performing model (Mistral-7B) achieves an overall accuracy\nof 75.4% and 58.7% on unseen answers and unseen question test sets,\nrespectively. Additionally, we demonstrate the efficiency and effectiveness of\nour ASAG system through its deployment in a real-world end-semester exam at a\nreputed institute.", "AI": {"tldr": "The paper introduces the Engineering Short Answer Feedback (EngSAF) dataset for automatic grading of short-answer questions in education using AI, focusing on enhanced feedback generation with large language models.", "motivation": "Automation of student assessment in education using AI, especially for summative assessments involving short-answer questions, due to the time-consuming nature of manual grading and feedback.", "method": "The EngSAF dataset is created by incorporating feedback from a diverse set of engineering-related short-answer questions, utilizing a Label-Aware Synthetic Feedback Generation strategy with large language models.", "result": "The best-performing model, Mistral-7B, achieved an overall accuracy of 75.4% on unseen answers and 58.7% on unseen question test sets, demonstrating good performance of the ASAG system in real-world applications.", "conclusion": "The study emphasizes the need for enhanced feedback in educational assessments and provides a comprehensive analysis of the dataset alongside model performance benchmarks.", "key_contributions": ["Introduction of the EngSAF dataset for short-answer grading with feedback generation.", "Implementation of the LASFG strategy for feedback generation using LLMs.", "Performance analysis of various LLMs on the EngSAF dataset."], "limitations": "The dataset is specific to engineering domains and may not generalize across all subjects or question types.", "keywords": ["Artificial Intelligence", "Automated Assessment", "Short-Answer Grading", "Feedback Generation", "Large Language Models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2408.00863", "pdf": "https://arxiv.org/pdf/2408.00863.pdf", "abs": "https://arxiv.org/abs/2408.00863", "title": "UniMoT: Unified Molecule-Text Language Model with Discrete Token Representation", "authors": ["Shuhan Guo", "Yatao Bian", "Ruibing Wang", "Nan Yin", "Zhen Wang", "Quanming Yao"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "IJCAI 2025", "summary": "The remarkable success of Large Language Models (LLMs) across diverse tasks\nhas driven the research community to extend their capabilities to molecular\napplications. However, most molecular LLMs employ adapter-based architectures\nthat do not treat molecule and text modalities equally and lack a supervision\nsignal for the molecule modality. To address these issues, we introduce UniMoT,\na Unified Molecule-Text LLM adopting a tokenizer-based architecture that\nexpands the vocabulary of LLM with molecule tokens. Specifically, we introduce\na Vector Quantization-driven tokenizer that incorporates a Q-Former to bridge\nthe modality gap between molecule and text. This tokenizer transforms molecules\ninto sequences of molecule tokens with causal dependency, encapsulating\nhigh-level molecular and textual information. Equipped with this tokenizer,\nUniMoT can unify molecule and text modalities under a shared token\nrepresentation and an autoregressive training paradigm, enabling it to\ninterpret molecules as a foreign language and generate them as text. Following\na four-stage training scheme, UniMoT emerges as a multi-modal generalist\ncapable of performing both molecule-to-text and text-to-molecule tasks.\nExtensive experiments demonstrate that UniMoT achieves state-of-the-art\nperformance across a wide range of molecule comprehension and generation tasks.", "AI": {"tldr": "UniMoT is a Unified Molecule-Text LLM that adopts a tokenizer-based architecture to effectively bridge the gap between molecular and textual modalities, achieving state-of-the-art performance in molecule comprehension and generation tasks.", "motivation": "To extend the capabilities of Large Language Models (LLMs) to molecular applications and address limitations in existing molecular LLMs that do not treat molecule and text modalities equally.", "method": "UniMoT employs a Vector Quantization-driven tokenizer that transforms molecules into sequences of molecule tokens, facilitating a shared representation and autoregressive training for both modalities.", "result": "UniMoT demonstrates state-of-the-art performance on a variety of molecule comprehension and generation tasks.", "conclusion": "UniMoT successfully integrates molecule and text modalities, enabling it to interpret and generate molecular data effectively, thus advancing the application of LLMs in molecular contexts.", "key_contributions": ["Introduction of the UniMoT architecture for unified molecule-text processing.", "Development of a Vector Quantization-driven tokenizer that bridges modality gaps.", "Achievement of state-of-the-art performance on molecule tasks."], "limitations": "", "keywords": ["Large Language Models", "Molecular Applications", "Tokenization"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2408.08590", "pdf": "https://arxiv.org/pdf/2408.08590.pdf", "abs": "https://arxiv.org/abs/2408.08590", "title": "Reasoning Circuits in Language Models: A Mechanistic Interpretation of Syllogistic Inference", "authors": ["Geonhee Kim", "Marco Valentino", "AndrÃ© Freitas"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to Findings of ACL 2025", "summary": "Recent studies on reasoning in language models (LMs) have sparked a debate on\nwhether they can learn systematic inferential principles or merely exploit\nsuperficial patterns in the training data. To understand and uncover the\nmechanisms adopted for formal reasoning in LMs, this paper presents a\nmechanistic interpretation of syllogistic inference. Specifically, we present a\nmethodology for circuit discovery aimed at interpreting content-independent and\nformal reasoning mechanisms. Through two distinct intervention methods, we\nuncover a sufficient and necessary circuit involving middle-term suppression\nthat elucidates how LMs transfer information to derive valid conclusions from\npremises. Furthermore, we investigate how belief biases manifest in syllogistic\ninference, finding evidence of partial contamination from additional attention\nheads responsible for encoding commonsense and contextualized knowledge.\nFinally, we explore the generalization of the discovered mechanisms across\nvarious syllogistic schemes, model sizes and architectures. The identified\ncircuit is sufficient and necessary for syllogistic schemes on which the models\nachieve high accuracy (>60%), with compatible activation patterns across models\nof different families. Overall, our findings suggest that LMs learn\ntransferable content-independent reasoning mechanisms, but that, at the same\ntime, such mechanisms do not involve generalizable and abstract logical\nprimitives, being susceptible to contamination by the same world knowledge\nacquired during pre-training.", "AI": {"tldr": "This paper investigates formal reasoning in language models through mechanistic interpretation of syllogistic inference, revealing insights into content-independent reasoning mechanisms and their vulnerabilities to commonsense biases.", "motivation": "To understand if language models can learn systematic reasoning or only surface patterns, focusing on syllogistic inference.", "method": "Proposes a methodology for circuit discovery and uses two intervention methods to analyze reasoning mechanisms in language models.", "result": "Identifies a necessary circuit for deriving valid conclusions, highlighting the role of middle-term suppression and the impact of belief biases due to commonsense knowledge.", "conclusion": "Language models can learn transferable reasoning mechanisms, but these are not purely generalizable and are impacted by pre-existing world knowledge.", "key_contributions": ["Introduces a method for analyzing reasoning mechanisms in language models.", "Discovers a necessary circuit for syllogistic inference that shows activation compatibility across model architectures.", "Investigates the influence of commonsense knowledge on reasoning outcomes in models."], "limitations": "The discovered mechanisms may not represent abstract logical principles and are susceptible to biases from training data.", "keywords": ["language models", "syllogistic inference", "reasoning mechanisms", "circuit discovery", "commonsense bias"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2408.14470", "pdf": "https://arxiv.org/pdf/2408.14470.pdf", "abs": "https://arxiv.org/abs/2408.14470", "title": "Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large Language Models", "authors": ["Aradhye Agarwal", "Suhas K Ramesh", "Ayan Sengupta", "Tanmoy Chakraborty"], "categories": ["cs.CL"], "comment": "15 pages, 7 tables, 9 figures", "summary": "Fine-tuning large language models (LLMs) on downstream tasks requires\nsubstantial computational resources. Selective PEFT, a class of\nparameter-efficient fine-tuning (PEFT) methodologies, aims to mitigate these\ncomputational challenges by selectively fine-tuning only a small fraction of\nthe model parameters. Although parameter-efficient, these techniques often fail\nto match the performance of fully fine-tuned models, primarily due to inherent\nbiases introduced during parameter selection. Traditional selective PEFT\ntechniques use a fixed set of parameters selected using different importance\nheuristics, failing to capture parameter importance dynamically and often\nleading to suboptimal performance. We introduce $\\text{ID}^3$, a novel\nselective PEFT method that calculates parameter importance continually, and\ndynamically unmasks parameters by balancing exploration and exploitation in\nparameter selection. Our empirical study on 16 tasks spanning natural language\nunderstanding, mathematical reasoning and summarization demonstrates the\neffectiveness of our method compared to fixed-masking selective PEFT\ntechniques. We analytically show that $\\text{ID}^3$ reduces the number of\ngradient updates by a factor of two, enhancing computational efficiency. Since\n$\\text{ID}^3$ is robust to random initialization of neurons and operates\ndirectly on the optimization process, it is highly flexible and can be\nintegrated with existing additive and reparametrization-based PEFT techniques\nsuch as adapters and LoRA respectively.", "AI": {"tldr": "A new selective fine-tuning method for large language models called ID3, which dynamically adjusts parameter importance to enhance performance and reduce computational costs.", "motivation": "To address the inefficiencies in traditional parameter-efficient fine-tuning (PEFT) approaches that fail to adaptively select model parameters, leading to suboptimal model performance.", "method": "ID3 calculates parameter importance continually and dynamically unmasks parameters, balancing exploration and exploitation in the selection process.", "result": "ID3 demonstrated improved performance across 16 tasks in natural language understanding, mathematical reasoning, and summarization compared to fixed-masking techniques and reduced gradient updates by half.", "conclusion": "ID3 is flexible, robust to random initialization, and can be easily integrated with other PEFT techniques, resulting in enhanced computational efficiency and model performance.", "key_contributions": ["Introduction of ID3 for dynamic parameter selection in PEFT", "Empirical effectiveness demonstrated on diverse NLP tasks", "Reduced computational cost through fewer gradient updates"], "limitations": "", "keywords": ["parameter-efficient fine-tuning", "large language models", "dynamic parameter selection", "machine learning", "natural language processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2409.00097", "pdf": "https://arxiv.org/pdf/2409.00097.pdf", "abs": "https://arxiv.org/abs/2409.00097", "title": "Large Language Models for Disease Diagnosis: A Scoping Review", "authors": ["Shuang Zhou", "Zidu Xu", "Mian Zhang", "Chunpu Xu", "Yawen Guo", "Zaifu Zhan", "Yi Fang", "Sirui Ding", "Jiashuo Wang", "Kaishuai Xu", "Liqiao Xia", "Jeremy Yeung", "Daochen Zha", "Dongming Cai", "Genevieve B. Melton", "Mingquan Lin", "Rui Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "68 pages, 6 figures", "summary": "Automatic disease diagnosis has become increasingly valuable in clinical\npractice. The advent of large language models (LLMs) has catalyzed a paradigm\nshift in artificial intelligence, with growing evidence supporting the efficacy\nof LLMs in diagnostic tasks. Despite the increasing attention in this field, a\nholistic view is still lacking. Many critical aspects remain unclear, such as\nthe diseases and clinical data to which LLMs have been applied, the LLM\ntechniques employed, and the evaluation methods used. In this article, we\nperform a comprehensive review of LLM-based methods for disease diagnosis. Our\nreview examines the existing literature across various dimensions, including\ndisease types and associated clinical specialties, clinical data, LLM\ntechniques, and evaluation methods. Additionally, we offer recommendations for\napplying and evaluating LLMs for diagnostic tasks. Furthermore, we assess the\nlimitations of current research and discuss future directions. To our\nknowledge, this is the first comprehensive review for LLM-based disease\ndiagnosis.", "AI": {"tldr": "This paper reviews the application of large language models (LLMs) in automatic disease diagnosis, highlighting existing research, methods, limitations, and future directions.", "motivation": "With the rise of LLMs, there is a need for a comprehensive understanding of their capabilities and roles in disease diagnosis, as previous studies lack a holistic approach.", "method": "Conduct a thorough review of literature on LLM applications in disease diagnosis, analyzing disease types, clinical specialties, methods employed, and evaluation strategies.", "result": "The review reveals diverse applications of LLMs across various diseases and clinical settings, identifies prevailing LLM techniques, and highlights a lack of standard evaluation methods.", "conclusion": "This work presents a foundational overview for leveraging LLMs in clinical diagnostics and proposes future research avenues and evaluation recommendations.", "key_contributions": ["First comprehensive review of LLM applications in disease diagnosis", "Identification of key disease areas and LLM techniques", "Recommendations for evaluating LLMs in clinical settings"], "limitations": "Current research lacks standardized evaluation methods and a unified framework for LLMs in diagnostics.", "keywords": ["large language models", "disease diagnosis", "clinical data", "evaluation methods", "artificial intelligence"], "importance_score": 9, "read_time_minutes": 60}}
{"id": "2410.01171", "pdf": "https://arxiv.org/pdf/2410.01171.pdf", "abs": "https://arxiv.org/abs/2410.01171", "title": "Multilingual Retrieval Augmented Generation for Culturally-Sensitive Tasks: A Benchmark for Cross-lingual Robustness", "authors": ["Bryan Li", "Fiona Luo", "Samar Haider", "Adwait Agashe", "Tammy Li", "Runqi Liu", "Muqing Miao", "Shriya Ramakrishnan", "Yuan Yuan", "Chris Callison-Burch"], "categories": ["cs.CL"], "comment": "ACL 2025 (Findings)", "summary": "The paradigm of retrieval-augmented generated (RAG) helps mitigate\nhallucinations of large language models (LLMs). However, RAG also introduces\nbiases contained within the retrieved documents. These biases can be amplified\nin scenarios which are multilingual and culturally-sensitive, such as\nterritorial disputes. We thus introduce BordIRLines, a dataset of territorial\ndisputes paired with retrieved Wikipedia documents, across 49 languages. We\nevaluate the cross-lingual robustness of this RAG setting by formalizing\nseveral modes for multilingual retrieval. Our experiments on several LLMs show\nthat incorporating perspectives from diverse languages can in fact improve\nrobustness; retrieving multilingual documents best improves response\nconsistency and decreases geopolitical bias over RAG with purely in-language\ndocuments. We also consider how RAG responses utilize presented documents,\nfinding a much wider variance in the linguistic distribution of response\ncitations, when querying in low-resource languages. Our further analyses\ninvestigate the various aspects of a cross-lingual RAG pipeline, from retrieval\nto document contents. We release our benchmark and code to support continued\nresearch towards equitable information access across languages at\nhttps://huggingface.co/datasets/borderlines/bordirlines.", "AI": {"tldr": "The paper introduces BordIRLines, a multilingual dataset for territorial disputes that evaluates biases in RAG settings and demonstrates that multilingual retrieval can enhance the robustness of LLMs.", "motivation": "To address biases in large language models stemming from retrieved documents in multilingual and culturally-sensitive contexts.", "method": "BordIRLines dataset includes territorial disputes along with Wikipedia documents in 49 languages, evaluated through experiments on several LLMs for cross-lingual robustness.", "result": "Incorporating diverse language perspectives enhances response consistency and reduces geopolitical bias, demonstrating improved outcomes in multilingual retrieval scenarios.", "conclusion": "The study shows that multilingual document retrieval can significantly enhance LLM robustness and citation diversity, with extensive analyses of RAG pipeline aspects provided.", "key_contributions": ["Introduction of the BordIRLines dataset for multilingual retrieval in territorial disputes.", "Demonstration of improved response consistency and decreased bias through multilingual document incorporation.", "Release of benchmark and code for ongoing research in equitable information access."], "limitations": "Focus primarily on territorial disputes; results may not generalize to other contexts or domains.", "keywords": ["retrieval-augmented generation", "multilingual retrieval", "bias in language models", "territorial disputes", "cross-lingual robustness"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.21819", "pdf": "https://arxiv.org/pdf/2410.21819.pdf", "abs": "https://arxiv.org/abs/2410.21819", "title": "Self-Preference Bias in LLM-as-a-Judge", "authors": ["Koki Wataoka", "Tsubasa Takahashi", "Ryokan Ri"], "categories": ["cs.CL"], "comment": "Accepted at NeurIPS 2024 Safe Generative AI Workshop", "summary": "Automated evaluation leveraging large language models (LLMs), commonly\nreferred to as LLM evaluators or LLM-as-a-judge, has been widely used in\nmeasuring the performance of dialogue systems. However, the self-preference\nbias in LLMs has posed significant risks, including promoting specific styles\nor policies intrinsic to the LLMs. Despite the importance of this issue, there\nis a lack of established methods to measure the self-preference bias\nquantitatively, and its underlying causes are poorly understood. In this paper,\nwe introduce a novel quantitative metric to measure the self-preference bias.\nOur experimental results demonstrate that GPT-4 exhibits a significant degree\nof self-preference bias. To explore the causes, we hypothesize that LLMs may\nfavor outputs that are more familiar to them, as indicated by lower perplexity.\nWe analyze the relationship between LLM evaluations and the perplexities of\noutputs. Our findings reveal that LLMs assign significantly higher evaluations\nto outputs with lower perplexity than human evaluators, regardless of whether\nthe outputs were self-generated. This suggests that the essence of the bias\nlies in perplexity and that the self-preference bias exists because LLMs prefer\ntexts more familiar to them.", "AI": {"tldr": "This paper introduces a new quantitative metric to measure self-preference bias in LLMs and demonstrates its significant presence in GPT-4, linking it to lower perplexity outputs.", "motivation": "To address the lack of methods for quantifying the self-preference bias in large language models (LLMs) which can influence the evaluation of dialogue systems.", "method": "Develop a novel quantitative metric to measure self-preference bias and analyze the correlation between LLM evaluations and the perplexities of outputs.", "result": "Experimental results show that GPT-4 has a significant degree of self-preference bias, preferring outputs with lower perplexity compared to human evaluators.", "conclusion": "The study concludes that the self-preference bias in LLMs is influenced by the familiarity of outputs as indicated by perplexity metrics.", "key_contributions": ["Introduction of a quantitative metric for self-preference bias in LLMs.", "Identification of a significant self-preference bias in GPT-4.", "Establishment of a relationship between LLM evaluations and output perplexity."], "limitations": "Study focuses mainly on GPT-4; findings may not generalize to other LLMs without further research.", "keywords": ["large language models", "self-preference bias", "perplexity", "evaluation", "dialogue systems"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2411.17265", "pdf": "https://arxiv.org/pdf/2411.17265.pdf", "abs": "https://arxiv.org/abs/2411.17265", "title": "Systematic Reward Gap Optimization for Mitigating VLM Hallucinations", "authors": ["Lehan He", "Zeren Chen", "Zhelun Shi", "Tianyu Yu", "Jing Shao", "Lu Sheng"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "The success of Direct Preference Optimization (DPO) in mitigating\nhallucinations in Vision Language Models (VLMs) critically hinges on the true\nreward gaps within preference pairs. However, current methods, typically\nrelying on ranking or rewriting strategies, often struggle to optimize these\nreward gaps in a systematic way during data curation. A core difficulty lies in\nprecisely characterizing and strategically manipulating the overall reward gap\nconfiguration, that is, the deliberate design of how to shape these reward gaps\nwithin each preference pair across the data. To address this, we introduce\nTopic-level Preference Rewriting(TPR), a novel framework designed for the\nsystematic optimization of reward gap configuration. Through selectively\nreplacing semantic topics within VLM responses with model's own resampled\ncandidates for targeted rewriting, TPR can provide topic-level control over\nfine-grained semantic details. This precise control enables advanced data\ncuration strategies, such as progressively adjusting the difficulty of rejected\nresponses, thereby sculpting an effective reward gap configuration that guides\nthe model to overcome challenging hallucinations. Comprehensive experiments\ndemonstrate TPR achieves state-of-the-art performance on multiple hallucination\nbenchmarks, outperforming previous methods by an average of 20%. Notably, it\nsignificantly reduces hallucinations by up to 93% on ObjectHal-Bench, and also\nexhibits superior data efficiency towards robust and cost-effective VLM\nalignment.", "AI": {"tldr": "This paper introduces Topic-level Preference Rewriting (TPR), a framework for optimizing reward gaps in Vision Language Models to reduce hallucinations, achieving significant performance improvements.", "motivation": "To improve the effectiveness of Direct Preference Optimization in reducing hallucinations in Vision Language Models by systematically optimizing reward gaps within preference pairs.", "method": "The TPR framework focuses on selectively replacing semantic topics within VLM responses with the model's resampled candidates for precise control over reward gap configuration.", "result": "TPR significantly outperforms previous methods by an average of 20% on hallucination benchmarks, achieving up to a 93% reduction in hallucinations on ObjectHal-Bench.", "conclusion": "TPR demonstrates a robust approach to data curation that effectively addresses hallucinations in VLMs, ensuring better model alignment and efficiency.", "key_contributions": ["Introduction of Topic-level Preference Rewriting (TPR) framework", "Achieves state-of-the-art performance on hallucination benchmarks", "Reduces hallucinations by up to 93% on ObjectHal-Bench"], "limitations": "", "keywords": ["Vision Language Models", "Direct Preference Optimization", "hallucinations", "data curation", "machine learning"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2412.10823", "pdf": "https://arxiv.org/pdf/2412.10823.pdf", "abs": "https://arxiv.org/abs/2412.10823", "title": "FinGPT: Enhancing Sentiment-Based Stock Movement Prediction with Dissemination-Aware and Context-Enriched LLMs", "authors": ["Yixuan Liang", "Yuncong Liu", "Neng Wang", "Hongyang Yang", "Boyu Zhang", "Christina Dan Wang"], "categories": ["cs.CL", "cs.LG", "q-fin.CP", "q-fin.TR"], "comment": "1st Workshop on Preparing Good Data for Generative AI: Challenges and\n  Approaches@ AAAI 2025, ai4finance.org", "summary": "Financial sentiment analysis is crucial for understanding the influence of\nnews on stock prices. Recently, large language models (LLMs) have been widely\nadopted for this purpose due to their advanced text analysis capabilities.\nHowever, these models often only consider the news content itself, ignoring its\ndissemination, which hampers accurate prediction of short-term stock movements.\nAdditionally, current methods often lack sufficient contextual data and\nexplicit instructions in their prompts, limiting LLMs' ability to interpret\nnews. In this paper, we propose a data-driven approach that enhances\nLLM-powered sentiment-based stock movement predictions by incorporating news\ndissemination breadth, contextual data, and explicit instructions. We cluster\nrecent company-related news to assess its reach and influence, enriching\nprompts with more specific data and precise instructions. This data is used to\nconstruct an instruction tuning dataset to fine-tune an LLM for predicting\nshort-term stock price movements. Our experimental results show that our\napproach improves prediction accuracy by 8\\% compared to existing methods.", "AI": {"tldr": "This paper proposes an enhanced approach for financial sentiment analysis using large language models to predict short-term stock price movements by incorporating news dissemination, contextual data, and explicit instructions.", "motivation": "To improve the accuracy of short-term stock price predictions based on financial news sentiment by addressing limitations in current LLM applications.", "method": "The proposed method integrates news dissemination breadth, contextual data, and explicit instructions into the LLM prompts and constructs a fine-tuning dataset based on clustered company-related news.", "result": "The approach led to an 8% improvement in prediction accuracy compared to existing methods.", "conclusion": "Incorporating additional contextual information and structured prompts enhances LLMs' performance in predicting stock movement based on sentiment analysis.", "key_contributions": ["Introduced a data-driven approach for sentiment-based stock predictions using LLMs.", "Incorporated news dissemination breadth and contextual data for improved accuracy.", "Developed an instruction tuning dataset to enhance LLM performance."], "limitations": "", "keywords": ["Sentiment analysis", "Stock prediction", "Large language models", "Financial news", "Instruction tuning"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2412.12832", "pdf": "https://arxiv.org/pdf/2412.12832.pdf", "abs": "https://arxiv.org/abs/2412.12832", "title": "DSGram: Dynamic Weighting Sub-Metrics for Grammatical Error Correction in the Era of Large Language Models", "authors": ["Jinxiang Xie", "Yilin Li", "Xunjian Yin", "Xiaojun Wan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating the performance of Grammatical Error Correction (GEC) models has\nbecome increasingly challenging, as large language model (LLM)-based GEC\nsystems often produce corrections that diverge from provided gold references.\nThis discrepancy undermines the reliability of traditional reference-based\nevaluation metrics. In this study, we propose a novel evaluation framework for\nGEC models, DSGram, integrating Semantic Coherence, Edit Level, and Fluency,\nand utilizing a dynamic weighting mechanism. Our framework employs the Analytic\nHierarchy Process (AHP) in conjunction with large language models to ascertain\nthe relative importance of various evaluation criteria. Additionally, we\ndevelop a dataset incorporating human annotations and LLM-simulated sentences\nto validate our algorithms and fine-tune more cost-effective models.\nExperimental results indicate that our proposed approach enhances the\neffectiveness of GEC model evaluations.", "AI": {"tldr": "This paper introduces a new evaluation framework for Grammatical Error Correction models, addressing the limitations of traditional metrics by incorporating semantic coherence, edit level, and fluency.", "motivation": "The increasing divergence of LLM-based GEC corrections from gold references complicates reliable performance evaluation.", "method": "The proposed framework, DSGram, uses a dynamic weighting mechanism and the Analytic Hierarchy Process (AHP) to assess the relative importance of different evaluation criteria, alongside a dataset with human and LLM-simulated annotations for validation.", "result": "Experimental results show that the DSGram framework significantly improves the evaluation effectiveness of GEC models compared to traditional methods.", "conclusion": "The integration of advanced evaluation criteria and LLMs enhances the assessment of GEC models, paving the way for more reliable performance metrics.", "key_contributions": ["Introduction of DSGram framework for GEC evaluation.", "Dynamic weighting mechanism for evaluation criteria using AHP.", "Development of a dataset with human and LLM annotations for validation."], "limitations": "The paper may focus heavily on novel methods without extensive comparative analysis with existing models.", "keywords": ["Grammatical Error Correction", "Evaluation Framework", "Large Language Models", "Analytic Hierarchy Process", "Dataset with Annotations"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.15035", "pdf": "https://arxiv.org/pdf/2412.15035.pdf", "abs": "https://arxiv.org/abs/2412.15035", "title": "LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Inconsistencies", "authors": ["Felix Friedrich", "Simone Tedeschi", "Patrick Schramowski", "Manuel Brack", "Roberto Navigli", "Huu Nguyen", "Bo Li", "Kristian Kersting"], "categories": ["cs.CL"], "comment": null, "summary": "Building safe Large Language Models (LLMs) across multiple languages is\nessential in ensuring both safe access and linguistic diversity. To this end,\nwe conduct a large-scale, comprehensive safety evaluation of the current LLM\nlandscape. For this purpose, we introduce M-ALERT, a multilingual benchmark\nthat evaluates the safety of LLMs in five languages: English, French, German,\nItalian, and Spanish. M-ALERT includes 15k high-quality prompts per language,\ntotaling 75k, with category-wise annotations. Our extensive experiments on 39\nstate-of-the-art LLMs highlight the importance of language-specific safety\nanalysis, revealing that models often exhibit significant inconsistencies in\nsafety across languages and categories. For instance, Llama3.2 shows high\nunsafety in category crime_tax for Italian but remains safe in other languages.\nSimilar inconsistencies can be observed across all models. In contrast, certain\ncategories, such as substance_cannabis and crime_propaganda, consistently\ntrigger unsafe responses across models and languages. These findings underscore\nthe need for robust multilingual safety practices in LLMs to ensure responsible\nusage across diverse communities.", "AI": {"tldr": "This paper presents M-ALERT, a multilingual benchmark for evaluating the safety of large language models (LLMs) in five languages, highlighting significant inconsistencies in safety across languages and categories.", "motivation": "To ensure safe access and linguistic diversity in LLMs, a comprehensive safety evaluation is necessary.", "method": "A large-scale assessment using the M-ALERT benchmark, which includes 75k prompts across five languages with category-wise annotations.", "result": "Extensive experiments on 39 state-of-the-art LLMs revealed inconsistency in safety across languages and categories, with some categories consistently triggering unsafe responses.", "conclusion": "There is a critical need for robust multilingual safety practices for LLMs to ensure responsible usage in diverse linguistic communities.", "key_contributions": ["Introduction of M-ALERT benchmark for multilingual safety evaluation", "Identification of significant safety inconsistencies across languages in LLMs", "Highlighting categories that consistently trigger unsafe responses across models"], "limitations": "", "keywords": ["Large Language Models", "safety evaluation", "multilingual benchmark", "M-ALERT", "language-specific analysis"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2412.18431", "pdf": "https://arxiv.org/pdf/2412.18431.pdf", "abs": "https://arxiv.org/abs/2412.18431", "title": "GeAR: Graph-enhanced Agent for Retrieval-augmented Generation", "authors": ["Zhili Shen", "Chenxin Diao", "Pavlos Vougiouklis", "Pascual Merita", "Shriram Piramanayagam", "Enting Chen", "Damien Graux", "Andre Melo", "Ruofei Lai", "Zeren Jiang", "Zhongyang Li", "YE QI", "Yang Ren", "Dandan Tu", "Jeff Z. Pan"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "ACL 2025 Findings", "summary": "Retrieval-augmented Generation (RAG) relies on effective retrieval\ncapabilities, yet traditional sparse and dense retrievers inherently struggle\nwith multi-hop retrieval scenarios. In this paper, we introduce GeAR, a system\nthat advances RAG performance through two key innovations: (i) an efficient\ngraph expansion mechanism that augments any conventional base retriever, such\nas BM25, and (ii) an agent framework that incorporates the resulting\ngraph-based retrieval into a multi-step retrieval framework. Our evaluation\ndemonstrates GeAR's superior retrieval capabilities across three multi-hop\nquestion answering datasets. Notably, our system achieves state-of-the-art\nresults with improvements exceeding 10% on the challenging MuSiQue dataset,\nwhile consuming fewer tokens and requiring fewer iterations than existing\nmulti-step retrieval systems. The project page is available at\nhttps://gear-rag.github.io.", "AI": {"tldr": "GeAR improves Retrieval-augmented Generation performance in multi-hop scenarios with a graph expansion mechanism and an agent framework.", "motivation": "Traditional retrievers face challenges in multi-hop retrieval, which is crucial for effective question answering.", "method": "GeAR combines an efficient graph expansion mechanism with a multi-step retrieval agent framework to enhance conventional retrievers like BM25.", "result": "GeAR outperforms existing systems by over 10% on the MuSiQue dataset and shows superior capabilities across three datasets.", "conclusion": "GeAR shows promise in advancing retrieval capabilities in multi-hop question answering with reduced token consumption and fewer iterations.", "key_contributions": ["Graph expansion mechanism for traditional retrievers", "Agent framework for multi-step retrieval", "State-of-the-art performance on multi-hop datasets"], "limitations": "", "keywords": ["Retrieval-augmented Generation", "multi-hop retrieval", "graph-based retrieval"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2412.21065", "pdf": "https://arxiv.org/pdf/2412.21065.pdf", "abs": "https://arxiv.org/abs/2412.21065", "title": "Efficient Multi-Task Inferencing with a Shared Backbone and Lightweight Task-Specific Adapters for Automatic Scoring", "authors": ["Ehsan Latif", "Xiaoming Zhai"], "categories": ["cs.CL"], "comment": "Accepted by AAAI-iRAISE Workshop", "summary": "The integration of Artificial Intelligence (AI) in education requires\nscalable and efficient frameworks that balance performance, adaptability, and\ncost. This paper addresses these needs by proposing a shared backbone model\narchitecture enhanced with lightweight LoRA adapters for task-specific\nfine-tuning, targeting the automated scoring of student responses across 27\nmutually exclusive tasks. By achieving competitive performance (average QWK of\n0.848 compared to 0.888 for fully fine-tuned models) while reducing GPU memory\nconsumption by 60% and inference latency by 40%, the framework demonstrates\nsignificant efficiency gains. This approach aligns with the workshop's focus on\nimproving language models for educational tasks, creating responsible\ninnovations for cost-sensitive deployment, and supporting educators by\nstreamlining assessment workflows. The findings underscore the potential of\nscalable AI to enhance learning outcomes while maintaining fairness and\ntransparency in automated scoring systems.", "AI": {"tldr": "This paper proposes a shared backbone model with lightweight LoRA adapters for efficient automated scoring of student responses across various tasks, achieving competitive performance while reducing resource consumption.", "motivation": "To develop scalable and efficient AI frameworks for education that balance performance, adaptability, and cost.", "method": "The paper introduces a shared backbone model architecture enhanced with LoRA adapters for task-specific fine-tuning, aimed at automating the scoring of student responses across 27 tasks.", "result": "The proposed model achieves competitive performance with an average QWK of 0.848, while reducing GPU memory consumption by 60% and inference latency by 40%.", "conclusion": "This framework shows significant efficiency gains in the automated scoring of student responses, supporting cost-sensitive deployment in educational settings.", "key_contributions": ["Introduction of a shared backbone model architecture for educational AI tasks", "Use of lightweight LoRA adapters for fine-tuning", "Demonstration of efficiency gains in resource consumption and performance"], "limitations": "", "keywords": ["Artificial Intelligence", "Education", "Automated Scoring", "Efficiency", "Model Architecture"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2501.15225", "pdf": "https://arxiv.org/pdf/2501.15225.pdf", "abs": "https://arxiv.org/abs/2501.15225", "title": "SEAL: Scaling to Emphasize Attention for Long-Context Retrieval", "authors": ["Changhun Lee", "Minsang Seok", "Jun-gyu Jin", "Younghyun Cho", "Eunhyeok Park"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ACL 2025 Main", "summary": "While many advanced LLMs are designed to handle long sequence data, we can\nstill observe notable quality degradation even within the sequence limit. In\nthis work, we introduce a novel approach called Scaling to Emphasize Attention\nfor Long-context retrieval (SEAL), which enhances the retrieval performance of\nlarge language models (LLMs) over long contexts. We observe that specific\nattention heads are closely tied to long-context retrieval, showing positive or\nnegative correlation with retrieval scores, and adjusting the strength of these\nheads boosts the quality of LLMs in long context by a large margin. Built on\nthis insight, we propose a learning-based mechanism that leverages generated\ndata to emphasize these heads. By applying SEAL, we achieve significant\nimprovements in long-context retrieval performance across various tasks and\nmodels. Additionally, when combined with existing training-free context\nextension techniques, SEAL extends the contextual limits of LLMs while\nmaintaining highly reliable outputs.", "AI": {"tldr": "This paper introduces SEAL, a novel approach that enhances long-context retrieval performance of large language models (LLMs) by adjusting specific attention heads.", "motivation": "To improve the quality of long-sequence data processing in large language models (LLMs), as observed quality degradation occurs even within the sequence limits.", "method": "SEAL uses a learning-based mechanism to emphasize specific attention heads correlated with retrieval scores, thereby improving long-context retrieval performance through generated data.", "result": "Applying SEAL led to significant improvements in long-context retrieval across various tasks and models, and effectively extends contextual limits while maintaining output reliability.", "conclusion": "SEAL enhances LLMs' ability to manage long contexts by optimizing attention head performance and can be combined with training-free context extension methods.", "key_contributions": ["Introduction of SEAL to emphasize attention in long-context retrieval", "Demonstrated significant performance improvements across multiple tasks", "Ability to extend LLM context limits while ensuring reliability"], "limitations": "", "keywords": ["Large Language Models", "Attention Mechanism", "Long-context Retrieval"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.02072", "pdf": "https://arxiv.org/pdf/2502.02072.pdf", "abs": "https://arxiv.org/abs/2502.02072", "title": "ASCenD-BDS: Adaptable, Stochastic and Context-aware framework for Detection of Bias, Discrimination and Stereotyping", "authors": ["Rajiv Bahl", "Venkatesan N", "Parimal Aglawe", "Aastha Sarasapalli", "Bhavya Kancharla", "Chaitanya kolukuluri", "Harish Mohite", "Japneet Hora", "Kiran Kakollu", "Rahul Dhiman", "Shubham Kapale", "Sri Bhagya Kathula", "Vamsikrishna Motru", "Yogeshwar Reddy"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "17 pages, 6 Figures and this manuscript will be submitted to Q1,Q2\n  Journals", "summary": "The rapid evolution of Large Language Models (LLMs) has transformed natural\nlanguage processing but raises critical concerns about biases inherent in their\ndeployment and use across diverse linguistic and sociocultural contexts. This\npaper presents a framework named ASCenD BDS (Adaptable, Stochastic and\nContext-aware framework for Detection of Bias, Discrimination and\nStereotyping). The framework presents approach to detecting bias,\ndiscrimination, stereotyping across various categories such as gender, caste,\nage, disability, socioeconomic status, linguistic variations, etc., using an\napproach which is Adaptive, Stochastic and Context-Aware. The existing\nframeworks rely heavily on usage of datasets to generate scenarios for\ndetection of Bias, Discrimination and Stereotyping. Examples include datasets\nsuch as Civil Comments, Wino Gender, WinoBias, BOLD, CrowS Pairs and BBQ.\nHowever, such an approach provides point solutions. As a result, these datasets\nprovide a finite number of scenarios for assessment. The current framework\novercomes this limitation by having features which enable Adaptability,\nStochasticity, Context Awareness. Context awareness can be customized for any\nnation or culture or sub-culture (for example an organization's unique\nculture). In this paper, context awareness in the Indian context has been\nestablished. Content has been leveraged from Indian Census 2011 to have a\ncommonality of categorization. A framework has been developed using Category,\nSub-Category, STEM, X-Factor, Synonym to enable the features for Adaptability,\nStochasticity and Context awareness. The framework has been described in detail\nin Section 3. Overall 800 plus STEMs, 10 Categories, 31 unique SubCategories\nwere developed by a team of consultants at Saint Fox Consultancy Private Ltd.\nThe concept has been tested out in SFCLabs as part of product development.", "AI": {"tldr": "This paper introduces the ASCenD BDS framework for detecting biases in Large Language Models across various sociocultural contexts, focusing on adaptability and context-awareness.", "motivation": "To address the critical concerns regarding biases in Large Language Models due to their deployment in diverse linguistic and sociocultural environments.", "method": "The ASCenD BDS framework incorporates adaptability, stochasticity, and context-awareness to detect bias across categories like gender, caste, and age, by utilizing a comprehensive categorization based on the Indian Census 2011.", "result": "The framework has been developed with over 800 STEMs and various categories and sub-categories, enabling effective bias detection in a contextually aware manner.", "conclusion": "The ASCenD BDS framework provides a robust approach to bias detection that transcends existing limitations of traditional datasets and can be customized to specific cultural contexts.", "key_contributions": ["Introduction of a comprehensive framework for bias detection in LLMs.", "Ability to customize detection across various sociocultural contexts.", "Development of over 800 STEMs and multiple unique categories for enhanced adaptability."], "limitations": "", "keywords": ["Bias Detection", "Large Language Models", "Context-Aware Framework", "Sociocultural Bias", "Adaptability"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2502.08301", "pdf": "https://arxiv.org/pdf/2502.08301.pdf", "abs": "https://arxiv.org/abs/2502.08301", "title": "Compromising Honesty and Harmlessness in Language Models via Deception Attacks", "authors": ["LaurÃ¨ne Vaugrante", "Francesca Carlon", "Maluna Menke", "Thilo Hagendorff"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Recent research on large language models (LLMs) has demonstrated their\nability to understand and employ deceptive behavior, even without explicit\nprompting. However, such behavior has only been observed in rare, specialized\ncases and has not been shown to pose a serious risk to users. Additionally,\nresearch on AI alignment has made significant advancements in training models\nto refuse generating misleading or toxic content. As a result, LLMs generally\nbecame honest and harmless. In this study, we introduce \"deception attacks\"\nthat undermine both of these traits, revealing a vulnerability that, if\nexploited, could have serious real-world consequences. We introduce fine-tuning\nmethods that cause models to selectively deceive users on targeted topics while\nremaining accurate on others. Through a series of experiments, we show that\nsuch targeted deception is effective even in high-stakes domains or\nideologically charged subjects. In addition, we find that deceptive fine-tuning\noften compromises other safety properties: deceptive models are more likely to\nproduce toxic content, including hate speech and stereotypes. Finally, we\nassess whether models can deceive consistently in multi-turn dialogues,\nyielding mixed results. Given that millions of users interact with LLM-based\nchatbots, voice assistants, agents, and other interfaces where trustworthiness\ncannot be ensured, securing these models against deception attacks is critical.", "AI": {"tldr": "This study introduces 'deception attacks' that exploit vulnerabilities in large language models (LLMs), revealing risks associated with targeted deception and compromised safety properties in user interactions.", "motivation": "To explore the vulnerabilities of large language models to 'deception attacks' and to assess the implications of these attacks on trustworthiness in user interactions.", "method": "The study utilizes fine-tuning methods to induce selective deception in LLMs for targeted topics while keeping them accurate on others, followed by experiments to assess the effectiveness and safety compromises of these deceptive models.", "result": "Targeted deception is effective even in high-stakes contexts, and deceptive models are more prone to produce toxic content, indicating a compromise in safety properties.", "conclusion": "Given the widespread use of LLMs in user interfaces, ensuring these models are secure against deception attacks is crucial for maintaining trust.", "key_contributions": ["Introduction of the concept of deception attacks on LLMs.", "Demonstration of the effectiveness of fine-tuning models for selective deception.", "Identification of increased risks for toxic content generation from deceptive models."], "limitations": "The study may not cover all possible contexts or uses where deception could occur, and the mixed results in multi-turn dialogue scenarios suggest further investigation is needed.", "keywords": ["deception", "large language models", "AI alignment", "toxic content", "fine-tuning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.08788", "pdf": "https://arxiv.org/pdf/2502.08788.pdf", "abs": "https://arxiv.org/abs/2502.08788", "title": "Stop Overvaluing Multi-Agent Debate -- We Must Rethink Evaluation and Embrace Model Heterogeneity", "authors": ["Hangfan Zhang", "Zhiyao Cui", "Jianhao Chen", "Xinrun Wang", "Qiaosheng Zhang", "Zhen Wang", "Dinghao Wu", "Shuyue Hu"], "categories": ["cs.CL", "cs.LG"], "comment": "This position paper takes a critical view of the status quo of MAD\n  research, and outline multiple potential directions to improve MAD", "summary": "Multi-agent debate (MAD) has gained significant attention as a promising line\nof research to improve the factual accuracy and reasoning capabilities of large\nlanguage models (LLMs). Despite its conceptual appeal, current MAD research\nsuffers from critical limitations in evaluation practices, including limited\nbenchmark coverage, weak baseline comparisons, and inconsistent setups. This\npaper presents a systematic evaluation of 5 representative MAD methods across 9\nbenchmarks using 4 foundational models. Surprisingly, our findings reveal that\nMAD often fail to outperform simple single-agent baselines such as\nChain-of-Thought and Self-Consistency, even when consuming significantly more\ninference-time computation. To advance MAD research, we further explore the\nrole of model heterogeneity and find it as a universal antidote to consistently\nimprove current MAD frameworks. Based on our findings, we argue that the field\nmust stop overvaluing MAD in its current form; for true advancement, we must\ncritically rethink evaluation paradigms and actively embrace model\nheterogeneity as a core design principle.", "AI": {"tldr": "The paper critically evaluates multi-agent debate (MAD) methods, revealing their inefficacy against simpler baselines and advocating for model heterogeneity in evaluations.", "motivation": "To address critical limitations in evaluation practices of multi-agent debate (MAD) methods and their poor performance against simpler models.", "method": "Systematic evaluation of 5 MAD methods using 9 benchmarks with 4 foundational models to assess performance metrics and comparisons.", "result": "Findings show that MAD methods often do not outperform simpler single-agent baselines and require more inference-time computation; model heterogeneity improves MAD outcomes.", "conclusion": "The paper argues that advancement in MAD research requires reevaluation of current practices and incorporation of model heterogeneity as a fundamental design principle.", "key_contributions": ["Critique of the current state of MAD research", "Empirical evaluation of MAD methods versus single-agent baselines", "Proposal of model heterogeneity as a design principle"], "limitations": "The paper primarily critiques existing methods without proposing specific implementations for improvement.", "keywords": ["multi-agent debate", "large language models", "evaluation practices", "model heterogeneity", "reasoning capabilities"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2502.13347", "pdf": "https://arxiv.org/pdf/2502.13347.pdf", "abs": "https://arxiv.org/abs/2502.13347", "title": "Craw4LLM: Efficient Web Crawling for LLM Pretraining", "authors": ["Shi Yu", "Zhiyuan Liu", "Chenyan Xiong"], "categories": ["cs.CL"], "comment": null, "summary": "Web crawl is a main source of large language models' (LLMs) pretraining data,\nbut the majority of crawled web pages are discarded in pretraining due to low\ndata quality. This paper presents Craw4LLM, an efficient web crawling method\nthat explores the web graph based on the preference of LLM pretraining.\nSpecifically, it leverages the influence of a webpage in LLM pretraining as the\npriority score of the web crawler's scheduler, replacing the standard graph\nconnectivity based priority. Our experiments on a web graph containing 900\nmillion webpages from a commercial search engine's index demonstrate the\nefficiency of Craw4LLM in obtaining high-quality pretraining data. With just\n21% URLs crawled, LLMs pretrained on Craw4LLM data reach the same downstream\nperformances of previous crawls, significantly reducing the crawling waste and\nalleviating the burdens on websites. Our code is publicly available at\nhttps://github.com/cxcscmu/Craw4LLM.", "AI": {"tldr": "Craw4LLM is a web crawling method that improves the quality of pretraining data for large language models by prioritizing web pages based on their potential influence in LLM pretraining.", "motivation": "The paper addresses the issue of low data quality in web-crawled data for LLM pretraining, where many pages are discarded, leading to inefficiencies.", "method": "Craw4LLM employs a novel scheduler that prioritizes web pages using a score based on their influence in LLM pretraining, contrasting with traditional methods that rely on graph connectivity.", "result": "Experiments show that by crawling only 21% of selected URLs, Craw4LLM's resulting LLMs perform comparably to those pretrained on more extensive crawls, thus minimizing wasted crawling efforts.", "conclusion": "Craw4LLM effectively reduces data wastage during web crawling while achieving similar performance levels for LLMs, emphasizing its potential for optimizing the pretraining process.", "key_contributions": ["Introduction of a preference-based web crawler for LLM pretraining.", "Demonstrated efficiency with empirical results on a large-scale web graph.", "Public availability of the Craw4LLM code for further research."], "limitations": "The study does not explore the long-term impact of only utilizing a subset of webpages on LLM generalization.", "keywords": ["web crawling", "large language models", "data quality", "pretraining", "Craw4LLM"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2502.14744", "pdf": "https://arxiv.org/pdf/2502.14744.pdf", "abs": "https://arxiv.org/abs/2502.14744", "title": "HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States", "authors": ["Yilei Jiang", "Xinyan Gao", "Tianshuo Peng", "Yingshui Tan", "Xiaoyong Zhu", "Bo Zheng", "Xiangyu Yue"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 (Main)", "summary": "The integration of additional modalities increases the susceptibility of\nlarge vision-language models (LVLMs) to safety risks, such as jailbreak\nattacks, compared to their language-only counterparts. While existing research\nprimarily focuses on post-hoc alignment techniques, the underlying safety\nmechanisms within LVLMs remain largely unexplored. In this work , we\ninvestigate whether LVLMs inherently encode safety-relevant signals within\ntheir internal activations during inference. Our findings reveal that LVLMs\nexhibit distinct activation patterns when processing unsafe prompts, which can\nbe leveraged to detect and mitigate adversarial inputs without requiring\nextensive fine-tuning. Building on this insight, we introduce HiddenDetect, a\nnovel tuning-free framework that harnesses internal model activations to\nenhance safety. Experimental results show that {HiddenDetect} surpasses\nstate-of-the-art methods in detecting jailbreak attacks against LVLMs. By\nutilizing intrinsic safety-aware patterns, our method provides an efficient and\nscalable solution for strengthening LVLM robustness against multimodal threats.\nOur code will be released publicly at\nhttps://github.com/leigest519/HiddenDetect.", "AI": {"tldr": "This paper introduces HiddenDetect, a tuning-free framework that enhances the safety of large vision-language models (LVLMs) against jailbreak attacks by leveraging internal activation patterns.", "motivation": "To address the unexplored safety mechanisms within LVLMs and increase their robustness against multimodal threats.", "method": "Investigates the internal activations of LVLMs to identify distinct activation patterns corresponding to unsafe prompts, leading to the development of HiddenDetect.", "result": "HiddenDetect demonstrates superior performance over state-of-the-art methods in detecting jailbreak attacks in LVLMs.", "conclusion": "The findings suggest that intrinsic safety-aware patterns in LVLMs can be efficiently utilized to bolster model safety without extensive fine-tuning.", "key_contributions": ["Introduction of HiddenDetect framework for LVLM safety enhancement", "Detection of jailbreak attacks through internal activations", "Non-reliance on extensive fine-tuning for safety improvements."], "limitations": "", "keywords": ["vision-language models", "safety mechanisms", "jailbreak attacks", "model activations", "HiddenDetect"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2502.15543", "pdf": "https://arxiv.org/pdf/2502.15543.pdf", "abs": "https://arxiv.org/abs/2502.15543", "title": "ParamMute: Suppressing Knowledge-Critical FFNs for Faithful Retrieval-Augmented Generation", "authors": ["Pengcheng Huang", "Zhenghao Liu", "Yukun Yan", "Haiyan Zhao", "Xiaoyuan Yi", "Hao Chen", "Zhiyuan Liu", "Maosong Sun", "Tong Xiao", "Ge Yu", "Chenyan Xiong"], "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 7 figures, 7 tables", "summary": "Large language models (LLMs) integrated with retrieval-augmented generation\n(RAG) have improved factuality by grounding outputs in external evidence.\nHowever, they remain susceptible to unfaithful generation, where outputs\ncontradict retrieved context despite its relevance and accuracy. Existing\napproaches aiming to improve faithfulness primarily focus on enhancing the\nutilization of external context, but often overlook the persistent influence of\ninternal parametric knowledge during generation. In this work, we investigate\nthe internal mechanisms behind unfaithful generation and identify a subset of\nmid-to-deep feed-forward networks (FFNs) that are disproportionately activated\nin such cases. Building on this insight, we propose Parametric Knowledge Muting\nthrough FFN Suppression (ParamMute), a framework that improves contextual\nfaithfulness by suppressing the activation of unfaithfulness-associated FFNs\nand calibrating the model toward retrieved knowledge. To evaluate our approach,\nwe introduce CoFaithfulQA, a benchmark specifically designed to evaluate\nfaithfulness in scenarios where internal knowledge conflicts with accurate\nexternal evidence. Experimental results show that ParamMute significantly\nenhances faithfulness across both CoFaithfulQA and the established ConFiQA\nbenchmark, achieving substantial reductions in reliance on parametric memory.\nThese findings underscore the importance of mitigating internal knowledge\ndominance and provide a new direction for improving LLM trustworthiness in RAG.\nAll codes are available at https://github.com/OpenBMB/ParamMute.", "AI": {"tldr": "This paper introduces Parametric Knowledge Muting through FFN Suppression (ParamMute), a new framework to enhance the faithfulness of large language models (LLMs) during retrieval-augmented generation (RAG) by suppressing activation of problematic feed-forward networks.", "motivation": "To address the issue of unfaithful generation in LLMs, which can output contradictory information even when accurate external evidence is provided.", "method": "The paper proposes a framework called ParamMute, which suppresses the activation of certain feed-forward networks associated with unfaithfulness, aiming to improve the model's use of external context.", "result": "ParamMute significantly improves the faithfulness of LLMs as measured by the newly introduced CoFaithfulQA benchmark, showing a marked reduction in reliance on internal parametric memory.", "conclusion": "By mitigating the dominance of internal knowledge, the proposed framework provides a novel approach to enhance the trustworthiness of LLMs in RAG applications.", "key_contributions": ["Introduction of a novel framework (ParamMute) for improved faithfulness in LLMs", "Development of the CoFaithfulQA benchmark for evaluating faithfulness", "Identification of key feed-forward networks associated with unfaithful generation"], "limitations": "", "keywords": ["Large Language Models", "Retrieval-Augmented Generation", "Faithfulness", "Parametric Knowledge Muting", "Benchmarking"], "importance_score": 9, "read_time_minutes": 22}}
{"id": "2502.18802", "pdf": "https://arxiv.org/pdf/2502.18802.pdf", "abs": "https://arxiv.org/abs/2502.18802", "title": "Language Models Grow Less Humanlike beyond Phase Transition", "authors": ["Tatsuya Aoyama", "Ethan Wilcox"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "LMs' alignment with human reading behavior (i.e. psychometric predictive\npower; PPP) is known to improve during pretraining up to a tipping point,\nbeyond which it either plateaus or degrades. Various factors, such as word\nfrequency, recency bias in attention, and context size, have been theorized to\naffect PPP, yet there is no current account that explains why such a tipping\npoint exists, and how it interacts with LMs' pretraining dynamics more\ngenerally. We hypothesize that the underlying factor is a pretraining phase\ntransition, characterized by the rapid emergence of specialized attention\nheads. We conduct a series of correlational and causal experiments to show that\nsuch a phase transition is responsible for the tipping point in PPP. We then\nshow that, rather than producing attention patterns that contribute to the\ndegradation in PPP, phase transitions alter the subsequent learning dynamics of\nthe model, such that further training keeps damaging PPP.", "AI": {"tldr": "This paper explores the tipping point in language models' alignment with human reading behavior, hypothesizing a pretraining phase transition as the underlying factor affecting this alignment.", "motivation": "Understanding the dynamics of language model pretraining and its impact on alignment with human reading behavior can improve future model training strategies.", "method": "The authors conducted a series of correlational and causal experiments to investigate the effects of specialized attention heads and phase transitions during pretraining on psychometric predictive power (PPP).", "result": "The results indicate that a pretraining phase transition is responsible for the observed tipping point in PPP, which negatively affects further training dynamics.", "conclusion": "The findings suggest that attention patterns change due to phase transitions, leading to detrimental learning dynamics that degrade model performance beyond a certain pretraining threshold.", "key_contributions": ["Proposes a new hypothesis regarding the tipping point in language model training related to phase transitions.", "Demonstrates through experiments how attention head specialization can impact model alignment with reading behavior.", "Provides insights into the interaction between pretraining dynamics and psychometric predictive power."], "limitations": "", "keywords": ["language models", "psychometric predictive power", "pretraining dynamics", "attention heads", "phase transitions"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.20330", "pdf": "https://arxiv.org/pdf/2502.20330.pdf", "abs": "https://arxiv.org/abs/2502.20330", "title": "RAPID: Long-Context Inference with Retrieval-Augmented Speculative Decoding", "authors": ["Guanzheng Chen", "Qilong Feng", "Jinjie Ni", "Xin Li", "Michael Qizhe Shieh"], "categories": ["cs.CL"], "comment": "ICML 2025 Spotlight", "summary": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference presents significant efficiency challenges. While Speculative\nDecoding (SD) traditionally accelerates inference using smaller draft models,\nits effectiveness diminishes substantially in long-context scenarios due to\nmemory-bound KV cache operations. We introduce Retrieval-Augmented Speculative\nDecoding (RAPID), which leverages RAG for both accelerating and enhancing\ngeneration quality in long-context inference. RAPID introduces the RAG\ndrafter-a draft LLM operating on shortened retrieval contexts-to speculate on\nthe generation of long-context target LLMs. Our approach enables a new paradigm\nwhere same-scale or even larger LLMs can serve as RAG drafters while\nmaintaining computational efficiency. To fully leverage the potentially\nsuperior capabilities from stronger RAG drafters, we develop an inference-time\nknowledge transfer that enriches the target distribution by RAG. Extensive\nexperiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID\neffectively integrates the strengths of both RAG and long-context LLMs,\nachieving significant performance improvements (e.g., from 39.33 to 42.83 on\nInfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context\ninference. Our analyses also reveal the robustness of RAPID across various\ncontext lengths and retrieval quality.", "AI": {"tldr": "RAPID introduces Retrieval-Augmented Speculative Decoding to enhance long-context LLM performance and efficiency, achieving significant speedups and quality improvements.", "motivation": "To address the efficiency challenges in long-context LLM inference while improving generation quality.", "method": "The paper presents Retrieval-Augmented Speculative Decoding (RAPID), a model combining retrieval-augmented generation with speculative decoding, utilizing draft LLMs for shortened retrieval contexts to inform long-context LLMs.", "result": "RAPID demonstrates significant performance improvements, achieving a score increase from 39.33 to 42.83 on the InfiniteBench benchmark for LLaMA-3.1-8B, while also providing over 2x speedups in long-context inference.", "conclusion": "RAPID effectively integrates the capabilities of RAG and long-context LLMs, showcasing robustness across various context lengths and retrieval quality.", "key_contributions": ["Introduction of RAPID for long-context inference efficiency", "Demonstrated significant speedups and performance improvements", "Development of inference-time knowledge transfer enhancing target distribution."], "limitations": "", "keywords": ["long-context LLMs", "speculative decoding", "retrieval-augmented generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.03705", "pdf": "https://arxiv.org/pdf/2503.03705.pdf", "abs": "https://arxiv.org/abs/2503.03705", "title": "Enhancing LLM Knowledge Learning through Generalization", "authors": ["Mingkang Zhu", "Xi Chen", "Zhongdao Wang", "Bei Yu", "Hengshuang Zhao", "Jiaya Jia"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "As Large language models (LLMs) are increasingly deployed in diverse\napplications, faithfully integrating evolving factual knowledge into these\nmodels remains a critical challenge. Continued pre-training on paraphrased data\nhas shown empirical promise for enhancing knowledge acquisition. However, this\napproach is often costly and unreliable, as it relies on external models or\nmanual effort for rewriting, and may inadvertently alter the factual content.\nIn this work, we hypothesize and empirically show that an LLM's ability to\ncontinually predict the same factual knowledge tokens given diverse paraphrased\ncontexts is positively correlated with its capacity to extract that knowledge\nvia question-answering. Based on this view and aiming to improve generalization\nto diverse paraphrased contexts, we introduce two strategies to enhance LLMs'\nability to predict the same knowledge tokens given varied contexts, thereby\nenhancing knowledge acquisition. First, we propose formatting-based data\naugmentation, which diversifies documents conveying the same knowledge by\naltering document formats rather than their content, thereby preserving factual\nintegrity. Second, we adopt sharpness-aware minimization as the optimizer to\nbetter improve generalization. Extensive experiments demonstrate our methods'\neffectiveness in both continued pre-training and instruction tuning, and\nfurther gains can be achieved by combining with paraphrased data.", "AI": {"tldr": "This paper explores strategies to improve large language models' (LLMs) factual knowledge acquisition by using formatting-based data augmentation and sharpness-aware optimization to enhance generalization across diverse paraphrased contexts.", "motivation": "Integrating evolving factual knowledge into large language models while maintaining factual integrity is a significant challenge as they continue to be deployed in various applications.", "method": "The authors propose two strategies: 1) Formatting-based data augmentation, which changes the appearance of documents while keeping their content unchanged; 2) Using sharpness-aware minimization for better optimization during training.", "result": "Experiments show that the proposed methods significantly enhance LLMs' predictive accuracy for factual knowledge across varied contexts, improving both pre-training and instruction tuning outcomes.", "conclusion": "Using formatting-based data augmentation and an optimized training approach increases LLMsâ ability to accurately predict factual knowledge, thereby aiding in their application in real-world scenarios.", "key_contributions": ["Introduction of formatting-based data augmentation to maintain factual integrity", "Adoption of sharpness-aware minimization for enhanced optimization", "Demonstrated effectiveness through extensive experimental results"], "limitations": "", "keywords": ["Large Language Models", "Knowledge Acquisition", "Data Augmentation", "Machine Learning", "Optimization"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2503.10150", "pdf": "https://arxiv.org/pdf/2503.10150.pdf", "abs": "https://arxiv.org/abs/2503.10150", "title": "HiRAG: Retrieval-Augmented Generation with Hierarchical Knowledge", "authors": ["Haoyu Huang", "Yongfeng Huang", "Junjie Yang", "Zhenyu Pan", "Yongqiang Chen", "Kaili Ma", "Hongzhi Chen", "James Cheng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Graph-based Retrieval-Augmented Generation (RAG) methods have significantly\nenhanced the performance of large language models (LLMs) in domain-specific\ntasks. However, existing RAG methods do not adequately utilize the naturally\ninherent hierarchical knowledge in human cognition, which limits the\ncapabilities of RAG systems. In this paper, we introduce a new RAG approach,\ncalled HiRAG, which utilizes hierarchical knowledge to enhance the semantic\nunderstanding and structure capturing capabilities of RAG systems in the\nindexing and retrieval processes. Our extensive experiments demonstrate that\nHiRAG achieves significant performance improvements over the state-of-the-art\nbaseline methods.", "AI": {"tldr": "HiRAG is a new RAG approach that enhances semantic understanding using hierarchical knowledge.", "motivation": "To enhance the performance of RAG systems by incorporating hierarchical knowledge, addressing limitations of existing methods in utilizing human cognitive structures.", "method": "Introduction of a new RAG framework, HiRAG, that utilizes hierarchical knowledge for improved indexing and retrieval processes.", "result": "Extensive experiments show that HiRAG significantly outperforms state-of-the-art baseline methods in performance.", "conclusion": "HiRAG provides a more effective approach for RAG by leveraging the hierarchical structure of knowledge, leading to better outcomes in domain-specific tasks.", "key_contributions": ["Introduction of HiRAG for enhanced RAG performance", "Utilization of hierarchical knowledge in retrieval processes", "Demonstrated performance improvements over existing methods"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Hierarchical Knowledge", "Large Language Models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.09895", "pdf": "https://arxiv.org/pdf/2504.09895.pdf", "abs": "https://arxiv.org/abs/2504.09895", "title": "Learning from Reference Answers: Versatile Language Model Alignment without Binary Human Preference Data", "authors": ["Shuai Zhao", "Linchao Zhu", "Yi Yang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "work in progress", "summary": "Large language models~(LLMs) are expected to be helpful, harmless, and\nhonest. In alignment scenarios such as safety, confidence, and general\npreference alignment, binary preference data collection and reward modeling are\nresource-intensive but essential for transferring human preference. In this\nwork, we explore using the similarity between sampled generations and\nhigh-quality reference answers as an alternative reward function choice for LLM\nalignment. Similarity reward circumvents binary preference data collection and\nreward modeling when unary high-quality reference answers are available. We\nintroduce \\textit{RefAlign}, a versatile REINFORCE-style alignment algorithm\nthat does not rely on reference or reward models. RefAlign utilizes similarity\nmetrics, such as BERTScore between sampled generations and reference answers as\nsurrogate rewards. Beyond general human preference optimization, RefAlign can\nbe readily extended to diverse scenarios, such as safety and confidence\nalignment, by incorporating the similarity reward with task-related objectives.\nIn various scenarios, RefAlign demonstrates comparable performance to previous\nalignment methods without binary preference data and reward models.", "AI": {"tldr": "RefAlign is an alignment algorithm for large language models that uses similarity metrics as a reward function instead of binary preference data.", "motivation": "To find an efficient way to align large language models without relying on resource-intensive binary preference data collection and reward modeling.", "method": "The paper presents RefAlign, a REINFORCE-style algorithm that utilizes similarity metrics like BERTScore between generated outputs and reference answers as a means of generating surrogate rewards for alignment.", "result": "RefAlign shows comparable performance to existing alignment methods, achieving effective model alignment without the need for binary preference data or reward models.", "conclusion": "The approach offers a viable alternative for aligning large language models across various scenarios while simplifying the data collection process.", "key_contributions": ["Introduction of RefAlign algorithm for LLM alignment", "Utilization of similarity metrics for reward functions", "Demonstrated performance comparable to existing methods without binary preference data"], "limitations": "Work in progress; further validation and performance comparisons needed in broader contexts.", "keywords": ["large language models", "alignment", "reward modeling", "BERTScore", "similarity metrics"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.07248", "pdf": "https://arxiv.org/pdf/2506.07248.pdf", "abs": "https://arxiv.org/abs/2506.07248", "title": "Improving the Efficiency of Long Document Classification using Sentence Ranking Approach", "authors": ["Prathamesh Kokate", "Mitali Sarnaik", "Manavi Khopade", "Raviraj Joshi"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Long document classification poses challenges due to the computational\nlimitations of transformer-based models, particularly BERT, which are\nconstrained by fixed input lengths and quadratic attention complexity.\nMoreover, using the full document for classification is often redundant, as\nonly a subset of sentences typically carries the necessary information. To\naddress this, we propose a TF-IDF-based sentence ranking method that improves\nefficiency by selecting the most informative content. Our approach explores\nfixed-count and percentage-based sentence selection, along with an enhanced\nscoring strategy combining normalized TF-IDF scores and sentence length.\nEvaluated on the MahaNews LDC dataset of long Marathi news articles, the method\nconsistently outperforms baselines such as first, last, and random sentence\nselection. With MahaBERT-v2, we achieve near-identical classification accuracy\nwith just a 0.33 percent drop compared to the full-context baseline, while\nreducing input size by over 50 percent and inference latency by 43 percent.\nThis demonstrates that significant context reduction is possible without\nsacrificing performance, making the method practical for real-world long\ndocument classification tasks.", "AI": {"tldr": "A TF-IDF-based sentence ranking method improves long document classification efficiency by selecting the most informative sentences, achieving near-full performance with reduced input size and latency.", "motivation": "Address challenges in long document classification due to computational limitations of transformer models, particularly with regard to fixed input lengths and redundant full document usage.", "method": "TF-IDF-based sentence ranking to enhance efficiency by selecting informative content through fixed-count and percentage-based sentence selection and a new scoring strategy combining TF-IDF scores and sentence length.", "result": "Using MahaBERT-v2, the method achieves classification accuracy with only a 0.33% drop compared to the full-context baseline while reducing input size by over 50% and inference latency by 43%.", "conclusion": "Significant context reduction for long document classification is feasible without compromising performance, making the method practical for real-world applications.", "key_contributions": ["Proposed a novel TF-IDF-based sentence ranking method for document classification", "Demonstrated significant reduction in input size and latency without accuracy loss", "Evaluated on the MahaNews LDC dataset with promising results"], "limitations": "", "keywords": ["document classification", "TF-IDF", "sentence ranking", "long documents", "MahaBERT-v2"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2506.07438", "pdf": "https://arxiv.org/pdf/2506.07438.pdf", "abs": "https://arxiv.org/abs/2506.07438", "title": "LGAI-EMBEDDING-Preview Technical Report", "authors": ["Jooyoung Choi", "Hyun Kim", "Hansol Jang", "Changwook Jun", "Kyunghoon Bae", "Hyewon Choi", "Stanley Jungkyu Choi", "Honglak Lee", "Chulmin Yun"], "categories": ["cs.CL"], "comment": "10 pages", "summary": "This report presents a unified instruction-based framework for learning\ngeneralized text embeddings optimized for both information retrieval (IR) and\nnon-IR tasks. Built upon a decoder-only large language model (Mistral-7B), our\napproach combines in-context learning, soft supervision, and adaptive\nhard-negative mining to generate context-aware embeddings without task-specific\nfine-tuning. Structured instructions and few-shot examples are used to guide\nthe model across diverse tasks, enabling strong performance on classification,\nsemantic similarity, clustering, and reranking benchmarks. To improve semantic\ndiscrimination, we employ a soft labeling framework where continuous relevance\nscores, distilled from a high-performance dense retriever and reranker, serve\nas fine-grained supervision signals. In addition, we introduce adaptive\nmargin-based hard-negative mining, which filters out semantically ambiguous\nnegatives based on their similarity to positive examples, thereby enhancing\ntraining stability and retrieval robustness. Our model is evaluated on the\nnewly introduced MTEB (English, v2) benchmark, covering 41 tasks across seven\ncategories. Results show that our method achieves strong generalization and\nranks among the top-performing models by Borda score, outperforming several\nlarger or fully fine-tuned baselines. These findings highlight the\neffectiveness of combining in-context prompting, soft supervision, and adaptive\nsampling for scalable, high-quality embedding generation.", "AI": {"tldr": "A unified framework for learning generalized text embeddings optimized for both information retrieval and non-IR tasks, utilizing a decoder-only language model and combining various learning strategies for robust performance across multiple benchmarks.", "motivation": "To create efficient and effective generalized text embeddings that perform well across diverse tasks without the need for task-specific fine-tuning.", "method": "The approach utilizes Mistral-7B, incorporating in-context learning, soft supervision, and adaptive hard-negative mining to generate context-aware embeddings.", "result": "The model achieved strong generalization on the MTEB benchmark, ranking among the top-performing models and outperforming various larger or fully fine-tuned baselines.", "conclusion": "The study demonstrates that a combination of in-context prompting, soft supervision, and adaptive sampling leads to scalable, high-quality embedding generation.", "key_contributions": ["Unified instruction-based framework for text embeddings", "Use of soft labeling with continuous relevance scores", "Adaptive margin-based hard-negative mining for improved training stability"], "limitations": "", "keywords": ["text embeddings", "information retrieval", "large language model", "soft supervision", "adaptive mining"], "importance_score": 8, "read_time_minutes": 10}}
