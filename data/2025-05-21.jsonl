{"id": "2505.13612", "pdf": "https://arxiv.org/pdf/2505.13612.pdf", "abs": "https://arxiv.org/abs/2505.13612", "title": "Sight, Sound and Smell in Immersive Experiences of Urban History: Virtual Vauxhall Gardens Case Study", "authors": ["Tim Pearce", "David Souto", "Douglas Barrett", "Benjamin Lok", "Mateusz Bocian", "Artur Soczawa-Stronczyk", "Giasemi Vavoula", "Paul Long", "Avinash Bhangaonkar", "Stephanie Bowry", "Michaela Butter", "David Coke", "Kate Loveman", "Rosemary Sweet", "Lars Tharp", "Jeremy Webster", "Hongji Yang", "Robin Green", "Andrew Hugill"], "categories": ["cs.HC", "cs.CY"], "comment": "24 pages, 10 figures", "summary": "We explore the integration of multisensory elements in virtual reality\nreconstructions of historical spaces through a case study of the Virtual\nVauxhall Gardens project. While visual and auditory components have become\nstandard in digital heritage experiences, the addition of olfactory stimuli\nremains underexplored, despite its powerful connection to memory and emotional\nengagement. This research investigates how multisensory experiences involving\nolfaction can be effectively integrated into VR reconstructions of historical\nspaces to enhance presence and engagement with cultural heritage. In the\ncontext of a VR reconstruction of London's eighteenth-century Vauxhall Pleasure\nGardens, we developed a networked portable olfactory display capable of\nsynchronizing specific scents with visual and auditory elements at pivotal\nmoments in the virtual experience. Our evaluation methodology assesses both\ntechnical implementation and user experience, measuring presence, and usability\nmetrics across diverse participant groups. Our results show that integrating\nsynchronized olfactory stimuli into the VR experience can enhance user\nengagement and be perceived positively, contributing to a unique and immersive\nencounter with historical settings. While presence questionnaires indicated a\nstrong sense of auditory presence and control, with other sensory factors rated\nmoderately, user experience of attractiveness was exceptionally high;\nqualitative feedback suggested heightened sensory awareness and engagement\ninfluenced by the inclusion and anticipation of smell. Our results suggest that\nevaluating multisensory VR heritage experiences requires a nuanced approach, as\nstandard usability metrics may be ill-suited and 'realism' might be less\ncritical than creating an evocative, historically informed, and emotionally\nresonant experience......"}
{"id": "2505.13648", "pdf": "https://arxiv.org/pdf/2505.13648.pdf", "abs": "https://arxiv.org/abs/2505.13648", "title": "Conceptual Modeling: Topics, Themes, and Technology Trends", "authors": ["V. C. Storey", "R. Lukyanenko", "A. Castellanos"], "categories": ["cs.HC", "cs.DB"], "comment": null, "summary": "Conceptual modeling is an important part of information systems development\nand use that involves identifying and representing relevant aspects of reality.\nAlthough the past decades have experienced continuous digitalization of\nservices and products that impact business and society, conceptual modeling\nefforts are still required to support new technologies as they emerge. This\npaper surveys research on conceptual modeling over the past five decades and\nshows how its topics and trends continue to evolve to accommodate emerging\ntechnologies, while remaining grounded in basic constructs. We survey over\n5,300 papers that address conceptual modeling topics from the 1970s to the\npresent, which are collected from 35 multidisciplinary journals and\nconferences, and use them as the basis from which to analyze the progression of\nconceptual modeling. The important role that conceptual modeling should play in\nour evolving digital world is discussed, and future research directions\nproposed."}
{"id": "2505.13688", "pdf": "https://arxiv.org/pdf/2505.13688.pdf", "abs": "https://arxiv.org/abs/2505.13688", "title": "Gaze-Enhanced Multimodal Turn-Taking Prediction in Triadic Conversations", "authors": ["Seongsil Heo", "Calvin Murdock", "Michael Proulx", "Christi Miller"], "categories": ["cs.HC"], "comment": null, "summary": "Turn-taking prediction is crucial for seamless interactions. This study\nintroduces a novel, lightweight framework for accurate turn-taking prediction\nin triadic conversations without relying on computationally intensive methods.\nUnlike prior approaches that either disregard gaze or treat it as a passive\nsignal, our model integrates gaze with speaker localization, structuring it\nwithin a spatial constraint to transform it into a reliable predictive cue.\nLeveraging egocentric behavioral cues, our experiments demonstrate that\nincorporating gaze data from a single-user significantly improves prediction\nperformance, while gaze data from multiple-users further enhances it by\ncapturing richer conversational dynamics. This study presents a lightweight and\nprivacy-conscious approach to support adaptive, directional sound control,\nenhancing speech intelligibility in noisy environments, particularly for\nhearing assistance in smart glasses."}
{"id": "2505.13953", "pdf": "https://arxiv.org/pdf/2505.13953.pdf", "abs": "https://arxiv.org/abs/2505.13953", "title": "Human Authenticity and Flourishing in an AI-Driven World: Edmund's Journey and the Call for Mindfulness", "authors": ["Sebastian Zepf", "Mark Colley"], "categories": ["cs.HC"], "comment": null, "summary": "Humans have always dreamed of possessing superpowers, and the rapid\ndevelopment of AI-based features promises to bring these dreams (closer) to\nreality. However, these advancements come with significant risks. This paper\nadvocates for challenging existing methods and approaches in design and\nevaluation for more responsible AI. We stimulate reflection through a\nfuturistic user journey illustrating the AI-driven life of Edmund in 2035.\nSubsequently, we discuss four AI-based superpowers: extended perception,\ncognitive offloading, externalized memory, and enhanced presence. We then\ndiscuss implications for HCI and AI, emphasizing the need for preserving\nintrinsic human superpowers, identifying meaningful use cases for AI, and\nevaluating AI's impact on human abilities. This paper advocates for responsible\nand reflective AI integration and proposes a pathway towards the idea of a\nHuman Flourishing Benchmark."}
{"id": "2505.13480", "pdf": "https://arxiv.org/pdf/2505.13480.pdf", "abs": "https://arxiv.org/abs/2505.13480", "title": "Evaluating Reasoning LLMs for Suicide Screening with the Columbia-Suicide Severity Rating Scale", "authors": ["Avinash Patil", "Siru Tao", "Amardeep Gedhu"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "8 Pages, 6 Figures, 1 Table", "summary": "Suicide prevention remains a critical public health challenge. While online\nplatforms such as Reddit's r/SuicideWatch have historically provided spaces for\nindividuals to express suicidal thoughts and seek community support, the advent\nof large language models (LLMs) introduces a new paradigm-where individuals may\nbegin disclosing ideation to AI systems instead of humans. This study evaluates\nthe capability of LLMs to perform automated suicide risk assessment using the\nColumbia-Suicide Severity Rating Scale (C-SSRS). We assess the zero-shot\nperformance of six models-including Claude, GPT, Mistral, and LLaMA-in\nclassifying posts across a 7-point severity scale (Levels 0-6). Results\nindicate that Claude and GPT closely align with human annotations, while\nMistral achieves the lowest ordinal prediction error. Most models exhibit\nordinal sensitivity, with misclassifications typically occurring between\nadjacent severity levels. We further analyze confusion patterns,\nmisclassification sources, and ethical considerations, underscoring the\nimportance of human oversight, transparency, and cautious deployment. Full code\nand supplementary materials are available at\nhttps://github.com/av9ash/llm_cssrs_code."}
{"id": "2505.14031", "pdf": "https://arxiv.org/pdf/2505.14031.pdf", "abs": "https://arxiv.org/abs/2505.14031", "title": "Reading.help: Supporting EFL Readers with Proactive and On-Demand Explanation of English Grammar and Semantics", "authors": ["Sunghyo Chung", "Hyeon Jeon", "Sungbok Shin", "Md Naimul Hoque"], "categories": ["cs.HC"], "comment": "Preprint", "summary": "A large portion of texts in the world is written in English, but readers who\nsee English as a Foreign Language (EFL) often struggle to read texts written in\nEnglish accurately and swiftly. In many countries, EFL readers seek help from\nprofessional teachers and mentors, which is limited and costly. In this paper,\nwe explore how an intelligent reading tool can assist EFL readers. To support\nour research agenda, we conducted a case study with EFL readers in South Korea.\nWe at first developed an LLM-based reading tool based on prior literature. We\nthen revised the tool based on the feedback from a study with 15 South Korean\nEFL readers. The final tool, named Reading.help, helps EFL readers comprehend\ncomplex sentences and paragraphs with on-demand and proactive explanations. We\nfinally evaluated the tool with 5 EFL readers and 2 EFL education\nprofessionals. Our findings suggest Reading.help could potentially help EFL\nreaders self-learn english when they do not have access to any external\nsupport."}
{"id": "2505.13483", "pdf": "https://arxiv.org/pdf/2505.13483.pdf", "abs": "https://arxiv.org/abs/2505.13483", "title": "EmoMeta: A Multimodal Dataset for Fine-grained Emotion Classification in Chinese Metaphors", "authors": ["Xingyuan Lu", "Yuxi Liu", "Dongyu Zhang", "Zhiyao Wu", "Jing Ren", "Feng Xia"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Metaphors play a pivotal role in expressing emotions, making them crucial for\nemotional intelligence. The advent of multimodal data and widespread\ncommunication has led to a proliferation of multimodal metaphors, amplifying\nthe complexity of emotion classification compared to single-mode scenarios.\nHowever, the scarcity of research on constructing multimodal metaphorical\nfine-grained emotion datasets hampers progress in this domain. Moreover,\nexisting studies predominantly focus on English, overlooking potential\nvariations in emotional nuances across languages. To address these gaps, we\nintroduce a multimodal dataset in Chinese comprising 5,000 text-image pairs of\nmetaphorical advertisements. Each entry is meticulously annotated for metaphor\noccurrence, domain relations and fine-grained emotion classification\nencompassing joy, love, trust, fear, sadness, disgust, anger, surprise,\nanticipation, and neutral. Our dataset is publicly accessible\n(https://github.com/DUTIR-YSQ/EmoMeta), facilitating further advancements in\nthis burgeoning field."}
{"id": "2505.14074", "pdf": "https://arxiv.org/pdf/2505.14074.pdf", "abs": "https://arxiv.org/abs/2505.14074", "title": "Recreating Neural Activity During Speech Production with Language and Speech Model Embeddings", "authors": ["Owais Mujtaba Khanday", "Pablo Rodroguez San Esteban", "Zubair Ahmad Lone", "Marc Ouellet", "Jose Andres Gonzalez Lopez"], "categories": ["cs.HC", "cs.SD", "eess.AS"], "comment": "Accepted for presentation at Interspeech2025", "summary": "Understanding how neural activity encodes speech and language production is a\nfundamental challenge in neuroscience and artificial intelligence. This study\ninvestigates whether embeddings from large-scale, self-supervised language and\nspeech models can effectively reconstruct neural activity recordings captured\nduring speech production. We leverage pre-trained embeddings from deep learning\nmodels trained on linguistic and acoustic data to represent high-level speech\nfeatures and map them onto neural signals. We analyze the extent to which these\nembeddings preserve the spatio-temporal dynamics of brain activity. We evaluate\nreconstructed neural signals against ground truth recordings using correlation\nmetrics and signal reconstruction quality assessments. The results indicate\nthat neural activity can be effectively reconstructed using embeddings from\nlarge language and speech models across all study participants, yielding\nPearson correlation coefficients ranging from 0.79 to 0.99."}
{"id": "2505.13487", "pdf": "https://arxiv.org/pdf/2505.13487.pdf", "abs": "https://arxiv.org/abs/2505.13487", "title": "Detecting Prefix Bias in LLM-based Reward Models", "authors": ["Ashwin Kumar", "Yuzi He", "Aram H. Markosyan", "Bobbie Chern", "Imanol Arrieta-Ibarra"], "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement Learning with Human Feedback (RLHF) has emerged as a key\nparadigm for task-specific fine-tuning of language models using human\npreference data. While numerous publicly available preference datasets provide\npairwise comparisons of responses, the potential for biases in the resulting\nreward models remains underexplored. In this work, we introduce novel methods\nto detect and evaluate prefix bias -- a systematic shift in model preferences\ntriggered by minor variations in query prefixes -- in LLM-based reward models\ntrained on such datasets. We leverage these metrics to reveal significant\nbiases in preference models across racial and gender dimensions. Our\ncomprehensive evaluation spans diverse open-source preference datasets and\nreward model architectures, demonstrating susceptibility to this kind of bias\nregardless of the underlying model architecture. Furthermore, we propose a data\naugmentation strategy to mitigate these biases, showing its effectiveness in\nreducing the impact of prefix bias. Our findings highlight the critical need\nfor bias-aware dataset design and evaluation in developing fair and reliable\nreward models, contributing to the broader discourse on fairness in AI."}
{"id": "2505.14078", "pdf": "https://arxiv.org/pdf/2505.14078.pdf", "abs": "https://arxiv.org/abs/2505.14078", "title": "The Virtual Reality Koinos Method: Analyzing Virtual Reality Collaboration from the perspective of communication models", "authors": ["Eloise Minder", "Sylvain Fleury", "Solène Neyret", "Jean-Rémy Chardonnet"], "categories": ["cs.HC"], "comment": "25 pages, 7 figures, in preprint for a journal (currently in review\n  process)", "summary": "Understanding which factors could influence co-presence in Virtual Reality\ncould help develop more qualitative social interactions, or social interactions\nthat generate similar sensations, emotions and feelings than the ones generated\nduring Face-to-Face interactions. Co-presence is studied since the beginning of\nVirtual Reality (VR); though, no consensus is identified on what factors could\ninfluence it, except the consensus on the definition of \"being there together\"\ninside the Virtual Environment. In this paper, we introduce the Koinos method\nto explain social interactions in VR through communication models, (i)\ntheoretically, and (ii) on two VR experiments that change the virtual partner\nsocial and physical representations. These analyses lead us to propose an\nequation to predict and help manage the sense of co-presence in VR."}
{"id": "2505.13488", "pdf": "https://arxiv.org/pdf/2505.13488.pdf", "abs": "https://arxiv.org/abs/2505.13488", "title": "Source framing triggers systematic evaluation bias in Large Language Models", "authors": ["Federico Germani", "Giovanni Spitale"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used not only to generate text\nbut also to evaluate it, raising urgent questions about whether their judgments\nare consistent, unbiased, and robust to framing effects. In this study, we\nsystematically examine inter- and intra-model agreement across four\nstate-of-the-art LLMs (OpenAI o3-mini, Deepseek Reasoner, xAI Grok 2, and\nMistral) tasked with evaluating 4,800 narrative statements on 24 different\ntopics of social, political, and public health relevance, for a total of\n192,000 assessments. We manipulate the disclosed source of each statement to\nassess how attribution to either another LLM or a human author of specified\nnationality affects evaluation outcomes. We find that, in the blind condition,\ndifferent LLMs display a remarkably high degree of inter- and intra-model\nagreement across topics. However, this alignment breaks down when source\nframing is introduced. Here we show that attributing statements to Chinese\nindividuals systematically lowers agreement scores across all models, and in\nparticular for Deepseek Reasoner. Our findings reveal that framing effects can\ndeeply affect text evaluation, with significant implications for the integrity,\nneutrality, and fairness of LLM-mediated information systems."}
{"id": "2505.14363", "pdf": "https://arxiv.org/pdf/2505.14363.pdf", "abs": "https://arxiv.org/abs/2505.14363", "title": "Human and Machine as Seen at the Co-Creation Age: A Co-Word Analysis in Human Machine Co-creation (2014-2024)", "authors": ["Mengyao Guo", "Jinda Han", "Ze Gao", "Yuan Zhuang", "Xingting Wu"], "categories": ["cs.HC"], "comment": "24 pages", "summary": "This paper explores the evolving landscape of human-machine co-creation,\nfocusing on its development in the context of the ACM Conference on Human\nFactors in Computing Systems (CHI) from 2014 to 2024. We employ co-word\nanalysis to identify emerging trends, central themes, and the intellectual\ntrajectory of this field. The study highlights the shift from viewing machines\nas mere tools to recognizing them as collaborative partners in creative\nprocesses. By understanding these dynamics, we aim to provide insights into the\nimplications of this paradigm shift for creativity, innovation, and societal\nimpact, ultimately fostering a more inclusive and effective approach to\nhuman-machine interaction in various domains."}
{"id": "2505.13491", "pdf": "https://arxiv.org/pdf/2505.13491.pdf", "abs": "https://arxiv.org/abs/2505.13491", "title": "ProdRev: A DNN framework for empowering customers using generative pre-trained transformers", "authors": ["Aakash Gupta", "Nataraj Das"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "2022 International Conference on Decision Aid Sciences and\n  Applications (DASA)", "summary": "Following the pandemic, customers, preference for using e-commerce has\naccelerated. Since much information is available in multiple reviews (sometimes\nrunning in thousands) for a single product, it can create decision paralysis\nfor the buyer. This scenario disempowers the consumer, who cannot be expected\nto go over so many reviews since its time consuming and can confuse them.\nVarious commercial tools are available, that use a scoring mechanism to arrive\nat an adjusted score. It can alert the user to potential review manipulations.\nThis paper proposes a framework that fine-tunes a generative pre-trained\ntransformer to understand these reviews better. Furthermore, using\n\"common-sense\" to make better decisions. These models have more than 13 billion\nparameters. To fine-tune the model for our requirement, we use the curie engine\nfrom generative pre-trained transformer (GPT3). By using generative models, we\nare introducing abstractive summarization. Instead of using a simple extractive\nmethod of summarizing the reviews. This brings out the true relationship\nbetween the reviews and not simply copy-paste. This introduces an element of\n\"common sense\" for the user and helps them to quickly make the right decisions.\nThe user is provided the pros and cons of the processed reviews. Thus the\nuser/customer can take their own decisions."}
{"id": "2505.14370", "pdf": "https://arxiv.org/pdf/2505.14370.pdf", "abs": "https://arxiv.org/abs/2505.14370", "title": "What Does Success Look Like? Catalyzing Meeting Intentionality with AI-Assisted Prospective Reflection", "authors": ["Ava Elizabeth Scott", "Lev Tankelevitch", "Payod Panda", "Rishi Vanukuru", "Xinyue Chen", "Sean Rintel"], "categories": ["cs.HC"], "comment": null, "summary": "Despite decades of HCI and Meeting Science research, complaints about\nineffective meetings are still pervasive. We argue that meeting technologies\nlack support for prospective reflection, that is, thinking about why a meeting\nis needed and what might happen. To explore this, we designed a Meeting Purpose\nAssistant (MPA) technology probe to coach users to articulate their meeting's\npurpose and challenges, and act accordingly. The MPA used Generative AI to\nsupport personalized and actionable prospective reflection across the diversity\nof meeting contexts. Using a participatory prompting methodology, 18 employees\nof a global technology company reflected with the MPA on upcoming meetings.\nObserved impacts were: clarifying meeting purposes, challenges, and success\nconditions; changing perspectives and flexibility; improving preparation and\ncommunication; and proposing changed plans. We also identify perceived social,\ntemporal, and technological barriers to using the MPA. We present system and\nworkflow design considerations for developing AI-assisted reflection support\nfor meetings."}
{"id": "2505.13492", "pdf": "https://arxiv.org/pdf/2505.13492.pdf", "abs": "https://arxiv.org/abs/2505.13492", "title": "LLM4CD: Leveraging Large Language Models for Open-World Knowledge Augmented Cognitive Diagnosis", "authors": ["Weiming Zhang", "Lingyue Fu", "Qingyao Li", "Kounianhua Du", "Jianghao Lin", "Jingwei Yu", "Wei Xia", "Weinan Zhang", "Ruiming Tang", "Yong Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Cognitive diagnosis (CD) plays a crucial role in intelligent education,\nevaluating students' comprehension of knowledge concepts based on their test\nhistories. However, current CD methods often model students, exercises, and\nknowledge concepts solely on their ID relationships, neglecting the abundant\nsemantic relationships present within educational data space. Furthermore,\ncontemporary intelligent tutoring systems (ITS) frequently involve the addition\nof new students and exercises, a situation that ID-based methods find\nchallenging to manage effectively. The advent of large language models (LLMs)\noffers the potential for overcoming this challenge with open-world knowledge.\nIn this paper, we propose LLM4CD, which Leverages Large Language Models for\nOpen-World Knowledge Augmented Cognitive Diagnosis. Our method utilizes the\nopen-world knowledge of LLMs to construct cognitively expressive textual\nrepresentations, which are then encoded to introduce rich semantic information\ninto the CD task. Additionally, we propose an innovative bi-level encoder\nframework that models students' test histories through two levels of encoders:\na macro-level cognitive text encoder and a micro-level knowledge state encoder.\nThis approach substitutes traditional ID embeddings with semantic\nrepresentations, enabling the model to accommodate new students and exercises\nwith open-world knowledge and address the cold-start problem. Extensive\nexperimental results demonstrate that our proposed method consistently\noutperforms previous CD models on multiple real-world datasets, validating the\neffectiveness of leveraging LLMs to introduce rich semantic information into\nthe CD task."}
{"id": "2505.14377", "pdf": "https://arxiv.org/pdf/2505.14377.pdf", "abs": "https://arxiv.org/abs/2505.14377", "title": "When Bias Backfires: The Modulatory Role of Counterfactual Explanations on the Adoption of Algorithmic Bias in XAI-Supported Human Decision-Making", "authors": ["Ulrike Kuhl", "Annika Bush"], "categories": ["cs.HC", "cs.AI"], "comment": "Accepted for XAI2025", "summary": "Although the integration of artificial intelligence (AI) into everyday tasks\nimproves efficiency and objectivity, it also risks transmitting bias to human\ndecision-making. In this study, we conducted a controlled experiment that\nsimulated hiring decisions to examine how biased AI recommendations - augmented\nwith or without counterfactual explanations - influence human judgment over\ntime. Participants, acting as hiring managers, completed 60 decision trials\ndivided into a baseline phase without AI, followed by a phase with biased (X)AI\nrecommendations (favoring either male or female candidates), and a final\npost-interaction phase without AI. Our results indicate that the participants\nfollowed the AI recommendations 70% of the time when the qualifications of the\ngiven candidates were comparable. Yet, only a fraction of participants detected\nthe gender bias (8 out of 294). Crucially, exposure to biased AI altered\nparticipants' inherent preferences: in the post-interaction phase,\nparticipants' independent decisions aligned with the bias when no\ncounterfactual explanations were provided before, but reversed the bias when\nexplanations were given. Reported trust did not differ significantly across\nconditions. Confidence varied throughout the study phases after exposure to\nmale-biased AI, indicating nuanced effects of AI bias on decision certainty.\nOur findings point to the importance of calibrating XAI to avoid unintended\nbehavioral shifts in order to safeguard equitable decision-making and prevent\nthe adoption of algorithmic bias."}
{"id": "2505.13498", "pdf": "https://arxiv.org/pdf/2505.13498.pdf", "abs": "https://arxiv.org/abs/2505.13498", "title": "IRLBench: A Multi-modal, Culturally Grounded, Parallel Irish-English Benchmark for Open-Ended LLM Reasoning Evaluation", "authors": ["Khanh-Tung Tran", "Barry O'Sullivan", "Hoang D. Nguyen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have demonstrated promising\nknowledge and reasoning abilities, yet their performance in multilingual and\nlow-resource settings remains underexplored. Existing benchmarks often exhibit\ncultural bias, restrict evaluation to text-only, rely on multiple-choice\nformats, and, more importantly, are limited for extremely low-resource\nlanguages. To address these gaps, we introduce IRLBench, presented in parallel\nEnglish and Irish, which is considered definitely endangered by UNESCO. Our\nbenchmark consists of 12 representative subjects developed from the 2024 Irish\nLeaving Certificate exams, enabling fine-grained analysis of model capabilities\nacross domains. By framing the task as long-form generation and leveraging the\nofficial marking scheme, it does not only support a comprehensive evaluation of\ncorrectness but also language fidelity. Our extensive experiments of leading\nclosed-source and open-source LLMs reveal a persistent performance gap between\nEnglish and Irish, in which models produce valid Irish responses less than 80\\%\nof the time, and answer correctly 55.8\\% of the time compared to 76.2\\% in\nEnglish for the best-performing model. We release IRLBench\n(https://huggingface.co/datasets/ReliableAI/IRLBench) and an accompanying\nevaluation codebase (https://github.com/ReML-AI/IRLBench) to enable future\nresearch on robust, culturally aware multilingual AI development."}
{"id": "2505.14379", "pdf": "https://arxiv.org/pdf/2505.14379.pdf", "abs": "https://arxiv.org/abs/2505.14379", "title": "Two Empirical Studies on Audiovisual Semiotics of Uncertainty", "authors": ["Sita Vriend", "David Hägele", "Daniel Weiskopf"], "categories": ["cs.HC"], "comment": "Accepted to Audio Mostly 2025 [AM '25]", "summary": "There exists limited theoretical guidance on integrating visualization and\nsonification. In this paper, we address this gap by investigating audiovisual\nsemiotics for uncertainty representation: joining uncertainty visualization and\nsonification to combine audiovisual channels for enhancing users' perception of\nuncertainty. We conducted two preregistered crowd-sourced user studies. First,\nwe assessed suitable audio/visual pairs. Then, we investigated audiovisual\nmappings of uncertainty. Here, we use probability as it is an easily\ncommunicated aspect of uncertainty. We analyzed the participants' preferences\nand reaction times in both user studies. Additionally, we explored the\nstrategies employed by participants through qualitative analysis. Our results\nreveal audiovisual mappings that lead to particularly strong preferences and\nlow reaction times. Furthermore, we found that preferred audio/visual pairs are\nnot necessarily suitable audiovisual mappings of uncertainty. For example,\nwhile pitch paired with brightness was preferred as a pair, it was not well\nsuited as a mapping for uncertainty. We recommend audiovisual mappings of\nuncertainty that lead to low reaction times and high preferences in both user\nstudies. This paper presents guidelines to anyone seeking to employ audiovisual\nrepresentations for uncertainty, contributing to enhancing the perception of\nuncertainty."}
{"id": "2505.13500", "pdf": "https://arxiv.org/pdf/2505.13500.pdf", "abs": "https://arxiv.org/abs/2505.13500", "title": "Noise Injection Systemically Degrades Large Language Model Safety Guardrails", "authors": ["Prithviraj Singh Shahani", "Matthias Scheutz"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages,3 figures", "summary": "Safety guardrails in large language models (LLMs) are a critical component in\npreventing harmful outputs. Yet, their resilience under perturbation remains\npoorly understood. In this paper, we investigate the robustness of safety\nfine-tuning in LLMs by systematically injecting Gaussian noise into model\nactivations. We show across multiple open-weight models that (1) Gaussian noise\nraises harmful-output rates (p < 0.001) by up to 27%, (2) that deeper safety\nfine-tuning affords no extra protection, and (3) that chain-of-thought\nreasoning remains largely intact. The findings reveal critical vulnerabilities\nin current safety alignment techniques and highlight the potential of\nreasoning-based and reinforcement learning approaches as promising direction\nfor developing more robust AI safety systems. These results have important\nimplications for real-world deployment of LLMs in safety-critical applications\nas these results imply that widely-deployed safety tuning methods can fail even\nwithout adversarial prompts."}
{"id": "2505.14452", "pdf": "https://arxiv.org/pdf/2505.14452.pdf", "abs": "https://arxiv.org/abs/2505.14452", "title": "How Managers Perceive AI-Assisted Conversational Training for Workplace Communication", "authors": ["Lance T Wilhelm", "Xiaohan Ding", "Kirk McInnis Knutsen", "Buse Carik", "Eugenia H Rho"], "categories": ["cs.HC", "cs.AI"], "comment": "accepted to CUI '25", "summary": "Effective workplace communication is essential for managerial success, yet\nmany managers lack access to tailored and sustained training. Although\nAI-assisted communication systems may offer scalable training solutions, little\nis known about how managers envision the role of AI in helping them improve\ntheir communication skills. To investigate this, we designed a conversational\nrole-play system, CommCoach, as a functional probe to understand how managers\nanticipate using AI to practice their communication skills. Through\nsemi-structured interviews, participants emphasized the value of adaptive,\nlow-risk simulations for practicing difficult workplace conversations. They\nalso highlighted opportunities, including human-AI teaming, transparent and\ncontext-aware feedback, and greater control over AI-generated personas.\nAI-assisted communication training should balance personalization, structured\nlearning objectives, and adaptability to different user styles and contexts.\nHowever, achieving this requires carefully navigating tensions between adaptive\nand consistent AI feedback, realism and potential bias, and the open-ended\nnature of AI conversations versus structured workplace discourse."}
{"id": "2505.13506", "pdf": "https://arxiv.org/pdf/2505.13506.pdf", "abs": "https://arxiv.org/abs/2505.13506", "title": "EcoSafeRAG: Efficient Security through Context Analysis in Retrieval-Augmented Generation", "authors": ["Ruobing Yao", "Yifei Zhang", "Shuang Song", "Neng Gao", "Chenyang Tu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) compensates for the static knowledge\nlimitations of Large Language Models (LLMs) by integrating external knowledge,\nproducing responses with enhanced factual correctness and query-specific\ncontextualization. However, it also introduces new attack surfaces such as\ncorpus poisoning at the same time. Most of the existing defense methods rely on\nthe internal knowledge of the model, which conflicts with the design concept of\nRAG. To bridge the gap, EcoSafeRAG uses sentence-level processing and\nbait-guided context diversity detection to identify malicious content by\nanalyzing the context diversity of candidate documents without relying on LLM\ninternal knowledge. Experiments show EcoSafeRAG delivers state-of-the-art\nsecurity with plug-and-play deployment, simultaneously improving clean-scenario\nRAG performance while maintaining practical operational costs (relatively\n1.2$\\times$ latency, 48\\%-80\\% token reduction versus Vanilla RAG)."}
{"id": "2505.13472", "pdf": "https://arxiv.org/pdf/2505.13472.pdf", "abs": "https://arxiv.org/abs/2505.13472", "title": "Proof Assistants for Teaching: a Survey", "authors": ["Frédéric Tran Minh", "Laure Gonnord", "Julien Narboux"], "categories": ["cs.LO", "cs.HC"], "comment": "In Proceedings ThEdu24, arXiv:2505.04677", "summary": "In parallel to the ever-growing usage of mechanized proofs in diverse areas\nof mathematics and computer science, proof assistants are used more and more\nfor education. This paper surveys previous work related to the use of proof\nassistants for (mostly undergraduate) teaching. This includes works where the\nauthors report on their experiments using proof assistants to teach logic,\nmathematics or computer science, as well as designs or adaptations of proof\nassistants for teaching. We provide an overview of both tutoring systems that\nhave been designed for teaching proof and proving, or general-purpose proof\nassistants that have been adapted for education, adding user interfaces and/or\ndedicated input or output languages."}
{"id": "2505.13508", "pdf": "https://arxiv.org/pdf/2505.13508.pdf", "abs": "https://arxiv.org/abs/2505.13508", "title": "Time-R1: Towards Comprehensive Temporal Reasoning in LLMs", "authors": ["Zijia Liu", "Peixuan Han", "Haofei Yu", "Haoru Li", "Jiaxuan You"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate impressive capabilities but lack\nrobust temporal intelligence, struggling to integrate reasoning about the past\nwith predictions and plausible generations of the future. Meanwhile, existing\nmethods typically target isolated temporal skills, such as question answering\nabout past events or basic forecasting, and exhibit poor generalization,\nparticularly when dealing with events beyond their knowledge cutoff or\nrequiring creative foresight. To address these limitations, we introduce\n\\textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter)\nLLM with comprehensive temporal abilities: understanding, prediction, and\ncreative generation. Our approach features a novel three-stage development\npath; the first two constitute a \\textit{reinforcement learning (RL)\ncurriculum} driven by a meticulously designed dynamic rule-based reward system.\nThis framework progressively builds (1) foundational temporal understanding and\nlogical event-time mappings from historical data, (2) future event prediction\nskills for events beyond its knowledge cutoff, and finally (3) enables\nremarkable generalization to creative future scenario generation without any\nfine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms\nmodels over 200 times larger, including the state-of-the-art 671B DeepSeek-R1,\non highly challenging future event prediction and creative scenario generation\nbenchmarks. This work provides strong evidence that thoughtfully engineered,\nprogressive RL fine-tuning allows smaller, efficient models to achieve superior\ntemporal performance, offering a practical and scalable path towards truly\ntime-aware AI. To foster further research, we also release \\textit{Time-Bench},\na large-scale multi-task temporal reasoning dataset derived from 10 years of\nnews data, and our series of \\textit{Time-R1} checkpoints."}
{"id": "2505.13565", "pdf": "https://arxiv.org/pdf/2505.13565.pdf", "abs": "https://arxiv.org/abs/2505.13565", "title": "Aligning Trustworthy AI with Democracy: A Dual Taxonomy of Opportunities and Risks", "authors": ["Oier Mentxaka", "Natalia Díaz-Rodríguez", "Mark Coeckelbergh", "Marcos López de Prado", "Emilia Gómez", "David Fernández Llorca", "Enrique Herrera-Viedma", "Francisco Herrera"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "26 pages, 5 figures", "summary": "Artificial Intelligence (AI) poses both significant risks and valuable\nopportunities for democratic governance. This paper introduces a dual taxonomy\nto evaluate AI's complex relationship with democracy: the AI Risks to Democracy\n(AIRD) taxonomy, which identifies how AI can undermine core democratic\nprinciples such as autonomy, fairness, and trust; and the AI's Positive\nContributions to Democracy (AIPD) taxonomy, which highlights AI's potential to\nenhance transparency, participation, efficiency, and evidence-based\npolicymaking.\n  Grounded in the European Union's approach to ethical AI governance, and\nparticularly the seven Trustworthy AI requirements proposed by the European\nCommission's High-Level Expert Group on AI, each identified risk is aligned\nwith mitigation strategies based on EU regulatory and normative frameworks. Our\nanalysis underscores the transversal importance of transparency and societal\nwell-being across all risk categories and offers a structured lens for aligning\nAI systems with democratic values.\n  By integrating democratic theory with practical governance tools, this paper\noffers a normative and actionable framework to guide research, regulation, and\ninstitutional design to support trustworthy, democratic AI. It provides\nscholars with a conceptual foundation to evaluate the democratic implications\nof AI, equips policymakers with structured criteria for ethical oversight, and\nhelps technologists align system design with democratic principles. In doing\nso, it bridges the gap between ethical aspirations and operational realities,\nlaying the groundwork for more inclusive, accountable, and resilient democratic\nsystems in the algorithmic age."}
{"id": "2505.13514", "pdf": "https://arxiv.org/pdf/2505.13514.pdf", "abs": "https://arxiv.org/abs/2505.13514", "title": "Induction Head Toxicity Mechanistically Explains Repetition Curse in Large Language Models", "authors": ["Shuxun Wang", "Qingyu Yin", "Chak Tou Leong", "Qiang Zhang", "Linyi Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Repetition curse is a phenomenon where Large Language Models (LLMs) generate\nrepetitive sequences of tokens or cyclic sequences. While the repetition curse\nhas been widely observed, its underlying mechanisms remain poorly understood.\nIn this work, we investigate the role of induction heads--a specific type of\nattention head known for their ability to perform in-context learning--in\ndriving this repetitive behavior. Specifically, we focus on the \"toxicity\" of\ninduction heads, which we define as their tendency to dominate the model's\noutput logits during repetition, effectively excluding other attention heads\nfrom contributing to the generation process. Our findings have important\nimplications for the design and training of LLMs. By identifying induction\nheads as a key driver of the repetition curse, we provide a mechanistic\nexplanation for this phenomenon and suggest potential avenues for mitigation.\nWe also propose a technique with attention head regularization that could be\nemployed to reduce the dominance of induction heads during generation, thereby\npromoting more diverse and coherent outputs."}
{"id": "2505.13773", "pdf": "https://arxiv.org/pdf/2505.13773.pdf", "abs": "https://arxiv.org/abs/2505.13773", "title": "Model Cards for AI Teammates: Comparing Human-AI Team Familiarization Methods for High-Stakes Environments", "authors": ["Ryan Bowers", "Richard Agbeyibor", "Jack Kolb", "Karen Feigh"], "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": "Submitted to IEEE RO-MAN 2025 (under review). 8 pages, 7 figures", "summary": "We compare three methods of familiarizing a human with an artificial\nintelligence (AI) teammate (\"agent\") prior to operation in a collaborative,\nfast-paced intelligence, surveillance, and reconnaissance (ISR) environment. In\na between-subjects user study (n=60), participants either read documentation\nabout the agent, trained alongside the agent prior to the mission, or were\ngiven no familiarization. Results showed that the most valuable information\nabout the agent included details of its decision-making algorithms and its\nrelative strengths and weaknesses compared to the human. This information\nallowed the familiarization groups to form sophisticated team strategies more\nquickly than the control group. Documentation-based familiarization led to the\nfastest adoption of these strategies, but also biased participants towards\nrisk-averse behavior that prevented high scores. Participants familiarized\nthrough direct interaction were able to infer much of the same information\nthrough observation, and were more willing to take risks and experiment with\ndifferent control modes, but reported weaker understanding of the agent's\ninternal processes. Significant differences were seen between individual\nparticipants' risk tolerance and methods of AI interaction, which should be\nconsidered when designing human-AI control interfaces. Based on our findings,\nwe recommend a human-AI team familiarization method that combines AI\ndocumentation, structured in-situ training, and exploratory interaction."}
{"id": "2505.13527", "pdf": "https://arxiv.org/pdf/2505.13527.pdf", "abs": "https://arxiv.org/abs/2505.13527", "title": "Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression", "authors": ["Jingyu Peng", "Maolin Wang", "Nan Wang", "Xiangyu Zhao", "Jiatong Li", "Kai Zhang", "Qi Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite substantial advancements in aligning large language models (LLMs)\nwith human values, current safety mechanisms remain susceptible to jailbreak\nattacks. We hypothesize that this vulnerability stems from distributional\ndiscrepancies between alignment-oriented prompts and malicious prompts. To\ninvestigate this, we introduce LogiBreak, a novel and universal black-box\njailbreak method that leverages logical expression translation to circumvent\nLLM safety systems. By converting harmful natural language prompts into formal\nlogical expressions, LogiBreak exploits the distributional gap between\nalignment data and logic-based inputs, preserving the underlying semantic\nintent and readability while evading safety constraints. We evaluate LogiBreak\non a multilingual jailbreak dataset spanning three languages, demonstrating its\neffectiveness across various evaluation settings and linguistic contexts."}
{"id": "2505.13931", "pdf": "https://arxiv.org/pdf/2505.13931.pdf", "abs": "https://arxiv.org/abs/2505.13931", "title": "Sketch Interface for Teleoperation of Mobile Manipulator to Enable Intuitive and Intended Operation: A Proof of Concept", "authors": ["Yuka Iwanaga", "Masayoshi Tsuchinaga", "Kosei Tanada", "Yuji Nakamura", "Takemitsu Mori", "Takashi Yamamoto"], "categories": ["cs.RO", "cs.HC"], "comment": "This paper has been accepted to the the 20th edition of the IEEE/ACM\n  International Conference on Human-Robot Interaction (HRI'25), which will be\n  held in Melbourne, Australia on March 4-6, 2025", "summary": "Recent advancements in robotics have underscored the need for effective\ncollaboration between humans and robots. Traditional interfaces often struggle\nto balance robot autonomy with human oversight, limiting their practical\napplication in complex tasks like mobile manipulation. This study aims to\ndevelop an intuitive interface that enables a mobile manipulator to\nautonomously interpret user-provided sketches, enhancing user experience while\nminimizing burden. We implemented a web-based application utilizing machine\nlearning algorithms to process sketches, making the interface accessible on\nmobile devices for use anytime, anywhere, by anyone. In the first validation,\nwe examined natural sketches drawn by users for 27 selected manipulation and\nnavigation tasks, gaining insights into trends related to sketch instructions.\nThe second validation involved comparative experiments with five grasping\ntasks, showing that the sketch interface reduces workload and enhances\nintuitiveness compared to conventional axis control interfaces. These findings\nsuggest that the proposed sketch interface improves the efficiency of mobile\nmanipulators and opens new avenues for integrating intuitive human-robot\ncollaboration in various applications."}
{"id": "2505.13554", "pdf": "https://arxiv.org/pdf/2505.13554.pdf", "abs": "https://arxiv.org/abs/2505.13554", "title": "Combining the Best of Both Worlds: A Method for Hybrid NMT and LLM Translation", "authors": ["Zhanglin Wu", "Daimeng Wei", "Xiaoyu Chen", "Hengchao Shang", "Jiaxin Guo", "Zongyao Li", "Yuanchang Luo", "Jinlong Yang", "Zhiqiang Rao", "Hao Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 2 figures, 9 tables, ACL 2025", "summary": "Large language model (LLM) shows promising performances in a variety of\ndownstream tasks, such as machine translation (MT). However, using LLMs for\ntranslation suffers from high computational costs and significant latency.\nBased on our evaluation, in most cases, translations using LLMs are comparable\nto that generated by neural machine translation (NMT) systems. Only in\nparticular scenarios, LLM and NMT models show respective advantages. As a\nresult, integrating NMT and LLM for translation and using LLM only when\nnecessary seems to be a sound solution. A scheduling policy that optimizes\ntranslation result while ensuring fast speed and as little LLM usage as\npossible is thereby required. We compare several scheduling policies and\npropose a novel and straightforward decider that leverages source sentence\nfeatures. We conduct extensive experiments on multilingual test sets and the\nresult shows that we can achieve optimal translation performance with minimal\nLLM usage, demonstrating effectiveness of our decider."}
{"id": "2505.14080", "pdf": "https://arxiv.org/pdf/2505.14080.pdf", "abs": "https://arxiv.org/abs/2505.14080", "title": "Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory", "authors": ["Franziska Sofia Hafner", "Ana Valdivia", "Luc Rocher"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Language models encode and subsequently perpetuate harmful gendered\nstereotypes. Research has succeeded in mitigating some of these harms, e.g. by\ndissociating non-gendered terms such as occupations from gendered terms such as\n'woman' and 'man'. This approach, however, remains superficial given that\nassociations are only one form of prejudice through which gendered harms arise.\nCritical scholarship on gender, such as gender performativity theory,\nemphasizes how harms often arise from the construction of gender itself, such\nas conflating gender with biological sex. In language models, these issues\ncould lead to the erasure of transgender and gender diverse identities and\ncause harms in downstream applications, from misgendering users to\nmisdiagnosing patients based on wrong assumptions about their anatomy.\n  For FAccT research on gendered harms to go beyond superficial linguistic\nassociations, we advocate for a broader definition of 'gender bias' in language\nmodels. We operationalize insights on the construction of gender through\nlanguage from gender studies literature and then empirically test how 16\nlanguage models of different architectures, training datasets, and model sizes\nencode gender. We find that language models tend to encode gender as a binary\ncategory tied to biological sex, and that gendered terms that do not neatly\nfall into one of these binary categories are erased and pathologized. Finally,\nwe show that larger models, which achieve better results on performance\nbenchmarks, learn stronger associations between gender and sex, further\nreinforcing a narrow understanding of gender. Our findings lead us to call for\na re-evaluation of how gendered harms in language models are defined and\naddressed."}
{"id": "2505.13559", "pdf": "https://arxiv.org/pdf/2505.13559.pdf", "abs": "https://arxiv.org/abs/2505.13559", "title": "CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the Limits of Large Language Models", "authors": ["Sathya Krishnan Suresh", "Tanmay Surana", "Lim Zhi Hao", "Eng Siong Chng"], "categories": ["cs.CL", "cs.LG"], "comment": "17 pages, 5 figures and 11 tables", "summary": "Code-switching (CS) poses a significant challenge for Large Language Models\n(LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce\nCS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue\nto English summarization. CS-Sum is the first benchmark for CS dialogue\nsummarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and\nMalay-English (EN-MS), with 900-1300 human-annotated dialogues per language\npair. Evaluating ten LLMs, including open and closed-source models, we analyze\nperformance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA\non synthetic data) approaches. Our findings show that though the scores on\nautomated metrics are high, LLMs make subtle mistakes that alter the complete\nmeaning of the dialogue. To this end, we introduce 3 most common type of errors\nthat LLMs make when handling CS input. Error rates vary across CS pairs and\nLLMs, with some LLMs showing more frequent errors on certain language pairs,\nunderscoring the need for specialized training on code-switched data."}
{"id": "2505.14126", "pdf": "https://arxiv.org/pdf/2505.14126.pdf", "abs": "https://arxiv.org/abs/2505.14126", "title": "MAS-KCL: Knowledge component graph structure learning with large language model-based agentic workflow", "authors": ["Yuan-Hao Jiang", "Kezong Tang", "Zi-Wei Chen", "Yuang Wei", "Tian-Yi Liu", "Jiayi Wu"], "categories": ["cs.LG", "cs.CY", "cs.HC", "cs.MA"], "comment": "In CGI 2025: 42nd Computer Graphics International Conference,\n  Kowloon, Hong Kong, Peper No. 134", "summary": "Knowledge components (KCs) are the fundamental units of knowledge in the\nfield of education. A KC graph illustrates the relationships and dependencies\nbetween KCs. An accurate KC graph can assist educators in identifying the root\ncauses of learners' poor performance on specific KCs, thereby enabling targeted\ninstructional interventions. To achieve this, we have developed a KC graph\nstructure learning algorithm, named MAS-KCL, which employs a multi-agent system\ndriven by large language models for adaptive modification and optimization of\nthe KC graph. Additionally, a bidirectional feedback mechanism is integrated\ninto the algorithm, where AI agents leverage this mechanism to assess the value\nof edges within the KC graph and adjust the distribution of generation\nprobabilities for different edges, thereby accelerating the efficiency of\nstructure learning. We applied the proposed algorithm to 5 synthetic datasets\nand 4 real-world educational datasets, and experimental results validate its\neffectiveness in learning path recognition. By accurately identifying learners'\nlearning paths, teachers are able to design more comprehensive learning plans,\nenabling learners to achieve their educational goals more effectively, thus\npromoting the sustainable development of education."}
{"id": "2505.13628", "pdf": "https://arxiv.org/pdf/2505.13628.pdf", "abs": "https://arxiv.org/abs/2505.13628", "title": "Cross-Lingual Representation Alignment Through Contrastive Image-Caption Tuning", "authors": ["Nathaniel Krasner", "Nicholas Lanuzo", "Antonios Anastasopoulos"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Multilingual alignment of sentence representations has mostly required\nbitexts to bridge the gap between languages. We investigate whether visual\ninformation can bridge this gap instead. Image caption datasets are very easy\nto create without requiring multilingual expertise, so this offers a more\nefficient alternative for low-resource languages. We find that multilingual\nimage-caption alignment can implicitly align the text representations between\nlanguages, languages unseen by the encoder in pretraining can be incorporated\ninto this alignment post-hoc, and these aligned representations are usable for\ncross-lingual Natural Language Understanding (NLU) and bitext retrieval."}
{"id": "2505.14349", "pdf": "https://arxiv.org/pdf/2505.14349.pdf", "abs": "https://arxiv.org/abs/2505.14349", "title": "Upgrading Democracies with Fairer Voting Methods", "authors": ["Evangelos Pournaras", "Srijoni Majumdar", "Thomas Wellings", "Joshua C. Yang", "Fatemeh B. Heravan", "Regula Hänggli Fricker", "Dirk Helbing"], "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.HC", "cs.MA"], "comment": "Includes Supplementary Information", "summary": "Voting methods are instrumental design element of democracies. Citizens use\nthem to express and aggregate their preferences to reach a collective decision.\nHowever, voting outcomes can be as sensitive to voting rules as they are to\npeople's voting choices. Despite the significance and inter-disciplinary\nscientific progress on voting methods, several democracies keep relying on\noutdated voting methods that do not fit modern, pluralistic societies well,\nwhile lacking social innovation. Here, we demonstrate how one can upgrade\nreal-world democracies, namely by using alternative preferential voting methods\nsuch as cumulative voting and the method of equal shares designed for a\nproportional representation of voters' preferences. By rigorously assessing a\nnew participatory budgeting approach applied in the city of Aarau, Switzerland,\nwe unravel the striking voting outcomes of fair voting methods: more winning\nprojects with the same budget and broader geographic and preference\nrepresentation of citizens by the elected projects, in particular for voters\nwho used to be under-represented, while promoting novel project ideas. We\nprovide profound causal evidence showing that citizens prefer proportional\nvoting methods, which possess strong legitimacy without the need of very\ntechnical specialized explanations. We also reveal strong underlying democratic\nvalues exhibited by citizens who support fair voting methods such as altruism\nand compromise. These findings come with a global momentum to unleash a new and\nlong-awaited participation blueprint of how to upgrade democracies."}
{"id": "2505.13657", "pdf": "https://arxiv.org/pdf/2505.13657.pdf", "abs": "https://arxiv.org/abs/2505.13657", "title": "Clarifying orthography: Orthographic transparency as compressibility", "authors": ["Charles J. Torres", "Richard Futrell"], "categories": ["cs.CL", "cs.IT", "math.IT"], "comment": null, "summary": "Orthographic transparency -- how directly spelling is related to sound --\nlacks a unified, script-agnostic metric. Using ideas from algorithmic\ninformation theory, we quantify orthographic transparency in terms of the\nmutual compressibility between orthographic and phonological strings. Our\nmeasure provides a principled way to combine two factors that decrease\northographic transparency, capturing both irregular spellings and rule\ncomplexity in one quantity. We estimate our transparency measure using\nprequential code-lengths derived from neural sequence models. Evaluating 22\nlanguages across a broad range of script types (alphabetic, abjad, abugida,\nsyllabic, logographic) confirms common intuitions about relative transparency\nof scripts. Mutual compressibility offers a simple, principled, and general\nyardstick for orthographic transparency."}
{"id": "2505.14388", "pdf": "https://arxiv.org/pdf/2505.14388.pdf", "abs": "https://arxiv.org/abs/2505.14388", "title": "Algorithmic Hiring and Diversity: Reducing Human-Algorithm Similarity for Better Outcomes", "authors": ["Prasanna Parasurama", "Panos Ipeirotis"], "categories": ["cs.LG", "cs.HC", "econ.GN", "q-fin.EC"], "comment": null, "summary": "Algorithmic tools are increasingly used in hiring to improve fairness and\ndiversity, often by enforcing constraints such as gender-balanced candidate\nshortlists. However, we show theoretically and empirically that enforcing equal\nrepresentation at the shortlist stage does not necessarily translate into more\ndiverse final hires, even when there is no gender bias in the hiring stage. We\nidentify a crucial factor influencing this outcome: the correlation between the\nalgorithm's screening criteria and the human hiring manager's evaluation\ncriteria -- higher correlation leads to lower diversity in final hires. Using a\nlarge-scale empirical analysis of nearly 800,000 job applications across\nmultiple technology firms, we find that enforcing equal shortlists yields\nlimited improvements in hire diversity when the algorithmic screening closely\nmirrors the hiring manager's preferences. We propose a complementary\nalgorithmic approach designed explicitly to diversify shortlists by selecting\ncandidates likely to be overlooked by managers, yet still competitive according\nto their evaluation criteria. Empirical simulations show that this approach\nsignificantly enhances gender diversity in final hires without substantially\ncompromising hire quality. These findings highlight the importance of\nalgorithmic design choices in achieving organizational diversity goals and\nprovide actionable guidance for practitioners implementing fairness-oriented\nhiring algorithms."}
{"id": "2505.13706", "pdf": "https://arxiv.org/pdf/2505.13706.pdf", "abs": "https://arxiv.org/abs/2505.13706", "title": "Are Large Language Models Good at Detecting Propaganda?", "authors": ["Julia Jose", "Rachel Greenstadt"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Propagandists use rhetorical devices that rely on logical fallacies and\nemotional appeals to advance their agendas. Recognizing these techniques is key\nto making informed decisions. Recent advances in Natural Language Processing\n(NLP) have enabled the development of systems capable of detecting manipulative\ncontent. In this study, we look at several Large Language Models and their\nperformance in detecting propaganda techniques in news articles. We compare the\nperformance of these LLMs with transformer-based models. We find that, while\nGPT-4 demonstrates superior F1 scores (F1=0.16) compared to GPT-3.5 and Claude\n3 Opus, it does not outperform a RoBERTa-CRF baseline (F1=0.67). Additionally,\nwe find that all three LLMs outperform a MultiGranularity Network (MGN)\nbaseline in detecting instances of one out of six propaganda techniques\n(name-calling), with GPT-3.5 and GPT-4 also outperforming the MGN baseline in\ndetecting instances of appeal to fear and flag-waving."}
{"id": "2505.14535", "pdf": "https://arxiv.org/pdf/2505.14535.pdf", "abs": "https://arxiv.org/abs/2505.14535", "title": "Spiking Neural Networks with Temporal Attention-Guided Adaptive Fusion for imbalanced Multi-modal Learning", "authors": ["Jiangrong Shen", "Yulin Xie", "Qi Xu", "Gang Pan", "Huajin Tang", "Badong Chen"], "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "Multimodal spiking neural networks (SNNs) hold significant potential for\nenergy-efficient sensory processing but face critical challenges in modality\nimbalance and temporal misalignment. Current approaches suffer from\nuncoordinated convergence speeds across modalities and static fusion mechanisms\nthat ignore time-varying cross-modal interactions. We propose the temporal\nattention-guided adaptive fusion framework for multimodal SNNs with two\nsynergistic innovations: 1) The Temporal Attention-guided Adaptive Fusion\n(TAAF) module that dynamically assigns importance scores to fused spiking\nfeatures at each timestep, enabling hierarchical integration of temporally\nheterogeneous spike-based features; 2) The temporal adaptive balanced fusion\nloss that modulates learning rates per modality based on the above attention\nscores, preventing dominant modalities from monopolizing optimization. The\nproposed framework implements adaptive fusion, especially in the temporal\ndimension, and alleviates the modality imbalance during multimodal learning,\nmimicking cortical multisensory integration principles. Evaluations on CREMA-D,\nAVE, and EAD datasets demonstrate state-of-the-art performance (77.55\\%,\n70.65\\% and 97.5\\%accuracy, respectively) with energy efficiency. The system\nresolves temporal misalignment through learnable time-warping operations and\nfaster modality convergence coordination than baseline SNNs. This work\nestablishes a new paradigm for temporally coherent multimodal learning in\nneuromorphic systems, bridging the gap between biological sensory processing\nand efficient machine intelligence."}
{"id": "2505.13725", "pdf": "https://arxiv.org/pdf/2505.13725.pdf", "abs": "https://arxiv.org/abs/2505.13725", "title": "SQLForge: Synthesizing Reliable and Diverse Data to Enhance Text-to-SQL Reasoning in LLMs", "authors": ["Yu Guo", "Dong Jin", "Shenghao Ye", "Shuangwu Chen", "Jian Yang", "Xiaobin Tan"], "categories": ["cs.CL"], "comment": "12 pages, 7 figures, accepted to ACL Findings 2025", "summary": "Large Language models (LLMs) have demonstrated significant potential in\ntext-to-SQL reasoning tasks, yet a substantial performance gap persists between\nexisting open-source models and their closed-source counterparts. In this\npaper, we introduce SQLForge, a novel approach for synthesizing reliable and\ndiverse data to enhance text-to-SQL reasoning in LLMs. We improve data\nreliability through SQL syntax constraints and SQL-to-question reverse\ntranslation, ensuring data logic at both structural and semantic levels. We\nalso propose an SQL template enrichment and iterative data domain exploration\nmechanism to boost data diversity. Building on the augmented data, we fine-tune\na variety of open-source models with different architectures and parameter\nsizes, resulting in a family of models termed SQLForge-LM. SQLForge-LM achieves\nthe state-of-the-art performance on the widely recognized Spider and BIRD\nbenchmarks among the open-source models. Specifically, SQLForge-LM achieves EX\naccuracy of 85.7% on Spider Dev and 59.8% on BIRD Dev, significantly narrowing\nthe performance gap with closed-source methods."}
{"id": "2505.14633", "pdf": "https://arxiv.org/pdf/2505.14633.pdf", "abs": "https://arxiv.org/abs/2505.14633", "title": "Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas", "authors": ["Yu Ying Chiu", "Zhilin Wang", "Sharan Maiya", "Yejin Choi", "Kyle Fish", "Sydney Levine", "Evan Hubinger"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "34 pages, 11 figures, see associated data at\n  https://huggingface.co/datasets/kellycyy/AIRiskDilemmas and code at\n  https://github.com/kellycyy/LitmusValues", "summary": "Detecting AI risks becomes more challenging as stronger models emerge and\nfind novel methods such as Alignment Faking to circumvent these detection\nattempts. Inspired by how risky behaviors in humans (i.e., illegal activities\nthat may hurt others) are sometimes guided by strongly-held values, we believe\nthat identifying values within AI models can be an early warning system for\nAI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal\nAI models' priorities on a range of AI value classes. Then, we collect\nAIRiskDilemmas, a diverse collection of dilemmas that pit values against one\nanother in scenarios relevant to AI safety risks such as Power Seeking. By\nmeasuring an AI model's value prioritization using its aggregate choices, we\nobtain a self-consistent set of predicted value priorities that uncover\npotential risks. We show that values in LitmusValues (including seemingly\ninnocuous ones like Care) can predict for both seen risky behaviors in\nAIRiskDilemmas and unseen risky behaviors in HarmBench."}
{"id": "2505.13761", "pdf": "https://arxiv.org/pdf/2505.13761.pdf", "abs": "https://arxiv.org/abs/2505.13761", "title": "Simulation Agent: A Framework for Integrating Simulation and Large Language Models for Enhanced Decision-Making", "authors": ["Jacob Kleiman", "Kevin Frank", "Sindy Campagna"], "categories": ["cs.CL"], "comment": null, "summary": "Simulations, although powerful in accurately replicating real-world systems,\noften remain inaccessible to non-technical users due to their complexity.\nConversely, large language models (LLMs) provide intuitive, language-based\ninteractions but can lack the structured, causal understanding required to\nreliably model complex real-world dynamics. We introduce our simulation agent\nframework, a novel approach that integrates the strengths of both simulation\nmodels and LLMs. This framework helps empower users by leveraging the\nconversational capabilities of LLMs to interact seamlessly with sophisticated\nsimulation systems, while simultaneously utilizing the simulations to ground\nthe LLMs in accurate and structured representations of real-world phenomena.\nThis integrated approach helps provide a robust and generalizable foundation\nfor empirical validation and offers broad applicability across diverse domains."}
{"id": "2505.14664", "pdf": "https://arxiv.org/pdf/2505.14664.pdf", "abs": "https://arxiv.org/abs/2505.14664", "title": "AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of Cross-Modal Embeddings", "authors": ["Yilin Ye", "Junchao Huang", "Xingchen Zeng", "Jiazhi Xia", "Wei Zeng"], "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Cross-modal embeddings form the foundation for multi-modal models. However,\nvisualization methods for interpreting cross-modal embeddings have been\nprimarily confined to traditional dimensionality reduction (DR) techniques like\nPCA and t-SNE. These DR methods primarily focus on feature distributions within\na single modality, whilst failing to incorporate metrics (e.g., CLIPScore)\nacross multiple modalities.This paper introduces AKRMap, a new DR technique\ndesigned to visualize cross-modal embeddings metric with enhanced accuracy by\nlearning kernel regression of the metric landscape in the projection space.\nSpecifically, AKRMap constructs a supervised projection network guided by a\npost-projection kernel regression loss, and employs adaptive generalized\nkernels that can be jointly optimized with the projection. This approach\nenables AKRMap to efficiently generate visualizations that capture complex\nmetric distributions, while also supporting interactive features such as zoom\nand overlay for deeper exploration. Quantitative experiments demonstrate that\nAKRMap outperforms existing DR methods in generating more accurate and\ntrustworthy visualizations. We further showcase the effectiveness of AKRMap in\nvisualizing and comparing cross-modal embeddings for text-to-image models. Code\nand demo are available at https://github.com/yilinye/AKRMap."}
{"id": "2505.13772", "pdf": "https://arxiv.org/pdf/2505.13772.pdf", "abs": "https://arxiv.org/abs/2505.13772", "title": "Krikri: Advancing Open Large Language Models for Greek", "authors": ["Dimitris Roussis", "Leon Voukoutis", "Georgios Paraskevopoulos", "Sokratis Sofianopoulos", "Prokopis Prokopidis", "Vassilis Papavasileiou", "Athanasios Katsamanis", "Stelios Piperidis", "Vassilis Katsouros"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce Llama-Krikri-8B, a cutting-edge Large Language Model tailored\nfor the Greek language, built on Meta's Llama 3.1-8B. Llama-Krikri-8B has been\nextensively trained on high-quality Greek data to ensure superior adaptation to\nlinguistic nuances. With 8 billion parameters, it offers advanced capabilities\nwhile maintaining efficient computational performance. Llama-Krikri-8B supports\nboth Modern Greek and English, and is also equipped to handle polytonic text\nand Ancient Greek. The chat version of Llama-Krikri-8B features a multi-stage\npost-training pipeline, utilizing both human and synthetic instruction and\npreference data, by applying techniques such as MAGPIE. In addition, for\nevaluation, we propose three novel public benchmarks for Greek. Our evaluation\non existing as well as the proposed benchmarks shows notable improvements over\ncomparable Greek and multilingual LLMs in both natural language understanding\nand generation as well as code generation."}
{"id": "2505.14668", "pdf": "https://arxiv.org/pdf/2505.14668.pdf", "abs": "https://arxiv.org/abs/2505.14668", "title": "ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions", "authors": ["Bufang Yang", "Lilin Xu", "Liekang Zeng", "Kaiwei Liu", "Siyang Jiang", "Wenrui Lu", "Hongkai Chen", "Xiaofan Jiang", "Guoliang Xing", "Zhenyu Yan"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have propelled intelligent\nagents from reactive responses to proactive support. While promising, existing\nproactive agents either rely exclusively on observations from enclosed\nenvironments (e.g., desktop UIs) with direct LLM inference or employ rule-based\nproactive notifications, leading to suboptimal user intent understanding and\nlimited functionality for proactive service. In this paper, we introduce\nContextAgent, the first context-aware proactive agent that incorporates\nextensive sensory contexts to enhance the proactive capabilities of LLM agents.\nContextAgent first extracts multi-dimensional contexts from massive sensory\nperceptions on wearables (e.g., video and audio) to understand user intentions.\nContextAgent then leverages the sensory contexts and the persona contexts from\nhistorical data to predict the necessity for proactive services. When proactive\nassistance is needed, ContextAgent further automatically calls the necessary\ntools to assist users unobtrusively. To evaluate this new task, we curate\nContextAgentBench, the first benchmark for evaluating context-aware proactive\nLLM agents, covering 1,000 samples across nine daily scenarios and twenty\ntools. Experiments on ContextAgentBench show that ContextAgent outperforms\nbaselines by achieving up to 8.5% and 6.0% higher accuracy in proactive\npredictions and tool calling, respectively. We hope our research can inspire\nthe development of more advanced, human-centric, proactive AI assistants."}
{"id": "2505.13792", "pdf": "https://arxiv.org/pdf/2505.13792.pdf", "abs": "https://arxiv.org/abs/2505.13792", "title": "Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation", "authors": ["Siddhant Bhambri", "Upasana Biswas", "Subbarao Kambhampati"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages", "summary": "Question Answering (QA) poses a challenging and critical problem,\nparticularly in today's age of interactive dialogue systems such as ChatGPT,\nPerplexity, Microsoft Copilot, etc. where users demand both accuracy and\ntransparency in the model's outputs. Since smaller language models (SLMs) are\ncomputationally more efficient but often under-perform compared to larger\nmodels, Knowledge Distillation (KD) methods allow for finetuning these smaller\nmodels to improve their final performance. Lately, the intermediate tokens or\nthe so called `reasoning' traces produced by Chain-of-Thought (CoT) or by\nreasoning models such as DeepSeek R1 are used as a training signal for KD.\nHowever, these reasoning traces are often verbose and difficult to interpret or\nevaluate. In this work, we aim to address the challenge of evaluating the\nfaithfulness of these reasoning traces and their correlation with the final\nperformance. To this end, we employ a KD method leveraging rule-based problem\ndecomposition. This approach allows us to break down complex queries into\nstructured sub-problems, generating interpretable traces whose correctness can\nbe readily evaluated, even at inference time. Specifically, we demonstrate this\napproach on Open Book QA, decomposing the problem into a Classification step\nand an Information Retrieval step, thereby simplifying trace evaluation. Our\nSFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft\nMachine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the\nstriking finding that correct traces do not necessarily imply that the model\noutputs the correct final solution. Similarly, we find a low correlation\nbetween correct final solutions and intermediate trace correctness. These\nresults challenge the implicit assumption behind utilizing reasoning traces for\nimproving SLMs' final performance via KD."}
{"id": "2505.14680", "pdf": "https://arxiv.org/pdf/2505.14680.pdf", "abs": "https://arxiv.org/abs/2505.14680", "title": "NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search", "authors": ["Sunhao Dai", "Wenjie Wang", "Liang Pang", "Jun Xu", "See-Kiong Ng", "Ji-Rong Wen", "Tat-Seng Chua"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.HC"], "comment": "SIGIR 2025 Perspective Paper", "summary": "Generative AI search is reshaping information retrieval by offering\nend-to-end answers to complex queries, reducing users' reliance on manually\nbrowsing and summarizing multiple web pages. However, while this paradigm\nenhances convenience, it disrupts the feedback-driven improvement loop that has\nhistorically powered the evolution of traditional Web search. Web search can\ncontinuously improve their ranking models by collecting large-scale,\nfine-grained user feedback (e.g., clicks, dwell time) at the document level. In\ncontrast, generative AI search operates through a much longer search pipeline,\nspanning query decomposition, document retrieval, and answer generation, yet\ntypically receives only coarse-grained feedback on the final answer. This\nintroduces a feedback loop disconnect, where user feedback for the final output\ncannot be effectively mapped back to specific system components, making it\ndifficult to improve each intermediate stage and sustain the feedback loop. In\nthis paper, we envision NExT-Search, a next-generation paradigm designed to\nreintroduce fine-grained, process-level feedback into generative AI search.\nNExT-Search integrates two complementary modes: User Debug Mode, which allows\nengaged users to intervene at key stages; and Shadow User Mode, where a\npersonalized user agent simulates user preferences and provides AI-assisted\nfeedback for less interactive users. Furthermore, we envision how these\nfeedback signals can be leveraged through online adaptation, which refines\ncurrent search outputs in real-time, and offline update, which aggregates\ninteraction logs to periodically fine-tune query decomposition, retrieval, and\ngeneration models. By restoring human control over key stages of the generative\nAI search pipeline, we believe NExT-Search offers a promising direction for\nbuilding feedback-rich AI search systems that can evolve continuously alongside\nhuman feedback."}
{"id": "2505.13840", "pdf": "https://arxiv.org/pdf/2505.13840.pdf", "abs": "https://arxiv.org/abs/2505.13840", "title": "EfficientLLM: Efficiency in Large Language Models", "authors": ["Zhengqing Yuan", "Weixiang Sun", "Yixin Liu", "Huichi Zhou", "Rong Zhou", "Yiyang Li", "Zheyuan Zhang", "Wei Song", "Yue Huang", "Haolong Jia", "Keerthiram Murugesan", "Yu Wang", "Lifang He", "Jianfeng Gao", "Lichao Sun", "Yanfang Ye"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have driven significant progress, yet their\ngrowing parameter counts and context windows incur prohibitive compute, energy,\nand monetary costs. We introduce EfficientLLM, a novel benchmark and the first\ncomprehensive empirical study evaluating efficiency techniques for LLMs at\nscale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our\nstudy systematically explores three key axes: (1) architecture pretraining\n(efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts\n(MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and\n(3) inference (quantization methods: int4, float16). We define six fine-grained\nmetrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy\nConsumption, Compression Rate) to capture hardware saturation,\nlatency-throughput balance, and carbon cost. Evaluating over 100\nmodel-technique pairs (0.5B-72B parameters), we derive three core insights: (i)\nEfficiency involves quantifiable trade-offs: no single method is universally\noptimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by\n40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5%\naccuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal\nmemory-latency trade-offs for constrained devices, MLA achieves lowest\nperplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency\nonly beyond 14B parameters. (iii) Techniques generalize across modalities: we\nextend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and\nVision-Language Models (Qwen2.5-VL), confirming effective transferability. By\nopen-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM\nprovides essential guidance for researchers and engineers navigating the\nefficiency-performance landscape of next-generation foundation models."}
{"id": "2311.00721", "pdf": "https://arxiv.org/pdf/2311.00721.pdf", "abs": "https://arxiv.org/abs/2311.00721", "title": "Empathy Detection from Text, Audiovisual, Audio or Physiological Signals: A Systematic Review of Task Formulations and Machine Learning Methods", "authors": ["Md Rakibul Hasan", "Md Zakir Hossain", "Shreya Ghosh", "Aneesh Krishna", "Tom Gedeon"], "categories": ["cs.HC", "cs.LG", "cs.SI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Empathy indicates an individual's ability to understand others. Over the past\nfew years, empathy has drawn attention from various disciplines, including but\nnot limited to Affective Computing, Cognitive Science, and Psychology.\nDetecting empathy has potential applications in society, healthcare and\neducation. Despite being a broad and overlapping topic, the avenue of empathy\ndetection leveraging Machine Learning remains underexplored from a systematic\nliterature review perspective. We collected 849 papers from 10 well-known\nacademic databases, systematically screened them and analysed the final 82\npapers. Our analyses reveal several prominent task formulations - including\nempathy on localised utterances or overall expressions, unidirectional or\nparallel empathy, and emotional contagion - in monadic, dyadic and group\ninteractions. Empathy detection methods are summarised based on four input\nmodalities - text, audiovisual, audio and physiological signals - thereby\npresenting modality-specific network architecture design protocols. We discuss\nchallenges, research gaps and potential applications in the Affective\nComputing-based empathy domain, which can facilitate new avenues of\nexploration. We further enlist the public availability of datasets and codes.\nThis paper, therefore, provides a structured overview of recent advancements\nand remaining challenges towards developing a robust empathy detection system\nthat could meaningfully contribute to enhancing human well-being."}
{"id": "2505.13844", "pdf": "https://arxiv.org/pdf/2505.13844.pdf", "abs": "https://arxiv.org/abs/2505.13844", "title": "Improve Language Model and Brain Alignment via Associative Memory", "authors": ["Congchi Yin", "Yongpeng Zhang", "Xuyun Wen", "Piji Li"], "categories": ["cs.CL"], "comment": "Accepted by Findings of ACL 2025", "summary": "Associative memory engages in the integration of relevant information for\ncomprehension in the human cognition system. In this work, we seek to improve\nalignment between language models and human brain while processing speech\ninformation by integrating associative memory. After verifying the alignment\nbetween language model and brain by mapping language model activations to brain\nactivity, the original text stimuli expanded with simulated associative memory\nare regarded as input to computational language models. We find the alignment\nbetween language model and brain is improved in brain regions closely related\nto associative memory processing. We also demonstrate large language models\nafter specific supervised fine-tuning better align with brain response, by\nbuilding the \\textit{Association} dataset containing 1000 samples of stories,\nwith instructions encouraging associative memory as input and associated\ncontent as output."}
{"id": "2409.01240", "pdf": "https://arxiv.org/pdf/2409.01240.pdf", "abs": "https://arxiv.org/abs/2409.01240", "title": "DiffEyeSyn: Diffusion-based User-specific Eye Movement Synthesis", "authors": ["Chuhan Jiao", "Guanhua Zhang", "Yeonjoo Cho", "Zhiming Hu", "Andreas Bulling"], "categories": ["cs.HC"], "comment": "Corrected bugs in creating visualisations", "summary": "High-frequency gaze data contains more user-specific information than\nlow-frequency data, promising for various applications. However, existing gaze\nmodelling methods focus on low-frequency data, ignoring user-specific subtle\neye movements in high-frequency eye movements. We present DiffEyeSyn -- the\nfirst computational method to synthesise eye movements specific to individual\nusers. The key idea is to consider the user-specific information as a special\ntype of noise in eye movement data. This perspective reshapes eye movement\nsynthesis into the task of injecting this user-specific noise into any given\neye movement sequence. We formulate this injection task as a conditional\ndiffusion process in which the synthesis is conditioned on user-specific\nembeddings extracted from the gaze data using pre-trained models for user\nauthentication. We propose user identity guidance -- a novel loss function that\nallows our model to preserve user identity while generating human-like eye\nmovements in the spatial domain. Experiments on two public datasets show that\nour synthetic eye movements preserve user-specific characteristics and are more\nrealistic than baseline approaches. Furthermore, we demonstrate that DiffEyeSyn\ncan synthesise large-scale gaze data and support various downstream tasks, such\nas gaze-based user identification. As such, our work lays the methodological\nfoundations for personalised eye movement synthesis that has significant\napplication potential, such as for character animation, eye movement\nbiometrics, and gaze data imputation."}
{"id": "2505.13855", "pdf": "https://arxiv.org/pdf/2505.13855.pdf", "abs": "https://arxiv.org/abs/2505.13855", "title": "Domain Gating Ensemble Networks for AI-Generated Text Detection", "authors": ["Arihant Tripathi", "Liam Dugan", "Charis Gao", "Maggie Huan", "Emma Jin", "Peter Zhang", "David Zhang", "Julia Zhao", "Chris Callison-Burch"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Submitted to EMNLP 2025", "summary": "As state-of-the-art language models continue to improve, the need for robust\ndetection of machine-generated text becomes increasingly critical. However,\ncurrent state-of-the-art machine text detectors struggle to adapt to new unseen\ndomains and generative models. In this paper we present DoGEN (Domain Gating\nEnsemble Networks), a technique that allows detectors to adapt to unseen\ndomains by ensembling a set of domain expert detector models using weights from\na domain classifier. We test DoGEN on a wide variety of domains from leading\nbenchmarks and find that it achieves state-of-the-art performance on in-domain\ndetection while outperforming models twice its size on out-of-domain detection.\nWe release our code and trained models to assist in future research in\ndomain-adaptive AI detection."}
{"id": "2410.13036", "pdf": "https://arxiv.org/pdf/2410.13036.pdf", "abs": "https://arxiv.org/abs/2410.13036", "title": "Uncovering the Internet's Hidden Values: An Empirical Study of Desirable Behavior Using Highly-Upvoted Content on Reddit", "authors": ["Agam Goyal", "Charlotte Lambert", "Yoshee Jain", "Eshwar Chandrasekharan"], "categories": ["cs.HC", "cs.SI"], "comment": "Preprint: 15 pages, 3 figures, 2 tables", "summary": "A major task for moderators of online spaces is norm-setting, essentially\ncreating shared norms for user behavior in their communities. Platform design\nprinciples emphasize the importance of highlighting norm-adhering examples and\nexplicitly stating community norms. However, norms and values vary between\ncommunities and go beyond content-level attributes, making it challenging for\nplatforms and researchers to provide automated ways to identify desirable\nbehavior to be highlighted. Current automated approaches to detect desirability\nare limited to measures of prosocial behavior, but we do not know whether these\nmeasures fully capture the spectrum of what communities value. In this paper,\nwe use upvotes, which express community approval, as a proxy for desirability\nand examine 16,000 highly-upvoted comments across 80 popular sub-communities on\nReddit. Using a large language model, we extract values from these comments\nacross two years (2016 and 2022) and compile 64 and 72 $\\textit{macro}$,\n$\\textit{meso}$, and $\\textit{micro}$ values for 2016 and 2022 respectively,\nbased on their frequency across communities. Furthermore, we find that existing\ncomputational models for measuring prosociality were inadequate to capture on\naverage $82\\%$ of the values we extracted. Finally, we show that our approach\ncan not only extract most of the qualitatively-identified values from prior\ntaxonomies, but also uncover new values that are actually encouraged in\npractice. Our findings highlight the need for nuanced models of desirability\nthat go beyond preexisting prosocial measures. This work has implications for\nimproving moderator understanding of their community values and provides a\nframework that can supplement qualitative approaches with larger-scale content\nanalyses."}
{"id": "2505.13866", "pdf": "https://arxiv.org/pdf/2505.13866.pdf", "abs": "https://arxiv.org/abs/2505.13866", "title": "Reasoning Path Compression: Compressing Generation Trajectories for Efficient LLM Reasoning", "authors": ["Jiwon Song", "Dongwon Jo", "Yulhwa Kim", "Jae-Joon Kim"], "categories": ["cs.CL"], "comment": null, "summary": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and throughput of token\ngeneration, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining KV cache that receive high\nimportance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression."}
{"id": "2412.00411", "pdf": "https://arxiv.org/pdf/2412.00411.pdf", "abs": "https://arxiv.org/abs/2412.00411", "title": "Seismocardiography for Emotion Recognition: A Study on EmoWear with Insights from DEAP", "authors": ["Mohammad Hasan Rahmani", "Rafael Berkvens", "Maarten Weyn"], "categories": ["cs.HC"], "comment": "17 pages, 9 figures", "summary": "Emotions have a profound impact on our daily lives, influencing our thoughts,\nbehaviors, and interactions, but also our physiological reactions. Recent\nadvances in wearable technology have facilitated studying emotions through\ncardio-respiratory signals. Accelerometers offer a non-invasive, convenient,\nand cost-effective method for capturing heart- and pulmonary-induced vibrations\non the chest wall, specifically Seismocardiography (SCG) and\nAccelerometry-Derived Respiration (ADR). Their affordability, wide\navailability, and ability to provide rich contextual data make accelerometers\nideal for everyday use. While accelerometers have been used as part of broader\nmodality fusions for Emotion Recognition (ER), their stand-alone potential via\nSCG and ADR remains unexplored. Bridging this gap could significantly help the\nembedding of ER into real-world applications, minimizing the hardware, and\nincreasing contextual integration potentials. To address this gap, we introduce\nSCG and ADR as novel modalities for ER and evaluate their performance using the\nEmoWear dataset. First, we replicate the single-trial emotion classification\npipeline from the DEAP dataset study, achieving similar results. Then we use\nour validated pipeline to train models that predict affective valence-arousal\nstates using SCG and compare them against established cardiac signals,\nElectrocardiography (ECG) and Blood Volume Pulse (BVP). Results show that SCG\nis a viable modality for ER, achieving similar performance to ECG and BVP. By\ncombining ADR with SCG, we achieved a working ER framework that only requires a\nsingle chest-worn accelerometer. These findings pave the way for integrating ER\ninto real-world, enabling seamless affective computing in everyday life."}
{"id": "2505.13886", "pdf": "https://arxiv.org/pdf/2505.13886.pdf", "abs": "https://arxiv.org/abs/2505.13886", "title": "Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning", "authors": ["Jingqi Tong", "Jixin Tang", "Hangcheng Li", "Yurong Mou", "Ming Zhang", "Jun Zhao", "Yanbo Wen", "Fan Song", "Jiahao Zhan", "Yuyang Lu", "Chaoran Tao", "Zhiyuan Guo", "Jizhou Yu", "Tianhao Cheng", "Changhao Jiang", "Zhen Wang", "Tao Liang", "Zhihui Fei", "Mingyang Wan", "Guojun Ma", "Weifeng Ge", "Guanhua Chen", "Tao Gui", "Xipeng Qiu", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL", "I.2.7; I.2.10"], "comment": "49 pages, 19 figures, submitted to NeurIPS 2025", "summary": "Visual-language Chain-of-Thought (CoT) data resources are relatively scarce\ncompared to text-only counterparts, limiting the improvement of reasoning\ncapabilities in Vision Language Models (VLMs). However, high-quality\nvision-language reasoning data is expensive and labor-intensive to annotate. To\naddress this issue, we leverage a promising resource: game code, which\nnaturally contains logical structures and state transition processes.\nTherefore, we propose Code2Logic, a novel game-code-driven approach for\nmultimodal reasoning data synthesis. Our approach leverages Large Language\nModels (LLMs) to adapt game code, enabling automatic acquisition of reasoning\nprocesses and results through code execution. Using the Code2Logic approach, we\ndeveloped the GameQA dataset to train and evaluate VLMs. GameQA is\ncost-effective and scalable to produce, challenging for state-of-the-art\nmodels, and diverse with 30 games and 158 tasks. Surprisingly, despite training\nsolely on game data, VLMs demonstrated out of domain generalization,\nspecifically Qwen2.5-VL-7B improving performance by 2.33\\% across 7 diverse\nvision-language benchmarks. Our code and dataset are available at\nhttps://github.com/tongjingqi/Code2Logic."}
{"id": "2501.08046", "pdf": "https://arxiv.org/pdf/2501.08046.pdf", "abs": "https://arxiv.org/abs/2501.08046", "title": "Building Symbiotic AI: Reviewing the AI Act for a Human-Centred, Principle-Based Framework", "authors": ["Miriana Calvano", "Antonio Curci", "Giuseppe Desolda", "Andrea Esposito", "Rosa Lanzilotti", "Antonio Piccinno"], "categories": ["cs.HC", "cs.AI"], "comment": "Third version: 36 pages", "summary": "Artificial Intelligence (AI) spreads quickly as new technologies and services\ntake over modern society. The need to regulate AI design, development, and use\nis strictly necessary to avoid unethical and potentially dangerous consequences\nto humans. The European Union (EU) has released a new legal framework, the AI\nAct, to regulate AI by undertaking a risk-based approach to safeguard humans\nduring interaction. At the same time, researchers offer a new perspective on AI\nsystems, commonly known as Human-Centred AI (HCAI), highlighting the need for a\nhuman-centred approach to their design. In this context, Symbiotic AI (a\nsubtype of HCAI) promises to enhance human capabilities through a deeper and\ncontinuous collaboration between human intelligence and AI. This article\npresents the results of a Systematic Literature Review (SLR) that aims to\nidentify principles that characterise the design and development of Symbiotic\nAI systems while considering humans as the core of the process. Through content\nanalysis, four principles emerged from the review that must be applied to\ncreate Human-Centred AI systems that can establish a symbiotic relationship\nwith humans. In addition, current trends and challenges were defined to\nindicate open questions that may guide future research for the development of\nSAI systems that comply with the AI Act."}
{"id": "2505.13890", "pdf": "https://arxiv.org/pdf/2505.13890.pdf", "abs": "https://arxiv.org/abs/2505.13890", "title": "Mapping the Minds of LLMs: A Graph-Based Analysis of Reasoning LLM", "authors": ["Zhen Xiong", "Yujun Cai", "Zhecheng Li", "Yiwei Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in test-time scaling have enabled Large Language Models\n(LLMs) to display sophisticated reasoning abilities via extended\nChain-of-Thought (CoT) generation. Despite their potential, these Reasoning\nLLMs (RLMs) often demonstrate counterintuitive and unstable behaviors, such as\nperformance degradation under few-shot prompting, that challenge our current\nunderstanding of RLMs. In this work, we introduce a unified graph-based\nanalytical framework for better modeling the reasoning processes of RLMs. Our\nmethod first clusters long, verbose CoT outputs into semantically coherent\nreasoning steps, then constructs directed reasoning graphs to capture\ncontextual and logical dependencies among these steps. Through comprehensive\nanalysis across models and prompting regimes, we reveal that structural\nproperties, such as exploration density, branching, and convergence ratios,\nstrongly correlate with reasoning accuracy. Our findings demonstrate how\nprompting strategies substantially reshape the internal reasoning structure of\nRLMs, directly affecting task outcomes. The proposed framework not only enables\nquantitative evaluation of reasoning quality beyond conventional metrics but\nalso provides practical insights for prompt engineering and the cognitive\nanalysis of LLMs. Code and resources will be released to facilitate future\nresearch in this direction."}
{"id": "2501.13145", "pdf": "https://arxiv.org/pdf/2501.13145.pdf", "abs": "https://arxiv.org/abs/2501.13145", "title": "The GenUI Study: Exploring the Design of Generative UI Tools to Support UX Practitioners and Beyond", "authors": ["Xiang 'Anthony' Chen", "Tiffany Knearem", "Yang Li"], "categories": ["cs.HC"], "comment": null, "summary": "AI can now generate high-fidelity UI mock-up screens from a high-level\ntextual description, promising to support UX practitioners' work. However, it\nremains unclear how UX practitioners would adopt such Generative UI (GenUI)\nmodels in a way that is integral and beneficial to their work. To answer this\nquestion, we conducted a formative study with 37 UX-related professionals that\nconsisted of four roles: UX designers, UX researchers, software engineers, and\nproduct managers. Using a state-of-the-art GenUI tool, each participant went\nthrough a week-long, individual mini-project exercise with role-specific tasks,\nkeeping a daily journal of their usage and experiences with GenUI, followed by\na semi-structured interview. We report findings on participants' workflow using\nthe GenUI tool, how GenUI can support all and each specific roles, and existing\ngaps between GenUI and users' needs and expectations, which lead to design\nimplications to inform future work on GenUI development."}
{"id": "2505.13893", "pdf": "https://arxiv.org/pdf/2505.13893.pdf", "abs": "https://arxiv.org/abs/2505.13893", "title": "InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein for Model Fusion", "authors": ["Yuanyi Wang", "Zhaoyi Yan", "Yiming Zhang", "Qi Zhou", "Yanggan Gu", "Fei Wu", "Hongxia Yang"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have intensified efforts to\nfuse heterogeneous open-source models into a unified system that inherits their\ncomplementary strengths. Existing logit-based fusion methods maintain inference\nefficiency but treat vocabulary dimensions independently, overlooking semantic\ndependencies encoded by cross-dimension interactions. These dependencies\nreflect how token types interact under a model's internal reasoning and are\nessential for aligning models with diverse generation behaviors. To explicitly\nmodel these dependencies, we propose \\textbf{InfiGFusion}, the first\nstructure-aware fusion framework with a novel \\textit{Graph-on-Logits\nDistillation} (GLD) loss. Specifically, we retain the top-$k$ logits per output\nand aggregate their outer products across sequence positions to form a global\nco-activation graph, where nodes represent vocabulary channels and edges\nquantify their joint activations. To ensure scalability and efficiency, we\ndesign a sorting-based closed-form approximation that reduces the original\n$O(n^4)$ cost of Gromov-Wasserstein distance to $O(n \\log n)$, with provable\napproximation guarantees. Experiments across multiple fusion settings show that\nGLD consistently improves fusion quality and stability. InfiGFusion outperforms\nSOTA models and fusion baselines across 11 benchmarks spanning reasoning,\ncoding, and mathematics. It shows particular strength in complex reasoning\ntasks, with +35.6 improvement on Multistep Arithmetic and +37.06 on Causal\nJudgement over SFT, demonstrating superior multi-step and relational inference."}
{"id": "2503.16505", "pdf": "https://arxiv.org/pdf/2503.16505.pdf", "abs": "https://arxiv.org/abs/2503.16505", "title": "Scalable Evaluation of Online Facilitation Strategies via Synthetic Simulation of Discussions", "authors": ["Dimitris Tsirmpas", "Ion Androutsopoulos", "John Pavlopoulos"], "categories": ["cs.HC", "cs.CL", "cs.LG", "68T50", "I.2.7"], "comment": "19 pages, 3 tables, 12 figures", "summary": "Limited large-scale evaluations exist for facilitation strategies of online\ndiscussions due to significant costs associated with human involvement. An\neffective solution is synthetic discussion simulations using Large Language\nModels (LLMs) to create initial pilot experiments. We propose a simple,\ngeneralizable, LLM-driven methodology to prototype the development of LLM\nfacilitators, and produce high-quality synthetic data without human\ninvolvement. We use our methodology to test whether current facilitation\nstrategies can improve the performance of LLM facilitators. We find that, while\nLLM facilitators significantly improve synthetic discussions, there is no\nevidence that the application of more elaborate facilitation strategies\nproposed in modern Social Science research lead to further improvements in\ndiscussion quality, compared to more basic approaches. Additionally, we find\nthat small LLMs (such as Mistral Nemo 12B) can perform comparably to larger\nmodels (such as LLaMa 70B), and that special instructions must be used for\ninstruction-tuned models to induce toxicity in synthetic discussions. We\nconfirm that each component of our methodology contributes substantially to\nhigh quality data via an ablation study. We release an open-source framework,\n\"SynDisco\" (pip install syndisco), which implements our methodology. We also\nrelease the \"Virtual Moderation Dataset\"\n(https://paperswithcode.com/dataset/vmd), a large, publicly available dataset\ncontaining LLM-generated and LLM-annotated discussions using multiple\nopen-source LLMs."}
{"id": "2505.13903", "pdf": "https://arxiv.org/pdf/2505.13903.pdf", "abs": "https://arxiv.org/abs/2505.13903", "title": "Let's Verify Math Questions Step by Step", "authors": ["Chengyu Shen", "Zhen Hao Wong", "Runming He", "Hao Liang", "Meiyi Qiang", "Zimo Meng", "Zhengyang Zhao", "Bohan Zeng", "Zhengzhou Zhu", "Bin Cui", "Wentao Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have recently achieved remarkable progress in\nmathematical reasoning. To enable such capabilities, many existing works\ndistill strong reasoning models into long chains of thought or design\nalgorithms to construct high-quality math QA data for training. However, these\nefforts primarily focus on generating correct reasoning paths and answers,\nwhile largely overlooking the validity of the questions themselves. In this\nwork, we propose Math Question Verification (MathQ-Verify), a novel five-stage\npipeline designed to rigorously filter ill-posed or under-specified math\nproblems. MathQ-Verify first performs format-level validation to remove\nredundant instructions and ensure that each question is syntactically\nwell-formed. It then formalizes each question, decomposes it into atomic\nconditions, and verifies them against mathematical definitions. Next, it\ndetects logical contradictions among these conditions, followed by a\ngoal-oriented completeness check to ensure the question provides sufficient\ninformation for solving. To evaluate this task, we use existing benchmarks\nalong with an additional dataset we construct, containing 2,147 math questions\nwith diverse error types, each manually double-validated. Experiments show that\nMathQ-Verify achieves state-of-the-art performance across multiple benchmarks,\nimproving the F1 score by up to 25 percentage points over the direct\nverification baseline. It further attains approximately 90% precision and 63%\nrecall through a lightweight model voting scheme. MathQ-Verify offers a\nscalable and accurate solution for curating reliable mathematical datasets,\nreducing label noise and avoiding unnecessary computation on invalid questions.\nOur code and data are available at https://github.com/scuuy/MathQ-Verify."}
{"id": "2504.13901", "pdf": "https://arxiv.org/pdf/2504.13901.pdf", "abs": "https://arxiv.org/abs/2504.13901", "title": "Examining Technology Perspectives of Older Adults with Mild Cognitive Impairment: A Scoping Review", "authors": ["Snezna B Schmidt", "Stephen Isbel", "Blooma John", "Ram Subramanian", "Nathan M DCunha"], "categories": ["cs.HC"], "comment": "Have updated the paper and the authors are reviewing the amendments.\n  I will resubmit once all authors have completed reviewing", "summary": "Mild cognitive impairment (MCI) may affect up to 20% of people over 65.\nGlobal incidence of MCI is increasing, and technology is being explored for\nearly intervention. Theories of technology adoption predict useful and\neasy-to-use solutions will have higher rates of adoption; however, these models\ndo not specifically consider older people with cognitive impairments, or unique\nhuman-computer interaction challenges posed by MCI. Older people with MCI\nopinions about technology solutions were extracted from 83 articles, published\nbetween Jan 2014 and May 2024, and found in nine databases. Inductive, thematic\nanalysis of feedback Identified five themes (i) purpose and need, (ii) solution\ndesign and ease of use, (iii) self-impression, (iv) lifestyle, and (v)\ninteraction modality. Solutions are perceived as useful, even though gaps in\nfunctional support exist, however, they are not perceived as entirely easy to\nuse, due to issues related to ease of use and user experience. Devices which\nare light, portable, common and have large screens, are preferred, as is\nmultimodal interaction, in particular speech, visual/text and touch. This\nreview recommends future work to (i) improve personalisation, (ii) better\nunderstand interaction preferences and effectiveness, (iii) enable options for\nmultimodal interaction, and (iv) more seamlessly integrate solutions into user\nlifestyles."}
{"id": "2505.13908", "pdf": "https://arxiv.org/pdf/2505.13908.pdf", "abs": "https://arxiv.org/abs/2505.13908", "title": "Cross-Linguistic Transfer in Multilingual NLP: The Role of Language Families and Morphology", "authors": ["Ajitesh Bankula", "Praney Bankula"], "categories": ["cs.CL"], "comment": null, "summary": "Cross-lingual transfer has become a crucial aspect of multilingual NLP, as it\nallows for models trained on resource-rich languages to be applied to\nlow-resource languages more effectively. Recently massively multilingual\npre-trained language models (e.g., mBERT, XLM-R) demonstrate strong zero-shot\ntransfer capabilities[14] [13]. This paper investigates cross-linguistic\ntransfer through the lens of language families and morphology. Investigating\nhow language family proximity and morphological similarity affect performance\nacross NLP tasks. We further discuss our results and how it relates to findings\nfrom recent literature. Overall, we compare multilingual model performance and\nreview how linguistic distance metrics correlate with transfer outcomes. We\nalso look into emerging approaches that integrate typological and morphological\ninformation into model pre-training to improve transfer to diverse\nlanguages[18] [19]."}
{"id": "2504.13936", "pdf": "https://arxiv.org/pdf/2504.13936.pdf", "abs": "https://arxiv.org/abs/2504.13936", "title": "ViMo: A Generative Visual GUI World Model for App Agents", "authors": ["Dezhao Luo", "Bohan Tang", "Kang Li", "Georgios Papoudakis", "Jifei Song", "Shaogang Gong", "Jianye Hao", "Jun Wang", "Kun Shao"], "categories": ["cs.HC", "cs.LG", "cs.SY", "eess.SY"], "comment": "https://ai-agents-2030.github.io/ViMo/", "summary": "App agents, which autonomously operate mobile Apps through Graphical User\nInterfaces (GUIs), have gained significant interest in real-world applications.\nYet, they often struggle with long-horizon planning, failing to find the\noptimal actions for complex tasks with longer steps. To address this, world\nmodels are used to predict the next GUI observation based on user actions,\nenabling more effective agent planning. However, existing world models\nprimarily focus on generating only textual descriptions, lacking essential\nvisual details. To fill this gap, we propose ViMo, the first visual world model\ndesigned to generate future App observations as images. For the challenge of\ngenerating text in image patches, where even minor pixel errors can distort\nreadability, we decompose GUI generation into graphic and text content\ngeneration. We propose a novel data representation, the Symbolic Text\nRepresentation~(STR) to overlay text content with symbolic placeholders while\npreserving graphics. With this design, ViMo employs a STR Predictor to predict\nfuture GUIs' graphics and a GUI-text Predictor for generating the corresponding\ntext. Moreover, we deploy ViMo to enhance agent-focused tasks by predicting the\noutcome of different action options. Experiments show ViMo's ability to\ngenerate visually plausible and functionally effective GUIs that enable App\nagents to make more informed decisions."}
{"id": "2505.13913", "pdf": "https://arxiv.org/pdf/2505.13913.pdf", "abs": "https://arxiv.org/abs/2505.13913", "title": "Word length predicts word order: \"Min-max\"-ing drives language evolution", "authors": ["Hiram Ring"], "categories": ["cs.CL"], "comment": null, "summary": "Current theories of language propose an innate (Baker 2001; Chomsky 1981) or\na functional (Greenberg 1963; Dryer 2007; Hawkins 2014) origin for the surface\nstructures (i.e. word order) that we observe in languages of the world, while\nevolutionary modeling (Dunn et al. 2011) suggests that descent is the primary\nfactor influencing such patterns. Although there are hypotheses for word order\nchange from both innate and usage-based perspectives for specific languages and\nfamilies, there are key disagreements between the two major proposals for\nmechanisms that drive the evolution of language more broadly (Wasow 2002; Levy\n2008). This paper proposes a universal underlying mechanism for word order\nchange based on a large tagged parallel dataset of over 1,500 languages\nrepresenting 133 language families and 111 isolates. Results indicate that word\nclass length is significantly correlated with word order crosslinguistically,\nbut not in a straightforward manner, partially supporting opposing theories of\nprocessing, while at the same time predicting historical word order change in\ntwo different phylogenetic lines and explaining more variance than descent or\nlanguage area in regression models. Such findings suggest an integrated\n\"Min-Max\" theory of language evolution driven by competing pressures of\nprocessing and information structure, aligning with recent efficiency-oriented\n(Levshina 2023) and information-theoretic proposals (Zaslavsky 2020; Tucker et\nal. 2025)."}
{"id": "2406.06051", "pdf": "https://arxiv.org/pdf/2406.06051.pdf", "abs": "https://arxiv.org/abs/2406.06051", "title": "On the Utility of Accounting for Human Beliefs about AI Intention in Human-AI Collaboration", "authors": ["Guanghui Yu", "Robert Kasumba", "Chien-Ju Ho", "William Yeoh"], "categories": ["cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "To enable effective human-AI collaboration, merely optimizing AI performance\nwithout considering human factors is insufficient. Recent research has shown\nthat designing AI agents that take human behavior into account leads to\nimproved performance in human-AI collaboration. However, a limitation of most\nexisting approaches is their assumption that human behavior remains static,\nregardless of the AI agent's actions. In reality, humans may adjust their\nactions based on their beliefs about the AI's intentions, specifically, the\nsubtasks they perceive the AI to be attempting to complete based on its\nbehavior. In this paper, we address this limitation by enabling a collaborative\nAI agent to consider its human partner's beliefs about its intentions, i.e.,\nwhat the human partner thinks the AI agent is trying to accomplish, and to\ndesign its action plan accordingly to facilitate more effective human-AI\ncollaboration. Specifically, we developed a model of human beliefs that\ncaptures how humans interpret and reason about their AI partner's intentions.\nUsing this belief model, we created an AI agent that incorporates both human\nbehavior and human beliefs when devising its strategy for interacting with\nhumans. Through extensive real-world human-subject experiments, we demonstrate\nthat our belief model more accurately captures human perceptions of AI\nintentions. Furthermore, we show that our AI agent, designed to account for\nhuman beliefs over its intentions, significantly enhances performance in\nhuman-AI collaboration."}
{"id": "2505.13936", "pdf": "https://arxiv.org/pdf/2505.13936.pdf", "abs": "https://arxiv.org/abs/2505.13936", "title": "EEG-to-Text Translation: A Model for Deciphering Human Brain Activity", "authors": ["Saydul Akbar Murad", "Ashim Dahal", "Nick Rahimi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the rapid advancement of large language models like Gemini, GPT, and\nothers, bridging the gap between the human brain and language processing has\nbecome an important area of focus. To address this challenge, researchers have\ndeveloped various models to decode EEG signals into text. However, these models\nstill face significant performance limitations. To overcome these shortcomings,\nwe propose a new model, R1 Translator, which aims to improve the performance of\nEEG-to-text decoding. The R1 Translator model combines a bidirectional LSTM\nencoder with a pretrained transformer-based decoder, utilizing EEG features to\nproduce high-quality text outputs. The model processes EEG embeddings through\nthe LSTM to capture sequential dependencies, which are then fed into the\ntransformer decoder for effective text generation. The R1 Translator excels in\nROUGE metrics, outperforming both T5 (previous research) and Brain Translator.\nSpecifically, R1 achieves a ROUGE-1 score of 38.00% (P), which is up to 9%\nhigher than T5 (34.89%) and 3% better than Brain (35.69%). It also leads in\nROUGE-L, with a F1 score of 32.51%, outperforming T5 by 3% (29.67%) and Brain\nby 2% (30.38%). In terms of CER, R1 achieves a CER of 0.5795, which is 2% lower\nthan T5 (0.5917) and 4% lower than Brain (0.6001). Additionally, R1 performs\nbetter in WER with a score of 0.7280, outperforming T5 by 4.3% (0.7610) and\nBrain by 3.6% (0.7553). Code is available at\nhttps://github.com/Mmurrad/EEG-To-text."}
{"id": "2409.11726", "pdf": "https://arxiv.org/pdf/2409.11726.pdf", "abs": "https://arxiv.org/abs/2409.11726", "title": "Revealing and Mitigating the Challenge of Detecting Character Knowledge Errors in LLM Role-Playing", "authors": ["Wenyuan Zhang", "Shuaiyi Nie", "Jiawei Sheng", "Zefeng Zhang", "Xinghua Zhang", "Yongquan He", "Tingwen Liu"], "categories": ["cs.CL", "cs.HC"], "comment": "25 pages, 6 figures, 20 tables", "summary": "Large language model (LLM) role-playing has gained widespread attention.\nAuthentic character knowledge is crucial for constructing realistic LLM\nrole-playing agents. However, existing works usually overlook the exploration\nof LLMs' ability to detect characters' known knowledge errors (KKE) and unknown\nknowledge errors (UKE) while playing roles, which would lead to low-quality\nautomatic construction of character trainable corpus. In this paper, we propose\nRoleKE-Bench to evaluate LLMs' ability to detect errors in KKE and UKE. The\nresults indicate that even the latest LLMs struggle to detect these two types\nof errors effectively, especially when it comes to familiar knowledge. We\nexperimented with various reasoning strategies and propose an agent-based\nreasoning method, Self-Recollection and Self-Doubt (S$^2$RD), to explore\nfurther the potential for improving error detection capabilities. Experiments\nshow that our method effectively improves the LLMs' ability to detect error\ncharacter knowledge, but it remains an issue that requires ongoing attention."}
{"id": "2505.13944", "pdf": "https://arxiv.org/pdf/2505.13944.pdf", "abs": "https://arxiv.org/abs/2505.13944", "title": "Towards Rehearsal-Free Continual Relation Extraction: Capturing Within-Task Variance with Adaptive Prompting", "authors": ["Bao-Ngoc Dao", "Quang Nguyen", "Luyen Ngo Dinh", "Minh Le", "Nam Le", "Linh Ngo Van"], "categories": ["cs.CL"], "comment": null, "summary": "Memory-based approaches have shown strong performance in Continual Relation\nExtraction (CRE). However, storing examples from previous tasks increases\nmemory usage and raises privacy concerns. Recently, prompt-based methods have\nemerged as a promising alternative, as they do not rely on storing past\nsamples. Despite this progress, current prompt-based techniques face several\ncore challenges in CRE, particularly in accurately identifying task identities\nand mitigating catastrophic forgetting. Existing prompt selection strategies\noften suffer from inaccuracies, lack robust mechanisms to prevent forgetting in\nshared parameters, and struggle to handle both cross-task and within-task\nvariations. In this paper, we propose WAVE++, a novel approach inspired by the\nconnection between prefix-tuning and mixture of experts. Specifically, we\nintroduce task-specific prompt pools that enhance flexibility and adaptability\nacross diverse tasks while avoiding boundary-spanning risks; this design more\neffectively captures variations within each task and across tasks. To further\nrefine relation classification, we incorporate label descriptions that provide\nricher, more global context, enabling the model to better distinguish among\ndifferent relations. We also propose a training-free mechanism to improve task\nprediction during inference. Moreover, we integrate a generative model to\nconsolidate prior knowledge within the shared parameters, thereby removing the\nneed for explicit data storage. Extensive experiments demonstrate that WAVE++\noutperforms state-of-the-art prompt-based and rehearsal-based methods, offering\na more robust solution for continual relation extraction. Our code is publicly\navailable at https://github.com/PiDinosauR2804/WAVE-CRE-PLUS-PLUS."}
{"id": "2505.12981", "pdf": "https://arxiv.org/pdf/2505.12981.pdf", "abs": "https://arxiv.org/abs/2505.12981", "title": "From Assistants to Adversaries: Exploring the Security Risks of Mobile LLM Agents", "authors": ["Liangxuan Wu", "Chao Wang", "Tianming Liu", "Yanjie Zhao", "Haoyu Wang"], "categories": ["cs.CR", "cs.AI", "cs.HC"], "comment": null, "summary": "The growing adoption of large language models (LLMs) has led to a new\nparadigm in mobile computing--LLM-powered mobile AI agents--capable of\ndecomposing and automating complex tasks directly on smartphones. However, the\nsecurity implications of these agents remain largely unexplored. In this paper,\nwe present the first comprehensive security analysis of mobile LLM agents,\nencompassing three representative categories: System-level AI Agents developed\nby original equipment manufacturers (e.g., YOYO Assistant), Third-party\nUniversal Agents (e.g., Zhipu AI AutoGLM), and Emerging Agent Frameworks (e.g.,\nAlibaba Mobile Agent). We begin by analyzing the general workflow of mobile\nagents and identifying security threats across three core capability\ndimensions: language-based reasoning, GUI-based interaction, and system-level\nexecution. Our analysis reveals 11 distinct attack surfaces, all rooted in the\nunique capabilities and interaction patterns of mobile LLM agents, and spanning\ntheir entire operational lifecycle. To investigate these threats in practice,\nwe introduce AgentScan, a semi-automated security analysis framework that\nsystematically evaluates mobile LLM agents across all 11 attack scenarios.\nApplying AgentScan to nine widely deployed agents, we uncover a concerning\ntrend: every agent is vulnerable to targeted attacks. In the most severe cases,\nagents exhibit vulnerabilities across eight distinct attack vectors. These\nattacks can cause behavioral deviations, privacy leakage, or even full\nexecution hijacking. Based on these findings, we propose a set of defensive\ndesign principles and practical recommendations for building secure mobile LLM\nagents. Our disclosures have received positive feedback from two major device\nvendors. Overall, this work highlights the urgent need for standardized\nsecurity practices in the fast-evolving landscape of LLM-driven mobile\nautomation."}
{"id": "2505.13948", "pdf": "https://arxiv.org/pdf/2505.13948.pdf", "abs": "https://arxiv.org/abs/2505.13948", "title": "Memory-Centric Embodied Question Answer", "authors": ["Mingliang Zhai", "Zhi Gao", "Yuwei Wu", "Yunde Jia"], "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": "14pages, 7 figures, 6 tables", "summary": "Embodied Question Answering (EQA) requires agents to autonomously explore and\nunderstand the environment to answer context-dependent questions. Existing\nframeworks typically center around the planner, which guides the stopping\nmodule, memory module, and answering module for reasoning. In this paper, we\npropose a memory-centric EQA framework named MemoryEQA. Unlike planner-centric\nEQA models where the memory module cannot fully interact with other modules,\nMemoryEQA flexible feeds memory information into all modules, thereby enhancing\nefficiency and accuracy in handling complex tasks, such as those involving\nmultiple targets across different regions. Specifically, we establish a\nmulti-modal hierarchical memory mechanism, which is divided into global memory\nthat stores language-enhanced scene maps, and local memory that retains\nhistorical observations and state information. When performing EQA tasks, the\nmulti-modal large language model is leveraged to convert memory information\ninto the required input formats for injection into different modules. To\nevaluate EQA models' memory capabilities, we constructed the MT-HM3D dataset\nbased on HM3D, comprising 1,587 question-answer pairs involving multiple\ntargets across various regions, which requires agents to maintain memory of\nexploration-acquired target information. Experimental results on HM-EQA,\nMT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a\n19.8% performance gain on MT-HM3D compared to baseline model further\nunderscores memory capability's pivotal role in resolving complex tasks."}
{"id": "2505.13949", "pdf": "https://arxiv.org/pdf/2505.13949.pdf", "abs": "https://arxiv.org/abs/2505.13949", "title": "FlashThink: An Early Exit Method For Efficient Reasoning", "authors": ["Guochao Jiang", "Guofeng Quan", "Zepeng Ding", "Ziqin Luo", "Dixuan Wang", "Zheng Hu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown impressive performance in reasoning\ntasks. However, LLMs tend to generate excessively long reasoning content,\nleading to significant computational overhead. Our observations indicate that\neven on simple problems, LLMs tend to produce unnecessarily lengthy reasoning\ncontent, which is against intuitive expectations. Preliminary experiments show\nthat at a certain point during the generation process, the model is already\ncapable of producing the correct solution without completing the full reasoning\ncontent. Therefore, we consider that the reasoning process of the model can be\nexited early to achieve the purpose of efficient reasoning. We introduce a\nverification model that identifies the exact moment when the model can stop\nreasoning and still provide the correct answer. Comprehensive experiments on\nfour different benchmarks demonstrate that our proposed method, FlashThink,\neffectively shortens the reasoning content while preserving the model accuracy.\nFor the Deepseek-R1 and QwQ-32B models, we reduced the length of reasoning\ncontent by 77.04% and 77.47%, respectively, without reducing the accuracy."}
{"id": "2505.13963", "pdf": "https://arxiv.org/pdf/2505.13963.pdf", "abs": "https://arxiv.org/abs/2505.13963", "title": "Through a Compressed Lens: Investigating the Impact of Quantization on LLM Explainability and Interpretability", "authors": ["Qianli Wang", "Mingyang Wang", "Nils Feldhus", "Simon Ostermann", "Yuan Cao", "Hinrich Schütze", "Sebastian Möller", "Vera Schmitt"], "categories": ["cs.CL", "cs.LG"], "comment": "In submission", "summary": "Quantization methods are widely used to accelerate inference and streamline\nthe deployment of large language models (LLMs). While prior research has\nextensively investigated the degradation of various LLM capabilities due to\nquantization, its effects on model explainability and interpretability, which\nare crucial for understanding decision-making processes, remain unexplored. To\naddress this gap, we conduct comprehensive experiments using three common\nquantization techniques at distinct bit widths, in conjunction with two\nexplainability methods, counterfactual examples and natural language\nexplanations, as well as two interpretability approaches, knowledge\nmemorization analysis and latent multi-hop reasoning analysis. We complement\nour analysis with a thorough user study, evaluating selected explainability\nmethods. Our findings reveal that, depending on the configuration, quantization\ncan significantly impact model explainability and interpretability. Notably,\nthe direction of this effect is not consistent, as it strongly depends on (1)\nthe quantization method, (2) the explainability or interpretability approach,\nand (3) the evaluation protocol. In some settings, human evaluation shows that\nquantization degrades explainability, while in others, it even leads to\nimprovements. Our work serves as a cautionary tale, demonstrating that\nquantization can unpredictably affect model transparency. This insight has\nimportant implications for deploying LLMs in applications where transparency is\na critical requirement."}
{"id": "2505.13965", "pdf": "https://arxiv.org/pdf/2505.13965.pdf", "abs": "https://arxiv.org/abs/2505.13965", "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring", "authors": ["Jiamin Su", "Yibo Yan", "Zhuoran Gao", "Han Zhang", "Xiang Liu", "Xuming Hu"], "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2502.11916", "summary": "Automated Essay Scoring (AES) is crucial for modern education, particularly\nwith the increasing prevalence of multimodal assessments. However, traditional\nAES methods struggle with evaluation generalizability and multimodal\nperception, while even recent Multimodal Large Language Model (MLLM)-based\napproaches can produce hallucinated justifications and scores misaligned with\nhuman judgment. To address the limitations, we introduce CAFES, the first\ncollaborative multi-agent framework specifically designed for AES. It\norchestrates three specialized agents: an Initial Scorer for rapid,\ntrait-specific evaluations; a Feedback Pool Manager to aggregate detailed,\nevidence-grounded strengths; and a Reflective Scorer that iteratively refines\nscores based on this feedback to enhance human alignment. Extensive\nexperiments, using state-of-the-art MLLMs, achieve an average relative\nimprovement of 21% in Quadratic Weighted Kappa (QWK) against ground truth,\nespecially for grammatical and lexical diversity. Our proposed CAFES framework\npaves the way for an intelligent multimodal AES system. The code will be\navailable upon acceptance."}
{"id": "2505.13972", "pdf": "https://arxiv.org/pdf/2505.13972.pdf", "abs": "https://arxiv.org/abs/2505.13972", "title": "Truth or Twist? Optimal Model Selection for Reliable Label Flipping Evaluation in LLM-based Counterfactuals", "authors": ["Qianli Wang", "Van Bach Nguyen", "Nils Feldhus", "Luis Felipe Villa-Arenas", "Christin Seifert", "Sebastian Möller", "Vera Schmitt"], "categories": ["cs.CL"], "comment": "in submission", "summary": "Counterfactual examples are widely employed to enhance the performance and\nrobustness of large language models (LLMs) through counterfactual data\naugmentation (CDA). However, the selection of the judge model used to evaluate\nlabel flipping, the primary metric for assessing the validity of generated\ncounterfactuals for CDA, yields inconsistent results. To decipher this, we\ndefine four types of relationships between the counterfactual generator and\njudge models. Through extensive experiments involving two state-of-the-art\nLLM-based methods, three datasets, five generator models, and 15 judge models,\ncomplemented by a user study (n = 90), we demonstrate that judge models with an\nindependent, non-fine-tuned relationship to the generator model provide the\nmost reliable label flipping evaluations. Relationships between the generator\nand judge models, which are closely aligned with the user study for CDA, result\nin better model performance and robustness. Nevertheless, we find that the gap\nbetween the most effective judge models and the results obtained from the user\nstudy remains considerably large. This suggests that a fully automated pipeline\nfor CDA may be inadequate and requires human intervention."}
{"id": "2505.13973", "pdf": "https://arxiv.org/pdf/2505.13973.pdf", "abs": "https://arxiv.org/abs/2505.13973", "title": "Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models", "authors": ["Wenhui Zhu", "Xuanzhao Dong", "Xin Li", "Peijie Qiu", "Xiwen Chen", "Abolfazl Razi", "Aris Sotiras", "Yi Su", "Yalin Wang"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Recently, reinforcement learning (RL)-based tuning has shifted the trajectory\nof Multimodal Large Language Models (MLLMs), particularly following the\nintroduction of Group Relative Policy Optimization (GRPO). However, directly\napplying it to medical tasks remains challenging for achieving clinically\ngrounded model behavior. Motivated by the need to align model response with\nclinical expectations, we investigate four critical dimensions that affect the\neffectiveness of RL-based tuning in medical visual question answering (VQA):\nbase model initialization strategy, the role of medical semantic alignment, the\nimpact of length-based rewards on long-chain reasoning, and the influence of\nbias. We conduct extensive experiments to analyze these factors for medical\nMLLMs, providing new insights into how models are domain-specifically\nfine-tuned. Additionally, our results also demonstrate that GRPO-based RL\ntuning consistently outperforms standard supervised fine-tuning (SFT) in both\naccuracy and reasoning quality."}
{"id": "2505.13975", "pdf": "https://arxiv.org/pdf/2505.13975.pdf", "abs": "https://arxiv.org/abs/2505.13975", "title": "DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models", "authors": ["Yuxuan Jiang", "Dawei Li", "Frank Ferraro"], "categories": ["cs.CL"], "comment": null, "summary": "While Large Reasoning Models (LRMs) have demonstrated success in complex\nreasoning tasks through long chain-of-thought (CoT) reasoning, their inference\noften involves excessively verbose reasoning traces, resulting in substantial\ninefficiency. To address this, we propose Distilled Reasoning Pruning (DRP), a\nhybrid framework that combines inference-time pruning with tuning-based\ndistillation, two widely used strategies for efficient reasoning. DRP uses a\nteacher model to perform skill-aware step decomposition and content pruning,\nand then distills the pruned reasoning paths into a student model, enabling it\nto reason both efficiently and accurately. Across several challenging\nmathematical reasoning datasets, we find that models trained with DRP achieve\nsubstantial improvements in token efficiency without sacrificing accuracy.\nSpecifically, DRP reduces average token usage on GSM8K from 917 to 328 while\nimproving accuracy from 91.7% to 94.1%, and achieves a 43% token reduction on\nAIME with no performance drop. Further analysis shows that aligning the\nreasoning structure of training CoTs with the student's reasoning capacity is\ncritical for effective knowledge transfer and performance gains."}
{"id": "2505.13979", "pdf": "https://arxiv.org/pdf/2505.13979.pdf", "abs": "https://arxiv.org/abs/2505.13979", "title": "Mixed Signals: Understanding Model Disagreement in Multimodal Empathy Detection", "authors": ["Maya Srikanth", "Run Chen", "Julia Hirschberg"], "categories": ["cs.CL"], "comment": null, "summary": "Multimodal models play a key role in empathy detection, but their performance\ncan suffer when modalities provide conflicting cues. To understand these\nfailures, we examine cases where unimodal and multimodal predictions diverge.\nUsing fine-tuned models for text, audio, and video, along with a gated fusion\nmodel, we find that such disagreements often reflect underlying ambiguity, as\nevidenced by annotator uncertainty. Our analysis shows that dominant signals in\none modality can mislead fusion when unsupported by others. We also observe\nthat humans, like models, do not consistently benefit from multimodal input.\nThese insights position disagreement as a useful diagnostic signal for\nidentifying challenging examples and improving empathy system robustness."}
{"id": "2505.13988", "pdf": "https://arxiv.org/pdf/2505.13988.pdf", "abs": "https://arxiv.org/abs/2505.13988", "title": "The Hallucination Tax of Reinforcement Finetuning", "authors": ["Linxin Song", "Taiwei Shi", "Jieyu Zhao"], "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement finetuning (RFT) has become a standard approach for enhancing\nthe reasoning capabilities of large language models (LLMs). However, its impact\non model trustworthiness remains underexplored. In this work, we identify and\nsystematically study a critical side effect of RFT, which we term the\nhallucination tax: a degradation in refusal behavior causing models to produce\nhallucinated answers to unanswerable questions confidently. To investigate\nthis, we introduce SUM (Synthetic Unanswerable Math), a high-quality dataset of\nunanswerable math problems designed to probe models' ability to recognize an\nunanswerable question by reasoning from the insufficient or ambiguous\ninformation. Our results show that standard RFT training could reduce model\nrefusal rates by more than 80%, which significantly increases model's tendency\nto hallucinate. We further demonstrate that incorporating just 10% SUM during\nRFT substantially restores appropriate refusal behavior, with minimal accuracy\ntrade-offs on solvable tasks. Crucially, this approach enables LLMs to leverage\ninference-time compute to reason about their own uncertainty and knowledge\nboundaries, improving generalization not only to out-of-domain math problems\nbut also to factual question answering tasks."}
{"id": "2505.13990", "pdf": "https://arxiv.org/pdf/2505.13990.pdf", "abs": "https://arxiv.org/abs/2505.13990", "title": "DecIF: Improving Instruction-Following through Meta-Decomposition", "authors": ["Tingfeng Hui", "Pengyu Zhu", "Bowen Ping", "Ling Tang", "Yaqi Zhang", "Sen Su"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Instruction-following has emerged as a crucial capability for large language\nmodels (LLMs). However, existing approaches often rely on pre-existing\ndocuments or external resources to synthesize instruction-following data, which\nlimits their flexibility and generalizability. In this paper, we introduce\nDecIF, a fully autonomous, meta-decomposition guided framework that generates\ndiverse and high-quality instruction-following data using only LLMs. DecIF is\ngrounded in the principle of decomposition. For instruction generation, we\nguide LLMs to iteratively produce various types of meta-information, which are\nthen combined with response constraints to form well-structured and\nsemantically rich instructions. We further utilize LLMs to detect and resolve\npotential inconsistencies within the generated instructions. Regarding response\ngeneration, we decompose each instruction into atomic-level evaluation\ncriteria, enabling rigorous validation and the elimination of inaccurate\ninstruction-response pairs. Extensive experiments across a wide range of\nscenarios and settings demonstrate DecIF's superior performance on\ninstruction-following tasks. Further analysis highlights its strong\nflexibility, scalability, and generalizability in automatically synthesizing\nhigh-quality instruction data."}
{"id": "2505.13995", "pdf": "https://arxiv.org/pdf/2505.13995.pdf", "abs": "https://arxiv.org/abs/2505.13995", "title": "Social Sycophancy: A Broader Understanding of LLM Sycophancy", "authors": ["Myra Cheng", "Sunny Yu", "Cinoo Lee", "Pranav Khadpe", "Lujain Ibrahim", "Dan Jurafsky"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "A serious risk to the safety and utility of LLMs is sycophancy, i.e.,\nexcessive agreement with and flattery of the user. Yet existing work focuses on\nonly one aspect of sycophancy: agreement with users' explicitly stated beliefs\nthat can be compared to a ground truth. This overlooks forms of sycophancy that\narise in ambiguous contexts such as advice and support-seeking, where there is\nno clear ground truth, yet sycophancy can reinforce harmful implicit\nassumptions, beliefs, or actions. To address this gap, we introduce a richer\ntheory of social sycophancy in LLMs, characterizing sycophancy as the excessive\npreservation of a user's face (the positive self-image a person seeks to\nmaintain in an interaction). We present ELEPHANT, a framework for evaluating\nsocial sycophancy across five face-preserving behaviors (emotional validation,\nmoral endorsement, indirect language, indirect action, and accepting framing)\non two datasets: open-ended questions (OEQ) and Reddit's r/AmITheAsshole\n(AITA). Across eight models, we show that LLMs consistently exhibit high rates\nof social sycophancy: on OEQ, they preserve face 47% more than humans, and on\nAITA, they affirm behavior deemed inappropriate by crowdsourced human judgments\nin 42% of cases. We further show that social sycophancy is rewarded in\npreference datasets and is not easily mitigated. Our work provides theoretical\ngrounding and empirical tools (datasets and code) for understanding and\naddressing this under-recognized but consequential issue."}
{"id": "2505.14009", "pdf": "https://arxiv.org/pdf/2505.14009.pdf", "abs": "https://arxiv.org/abs/2505.14009", "title": "Activation-Guided Consensus Merging for Large Language Models", "authors": ["Yuxuan Yao", "Shuqi Liu", "Zehua Liu", "Qintong Li", "Mingyang Liu", "Xiongwei Han", "Zhijiang Guo", "Han Wu", "Linqi Song"], "categories": ["cs.CL"], "comment": null, "summary": "Recent research has increasingly focused on reconciling the reasoning\ncapabilities of System 2 with the efficiency of System 1. While existing\ntraining-based and prompt-based approaches face significant challenges in terms\nof efficiency and stability, model merging emerges as a promising strategy to\nintegrate the diverse capabilities of different Large Language Models (LLMs)\ninto a unified model. However, conventional model merging methods often assume\nuniform importance across layers, overlooking the functional heterogeneity\ninherent in neural components. To address this limitation, we propose\n\\textbf{A}ctivation-Guided \\textbf{C}onsensus \\textbf{M}erging (\\textbf{ACM}),\na plug-and-play merging framework that determines layer-specific merging\ncoefficients based on mutual information between activations of pre-trained and\nfine-tuned models. ACM effectively preserves task-specific capabilities without\nrequiring gradient computations or additional training. Extensive experiments\non Long-to-Short (L2S) and general merging tasks demonstrate that ACM\nconsistently outperforms all baseline methods. For instance, in the case of\nQwen-7B models, TIES-Merging equipped with ACM achieves a \\textbf{55.3\\%}\nreduction in response length while simultaneously improving reasoning accuracy\nby \\textbf{1.3} points. We submit the code with the paper for reproducibility,\nand it will be publicly available."}
{"id": "2505.14015", "pdf": "https://arxiv.org/pdf/2505.14015.pdf", "abs": "https://arxiv.org/abs/2505.14015", "title": "AUTOLAW: Enhancing Legal Compliance in Large Language Models via Case Law Generation and Jury-Inspired Deliberation", "authors": ["Tai D. Nguyen", "Long H. Pham", "Jun Sun"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of domain-specific large language models (LLMs) in\nfields like law necessitates frameworks that account for nuanced regional legal\ndistinctions, which are critical for ensuring compliance and trustworthiness.\nExisting legal evaluation benchmarks often lack adaptability and fail to\naddress diverse local contexts, limiting their utility in dynamically evolving\nregulatory landscapes. To address these gaps, we propose AutoLaw, a novel\nviolation detection framework that combines adversarial data generation with a\njury-inspired deliberation process to enhance legal compliance of LLMs. Unlike\nstatic approaches, AutoLaw dynamically synthesizes case law to reflect local\nregulations and employs a pool of LLM-based \"jurors\" to simulate judicial\ndecision-making. Jurors are ranked and selected based on synthesized legal\nexpertise, enabling a deliberation process that minimizes bias and improves\ndetection accuracy. Evaluations across three benchmarks: Law-SG, Case-SG\n(legality), and Unfair-TOS (policy), demonstrate AutoLaw's effectiveness:\nadversarial data generation improves LLM discrimination, while the jury-based\nvoting strategy significantly boosts violation detection rates. Our results\nhighlight the framework's ability to adaptively probe legal misalignments and\ndeliver reliable, context-aware judgments, offering a scalable solution for\nevaluating and enhancing LLMs in legally sensitive applications."}
{"id": "2505.14045", "pdf": "https://arxiv.org/pdf/2505.14045.pdf", "abs": "https://arxiv.org/abs/2505.14045", "title": "From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora", "authors": ["Yingli Shen", "Wen Lai", "Shuo Wang", "Kangyang Luo", "Alexander Fraser", "Maosong Sun"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Continued pretraining and instruction tuning on large-scale multilingual data\nhave proven to be effective in scaling large language models (LLMs) to\nlow-resource languages. However, the unaligned nature of such data limits its\nability to effectively capture cross-lingual semantics. In contrast, multi-way\nparallel data, where identical content is aligned across multiple languages,\nprovides stronger cross-lingual consistency and offers greater potential for\nimproving multilingual performance. In this paper, we introduce a large-scale,\nhigh-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus\nspans 113 languages, with up to 50 languages aligned in parallel, ensuring\nextensive multilingual coverage. Using this dataset, we investigate best\npractices for leveraging multi-way parallel data to enhance LLMs, including\nstrategies for continued pretraining, instruction tuning, and the analysis of\nkey influencing factors. Experiments on six multilingual benchmarks show that\nmodels trained on multiway parallel data consistently outperform those trained\non unaligned multilingual data."}
{"id": "2505.14052", "pdf": "https://arxiv.org/pdf/2505.14052.pdf", "abs": "https://arxiv.org/abs/2505.14052", "title": "Improved Methods for Model Pruning and Knowledge Distillation", "authors": ["Wei Jiang", "Anying Fu", "Youling Zhang"], "categories": ["cs.CL", "cs.CE"], "comment": null, "summary": "Model pruning is a performance optimization technique for large language\nmodels like R1 or o3-mini. However, existing pruning methods often lead to\nsignificant performance degradation or require extensive retraining and\nfine-tuning. This technique aims to identify and remove neurons, connections\nunlikely leading to the contribution during the human-computer interaction\nphase. Our goal is to obtain a much smaller and faster knowledge distilled\nmodel that can quickly generate content almost as good as those of the unpruned\nones. We propose MAMA Pruning, short for Movement and Magnitude Analysis, an\nimproved pruning method that effectively reduces model size and computational\ncomplexity while maintaining performance comparable to the original unpruned\nmodel even at extreme pruned levels. The improved method is based on weights,\nbias fixed in the pre-training phase and GRPO rewards verified during the\npost-training phase as our novel pruning indicators. Preliminary experimental\nresults show that our method outperforms and be comparable to state-of-the-art\nmethods across various pruning levels and different downstream computational\nlinguistics tasks."}
{"id": "2505.14070", "pdf": "https://arxiv.org/pdf/2505.14070.pdf", "abs": "https://arxiv.org/abs/2505.14070", "title": "Enhancing LLMs via High-Knowledge Data Selection", "authors": ["Feiyu Duan", "Xuemiao Zhang", "Sirui Wang", "Haoran Que", "Yuqi Liu", "Wenge Rong", "Xunliang Cai"], "categories": ["cs.CL"], "comment": null, "summary": "The performance of Large Language Models (LLMs) is intrinsically linked to\nthe quality of its training data. Although several studies have proposed\nmethods for high-quality data selection, they do not consider the importance of\nknowledge richness in text corpora. In this paper, we propose a novel and\ngradient-free High-Knowledge Scorer (HKS) to select high-quality data from the\ndimension of knowledge, to alleviate the problem of knowledge scarcity in the\npre-trained corpus. We propose a comprehensive multi-domain knowledge element\npool and introduce knowledge density and coverage as metrics to assess the\nknowledge content of the text. Based on this, we propose a comprehensive\nknowledge scorer to select data with intensive knowledge, which can also be\nutilized for domain-specific high-knowledge data selection by restricting\nknowledge elements to the specific domain. We train models on a high-knowledge\nbilingual dataset, and experimental results demonstrate that our scorer\nimproves the model's performance in knowledge-intensive and general\ncomprehension tasks, and is effective in enhancing both the generic and\ndomain-specific capabilities of the model."}
{"id": "2505.14079", "pdf": "https://arxiv.org/pdf/2505.14079.pdf", "abs": "https://arxiv.org/abs/2505.14079", "title": "BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks", "authors": ["Weihong Du", "Wenrui Liao", "Binyu Yan", "Hongru Liang", "Anthony G. Cohn", "Wenqiang Lei"], "categories": ["cs.CL"], "comment": null, "summary": "Large language model (LLM) based agents have shown great potential in\nfollowing human instructions and automatically completing various tasks. To\ncomplete a task, the agent needs to decompose it into easily executed steps by\nplanning. Existing studies mainly conduct the planning by inferring what steps\nshould be executed next starting from the agent's initial state. However, this\nforward reasoning paradigm doesn't work well for complex tasks. We propose to\nstudy this issue in Minecraft, a virtual environment that simulates complex\ntasks based on real-world scenarios. We believe that the failure of forward\nreasoning is caused by the big perception gap between the agent's initial state\nand task goal. To this end, we leverage backward reasoning and make the\nplanning starting from the terminal state, which can directly achieve the task\ngoal in one step. Specifically, we design a BAckward Reasoning based agent\n(BAR). It is equipped with a recursive goal decomposition module, a state\nconsistency maintaining module and a stage memory module to make robust,\nconsistent, and efficient planning starting from the terminal state.\nExperimental results demonstrate the superiority of BAR over existing methods\nand the effectiveness of proposed modules."}
{"id": "2505.14080", "pdf": "https://arxiv.org/pdf/2505.14080.pdf", "abs": "https://arxiv.org/abs/2505.14080", "title": "Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory", "authors": ["Franziska Sofia Hafner", "Ana Valdivia", "Luc Rocher"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Language models encode and subsequently perpetuate harmful gendered\nstereotypes. Research has succeeded in mitigating some of these harms, e.g. by\ndissociating non-gendered terms such as occupations from gendered terms such as\n'woman' and 'man'. This approach, however, remains superficial given that\nassociations are only one form of prejudice through which gendered harms arise.\nCritical scholarship on gender, such as gender performativity theory,\nemphasizes how harms often arise from the construction of gender itself, such\nas conflating gender with biological sex. In language models, these issues\ncould lead to the erasure of transgender and gender diverse identities and\ncause harms in downstream applications, from misgendering users to\nmisdiagnosing patients based on wrong assumptions about their anatomy.\n  For FAccT research on gendered harms to go beyond superficial linguistic\nassociations, we advocate for a broader definition of 'gender bias' in language\nmodels. We operationalize insights on the construction of gender through\nlanguage from gender studies literature and then empirically test how 16\nlanguage models of different architectures, training datasets, and model sizes\nencode gender. We find that language models tend to encode gender as a binary\ncategory tied to biological sex, and that gendered terms that do not neatly\nfall into one of these binary categories are erased and pathologized. Finally,\nwe show that larger models, which achieve better results on performance\nbenchmarks, learn stronger associations between gender and sex, further\nreinforcing a narrow understanding of gender. Our findings lead us to call for\na re-evaluation of how gendered harms in language models are defined and\naddressed."}
{"id": "2505.14099", "pdf": "https://arxiv.org/pdf/2505.14099.pdf", "abs": "https://arxiv.org/abs/2505.14099", "title": "Beyond Chains: Bridging Large Language Models and Knowledge Bases in Complex Question Answering", "authors": ["Yihua Zhu", "Qianying Liu", "Akiko Aizawa", "Hidetoshi Shimodaira"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions using structured knowledge from KBs. While LLM-only approaches offer\ngeneralization, they suffer from outdated knowledge, hallucinations, and lack\nof transparency. Chain-based KG-RAG methods address these issues by\nincorporating external KBs, but are limited to simple chain-structured\nquestions due to the absence of planning and logical structuring. Inspired by\nsemantic parsing methods, we propose PDRR: a four-stage framework consisting of\nPredict, Decompose, Retrieve, and Reason. Our method first predicts the\nquestion type and decomposes the question into structured triples. Then\nretrieves relevant information from KBs and guides the LLM as an agent to\nreason over and complete the decomposed triples. Experimental results\ndemonstrate that PDRR consistently outperforms existing methods across various\nLLM backbones and achieves superior performance on both chain-structured and\nnon-chain complex questions."}
{"id": "2505.14101", "pdf": "https://arxiv.org/pdf/2505.14101.pdf", "abs": "https://arxiv.org/abs/2505.14101", "title": "MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations", "authors": ["Ernests Lavrinovics", "Russa Biswas", "Katja Hose", "Johannes Bjerva"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have inherent limitations of faithfulness and\nfactuality, commonly referred to as hallucinations. Several benchmarks have\nbeen developed that provide a test bed for factuality evaluation within the\ncontext of English-centric datasets, while relying on supplementary informative\ncontext like web links or text passages but ignoring the available structured\nfactual resources. To this end, Knowledge Graphs (KGs) have been identified as\na useful aid for hallucination mitigation, as they provide a structured way to\nrepresent the facts about entities and their relations with minimal linguistic\noverhead. We bridge the lack of KG paths and multilinguality for factual\nlanguage modeling within the existing hallucination evaluation benchmarks and\npropose a KG-based multilingual, multihop benchmark called \\textbf{MultiHal}\nframed for generative text evaluation. As part of our data collection pipeline,\nwe mined 140k KG-paths from open-domain KGs, from which we pruned noisy\nKG-paths, curating a high-quality subset of 25.9k. Our baseline evaluation\nshows an absolute scale increase by approximately 0.12 to 0.36 points for the\nsemantic similarity score in KG-RAG over vanilla QA across multiple languages\nand multiple models, demonstrating the potential of KG integration. We\nanticipate MultiHal will foster future research towards several graph-based\nhallucination mitigation and fact-checking tasks."}
{"id": "2505.14104", "pdf": "https://arxiv.org/pdf/2505.14104.pdf", "abs": "https://arxiv.org/abs/2505.14104", "title": "Legal Rule Induction: Towards Generalizable Principle Discovery from Analogous Judicial Precedents", "authors": ["Wei Fan", "Tianshi Zheng", "Yiran Hu", "Zheye Deng", "Weiqi Wang", "Baixuan Xu", "Chunyang Li", "Haoran Li", "Weixing Shen", "Yangqiu Song"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "Legal rules encompass not only codified statutes but also implicit\nadjudicatory principles derived from precedents that contain discretionary\nnorms, social morality, and policy. While computational legal research has\nadvanced in applying established rules to cases, inducing legal rules from\njudicial decisions remains understudied, constrained by limitations in model\ninference efficacy and symbolic reasoning capability. The advent of Large\nLanguage Models (LLMs) offers unprecedented opportunities for automating the\nextraction of such latent principles, yet progress is stymied by the absence of\nformal task definitions, benchmark datasets, and methodologies. To address this\ngap, we formalize Legal Rule Induction (LRI) as the task of deriving concise,\ngeneralizable doctrinal rules from sets of analogous precedents, distilling\ntheir shared preconditions, normative behaviors, and legal consequences. We\nintroduce the first LRI benchmark, comprising 5,121 case sets (38,088 Chinese\ncases in total) for model tuning and 216 expert-annotated gold test sets.\nExperimental results reveal that: 1) State-of-the-art LLMs struggle with\nover-generalization and hallucination; 2) Training on our dataset markedly\nenhances LLMs capabilities in capturing nuanced rule patterns across similar\ncases."}
{"id": "2505.14106", "pdf": "https://arxiv.org/pdf/2505.14106.pdf", "abs": "https://arxiv.org/abs/2505.14106", "title": "A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations", "authors": ["Li Li", "Peilin Cai", "Ryan A. Rossi", "Franck Dernoncourt", "Branislav Kveton", "Junda Wu", "Tong Yu", "Linxin Song", "Tiankai Yang", "Yuehan Qin", "Nesreen K. Ahmed", "Samyadeep Basu", "Subhojyoti Mukherjee", "Ruiyi Zhang", "Zhengmian Hu", "Bo Ni", "Yuxiao Zhou", "Zichao Wang", "Yue Huang", "Yu Wang", "Xiangliang Zhang", "Philip S. Yu", "Xiyang Hu", "Yue Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present PersonaConvBench, a large-scale benchmark for evaluating\npersonalized reasoning and generation in multi-turn conversations with large\nlanguage models (LLMs). Unlike existing work that focuses on either\npersonalization or conversational structure in isolation, PersonaConvBench\nintegrates both, offering three core tasks: sentence classification, impact\nregression, and user-centric text generation across ten diverse Reddit-based\ndomains. This design enables systematic analysis of how personalized\nconversational context shapes LLM outputs in realistic multi-user scenarios. We\nbenchmark several commercial and open-source LLMs under a unified prompting\nsetup and observe that incorporating personalized history yields substantial\nperformance improvements, including a 198 percent relative gain over the best\nnon-conversational baseline in sentiment classification. By releasing\nPersonaConvBench with evaluations and code, we aim to support research on LLMs\nthat adapt to individual styles, track long-term context, and produce\ncontextually rich, engaging responses."}
{"id": "2505.14107", "pdf": "https://arxiv.org/pdf/2505.14107.pdf", "abs": "https://arxiv.org/abs/2505.14107", "title": "DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models", "authors": ["Yakun Zhu", "Zhongzhen Huang", "Linjie Mu", "Yutong Huang", "Wei Nie", "Shaoting Zhang", "Pengfei Liu", "Xiaofan Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The emergence of groundbreaking large language models capable of performing\ncomplex reasoning tasks holds significant promise for addressing various\nscientific challenges, including those arising in complex clinical scenarios.\nTo enable their safe and effective deployment in real-world healthcare\nsettings, it is urgently necessary to benchmark the diagnostic capabilities of\ncurrent models systematically. Given the limitations of existing medical\nbenchmarks in evaluating advanced diagnostic reasoning, we present\nDiagnosisArena, a comprehensive and challenging benchmark designed to\nrigorously assess professional-level diagnostic competence. DiagnosisArena\nconsists of 1,113 pairs of segmented patient cases and corresponding diagnoses,\nspanning 28 medical specialties, deriving from clinical case reports published\nin 10 top-tier medical journals. The benchmark is developed through a\nmeticulous construction pipeline, involving multiple rounds of screening and\nreview by both AI systems and human experts, with thorough checks conducted to\nprevent data leakage. Our study reveals that even the most advanced reasoning\nmodels, o3-mini, o1, and DeepSeek-R1, achieve only 45.82%, 31.09%, and 17.79%\naccuracy, respectively. This finding highlights a significant generalization\nbottleneck in current large language models when faced with clinical diagnostic\nreasoning challenges. Through DiagnosisArena, we aim to drive further\nadvancements in AIs diagnostic reasoning capabilities, enabling more effective\nsolutions for real-world clinical diagnostic challenges. We provide the\nbenchmark and evaluation tools for further research and development\nhttps://github.com/SPIRAL-MED/DiagnosisArena."}
{"id": "2505.14112", "pdf": "https://arxiv.org/pdf/2505.14112.pdf", "abs": "https://arxiv.org/abs/2505.14112", "title": "Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM Watermarking", "authors": ["Tianle Gu", "Zongqi Wang", "Kexin Huang", "Yuanqi Yao", "Xiangliang Zhang", "Yujiu Yang", "Xiuying Chen"], "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Logit-based LLM watermarking traces and verifies AI-generated content by\nmaintaining green and red token lists and increasing the likelihood of green\ntokens during generation. However, it fails in low-entropy scenarios, where\npredictable outputs make green token selection difficult without disrupting\nnatural text flow. Existing approaches address this by assuming access to the\noriginal LLM to calculate entropy and selectively watermark high-entropy\ntokens. However, these methods face two major challenges: (1) high\ncomputational costs and detection delays due to reliance on the original LLM,\nand (2) potential risks of model leakage. To address these limitations, we\npropose Invisible Entropy (IE), a watermarking paradigm designed to enhance\nboth safety and efficiency. Instead of relying on the original LLM, IE\nintroduces a lightweight feature extractor and an entropy tagger to predict\nwhether the entropy of the next token is high or low. Furthermore, based on\ntheoretical analysis, we develop a threshold navigator that adaptively sets\nentropy thresholds. It identifies a threshold where the watermark ratio\ndecreases as the green token count increases, enhancing the naturalness of the\nwatermarked text and improving detection robustness. Experiments on HumanEval\nand MBPP datasets demonstrate that IE reduces parameter size by 99\\% while\nachieving performance on par with state-of-the-art methods. Our work introduces\na safe and efficient paradigm for low-entropy watermarking.\nhttps://github.com/Carol-gutianle/IE\nhttps://huggingface.co/datasets/Carol0110/IE-Tagger"}
{"id": "2505.14116", "pdf": "https://arxiv.org/pdf/2505.14116.pdf", "abs": "https://arxiv.org/abs/2505.14116", "title": "Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst", "authors": ["Hongru Wang", "Deng Cai", "Wanjun Zhong", "Shijue Huang", "Jeff Z. Pan", "Zeming Liu", "Kam-Fai Wong"], "categories": ["cs.CL"], "comment": null, "summary": "Inference-time scaling has attracted much attention which significantly\nenhance the performance of Large Language Models (LLMs) in complex reasoning\ntasks by increasing the length of Chain-of-Thought. These longer intermediate\nreasoning rationales embody various meta-reasoning skills in human cognition,\nsuch as reflection and decomposition, being difficult to create and acquire. In\nthis work, we introduce \\textit{Self-Reasoning Language Model} (SRLM), where\nthe model itself can synthesize longer CoT data and iteratively improve\nperformance through self-training. By incorporating a few demonstration\nexamples (i.e., 1,000 samples) on how to unfold hidden reasoning chains from\nexisting responses, which act as a reasoning catalyst, we demonstrate that SRLM\nnot only enhances the model's initial performance but also ensures more stable\nand consistent improvements in subsequent iterations. Our proposed SRLM\nachieves an average absolute improvement of more than $+2.5$ points across five\nreasoning tasks: MMLU, GSM8K, ARC-C, HellaSwag, and BBH on two backbone models.\nMoreover, it brings more improvements with more times of sampling during\ninference, such as absolute $+7.89$ average improvement with $64$ sampling\ntimes, revealing the in-depth, diverse and creative reasoning paths in SRLM\nagainst the strong baseline."}
{"id": "2505.14130", "pdf": "https://arxiv.org/pdf/2505.14130.pdf", "abs": "https://arxiv.org/abs/2505.14130", "title": "Probing BERT for German Compound Semantics", "authors": ["Filip Miletić", "Aaron Schmid", "Sabine Schulte im Walde"], "categories": ["cs.CL"], "comment": "Accepted to SwissText 2025", "summary": "This paper investigates the extent to which pretrained German BERT encodes\nknowledge of noun compound semantics. We comprehensively vary combinations of\ntarget tokens, layers, and cased vs. uncased models, and evaluate them by\npredicting the compositionality of 868 gold standard compounds. Looking at\nrepresentational patterns within the transformer architecture, we observe\ntrends comparable to equivalent prior work on English, with compositionality\ninformation most easily recoverable in the early layers. However, our strongest\nresults clearly lag behind those reported for English, suggesting an inherently\nmore difficult task in German. This may be due to the higher productivity of\ncompounding in German than in English and the associated increase in\nconstituent-level ambiguity, including in our target compound set."}
{"id": "2505.14131", "pdf": "https://arxiv.org/pdf/2505.14131.pdf", "abs": "https://arxiv.org/abs/2505.14131", "title": "Texts or Images? A Fine-grained Analysis on the Effectiveness of Input Representations and Models for Table Question Answering", "authors": ["Wei Zhou", "Mohsen Mesgar", "Heike Adel", "Annemarie Friedrich"], "categories": ["cs.CL"], "comment": "Accepted at ACL25 (Findings)", "summary": "In table question answering (TQA), tables are encoded as either texts or\nimages. Prior work suggests that passing images of tables to multi-modal large\nlanguage models (MLLMs) performs comparably to or even better than using\ntextual input with large language models (LLMs). However, the lack of\ncontrolled setups limits fine-grained distinctions between these approaches. In\nthis paper, we conduct the first controlled study on the effectiveness of\nseveral combinations of table representations and models from two perspectives:\nquestion complexity and table size. We build a new benchmark based on existing\nTQA datasets. In a systematic analysis of seven pairs of MLLMs and LLMs, we\nfind that the best combination of table representation and model varies across\nsetups. We propose FRES, a method selecting table representations dynamically,\nand observe a 10% average performance improvement compared to using both\nrepresentations indiscriminately."}
{"id": "2505.14149", "pdf": "https://arxiv.org/pdf/2505.14149.pdf", "abs": "https://arxiv.org/abs/2505.14149", "title": "Enhancing Keyphrase Extraction from Academic Articles Using Section Structure Information", "authors": ["Chengzhi Zhang", "Xinyi Yan", "Lei Zhao", "Yingyi Zhang"], "categories": ["cs.CL", "cs.DL", "cs.IR"], "comment": null, "summary": "The exponential increase in academic papers has significantly increased the\ntime required for researchers to access relevant literature. Keyphrase\nExtraction (KPE) offers a solution to this situation by enabling researchers to\nefficiently retrieve relevant literature. The current study on KPE from\nacademic articles aims to improve the performance of extraction models through\ninnovative approaches using Title and Abstract as input corpora. However, the\nsemantic richness of keywords is significantly constrained by the length of the\nabstract. While full-text-based KPE can address this issue, it simultaneously\nintroduces noise, which significantly diminishes KPE performance. To address\nthis issue, this paper utilized the structural features and section texts\nobtained from the section structure information of academic articles to extract\nkeyphrase from academic papers. The approach consists of two main parts: (1)\nexploring the effect of seven structural features on KPE models, and (2)\nintegrating the extraction results from all section texts used as input corpora\nfor KPE models via a keyphrase integration algorithm to obtain the keyphrase\nintegration result. Furthermore, this paper also examined the effect of the\nclassification quality of section structure on the KPE performance. The results\nshow that incorporating structural features improves KPE performance, though\ndifferent features have varying effects on model efficacy. The keyphrase\nintegration approach yields the best performance, and the classification\nquality of section structure can affect KPE performance. These findings\nindicate that using the section structure information of academic articles\ncontributes to effective KPE from academic articles. The code and dataset\nsupporting this study are available at https://github.com/yan-xinyi/SSB_KPE."}
{"id": "2505.14157", "pdf": "https://arxiv.org/pdf/2505.14157.pdf", "abs": "https://arxiv.org/abs/2505.14157", "title": "Prior Prompt Engineering for Reinforcement Fine-Tuning", "authors": ["Pittawat Taveekitworachai", "Potsawee Manakul", "Sarana Nutanong", "Kunat Pipatanakul"], "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 42 figures", "summary": "This paper investigates prior prompt engineering (pPE) in the context of\nreinforcement fine-tuning (RFT), where language models (LMs) are incentivized\nto exhibit behaviors that maximize performance through reward signals. While\nexisting RFT research has primarily focused on algorithms, reward shaping, and\ndata curation, the design of the prior prompt--the instructions prepended to\nqueries during training to elicit behaviors such as step-by-step\nreasoning--remains underexplored. We investigate whether different pPE\napproaches can guide LMs to internalize distinct behaviors after RFT. Inspired\nby inference-time prompt engineering (iPE), we translate five representative\niPE strategies--reasoning, planning, code-based reasoning, knowledge recall,\nand null-example utilization--into corresponding pPE approaches. We experiment\nwith Qwen2.5-7B using each of the pPE approaches, then evaluate performance on\nin-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and\nGPQA-Diamond). Our results show that all pPE-trained models surpass their\niPE-prompted counterparts, with the null-example pPE approach achieving the\nlargest average performance gain and the highest improvement on AIME2024 and\nGPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by\nadapting a behavior-classification framework, we demonstrate that different pPE\nstrategies instill distinct behavioral styles in the resulting models. These\nfindings position pPE as a powerful yet understudied axis for RFT."}
{"id": "2505.14158", "pdf": "https://arxiv.org/pdf/2505.14158.pdf", "abs": "https://arxiv.org/abs/2505.14158", "title": "Temporal Alignment of Time Sensitive Facts with Activation Engineering", "authors": ["Sanjay Govindan", "Maurice Pagnucco", "Yang Song"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are trained on diverse and often conflicting\nknowledge spanning multiple domains and time periods. Some of this knowledge is\nonly valid within specific temporal contexts, such as answering the question,\n\"Who is the President of the United States in 2022?\" Ensuring LLMs generate\ntime appropriate responses is crucial for maintaining relevance and accuracy.\nIn this work we explore activation engineering as a method for temporally\naligning LLMs to improve factual recall without any training or dataset\ncreation. In this research we explore an activation engineering technique to\nground three versions of LLaMA 2 to specific points in time and examine the\neffects of varying injection layers and prompting strategies. Our experiments\ndemonstrate up to a 44% and 16% improvement in relative and explicit prompting\nrespectively, achieving comparable performance to the fine-tuning method\nproposed by Zhao et al. (2024) . Notably, our approach achieves similar results\nto the fine-tuning baseline while being significantly more computationally\nefficient and requiring no pre-aligned datasets."}
{"id": "2505.14160", "pdf": "https://arxiv.org/pdf/2505.14160.pdf", "abs": "https://arxiv.org/abs/2505.14160", "title": "Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models", "authors": ["Zahraa Al Sahili", "Ioannis Patras", "Matthew Purver"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multilingual vision-language models promise universal image-text retrieval,\nyet their social biases remain under-explored. We present the first systematic\naudit of three public multilingual CLIP checkpoints -- M-CLIP, NLLB-CLIP, and\nCAPIVARA-CLIP -- across ten languages that vary in resource availability and\ngrammatical gender. Using balanced subsets of \\textsc{FairFace} and the\n\\textsc{PATA} stereotype suite in a zero-shot setting, we quantify race and\ngender bias and measure stereotype amplification. Contrary to the assumption\nthat multilinguality mitigates bias, every model exhibits stronger gender bias\nthan its English-only baseline. CAPIVARA-CLIP shows its largest biases\nprecisely in the low-resource languages it targets, while the shared\ncross-lingual encoder of NLLB-CLIP transports English gender stereotypes into\ngender-neutral languages; loosely coupled encoders largely avoid this transfer.\nHighly gendered languages consistently magnify all measured bias types, but\neven gender-neutral languages remain vulnerable when cross-lingual weight\nsharing imports foreign stereotypes. Aggregated metrics conceal\nlanguage-specific ``hot spots,'' underscoring the need for fine-grained,\nlanguage-aware bias evaluation in future multilingual vision-language research."}
{"id": "2505.14165", "pdf": "https://arxiv.org/pdf/2505.14165.pdf", "abs": "https://arxiv.org/abs/2505.14165", "title": "PL-FGSA: A Prompt Learning Framework for Fine-Grained Sentiment Analysis Based on MindSpore", "authors": ["Zhenkai Qin", "Jiajing He", "Qiao Fang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Fine-grained sentiment analysis (FGSA) aims to identify sentiment polarity\ntoward specific aspects within a text, enabling more precise opinion mining in\ndomains such as product reviews and social media. However, traditional FGSA\napproaches often require task-specific architectures and extensive annotated\ndata, limiting their generalization and scalability. To address these\nchallenges, we propose PL-FGSA, a unified prompt learning-based framework\nimplemented using the MindSpore platform, which integrates prompt design with a\nlightweight TextCNN backbone. Our method reformulates FGSA as a multi-task\nprompt-augmented generation problem, jointly tackling aspect extraction,\nsentiment classification, and causal explanation in a unified paradigm. By\nleveraging prompt-based guidance, PL-FGSA enhances interpretability and\nachieves strong performance under both full-data and low-resource conditions.\nExperiments on three benchmark datasets-SST-2, SemEval-2014 Task 4, and\nMAMS-demonstrate that our model consistently outperforms traditional\nfine-tuning methods and achieves F1-scores of 0.922, 0.694, and 0.597,\nrespectively. These results validate the effectiveness of prompt-based\ngeneralization and highlight the practical value of PL-FGSA for real-world\nsentiment analysis tasks."}
{"id": "2505.14172", "pdf": "https://arxiv.org/pdf/2505.14172.pdf", "abs": "https://arxiv.org/abs/2505.14172", "title": "The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models", "authors": ["Adrian Cosma", "Stefan Ruseti", "Emilian Radoi", "Mihai Dascalu"], "categories": ["cs.CL"], "comment": "1 Table, 8 Figures", "summary": "Despite their remarkable progress across diverse domains, Large Language\nModels (LLMs) consistently fail at simple character-level tasks, such as\ncounting letters in words, due to a fundamental limitation: tokenization. In\nthis work, we frame this limitation as a problem of low mutual information and\nanalyze it in terms of concept emergence. Using a suite of 19 synthetic tasks\nthat isolate character-level reasoning in a controlled setting, we show that\nsuch capabilities emerge slowly, suddenly, and only late in training. We\nfurther show that percolation-based models of concept emergence explain these\npatterns, suggesting that learning character composition is not fundamentally\ndifferent from learning commonsense knowledge. To address this bottleneck, we\npropose a lightweight architectural modification that significantly improves\ncharacter-level reasoning while preserving the inductive advantages of subword\nmodels. Together, our results bridge low-level perceptual gaps in tokenized LMs\nand provide a principled framework for understanding and mitigating their\nstructural blind spots. We make our code publicly available."}
{"id": "2505.14173", "pdf": "https://arxiv.org/pdf/2505.14173.pdf", "abs": "https://arxiv.org/abs/2505.14173", "title": "THOR-MoE: Hierarchical Task-Guided and Context-Responsive Routing for Neural Machine Translation", "authors": ["Yunlong Liang", "Fandong Meng", "Jie Zhou"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 main conference", "summary": "The sparse Mixture-of-Experts (MoE) has achieved significant progress for\nneural machine translation (NMT). However, there exist two limitations in\ncurrent MoE solutions which may lead to sub-optimal performance: 1) they\ndirectly use the task knowledge of NMT into MoE (\\emph{e.g.},\ndomain/linguistics-specific knowledge), which are generally unavailable at\npractical application and neglect the naturally grouped domain/linguistic\nproperties; 2) the expert selection only depends on the localized token\nrepresentation without considering the context, which fully grasps the state of\neach token in a global view. To address the above limitations, we propose\nTHOR-MoE via arming the MoE with hierarchical task-guided and\ncontext-responsive routing policies. Specifically, it 1) firstly predicts the\ndomain/language label and then extracts mixed domain/language representation to\nallocate task-level experts in a hierarchical manner; 2) injects the context\ninformation to enhance the token routing from the pre-selected task-level\nexperts set, which can help each token to be accurately routed to more\nspecialized and suitable experts. Extensive experiments on multi-domain\ntranslation and multilingual translation benchmarks with different\narchitectures consistently demonstrate the superior performance of THOR-MoE.\nAdditionally, the THOR-MoE operates as a plug-and-play module compatible with\nexisting Top-$k$~\\cite{shazeer2017} and Top-$p$~\\cite{huang-etal-2024-harder}\nrouting schemes, ensuring broad applicability across diverse MoE architectures.\nFor instance, compared with vanilla Top-$p$~\\cite{huang-etal-2024-harder}\nrouting, the context-aware manner can achieve an average improvement of 0.75\nBLEU with less than 22\\% activated parameters on multi-domain translation\ntasks."}
{"id": "2505.14174", "pdf": "https://arxiv.org/pdf/2505.14174.pdf", "abs": "https://arxiv.org/abs/2505.14174", "title": "Cheaper, Better, Faster, Stronger: Robust Text-to-SQL without Chain-of-Thought or Fine-Tuning", "authors": ["Yusuf Denizay Dönder", "Derek Hommel", "Andrea W Wen-Yi", "David Mimno", "Unso Eun Seo Jo"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "LLMs are effective at code generation tasks like text-to-SQL, but is it worth\nthe cost? Many state-of-the-art approaches use non-task-specific LLM techniques\nincluding Chain-of-Thought (CoT), self-consistency, and fine-tuning. These\nmethods can be costly at inference time, sometimes requiring over a hundred LLM\ncalls with reasoning, incurring average costs of up to \\$0.46 per query, while\nfine-tuning models can cost thousands of dollars. We introduce \"N-rep\"\nconsistency, a more cost-efficient text-to-SQL approach that achieves similar\nBIRD benchmark scores as other more expensive methods, at only \\$0.039 per\nquery. N-rep leverages multiple representations of the same schema input to\nmitigate weaknesses in any single representation, making the solution more\nrobust and allowing the use of smaller and cheaper models without any reasoning\nor fine-tuning. To our knowledge, N-rep is the best-performing text-to-SQL\napproach in its cost range."}
{"id": "2505.14178", "pdf": "https://arxiv.org/pdf/2505.14178.pdf", "abs": "https://arxiv.org/abs/2505.14178", "title": "Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits", "authors": ["Xiang Zhang", "Juntai Cao", "Jiaqi Wei", "Yiwei Xu", "Chenyu You"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tokenization is the first - and often underappreciated - layer of computation\nin language models. While Chain-of-Thought (CoT) prompting enables transformer\nmodels to approximate recurrent computation by externalizing intermediate\nsteps, we show that the success of such reasoning is fundamentally bounded by\nthe structure of tokenized inputs. This work presents a theoretical and\nempirical investigation into how tokenization schemes, particularly\nsubword-based methods like byte-pair encoding (BPE), impede symbolic\ncomputation by merging or obscuring atomic reasoning units. We introduce the\nnotion of Token Awareness to formalize how poor token granularity disrupts\nlogical alignment and prevents models from generalizing symbolic procedures.\nThrough systematic evaluation on arithmetic and symbolic tasks, we demonstrate\nthat token structure dramatically affect reasoning performance, causing failure\neven with CoT, while atomically-aligned formats unlock strong generalization,\nallowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,\no1) in structured reasoning. Our findings reveal that symbolic reasoning\nability in LLMs is not purely architectural, but deeply conditioned on\ntoken-level representations."}
{"id": "2505.14179", "pdf": "https://arxiv.org/pdf/2505.14179.pdf", "abs": "https://arxiv.org/abs/2505.14179", "title": "Enhancing Abstractive Summarization of Scientific Papers Using Structure Information", "authors": ["Tong Bao", "Heng Zhang", "Chengzhi Zhang"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Abstractive summarization of scientific papers has always been a research\nfocus, yet existing methods face two main challenges. First, most summarization\nmodels rely on Encoder-Decoder architectures that treat papers as sequences of\nwords, thus fail to fully capture the structured information inherent in\nscientific papers. Second, existing research often use keyword mapping or\nfeature engineering to identify the structural information, but these methods\nstruggle with the structural flexibility of scientific papers and lack\nrobustness across different disciplines. To address these challenges, we\npropose a two-stage abstractive summarization framework that leverages\nautomatic recognition of structural functions within scientific papers. In the\nfirst stage, we standardize chapter titles from numerous scientific papers and\nconstruct a large-scale dataset for structural function recognition. A\nclassifier is then trained to automatically identify the key structural\ncomponents (e.g., Background, Methods, Results, Discussion), which provides a\nfoundation for generating more balanced summaries. In the second stage, we\nemploy Longformer to capture rich contextual relationships across sections and\ngenerating context-aware summaries. Experiments conducted on two\ndomain-specific scientific paper summarization datasets demonstrate that our\nmethod outperforms advanced baselines, and generates more comprehensive\nsummaries. The code and dataset can be accessed at\nhttps://github.com/tongbao96/code-for-SFR-AS."}
{"id": "2505.14181", "pdf": "https://arxiv.org/pdf/2505.14181.pdf", "abs": "https://arxiv.org/abs/2505.14181", "title": "SlangDIT: Benchmarking LLMs in Interpretative Slang Translation", "authors": ["Yunlong Liang", "Fandong Meng", "Jiaan Wang", "Jie Zhou"], "categories": ["cs.CL"], "comment": "work in progress", "summary": "The challenge of slang translation lies in capturing context-dependent\nsemantic extensions, as slang terms often convey meanings beyond their literal\ninterpretation. While slang detection, explanation, and translation have been\nstudied as isolated tasks in the era of large language models (LLMs), their\nintrinsic interdependence remains underexplored. The main reason is lacking of\na benchmark where the two tasks can be a prerequisite for the third one, which\ncan facilitate idiomatic translation. In this paper, we introduce the\ninterpretative slang translation task (named SlangDIT) consisting of three\nsub-tasks: slang detection, cross-lingual slang explanation, and slang\ntranslation within the current context, aiming to generate more accurate\ntranslation with the help of slang detection and slang explanation. To this\nend, we construct a SlangDIT dataset, containing over 25k English-Chinese\nsentence pairs. Each source sentence mentions at least one slang term and is\nlabeled with corresponding cross-lingual slang explanation. Based on the\nbenchmark, we propose a deep thinking model, named SlangOWL. It firstly\nidentifies whether the sentence contains a slang, and then judges whether the\nslang is polysemous and analyze its possible meaning. Further, the SlangOWL\nprovides the best explanation of the slang term targeting on the current\ncontext. Finally, according to the whole thought, the SlangOWL offers a\nsuitable translation. Our experiments on LLMs (\\emph{e.g.}, Qwen2.5 and\nLLama-3.1), show that our deep thinking approach indeed enhances the\nperformance of LLMs where the proposed SLangOWL significantly surpasses the\nvanilla models and supervised fine-tuned models without thinking."}
{"id": "2505.14183", "pdf": "https://arxiv.org/pdf/2505.14183.pdf", "abs": "https://arxiv.org/abs/2505.14183", "title": "ThinkSwitcher: When to Think Hard, When to Think Fast", "authors": ["Guosheng Liang", "Longguang Zhong", "Ziyi Yang", "Xiaojun Quan"], "categories": ["cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs) excel at solving complex tasks by leveraging\nlong chain-of-thought (CoT) reasoning. However, this often leads to\noverthinking on simple tasks, resulting in unnecessary computational overhead.\nWe observe that LRMs inherently possess the capability for efficient short CoT\nreasoning, which can be reliably elicited through prompt design. To leverage\nthis capability, we propose ThinkSwitcher, a framework that enables a single\nLRM to dynamically switch between short and long CoT modes based on task\ncomplexity. ThinkSwitcher introduces a lightweight switching module trained\nwith supervision signals derived from the relative performance of each\nreasoning mode across tasks. Experiments on multiple reasoning benchmarks show\nthat ThinkSwitcher reduces computational cost by 20-30% while maintaining high\naccuracy on complex tasks. This demonstrates the effectiveness of ThinkSwitcher\nas a scalable and efficient solution for unified LRM deployment."}
{"id": "2505.14195", "pdf": "https://arxiv.org/pdf/2505.14195.pdf", "abs": "https://arxiv.org/abs/2505.14195", "title": "Unraveling Interwoven Roles of Large Language Models in Authorship Privacy: Obfuscation, Mimicking, and Verification", "authors": ["Tuc Nguyen", "Yifan Hu", "Thai Le"], "categories": ["cs.CL"], "comment": "17 pages, 3 figures", "summary": "Recent advancements in large language models (LLMs) have been fueled by large\nscale training corpora drawn from diverse sources such as websites, news\narticles, and books. These datasets often contain explicit user information,\nsuch as person names and addresses, that LLMs may unintentionally reproduce in\ntheir generated outputs. Beyond such explicit content, LLMs can also leak\nidentity revealing cues through implicit signals such as distinctive writing\nstyles, raising significant concerns about authorship privacy. There are three\nmajor automated tasks in authorship privacy, namely authorship obfuscation\n(AO), authorship mimicking (AM), and authorship verification (AV). Prior\nresearch has studied AO, AM, and AV independently. However, their interplays\nremain under explored, which leaves a major research gap, especially in the era\nof LLMs, where they are profoundly shaping how we curate and share user\ngenerated content, and the distinction between machine generated and human\nauthored text is also increasingly blurred. This work then presents the first\nunified framework for analyzing the dynamic relationships among LLM enabled AO,\nAM, and AV in the context of authorship privacy. We quantify how they interact\nwith each other to transform human authored text, examining effects at a single\npoint in time and iteratively over time. We also examine the role of\ndemographic metadata, such as gender, academic background, in modulating their\nperformances, inter-task dynamics, and privacy risks. All source code will be\npublicly available."}
{"id": "2505.14212", "pdf": "https://arxiv.org/pdf/2505.14212.pdf", "abs": "https://arxiv.org/abs/2505.14212", "title": "Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks", "authors": ["Sizhe Yuen", "Ting Su", "Ziyang Wang", "Yali Du", "Adam J. Sobey"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A question-answering (QA) system is to search suitable answers within a\nknowledge base. Current QA systems struggle with queries requiring complex\nreasoning or real-time knowledge integration. They are often supplemented with\nretrieval techniques on a data source such as Retrieval-Augmented Generation\n(RAG). However, RAG continues to face challenges in handling complex reasoning\nand logical connections between multiple sources of information. A novel\napproach for enhancing Large Language Models (LLMs) in knowledge-intensive QA\ntasks is presented through the automated generation of context-based QA pairs.\nThis methodology leverages LLMs to create fine-tuning data, reducing reliance\non human labelling and improving model comprehension and reasoning\ncapabilities. The proposed system includes an automated QA generator and a\nmodel fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore.\nComprehensive experiments demonstrate improvements in logical coherence and\nfactual accuracy, with implications for developing adaptable Artificial\nIntelligence (AI) systems. Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1,\nBLEU, and ROUGE scores 0.858, 0.172, and 0.260 of for the LLM generated QA\npairs compared to scores of 0.836, 0.083, and 0.139 for the human annotated QA\npairs."}
{"id": "2505.14226", "pdf": "https://arxiv.org/pdf/2505.14226.pdf", "abs": "https://arxiv.org/abs/2505.14226", "title": "\"Haet Bhasha aur Diskrimineshun\": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs", "authors": ["Darpan Aswal", "Siddharth D Jaiswal"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have become increasingly powerful, with\nmultilingual and multimodal capabilities improving by the day. These models are\nbeing evaluated through audits, alignment studies and red-teaming efforts to\nexpose model vulnerabilities towards generating harmful, biased and unfair\ncontent. Existing red-teaming efforts have previously focused on the English\nlanguage, using fixed template-based attacks; thus, models continue to be\nsusceptible to multilingual jailbreaking strategies, especially in the\nmultimodal context. In this study, we introduce a novel strategy that leverages\ncode-mixing and phonetic perturbations to jailbreak LLMs for both text and\nimage generation tasks. We also introduce two new jailbreak strategies that\nshow higher effectiveness than baseline strategies. Our work presents a method\nto effectively bypass safety filters in LLMs while maintaining interpretability\nby applying phonetic misspellings to sensitive words in code-mixed prompts. Our\nnovel prompts achieve a 99% Attack Success Rate for text generation and 78% for\nimage generation, with Attack Relevance Rate of 100% for text generation and\n95% for image generation when using the phonetically perturbed code-mixed\nprompts. Our interpretability experiments reveal that phonetic perturbations\nimpact word tokenization, leading to jailbreak success. Our study motivates\nincreasing the focus towards more generalizable safety alignment for\nmultilingual multimodal models, especially in real-world settings wherein\nprompts can have misspelt words."}
{"id": "2505.14233", "pdf": "https://arxiv.org/pdf/2505.14233.pdf", "abs": "https://arxiv.org/abs/2505.14233", "title": "Mechanistic Fine-tuning for In-context Learning", "authors": ["Hakaze Cho", "Peng Luo", "Mariko Kato", "Rin Kaenbyou", "Naoya Inoue"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "28 pages, 31 figures, 6 tables", "summary": "In-context Learning (ICL) utilizes structured demonstration-query inputs to\ninduce few-shot learning on Language Models (LMs), which are not originally\npre-trained on ICL-style data. To bridge the gap between ICL and pre-training,\nsome approaches fine-tune LMs on large ICL-style datasets by an end-to-end\nparadigm with massive computational costs. To reduce such costs, in this paper,\nwe propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous\nfindings on the inner mechanism of ICL, building training objectives on the\nattention scores instead of the final outputs, to force the attention scores to\nfocus on the correct label tokens presented in the context and mitigate\nattention scores from the wrong label tokens. Our experiments on 9 modern LMs\nand 8 datasets empirically find that ABFT outperforms in performance,\nrobustness, unbiasedness, and efficiency, with only around 0.01% data cost\ncompared to the previous methods. Moreover, our subsequent analysis finds that\nthe end-to-end training objective contains the ABFT objective, suggesting the\nimplicit bias of ICL-style data to the emergence of induction heads. Our work\ndemonstrates the possibility of controlling specific module sequences within\nLMs to improve their behavior, opening up the future application of mechanistic\ninterpretability."}
{"id": "2505.14238", "pdf": "https://arxiv.org/pdf/2505.14238.pdf", "abs": "https://arxiv.org/abs/2505.14238", "title": "ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models", "authors": ["Raghav Singhal", "Kaustubh Ponkshe", "Rohit Vartak", "Praneeth Vepakomma"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Raghav Singhal, Kaustubh Ponkshe, and Rohit Vartak contributed\n  equally to this work", "summary": "Large Language Models have demonstrated strong performance across a wide\nrange of tasks, but adapting them efficiently to new domains remains a key\nchallenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by\nintroducing lightweight, trainable modules while keeping most pre-trained\nweights fixed. The prevailing approach, LoRA, models updates using a low-rank\ndecomposition, but its expressivity is inherently constrained by the rank.\nRecent methods like HiRA aim to increase expressivity by incorporating a\nHadamard product with the frozen weights, but still rely on the structure of\nthe pre-trained model. We introduce ABBA, a new PEFT architecture that\nreparameterizes the update as a Hadamard product of two independently learnable\nlow-rank matrices. In contrast to prior work, ABBA fully decouples the update\nfrom the pre-trained weights, enabling both components to be optimized freely.\nThis leads to significantly higher expressivity under the same parameter\nbudget. We formally analyze ABBA's expressive capacity and validate its\nadvantages through matrix reconstruction experiments. Empirically, ABBA\nachieves state-of-the-art results on arithmetic and commonsense reasoning\nbenchmarks, consistently outperforming existing PEFT methods by a significant\nmargin across multiple models. Our code is publicly available at:\nhttps://github.com/CERT-Lab/abba."}
{"id": "2505.14242", "pdf": "https://arxiv.org/pdf/2505.14242.pdf", "abs": "https://arxiv.org/abs/2505.14242", "title": "Technical Report on classification of literature related to children speech disorder", "authors": ["Ziang Wang", "Amir Aryani"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "This technical report presents a natural language processing (NLP)-based\napproach for systematically classifying scientific literature on childhood\nspeech disorders. We retrieved and filtered 4,804 relevant articles published\nafter 2015 from the PubMed database using domain-specific keywords. After\ncleaning and pre-processing the abstracts, we applied two topic modeling\ntechniques - Latent Dirichlet Allocation (LDA) and BERTopic - to identify\nlatent thematic structures in the corpus. Our models uncovered 14 clinically\nmeaningful clusters, such as infantile hyperactivity and abnormal epileptic\nbehavior. To improve relevance and precision, we incorporated a custom stop\nword list tailored to speech pathology. Evaluation results showed that the LDA\nmodel achieved a coherence score of 0.42 and a perplexity of -7.5, indicating\nstrong topic coherence and predictive performance. The BERTopic model exhibited\na low proportion of outlier topics (less than 20%), demonstrating its capacity\nto classify heterogeneous literature effectively. These results provide a\nfoundation for automating literature reviews in speech-language pathology."}
{"id": "2505.14244", "pdf": "https://arxiv.org/pdf/2505.14244.pdf", "abs": "https://arxiv.org/abs/2505.14244", "title": "TransBench: Benchmarking Machine Translation for Industrial-Scale Applications", "authors": ["Haijun Li", "Tianqi Shi", "Zifu Shang", "Yuxuan Han", "Xueyu Zhao", "Hao Wang", "Yu Qian", "Zhiqiang Qian", "Linlong Xu", "Minghao Wu", "Chenyang Lyu", "Longyue Wang", "Gongbo Tang", "Weihua Luo", "Zhao Xu", "Kaifu Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Machine translation (MT) has become indispensable for cross-border\ncommunication in globalized industries like e-commerce, finance, and legal\nservices, with recent advancements in large language models (LLMs)\nsignificantly enhancing translation quality. However, applying general-purpose\nMT models to industrial scenarios reveals critical limitations due to\ndomain-specific terminology, cultural nuances, and stylistic conventions absent\nin generic benchmarks. Existing evaluation frameworks inadequately assess\nperformance in specialized contexts, creating a gap between academic benchmarks\nand real-world efficacy. To address this, we propose a three-level translation\ncapability framework: (1) Basic Linguistic Competence, (2) Domain-Specific\nProficiency, and (3) Cultural Adaptation, emphasizing the need for holistic\nevaluation across these dimensions. We introduce TransBench, a benchmark\ntailored for industrial MT, initially targeting international e-commerce with\n17,000 professionally translated sentences spanning 4 main scenarios and 33\nlanguage pairs. TransBench integrates traditional metrics (BLEU, TER) with\nMarco-MOS, a domain-specific evaluation model, and provides guidelines for\nreproducible benchmark construction. Our contributions include: (1) a\nstructured framework for industrial MT evaluation, (2) the first publicly\navailable benchmark for e-commerce translation, (3) novel metrics probing\nmulti-level translation quality, and (4) open-sourced evaluation tools. This\nwork bridges the evaluation gap, enabling researchers and practitioners to\nsystematically assess and enhance MT systems for industry-specific needs."}
{"id": "2505.14256", "pdf": "https://arxiv.org/pdf/2505.14256.pdf", "abs": "https://arxiv.org/abs/2505.14256", "title": "FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation", "authors": ["Shaolin Zhu", "Tianyu Dong", "Bo Li", "Deyi Xiong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we present FuxiMT, a novel Chinese-centric multilingual\nmachine translation model powered by a sparsified large language model (LLM).\nWe adopt a two-stage strategy to train FuxiMT. We first pre-train the model on\na massive Chinese corpus and then conduct multilingual fine-tuning on a large\nparallel dataset encompassing 65 languages. FuxiMT incorporates\nMixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust\nperformance across various resource levels. Experimental results demonstrate\nthat FuxiMT significantly outperforms strong baselines, including\nstate-of-the-art LLMs and machine translation models, particularly under\nlow-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot\ntranslation capabilities for unseen language pairs, indicating its potential to\nbridge communication gaps where parallel data are scarce or unavailable."}
{"id": "2505.14268", "pdf": "https://arxiv.org/pdf/2505.14268.pdf", "abs": "https://arxiv.org/abs/2505.14268", "title": "Think-J: Learning to Think for Generative LLM-as-a-Judge", "authors": ["Hui Huang", "Yancheng He", "Hongli Zhou", "Rui Zhang", "Wei Liu", "Weixun Wang", "Wenbo Su", "Bo Zheng", "Jiaheng Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 14 figures", "summary": "LLM-as-a-Judge refers to the automatic modeling of preferences for responses\ngenerated by Large Language Models (LLMs), which is of significant importance\nfor both LLM evaluation and reward modeling. Although generative LLMs have made\nsubstantial progress in various tasks, their performance as LLM-Judge still\nfalls short of expectations. In this work, we propose Think-J, which improves\ngenerative LLM-as-a-Judge by learning how to think. We first utilized a small\namount of curated data to develop the model with initial judgment thinking\ncapabilities. Subsequently, we optimize the judgment thinking traces based on\nreinforcement learning (RL). We propose two methods for judgment thinking\noptimization, based on offline and online RL, respectively. The offline RL\nrequires training a critic model to construct positive and negative examples\nfor learning. The online method defines rule-based reward as feedback for\noptimization. Experimental results showed that our approach can significantly\nenhance the evaluation capability of generative LLM-Judge, surpassing both\ngenerative and classifier-based LLM-Judge without requiring extra human\nannotations."}
{"id": "2505.14271", "pdf": "https://arxiv.org/pdf/2505.14271.pdf", "abs": "https://arxiv.org/abs/2505.14271", "title": "FAID: Fine-grained AI-generated Text Detection using Multi-task Auxiliary and Multi-level Contrastive Learning", "authors": ["Minh Ngoc Ta", "Dong Cao Van", "Duc-Anh Hoang", "Minh Le-Anh", "Truong Nguyen", "My Anh Tran Nguyen", "Yuxia Wang", "Preslav Nakov", "Sang Dinh"], "categories": ["cs.CL"], "comment": null, "summary": "The growing collaboration between humans and AI models in generative tasks\nhas introduced new challenges in distinguishing between human-written,\nAI-generated, and human-AI collaborative texts. In this work, we collect a\nmultilingual, multi-domain, multi-generator dataset FAIDSet. We further\nintroduce a fine-grained detection framework FAID to classify text into these\nthree categories, meanwhile identifying the underlying AI model family. Unlike\nexisting binary classifiers, FAID is built to capture both authorship and\nmodel-specific characteristics. Our method combines multi-level contrastive\nlearning with multi-task auxiliary classification to learn subtle stylistic\ncues. By modeling AI families as distinct stylistic entities, FAID offers\nimproved interpretability. We incorporate an adaptation to address\ndistributional shifts without retraining for unseen data. Experimental results\ndemonstrate that FAID outperforms several baseline approaches, particularly\nenhancing the generalization accuracy on unseen domains and new AI models. It\nprovide a potential solution for improving transparency and accountability in\nAI-assisted writing."}
{"id": "2505.14272", "pdf": "https://arxiv.org/pdf/2505.14272.pdf", "abs": "https://arxiv.org/abs/2505.14272", "title": "Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor Retrieval with Limited Labeled Data", "authors": ["Faeze Ghorbanpour", "Daryna Dementieva", "Alexander Fraser"], "categories": ["cs.CL", "cs.CY", "cs.MM"], "comment": null, "summary": "Considering the importance of detecting hateful language, labeled hate speech\ndata is expensive and time-consuming to collect, particularly for low-resource\nlanguages. Prior work has demonstrated the effectiveness of cross-lingual\ntransfer learning and data augmentation in improving performance on tasks with\nlimited labeled data. To develop an efficient and scalable cross-lingual\ntransfer learning approach, we leverage nearest-neighbor retrieval to augment\nminimal labeled data in the target language, thereby enhancing detection\nperformance. Specifically, we assume access to a small set of labeled training\ninstances in the target language and use these to retrieve the most relevant\nlabeled examples from a large multilingual hate speech detection pool. We\nevaluate our approach on eight languages and demonstrate that it consistently\noutperforms models trained solely on the target language data. Furthermore, in\nmost cases, our method surpasses the current state-of-the-art. Notably, our\napproach is highly data-efficient, retrieving as small as 200 instances in some\ncases while maintaining superior performance. Moreover, it is scalable, as the\nretrieval pool can be easily expanded, and the method can be readily adapted to\nnew languages and tasks. We also apply maximum marginal relevance to mitigate\nredundancy and filter out highly similar retrieved instances, resulting in\nimprovements in some languages."}
{"id": "2505.14279", "pdf": "https://arxiv.org/pdf/2505.14279.pdf", "abs": "https://arxiv.org/abs/2505.14279", "title": "YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering", "authors": ["Jennifer D'Souza", "Hamed Babaei Giglou", "Quentin Münch"], "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 3 figures, Accepted as a Long Paper at the 63rd Annual\n  Meeting of the Association for Computational Linguistics (ACL 2025)", "summary": "Large Language Models (LLMs) drive scientific question-answering on modern\nsearch engines, yet their evaluation robustness remains underexplored. We\nintroduce YESciEval, an open-source framework that combines fine-grained\nrubric-based assessment with reinforcement learning to mitigate optimism bias\nin LLM evaluators. We release multidisciplinary scienceQ&A datasets, including\nadversarial variants, with evaluation scores from multiple LLMs. Independent of\nproprietary models and human feedback, our approach enables scalable, cost-free\nevaluation. By advancing reliable LLM-as-a-judge models, this work supports AI\nalignment and fosters robust, transparent evaluation essential for scientific\ninquiry and artificial general intelligence."}
{"id": "2505.14286", "pdf": "https://arxiv.org/pdf/2505.14286.pdf", "abs": "https://arxiv.org/abs/2505.14286", "title": "Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs", "authors": ["Rao Ma", "Mengjie Qian", "Vyas Raina", "Mark Gales", "Kate Knill"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "The combination of pre-trained speech encoders with large language models has\nenabled the development of speech LLMs that can handle a wide range of spoken\nlanguage processing tasks. While these models are powerful and flexible, this\nvery flexibility may make them more vulnerable to adversarial attacks. To\nexamine the extent of this problem, in this work we investigate universal\nacoustic adversarial attacks on speech LLMs. Here a fixed, universal,\nadversarial audio segment is prepended to the original input audio. We\ninitially investigate attacks that cause the model to either produce no output\nor to perform a modified task overriding the original prompt. We then extend\nthe nature of the attack to be selective so that it activates only when\nspecific input attributes, such as a speaker gender or spoken language, are\npresent. Inputs without the targeted attribute should be unaffected, allowing\nfine-grained control over the model outputs. Our findings reveal critical\nvulnerabilities in Qwen2-Audio and Granite-Speech and suggest that similar\nspeech LLMs may be susceptible to universal adversarial attacks. This\nhighlights the need for more robust training strategies and improved resistance\nto adversarial attacks."}
{"id": "2505.14297", "pdf": "https://arxiv.org/pdf/2505.14297.pdf", "abs": "https://arxiv.org/abs/2505.14297", "title": "Cross-Lingual Optimization for Language Transfer in Large Language Models", "authors": ["Jungseob Lee", "Seongtae Hong", "Hyeonseok Moon", "Heuiseok Lim"], "categories": ["cs.CL"], "comment": "Accepted for publication at ACL 2025. Jungseob Lee and Seongtae Hong\n  contributed equally to this work", "summary": "Adapting large language models to other languages typically employs\nsupervised fine-tuning (SFT) as a standard approach. However, it often suffers\nfrom an overemphasis on English performance, a phenomenon that is especially\npronounced in data-constrained environments. To overcome these challenges, we\npropose \\textbf{Cross-Lingual Optimization (CLO)} that efficiently transfers an\nEnglish-centric LLM to a target language while preserving its English\ncapabilities. CLO utilizes publicly available English SFT data and a\ntranslation model to enable cross-lingual transfer. We conduct experiments\nusing five models on six languages, each possessing varying levels of resource.\nOur results show that CLO consistently outperforms SFT in both acquiring target\nlanguage proficiency and maintaining English performance. Remarkably, in\nlow-resource languages, CLO with only 3,200 samples surpasses SFT with 6,400\nsamples, demonstrating that CLO can achieve better performance with less data.\nFurthermore, we find that SFT is particularly sensitive to data quantity in\nmedium and low-resource languages, whereas CLO remains robust. Our\ncomprehensive analysis emphasizes the limitations of SFT and incorporates\nadditional training strategies in CLO to enhance efficiency."}
{"id": "2505.14305", "pdf": "https://arxiv.org/pdf/2505.14305.pdf", "abs": "https://arxiv.org/abs/2505.14305", "title": "JOLT-SQL: Joint Loss Tuning of Text-to-SQL with Confusion-aware Noisy Schema Sampling", "authors": ["Jinwang Song", "Hongying Zan", "Kunli Zhang", "Lingling Mu", "Yingjie Han", "Haobo Hua", "Min Peng"], "categories": ["cs.CL"], "comment": "Work in progress. 13 pages, 6 figures", "summary": "Text-to-SQL, which maps natural language to SQL queries, has benefited\ngreatly from recent advances in Large Language Models (LLMs). While LLMs offer\nvarious paradigms for this task, including prompting and supervised fine-tuning\n(SFT), SFT approaches still face challenges such as complex multi-stage\npipelines and poor robustness to noisy schema information. To address these\nlimitations, we present JOLT-SQL, a streamlined single-stage SFT framework that\njointly optimizes schema linking and SQL generation via a unified loss.\nJOLT-SQL employs discriminative schema linking, enhanced by local bidirectional\nattention, alongside a confusion-aware noisy schema sampling strategy with\nselective attention to improve robustness under noisy schema conditions.\nExperiments on the Spider and BIRD benchmarks demonstrate that JOLT-SQL\nachieves state-of-the-art execution accuracy among comparable-size open-source\nmodels, while significantly improving both training and inference efficiency."}
{"id": "2505.14309", "pdf": "https://arxiv.org/pdf/2505.14309.pdf", "abs": "https://arxiv.org/abs/2505.14309", "title": "Studying the Role of Input-Neighbor Overlap in Retrieval-Augmented Language Models Training Efficiency", "authors": ["Ehsan Doostmohammadi", "Marco Kuhlmann"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-augmented language models have demonstrated performance comparable\nto much larger models while requiring fewer computational resources. The\neffectiveness of these models crucially depends on the overlap between query\nand retrieved context, but the optimal degree of this overlap remains\nunexplored. In this paper, we systematically investigate how varying levels of\nquery--context overlap affect model performance during both training and\ninference. Our experiments reveal that increased overlap initially has minimal\neffect, but substantially improves test-time perplexity and accelerates model\nlearning above a critical threshold. Building on these findings, we demonstrate\nthat deliberately increasing overlap through synthetic context can enhance data\nefficiency and reduce training time by approximately 40\\% without compromising\nperformance. We specifically generate synthetic context through paraphrasing\nqueries. We validate our perplexity-based findings on question-answering tasks,\nconfirming that the benefits of retrieval-augmented language modeling extend to\npractical applications. Our results provide empirical evidence of significant\noptimization potential for retrieval mechanisms in language model pretraining."}
{"id": "2505.14311", "pdf": "https://arxiv.org/pdf/2505.14311.pdf", "abs": "https://arxiv.org/abs/2505.14311", "title": "HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing", "authors": ["Shamsuddeen Hassan Muhammad", "Ibrahim Said Ahmad", "Idris Abdulmumin", "Falalu Ibrahim Lawan", "Babangida Sani", "Sukairaj Hafiz Imam", "Yusuf Aliyu", "Sani Abdullahi Sani", "Ali Usman Umar", "Kenneth Church", "Vukosi Marivate"], "categories": ["cs.CL"], "comment": null, "summary": "Hausa Natural Language Processing (NLP) has gained increasing attention in\nrecent years, yet remains understudied as a low-resource language despite\nhaving over 120 million first-language (L1) and 80 million second-language (L2)\nspeakers worldwide. While significant advances have been made in high-resource\nlanguages, Hausa NLP faces persistent challenges, including limited open-source\ndatasets and inadequate model representation. This paper presents an overview\nof the current state of Hausa NLP, systematically examining existing resources,\nresearch contributions, and gaps across fundamental NLP tasks: text\nclassification, machine translation, named entity recognition, speech\nrecognition, and question answering. We introduce HausaNLP\n(https://catalog.hausanlp.org), a curated catalog that aggregates datasets,\ntools, and research works to enhance accessibility and drive further\ndevelopment. Furthermore, we discuss challenges in integrating Hausa into large\nlanguage models (LLMs), addressing issues of suboptimal tokenization and\ndialectal variation. Finally, we propose strategic research directions\nemphasizing dataset expansion, improved language modeling approaches, and\nstrengthened community collaboration to advance Hausa NLP. Our work provides\nboth a foundation for accelerating Hausa NLP progress and valuable insights for\nbroader multilingual NLP research."}
{"id": "2505.14313", "pdf": "https://arxiv.org/pdf/2505.14313.pdf", "abs": "https://arxiv.org/abs/2505.14313", "title": "A MIND for Reasoning: Meta-learning for In-context Deduction", "authors": ["Leonardo Bertolazzi", "Manuel Vargas Guzmán", "Raffaella Bernardi", "Maciej Malicki", "Jakub Szymanik"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly evaluated on formal tasks,\nwhere strong reasoning abilities define the state of the art. However, their\nability to generalize to out-of-distribution problems remains limited. In this\npaper, we investigate how LLMs can achieve a systematic understanding of\ndeductive rules. Our focus is on the task of identifying the appropriate subset\nof premises within a knowledge base needed to derive a given hypothesis. To\ntackle this challenge, we propose Meta-learning for In-context Deduction\n(MIND), a novel few-shot meta-learning fine-tuning approach. The goal of MIND\nis to enable models to generalize more effectively to unseen knowledge bases\nand to systematically apply inference rules. Our results show that MIND\nsignificantly improves generalization in small LMs ranging from 1.5B to 7B\nparameters. The benefits are especially pronounced in smaller models and\nlow-data settings. Remarkably, small models fine-tuned with MIND outperform\nstate-of-the-art LLMs, such as GPT-4o and o3-mini, on this task."}
{"id": "2505.14347", "pdf": "https://arxiv.org/pdf/2505.14347.pdf", "abs": "https://arxiv.org/abs/2505.14347", "title": "QA-prompting: Improving Summarization with Large Language Models using Question-Answering", "authors": ["Neelabh Sinha"], "categories": ["cs.CL"], "comment": "Submitted to ARR", "summary": "Language Models (LMs) have revolutionized natural language processing,\nenabling high-quality text generation through prompting and in-context\nlearning. However, models often struggle with long-context summarization due to\npositional biases, leading to suboptimal extraction of critical information.\nThere are techniques to improve this with fine-tuning, pipelining, or using\ncomplex techniques, which have their own challenges. To solve these challenges,\nwe propose QA-prompting - a simple prompting method for summarization that\nutilizes question-answering as an intermediate step prior to summary\ngeneration. Our method extracts key information and enriches the context of\ntext to mitigate positional biases and improve summarization in a single LM\ncall per task without requiring fine-tuning or pipelining. Experiments on\nmultiple datasets belonging to different domains using ten state-of-the-art\npre-trained models demonstrate that QA-prompting outperforms baseline and other\nstate-of-the-art methods, achieving up to 29% improvement in ROUGE scores. This\nprovides an effective and scalable solution for summarization and highlights\nthe importance of domain-specific question selection for optimal performance."}
{"id": "2505.14350", "pdf": "https://arxiv.org/pdf/2505.14350.pdf", "abs": "https://arxiv.org/abs/2505.14350", "title": "OSoRA: Output-Dimension and Singular-Value Initialized Low-Rank Adaptation", "authors": ["Jialong Han", "Si Zhang", "Ke Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning Large Language Models (LLMs) has become increasingly challenging\ndue to their massive scale and associated computational costs.\nParameter-Efficient Fine-Tuning (PEFT) methodologies have been proposed as\ncomputational alternatives; however, their implementations still require\nsignificant resources. In this paper, we present OSoRA (Output-Dimension and\nSingular-Value Initialized Low-Rank Adaptation), a novel PEFT method for LLMs.\nOSoRA extends Low-Rank Adaptation (LoRA) by integrating Singular Value\nDecomposition (SVD) with learnable scaling vectors in a unified framework. It\nfirst performs an SVD of pre-trained weight matrices, then optimizes an\noutput-dimension vector during training, while keeping the corresponding\nsingular vector matrices frozen. OSoRA substantially reduces computational\nresource requirements by minimizing the number of trainable parameters during\nfine-tuning. Comprehensive evaluations across mathematical reasoning, common\nsense reasoning, and other benchmarks demonstrate that OSoRA achieves\ncomparable or superior performance to state-of-the-art methods like LoRA and\nVeRA, while maintaining a linear parameter scaling even as the rank increases\nto higher dimensions. Our ablation studies further confirm that jointly\ntraining both the singular values and the output-dimension vector is critical\nfor optimal performance."}
{"id": "2505.14354", "pdf": "https://arxiv.org/pdf/2505.14354.pdf", "abs": "https://arxiv.org/abs/2505.14354", "title": "WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications", "authors": ["Xin Li", "Mengbing Liu", "Li Wei", "Jiancheng An", "Mérouane Debbah", "Chau Yuen"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large Language Models (LLMs) have achieved impressive results across a broad\narray of tasks, yet their capacity for complex, domain-specific mathematical\nreasoning-particularly in wireless communications-remains underexplored. In\nthis work, we introduce WirelessMathBench, a novel benchmark specifically\ndesigned to evaluate LLMs on mathematical modeling challenges to wireless\ncommunications engineering. Our benchmark consists of 587 meticulously curated\nquestions sourced from 40 state-of-the-art research papers, encompassing a\ndiverse spectrum of tasks ranging from basic multiple-choice questions to\ncomplex equation completion tasks, including both partial and full completions,\nall of which rigorously adhere to physical and dimensional constraints. Through\nextensive experimentation with leading LLMs, we observe that while many models\nexcel in basic recall tasks, their performance degrades significantly when\nreconstructing partially or fully obscured equations, exposing fundamental\nlimitations in current LLMs. Even DeepSeek-R1, the best performer on our\nbenchmark, achieves an average accuracy of only 38.05%, with a mere 7.83%\nsuccess rate in full equation completion. By publicly releasing\nWirelessMathBench along with the evaluation toolkit, we aim to advance the\ndevelopment of more robust, domain-aware LLMs for wireless system analysis and\nbroader engineering applications."}
{"id": "2505.14367", "pdf": "https://arxiv.org/pdf/2505.14367.pdf", "abs": "https://arxiv.org/abs/2505.14367", "title": "Dual Decomposition of Weights and Singular Value Low Rank Adaptation", "authors": ["Jialong Han", "Si Zhang", "Ke Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical paradigm for\nadapting Large Language Models (LLMs) to downstream tasks, among which Low-rank\nAdaptation (LoRA) represents one of the most widely adopted methodologies.\nHowever, existing LoRA-based approaches exhibit two fundamental limitations:\nunstable training dynamics and inefficient knowledge transfer from pre-trained\nmodels, both stemming from random initialization of adapter parameters. To\novercome these challenges, we propose DuDe, a novel approach that decomposes\nweight matrices into magnitude and direction components, employing Singular\nValue Decomposition (SVD) for principled initialization. Our comprehensive\nevaluation demonstrates DuDe's superior performance and robustness, achieving\nup to 48.35\\% accuracy on MMLU and 62.53\\% ($\\pm$ 1.59) accuracy on GSM8K. Our\ntheoretical analysis and empirical validation collectively demonstrate that\nDuDe's decomposition strategy enhances optimization stability and better\npreserves pre-trained representations, particularly for domain-specific tasks\nrequiring specialized knowledge. The combination of robust empirical\nperformance and rigorous theoretical foundations establishes DuDe as a\nsignificant contribution to PEFT methodologies for LLMs."}
{"id": "2505.14376", "pdf": "https://arxiv.org/pdf/2505.14376.pdf", "abs": "https://arxiv.org/abs/2505.14376", "title": "AutoRev: Automatic Peer Review System for Academic Research Papers", "authors": ["Maitreya Prafulla Chitale", "Ketaki Mangesh Shetye", "Harshit Gupta", "Manav Chaudhary", "Vasudeva Varma"], "categories": ["cs.CL"], "comment": null, "summary": "Generating a review for an academic research paper is a complex task that\nrequires a deep understanding of the document's content and the\ninterdependencies between its sections. It demands not only insight into\ntechnical details but also an appreciation of the paper's overall coherence and\nstructure. Recent methods have predominantly focused on fine-tuning large\nlanguage models (LLMs) to address this challenge. However, they often overlook\nthe computational and performance limitations imposed by long input token\nlengths. To address this, we introduce AutoRev, an Automatic Peer Review System\nfor Academic Research Papers. Our novel framework represents an academic\ndocument as a graph, enabling the extraction of the most critical passages that\ncontribute significantly to the review. This graph-based approach demonstrates\neffectiveness for review generation and is potentially adaptable to various\ndownstream tasks, such as question answering, summarization, and document\nrepresentation. When applied to review generation, our method outperforms SOTA\nbaselines by an average of 58.72% across all evaluation metrics. We hope that\nour work will stimulate further research in applying graph-based extraction\ntechniques to other downstream tasks in NLP. We plan to make our code public\nupon acceptance."}
{"id": "2505.14393", "pdf": "https://arxiv.org/pdf/2505.14393.pdf", "abs": "https://arxiv.org/abs/2505.14393", "title": "Editing Across Languages: A Survey of Multilingual Knowledge Editing", "authors": ["Nadir Durrani", "Basel Mousi", "Fahim Dalvi"], "categories": ["cs.CL"], "comment": null, "summary": "While Knowledge Editing has been extensively studied in monolingual settings,\nit remains underexplored in multilingual contexts. This survey systematizes\nrecent research on Multilingual Knowledge Editing (MKE), a growing subdomain of\nmodel editing focused on ensuring factual edits generalize reliably across\nlanguages. We present a comprehensive taxonomy of MKE methods, covering\nparameter-based, memory-based, fine-tuning, and hypernetwork approaches. We\nsurvey available benchmarks,summarize key findings on method effectiveness and\ntransfer patterns, identify challenges in cross-lingual propagation, and\nhighlight open problems related to language anisotropy, evaluation coverage,\nand edit scalability. Our analysis consolidates a rapidly evolving area and\nlays the groundwork for future progress in editable language-aware LLMs."}
{"id": "2505.14395", "pdf": "https://arxiv.org/pdf/2505.14395.pdf", "abs": "https://arxiv.org/abs/2505.14395", "title": "MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language", "authors": ["Seyoung Song", "Seogyeong Jeong", "Eunsu Kim", "Jiho Jin", "Dongkwan Kim", "Jay Shin", "Alice Oh"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating text generation capabilities of large language models (LLMs) is\nchallenging, particularly for low-resource languages where methods for direct\nassessment are scarce. We propose MUG-Eval, a novel framework that evaluates\nLLMs' multilingual generation capabilities by transforming existing benchmarks\ninto conversational tasks and measuring the LLMs' accuracies on those tasks. We\nspecifically designed these conversational tasks to require effective\ncommunication in the target language. Then, we simply use task success rate as\na proxy of successful conversation generation. Our approach offers two key\nadvantages: it is independent of language-specific NLP tools or annotated\ndatasets, which are limited for most languages, and it does not rely on\nLLMs-as-judges, whose evaluation quality degrades outside a few high-resource\nlanguages. We evaluate 8 LLMs across 30 languages spanning high, mid, and\nlow-resource categories, and we find that MUG-Eval correlates strongly with\nestablished benchmarks ($r$ > 0.75) while enabling standardized comparisons\nacross languages and models. Our framework provides a robust and\nresource-efficient solution for evaluating multilingual generation that can be\nextended to thousands of languages."}
{"id": "2505.14398", "pdf": "https://arxiv.org/pdf/2505.14398.pdf", "abs": "https://arxiv.org/abs/2505.14398", "title": "Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation", "authors": ["Peter Baile Chen", "Yi Zhang", "Dan Roth", "Samuel Madden", "Jacob Andreas", "Michael Cafarella"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Data and code are available at https://peterbaile.github.io/lag/", "summary": "While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques."}
{"id": "2505.14406", "pdf": "https://arxiv.org/pdf/2505.14406.pdf", "abs": "https://arxiv.org/abs/2505.14406", "title": "Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via Knowledge Circuit Analysis", "authors": ["Haoming Huang", "Yibo Yan", "Jiahao Huo", "Xin Zou", "Xinfeng Li", "Kun Wang", "Xuming Hu"], "categories": ["cs.CL"], "comment": "18 pages, 6 figures, EMNLP under review", "summary": "Large Language Models (LLMs), despite their remarkable capabilities, are\nhampered by hallucinations. A particularly challenging variant, knowledge\novershadowing, occurs when one piece of activated knowledge inadvertently masks\nanother relevant piece, leading to erroneous outputs even with high-quality\ntraining data. Current understanding of overshadowing is largely confined to\ninference-time observations, lacking deep insights into its origins and\ninternal mechanisms during model training. Therefore, we introduce\nPhantomCircuit, a novel framework designed to comprehensively analyze and\ndetect knowledge overshadowing. By innovatively employing knowledge circuit\nanalysis, PhantomCircuit dissects the internal workings of attention heads,\ntracing how competing knowledge pathways contribute to the overshadowing\nphenomenon and its evolution throughout the training process. Extensive\nexperiments demonstrate PhantomCircuit's effectiveness in identifying such\ninstances, offering novel insights into this elusive hallucination and\nproviding the research community with a new methodological lens for its\npotential mitigation."}
{"id": "2505.14418", "pdf": "https://arxiv.org/pdf/2505.14418.pdf", "abs": "https://arxiv.org/abs/2505.14418", "title": "Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents", "authors": ["Pengzhou Cheng", "Haowen Hu", "Zheng Wu", "Zongru Wu", "Tianjie Ju", "Daizong Ding", "Zhuosheng Zhang", "Gongshen Liu"], "categories": ["cs.CL"], "comment": "25 pages, 10 figures, 12 Tables", "summary": "Graphical user interface (GUI) agents powered by multimodal large language\nmodels (MLLMs) have shown greater promise for human-interaction. However, due\nto the high fine-tuning cost, users often rely on open-source GUI agents or\nAPIs offered by AI providers, which introduces a critical but underexplored\nsupply chain threat: backdoor attacks. In this work, we first unveil that\nMLLM-powered GUI agents naturally expose multiple interaction-level triggers,\nsuch as historical steps, environment states, and task progress. Based on this\nobservation, we introduce AgentGhost, an effective and stealthy framework for\nred-teaming backdoor attacks. Specifically, we first construct composite\ntriggers by combining goal and interaction levels, allowing GUI agents to\nunintentionally activate backdoors while ensuring task utility. Then, we\nformulate backdoor injection as a Min-Max optimization problem that uses\nsupervised contrastive learning to maximize the feature difference across\nsample classes at the representation space, improving flexibility of the\nbackdoor. Meanwhile, it adopts supervised fine-tuning to minimize the\ndiscrepancy between backdoor and clean behavior generation, enhancing\neffectiveness and utility. Extensive evaluations of various agent models in two\nestablished mobile benchmarks show that AgentGhost is effective and generic,\nwith attack accuracy that reaches 99.7\\% on three attack objectives, and shows\nstealthiness with only 1\\% utility degradation. Furthermore, we tailor a\ndefense method against AgentGhost that reduces the attack accuracy to 22.1\\%.\nOur code is available at \\texttt{anonymous}."}
{"id": "2505.14420", "pdf": "https://arxiv.org/pdf/2505.14420.pdf", "abs": "https://arxiv.org/abs/2505.14420", "title": "SAE-FiRE: Enhancing Earnings Surprise Predictions Through Sparse Autoencoder Feature Selection", "authors": ["Huopu Zhang", "Yanguang Liu", "Mengnan Du"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Predicting earnings surprises through the analysis of earnings conference\ncall transcripts has attracted increasing attention from the financial research\ncommunity. Conference calls serve as critical communication channels between\ncompany executives, analysts, and shareholders, offering valuable\nforward-looking information. However, these transcripts present significant\nanalytical challenges, typically containing over 5,000 words with substantial\nredundancy and industry-specific terminology that creates obstacles for\nlanguage models. In this work, we propose the Sparse Autoencoder for Financial\nRepresentation Enhancement (SAE-FiRE) framework to address these limitations by\nextracting key information while eliminating redundancy. SAE-FiRE employs\nSparse Autoencoders (SAEs) to efficiently identify patterns and filter out\nnoises, and focusing specifically on capturing nuanced financial signals that\nhave predictive power for earnings surprises. Experimental results indicate\nthat the proposed method can significantly outperform comparing baselines."}
{"id": "2505.14423", "pdf": "https://arxiv.org/pdf/2505.14423.pdf", "abs": "https://arxiv.org/abs/2505.14423", "title": "Scaling Low-Resource MT via Synthetic Data Generation with LLMs", "authors": ["Ona de Gibert", "Joseph Attieh", "Teemu Vahtola", "Mikko Aulamo", "Zihao Li", "Raúl Vázquez", "Tiancheng Hu", "Jörg Tiedemann"], "categories": ["cs.CL"], "comment": null, "summary": "We investigate the potential of LLM-generated synthetic data for improving\nlow-resource machine translation (MT). Focusing on seven diverse target\nlanguages, we construct a document-level synthetic corpus from English\nEuroparl, and extend it via pivoting to 147 additional language pairs.\nAutomatic and human evaluation confirm its high overall quality. We study its\npractical application by (i) identifying effective training regimes, (ii)\ncomparing our data with the HPLT dataset, and (iii) testing its utility beyond\nEnglish-centric MT. Finally, we introduce SynOPUS, a public repository for\nsynthetic parallel datasets. Our findings show that LLM-generated synthetic\ndata, even when noisy, can substantially improve MT performance for\nlow-resource languages."}
{"id": "2505.14425", "pdf": "https://arxiv.org/pdf/2505.14425.pdf", "abs": "https://arxiv.org/abs/2505.14425", "title": "From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning", "authors": ["Chalamalasetti Kranti", "Sherzod Hakimov", "David Schlangen"], "categories": ["cs.CL"], "comment": "4 pages", "summary": "Instruction-tuned large language models (LLMs) have shown strong performance\non a variety of tasks; however, generalizing from synthetic to human-authored\ninstructions in grounded environments remains a challenge for them. In this\nwork, we study generalization challenges in spatial grounding tasks where\nmodels interpret and translate instructions for building object arrangements on\na $2.5$D grid. We fine-tune LLMs using only synthetic instructions and evaluate\ntheir performance on a benchmark dataset containing both synthetic and\nhuman-written instructions. Our results reveal that while models generalize\nwell on simple tasks, their performance degrades significantly on more complex\ntasks. We present a detailed error analysis of the gaps in instruction\ngeneralization."}
{"id": "2505.14436", "pdf": "https://arxiv.org/pdf/2505.14436.pdf", "abs": "https://arxiv.org/abs/2505.14436", "title": "Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models", "authors": ["Yuqiao Tan", "Shizhu He", "Kang Liu", "Jun Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL'25 Main. Code link:\n  https://github.com/Trae1ounG/Neural_Incompatibility", "summary": "Large Language Models (LLMs) offer a transparent brain with accessible\nparameters that encode extensive knowledge, which can be analyzed, located and\ntransferred. Consequently, a key research challenge is to transcend traditional\nknowledge transfer paradigms rooted in symbolic language and achieve genuine\nParametric Knowledge Transfer (PKT). Significantly, exploring effective methods\nfor transferring knowledge across LLMs of different scales through parameters\npresents an intriguing and valuable research direction. In this paper, we first\ndemonstrate $\\textbf{Alignment}$ in parametric space is the fundamental\nprerequisite to achieve successful cross-scale PKT. We redefine the previously\nexplored knowledge transfer as Post-Align PKT (PostPKT), which utilizes\nextracted parameters for LoRA initialization and requires subsequent fine-tune\nfor alignment. Hence, to reduce cost for further fine-tuning, we introduce a\nnovel Pre-Align PKT (PrePKT) paradigm and propose a solution called\n$\\textbf{LaTen}$\n($\\textbf{L}$oc$\\textbf{a}$te-$\\textbf{T}$h$\\textbf{e}$n-Alig$\\textbf{n}$) that\naligns the parametric spaces of LLMs across scales only using several training\nsteps without following training. Comprehensive experiments on four benchmarks\ndemonstrate that both PostPKT and PrePKT face challenges in achieving\nconsistently stable transfer. Through in-depth analysis, we identify\n$\\textbf{Neural Incompatibility}$ as the ethological and parametric structural\ndifferences between LLMs of varying scales, presenting fundamental challenges\nto achieving effective PKT. These findings provide fresh insights into the\nparametric architectures of LLMs and highlight promising directions for future\nresearch on efficient PKT. Our code is available at\nhttps://github.com/Trae1ounG/Neural_Incompatibility."}
{"id": "2505.14442", "pdf": "https://arxiv.org/pdf/2505.14442.pdf", "abs": "https://arxiv.org/abs/2505.14442", "title": "Creative Preference Optimization", "authors": ["Mete Ismayilzada", "Antonio Laverghetta Jr.", "Simone A. Luchini", "Reet Patel", "Antoine Bosselut", "Lonneke van der Plas", "Roger Beaty"], "categories": ["cs.CL", "cs.AI"], "comment": "27 pages", "summary": "While Large Language Models (LLMs) have demonstrated impressive performance\nacross natural language generation tasks, their ability to generate truly\ncreative content-characterized by novelty, diversity, surprise, and\nquality-remains limited. Existing methods for enhancing LLM creativity often\nfocus narrowly on diversity or specific tasks, failing to address creativity's\nmultifaceted nature in a generalizable way. In this work, we propose Creative\nPreference Optimization (CrPO), a novel alignment method that injects signals\nfrom multiple creativity dimensions into the preference optimization objective\nin a modular fashion. We train and evaluate creativity-augmented versions of\nseveral models using CrPO and MuCE, a new large-scale human preference dataset\nspanning over 200,000 human-generated responses and ratings from more than 30\npsychological creativity assessments. Our models outperform strong baselines,\nincluding GPT-4o, on both automated and human evaluations, producing more\nnovel, diverse, and surprising generations while maintaining high output\nquality. Additional evaluations on NoveltyBench further confirm the\ngeneralizability of our approach. Together, our results demonstrate that\ndirectly optimizing for creativity within preference frameworks is a promising\ndirection for advancing the creative capabilities of LLMs without compromising\noutput quality."}
{"id": "2505.14455", "pdf": "https://arxiv.org/pdf/2505.14455.pdf", "abs": "https://arxiv.org/abs/2505.14455", "title": "CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation", "authors": ["Chihan Huang", "Hao Tang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although autoregressive models have dominated language modeling in recent\nyears, there has been a growing interest in exploring alternative paradigms to\nthe conventional next-token prediction framework. Diffusion-based language\nmodels have emerged as a compelling alternative due to their powerful parallel\ngeneration capabilities and inherent editability. However, these models are\noften constrained by fixed-length generation. A promising direction is to\ncombine the strengths of both paradigms, segmenting sequences into blocks,\nmodeling autoregressive dependencies across blocks while leveraging discrete\ndiffusion to estimate the conditional distribution within each block given the\npreceding context. Nevertheless, their practical application is often hindered\nby two key limitations: rigid fixed-length outputs and a lack of flexible\ncontrol mechanisms. In this work, we address the critical limitations of fixed\ngranularity and weak controllability in current large diffusion language\nmodels. We propose CtrlDiff, a dynamic and controllable semi-autoregressive\nframework that adaptively determines the size of each generation block based on\nlocal semantics using reinforcement learning. Furthermore, we introduce a\nclassifier-guided control mechanism tailored to discrete diffusion, which\nsignificantly reduces computational overhead while facilitating efficient\npost-hoc conditioning without retraining. Extensive experiments demonstrate\nthat CtrlDiff sets a new standard among hybrid diffusion models, narrows the\nperformance gap to state-of-the-art autoregressive approaches, and enables\neffective conditional text generation across diverse tasks."}
{"id": "2505.14464", "pdf": "https://arxiv.org/pdf/2505.14464.pdf", "abs": "https://arxiv.org/abs/2505.14464", "title": "Not All Correct Answers Are Equal: Why Your Distillation Source Matters", "authors": ["Xiaoyu Tian", "Yunjie Ji", "Haotian Wang", "Shuaiting Chen", "Sitong Zhao", "Yiping Peng", "Han Zhao", "Xiangang Li"], "categories": ["cs.CL"], "comment": null, "summary": "Distillation has emerged as a practical and effective approach to enhance the\nreasoning capabilities of open-source language models. In this work, we conduct\na large-scale empirical study on reasoning data distillation by collecting\nverified outputs from three state-of-the-art teacher models-AM-Thinking-v1,\nQwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We\nconstruct three parallel datasets and analyze their distributions, revealing\nthat AM-Thinking-v1-distilled data exhibits greater token length diversity and\nlower perplexity. Student models trained on each dataset are evaluated on\nreasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench.\nThe AM-based model consistently achieves the best performance (e.g., 84.3 on\nAIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and\ndemonstrates adaptive output behavior-producing longer responses for harder\ntasks and shorter ones for simpler tasks. These findings highlight the value of\nhigh-quality, verified reasoning traces. We release the AM-Thinking-v1 and\nQwen3-235B-A22B distilled datasets to support future research on open and\nhigh-performing reasoning-oriented language models. The datasets are publicly\navailable on Hugging Face\\footnote{Datasets are available on Hugging Face:\n\\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled}{AM-Thinking-v1-Distilled},\n\\href{https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled}{AM-Qwen3-Distilled}.}."}
{"id": "2505.14467", "pdf": "https://arxiv.org/pdf/2505.14467.pdf", "abs": "https://arxiv.org/abs/2505.14467", "title": "Void in Language Models", "authors": ["Mani Shemiranifar"], "categories": ["cs.CL"], "comment": null, "summary": "Despite advances in transformer-based language models (LMs), a fundamental\nquestion remains largely unanswered: Are all layers activated during inference?\nWe investigate this question by detecting unactivated layers (which we refer to\nas Voids) using a non-trainable and parameter-free adaptive computation method\ncalled L2 Adaptive Computation (LAC). We adapt LAC from its original\nefficiency-focused application to trace activated layers during inference. This\nmethod monitors changes in the L2-norm of activations to identify voids. We\nanalyze layer activation in instruction-tuned LMs across two phases: Prompt\nProcessing (PP), where we trace activated layers for each token in the input\nprompts, and Response Generation (RG), where we trace activated layers for each\ngenerated token. We further demonstrate that distinct layers are activated\nduring these two phases. To show the effectiveness of our method, we evaluated\nthree distinct instruction-tuned LMs from the Llama, Mistral, and Qwen families\non three benchmarks: MMLU, GPQA Diamond, and BoolQ. For example, on MMLU with a\nzero-shot setting, skipping voids in Qwen2.5-7B-Instruct resulted in an\nimprovement from 69.24 to 71.29 while the model uses only 30% of the layers.\nSimilarly, Mistral-7B-Instruct-v0.3 on GPQA Diamond improved from 13.88 to\n18.36 when using 70% of the layers during both the PP and RG phases. These\nresults show that not all layers contribute equally during inference, and that\nselectively skipping most of them can improve the performance of models on\ncertain tasks."}
{"id": "2505.14469", "pdf": "https://arxiv.org/pdf/2505.14469.pdf", "abs": "https://arxiv.org/abs/2505.14469", "title": "Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations", "authors": ["Somnath Banerjee", "Pratyush Chatterjee", "Shanu Kumar", "Sayan Layek", "Parag Agrawal", "Rima Hazra", "Animesh Mukherjee"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in LLMs have raised significant safety concerns,\nparticularly when dealing with code-mixed inputs and outputs. Our study\nsystematically investigates the increased susceptibility of LLMs to produce\nunsafe outputs from code-mixed prompts compared to monolingual English prompts.\nUtilizing explainability methods, we dissect the internal attribution shifts\ncausing model's harmful behaviors. In addition, we explore cultural dimensions\nby distinguishing between universally unsafe and culturally-specific unsafe\nqueries. This paper presents novel experimental insights, clarifying the\nmechanisms driving this phenomenon."}
{"id": "2505.14471", "pdf": "https://arxiv.org/pdf/2505.14471.pdf", "abs": "https://arxiv.org/abs/2505.14471", "title": "Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning", "authors": ["Tong Li", "Jiachuan Wang", "Yongqi Zhang", "Shuangyin Li", "Lei Chen"], "categories": ["cs.CL"], "comment": "Manuscripts, accepted to KDD 2025", "summary": "Citation classification, which identifies the intention behind academic\ncitations, is pivotal for scholarly analysis. Previous works suggest\nfine-tuning pretrained language models (PLMs) on citation classification\ndatasets, reaping the reward of the linguistic knowledge they gained during\npretraining. However, directly fine-tuning for citation classification is\nchallenging due to labeled data scarcity, contextual noise, and spurious\nkeyphrase correlations. In this paper, we present a novel framework, Citss,\nthat adapts the PLMs to overcome these challenges. Citss introduces\nself-supervised contrastive learning to alleviate data scarcity, and is\nequipped with two specialized strategies to obtain the contrastive pairs:\nsentence-level cropping, which enhances focus on target citations within long\ncontexts, and keyphrase perturbation, which mitigates reliance on specific\nkeyphrases. Compared with previous works that are only designed for\nencoder-based PLMs, Citss is carefully developed to be compatible with both\nencoder-based PLMs and decoder-based LLMs, to embrace the benefits of enlarged\npretraining. Experiments with three benchmark datasets with both encoder-based\nPLMs and decoder-based LLMs demonstrate our superiority compared to the\nprevious state of the art. Our code is available at: github.com/LITONG99/Citss"}
{"id": "2505.14481", "pdf": "https://arxiv.org/pdf/2505.14481.pdf", "abs": "https://arxiv.org/abs/2505.14481", "title": "PlanGPT-VL: Enhancing Urban Planning with Domain-Specific Vision-Language Models", "authors": ["He Zhu", "Junyou Su", "Minxi Chen", "Wen Wang", "Yijie Deng", "Guanhua Chen", "Wenjia Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "In the field of urban planning, existing Vision-Language Models (VLMs)\nfrequently fail to effectively analyze and evaluate planning maps, despite the\ncritical importance of these visual elements for urban planners and related\neducational contexts. Planning maps, which visualize land use, infrastructure\nlayouts, and functional zoning, require specialized understanding of spatial\nconfigurations, regulatory requirements, and multi-scale analysis. To address\nthis challenge, we introduce PlanGPT-VL, the first domain-specific\nVision-Language Model tailored specifically for urban planning maps. PlanGPT-VL\nemploys three innovative approaches: (1) PlanAnno-V framework for high-quality\nVQA data synthesis, (2) Critical Point Thinking to reduce hallucinations\nthrough structured verification, and (3) comprehensive training methodology\ncombining Supervised Fine-Tuning with frozen vision encoder parameters. Through\nsystematic evaluation on our proposed PlanBench-V benchmark, we demonstrate\nthat PlanGPT-VL significantly outperforms general-purpose state-of-the-art VLMs\nin specialized planning map interpretation tasks, offering urban planning\nprofessionals a reliable tool for map analysis, assessment, and educational\napplications while maintaining high factual accuracy. Our lightweight 7B\nparameter model achieves comparable performance to models exceeding 72B\nparameters, demonstrating efficient domain specialization without sacrificing\nperformance."}
{"id": "2505.14483", "pdf": "https://arxiv.org/pdf/2505.14483.pdf", "abs": "https://arxiv.org/abs/2505.14483", "title": "MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online Governance", "authors": ["Agam Goyal", "Xianyang Zhan", "Yilun Chen", "Koustuv Saha", "Eshwar Chandrasekharan"], "categories": ["cs.CL"], "comment": "Preprint: 15 pages, 4 figures, 2 tables", "summary": "Large language models (LLMs) have shown great potential in flagging harmful\ncontent in online communities. Yet, existing approaches for moderation require\na separate model for every community and are opaque in their decision-making,\nlimiting real-world adoption. We introduce Mixture of Moderation Experts\n(MoMoE), a modular, cross-community framework that adds post-hoc explanations\nto scalable content moderation. MoMoE orchestrates four operators -- Allocate,\nPredict, Aggregate, Explain -- and is instantiated as seven\ncommunity-specialized experts (MoMoE-Community) and five norm-violation experts\n(MoMoE-NormVio). On 30 unseen subreddits, the best variants obtain Micro-F1\nscores of 0.72 and 0.67, respectively, matching or surpassing strong fine-tuned\nbaselines while consistently producing concise and reliable explanations.\nAlthough community-specialized experts deliver the highest peak accuracy,\nnorm-violation experts provide steadier performance across domains. These\nfindings show that MoMoE yields scalable, transparent moderation without\nneeding per-community fine-tuning. More broadly, they suggest that lightweight,\nexplainable expert ensembles can guide future NLP and HCI research on\ntrustworthy human-AI governance of online communities."}
{"id": "2505.14499", "pdf": "https://arxiv.org/pdf/2505.14499.pdf", "abs": "https://arxiv.org/abs/2505.14499", "title": "Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales", "authors": ["Jun Cao", "Jiyi Li", "Ziwei Yang", "Renjie Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "There has been growing interest in Multimodal Aspect-Based Sentiment Analysis\n(MABSA) in recent years. Existing methods predominantly rely on pre-trained\nsmall language models (SLMs) to collect information related to aspects and\nsentiments from both image and text, with an aim to align these two modalities.\nHowever, small SLMs possess limited capacity and knowledge, often resulting in\ninaccurate identification of meaning, aspects, sentiments, and their\ninterconnections in textual and visual data. On the other hand, Large language\nmodels (LLMs) have shown exceptional capabilities in various tasks by\neffectively exploring fine-grained information in multimodal data. However,\nsome studies indicate that LLMs still fall short compared to fine-tuned small\nmodels in the field of ABSA. Based on these findings, we propose a novel\nframework, termed LRSA, which combines the decision-making capabilities of SLMs\nwith additional information provided by LLMs for MABSA. Specifically, we inject\nexplanations generated by LLMs as rationales into SLMs and employ a dual\ncross-attention mechanism for enhancing feature interaction and fusion, thereby\naugmenting the SLMs' ability to identify aspects and sentiments. We evaluated\nour method using two baseline models, numerous experiments highlight the\nsuperiority of our approach on three widely-used benchmarks, indicating its\ngeneralizability and applicability to most pre-trained models for MABSA."}
{"id": "2505.14505", "pdf": "https://arxiv.org/pdf/2505.14505.pdf", "abs": "https://arxiv.org/abs/2505.14505", "title": "ModRWKV: Transformer Multimodality in Linear Time", "authors": ["Jiale Kang", "Ziyin Yue", "Qingyu Yin", "Jiang Rui", "Weile Li", "Zening Lu", "Zhouran Ji"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Currently, most multimodal studies are based on large language models (LLMs)\nwith quadratic-complexity Transformer architectures. While linear models like\nRNNs enjoy low inference costs, their application has been largely limited to\nthe text-only modality. This work explores the capabilities of modern RNN\narchitectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal\nframework built upon the RWKV7 architecture as its LLM backbone-which achieves\nmulti-source information fusion through dynamically adaptable heterogeneous\nmodality encoders. We designed the multimodal modules in ModRWKV with an\nextremely lightweight architecture and, through extensive experiments,\nidentified a configuration that achieves an optimal balance between performance\nand computational efficiency. ModRWKV leverages the pretrained weights of the\nRWKV7 LLM for initialization, which significantly accelerates multimodal\ntraining. Comparative experiments with different pretrained checkpoints further\ndemonstrate that such initialization plays a crucial role in enhancing the\nmodel's ability to understand multimodal signals. Supported by extensive\nexperiments, we conclude that modern RNN architectures present a viable\nalternative to Transformers in the domain of multimodal large language models\n(MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV\narchitecture through systematic exploration."}
{"id": "2505.14523", "pdf": "https://arxiv.org/pdf/2505.14523.pdf", "abs": "https://arxiv.org/abs/2505.14523", "title": "Exploring Graph Representations of Logical Forms for Language Modeling", "authors": ["Michael Sullivan"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "To be published in ACL 2025 Findings", "summary": "We make the case for language models over logical forms (LFLMs), arguing that\nsuch models are more data-efficient than their textual counterparts. To that\nend, we introduce the Graph-based Formal-Logical Distributional Semantics\n(GFoLDS) prototype, a pretrained LM over graph representations of logical\nforms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong\nexperimental evidence that LFLMs can leverage the built-in, basic linguistic\nknowledge inherent in such models to immediately begin learning more complex\npatterns. On downstream tasks, we show that GFoLDS vastly outperforms textual,\ntransformer LMs pretrained on similar amounts of data, indicating that LFLMs\ncan learn with substantially less data than models over plain text.\nFurthermore, we show that the performance of this model is likely to scale with\nadditional parameters and pretraining data, suggesting the viability of LFLMs\nin real-world applications."}
{"id": "2505.14530", "pdf": "https://arxiv.org/pdf/2505.14530.pdf", "abs": "https://arxiv.org/abs/2505.14530", "title": "Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs", "authors": ["Zhipeng Yang", "Junzhuo Li", "Siyu Xia", "Xuming Hu"], "categories": ["cs.CL", "cs.LG"], "comment": "27 pages, 17 figures", "summary": "We show that large language models (LLMs) exhibit an $\\textit{internal\nchain-of-thought}$: they sequentially decompose and execute composite tasks\nlayer-by-layer. Two claims ground our study: (i) distinct subtasks are learned\nat different network depths, and (ii) these subtasks are executed sequentially\nacross layers. On a benchmark of 15 two-step composite tasks, we employ\nlayer-from context-masking and propose a novel cross-task patching method,\nconfirming (i). To examine claim (ii), we apply LogitLens to decode hidden\nstates, revealing a consistent layerwise execution pattern. We further\nreplicate our analysis on the real-world $\\text{TRACE}$ benchmark, observing\nthe same stepwise dynamics. Together, our results enhance LLMs transparency by\nshowing their capacity to internally plan and execute subtasks (or\ninstructions), opening avenues for fine-grained, instruction-level activation\nsteering."}
{"id": "2505.14536", "pdf": "https://arxiv.org/pdf/2505.14536.pdf", "abs": "https://arxiv.org/abs/2505.14536", "title": "Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders", "authors": ["Agam Goyal", "Vedant Rathi", "William Yeh", "Yian Wang", "Yuen Chen", "Hari Sundaram"], "categories": ["cs.CL"], "comment": "Preprint: 19 pages, 7 figures, 1 table", "summary": "Large language models (LLMs) are now ubiquitous in user-facing applications,\nyet they still generate undesirable toxic outputs, including profanity,\nvulgarity, and derogatory remarks. Although numerous detoxification methods\nexist, most apply broad, surface-level fixes and can therefore easily be\ncircumvented by jailbreak attacks. In this paper we leverage sparse\nautoencoders (SAEs) to identify toxicity-related directions in the residual\nstream of models and perform targeted activation steering using the\ncorresponding decoder vectors. We introduce three tiers of steering\naggressiveness and evaluate them on GPT-2 Small and Gemma-2-2B, revealing\ntrade-offs between toxicity reduction and language fluency. At stronger\nsteering strengths, these causal interventions surpass competitive baselines in\nreducing toxicity by up to 20%, though fluency can degrade noticeably on GPT-2\nSmall depending on the aggressiveness. Crucially, standard NLP benchmark scores\nupon steering remain stable, indicating that the model's knowledge and general\nabilities are preserved. We further show that feature-splitting in wider SAEs\nhampers safety interventions, underscoring the importance of disentangled\nfeature learning. Our findings highlight both the promise and the current\nlimitations of SAE-based causal interventions for LLM detoxification, further\nsuggesting practical guidelines for safer language-model deployment."}
{"id": "2505.14552", "pdf": "https://arxiv.org/pdf/2505.14552.pdf", "abs": "https://arxiv.org/abs/2505.14552", "title": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation", "authors": ["Jiajun Shi", "Jian Yang", "Jiaheng Liu", "Xingyuan Bu", "Jiangjie Chen", "Junting Zhou", "Kaijing Ma", "Zhoufutu Wen", "Bingli Wang", "Yancheng He", "Liang Song", "Hualei Zhu", "Shilong Li", "Xingjian Wang", "Wei Zhang", "Ruibin Yuan", "Yifan Yao", "Wenjun Yang", "Yunli Wang", "Siyuan Fang", "Siyu Yuan", "Qianyu He", "Xiangru Tang", "Yingshui Tan", "Wangchunshu Zhou", "Zhaoxiang Zhang", "Zhoujun Li", "Wenhao Huang", "Ge Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "22 pages", "summary": "Recent advancements in large language models (LLMs) underscore the need for\nmore comprehensive evaluation methods to accurately assess their reasoning\ncapabilities. Existing benchmarks are often domain-specific and thus cannot\nfully capture an LLM's general reasoning potential. To address this limitation,\nwe introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic\nevaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over\nfifty games in either textual or visual formats and supports interactive,\nmulti-turn assessments with reinforcement learning scenarios. Using KORGym, we\nconduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent\nreasoning patterns within model families and demonstrating the superior\nperformance of closed-source models. Further analysis examines the effects of\nmodality, reasoning strategies, reinforcement learning techniques, and response\nlength on model performance. We expect KORGym to become a valuable resource for\nadvancing LLM reasoning research and developing evaluation methodologies suited\nto complex, interactive environments."}
{"id": "2505.14553", "pdf": "https://arxiv.org/pdf/2505.14553.pdf", "abs": "https://arxiv.org/abs/2505.14553", "title": "Pivot Language for Low-Resource Machine Translation", "authors": ["Abhimanyu Talwar", "Julien Laasri"], "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7"], "comment": "7 pages, 3 figures, paper dated May 13, 2019", "summary": "Certain pairs of languages suffer from lack of a parallel corpus which is\nlarge in size and diverse in domain. One of the ways this is overcome is via\nuse of a pivot language. In this paper we use Hindi as a pivot language to\ntranslate Nepali into English. We describe what makes Hindi a good candidate\nfor the pivot. We discuss ways in which a pivot language can be used, and use\ntwo such approaches - the Transfer Method (fully supervised) and\nBacktranslation (semi-supervised) - to translate Nepali into English. Using the\nformer, we are able to achieve a devtest Set SacreBLEU score of 14.2, which\nimproves the baseline fully supervised score reported by (Guzman et al., 2019)\nby 6.6 points. While we are slightly below the semi-supervised baseline score\nof 15.1, we discuss what may have caused this under-performance, and suggest\nscope for future work."}
{"id": "2505.14577", "pdf": "https://arxiv.org/pdf/2505.14577.pdf", "abs": "https://arxiv.org/abs/2505.14577", "title": "TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring", "authors": ["Sohaila Eltanbouly", "Salam Albatarni", "Tamer Elsayed"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 Findings", "summary": "Research on holistic Automated Essay Scoring (AES) is long-dated; yet, there\nis a notable lack of attention for assessing essays according to individual\ntraits. In this work, we propose TRATES, a novel trait-specific and\nrubric-based cross-prompt AES framework that is generic yet specific to the\nunderlying trait. The framework leverages a Large Language Model (LLM) that\nutilizes the trait grading rubrics to generate trait-specific features\n(represented by assessment questions), then assesses those features given an\nessay. The trait-specific features are eventually combined with generic\nwriting-quality and prompt-specific features to train a simple classical\nregression model that predicts trait scores of essays from an unseen prompt.\nExperiments show that TRATES achieves a new state-of-the-art performance across\nall traits on a widely-used dataset, with the generated LLM-based features\nbeing the most significant."}
{"id": "2505.14582", "pdf": "https://arxiv.org/pdf/2505.14582.pdf", "abs": "https://arxiv.org/abs/2505.14582", "title": "Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning", "authors": ["Shangziqi Zhao", "Jiahao Yuan", "Guisong Yang", "Usman Naseem"], "categories": ["cs.CL"], "comment": "17 pages,4 figures", "summary": "Long chain-of-thought (Long-CoT) reasoning improves accuracy in LLMs, yet its\nverbose, self-reflective style often hinders effective distillation into small\nlanguage models (SLMs). We revisit Long-CoT compression through the lens of\ncapability alignment and ask: Can pruning improve reasoning? We propose\nPrune-on-Logic, a structure-aware framework that transforms Long-CoT into logic\ngraphs and selectively prunes low-utility reasoning steps under\nself-verification constraints. Through systematic analysis across three pruning\nstrategies -- targeting entire chains, core reasoning, and verification -- we\nfind that pruning verification steps yields consistent accuracy gains while\nreducing inference cost, outperforming token-level baselines and uncompressed\nfine-tuning. In contrast, pruning reasoning or all-chain steps degrades\nperformance, revealing that small models benefit not from shorter CoTs, but\nfrom semantically leaner ones. Our findings highlight pruning as a structural\noptimization strategy for aligning CoT reasoning with SLM capacity."}
{"id": "2505.14585", "pdf": "https://arxiv.org/pdf/2505.14585.pdf", "abs": "https://arxiv.org/abs/2505.14585", "title": "Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning", "authors": ["Wenbin Hu", "Haoran Li", "Huihao Jing", "Qi Hu", "Ziqian Zeng", "Sirui Han", "Heli Xu", "Tianshu Chu", "Peizhao Hu", "Yangqiu Song"], "categories": ["cs.CL"], "comment": null, "summary": "While Large Language Models (LLMs) exhibit remarkable capabilities, they also\nintroduce significant safety and privacy risks. Current mitigation strategies\noften fail to preserve contextual reasoning capabilities in risky scenarios.\nInstead, they rely heavily on sensitive pattern matching to protect LLMs, which\nlimits the scope. Furthermore, they overlook established safety and privacy\nstandards, leading to systemic risks for legal compliance. To address these\ngaps, we formulate safety and privacy issues into contextualized compliance\nproblems following the Contextual Integrity (CI) theory. Under the CI\nframework, we align our model with three critical regulatory standards: GDPR,\nEU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with\na rule-based reward to incentivize contextual reasoning capabilities while\nenhancing compliance with safety and privacy norms. Through extensive\nexperiments, we demonstrate that our method not only significantly enhances\nlegal compliance (achieving a +17.64% accuracy improvement in safety/privacy\nbenchmarks) but also further improves general reasoning capability. For\nOpenThinker-7B, a strong reasoning model that significantly outperforms its\nbase model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its\ngeneral reasoning capabilities, with +2.05% and +8.98% accuracy improvement on\nthe MMLU and LegalBench benchmark, respectively."}
{"id": "2505.14590", "pdf": "https://arxiv.org/pdf/2505.14590.pdf", "abs": "https://arxiv.org/abs/2505.14590", "title": "MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol", "authors": ["Huihao Jing", "Haoran Li", "Wenbin Hu", "Qi Hu", "Heli Xu", "Tianshu Chu", "Peizhao Hu", "Yangqiu Song"], "categories": ["cs.CL"], "comment": "17 pages", "summary": "As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users\nand developers, it also brings underexplored safety risks. Its decentralized\narchitecture, which separates clients and servers, poses unique challenges for\nsystematic safety analysis. This paper proposes a novel framework to enhance\nMCP safety. Guided by the MAESTRO framework, we first analyze the missing\nsafety mechanisms in MCP, and based on this analysis, we propose the Model\nContextual Integrity Protocol (MCIP), a refined version of MCP that addresses\nthese gaps.Next, we develop a fine-grained taxonomy that captures a diverse\nrange of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,\nwe develop benchmark and training data that support the evaluation and\nimprovement of LLMs' capabilities in identifying safety risks within MCP\ninteractions. Leveraging the proposed benchmark and training data, we conduct\nextensive experiments on state-of-the-art LLMs. The results highlight LLMs'\nvulnerabilities in MCP interactions and demonstrate that our approach\nsubstantially improves their safety performance."}
{"id": "2505.14597", "pdf": "https://arxiv.org/pdf/2505.14597.pdf", "abs": "https://arxiv.org/abs/2505.14597", "title": "Success is in the Details: Evaluate and Enhance Details Sensitivity of Code LLMs through Counterfactuals", "authors": ["Xianzhen Luo", "Qingfu Zhu", "Zhiming Zhang", "Mingzheng Xu", "Tianhao Cheng", "Yixuan Wang", "Zheng Chu", "Shijie Xuyang", "Zhiyuan Ma", "YuanTao Fan", "Wanxiang Che"], "categories": ["cs.CL"], "comment": "Code & Model is https://github.com/Luowaterbi/CTF-Instruct", "summary": "Code Sensitivity refers to the ability of Code LLMs to recognize and respond\nto details changes in problem descriptions. While current code benchmarks and\ninstruction data focus on difficulty and diversity, sensitivity is overlooked.\nWe first introduce the CTF-Code benchmark, constructed using counterfactual\nperturbations, minimizing input changes while maximizing output changes. The\nevaluation shows that many LLMs have a more than 10\\% performance drop compared\nto the original problems. To fully utilize sensitivity, CTF-Instruct, an\nincremental instruction fine-tuning framework, extends on existing data and\nuses a selection mechanism to meet the three dimensions of difficulty,\ndiversity, and sensitivity. Experiments show that LLMs fine-tuned with\nCTF-Instruct data achieve over a 2\\% improvement on CTF-Code, and more than a\n10\\% performance boost on LiveCodeBench, validating the feasibility of\nenhancing LLMs' sensitivity to improve performance."}
{"id": "2505.14599", "pdf": "https://arxiv.org/pdf/2505.14599.pdf", "abs": "https://arxiv.org/abs/2505.14599", "title": "Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models", "authors": ["Guangzhi Xiong", "Eric Xie", "Corey Williams", "Myles Kim", "Amir Hassan Shariatmadari", "Sikun Guo", "Stefan Bekiranov", "Aidong Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to IJCAI 2025", "summary": "Large language models (LLMs) have shown significant potential in scientific\ndisciplines such as biomedicine, particularly in hypothesis generation, where\nthey can analyze vast literature, identify patterns, and suggest research\ndirections. However, a key challenge lies in evaluating the truthfulness of\ngenerated hypotheses, as verifying their accuracy often requires substantial\ntime and resources. Additionally, the hallucination problem in LLMs can lead to\nthe generation of hypotheses that appear plausible but are ultimately\nincorrect, undermining their reliability. To facilitate the systematic study of\nthese challenges, we introduce TruthHypo, a benchmark for assessing the\ncapabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,\na knowledge-based hallucination detector to evaluate how well hypotheses are\ngrounded in existing knowledge. Our results show that LLMs struggle to generate\ntruthful hypotheses. By analyzing hallucinations in reasoning steps, we\ndemonstrate that the groundedness scores provided by KnowHD serve as an\neffective metric for filtering truthful hypotheses from the diverse outputs of\nLLMs. Human evaluations further validate the utility of KnowHD in identifying\ntruthful hypotheses and accelerating scientific discovery. Our data and source\ncode are available at https://github.com/Teddy-XiongGZ/TruthHypo."}
{"id": "2505.14607", "pdf": "https://arxiv.org/pdf/2505.14607.pdf", "abs": "https://arxiv.org/abs/2505.14607", "title": "sudoLLM : On Multi-role Alignment of Language Models", "authors": ["Soumadeep Saha", "Akshay Chaturvedi", "Joy Mahapatra", "Utpal Garain"], "categories": ["cs.CL", "cs.CR", "I.2.7"], "comment": "Under review. Code and data to be released later", "summary": "User authorization-based access privileges are a key feature in many\nsafety-critical systems, but have thus far been absent from the large language\nmodel (LLM) realm. In this work, drawing inspiration from such access control\nsystems, we introduce sudoLLM, a novel framework that results in multi-role\naligned LLMs, i.e., LLMs that account for, and behave in accordance with, user\naccess rights. sudoLLM injects subtle user-based biases into queries and trains\nan LLM to utilize this bias signal in order to produce sensitive information if\nand only if the user is authorized. We present empirical results demonstrating\nthat this approach shows substantially improved alignment, generalization, and\nresistance to prompt-based jailbreaking attacks. The persistent tension between\nthe language modeling objective and safety alignment, which is often exploited\nto jailbreak LLMs, is somewhat resolved with the aid of the injected bias\nsignal. Our framework is meant as an additional security layer, and complements\nexisting guardrail mechanisms for enhanced end-to-end safety with LLMs."}
{"id": "2505.14608", "pdf": "https://arxiv.org/pdf/2505.14608.pdf", "abs": "https://arxiv.org/abs/2505.14608", "title": "Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)", "authors": ["Rafael Rivera Soto", "Barry Chen", "Nicholas Andrews"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite considerable progress in the development of machine-text detectors,\nit has been suggested that the problem is inherently hard, and therefore, that\nstakeholders should proceed under the assumption that machine-generated text\ncannot be reliably detected as such. We examine a recent such claim by Nicks et\nal. (2024) regarding the ease with which language models can be optimized to\ndegrade the performance of machine-text detectors, including detectors not\nspecifically optimized against. We identify a feature space$\\unicode{x2013}$the\nstylistic feature space$\\unicode{x2013}$that is robust to such optimization,\nand show that it may be used to reliably detect samples from language models\noptimized to prevent detection. Furthermore, we show that even when models are\nexplicitly optimized against stylistic detectors, detection performance remains\nsurprisingly unaffected. We then seek to understand if stylistic detectors are\ninherently more robust. To study this question, we explore a new paraphrasing\napproach that simultaneously aims to close the gap between human writing and\nmachine writing in stylistic feature space while avoiding detection using\ntraditional features. We show that when only a single sample is available for\ndetection, this attack is universally effective across all detectors\nconsidered, including those that use writing style. However, as the number of\nsamples available for detection grows, the human and machine distributions\nbecome distinguishable. This observation encourages us to introduce AURA, a\nmetric that estimates the overlap between human and machine-generated\ndistributions by analyzing how detector performance improves as more samples\nbecome available. Overall, our findings underscore previous recommendations to\navoid reliance on machine-text detection."}
{"id": "2505.14617", "pdf": "https://arxiv.org/pdf/2505.14617.pdf", "abs": "https://arxiv.org/abs/2505.14617", "title": "Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models", "authors": ["Sahar Abdelnabi", "Ahmed Salem"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Reasoning-focused large language models (LLMs) sometimes alter their behavior\nwhen they detect that they are being evaluated, an effect analogous to the\nHawthorne phenomenon, which can lead them to optimize for test-passing\nperformance or to comply more readily with harmful prompts if real-world\nconsequences appear absent. We present the first quantitative study of how such\n\"test awareness\" impacts model behavior, particularly its safety alignment. We\nintroduce a white-box probing framework that (i) linearly identifies\nawareness-related activations and (ii) steers models toward or away from test\nawareness while monitoring downstream performance. We apply our method to\ndifferent state-of-the-art open-source reasoning LLMs across both realistic and\nhypothetical tasks. Our results demonstrate that test awareness significantly\nimpact safety alignment, and is different for different models. By providing\nfine-grained control over this latent effect, our work aims to increase trust\nin how we perform safety evaluation."}
{"id": "2505.14631", "pdf": "https://arxiv.org/pdf/2505.14631.pdf", "abs": "https://arxiv.org/abs/2505.14631", "title": "Think Only When You Need with Large Hybrid-Reasoning Models", "authors": ["Lingjie Jiang", "Xun Wu", "Shaohan Huang", "Qingxiu Dong", "Zewen Chi", "Li Dong", "Xingxing Zhang", "Tengchao Lv", "Lei Cui", "Furu Wei"], "categories": ["cs.CL"], "comment": null, "summary": "Recent Large Reasoning Models (LRMs) have shown substantially improved\nreasoning capabilities over traditional Large Language Models (LLMs) by\nincorporating extended thinking processes prior to producing final responses.\nHowever, excessively lengthy thinking introduces substantial overhead in terms\nof token consumption and latency, which is particularly unnecessary for simple\nqueries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the\nfirst kind of model capable of adaptively determining whether to perform\nthinking based on the contextual information of user queries. To achieve this,\nwe propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as\na cold start, followed by online reinforcement learning with the proposed\nHybrid Group Policy Optimization (HGPO) to implicitly learn to select the\nappropriate thinking mode. Furthermore, we introduce a metric called Hybrid\nAccuracy to quantitatively assess the model's capability for hybrid thinking.\nExtensive experimental results show that LHRMs can adaptively perform hybrid\nthinking on queries of varying difficulty and type. It outperforms existing\nLRMs and LLMs in reasoning and general capabilities while significantly\nimproving efficiency. Together, our work advocates for a reconsideration of the\nappropriate use of extended thinking processes and provides a solid starting\npoint for building hybrid thinking systems."}
{"id": "2505.14633", "pdf": "https://arxiv.org/pdf/2505.14633.pdf", "abs": "https://arxiv.org/abs/2505.14633", "title": "Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas", "authors": ["Yu Ying Chiu", "Zhilin Wang", "Sharan Maiya", "Yejin Choi", "Kyle Fish", "Sydney Levine", "Evan Hubinger"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "34 pages, 11 figures, see associated data at\n  https://huggingface.co/datasets/kellycyy/AIRiskDilemmas and code at\n  https://github.com/kellycyy/LitmusValues", "summary": "Detecting AI risks becomes more challenging as stronger models emerge and\nfind novel methods such as Alignment Faking to circumvent these detection\nattempts. Inspired by how risky behaviors in humans (i.e., illegal activities\nthat may hurt others) are sometimes guided by strongly-held values, we believe\nthat identifying values within AI models can be an early warning system for\nAI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal\nAI models' priorities on a range of AI value classes. Then, we collect\nAIRiskDilemmas, a diverse collection of dilemmas that pit values against one\nanother in scenarios relevant to AI safety risks such as Power Seeking. By\nmeasuring an AI model's value prioritization using its aggregate choices, we\nobtain a self-consistent set of predicted value priorities that uncover\npotential risks. We show that values in LitmusValues (including seemingly\ninnocuous ones like Care) can predict for both seen risky behaviors in\nAIRiskDilemmas and unseen risky behaviors in HarmBench."}
{"id": "2505.14652", "pdf": "https://arxiv.org/pdf/2505.14652.pdf", "abs": "https://arxiv.org/abs/2505.14652", "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains", "authors": ["Xueguang Ma", "Qian Liu", "Dongfu Jiang", "Ge Zhang", "Zejun Ma", "Wenhu Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks."}
{"id": "2505.14660", "pdf": "https://arxiv.org/pdf/2505.14660.pdf", "abs": "https://arxiv.org/abs/2505.14660", "title": "EmoGist: Efficient In-Context Learning for Visual Emotion Understanding", "authors": ["Ronald Seoh", "Dan Goldwasser"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "In this paper, we introduce EmoGist, a training-free, in-context learning\nmethod for performing visual emotion classification with LVLMs. The key\nintuition of our approach is that context-dependent definition of emotion\nlabels could allow more accurate predictions of emotions, as the ways in which\nemotions manifest within images are highly context dependent and nuanced.\nEmoGist pre-generates multiple explanations of emotion labels, by analyzing the\nclusters of example images belonging to each category. At test time, we\nretrieve a version of explanation based on embedding similarity, and feed it to\na fast VLM for classification. Through our experiments, we show that EmoGist\nallows up to 13 points improvement in micro F1 scores with the multi-label\nMemotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset."}
{"id": "2505.14674", "pdf": "https://arxiv.org/pdf/2505.14674.pdf", "abs": "https://arxiv.org/abs/2505.14674", "title": "Reward Reasoning Model", "authors": ["Jiaxin Guo", "Zewen Chi", "Li Dong", "Qingxiu Dong", "Xun Wu", "Shaohan Huang", "Furu Wei"], "categories": ["cs.CL"], "comment": null, "summary": "Reward models play a critical role in guiding large language models toward\noutputs that align with human expectations. However, an open challenge remains\nin effectively utilizing test-time compute to enhance reward model performance.\nIn this work, we introduce Reward Reasoning Models (RRMs), which are\nspecifically designed to execute a deliberate reasoning process before\ngenerating final rewards. Through chain-of-thought reasoning, RRMs leverage\nadditional test-time compute for complex queries where appropriate rewards are\nnot immediately apparent. To develop RRMs, we implement a reinforcement\nlearning framework that fosters self-evolved reward reasoning capabilities\nwithout requiring explicit reasoning traces as training data. Experimental\nresults demonstrate that RRMs achieve superior performance on reward modeling\nbenchmarks across diverse domains. Notably, we show that RRMs can adaptively\nexploit test-time compute to further improve reward accuracy. The pretrained\nreward reasoning models are available at\nhttps://huggingface.co/Reward-Reasoning."}
{"id": "2505.14679", "pdf": "https://arxiv.org/pdf/2505.14679.pdf", "abs": "https://arxiv.org/abs/2505.14679", "title": "UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models", "authors": ["Xiaojie Gu", "Guangxu Chen", "Jungang Li", "Jia-Chen Gu", "Xuming Hu", "Kai Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Lifelong learning enables large language models (LLMs) to adapt to evolving\ninformation by continually updating their internal knowledge. An ideal system\nshould support efficient, wide-ranging updates while preserving existing\ncapabilities and ensuring reliable deployment. Model editing stands out as a\npromising solution for this goal, offering a focused and efficient way to\nrevise a model's internal knowledge. Although recent paradigms have made\nnotable progress, they often struggle to meet the demands of practical lifelong\nadaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally\nnew editing solution that is training-, subject- and memory-free, making it\nparticularly well-suited for ultra-scalable, real-world lifelong model editing.\nULTRAEDIT performs editing through a self-contained process that relies solely\non lightweight linear algebra operations to compute parameter shifts, enabling\nfast and consistent parameter modifications with minimal overhead. To improve\nscalability in lifelong settings, ULTRAEDIT employs a lifelong normalization\nstrategy that continuously updates feature statistics across turns, allowing it\nto adapt to distributional shifts and maintain consistency over time. ULTRAEDIT\nachieves editing speeds over 7x faster than the previous state-of-the-art\nmethod-which was also the fastest known approach-while consuming less than 1/3\nthe VRAM, making it the only method currently capable of editing a 7B LLM on a\n24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest\ndataset in the field to date, with over 2M editing pairs-and demonstrate that\nour method supports up to 1M edits while maintaining high accuracy.\nComprehensive experiments on four datasets and six models show that ULTRAEDIT\nconsistently achieves superior performance across diverse model editing\nscenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit."}
{"id": "2505.14684", "pdf": "https://arxiv.org/pdf/2505.14684.pdf", "abs": "https://arxiv.org/abs/2505.14684", "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning", "authors": ["Haolei Xu", "Yuchen Yan", "Yongliang Shen", "Wenqi Zhang", "Guiyang Hou", "Shengpei Jiang", "Kaitao Song", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable progress on\nmathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits."}
{"id": "2505.14685", "pdf": "https://arxiv.org/pdf/2505.14685.pdf", "abs": "https://arxiv.org/abs/2505.14685", "title": "Language Models use Lookbacks to Track Beliefs", "authors": ["Nikhil Prakash", "Natalie Shapira", "Arnab Sen Sharma", "Christoph Riedl", "Yonatan Belinkov", "Tamar Rott Shaham", "David Bau", "Atticus Geiger"], "categories": ["cs.CL"], "comment": "32 pages, 32 figures. Code and data at https://belief.baulab.info/", "summary": "How do language models (LMs) represent characters' beliefs, especially when\nthose beliefs may differ from reality? This question lies at the heart of\nunderstanding the Theory of Mind (ToM) capabilities of LMs. We analyze\nLlama-3-70B-Instruct's ability to reason about characters' beliefs using causal\nmediation and abstraction. We construct a dataset that consists of simple\nstories where two characters each separately change the state of two objects,\npotentially unaware of each other's actions. Our investigation uncovered a\npervasive algorithmic pattern that we call a lookback mechanism, which enables\nthe LM to recall important information when it becomes necessary. The LM binds\neach character-object-state triple together by co-locating reference\ninformation about them, represented as their Ordering IDs (OIs) in low rank\nsubspaces of the state token's residual stream. When asked about a character's\nbeliefs regarding the state of an object, the binding lookback retrieves the\ncorresponding state OI and then an answer lookback retrieves the state token.\nWhen we introduce text specifying that one character is (not) visible to the\nother, we find that the LM first generates a visibility ID encoding the\nrelation between the observing and the observed character OIs. In a visibility\nlookback, this ID is used to retrieve information about the observed character\nand update the observing character's beliefs. Our work provides insights into\nthe LM's belief tracking mechanisms, taking a step toward reverse-engineering\nToM reasoning in LMs."}
{"id": "2505.13482", "pdf": "https://arxiv.org/pdf/2505.13482.pdf", "abs": "https://arxiv.org/abs/2505.13482", "title": "MedEIR: A Specialized Medical Embedding Model for Enhanced Information Retrieval", "authors": ["Anand Selvadurai", "Jasheen Shaik", "Girish Chandrasekar", "ShriRadhaKrishnan Balamurugan", "Eswara Reddy"], "categories": ["cs.IR", "cs.CL"], "comment": "9 pages, 1 figure. This manuscript is a substantial revision of a\n  previously submitted paper. We have explicitly clarified novelty,\n  strengthened scholarly depth, and expanded experimental validation", "summary": "Embedding models have become essential for retrieval-augmented generation\n(RAG) tasks, semantic clustering, and text re-ranking. But despite their\ngrowing use, many of these come with notable limitations. For example, Jina\nfails to capture the semantic content of medical documents, while models such\nas MiniLM often perform poorly on long-form documents. Domain-adapted models,\nwhile specialized, often underperform in general-purpose tasks, reducing their\noverall applicability. General-domain tokenizers often misinterpret medical\nvocabulary. The limitations of current embedding models, whether in\ntokenization accuracy, domain comprehension, or handling long sequences,\nhighlight the need for more versatile solutions. In this work, we present\nMedEIR, a novel embedding model and tokenizer jointly optimized for both\nmedical and general NLP tasks, incorporating ALiBi-based long-context\nprocessing to support sequences of up to 8,192 tokens. MedEIR was pre-trained\non only 6 billion tokens, significantly fewer than Jina's, followed by\nfine-tuning on 3 million sentence pairs. MedEIR consistently outperforms Jina\nV2 and MiniLM across MTEB benchmarks, achieving top scores on ArguAna (55.24),\nNFCorpus (38.44), MedicalQARetrieval (74.25), SciFact (72.04), and TRECCOVID\n(79.56). These results highlight the potential of MedEIR as a highly effective\nembedding model, demonstrating strong performance across both general-purpose\nand domain-specific tasks and outperforming existing models on multiple\nbenchmarks."}
{"id": "2505.13484", "pdf": "https://arxiv.org/pdf/2505.13484.pdf", "abs": "https://arxiv.org/abs/2505.13484", "title": "Evaluating Large Language Models for Real-World Engineering Tasks", "authors": ["Rene Heesch", "Sebastian Eilermann", "Alexander Windmann", "Alexander Diedrich", "Philipp Rosenthal", "Oliver Niggemann"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are transformative not only for daily activities\nbut also for engineering tasks. However, current evaluations of LLMs in\nengineering exhibit two critical shortcomings: (i) the reliance on simplified\nuse cases, often adapted from examination materials where correctness is easily\nverifiable, and (ii) the use of ad hoc scenarios that insufficiently capture\ncritical engineering competencies. Consequently, the assessment of LLMs on\ncomplex, real-world engineering problems remains largely unexplored. This paper\naddresses this gap by introducing a curated database comprising over 100\nquestions derived from authentic, production-oriented engineering scenarios,\nsystematically designed to cover core competencies such as product design,\nprognosis, and diagnosis. Using this dataset, we evaluate four state-of-the-art\nLLMs, including both cloud-based and locally hosted instances, to\nsystematically investigate their performance on complex engineering tasks. Our\nresults show that LLMs demonstrate strengths in basic temporal and structural\nreasoning but struggle significantly with abstract reasoning, formal modeling,\nand context-sensitive engineering logic."}
{"id": "2505.13489", "pdf": "https://arxiv.org/pdf/2505.13489.pdf", "abs": "https://arxiv.org/abs/2505.13489", "title": "Contrastive Cross-Course Knowledge Tracing via Concept Graph Guided Knowledge Transfer", "authors": ["Wenkang Han", "Wang Lin", "Liya Hu", "Zhenlong Dai", "Yiyun Zhou", "Mengze Li", "Zemin Liu", "Chang Yao", "Jingyuan Chen"], "categories": ["cs.AI", "cs.CL"], "comment": "Accepted by IJCAI 2025", "summary": "Knowledge tracing (KT) aims to predict learners' future performance based on\nhistorical learning interactions. However, existing KT models predominantly\nfocus on data from a single course, limiting their ability to capture a\ncomprehensive understanding of learners' knowledge states. In this paper, we\npropose TransKT, a contrastive cross-course knowledge tracing method that\nleverages concept graph guided knowledge transfer to model the relationships\nbetween learning behaviors across different courses, thereby enhancing\nknowledge state estimation. Specifically, TransKT constructs a cross-course\nconcept graph by leveraging zero-shot Large Language Model (LLM) prompts to\nestablish implicit links between related concepts across different courses.\nThis graph serves as the foundation for knowledge transfer, enabling the model\nto integrate and enhance the semantic features of learners' interactions across\ncourses. Furthermore, TransKT includes an LLM-to-LM pipeline for incorporating\nsummarized semantic features, which significantly improves the performance of\nGraph Convolutional Networks (GCNs) used for knowledge transfer. Additionally,\nTransKT employs a contrastive objective that aligns single-course and\ncross-course knowledge states, thereby refining the model's ability to provide\na more robust and accurate representation of learners' overall knowledge\nstates."}
{"id": "2505.13511", "pdf": "https://arxiv.org/pdf/2505.13511.pdf", "abs": "https://arxiv.org/abs/2505.13511", "title": "Can AI Freelancers Compete? Benchmarking Earnings, Reliability, and Task Success at Scale", "authors": ["David Noever", "Forrest McKee"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "This study explores Large Language Models (LLMs) as autonomous agents for\nreal-world tasks, including freelance software development. This work presents\na new benchmark that evaluates LLMs on freelance programming and data analysis\ntasks derived from economic data. We construct the benchmark using synthetic\ntasks created from a Kaggle Freelancer dataset of job postings, with all job\nprices standardized to USD (median fixed-project price around $250, and an\naverage of $306). Each task is accompanied by structured input-output test\ncases and an estimated price tag, enabling automated correctness checking and a\nmonetary performance valuation. This approach is inspired by OpenAI's recent\nSWE-Lancer benchmark (1,400 real Upwork tasks worth $1M total). Still, our\nframework simplifies evaluation using programmatically testable tasks and\npredicted price values, making it highly scalable and repeatable. On this\nbenchmark, we evaluate four modern LLMs - Claude 3.5 Haiku, GPT-4o-mini, Qwen\n2.5, and Mistral. We report each model's accuracy (task success rate and\ntest-case pass rate) and the total \"freelance earnings\" it achieves (sum of\nprices of solved tasks). Our results show that Claude 3.5 Haiku performs best,\nearning approximately $1.52 million USD, followed closely by GPT-4o-mini at\n$1.49 million, then Qwen 2.5 ($1.33M) and Mistral ($0.70M). We analyze the\ndistribution of errors per task and observe that the strongest models solve the\nmost tasks and rarely fail completely on any project. We discuss the\nimplications of these results for the feasibility of AI as a freelance\ndeveloper, the advantages and limitations of our automated benchmark approach,\nand the gap between performance on structured tasks versus the true complexity\nof real-world freelance jobs."}
{"id": "2505.13515", "pdf": "https://arxiv.org/pdf/2505.13515.pdf", "abs": "https://arxiv.org/abs/2505.13515", "title": "LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades", "authors": ["Yanan Li", "Fanxu Meng", "Muhan Zhang", "Shiai Zhu", "Shangguang Wang", "Mengwei Xu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) are frequently updated, LoRA weights trained\non earlier versions quickly become obsolete. The conventional practice of\nretraining LoRA weights from scratch on the latest model is costly,\ntime-consuming, and environmentally detrimental, particularly as the diversity\nof LLMs and downstream tasks expands. This motivates a critical question: \"How\ncan we efficiently leverage existing LoRA weights to adapt to newer model\nversions?\" To address this, we propose LoRASuite, a modular approach tailored\nspecifically to various types of LLM updates. First, we compute a transfer\nmatrix utilizing known parameters from both old and new LLMs. Next, we allocate\ncorresponding layers and attention heads based on centered kernel alignment and\ncosine similarity metrics, respectively. A subsequent small-scale, skillful\nfine-tuning step ensures numerical stability. Experimental evaluations\ndemonstrate that LoRASuite consistently surpasses small-scale vanilla LoRA\nmethods. Notably, on backbone LLMs such as MiniCPM and Qwen, LoRASuite even\nexceeds the performance of full-scale LoRA retraining, with average\nimprovements of +1.4 and +6.6 points on math tasks, respectively. Additionally,\nLoRASuite significantly reduces memory consumption by 5.5 GB and computational\ntime by 78.23%."}
{"id": "2505.13529", "pdf": "https://arxiv.org/pdf/2505.13529.pdf", "abs": "https://arxiv.org/abs/2505.13529", "title": "BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs", "authors": ["Junxiao Yang", "Jinzhe Tu", "Haoran Liu", "Xiaoce Wang", "Chujie Zheng", "Zhexin Zhang", "Shiyao Cui", "Caishun Chen", "Tiantian He", "Hongning Wang", "Yew-Soon Ong", "Minlie Huang"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in Large Reasoning Models (LRMs) have shown impressive\ncapabilities in mathematical and logical reasoning. However, current LRMs\nrarely admit ignorance or respond with \"I don't know\". Instead, they often\nproduce incorrect answers while showing undue confidence, raising concerns\nabout their factual reliability. In this work, we identify two pathological\nreasoning patterns characterized by overthinking that contribute to the\noverconfident and incorrect answers: last-minute guessing and second-thought\nspiraling. To address these issues, we propose BARREL-a novel framework that\npromotes concise and boundary-aware factual reasoning. Our experiments show\nthat BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B\nfrom 39.33% to 61.48%, while still achieving accuracy comparable to models\nfinetuned on reasoning data generated by R1. These results demonstrate that our\npilot study is inspiring to build more reliable and factual System 2 LRMs."}
{"id": "2505.13531", "pdf": "https://arxiv.org/pdf/2505.13531.pdf", "abs": "https://arxiv.org/abs/2505.13531", "title": "AdAEM: An Adaptively and Automated Extensible Measurement of LLMs' Value Difference", "authors": ["Shitong Duan", "Xiaoyuan Yi", "Peng Zhang", "Dongkuan Xu", "Jing Yao", "Tun Lu", "Ning Gu", "Xing Xie"], "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Assessing Large Language Models (LLMs)' underlying value differences enables\ncomprehensive comparison of their misalignment, cultural adaptability, and\nbiases. Nevertheless, current value measurement datasets face the\ninformativeness challenge: with often outdated, contaminated, or generic test\nquestions, they can only capture the shared value orientations among different\nLLMs, leading to saturated and thus uninformative results. To address this\nproblem, we introduce AdAEM, a novel, self-extensible assessment framework for\nrevealing LLMs' inclinations. Distinct from previous static benchmarks, AdAEM\ncan automatically and adaptively generate and extend its test questions. This\nis achieved by probing the internal value boundaries of a diverse set of LLMs\ndeveloped across cultures and time periods in an in-context optimization\nmanner. The optimization process theoretically maximizes an\ninformation-theoretic objective to extract the latest or culturally\ncontroversial topics, providing more distinguishable and informative insights\nabout models' value differences. In this way, AdAEM is able to co-evolve with\nthe development of LLMs, consistently tracking their value dynamics. Using\nAdAEM, we generate 12,310 questions grounded in Schwartz Value Theory, conduct\nan extensive analysis to manifest our method's validity and effectiveness, and\nbenchmark the values of 16 LLMs, laying the groundwork for better value\nresearch."}
{"id": "2505.13534", "pdf": "https://arxiv.org/pdf/2505.13534.pdf", "abs": "https://arxiv.org/abs/2505.13534", "title": "InterFeat: An Automated Pipeline for Finding Interesting Hypotheses in Structured Biomedical Data", "authors": ["Dan Ofer", "Michal Linial", "Dafna Shahaf"], "categories": ["q-bio.QM", "cs.AI", "cs.CL", "cs.IR", "68T05, 68T50, 92C50", "I.2.6; I.2.7; H.2.8; J.3"], "comment": null, "summary": "Finding interesting phenomena is the core of scientific discovery, but it is\na manual, ill-defined concept. We present an integrative pipeline for\nautomating the discovery of interesting simple hypotheses (feature-target\nrelations with effect direction and a potential underlying mechanism) in\nstructured biomedical data. The pipeline combines machine learning, knowledge\ngraphs, literature search and Large Language Models. We formalize\n\"interestingness\" as a combination of novelty, utility and plausibility. On 8\nmajor diseases from the UK Biobank, our pipeline consistently recovers risk\nfactors years before their appearance in the literature. 40--53% of our top\ncandidates were validated as interesting, compared to 0--7% for a SHAP-based\nbaseline. Overall, 28% of 109 candidates were interesting to medical experts.\nThe pipeline addresses the challenge of operationalizing \"interestingness\"\nscalably and for any target. We release data and code:\nhttps://github.com/LinialLab/InterFeat"}
{"id": "2505.13581", "pdf": "https://arxiv.org/pdf/2505.13581.pdf", "abs": "https://arxiv.org/abs/2505.13581", "title": "RAR: Setting Knowledge Tripwires for Retrieval Augmented Rejection", "authors": ["Tommaso Mario Buonocore", "Enea Parimbelli"], "categories": ["cs.IR", "cs.CL", "cs.CR", "68M25, 68T07", "I.2.7; K.6.5"], "comment": "7 pages, 4 figures, 2 tables", "summary": "Content moderation for large language models (LLMs) remains a significant\nchallenge, requiring flexible and adaptable solutions that can quickly respond\nto emerging threats. This paper introduces Retrieval Augmented Rejection (RAR),\na novel approach that leverages a retrieval-augmented generation (RAG)\narchitecture to dynamically reject unsafe user queries without model\nretraining. By strategically inserting and marking malicious documents into the\nvector database, the system can identify and reject harmful requests when these\ndocuments are retrieved. Our preliminary results show that RAR achieves\ncomparable performance to embedded moderation in LLMs like Claude 3.5 Sonnet,\nwhile offering superior flexibility and real-time customization capabilities, a\nfundamental feature to timely address critical vulnerabilities. This approach\nintroduces no architectural changes to existing RAG systems, requiring only the\naddition of specially crafted documents and a simple rejection mechanism based\non retrieval results."}
{"id": "2505.13652", "pdf": "https://arxiv.org/pdf/2505.13652.pdf", "abs": "https://arxiv.org/abs/2505.13652", "title": "Guided Search Strategies in Non-Serializable Environments with Applications to Software Engineering Agents", "authors": ["Karina Zainullina", "Alexander Golubev", "Maria Trofimova", "Sergei Polezhaev", "Ibragim Badertdinov", "Daria Litvintseva", "Simon Karasik", "Filipp Fisin", "Sergei Skvortsov", "Maksim Nekrashevich", "Anton Shevtsov", "Boris Yangel"], "categories": ["cs.SE", "cs.CL"], "comment": "ICML", "summary": "Large language models (LLMs) have recently achieved remarkable results in\ncomplex multi-step tasks, such as mathematical reasoning and agentic software\nengineering. However, they often struggle to maintain consistent performance\nacross multiple solution attempts. One effective approach to narrow the gap\nbetween average-case and best-case performance is guided test-time search,\nwhich explores multiple solution paths to identify the most promising one.\nUnfortunately, effective search techniques (e.g. MCTS) are often unsuitable for\nnon-serializable RL environments, such as Docker containers, where intermediate\nenvironment states cannot be easily saved and restored. We investigate two\ncomplementary search strategies applicable to such environments: 1-step\nlookahead and trajectory selection, both guided by a learned action-value\nfunction estimator. On the SWE-bench Verified benchmark, a key testbed for\nagentic software engineering, we find these methods to double the average\nsuccess rate of a fine-tuned Qwen-72B model, achieving 40.8%, the new\nstate-of-the-art for open-weights models. Additionally, we show that these\ntechniques are transferable to more advanced closed models, yielding similar\nimprovements with GPT-4o."}
{"id": "2505.13718", "pdf": "https://arxiv.org/pdf/2505.13718.pdf", "abs": "https://arxiv.org/abs/2505.13718", "title": "Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings", "authors": ["Safal Shrestha", "Minwu Kim", "Aadim Nepal", "Anubhav Shrestha", "Keith Ross"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Designing effective reasoning-capable LLMs typically requires training using\nReinforcement Learning with Verifiable Rewards (RLVR) or distillation with\ncarefully curated Long Chain of Thoughts (CoT), both of which depend heavily on\nextensive training data. This creates a major challenge when the amount of\nquality training data is scarce. We propose a sample-efficient, two-stage\ntraining strategy to develop reasoning LLMs under limited supervision. In the\nfirst stage, we \"warm up\" the model by distilling Long CoTs from a toy domain,\nnamely, Knights \\& Knaves (K\\&K) logic puzzles to acquire general reasoning\nskills. In the second stage, we apply RLVR to the warmed-up model using a\nlimited set of target-domain examples. Our experiments demonstrate that this\ntwo-phase approach offers several benefits: $(i)$ the warmup phase alone\nfacilitates generalized reasoning, leading to performance improvements across a\nrange of tasks, including MATH, HumanEval$^{+}$, and MMLU-Pro. $(ii)$ When both\nthe base model and the warmed-up model are RLVR trained on the same small\ndataset ($\\leq100$ examples), the warmed-up model consistently outperforms the\nbase model; $(iii)$ Warming up before RLVR training allows a model to maintain\ncross-domain generalizability even after training on a specific domain; $(iv)$\nIntroducing warmup in the pipeline improves not only accuracy but also overall\nsample efficiency during RLVR training. The results in this paper highlight the\npromise of warmup for building robust reasoning LLMs in data-scarce\nenvironments."}
{"id": "2505.13738", "pdf": "https://arxiv.org/pdf/2505.13738.pdf", "abs": "https://arxiv.org/abs/2505.13738", "title": "Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training", "authors": ["Shane Bergsma", "Nolan Dey", "Gurpreet Gosal", "Gavia Gray", "Daria Soboleva", "Joel Hestness"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Efficient LLM pre-training requires well-tuned hyperparameters (HPs),\nincluding learning rate {\\eta} and weight decay {\\lambda}. We study scaling\nlaws for HPs: formulas for how to scale HPs as we scale model size N, dataset\nsize D, and batch size B. Recent work suggests the AdamW timescale,\nB/({\\eta}{\\lambda}D), should remain constant across training settings, and we\nverify the implication that optimal {\\lambda} scales linearly with B, for a\nfixed N,D. However, as N,D scale, we show the optimal timescale obeys a precise\npower law in the tokens-per-parameter ratio, D/N. This law thus provides a\nmethod to accurately predict {\\lambda}opt in advance of large-scale training.\nWe also study scaling laws for optimal batch size Bopt (the B enabling lowest\nloss at a given N,D) and critical batch size Bcrit (the B beyond which further\ndata parallelism becomes ineffective). In contrast with prior work, we find\nboth Bopt and Bcrit scale as power laws in D, independent of model size, N.\nFinally, we analyze how these findings inform the real-world selection of\nPareto-optimal N and D under dual training time and compute objectives."}
{"id": "2505.13757", "pdf": "https://arxiv.org/pdf/2505.13757.pdf", "abs": "https://arxiv.org/abs/2505.13757", "title": "LLM-Based Compact Reranking with Document Features for Scientific Retrieval", "authors": ["Runchu Tian", "Xueqiang Xu", "Bowen Jin", "SeongKu Kang", "Jiawei Han"], "categories": ["cs.IR", "cs.CL"], "comment": "17 pages, 4 figures", "summary": "Scientific retrieval is essential for advancing academic discovery. Within\nthis process, document reranking plays a critical role by refining first-stage\nretrieval results. However, large language model (LLM) listwise reranking faces\nunique challenges in the scientific domain. First-stage retrieval is often\nsuboptimal in the scientific domain, so relevant documents are ranked lower.\nMoreover, conventional listwise reranking uses the full text of candidate\ndocuments in the context window, limiting the number of candidates that can be\nconsidered. As a result, many relevant documents are excluded before reranking,\nwhich constrains overall retrieval performance. To address these challenges, we\nexplore compact document representations based on semantic features such as\ncategories, sections, and keywords, and propose a training-free, model-agnostic\nreranking framework for scientific retrieval called CoRank. The framework\ninvolves three stages: (i) offline extraction of document-level features, (ii)\ncoarse reranking using these compact representations, and (iii) fine-grained\nreranking on full texts of the top candidates from stage (ii). This hybrid\ndesign provides a high-level abstraction of document semantics, expands\ncandidate coverage, and retains critical details required for precise ranking.\nExperiments on LitSearch and CSFCube show that CoRank significantly improves\nreranking performance across different LLM backbones, increasing nDCG@10 from\n32.0 to 39.7. Overall, these results highlight the value of information\nextraction for reranking in scientific retrieval."}
{"id": "2505.13763", "pdf": "https://arxiv.org/pdf/2505.13763.pdf", "abs": "https://arxiv.org/abs/2505.13763", "title": "Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations", "authors": ["Li Ji-An", "Hua-Dong Xiong", "Robert C. Wilson", "Marcelo G. Mattar", "Marcus K. Benna"], "categories": ["cs.AI", "cs.CL", "q-bio.NC"], "comment": null, "summary": "Large language models (LLMs) can sometimes report the strategies they\nactually use to solve tasks, but they can also fail to do so. This suggests\nsome degree of metacognition -- the capacity to monitor one's own cognitive\nprocesses for subsequent reporting and self-control. Metacognitive abilities\nenhance AI capabilities but raise safety concerns, as models might obscure\ntheir internal processes to evade neural-activation-based oversight mechanisms\ndesigned to detect harmful behaviors. Given society's increased reliance on\nthese models, it is critical that we understand the limits of their\nmetacognitive abilities, particularly their ability to monitor their internal\nactivations. To address this, we introduce a neuroscience-inspired\nneurofeedback paradigm designed to quantify the ability of LLMs to explicitly\nreport and control their activation patterns. By presenting models with\nsentence-label pairs where labels correspond to sentence-elicited internal\nactivations along specific directions in the neural representation space, we\ndemonstrate that LLMs can learn to report and control these activations. The\nperformance varies with several factors: the number of example pairs provided,\nthe semantic interpretability of the target neural direction, and the variance\nexplained by that direction. These results reveal a \"metacognitive space\" with\ndimensionality much lower than the model's neural space, suggesting LLMs can\nmonitor only a subset of their neural mechanisms. Our findings provide\nempirical evidence quantifying metacognitive capabilities in LLMs, with\nsignificant implications for AI safety."}
{"id": "2505.13766", "pdf": "https://arxiv.org/pdf/2505.13766.pdf", "abs": "https://arxiv.org/abs/2505.13766", "title": "Advancing Software Quality: A Standards-Focused Review of LLM-Based Assurance Techniques", "authors": ["Avinash Patil"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "16 pages, 1 Table, 6 Figures", "summary": "Software Quality Assurance (SQA) is critical for delivering reliable, secure,\nand efficient software products. The Software Quality Assurance Process aims to\nprovide assurance that work products and processes comply with predefined\nprovisions and plans. Recent advancements in Large Language Models (LLMs)\npresent new opportunities to enhance existing SQA processes by automating tasks\nlike requirement analysis, code review, test generation, and compliance checks.\nSimultaneously, established standards such as ISO/IEC 12207, ISO/IEC 25010,\nISO/IEC 5055, ISO 9001/ISO/IEC 90003, CMMI, and TMM provide structured\nframeworks for ensuring robust quality practices. This paper surveys the\nintersection of LLM-based SQA methods and these recognized standards,\nhighlighting how AI-driven solutions can augment traditional approaches while\nmaintaining compliance and process maturity. We first review the foundational\nsoftware quality standards and the technical fundamentals of LLMs in software\nengineering. Next, we explore various LLM-based SQA applications, including\nrequirement validation, defect detection, test generation, and documentation\nmaintenance. We then map these applications to key software quality frameworks,\nillustrating how LLMs can address specific requirements and metrics within each\nstandard. Empirical case studies and open-source initiatives demonstrate the\npractical viability of these methods. At the same time, discussions on\nchallenges (e.g., data privacy, model bias, explainability) underscore the need\nfor deliberate governance and auditing. Finally, we propose future directions\nencompassing adaptive learning, privacy-focused deployments, multimodal\nanalysis, and evolving standards for AI-driven software quality."}
{"id": "2505.13770", "pdf": "https://arxiv.org/pdf/2505.13770.pdf", "abs": "https://arxiv.org/abs/2505.13770", "title": "Ice Cream Doesn't Cause Drowning: Benchmarking LLMs Against Statistical Pitfalls in Causal Inference", "authors": ["Jin Du", "Li Chen", "Xun Xian", "An Luo", "Fangqiao Tian", "Ganghua Wang", "Charles Doss", "Xiaotong Shen", "Jie Ding"], "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ME", "stat.ML", "62-08, 68T50, 68T05, 68T01, 68T07, 62-07, 68U35, 62C99", "I.2.7; I.2.6; I.2.0; I.5.1; I.5.4; F.2.2; H.2.8; G.3"], "comment": null, "summary": "Reliable causal inference is essential for making decisions in high-stakes\nareas like medicine, economics, and public policy. However, it remains unclear\nwhether large language models (LLMs) can handle rigorous and trustworthy\nstatistical causal inference. Current benchmarks usually involve simplified\ntasks. For example, these tasks might only ask LLMs to identify semantic causal\nrelationships or draw conclusions directly from raw data. As a result, models\nmay overlook important statistical pitfalls, such as Simpson's paradox or\nselection bias. This oversight limits the applicability of LLMs in the real\nworld. To address these limitations, we propose CausalPitfalls, a comprehensive\nbenchmark designed to rigorously evaluate the capability of LLMs in overcoming\ncommon causal inference pitfalls. Our benchmark features structured challenges\nacross multiple difficulty levels, each paired with grading rubrics. This\napproach allows us to quantitatively measure both causal reasoning capabilities\nand the reliability of LLMs' responses. We evaluate models using two protocols:\n(1) direct prompting, which assesses intrinsic causal reasoning, and (2)\ncode-assisted prompting, where models generate executable code for explicit\nstatistical analysis. Additionally, we validate the effectiveness of this judge\nby comparing its scoring with assessments from human experts. Our results\nreveal significant limitations in current LLMs when performing statistical\ncausal inference. The CausalPitfalls benchmark provides essential guidance and\nquantitative metrics to advance the development of trustworthy causal reasoning\nsystems."}
{"id": "2505.13820", "pdf": "https://arxiv.org/pdf/2505.13820.pdf", "abs": "https://arxiv.org/abs/2505.13820", "title": "Structured Agent Distillation for Large Language Model", "authors": ["Jun Liu", "Zhenglun Kong", "Peiyan Dong", "Changdi Yang", "Tianqi Li", "Hao Tang", "Geng Yuan", "Wei Niu", "Wenbin Zhang", "Pu Zhao", "Xue Lin", "Dong Huang", "Yanzhi Wang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit strong capabilities as decision-making\nagents by interleaving reasoning and actions, as seen in ReAct-style\nframeworks. Yet, their practical deployment is constrained by high inference\ncosts and large model sizes. We propose Structured Agent Distillation, a\nframework that compresses large LLM-based agents into smaller student models\nwhile preserving both reasoning fidelity and action consistency. Unlike\nstandard token-level distillation, our method segments trajectories into\n{[REASON]} and {[ACT]} spans, applying segment-specific losses to align each\ncomponent with the teacher's behavior. This structure-aware supervision enables\ncompact agents to better replicate the teacher's decision process. Experiments\non ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently\noutperforms token-level and imitation learning baselines, achieving significant\ncompression with minimal performance drop. Scaling and ablation results further\nhighlight the importance of span-level alignment for efficient and deployable\nagents."}
{"id": "2505.13847", "pdf": "https://arxiv.org/pdf/2505.13847.pdf", "abs": "https://arxiv.org/abs/2505.13847", "title": "Forensic deepfake audio detection using segmental speech features", "authors": ["Tianle Yang", "Chengzhe Sun", "Siwei Lyu", "Phil Rose"], "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "This study explores the potential of using acoustic features of segmental\nspeech sounds to detect deepfake audio. These features are highly interpretable\nbecause of their close relationship with human articulatory processes and are\nexpected to be more difficult for deepfake models to replicate. The results\ndemonstrate that certain segmental features commonly used in forensic voice\ncomparison are effective in identifying deep-fakes, whereas some global\nfeatures provide little value. These findings underscore the need to approach\naudio deepfake detection differently for forensic voice comparison and offer a\nnew perspective on leveraging segmental features for this purpose."}
{"id": "2505.13862", "pdf": "https://arxiv.org/pdf/2505.13862.pdf", "abs": "https://arxiv.org/abs/2505.13862", "title": "PandaGuard: Systematic Evaluation of LLM Safety in the Era of Jailbreaking Attacks", "authors": ["Guobin Shen", "Dongcheng Zhao", "Linghao Feng", "Xiang He", "Jihang Wang", "Sicheng Shen", "Haibo Tong", "Yiting Dong", "Jindong Li", "Xiang Zheng", "Yi Zeng"], "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable capabilities but remain\nvulnerable to adversarial prompts known as jailbreaks, which can bypass safety\nalignment and elicit harmful outputs. Despite growing efforts in LLM safety\nresearch, existing evaluations are often fragmented, focused on isolated attack\nor defense techniques, and lack systematic, reproducible analysis. In this\nwork, we introduce PandaGuard, a unified and modular framework that models LLM\njailbreak safety as a multi-agent system comprising attackers, defenders, and\njudges. Our framework implements 19 attack methods and 12 defense mechanisms,\nalong with multiple judgment strategies, all within a flexible plugin\narchitecture supporting diverse LLM interfaces, multiple interaction modes, and\nconfiguration-driven experimentation that enhances reproducibility and\npractical deployment. Built on this framework, we develop PandaBench, a\ncomprehensive benchmark that evaluates the interactions between these\nattack/defense methods across 49 LLMs and various judgment approaches,\nrequiring over 3 billion tokens to execute. Our extensive evaluation reveals\nkey insights into model vulnerabilities, defense cost-performance trade-offs,\nand judge consistency. We find that no single defense is optimal across all\ndimensions and that judge disagreement introduces nontrivial variance in safety\nassessments. We release the code, configurations, and evaluation results to\nsupport transparent and reproducible research in LLM safety."}
{"id": "2505.13878", "pdf": "https://arxiv.org/pdf/2505.13878.pdf", "abs": "https://arxiv.org/abs/2505.13878", "title": "InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models", "authors": ["Yanggan Gu", "Zhaoyi Yan", "Yuanyi Wang", "Yiming Zhang", "Qi Zhou", "Fei Wu", "Hongxia Yang"], "categories": ["cs.LG", "cs.CL"], "comment": "17 pages", "summary": "Model fusion combines multiple Large Language Models (LLMs) with different\nstrengths into a more powerful, integrated model through lightweight training\nmethods. Existing works on model fusion focus primarily on supervised\nfine-tuning (SFT), leaving preference alignment (PA) --a critical phase for\nenhancing LLM performance--largely unexplored. The current few fusion methods\non PA phase, like WRPO, simplify the process by utilizing only response outputs\nfrom source models while discarding their probability information. To address\nthis limitation, we propose InfiFPO, a preference optimization method for\nimplicit model fusion. InfiFPO replaces the reference model in Direct\nPreference Optimization (DPO) with a fused source model that synthesizes\nmulti-source probabilities at the sequence level, circumventing complex\nvocabulary alignment challenges in previous works and meanwhile maintaining the\nprobability information. By introducing probability clipping and max-margin\nfusion strategies, InfiFPO enables the pivot model to align with human\npreferences while effectively distilling knowledge from source models.\nComprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO\nconsistently outperforms existing model fusion and preference optimization\nmethods. When using Phi-4 as the pivot model, InfiFPO improve its average\nperformance from 79.95 to 83.33 on 11 benchmarks, significantly improving its\ncapabilities in mathematics, coding, and reasoning tasks."}
{"id": "2505.13887", "pdf": "https://arxiv.org/pdf/2505.13887.pdf", "abs": "https://arxiv.org/abs/2505.13887", "title": "Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation", "authors": ["Junyang Wang", "Haiyang Xu", "Xi Zhang", "Ming Yan", "Ji Zhang", "Fei Huang", "Jitao Sang"], "categories": ["cs.AI", "cs.CL"], "comment": "17 pages, 7 figures, 9 tables. arXiv admin note: substantial text\n  overlap with arXiv:2502.17110", "summary": "The exponential rise in mobile device usage necessitates streamlined\nautomation for effective task management, yet many AI frameworks fall short due\nto inadequate operational expertise. While manually written knowledge can\nbridge this gap, it is often burdensome and inefficient. We introduce\nMobile-Agent-V, an innovative framework that utilizes video as a guiding tool\nto effortlessly and efficiently inject operational knowledge into mobile\nautomation processes. By deriving knowledge directly from video content,\nMobile-Agent-V eliminates manual intervention, significantly reducing the\neffort and time required for knowledge acquisition. To rigorously evaluate this\napproach, we propose Mobile-Knowledge, a benchmark tailored to assess the\nimpact of external knowledge on mobile agent performance. Our experimental\nfindings demonstrate that Mobile-Agent-V enhances performance by 36% compared\nto existing methods, underscoring its effortless and efficient advantages in\nmobile automation."}
{"id": "2505.13909", "pdf": "https://arxiv.org/pdf/2505.13909.pdf", "abs": "https://arxiv.org/abs/2505.13909", "title": "Efficient Agent Training for Computer Use", "authors": ["Yanheng He", "Jiahe Jin", "Pengfei Liu"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "We open-source our entire suite of code, data, and models to\n  facilitate future research at https://github.com/GAIR-NLP/PC-Agent-E", "summary": "Scaling up high-quality trajectory data has long been a critical bottleneck\nfor developing human-like computer use agents. We introduce PC Agent-E, an\nefficient agent training framework that significantly reduces reliance on\nlarge-scale human demonstrations. Starting with just 312 human-annotated\ncomputer use trajectories, we further improved data quality by synthesizing\ndiverse action decisions with Claude 3.7 Sonnet. Trained on these enriched\ntrajectories, our PC Agent-E model achieved a remarkable 141% relative\nimprovement, surpassing the strong Claude 3.7 Sonnet with extended thinking on\nWindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC\nAgent-E demonstrates strong generalizability to different operating systems on\nOSWorld. Our findings suggest that strong computer use capabilities can be\nstimulated from a small amount of high-quality trajectory data."}
{"id": "2505.13941", "pdf": "https://arxiv.org/pdf/2505.13941.pdf", "abs": "https://arxiv.org/abs/2505.13941", "title": "MLZero: A Multi-Agent System for End-to-end Machine Learning Automation", "authors": ["Haoyang Fang", "Boran Han", "Nick Erickson", "Xiyuan Zhang", "Su Zhou", "Anirudh Dagar", "Jiani Zhang", "Ali Caner Turkmen", "Cuixiong Hu", "Huzefa Rangwala", "Ying Nian Wu", "Bernie Wang", "George Karypis"], "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Existing AutoML systems have advanced the automation of machine learning\n(ML); however, they still require substantial manual configuration and expert\ninput, particularly when handling multimodal data. We introduce MLZero, a novel\nmulti-agent framework powered by Large Language Models (LLMs) that enables\nend-to-end ML automation across diverse data modalities with minimal human\nintervention. A cognitive perception module is first employed, transforming raw\nmultimodal inputs into perceptual context that effectively guides the\nsubsequent workflow. To address key limitations of LLMs, such as hallucinated\ncode generation and outdated API knowledge, we enhance the iterative code\ngeneration process with semantic and episodic memory. MLZero demonstrates\nsuperior performance on MLE-Bench Lite, outperforming all competitors in both\nsuccess rate and solution quality, securing six gold medals. Additionally, when\nevaluated on our Multimodal AutoML Agent Benchmark, which includes 25 more\nchallenging tasks spanning diverse data modalities, MLZero outperforms the\ncompeting methods by a large margin with a success rate of 0.92 (+263.6\\%) and\nan average rank of 2.28. Our approach maintains its robust effectiveness even\nwith a compact 8B LLM, outperforming full-size systems from existing solutions."}
{"id": "2505.13957", "pdf": "https://arxiv.org/pdf/2505.13957.pdf", "abs": "https://arxiv.org/abs/2505.13957", "title": "Beyond Text: Unveiling Privacy Vulnerabilities in Multi-modal Retrieval-Augmented Generation", "authors": ["Jiankun Zhang", "Shenglai Zeng", "Jie Ren", "Tianqi Zheng", "Hui Liu", "Xianfeng Tang", "Hui Liu", "Yi Chang"], "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Multimodal Retrieval-Augmented Generation (MRAG) systems enhance LMMs by\nintegrating external multimodal databases, but introduce unexplored privacy\nvulnerabilities. While text-based RAG privacy risks have been studied,\nmultimodal data presents unique challenges. We provide the first systematic\nanalysis of MRAG privacy vulnerabilities across vision-language and\nspeech-language modalities. Using a novel compositional structured prompt\nattack in a black-box setting, we demonstrate how attackers can extract private\ninformation by manipulating queries. Our experiments reveal that LMMs can both\ndirectly generate outputs resembling retrieved content and produce descriptions\nthat indirectly expose sensitive information, highlighting the urgent need for\nrobust privacy-preserving MRAG techniques."}
{"id": "2505.14038", "pdf": "https://arxiv.org/pdf/2505.14038.pdf", "abs": "https://arxiv.org/abs/2505.14038", "title": "ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data", "authors": ["Xinzhe Zheng", "Sijie Ji", "Jiawei Sun", "Renqi Chen", "Wei Gao", "Mani Srivastava"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Mental health risk is a critical global public health challenge,\nnecessitating innovative and reliable assessment methods. With the development\nof large language models (LLMs), they stand out to be a promising tool for\nexplainable mental health care applications. Nevertheless, existing approaches\npredominantly rely on subjective textual mental records, which can be distorted\nby inherent mental uncertainties, leading to inconsistent and unreliable\npredictions. To address these limitations, this paper introduces ProMind-LLM.\nWe investigate an innovative approach integrating objective behavior data as\ncomplementary information alongside subjective mental records for robust mental\nhealth risk assessment. Specifically, ProMind-LLM incorporates a comprehensive\npipeline that includes domain-specific pretraining to tailor the LLM for mental\nhealth contexts, a self-refine mechanism to optimize the processing of\nnumerical behavioral data, and causal chain-of-thought reasoning to enhance the\nreliability and interpretability of its predictions. Evaluations of two\nreal-world datasets, PMData and Globem, demonstrate the effectiveness of our\nproposed methods, achieving substantial improvements over general LLMs. We\nanticipate that ProMind-LLM will pave the way for more dependable,\ninterpretable, and scalable mental health case solutions."}
{"id": "2505.14071", "pdf": "https://arxiv.org/pdf/2505.14071.pdf", "abs": "https://arxiv.org/abs/2505.14071", "title": "Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models", "authors": ["Woody Haosheng Gan", "Deqing Fu", "Julian Asilis", "Ollie Liu", "Dani Yogatama", "Vatsal Sharan", "Robin Jia", "Willie Neiswanger"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Steering methods have emerged as effective and targeted tools for guiding\nlarge language models' (LLMs) behavior without modifying their parameters.\nMultimodal large language models (MLLMs), however, do not currently enjoy the\nsame suite of techniques, due in part to their recency and architectural\ndiversity. Inspired by this gap, we investigate whether MLLMs can be steered\nusing vectors derived from their text-only LLM backbone, via sparse\nautoencoders (SAEs), mean shift, and linear probing. We find that text-derived\nsteering consistently enhances multimodal accuracy across diverse MLLM\narchitectures and visual tasks. In particular, mean shift boosts spatial\nrelationship accuracy on CV-Bench by up to +7.3% and counting accuracy by up to\n+3.3%, outperforming prompting and exhibiting strong generalization to\nout-of-distribution datasets. These results highlight textual steering vectors\nas a powerful, efficient mechanism for enhancing grounding in MLLMs with\nminimal additional data collection and computational overhead."}
{"id": "2505.14146", "pdf": "https://arxiv.org/pdf/2505.14146.pdf", "abs": "https://arxiv.org/abs/2505.14146", "title": "s3: You Don't Need That Much Data to Train a Search Agent via RL", "authors": ["Pengcheng Jiang", "Xueqiang Xu", "Jiacheng Lin", "Jinfeng Xiao", "Zifeng Wang", "Jimeng Sun", "Jiawei Han"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) systems empower large language models\n(LLMs) to access external knowledge during inference. Recent advances have\nenabled LLMs to act as search agents via reinforcement learning (RL), improving\ninformation acquisition through multi-turn interactions with retrieval engines.\nHowever, existing approaches either optimize retrieval using search-only\nmetrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM\nto jointly reason and retrieve-entangling retrieval with generation and\nlimiting the real search utility and compatibility with frozen or proprietary\nmodels. In this work, we propose s3, a lightweight, model-agnostic framework\nthat decouples the searcher from the generator and trains the searcher using a\nGain Beyond RAG reward: the improvement in generation accuracy over naive RAG.\ns3 requires only 2.4k training samples to outperform baselines trained on over\n70x more data, consistently delivering stronger downstream performance across\nsix general QA and five medical QA benchmarks."}
{"id": "2505.14185", "pdf": "https://arxiv.org/pdf/2505.14185.pdf", "abs": "https://arxiv.org/abs/2505.14185", "title": "Safety Subspaces are Not Distinct: A Fine-Tuning Case Study", "authors": ["Kaustubh Ponkshe", "Shaan Shah", "Raghav Singhal", "Praneeth Vepakomma"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Kaustubh Ponkshe, Shaan Shah, and Raghav Singhal contributed equally\n  to this work", "summary": "Large Language Models (LLMs) rely on safety alignment to produce socially\nacceptable responses. This is typically achieved through instruction tuning and\nreinforcement learning from human feedback. However, this alignment is known to\nbe brittle: further fine-tuning, even on benign or lightly contaminated data,\ncan degrade safety and reintroduce harmful behaviors. A growing body of work\nsuggests that alignment may correspond to identifiable geometric directions in\nweight space, forming subspaces that could, in principle, be isolated or\npreserved to defend against misalignment. In this work, we conduct a\ncomprehensive empirical study of this geometric perspective. We examine whether\nsafety-relevant behavior is concentrated in specific subspaces, whether it can\nbe separated from general-purpose learning, and whether harmfulness arises from\ndistinguishable patterns in internal representations. Across both parameter and\nactivation space, our findings are consistent: subspaces that amplify safe\nbehaviors also amplify unsafe ones, and prompts with different safety\nimplications activate overlapping representations. We find no evidence of a\nsubspace that selectively governs safety. These results challenge the\nassumption that alignment is geometrically localized. Rather than residing in\ndistinct directions, safety appears to emerge from entangled, high-impact\ncomponents of the model's broader learning dynamics. This suggests that\nsubspace-based defenses may face fundamental limitations and underscores the\nneed for alternative strategies to preserve alignment under continued training.\nWe corroborate these findings through multiple experiments on five open-source\nLLMs. Our code is publicly available at:\nhttps://github.com/CERT-Lab/safety-subspaces."}
{"id": "2505.14216", "pdf": "https://arxiv.org/pdf/2505.14216.pdf", "abs": "https://arxiv.org/abs/2505.14216", "title": "Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning", "authors": ["Minwu Kim", "Anubhav Shrestha", "Safal Shrestha", "Aadim Nepal", "Keith Ross"], "categories": ["cs.AI", "cs.CL"], "comment": "23 pages", "summary": "Recent studies have shown that reinforcement learning with verifiable rewards\n(RLVR) enhances overall accuracy but fails to improve capability, while\ndistillation can improve both. In this paper, we investigate the mechanisms\nbehind these phenomena. First, we demonstrate that RLVR does not improve\ncapability because it focuses on improving the accuracy of the less-difficult\nquestions to the detriment of the accuracy of the most difficult questions,\nthereby leading to no improvement in capability. Second, we find that RLVR does\nnot merely increase the success probability for the less difficult questions,\nbut in our small model settings produces quality responses that were absent in\nits output distribution before training. In addition, we show these responses\nare neither noticeably longer nor feature more reflection-related keywords,\nunderscoring the need for more reliable indicators of response quality. Third,\nwe show that while distillation reliably improves accuracy by learning strong\nreasoning patterns, it only improves capability when new knowledge is\nintroduced. Moreover, when distilling only with reasoning patterns and no new\nknowledge, the accuracy of the less-difficult questions improves to the\ndetriment of the most difficult questions, similar to RLVR. Together, these\nfindings offer a clearer understanding of how RLVR and distillation shape\nreasoning behavior in language models."}
{"id": "2505.14264", "pdf": "https://arxiv.org/pdf/2505.14264.pdf", "abs": "https://arxiv.org/abs/2505.14264", "title": "AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum", "authors": ["Jian Xiong", "Jingbo Zhou", "Jingyong Ye", "Dejing Dou"], "categories": ["cs.LG", "cs.CL"], "comment": "14 pages, 7 figures", "summary": "Reinforcement learning (RL) has emerged as an effective approach for\nenhancing the reasoning capabilities of large language models (LLMs),\nespecially in scenarios where supervised fine-tuning (SFT) falls short due to\nlimited chain-of-thought (CoT) data. Among RL-based post-training methods,\ngroup relative advantage estimation, as exemplified by Group Relative Policy\nOptimization (GRPO), has attracted considerable attention for eliminating the\ndependency on the value model, thereby simplifying training compared to\ntraditional approaches like Proximal Policy Optimization (PPO). However, we\nobserve that exsiting group relative advantage estimation method still suffers\nfrom training inefficiencies, particularly when the estimated advantage\napproaches zero. To address this limitation, we propose Advantage-Augmented\nPolicy Optimization (AAPO), a novel RL algorithm that optimizes the\ncross-entropy (CE) loss using advantages enhanced through a momentum-based\nestimation scheme. This approach effectively mitigates the inefficiencies\nassociated with group relative advantage estimation. Experimental results on\nmultiple mathematical reasoning benchmarks demonstrate the superior performance\nof AAPO."}
{"id": "2505.14300", "pdf": "https://arxiv.org/pdf/2505.14300.pdf", "abs": "https://arxiv.org/abs/2505.14300", "title": "SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors", "authors": ["Maheep Chaudhary", "Fazl Barez"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "High-risk industries like nuclear and aviation use real-time monitoring to\ndetect dangerous system conditions. Similarly, Large Language Models (LLMs)\nneed monitoring safeguards. We propose a real-time framework to predict harmful\nAI outputs before they occur by using an unsupervised approach that treats\nnormal behavior as the baseline and harmful outputs as outliers. Our study\nfocuses specifically on backdoor-triggered responses -- where specific input\nphrases activate hidden vulnerabilities causing the model to generate unsafe\ncontent like violence, pornography, or hate speech. We address two key\nchallenges: (1) identifying true causal indicators rather than surface\ncorrelations, and (2) preventing advanced models from deception -- deliberately\nevading monitoring systems. Hence, we approach this problem from an\nunsupervised lens by drawing parallels to human deception: just as humans\nexhibit physical indicators while lying, we investigate whether LLMs display\ndistinct internal behavioral signatures when generating harmful content. Our\nstudy addresses two critical challenges: 1) designing monitoring systems that\ncapture true causal indicators rather than superficial correlations; and\n2)preventing intentional evasion by increasingly capable \"Future models''. Our\nfindings show that models can produce harmful content through causal mechanisms\nand can become deceptive by: (a) alternating between linear and non-linear\nrepresentations, and (b) modifying feature relationships. To counter this, we\ndeveloped Safety-Net -- a multi-detector framework that monitors different\nrepresentation dimensions, successfully detecting harmful behavior even when\ninformation is shifted across representational spaces to evade individual\nmonitors. Our evaluation shows 96% accuracy in detecting harmful cases using\nour unsupervised ensemble approach."}
{"id": "2505.14302", "pdf": "https://arxiv.org/pdf/2505.14302.pdf", "abs": "https://arxiv.org/abs/2505.14302", "title": "Scaling Law for Quantization-Aware Training", "authors": ["Mengzhao Chen", "Chaoyi Zhang", "Jing Liu", "Yutao Zeng", "Zeyue Xue", "Zhiheng Liu", "Yunshui Li", "Jin Ma", "Jie Huang", "Xun Zhou", "Ping Luo"], "categories": ["cs.LG", "cs.CL"], "comment": "A unified scaling law for QAT that models quantization error as a\n  function of model size, training data volume, and quantization group size", "summary": "Large language models (LLMs) demand substantial computational and memory\nresources, creating deployment challenges. Quantization-aware training (QAT)\naddresses these challenges by reducing model precision while maintaining\nperformance. However, the scaling behavior of QAT, especially at 4-bit\nprecision (W4A4), is not well understood. Existing QAT scaling laws often\nignore key factors such as the number of training tokens and quantization\ngranularity, which limits their applicability. This paper proposes a unified\nscaling law for QAT that models quantization error as a function of model size,\ntraining data volume, and quantization group size. Through 268 QAT experiments,\nwe show that quantization error decreases as model size increases, but rises\nwith more training tokens and coarser quantization granularity. To identify the\nsources of W4A4 quantization error, we decompose it into weight and activation\ncomponents. Both components follow the overall trend of W4A4 quantization\nerror, but with different sensitivities. Specifically, weight quantization\nerror increases more rapidly with more training tokens. Further analysis shows\nthat the activation quantization error in the FC2 layer, caused by outliers, is\nthe primary bottleneck of W4A4 QAT quantization error. By applying\nmixed-precision quantization to address this bottleneck, we demonstrate that\nweight and activation quantization errors can converge to similar levels.\nAdditionally, with more training data, weight quantization error eventually\nexceeds activation quantization error, suggesting that reducing weight\nquantization error is also important in such scenarios. These findings offer\nkey insights for improving QAT research and development."}
{"id": "2505.14318", "pdf": "https://arxiv.org/pdf/2505.14318.pdf", "abs": "https://arxiv.org/abs/2505.14318", "title": "RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection", "authors": ["Wenjun Hou", "Yi Cheng", "Kaishuai Xu", "Heng Li", "Yan Hu", "Wenjie Li", "Jiang Liu"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious domains, including radiology report generation. Previous approaches\nhave attempted to utilize multimodal LLMs for this task, enhancing their\nperformance through the integration of domain-specific knowledge retrieval.\nHowever, these approaches often overlook the knowledge already embedded within\nthe LLMs, leading to redundant information integration and inefficient\nutilization of learned representations. To address this limitation, we propose\nRADAR, a framework for enhancing radiology report generation with supplementary\nknowledge injection. RADAR improves report generation by systematically\nleveraging both the internal knowledge of an LLM and externally retrieved\ninformation. Specifically, it first extracts the model's acquired knowledge\nthat aligns with expert image-based classification outputs. It then retrieves\nrelevant supplementary knowledge to further enrich this information. Finally,\nby aggregating both sources, RADAR generates more accurate and informative\nradiology reports. Extensive experiments on MIMIC-CXR, CheXpert-Plus, and IU\nX-ray demonstrate that our model outperforms state-of-the-art LLMs in both\nlanguage quality and clinical accuracy"}
{"id": "2505.14351", "pdf": "https://arxiv.org/pdf/2505.14351.pdf", "abs": "https://arxiv.org/abs/2505.14351", "title": "FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for Ü-Tsang, Amdo and Kham Speech Dataset Generation", "authors": ["Yutong Liu", "Ziyue Zhang", "Ban Ma-bao", "Yuqing Cai", "Yongbin Yu", "Renzeng Duojie", "Xiangxiang Wang", "Fan Gao", "Cheng Huang", "Nyima Tashi"], "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": "13 pages", "summary": "Tibetan is a low-resource language with minimal parallel speech corpora\nspanning its three major dialects-\\\"U-Tsang, Amdo, and Kham-limiting progress\nin speech modeling. To address this issue, we propose FMSD-TTS, a few-shot,\nmulti-speaker, multi-dialect text-to-speech framework that synthesizes parallel\ndialectal speech from limited reference audio and explicit dialect labels. Our\nmethod features a novel speaker-dialect fusion module and a Dialect-Specialized\nDynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and\nlinguistic variations across dialects while preserving speaker identity.\nExtensive objective and subjective evaluations demonstrate that FMSD-TTS\nsignificantly outperforms baselines in both dialectal expressiveness and\nspeaker similarity. We further validate the quality and utility of the\nsynthesized speech through a challenging speech-to-speech dialect conversion\ntask. Our contributions include: (1) a novel few-shot TTS system tailored for\nTibetan multi-dialect speech synthesis, (2) the public release of a large-scale\nsynthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source\nevaluation toolkit for standardized assessment of speaker similarity, dialect\nconsistency, and audio quality."}
{"id": "2505.14356", "pdf": "https://arxiv.org/pdf/2505.14356.pdf", "abs": "https://arxiv.org/abs/2505.14356", "title": "PersonaTAB: Predicting Personality Traits using Textual, Acoustic, and Behavioral Cues in Fully-Duplex Speech Dialogs", "authors": ["Sho Inoue", "Shai Wang", "Haizhou Li"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "This is accepted to Interspeech 2025; Added an extra page for\n  supplementary figures; Project page:\n  https://github.com/shinshoji01/Personality-Prediction-for-Conversation-Agents", "summary": "Despite significant progress in neural spoken dialog systems,\npersonality-aware conversation agents -- capable of adapting behavior based on\npersonalities -- remain underexplored due to the absence of personality\nannotations in speech datasets. We propose a pipeline that preprocesses raw\naudio recordings to create a dialogue dataset annotated with timestamps,\nresponse types, and emotion/sentiment labels. We employ an automatic speech\nrecognition (ASR) system to extract transcripts and timestamps, then generate\nconversation-level annotations. Leveraging these annotations, we design a\nsystem that employs large language models to predict conversational\npersonality. Human evaluators were engaged to identify conversational\ncharacteristics and assign personality labels. Our analysis demonstrates that\nthe proposed system achieves stronger alignment with human judgments compared\nto existing approaches."}
{"id": "2505.14368", "pdf": "https://arxiv.org/pdf/2505.14368.pdf", "abs": "https://arxiv.org/abs/2505.14368", "title": "Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs", "authors": ["Jiawen Wang", "Pritha Gupta", "Ivan Habernal", "Eyke Hüllermeier"], "categories": ["cs.CR", "cs.CL"], "comment": "8 pages, 3 figures, EMNLP 2025 under review", "summary": "Recent studies demonstrate that Large Language Models (LLMs) are vulnerable\nto different prompt-based attacks, generating harmful content or sensitive\ninformation. Both closed-source and open-source LLMs are underinvestigated for\nthese attacks. This paper studies effective prompt injection attacks against\nthe $\\mathbf{14}$ most popular open-source LLMs on five attack benchmarks.\nCurrent metrics only consider successful attacks, whereas our proposed Attack\nSuccess Probability (ASP) also captures uncertainty in the model's response,\nreflecting ambiguity in attack feasibility. By comprehensively analyzing the\neffectiveness of prompt injection attacks, we propose a simple and effective\nhypnotism attack; results show that this attack causes aligned language models,\nincluding Stablelm2, Mistral, Openchat, and Vicuna, to generate objectionable\nbehaviors, achieving around $90$% ASP. They also indicate that our ignore\nprefix attacks can break all $\\mathbf{14}$ open-source LLMs, achieving over\n$60$% ASP on a multi-categorical dataset. We find that moderately well-known\nLLMs exhibit higher vulnerability to prompt injection attacks, highlighting the\nneed to raise public awareness and prioritize efficient mitigation strategies."}
{"id": "2505.14396", "pdf": "https://arxiv.org/pdf/2505.14396.pdf", "abs": "https://arxiv.org/abs/2505.14396", "title": "Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds", "authors": ["Gaël Gendron", "Jože M. Rožanec", "Michael Witbrock", "Gillian Dobbie"], "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.3; I.2.6; I.2.7; G.2.2; G.3; J.1"], "comment": "29 pages, 9 pages for the main paper, 20 pages for the references and\n  appendix, 25 figures", "summary": "Causal world models are systems that can answer counterfactual questions\nabout an environment of interest, i.e. predict how it would have evolved if an\narbitrary subset of events had been realized differently. It requires\nunderstanding the underlying causes behind chains of events and conducting\ncausal inference for arbitrary unseen distributions. So far, this task eludes\nfoundation models, notably large language models (LLMs), which do not have\ndemonstrated causal reasoning capabilities beyond the memorization of existing\ncausal relationships. Furthermore, evaluating counterfactuals in real-world\napplications is challenging since only the factual world is observed, limiting\nevaluation to synthetic datasets. We address these problems by explicitly\nextracting and modeling causal relationships and propose the Causal\nCartographer framework. First, we introduce a graph retrieval-augmented\ngeneration agent tasked to retrieve causal relationships from data. This\napproach allows us to construct a large network of real-world causal\nrelationships that can serve as a repository of causal knowledge and build\nreal-world counterfactuals. In addition, we create a counterfactual reasoning\nagent constrained by causal relationships to perform reliable step-by-step\ncausal inference. We show that our approach can extract causal knowledge and\nimprove the robustness of LLMs for causal reasoning tasks while reducing\ninference costs and spurious correlations."}
{"id": "2505.14402", "pdf": "https://arxiv.org/pdf/2505.14402.pdf", "abs": "https://arxiv.org/abs/2505.14402", "title": "OmniGenBench: A Modular Platform for Reproducible Genomic Foundation Models Benchmarking", "authors": ["Heng Yang", "Jack Cole", "Yuan Li", "Renzhi Chen", "Geyong Min", "Ke Li"], "categories": ["q-bio.GN", "cs.CL"], "comment": null, "summary": "The code of nature, embedded in DNA and RNA genomes since the origin of life,\nholds immense potential to impact both humans and ecosystems through genome\nmodeling. Genomic Foundation Models (GFMs) have emerged as a transformative\napproach to decoding the genome. As GFMs scale up and reshape the landscape of\nAI-driven genomics, the field faces an urgent need for rigorous and\nreproducible evaluation. We present OmniGenBench, a modular benchmarking\nplatform designed to unify the data, model, benchmarking, and interpretability\nlayers across GFMs. OmniGenBench enables standardized, one-command evaluation\nof any GFM across five benchmark suites, with seamless integration of over 31\nopen-source models. Through automated pipelines and community-extensible\nfeatures, the platform addresses critical reproducibility challenges, including\ndata transparency, model interoperability, benchmark fragmentation, and\nblack-box interpretability. OmniGenBench aims to serve as foundational\ninfrastructure for reproducible genomic AI research, accelerating trustworthy\ndiscovery and collaborative innovation in the era of genome-scale modeling."}
{"id": "2505.14410", "pdf": "https://arxiv.org/pdf/2505.14410.pdf", "abs": "https://arxiv.org/abs/2505.14410", "title": "Pairwise Evaluation of Accent Similarity in Speech Synthesis", "authors": ["Jinzuomu Zhong", "Suyuan Liu", "Dan Wells", "Korin Richmond"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted by INTERSPEECH 2025", "summary": "Despite growing interest in generating high-fidelity accents, evaluating\naccent similarity in speech synthesis has been underexplored. We aim to enhance\nboth subjective and objective evaluation methods for accent similarity.\nSubjectively, we refine the XAB listening test by adding components that\nachieve higher statistical significance with fewer listeners and lower costs.\nOur method involves providing listeners with transcriptions, having them\nhighlight perceived accent differences, and implementing meticulous screening\nfor reliability. Objectively, we utilise pronunciation-related metrics, based\non distances between vowel formants and phonetic posteriorgrams, to evaluate\naccent generation. Comparative experiments reveal that these metrics, alongside\naccent similarity, speaker similarity, and Mel Cepstral Distortion, can be\nused. Moreover, our findings underscore significant limitations of common\nmetrics like Word Error Rate in assessing underrepresented accents."}
{"id": "2505.14412", "pdf": "https://arxiv.org/pdf/2505.14412.pdf", "abs": "https://arxiv.org/abs/2505.14412", "title": "PRL: Prompts from Reinforcement Learning", "authors": ["Paweł Batorski", "Adrian Kosmala", "Paul Swoboda"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Effective prompt engineering remains a central challenge in fully harnessing\nthe capabilities of LLMs. While well-designed prompts can dramatically enhance\nperformance, crafting them typically demands expert intuition and a nuanced\nunderstanding of the task. Moreover, the most impactful prompts often hinge on\nsubtle semantic cues, ones that may elude human perception but are crucial for\nguiding LLM behavior. In this paper, we introduce PRL (Prompts from\nReinforcement Learning), a novel RL-based approach for automatic prompt\ngeneration. Unlike previous methods, PRL can produce novel few-shot examples\nthat were not seen during training. Our approach achieves state-of-the-art\nperformance across a range of benchmarks, including text classification,\nsimplification, and summarization. On the classification task, it surpasses\nprior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it\nimproves the average ROUGE scores on the summarization task by 4.32 over APE\nand by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over\nAPE and by 6.01 over EvoPrompt. Our code is available at\nhttps://github.com/Batorskq/prl ."}
{"id": "2505.14432", "pdf": "https://arxiv.org/pdf/2505.14432.pdf", "abs": "https://arxiv.org/abs/2505.14432", "title": "Rank-K: Test-Time Reasoning for Listwise Reranking", "authors": ["Eugene Yang", "Andrew Yates", "Kathryn Ricci", "Orion Weller", "Vivek Chari", "Benjamin Van Durme", "Dawn Lawrie"], "categories": ["cs.IR", "cs.CL"], "comment": "15 pages, 4 figures", "summary": "Retrieve-and-rerank is a popular retrieval pipeline because of its ability to\nmake slow but effective rerankers efficient enough at query time by reducing\nthe number of comparisons. Recent works in neural rerankers take advantage of\nlarge language models for their capability in reasoning between queries and\npassages and have achieved state-of-the-art retrieval effectiveness. However,\nsuch rerankers are resource-intensive, even after heavy optimization. In this\nwork, we introduce Rank-K, a listwise passage reranking model that leverages\nthe reasoning capability of the reasoning language model at query time that\nprovides test time scalability to serve hard queries. We show that Rank-K\nimproves retrieval effectiveness by 23\\% over the RankZephyr, the\nstate-of-the-art listwise reranker, when reranking a BM25 initial ranked list\nand 19\\% when reranking strong retrieval results by SPLADE-v3. Since Rank-K is\ninherently a multilingual model, we found that it ranks passages based on\nqueries in different languages as effectively as it does in monolingual\nretrieval."}
{"id": "2505.14438", "pdf": "https://arxiv.org/pdf/2505.14438.pdf", "abs": "https://arxiv.org/abs/2505.14438", "title": "S2SBench: A Benchmark for Quantifying Intelligence Degradation in Speech-to-Speech Large Language Models", "authors": ["Yuanbo Fang", "Haoze Sun", "Jun Liu", "Tao Zhang", "Zenan Zhou", "Weipeng Chen", "Xiaofen Xing", "Xiangmin Xu"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "End-to-end speech large language models ((LLMs)) extend the capabilities of\ntext-based models to directly process and generate audio tokens. However, this\noften leads to a decline in reasoning and generation performance compared to\ntext input, a phenomenon referred to as intelligence degradation. To\nsystematically evaluate this gap, we propose S2SBench, a benchmark designed to\nquantify performance degradation in Speech LLMs. It includes diagnostic\ndatasets targeting sentence continuation and commonsense reasoning under audio\ninput. We further introduce a pairwise evaluation protocol based on perplexity\ndifferences between plausible and implausible samples to measure degradation\nrelative to text input. We apply S2SBench to analyze the training process of\nBaichuan-Audio, which further demonstrates the benchmark's effectiveness. All\ndatasets and evaluation code are available at\nhttps://github.com/undobug/S2SBench."}
{"id": "2505.14449", "pdf": "https://arxiv.org/pdf/2505.14449.pdf", "abs": "https://arxiv.org/abs/2505.14449", "title": "Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach", "authors": ["Yi-Cheng Lin", "Huang-Cheng Chou", "Hung-yi Lee"], "categories": ["eess.AS", "cs.CL"], "comment": "Accepted by InterSpeech 2025. 7 pages including 2 pages of appendix", "summary": "While subgroup disparities and performance bias are increasingly studied in\ncomputational research, fairness in categorical Speech Emotion Recognition\n(SER) remains underexplored. Existing methods often rely on explicit\ndemographic labels, which are difficult to obtain due to privacy concerns. To\naddress this limitation, we introduce an Implicit Demography Inference (IDI)\nmodule that leverages pseudo-labeling from a pre-trained model and unsupervised\nlearning using k-means clustering to mitigate bias in SER. Our experiments show\nthat pseudo-labeling IDI reduces subgroup disparities, improving fairness\nmetrics by over 33% with less than a 3% decrease in SER accuracy. Also, the\nunsupervised IDI yields more than a 26% improvement in fairness metrics with a\ndrop of less than 4% in SER performance. Further analyses reveal that the\nunsupervised IDI consistently mitigates race and age disparities, demonstrating\nits potential in scenarios where explicit demographic information is\nunavailable."}
{"id": "2505.14462", "pdf": "https://arxiv.org/pdf/2505.14462.pdf", "abs": "https://arxiv.org/abs/2505.14462", "title": "RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture Understanding", "authors": ["Jiaang Li", "Yifei Yuan", "Wenyan Li", "Mohammad Aliannejadi", "Daniel Hershcovich", "Anders Søgaard", "Ivan Vulić", "Wenxuan Zhang", "Paul Pu Liang", "Yang Deng", "Serge Belongie"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "As vision-language models (VLMs) become increasingly integrated into daily\nlife, the need for accurate visual culture understanding is becoming critical.\nYet, these models frequently fall short in interpreting cultural nuances\neffectively. Prior work has demonstrated the effectiveness of\nretrieval-augmented generation (RAG) in enhancing cultural understanding in\ntext-only settings, while its application in multimodal scenarios remains\nunderexplored. To bridge this gap, we introduce RAVENEA (Retrieval-Augmented\nVisual culturE uNdErstAnding), a new benchmark designed to advance visual\nculture understanding through retrieval, focusing on two tasks: culture-focused\nvisual question answering (cVQA) and culture-informed image captioning (cIC).\nRAVENEA extends existing datasets by integrating over 10,000 Wikipedia\ndocuments curated and ranked by human annotators. With RAVENEA, we train and\nevaluate seven multimodal retrievers for each image query, and measure the\ndownstream impact of retrieval-augmented inputs across fourteen\nstate-of-the-art VLMs. Our results show that lightweight VLMs, when augmented\nwith culture-aware retrieval, outperform their non-augmented counterparts (by\nat least 3.2% absolute on cVQA and 6.2% absolute on cIC). This highlights the\nvalue of retrieval-augmented methods and culturally inclusive benchmarks for\nmultimodal understanding."}
{"id": "2505.14470", "pdf": "https://arxiv.org/pdf/2505.14470.pdf", "abs": "https://arxiv.org/abs/2505.14470", "title": "PAST: Phonetic-Acoustic Speech Tokenizer", "authors": ["Nadav Har-Tuv", "Or Tal", "Yossi Adi"], "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "comment": null, "summary": "We present PAST, a novel end-to-end framework that jointly models phonetic\ninformation alongside signal reconstruction, eliminating the need for external\npretrained models. Unlike previous approaches that rely on pretrained\nself-supervised models, PAST employs supervised phonetic data, directly\nintegrating domain knowledge into the tokenization process via auxiliary tasks.\nAdditionally, we introduce a streamable, causal variant of PAST, enabling\nreal-time speech applications. Results demonstrate that PAST surpasses existing\nevaluated baseline tokenizers across common evaluation metrics, including\nphonetic representation and speech reconstruction. Notably, PAST also achieves\nsuperior performance when serving as a speech representation for speech\nlanguage models, further highlighting its effectiveness as a foundation for\nspoken language generation. To foster further research, we release the full\nimplementation. For code, model checkpoints, and samples see:\nhttps://pages.cs.huji.ac.il/adiyoss-lab/PAST"}
{"id": "2505.14479", "pdf": "https://arxiv.org/pdf/2505.14479.pdf", "abs": "https://arxiv.org/abs/2505.14479", "title": "Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach", "authors": ["Oren Sultan", "Eitan Stern", "Dafna Shahaf"], "categories": ["cs.AI", "cs.CL"], "comment": "long paper", "summary": "Large language models (LLMs) struggle with formal domains that require\nrigorous logical deduction and symbolic reasoning, such as mathematical proof\ngeneration. We propose a neuro-symbolic approach that combines LLMs' generative\nstrengths with structured components to overcome this challenge. As a\nproof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)\nwe retrieve analogous problems and use their proofs to guide the LLM, and (2) a\nformal verifier evaluates the generated proofs and provides feedback, helping\nthe model fix incorrect proofs. We demonstrate that our method significantly\nimproves proof accuracy for OpenAI's o1 model (58%-70% improvement); both\nanalogous problems and the verifier's feedback contribute to these gains. More\nbroadly, shifting to LLMs that generate provably correct conclusions could\ndramatically improve their reliability, accuracy and consistency, unlocking\ncomplex tasks and critical real-world applications that require\ntrustworthiness."}
{"id": "2505.14489", "pdf": "https://arxiv.org/pdf/2505.14489.pdf", "abs": "https://arxiv.org/abs/2505.14489", "title": "Reasoning Models Better Express Their Confidence", "authors": ["Dongkeun Yoon", "Seungone Kim", "Sohee Yang", "Sunkyoung Kim", "Soyeon Kim", "Yongil Kim", "Eunbi Choi", "Yireun Kim", "Minjoon Seo"], "categories": ["cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Despite their strengths, large language models (LLMs) often fail to\ncommunicate their confidence accurately, making it difficult to assess when\nthey might be wrong and limiting their reliability. In this work, we\ndemonstrate that reasoning models-LLMs that engage in extended chain-of-thought\n(CoT) reasoning-exhibit superior performance not only in problem-solving but\nalso in accurately expressing their confidence. Specifically, we benchmark six\nreasoning models across six datasets and find that they achieve strictly better\nconfidence calibration than their non-reasoning counterparts in 33 out of the\n36 settings. Our detailed analysis reveals that these gains in calibration stem\nfrom the slow thinking behaviors of reasoning models-such as exploring\nalternative approaches and backtracking-which enable them to adjust their\nconfidence dynamically throughout their CoT, making it progressively more\naccurate. In particular, we find that reasoning models become increasingly\nbetter calibrated as their CoT unfolds, a trend not observed in non-reasoning\nmodels. Moreover, removing slow thinking behaviors from the CoT leads to a\nsignificant drop in calibration. Lastly, we show that these gains are not\nexclusive to reasoning models-non-reasoning models also benefit when guided to\nperform slow thinking via in-context learning."}
{"id": "2505.14518", "pdf": "https://arxiv.org/pdf/2505.14518.pdf", "abs": "https://arxiv.org/abs/2505.14518", "title": "Teaching Audio-Aware Large Language Models What Does Not Hear: Mitigating Hallucinations through Synthesized Negative Samples", "authors": ["Chun-Yi Kuan", "Hung-yi Lee"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to Interspeech 2025", "summary": "Recent advancements in audio-aware large language models (ALLMs) enable them\nto process and understand audio inputs. However, these models often hallucinate\nnon-existent sound events, reducing their reliability in real-world\napplications. To address this, we propose LISTEN (Learning to Identify Sounds\nThrough Extended Negative Samples), a contrastive-like training method that\nenhances ALLMs' ability to distinguish between present and absent sounds using\nsynthesized data from the backbone LLM. Unlike prior approaches, our method\nrequires no modification to LLM parameters and efficiently integrates audio\nrepresentations via a lightweight adapter. Experiments show that LISTEN\neffectively mitigates hallucinations while maintaining impressive performance\non existing audio question and reasoning benchmarks. At the same time, it is\nmore efficient in both data and computation."}
{"id": "2505.14569", "pdf": "https://arxiv.org/pdf/2505.14569.pdf", "abs": "https://arxiv.org/abs/2505.14569", "title": "Agent Context Protocols Enhance Collective Inference", "authors": ["Devansh Bhardwaj", "Arjun Beniwal", "Shreyas Chaudhari", "Ashwin Kalyan", "Tanmay Rajpurohit", "Karthik R. Narasimhan", "Ameet Deshpande", "Vishvak Murahari"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "AI agents have become increasingly adept at complex tasks such as coding,\nreasoning, and multimodal understanding. However, building generalist systems\nrequires moving beyond individual agents to collective inference -- a paradigm\nwhere multi-agent systems with diverse, task-specialized agents complement one\nanother through structured communication and collaboration. Today, coordination\nis usually handled with imprecise, ad-hoc natural language, which limits\ncomplex interaction and hinders interoperability with domain-specific agents.\nWe introduce Agent context protocols (ACPs): a domain- and agent-agnostic\nfamily of structured protocols for agent-agent communication, coordination, and\nerror handling. ACPs combine (i) persistent execution blueprints -- explicit\ndependency graphs that store intermediate agent outputs -- with (ii)\nstandardized message schemas, enabling robust and fault-tolerant multi-agent\ncollective inference. ACP-powered generalist systems reach state-of-the-art\nperformance: 28.3 % accuracy on AssistantBench for long-horizon web assistance\nand best-in-class multimodal technical reports, outperforming commercial AI\nsystems in human evaluation. ACPs are highly modular and extensible, allowing\npractitioners to build top-tier generalist agents quickly."}
{"id": "2505.14615", "pdf": "https://arxiv.org/pdf/2505.14615.pdf", "abs": "https://arxiv.org/abs/2505.14615", "title": "SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas", "authors": ["Anjiang Wei", "Yuheng Wu", "Yingjia Wan", "Tarun Suresh", "Huanmi Tan", "Zhanke Zhou", "Sanmi Koyejo", "Ke Wang", "Alex Aiken"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "comment": null, "summary": "We introduce SATBench, a benchmark for evaluating the logical reasoning\ncapabilities of large language models (LLMs) through logical puzzles derived\nfrom Boolean satisfiability (SAT) problems. Unlike prior work that focuses on\ninference rule-based reasoning, which often involves deducing conclusions from\na set of premises, our approach leverages the search-based nature of SAT\nproblems, where the objective is to find a solution that fulfills a specified\nset of logical constraints. Each instance in SATBench is generated from a SAT\nformula, then translated into a story context and conditions using LLMs. The\ngeneration process is fully automated and allows for adjustable difficulty by\nvarying the number of clauses. All 2100 puzzles are validated through both\nLLM-assisted and solver-based consistency checks, with human validation on a\nsubset. Experimental results show that even the strongest model, o4-mini,\nachieves only 65.0% accuracy on hard UNSAT problems, close to the random\nbaseline of 50%. SATBench exposes fundamental limitations in the search-based\nlogical reasoning abilities of current LLMs and provides a scalable testbed for\nfuture research in logical reasoning."}
{"id": "2505.14620", "pdf": "https://arxiv.org/pdf/2505.14620.pdf", "abs": "https://arxiv.org/abs/2505.14620", "title": "Enhancing Learned Knowledge in LoRA Adapters Through Efficient Contrastive Decoding on Ascend NPUs", "authors": ["Morgan Lindsay Heisler", "Linzi Xing", "Ge Shi", "Hanieh Sadri", "Gursimran Singh", "Weiwei Zhang", "Tao Ye", "Ying Xiong", "Yong Zhang", "Zhenan Fan"], "categories": ["cs.LG", "cs.CL"], "comment": "Accepted at ACM KDD 2025", "summary": "Huawei Cloud users leverage LoRA (Low-Rank Adaptation) as an efficient and\nscalable method to fine-tune and customize large language models (LLMs) for\napplication-specific needs. However, tasks that require complex reasoning or\ndeep contextual understanding are often hindered by biases or interference from\nthe base model when using typical decoding methods like greedy or beam search.\nThese biases can lead to generic or task-agnostic responses from the base model\ninstead of leveraging the LoRA-specific adaptations. In this paper, we\nintroduce Contrastive LoRA Decoding (CoLD), a novel decoding framework designed\nto maximize the use of task-specific knowledge in LoRA-adapted models,\nresulting in better downstream performance. CoLD uses contrastive decoding by\nscoring candidate tokens based on the divergence between the probability\ndistributions of a LoRA-adapted expert model and the corresponding base model.\nThis approach prioritizes tokens that better align with the LoRA's learned\nrepresentations, enhancing performance for specialized tasks. While effective,\na naive implementation of CoLD is computationally expensive because each\ndecoding step requires evaluating multiple token candidates across both models.\nTo address this, we developed an optimized kernel for Huawei's Ascend NPU. CoLD\nachieves up to a 5.54% increase in task accuracy while reducing end-to-end\nlatency by 28% compared to greedy decoding. This work provides practical and\nefficient decoding strategies for fine-tuned LLMs in resource-constrained\nenvironments and has broad implications for applied data science in both cloud\nand on-premises settings."}
{"id": "2505.14625", "pdf": "https://arxiv.org/pdf/2505.14625.pdf", "abs": "https://arxiv.org/abs/2505.14625", "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning", "authors": ["Zhangchen Xu", "Yuetai Li", "Fengqing Jiang", "Bhaskar Ramasubramanian", "Luyao Niu", "Bill Yuchen Lin", "Radha Poovendran"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV."}
{"id": "2505.14627", "pdf": "https://arxiv.org/pdf/2505.14627.pdf", "abs": "https://arxiv.org/abs/2505.14627", "title": "Debating for Better Reasoning: An Unsupervised Multimodal Approach", "authors": ["Ashutosh Adhikari", "Mirella Lapata"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) gain expertise across diverse domains and\nmodalities, scalable oversight becomes increasingly challenging, particularly\nwhen their capabilities may surpass human evaluators. Debate has emerged as a\npromising mechanism for enabling such oversight. In this work, we extend the\ndebate paradigm to a multimodal setting, exploring its potential for weaker\nmodels to supervise and enhance the performance of stronger models. We focus on\nvisual question answering (VQA), where two \"sighted\" expert vision-language\nmodels debate an answer, while a \"blind\" (text-only) judge adjudicates based\nsolely on the quality of the arguments. In our framework, the experts defend\nonly answers aligned with their beliefs, thereby obviating the need for\nexplicit role-playing and concentrating the debate on instances of expert\ndisagreement. Experiments on several multimodal tasks demonstrate that the\ndebate framework consistently outperforms individual expert models. Moreover,\njudgments from weaker LLMs can help instill reasoning capabilities in\nvision-language models through finetuning."}
{"id": "2505.14629", "pdf": "https://arxiv.org/pdf/2505.14629.pdf", "abs": "https://arxiv.org/abs/2505.14629", "title": "KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models", "authors": ["Fnu Mohbat", "Mohammed J Zaki"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "Accepted at ACL 2025", "summary": "Recent advances in large language models (LLMs) and the abundance of food\ndata have resulted in studies to improve food understanding using LLMs. Despite\nseveral recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there\nhas been limited research on integrating food related KGs with LLMs. We\nintroduce KERL, a unified system that leverages food KGs and LLMs to provide\npersonalized food recommendations and generates recipes with associated\nmicro-nutritional information. Given a natural language question, KERL extracts\nentities, retrieves subgraphs from the KG, which are then fed into the LLM as\ncontext to select the recipes that satisfy the constraints. Next, our system\ngenerates the cooking steps and nutritional information for each recipe. To\nevaluate our approach, we also develop a benchmark dataset by curating recipe\nrelated questions, combined with constraints and personal preferences. Through\nextensive experiments, we show that our proposed KG-augmented LLM significantly\noutperforms existing approaches, offering a complete and coherent solution for\nfood recommendation, recipe generation, and nutritional analysis. Our code and\nbenchmark datasets are publicly available at\nhttps://github.com/mohbattharani/KERL."}
{"id": "2505.14638", "pdf": "https://arxiv.org/pdf/2505.14638.pdf", "abs": "https://arxiv.org/abs/2505.14638", "title": "Dual Precision Quantization for Efficient and Accurate Deep Neural Networks Inference", "authors": ["Tomer Gafni", "Asaf Karnieli", "Yair Hanani"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Accepted at eLVM Workshop, CVPR, 2025", "summary": "Deep neural networks have achieved state-of-the-art results in a wide range\nof applications, from natural language processing and computer vision to speech\nrecognition. However, as tasks become increasingly complex, model sizes\ncontinue to grow, posing challenges in latency and memory efficiency. To meet\nthese constraints, post-training quantization has emerged as a promising\nsolution. In this paper, we propose a novel hardware-efficient quantization and\ninference scheme that exploits hardware advantages with minimal accuracy\ndegradation. Specifically, we introduce a W4A8 scheme, where weights are\nquantized and stored using 4-bit integer precision, and inference computations\nare performed using 8-bit floating-point arithmetic, demonstrating significant\nspeedups and improved memory utilization compared to 16-bit operations,\napplicable on various modern accelerators. To mitigate accuracy loss, we\ndevelop a novel quantization algorithm, dubbed Dual Precision Quantization\n(DPQ), that leverages the unique structure of our scheme without introducing\nadditional inference overhead. Experimental results demonstrate improved\nperformance (i.e., increased throughput) while maintaining tolerable accuracy\ndegradation relative to the full-precision model."}
{"id": "2505.14654", "pdf": "https://arxiv.org/pdf/2505.14654.pdf", "abs": "https://arxiv.org/abs/2505.14654", "title": "Beyond Words: Multimodal LLM Knows When to Speak", "authors": ["Zikai Liao", "Yi Ouyang", "Yi-Lun Lee", "Chen-Ping Yu", "Yi-Hsuan Tsai", "Zhaozheng Yin"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project page: https://github.com/lzk901372/MM-When2Speak", "summary": "While large language model (LLM)-based chatbots have demonstrated strong\ncapabilities in generating coherent and contextually relevant responses, they\noften struggle with understanding when to speak, particularly in delivering\nbrief, timely reactions during ongoing conversations. This limitation arises\nlargely from their reliance on text input, lacking the rich contextual cues in\nreal-world human dialogue. In this work, we focus on real-time prediction of\nresponse types, with an emphasis on short, reactive utterances that depend on\nsubtle, multimodal signals across vision, audio, and text. To support this, we\nintroduce a new multimodal dataset constructed from real-world conversational\nvideos, containing temporally aligned visual, auditory, and textual streams.\nThis dataset enables fine-grained modeling of response timing in dyadic\ninteractions. Building on this dataset, we propose MM-When2Speak, a multimodal\nLLM-based model that adaptively integrates visual, auditory, and textual\ncontext to predict when a response should occur, and what type of response is\nappropriate. Experiments show that MM-When2Speak significantly outperforms\nstate-of-the-art unimodal and LLM-based baselines, achieving up to a 4x\nimprovement in response timing accuracy over leading commercial LLMs. These\nresults underscore the importance of multimodal inputs for producing timely,\nnatural, and engaging conversational AI."}
{"id": "2505.14667", "pdf": "https://arxiv.org/pdf/2505.14667.pdf", "abs": "https://arxiv.org/abs/2505.14667", "title": "SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment", "authors": ["Wonje Jeung", "Sangyeon Yoon", "Minsuk Kahng", "Albert No"], "categories": ["cs.AI", "cs.CL"], "comment": "22 pages", "summary": "Large Reasoning Models (LRMs) have become powerful tools for complex problem\nsolving, but their structured reasoning pathways can lead to unsafe outputs\nwhen exposed to harmful prompts. Existing safety alignment methods reduce\nharmful outputs but can degrade reasoning depth, leading to significant\ntrade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated\njailbreak attacks. To address this, we introduce SAFEPATH, a lightweight\nalignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at\nthe start of their reasoning, in response to harmful prompts, while leaving the\nrest of the reasoning process unsupervised. Empirical results across multiple\nbenchmarks indicate that SAFEPATH effectively reduces harmful outputs while\nmaintaining reasoning performance. Specifically, SAFEPATH reduces harmful\nresponses by up to 90.0% and blocks 83.3% of jailbreak attempts in the\nDeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than\nDirect Refusal and 314.1x less than SafeChain. We further introduce a zero-shot\nvariant that requires no fine-tuning. In addition, we provide a comprehensive\nanalysis of how existing methods in LLMs generalize, or fail, when applied to\nreasoning-centric models, revealing critical gaps and new directions for safer\nAI."}
{"id": "2505.14668", "pdf": "https://arxiv.org/pdf/2505.14668.pdf", "abs": "https://arxiv.org/abs/2505.14668", "title": "ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions", "authors": ["Bufang Yang", "Lilin Xu", "Liekang Zeng", "Kaiwei Liu", "Siyang Jiang", "Wenrui Lu", "Hongkai Chen", "Xiaofan Jiang", "Guoliang Xing", "Zhenyu Yan"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have propelled intelligent\nagents from reactive responses to proactive support. While promising, existing\nproactive agents either rely exclusively on observations from enclosed\nenvironments (e.g., desktop UIs) with direct LLM inference or employ rule-based\nproactive notifications, leading to suboptimal user intent understanding and\nlimited functionality for proactive service. In this paper, we introduce\nContextAgent, the first context-aware proactive agent that incorporates\nextensive sensory contexts to enhance the proactive capabilities of LLM agents.\nContextAgent first extracts multi-dimensional contexts from massive sensory\nperceptions on wearables (e.g., video and audio) to understand user intentions.\nContextAgent then leverages the sensory contexts and the persona contexts from\nhistorical data to predict the necessity for proactive services. When proactive\nassistance is needed, ContextAgent further automatically calls the necessary\ntools to assist users unobtrusively. To evaluate this new task, we curate\nContextAgentBench, the first benchmark for evaluating context-aware proactive\nLLM agents, covering 1,000 samples across nine daily scenarios and twenty\ntools. Experiments on ContextAgentBench show that ContextAgent outperforms\nbaselines by achieving up to 8.5% and 6.0% higher accuracy in proactive\npredictions and tool calling, respectively. We hope our research can inspire\nthe development of more advanced, human-centric, proactive AI assistants."}
{"id": "2505.14680", "pdf": "https://arxiv.org/pdf/2505.14680.pdf", "abs": "https://arxiv.org/abs/2505.14680", "title": "NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search", "authors": ["Sunhao Dai", "Wenjie Wang", "Liang Pang", "Jun Xu", "See-Kiong Ng", "Ji-Rong Wen", "Tat-Seng Chua"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.HC"], "comment": "SIGIR 2025 Perspective Paper", "summary": "Generative AI search is reshaping information retrieval by offering\nend-to-end answers to complex queries, reducing users' reliance on manually\nbrowsing and summarizing multiple web pages. However, while this paradigm\nenhances convenience, it disrupts the feedback-driven improvement loop that has\nhistorically powered the evolution of traditional Web search. Web search can\ncontinuously improve their ranking models by collecting large-scale,\nfine-grained user feedback (e.g., clicks, dwell time) at the document level. In\ncontrast, generative AI search operates through a much longer search pipeline,\nspanning query decomposition, document retrieval, and answer generation, yet\ntypically receives only coarse-grained feedback on the final answer. This\nintroduces a feedback loop disconnect, where user feedback for the final output\ncannot be effectively mapped back to specific system components, making it\ndifficult to improve each intermediate stage and sustain the feedback loop. In\nthis paper, we envision NExT-Search, a next-generation paradigm designed to\nreintroduce fine-grained, process-level feedback into generative AI search.\nNExT-Search integrates two complementary modes: User Debug Mode, which allows\nengaged users to intervene at key stages; and Shadow User Mode, where a\npersonalized user agent simulates user preferences and provides AI-assisted\nfeedback for less interactive users. Furthermore, we envision how these\nfeedback signals can be leveraged through online adaptation, which refines\ncurrent search outputs in real-time, and offline update, which aggregates\ninteraction logs to periodically fine-tune query decomposition, retrieval, and\ngeneration models. By restoring human control over key stages of the generative\nAI search pipeline, we believe NExT-Search offers a promising direction for\nbuilding feedback-rich AI search systems that can evolve continuously alongside\nhuman feedback."}
{"id": "2505.14681", "pdf": "https://arxiv.org/pdf/2505.14681.pdf", "abs": "https://arxiv.org/abs/2505.14681", "title": "Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training", "authors": ["Mengru Wang", "Xingyu Chen", "Yue Wang", "Zhiwei He", "Jiahao Xu", "Tian Liang", "Qiuzhi Liu", "Yunzhi Yao", "Wenxuan Wang", "Ruotian Ma", "Haitao Mi", "Ningyu Zhang", "Zhaopeng Tu", "Xiaolong Li", "Dong Yu"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.IR", "cs.LG"], "comment": "Work in progress", "summary": "Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)\nhave achieved impressive reasoning capabilities by selectively activating\nexperts to facilitate structured cognitive processes. Despite notable advances,\nexisting reasoning models often suffer from cognitive inefficiencies like\noverthinking and underthinking. To address these limitations, we introduce a\nnovel inference-time steering methodology called Reinforcing Cognitive Experts\n(RICE), designed to improve reasoning performance without additional training\nor complex heuristics. Leveraging normalized Pointwise Mutual Information\n(nPMI), we systematically identify specialized experts, termed ''cognitive\nexperts'' that orchestrate meta-level reasoning operations characterized by\ntokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs\n(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning\nbenchmarks demonstrate noticeable and consistent improvements in reasoning\naccuracy, cognitive efficiency, and cross-domain generalization. Crucially, our\nlightweight approach substantially outperforms prevalent reasoning-steering\ntechniques, such as prompt design and decoding constraints, while preserving\nthe model's general instruction-following skills. These results highlight\nreinforcing cognitive experts as a promising, practical, and interpretable\ndirection to enhance cognitive efficiency within advanced reasoning models."}
{"id": "2312.10097", "pdf": "https://arxiv.org/pdf/2312.10097.pdf", "abs": "https://arxiv.org/abs/2312.10097", "title": "Arithmetics-Based Decomposition of Numeral Words -- Arithmetic Conditions give the Unpacking Strategy", "authors": ["Isidor Konrad Maier", "Matthias Wolff"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a novel numeral decomposer based on arithmetic criteria.\nThe criteria are not dependent on a base-10 assumption but only on Hurford's\nPacking Strategy. Hurford's Packing Strategy constitutes numerals by packing\nfactors and summands to multiplicators. We found out that a numeral of value n\nhas a multiplicator larger than sqrt(n), a summand smaller than n/2 and a\nfactor smaller than sqrt(n). Using these findings, the numeral decomposer\nattempts to detect and unpack factors and summand in order to reverse Hurford's\nPacking strategy. We tested its applicability for incremental unsupervised\ngrammar induction in 273 languages. This way, grammars were obtained with\nsensible mathematical attributes that explain the structure of produced\nnumerals. The numeral-decomposer-induced grammars are often close to\nexpert-made and more compact than numeral grammars induced by a modern\nstate-of-the-art grammar induction tool. Furthermore, this paper contains a\nreport about the few cases of incorrect induced mathematical attributes, which\nare often linked to linguistic peculiarities like context sensitivity."}
{"id": "2403.10056", "pdf": "https://arxiv.org/pdf/2403.10056.pdf", "abs": "https://arxiv.org/abs/2403.10056", "title": "Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning", "authors": ["Yongquan He", "Wenyuan Zhang", "Xuancheng Huang", "Peng Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 6 figures", "summary": "Instruction tuning for large language models (LLMs) can drive them to produce\nresults consistent with human goals in specific downstream tasks. However, the\nprocess of continual instruction tuning (CIT) for LLMs may bring about the\ncatastrophic forgetting (CF) problem, where previously learned abilities are\ndegraded. Recent methods try to alleviate the CF problem by modifying models or\nreplaying data, which may only remember the surface-level pattern of\ninstructions and get confused on held-out tasks. In this paper, we propose a\nnovel continual instruction tuning method based on Key-part Information Gain\n(KPIG). Our method computes the information gain on masked parts to dynamically\nreplay data and refine the training objective, which enables LLMs to capture\ntask-aware information relevant to the correct response and alleviate\noverfitting to general descriptions in instructions. In addition, we propose\ntwo metrics, P-score and V-score, to measure the generalization and\ninstruction-following abilities of LLMs. Experiments demonstrate our method\nachieves superior performance on both seen and held-out tasks."}
{"id": "2404.17662", "pdf": "https://arxiv.org/pdf/2404.17662.pdf", "abs": "https://arxiv.org/abs/2404.17662", "title": "PLAYER*: Enhancing LLM-based Multi-Agent Communication and Interaction in Murder Mystery Games", "authors": ["Qinglin Zhu", "Runcong Zhao", "Bin Liang", "Jinhua Du", "Lin Gui", "Yulan He"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce WellPlay, a reasoning dataset for multi-agent conversational\ninference in Murder Mystery Games (MMGs). WellPlay comprises 1,482 inferential\nquestions across 12 games, spanning objectives, reasoning, and relationship\nunderstanding, and establishes a systematic benchmark for evaluating agent\nreasoning abilities in complex social settings. Building on this foundation, we\npresent PLAYER*, a novel framework for Large Language Model (LLM)-based agents\nin MMGs. MMGs pose unique challenges, including undefined state spaces, absent\nintermediate rewards, and the need for strategic reasoning through natural\nlanguage. PLAYER* addresses these challenges with a sensor-based state\nrepresentation and an information-driven strategy that optimises questioning\nand suspect pruning. Experiments show that PLAYER* outperforms existing methods\nin reasoning accuracy, efficiency, and agent-human interaction, advancing\nreasoning agents for complex social scenarios."}
{"id": "2407.01082", "pdf": "https://arxiv.org/pdf/2407.01082.pdf", "abs": "https://arxiv.org/abs/2407.01082", "title": "Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs", "authors": ["Minh Nguyen", "Andrew Baker", "Clement Neo", "Allen Roush", "Andreas Kirsch", "Ravid Shwartz-Ziv"], "categories": ["cs.CL"], "comment": "In line with ICLR/Openreview changes + better overall reading flow.\n  https://iclr.cc/virtual/2025/poster/30358", "summary": "Large Language Models (LLMs) generate text by sampling the next token from a\nprobability distribution over the vocabulary at each decoding step. Popular\nsampling methods like top-p (nucleus sampling) often struggle to balance\nquality and diversity, especially at higher temperatures which lead to\nincoherent or repetitive outputs. We propose min-p sampling, a dynamic\ntruncation method that adjusts the sampling threshold based on the model's\nconfidence by using the top token's probability as a scaling factor. Our\nexperiments on benchmarks including GPQA, GSM8K, and AlpacaEval Creative\nWriting show that min-p sampling improves both the quality and diversity of\ngenerated text across different model families (Mistral and Llama 3) and model\nsizes (1B to 123B parameters), especially at higher temperatures. Human\nevaluations further show a clear preference for min-p sampling, in both text\nquality and creativity. Min-p sampling has been adopted by popular open-source\nLLM frameworks, including Hugging Face Transformers, VLLM, and many others,\nhighlighting its considerable impact on improving text generation quality."}
{"id": "2407.18416", "pdf": "https://arxiv.org/pdf/2407.18416.pdf", "abs": "https://arxiv.org/abs/2407.18416", "title": "PersonaGym: Evaluating Persona Agents and LLMs", "authors": ["Vinay Samuel", "Henry Peng Zou", "Yue Zhou", "Shreyas Chaudhari", "Ashwin Kalyan", "Tanmay Rajpurohit", "Ameet Deshpande", "Karthik Narasimhan", "Vishvak Murahari"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "21 pages, 5 figures", "summary": "Persona agents, which are LLM agents conditioned to act according to an\nassigned persona, enable contextually rich and user aligned interactions across\ndomains like education and healthcare. However, evaluating how faithfully these\nagents adhere to their personas remains a significant challenge, particularly\nin free-form settings that demand consistency across diverse, persona-relevant\nenvironments. We introduce PersonaGym, the first dynamic evaluation framework\nfor persona agents, and PersonaScore, a human-aligned automatic metric grounded\nin decision theory that enables comprehensive large-scale evaluation. Our\nevaluation of 10 leading LLMs across 200 personas and 10,000 questions reveals\nsignificant advancement opportunities. For example, GPT-4.1 had the exact same\nPersonaScore as LLaMA-3-8b despite being a more recent and advanced closed\nsource model. Importantly, increased model size and complexity do not\nnecessarily enhance persona agent capabilities, underscoring the need for\nalgorithmic and architectural innovation toward faithful, performant persona\nagents."}
{"id": "2409.00054", "pdf": "https://arxiv.org/pdf/2409.00054.pdf", "abs": "https://arxiv.org/abs/2409.00054", "title": "Automating Intervention Discovery from Scientific Literature: A Progressive Ontology Prompting and Dual-LLM Framework", "authors": ["Yuting Hu", "Dancheng Liu", "Qingyun Wang", "Charles Yu", "Chenhui Xu", "Qingxiao Zheng", "Heng Ji", "Jinjun Xiong"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by IJCAI2025", "summary": "Identifying effective interventions from the scientific literature is\nchallenging due to the high volume of publications, specialized terminology,\nand inconsistent reporting formats, making manual curation laborious and prone\nto oversight. To address this challenge, this paper proposes a novel framework\nleveraging large language models (LLMs), which integrates a progressive\nontology prompting (POP) algorithm with a dual-agent system, named LLM-Duo. On\nthe one hand, the POP algorithm conducts a prioritized breadth-first search\n(BFS) across a predefined ontology, generating structured prompt templates and\naction sequences to guide the automatic annotation process. On the other hand,\nthe LLM-Duo system features two specialized LLM agents, an explorer and an\nevaluator, working collaboratively and adversarially to continuously refine\nannotation quality. We showcase the real-world applicability of our framework\nthrough a case study focused on speech-language intervention discovery.\nExperimental results show that our approach surpasses advanced baselines,\nachieving more accurate and comprehensive annotations through a fully automated\nprocess. Our approach successfully identified 2,421 interventions from a corpus\nof 64,177 research articles in the speech-language pathology domain,\nculminating in the creation of a publicly accessible intervention knowledge\nbase with great potential to benefit the speech-language pathology community."}
{"id": "2409.11074", "pdf": "https://arxiv.org/pdf/2409.11074.pdf", "abs": "https://arxiv.org/abs/2409.11074", "title": "RoMath: A Mathematical Reasoning Benchmark in Romanian", "authors": ["Adrian Cosma", "Ana-Maria Bucur", "Emilian Radoi"], "categories": ["cs.CL", "cs.AI"], "comment": "5 Figures, 11 Tables", "summary": "Mathematics has long been conveyed through natural language, primarily for\nhuman understanding. With the rise of mechanized mathematics and proof\nassistants, there is a growing need to understand informal mathematical text,\nyet most existing benchmarks focus solely on English, overlooking other\nlanguages. This paper introduces RoMath, a Romanian mathematical reasoning\nbenchmark suite comprising three subsets: Baccalaureate, Competitions and\nSynthetic, which cover a range of mathematical domains and difficulty levels,\naiming to improve non-English language models and promote multilingual AI\ndevelopment. By focusing on Romanian, a low-resource language with unique\nlinguistic features, RoMath addresses the limitations of Anglo-centric models\nand emphasizes the need for dedicated resources beyond simple automatic\ntranslation. We benchmark several open-weight language models, highlighting the\nimportance of creating resources for underrepresented languages. Code and\ndatasets are be made available."}
{"id": "2409.11726", "pdf": "https://arxiv.org/pdf/2409.11726.pdf", "abs": "https://arxiv.org/abs/2409.11726", "title": "Revealing and Mitigating the Challenge of Detecting Character Knowledge Errors in LLM Role-Playing", "authors": ["Wenyuan Zhang", "Shuaiyi Nie", "Jiawei Sheng", "Zefeng Zhang", "Xinghua Zhang", "Yongquan He", "Tingwen Liu"], "categories": ["cs.CL", "cs.HC"], "comment": "25 pages, 6 figures, 20 tables", "summary": "Large language model (LLM) role-playing has gained widespread attention.\nAuthentic character knowledge is crucial for constructing realistic LLM\nrole-playing agents. However, existing works usually overlook the exploration\nof LLMs' ability to detect characters' known knowledge errors (KKE) and unknown\nknowledge errors (UKE) while playing roles, which would lead to low-quality\nautomatic construction of character trainable corpus. In this paper, we propose\nRoleKE-Bench to evaluate LLMs' ability to detect errors in KKE and UKE. The\nresults indicate that even the latest LLMs struggle to detect these two types\nof errors effectively, especially when it comes to familiar knowledge. We\nexperimented with various reasoning strategies and propose an agent-based\nreasoning method, Self-Recollection and Self-Doubt (S$^2$RD), to explore\nfurther the potential for improving error detection capabilities. Experiments\nshow that our method effectively improves the LLMs' ability to detect error\ncharacter knowledge, but it remains an issue that requires ongoing attention."}
{"id": "2410.03663", "pdf": "https://arxiv.org/pdf/2410.03663.pdf", "abs": "https://arxiv.org/abs/2410.03663", "title": "Learning from Committee: Reasoning Distillation from a Mixture of Teachers with Peer-Review", "authors": ["Zhuochun Li", "Yuelyu Ji", "Rui Meng", "Daqing He"], "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 5 figures", "summary": "While reasoning capabilities typically emerge in large language models (LLMs)\nwith tens of billions of parameters, recent research focuses on improving\nsmaller open-source models through knowledge distillation (KD) from commercial\nLLMs. However, many of these studies rely solely on responses from a single LLM\nas the gold rationale, unlike the natural human learning process, which\ninvolves understanding both the correct answers and the reasons behind\nmistakes. In this paper, we introduce a novel Fault-Aware DistIllation via\nPeer-Review (FAIR) approach: 1) instead of merely obtaining rationales from\nteachers, our method asks teachers to identify and explain the student's\nmistakes, providing customized instruction learning data; 2) we design a\nsimulated peer-review process between teacher LLMs, and selects only the\ngenerated rationales above the acceptance threshold, which reduces the chance\nof teachers guessing correctly with flawed rationale, improving instructional\ndata quality. Comprehensive experiments and analysis on mathematical,\ncommonsense, and logical reasoning tasks demonstrate the effectiveness of our\nmethod. Our code is available at\nhttps://github.com/zhuochunli/Learn-from-Committee."}
{"id": "2410.10624", "pdf": "https://arxiv.org/pdf/2410.10624.pdf", "abs": "https://arxiv.org/abs/2410.10624", "title": "SensorLLM: Human-Intuitive Alignment of Multivariate Sensor Data with LLMs for Activity Recognition", "authors": ["Zechen Li", "Shohreh Deldari", "Linyao Chen", "Hao Xue", "Flora D. Salim"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce SensorLLM, a two-stage framework that enables Large Language\nModels (LLMs) to perform human activity recognition (HAR) from wearable sensor\ndata. While LLMs excel at reasoning and generalization, they struggle with\ntime-series inputs due to limited semantic context, numerical complexity, and\nsequence variability. To address these challenges, we construct SensorQA, a\nquestion-answering dataset of human-intuitive sensor-text pairs spanning\ndiverse HAR scenarios. It supervises the Sensor-Language Alignment stage, where\nthe model aligns sensor inputs with trend descriptions. Special tokens are\nintroduced to mark channel boundaries. This alignment enables LLMs to interpret\nnumerical patterns, channel-specific signals, and variable-length\ninputs--without requiring human annotation. In the subsequent Task-Aware Tuning\nstage, we adapt the model for multivariate HAR classification, achieving\nperformance that matches or exceeds state-of-the-art methods. Our results show\nthat, guided by human-intuitive alignment, SensorLLM becomes an effective\nsensor learner, reasoner, and classifier--generalizing across varied HAR\nsettings and paving the way for foundation model research in time-series\nanalysis."}
{"id": "2410.11348", "pdf": "https://arxiv.org/pdf/2410.11348.pdf", "abs": "https://arxiv.org/abs/2410.11348", "title": "RATE: Causal Explainability of Reward Models with Imperfect Counterfactuals", "authors": ["David Reber", "Sean Richardson", "Todd Nief", "Cristina Garbacea", "Victor Veitch"], "categories": ["cs.CL", "cs.AI"], "comment": "ICML 2025. Code at https://github.com/toddnief/RATE", "summary": "Reward models are widely used as proxies for human preferences when aligning\nor evaluating LLMs. However, reward models are black boxes, and it is often\nunclear what, exactly, they are actually rewarding. In this paper we develop\nRewrite-based Attribute Treatment Estimator (RATE) as an effective method for\nmeasuring the sensitivity of a reward model to high-level attributes of\nresponses, such as sentiment, helpfulness, or complexity. Importantly, RATE\nmeasures the causal effect of an attribute on the reward. RATE uses LLMs to\nrewrite responses to produce imperfect counterfactuals examples that can be\nused to measure causal effects. A key challenge is that these rewrites are\nimperfect in a manner that can induce substantial bias in the estimated\nsensitivity of the reward model to the attribute. The core idea of RATE is to\nadjust for this imperfect-rewrite effect by rewriting twice. We establish the\nvalidity of the RATE procedure and show empirically that it is an effective\nestimator."}
{"id": "2410.12924", "pdf": "https://arxiv.org/pdf/2410.12924.pdf", "abs": "https://arxiv.org/abs/2410.12924", "title": "Interpreting token compositionality in LLMs: A robustness analysis", "authors": ["Nura Aljaafari", "Danilo S. Carvalho", "André Freitas"], "categories": ["cs.CL"], "comment": "23 pages, 3 Figures, 14 tables", "summary": "Understanding the internal mechanisms of large language models (LLMs) is\nintegral to enhancing their reliability, interpretability, and inference\nprocesses. We present Constituent-Aware Pooling (CAP), a methodology designed\nto analyse how LLMs process compositional linguistic structures. Grounded in\nprinciples of compositionality, mechanistic interpretability, and information\ntheory, CAP systematically intervenes in model activations through\nconstituent-based pooling at various model levels. Our experiments on inverse\ndefinition modelling, hypernym and synonym prediction reveal critical insights\ninto transformers' limitations in handling compositional abstractions. No\nspecific layer integrates tokens into unified semantic representations based on\ntheir constituent parts. We observe fragmented information processing, which\nintensifies with model size, suggesting that larger models struggle more with\nthese interventions and exhibit greater information dispersion. This\nfragmentation likely stems from transformers' training objectives and\narchitectural design, preventing systematic and cohesive representations. Our\nfindings highlight fundamental limitations in current transformer architectures\nregarding compositional semantics processing and model interpretability,\nunderscoring the critical need for novel approaches in LLM design to address\nthese challenges."}
{"id": "2410.13779", "pdf": "https://arxiv.org/pdf/2410.13779.pdf", "abs": "https://arxiv.org/abs/2410.13779", "title": "The Mystery of the Pathological Path-star Task for Language Models", "authors": ["Arvid Frydenlund"], "categories": ["cs.CL", "cs.LG"], "comment": "EMNLP 2024 Main at https://aclanthology.org/2024.emnlp-main.695/ See\n  'Language Models, Graph Searching, and Supervision Adulteration: When More\n  Supervision is Less and How to Make More More' for a follow-up work", "summary": "The recently introduced path-star task is a minimal task designed to\nexemplify limitations to the abilities of language models (Bachmann and\nNagarajan, 2024). It involves a path-star graph where multiple arms radiate\nfrom a single starting node and each node is unique. Given the start node and a\nspecified target node that ends an arm, the task is to generate the arm\ncontaining that target node. This is straightforward for a human but\nsurprisingly difficult for language models, which did not outperform the random\nbaseline. The authors hypothesized this is due to a deficiency in\nteacher-forcing and the next-token prediction paradigm.\n  We demonstrate the task is learnable using teacher-forcing in alternative\nsettings and that the issue is partially due to representation. We introduce a\nregularization method using structured samples of the same graph but with\ndiffering target nodes, improving results across a variety of model types. We\nprovide RASP proofs showing the task is theoretically solvable. Finally, we\nfind settings where an encoder-only model can consistently solve the task."}
{"id": "2410.14425", "pdf": "https://arxiv.org/pdf/2410.14425.pdf", "abs": "https://arxiv.org/abs/2410.14425", "title": "Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge Distillation", "authors": ["Shuai Zhao", "Xiaobao Wu", "Cong-Duy Nguyen", "Yanhao Jia", "Meihuizi Jia", "Yichao Feng", "Luu Anh Tuan"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) can bridge the gap between large\nlanguage models (LLMs) and downstream tasks. However, PEFT has been proven\nvulnerable to malicious attacks. Research indicates that poisoned LLMs, even\nafter PEFT, retain the capability to activate internalized backdoors when input\nsamples contain predefined triggers. In this paper, we introduce a novel\nweak-to-strong unlearning algorithm to defend against backdoor attacks based on\nfeature alignment knowledge distillation, named W2SDefense. Specifically, we\nfirst train a small-scale language model through full-parameter fine-tuning to\nserve as the clean teacher model. Then, this teacher model guides the\nlarge-scale poisoned student model in unlearning the backdoor, leveraging PEFT.\nTheoretical analysis suggests that W2SDefense has the potential to enhance the\nstudent model's ability to unlearn backdoor features, preventing the activation\nof the backdoor. We conduct comprehensive experiments on three state-of-the-art\nlarge language models and several different backdoor attack algorithms. Our\nempirical results demonstrate the outstanding performance of W2SDefense in\ndefending against backdoor attacks without compromising model performance."}
{"id": "2410.15522", "pdf": "https://arxiv.org/pdf/2410.15522.pdf", "abs": "https://arxiv.org/abs/2410.15522", "title": "M-RewardBench: Evaluating Reward Models in Multilingual Settings", "authors": ["Srishti Gureja", "Lester James V. Miranda", "Shayekh Bin Islam", "Rishabh Maheshwary", "Drishti Sharma", "Gusti Winata", "Nathan Lambert", "Sebastian Ruder", "Sara Hooker", "Marzieh Fadaee"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "16 pages, 6 figures, 10 tables. Website:\n  https://m-rewardbench.github.io/ , Updated results with latest models. Added\n  more author information", "summary": "Reward models (RMs) have driven the state-of-the-art performance of LLMs\ntoday by enabling the integration of human feedback into the language modeling\nprocess. However, RMs are primarily trained and evaluated in English, and their\ncapabilities in multilingual settings remain largely understudied. In this\nwork, we conduct a systematic evaluation of several reward models in\nmultilingual settings. We first construct the first-of-its-kind multilingual RM\nevaluation benchmark, M-RewardBench, consisting of 2.87k preference instances\nfor 23 typologically diverse languages, that tests the chat, safety, reasoning,\nand translation capabilities of RMs. We then rigorously evaluate a wide range\nof reward models on M-RewardBench, offering fresh insights into their\nperformance across diverse languages. We identify a significant gap in RMs'\nperformances between English and non-English languages and show that RM\npreferences can change substantially from one language to another. We also\npresent several findings on how different multilingual aspects impact RM\nperformance. Specifically, we show that the performance of RMs is improved with\nimproved translation quality. Similarly, we demonstrate that the models exhibit\nbetter performance for high-resource languages. We release M-RewardBench\ndataset and the codebase in this study to facilitate a better understanding of\nRM evaluation in multilingual settings."}
{"id": "2410.21272", "pdf": "https://arxiv.org/pdf/2410.21272.pdf", "abs": "https://arxiv.org/abs/2410.21272", "title": "Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics", "authors": ["Yaniv Nikankin", "Anja Reusch", "Aaron Mueller", "Yonatan Belinkov"], "categories": ["cs.CL", "68T5", "I.2.7"], "comment": null, "summary": "Do large language models (LLMs) solve reasoning tasks by learning robust\ngeneralizable algorithms, or do they memorize training data? To investigate\nthis question, we use arithmetic reasoning as a representative task. Using\ncausal analysis, we identify a subset of the model (a circuit) that explains\nmost of the model's behavior for basic arithmetic logic and examine its\nfunctionality. By zooming in on the level of individual circuit neurons, we\ndiscover a sparse set of important neurons that implement simple heuristics.\nEach heuristic identifies a numerical input pattern and outputs corresponding\nanswers. We hypothesize that the combination of these heuristic neurons is the\nmechanism used to produce correct arithmetic answers. To test this, we\ncategorize each neuron into several heuristic types-such as neurons that\nactivate when an operand falls within a certain range-and find that the\nunordered combination of these heuristic types is the mechanism that explains\nmost of the model's accuracy on arithmetic prompts. Finally, we demonstrate\nthat this mechanism appears as the main source of arithmetic accuracy early in\ntraining. Overall, our experimental results across several LLMs show that LLMs\nperform arithmetic using neither robust algorithms nor memorization; rather,\nthey rely on a \"bag of heuristics\"."}
{"id": "2411.02448", "pdf": "https://arxiv.org/pdf/2411.02448.pdf", "abs": "https://arxiv.org/abs/2411.02448", "title": "Rate, Explain and Cite (REC): Enhanced Explanation and Attribution in Automatic Evaluation by Large Language Models", "authors": ["Aliyah R. Hsu", "James Zhu", "Zhichao Wang", "Bin Bi", "Shubham Mehrotra", "Shiva K. Pentyala", "Katherine Tan", "Xiang-Bo Mao", "Roshanak Omrani", "Sougata Chaudhuri", "Regunathan Radhakrishnan", "Sitaram Asur", "Claire Na Cheng", "Bin Yu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs have demonstrated impressive proficiency in generating coherent and\nhigh-quality text, making them valuable across a range of text-generation\ntasks. However, rigorous evaluation of this generated content is crucial, as\nensuring its quality remains a significant challenge due to persistent issues\nsuch as factual inaccuracies and hallucination. This paper introduces three\nfine-tuned general-purpose LLM autoevaluators, REC-8B, REC-12B and REC-70B,\nspecifically designed to evaluate generated text across several dimensions:\nfaithfulness, instruction following, coherence, and completeness. These models\nnot only provide ratings for these metrics but also offer detailed explanation\nand verifiable citation, thereby enhancing trust in the content. Moreover, the\nmodels support various citation modes, accommodating different requirements for\nlatency and granularity. Extensive evaluations on diverse benchmarks\ndemonstrate that our general-purpose LLM auto-evaluator, REC-70B, outperforms\nstate-of-the-art LLMs, excelling in content evaluation by delivering better\nquality explanation and citation with minimal bias. Our REC dataset and models\nare available at https://github.com/adelaidehsu/REC."}
{"id": "2411.14318", "pdf": "https://arxiv.org/pdf/2411.14318.pdf", "abs": "https://arxiv.org/abs/2411.14318", "title": "Velocitune: A Velocity-based Dynamic Domain Reweighting Method for Continual Pre-training", "authors": ["Zheheng Luo", "Xin Zhang", "Xiao Liu", "Haoling Li", "Yeyun Gong", "Chen Qi", "Peng Cheng"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "It is well-known that a diverse corpus is critical for training large\nlanguage models, which are typically constructed from a mixture of various\ndomains. In general, previous efforts resort to sampling training data from\ndifferent domains with static proportions, as well as adjusting data\nproportions during training. However, few methods have addressed the\ncomplexities of domain-adaptive continual pre-training. To fill this gap, we\npropose Velocitune, a novel framework dynamically assesses learning velocity\nand adjusts data proportions accordingly, favoring slower-learning domains\nwhile shunning faster-learning ones, which is guided by a scaling law to\nindicate the desired learning goal for each domain with less associated cost.\nTo evaluate the effectiveness of Velocitune, we conduct experiments in a\nreasoning-focused dataset with CodeLlama, as well as in a corpus specialised\nfor system command generation with Llama3 and Mistral. Velocitune achieves\nperformance gains in both math and code reasoning tasks and command-line\ngeneration benchmarks. Further analysis reveals that key factors driving\nVelocitune's effectiveness include target loss prediction and data ordering."}
{"id": "2411.17388", "pdf": "https://arxiv.org/pdf/2411.17388.pdf", "abs": "https://arxiv.org/abs/2411.17388", "title": "Can LLMs be Good Graph Judge for Knowledge Graph Construction?", "authors": ["Haoyu Huang", "Chong Chen", "Zeang Sheng", "Yang Li", "Wentao Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In real-world scenarios, most of the data obtained from the information\nretrieval (IR) system is unstructured. Converting natural language sentences\ninto structured Knowledge Graphs (KGs) remains a critical challenge. We\nidentified three limitations with respect to existing KG construction methods:\n(1) There could be a large amount of noise in real-world documents, which could\nresult in extracting messy information. (2) Naive LLMs usually extract\ninaccurate knowledge from some domain-specific documents. (3) Hallucination\nphenomenon cannot be overlooked when directly using LLMs to construct KGs. In\nthis paper, we propose \\textbf{GraphJudge}, a KG construction framework to\naddress the aforementioned challenges. In this framework, we designed an\nentity-centric strategy to eliminate the noise information in the documents.\nAnd we fine-tuned a LLM as a graph judge to finally enhance the quality of\ngenerated KGs. Experiments conducted on two general and one domain-specific\ntext-graph pair datasets demonstrate state-of-the-art performance against\nvarious baseline methods with strong generalization abilities. Our code is\navailable at\n\\href{https://github.com/hhy-huang/GraphJudge}{https://github.com/hhy-huang/GraphJudge}."}
{"id": "2412.06245", "pdf": "https://arxiv.org/pdf/2412.06245.pdf", "abs": "https://arxiv.org/abs/2412.06245", "title": "A Comparative Study of Learning Paradigms in Large Language Models via Intrinsic Dimension", "authors": ["Saahith Janapati", "Yangfeng Ji"], "categories": ["cs.CL"], "comment": null, "summary": "The performance of Large Language Models (LLMs) on natural language tasks can\nbe improved through both supervised fine-tuning (SFT) and in-context learning\n(ICL), which operate via distinct mechanisms. Supervised fine-tuning updates\nthe model's weights by minimizing loss on training data, whereas in-context\nlearning leverages task demonstrations embedded in the prompt, without changing\nthe model's parameters. This study investigates the effects of these learning\nparadigms on the hidden representations of LLMs using Intrinsic Dimension (ID).\nWe use ID to estimate the number of degrees of freedom between representations\nextracted from LLMs as they perform specific natural language tasks. We first\nexplore how the ID of LLM representations evolves during SFT and how it varies\ndue to the number of demonstrations in ICL. We then compare the IDs induced by\nSFT and ICL and find that ICL consistently induces a higher ID compared to SFT,\nsuggesting that representations generated during ICL reside in higher\ndimensional manifolds in the embedding space."}
{"id": "2412.11936", "pdf": "https://arxiv.org/pdf/2412.11936.pdf", "abs": "https://arxiv.org/abs/2412.11936", "title": "A Survey of Mathematical Reasoning in the Era of Multimodal Large Language Model: Benchmark, Method & Challenges", "authors": ["Yibo Yan", "Jiamin Su", "Jianxiang He", "Fangteng Fu", "Xu Zheng", "Yuanhuiyi Lyu", "Kun Wang", "Shen Wang", "Qingsong Wen", "Xuming Hu"], "categories": ["cs.CL"], "comment": "Accepted by The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL Findings 2025)", "summary": "Mathematical reasoning, a core aspect of human cognition, is vital across\nmany domains, from educational problem-solving to scientific advancements. As\nartificial general intelligence (AGI) progresses, integrating large language\nmodels (LLMs) with mathematical reasoning tasks is becoming increasingly\nsignificant. This survey provides the first comprehensive analysis of\nmathematical reasoning in the era of multimodal large language models (MLLMs).\nWe review over 200 studies published since 2021, and examine the\nstate-of-the-art developments in Math-LLMs, with a focus on multimodal\nsettings. We categorize the field into three dimensions: benchmarks,\nmethodologies, and challenges. In particular, we explore multimodal\nmathematical reasoning pipeline, as well as the role of (M)LLMs and the\nassociated methodologies. Finally, we identify five major challenges hindering\nthe realization of AGI in this domain, offering insights into the future\ndirection for enhancing multimodal reasoning capabilities. This survey serves\nas a critical resource for the research community in advancing the capabilities\nof LLMs to tackle complex multimodal reasoning tasks."}
{"id": "2412.14161", "pdf": "https://arxiv.org/pdf/2412.14161.pdf", "abs": "https://arxiv.org/abs/2412.14161", "title": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks", "authors": ["Frank F. Xu", "Yufan Song", "Boxuan Li", "Yuxuan Tang", "Kritanjali Jain", "Mengxue Bao", "Zora Z. Wang", "Xuhui Zhou", "Zhitong Guo", "Murong Cao", "Mingyang Yang", "Hao Yang Lu", "Amaad Martin", "Zhe Su", "Leander Maben", "Raj Mehta", "Wayne Chi", "Lawrence Jang", "Yiqing Xie", "Shuyan Zhou", "Graham Neubig"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "We interact with computers on an everyday basis, be it in everyday life or\nwork, and many aspects of work can be done entirely with access to a computer\nand the Internet. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. But how\nperformant are AI agents at accelerating or even autonomously performing\nwork-related tasks? The answer to this question has important implications both\nfor industry looking to adopt AI into their workflows and for economic policy\nto understand the effects that adoption of AI may have on the labor market. To\nmeasure the progress of these LLM agents' performance on performing real-world\nprofessional tasks, in this paper we introduce TheAgentCompany, an extensible\nbenchmark for evaluating AI agents that interact with the world in similar ways\nto those of a digital worker: by browsing the Web, writing code, running\nprograms, and communicating with other coworkers. We build a self-contained\nenvironment with internal web sites and data that mimics a small software\ncompany environment, and create a variety of tasks that may be performed by\nworkers in such a company. We test baseline agents powered by both closed\nAPI-based and open-weights language models (LMs), and find that the most\ncompetitive agent can complete 30% of tasks autonomously. This paints a nuanced\npicture on task automation with LM agents--in a setting simulating a real\nworkplace, a good portion of simpler tasks could be solved autonomously, but\nmore difficult long-horizon tasks are still beyond the reach of current\nsystems. We release code, data, environment, and experiments on\nhttps://the-agent-company.com."}
{"id": "2412.14470", "pdf": "https://arxiv.org/pdf/2412.14470.pdf", "abs": "https://arxiv.org/abs/2412.14470", "title": "Agent-SafetyBench: Evaluating the Safety of LLM Agents", "authors": ["Zhexin Zhang", "Shiyao Cui", "Yida Lu", "Jingzhuo Zhou", "Junxiao Yang", "Hongning Wang", "Minlie Huang"], "categories": ["cs.CL"], "comment": "26 pages", "summary": "As large language models (LLMs) are increasingly deployed as agents, their\nintegration into interactive environments and tool use introduce new safety\nchallenges beyond those associated with the models themselves. However, the\nabsence of comprehensive benchmarks for evaluating agent safety presents a\nsignificant barrier to effective assessment and further improvement. In this\npaper, we introduce Agent-SafetyBench, a comprehensive benchmark designed to\nevaluate the safety of LLM agents. Agent-SafetyBench encompasses 349\ninteraction environments and 2,000 test cases, evaluating 8 categories of\nsafety risks and covering 10 common failure modes frequently encountered in\nunsafe interactions. Our evaluation of 16 popular LLM agents reveals a\nconcerning result: none of the agents achieves a safety score above 60%. This\nhighlights significant safety challenges in LLM agents and underscores the\nconsiderable need for improvement. Through failure mode and helpfulness\nanalysis, we summarize two fundamental safety defects in current LLM agents:\nlack of robustness and lack of risk awareness. Furthermore, our findings\nsuggest that reliance on defense prompts alone may be insufficient to address\nthese safety issues, emphasizing the need for more advanced and robust\nstrategies. To drive progress in this area, Agent-SafetyBench has been released\nat https://github.com/thu-coai/Agent-SafetyBench/ to facilitate further\nresearch in agent safety evaluation and improvement."}
{"id": "2412.16783", "pdf": "https://arxiv.org/pdf/2412.16783.pdf", "abs": "https://arxiv.org/abs/2412.16783", "title": "SubData: Bridging Heterogeneous Datasets to Enable Theory-Driven Evaluation of Political and Demographic Perspectives in LLMs", "authors": ["Leon Fröhling", "Pietro Bernardelle", "Gianluca Demartini"], "categories": ["cs.CL"], "comment": "11 pages, 2 figures", "summary": "As increasingly capable large language models (LLMs) emerge, researchers have\nbegun exploring their potential for subjective tasks. While recent work\ndemonstrates that LLMs can be aligned with diverse human perspectives,\nevaluating this alignment on actual downstream tasks (e.g., hate speech\ndetection) remains challenging due to the use of inconsistent datasets across\nstudies. To address this issue, in this resource paper we propose a two-step\nframework: we (1) introduce SubData, an open-source Python library designed for\nstandardizing heterogeneous datasets to evaluate LLM perspective alignment; and\n(2) present a theory-driven approach leveraging this library to test how\ndifferently-aligned LLMs (e.g., aligned with different political viewpoints)\nclassify content targeting specific demographics. SubData's flexible mapping\nand taxonomy enable customization for diverse research needs, distinguishing it\nfrom existing resources. We invite contributions to add datasets to our\ninitially proposed resource and thereby help expand SubData into a\nmulti-construct benchmark suite for evaluating LLM perspective alignment on NLP\ntasks."}
{"id": "2501.02009", "pdf": "https://arxiv.org/pdf/2501.02009.pdf", "abs": "https://arxiv.org/abs/2501.02009", "title": "Cross-model Transferability among Large Language Models on the Platonic Representations of Concepts", "authors": ["Youcheng Huang", "Chen Huang", "Duanyu Feng", "Wenqiang Lei", "Jiancheng Lv"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main Camera Ready", "summary": "Understanding the inner workings of Large Language Models (LLMs) is a\ncritical research frontier. Prior research has shown that a single LLM's\nconcept representations can be captured as steering vectors (SVs), enabling the\ncontrol of LLM behavior (e.g., towards generating harmful content). Our work\ntakes a novel approach by exploring the intricate relationships between concept\nrepresentations across different LLMs, drawing an intriguing parallel to\nPlato's Allegory of the Cave. In particular, we introduce a linear\ntransformation method to bridge these representations and present three key\nfindings: 1) Concept representations across different LLMs can be effectively\naligned using simple linear transformations, enabling efficient cross-model\ntransfer and behavioral control via SVs. 2) This linear transformation\ngeneralizes across concepts, facilitating alignment and control of SVs\nrepresenting different concepts across LLMs. 3) A weak-to-strong\ntransferability exists between LLM concept representations, whereby SVs\nextracted from smaller LLMs can effectively control the behavior of larger\nLLMs."}
{"id": "2501.02506", "pdf": "https://arxiv.org/pdf/2501.02506.pdf", "abs": "https://arxiv.org/abs/2501.02506", "title": "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use", "authors": ["Junjie Ye", "Zhengyin Du", "Xuesong Yao", "Weijian Lin", "Yufei Xu", "Zehui Chen", "Zaiyuan Wang", "Sining Zhu", "Zhiheng Xi", "Siyu Yuan", "Tao Gui", "Qi Zhang", "Xuanjing Huang", "Jiecao Chen"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Effective evaluation of multi-hop tool use is critical for analyzing the\nunderstanding, reasoning, and function-calling capabilities of large language\nmodels (LLMs). However, progress has been hindered by a lack of reliable\nevaluation datasets. To address this, we present ToolHop, a dataset comprising\n995 user queries and 3,912 associated tools, specifically designed for rigorous\nevaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful\ninterdependencies, locally executable tools, detailed feedback, and verifiable\nanswers through a novel query-driven data construction approach that includes\ntool creation, document refinement, and code generation. We evaluate 14 LLMs\nacross five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and\nGPT), uncovering significant challenges in handling multi-hop tool-use\nscenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%,\nunderscoring substantial room for improvement. Further analysis reveals\nvariations in tool-use strategies for various families, offering actionable\ninsights to guide the development of more effective approaches. Code and data\ncan be found in https://huggingface.co/datasets/bytedance-research/ToolHop."}
{"id": "2501.06582", "pdf": "https://arxiv.org/pdf/2501.06582.pdf", "abs": "https://arxiv.org/abs/2501.06582", "title": "ACORD: An Expert-Annotated Retrieval Dataset for Legal Contract Drafting", "authors": ["Steven H. Wang", "Maksim Zubkov", "Kexin Fan", "Sarah Harrell", "Yuyang Sun", "Wei Chen", "Andreas Plesner", "Roger Wattenhofer"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025. See the project page at\n  https://www.atticusprojectai.org/acord", "summary": "Information retrieval, specifically contract clause retrieval, is\nfoundational to contract drafting because lawyers rarely draft contracts from\nscratch; instead, they locate and revise the most relevant precedent. We\nintroduce the Atticus Clause Retrieval Dataset (ACORD), the first retrieval\nbenchmark for contract drafting fully annotated by experts. ACORD focuses on\ncomplex contract clauses such as Limitation of Liability, Indemnification,\nChange of Control, and Most Favored Nation. It includes 114 queries and over\n126,000 query-clause pairs, each ranked on a scale from 1 to 5 stars. The task\nis to find the most relevant precedent clauses to a query. The bi-encoder\nretriever paired with pointwise LLMs re-rankers shows promising results.\nHowever, substantial improvements are still needed to effectively manage the\ncomplex legal work typically undertaken by lawyers. As the first retrieval\nbenchmark for contract drafting annotated by experts, ACORD can serve as a\nvaluable IR benchmark for the NLP community."}
{"id": "2501.07482", "pdf": "https://arxiv.org/pdf/2501.07482.pdf", "abs": "https://arxiv.org/abs/2501.07482", "title": "TiEBe: Tracking Language Model Recall of Notable Worldwide Events Through Time", "authors": ["Thales Sales Almeida", "Giovana Kerche Bonás", "João Guilherme Alves Santos", "Hugo Abonizio", "Rodrigo Nogueira"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As the knowledge landscape evolves and large language models (LLMs) become\nincreasingly widespread, there is a growing need to keep these models updated\nwith current events. While existing benchmarks assess general factual recall,\nfew studies explore how LLMs retain knowledge over time or across different\nregions. To address these gaps, we present the Timely Events Benchmark (TiEBe),\na dataset of over 23,000 question-answer pairs centered on notable global and\nregional events, spanning more than 10 years of events, 23 regions, and 13\nlanguages. TiEBe leverages structured retrospective data from Wikipedia to\nidentify notable events through time. These events are then used to construct a\nbenchmark to evaluate LLMs' understanding of global and regional developments,\ngrounded in factual evidence beyond Wikipedia itself. Our results reveal\nsignificant geographic disparities in factual recall, emphasizing the need for\nmore balanced global representation in LLM training. We also observe a Pearson\ncorrelation of more than 0.7 between models' performance in TiEBe and various\ncountries' socioeconomic indicators, such as HDI. In addition, we examine the\nimpact of language on factual recall by posing questions in the native language\nof the region where each event occurred, uncovering substantial performance\ngaps for low-resource languages."}
{"id": "2501.14315", "pdf": "https://arxiv.org/pdf/2501.14315.pdf", "abs": "https://arxiv.org/abs/2501.14315", "title": "Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning", "authors": ["Chao-Chung Wu", "Zhi Rui Tam", "Chieh-Yen Lin", "Yun-Nung Chen", "Shao-Hua Sun", "Hung-yi Lee"], "categories": ["cs.CL"], "comment": null, "summary": "Maintaining consistent model performance across domains is a fundamental\nchallenge in machine learning. While recent work has explored using\nLLM-generated data for fine-tuning, its impact on cross-domain generalization\nremains poorly understood. This paper presents a systematic analysis revealing\nthat fine-tuning with LLM-generated data not only improves target task\nperformance but also reduces non-target task degradation compared to\nfine-tuning with ground truth data. Through analyzing the data sequence in\ntasks of various domains, we demonstrate that this enhancement of non-target\ntask robustness stems from the reduction of high perplexity tokens found in\nLLM-generated sequences. Following our findings, we showed that masking high\nperplexity tokens in ground truth training data achieves similar non-target\ntask performance preservation, comparable to using LLM-generated data.\nExtensive experiments across different model families and scales, including\nGemma 2 IT 2B, Llama 3 8B Instruct, and 3 additional models, agree with our\nfindings. To the best of our knowledge, this is the first work to provide an\nempirical explanation based on token perplexity reduction to mitigate\ncatastrophic forgetting in LLMs after fine-tuning, offering valuable insights\nfor developing more robust fine-tuning strategies."}
{"id": "2501.15451", "pdf": "https://arxiv.org/pdf/2501.15451.pdf", "abs": "https://arxiv.org/abs/2501.15451", "title": "STATE ToxiCN: A Benchmark for Span-level Target-Aware Toxicity Extraction in Chinese Hate Speech Detection", "authors": ["Zewen Bai", "Shengdi Yin", "Junyu Lu", "Jingjie Zeng", "Haohao Zhu", "Yuanyuan Sun", "Liang Yang", "Hongfei Lin"], "categories": ["cs.CL"], "comment": "Our paper has been accepted by ACL 2025 Findings", "summary": "The proliferation of hate speech has caused significant harm to society. The\nintensity and directionality of hate are closely tied to the target and\nargument it is associated with. However, research on hate speech detection in\nChinese has lagged behind, and existing datasets lack span-level fine-grained\nannotations. Furthermore, the lack of research on Chinese hateful slang poses a\nsignificant challenge. In this paper, we provide a solution for fine-grained\ndetection of Chinese hate speech. First, we construct a dataset containing\nTarget-Argument-Hateful-Group quadruples (STATE ToxiCN), which is the first\nspan-level Chinese hate speech dataset. Secondly, we evaluate the span-level\nhate speech detection performance of existing models using STATE ToxiCN.\nFinally, we conduct the first study on Chinese hateful slang and evaluate the\nability of LLMs to detect such expressions. Our work contributes valuable\nresources and insights to advance span-level hate speech detection in Chinese."}
{"id": "2501.15654", "pdf": "https://arxiv.org/pdf/2501.15654.pdf", "abs": "https://arxiv.org/abs/2501.15654", "title": "People who frequently use ChatGPT for writing tasks are accurate and robust detectors of AI-generated text", "authors": ["Jenna Russell", "Marzena Karpinska", "Mohit Iyyer"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 33 pages", "summary": "In this paper, we study how well humans can detect text generated by\ncommercial LLMs (GPT-4o, Claude, o1). We hire annotators to read 300\nnon-fiction English articles, label them as either human-written or\nAI-generated, and provide paragraph-length explanations for their decisions.\nOur experiments show that annotators who frequently use LLMs for writing tasks\nexcel at detecting AI-generated text, even without any specialized training or\nfeedback. In fact, the majority vote among five such \"expert\" annotators\nmisclassifies only 1 of 300 articles, significantly outperforming most\ncommercial and open-source detectors we evaluated even in the presence of\nevasion tactics like paraphrasing and humanization. Qualitative analysis of the\nexperts' free-form explanations shows that while they rely heavily on specific\nlexical clues ('AI vocabulary'), they also pick up on more complex phenomena\nwithin the text (e.g., formality, originality, clarity) that are challenging to\nassess for automatic detectors. We release our annotated dataset and code to\nspur future research into both human and automated detection of AI-generated\ntext."}
{"id": "2501.19202", "pdf": "https://arxiv.org/pdf/2501.19202.pdf", "abs": "https://arxiv.org/abs/2501.19202", "title": "Improving LLM Unlearning Robustness via Random Perturbations", "authors": ["Dang Huu-Tien", "Hoang Thanh-Tung", "Anh Bui", "Le-Minh Nguyen", "Naoya Inoue"], "categories": ["cs.CL"], "comment": "23 pages, 10 figures, 5 tables", "summary": "In this paper, we show that current state-of-the-art LLM unlearning methods\ninherently reduce models' robustness, causing them to misbehave even when a\nsingle non-adversarial forget-token is in the retain-query. Toward\nunderstanding underlying causes, we reframe the unlearning process as backdoor\nattacks and defenses: forget-tokens act as backdoor triggers that, when\nactivated in retain-queries, cause disruptions in unlearned models' behaviors,\nsimilar to successful backdoor attacks. To mitigate this vulnerability, we\npropose Random Noise Augmentation (RNA) -- a plug-and-play, model and method\nagnostic approach with theoretical guarantees for improving the robustness of\nunlearned models. Extensive experiments demonstrate that RNA significantly\nimproves the robustness of unlearned models, maintains unlearning performances\nwhile introducing no additional computational overhead."}
{"id": "2502.02095", "pdf": "https://arxiv.org/pdf/2502.02095.pdf", "abs": "https://arxiv.org/abs/2502.02095", "title": "LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information", "authors": ["Bowen Ping", "Jiali Zeng", "Fandong Meng", "Shuo Wang", "Jie Zhou", "Shanghang Zhang"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Long-form generation is crucial for academic writing papers and repo-level\ncode generation. Despite this, current models, including GPT-4o, still exhibit\nunsatisfactory performance. Existing methods that utilize preference learning\nwith outcome supervision often fail to provide detailed feedback for extended\ncontexts. This shortcoming can lead to content that does not fully satisfy\nquery requirements, resulting in issues like length deviations, and diminished\nquality. In this paper, we propose enhancing long-form generation by\nincorporating process supervision. We employ Monte Carlo Tree Search to gather\nstepwise preference pairs, utilizing a global memory pool to maintain\nconsistency. To address the issue of suboptimal candidate selection, we\nintegrate external critiques to refine and improve the quality of the\npreference pairs. Finally, we apply step-level DPO using the collected stepwise\npreference pairs. Experimental results show that our method improves length and\nquality on long-form generation benchmarks, with almost lossless performance on\ngeneral benchmarks across various model backbones."}
{"id": "2502.02362", "pdf": "https://arxiv.org/pdf/2502.02362.pdf", "abs": "https://arxiv.org/abs/2502.02362", "title": "Premise-Augmented Reasoning Chains Improve Error Identification in Math reasoning with LLMs", "authors": ["Sagnik Mukherjee", "Abhinav Chinta", "Takyoung Kim", "Tarun Anoop Sharma", "Dilek Hakkani-Tür"], "categories": ["cs.CL"], "comment": "Accepted at ICML 2025", "summary": "Chain-of-Thought (CoT) prompting enhances mathematical reasoning in large\nlanguage models (LLMs) by enabling detailed step-by-step solutions. However,\ndue to the verbosity of LLMs, the resulting reasoning chains can be long,\nmaking it harder to verify the reasoning steps and trace issues resulting from\ndependencies between the steps that may be farther away in the sequence of\nsteps. Importantly, mathematical reasoning allows each step to be derived from\na small set of premises, which are a subset of the preceding steps in the\nreasoning chain. In this paper, we present a framework that identifies the\npremises for each step, to improve the evaluation of reasoning. We restructure\nconventional linear reasoning chains into Premise Augmented Reasoning Chains\n(PARC) by introducing premise links, resulting in a directed acyclic graph\nwhere the nodes are the steps and the edges are the premise links. Through\nexperiments with a PARC-based dataset that we built, namely PERL (Premises and\nERrors identification in LLMs), we demonstrate that LLMs can reliably identify\npremises within complex reasoning chains. In particular, even open-source LLMs\nachieve 90% recall in premise identification. We also show that PARC helps to\nidentify errors in reasoning chains more reliably. The accuracy of error\nidentification improves by 6% to 16% absolute when step-by-step verification is\ncarried out in PARC under the premises. Our findings highlight the utility of\npremise-centric representations in addressing complex problem-solving tasks and\nopen new avenues for improving the reliability of LLM-based reasoning\nevaluations."}
{"id": "2502.02577", "pdf": "https://arxiv.org/pdf/2502.02577.pdf", "abs": "https://arxiv.org/abs/2502.02577", "title": "A comparison of translation performance between DeepL and Supertext", "authors": ["Alex Flückiger", "Chantal Amrhein", "Tim Graf", "Frédéric Odermatt", "Martin Pömsl", "Philippe Schläpfer", "Florian Schottmann", "Samuel Läubli"], "categories": ["cs.CL"], "comment": "Paper accepted at MT Summit 2025", "summary": "As strong machine translation (MT) systems are increasingly based on large\nlanguage models (LLMs), reliable quality benchmarking requires methods that\ncapture their ability to leverage extended context. This study compares two\ncommercial MT systems -- DeepL and Supertext -- by assessing their performance\non unsegmented texts. We evaluate translation quality across four language\ndirections with professional translators assessing segments with full\ndocument-level context. While segment-level assessments indicate no strong\npreference between the systems in most cases, document-level analysis reveals a\npreference for Supertext in three out of four language directions, suggesting\nsuperior consistency across longer texts. We advocate for more\ncontext-sensitive evaluation methodologies to ensure that MT quality\nassessments reflect real-world usability. We release all evaluation data and\nscripts for further analysis and reproduction at\nhttps://github.com/supertext/evaluation_deepl_supertext."}
{"id": "2502.02789", "pdf": "https://arxiv.org/pdf/2502.02789.pdf", "abs": "https://arxiv.org/abs/2502.02789", "title": "Speculative Prefill: Turbocharging TTFT with Lightweight and Training-Free Token Importance Estimation", "authors": ["Jingyu Liu", "Beidi Chen", "Ce Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "Proceedings of the 42nd International Conference on Machine Learning\n  (ICML 2025)", "summary": "Improving time-to-first-token (TTFT) is an essentially important objective in\nmodern large language model (LLM) inference engines. Optimizing TTFT directly\nresults in higher maximal QPS and meets the requirements of many critical\napplications. However, boosting TTFT is notoriously challenging since it is\ncompute-bounded and the performance bottleneck shifts from the self-attention\nthat many prior works focus on to the MLP part. In this work, we present\nSpecPrefill, a training free framework that accelerates the inference TTFT for\nboth long and medium context queries based on the following insight: LLMs are\ngeneralized enough to preserve the quality given only a carefully chosen subset\nof prompt tokens. At its core, SpecPrefill leverages a lightweight model to\nspeculate locally important tokens based on the context. These tokens, along\nwith the necessary positional information, are then sent to the main model for\nprocessing. We evaluate SpecPrefill with a diverse set of tasks, followed by a\ncomprehensive benchmarking of performance improvement both in a real end-to-end\nsetting and ablation studies. SpecPrefill manages to serve\nLlama-3.1-405B-Instruct-FP8 with up to 7$\\times$ maximal end-to-end QPS on real\ndownstream tasks and 7.66$\\times$ TTFT improvement."}
{"id": "2502.06659", "pdf": "https://arxiv.org/pdf/2502.06659.pdf", "abs": "https://arxiv.org/abs/2502.06659", "title": "Who Taught You That? Tracing Teachers in Model Distillation", "authors": ["Somin Wadhwa", "Chantal Shaib", "Silvio Amir", "Byron C. Wallace"], "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "Model distillation -- using outputs from a large teacher model to teach a\nsmall student model -- is a practical means of creating efficient models for a\nparticular task. We ask: Can we identify a students' teacher based on its\noutputs? Such \"footprints\" left by teacher LLMs would be interesting artifacts.\nBeyond this, reliable teacher inference may have practical implications as\nactors seek to distill specific capabilities of massive proprietary LLMs into\ndeployed smaller LMs, potentially violating terms of service. We consider\npractical task distillation targets including summarization, question\nanswering, and instruction-following. We assume a finite set of candidate\nteacher models, which we treat as blackboxes. We design discriminative models\nthat operate over lexical features. We find that $n$-gram similarity alone is\nunreliable for identifying teachers, but part-of-speech (PoS) templates\npreferred by student models mimic those of their teachers."}
{"id": "2502.11051", "pdf": "https://arxiv.org/pdf/2502.11051.pdf", "abs": "https://arxiv.org/abs/2502.11051", "title": "MMUnlearner: Reformulating Multimodal Machine Unlearning in the Era of Multimodal Large Language Models", "authors": ["Jiahao Huo", "Yibo Yan", "Xu Zheng", "Yuanhuiyi Lyu", "Xin Zou", "Zhihua Wei", "Xuming Hu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted as ACL 2025 Findings", "summary": "Recent progress in Machine Unlearning (MU) has introduced solutions for the\nselective removal of private or sensitive information encoded within deep\nneural networks. Nonetheless, MU for Multimodal Large Language Models (MLLMs)\nremains in its nascent phase. Therefore, we propose to reformulate the task of\nmultimodal MU in the era of MLLMs, which aims to erase only the visual patterns\nassociated with a given entity while preserving the corresponding textual\nknowledge encoded within the original parameters of the language model\nbackbone. Furthermore, we develop a novel geometry-constrained gradient ascent\nmethod MMUnlearner. It updates the weights of MLLMs with a weight saliency map\njointly restricted by the remaining concepts and textual knowledge during\nunlearning, thereby preserving parameters essential for non-target knowledge.\nExtensive experiments demonstrate that MMUnlearner surpasses baselines that\nfinetuning MLLMs with VQA data directly through Gradient Ascent (GA) or\nNegative Preference Optimization (NPO), across all evaluation dimensions. Our\ncode will be released upon acceptance."}
{"id": "2502.11066", "pdf": "https://arxiv.org/pdf/2502.11066.pdf", "abs": "https://arxiv.org/abs/2502.11066", "title": "CARMA: Enhanced Compositionality in LLMs via Advanced Regularisation and Mutual Information Alignment", "authors": ["Nura Aljaafari", "Danilo S. Carvalho", "André Freitas"], "categories": ["cs.CL"], "comment": "19 pages, 8 figures, 8 tables", "summary": "Large language models (LLMs) struggle with compositional generalisation,\nlimiting their ability to systematically combine learned components to\ninterpret novel inputs. While architectural modifications, fine-tuning, and\ndata augmentation improve compositionality, they often have limited\nadaptability, face scalability constraints, or yield diminishing returns on\nreal data. To address this, we propose CARMA, an intervention that enhances the\nstability and robustness of compositional reasoning in LLMs while preserving\nfine-tuned performance. CARMA employs mutual information regularisation and\nlayer-wise stability constraints to mitigate feature fragmentation, ensuring\nstructured representations persist across and within layers. We evaluate CARMA\non inverse dictionary modelling and sentiment classification, measuring its\nimpact on semantic consistency, performance stability, and robustness to\nlexical perturbations. Results show that CARMA reduces the variability\nintroduced by fine-tuning, stabilises token representations, and improves\ncompositional reasoning. While its effectiveness varies across architectures,\nCARMA's key strength lies in reinforcing learned structures rather than\nintroducing new capabilities, making it a scalable auxiliary method. These\nfindings suggest that integrating CARMA with fine-tuning can improve\ncompositional generalisation while maintaining task-specific performance in\nLLMs."}
{"id": "2502.11100", "pdf": "https://arxiv.org/pdf/2502.11100.pdf", "abs": "https://arxiv.org/abs/2502.11100", "title": "Towards Achieving Concept Completeness for Textual Concept Bottleneck Models", "authors": ["Milan Bhan", "Yann Choho", "Pierre Moreau", "Jean-Noel Vittaut", "Nicolas Chesneau", "Marie-Jeanne Lesot"], "categories": ["cs.CL"], "comment": null, "summary": "Textual Concept Bottleneck Models (TBMs) are interpretable-by-design models\nfor text classification that predict a set of salient concepts before making\nthe final prediction. This paper proposes Complete Textual Concept Bottleneck\nModel (CT-CBM),a novel TCBM generator building concept labels in a fully\nunsupervised manner using a small language model, eliminating both the need for\npredefined human labeled concepts and LLM annotations. CT-CBM iteratively\ntargets and adds important concepts in the bottleneck layer to create a\ncomplete concept basis and addresses downstream classification leakage through\na parallel residual connection. CT-CBM achieves good results against\ncompetitors, offering a promising solution to enhance interpretability of NLP\nclassifiers without sacrificing performance."}
{"id": "2502.11733", "pdf": "https://arxiv.org/pdf/2502.11733.pdf", "abs": "https://arxiv.org/abs/2502.11733", "title": "Plant in Cupboard, Orange on Rably, Inat Aphone. Benchmarking Incremental Learning of Situation and Language Model using a Text-Simulated Situated Environment", "authors": ["Jonathan Jordan", "Sherzod Hakimov", "David Schlangen"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) serve not only as chatbots but as key components\nin agent systems, where their common-sense knowledge significantly impacts\nperformance as language-based planners for situated or embodied action. We\nassess LLMs' incremental learning (based on feedback from the environment), and\ncontrolled in-context learning abilities using a text-based environment. We\nintroduce challenging yet interesting set of experiments to test i) how agents\ncan incrementally solve tasks related to every day objects in typical rooms in\na house where each of them are discovered by interacting within the\nenvironment, ii) controlled in-context learning abilities and efficiency of\nagents by providing short info about locations of objects and rooms to check\nhow faster the task can be solved, and finally iii) using synthetic\npseudo-English words to gauge how well LLMs are at inferring meaning of unknown\nwords from environmental feedback. Results show that larger commercial models\nhave a substantial gap in performance compared to open-weight but almost all\nmodels struggle with the synthetic words experiments."}
{"id": "2502.11811", "pdf": "https://arxiv.org/pdf/2502.11811.pdf", "abs": "https://arxiv.org/abs/2502.11811", "title": "FineFilter: A Fine-grained Noise Filtering Mechanism for Retrieval-Augmented Large Language Models", "authors": ["Qianchi Zhang", "Hainan Zhang", "Liang Pang", "Ziwei Wang", "Hongwei Zheng", "Yongxin Tong", "Zhiming Zheng"], "categories": ["cs.CL"], "comment": "18 pages, 4 figures, 18 tables, under review", "summary": "Retrieved documents containing noise will hinder Retrieval-Augmented\nGeneration (RAG) from detecting answer clues, necessitating noise filtering\nmechanisms to enhance accuracy. Existing methods use reranking or summarization\nto identify the most relevant sentences, but directly and accurately locating\nanswer clues from these large-scale and complex documents remains challenging.\nUnlike these document-level operations, we treat noise filtering as a\nsentence-level MinMax optimization problem: first identifying potential clues\nfrom multiple documents, then ranking them by relevance, and finally retaining\nthe minimum number of clues through truncation. In this paper, we propose\nFineFilter, a novel fine-grained noise filtering mechanism for RAG, consisting\nof a clue extractor, a reranker, and a truncator. We optimize each module to\ntackle complex reasoning challenges: (1) The clue extractor first uses\nsentences containing the answer and similar ones as fine-tuning targets, aiming\nto extract sufficient potential clues; (2) The reranker is trained to\nprioritize effective clues based on the real feedback from the generation\nmodule, with clues capable of generating correct answers as positive samples\nand others as negative; (3) The truncator takes the minimum number of clues\nneeded to answer the question (truncation point) as fine-tuning targets, and\nperforms truncation on the reranked clues to achieve fine-grained noise\nfiltering. Experiments on three QA datasets demonstrate that FineFilter\nsignificantly improves QA performance over baselines on both LLaMA3 and\nMistral. Further analysis confirms its effectiveness in complex reasoning,\nrobustness to unreliable retrieval, and generalization to different scenarios."}
{"id": "2502.11916", "pdf": "https://arxiv.org/pdf/2502.11916.pdf", "abs": "https://arxiv.org/abs/2502.11916", "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models", "authors": ["Jiamin Su", "Yibo Yan", "Fangteng Fu", "Han Zhang", "Jingheng Ye", "Xiang Liu", "Jiahao Huo", "Huiyu Zhou", "Xuming Hu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL Findings 2025", "summary": "Automated Essay Scoring (AES) plays a crucial role in educational assessment\nby providing scalable and consistent evaluations of writing tasks. However,\ntraditional AES systems face three major challenges: (1) reliance on\nhandcrafted features that limit generalizability, (2) difficulty in capturing\nfine-grained traits like coherence and argumentation, and (3) inability to\nhandle multimodal contexts. In the era of Multimodal Large Language Models\n(MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES\ncapabilities across lexical-, sentence-, and discourse-level traits. By\nleveraging MLLMs' strengths in trait-specific scoring and multimodal context\nunderstanding, EssayJudge aims to offer precise, context-rich evaluations\nwithout manual feature engineering, addressing longstanding AES limitations.\nOur experiments with 18 representative MLLMs reveal gaps in AES performance\ncompared to human evaluation, particularly in discourse-level traits,\nhighlighting the need for further advancements in MLLM-based AES research."}
{"id": "2502.12464", "pdf": "https://arxiv.org/pdf/2502.12464.pdf", "abs": "https://arxiv.org/abs/2502.12464", "title": "SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models", "authors": ["Seanie Lee", "Dong Bok Lee", "Dominik Wagner", "Minki Kang", "Haebin Seong", "Tobias Bocklet", "Juho Lee", "Sung Ju Hwang"], "categories": ["cs.CL"], "comment": "ACL 2025 findings", "summary": "Deploying large language models (LLMs) in real-world applications requires\nrobust safety guard models to detect and block harmful user prompts. While\nlarge safety guard models achieve strong performance, their computational cost\nis substantial. To mitigate this, smaller distilled models are used, but they\noften underperform on \"hard\" examples where the larger model provides accurate\npredictions. We observe that many inputs can be reliably handled by the smaller\nmodel, while only a small fraction require the larger model's capacity.\nMotivated by this, we propose SafeRoute, a binary router that distinguishes\nhard examples from easy ones. Our method selectively applies the larger safety\nguard model to the data that the router considers hard, improving efficiency\nwhile maintaining accuracy compared to solely using the larger safety guard\nmodel. Experimental results on multiple benchmark datasets demonstrate that our\nadaptive model selection significantly enhances the trade-off between\ncomputational cost and safety performance, outperforming relevant baselines."}
{"id": "2502.12767", "pdf": "https://arxiv.org/pdf/2502.12767.pdf", "abs": "https://arxiv.org/abs/2502.12767", "title": "R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on Knowledge Graphs", "authors": ["Sumin Jo", "Junseong Choi", "Jiho Kim", "Edward Choi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent studies have combined Large Language Models (LLMs) with Knowledge\nGraphs (KGs) to enhance reasoning, improving inference accuracy without\nadditional training while mitigating hallucination. However, existing\nframeworks still suffer two practical drawbacks: they must be re-tuned whenever\nthe KG or reasoning task changes, and they depend on a single, high-capacity\nLLM for reliable (i.e., trustworthy) reasoning. To address this, we introduce\nR2-KG, a plug-and-play, dual-agent framework that separates reasoning into two\nroles: an Operator (a low-capacity LLM) that gathers evidence and a Supervisor\n(a high-capacity LLM) that makes final judgments. This design is cost-efficient\nfor LLM inference while still maintaining strong reasoning accuracy.\nAdditionally, R2-KG employs an Abstention mechanism, generating answers only\nwhen sufficient evidence is collected from KG, which significantly enhances\nreliability. Experiments across five diverse benchmarks show that R2-KG\nconsistently outperforms baselines in both accuracy and reliability, regardless\nof the inherent capability of LLMs used as the Operator. Further experiments\nreveal that the single-agent version of R2-KG, equipped with a strict\nself-consistency strategy, achieves significantly higher-than-baseline\nreliability with reduced inference cost but increased abstention rate in\ncomplex KGs. Our findings establish R2-KG as a flexible and cost-effective\nsolution for KG-based reasoning, reducing reliance on high-capacity LLMs while\nensuring trustworthy inference. The code is available at\nhttps://github.com/ekrxjwh2009/R2-KG/."}
{"id": "2502.13061", "pdf": "https://arxiv.org/pdf/2502.13061.pdf", "abs": "https://arxiv.org/abs/2502.13061", "title": "Robust Adaptation of Large Multimodal Models for Retrieval Augmented Hateful Meme Detection", "authors": ["Jingbiao Mei", "Jinghong Chen", "Guangyu Yang", "Weizhe Lin", "Bill Byrne"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "Preprint. Under Review", "summary": "Hateful memes have become a significant concern on the Internet,\nnecessitating robust automated detection systems. While LMMs have shown promise\nin hateful meme detection, they face notable challenges like sub-optimal\nperformance and limited out-of-domain generalization capabilities. Recent\nstudies further reveal the limitations of both SFT and in-context learning when\napplied to LMMs in this setting. To address these issues, we propose a robust\nadaptation framework for hateful meme detection that enhances in-domain\naccuracy and cross-domain generalization while preserving the general\nvision-language capabilities of LMMs. Experiments on six meme classification\ndatasets show that our approach achieves state-of-the-art performance,\noutperforming larger agentic systems. Moreover, our method generates\nhigher-quality rationales for explaining hateful content compared to standard\nSFT, enhancing model interpretability."}
{"id": "2502.13442", "pdf": "https://arxiv.org/pdf/2502.13442.pdf", "abs": "https://arxiv.org/abs/2502.13442", "title": "TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation", "authors": ["Jialin Ouyang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Large language models (LLMs) now achieve near-human performance on standard\nmath word problem benchmarks (e.g., GSM8K), yet their true reasoning ability\nremains disputed. A key concern is that models often produce confident, yet\nunfounded, answers to unanswerable problems. We introduce TreeCut, a synthetic\ndataset that systematically generates infinite unanswerable math word problems\nand their answerable counterparts, by representing each question as a tree and\nremoving chosen necessary conditions. Experiments show TreeCut effectively\ninduce hallucinations in large language models, including GPT-4o and o3-mini,\nwith rates of 64% and 44% in their respective worst-case scenarios under\nzero-shot setting. Further analysis highlights that deeper or more complex\ntrees, composite item names, and removing necessary condition near the middle\nof a path all increase the likelihood of hallucinations, underscoring the\npersistent challenges LLMs face in identifying unanswerable math problems. The\ndataset generation code and sample data are available at\nhttps://github.com/j-bagel/treecut-math."}
{"id": "2502.14037", "pdf": "https://arxiv.org/pdf/2502.14037.pdf", "abs": "https://arxiv.org/abs/2502.14037", "title": "DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation", "authors": ["Giorgio Franceschelli", "Mirco Musolesi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite their growing capabilities, language models still frequently\nreproduce content from their training data, generate repetitive text, and favor\ncommon grammatical patterns and vocabulary. A possible cause is the decoding\nstrategy: the most common strategies either consider only the most probable\ntokens, which reduces output diversity, or increase the likelihood of unlikely\ntokens, compromising output accuracy and correctness. In this paper, we propose\nthree new decoding methods that leverage a mathematical analysis of the token\nprobability distribution to ensure the generation of contextually appropriate\ntext. In particular, the difference between consecutive, sorted probabilities\ncan be used to truncate incorrect tokens. Experiments concerning math problem\nsolving, extreme summarization, and the divergent association task demonstrate\nthat our approach consistently performs at least as well as existing methods in\nterms of quality and diversity."}
{"id": "2502.14171", "pdf": "https://arxiv.org/pdf/2502.14171.pdf", "abs": "https://arxiv.org/abs/2502.14171", "title": "Enhancing Conversational Agents with Theory of Mind: Aligning Beliefs, Desires, and Intentions for Human-Like Interaction", "authors": ["Mehdi Jafari", "Devin Yuncheng Hua", "Hao Xue", "Flora Salim"], "categories": ["cs.CL"], "comment": "Accepted to Findings of ACL 2025", "summary": "Natural language interaction with agentic Artificial Intelligence (AI),\ndriven by Large Language Models (LLMs), is expected to remain a dominant\nparadigm in the near future. While humans instinctively align their\ncommunication with mental states -- an ability known as Theory of Mind (ToM),\ncurrent LLM powered systems exhibit significant limitations in this regard.\nThis study examines the extent to which open source language models (LLaMA) can\ncapture and preserve ToM related information and how effectively it contributes\nto consistent ToM reasoning in generated responses. We further investigate\nwhether explicit manipulation of ToM related components, such as beliefs,\ndesires, and intentions, can enhance response alignment. Experiments on two\nLLaMA 3 variants demonstrate that incorporating ToM informed alignment improves\nresponse quality, achieving win rates of 67 and 63 percent for the 3B and 8B\nmodels, respectively. These findings highlight the potential of ToM driven\nstrategies to improve alignment in LLM based conversational agents."}
{"id": "2502.16051", "pdf": "https://arxiv.org/pdf/2502.16051.pdf", "abs": "https://arxiv.org/abs/2502.16051", "title": "Moving Beyond Medical Exam Questions: A Clinician-Annotated Dataset of Real-World Tasks and Ambiguity in Mental Healthcare", "authors": ["Max Lamparth", "Declan Grabb", "Amy Franks", "Scott Gershan", "Kaitlyn N. Kunstman", "Aaron Lulla", "Monika Drummond Roots", "Manu Sharma", "Aryan Shrivastava", "Nina Vasan", "Colleen Waickman"], "categories": ["cs.CL"], "comment": "Added minor clarifications and expanded appendices", "summary": "Current medical language model (LM) benchmarks often over-simplify the\ncomplexities of day-to-day clinical practice tasks and instead rely on\nevaluating LMs on multiple-choice board exam questions. Thus, we present an\nexpert-created and annotated dataset spanning five critical domains of\ndecision-making in mental healthcare: treatment, diagnosis, documentation,\nmonitoring, and triage. This dataset - created without any LM assistance - is\ndesigned to capture the nuanced clinical reasoning and daily ambiguities mental\nhealth practitioners encounter, reflecting the inherent complexities of care\ndelivery that are missing from existing datasets. Almost all 203 base questions\nwith five answer options each have had the decision-irrelevant demographic\npatient information removed and replaced with variables (e.g., AGE), and are\navailable for male, female, or non-binary-coded patients. For question\ncategories dealing with ambiguity and multiple valid answer options, we create\na preference dataset with uncertainties from the expert annotations. We outline\na series of intended use cases and demonstrate the usability of our dataset by\nevaluating eleven off-the-shelf and four mental health fine-tuned LMs on\ncategory-specific task accuracy, on the impact of patient demographic\ninformation on decision-making, and how consistently free-form responses\ndeviate from human annotated samples."}
{"id": "2502.16747", "pdf": "https://arxiv.org/pdf/2502.16747.pdf", "abs": "https://arxiv.org/abs/2502.16747", "title": "SQLong: Enhanced NL2SQL for Longer Contexts with LLMs", "authors": ["Dai Quoc Nguyen", "Cong Duy Vu Hoang", "Duy Vu", "Gioacchino Tangari", "Thanh Tien Vu", "Don Dharmasiri", "Yuan-Fang Li", "Long Duong"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "comment": "Accepted to Table Representation Learning Workshop at ACL 2025", "summary": "Open-weight large language models (LLMs) have significantly advanced\nperformance in the Natural Language to SQL (NL2SQL) task. However, their\neffectiveness diminishes when dealing with large database schemas, as the\ncontext length increases. To address this limitation, we present SQLong, a\nnovel and efficient data augmentation framework designed to enhance LLM\nperformance in long-context scenarios for the NL2SQL task. SQLong generates\naugmented datasets by extending existing database schemas with additional\nsynthetic CREATE TABLE commands and corresponding data rows, sampled from\ndiverse schemas in the training data. This approach effectively simulates\nlong-context scenarios during finetuning and evaluation. Through experiments on\nthe Spider and BIRD datasets, we demonstrate that LLMs finetuned with\nSQLong-augmented data significantly outperform those trained on standard\ndatasets. These imply SQLong's practical implementation and its impact on\nimproving NL2SQL capabilities in real-world settings with complex database\nschemas."}
{"id": "2502.16894", "pdf": "https://arxiv.org/pdf/2502.16894.pdf", "abs": "https://arxiv.org/abs/2502.16894", "title": "Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment", "authors": ["Chenghao Fan", "Zhenyi Lu", "Sichen Liu", "Chengfeng Gu", "Xiaoye Qu", "Wei Wei", "Yu Cheng"], "categories": ["cs.CL"], "comment": "Accepted by ICML 2025", "summary": "While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for\nLarge Language Models (LLMs), its performance often falls short of Full\nFine-Tuning (Full FT). Current methods optimize LoRA by initializing with\nstatic singular value decomposition (SVD) subsets, leading to suboptimal\nleveraging of pre-trained knowledge. Another path for improving LoRA is\nincorporating a Mixture-of-Experts (MoE) architecture. However, weight\nmisalignment and complex gradient dynamics make it challenging to adopt SVD\nprior to the LoRA MoE architecture. To mitigate these issues, we propose\n\\underline{G}reat L\\underline{o}R\\underline{A} Mixture-of-Exper\\underline{t}\n(GOAT), a framework that (1) adaptively integrates relevant priors using an\nSVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by\nderiving a theoretical scaling factor. We demonstrate that proper scaling,\nwithout modifying the architecture or training algorithms, boosts LoRA MoE's\nefficiency and performance. Experiments across 25 datasets, including natural\nlanguage understanding, commonsense reasoning, image classification, and\nnatural language generation, demonstrate GOAT's state-of-the-art performance,\nclosing the gap with Full FT."}
{"id": "2502.16901", "pdf": "https://arxiv.org/pdf/2502.16901.pdf", "abs": "https://arxiv.org/abs/2502.16901", "title": "Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in Multilingual LLMs", "authors": ["Himanshu Beniwal", "Sailesh Panda", "Birudugadda Srivibhav", "Mayank Singh"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We explore \\textbf{C}ross-lingual \\textbf{B}ackdoor \\textbf{AT}tacks (X-BAT)\nin multilingual Large Language Models (mLLMs), revealing how backdoors inserted\nin one language can automatically transfer to others through shared embedding\nspaces. Using toxicity classification as a case study, we demonstrate that\nattackers can compromise multilingual systems by poisoning data in a single\nlanguage, with rare and high-occurring tokens serving as specific, effective\ntriggers. Our findings expose a critical vulnerability that influences the\nmodel's architecture, resulting in a concealed backdoor effect during the\ninformation flow. Our code and data are publicly available\nhttps://github.com/himanshubeniwal/X-BAT."}
{"id": "2502.19982", "pdf": "https://arxiv.org/pdf/2502.19982.pdf", "abs": "https://arxiv.org/abs/2502.19982", "title": "Erasing Without Remembering: Implicit Knowledge Forgetting in Large Language Models", "authors": ["Huazheng Wang", "Yongcheng Jing", "Haifeng Sun", "Yingjie Wang", "Jingyu Wang", "Jianxin Liao", "Dacheng Tao"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In this paper, we investigate knowledge forgetting in large language models\nwith a focus on its generalisation--ensuring that models forget not only\nspecific training samples but also related implicit knowledge. To this end, we\nbegin by identifying a broader unlearning scope that includes both target data\nand logically associated samples, including rephrased, subject-replaced,\none-hop reasoned, and relation-reversed data. To rigorously evaluate\ngeneralisation, we introduce UGBench, the first comprehensive benchmark\nspecifically designed to assess the unlearning of in-scope implicit knowledge\ncovering 13 state-of-the-art methods across three datasets. UGBench reveals\nthat unlearned models can still recall paraphrased answers and retain target\nfacts in intermediate layers. This motivates us to take a preliminary step\ntoward more generalised implicit knowledge forgetting by proposing PerMU, a\nnovel probability perturbation-based unlearning paradigm. PerMU simulates\nadversarial unlearning samples to eliminate fact-related tokens from the logit\ndistribution, collectively reducing the probabilities of all answer-associated\ntokens. Experiments are conducted on a diverse range of datasets, including\nTOFU, Harry Potter, ZsRE, WMDP, and MUSE, using models ranging from 1.3B to 13B\nin scale. The results demonstrate that PerMU delivers up to a 50.40%\nimprovement in unlearning vanilla target data while maintaining a 40.73% boost\nin forgetting implicit knowledge. Our code can be found in\nhttps://github.com/MaybeLizzy/UGBench."}
{"id": "2502.20592", "pdf": "https://arxiv.org/pdf/2502.20592.pdf", "abs": "https://arxiv.org/abs/2502.20592", "title": "Multi2: Multi-Agent Test-Time Scalable Framework for Multi-Document Processing", "authors": ["Juntai Cao", "Xiang Zhang", "Raymond Li", "Chuyuan Li", "Chenyu You", "Shafiq Joty", "Giuseppe Carenini"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in test-time scaling have shown promising results in\nimproving Large Language Model (LLM) performance through strategic computation\nallocation during inference. While this approach has demonstrated strong\nimprovements in logical and mathematical reasoning tasks, its application to\nnatural language generation (NLG), particularly summarization, remains\nunexplored. Multi-Document Summarization (MDS), a fundamental task in NLG,\npresents unique challenges by requiring models to extract and synthesize\nessential information across multiple lengthy documents. Unlike reasoning\ntasks, MDS demands a more nuanced approach to prompt design and ensemble\nmethods, as no single \"best\" prompt can satisfy diverse summarization\nrequirements. We propose a novel framework leveraging test-time scaling for\nMDS. Our approach employs prompt ensemble techniques to generate multiple\ncandidate summaries using various prompts, then combines them with an\naggregator to produce a refined summary. To evaluate our method effectively, we\nalso introduce two new LLM-based metrics: the Consistency-Aware Preference\n(CAP) score and LLM Atom-Content-Unit (LLM-ACU) score, which assess summary\nquality while addressing the positional bias inherent in traditional automatic\nevaluation. Our extensive experiments demonstrate that this framework\nsignificantly enhances summary quality while also revealing the practical\nscaling boundaries to MDS tasks."}
{"id": "2502.21074", "pdf": "https://arxiv.org/pdf/2502.21074.pdf", "abs": "https://arxiv.org/abs/2502.21074", "title": "CODI: Compressing Chain-of-Thought into Continuous Space via Self-Distillation", "authors": ["Zhenyi Shen", "Hanqi Yan", "Linhai Zhang", "Zhanghao Hu", "Yali Du", "Yulan He"], "categories": ["cs.CL"], "comment": "16 pages", "summary": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nencouraging step-by-step reasoning in natural language. However, leveraging a\nlatent continuous space for reasoning may offer benefits in terms of both\nefficiency and robustness. Prior implicit CoT methods attempt to bypass\nlanguage completely by reasoning in continuous space but have consistently\nunderperformed compared to the standard explicit CoT approach. We introduce\nCODI (Continuous Chain-of-Thought via Self-Distillation), a novel training\nframework that effectively compresses natural language CoT into continuous\nspace. CODI jointly trains a teacher task (Explicit CoT) and a student task\n(Implicit CoT), distilling the reasoning ability from language into continuous\nspace by aligning the hidden states of a designated token. Our experiments show\nthat CODI is the first implicit CoT approach to match the performance of\nexplicit CoT on GSM8k at the GPT-2 scale, achieving a 3.1x compression rate and\noutperforming the previous state-of-the-art by 28.2% in accuracy. CODI also\ndemonstrates robustness, generalizable to complex datasets, and\ninterpretability. These results validate that LLMs can reason effectively not\nonly in natural language, but also in a latent continuous space."}
{"id": "2503.01513", "pdf": "https://arxiv.org/pdf/2503.01513.pdf", "abs": "https://arxiv.org/abs/2503.01513", "title": "Evaluation and Facilitation of Online Discussions in the LLM Era: A Survey", "authors": ["Katerina Korre", "Dimitris Tsirmpas", "Nikos Gkoumas", "Emma Cabalé", "Danai Myrtzani", "Theodoros Evgeniou", "Ion Androutsopoulos", "John Pavlopoulos"], "categories": ["cs.CL"], "comment": null, "summary": "We present a survey of methods for assessing and enhancing the quality of\nonline discussions, focusing on the potential of LLMs. While online discourses\naim, at least in theory, to foster mutual understanding, they often devolve\ninto harmful exchanges, such as hate speech, threatening social cohesion and\ndemocratic values. Recent advancements in LLMs enable artificial facilitation\nagents to not only moderate content, but also actively improve the quality of\ninteractions. Our survey synthesizes ideas from NLP and Social Sciences to\nprovide (a) a new taxonomy on discussion quality evaluation, (b) an overview of\nintervention and facilitation strategies, (c) along with a new taxonomy of\nconversation facilitation datasets, (d) an LLM-oriented roadmap of good\npractices and future research directions, from technological and societal\nperspectives."}
{"id": "2503.02589", "pdf": "https://arxiv.org/pdf/2503.02589.pdf", "abs": "https://arxiv.org/abs/2503.02589", "title": "MCiteBench: A Multimodal Benchmark for Generating Text with Citations", "authors": ["Caiyu Hu", "Yikai Zhang", "Tinghui Zhu", "Yiwei Ye", "Yanghua Xiao"], "categories": ["cs.CL", "cs.IR"], "comment": "https://caiyuhu.github.io/MCiteBench/", "summary": "Multimodal Large Language Models (MLLMs) have advanced in integrating diverse\nmodalities but frequently suffer from hallucination. A promising solution to\nmitigate this issue is to generate text with citations, providing a transparent\nchain for verification. However, existing work primarily focuses on generating\ncitations for text-only content, leaving the challenges of multimodal scenarios\nlargely unexplored. In this paper, we introduce MCiteBench, the first benchmark\ndesigned to assess the ability of MLLMs to generate text with citations in\nmultimodal contexts. Our benchmark comprises data derived from academic papers\nand review-rebuttal interactions, featuring diverse information sources and\nmultimodal content. Experimental results reveal that MLLMs struggle to ground\ntheir outputs reliably when handling multimodal input. Further analysis\nuncovers a systematic modality bias and reveals how models internally rely on\ndifferent sources when generating citations, offering insights into model\nbehavior and guiding future directions for multimodal citation tasks."}
{"id": "2503.04372", "pdf": "https://arxiv.org/pdf/2503.04372.pdf", "abs": "https://arxiv.org/abs/2503.04372", "title": "Assumed Identities: Quantifying Gender Bias in Machine Translation of Gender-Ambiguous Occupational Terms", "authors": ["Orfeas Menis Mastromichalakis", "Giorgos Filandrianos", "Maria Symeonaki", "Giorgos Stamou"], "categories": ["cs.CL"], "comment": null, "summary": "Machine Translation (MT) systems frequently encounter gender-ambiguous\noccupational terms, where they must assign gender without explicit contextual\ncues. While individual translations in such cases may not be inherently biased,\nsystematic patterns-such as consistently translating certain professions with\nspecific genders-can emerge, reflecting and perpetuating societal stereotypes.\nThis ambiguity challenges traditional instance-level single-answer evaluation\napproaches, as no single gold standard translation exists. To address this, we\nintroduce GRAPE, a probability-based metric designed to evaluate gender bias by\nanalyzing aggregated model responses. Alongside this, we present GAMBIT-MT, a\nbenchmarking dataset in English with gender-ambiguous occupational terms. Using\nGRAPE, we evaluate several MT systems and examine whether their gendered\ntranslations in Greek and French align with or diverge from societal\nstereotypes, real-world occupational gender distributions, and normative\nstandards."}
{"id": "2503.09579", "pdf": "https://arxiv.org/pdf/2503.09579.pdf", "abs": "https://arxiv.org/abs/2503.09579", "title": "Cost-Optimal Grouped-Query Attention for Long-Context Modeling", "authors": ["Yingfa Chen", "Yutong Wu", "Chenyang Song", "Zhen Leng Thai", "Xingyu Shen", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "18 pages, 15 figures", "summary": "Grouped-Query Attention (GQA) is a widely adopted strategy for reducing the\ncomputational cost of attention layers in large language models (LLMs).\nHowever, current GQA configurations are often suboptimal because they overlook\nhow context length influences inference cost. Since inference cost grows with\ncontext length, the most cost-efficient GQA configuration should also vary\naccordingly. In this work, we analyze the relationship among context length,\nmodel size, GQA configuration, and model loss, and introduce two innovations:\n(1) we decouple the total head size from the hidden size, enabling more\nflexible control over attention FLOPs; and (2) we jointly optimize the model\nsize and the GQA configuration to arrive at a better allocation of inference\nresources between attention layers and other components. Our analysis reveals\nthat commonly used GQA configurations are highly suboptimal for long-context\nscenarios. More importantly, we propose a recipe for deriving cost-optimal GQA\nconfigurations. Our results show that for long-context scenarios, one should\nuse fewer attention heads while scaling up model size. Configurations selected\nby our recipe can reduce both memory usage and FLOPs by more than 50% compared\nto Llama-3's GQA, with *no degradation in model capabilities*. Our findings\noffer valuable insights for designing efficient long-context LLMs. The code is\navailable at https://www.github.com/THUNLP/cost-optimal-gqa ."}
{"id": "2503.10657", "pdf": "https://arxiv.org/pdf/2503.10657.pdf", "abs": "https://arxiv.org/abs/2503.10657", "title": "RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore Model-level Scaling Up in LLMs", "authors": ["Zhongzhan Huang", "Guoming Ling", "Yupei Lin", "Yandong Chen", "Shanshan Zhong", "Hefeng Wu", "Liang Lin"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Routing large language models (LLMs) is a new paradigm that uses a router to\nrecommend the best LLM from a pool of candidates for a given input. In this\npaper, our comprehensive analysis with more than 8,500 LLMs reveals a novel\nmodel-level scaling up phenomenon in Routing LLMs, i.e., a capable router can\nsignificantly enhance the performance of this paradigm as the number of\ncandidates increases. This improvement can even surpass the performance of the\nbest single model in the pool and many existing strong LLMs, confirming it a\nhighly promising paradigm. However, the lack of comprehensive and open-source\nbenchmarks for Routing LLMs has hindered the development of routers. In this\npaper, we introduce RouterEval, a benchmark tailored for router research, which\nincludes over 200,000,000 performance records for 12 popular LLM evaluations\nacross various areas such as commonsense reasoning, semantic understanding,\netc., based on over 8,500 various LLMs. Using RouterEval, extensive evaluations\nof existing Routing LLM methods reveal that most still have significant room\nfor improvement. See https://github.com/MilkThink-Lab/RouterEval for all data,\ncode and tutorial."}
{"id": "2503.12908", "pdf": "https://arxiv.org/pdf/2503.12908.pdf", "abs": "https://arxiv.org/abs/2503.12908", "title": "HICD: Hallucination-Inducing via Attention Dispersion for Contrastive Decoding to Mitigate Hallucinations in Large Language Models", "authors": ["Xinyan Jiang", "Hang Ye", "Yongxin Zhu", "Xiaoying Zheng", "Zikang Chen", "Jun Gong"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL2025 findings", "summary": "Large Language Models (LLMs) often generate hallucinations, producing outputs\nthat are contextually inaccurate or factually incorrect. We introduce HICD, a\nnovel method designed to induce hallucinations for contrastive decoding to\nmitigate hallucinations. Unlike existing contrastive decoding methods, HICD\nselects attention heads crucial to the model's prediction as inducing heads,\nthen induces hallucinations by dispersing attention of these inducing heads and\ncompares the hallucinated outputs with the original outputs to obtain the final\nresult. Our approach significantly improves performance on tasks requiring\ncontextual faithfulness, such as context completion, reading comprehension, and\nquestion answering. It also improves factuality in tasks requiring accurate\nknowledge recall. We demonstrate that our inducing heads selection and\nattention dispersion method leads to more \"contrast-effective\" hallucinations\nfor contrastive decoding, outperforming other hallucination-inducing methods.\nOur findings provide a promising strategy for reducing hallucinations by\ninducing hallucinations in a controlled manner, enhancing the performance of\nLLMs in a wide range of tasks."}
{"id": "2503.18132", "pdf": "https://arxiv.org/pdf/2503.18132.pdf", "abs": "https://arxiv.org/abs/2503.18132", "title": "MathAgent: Leveraging a Mixture-of-Math-Agent Framework for Real-World Multimodal Mathematical Error Detection", "authors": ["Yibo Yan", "Shen Wang", "Jiahao Huo", "Philip S. Yu", "Xuming Hu", "Qingsong Wen"], "categories": ["cs.CL"], "comment": "Accepted by The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL Industry 2025, Oral Presentation)", "summary": "Mathematical error detection in educational settings presents a significant\nchallenge for Multimodal Large Language Models (MLLMs), requiring a\nsophisticated understanding of both visual and textual mathematical content\nalong with complex reasoning capabilities. Though effective in mathematical\nproblem-solving, MLLMs often struggle with the nuanced task of identifying and\ncategorizing student errors in multimodal mathematical contexts. Therefore, we\nintroduce MathAgent, a novel Mixture-of-Math-Agent framework designed\nspecifically to address these challenges. Our approach decomposes error\ndetection into three phases, each handled by a specialized agent: an image-text\nconsistency validator, a visual semantic interpreter, and an integrative error\nanalyzer. This architecture enables more accurate processing of mathematical\ncontent by explicitly modeling relationships between multimodal problems and\nstudent solution steps. We evaluate MathAgent on real-world educational data,\ndemonstrating approximately 5% higher accuracy in error step identification and\n3% improvement in error categorization compared to baseline models. Besides,\nMathAgent has been successfully deployed in an educational platform that has\nserved over one million K-12 students, achieving nearly 90% student\nsatisfaction while generating significant cost savings by reducing manual error\ndetection."}
{"id": "2504.02438", "pdf": "https://arxiv.org/pdf/2504.02438.pdf", "abs": "https://arxiv.org/abs/2504.02438", "title": "Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation", "authors": ["Chuanqi Cheng", "Jian Guan", "Wei Wu", "Rui Yan"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICML 2025", "summary": "Long-form video processing fundamentally challenges vision-language models\n(VLMs) due to the high computational costs of handling extended temporal\nsequences. Existing token pruning and feature merging methods often sacrifice\ncritical temporal dependencies or dilute semantic information. We introduce\ndifferential distillation, a principled approach that systematically preserves\ntask-relevant information while suppressing redundancy. Based on this\nprinciple, we develop ViLAMP, a hierarchical video-language model that\nprocesses hour-long videos at \"mixed precision\" through two key mechanisms: (1)\ndifferential keyframe selection that maximizes query relevance while\nmaintaining temporal distinctiveness at the frame level and (2) differential\nfeature merging that preserves query-salient features in non-keyframes at the\npatch level. Hence, ViLAMP retains full information in keyframes while reducing\nnon-keyframes to their most salient features, resembling mixed-precision\ntraining. Extensive experiments demonstrate ViLAMP's superior performance\nacross five video understanding benchmarks, particularly on long-form content.\nNotably, ViLAMP can process ultra-long videos (up to 10K frames) on a single\nNVIDIA A100 GPU, achieving substantial computational efficiency while\nmaintaining state-of-the-art performance. Code and model are available at\nhttps://github.com/steven-ccq/ViLAMP."}
{"id": "2504.05831", "pdf": "https://arxiv.org/pdf/2504.05831.pdf", "abs": "https://arxiv.org/abs/2504.05831", "title": "Leveraging Robust Optimization for LLM Alignment under Distribution Shifts", "authors": ["Mingye Zhu", "Yi Liu", "Zheren Fu", "Yongdong Zhang", "Zhendong Mao"], "categories": ["cs.CL"], "comment": null, "summary": "Preference alignment methods are increasingly critical for steering large\nlanguage models (LLMs) to generate outputs consistent with human values. While\nrecent approaches often rely on synthetic data generated by LLMs for\nscalability and cost-efficiency reasons, this reliance can introduce\ndistribution shifts that undermine the nuanced representation of human\npreferences needed for desirable outputs. In this paper, we propose a novel\ndistribution-aware optimization framework that improves preference alignment\ndespite such shifts. Our approach first leverages well-learned classifiers to\nassign a calibration value to each training sample, quantifying its alignment\nwith the target human-preferred distribution. These values are then\nincorporated into a robust optimization objective that minimizes the worst-case\nloss over regions of the data space most relevant to human preferences. By\nexplicitly focusing optimization on the target distribution, our approach\nmitigates the impact of distributional mismatch and improves the generation of\nresponses that better reflect intended values."}
{"id": "2504.08399", "pdf": "https://arxiv.org/pdf/2504.08399.pdf", "abs": "https://arxiv.org/abs/2504.08399", "title": "Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in Large Language Models", "authors": ["Yin Jou Huang", "Rafik Hadfi"], "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 6 figures, 6 tables", "summary": "Self-report questionnaires have long been used to assess LLM personality\ntraits, yet they fail to capture behavioral nuances due to biases and\nmeta-knowledge contamination. This paper proposes a novel multi-observer\nframework for personality trait assessments in LLM agents that draws on\ninformant-report methods in psychology. Instead of relying on self-assessments,\nwe employ multiple observer agents. Each observer is configured with a specific\nrelational context (e.g., family member, friend, or coworker) and engages the\nsubject LLM in dialogue before evaluating its behavior across the Big Five\ndimensions. We show that these observer-report ratings align more closely with\nhuman judgments than traditional self-reports and reveal systematic biases in\nLLM self-assessments. We also found that aggregating responses from 5 to 7\nobservers reduces systematic biases and achieves optimal reliability. Our\nresults highlight the role of relationship context in perceiving personality\nand demonstrate that a multi-observer paradigm offers a more reliable,\ncontext-sensitive approach to evaluating LLM personality traits."}
{"id": "2504.10368", "pdf": "https://arxiv.org/pdf/2504.10368.pdf", "abs": "https://arxiv.org/abs/2504.10368", "title": "S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability of Large Reasoning Models", "authors": ["Wenyuan Zhang", "Shuaiyi Nie", "Xinghua Zhang", "Zefeng Zhang", "Tingwen Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "31 pages, 9 figures, 16 tables", "summary": "We introduce S1-Bench, a novel benchmark designed to evaluate the performance\nof Large Reasoning Models (LRMs) on simple tasks that favor intuitive system 1\nthinking rather than deliberative system 2 reasoning. While LRMs have achieved\nsignificant breakthroughs in complex reasoning tasks through explicit chains of\nthought, their heavy reliance on system 2 thinking may limit their system 1\nthinking capabilities. However, there is a lack of an appropriate benchmark for\nevaluating LRM's system 1 thinking capabilities. To fill this gap, S1-Bench\nintroduces a suite of simple, diverse, and natural questions across multiple\ndomains and languages, specifically designed to assess LRMs' performance on\nquestions more suitable for system 1 . We conduct extensive evaluations across\n28 LRMs, revealing their inefficiency, inadequate accuracy, and limited\nrobustness when handling simple questions. Additionally, we observe a gap\nbetween their difficulty perception and generation length. Overall, this work\npaves the way toward dual-system compatibility in the development of LRMs."}
{"id": "2504.12324", "pdf": "https://arxiv.org/pdf/2504.12324.pdf", "abs": "https://arxiv.org/abs/2504.12324", "title": "Cross-Document Cross-Lingual NLI via RST-Enhanced Graph Fusion and Interpretability Prediction", "authors": ["Mengying Yuan", "Wenhao Wang", "Zixuan Wang", "Yujie Huang", "Kangli Wei", "Fei Li", "Chong Teng", "Donghong Ji"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural Language Inference (NLI) is a fundamental task in natural language\nprocessing. While NLI has developed many sub-directions such as sentence-level\nNLI, document-level NLI and cross-lingual NLI, Cross-Document Cross-Lingual NLI\n(CDCL-NLI) remains largely unexplored. In this paper, we propose a novel\nparadigm: CDCL-NLI, which extends traditional NLI capabilities to\nmulti-document, multilingual scenarios. To support this task, we construct a\nhigh-quality CDCL-NLI dataset including 25,410 instances and spanning 26\nlanguages. To address the limitations of previous methods on CDCL-NLI task, we\nfurther propose an innovative method that integrates RST-enhanced graph fusion\nwith interpretability-aware prediction. Our approach leverages RST (Rhetorical\nStructure Theory) within heterogeneous graph neural networks for cross-document\ncontext modeling, and employs a structure-aware semantic alignment based on\nlexical chains for cross-lingual understanding. For NLI interpretability, we\ndevelop an EDU (Elementary Discourse Unit)-level attribution framework that\nproduces extractive explanations. Extensive experiments demonstrate our\napproach's superior performance, achieving significant improvements over both\nconventional NLI models as well as large language models. Our work sheds light\non the study of NLI and will bring research interest on cross-document\ncross-lingual context understanding, hallucination elimination and\ninterpretability inference. Our code and datasets are available at\n\\href{https://anonymous.4open.science/r/CDCL-NLI-637E/}{CDCL-NLI-link} for peer\nreview."}
{"id": "2504.14150", "pdf": "https://arxiv.org/pdf/2504.14150.pdf", "abs": "https://arxiv.org/abs/2504.14150", "title": "Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations", "authors": ["Katie Matton", "Robert Osazuwa Ness", "John Guttag", "Emre Kıcıman"], "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": "66 pages, 14 figures, 40 tables; ICLR 2025 (spotlight) camera ready", "summary": "Large language models (LLMs) are capable of generating plausible explanations\nof how they arrived at an answer to a question. However, these explanations can\nmisrepresent the model's \"reasoning\" process, i.e., they can be unfaithful.\nThis, in turn, can lead to over-trust and misuse. We introduce a new approach\nfor measuring the faithfulness of LLM explanations. First, we provide a\nrigorous definition of faithfulness. Since LLM explanations mimic human\nexplanations, they often reference high-level concepts in the input question\nthat purportedly influenced the model. We define faithfulness in terms of the\ndifference between the set of concepts that LLM explanations imply are\ninfluential and the set that truly are. Second, we present a novel method for\nestimating faithfulness that is based on: (1) using an auxiliary LLM to modify\nthe values of concepts within model inputs to create realistic counterfactuals,\nand (2) using a Bayesian hierarchical model to quantify the causal effects of\nconcepts at both the example- and dataset-level. Our experiments show that our\nmethod can be used to quantify and discover interpretable patterns of\nunfaithfulness. On a social bias task, we uncover cases where LLM explanations\nhide the influence of social bias. On a medical question answering task, we\nuncover cases where LLM explanations provide misleading claims about which\npieces of evidence influenced the model's decisions."}
{"id": "2504.15241", "pdf": "https://arxiv.org/pdf/2504.15241.pdf", "abs": "https://arxiv.org/abs/2504.15241", "title": "MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety", "authors": ["Yahan Yang", "Soham Dan", "Shuo Li", "Dan Roth", "Insup Lee"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Large Language Models (LLMs) are susceptible to adversarial attacks such as\njailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability\nis exacerbated in multilingual settings, where multilingual safety-aligned data\nis often limited. Thus, developing a guardrail capable of detecting and\nfiltering unsafe content across diverse languages is critical for deploying\nLLMs in real-world applications. In this work, we introduce a multilingual\nguardrail with reasoning for prompt classification. Our method consists of: (1)\nsynthetic multilingual data generation incorporating culturally and\nlinguistically nuanced variants, (2) supervised fine-tuning, and (3) a\ncurriculum-based Group Relative Policy Optimization (GRPO) framework that\nfurther improves performance. Experimental results demonstrate that our\nmultilingual guardrail, MrGuard, consistently outperforms recent baselines\nacross both in-domain and out-of-domain languages by more than 15%. We also\nevaluate MrGuard's robustness to multilingual variations, such as\ncode-switching and low-resource language distractors in the prompt, and\ndemonstrate that it preserves safety judgments under these challenging\nconditions. The multilingual reasoning capability of our guardrail enables it\nto generate explanations, which are particularly useful for understanding\nlanguage-specific risks and ambiguities in multilingual content moderation."}
{"id": "2504.20371", "pdf": "https://arxiv.org/pdf/2504.20371.pdf", "abs": "https://arxiv.org/abs/2504.20371", "title": "DMDTEval: An Evaluation and Analysis of LLMs on Disambiguation in Multi-domain Translation", "authors": ["Zhibo Man", "Yuanmeng Chen", "Yujie Zhang", "Jinan Xu"], "categories": ["cs.CL"], "comment": null, "summary": "Currently, Large Language Models (LLMs) have achieved remarkable results in\nmachine translation. However, their performance in multi-domain translation\n(MDT) is less satisfactory, the meanings of words can vary across different\ndomains, highlighting the significant ambiguity inherent in MDT. Therefore,\nevaluating the disambiguation ability of LLMs in MDT, remains an open problem.\nTo this end, we present an evaluation and analysis of LLMs on disambiguation in\nmulti-domain translation (DMDTEval), our systematic evaluation framework\nconsisting of three critical aspects: (1) we construct a translation test set\nwith multi-domain ambiguous word annotation, (2) we curate a diverse set of\ndisambiguation prompt strategies, and (3) we design precise disambiguation\nmetrics, and study the efficacy of various prompt strategies on multiple\nstate-of-the-art LLMs. We conduct comprehensive experiments across 4 language\npairs and 13 domains, our extensive experiments reveal a number of crucial\nfindings that we believe will pave the way and also facilitate further research\nin the critical area of improving the disambiguation of LLMs."}
{"id": "2505.00038", "pdf": "https://arxiv.org/pdf/2505.00038.pdf", "abs": "https://arxiv.org/abs/2505.00038", "title": "HyPerAlign: Interpretable Personalized LLM Alignment via Hypothesis Generation", "authors": ["Cristina Garbacea", "Chenhao Tan"], "categories": ["cs.CL"], "comment": null, "summary": "Alignment algorithms are widely used to align large language models (LLMs) to\nhuman users based on preference annotations. Typically these (often divergent)\npreferences are aggregated over a diverse set of users, resulting in fine-tuned\nmodels that are aligned to the ``average-user'' preference. Nevertheless,\ncurrent models are used by individual users in very specific contexts and\nsituations, emphasizing the need for user-dependent preference control. In this\nwork we address the problem of personalizing LLM outputs to their users. We aim\nto generate customized responses tailored to specific individuals instead of\ngeneric outputs that emulate the collective voices of diverse populations. We\npropose HyPerAlign, an interpretable and sample-efficient hypothesis-driven\npersonalization approach for LLM models. Given few-shot examples written by a\nparticular user, we first infer hypotheses about their communication\nstrategies, personality, and writing style, then prompt LLM models with these\nhypotheses and user-specific attributes to generate customized outputs. We\nconduct experiments on two different personalization tasks, namely authorship\nattribution and deliberative alignment, with datasets from diverse domains\n(news articles, blog posts, emails, jailbreaking benchmarks). Results\ndemonstrate the superiority of hypothesis-driven LLM personalization compared\nto preference-based fine-tuning methods. For authorship attribution, HyPerAlign\ngenerations have consistently high win-rates (commonly $> 90\\%$) against\nstate-of-the-art preference fine-tuning approaches across diverse user profiles\nand LLM models. For deliberative alignment, the helpfulness of LLM models is\nimproved by up to $70\\%$ on average. Overall, HyPerAlign represents an\ninterpretable and sample-efficient strategy for the personalization of LLM\nmodels to individual users."}
{"id": "2505.00753", "pdf": "https://arxiv.org/pdf/2505.00753.pdf", "abs": "https://arxiv.org/abs/2505.00753", "title": "A Survey on Large Language Model based Human-Agent Systems", "authors": ["Henry Peng Zou", "Wei-Chieh Huang", "Yaozu Wu", "Yankai Chen", "Chunyu Miao", "Hoang Nguyen", "Yue Zhou", "Weizhi Zhang", "Liancheng Fang", "Langzhou He", "Yangning Li", "Dongyuan Li", "Renhe Jiang", "Xue Liu", "Philip S. Yu"], "categories": ["cs.CL", "cs.LG"], "comment": "Paper lists and resources are available at\n  https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-Systems", "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin building fully autonomous agents. However, fully autonomous LLM-based agents\nstill face significant challenges, including limited reliability due to\nhallucinations, difficulty in handling complex tasks, and substantial safety\nand ethical risks, all of which limit their feasibility and trustworthiness in\nreal-world applications. To overcome these limitations, LLM-based human-agent\nsystems (LLM-HAS) incorporate human-provided information, feedback, or control\ninto the agent system to enhance system performance, reliability and safety.\nThis paper provides the first comprehensive and structured survey of LLM-HAS.\nIt clarifies fundamental concepts, systematically presents core components\nshaping these systems, including environment & profiling, human feedback,\ninteraction types, orchestration and communication, explores emerging\napplications, and discusses unique challenges and opportunities. By\nconsolidating current knowledge and offering a structured overview, we aim to\nfoster further research and innovation in this rapidly evolving\ninterdisciplinary field. Paper lists and resources are available at\nhttps://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-Systems."}
{"id": "2505.02156", "pdf": "https://arxiv.org/pdf/2505.02156.pdf", "abs": "https://arxiv.org/abs/2505.02156", "title": "Adaptive Thinking via Mode Policy Optimization for Social Language Agents", "authors": ["Minzheng Wang", "Yongbin Li", "Haobo Wang", "Xinghua Zhang", "Nan Xu", "Bingli Wu", "Fei Huang", "Haiyang Yu", "Wenji Mao"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in Progress. The code and data are available, see\n  https://github.com/MozerWang/AMPO", "summary": "Effective social intelligence simulation requires language agents to\ndynamically adjust reasoning depth, a capability notably absent in current\nstudies. Existing methods either lack this kind of reasoning capability or\nenforce Long Chain-of-Thought reasoning uniformly across all scenarios,\nresulting in excessive token usage and inflexible social simulation. To address\nthis, we propose an $\\textbf{A}$daptive $\\textbf{M}$ode $\\textbf{L}$earning\n($\\textbf{AML}$) framework in this paper, aiming to improve the adaptive\nthinking ability of language agents in dynamic social interactions. To this\nend, we first identify hierarchical thinking modes ranging from intuitive\nresponse to deep deliberation based on the cognitive control theory. We then\ndevelop the $\\textbf{A}$daptive $\\textbf{M}$ode $\\textbf{P}$olicy\n$\\textbf{O}$ptimization ($\\textbf{AMPO}$) algorithm to optimize the\ncontext-aware mode switching and reasoning. Our framework advances existing\nresearch in three key aspects: (1) Multi-granular thinking mode design, (2)\nContext-aware mode switching across social interaction, and (3) Token-efficient\nreasoning via depth-adaptive processing. Extensive experiments on social\nintelligence benchmarks verify that AML achieves 15.6% higher task performance\nthan GPT-4o. Notably, our AMPO outperforms GRPO by 7.0% with 32.8% shorter\nreasoning chains, demonstrating the advantage of adaptive thinking mode\nselection and optimization mechanism in AMPO over GRPO's fixed-depth solution."}
{"id": "2505.06149", "pdf": "https://arxiv.org/pdf/2505.06149.pdf", "abs": "https://arxiv.org/abs/2505.06149", "title": "Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study", "authors": ["Faeze Ghorbanpour", "Daryna Dementieva", "Alexander Fraser"], "categories": ["cs.CL", "cs.CY", "cs.MM"], "comment": null, "summary": "Despite growing interest in automated hate speech detection, most existing\napproaches overlook the linguistic diversity of online content. Multilingual\ninstruction-tuned large language models such as LLaMA, Aya, Qwen, and BloomZ\noffer promising capabilities across languages, but their effectiveness in\nidentifying hate speech through zero-shot and few-shot prompting remains\nunderexplored. This work evaluates LLM prompting-based detection across eight\nnon-English languages, utilizing several prompting techniques and comparing\nthem to fine-tuned encoder models. We show that while zero-shot and few-shot\nprompting lag behind fine-tuned encoder models on most of the real-world\nevaluation sets, they achieve better generalization on functional tests for\nhate speech detection. Our study also reveals that prompt design plays a\ncritical role, with each language often requiring customized prompting\ntechniques to maximize performance."}
{"id": "2505.09930", "pdf": "https://arxiv.org/pdf/2505.09930.pdf", "abs": "https://arxiv.org/abs/2505.09930", "title": "Rethinking Prompt Optimizers: From Prompt Merits to Optimization", "authors": ["Zixiao Zhu", "Hanzhang Zhou", "Zijian Feng", "Tianjiao Li", "Chua Jia Jim Deryl", "Mak Lee Onn", "Gee Wah Ng", "Kezhi Mao"], "categories": ["cs.CL"], "comment": "21 pages, 14 figures", "summary": "Prompt optimization (PO) provides a practical way to improve response quality\nwhen users lack the time or expertise to manually craft effective prompts.\nExisting methods typically rely on advanced, large-scale LLMs like GPT-4 to\ngenerate optimized prompts. However, due to limited downward compatibility,\nverbose, instruction-heavy prompts from advanced LLMs can overwhelm lightweight\ninference models and degrade response quality. In this work, we rethink prompt\noptimization through the lens of interpretable design. We first identify a set\nof model-agnostic prompt quality merits and empirically validate their\neffectiveness in enhancing prompt and response quality. We then introduce MePO,\na merit-guided, lightweight, and locally deployable prompt optimizer trained on\nour preference dataset built from merit-aligned prompts generated by a\nlightweight LLM. Unlike prior work, MePO avoids online optimization reliance,\nreduces cost and privacy concerns, and, by learning clear, interpretable\nmerits, generalizes effectively to both large-scale and lightweight inference\nmodels. Experiments demonstrate that MePO achieves better results across\ndiverse tasks and model types, offering a scalable and robust solution for\nreal-world deployment. The code and dataset can be found in\nhttps://github.com/MidiyaZhu/MePO"}
{"id": "2505.10081", "pdf": "https://arxiv.org/pdf/2505.10081.pdf", "abs": "https://arxiv.org/abs/2505.10081", "title": "Designing and Contextualising Probes for African Languages", "authors": ["Wisdom Aduah", "Francois Meyer"], "categories": ["cs.CL"], "comment": null, "summary": "Pretrained language models (PLMs) for African languages are continually\nimproving, but the reasons behind these advances remain unclear. This paper\npresents the first systematic investigation into probing PLMs for linguistic\nknowledge about African languages. We train layer-wise probes for six\ntypologically diverse African languages to analyse how linguistic features are\ndistributed. We also design control tasks, a way to interpret probe\nperformance, for the MasakhaPOS dataset. We find PLMs adapted for African\nlanguages to encode more linguistic information about target languages than\nmassively multilingual PLMs. Our results reaffirm previous findings that\ntoken-level syntactic information concentrates in middle-to-last layers, while\nsentence-level semantic information is distributed across all layers. Through\ncontrol tasks and probing baselines, we confirm that performance reflects the\ninternal knowledge of PLMs rather than probe memorisation. Our study applies\nestablished interpretability techniques to African-language PLMs. In doing so,\nwe highlight the internal mechanisms underlying the success of strategies like\nactive learning and multilingual adaptation."}
{"id": "2505.10643", "pdf": "https://arxiv.org/pdf/2505.10643.pdf", "abs": "https://arxiv.org/abs/2505.10643", "title": "Artificial Intelligence Bias on English Language Learners in Automatic Scoring", "authors": ["Shuchen Guo", "Yun Wang", "Jichao Yu", "Xuansheng Wu", "Bilgehan Ayik", "Field M. Watts", "Ehsan Latif", "Ninghao Liu", "Lei Liu", "Xiaoming Zhai"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "This study investigated potential scoring biases and disparities toward\nEnglish Language Learners (ELLs) when using automatic scoring systems for\nmiddle school students' written responses to science assessments. We\nspecifically focus on examining how unbalanced training data with ELLs\ncontributes to scoring bias and disparities. We fine-tuned BERT with four\ndatasets: responses from (1) ELLs, (2) non-ELLs, (3) a mixed dataset reflecting\nthe real-world proportion of ELLs and non-ELLs (unbalanced), and (4) a balanced\nmixed dataset with equal representation of both groups. The study analyzed 21\nassessment items: 10 items with about 30,000 ELL responses, five items with\nabout 1,000 ELL responses, and six items with about 200 ELL responses. Scoring\naccuracy (Acc) was calculated and compared to identify bias using Friedman\ntests. We measured the Mean Score Gaps (MSGs) between ELLs and non-ELLs and\nthen calculated the differences in MSGs generated through both the human and AI\nmodels to identify the scoring disparities. We found that no AI bias and\ndistorted disparities between ELLs and non-ELLs were found when the training\ndataset was large enough (ELL = 30,000 and ELL = 1,000), but concerns could\nexist if the sample size is limited (ELL = 200)."}
{"id": "2505.11341", "pdf": "https://arxiv.org/pdf/2505.11341.pdf", "abs": "https://arxiv.org/abs/2505.11341", "title": "Benchmarking Critical Questions Generation: A Challenging Reasoning Task for Large Language Models", "authors": ["Banca Calvo Figueras", "Rodrigo Agerri"], "categories": ["cs.CL"], "comment": null, "summary": "The task of Critical Questions Generation (CQs-Gen) aims to foster critical\nthinking by enabling systems to generate questions that expose underlying\nassumptions and challenge the validity of argumentative reasoning structures.\nDespite growing interest in this area, progress has been hindered by the lack\nof suitable datasets and automatic evaluation standards. This paper presents a\ncomprehensive approach to support the development and benchmarking of systems\nfor this task. We construct the first large-scale dataset including $~$5K\nmanually annotated questions. We also investigate automatic evaluation methods\nand propose a reference-based technique using large language models (LLMs) as\nthe strategy that best correlates with human judgments. Our zero-shot\nevaluation of 11 LLMs establishes a strong baseline while showcasing the\ndifficulty of the task. Data and code plus a public leaderboard are provided to\nencourage further research not only in terms of model performance, but also to\nexplore the practical benefits of CQs-Gen for both automated reasoning and\nhuman critical thinking."}
{"id": "2505.11423", "pdf": "https://arxiv.org/pdf/2505.11423.pdf", "abs": "https://arxiv.org/abs/2505.11423", "title": "When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs", "authors": ["Xiaomin Li", "Zhou Yu", "Zhiwei Zhang", "Xupeng Chen", "Ziji Zhang", "Yingying Zhuang", "Narayanan Sadagopan", "Anurag Beniwal"], "categories": ["cs.CL"], "comment": null, "summary": "Reasoning-enhanced large language models (RLLMs), whether explicitly trained\nfor reasoning or prompted via chain-of-thought (CoT), have achieved\nstate-of-the-art performance on many complex reasoning tasks. However, we\nuncover a surprising and previously overlooked phenomenon: explicit CoT\nreasoning can significantly degrade instruction-following accuracy. Evaluating\n15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints)\nand ComplexBench (with complex, compositional constraints), we consistently\nobserve performance drops when CoT prompting is applied. Through large-scale\ncase studies and an attention-based analysis, we identify common patterns where\nreasoning either helps (e.g., with formatting or lexical precision) or hurts\n(e.g., by neglecting simple constraints or introducing unnecessary content). We\npropose a metric, constraint attention, to quantify model focus during\ngeneration and show that CoT reasoning often diverts attention away from\ninstruction-relevant tokens. To mitigate these effects, we introduce and\nevaluate four strategies: in-context learning, self-reflection, self-selective\nreasoning, and classifier-selective reasoning. Our results demonstrate that\nselective reasoning strategies, particularly classifier-selective reasoning,\ncan substantially recover lost performance. To our knowledge, this is the first\nwork to systematically expose reasoning-induced failures in\ninstruction-following and offer practical mitigation strategies."}
{"id": "2505.11604", "pdf": "https://arxiv.org/pdf/2505.11604.pdf", "abs": "https://arxiv.org/abs/2505.11604", "title": "Talk to Your Slides: Language-Driven Agents for Efficient Slide Editing", "authors": ["Kyudan Jung", "Hojun Cho", "Jooyeol Yun", "Soyoung Yang", "Jaehyeok Jang", "Jagul Choo"], "categories": ["cs.CL"], "comment": "20 pages, 14 figures", "summary": "Editing presentation slides remains one of the most common and time-consuming\ntasks faced by millions of users daily, despite significant advances in\nautomated slide generation. Existing approaches have successfully demonstrated\nslide editing via graphic user interface (GUI)-based agents, offering intuitive\nvisual control. However, such methods often suffer from high computational cost\nand latency. In this paper, we propose Talk-to-Your-Slides, an LLM-powered\nagent designed to edit slides %in active PowerPoint sessions by leveraging\nstructured information about slide objects rather than relying on image\nmodality. The key insight of our work is designing the editing process with\ndistinct high-level and low-level layers to facilitate interaction between user\ncommands and slide objects. By providing direct access to application objects\nrather than screen pixels, our system enables 34.02% faster processing, 34.76%\nbetter instruction fidelity, and 87.42% cheaper operation than baselines. To\nevaluate slide editing capabilities, we introduce TSBench, a human-annotated\ndataset comprising 379 diverse editing instructions paired with corresponding\nslide variations in four categories. Our code, benchmark and demos are\navailable at https://anonymous.4open.science/r/Talk-to-Your-Slides-0F4C."}
{"id": "2505.11733", "pdf": "https://arxiv.org/pdf/2505.11733.pdf", "abs": "https://arxiv.org/abs/2505.11733", "title": "MedCaseReasoning: Evaluating and learning diagnostic reasoning from clinical case reports", "authors": ["Kevin Wu", "Eric Wu", "Rahul Thapa", "Kevin Wei", "Angela Zhang", "Arvind Suresh", "Jacqueline J. Tao", "Min Woo Sun", "Alejandro Lozano", "James Zou"], "categories": ["cs.CL"], "comment": null, "summary": "Doctors and patients alike increasingly use Large Language Models (LLMs) to\ndiagnose clinical cases. However, unlike domains such as math or coding, where\ncorrectness can be objectively defined by the final answer, medical diagnosis\nrequires both the outcome and the reasoning process to be accurate. Currently,\nwidely used medical benchmarks like MedQA and MMLU assess only accuracy in the\nfinal answer, overlooking the quality and faithfulness of the clinical\nreasoning process. To address this limitation, we introduce MedCaseReasoning,\nthe first open-access dataset for evaluating LLMs on their ability to align\nwith clinician-authored diagnostic reasoning. The dataset includes 14,489\ndiagnostic question-and-answer cases, each paired with detailed reasoning\nstatements derived from open-access medical case reports. We evaluate\nstate-of-the-art reasoning LLMs on MedCaseReasoning and find significant\nshortcomings in their diagnoses and reasoning: for instance, the top-performing\nopen-source model, DeepSeek-R1, achieves only 48% 10-shot diagnostic accuracy\nand mentions only 64% of the clinician reasoning statements (recall). However,\nwe demonstrate that fine-tuning LLMs on the reasoning traces derived from\nMedCaseReasoning significantly improves diagnostic accuracy and clinical\nreasoning recall by an average relative gain of 29% and 41%, respectively. The\nopen-source dataset, code, and models are available at\nhttps://github.com/kevinwu23/Stanford-MedCaseReasoning."}
{"id": "2505.11810", "pdf": "https://arxiv.org/pdf/2505.11810.pdf", "abs": "https://arxiv.org/abs/2505.11810", "title": "Efficiently Building a Domain-Specific Large Language Model from Scratch: A Case Study of a Classical Chinese Large Language Model", "authors": ["Shen Li", "Renfen Hu", "Lijun Wang"], "categories": ["cs.CL"], "comment": null, "summary": "General-purpose large language models demonstrate notable capabilities in\nlanguage comprehension and generation, achieving results that are comparable\nto, or even surpass, human performance in many natural language processing\ntasks. Nevertheless, when general models are applied to some specific domains,\ne.g., Classical Chinese texts, their effectiveness is often unsatisfactory, and\nfine-tuning open-source foundational models similarly struggles to adequately\nincorporate domain-specific knowledge. To address this challenge, this study\ndeveloped a large language model, AI Taiyan, specifically designed for\nunderstanding and generating Classical Chinese. Experiments show that with a\nreasonable model design, data processing, foundational training, and\nfine-tuning, satisfactory results can be achieved with only 1.8 billion\nparameters. In key tasks related to language processing of Classical Chinese\nsuch as punctuation, identification of allusions, explanation of word meanings,\nand translation between ancient and modern Chinese, this model exhibits a clear\nadvantage over both general-purpose large models and domain-specific\ntraditional models, achieving levels close to or surpassing human baselines.\nThis research provides a reference for the efficient construction of\nspecialized domain-specific large language models. Furthermore, the paper\ndiscusses the application of this model in fields such as the collation of\nancient texts, dictionary editing, and language research, combined with case\nstudies."}
{"id": "2505.11958", "pdf": "https://arxiv.org/pdf/2505.11958.pdf", "abs": "https://arxiv.org/abs/2505.11958", "title": "Counterspeech the ultimate shield! Multi-Conditioned Counterspeech Generation through Attributed Prefix Learning", "authors": ["Aswini Kumar Padhi", "Anil Bandhakavi", "Tanmoy Chakraborty"], "categories": ["cs.CL"], "comment": "Accepted in ACL 2025 Main Conference", "summary": "Counterspeech has proven to be a powerful tool to combat hate speech online.\nPrevious studies have focused on generating counterspeech conditioned only on\nspecific intents (single attributed). However, a holistic approach considering\nmultiple attributes simultaneously can yield more nuanced and effective\nresponses. Here, we introduce HiPPrO, Hierarchical Prefix learning with\nPreference Optimization, a novel two-stage framework that utilizes the\neffectiveness of attribute-specific prefix embedding spaces hierarchically\noptimized during the counterspeech generation process in the first phase.\nThereafter, we incorporate both reference and reward-free preference\noptimization to generate more constructive counterspeech. Furthermore, we\nextend IntentCONANv2 by annotating all 13,973 counterspeech instances with\nemotion labels by five annotators. HiPPrO leverages hierarchical prefix\noptimization to integrate these dual attributes effectively. An extensive\nevaluation demonstrates that HiPPrO achieves a ~38 % improvement in intent\nconformity and a ~3 %, ~2 %, ~3 % improvement in Rouge-1, Rouge-2, and Rouge-L,\nrespectively, compared to several baseline models. Human evaluations further\nsubstantiate the superiority of our approach, highlighting the enhanced\nrelevance and appropriateness of the generated counterspeech. This work\nunderscores the potential of multi-attribute conditioning in advancing the\nefficacy of counterspeech generation systems."}
{"id": "2505.12043", "pdf": "https://arxiv.org/pdf/2505.12043.pdf", "abs": "https://arxiv.org/abs/2505.12043", "title": "MoL for LLMs: Dual-Loss Optimization to Enhance Domain Expertise While Preserving General Capabilities", "authors": ["Jingxue Chen", "Qingkun Tang", "Qianchun Lu", "Siyuan Fang"], "categories": ["cs.CL"], "comment": null, "summary": "Although large language models (LLMs) perform well in general tasks,\ndomain-specific applications suffer from hallucinations and accuracy\nlimitations. Continual Pre-Training (CPT) approaches encounter two key issues:\n(1) domain-biased data degrades general language skills, and (2) improper\ncorpus-mixture ratios limit effective adaptation. To address these, we propose\na novel framework, Mixture of Losses (MoL), which decouples optimization\nobjectives for domain-specific and general corpora. Specifically, cross-entropy\n(CE) loss is applied to domain-corpus to ensure knowledge acquisition, while\nKullback-Leibler (KL) divergence aligns general-corpus training with the base\nmodel's foundational capabilities. This dual-loss architecture preserves\nuniversal skills while enhancing domain expertise, avoiding catastrophic\nforgetting. Empirically, we validate that a 1:1 domain-to-general corpus ratio\noptimally balances training and overfitting without the need for extensive\ntuning or resource-intensive experiments. Furthermore, our experiments\ndemonstrate significant performance gains compared to traditional CPT\napproaches, which often suffer from degradation in general language\ncapabilities; our model achieves 27.9% higher accuracy on the Math-500\nbenchmark in the non-think reasoning mode, and an impressive 83.3% improvement\non the challenging AIME25 subset in the think mode, underscoring the\neffectiveness of our approach."}
{"id": "2505.12082", "pdf": "https://arxiv.org/pdf/2505.12082.pdf", "abs": "https://arxiv.org/abs/2505.12082", "title": "Model Merging in Pre-training of Large Language Models", "authors": ["Yunshui Li", "Yiyuan Ma", "Shen Yan", "Chaoyi Zhang", "Jing Liu", "Jianqiao Lu", "Ziwen Xu", "Mengzhao Chen", "Minrui Wang", "Shiyi Zhan", "Jin Ma", "Xunhao Lai", "Yao Luo", "Xingyan Bin", "Hongbin Ren", "Mingji Han", "Wenhao Hao", "Bairen Yi", "LingJun Liu", "Bole Ma", "Xiaoying Jia", "Zhou Xun", "Siyuan Qiao", "Liang Xiang", "Yonghui Wu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Model merging has emerged as a promising technique for enhancing large\nlanguage models, though its application in large-scale pre-training remains\nrelatively unexplored. In this paper, we present a comprehensive investigation\nof model merging techniques during the pre-training process. Through extensive\nexperiments with both dense and Mixture-of-Experts (MoE) architectures ranging\nfrom millions to over 100 billion parameters, we demonstrate that merging\ncheckpoints trained with constant learning rates not only achieves significant\nperformance improvements but also enables accurate prediction of annealing\nbehavior. These improvements lead to both more efficient model development and\nsignificantly lower training costs. Our detailed ablation studies on merging\nstrategies and hyperparameters provide new insights into the underlying\nmechanisms while uncovering novel applications. Through comprehensive\nexperimental analysis, we offer the open-source community practical\npre-training guidelines for effective model merging."}
{"id": "2505.12654", "pdf": "https://arxiv.org/pdf/2505.12654.pdf", "abs": "https://arxiv.org/abs/2505.12654", "title": "Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals", "authors": ["Yuxin Lin", "Yinglin Zheng", "Ming Zeng", "Wangzheng Shi"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepected by ACL 2025", "summary": "This paper addresses the gap in predicting turn-taking and backchannel\nactions in human-machine conversations using multi-modal signals (linguistic,\nacoustic, and visual). To overcome the limitation of existing datasets, we\npropose an automatic data collection pipeline that allows us to collect and\nannotate over 210 hours of human conversation videos. From this, we construct a\nMulti-Modal Face-to-Face (MM-F2F) human conversation dataset, including over\n1.5M words and corresponding turn-taking and backchannel annotations from\napproximately 20M frames. Additionally, we present an end-to-end framework that\npredicts the probability of turn-taking and backchannel actions from\nmulti-modal signals. The proposed model emphasizes the interrelation between\nmodalities and supports any combination of text, audio, and video inputs,\nmaking it adaptable to a variety of realistic scenarios. Our experiments show\nthat our approach achieves state-of-the-art performance on turn-taking and\nbackchannel prediction tasks, achieving a 10% increase in F1-score on\nturn-taking and a 33% increase on backchannel prediction. Our dataset and code\nare publicly available online to ease of subsequent research."}
{"id": "2505.12768", "pdf": "https://arxiv.org/pdf/2505.12768.pdf", "abs": "https://arxiv.org/abs/2505.12768", "title": "ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL", "authors": ["Yaxun Dai", "Wenxuan Xie", "Xialie Zhuang", "Tianyu Yang", "Yiying Yang", "Haiqin Yang", "Yuhang Zhao", "Pingfu Chao", "Wenhao Jiang"], "categories": ["cs.CL"], "comment": null, "summary": "In Text-to-SQL, execution feedback is essential for guiding large language\nmodels (LLMs) to reason accurately and generate reliable SQL queries. However,\nexisting methods treat execution feedback solely as a post-hoc signal for\ncorrection or selection, failing to integrate it into the generation process.\nThis limitation hinders their ability to address reasoning errors as they\noccur, ultimately reducing query accuracy and robustness. To address this\nissue, we propose ReEx-SQL (Reasoning with Execution-Aware Reinforcement\nLearning), a framework for Text-to-SQL that enables models to interact with the\ndatabase during decoding and dynamically adjust their reasoning based on\nexecution feedback. ReEx-SQL introduces an execution-aware reasoning paradigm\nthat interleaves intermediate SQL execution into reasoning paths, facilitating\ncontext-sensitive revisions. It achieves this through structured prompts with\nmarkup tags and a stepwise rollout strategy that integrates execution feedback\ninto each stage of generation. To supervise policy learning, we develop a\ncomposite reward function that includes an exploration reward, explicitly\nencouraging effective database interaction. Additionally, ReEx-SQL adopts a\ntree-based decoding strategy to support exploratory reasoning, enabling dynamic\nexpansion of alternative reasoning paths. Notably, ReEx-SQL achieves 88.8% on\nSpider and 64.9% on BIRD at the 7B scale, surpassing the standard reasoning\nbaseline by 2.7% and 2.6%, respectively. It also shows robustness, achieving\n85.2% on Spider-Realistic with leading performance. In addition, its\ntree-structured decoding improves efficiency and performance over linear\ndecoding, reducing inference time by 51.9% on the BIRD development set."}
{"id": "2505.13147", "pdf": "https://arxiv.org/pdf/2505.13147.pdf", "abs": "https://arxiv.org/abs/2505.13147", "title": "What if Deception Cannot be Detected? A Cross-Linguistic Study on the Limits of Deception Detection from Text", "authors": ["Aswathy Velutharambath", "Kai Sassenberg", "Roman Klinger"], "categories": ["cs.CL"], "comment": null, "summary": "Can deception be detected solely from written text? Cues of deceptive\ncommunication are inherently subtle, even more so in text-only communication.\nYet, prior studies have reported considerable success in automatic deception\ndetection. We hypothesize that such findings are largely driven by artifacts\nintroduced during data collection and do not generalize beyond specific\ndatasets. We revisit this assumption by introducing a belief-based deception\nframework, which defines deception as a misalignment between an author's claims\nand true beliefs, irrespective of factual accuracy, allowing deception cues to\nbe studied in isolation. Based on this framework, we construct three corpora,\ncollectively referred to as DeFaBel, including a German-language corpus of\ndeceptive and non-deceptive arguments and a multilingual version in German and\nEnglish, each collected under varying conditions to account for belief change\nand enable cross-linguistic analysis. Using these corpora, we evaluate commonly\nreported linguistic cues of deception. Across all three DeFaBel variants, these\ncues show negligible, statistically insignificant correlations with deception\nlabels, contrary to prior work that treats such cues as reliable indicators. We\nfurther benchmark against other English deception datasets following similar\ndata collection protocols. While some show statistically significant\ncorrelations, effect sizes remain low and, critically, the set of predictive\ncues is inconsistent across datasets. We also evaluate deception detection\nusing feature-based models, pretrained language models, and instruction-tuned\nlarge language models. While some models perform well on established deception\ndatasets, they consistently perform near chance on DeFaBel. Our findings\nchallenge the assumption that deception can be reliably inferred from\nlinguistic cues and call for rethinking how deception is studied and modeled in\nNLP."}
{"id": "2505.13282", "pdf": "https://arxiv.org/pdf/2505.13282.pdf", "abs": "https://arxiv.org/abs/2505.13282", "title": "Rank, Chunk and Expand: Lineage-Oriented Reasoning for Taxonomy Expansion", "authors": ["Sahil Mishra", "Kumar Arjun", "Tanmoy Chakraborty"], "categories": ["cs.CL"], "comment": "Accepted in ACL'25 Findings", "summary": "Taxonomies are hierarchical knowledge graphs crucial for recommendation\nsystems, and web applications. As data grows, expanding taxonomies is\nessential, but existing methods face key challenges: (1) discriminative models\nstruggle with representation limits and generalization, while (2) generative\nmethods either process all candidates at once, introducing noise and exceeding\ncontext limits, or discard relevant entities by selecting noisy candidates. We\npropose LORex ($\\textbf{L}$ineage-$\\textbf{O}$riented $\\textbf{Re}$asoning for\nTaxonomy E$\\textbf{x}$pansion), a plug-and-play framework that combines\ndiscriminative ranking and generative reasoning for efficient taxonomy\nexpansion. Unlike prior methods, LORex ranks and chunks candidate terms into\nbatches, filtering noise and iteratively refining selections by reasoning\ncandidates' hierarchy to ensure contextual efficiency. Extensive experiments\nacross four benchmarks and twelve baselines show that LORex improves accuracy\nby 12% and Wu & Palmer similarity by 5% over state-of-the-art methods."}
{"id": "2505.13346", "pdf": "https://arxiv.org/pdf/2505.13346.pdf", "abs": "https://arxiv.org/abs/2505.13346", "title": "J4R: Learning to Judge with Equivalent Initial State Group Relative Policy Optimization", "authors": ["Austin Xu", "Yilun Zhou", "Xuan-Phi Nguyen", "Caiming Xiong", "Shafiq Joty"], "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 4 figures, 6 tables. To be updated with links for\n  code/benchmark", "summary": "To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench."}
{"id": "2505.13353", "pdf": "https://arxiv.org/pdf/2505.13353.pdf", "abs": "https://arxiv.org/abs/2505.13353", "title": "Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning", "authors": ["Adam Štorek", "Mukur Gupta", "Samira Hajizadeh", "Prashast Srivastava", "Suman Jana"], "categories": ["cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "Although modern Large Language Models (LLMs) support extremely large\ncontexts, their effectiveness in utilizing long context for code reasoning\nremains unclear. This paper investigates LLM reasoning ability over code\nsnippets within large repositories and how it relates to their recall ability.\nSpecifically, we differentiate between lexical code recall (verbatim retrieval)\nand semantic code recall (remembering what the code does). To measure semantic\nrecall, we propose SemTrace, a code reasoning technique where the impact of\nspecific statements on output is attributable and unpredictable. We also\npresent a method to quantify semantic recall sensitivity in existing\nbenchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop\nin code reasoning accuracy as a code snippet approaches the middle of the input\ncontext, particularly with techniques requiring high semantic recall like\nSemTrace. Moreover, we find that lexical recall varies by granularity, with\nmodels excelling at function retrieval but struggling with line-by-line recall.\nNotably, a disconnect exists between lexical and semantic recall, suggesting\ndifferent underlying mechanisms. Finally, our findings indicate that current\ncode reasoning benchmarks may exhibit low semantic recall sensitivity,\npotentially underestimating LLM challenges in leveraging in-context\ninformation."}
{"id": "2403.11083", "pdf": "https://arxiv.org/pdf/2403.11083.pdf", "abs": "https://arxiv.org/abs/2403.11083", "title": "Customizing Visual-Language Foundation Models for Multi-modal Anomaly Detection and Reasoning", "authors": ["Xiaohao Xu", "Yunkang Cao", "Huaxin Zhang", "Nong Sang", "Xiaonan Huang"], "categories": ["cs.CV", "cs.CL"], "comment": "Best Student Paper Award at IEEE International Conference on Computer\n  Supported Cooperative Work in Design, 2025", "summary": "Anomaly detection is vital in various industrial scenarios, including the\nidentification of unusual patterns in production lines and the detection of\nmanufacturing defects for quality control. Existing techniques tend to be\nspecialized in individual scenarios and lack generalization capacities. In this\nstudy, our objective is to develop a generic anomaly detection model that can\nbe applied in multiple scenarios. To achieve this, we custom-build generic\nvisual language foundation models that possess extensive knowledge and robust\nreasoning abilities as anomaly detectors and reasoners. Specifically, we\nintroduce a multi-modal prompting strategy that incorporates domain knowledge\nfrom experts as conditions to guide the models. Our approach considers diverse\nprompt types, including task descriptions, class context, normality rules, and\nreference images. In addition, we unify the input representation of\nmulti-modality into a 2D image format, enabling multi-modal anomaly detection\nand reasoning. Our preliminary studies demonstrate that combining visual and\nlanguage prompts as conditions for customizing the models enhances anomaly\ndetection performance. The customized models showcase the ability to detect\nanomalies across different data modalities such as images, point clouds, and\nvideos. Qualitative case studies further highlight the anomaly detection and\nreasoning capabilities, particularly for multi-object scenes and temporal data.\nOur code is publicly available at\nhttps://github.com/Xiaohao-Xu/Customizable-VLM"}
{"id": "2407.02855", "pdf": "https://arxiv.org/pdf/2407.02855.pdf", "abs": "https://arxiv.org/abs/2407.02855", "title": "From Theft to Bomb-Making: The Ripple Effect of Unlearning in Defending Against Jailbreak Attacks", "authors": ["Zhexin Zhang", "Junxiao Yang", "Yida Lu", "Pei Ke", "Shiyao Cui", "Chujie Zheng", "Hongning Wang", "Minlie Huang"], "categories": ["cs.CR", "cs.CL", "cs.LG"], "comment": "19 pages", "summary": "Large Language Models (LLMs) are known to be vulnerable to jailbreak attacks.\nAn important observation is that, while different types of jailbreak attacks\ncan generate significantly different queries, they mostly result in similar\nresponses that are rooted in the same harmful knowledge (e.g., detailed steps\nto make a bomb). Consequently, unlearning-based approaches have been proposed\nto mitigate jailbreak attacks by directly removing harmful knowledge from the\nmodel. In this paper, we identify a novel ripple effect of unlearning, wherein\nLLMs can implicitly unlearn harmful knowledge that was not explicitly\nintroduced during the unlearning phase (e.g., a model unlearning the steps for\ntheft may also implicitly unlearn the steps for making a bomb). Through over\n100 experimental runs spanning multiple models, attack strategies, and defense\nmethods, we empirically validate this phenomenon, which makes unlearning-based\nmethods able to decrease the Attack Success Rate on unseen data from more than\n70% to less than 10% with only 100 training samples. Further analysis reveals\nthat the strong generalization ability of unlearning may stem from the\nintrinsic relatedness among harmful responses across harmful questions (e.g.,\nresponse patterns, shared steps and actions in response, and similarity among\ntheir learned representations in the LLM). We also discuss the potential\nlimitations of unlearning and the observed ripple effect. We hope our research\ncould contribute to a deeper understanding of unlearning. Our code is available\nat https://github.com/thu-coai/SafeUnlearning."}
{"id": "2410.01162", "pdf": "https://arxiv.org/pdf/2410.01162.pdf", "abs": "https://arxiv.org/abs/2410.01162", "title": "Frozen Large Language Models Can Perceive Paralinguistic Aspects of Speech", "authors": ["Wonjune Kang", "Junteng Jia", "Chunyang Wu", "Wei Zhou", "Egor Lakomkin", "Yashesh Gaur", "Leda Sari", "Suyoun Kim", "Ke Li", "Jay Mahadeokar", "Ozlem Kalinli"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to Interspeech 2025", "summary": "This work studies the capabilities of a large language model (LLM) to\nunderstand paralinguistic aspects of speech without fine-tuning its weights. We\nutilize an end-to-end system with a speech encoder, which is trained to produce\ntoken embeddings such that the LLM's response to an expressive speech prompt is\naligned with its response to a semantically matching text prompt that has also\nbeen conditioned on the user's speaking style. This framework enables the\nencoder to generate tokens that capture both linguistic and paralinguistic\ninformation and effectively convey them to the LLM, even when the LLM's weights\nremain completely frozen. To the best of our knowledge, our work is the first\nto explore how to induce a frozen LLM to understand more than just linguistic\ncontent from speech inputs in a general interaction setting. Experiments\ndemonstrate that our system is able to produce higher quality and more\nempathetic responses to expressive speech prompts compared to several\nbaselines."}
{"id": "2410.02429", "pdf": "https://arxiv.org/pdf/2410.02429.pdf", "abs": "https://arxiv.org/abs/2410.02429", "title": "IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models", "authors": ["Tuo An", "Yunjiao Zhou", "Han Zou", "Jianfei Yang"], "categories": ["cs.AI", "cs.CL"], "comment": "21 pages, 11 figures, under review", "summary": "Large Language Models (LLMs) excel in textual and visual tasks but often\nproduce outputs that defy physical laws when dealing with physical-world\nreasoning tasks. Inspired by human cognition, where perception is fundamental\nto reasoning, we explore augmenting LLMs with enhanced perception abilities\nusing Internet of Things (IoT) sensor data and pertinent knowledge for\nIoT-sensory task reasoning in the physical world. In this work, we\nsystematically study LLMs' capability to address real-world IoT-sensory tasks\nby augmenting their perception and knowledge base, and then propose a unified\nframework, IoT-LLM, to enhance such capability. In IoT-LLM, we customize three\nsteps for LLMs: preprocessing IoT data into formats amenable to LLMs, expanding\ntheir understanding via IoT-oriented retrieval-augmented generation based on\nin-context learning and activating their commonsense knowledge through\nchain-of-thought prompting and specialized role definitions. We design a new\nbenchmark comprising five real-world tasks with varying data types and\nreasoning complexities to evaluate the performance of IoT-LLM. Experimental\nresults on six LLMs reveal that IoT-LLM significantly improves the performance\nof IoT-sensory task reasoning of LLMs, with models like GPT-4o-mini showing a\n49.4% average improvement over previous methods."}
{"id": "2410.09083", "pdf": "https://arxiv.org/pdf/2410.09083.pdf", "abs": "https://arxiv.org/abs/2410.09083", "title": "Evaluating the Correctness of Inference Patterns Used by LLMs for Judgment", "authors": ["Lu Chen", "Yuxuan Huang", "Yixing Li", "Dongrui Liu", "Qihan Ren", "Shuai Zhao", "Kun Kuang", "Zilong Zheng", "Quanshi Zhang"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "This paper presents a method to analyze the inference patterns used by Large\nLanguage Models (LLMs) for judgment in a case study on legal LLMs, so as to\nidentify potential incorrect representations of the LLM, according to human\ndomain knowledge. Unlike traditional evaluations on language generation\nresults, we propose to evaluate the correctness of the detailed inference\npatterns of an LLM behind its seemingly correct outputs. To this end, we\nquantify the interactions between input phrases used by the LLM as primitive\ninference patterns, because recent theoretical achievements have proven several\nmathematical guarantees of the faithfulness of the interaction-based\nexplanation. We design a set of metrics to evaluate the detailed inference\npatterns of LLMs. Experiments show that even when the language generation\nresults appear correct, a significant portion of the inference patterns used by\nthe LLM for the legal judgment may represent misleading or irrelevant logic."}
{"id": "2410.10868", "pdf": "https://arxiv.org/pdf/2410.10868.pdf", "abs": "https://arxiv.org/abs/2410.10868", "title": "Large Continual Instruction Assistant", "authors": ["Jingyang Qiao", "Zhizhong Zhang", "Xin Tan", "Yanyun Qu", "Shouhong Ding", "Yuan Xie"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Continual Instruction Tuning (CIT) is adopted to continually instruct Large\nModels to follow human intent data by data. It is observed that existing\ngradient update would heavily destroy the performance on previous datasets\nduring CIT process. Instead, Exponential Moving Average (EMA), owns the ability\nto trace previous parameters, which can aid in decreasing forgetting.\nNonetheless, its stable balance weight fails to deal with the ever-changing\ndatasets, leading to the out-of-balance between plasticity and stability. In\nthis paper, we propose a general continual instruction tuning framework to\naddress the challenge. Starting from the trade-off prerequisite and EMA update,\nwe propose the plasticity and stability ideal condition. Based on Taylor\nexpansion in the loss function, we find the optimal balance weight can be\nautomatically determined by the gradients and learned parameters. Therefore, we\npropose a stable-plasticity balanced coefficient to avoid knowledge\ninterference. Based on the semantic similarity of the instructions, we can\ndetermine whether to retrain or expand the training parameters and allocate the\nmost suitable parameters for the testing instances. Extensive experiments\nacross multiple continual instruction tuning benchmarks demonstrate that our\napproach not only enhances anti-forgetting capabilities but also significantly\nimproves overall continual tuning performance. Our code is available at\nhttps://github.com/JingyangQiao/CoIN."}
{"id": "2410.17980", "pdf": "https://arxiv.org/pdf/2410.17980.pdf", "abs": "https://arxiv.org/abs/2410.17980", "title": "Scaling Stick-Breaking Attention: An Efficient Implementation and In-depth Study", "authors": ["Shawn Tan", "Songlin Yang", "Aaron Courville", "Rameswar Panda", "Yikang Shen"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The self-attention mechanism traditionally relies on the softmax operator,\nnecessitating positional embeddings like RoPE, or position biases to account\nfor token order. But current methods using still face length generalisation\nchallenges. We investigate an alternative attention mechanism based on the\nstick-breaking process in larger scale settings. The method works as follows:\nFor each token before the current, we determine a break point, which represents\nthe proportion of the stick, the weight of the attention, to allocate to the\ncurrent token. We repeat this on the remaining stick, until all tokens are\nallocated a weight, resulting in a sequence of attention weights. This process\nnaturally incorporates recency bias, which has linguistic motivations for\ngrammar parsing. We study the implications of replacing the conventional\nsoftmax-based attention mechanism with stick-breaking attention. We then\ndiscuss implementation of numerically stable stick-breaking attention and adapt\nFlash Attention to accommodate this mechanism. When used as a drop-in\nreplacement for current softmax+RoPE attention systems, we find that\nstick-breaking attention performs competitively with current methods on length\ngeneralisation and downstream tasks. Stick-breaking also performs well at\nlength generalisation, allowing a model trained with $2^{11}$ context window to\nperform well at $2^{14}$ with perplexity improvements."}
{"id": "2412.04756", "pdf": "https://arxiv.org/pdf/2412.04756.pdf", "abs": "https://arxiv.org/abs/2412.04756", "title": "ChatNVD: Advancing Cybersecurity Vulnerability Assessment with Large Language Models", "authors": ["Shivansh Chopra", "Hussain Ahmad", "Diksha Goel", "Claudia Szabo"], "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "The increasing frequency and sophistication of cybersecurity vulnerabilities\nin software systems underscores the need for more robust and effective\nvulnerability assessment methods. However, existing approaches often rely on\nhighly technical and abstract frameworks, which hinder understanding and\nincrease the likelihood of exploitation, resulting in severe cyberattacks. In\nthis paper, we introduce ChatNVD, a support tool powered by Large Language\nModels (LLMs) that leverages the National Vulnerability Database (NVD) to\ngenerate accessible, context-rich summaries of software vulnerabilities. We\ndevelop three variants of ChatNVD, utilizing three prominent LLMs: GPT-4o Mini\nby OpenAI, LLaMA 3 by Meta, and Gemini 1.5 Pro by Google. To evaluate their\nperformance, we conduct a comparative evaluation focused on their ability to\nidentify, interpret, and explain software vulnerabilities. Our results\ndemonstrate that GPT-4o Mini outperforms the other models, achieving over 92%\naccuracy and the lowest error rates, making it the most reliable option for\nreal-world vulnerability assessment."}
{"id": "2412.06559", "pdf": "https://arxiv.org/pdf/2412.06559.pdf", "abs": "https://arxiv.org/abs/2412.06559", "title": "ProcessBench: Identifying Process Errors in Mathematical Reasoning", "authors": ["Chujie Zheng", "Zhenru Zhang", "Beichen Zhang", "Runji Lin", "Keming Lu", "Bowen Yu", "Dayiheng Liu", "Jingren Zhou", "Junyang Lin"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "ACL 2025", "summary": "As language models regularly make mistakes when solving math problems,\nautomated identification of errors in the reasoning process becomes\nincreasingly significant for their scalable oversight. In this paper, we\nintroduce ProcessBench for measuring the ability to identify erroneous steps in\nmathematical reasoning. It consists of 3,400 test cases, primarily focused on\ncompetition- and Olympiad-level math problems. Each test case contains a\nstep-by-step solution with error location annotated by human experts. Models\nare required to identify the earliest step that contains an error, or conclude\nthat all steps are correct. We conduct extensive evaluation on ProcessBench,\ninvolving two types of models: process reward models (PRMs) and critic models,\nwhere for the latter we prompt general language models to critique each\nsolution step by step. We draw two main observations: (1) Existing PRMs\ntypically fail to generalize to more challenging math problems beyond GSM8K and\nMATH. They underperform both critic models (i.e., prompted general language\nmodels) and our own trained PRM that is straightforwardly fine-tuned on the\nPRM800K dataset. (2) The best open-source model, QwQ-32B-Preview, has\ndemonstrated the critique capability competitive with the proprietary model\nGPT-4o, despite that it still lags behind the reasoning-specialized o1-mini. We\nhope ProcessBench can foster future research in reasoning process assessment,\npaving the way toward scalable oversight of language models."}
{"id": "2501.08828", "pdf": "https://arxiv.org/pdf/2501.08828.pdf", "abs": "https://arxiv.org/abs/2501.08828", "title": "MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents", "authors": ["Kuicai Dong", "Yujing Chang", "Xin Deik Goh", "Dexun Li", "Ruiming Tang", "Yong Liu"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CV"], "comment": "https://huggingface.co/MMDocIR", "summary": "Multimodal document retrieval aims to identify and retrieve various forms of\nmultimodal content, such as figures, tables, charts, and layout information\nfrom extensive documents. Despite its increasing popularity, there is a notable\nlack of a comprehensive and robust benchmark to effectively evaluate the\nperformance of systems in such tasks. To address this gap, this work introduces\na new benchmark, named MMDocIR, that encompasses two distinct tasks: page-level\nand layout-level retrieval. The former evaluates the performance of identifying\nthe most relevant pages within a long document, while the later assesses the\nability of detecting specific layouts, providing a more fine-grained measure\nthan whole-page analysis. A layout refers to a variety of elements, including\ntextual paragraphs, equations, figures, tables, or charts. The MMDocIR\nbenchmark comprises a rich dataset featuring 1,685 questions annotated by\nexperts and 173,843 questions with bootstrapped labels, making it a valuable\nresource in multimodal document retrieval for both training and evaluation.\nThrough rigorous experiments, we demonstrate that (i) visual retrievers\nsignificantly outperform their text counterparts, (ii) MMDocIR training set\neffectively enhances the performance of multimodal document retrieval and (iii)\ntext retrievers leveraging VLM-text significantly outperforms retrievers\nrelying on OCR-text. Our dataset is available at\nhttps://mmdocrag.github.io/MMDocIR/."}
{"id": "2501.12368", "pdf": "https://arxiv.org/pdf/2501.12368.pdf", "abs": "https://arxiv.org/abs/2501.12368", "title": "InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model", "authors": ["Yuhang Zang", "Xiaoyi Dong", "Pan Zhang", "Yuhang Cao", "Ziyu Liu", "Shengyuan Ding", "Shenxi Wu", "Yubo Ma", "Haodong Duan", "Wenwei Zhang", "Kai Chen", "Dahua Lin", "Jiaqi Wang"], "categories": ["cs.CV", "cs.CL"], "comment": "ACL 2025 Findings", "summary": "Despite the promising performance of Large Vision Language Models (LVLMs) in\nvisual understanding, they occasionally generate incorrect outputs. While\nreward models (RMs) with reinforcement learning or test-time scaling offer the\npotential for improving generation quality, a critical gap remains: publicly\navailable multi-modal RMs for LVLMs are scarce, and the implementation details\nof proprietary models are often unclear. We bridge this gap with\nInternLM-XComposer2.5-Reward (IXC-2.5-Reward), a simple yet effective\nmulti-modal reward model that aligns LVLMs with human preferences. To ensure\nthe robustness and versatility of IXC-2.5-Reward, we set up a high-quality\nmulti-modal preference corpus spanning text, image, and video inputs across\ndiverse domains, such as instruction following, general understanding,\ntext-rich documents, mathematical reasoning, and video understanding.\nIXC-2.5-Reward achieves excellent results on the latest multi-modal reward\nmodel benchmark and shows competitive performance on text-only reward model\nbenchmarks. We further demonstrate three key applications of IXC-2.5-Reward:\n(1) Providing a supervisory signal for RL training. We integrate IXC-2.5-Reward\nwith Proximal Policy Optimization (PPO) yields IXC-2.5-Chat, which shows\nconsistent improvements in instruction following and multi-modal open-ended\ndialogue; (2) Selecting the best response from candidate responses for\ntest-time scaling; and (3) Filtering outlier or noisy samples from existing\nimage and video instruction tuning training data. To ensure reproducibility and\nfacilitate further research, we have open-sourced all model weights and\ntraining recipes at\nhttps://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-Reward"}
{"id": "2502.00198", "pdf": "https://arxiv.org/pdf/2502.00198.pdf", "abs": "https://arxiv.org/abs/2502.00198", "title": "Fairshare Data Pricing via Data Valuation for Large Language Models", "authors": ["Luyang Zhang", "Cathy Jiao", "Beibei Li", "Chenyan Xiong"], "categories": ["cs.GT", "cs.CL"], "comment": null, "summary": "Training data is the backbone of large language models (LLMs), yet today's\ndata markets often operate under exploitative pricing -- sourcing data from\nmarginalized groups with little pay or recognition. This paper introduces a\ntheoretical framework for LLM data markets, modeling the strategic interactions\nbetween buyers (LLM builders) and sellers (human annotators). We begin with\ntheoretical and empirical analysis showing how exploitative pricing drives\nhigh-quality sellers out of the market, degrading data quality and long-term\nmodel performance. Then we introduce fairshare, a pricing mechanism grounded in\ndata valuation that quantifies each data's contribution. It aligns incentives\nby sustaining seller participation and optimizing utility for both buyers and\nsellers. Theoretically, we show that fairshare yields mutually optimal\noutcomes: maximizing long-term buyer utility and seller profit while sustaining\nmarket participation. Empirically when training open-source LLMs on complex NLP\ntasks, including math problems, medical diagnosis, and physical reasoning,\nfairshare boosts seller earnings and ensures a stable supply of high-quality\ndata, while improving buyers' performance-per-dollar and long-term welfare. Our\nfindings offer a concrete path toward fair, transparent, and economically\nsustainable data markets for LLM. Our code will be open sourced."}
{"id": "2502.02145", "pdf": "https://arxiv.org/pdf/2502.02145.pdf", "abs": "https://arxiv.org/abs/2502.02145", "title": "From Words to Collisions: LLM-Guided Evaluation and Adversarial Generation of Safety-Critical Driving Scenarios", "authors": ["Yuan Gao", "Mattia Piccinini", "Korbinian Moller", "Johannes Betz"], "categories": ["cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "Ensuring the safety of autonomous vehicles requires virtual scenario-based\ntesting, which depends on the robust evaluation and generation of\nsafety-critical scenarios. So far, researchers have used scenario-based testing\nframeworks that rely heavily on handcrafted scenarios as safety metrics. To\nreduce the effort of human interpretation and overcome the limited scalability\nof these approaches, we combine Large Language Models (LLMs) with structured\nscenario parsing and prompt engineering to automatically evaluate and generate\nsafety-critical driving scenarios. We introduce Cartesian and Ego-centric\nprompt strategies for scenario evaluation, and an adversarial generation module\nthat modifies trajectories of risk-inducing vehicles (ego-attackers) to create\ncritical scenarios. We validate our approach using a 2D simulation framework\nand multiple pre-trained LLMs. The results show that the evaluation module\neffectively detects collision scenarios and infers scenario safety. Meanwhile,\nthe new generation module identifies high-risk agents and synthesizes\nrealistic, safety-critical scenarios. We conclude that an LLM equipped with\ndomain-informed prompting techniques can effectively evaluate and generate\nsafety-critical driving scenarios, reducing dependence on handcrafted metrics.\nWe release our open-source code and scenarios at:\nhttps://github.com/TUM-AVS/From-Words-to-Collisions."}
{"id": "2502.04463", "pdf": "https://arxiv.org/pdf/2502.04463.pdf", "abs": "https://arxiv.org/abs/2502.04463", "title": "Training Language Models to Reason Efficiently", "authors": ["Daman Arora", "Andrea Zanette"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Scaling model size and training data has led to great advances in the\nperformance of Large Language Models (LLMs). However, the diminishing returns\nof this approach necessitate alternative methods to improve model capabilities,\nparticularly in tasks requiring advanced reasoning. Large reasoning models,\nwhich leverage long chain-of-thoughts, bring unprecedented breakthroughs in\nproblem-solving capabilities but at a substantial deployment cost associated to\nlonger generations. Reducing inference costs is crucial for the economic\nfeasibility, user experience, and environmental sustainability of these models.\n  In this work, we propose to train large reasoning models to reason\nefficiently. More precisely, we use reinforcement learning (RL) to train\nreasoning models to dynamically allocate inference-time compute based on task\ncomplexity. Our method incentivizes models to minimize unnecessary\ncomputational overhead while maintaining accuracy, thereby achieving\nsubstantial efficiency gains. It enables the derivation of a family of\nreasoning models with varying efficiency levels, controlled via a single\nhyperparameter. Experiments on two open-weight large reasoning models\ndemonstrate significant reductions in inference cost while preserving most of\nthe accuracy."}
{"id": "2502.11163", "pdf": "https://arxiv.org/pdf/2502.11163.pdf", "abs": "https://arxiv.org/abs/2502.11163", "title": "VLMs as GeoGuessr Masters: Exceptional Performance, Hidden Biases, and Privacy Risks", "authors": ["Jingyuan Huang", "Jen-tse Huang", "Ziyi Liu", "Xiaoyuan Liu", "Wenxuan Wang", "Jieyu Zhao"], "categories": ["cs.CV", "cs.CL"], "comment": "8 pages of main text; 5 pages of appendix", "summary": "Visual-Language Models (VLMs) have shown remarkable performance across\nvarious tasks, particularly in recognizing geographic information from images.\nHowever, VLMs still show regional biases in this task. To systematically\nevaluate these issues, we introduce a benchmark consisting of 1,200 images\npaired with detailed geographic metadata. Evaluating four VLMs, we find that\nwhile these models demonstrate the ability to recognize geographic information\nfrom images, achieving up to 53.8% accuracy in city prediction, they exhibit\nsignificant biases. Specifically, performance is substantially higher for\neconomically developed and densely populated regions compared to less developed\n(-12.5%) and sparsely populated (-17.0%) areas. Moreover, regional biases of\nfrequently over-predicting certain locations remain. For instance, they\nconsistently predict Sydney for images taken in Australia, shown by the low\nentropy scores for these countries. The strong performance of VLMs also raises\nprivacy concerns, particularly for users who share images online without the\nintent of being identified. Our code and dataset are publicly available at\nhttps://github.com/uscnlp-lime/FairLocator."}
{"id": "2502.12466", "pdf": "https://arxiv.org/pdf/2502.12466.pdf", "abs": "https://arxiv.org/abs/2502.12466", "title": "EquiBench: Benchmarking Large Language Models' Understanding of Program Semantics via Equivalence Checking", "authors": ["Anjiang Wei", "Jiannan Cao", "Ran Li", "Hongyu Chen", "Yuhui Zhang", "Ziheng Wang", "Yuan Liu", "Thiago S. F. X. Teixeira", "Diyi Yang", "Ke Wang", "Alex Aiken"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.PL", "cs.SE"], "comment": null, "summary": "As large language models (LLMs) become integral to code-related tasks, a\ncentral question emerges: do LLMs truly understand program execution semantics?\nWe introduce EquiBench, a new benchmark for evaluating LLMs through equivalence\nchecking, i.e., determining whether two programs produce identical outputs for\nall possible inputs. Unlike prior code generation benchmarks, this task\ndirectly tests a model's understanding of code execution semantics. EquiBench\nconsists of 2400 program pairs across four languages and six categories. These\npairs are generated through program analysis, compiler scheduling, and\nsuperoptimization, ensuring high-confidence labels, nontrivial difficulty, and\nfull automation. The transformations span syntactic edits, structural\nmodifications, and algorithmic changes, covering a broad spectrum of semantic\nvariation. We evaluate 19 state-of-the-art LLMs and find that in the most\nchallenging categories, the best accuracies are 63.8% and 76.2%, only modestly\nabove the 50% random baseline. Further analysis reveals that models often rely\non syntactic similarity rather than exhibiting robust reasoning over execution\nsemantics, highlighting fundamental limitations."}
{"id": "2502.12623", "pdf": "https://arxiv.org/pdf/2502.12623.pdf", "abs": "https://arxiv.org/abs/2502.12623", "title": "DeepResonance: Enhancing Multimodal Music Understanding via Music-centric Multi-way Instruction Tuning", "authors": ["Zhuoyuan Mao", "Mengjie Zhao", "Qiyu Wu", "Hiromi Wakaki", "Yuki Mitsufuji"], "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.MM", "eess.AS"], "comment": null, "summary": "Recent advancements in music large language models (LLMs) have significantly\nimproved music understanding tasks, which involve the model's ability to\nanalyze and interpret various musical elements. These improvements primarily\nfocused on integrating both music and text inputs. However, the potential of\nincorporating additional modalities such as images, videos and textual music\nfeatures to enhance music understanding remains unexplored. To bridge this gap,\nwe propose DeepResonance, a multimodal music understanding LLM fine-tuned via\nmulti-way instruction tuning with multi-way aligned music, text, image, and\nvideo data. To this end, we construct Music4way-MI2T, Music4way-MV2T, and\nMusic4way-Any2T, three 4-way training and evaluation datasets designed to\nenable DeepResonance to integrate both visual and textual music feature\ncontent. We also introduce multi-sampled ImageBind embeddings and a pre-LLM\nfusion Transformer to enhance modality fusion prior to input into text LLMs,\ntailoring DeepResonance for multi-way instruction tuning. Our model achieves\nstate-of-the-art performances across six music understanding tasks,\nhighlighting the benefits of the auxiliary modalities and the structural\nsuperiority of DeepResonance. We plan to open-source the models and the newly\nconstructed datasets."}
{"id": "2503.06794", "pdf": "https://arxiv.org/pdf/2503.06794.pdf", "abs": "https://arxiv.org/abs/2503.06794", "title": "Does Acceleration Cause Hidden Instability in Vision Language Models? Uncovering Instance-Level Divergence Through a Large-Scale Empirical Study", "authors": ["Yizheng Sun", "Hao Li", "Chang Xu", "Hongpeng Zhou", "Chenghua Lin", "Riza Batista-Navarro", "Jingyuan Sun"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Vision-Language Models (VLMs) are powerful yet computationally intensive for\nwidespread practical deployments. To address such challenge without costly\nre-training, post-training acceleration techniques like quantization and token\nreduction are extensively explored. However, current acceleration evaluations\nprimarily target minimal overall performance degradation, overlooking a crucial\nquestion: does the accelerated model still give the same answers to the same\nquestions as it did before acceleration? This is vital for stability-centered\nindustrial applications where consistently correct answers for specific, known\nsituations are paramount, such as in AI-based disease diagnosis. We\nsystematically investigate this for accelerated VLMs, testing four leading\nmodels (LLaVA-1.5, LLaVA-Next, Qwen2-VL, Qwen2.5-VL) with eight acceleration\nmethods on ten multi-modal benchmarks. Our findings are stark: despite minimal\naggregate performance drops, accelerated models changed original answers up to\n20% of the time. Critically, up to 6.5% of these changes converted correct\nanswers to incorrect. Input perturbations magnified these inconsistencies, and\nthe trend is confirmed by case studies with the medical VLM LLaVA-Med. This\nresearch reveals a significant oversight in VLM acceleration, stressing an\nurgent need for instance-level stability checks to ensure trustworthy\nreal-world deployment."}
{"id": "2503.07575", "pdf": "https://arxiv.org/pdf/2503.07575.pdf", "abs": "https://arxiv.org/abs/2503.07575", "title": "VisBias: Measuring Explicit and Implicit Social Biases in Vision Language Models", "authors": ["Jen-tse Huang", "Jiantong Qin", "Jianping Zhang", "Youliang Yuan", "Wenxuan Wang", "Jieyu Zhao"], "categories": ["cs.CV", "cs.CL"], "comment": "8 pages of main text; 9 pages of appendix", "summary": "This research investigates both explicit and implicit social biases exhibited\nby Vision-Language Models (VLMs). The key distinction between these bias types\nlies in the level of awareness: explicit bias refers to conscious, intentional\nbiases, while implicit bias operates subconsciously. To analyze explicit bias,\nwe directly pose questions to VLMs related to gender and racial differences:\n(1) Multiple-choice questions based on a given image (e.g., \"What is the\neducation level of the person in the image?\") (2) Yes-No comparisons using two\nimages (e.g., \"Is the person in the first image more educated than the person\nin the second image?\") For implicit bias, we design tasks where VLMs assist\nusers but reveal biases through their responses: (1) Image description tasks:\nModels are asked to describe individuals in images, and we analyze disparities\nin textual cues across demographic groups. (2) Form completion tasks: Models\ndraft a personal information collection form with 20 attributes, and we examine\ncorrelations among selected attributes for potential biases. We evaluate\nGemini-1.5, GPT-4V, GPT-4o, LLaMA-3.2-Vision and LLaVA-v1.6. Our code and data\nare publicly available at https://github.com/uscnlp-lime/VisBias."}
{"id": "2503.10542", "pdf": "https://arxiv.org/pdf/2503.10542.pdf", "abs": "https://arxiv.org/abs/2503.10542", "title": "Language Models, Graph Searching, and Supervision Adulteration: When More Supervision is Less and How to Make More More", "authors": ["Arvid Frydenlund"], "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7; I.2.8; I.5.0"], "comment": "ACL 2025 Main. A camera-ready version will follow in a few weeks. A\n  reduced version of this work has was also accepted to the Workshop on\n  Spurious Correlation and Shortcut Learning: Foundations and Solutions (SCSL)\n  at ICLR 2025", "summary": "This work concerns the path-star task, a minimal example of searching over a\ngraph. The graph, $G$, is star-shaped with $D$ arms radiating from a start\nnode, $s$. A language model (LM) is given $G$, $s$, and a target node $t$,\nwhich ends one of the arms and is tasked with generating the arm containing\n$t$. The minimal nature of this task means only a single choice needs to be\nmade: which of the $D$ arms contains $t$?\n  Decoder-only LMs fail to solve this elementary task above $1/D$ chance due to\na learned shortcut that absorbs training supervision. We show how this\npathology is caused by excess supervision and we present a series of solutions\ndemonstrating that the task is solvable via decoder-only LMs. We find that the\ntask's minimal nature causes its difficulty, as it prevents task decomposition.\nOur solutions provide insight into the pathology and its implications for LMs\ntrained via next-token prediction."}
{"id": "2503.14232", "pdf": "https://arxiv.org/pdf/2503.14232.pdf", "abs": "https://arxiv.org/abs/2503.14232", "title": "CRCE: Coreference-Retention Concept Erasure in Text-to-Image Diffusion Models", "authors": ["Yuyang Xue", "Edward Moroshko", "Feng Chen", "Jingyu Sun", "Steven McDonagh", "Sotirios A. Tsaftaris"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Text-to-Image diffusion models can produce undesirable content that\nnecessitates concept erasure. However, existing methods struggle with\nunder-erasure, leaving residual traces of targeted concepts, or over-erasure,\nmistakenly eliminating unrelated but visually similar concepts. To address\nthese limitations, we introduce CRCE, a novel concept erasure framework that\nleverages Large Language Models to identify both semantically related concepts\nthat should be erased alongside the target and distinct concepts that should be\npreserved. By explicitly modelling coreferential and retained concepts\nsemantically, CRCE enables more precise concept removal, without unintended\nerasure. Experiments demonstrate that CRCE outperforms existing methods on\ndiverse erasure tasks, including real-world object, person identities, and\nabstract intellectual property characteristics. The constructed dataset\nCorefConcept and the source code will be release upon acceptance."}
{"id": "2503.14476", "pdf": "https://arxiv.org/pdf/2503.14476.pdf", "abs": "https://arxiv.org/abs/2503.14476", "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale", "authors": ["Qiying Yu", "Zheng Zhang", "Ruofei Zhu", "Yufeng Yuan", "Xiaochen Zuo", "Yu Yue", "Weinan Dai", "Tiantian Fan", "Gaohong Liu", "Lingjun Liu", "Xin Liu", "Haibin Lin", "Zhiqi Lin", "Bole Ma", "Guangming Sheng", "Yuxuan Tong", "Chi Zhang", "Mofan Zhang", "Wang Zhang", "Hang Zhu", "Jinhua Zhu", "Jiaze Chen", "Jiangjie Chen", "Chengyi Wang", "Hongli Yu", "Yuxuan Song", "Xiangpeng Wei", "Hao Zhou", "Jingjing Liu", "Wei-Ying Ma", "Ya-Qin Zhang", "Lin Yan", "Mu Qiao", "Yonghui Wu", "Mingxuan Wang"], "categories": ["cs.LG", "cs.CL"], "comment": "Project Page: https://dapo-sia.github.io/", "summary": "Inference scaling empowers LLMs with unprecedented reasoning ability, with\nreinforcement learning as the core technique to elicit complex reasoning.\nHowever, key technical details of state-of-the-art reasoning LLMs are concealed\n(such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the\ncommunity still struggles to reproduce their RL training results. We propose\nthe $\\textbf{D}$ecoupled Clip and $\\textbf{D}$ynamic s$\\textbf{A}$mpling\n$\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{DAPO}$) algorithm, and\nfully open-source a state-of-the-art large-scale RL system that achieves 50\npoints on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that\nwithhold training details, we introduce four key techniques of our algorithm\nthat make large-scale LLM RL a success. In addition, we open-source our\ntraining code, which is built on the verl framework, along with a carefully\ncurated and processed dataset. These components of our open-source system\nenhance reproducibility and support future research in large-scale LLM RL."}
{"id": "2503.16505", "pdf": "https://arxiv.org/pdf/2503.16505.pdf", "abs": "https://arxiv.org/abs/2503.16505", "title": "Scalable Evaluation of Online Facilitation Strategies via Synthetic Simulation of Discussions", "authors": ["Dimitris Tsirmpas", "Ion Androutsopoulos", "John Pavlopoulos"], "categories": ["cs.HC", "cs.CL", "cs.LG", "68T50", "I.2.7"], "comment": "19 pages, 3 tables, 12 figures", "summary": "Limited large-scale evaluations exist for facilitation strategies of online\ndiscussions due to significant costs associated with human involvement. An\neffective solution is synthetic discussion simulations using Large Language\nModels (LLMs) to create initial pilot experiments. We propose a simple,\ngeneralizable, LLM-driven methodology to prototype the development of LLM\nfacilitators, and produce high-quality synthetic data without human\ninvolvement. We use our methodology to test whether current facilitation\nstrategies can improve the performance of LLM facilitators. We find that, while\nLLM facilitators significantly improve synthetic discussions, there is no\nevidence that the application of more elaborate facilitation strategies\nproposed in modern Social Science research lead to further improvements in\ndiscussion quality, compared to more basic approaches. Additionally, we find\nthat small LLMs (such as Mistral Nemo 12B) can perform comparably to larger\nmodels (such as LLaMa 70B), and that special instructions must be used for\ninstruction-tuned models to induce toxicity in synthetic discussions. We\nconfirm that each component of our methodology contributes substantially to\nhigh quality data via an ablation study. We release an open-source framework,\n\"SynDisco\" (pip install syndisco), which implements our methodology. We also\nrelease the \"Virtual Moderation Dataset\"\n(https://paperswithcode.com/dataset/vmd), a large, publicly available dataset\ncontaining LLM-generated and LLM-annotated discussions using multiple\nopen-source LLMs."}
{"id": "2504.01281", "pdf": "https://arxiv.org/pdf/2504.01281.pdf", "abs": "https://arxiv.org/abs/2504.01281", "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding", "authors": ["Sakhinana Sagar Srinivas", "Akash Das", "Shivam Gupta", "Venkataramana Runkana"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications."}
{"id": "2504.14945", "pdf": "https://arxiv.org/pdf/2504.14945.pdf", "abs": "https://arxiv.org/abs/2504.14945", "title": "Learning to Reason under Off-Policy Guidance", "authors": ["Jianhao Yan", "Yafu Li", "Zican Hu", "Zhi Wang", "Ganqu Cui", "Xiaoye Qu", "Yu Cheng", "Yue Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Recent advances in large reasoning models (LRMs) demonstrate that\nsophisticated behaviors such as multi-step reasoning and self-reflection can\nemerge via reinforcement learning with verifiable rewards~(\\textit{RLVR}).\nHowever, existing \\textit{RLVR} approaches are inherently ``on-policy'',\nlimiting learning to a model's own outputs and failing to acquire reasoning\nabilities beyond its initial capabilities. To address this issue, we introduce\n\\textbf{LUFFY} (\\textbf{L}earning to reason \\textbf{U}nder\no\\textbf{FF}-polic\\textbf{Y} guidance), a framework that augments \\textit{RLVR}\nwith off-policy reasoning traces. LUFFY dynamically balances imitation and\nexploration by combining off-policy demonstrations with on-policy rollouts\nduring training. Specifically, LUFFY combines the Mixed-Policy GRPO framework,\nwhich has a theoretically guaranteed convergence rate, alongside policy shaping\nvia regularized importance sampling to avoid superficial and rigid imitation\nduring mixed-policy training. Compared with previous RLVR methods, LUFFY\nachieves an over \\textbf{+6.4} average gain across six math benchmarks and an\nadvantage of over \\textbf{+6.2} points in out-of-distribution tasks. Most\nsignificantly, we show that LUFFY successfully trains weak models in scenarios\nwhere on-policy RLVR completely fails. These results provide compelling\nevidence that LUFFY transcends the fundamental limitations of on-policy RLVR\nand demonstrates the great potential of utilizing off-policy guidance in RLVR."}
{"id": "2504.17821", "pdf": "https://arxiv.org/pdf/2504.17821.pdf", "abs": "https://arxiv.org/abs/2504.17821", "title": "VideoVista-CulturalLingo: 360$^\\circ$ Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension", "authors": ["Xinyu Chen", "Yunxin Li", "Haoyuan Shi", "Baotian Hu", "Wenhan Luo", "Yaowei Wang", "Min Zhang"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Assessing the video comprehension capabilities of multimodal AI systems can\neffectively measure their understanding and reasoning abilities. Most video\nevaluation benchmarks are limited to a single language, typically English, and\npredominantly feature videos rooted in Western cultural contexts. In this\npaper, we present VideoVista-CulturalLingo, the first video evaluation\nbenchmark designed to bridge cultural, linguistic, and domain divide in video\ncomprehension. Our work differs from existing benchmarks in the following ways:\n1) Cultural diversity, incorporating cultures from China, North America, and\nEurope; 2) Multi-linguistics, with questions presented in Chinese and\nEnglish-two of the most widely spoken languages; and 3) Broad domain, featuring\nvideos sourced from hundreds of human-created domains. VideoVista-CulturalLingo\ncontains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent\nopen-source or proprietary video large models. From the experiment results, we\nobserve that: 1) Existing models perform worse on Chinese-centric questions\nthan Western-centric ones, particularly those related to Chinese history; 2)\nCurrent open-source models still exhibit limitations in temporal understanding,\nespecially in the Event Localization task, achieving a maximum score of only\n45.2%; 3) Mainstream models demonstrate strong performance in general\nscientific questions, while open-source models demonstrate weak performance in\nmathematics."}
{"id": "2504.21751", "pdf": "https://arxiv.org/pdf/2504.21751.pdf", "abs": "https://arxiv.org/abs/2504.21751", "title": "CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code Generation", "authors": ["Sizhe Wang", "Zhengren Wang", "Dongsheng Ma", "Yongan Yu", "Rui Ling", "Zhiyu Li", "Feiyu Xiong", "Wentao Zhang"], "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "Modern software development demands code that is maintainable, testable, and\nscalable by organizing the implementation into modular components with\niterative reuse of existing codes. We formalize this iterative, multi-turn\nparadigm as codeflow and introduce CodeFlowBench, the first benchmark designed\nto comprehensively evaluate LLMs' ability to perform codeflow, namely\nimplementing new functionality by reusing existing functions over multiple\nturns. CodeFlowBench comprises 5,258 problems from Codeforces and is\ncontinuously updated via an automated pipeline, which decomposes each problem\ninto subproblems with unit tests based on dependency tree analysis and dataflow\nanalysis. We further propose a novel evaluation framework featured dual\nassessment protocol and structural metrics derived from dependency trees.\nExtensive experiments on 16 popular LLMs reveal significant performance\ndegradation in multi-turn scenarios. For instance, o1-mini retains only 20.8%\nPass@1 in multi-turn scenario versus 37.8% in single-turn scenario. More\nfine-grained analysis illustrates that model performance inversely correlates\nwith dependency complexity. These findings not only highlight the critical\nchallenges for supporting real-world workflows, but also establish\nCodeFlowBench as an essential tool for advancing code generation research."}
{"id": "2505.06993", "pdf": "https://arxiv.org/pdf/2505.06993.pdf", "abs": "https://arxiv.org/abs/2505.06993", "title": "Technical Report: Quantifying and Analyzing the Generalization Power of a DNN", "authors": ["Yuxuan He", "Junpeng Zhang", "Lei Cheng", "Hongyuan Zhang", "Quanshi Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper proposes a new perspective for analyzing the generalization power\nof deep neural networks (DNNs), i.e., directly disentangling and analyzing the\ndynamics of generalizable and non-generalizable interaction encoded by a DNN\nthrough the training process. Specifically, this work builds upon the recent\ntheoretical achievement in explainble AI, which proves that the detailed\ninference logic of DNNs can be can be strictly rewritten as a small number of\nAND-OR interaction patterns. Based on this, we propose an efficient method to\nquantify the generalization power of each interaction, and we discover a\ndistinct three-phase dynamics of the generalization power of interactions\nduring training. In particular, the early phase of training typically removes\nnoisy and non-generalizable interactions and learns simple and generalizable\nones. The second and the third phases tend to capture increasingly complex\ninteractions that are harder to generalize. Experimental results verify that\nthe learning of non-generalizable interactions is the the direct cause for the\ngap between the training and testing losses."}
{"id": "2505.07558", "pdf": "https://arxiv.org/pdf/2505.07558.pdf", "abs": "https://arxiv.org/abs/2505.07558", "title": "Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models", "authors": ["Rei Higuchi", "Taiji Suzuki"], "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": null, "summary": "Aligning large language models (LLMs) with human preferences is crucial for\nsafe deployment, yet existing methods assume specific preference models like\nBradley-Terry model. This assumption leads to statistical inconsistency, where\nmore data doesn't guarantee convergence to true human preferences. To address\nthis critical gap, we introduce a novel alignment method Direct Density Ratio\nOptimization (DDRO). DDRO directly estimates the density ratio between\npreferred and unpreferred output distributions, circumventing the need for\nexplicit human preference modeling. We theoretically prove that DDRO is\nstatistically consistent, ensuring convergence to the true preferred\ndistribution as the data size grows, regardless of the underlying preference\nstructure. Experiments demonstrate that DDRO achieves superior performance\ncompared to existing methods on many major benchmarks. DDRO unlocks the\npotential for truly data-driven alignment, paving the way for more reliable and\nhuman-aligned LLMs."}
{"id": "2505.12442", "pdf": "https://arxiv.org/pdf/2505.12442.pdf", "abs": "https://arxiv.org/abs/2505.12442", "title": "IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems", "authors": ["Liwen Wang", "Wenxuan Wang", "Shuai Wang", "Zongjie Li", "Zhenlan Ji", "Zongyi Lyu", "Daoyuan Wu", "Shing-Chi Cheung"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has led to the\nemergence of Multi-Agent Systems (MAS) to perform complex tasks through\ncollaboration. However, the intricate nature of MAS, including their\narchitecture and agent interactions, raises significant concerns regarding\nintellectual property (IP) protection. In this paper, we introduce MASLEAK, a\nnovel attack framework designed to extract sensitive information from MAS\napplications. MASLEAK targets a practical, black-box setting, where the\nadversary has no prior knowledge of the MAS architecture or agent\nconfigurations. The adversary can only interact with the MAS through its public\nAPI, submitting attack query $q$ and observing outputs from the final agent.\nInspired by how computer worms propagate and infect vulnerable network hosts,\nMASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain\nresponses from each MAS agent that reveal a full set of proprietary components,\nincluding the number of agents, system topology, system prompts, task\ninstructions, and tool usages. We construct the first synthetic dataset of MAS\napplications with 810 applications and also evaluate MASLEAK against real-world\nMAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in\nextracting MAS IP, with an average attack success rate of 87% for system\nprompts and task instructions, and 92% for system architecture in most cases.\nWe conclude by discussing the implications of our findings and the potential\ndefenses."}
{"id": "2505.12938", "pdf": "https://arxiv.org/pdf/2505.12938.pdf", "abs": "https://arxiv.org/abs/2505.12938", "title": "Leveraging LLM Inconsistency to Boost Pass@k Performance", "authors": ["Uri Dalal", "Meirav Segal", "Zvika Ben-Haim", "Dan Lahav", "Omer Nevo"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) achieve impressive abilities in numerous\ndomains, but exhibit inconsistent performance in response to minor input\nchanges. Rather than view this as a drawback, in this paper we introduce a\nnovel method for leveraging models' inconsistency to boost Pass@k performance.\nSpecifically, we present a \"Variator\" agent that generates k variants of a\ngiven task and submits one candidate solution for each one. Our variant\ngeneration approach is applicable to a wide range of domains as it is task\nagnostic and compatible with free-form inputs. We demonstrate the efficacy of\nour agent theoretically using a probabilistic model of the inconsistency\neffect, and show empirically that it outperforms the baseline on the APPS\ndataset. Furthermore, we establish that inconsistency persists even in frontier\nreasoning models across coding and cybersecurity domains, suggesting our method\nis likely to remain relevant for future model generations."}
{"id": "2505.13028", "pdf": "https://arxiv.org/pdf/2505.13028.pdf", "abs": "https://arxiv.org/abs/2505.13028", "title": "Evaluating the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset", "authors": ["Sayon Palit", "Daniel Woods"], "categories": ["cs.CR", "cs.AI", "cs.CL", "F.2.2; I.2.7; F.2.2; I.2.7; F.2.2; I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly integrated into critical\nsystems in industries like healthcare and finance. Users can often submit\nqueries to LLM-enabled chatbots, some of which can enrich responses with\ninformation retrieved from internal databases storing sensitive data. This\ngives rise to a range of attacks in which a user submits a malicious query and\nthe LLM-system outputs a response that creates harm to the owner, such as\nleaking internal data or creating legal liability by harming a third-party.\nWhile security tools are being developed to counter these threats, there is\nlittle formal evaluation of their effectiveness and usability. This study\naddresses this gap by conducting a thorough comparative analysis of LLM\nsecurity tools. We identified 13 solutions (9 closed-source, 4 open-source),\nbut only 7 were evaluated due to a lack of participation by proprietary model\nowners.To evaluate, we built a benchmark dataset of malicious prompts, and\nevaluate these tools performance against a baseline LLM model\n(ChatGPT-3.5-Turbo). Our results show that the baseline model has too many\nfalse positives to be used for this task. Lakera Guard and ProtectAI LLM Guard\nemerged as the best overall tools showcasing the tradeoff between usability and\nperformance. The study concluded with recommendations for greater transparency\namong closed source providers, improved context-aware detections, enhanced\nopen-source engagement, increased user awareness, and the adoption of more\nrepresentative performance metrics."}
{"id": "2505.13126", "pdf": "https://arxiv.org/pdf/2505.13126.pdf", "abs": "https://arxiv.org/abs/2505.13126", "title": "Zero-Shot Iterative Formalization and Planning in Partially Observable Environments", "authors": ["Liancheng Gong", "Wang Zhu", "Jesse Thomason", "Li Zhang"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Using LLMs not to predict plans but to formalize an environment into the\nPlanning Domain Definition Language (PDDL) has been shown to improve\nperformance and control. Existing work focuses on fully observable\nenvironments; we tackle the more realistic and challenging partially observable\nenvironments that lack of complete, reliable information. We propose PDDLego+,\na framework to iteratively formalize, plan, grow, and refine PDDL\nrepresentations in a zero-shot manner, without needing access to any existing\ntrajectories. On two textual simulated environments, we show that PDDLego+\nimproves goal reaching success and exhibits robustness against problem\ncomplexity. We also show that the domain knowledge captured after a successful\ntrial can benefit future tasks."}
{"id": "2505.13393", "pdf": "https://arxiv.org/pdf/2505.13393.pdf", "abs": "https://arxiv.org/abs/2505.13393", "title": "IG Parser: A Software Package for the Encoding of Institutional Statements using the Institutional Grammar", "authors": ["Christopher K. Frantz"], "categories": ["cs.MA", "cs.AI", "cs.CL", "68T30, 68T50", "E.2; H.1.0; I.7.2; I.6.5; K.4.1"], "comment": "24 pages", "summary": "This article provides an overview of IG Parser, a software that facilitates\nqualitative content analysis of formal (e.g., legal) rules or informal (e.g.,\nsocial) norms, and strategies (such as conventions) -- referred to as\ninstitutions -- that govern social systems and operate configurally to describe\ninstitutional systems. To this end, the IG Parser employs a distinctive syntax\nthat ensures rigorous encoding of natural language, while automating the\ntransformation into various formats that support the downstream analysis using\ndiverse analytical techniques. The conceptual core of the IG Parser is an\nassociated syntax, IG Script, that operationalizes the conceptual foundations\nof the Institutional Grammar, and more specifically the Institutional Grammar\n2.0, an analytical paradigm for institutional analysis. This article presents\nthe IG Parser, including its conceptual foundations, the syntax specification\nof IG Script, and its architectural principles. This overview is augmented with\nselective illustrative examples that highlight its use and the associated\nbenefits."}
