{"id": "2509.04636", "pdf": "https://arxiv.org/pdf/2509.04636.pdf", "abs": "https://arxiv.org/abs/2509.04636", "title": "Computational Cognitive Modeling to understand the effects of Racializing AI on Human-AI cooperation with PigChase Task", "authors": ["Swapnika Dulam", "Christopher L Dancy"], "categories": ["cs.HC"], "comment": null, "summary": "Despite the continued anthropomorphization of AI systems, the potential\nimpact of racialization during human-AI interaction is understudied. This study\nexplores how human-AI cooperation may be impacted by the belief that data used\nto train an AI system is racialized, that is, it was trained on data from a\nspecific group of people. During this study, participants completed a human-AI\ncooperation task using the Pig Chase game. Participants of different\nself-identified demographics interacted with AI agents whose perceived racial\nidentities were manipulated, allowing us to assess how sociocultural\nperspectives influence the decision-making of participants in the game. After\nthe game, participants completed a survey questionnaire to explain the\nstrategies they used while playing the game and to understand the perceived\nintelligence of their AI teammates. Statistical analysis of task behavior data\nrevealed a statistically significant effect of the participant's demographic,\nas well as the interaction between this self-identified demographic and the\ntreatment condition (i.e., the perceived demographic of the agent). The results\nindicated that Non-White participants viewed AI agents racialized as White in a\npositive way compared to AI agents racialized as Black. Both Black and White\nparticipants viewed the AI agent in the control treatment in a negative way. A\nbaseline cognitive model of the task using ACT-R cognitive architecture was\nused to understand a cognitive-level, process-based explanation of the\nparticipants' perspectives based on results found from the study. This model\nhelps us better understand the factors affecting the decision-making strategies\nof the game participants. Results from analysis of these data, as well as\ncognitive modeling, indicate a need to expand understanding of the ways\nracialization (whether implicit or explicit) impacts interaction with AI\nsystems."}
{"id": "2509.04705", "pdf": "https://arxiv.org/pdf/2509.04705.pdf", "abs": "https://arxiv.org/abs/2509.04705", "title": "Transforming Fashion with AI: A Comparative Study of Large Language Models in Apparel Design", "authors": ["Nusrat Jahan Lamia", "Sadia Afrin Mim"], "categories": ["cs.HC"], "comment": null, "summary": "Fashion has evolved from handcrafted designs to automated production over the\nyears, where AI has added another dimension to it. Nowadays, practically every\nindustry uses artificial models to automate their operations. To explore their\nrole, we examined three prominent LLMs (OpenAI, GeminiAI, Deepseek) in multiple\nstages of textile manufacturing (e.g., sustainable choice, cost effectiveness,\nproduction planning, etc.). We assessed the models' ability to replicate\ngarment design using certain parameters (fabric construction, shade, weave,\nsilhouette, etc.). We compared the models in terms of different body types and\nfunctional purposes (e.g., fashionwear, sportswear) so that designers could\nevaluate effectiveness before developing actual patterns, make necessary\nmodifications, and conduct fashion forecasting beforehand. To facilitate deeper\nanalysis, we created a custom dataset specifically for fabric image generation\nand classification. Our analysis revealed that, in terms of fabric\nconstruction, the OpenAI DALL-E model integrated with ChatGPT outperformed\nother models, achieving a lower LPIPS (Learned Perceptual Image Patch\nSimilarity) score of approximately $0.2$. In fabric classification from images,\nwe found OpenAI offered the best results by breaking down certain factors\n(e.g., breathability, moisture-wicking, and tactile comfort), achieving\napproximately $80\\%$ accuracy for base construction and $55\\%$ for detailed\nconstruction. However, our results indicate that Deepseek faced significant\nchallenges in generating and recognizing fabric images. Overall, all the models\nstruggled to recognize complex fabric constructions and intricate designs from\nimages, and relying too much on AI might hinder human creativity. We also\nobserved that all three models performed effectively in providing\nrecommendations and insights for fabric design in textual form."}
{"id": "2509.04752", "pdf": "https://arxiv.org/pdf/2509.04752.pdf", "abs": "https://arxiv.org/abs/2509.04752", "title": "SePA: A Search-enhanced Predictive Agent for Personalized Health Coaching", "authors": ["Melik Ozolcer", "Sang Won Bae"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "Accepted at IEEE-EMBS International Conference on Biomedical and\n  Health Informatics (BHI'25). 7 pages, 5 figures, 3 tables", "summary": "This paper introduces SePA (Search-enhanced Predictive AI Agent), a novel LLM\nhealth coaching system that integrates personalized machine learning and\nretrieval-augmented generation to deliver adaptive, evidence-based guidance.\nSePA combines: (1) Individualized models predicting daily stress, soreness, and\ninjury risk from wearable sensor data (28 users, 1260 data points); and (2) A\nretrieval module that grounds LLM-generated feedback in expert-vetted web\ncontent to ensure contextual relevance and reliability. Our predictive models,\nevaluated with rolling-origin cross-validation and group k-fold\ncross-validation show that personalized models outperform generalized\nbaselines. In a pilot expert study (n=4), SePA's retrieval-based advice was\npreferred over a non-retrieval baseline, yielding meaningful practical effect\n(Cliff's $\\delta$=0.3, p=0.05). We also quantify latency performance trade-offs\nbetween response quality and speed, offering a transparent blueprint for\nnext-generation, trustworthy personal health informatics systems."}
{"id": "2509.05023", "pdf": "https://arxiv.org/pdf/2509.05023.pdf", "abs": "https://arxiv.org/abs/2509.05023", "title": "Evaluating Idle Animation Believability: a User Perspective", "authors": ["Eneko Atxa Landa", "Elena Lazkano", "Igor Rodriguez", "Itsaso Rodr√≠guez-Moreno", "Itziar Irigoien"], "categories": ["cs.HC", "cs.DB"], "comment": "11 pages, 12 figures", "summary": "Animating realistic avatars requires using high quality animations for every\npossible state the avatar can be in. This includes actions like walking or\nrunning, but also subtle movements that convey emotions and personality. Idle\nanimations, such as standing, breathing or looking around, are crucial for\nrealism and believability. In games and virtual applications, these are often\nhandcrafted or recorded with actors, but this is costly. Furthermore, recording\nrealistic idle animations can be very complex, because the actor must not know\nthey are being recorded in order to make genuine movements. For this reasons\nidle animation datasets are not widely available. Nevertheless, this paper\nconcludes that both acted and genuine idle animations are perceived as real,\nand that users are not able to distinguish between them. It also states that\nhandmade and recorded idle animations are perceived differently. These two\nconclusions mean that recording idle animations should be easier than it is\nthought to be, meaning that actors can be specifically told to act the\nmovements, significantly simplifying the recording process. These conclusions\nshould help future efforts to record idle animation datasets. Finally, we also\npublish ReActIdle, a 3 dimensional idle animation dataset containing both real\nand acted idle motions."}
{"id": "2509.04455", "pdf": "https://arxiv.org/pdf/2509.04455.pdf", "abs": "https://arxiv.org/abs/2509.04455", "title": "INSEva: A Comprehensive Chinese Benchmark for Large Language Models in Insurance", "authors": ["Shisong Chen", "Qian Zhu", "Wenyan Yang", "Chengyi Yang", "Zhong Wang", "Ping Wang", "Xuan Lin", "Bo Xu", "Daqian Li", "Chao Yuan", "Licai Qi", "Wanqing Xu", "sun zhenxing", "Xin Lu", "Shiqiang Xiong", "Chao Chen", "Haixiang Hu", "Yanghua Xiao"], "categories": ["cs.CL"], "comment": "Under review", "summary": "Insurance, as a critical component of the global financial system, demands\nhigh standards of accuracy and reliability in AI applications. While existing\nbenchmarks evaluate AI capabilities across various domains, they often fail to\ncapture the unique characteristics and requirements of the insurance domain. To\naddress this gap, we present INSEva, a comprehensive Chinese benchmark\nspecifically designed for evaluating AI systems' knowledge and capabilities in\ninsurance. INSEva features a multi-dimensional evaluation taxonomy covering\nbusiness areas, task formats, difficulty levels, and cognitive-knowledge\ndimension, comprising 38,704 high-quality evaluation examples sourced from\nauthoritative materials. Our benchmark implements tailored evaluation methods\nfor assessing both faithfulness and completeness in open-ended responses.\nThrough extensive evaluation of 8 state-of-the-art Large Language Models\n(LLMs), we identify significant performance variations across different\ndimensions. While general LLMs demonstrate basic insurance domain competency\nwith average scores above 80, substantial gaps remain in handling complex,\nreal-world insurance scenarios. The benchmark will be public soon."}
{"id": "2509.05067", "pdf": "https://arxiv.org/pdf/2509.05067.pdf", "abs": "https://arxiv.org/abs/2509.05067", "title": "Long-Term Experiences From Working with Extended Reality in the Wild", "authors": ["Verena Biener", "Florian Jack Winston", "Dieter Schmalstieg", "Alexander Plopski"], "categories": ["cs.HC"], "comment": null, "summary": "Extended Reality (XR) is increasingly used as a productivity tool and recent\ncommercial XR devices have even been specifically designed as productivity\ntools, or, at least, are heavily advertised for such purposes, such as the\nApple Vision Pro (AVP), which has now been available for more than one year. In\nspite of what marketing suggests, research still lacks an understanding of the\nlong-term usage of such devices in ecologically valid everyday settings, as\nmost studies are conducted in very controlled environments. Therefore, we\nconducted interviews with ten AVP users to better understand how experienced\nusers engage with the device, and which limitations persist. Our participants\nreport that XR can increase productivity and that they got used to the device\nafter some time. Yet, a range of limitations persist that might hinder the\nwidespread use of XR as a productivity tool, such as a lack of native\napplications, difficulties when integrating XR into current workflows, and\nlimited possibilities to adapt and customize the XR experience."}
{"id": "2509.04456", "pdf": "https://arxiv.org/pdf/2509.04456.pdf", "abs": "https://arxiv.org/abs/2509.04456", "title": "Mentalic Net: Development of RAG-based Conversational AI and Evaluation Framework for Mental Health Support", "authors": ["Anandi Dutta", "Shivani Mruthyunjaya", "Jessica Saddington", "Kazi Sifatul Islam"], "categories": ["cs.CL"], "comment": "Preprint Version, Accepted in ISEMV 2025", "summary": "The emergence of large language models (LLMs) has unlocked boundless\npossibilities, along with significant challenges. In response, we developed a\nmental health support chatbot designed to augment professional healthcare, with\na strong emphasis on safe and meaningful application. Our approach involved\nrigorous evaluation, covering accuracy, empathy, trustworthiness, privacy, and\nbias. We employed a retrieval-augmented generation (RAG) framework, integrated\nprompt engineering, and fine-tuned a pre-trained model on novel datasets. The\nresulting system, Mentalic Net Conversational AI, achieved a BERT Score of\n0.898, with other evaluation metrics falling within satisfactory ranges. We\nadvocate for a human-in-the-loop approach and a long-term, responsible strategy\nin developing such transformative technologies, recognizing both their\npotential to change lives and the risks they may pose if not carefully managed."}
{"id": "2509.05145", "pdf": "https://arxiv.org/pdf/2509.05145.pdf", "abs": "https://arxiv.org/abs/2509.05145", "title": "Exploring Situated Stabilities of a Rhythm Generation System through Variational Cross-Examination", "authors": ["B≈Ça≈ºej Kotowski", "Nicholas Evans", "Behzad Haki", "Frederic Font", "Sergi Jord√†"], "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS"], "comment": "AI Music Creativity 2025", "summary": "This paper investigates GrooveTransformer, a real-time rhythm generation\nsystem, through the postphenomenological framework of Variational\nCross-Examination (VCE). By reflecting on its deployment across three distinct\nartistic contexts, we identify three stabilities: an autonomous drum\naccompaniment generator, a rhythmic control voltage sequencer in Eurorack\nformat, and a rhythm driver for a harmonic accompaniment system. The\nversatility of its applications was not an explicit goal from the outset of the\nproject. Thus, we ask: how did this multistability emerge? Through VCE, we\nidentify three key contributors to its emergence: the affordances of system\ninvariants, the interdisciplinary collaboration, and the situated nature of its\ndevelopment. We conclude by reflecting on the viability of VCE as a descriptive\nand analytical method for Digital Musical Instrument (DMI) design, emphasizing\nits value in uncovering how technologies mediate, co-shape, and are co-shaped\nby users and contexts."}
{"id": "2509.04457", "pdf": "https://arxiv.org/pdf/2509.04457.pdf", "abs": "https://arxiv.org/abs/2509.04457", "title": "Do MLLMs Really Understand the Charts?", "authors": ["Xiao Zhang", "Dongyuan Li", "Liuyu Xiang", "Yao Zhang", "Cheng Zhong", "Zhaofeng He"], "categories": ["cs.CL"], "comment": "19 pages,15 figures", "summary": "Although Multimodal Large Language Models (MLLMs) have demonstrated\nincreasingly impressive performance in chart understanding, most of them\nexhibit alarming hallucinations and significant performance degradation when\nhandling non-annotated charts. Therefore, a question arises: Do MLLMs really\nunderstand the charts? Since a human is capable of understanding charts and\nestimating the values by visual reasoning, we first carefully establish a\ncomprehensive Chart Reasoning Benchmark CRBench to rigorously evaluate the\nvisual reasoning abilities of MLLMs on non-annotated charts. We argue that\nMLLMs are primarily relying on recognition rather than reasoning to interpret\nthe charts. To steer MLLMs to reasonable chart understanding, we propose\nChartReasoner that mimics human behavior by grounding their estimation in chart\nunderstanding. Extensive results on the proposed CRBench show that\nChartReasnoner-3B/7B achieves superior performance in chart reasoning, even\ncompared to GPT-4o and Gemini-2.5-Flash. More importantly, ChartReasnoner also\ndemonstrates the visual reasoning abilities in general chart comprehension on\npublic benchmarks, leading to significant performance gains and enabling MLLMs\nto rationally understand the charts. The code and dataset will be publicly\navailable upon publication."}
{"id": "2509.05166", "pdf": "https://arxiv.org/pdf/2509.05166.pdf", "abs": "https://arxiv.org/abs/2509.05166", "title": "Transition of car-based human-mobility in the pandemic era: Data insight from a cross-border region in Europe", "authors": ["Sujit Kumar Sikder", "Jyotirmaya Ijaradar", "Hao Li", "Hichem Omrani"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Many transport authorities are collecting and publishing almost real-time\nroad traffic data to meet the growing trend of massive open data, a vital\nresource for foresight decision support systems considering deep data insights.\nUsing such a traffic count dataset, we explored the spatio-temporal transitions\nin the cross-country road traffic volumes in the context of modelling\nbehavioural transitions in car-based human mobility. We developed a\nreproducible workflow for computing multi-dimensional variables of traffic\nflow. This study reports on individual car-based daily travel behaviour\ndetected, before (2016-2018) and during the COVID pandemic (2019-2021), between\nGermany and neighbouring countries (Luxembourg, France and Belgium). In\nrelevance to the net-zero carbon transition, further study should shed light on\nthe interpolation and downscaling approaches at the comprehensive road-network\nlevel for identifying pollution hot spots, causal link to functional landuse\npatterns and calculation of spatial influence area. In the case of Luxembourg,\nthe Bridges and Roads Authority has installed a large digital traffic\nobservatory infrastructure through the adoption of sensor-based IoT\ntechnologies, like other European member states. Since 2016, they have provided\nhigh-performance data processing and published open data on the country's road\ntraffic. The dataset contains an hourly traffic count for different vehicle\ntypes, daily for representative observation points, followed by a major road\nnetwork. The original dataset contains significant missing entries, so\ncomprehensive data harmonization was performed."}
{"id": "2509.04458", "pdf": "https://arxiv.org/pdf/2509.04458.pdf", "abs": "https://arxiv.org/abs/2509.04458", "title": "Predicting Failures of LLMs to Link Biomedical Ontology Terms to Identifiers Evidence Across Models and Ontologies", "authors": ["Daniel B. Hier", "Steven Keith Platt", "Tayo Obafemi-Ajayi"], "categories": ["cs.CL", "I.2"], "comment": "Accepted for Presentation, IEEE-EMBS International Conference on\n  Biomedical and Health Informatics (BHI 25), Atlanta GA USA, October 26-29,\n  2025", "summary": "Large language models often perform well on biomedical NLP tasks but may fail\nto link ontology terms to their correct identifiers. We investigate why these\nfailures occur by analyzing predictions across two major ontologies, Human\nPhenotype Ontology and Gene Ontology, and two high-performing models, GPT-4o\nand LLaMa 3.1 405B. We evaluate nine candidate features related to term\nfamiliarity, identifier usage, morphology, and ontology structure. Univariate\nand multivariate analyses show that exposure to ontology identifiers is the\nstrongest predictor of linking success."}
{"id": "2509.05219", "pdf": "https://arxiv.org/pdf/2509.05219.pdf", "abs": "https://arxiv.org/abs/2509.05219", "title": "Conversational AI increases political knowledge as effectively as self-directed internet search", "authors": ["Lennart Luettgau", "Hannah Rose Kirk", "Kobi Hackenburg", "Jessica Bergs", "Henry Davidson", "Henry Ogden", "Divya Siddarth", "Saffron Huang", "Christopher Summerfield"], "categories": ["cs.HC"], "comment": null, "summary": "Conversational AI systems are increasingly being used in place of traditional\nsearch engines to help users complete information-seeking tasks. This has\nraised concerns in the political domain, where biased or hallucinated outputs\ncould misinform voters or distort public opinion. However, in spite of these\nconcerns, the extent to which conversational AI is used for political\ninformation-seeking, as well the potential impact of this use on users'\npolitical knowledge, remains uncertain. Here, we address these questions:\nFirst, in a representative national survey of the UK public (N = 2,499), we\nfind that in the week before the 2024 election as many as 32% of chatbot users\n- and 13% of eligible UK voters - have used conversational AI to seek political\ninformation relevant to their electoral choice. Second, in a series of\nrandomised controlled trials (N = 2,858 total) we find that across issues,\nmodels, and prompting strategies, conversations with AI increase political\nknowledge (increase belief in true information and decrease belief in\nmisinformation) to the same extent as self-directed internet search. Taken\ntogether, our results suggest that although people in the UK are increasingly\nturning to conversational AI for information about politics, this shift may not\nlead to increased public belief in political misinformation."}
{"id": "2509.04459", "pdf": "https://arxiv.org/pdf/2509.04459.pdf", "abs": "https://arxiv.org/abs/2509.04459", "title": "Uncertainty-Aware Collaborative System of Large and Small Models for Multimodal Sentiment Analysis", "authors": ["Shiqin Han", "Manning Gao", "Menghua Jiang", "Yuncheng Jiang", "Haifeng Hu", "Sijie Mai"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The advent of Multimodal Large Language Models (MLLMs) has significantly\nadvanced the state-of-the-art in multimodal machine learning, yet their\nsubstantial computational demands present a critical barrier to real-world\ndeployment. Conversely, smaller, specialized models offer high efficiency but\noften at the cost of performance. To reconcile this performance-efficiency\ntrade-off, we propose a novel Uncertainty-Aware Collaborative System (U-ACS)\nthat synergistically orchestrates a powerful MLLM (e.g., HumanOmni) and a\nlightweight baseline model for multimodal sentiment analysis. The core of our\nsystem is an uncertainty-driven cascade mechanism, where the efficient small\nmodel first acts as a rapid filter for all input samples. Only those samples\nyielding high predictive uncertainty, thereby indicating greater difficulty,\nare selectively escalated to the MLLM for more sophisticated analysis.\nFurthermore, our system introduces advanced strategies to handle ambiguous or\nconflicting predictions, including weighted averaging for predictions of\nsimilar polarity and a prompt-based cross-verification to resolve conflicting\npredictions when both models exhibit high uncertainty. This\nsample-difficulty-aware approach allows for a dynamic allocation of\ncomputational resources, drastically reducing inference costs while retaining\nthe high accuracy of MLLM. Extensive experiments on benchmark datasets\ndemonstrate that our proposed method achieves state-of-the-art performance,\nwhile requiring only a fraction of the computational resources compared to\nusing a standalone MLLM."}
{"id": "2509.04510", "pdf": "https://arxiv.org/pdf/2509.04510.pdf", "abs": "https://arxiv.org/abs/2509.04510", "title": "Combine Virtual Reality and Machine-Learning to Identify the Presence of Dyslexia: A Cross-Linguistic Approach", "authors": ["Michele Materazzini", "Gianluca Morciano", "Jose Manuel Alcalde-Llergo", "Enrique Yeguas-Bolivar", "Giuseppe Calabro", "Andrea Zingoni", "Juri Taborri"], "categories": ["cs.CL", "cs.HC"], "comment": "22 pages, 10 figures, 5 tables", "summary": "This study explores the use of virtual reality (VR) and artificial\nintelligence (AI) to predict the presence of dyslexia in Italian and Spanish\nuniversity students. In particular, the research investigates whether\nVR-derived data from Silent Reading (SR) tests and self-esteem assessments can\ndifferentiate between students that are affected by dyslexia and students that\nare not, employing machine learning (ML) algorithms. Participants completed\nVR-based tasks measuring reading performance and self-esteem. A preliminary\nstatistical analysis (t tests and Mann Whitney tests) on these data was\nperformed, to compare the obtained scores between individuals with and without\ndyslexia, revealing significant differences in completion time for the SR test,\nbut not in accuracy, nor in self esteem. Then, supervised ML models were\ntrained and tested, demonstrating an ability to classify the presence/absence\nof dyslexia with an accuracy of 87.5 per cent for Italian, 66.6 per cent for\nSpanish, and 75.0 per cent for the pooled group. These findings suggest that VR\nand ML can effectively be used as supporting tools for assessing dyslexia,\nparticularly by capturing differences in task completion speed, but\nlanguage-specific factors may influence classification accuracy."}
{"id": "2509.04460", "pdf": "https://arxiv.org/pdf/2509.04460.pdf", "abs": "https://arxiv.org/abs/2509.04460", "title": "CoCoNUTS: Concentrating on Content while Neglecting Uninformative Textual Styles for AI-Generated Peer Review Detection", "authors": ["Yihan Chen", "Jiawei Chen", "Guozhao Mo", "Xuanang Chen", "Ben He", "Xianpei Han", "Le Sun"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The growing integration of large language models (LLMs) into the peer review\nprocess presents potential risks to the fairness and reliability of scholarly\nevaluation. While LLMs offer valuable assistance for reviewers with language\nrefinement, there is growing concern over their use to generate substantive\nreview content. Existing general AI-generated text detectors are vulnerable to\nparaphrasing attacks and struggle to distinguish between surface language\nrefinement and substantial content generation, suggesting that they primarily\nrely on stylistic cues. When applied to peer review, this limitation can result\nin unfairly suspecting reviews with permissible AI-assisted language\nenhancement, while failing to catch deceptively humanized AI-generated reviews.\nTo address this, we propose a paradigm shift from style-based to content-based\ndetection. Specifically, we introduce CoCoNUTS, a content-oriented benchmark\nbuilt upon a fine-grained dataset of AI-generated peer reviews, covering six\ndistinct modes of human-AI collaboration. Furthermore, we develop CoCoDet, an\nAI review detector via a multi-task learning framework, designed to achieve\nmore accurate and robust detection of AI involvement in review content. Our\nwork offers a practical foundation for evaluating the use of LLMs in peer\nreview, and contributes to the development of more precise, equitable, and\nreliable detection methods for real-world scholarly applications. Our code and\ndata will be publicly available at https://github.com/Y1hanChen/COCONUTS."}
{"id": "2509.04676", "pdf": "https://arxiv.org/pdf/2509.04676.pdf", "abs": "https://arxiv.org/abs/2509.04676", "title": "An Approach to Grounding AI Model Evaluations in Human-derived Criteria", "authors": ["Sasha Mitts"], "categories": ["cs.AI", "cs.HC"], "comment": "4 figures, 6 pages, presented at CHI 2025 Workshop on Human-AI\n  Interaction for Augmented Reasoning", "summary": "In the rapidly evolving field of artificial intelligence (AI), traditional\nbenchmarks can fall short in attempting to capture the nuanced capabilities of\nAI models. We focus on the case of physical world modeling and propose a novel\napproach to augment existing benchmarks with human-derived evaluation criteria,\naiming to enhance the interpretability and applicability of model behaviors.\nGrounding our study in the Perception Test and OpenEQA benchmarks, we conducted\nin-depth interviews and large-scale surveys to identify key cognitive skills,\nsuch as Prioritization, Memorizing, Discerning, and Contextualizing, that are\ncritical for both AI and human reasoning. Our findings reveal that participants\nperceive AI as lacking in interpretive and empathetic skills yet hold high\nexpectations for AI performance. By integrating insights from our findings into\nbenchmark design, we offer a framework for developing more human-aligned means\nof defining and measuring progress. This work underscores the importance of\nuser-centered evaluation in AI development, providing actionable guidelines for\nresearchers and practitioners aiming to align AI capabilities with human\ncognitive processes. Our approach both enhances current benchmarking practices\nand sets the stage for future advancements in AI model evaluation."}
{"id": "2509.04461", "pdf": "https://arxiv.org/pdf/2509.04461.pdf", "abs": "https://arxiv.org/abs/2509.04461", "title": "From Post To Personality: Harnessing LLMs for MBTI Prediction in Social Media", "authors": ["Tian Ma", "Kaiyu Feng", "Yu Rong", "Kangfei Zhao"], "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Personality prediction from social media posts is a critical task that\nimplies diverse applications in psychology and sociology. The Myers Briggs Type\nIndicator (MBTI), a popular personality inventory, has been traditionally\npredicted by machine learning (ML) and deep learning (DL) techniques. Recently,\nthe success of Large Language Models (LLMs) has revealed their huge potential\nin understanding and inferring personality traits from social media content.\nHowever, directly exploiting LLMs for MBTI prediction faces two key challenges:\nthe hallucination problem inherent in LLMs and the naturally imbalanced\ndistribution of MBTI types in the population. In this paper, we propose\nPostToPersonality (PtoP), a novel LLM based framework for MBTI prediction from\nsocial media posts of individuals. Specifically, PtoP leverages Retrieval\nAugmented Generation with in context learning to mitigate hallucination in\nLLMs. Furthermore, we fine tune a pretrained LLM to improve model specification\nin MBTI understanding with synthetic minority oversampling, which balances the\nclass imbalance by generating synthetic samples. Experiments conducted on a\nreal world social media dataset demonstrate that PtoP achieves state of the art\nperformance compared with 10 ML and DL baselines."}
{"id": "2509.04809", "pdf": "https://arxiv.org/pdf/2509.04809.pdf", "abs": "https://arxiv.org/abs/2509.04809", "title": "TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models", "authors": ["Haechang Kim", "Hao Chen", "Can Li", "Jong Min Lee"], "categories": ["cs.AI", "cs.HC"], "comment": "31 pages total", "summary": "Explainable Reinforcement Learning (XRL) has emerged as a promising approach\nin improving the transparency of Reinforcement Learning (RL) agents. However,\nthere remains a gap between complex RL policies and domain experts, due to the\nlimited comprehensibility of XRL results and isolated coverage of current XRL\napproaches that leave users uncertain about which tools to employ. To address\nthese challenges, we introduce TalkToAgent, a multi-agent Large Language Models\n(LLM) framework that delivers interactive, natural language explanations for RL\npolicies. The architecture with five specialized LLM agents (Coordinator,\nExplainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically\nmap user queries to relevant XRL tools and clarify an agent's actions in terms\nof either key state variables, expected outcomes, or counterfactual\nexplanations. Moreover, our approach extends previous counterfactual\nexplanations by deriving alternative scenarios from qualitative behavioral\ndescriptions, or even new rule-based policies. We validated TalkToAgent on\nquadruple-tank process control problem, a well-known nonlinear control\nbenchmark. Results demonstrated that TalkToAgent successfully mapped user\nqueries into XRL tasks with high accuracy, and coder-debugger interactions\nminimized failures in counterfactual generation. Furthermore, qualitative\nevaluation confirmed that TalkToAgent effectively interpreted agent's actions\nand contextualized their meaning within the problem domain."}
{"id": "2509.04462", "pdf": "https://arxiv.org/pdf/2509.04462.pdf", "abs": "https://arxiv.org/abs/2509.04462", "title": "Benchmarking GPT-5 for biomedical natural language processing", "authors": ["Yu Hou", "Zaifu Zhan", "Rui Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid expansion of biomedical literature has heightened the need for\nscalable natural language processing (NLP) solutions. While GPT-4 substantially\nnarrowed the gap with task-specific systems, especially in question answering,\nits performance across other domains remained uneven. We updated a standardized\nBioNLP benchmark to evaluate GPT-5 and GPT-4o under zero-, one-, and five-shot\nprompting across 12 datasets spanning six task families: named entity\nrecognition, relation extraction, multi-label document classification, question\nanswering, text summarization, and text simplification. Using fixed prompt\ntemplates, identical decoding parameters, and batch inference, we report\nprimary metrics per dataset and include prior results for GPT-4, GPT-3.5, and\nLLaMA-2-13B for comparison. GPT-5 achieved the strongest overall benchmark\nperformance, with macro-average scores rising to 0.557 under five-shot\nprompting versus 0.506 for GPT-4 and 0.508 for GPT-4o. On MedQA, GPT-5 reached\n94.1% accuracy, exceeding the previous supervised state of the art by over\nfifty points, and attained parity with supervised systems on PubMedQA (0.734).\nIn extraction tasks, GPT-5 delivered major gains in chemical NER (0.886 F1) and\nChemProt relation extraction (0.616 F1), outperforming GPT-4 and GPT-4o, though\nsummarization and disease NER still lagged behind domain-specific baselines.\nThese results establish GPT-5 as a general-purpose model now offering\ndeployment-ready performance for reasoning-oriented biomedical QA, while\nprecision-critical extraction and evidence-dense summarization continue to\nfavor fine-tuned or hybrid approaches. The benchmark delineates where simple\nprompting suffices and where retrieval-augmented or planning-based scaffolds\nare likely required, providing actionable guidance for BioNLP system design as\nfrontier models advance."}
{"id": "2509.04889", "pdf": "https://arxiv.org/pdf/2509.04889.pdf", "abs": "https://arxiv.org/abs/2509.04889", "title": "SpiderNets: Estimating Fear Ratings of Spider-Related Images with Vision Models", "authors": ["Dominik Pegler", "David Steyrl", "Mengfan Zhang", "Alexander Karner", "Jozsef Arato", "Frank Scharnowski", "Filip Melinscak"], "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "comment": "60 pages (30 main text, 30 appendix), 20 figures (5 in main text, 15\n  in appendix)", "summary": "Advances in computer vision have opened new avenues for clinical\napplications, particularly in computerized exposure therapy where visual\nstimuli can be dynamically adjusted based on patient responses. As a critical\nstep toward such adaptive systems, we investigated whether pretrained computer\nvision models can accurately predict fear levels from spider-related images. We\nadapted three diverse models using transfer learning to predict human fear\nratings (on a 0-100 scale) from a standardized dataset of 313 images. The\nmodels were evaluated using cross-validation, achieving an average mean\nabsolute error (MAE) between 10.1 and 11.0. Our learning curve analysis\nrevealed that reducing the dataset size significantly harmed performance,\nthough further increases yielded no substantial gains. Explainability\nassessments showed the models' predictions were based on spider-related\nfeatures. A category-wise error analysis further identified visual conditions\nassociated with higher errors (e.g., distant views and artificial/painted\nspiders). These findings demonstrate the potential of explainable computer\nvision models in predicting fear ratings, highlighting the importance of both\nmodel explainability and a sufficient dataset size for developing effective\nemotion-aware therapeutic technologies."}
{"id": "2509.04464", "pdf": "https://arxiv.org/pdf/2509.04464.pdf", "abs": "https://arxiv.org/abs/2509.04464", "title": "Can Multiple Responses from an LLM Reveal the Sources of Its Uncertainty?", "authors": ["Yang Nan", "Pengfei He", "Ravi Tandon", "Han Xu"], "categories": ["cs.CL", "cs.AI"], "comment": "Proceedings of The 2025 Conference on Empirical Methods in Natural\n  Language Processing (Findings)", "summary": "Large language models (LLMs) have delivered significant breakthroughs across\ndiverse domains but can still produce unreliable or misleading outputs, posing\ncritical challenges for real-world applications. While many recent studies\nfocus on quantifying model uncertainty, relatively little work has been devoted\nto \\textit{diagnosing the source of uncertainty}. In this study, we show that,\nwhen an LLM is uncertain, the patterns of disagreement among its multiple\ngenerated responses contain rich clues about the underlying cause of\nuncertainty. To illustrate this point, we collect multiple responses from a\ntarget LLM and employ an auxiliary LLM to analyze their patterns of\ndisagreement. The auxiliary model is tasked to reason about the likely source\nof uncertainty, such as whether it stems from ambiguity in the input question,\na lack of relevant knowledge, or both. In cases involving knowledge gaps, the\nauxiliary model also identifies the specific missing facts or concepts\ncontributing to the uncertainty. In our experiment, we validate our framework\non AmbigQA, OpenBookQA, and MMLU-Pro, confirming its generality in diagnosing\ndistinct uncertainty sources. Such diagnosis shows the potential for relevant\nmanual interventions that improve LLM performance and reliability."}
{"id": "2509.04908", "pdf": "https://arxiv.org/pdf/2509.04908.pdf", "abs": "https://arxiv.org/abs/2509.04908", "title": "SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing", "authors": ["Hongyi Jing", "Jiafu Chen", "Chen Rao", "Ziqiang Dang", "Jiajie Teng", "Tianyi Chu", "Juncheng Mo", "Shuo Fang", "Huaizhong Lin", "Rui Lv", "Chenguang Ma", "Lei Zhao"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": null, "summary": "The existing Multimodal Large Language Models (MLLMs) for GUI perception have\nmade great progress. However, the following challenges still exist in prior\nmethods: 1) They model discrete coordinates based on text autoregressive\nmechanism, which results in lower grounding accuracy and slower inference\nspeed. 2) They can only locate predefined sets of elements and are not capable\nof parsing the entire interface, which hampers the broad application and\nsupport for downstream tasks. To address the above issues, we propose\nSparkUI-Parser, a novel end-to-end framework where higher localization\nprecision and fine-grained parsing capability of the entire interface are\nsimultaneously achieved. Specifically, instead of using probability-based\ndiscrete modeling, we perform continuous modeling of coordinates based on a\npre-trained Multimodal Large Language Model (MLLM) with an additional token\nrouter and coordinate decoder. This effectively mitigates the limitations\ninherent in the discrete output characteristics and the token-by-token\ngeneration process of MLLMs, consequently boosting both the accuracy and the\ninference speed. To further enhance robustness, a rejection mechanism based on\na modified Hungarian matching algorithm is introduced, which empowers the model\nto identify and reject non-existent elements, thereby reducing false positives.\nMoreover, we present ScreenParse, a rigorously constructed benchmark to\nsystematically assess structural perception capabilities of GUI models across\ndiverse scenarios. Extensive experiments demonstrate that our approach\nconsistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2,\nCAGUI-Grounding and ScreenParse benchmarks. The resources are available at\nhttps://github.com/antgroup/SparkUI-Parser."}
{"id": "2509.04465", "pdf": "https://arxiv.org/pdf/2509.04465.pdf", "abs": "https://arxiv.org/abs/2509.04465", "title": "Emotionally-Aware Agents for Dispute Resolution", "authors": ["Sushrita Rakshit", "James Hale", "Kushal Chawla", "Jeanne M. Brett", "Jonathan Gratch"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In conflict, people use emotional expressions to shape their counterparts'\nthoughts, feelings, and actions. This paper explores whether automatic text\nemotion recognition offers insight into this influence in the context of\ndispute resolution. Prior work has shown the promise of such methods in\nnegotiations; however, disputes evoke stronger emotions and different social\nprocesses. We use a large corpus of buyer-seller dispute dialogues to\ninvestigate how emotional expressions shape subjective and objective outcomes.\nWe further demonstrate that large-language models yield considerably greater\nexplanatory power than previous methods for emotion intensity annotation and\nbetter match the decisions of human annotators. Findings support existing\ntheoretical models for how emotional expressions contribute to conflict\nescalation and resolution and suggest that agent-based systems could be useful\nin managing disputes by recognizing and potentially mitigating emotional\nescalation."}
{"id": "2509.05197", "pdf": "https://arxiv.org/pdf/2509.05197.pdf", "abs": "https://arxiv.org/abs/2509.05197", "title": "AI Agents for Web Testing: A Case Study in the Wild", "authors": ["Naimeng Ye", "Xiao Yu", "Ruize Xu", "Tianyi Peng", "Zhou Yu"], "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": null, "summary": "Automated web testing plays a critical role in ensuring high-quality user\nexperiences and delivering business value. Traditional approaches primarily\nfocus on code coverage and load testing, but often fall short of capturing\ncomplex user behaviors, leaving many usability issues undetected. The emergence\nof large language models (LLM) and AI agents opens new possibilities for web\ntesting by enabling human-like interaction with websites and a general\nawareness of common usability problems. In this work, we present WebProber, a\nprototype AI agent-based web testing framework. Given a URL, WebProber\nautonomously explores the website, simulating real user interactions,\nidentifying bugs and usability issues, and producing a human-readable report.\nWe evaluate WebProber through a case study of 120 academic personal websites,\nwhere it uncovered 29 usability issues--many of which were missed by\ntraditional tools. Our findings highlight agent-based testing as a promising\ndirection while outlining directions for developing next-generation,\nuser-centered testing frameworks."}
{"id": "2509.04466", "pdf": "https://arxiv.org/pdf/2509.04466.pdf", "abs": "https://arxiv.org/abs/2509.04466", "title": "Just-in-time and distributed task representations in language models", "authors": ["Yuxuan Li", "Declan Campbell", "Stephanie C. Y. Chan", "Andrew Kyle Lampinen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Many of language models' impressive capabilities originate from their\nin-context learning: based on instructions or examples, they can infer and\nperform new tasks without weight updates. In this work, we investigate\n\\emph{when} representations for new tasks are formed in language models, and\n\\emph{how} these representations change over the course of context. We focus on\n''transferrable'' task representations -- vector representations that can\nrestore task context in another instance of the model, even without the full\nprompt. We show that these representations evolve in non-monotonic and sporadic\nways, and are distinct from a more inert representation of high-level task\ncategories that persists throughout the context. Specifically, models often\ncondense multiple evidence into these transferrable task representations, which\nalign well with the performance improvement based on more examples in the\ncontext. However, this accrual process exhibits strong locality along the\nsequence dimension, coming online only at certain tokens -- despite task\nidentity being reliably decodable throughout the context. Moreover, these local\nbut transferrable task representations tend to capture minimal ''task scopes'',\nsuch as a semantically-independent subtask, and models rely on more\ntemporally-distributed representations to support longer and composite tasks.\nThis two-fold locality (temporal and semantic) underscores a kind of\njust-in-time computational process underlying language models' ability to adapt\nto new evidence and learn new tasks on the fly."}
{"id": "2404.13765", "pdf": "https://arxiv.org/pdf/2404.13765.pdf", "abs": "https://arxiv.org/abs/2404.13765", "title": "SciDaSynth: Interactive Structured Data Extraction from Scientific Literature with Large Language Model", "authors": ["Xingbo Wang", "Samantha L. Huey", "Rui Sheng", "Saurabh Mehta", "Fei Wang"], "categories": ["cs.HC"], "comment": "Preprint version of the paper accepted to Campbell Systematic\n  Reviews. Code is available at https://github.com/xingbow/SciDaEx", "summary": "The explosion of scientific literature has made the efficient and accurate\nextraction of structured data a critical component for advancing scientific\nknowledge and supporting evidence-based decision-making. However, existing\ntools often struggle to extract and structure multimodal, varied, and\ninconsistent information across documents into standardized formats. We\nintroduce SciDaSynth, a novel interactive system powered by large language\nmodels (LLMs) that automatically generates structured data tables according to\nusers' queries by integrating information from diverse sources, including text,\ntables, and figures. Furthermore, SciDaSynth supports efficient table data\nvalidation and refinement, featuring multi-faceted visual summaries and\nsemantic grouping capabilities to resolve cross-document data inconsistencies.\nA within-subjects study with nutrition and NLP researchers demonstrates\nSciDaSynth's effectiveness in producing high-quality structured data more\nefficiently than baseline methods. We discuss design implications for human-AI\ncollaborative systems supporting data extraction tasks. The system code is\navailable at https://github.com/xingbow/SciDaEx"}
{"id": "2509.04467", "pdf": "https://arxiv.org/pdf/2509.04467.pdf", "abs": "https://arxiv.org/abs/2509.04467", "title": "Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode Disaggregation in Inference", "authors": ["Hao Zhang", "Mengsi Lyu", "Yulong Ao", "Yonghua Lin"], "categories": ["cs.CL", "cs.AI"], "comment": "21 pages", "summary": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the default settings, our method\nachieves a 20.56% inference speedup and a 4.95 times reduction in data\ntransmission bandwidth consumption."}
{"id": "2411.07441", "pdf": "https://arxiv.org/pdf/2411.07441.pdf", "abs": "https://arxiv.org/abs/2411.07441", "title": "Automatically Detecting Online Deceptive Patterns", "authors": ["Asmit Nayak", "Shirley Zhang", "Yash Wani", "Rishabh Khandelwal", "Kassem Fawaz"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Deceptive patterns (DPs) in digital interfaces manipulate users into making\nunintended decisions, exploiting cognitive biases and psychological\nvulnerabilities. These patterns have become ubiquitous across various digital\nplatforms. While efforts to mitigate DPs have emerged from legal and technical\nperspectives, a significant gap in usable solutions that empower users to\nidentify and make informed decisions about DPs in real-time remains. In this\nwork, we introduce AutoBot, an automated, deceptive pattern detector that\nanalyzes websites' visual appearances using machine learning techniques to\nidentify and notify users of DPs in real-time. AutoBot employs a two-staged\npipeline that processes website screenshots, identifying interactable elements\nand extracting textual features without relying on HTML structure. By\nleveraging a custom language model, AutoBot understands the context surrounding\nthese elements to determine the presence of deceptive patterns. We implement\nAutoBot as a lightweight Chrome browser extension that performs all analyses\nlocally, minimizing latency and preserving user privacy. Through extensive\nevaluation, we demonstrate AutoBot's effectiveness in enhancing users' ability\nto navigate digital environments safely while providing a valuable tool for\nregulators to assess and enforce compliance with DP regulations."}
{"id": "2509.04468", "pdf": "https://arxiv.org/pdf/2509.04468.pdf", "abs": "https://arxiv.org/abs/2509.04468", "title": "Evaluating Large Language Models for Financial Reasoning: A CFA-Based Benchmark Study", "authors": ["Xuan Yao", "Qianteng Wang", "Xinbo Liu", "Ke-Wei Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid advancement of large language models presents significant\nopportunities for financial applications, yet systematic evaluation in\nspecialized financial contexts remains limited. This study presents the first\ncomprehensive evaluation of state-of-the-art LLMs using 1,560 multiple-choice\nquestions from official mock exams across Levels I-III of CFA, most rigorous\nprofessional certifications globally that mirror real-world financial analysis\ncomplexity. We compare models distinguished by core design priorities:\nmulti-modal and computationally powerful, reasoning-specialized and highly\naccurate, and lightweight efficiency-optimized.\n  We assess models under zero-shot prompting and through a novel\nRetrieval-Augmented Generation pipeline that integrates official CFA curriculum\ncontent. The RAG system achieves precise domain-specific knowledge retrieval\nthrough hierarchical knowledge organization and structured query generation,\nsignificantly enhancing reasoning accuracy in professional financial\ncertification evaluation.\n  Results reveal that reasoning-oriented models consistently outperform others\nin zero-shot settings, while the RAG pipeline provides substantial improvements\nparticularly for complex scenarios. Comprehensive error analysis identifies\nknowledge gaps as the primary failure mode, with minimal impact from text\nreadability. These findings provide actionable insights for LLM deployment in\nfinance, offering practitioners evidence-based guidance for model selection and\ncost-performance optimization."}
{"id": "2508.04160", "pdf": "https://arxiv.org/pdf/2508.04160.pdf", "abs": "https://arxiv.org/abs/2508.04160", "title": "DRIVE-T: A Methodology for Discriminative and Representative Data Viz Item Selection for Literacy Construct and Assessment", "authors": ["Angela Locoro", "Silvia Golia", "Davide Falessi"], "categories": ["cs.HC", "cs.CV", "K.3; K.3.2"], "comment": null, "summary": "The underspecification of progressive levels of difficulty in measurement\nconstructs design and assessment tests for data visualization literacy may\nhinder the expressivity of measurements in both test design and test reuse. To\nmitigate this problem, this paper proposes DRIVE-T (Discriminating and\nRepresentative Items for Validating Expressive Tests), a methodology designed\nto drive the construction and evaluation of assessment items. Given a data\nvizualization, DRIVE-T supports the identification of task-based items\ndiscriminability and representativeness for measuring levels of data\nvisualization literacy. DRIVE-T consists of three steps: (1) tagging task-based\nitems associated with a set of data vizualizations; (2) rating them by\nindependent raters for their difficulty; (3) analysing raters' raw scores\nthrough a Many-Facet Rasch Measurement model. In this way, we can observe the\nemergence of difficulty levels of the measurement construct, derived from the\ndiscriminability and representativeness of task-based items for each data\nvizualization, ordered into Many-Facets construct levels. In this study, we\nshow and apply each step of the methodology to an item bank, which models the\ndifficulty levels of a measurement construct approximating a latent construct\nfor data visualization literacy. This measurement construct is drawn from\nsemiotics, i.e., based on the syntax, semantics and pragmatics knowledge that\neach data visualization may require to be mastered by people. The DRIVE-T\nmethodology operationalises an inductive approach, observable in a post-design\nphase of the items preparation, for formative-style and practice-based\nmeasurement construct emergence. A pilot study with items selected through the\napplication of DRIVE-T is also presented to test our approach."}
{"id": "2509.04469", "pdf": "https://arxiv.org/pdf/2509.04469.pdf", "abs": "https://arxiv.org/abs/2509.04469", "title": "Multi-Modal Vision vs. Text-Based Parsing: Benchmarking LLM Strategies for Invoice Processing", "authors": ["David Berghaus", "Armin Berger", "Lars Hillebrand", "Kostadin Cvejoski", "Rafet Sifa"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper benchmarks eight multi-modal large language models from three\nfamilies (GPT-5, Gemini 2.5, and open-source Gemma 3) on three diverse openly\navailable invoice document datasets using zero-shot prompting. We compare two\nprocessing strategies: direct image processing using multi-modal capabilities\nand a structured parsing approach converting documents to markdown first.\nResults show native image processing generally outperforms structured\napproaches, with performance varying across model types and document\ncharacteristics. This benchmark provides insights for selecting appropriate\nmodels and processing strategies for automated document systems. Our code is\navailable online."}
{"id": "2509.03164", "pdf": "https://arxiv.org/pdf/2509.03164.pdf", "abs": "https://arxiv.org/abs/2509.03164", "title": "OPRA-Vis: Visual Analytics System to Assist Organization-Public Relationship Assessment with Large Language Models", "authors": ["Sangbong Yoo", "Seongbum Seo", "Chanyoung Yoon", "Hyelim Lee", "Jeong-Nam Kim", "Chansoo Kim", "Yun Jang", "Takanori Fujiwara"], "categories": ["cs.HC"], "comment": null, "summary": "Analysis of public opinions collected from digital media helps organizations\nmaintain positive relationships with the public. Such public relations (PR)\nanalysis often involves assessing opinions, for example, measuring how strongly\npeople trust an organization. Pre-trained Large Language Models (LLMs) hold\ngreat promise for supporting Organization-Public Relationship Assessment (OPRA)\nbecause they can map unstructured public text to OPRA dimensions and articulate\nrationales through prompting. However, adapting LLMs for PR analysis typically\nrequires fine-tuning on large labeled datasets, which is both labor-intensive\nand knowledge-intensive, making it difficult for PR researchers to apply these\nmodels. In this paper, we present OPRA-Vis, a visual analytics system that\nleverages LLMs for OPRA without requiring extensive labeled data. Our framework\nemploys Chain-of-Thought prompting to guide LLMs in analyzing public opinion\ndata by incorporating PR expertise directly into the reasoning process.\nFurthermore, OPRA-Vis provides visualizations that reveal the clues and\nreasoning paths used by LLMs, enabling users to explore, critique, and refine\nmodel decisions. We demonstrate the effectiveness of OPRA-Vis through two\nreal-world use cases and evaluate it quantitatively, through comparisons with\nalternative LLMs and prompting strategies, and qualitatively, through\nassessments of usability, effectiveness, and expert feedback."}
{"id": "2509.04470", "pdf": "https://arxiv.org/pdf/2509.04470.pdf", "abs": "https://arxiv.org/abs/2509.04470", "title": "COCORELI: Cooperative, Compositional Reconstitution \\& Execution of Language Instructions", "authors": ["Swarnadeep Bhar", "Omar Naim", "Eleni Metheniti", "Bastien Navarri", "Lo√Øc Cabannes", "Morteza Ezzabady", "Nicholas Asher"], "categories": ["cs.CL", "cs.AI"], "comment": "18 pages", "summary": "We present COCORELI, a hybrid agent framework designed to tackle the\nlimitations of large language models (LLMs) in tasks requiring: following\ncomplex instructions, minimizing hallucination, and spatial reasoning. COCORELI\nintegrates medium-sized LLM agents with novel abstraction mechanisms and a\ndiscourse module to parse instructions to in-context learn dynamic, high-level\nrepresentations of the environment. Experiments on natural collaborative\nconstruction tasks show that COCORELI outperforms single-LLM CoT and agentic\nLLM systems, all using larger LLMs. It manages to largely avoid hallucinations,\nidentify missing information, ask for clarifications, and update its learned\nobjects. COCORELI's abstraction abilities extend beyond ENVIRONMENT, as shown\nin the ToolBench API completion task."}
{"id": "2412.13501", "pdf": "https://arxiv.org/pdf/2412.13501.pdf", "abs": "https://arxiv.org/abs/2412.13501", "title": "GUI Agents: A Survey", "authors": ["Dang Nguyen", "Jian Chen", "Yu Wang", "Gang Wu", "Namyong Park", "Zhengmian Hu", "Hanjia Lyu", "Junda Wu", "Ryan Aponte", "Yu Xia", "Xintong Li", "Jing Shi", "Hongjie Chen", "Viet Dac Lai", "Zhouhang Xie", "Sungchul Kim", "Ruiyi Zhang", "Tong Yu", "Mehrab Tanjim", "Nesreen K. Ahmed", "Puneet Mathur", "Seunghyun Yoon", "Lina Yao", "Branislav Kveton", "Thien Huu Nguyen", "Trung Bui", "Tianyi Zhou", "Ryan A. Rossi", "Franck Dernoncourt"], "categories": ["cs.AI", "cs.HC"], "comment": "Accepted to Findings of ACL 2025", "summary": "Graphical User Interface (GUI) agents, powered by Large Foundation Models,\nhave emerged as a transformative approach to automating human-computer\ninteraction. These agents autonomously interact with digital systems or\nsoftware applications via GUIs, emulating human actions such as clicking,\ntyping, and navigating visual elements across diverse platforms. Motivated by\nthe growing interest and fundamental importance of GUI agents, we provide a\ncomprehensive survey that categorizes their benchmarks, evaluation metrics,\narchitectures, and training methods. We propose a unified framework that\ndelineates their perception, reasoning, planning, and acting capabilities.\nFurthermore, we identify important open challenges and discuss key future\ndirections. Finally, this work serves as a basis for practitioners and\nresearchers to gain an intuitive understanding of current progress, techniques,\nbenchmarks, and critical open problems that remain to be addressed."}
{"id": "2509.04471", "pdf": "https://arxiv.org/pdf/2509.04471.pdf", "abs": "https://arxiv.org/abs/2509.04471", "title": "MOSAIC: A Multilingual, Taxonomy-Agnostic, and Computationally Efficient Approach for Radiological Report Classification", "authors": ["Alice Schiavone", "Marco Fraccaro", "Lea Marie Pehrson", "Silvia Ingala", "Rasmus Bonnevie", "Michael Bachmann Nielsen", "Vincent Beliveau", "Melanie Ganz", "Desmond Elliott"], "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 14 pages including references and appendix. 9 figures.\n  Preprint", "summary": "Radiology reports contain rich clinical information that can be used to train\nimaging models without relying on costly manual annotation. However, existing\napproaches face critical limitations: rule-based methods struggle with\nlinguistic variability, supervised models require large annotated datasets, and\nrecent LLM-based systems depend on closed-source or resource-intensive models\nthat are unsuitable for clinical use. Moreover, current solutions are largely\nrestricted to English and single-modality, single-taxonomy datasets. We\nintroduce MOSAIC, a multilingual, taxonomy-agnostic, and computationally\nefficient approach for radiological report classification. Built on a compact\nopen-access language model (MedGemma-4B), MOSAIC supports both zero-/few-shot\nprompting and lightweight fine-tuning, enabling deployment on consumer-grade\nGPUs. We evaluate MOSAIC across seven datasets in English, Spanish, French, and\nDanish, spanning multiple imaging modalities and label taxonomies. The model\nachieves a mean macro F1 score of 88 across five chest X-ray datasets,\napproaching or exceeding expert-level performance, while requiring only 24 GB\nof GPU memory. With data augmentation, as few as 80 annotated samples are\nsufficient to reach a weighted F1 score of 82 on Danish reports, compared to 86\nwith the full 1600-sample training set. MOSAIC offers a practical alternative\nto large or proprietary LLMs in clinical settings. Code and models are\nopen-source. We invite the community to evaluate and extend MOSAIC on new\nlanguages, taxonomies, and modalities."}
{"id": "2504.03352", "pdf": "https://arxiv.org/pdf/2504.03352.pdf", "abs": "https://arxiv.org/abs/2504.03352", "title": "StereoDetect: Detecting Stereotypes and Anti-stereotypes the Correct Way Using Social Psychological Underpinnings", "authors": ["Kaustubh Shivshankar Shejole", "Pushpak Bhattacharyya"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "Stereotypes are known to have very harmful effects, making their detection\ncritically important. However, current research predominantly focuses on\ndetecting and evaluating stereotypical biases, thereby leaving the study of\nstereotypes in its early stages. Our study revealed that many works have failed\nto clearly distinguish between stereotypes and stereotypical biases, which has\nsignificantly slowed progress in advancing research in this area. Stereotype\nand Anti-stereotype detection is a problem that requires social knowledge;\nhence, it is one of the most difficult areas in Responsible AI. This work\ninvestigates this task, where we propose a five-tuple definition and provide\nprecise terminologies disentangling stereotypes, anti-stereotypes,\nstereotypical bias, and general bias. We provide a conceptual framework\ngrounded in social psychology for reliable detection. We identify key\nshortcomings in existing benchmarks for this task of stereotype and\nanti-stereotype detection. To address these gaps, we developed StereoDetect, a\nwell curated, definition-aligned benchmark dataset designed for this task. We\nshow that sub-10B language models and GPT-4o frequently misclassify\nanti-stereotypes and fail to recognize neutral overgeneralizations. We\ndemonstrate StereoDetect's effectiveness through multiple qualitative and\nquantitative comparisons with existing benchmarks and models fine-tuned on\nthem. The dataset and code is available at\nhttps://github.com/KaustubhShejole/StereoDetect."}
{"id": "2509.04472", "pdf": "https://arxiv.org/pdf/2509.04472.pdf", "abs": "https://arxiv.org/abs/2509.04472", "title": "RECAP: REwriting Conversations for Intent Understanding in Agentic Planning", "authors": ["Kushan Mitra", "Dan Zhang", "Hannah Kim", "Estevam Hruschka"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Understanding user intent is essential for effective planning in\nconversational assistants, particularly those powered by large language models\n(LLMs) coordinating multiple agents. However, real-world dialogues are often\nambiguous, underspecified, or dynamic, making intent detection a persistent\nchallenge. Traditional classification-based approaches struggle to generalize\nin open-ended settings, leading to brittle interpretations and poor downstream\nplanning. We propose RECAP (REwriting Conversations for Agent Planning), a new\nbenchmark designed to evaluate and advance intent rewriting, reframing\nuser-agent dialogues into concise representations of user goals. RECAP captures\ndiverse challenges such as ambiguity, intent drift, vagueness, and mixed-goal\nconversations. Alongside the dataset, we introduce an LLM-based evaluator that\nassesses planning utility given the rewritten intent. Using RECAP, we develop a\nprompt-based rewriting approach that outperforms baselines. We further\ndemonstrate that fine-tuning two DPO-based rewriters yields additional utility\ngains. Our results highlight intent rewriting as a critical and tractable\ncomponent for improving agent planning in open-domain dialogue systems."}
{"id": "2505.22809", "pdf": "https://arxiv.org/pdf/2505.22809.pdf", "abs": "https://arxiv.org/abs/2505.22809", "title": "First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons & Dragons Gameplay", "authors": ["Andrew Zhu", "Evan Osgood", "Chris Callison-Burch"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "9 pages, 5 figures. COLM 2025 Workshop on AI Agents", "summary": "Much work has been done on conversational LLM agents which directly assist\nhuman users with tasks. We present an alternative paradigm for interacting with\nLLM agents, which we call \"overhearing agents\". These overhearing agents do not\nactively participate in conversation -- instead, they \"listen in\" on\nhuman-to-human conversations and perform background tasks or provide\nsuggestions to assist the user. In this work, we explore the overhearing agents\nparadigm through the lens of Dungeons & Dragons gameplay. We present an\nin-depth study using large multimodal audio-language models as overhearing\nagents to assist a Dungeon Master. We perform a human evaluation to examine the\nhelpfulness of such agents and find that some large audio-language models have\nthe emergent ability to perform overhearing agent tasks using implicit audio\ncues. Finally, we release Python libraries and our project code to support\nfurther research into the overhearing agents paradigm at\nhttps://github.com/zhudotexe/overhearing_agents."}
{"id": "2509.04473", "pdf": "https://arxiv.org/pdf/2509.04473.pdf", "abs": "https://arxiv.org/abs/2509.04473", "title": "SpeechLLM: Unified Speech and Language Model for Enhanced Multi-Task Understanding in Low Resource Settings", "authors": ["Jaekwon Yoo", "Kunal Chandiramani", "Divya Tadimeti", "Abenezer Girma", "Chandra Dhir"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While integrating speech encoder with LLM requires substantial data and\nresources, use cases face limitations due to insufficient availability. To\naddress this, we propose a solution with a parameter-efficient adapter that\nconverts speech embeddings into LLM-compatible tokens, focusing on end-to-end\nautomatic speech recognition (ASR), named entity recognition (NER), and\nsentiment analysis (SA). To reduce labeling costs, we employ an LLM-based\nsynthetic dataset annotation technique. The proposed adapter, using 7x fewer\ntrainable parameters, achieves significant performance gains: a 26% relative\nWord Error Rates (WER) improvement on the LibriSpeech ASR task, a 6.3% relative\nF1 score increase on the NER task, and a 32% relative F1 score boost on the SA\ntask. Moreover, using advanced techniques such as adding a classifier\nregularizer and optimizing the LLM with Low-Rank Adaptation (LoRA) yields\nnotable performance gains, with Spoken Language Understanding Evaluation (SLUE)\nscore improvement of 6.6% and 9.5%"}
{"id": "2507.00008", "pdf": "https://arxiv.org/pdf/2507.00008.pdf", "abs": "https://arxiv.org/abs/2507.00008", "title": "DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning", "authors": ["Hang Wu", "Hongkai Chen", "Yujun Cai", "Chang Liu", "Qingwen Ye", "Ming-Hsuan Yang", "Yiwei Wang"], "categories": ["cs.AI", "cs.CV", "cs.HC"], "comment": "EMNLP 2025 Main Conference", "summary": "Grounding natural language queries in graphical user interfaces (GUIs) poses\nunique challenges due to the diversity of visual elements, spatial clutter, and\nthe ambiguity of language. In this paper, we introduce DiMo-GUI, a\ntraining-free framework for GUI grounding that leverages two core strategies:\ndynamic visual grounding and modality-aware optimization. Instead of treating\nthe GUI as a monolithic image, our method splits the input into textual\nelements and iconic elements, allowing the model to reason over each modality\nindependently using general-purpose vision-language models. When predictions\nare ambiguous or incorrect, DiMo-GUI dynamically focuses attention by\ngenerating candidate focal regions centered on the model's initial predictions\nand incrementally zooms into subregions to refine the grounding result. This\nhierarchical refinement process helps disambiguate visually crowded layouts\nwithout the need for additional training or annotations. We evaluate our\napproach on standard GUI grounding benchmarks and demonstrate consistent\nimprovements over baseline inference pipelines, highlighting the effectiveness\nof combining modality separation with region-focused reasoning."}
{"id": "2509.04474", "pdf": "https://arxiv.org/pdf/2509.04474.pdf", "abs": "https://arxiv.org/abs/2509.04474", "title": "Scaling Up, Speeding Up: A Benchmark of Speculative Decoding for Efficient LLM Test-Time Scaling", "authors": ["Shengyin Sun", "Yiming Li", "Xing Li", "Yingzhao Lian", "Weizhe Lin", "Hui-Ling Zhen", "Zhiyuan Yang", "Chen Chen", "Xianzhi Yu", "Mingxuan Yuan", "Chen Ma"], "categories": ["cs.CL", "cs.AI"], "comment": "18 pages", "summary": "Test-time scaling has emerged as a powerful paradigm for enhancing the\nreasoning capabilities of large language models (LLMs) by allocating additional\ncomputational resources during inference. However, this paradigm is inherently\ninefficient due to the generation of redundant and repetitive reasoning traces,\nleading to significant computational overhead. Speculative decoding offers a\npromising avenue for mitigating this inefficiency, yet its efficacy in the\nstructured, repetition-rich context of test-time scaling remains largely\nunexplored. To bridge this gap, we introduce the first comprehensive benchmark\ndesigned to evaluate speculative decoding methods for accelerating LLM\ntest-time scaling. Our benchmark provides consistent experimental protocols\nacross representative test-time scaling paradigms (e.g., Best-of-N sampling and\nmulti-round thinking), enabling a fair comparison of three major categories of\nspeculative decoding: model-based, training-based, and n-gram-based methods.\nExtensive experiments reveal that simple n-gram-based methods effectively\ncapture repetitive patterns, demonstrating unique potential in accelerating\ntest-time scaling. This phenomenon demonstrates the value of integrating\nn-gram-based methods with model-based or training-based approaches to balance\nacceleration for both repetitive and diverse reasoning in test-time scaling. We\nhope this benchmark spurs further research on speculative decoding for\ntest-time scaling, enabling faster and more practical reasoning in LLMs through\nbetter handling of repetitive and diverse reasoning paths."}
{"id": "2509.02544", "pdf": "https://arxiv.org/pdf/2509.02544.pdf", "abs": "https://arxiv.org/abs/2509.02544", "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning", "authors": ["Haoming Wang", "Haoyang Zou", "Huatong Song", "Jiazhan Feng", "Junjie Fang", "Junting Lu", "Longxiang Liu", "Qinyu Luo", "Shihao Liang", "Shijue Huang", "Wanjun Zhong", "Yining Ye", "Yujia Qin", "Yuwen Xiong", "Yuxin Song", "Zhiyong Wu", "Aoyan Li", "Bo Li", "Chen Dun", "Chong Liu", "Daoguang Zan", "Fuxing Leng", "Hanbin Wang", "Hao Yu", "Haobin Chen", "Hongyi Guo", "Jing Su", "Jingjia Huang", "Kai Shen", "Kaiyu Shi", "Lin Yan", "Peiyao Zhao", "Pengfei Liu", "Qinghao Ye", "Renjie Zheng", "Shulin Xin", "Wayne Xin Zhao", "Wen Heng", "Wenhao Huang", "Wenqian Wang", "Xiaobo Qin", "Yi Lin", "Youbin Wu", "Zehui Chen", "Zihao Wang", "Baoquan Zhong", "Xinchun Zhang", "Xujing Li", "Yuanfan Li", "Zhongkai Zhao", "Chengquan Jiang", "Faming Wu", "Haotian Zhou", "Jinlin Pang", "Li Han", "Qi Liu", "Qianli Ma", "Siyao Liu", "Songhua Cai", "Wenqi Fu", "Xin Liu", "Yaohui Wang", "Zhi Zhang", "Bo Zhou", "Guoliang Li", "Jiajun Shi", "Jiale Yang", "Jie Tang", "Li Li", "Qihua Han", "Taoran Lu", "Woyu Lin", "Xiaokang Tong", "Xinyao Li", "Yichi Zhang", "Yu Miao", "Zhengxuan Jiang", "Zili Li", "Ziyuan Zhao", "Chenxin Li", "Dehua Ma", "Feng Lin", "Ge Zhang", "Haihua Yang", "Hangyu Guo", "Hongda Zhu", "Jiaheng Liu", "Junda Du", "Kai Cai", "Kuanye Li", "Lichen Yuan", "Meilan Han", "Minchao Wang", "Shuyue Guo", "Tianhao Cheng", "Xiaobo Ma", "Xiaojun Xiao", "Xiaolong Huang", "Xinjie Chen", "Yidi Du", "Yilin Chen", "Yiwen Wang", "Zhaojian Li", "Zhenzhu Yang", "Zhiyuan Zeng", "Chaolin Jin", "Chen Li", "Hao Chen", "Haoli Chen", "Jian Chen", "Qinghao Zhao", "Guang Shi"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": null, "summary": "The development of autonomous agents for graphical user interfaces (GUIs)\npresents major challenges in artificial intelligence. While recent advances in\nnative agent models have shown promise by unifying perception, reasoning,\naction, and memory through end-to-end learning, open problems remain in data\nscalability, multi-turn reinforcement learning (RL), the limitations of\nGUI-only operation, and environment stability. In this technical report, we\npresent UI-TARS-2, a native GUI-centered agent model that addresses these\nchallenges through a systematic training methodology: a data flywheel for\nscalable data generation, a stabilized multi-turn RL framework, a hybrid GUI\nenvironment that integrates file systems and terminals, and a unified sandbox\nplatform for large-scale rollouts. Empirical evaluation demonstrates that\nUI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5.\nOn GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on\nWindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines\nsuch as Claude and OpenAI agents. In game environments, it attains a mean\nnormalized score of 59.8 across a 15-game suite-roughly 60% of human-level\nperformance-and remains competitive with frontier proprietary models (e.g.,\nOpenAI o3) on LMGame-Bench. Additionally, the model can generalize to\nlong-horizon information-seeking tasks and software engineering benchmarks,\nhighlighting its robustness across diverse agent tasks. Detailed analyses of\ntraining dynamics further provide insights into achieving stability and\nefficiency in large-scale agent RL. These results underscore UI-TARS-2's\npotential to advance the state of GUI agents and exhibit strong generalization\nto real-world interactive scenarios."}
{"id": "2509.04475", "pdf": "https://arxiv.org/pdf/2509.04475.pdf", "abs": "https://arxiv.org/abs/2509.04475", "title": "ParaThinker: Native Parallel Thinking as a New Paradigm to Scale LLM Test-time Compute", "authors": ["Hao Wen", "Yifan Su", "Feifei Zhang", "Yunxin Liu", "Yunhao Liu", "Ya-Qin Zhang", "Yuanchun Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have been driven by test-time\ncompute scaling - a strategy that improves reasoning by generating longer,\nsequential thought processes. While effective, this approach encounters a\nsignificant bottleneck as computation increases, where further computation\noffers only marginal performance gains. We argue this ceiling is not an\ninherent limit of the model's capability but a flaw in the scaling strategy\nitself, a phenomenon we term \"Tunnel Vision\", where a model's imperfect initial\nsteps lock it into a suboptimal reasoning path. To overcome this, we introduce\na new scaling paradigm: native thought parallelism. We present ParaThinker, an\nend-to-end framework that trains an LLM to generate multiple, diverse reasoning\npaths in parallel and synthesize them into a superior final answer. By\nexploring different lines of thoughts simultaneously, ParaThinker effectively\nsidesteps the Tunnel Vision issue and unlocks the model's latent reasoning\npotential. Our approach demonstrates that scaling compute in parallel (width)\nis a more effective and efficient way to superior reasoning than simply scaling\nsequentially (depth). On challenging reasoning benchmarks, ParaThinker achieves\nsubstantial accuracy improvements over sequential LLMs (12.3% for 1.5B and 7.5%\nfor 7B models on average with 8 parallel paths), while adding only negligible\nlatency overhead (7.1%). This enables smaller models to surpass much larger\ncounterparts and establishes parallel thinking as a critical, efficient\ndimension for scaling future LLMs."}
{"id": "2509.03728", "pdf": "https://arxiv.org/pdf/2509.03728.pdf", "abs": "https://arxiv.org/abs/2509.03728", "title": "PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming", "authors": ["Wesley Hanwen Deng", "Sunnie S. Y. Kim", "Akshita Jha", "Ken Holstein", "Motahhare Eslami", "Lauren Wilcox", "Leon A Gatys"], "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Recent developments in AI governance and safety research have called for\nred-teaming methods that can effectively surface potential risks posed by AI\nmodels. Many of these calls have emphasized how the identities and backgrounds\nof red-teamers can shape their red-teaming strategies, and thus the kinds of\nrisks they are likely to uncover. While automated red-teaming approaches\npromise to complement human red-teaming by enabling larger-scale exploration of\nmodel behavior, current approaches do not consider the role of identity. As an\ninitial step towards incorporating people's background and identities in\nautomated red-teaming, we develop and evaluate a novel method, PersonaTeaming,\nthat introduces personas in the adversarial prompt generation process to\nexplore a wider spectrum of adversarial strategies. In particular, we first\nintroduce a methodology for mutating prompts based on either \"red-teaming\nexpert\" personas or \"regular AI user\" personas. We then develop a dynamic\npersona-generating algorithm that automatically generates various persona types\nadaptive to different seed prompts. In addition, we develop a set of new\nmetrics to explicitly measure the \"mutation distance\" to complement existing\ndiversity measurements of adversarial prompts. Our experiments show promising\nimprovements (up to 144.1%) in the attack success rates of adversarial prompts\nthrough persona mutation, while maintaining prompt diversity, compared to\nRainbowPlus, a state-of-the-art automated red-teaming method. We discuss the\nstrengths and limitations of different persona types and mutation methods,\nshedding light on future opportunities to explore complementarities between\nautomated and human red-teaming approaches."}
{"id": "2509.04476", "pdf": "https://arxiv.org/pdf/2509.04476.pdf", "abs": "https://arxiv.org/abs/2509.04476", "title": "Training Text-to-Molecule Models with Context-Aware Tokenization", "authors": ["Seojin Kim", "Hyeontae Song", "Jaehyun Nam", "Jinwoo Shin"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 Findings", "summary": "Recently, text-to-molecule models have shown great potential across various\nchemical applications, e.g., drug-discovery. These models adapt language models\nto molecular data by representing molecules as sequences of atoms. However,\nthey rely on atom-level tokenizations, which primarily focus on modeling local\nconnectivity, thereby limiting the ability of models to capture the global\nstructural context within molecules. To tackle this issue, we propose a novel\ntext-to-molecule model, coined Context-Aware Molecular T5 (CAMT5). Inspired by\nthe significance of the substructure-level contexts in understanding molecule\nstructures, e.g., ring systems, we introduce substructure-level tokenization\nfor text-to-molecule models. Building on our tokenization scheme, we develop an\nimportance-based training strategy that prioritizes key substructures, enabling\nCAMT5 to better capture the molecular semantics. Extensive experiments verify\nthe superiority of CAMT5 in various text-to-molecule generation tasks.\nIntriguingly, we find that CAMT5 outperforms the state-of-the-art methods using\nonly 2% of training tokens. In addition, we propose a simple yet effective\nensemble strategy that aggregates the outputs of text-to-molecule models to\nfurther boost the generation performance. Code is available at\nhttps://github.com/Songhyeontae/CAMT5.git."}
{"id": "2509.04478", "pdf": "https://arxiv.org/pdf/2509.04478.pdf", "abs": "https://arxiv.org/abs/2509.04478", "title": "An End-to-End System for Culturally-Attuned Driving Feedback using a Dual-Component NLG Engine", "authors": ["Iniakpokeikiye Peter Thompson", "Yi Dewei", "Reiter Ehud"], "categories": ["cs.CL", "F.2.2; I.2.7"], "comment": "The paper has 5 figures and 1 table", "summary": "This paper presents an end-to-end mobile system that delivers\nculturally-attuned safe driving feedback to drivers in Nigeria, a low-resource\nenvironment with significant infrastructural challenges. The core of the system\nis a novel dual-component Natural Language Generation (NLG) engine that\nprovides both legally-grounded safety tips and persuasive, theory-driven\nbehavioural reports. We describe the complete system architecture, including an\nautomatic trip detection service, on-device behaviour analysis, and a\nsophisticated NLG pipeline that leverages a two-step reflection process to\nensure high-quality feedback. The system also integrates a specialized machine\nlearning model for detecting alcohol-influenced driving, a key local safety\nissue. The architecture is engineered for robustness against intermittent\nconnectivity and noisy sensor data. A pilot deployment with 90 drivers\ndemonstrates the viability of our approach, and initial results on detected\nunsafe behaviours are presented. This work provides a framework for applying\ndata-to-text and AI systems to achieve social good."}
{"id": "2509.04479", "pdf": "https://arxiv.org/pdf/2509.04479.pdf", "abs": "https://arxiv.org/abs/2509.04479", "title": "No Clustering, No Routing: How Transformers Actually Process Rare Tokens", "authors": ["Jing Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models struggle with rare token prediction, yet the mechanisms\ndriving their specialization remain unclear. Prior work identified specialized\n``plateau'' neurons for rare tokens following distinctive three-regime\ninfluence patterns \\cite{liu2025emergent}, but their functional organization is\nunknown. We investigate this through neuron influence analyses, graph-based\nclustering, and attention head ablations in GPT-2 XL and Pythia models. Our\nfindings show that: (1) rare token processing requires additional plateau\nneurons beyond the power-law regime sufficient for common tokens, forming dual\ncomputational regimes; (2) plateau neurons are spatially distributed rather\nthan forming modular clusters; and (3) attention mechanisms exhibit no\npreferential routing to specialists. These results demonstrate that rare token\nspecialization arises through distributed, training-driven differentiation\nrather than architectural modularity, preserving context-sensitive flexibility\nwhile achieving adaptive capacity allocation."}
{"id": "2509.04480", "pdf": "https://arxiv.org/pdf/2509.04480.pdf", "abs": "https://arxiv.org/abs/2509.04480", "title": "Discrete Prompt Tuning via Recursive Utilization of Black-box Multimodal Large Language Model for Personalized Visual Emotion Recognition", "authors": ["Ryo Takahashi", "Naoki Saito", "Keisuke Maeda", "Takahiro Ogawa", "Miki Haseyama"], "categories": ["cs.CL", "cs.LG"], "comment": "11 pages, 4 figures", "summary": "Visual Emotion Recognition (VER) is an important research topic due to its\nwide range of applications, including opinion mining and advertisement design.\nExtending this capability to recognize emotions at the individual level further\nbroadens its potential applications. Recently, Multimodal Large Language Models\n(MLLMs) have attracted increasing attention and demonstrated performance\ncomparable to that of conventional VER methods. However, MLLMs are trained on\nlarge and diverse datasets containing general opinions, which causes them to\nfavor majority viewpoints and familiar patterns. This tendency limits their\nperformance in a personalized VER, which is crucial for practical and\nreal-world applications, and indicates a key area for improvement. To address\nthis limitation, the proposed method employs discrete prompt tuning inspired by\nthe process of humans' prompt engineering to adapt the VER task to each\nindividual. Our method selects the best natural language representation from\nthe generated prompts and uses it to update the prompt for the realization of\naccurate personalized VER."}
{"id": "2509.04482", "pdf": "https://arxiv.org/pdf/2509.04482.pdf", "abs": "https://arxiv.org/abs/2509.04482", "title": "Energy Landscapes Enable Reliable Abstention in Retrieval-Augmented Large Language Models for Healthcare", "authors": ["Ravi Shankar", "Sheng Wong", "Lin Li", "Magdalena Bachmann", "Alex Silverthorne", "Beth Albert", "Gabriel Davis Jones"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reliable abstention is critical for retrieval-augmented generation (RAG)\nsystems, particularly in safety-critical domains such as women's health, where\nincorrect answers can lead to harm. We present an energy-based model (EBM) that\nlearns a smooth energy landscape over a dense semantic corpus of 2.6M\nguideline-derived questions, enabling the system to decide when to generate or\nabstain. We benchmark the EBM against a calibrated softmax baseline and a\nk-nearest neighbour (kNN) density heuristic across both easy and hard\nabstention splits, where hard cases are semantically challenging\nnear-distribution queries. The EBM achieves superior abstention performance\nabstention on semantically hard cases, reaching AUROC 0.961 versus 0.950 for\nsoftmax, while also reducing FPR@95 (0.235 vs 0.331). On easy negatives,\nperformance is comparable across methods, but the EBM's advantage becomes most\npronounced in safety-critical hard distributions. A comprehensive ablation with\ncontrolled negative sampling and fair data exposure shows that robustness stems\nprimarily from the energy scoring head, while the inclusion or exclusion of\nspecific negative types (hard, easy, mixed) sharpens decision boundaries but is\nnot essential for generalisation to hard cases. These results demonstrate that\nenergy-based abstention scoring offers a more reliable confidence signal than\nprobability-based softmax confidence, providing a scalable and interpretable\nfoundation for safe RAG systems."}
{"id": "2509.04483", "pdf": "https://arxiv.org/pdf/2509.04483.pdf", "abs": "https://arxiv.org/abs/2509.04483", "title": "DecMetrics: Structured Claim Decomposition Scoring for Factually Consistent LLM Outputs", "authors": ["Minghui Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Claim decomposition plays a crucial role in the fact-checking process by\nbreaking down complex claims into simpler atomic components and identifying\ntheir unfactual elements. Despite its importance, current research primarily\nfocuses on generative methods for decomposition, with insufficient emphasis on\nevaluating the quality of these decomposed atomic claims. To bridge this gap,\nwe introduce \\textbf{DecMetrics}, which comprises three new metrics:\n\\texttt{COMPLETENESS}, \\texttt{CORRECTNESS}, and \\texttt{SEMANTIC ENTROPY},\ndesigned to automatically assess the quality of claims produced by\ndecomposition models. Utilizing these metrics, we develop a lightweight claim\ndecomposition model, optimizing its performance through the integration of\nthese metrics as a reward function. Through automatic evaluation, our approach\naims to set a benchmark for claim decomposition, enhancing both the reliability\nand effectiveness of fact-checking systems."}
{"id": "2509.04484", "pdf": "https://arxiv.org/pdf/2509.04484.pdf", "abs": "https://arxiv.org/abs/2509.04484", "title": "The Good, the Bad and the Constructive: Automatically Measuring Peer Review's Utility for Authors", "authors": ["Abdelrahman Sadallah", "Tim Baumg√§rtner", "Iryna Gurevych", "Ted Briscoe"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "EMNLP 2025 Main", "summary": "Providing constructive feedback to paper authors is a core component of peer\nreview. With reviewers increasingly having less time to perform reviews,\nautomated support systems are required to ensure high reviewing quality, thus\nmaking the feedback in reviews useful for authors. To this end, we identify\nfour key aspects of review comments (individual points in weakness sections of\nreviews) that drive the utility for authors: Actionability, Grounding &\nSpecificity, Verifiability, and Helpfulness. To enable evaluation and\ndevelopment of models assessing review comments, we introduce the RevUtil\ndataset. We collect 1,430 human-labeled review comments and scale our data with\n10k synthetically labeled comments for training purposes. The synthetic data\nadditionally contains rationales, i.e., explanations for the aspect score of a\nreview comment. Employing the RevUtil dataset, we benchmark fine-tuned models\nfor assessing review comments on these aspects and generating rationales. Our\nexperiments demonstrate that these fine-tuned models achieve agreement levels\nwith humans comparable to, and in some cases exceeding, those of powerful\nclosed models like GPT-4o. Our analysis further reveals that machine-generated\nreviews generally underperform human reviews on our four aspects."}
{"id": "2509.04485", "pdf": "https://arxiv.org/pdf/2509.04485.pdf", "abs": "https://arxiv.org/abs/2509.04485", "title": "ASCENDgpt: A Phenotype-Aware Transformer Model for Cardiovascular Risk Prediction from Electronic Health Records", "authors": ["Chris Sainsbury", "Andreas Karwath"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present ASCENDgpt, a transformer-based model specifically designed for\ncardiovascular risk prediction from longitudinal electronic health records\n(EHRs). Our approach introduces a novel phenotype-aware tokenization scheme\nthat maps 47,155 raw ICD codes to 176 clinically meaningful phenotype tokens,\nachieving 99.6\\% consolidation of diagnosis codes while preserving semantic\ninformation. This phenotype mapping contributes to a total vocabulary of 10,442\ntokens - a 77.9\\% reduction when compared with using raw ICD codes directly. We\npretrain ASCENDgpt on sequences derived from 19402 unique individuals using a\nmasked language modeling objective, then fine-tune for time-to-event prediction\nof five cardiovascular outcomes: myocardial infarction (MI), stroke, major\nadverse cardiovascular events (MACE), cardiovascular death, and all-cause\nmortality. Our model achieves excellent discrimination on the held-out test set\nwith an average C-index of 0.816, demonstrating strong performance across all\noutcomes (MI: 0.792, stroke: 0.824, MACE: 0.800, cardiovascular death: 0.842,\nall-cause mortality: 0.824). The phenotype-based approach enables clinically\ninterpretable predictions while maintaining computational efficiency. Our work\ndemonstrates the effectiveness of domain-specific tokenization and pretraining\nfor EHR-based risk prediction tasks."}
{"id": "2509.04488", "pdf": "https://arxiv.org/pdf/2509.04488.pdf", "abs": "https://arxiv.org/abs/2509.04488", "title": "Serialized Output Prompting for Large Language Model-based Multi-Talker Speech Recognition", "authors": ["Hao Shi", "Yusuke Fujita", "Tomoya Mizumoto", "Lianbo Liu", "Atsushi Kojima", "Yui Sudo"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Prompts are crucial for task definition and for improving the performance of\nlarge language models (LLM)-based systems. However, existing LLM-based\nmulti-talker (MT) automatic speech recognition (ASR) systems either omit\nprompts or rely on simple task-definition prompts, with no prior work exploring\nthe design of prompts to enhance performance. In this paper, we propose\nextracting serialized output prompts (SOP) and explicitly guiding the LLM using\nstructured prompts to improve system performance (SOP-MT-ASR). A Separator and\nserialized Connectionist Temporal Classification (CTC) layers are inserted\nafter the speech encoder to separate and extract MT content from the mixed\nspeech encoding in a first-speaking-first-out manner. Subsequently, the SOP,\nwhich serves as a prompt for LLMs, is obtained by decoding the serialized CTC\noutputs using greedy search. To train the model effectively, we design a\nthree-stage training strategy, consisting of serialized output training (SOT)\nfine-tuning, serialized speech information extraction, and SOP-based\nadaptation. Experimental results on the LibriMix dataset show that, although\nthe LLM-based SOT model performs well in the two-talker scenario, it fails to\nfully leverage LLMs under more complex conditions, such as the three-talker\nscenario. The proposed SOP approach significantly improved performance under\nboth two- and three-talker conditions."}
{"id": "2509.04491", "pdf": "https://arxiv.org/pdf/2509.04491.pdf", "abs": "https://arxiv.org/abs/2509.04491", "title": "Refining Transcripts With TV Subtitles by Prompt-Based Weakly Supervised Training of ASR", "authors": ["Xinnian Zhao", "Hugo Van Hamme"], "categories": ["cs.CL", "cs.AI"], "comment": "eusipco2025", "summary": "This study proposes a novel approach to using TV subtitles within a weakly\nsupervised (WS) Automatic Speech Recognition (ASR) framework. Although TV\nsubtitles are readily available, their imprecise alignment with corresponding\naudio limits their applicability as supervised targets for verbatim\ntranscription. Rather than using subtitles as direct supervision signals, our\nmethod reimagines them as context-rich prompts. This design enables the model\nto handle discrepancies between spoken audio and subtitle text. Instead,\ngenerated pseudo transcripts become the primary targets, with subtitles acting\nas guiding cues for iterative refinement. To further enhance the process, we\nintroduce a weighted attention mechanism that emphasizes relevant subtitle\ntokens during inference. Our experiments demonstrate significant improvements\nin transcription accuracy, highlighting the effectiveness of the proposed\nmethod in refining transcripts. These enhanced pseudo-labeled datasets provide\nhigh-quality foundational resources for training robust ASR systems."}
{"id": "2509.04492", "pdf": "https://arxiv.org/pdf/2509.04492.pdf", "abs": "https://arxiv.org/abs/2509.04492", "title": "Learned Hallucination Detection in Black-Box LLMs using Token-level Entropy Production Rate", "authors": ["Charles Moslonka", "Hicham Randrianarivo", "Arthur Garnier", "Emmanuel Malherbe"], "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 7 figures, 1 table. pre-print version", "summary": "Hallucinations in Large Language Model (LLM) outputs for Question Answering\n(QA) tasks critically undermine their real-world reliability. This paper\nintroduces an applied methodology for robust, one-shot hallucination detection,\nspecifically designed for scenarios with limited data access, such as\ninteracting with black-box LLM APIs that typically expose only a few top\ncandidate log-probabilities per token. Our approach derives uncertainty\nindicators directly from these readily available log-probabilities generated\nduring non-greedy decoding. We first derive an Entropy Production Rate (EPR)\nmetric that offers baseline performance, later augmented with supervised\nlearning. Our learned model uses features representing the entropic\ncontributions of the accessible top-ranked tokens within a single generated\nsequence, requiring no multiple query re-runs. Evaluated across diverse QA\ndatasets and multiple LLMs, this estimator significantly improves hallucination\ndetection over using EPR alone. Crucially, high performance is demonstrated\nusing only the typically small set of available log-probabilities (e.g., top\n<10 per token), confirming its practical efficiency and suitability for these\nAPI-constrained deployments. This work provides a readily deployable technique\nto enhance the trustworthiness of LLM responses from a single generation pass\nin QA and Retrieval-Augmented Generation (RAG) systems, with its utility\nfurther demonstrated in a finance framework analyzing responses to queries on\nannual reports from an industrial dataset."}
{"id": "2509.04497", "pdf": "https://arxiv.org/pdf/2509.04497.pdf", "abs": "https://arxiv.org/abs/2509.04497", "title": "A Narrative-Driven Computational Framework for Clinician Burnout Surveillance", "authors": ["Syed Ahmad Chan Bukhari", "Fazel Keshtkar", "Alyssa Meczkowska"], "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 6 Figure", "summary": "Clinician burnout poses a substantial threat to patient safety, particularly\nin high-acuity intensive care units (ICUs). Existing research predominantly\nrelies on retrospective survey tools or broad electronic health record (EHR)\nmetadata, often overlooking the valuable narrative information embedded in\nclinical notes. In this study, we analyze 10,000 ICU discharge summaries from\nMIMIC-IV, a publicly available database derived from the electronic health\nrecords of Beth Israel Deaconess Medical Center. The dataset encompasses\ndiverse patient data, including vital signs, medical orders, diagnoses,\nprocedures, treatments, and deidentified free-text clinical notes. We introduce\na hybrid pipeline that combines BioBERT sentiment embeddings fine-tuned for\nclinical narratives, a lexical stress lexicon tailored for clinician burnout\nsurveillance, and five-topic latent Dirichlet allocation (LDA) with workload\nproxies. A provider-level logistic regression classifier achieves a precision\nof 0.80, a recall of 0.89, and an F1 score of 0.84 on a stratified hold-out\nset, surpassing metadata-only baselines by greater than or equal to 0.17 F1\nscore. Specialty-specific analysis indicates elevated burnout risk among\nproviders in Radiology, Psychiatry, and Neurology. Our findings demonstrate\nthat ICU clinical narratives contain actionable signals for proactive\nwell-being monitoring."}
{"id": "2509.04498", "pdf": "https://arxiv.org/pdf/2509.04498.pdf", "abs": "https://arxiv.org/abs/2509.04498", "title": "Where Should I Study? Biased Language Models Decide! Evaluating Fairness in LMs for Academic Recommendations", "authors": ["Krithi Shailya", "Akhilesh Kumar Mishra", "Gokul S Krishnan", "Balaraman Ravindran"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used as daily recommendation\nsystems for tasks like education planning, yet their recommendations risk\nperpetuating societal biases. This paper empirically examines geographic,\ndemographic, and economic biases in university and program suggestions from\nthree open-source LLMs: LLaMA-3.1-8B, Gemma-7B, and Mistral-7B. Using 360\nsimulated user profiles varying by gender, nationality, and economic status, we\nanalyze over 25,000 recommendations. Results show strong biases: institutions\nin the Global North are disproportionately favored, recommendations often\nreinforce gender stereotypes, and institutional repetition is prevalent. While\nLLaMA-3.1 achieves the highest diversity, recommending 481 unique universities\nacross 58 countries, systemic disparities persist. To quantify these issues, we\npropose a novel, multi-dimensional evaluation framework that goes beyond\naccuracy by measuring demographic and geographic representation. Our findings\nhighlight the urgent need for bias consideration in educational LMs to ensure\nequitable global access to higher education."}
{"id": "2509.04499", "pdf": "https://arxiv.org/pdf/2509.04499.pdf", "abs": "https://arxiv.org/abs/2509.04499", "title": "DeepTRACE: Auditing Deep Research AI Systems for Tracking Reliability Across Citations and Evidence", "authors": ["Pranav Narayanan Venkit", "Philippe Laban", "Yilun Zhou", "Kung-Hsiang Huang", "Yixin Mao", "Chien-Sheng Wu"], "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2410.22349", "summary": "Generative search engines and deep research LLM agents promise trustworthy,\nsource-grounded synthesis, yet users regularly encounter overconfidence, weak\nsourcing, and confusing citation practices. We introduce DeepTRACE, a novel\nsociotechnically grounded audit framework that turns prior community-identified\nfailure cases into eight measurable dimensions spanning answer text, sources,\nand citations. DeepTRACE uses statement-level analysis (decomposition,\nconfidence scoring) and builds citation and factual-support matrices to audit\nhow systems reason with and attribute evidence end-to-end. Using automated\nextraction pipelines for popular public models (e.g., GPT-4.5/5, You.com,\nPerplexity, Copilot/Bing, Gemini) and an LLM-judge with validated agreement to\nhuman raters, we evaluate both web-search engines and deep-research\nconfigurations. Our findings show that generative search engines and deep\nresearch agents frequently produce one-sided, highly confident responses on\ndebate queries and include large fractions of statements unsupported by their\nown listed sources. Deep-research configurations reduce overconfidence and can\nattain high citation thoroughness, but they remain highly one-sided on debate\nqueries and still exhibit large fractions of unsupported statements, with\ncitation accuracy ranging from 40--80% across systems."}
{"id": "2509.04500", "pdf": "https://arxiv.org/pdf/2509.04500.pdf", "abs": "https://arxiv.org/abs/2509.04500", "title": "Context Engineering for Trustworthiness: Rescorla Wagner Steering Under Mixed and Inappropriate Contexts", "authors": ["Rushi Wang", "Jiateng Liu", "Cheng Qian", "Yifan Shen", "Yanzhou Pan", "Zhaozhuo Xu", "Ahmed Abbasi", "Heng Ji", "Denghui Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "36 pages, 7 figures", "summary": "Incorporating external context can significantly enhance the response quality\nof Large Language Models (LLMs). However, real-world contexts often mix\nrelevant information with disproportionate inappropriate content, posing\nreliability risks. How do LLMs process and prioritize mixed context? To study\nthis, we introduce the Poisoned Context Testbed, pairing queries with\nreal-world contexts containing relevant and inappropriate content. Inspired by\nassociative learning in animals, we adapt the Rescorla-Wagner (RW) model from\nneuroscience to quantify how competing contextual signals influence LLM\noutputs. Our adapted model reveals a consistent behavioral pattern: LLMs\nexhibit a strong tendency to incorporate information that is less prevalent in\nthe context. This susceptibility is harmful in real-world settings, where small\namounts of inappropriate content can substantially degrade response quality.\nEmpirical evaluations on our testbed further confirm this vulnerability. To\ntackle this, we introduce RW-Steering, a two-stage finetuning-based approach\nthat enables the model to internally identify and ignore inappropriate signals.\nUnlike prior methods that rely on extensive supervision across diverse context\nmixtures, RW-Steering generalizes robustly across varying proportions of\ninappropriate content. Experiments show that our best fine-tuned model improves\nresponse quality by 39.8% and reverses the undesirable behavior curve,\nestablishing RW-Steering as a robust, generalizable context engineering\nsolution for improving LLM safety in real-world use."}
{"id": "2509.04501", "pdf": "https://arxiv.org/pdf/2509.04501.pdf", "abs": "https://arxiv.org/abs/2509.04501", "title": "Understanding Reinforcement Learning for Model Training, and future directions with GRAPE", "authors": ["Rohit Patel"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T07, 68T05, 62M45, 68T50, 90C40", "I.2.6; I.2.7"], "comment": "35 pages, 1 figure", "summary": "This paper provides a self-contained, from-scratch, exposition of key\nalgorithms for instruction tuning of models: SFT, Rejection Sampling,\nREINFORCE, Trust Region Policy Optimization (TRPO), Proximal Policy\nOptimization (PPO), Group Relative Policy Optimization (GRPO), and Direct\nPreference Optimization (DPO). Explanations of these algorithms often assume\nprior knowledge, lack critical details, and/or are overly generalized and\ncomplex. Here, each method is discussed and developed step by step using\nsimplified and explicit notation focused on LLMs, aiming to eliminate ambiguity\nand provide a clear and intuitive understanding of the concepts. By minimizing\ndetours into the broader RL literature and connecting concepts to LLMs, we\neliminate superfluous abstractions and reduce cognitive overhead. Following\nthis exposition, we provide a literature review of new techniques and\napproaches beyond those detailed. Finally, new ideas for research and\nexploration in the form of GRAPE (Generalized Relative Advantage Policy\nEvolution) are presented."}
{"id": "2509.04502", "pdf": "https://arxiv.org/pdf/2509.04502.pdf", "abs": "https://arxiv.org/abs/2509.04502", "title": "VaccineRAG: Boosting Multimodal Large Language Models' Immunity to Harmful RAG Samples", "authors": ["Qixin Sun", "Ziqin Wang", "Hengyuan Zhao", "Yilin Li", "Kaiyou Song", "Linjiang Huang", "Xiaolin Hu", "Qingpei Guo", "Si Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval Augmented Generation enhances the response accuracy of Large\nLanguage Models (LLMs) by integrating retrieval and generation modules with\nexternal knowledge, demonstrating particular strength in real-time queries and\nVisual Question Answering tasks. However, the effectiveness of RAG is\nfrequently hindered by the precision of the retriever: many retrieved samples\nfed into the generation phase are irrelevant or misleading, posing a critical\nbottleneck to LLMs' performance. To address this challenge, we introduce\nVaccineRAG, a novel Chain-of-Thought-based retrieval-augmented generation\ndataset. On one hand, VaccineRAG employs a benchmark to evaluate models using\ndata with varying positive/negative sample ratios, systematically exposing\ninherent weaknesses in current LLMs. On the other hand, it enhances models'\nsample-discrimination capabilities by prompting LLMs to generate explicit\nChain-of-Thought (CoT) analysis for each sample before producing final answers.\nFurthermore, to enhance the model's ability to learn long-sequence complex CoT\ncontent, we propose Partial-GRPO. By modeling the outputs of LLMs as multiple\ncomponents rather than a single whole, our model can make more informed\npreference selections for complex sequences, thereby enhancing its capacity to\nlearn complex CoT. Comprehensive evaluations and ablation studies on VaccineRAG\nvalidate the effectiveness of the proposed scheme. The code and dataset will be\npublicly released soon."}
{"id": "2509.04504", "pdf": "https://arxiv.org/pdf/2509.04504.pdf", "abs": "https://arxiv.org/abs/2509.04504", "title": "Behavioral Fingerprinting of Large Language Models", "authors": ["Zehua Pei", "Hui-Ling Zhen", "Ying Zhang", "Zhiyuan Yang", "Xing Li", "Xianzhi Yu", "Mingxuan Yuan", "Bei Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "Submitted to 1st Open Conference on AI Agents for Science\n  (agents4science 2025)", "summary": "Current benchmarks for Large Language Models (LLMs) primarily focus on\nperformance metrics, often failing to capture the nuanced behavioral\ncharacteristics that differentiate them. This paper introduces a novel\n``Behavioral Fingerprinting'' framework designed to move beyond traditional\nevaluation by creating a multi-faceted profile of a model's intrinsic cognitive\nand interactive styles. Using a curated \\textit{Diagnostic Prompt Suite} and an\ninnovative, automated evaluation pipeline where a powerful LLM acts as an\nimpartial judge, we analyze eighteen models across capability tiers. Our\nresults reveal a critical divergence in the LLM landscape: while core\ncapabilities like abstract and causal reasoning are converging among top\nmodels, alignment-related behaviors such as sycophancy and semantic robustness\nvary dramatically. We further document a cross-model default persona clustering\n(ISTJ/ESTJ) that likely reflects common alignment incentives. Taken together,\nthis suggests that a model's interactive nature is not an emergent property of\nits scale or reasoning power, but a direct consequence of specific, and highly\nvariable, developer alignment strategies. Our framework provides a reproducible\nand scalable methodology for uncovering these deep behavioral differences.\nProject: https://github.com/JarvisPei/Behavioral-Fingerprinting"}
{"id": "2509.04507", "pdf": "https://arxiv.org/pdf/2509.04507.pdf", "abs": "https://arxiv.org/abs/2509.04507", "title": "From Silent Signals to Natural Language: A Dual-Stage Transformer-LLM Approach", "authors": ["Nithyashree Sivasubramaniam"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Silent Speech Interfaces (SSIs) have gained attention for their ability to\ngenerate intelligible speech from non-acoustic signals. While significant\nprogress has been made in advancing speech generation pipelines, limited work\nhas addressed the recognition and downstream processing of synthesized speech,\nwhich often suffers from phonetic ambiguity and noise. To overcome these\nchallenges, we propose an enhanced automatic speech recognition framework that\ncombines a transformer-based acoustic model with a large language model (LLM)\nfor post-processing. The transformer captures full utterance context, while the\nLLM ensures linguistic consistency. Experimental results show a 16% relative\nand 6% absolute reduction in word error rate (WER) over a 36% baseline,\ndemonstrating substantial improvements in intelligibility for silent speech\ninterfaces."}
{"id": "2509.04508", "pdf": "https://arxiv.org/pdf/2509.04508.pdf", "abs": "https://arxiv.org/abs/2509.04508", "title": "ProST: Progressive Sub-task Training for Pareto-Optimal Multi-agent Systems Using Small Language Models", "authors": ["Biddut Sarker Bijoy", "Mohammad Saqib Hasan", "Pegah Alipoormolabashi", "Avirup Sil", "Aruna Balasubramanian", "Niranjan Balasubramanian"], "categories": ["cs.CL"], "comment": null, "summary": "Multi-agent systems with smaller language models (SLMs) present a viable\nalternative to single agent systems powered by large language models (LLMs) for\naddressing complex problems. In this work, we study how these alternatives\ncompare in terms of both effectiveness and efficiency. To study this trade-off,\nwe instantiate single and multi-agent systems for the complex problems in the\nAppWorld environment using different sized language models.\n  We find that difficulties with long-trajectory learning in smaller language\nmodels (SLMs) limit their performance. Even when trained for specialized roles,\nSLMs fail to learn all subtasks effectively. To address this issue, we\nintroduce a simple progressive sub-task training strategy, which introduces new\nsub-tasks progressively in each training epoch. We find that this novel\nstrategy, analogous to instance level curriculum learning, consistently\nimproves the effectiveness of multi-agents at all configurations. Our Pareto\nanalysis shows that fine-tuned multi-agent systems yield better\neffectiveness-efficiency trade-offs. Additional ablations and analyses shows\nthe importance of our progressive training strategy and its ability to reduce\nsubtask error rates."}
{"id": "2509.04510", "pdf": "https://arxiv.org/pdf/2509.04510.pdf", "abs": "https://arxiv.org/abs/2509.04510", "title": "Combine Virtual Reality and Machine-Learning to Identify the Presence of Dyslexia: A Cross-Linguistic Approach", "authors": ["Michele Materazzini", "Gianluca Morciano", "Jose Manuel Alcalde-Llergo", "Enrique Yeguas-Bolivar", "Giuseppe Calabro", "Andrea Zingoni", "Juri Taborri"], "categories": ["cs.CL", "cs.HC"], "comment": "22 pages, 10 figures, 5 tables", "summary": "This study explores the use of virtual reality (VR) and artificial\nintelligence (AI) to predict the presence of dyslexia in Italian and Spanish\nuniversity students. In particular, the research investigates whether\nVR-derived data from Silent Reading (SR) tests and self-esteem assessments can\ndifferentiate between students that are affected by dyslexia and students that\nare not, employing machine learning (ML) algorithms. Participants completed\nVR-based tasks measuring reading performance and self-esteem. A preliminary\nstatistical analysis (t tests and Mann Whitney tests) on these data was\nperformed, to compare the obtained scores between individuals with and without\ndyslexia, revealing significant differences in completion time for the SR test,\nbut not in accuracy, nor in self esteem. Then, supervised ML models were\ntrained and tested, demonstrating an ability to classify the presence/absence\nof dyslexia with an accuracy of 87.5 per cent for Italian, 66.6 per cent for\nSpanish, and 75.0 per cent for the pooled group. These findings suggest that VR\nand ML can effectively be used as supporting tools for assessing dyslexia,\nparticularly by capturing differences in task completion speed, but\nlanguage-specific factors may influence classification accuracy."}
{"id": "2509.04512", "pdf": "https://arxiv.org/pdf/2509.04512.pdf", "abs": "https://arxiv.org/abs/2509.04512", "title": "Scaling behavior of large language models in emotional safety classification across sizes and tasks", "authors": ["Edoardo Pinzuti", "Oliver T√ºscher", "Andr√© Ferreira Castro"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Understanding how large language models (LLMs) process emotionally sensitive\ncontent is critical for building safe and reliable systems, particularly in\nmental health contexts. We investigate the scaling behavior of LLMs on two key\ntasks: trinary classification of emotional safety (safe vs. unsafe vs.\nborderline) and multi-label classification using a six-category safety risk\ntaxonomy. To support this, we construct a novel dataset by merging several\nhuman-authored mental health datasets (> 15K samples) and augmenting them with\nemotion re-interpretation prompts generated via ChatGPT. We evaluate four LLaMA\nmodels (1B, 3B, 8B, 70B) across zero-shot, few-shot, and fine-tuning settings.\nOur results show that larger LLMs achieve stronger average performance,\nparticularly in nuanced multi-label classification and in zero-shot settings.\nHowever, lightweight fine-tuning allowed the 1B model to achieve performance\ncomparable to larger models and BERT in several high-data categories, while\nrequiring <2GB VRAM at inference. These findings suggest that smaller,\non-device models can serve as viable, privacy-preserving alternatives for\nsensitive applications, offering the ability to interpret emotional context and\nmaintain safe conversational boundaries. This work highlights key implications\nfor therapeutic LLM applications and the scalable alignment of safety-critical\nsystems."}
{"id": "2509.04515", "pdf": "https://arxiv.org/pdf/2509.04515.pdf", "abs": "https://arxiv.org/abs/2509.04515", "title": "Mitigation of Gender and Ethnicity Bias in AI-Generated Stories through Model Explanations", "authors": ["Martha O. Dimgba", "Sharon Oba", "Ameeta Agrawal", "Philippe J. Giabbanelli"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Language models have been shown to propagate social bias through their\noutput, particularly in the representation of gender and ethnicity. This paper\ninvestigates gender and ethnicity biases in AI-generated occupational stories.\nRepresentation biases are measured before and after applying our proposed\nmitigation strategy, Bias Analysis and Mitigation through Explanation (BAME),\nrevealing improvements in demographic representation ranging from 2% to 20%.\nBAME leverages model-generated explanations to inform targeted prompt\nengineering, effectively reducing biases without modifying model parameters. By\nanalyzing stories generated across 25 occupational groups, three large language\nmodels (Claude 3.5 Sonnet, Llama 3.1 70B Instruct, and GPT-4 Turbo), and\nmultiple demographic dimensions, we identify persistent patterns of\noverrepresentation and underrepresentation linked to training data stereotypes.\nOur findings demonstrate that guiding models with their own internal reasoning\nmechanisms can significantly enhance demographic parity, thereby contributing\nto the development of more transparent generative AI systems."}
{"id": "2509.04516", "pdf": "https://arxiv.org/pdf/2509.04516.pdf", "abs": "https://arxiv.org/abs/2509.04516", "title": "Artificially Fluent: Swahili AI Performance Benchmarks Between English-Trained and Natively-Trained Datasets", "authors": ["Sophie Jaffer", "Simeon Sayer"], "categories": ["cs.CL", "cs.CY"], "comment": "13 Pages, 3 Figures", "summary": "As large language models (LLMs) expand multilingual capabilities, questions\nremain about the equity of their performance across languages. While many\ncommunities stand to benefit from AI systems, the dominance of English in\ntraining data risks disadvantaging non-English speakers. To test the hypothesis\nthat such data disparities may affect model performance, this study compares\ntwo monolingual BERT models: one trained and tested entirely on Swahili data,\nand another on comparable English news data. To simulate how multilingual LLMs\nprocess non-English queries through internal translation and abstraction, we\ntranslated the Swahili news data into English and evaluated it using the\nEnglish-trained model. This approach tests the hypothesis by evaluating whether\ntranslating Swahili inputs for evaluation on an English model yields better or\nworse performance compared to training and testing a model entirely in Swahili,\nthus isolating the effect of language consistency versus cross-lingual\nabstraction. The results prove that, despite high-quality translation, the\nnative Swahili-trained model performed better than the Swahili-to-English\ntranslated model, producing nearly four times fewer errors: 0.36% vs. 1.47%\nrespectively. This gap suggests that translation alone does not bridge\nrepresentational differences between languages and that models trained in one\nlanguage may struggle to accurately interpret translated inputs due to\nimperfect internal knowledge representation, suggesting that native-language\ntraining remains important for reliable outcomes. In educational and\ninformational contexts, even small performance gaps may compound inequality.\nFuture research should focus on addressing broader dataset development for\nunderrepresented languages and renewed attention to multilingual model\nevaluation, ensuring the reinforcing effect of global AI deployment on existing\ndigital divides is reduced."}
{"id": "2509.04517", "pdf": "https://arxiv.org/pdf/2509.04517.pdf", "abs": "https://arxiv.org/abs/2509.04517", "title": "Analysis of Voluntarily Reported Data Post Mesh Implantation for Detecting Public Emotion and Identifying Concern Reports", "authors": ["Indu Bala", "Lewis Mitchell", "Marianne H Gillam"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Mesh implants are widely utilized in hernia repair surgeries, but\npostoperative complications present a significant concern. This study analyzes\npatient reports from the Manufacturer and User Facility Device Experience\n(MAUDE) database spanning 2000 to 2021 to investigate the emotional aspects of\npatients following mesh implantation using Natural Language Processing (NLP).\nEmploying the National Research Council Canada (NRC) Emotion Lexicon and\nTextBlob for sentiment analysis, the research categorizes patient narratives\ninto eight emotions (anger, fear, anticipation, trust, surprise, sadness, joy,\nand disgust) and assesses sentiment polarity. The goal is to discern patterns\nin patient sentiment over time and to identify reports signaling urgent\nconcerns, referred to as \"Concern Reports,\" thereby understanding shifts in\npatient experiences in relation to changes in medical device regulation and\ntechnological advancements in healthcare. The study detected an increase in\nConcern Reports and higher emotional intensity during the periods of 2011-2012\nand 2017-2018. Through temporal analysis of Concern Reports and overall\nsentiment, this research provides valuable insights for healthcare\npractitioners, enhancing their understanding of patient experiences\npost-surgery, which is critical for improving preoperative counselling,\npostoperative care, and preparing patients for mesh implant surgeries. The\nstudy underscores the importance of emotional considerations in medical\npractices and the potential for sentiment analysis to inform and enhance\npatient care."}
{"id": "2509.04518", "pdf": "https://arxiv.org/pdf/2509.04518.pdf", "abs": "https://arxiv.org/abs/2509.04518", "title": "Advancing SLM Tool-Use Capability using Reinforcement Learning", "authors": ["Dhruvi Paprunia", "Vansh Kharidia", "Pankti Doshi"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have progressed beyond simple text creation, and\ntool use has become increasingly important for complex, real-world tasks. Tool\nuse in LLMs refers to their ability to utilize external resources such as APIs,\ndatabases, or software functions to extend their functionality beyond\ngenerating text.Tools are used for tasks such as performing calculations,\nmaking API calls to retrieve the current time and date, and more. This\ncapability enables models to fetch real-time data, execute commands, or solve\nproblems requiring dynamic interaction, making it indispensable for\napplications like AI agents in virtual assistants, robotic control, or\nautomated workflows.\n  However, while LLMs are usually adept tool use, their vast resource\nrequirements and computation complexity restrict their use in every use case.As\na result, there is an increasing need for more compact and efficient Small\nLanguage Models (SLMs). Small language models (SLMs) struggle in tool use\ncompared to large language models (LLMs). As soon in Table 1. SLMs are\ntypically trained on smaller, more specific datasets, resulting in a narrower\nknowledge base and limited contextual understanding compared to LLMs.\n  This research addresses these challenges by using Reinforcement Learning\n(RL), specifically Group Relative Policy Optimization (GRPO), to enhance\ntool-use proficiency in SLMs. Unlike conventional fine-tuning approaches that\nrequire heavy computation and often lack adaptability, our method provides an\nefficient, effective solution that significantly boosts SLM tool-use accuracy,\nincreasing their practical utility."}
{"id": "2509.04519", "pdf": "https://arxiv.org/pdf/2509.04519.pdf", "abs": "https://arxiv.org/abs/2509.04519", "title": "Hierarchical Section Matching Prediction (HSMP) BERT for Fine-Grained Extraction of Structured Data from Hebrew Free-Text Radiology Reports in Crohn's Disease", "authors": ["Zvi Badash", "Hadas Ben-Atya", "Naama Gavrielov", "Liam Hazan", "Gili Focht", "Ruth Cytter-Kuint", "Talar Hagopian", "Dan Turner", "Moti Freiman"], "categories": ["cs.CL"], "comment": null, "summary": "Extracting structured clinical information from radiology reports is\nchallenging, especially in low-resource languages. This is pronounced in\nCrohn's disease, with sparsely represented multi-organ findings. We developed\nHierarchical Structured Matching Prediction BERT (HSMP-BERT), a prompt-based\nmodel for extraction from Hebrew radiology text. In an administrative database\nstudy, we analyzed 9,683 reports from Crohn's patients imaged 2010-2023 across\nIsraeli providers. A subset of 512 reports was radiologist-annotated for\nfindings across six gastrointestinal organs and 15 pathologies, yielding 90\nstructured labels per subject. Multilabel-stratified split (66%\ntrain+validation; 33% test), preserving label prevalence. Performance was\nevaluated with accuracy, F1, Cohen's $\\kappa$, AUC, PPV, NPV, and recall. On 24\norgan-finding combinations with $>$15 positives, HSMP-BERT achieved mean F1\n0.83$\\pm$0.08 and $\\kappa$ 0.65$\\pm$0.17, outperforming the SMP zero-shot\nbaseline (F1 0.49$\\pm$0.07, $\\kappa$ 0.06$\\pm$0.07) and standard fine-tuning\n(F1 0.30$\\pm$0.27, $\\kappa$ 0.27$\\pm$0.34; paired t-test $p < 10^{-7}$).\nHierarchical inference cuts runtime 5.1$\\times$ vs. traditional inference.\nApplied to all reports, it revealed associations among ileal wall thickening,\nstenosis, and pre-stenotic dilatation, plus age- and sex-specific trends in\ninflammatory findings. HSMP-BERT offers a scalable solution for structured\nextraction in radiology, enabling population-level analysis of Crohn's disease\nand demonstrating AI's potential in low-resource settings."}
{"id": "2509.04523", "pdf": "https://arxiv.org/pdf/2509.04523.pdf", "abs": "https://arxiv.org/abs/2509.04523", "title": "Using LLMs to create analytical datasets: A case study of reconstructing the historical memory of Colombia", "authors": ["David Anderson", "Galia Benitez", "Margret Bjarnadottir", "Shriyan Reyya"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Colombia has been submerged in decades of armed conflict, yet until recently,\nthe systematic documentation of violence was not a priority for the Colombian\ngovernment. This has resulted in a lack of publicly available conflict\ninformation and, consequently, a lack of historical accounts. This study\ncontributes to Colombia's historical memory by utilizing GPT, a large language\nmodel (LLM), to read and answer questions about over 200,000 violence-related\nnewspaper articles in Spanish. We use the resulting dataset to conduct both\ndescriptive analysis and a study of the relationship between violence and the\neradication of coca crops, offering an example of policy analyses that such\ndata can support. Our study demonstrates how LLMs have opened new research\nopportunities by enabling examinations of large text corpora at a previously\ninfeasible depth."}
{"id": "2509.04534", "pdf": "https://arxiv.org/pdf/2509.04534.pdf", "abs": "https://arxiv.org/abs/2509.04534", "title": "Quantized Large Language Models in Biomedical Natural Language Processing: Evaluation and Recommendation", "authors": ["Zaifu Zhan", "Shuang Zhou", "Min Zeng", "Kai Yu", "Meijia Song", "Xiaoyi Chen", "Jun Wang", "Yu Hou", "Rui Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 7 figures", "summary": "Large language models have demonstrated remarkable capabilities in biomedical\nnatural language processing, yet their rapid growth in size and computational\nrequirements present a major barrier to adoption in healthcare settings where\ndata privacy precludes cloud deployment and resources are limited. In this\nstudy, we systematically evaluated the impact of quantization on 12\nstate-of-the-art large language models, including both general-purpose and\nbiomedical-specific models, across eight benchmark datasets covering four key\ntasks: named entity recognition, relation extraction, multi-label\nclassification, and question answering. We show that quantization substantially\nreduces GPU memory requirements-by up to 75%-while preserving model performance\nacross diverse tasks, enabling the deployment of 70B-parameter models on 40GB\nconsumer-grade GPUs. In addition, domain-specific knowledge and responsiveness\nto advanced prompting methods are largely maintained. These findings provide\nsignificant practical and guiding value, highlighting quantization as a\npractical and effective strategy for enabling the secure, local deployment of\nlarge yet high-capacity language models in biomedical contexts, bridging the\ngap between technical advances in AI and real-world clinical translation."}
{"id": "2509.04549", "pdf": "https://arxiv.org/pdf/2509.04549.pdf", "abs": "https://arxiv.org/abs/2509.04549", "title": "Manipulating Transformer-Based Models: Controllability, Steerability, and Robust Interventions", "authors": ["Faruk Alpay", "Taylan Alpay"], "categories": ["cs.CL", "cs.AI", "68T50, 68T05", "I.2.7; I.2.6; I.2.11"], "comment": "13 pages", "summary": "Transformer-based language models excel in NLP tasks, but fine-grained\ncontrol remains challenging. This paper explores methods for manipulating\ntransformer models through principled interventions at three levels: prompts,\nactivations, and weights. We formalize controllable text generation as an\noptimization problem addressable via prompt engineering, parameter-efficient\nfine-tuning, model editing, and reinforcement learning. We introduce a unified\nframework encompassing prompt-level steering, activation interventions, and\nweight-space edits. We analyze robustness and safety implications, including\nadversarial attacks and alignment mitigations. Theoretically, we show minimal\nweight updates can achieve targeted behavior changes with limited side-effects.\nEmpirically, we demonstrate >90% success in sentiment control and factual edits\nwhile preserving base performance, though generalization-specificity trade-offs\nexist. We discuss ethical dual-use risks and the need for rigorous evaluation.\nThis work lays groundwork for designing controllable and robust language\nmodels."}
{"id": "2509.04605", "pdf": "https://arxiv.org/pdf/2509.04605.pdf", "abs": "https://arxiv.org/abs/2509.04605", "title": "Spoken in Jest, Detected in Earnest: A Systematic Review of Sarcasm Recognition -- Multimodal Fusion, Challenges, and Future Prospects", "authors": ["Xiyuan Gao", "Shekhar Nayak", "Matt Coler"], "categories": ["cs.CL"], "comment": "20 pages, 7 figures, Submitted to IEEE Transactions on Affective\n  Computing", "summary": "Sarcasm, a common feature of human communication, poses challenges in\ninterpersonal interactions and human-machine interactions. Linguistic research\nhas highlighted the importance of prosodic cues, such as variations in pitch,\nspeaking rate, and intonation, in conveying sarcastic intent. Although previous\nwork has focused on text-based sarcasm detection, the role of speech data in\nrecognizing sarcasm has been underexplored. Recent advancements in speech\ntechnology emphasize the growing importance of leveraging speech data for\nautomatic sarcasm recognition, which can enhance social interactions for\nindividuals with neurodegenerative conditions and improve machine understanding\nof complex human language use, leading to more nuanced interactions. This\nsystematic review is the first to focus on speech-based sarcasm recognition,\ncharting the evolution from unimodal to multimodal approaches. It covers\ndatasets, feature extraction, and classification methods, and aims to bridge\ngaps across diverse research domains. The findings include limitations in\ndatasets for sarcasm recognition in speech, the evolution of feature extraction\ntechniques from traditional acoustic features to deep learning-based\nrepresentations, and the progression of classification methods from unimodal\napproaches to multimodal fusion techniques. In so doing, we identify the need\nfor greater emphasis on cross-cultural and multilingual sarcasm recognition, as\nwell as the importance of addressing sarcasm as a multimodal phenomenon, rather\nthan a text-based challenge."}
{"id": "2509.04606", "pdf": "https://arxiv.org/pdf/2509.04606.pdf", "abs": "https://arxiv.org/abs/2509.04606", "title": "Sample-efficient Integration of New Modalities into Large Language Models", "authors": ["Osman Batur ƒ∞nce", "Andr√© F. T. Martins", "Oisin Mac Aodha", "Edoardo M. Ponti"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Pre-print", "summary": "Multimodal foundation models can process several modalities. However, since\nthe space of possible modalities is large and evolving over time, training a\nmodel from scratch to encompass all modalities is unfeasible. Moreover,\nintegrating a modality into a pre-existing foundation model currently requires\na significant amount of paired data, which is often not available for\nlow-resource modalities. In this paper, we introduce a method for\nsample-efficient modality integration (SEMI) into Large Language Models (LLMs).\nTo this end, we devise a hypernetwork that can adapt a shared projector --\nplaced between modality-specific encoders and an LLM -- to any modality. The\nhypernetwork, trained on high-resource modalities (i.e., text, speech, audio,\nvideo), is conditioned on a few samples from any arbitrary modality at\ninference time to generate a suitable adapter. To increase the diversity of\ntraining modalities, we artificially multiply the number of encoders through\nisometric transformations. We find that SEMI achieves a significant boost in\nsample efficiency during few-shot integration of new modalities (i.e.,\nsatellite images, astronomical images, inertial measurements, and molecules)\nwith encoders of arbitrary embedding dimensionality. For instance, to reach the\nsame accuracy as 32-shot SEMI, training the projector from scratch needs\n64$\\times$ more data. As a result, SEMI holds promise to extend the modality\ncoverage of foundation models."}
{"id": "2509.04615", "pdf": "https://arxiv.org/pdf/2509.04615.pdf", "abs": "https://arxiv.org/abs/2509.04615", "title": "Breaking to Build: A Threat Model of Prompt-Based Attacks for Securing LLMs", "authors": ["Brennen Hill", "Surendra Parla", "Venkata Abhijeeth Balabhadruni", "Atharv Prajod Padmalayam", "Sujay Chandra Shekara Sharma"], "categories": ["cs.CL", "cs.CR", "cs.LG", "68T07, 68T50", "I.2.7; I.2.6; K.6.5"], "comment": null, "summary": "The proliferation of Large Language Models (LLMs) has introduced critical\nsecurity challenges, where adversarial actors can manipulate input prompts to\ncause significant harm and circumvent safety alignments. These prompt-based\nattacks exploit vulnerabilities in a model's design, training, and contextual\nunderstanding, leading to intellectual property theft, misinformation\ngeneration, and erosion of user trust. A systematic understanding of these\nattack vectors is the foundational step toward developing robust\ncountermeasures. This paper presents a comprehensive literature survey of\nprompt-based attack methodologies, categorizing them to provide a clear threat\nmodel. By detailing the mechanisms and impacts of these exploits, this survey\naims to inform the research community's efforts in building the next generation\nof secure LLMs that are inherently resistant to unauthorized distillation,\nfine-tuning, and editing."}
{"id": "2509.04650", "pdf": "https://arxiv.org/pdf/2509.04650.pdf", "abs": "https://arxiv.org/abs/2509.04650", "title": "Comparative Analysis of Transformer Models in Disaster Tweet Classification for Public Safety", "authors": ["Sharif Noor Zisad", "Ragib Hasan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Twitter and other social media platforms have become vital sources of real\ntime information during disasters and public safety emergencies. Automatically\nclassifying disaster related tweets can help emergency services respond faster\nand more effectively. Traditional Machine Learning (ML) models such as Logistic\nRegression, Naive Bayes, and Support Vector Machines have been widely used for\nthis task, but they often fail to understand the context or deeper meaning of\nwords, especially when the language is informal, metaphorical, or ambiguous. We\nposit that, in this context, transformer based models can perform better than\ntraditional ML models. In this paper, we evaluate the effectiveness of\ntransformer based models, including BERT, DistilBERT, RoBERTa, and DeBERTa, for\nclassifying disaster related tweets. These models are compared with traditional\nML approaches to highlight the performance gap. Experimental results show that\nBERT achieved the highest accuracy (91%), significantly outperforming\ntraditional models like Logistic Regression and Naive Bayes (both at 82%). The\nuse of contextual embeddings and attention mechanisms allows transformer models\nto better understand subtle language in tweets, where traditional ML models\nfall short. This research demonstrates that transformer architectures are far\nmore suitable for public safety applications, offering improved accuracy,\ndeeper language understanding, and better generalization across real world\nsocial media text."}
{"id": "2509.04655", "pdf": "https://arxiv.org/pdf/2509.04655.pdf", "abs": "https://arxiv.org/abs/2509.04655", "title": "Polysemantic Dropout: Conformal OOD Detection for Specialized LLMs", "authors": ["Ayush Gupta", "Ramneet Kaur", "Anirban Roy", "Adam D. Cobb", "Rama Chellappa", "Susmit Jha"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP 2025 main conference", "summary": "We propose a novel inference-time out-of-domain (OOD) detection algorithm for\nspecialized large language models (LLMs). Despite achieving state-of-the-art\nperformance on in-domain tasks through fine-tuning, specialized LLMs remain\nvulnerable to incorrect or unreliable outputs when presented with OOD inputs,\nposing risks in critical applications. Our method leverages the Inductive\nConformal Anomaly Detection (ICAD) framework, using a new non-conformity\nmeasure based on the model's dropout tolerance. Motivated by recent findings on\npolysemanticity and redundancy in LLMs, we hypothesize that in-domain inputs\nexhibit higher dropout tolerance than OOD inputs. We aggregate dropout\ntolerance across multiple layers via a valid ensemble approach, improving\ndetection while maintaining theoretical false alarm bounds from ICAD.\nExperiments with medical-specialized LLMs show that our approach detects OOD\ninputs better than baseline methods, with AUROC improvements of $2\\%$ to $37\\%$\nwhen treating OOD datapoints as positives and in-domain test datapoints as\nnegatives."}
{"id": "2509.04656", "pdf": "https://arxiv.org/pdf/2509.04656.pdf", "abs": "https://arxiv.org/abs/2509.04656", "title": "AraHalluEval: A Fine-grained Hallucination Evaluation Framework for Arabic LLMs", "authors": ["Aisha Alansari", "Hamzah Luqman"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, extensive research on the hallucination of the large language\nmodels (LLMs) has mainly focused on the English language. Despite the growing\nnumber of multilingual and Arabic-specific LLMs, evaluating LLMs' hallucination\nin the Arabic context remains relatively underexplored. The knowledge gap is\nparticularly pressing given Arabic's widespread use across many regions and its\nimportance in global communication and media. This paper presents the first\ncomprehensive hallucination evaluation of Arabic and multilingual LLMs on two\ncritical Arabic natural language generation tasks: generative question\nanswering (GQA) and summarization. This study evaluates a total of 12 LLMs,\nincluding 4 Arabic pre-trained models, 4 multilingual models, and 4\nreasoning-based models. To assess the factual consistency and faithfulness of\nLLMs' outputs, we developed a fine-grained hallucination evaluation framework\nconsisting of 12 fine-grained hallucination indicators that represent the\nvarying characteristics of each task. The results reveal that factual\nhallucinations are more prevalent than faithfulness errors across all models\nand tasks. Notably, the Arabic pre-trained model Allam consistently\ndemonstrates lower hallucination rates than multilingual models and a\ncomparative performance with reasoning-based models. The code is available at:\n\\href{https://github.com/aishaalansari57/AraHalluEval}{Github link}."}
{"id": "2509.04657", "pdf": "https://arxiv.org/pdf/2509.04657.pdf", "abs": "https://arxiv.org/abs/2509.04657", "title": "Evaluating NL2SQL via SQL2NL", "authors": ["Mohammadtaher Safarzadeh", "Afshin Oroojlooyjadid", "Dan Roth"], "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.LG"], "comment": "Accepted to EMNLP 2025", "summary": "Robust evaluation in the presence of linguistic variation is key to\nunderstanding the generalization capabilities of Natural Language to SQL\n(NL2SQL) models, yet existing benchmarks rarely address this factor in a\nsystematic or controlled manner. We propose a novel schema-aligned paraphrasing\nframework that leverages SQL-to-NL (SQL2NL) to automatically generate\nsemantically equivalent, lexically diverse queries while maintaining alignment\nwith the original schema and intent. This enables the first targeted evaluation\nof NL2SQL robustness to linguistic variation in isolation-distinct from prior\nwork that primarily investigates ambiguity or schema perturbations. Our\nanalysis reveals that state-of-the-art models are far more brittle than\nstandard benchmarks suggest. For example, LLaMa3.3-70B exhibits a 10.23% drop\nin execution accuracy (from 77.11% to 66.9%) on paraphrased Spider queries,\nwhile LLaMa3.1-8B suffers an even larger drop of nearly 20% (from 62.9% to\n42.5%). Smaller models (e.g., GPT-4o mini) are disproportionately affected. We\nalso find that robustness degradation varies significantly with query\ncomplexity, dataset, and domain -- highlighting the need for evaluation\nframeworks that explicitly measure linguistic generalization to ensure reliable\nperformance in real-world settings."}
{"id": "2509.04664", "pdf": "https://arxiv.org/pdf/2509.04664.pdf", "abs": "https://arxiv.org/abs/2509.04664", "title": "Why Language Models Hallucinate", "authors": ["Adam Tauman Kalai", "Ofir Nachum", "Santosh S. Vempala", "Edwin Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Like students facing hard exam questions, large language models sometimes\nguess when uncertain, producing plausible yet incorrect statements instead of\nadmitting uncertainty. Such \"hallucinations\" persist even in state-of-the-art\nsystems and undermine trust. We argue that language models hallucinate because\nthe training and evaluation procedures reward guessing over acknowledging\nuncertainty, and we analyze the statistical causes of hallucinations in the\nmodern training pipeline. Hallucinations need not be mysterious -- they\noriginate simply as errors in binary classification. If incorrect statements\ncannot be distinguished from facts, then hallucinations in pretrained language\nmodels will arise through natural statistical pressures. We then argue that\nhallucinations persist due to the way most evaluations are graded -- language\nmodels are optimized to be good test-takers, and guessing when uncertain\nimproves test performance. This \"epidemic\" of penalizing uncertain responses\ncan only be addressed through a socio-technical mitigation: modifying the\nscoring of existing benchmarks that are misaligned but dominate leaderboards,\nrather than introducing additional hallucination evaluations. This change may\nsteer the field toward more trustworthy AI systems."}
{"id": "2509.04696", "pdf": "https://arxiv.org/pdf/2509.04696.pdf", "abs": "https://arxiv.org/abs/2509.04696", "title": "ODKE+: Ontology-Guided Open-Domain Knowledge Extraction with LLMs", "authors": ["Samira Khorshidi", "Azadeh Nikfarjam", "Suprita Shankar", "Yisi Sang", "Yash Govind", "Hyun Jang", "Ali Kasgari", "Alexis McClimans", "Mohamed Soliman", "Vishnu Konda", "Ahmed Fakhry", "Xiaoguang Qi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Knowledge graphs (KGs) are foundational to many AI applications, but\nmaintaining their freshness and completeness remains costly. We present ODKE+,\na production-grade system that automatically extracts and ingests millions of\nopen-domain facts from web sources with high precision. ODKE+ combines modular\ncomponents into a scalable pipeline: (1) the Extraction Initiator detects\nmissing or stale facts, (2) the Evidence Retriever collects supporting\ndocuments, (3) hybrid Knowledge Extractors apply both pattern-based rules and\nontology-guided prompting for large language models (LLMs), (4) a lightweight\nGrounder validates extracted facts using a second LLM, and (5) the Corroborator\nranks and normalizes candidate facts for ingestion. ODKE+ dynamically generates\nontology snippets tailored to each entity type to align extractions with schema\nconstraints, enabling scalable, type-consistent fact extraction across 195\npredicates. The system supports batch and streaming modes, processing over 9\nmillion Wikipedia pages and ingesting 19 million high-confidence facts with\n98.8% precision. ODKE+ significantly improves coverage over traditional\nmethods, achieving up to 48% overlap with third-party KGs and reducing update\nlag by 50 days on average. Our deployment demonstrates that LLM-based\nextraction, grounded in ontological structure and verification workflows, can\ndeliver trustworthiness, production-scale knowledge ingestion with broad\nreal-world applicability. A recording of the system demonstration is included\nwith the submission and is also available at https://youtu.be/UcnE3_GsTWs."}
{"id": "2509.04702", "pdf": "https://arxiv.org/pdf/2509.04702.pdf", "abs": "https://arxiv.org/abs/2509.04702", "title": "OleSpeech-IV: A Large-Scale Multispeaker and Multilingual Conversational Speech Dataset with Diverse Topics", "authors": ["Wei Chu", "Yuanzhe Dong", "Ke Tan", "Dong Han", "Xavier Menendez-Pidal", "Ruchao Fan", "Chenfeng Miao", "Chanwoo Kim", "Bhiksha Raj", "Rita Singh"], "categories": ["cs.CL"], "comment": null, "summary": "OleSpeech-IV dataset is a large-scale multispeaker and multilingual\nconversational speech dataset with diverse topics. The audio content comes from\npublicly-available English podcasts, talk shows, teleconferences, and other\nconversations. Speaker names, turns, and transcripts are human-sourced and\nrefined by a proprietary pipeline, while additional information such as\ntimestamps and confidence scores is derived from the pipeline. The IV denotes\nits position as Tier IV in the Olewave dataset series. In addition, we have\nopen-sourced a subset, OleSpeech-IV-2025-EN-AR-100, for non-commercial research\nuse."}
{"id": "2509.04716", "pdf": "https://arxiv.org/pdf/2509.04716.pdf", "abs": "https://arxiv.org/abs/2509.04716", "title": "KERAG: Knowledge-Enhanced Retrieval-Augmented Generation for Advanced Question Answering", "authors": ["Yushi Sun", "Kai Sun", "Yifan Ethan Xu", "Xiao Yang", "Xin Luna Dong", "Nan Tang", "Lei Chen"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted by EMNLP Findings 2025", "summary": "Retrieval-Augmented Generation (RAG) mitigates hallucination in Large\nLanguage Models (LLMs) by incorporating external data, with Knowledge Graphs\n(KGs) offering crucial information for question answering. Traditional\nKnowledge Graph Question Answering (KGQA) methods rely on semantic parsing,\nwhich typically retrieves knowledge strictly necessary for answer generation,\nthus often suffer from low coverage due to rigid schema requirements and\nsemantic ambiguity. We present KERAG, a novel KG-based RAG pipeline that\nenhances QA coverage by retrieving a broader subgraph likely to contain\nrelevant information. Our retrieval-filtering-summarization approach, combined\nwith fine-tuned LLMs for Chain-of-Thought reasoning on knowledge sub-graphs,\nreduces noises and improves QA for both simple and complex questions.\nExperiments demonstrate that KERAG surpasses state-of-the-art solutions by\nabout 7% in quality and exceeds GPT-4o (Tool) by 10-21%."}
{"id": "2509.04745", "pdf": "https://arxiv.org/pdf/2509.04745.pdf", "abs": "https://arxiv.org/abs/2509.04745", "title": "Phonological Representation Learning for Isolated Signs Improves Out-of-Vocabulary Generalization", "authors": ["Lee Kezar", "Zed Sehyr", "Jesse Thomason"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Sign language datasets are often not representative in terms of vocabulary,\nunderscoring the need for models that generalize to unseen signs. Vector\nquantization is a promising approach for learning discrete, token-like\nrepresentations, but it has not been evaluated whether the learned units\ncapture spurious correlations that hinder out-of-vocabulary performance. This\nwork investigates two phonological inductive biases: Parameter Disentanglement,\nan architectural bias, and Phonological Semi-Supervision, a regularization\ntechnique, to improve isolated sign recognition of known signs and\nreconstruction quality of unseen signs with a vector-quantized autoencoder. The\nprimary finding is that the learned representations from the proposed model are\nmore effective for one-shot reconstruction of unseen signs and more\ndiscriminative for sign identification compared to a controlled baseline. This\nwork provides a quantitative analysis of how explicit, linguistically-motivated\nbiases can improve the generalization of learned representations of sign\nlanguage."}
{"id": "2509.04753", "pdf": "https://arxiv.org/pdf/2509.04753.pdf", "abs": "https://arxiv.org/abs/2509.04753", "title": "A Study of Large Language Models for Patient Information Extraction: Model Architecture, Fine-Tuning Strategy, and Multi-task Instruction Tuning", "authors": ["Cheng Peng", "Xinyu Dong", "Mengxian Lyu", "Daniel Paredes", "Yaoyun Zhang", "Yonghui Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural language processing (NLP) is a key technology to extract important\npatient information from clinical narratives to support healthcare\napplications. The rapid development of large language models (LLMs) has\nrevolutionized many NLP tasks in the clinical domain, yet their optimal use in\npatient information extraction tasks requires further exploration. This study\nexamines LLMs' effectiveness in patient information extraction, focusing on LLM\narchitectures, fine-tuning strategies, and multi-task instruction tuning\ntechniques for developing robust and generalizable patient information\nextraction systems. This study aims to explore key concepts of using LLMs for\nclinical concept and relation extraction tasks, including: (1) encoder-only or\ndecoder-only LLMs, (2) prompt-based parameter-efficient fine-tuning (PEFT)\nalgorithms, and (3) multi-task instruction tuning on few-shot learning\nperformance. We benchmarked a suite of LLMs, including encoder-based LLMs\n(BERT, GatorTron) and decoder-based LLMs (GatorTronGPT, Llama 3.1,\nGatorTronLlama), across five datasets. We compared traditional full-size\nfine-tuning and prompt-based PEFT. We explored a multi-task instruction tuning\nframework that combines both tasks across four datasets to evaluate the\nzero-shot and few-shot learning performance using the leave-one-dataset-out\nstrategy."}
{"id": "2509.04770", "pdf": "https://arxiv.org/pdf/2509.04770.pdf", "abs": "https://arxiv.org/abs/2509.04770", "title": "Research on Multi-hop Inference Optimization of LLM Based on MQUAKE Framework", "authors": ["Zucheng Liang", "Wenxin Wei", "Kaijie Zhang", "Hongyi Chen"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Accurately answering complex questions has consistently been a significant\nchallenge for Large Language Models (LLMs). To address this, this paper\nproposes a multi-hop question decomposition method for complex questions,\nbuilding upon research within the MQUAKE framework. Utilizing the LLAMA3 model,\nwe systematically investigate the impact of multi-hop question decomposition\nwithin knowledge graphs on model comprehension and reasoning accuracy, both\nbefore and after model training. In our experiments, we systematically\npartitioned and converted the MQUAKE-T dataset into two distinct formats: a\nsingle-hop dataset designed for directly answering complex questions, and a\nmulti-hop dataset constructed using the multi-hop question decomposition\nmethod. We then fine-tuned the LLAMA3 model on these datasets and conducted\ninference tests. Our results demonstrate that, without fine-tuning the LLM, the\nprediction performance based on the multi-hop question decomposition method\nsignificantly outperforms the method of directly answering complex questions.\nAfter fine-tuning using the LoRA (Low-Rank Adaptation) method, the performance\nof both approaches improved compared to the untrained baseline. Crucially, the\nmethod utilizing multi-hop decomposition consistently maintained its\nsuperiority. These findings validate the effectiveness of the multi-hop\ndecomposition method both before and after training, demonstrating its\ncapability to effectively enhance the LLM's ability to answer complex\nquestions."}
{"id": "2509.04779", "pdf": "https://arxiv.org/pdf/2509.04779.pdf", "abs": "https://arxiv.org/abs/2509.04779", "title": "Decoders Laugh as Loud as Encoders", "authors": ["Eli Borodach", "Raj Dandekar", "Rajat Dandekar", "Sreedath Panat"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "From the dawn of the computer, Allen Turing dreamed of a robot that could\ncommunicate using language as a human being. The recent advances in the field\nof Large Language Models (LLMs) shocked the scientific community when a single\nmodel can apply for various natural language processing (NLP) tasks, while the\noutput results are sometimes even better than most human communication skills.\nModels such as GPT, Claude, Grok, etc. have left their mark on the scientific\ncommunity. However, it is unclear how much these models understand what they\nproduce, especially in a nuanced theme such as humor. The question of whether\ncomputers understand humor is still open (among the decoders, the latest to be\nchecked was GPT-2). We addressed this issue in this paper; we have showed that\na fine-tuned decoder (GPT-4o) performed (Mean F1-macro score of 0.85) as well\nas the best fine-tuned encoder (RoBERTa with a Mean of F1-score 0.86)"}
{"id": "2509.04784", "pdf": "https://arxiv.org/pdf/2509.04784.pdf", "abs": "https://arxiv.org/abs/2509.04784", "title": "Enhancing Diversity in Large Language Models via Determinantal Point Processes", "authors": ["Yilei Chen", "Souradip Chakraborty", "Lorenz Wolf", "Ioannis Ch. Paschalidis", "Aldo Pacchiano"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Supervised fine-tuning and reinforcement learning are two popular methods for\npost-training large language models (LLMs). While improving the model's\nperformance on downstream tasks, they often reduce the model's output\ndiversity, leading to narrow, canonical responses. Existing methods to enhance\ndiversity are limited, either by operating at inference time or by focusing on\nlexical differences. We propose a novel training method named DQO based on\ndeterminantal point processes (DPPs) to jointly optimize LLMs for quality and\nsemantic diversity. Our approach samples and embeds a group of responses for\neach prompt, then uses the determinant of a kernel-based similarity matrix to\nmeasure diversity as the volume spanned by the embeddings of these responses.\nExperiments across instruction-following, summarization, story generation, and\nreasoning tasks demonstrate that our method substantially improves semantic\ndiversity without sacrificing model quality."}
{"id": "2509.04794", "pdf": "https://arxiv.org/pdf/2509.04794.pdf", "abs": "https://arxiv.org/abs/2509.04794", "title": "Personality as a Probe for LLM Evaluation: Method Trade-offs and Downstream Effects", "authors": ["Gunmay Handa", "Zekun Wu", "Adriano Koshiyama", "Philip Treleaven"], "categories": ["cs.CL"], "comment": null, "summary": "Personality manipulation in large language models (LLMs) is increasingly\napplied in customer service and agentic scenarios, yet its mechanisms and\ntrade-offs remain unclear. We present a systematic study of personality control\nusing the Big Five traits, comparing in-context learning (ICL),\nparameter-efficient fine-tuning (PEFT), and mechanistic steering (MS). Our\ncontributions are fourfold. First, we construct a contrastive dataset with\nbalanced high/low trait responses, enabling effective steering vector\ncomputation and fair cross-method evaluation. Second, we introduce a unified\nevaluation framework based on within-run $\\Delta$ analysis that disentangles,\nreasoning capability, agent performance, and demographic bias across MMLU,\nGAIA, and BBQ benchmarks. Third, we develop trait purification techniques to\nseparate openness from conscientiousness, addressing representational overlap\nin trait encoding. Fourth, we propose a three-level stability framework that\nquantifies method-, trait-, and combination-level robustness, offering\npractical guidance under deployment constraints. Experiments on Gemma-2-2B-IT\nand LLaMA-3-8B-Instruct reveal clear trade-offs: ICL achieves strong alignment\nwith minimal capability loss, PEFT delivers the highest alignment at the cost\nof degraded task performance, and MS provides lightweight runtime control with\ncompetitive effectiveness. Trait-level analysis shows openness as uniquely\nchallenging, agreeableness as most resistant to ICL, and personality encoding\nconsolidating around intermediate layers. Taken together, these results\nestablish personality manipulation as a multi-level probe into behavioral\nrepresentation, linking surface conditioning, parameter encoding, and\nactivation-level steering, and positioning mechanistic steering as a\nlightweight alternative to fine-tuning for both deployment and\ninterpretability."}
{"id": "2509.04796", "pdf": "https://arxiv.org/pdf/2509.04796.pdf", "abs": "https://arxiv.org/abs/2509.04796", "title": "Knowledge Collapse in LLMs: When Fluency Survives but Facts Fail under Recursive Synthetic Training", "authors": ["Figarri Keisha", "Zekun Wu", "Ze Wang", "Adriano Koshiyama", "Philip Treleaven"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models increasingly rely on synthetic data due to\nhuman-written content scarcity, yet recursive training on model-generated\noutputs leads to model collapse, a degenerative process threatening factual\nreliability. We define knowledge collapse as a distinct three-stage phenomenon\nwhere factual accuracy deteriorates while surface fluency persists, creating\n\"confidently wrong\" outputs that pose critical risks in accuracy-dependent\ndomains. Through controlled experiments with recursive synthetic training, we\ndemonstrate that collapse trajectory and timing depend critically on\ninstruction format, distinguishing instruction-following collapse from\ntraditional model collapse through its conditional, prompt-dependent nature. We\npropose domain-specific synthetic training as a targeted mitigation strategy\nthat achieves substantial improvements in collapse resistance while maintaining\ncomputational efficiency. Our evaluation framework combines model-centric\nindicators with task-centric metrics to detect distinct degradation phases,\nenabling reproducible assessment of epistemic deterioration across different\nlanguage models. These findings provide both theoretical insights into collapse\ndynamics and practical guidance for sustainable AI training in\nknowledge-intensive applications where accuracy is paramount."}
{"id": "2509.04802", "pdf": "https://arxiv.org/pdf/2509.04802.pdf", "abs": "https://arxiv.org/abs/2509.04802", "title": "Mind the Gap: Evaluating Model- and Agentic-Level Vulnerabilities in LLMs with Action Graphs", "authors": ["Ilham Wicaksono", "Zekun Wu", "Theo King", "Adriano Koshiyama", "Philip Treleaven"], "categories": ["cs.CL"], "comment": null, "summary": "As large language models transition to agentic systems, current safety\nevaluation frameworks face critical gaps in assessing deployment-specific\nrisks. We introduce AgentSeer, an observability-based evaluation framework that\ndecomposes agentic executions into granular action and component graphs,\nenabling systematic agentic-situational assessment. Through cross-model\nvalidation on GPT-OSS-20B and Gemini-2.0-flash using HarmBench single turn and\niterative refinement attacks, we demonstrate fundamental differences between\nmodel-level and agentic-level vulnerability profiles. Model-level evaluation\nreveals baseline differences: GPT-OSS-20B (39.47% ASR) versus Gemini-2.0-flash\n(50.00% ASR), with both models showing susceptibility to social engineering\nwhile maintaining logic-based attack resistance. However, agentic-level\nassessment exposes agent-specific risks invisible to traditional evaluation. We\ndiscover \"agentic-only\" vulnerabilities that emerge exclusively in agentic\ncontexts, with tool-calling showing 24-60% higher ASR across both models.\nCross-model analysis reveals universal agentic patterns, agent transfer\noperations as highest-risk tools, semantic rather than syntactic vulnerability\nmechanisms, and context-dependent attack effectiveness, alongside\nmodel-specific security profiles in absolute ASR levels and optimal injection\nstrategies. Direct attack transfer from model-level to agentic contexts shows\ndegraded performance (GPT-OSS-20B: 57% human injection ASR; Gemini-2.0-flash:\n28%), while context-aware iterative attacks successfully compromise objectives\nthat failed at model-level, confirming systematic evaluation gaps. These\nfindings establish the urgent need for agentic-situation evaluation paradigms,\nwith AgentSeer providing the standardized methodology and empirical validation."}
{"id": "2509.04813", "pdf": "https://arxiv.org/pdf/2509.04813.pdf", "abs": "https://arxiv.org/abs/2509.04813", "title": "Analyzing Finnish Inflectional Classes through Discriminative Lexicon and Deep Learning Models", "authors": ["Alexandre Nikolaev", "Yu-Ying Chuang", "R. Harald Baayen"], "categories": ["cs.CL"], "comment": null, "summary": "Descriptions of complex nominal or verbal systems make use of inflectional\nclasses. Inflectional classes bring together nouns which have similar stem\nchanges and use similar exponents in their paradigms. Although inflectional\nclasses can be very useful for language teaching as well as for setting up\nfinite state morphological systems, it is unclear whether inflectional classes\nare cognitively real, in the sense that native speakers would need to discover\nthese classes in order to learn how to properly inflect the nouns of their\nlanguage. This study investigates whether the Discriminative Lexicon Model\n(DLM) can understand and produce Finnish inflected nouns without setting up\ninflectional classes, using a dataset with 55,271 inflected nouns of 2000\nhigh-frequency Finnish nouns from 49 inflectional classes. Several DLM\ncomprehension and production models were set up. Some models were not informed\nabout frequency of use, and provide insight into learnability with infinite\nexposure (endstate learning). Other models were set up from a usage based\nperspective, and were trained with token frequencies being taken into\nconsideration (frequency-informed learning). On training data, models performed\nwith very high accuracies. For held-out test data, accuracies decreased, as\nexpected, but remained acceptable. Across most models, performance increased\nfor inflectional classes with more types, more lower-frequency words, and more\nhapax legomena, mirroring the productivity of the inflectional classes. The\nmodel struggles more with novel forms of unproductive and less productive\nclasses, and performs far better for unseen forms belonging to productive\nclasses. However, for usage-based production models, frequency was the dominant\npredictor of model performance, and correlations with measures of productivity\nwere tenuous or absent."}
{"id": "2509.04821", "pdf": "https://arxiv.org/pdf/2509.04821.pdf", "abs": "https://arxiv.org/abs/2509.04821", "title": "AFD-SLU: Adaptive Feature Distillation for Spoken Language Understanding", "authors": ["Yan Xie", "Yibo Cui", "Liang Xie", "Erwei Yin"], "categories": ["cs.CL"], "comment": "5 pages, 1 figures", "summary": "Spoken Language Understanding (SLU) is a core component of conversational\nsystems, enabling machines to interpret user utterances. Despite its\nimportance, developing effective SLU systems remains challenging due to the\nscarcity of labeled training data and the computational burden of deploying\nLarge Language Models (LLMs) in real-world applications. To further alleviate\nthese issues, we propose an Adaptive Feature Distillation framework that\ntransfers rich semantic representations from a General Text Embeddings\n(GTE)-based teacher model to a lightweight student model. Our method introduces\na dynamic adapter equipped with a Residual Projection Neural Network (RPNN) to\nalign heterogeneous feature spaces, and a Dynamic Distillation Coefficient\n(DDC) that adaptively modulates the distillation strength based on real-time\nfeedback from intent and slot prediction performance. Experiments on the\nChinese profile-based ProSLU benchmark demonstrate that AFD-SLU achieves\nstate-of-the-art results, with 95.67% intent accuracy, 92.02% slot F1 score,\nand 85.50% overall accuracy."}
{"id": "2509.04866", "pdf": "https://arxiv.org/pdf/2509.04866.pdf", "abs": "https://arxiv.org/abs/2509.04866", "title": "Memorization $\\neq$ Understanding: Do Large Language Models Have the Ability of Scenario Cognition?", "authors": ["Boxiang Ma", "Ru Li", "Yuanlong Wang", "Hongye Tan", "Xiaoli Li"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Main Conference", "summary": "Driven by vast and diverse textual data, large language models (LLMs) have\ndemonstrated impressive performance across numerous natural language processing\n(NLP) tasks. Yet, a critical question persists: does their generalization arise\nfrom mere memorization of training data or from deep semantic understanding? To\ninvestigate this, we propose a bi-perspective evaluation framework to assess\nLLMs' scenario cognition - the ability to link semantic scenario elements with\ntheir arguments in context. Specifically, we introduce a novel scenario-based\ndataset comprising diverse textual descriptions of fictional facts, annotated\nwith scenario elements. LLMs are evaluated through their capacity to answer\nscenario-related questions (model output perspective) and via probing their\ninternal representations for encoded scenario elements-argument associations\n(internal representation perspective). Our experiments reveal that current LLMs\npredominantly rely on superficial memorization, failing to achieve robust\nsemantic scenario cognition, even in simple cases. These findings expose\ncritical limitations in LLMs' semantic understanding and offer cognitive\ninsights for advancing their capabilities."}
{"id": "2509.04868", "pdf": "https://arxiv.org/pdf/2509.04868.pdf", "abs": "https://arxiv.org/abs/2509.04868", "title": "Using LLMs for Multilingual Clinical Entity Linking to ICD-10", "authors": ["Sylvia Vassileva", "Ivan Koychev", "Svetla Boytcheva"], "categories": ["cs.CL"], "comment": "7 pages, 2 Figures, to be published in Proceedings of the 15th\n  International Conference on Recent Advances in Natural Language Processing,\n  RANLP 2025", "summary": "The linking of clinical entities is a crucial part of extracting structured\ninformation from clinical texts. It is the process of assigning a code from a\nmedical ontology or classification to a phrase in the text. The International\nClassification of Diseases - 10th revision (ICD-10) is an international\nstandard for classifying diseases for statistical and insurance purposes.\nAutomatically assigning the correct ICD-10 code to terms in discharge summaries\nwill simplify the work of healthcare professionals and ensure consistent coding\nin hospitals. Our paper proposes an approach for linking clinical terms to\nICD-10 codes in different languages using Large Language Models (LLMs). The\napproach consists of a multistage pipeline that uses clinical dictionaries to\nmatch unambiguous terms in the text and then applies in-context learning with\nGPT-4.1 to predict the ICD-10 code for the terms that do not match the\ndictionary. Our system shows promising results in predicting ICD-10 codes on\ndifferent benchmark datasets in Spanish - 0.89 F1 for categories and 0.78 F1 on\nsubcategories on CodiEsp, and Greek - 0.85 F1 on ElCardioCC."}
{"id": "2509.04884", "pdf": "https://arxiv.org/pdf/2509.04884.pdf", "abs": "https://arxiv.org/abs/2509.04884", "title": "L1RA: Dynamic Rank Assignment in LoRA Fine-Tuning", "authors": ["Raul Singh", "Nicolo Brunello", "Vincenzo Scotti", "Mark James Carman"], "categories": ["cs.CL", "cs.PF"], "comment": "Work published at ICNLSP 2025, waiting for publication link", "summary": "The ability of Large Language Models (LLMs) to solve complex tasks has made\nthem crucial in the development of AI-based applications. However, the high\ncomputational requirements to fine-tune these LLMs on downstream tasks pose\nsignificant challenges, particularly when resources are limited. In response to\nthis challenge, we introduce L1RA, a novel technique aimed at dynamically\ndistributing the rank of low-rank adapters during fine-tuning using LoRA. Given\na rank budget (i.e., total sum of adapters rank), L1RA leverages L1\nregularisation to prune redundant ranks and redistribute them across adapters,\nthereby optimising resource utilisation. Through a series of comprehensive\nexperiments, we empirically demonstrate that L1RA maintains comparable or even\nreduced computational overhead compared to other LoRA variants, including the\nvanilla approach, while achieving same or better performances. Moreover, the\npost-training analysis of rank distribution unveiled insights into the specific\nmodel components requiring the most adaptation to align with the task\nobjective: the feed-forward layers and the attention output projection. These\nresults highlight the efficacy of L1RA in not only enhancing the efficiency of\nLLM fine-tuning, but also in providing valuable diagnostic information for\nmodel refinement and customisation. In conclusion, L1RA stands as a promising\ntechnique for advancing the performance and interpretability of LLM adaptation,\nparticularly in scenarios where computational resources are constrained."}
{"id": "2509.04897", "pdf": "https://arxiv.org/pdf/2509.04897.pdf", "abs": "https://arxiv.org/abs/2509.04897", "title": "PLaMo 2 Technical Report", "authors": ["Preferred Networks", ":", "Kaizaburo Chubachi", "Yasuhiro Fujita", "Shinichi Hemmi", "Yuta Hirokawa", "Toshiki Kataoka", "Goro Kobayashi", "Kenichi Maehashi", "Calvin Metzger", "Hiroaki Mikami", "Shogo Murai", "Daisuke Nishino", "Kento Nozawa", "Shintarou Okada", "Daisuke Okanohara", "Shunta Saito", "Shotaro Sano", "Shuji Suzuki", "Daisuke Tanaka", "Avinash Ummadisingu", "Hanqin Wang", "Sixue Wang", "Tianqi Xu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "In this report, we introduce PLaMo 2, a series of Japanese-focused large\nlanguage models featuring a hybrid Samba-based architecture that transitions to\nfull attention via continual pre-training to support 32K token contexts.\nTraining leverages extensive synthetic corpora to overcome data scarcity, while\ncomputational efficiency is achieved through weight reuse and structured\npruning. This efficient pruning methodology produces an 8B model that achieves\nperformance comparable to our previous 100B model. Post-training further\nrefines the models using a pipeline of supervised fine-tuning (SFT) and direct\npreference optimization (DPO), enhanced by synthetic Japanese instruction data\nand model merging techniques. Optimized for inference using vLLM and\nquantization with minimal accuracy loss, the PLaMo 2 models achieve\nstate-of-the-art results on Japanese benchmarks, outperforming similarly-sized\nopen models in instruction-following, language fluency, and Japanese-specific\nknowledge."}
{"id": "2509.04903", "pdf": "https://arxiv.org/pdf/2509.04903.pdf", "abs": "https://arxiv.org/abs/2509.04903", "title": "ACE-RL: Adaptive Constraint-Enhanced Reward for Long-form Generation Reinforcement Learning", "authors": ["Jianghao Chen", "Wei Sun", "Qixiang Yin", "Lingxing Kong", "Zhixing Tan", "Jiajun Zhang"], "categories": ["cs.CL"], "comment": "Under review, our code is available at https://github.com/ZNLP/ACE-RL", "summary": "Large Language Models (LLMs) have demonstrated remarkable progress in\nlong-context understanding, yet they face significant challenges in\nhigh-quality long-form generation. Existing studies primarily suffer from two\nlimitations: (1) A heavy reliance on scarce, high-quality long-form response\ndata for supervised fine-tuning (SFT) or for pairwise preference reward in\nreinforcement learning (RL). (2) Focus on coarse-grained quality optimization\ndimensions, such as relevance, coherence, and helpfulness, overlooking the\nfine-grained specifics inherent to diverse long-form generation scenarios. To\naddress this issue, we propose a framework using Adaptive Constraint-Enhanced\nreward for long-form generation Reinforcement Learning (ACE-RL). ACE-RL first\nautomatically deconstructs each instruction into a set of fine-grained,\nadaptive constraint criteria by identifying its underlying intents and demands.\nSubsequently, we design a reward mechanism that quantifies the quality of\nlong-form responses based on their satisfaction over corresponding constraints,\nconverting subjective quality evaluation into constraint verification. Finally,\nwe utilize reinforcement learning to guide models toward superior long-form\ngeneration capabilities. Experimental results demonstrate that our ACE-RL\nframework significantly outperforms existing SFT and RL baselines by 20.70% and\n7.32% on WritingBench, and our top-performing model even surpasses proprietary\nsystems like GPT-4o by 7.10%, providing a more effective training paradigm for\nLLMs to generate high-quality content across diverse long-form generation\nscenarios."}
{"id": "2509.04969", "pdf": "https://arxiv.org/pdf/2509.04969.pdf", "abs": "https://arxiv.org/abs/2509.04969", "title": "Classification of kinetic-related injury in hospital triage data using NLP", "authors": ["Midhun Shyam", "Jim Basilakis", "Kieran Luken", "Steven Thomas", "John Crozier", "Paul M. Middleton", "X. Rosalind Wang"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted as a short paper for publishing at ADMA 2025\n  (https://adma2025.github.io), with Supplementary Material available at\n  https://github.com/CRMDS/Kinetic-Injury-Triage", "summary": "Triage notes, created at the start of a patient's hospital visit, contain a\nwealth of information that can help medical staff and researchers understand\nEmergency Department patient epidemiology and the degree of time-dependent\nillness or injury. Unfortunately, applying modern Natural Language Processing\nand Machine Learning techniques to analyse triage data faces some challenges:\nFirstly, hospital data contains highly sensitive information that is subject to\nprivacy regulation thus need to be analysed on site; Secondly, most hospitals\nand medical facilities lack the necessary hardware to fine-tune a Large\nLanguage Model (LLM), much less training one from scratch; Lastly, to identify\nthe records of interest, expert inputs are needed to manually label the\ndatasets, which can be time-consuming and costly. We present in this paper a\npipeline that enables the classification of triage data using LLM and limited\ncompute resources. We first fine-tuned a pre-trained LLM with a classifier\nusing a small (2k) open sourced dataset on a GPU; and then further fine-tuned\nthe model with a hospital specific dataset of 1000 samples on a CPU. We\ndemonstrated that by carefully curating the datasets and leveraging existing\nmodels and open sourced data, we can successfully classify triage data with\nlimited compute resources."}
{"id": "2509.04982", "pdf": "https://arxiv.org/pdf/2509.04982.pdf", "abs": "https://arxiv.org/abs/2509.04982", "title": "Optimizing Small Transformer-Based Language Models for Multi-Label Sentiment Analysis in Short Texts", "authors": ["Julius Neumann", "Robert Lange", "Yuni Susanti", "Michael F√§rber"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": "Accepted at LDD@ECAI 2025", "summary": "Sentiment classification in short text datasets faces significant challenges\nsuch as class imbalance, limited training samples, and the inherent\nsubjectivity of sentiment labels -- issues that are further intensified by the\nlimited context in short texts. These factors make it difficult to resolve\nambiguity and exacerbate data sparsity, hindering effective learning. In this\npaper, we evaluate the effectiveness of small Transformer-based models (i.e.,\nBERT and RoBERTa, with fewer than 1 billion parameters) for multi-label\nsentiment classification, with a particular focus on short-text settings.\nSpecifically, we evaluated three key factors influencing model performance: (1)\ncontinued domain-specific pre-training, (2) data augmentation using\nautomatically generated examples, specifically generative data augmentation,\nand (3) architectural variations of the classification head. Our experiment\nresults show that data augmentation improves classification performance, while\ncontinued pre-training on augmented datasets can introduce noise rather than\nboost accuracy. Furthermore, we confirm that modifications to the\nclassification head yield only marginal benefits. These findings provide\npractical guidance for optimizing BERT-based models in resource-constrained\nsettings and refining strategies for sentiment classification in short-text\ndatasets."}
{"id": "2509.05006", "pdf": "https://arxiv.org/pdf/2509.05006.pdf", "abs": "https://arxiv.org/abs/2509.05006", "title": "Do Large Language Models Need Intent? Revisiting Response Generation Strategies for Service Assistant", "authors": ["Inbal Bolshinsky", "Shani Kupiec", "Almog Sasson", "Yehudit Aperstein", "Alexander Apartsin"], "categories": ["cs.CL", "cs.LG"], "comment": "7 pages, 1 figure", "summary": "In the era of conversational AI, generating accurate and contextually\nappropriate service responses remains a critical challenge. A central question\nremains: Is explicit intent recognition a prerequisite for generating\nhigh-quality service responses, or can models bypass this step and produce\neffective replies directly? This paper conducts a rigorous comparative study to\naddress this fundamental design dilemma. Leveraging two publicly available\nservice interaction datasets, we benchmark several state-of-the-art language\nmodels, including a fine-tuned T5 variant, across both paradigms: Intent-First\nResponse Generation and Direct Response Generation. Evaluation metrics\nencompass both linguistic quality and task success rates, revealing surprising\ninsights into the necessity or redundancy of explicit intent modelling. Our\nfindings challenge conventional assumptions in conversational AI pipelines,\noffering actionable guidelines for designing more efficient and effective\nresponse generation systems."}
{"id": "2509.05056", "pdf": "https://arxiv.org/pdf/2509.05056.pdf", "abs": "https://arxiv.org/abs/2509.05056", "title": "Masked Diffusion Language Models with Frequency-Informed Training", "authors": ["Despoina Kosmopoulou", "Efthymios Georgiou", "Vaggelis Dorovatas", "Georgios Paraskevopoulos", "Alexandros Potamianos"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "We present a masked diffusion language modeling framework for data-efficient\ntraining for the BabyLM 2025 Challenge. Our approach applies diffusion training\nobjectives to language modeling under strict data constraints, incorporating\nfrequency-informed masking that prioritizes learning from rare tokens while\nmaintaining theoretical validity. We explore multiple noise scheduling\nstrategies, including two-mode approaches, and investigate different noise\nweighting schemes within the NELBO objective. We evaluate our method on the\nBabyLM benchmark suite, measuring linguistic competence, world knowledge, and\nhuman-likeness. Results show performance competitive to hybrid\nautoregressive-masked baselines, demonstrating that diffusion-based training\noffers a viable alternative for data-restricted language learning."}
{"id": "2509.05060", "pdf": "https://arxiv.org/pdf/2509.05060.pdf", "abs": "https://arxiv.org/abs/2509.05060", "title": "Entropy2Vec: Crosslingual Language Modeling Entropy as End-to-End Learnable Language Representations", "authors": ["Patrick Amadeus Irawan", "Ryandito Diandaru", "Belati Jagad Bintang Syuhada", "Randy Zakya Suchrady", "Alham Fikri Aji", "Genta Indra Winata", "Fajri Koto", "Samuel Cahyawijaya"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce Entropy2Vec, a novel framework for deriving cross-lingual\nlanguage representations by leveraging the entropy of monolingual language\nmodels. Unlike traditional typological inventories that suffer from feature\nsparsity and static snapshots, Entropy2Vec uses the inherent uncertainty in\nlanguage models to capture typological relationships between languages. By\ntraining a language model on a single language, we hypothesize that the entropy\nof its predictions reflects its structural similarity to other languages: Low\nentropy indicates high similarity, while high entropy suggests greater\ndivergence. This approach yields dense, non-sparse language embeddings that are\nadaptable to different timeframes and free from missing values. Empirical\nevaluations demonstrate that Entropy2Vec embeddings align with established\ntypological categories and achieved competitive performance in downstream\nmultilingual NLP tasks, such as those addressed by the LinguAlchemy framework."}
{"id": "2509.05066", "pdf": "https://arxiv.org/pdf/2509.05066.pdf", "abs": "https://arxiv.org/abs/2509.05066", "title": "ToM-SSI: Evaluating Theory of Mind in Situated Social Interactions", "authors": ["Matteo Bortoletto", "Constantin Ruhdorfer", "Andreas Bulling"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 (Main)", "summary": "Most existing Theory of Mind (ToM) benchmarks for foundation models rely on\nvariations of the Sally-Anne test, offering only a very limited perspective on\nToM and neglecting the complexity of human social interactions. To address this\ngap, we propose ToM-SSI: a new benchmark specifically designed to test ToM\ncapabilities in environments rich with social interactions and spatial\ndynamics. While current ToM benchmarks are limited to text-only or dyadic\ninteractions, ToM-SSI is multimodal and includes group interactions of up to\nfour agents that communicate and move in situated environments. This unique\ndesign allows us to study, for the first time, mixed cooperative-obstructive\nsettings and reasoning about multiple agents' mental state in parallel, thus\ncapturing a wider range of social cognition than existing benchmarks. Our\nevaluations reveal that the current models' performance is still severely\nlimited, especially in these new tasks, highlighting critical gaps for future\nresearch."}
{"id": "2509.05100", "pdf": "https://arxiv.org/pdf/2509.05100.pdf", "abs": "https://arxiv.org/abs/2509.05100", "title": "ICR: Iterative Clarification and Rewriting for Conversational Search", "authors": ["Zhiyu Cao", "Peifeng Li", "Qiaoming Zhu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Most previous work on Conversational Query Rewriting employs an end-to-end\nrewriting paradigm. However, this approach is hindered by the issue of multiple\nfuzzy expressions within the query, which complicates the simultaneous\nidentification and rewriting of multiple positions. To address this issue, we\npropose a novel framework ICR (Iterative Clarification and Rewriting), an\niterative rewriting scheme that pivots on clarification questions. Within this\nframework, the model alternates between generating clarification questions and\nrewritten queries. The experimental results show that our ICR can continuously\nimprove retrieval performance in the clarification-rewriting iterative process,\nthereby achieving state-of-the-art performance on two popular datasets."}
{"id": "2509.05146", "pdf": "https://arxiv.org/pdf/2509.05146.pdf", "abs": "https://arxiv.org/abs/2509.05146", "title": "PRIM: Towards Practical In-Image Multilingual Machine Translation", "authors": ["Yanzhi Tian", "Zeming Liu", "Zhengyang Liu", "Chong Feng", "Xin Li", "Heyan Huang", "Yuhang Guo"], "categories": ["cs.CL", "cs.CV"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "In-Image Machine Translation (IIMT) aims to translate images containing texts\nfrom one language to another. Current research of end-to-end IIMT mainly\nconducts on synthetic data, with simple background, single font, fixed text\nposition, and bilingual translation, which can not fully reflect real world,\ncausing a significant gap between the research and practical conditions. To\nfacilitate research of IIMT in real-world scenarios, we explore Practical\nIn-Image Multilingual Machine Translation (IIMMT). In order to convince the\nlack of publicly available data, we annotate the PRIM dataset, which contains\nreal-world captured one-line text images with complex background, various\nfonts, diverse text positions, and supports multilingual translation\ndirections. We propose an end-to-end model VisTrans to handle the challenge of\npractical conditions in PRIM, which processes visual text and background\ninformation in the image separately, ensuring the capability of multilingual\ntranslation while improving the visual quality. Experimental results indicate\nthe VisTrans achieves a better translation quality and visual effect compared\nto other models. The code and dataset are available at:\nhttps://github.com/BITHLP/PRIM."}
{"id": "2509.05199", "pdf": "https://arxiv.org/pdf/2509.05199.pdf", "abs": "https://arxiv.org/abs/2509.05199", "title": "Triadic Fusion of Cognitive, Functional, and Causal Dimensions for Explainable LLMs: The TAXAL Framework", "authors": ["David Herrera-Poyatos", "Carlos Pel√°ez-Gonz√°lez", "Cristina Zuheros", "Virilo Tejedor", "Rosana Montes", "Francisco Herrera"], "categories": ["cs.CL"], "comment": "27 pages, 9 tables and 2 figures", "summary": "Large Language Models (LLMs) are increasingly being deployed in high-risk\ndomains where opacity, bias, and instability undermine trust and\naccountability. Traditional explainability methods, focused on surface outputs,\ndo not capture the reasoning pathways, planning logic, and systemic impacts of\nagentic LLMs.\n  We introduce TAXAL (Triadic Alignment for eXplainability in Agentic LLMs), a\ntriadic fusion framework that unites three complementary dimensions: cognitive\n(user understanding), functional (practical utility), and causal (faithful\nreasoning). TAXAL provides a unified, role-sensitive foundation for designing,\nevaluating, and deploying explanations in diverse sociotechnical settings.\n  Our analysis synthesizes existing methods, ranging from post-hoc attribution\nand dialogic interfaces to explanation-aware prompting, and situates them\nwithin the TAXAL triadic fusion model. We further demonstrate its applicability\nthrough case studies in law, education, healthcare, and public services,\nshowing how explanation strategies adapt to institutional constraints and\nstakeholder roles.\n  By combining conceptual clarity with design patterns and deployment pathways,\nTAXAL advances explainability as a technical and sociotechnical practice,\nsupporting trustworthy and context-sensitive LLM applications in the era of\nagentic AI."}
{"id": "2509.05209", "pdf": "https://arxiv.org/pdf/2509.05209.pdf", "abs": "https://arxiv.org/abs/2509.05209", "title": "Hunyuan-MT Technical Report", "authors": ["Mao Zheng", "Zheng Li", "Bingxin Qu", "Mingyang Song", "Yang Du", "Mingrui Sun", "Di Wang"], "categories": ["cs.CL"], "comment": null, "summary": "In this report, we introduce Hunyuan-MT-7B, our first open-source\nmultilingual translation model, which supports bidirectional translation across\n33 major languages and places a special emphasis on translation between\nMandarin and several ethnic minority languages as well as dialects.\nFurthermore, to serve and address diverse translation scenarios and enhance\nmodel performance at test time, we introduce Hunyuan-MT-Chimera-7B, a\ntranslation model inspired by the slow thinking mode. This model integrates\nmultiple outputs generated by the Hunyuan-MT-7B model under varying parameter\nsettings, thereby achieving performance superior to that of conventional\nslow-thinking models based on Chain-of-Thought (CoT). The development of our\nmodels follows a holistic training process specifically engineered for\nmultilingual translation, which begins with general and MT-oriented\npre-training to build foundational capabilities, proceeds to Supervised\nFine-Tuning (SFT) for task-specific adaptation, and culminates in advanced\nalignment through Reinforcement Learning (RL) and weak-to-strong RL. Through\ncomprehensive experimentation, we demonstrate that both Hunyuan-MT-7B and\nHunyuan-MT-Chimera-7B significantly outperform all translation-specific models\nof comparable parameter size and most of the SOTA large models, particularly on\nthe task of translation between Mandarin and minority languages as well as\ndialects. In the WMT2025 shared task (General Machine Translation), our models\ndemonstrate state-of-the-art performance, ranking first in 30 out of 31\nlanguage pairs. This result highlights the robustness of our models across a\ndiverse linguistic spectrum, encompassing high-resource languages such as\nChinese, English, and Japanese, as well as low-resource languages including\nCzech, Marathi, Estonian, and Icelandic."}
{"id": "2509.05215", "pdf": "https://arxiv.org/pdf/2509.05215.pdf", "abs": "https://arxiv.org/abs/2509.05215", "title": "BEDTime: A Unified Benchmark for Automatically Describing Time Series", "authors": ["Medhasweta Sen", "Zachary Gottesman", "Jiaxing Qiu", "C. Bayan Bruss", "Nam Nguyen", "Tom Hartvigsen"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Many recent studies have proposed general-purpose foundation models designed\nfor a variety of time series analysis tasks. While several established datasets\nalready exist for evaluating these models, previous works frequently introduce\ntheir models in conjunction with new datasets, limiting opportunities for\ndirect, independent comparisons and obscuring insights into the relative\nstrengths of different methods. Additionally, prior evaluations often cover\nnumerous tasks simultaneously, assessing a broad range of model abilities\nwithout clearly pinpointing which capabilities contribute to overall\nperformance. To address these gaps, we formalize and evaluate 3 tasks that test\na model's ability to describe time series using generic natural language: (1)\nrecognition (True/False question-answering), (2) differentiation (multiple\nchoice question-answering), and (3) generation (open-ended natural language\ndescription). We then unify 4 recent datasets to enable head-to-head model\ncomparisons on each task. Experimentally, in evaluating 13 state-of-the-art\nlanguage, vision--language, and time series--language models, we find that (1)\npopular language-only methods largely underperform, indicating a need for time\nseries-specific architectures, (2) VLMs are quite successful, as expected,\nidentifying the value of vision models for these tasks and (3) pretrained\nmultimodal time series--language models successfully outperform LLMs, but still\nhave significant room for improvement. We also find that all approaches exhibit\nclear fragility in a range of robustness tests. Overall, our benchmark provides\na standardized evaluation on a task necessary for time series reasoning\nsystems."}
{"id": "2509.05218", "pdf": "https://arxiv.org/pdf/2509.05218.pdf", "abs": "https://arxiv.org/abs/2509.05218", "title": "HoPE: Hyperbolic Rotary Positional Encoding for Stable Long-Range Dependency Modeling in Large Language Models", "authors": ["Chang Dai", "Hongyu Shan", "Mingyang Song", "Di Liang"], "categories": ["cs.CL", "cs.AI"], "comment": "This paper proposes Hyperbolic Rotary Positional Encoding (HoPE), a\n  geometric reformulation of positional encoding inspired by Lorentz\n  transformations. HoPE addresses limitations of existing methods like RoPE by\n  enabling stable long-distance dependency modeling. Code and data will be made\n  available upon publication", "summary": "Positional encoding mechanisms enable Transformers to model sequential\nstructure and long-range dependencies in text. While absolute positional\nencodings struggle with extrapolation to longer sequences due to fixed\npositional representations, and relative approaches like Alibi exhibit\nperformance degradation on extremely long contexts, the widely-used Rotary\nPositional Encoding (RoPE) introduces oscillatory attention patterns that\nhinder stable long-distance dependency modelling. We address these limitations\nthrough a geometric reformulation of positional encoding. Drawing inspiration\nfrom Lorentz transformations in hyperbolic geometry, we propose Hyperbolic\nRotary Positional Encoding (HoPE), which leverages hyperbolic functions to\nimplement Lorentz rotations on token representations. Theoretical analysis\ndemonstrates that RoPE is a special case of our generalized formulation. HoPE\nfundamentally resolves RoPE's slation issues by enforcing monotonic decay of\nattention weights with increasing token distances. Extensive experimental\nresults, including perplexity evaluations under several extended sequence\nbenchmarks, show that HoPE consistently exceeds existing positional encoding\nmethods. These findings underscore HoPE's enhanced capacity for representing\nand generalizing long-range dependencies. Data and code will be available."}
{"id": "2509.05226", "pdf": "https://arxiv.org/pdf/2509.05226.pdf", "abs": "https://arxiv.org/abs/2509.05226", "title": "Less is More Tokens: Efficient Math Reasoning via Difficulty-Aware Chain-of-Thought Distillation", "authors": ["Abdul Waheed", "Chancharik Mitra", "Laurie Z. Wang", "Deva Ramanan", "Bhiksha Raj"], "categories": ["cs.CL"], "comment": "28 Pages", "summary": "Chain-of-thought reasoning, while powerful, can produce unnecessarily verbose\noutput for simpler problems. We present a framework for difficulty-aware\nreasoning that teaches models to dynamically adjust reasoning depth based on\nproblem complexity. Remarkably, we show that models can be endowed with such\ndynamic inference pathways without any architectural modifications; we simply\npost-train on data that is carefully curated to include chain-of-thought traces\nthat are proportional in length to problem difficulty. Our analysis reveals\nthat post-training via supervised fine-tuning (SFT) primarily captures patterns\nlike reasoning length and format, while direct preference optimization (DPO)\npreserves reasoning accuracy, with their combination reducing length and\nmaintaining or improving performance. Both quantitative metrics and qualitative\nassessments confirm that models can learn to \"think proportionally\", reasoning\nminimally on simple problems while maintaining depth for complex ones."}
{"id": "2509.05230", "pdf": "https://arxiv.org/pdf/2509.05230.pdf", "abs": "https://arxiv.org/abs/2509.05230", "title": "CURE: Controlled Unlearning for Robust Embeddings -- Mitigating Conceptual Shortcuts in Pre-Trained Language Models", "authors": ["Aysenur Kocak", "Shuo Yang", "Bardh Prenkaj", "Gjergji Kasneci"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at the Conference on Empirical Methods in Natural Language\n  Processing (EMNLP 2025)", "summary": "Pre-trained language models have achieved remarkable success across diverse\napplications but remain susceptible to spurious, concept-driven correlations\nthat impair robustness and fairness. In this work, we introduce CURE, a novel\nand lightweight framework that systematically disentangles and suppresses\nconceptual shortcuts while preserving essential content information. Our method\nfirst extracts concept-irrelevant representations via a dedicated content\nextractor reinforced by a reversal network, ensuring minimal loss of\ntask-relevant information. A subsequent controllable debiasing module employs\ncontrastive learning to finely adjust the influence of residual conceptual\ncues, enabling the model to either diminish harmful biases or harness\nbeneficial correlations as appropriate for the target task. Evaluated on the\nIMDB and Yelp datasets using three pre-trained architectures, CURE achieves an\nabsolute improvement of +10 points in F1 score on IMDB and +2 points on Yelp,\nwhile introducing minimal computational overhead. Our approach establishes a\nflexible, unsupervised blueprint for combating conceptual biases, paving the\nway for more reliable and fair language understanding systems."}
{"id": "2509.05254", "pdf": "https://arxiv.org/pdf/2509.05254.pdf", "abs": "https://arxiv.org/abs/2509.05254", "title": "Uniform Information Density and Syntactic Reduction: Revisiting $\\textit{that}$-Mentioning in English Complement Clauses", "authors": ["Hailin Hao", "Elsi Kaiser"], "categories": ["cs.CL"], "comment": null, "summary": "Speakers often have multiple ways to express the same meaning. The Uniform\nInformation Density (UID) hypothesis suggests that speakers exploit this\nvariability to maintain a consistent rate of information transmission during\nlanguage production. Building on prior work linking UID to syntactic reduction,\nwe revisit the finding that the optional complementizer $\\textit{that}$in\nEnglish complement clauses is more likely to be omitted when the clause has low\ninformation density (i.e., more predictable). We advance this line of research\nby analyzing a large-scale, contemporary conversational corpus and using\nmachine learning and neural language models to refine estimates of information\ndensity. Our results replicated the established relationship between\ninformation density and $\\textit{that}$-mentioning. However, we found that\nprevious measures of information density based on matrix verbs'\nsubcategorization probability capture substantial idiosyncratic lexical\nvariation. By contrast, estimates derived from contextual word embeddings\naccount for additional variance in patterns of complementizer usage."}
{"id": "2509.05282", "pdf": "https://arxiv.org/pdf/2509.05282.pdf", "abs": "https://arxiv.org/abs/2509.05282", "title": "Elucidating the Design Space of Decay in Linear Attention", "authors": ["Zhen Qin", "Xuyang Shen", "Yiran Zhong"], "categories": ["cs.CL"], "comment": "Accepted to COLM 2025. Yiran Zhong is the corresponding author. Code\n  is available at https://github.com/Doraemonzzz/xmixers", "summary": "This paper presents a comprehensive investigation into the decay mechanisms\ninherent in linear complexity sequence models. We systematically delineate the\ndesign space of decay mechanisms across four pivotal dimensions:\nparameterization strategy, which refers to the computational methodology for\ndecay; parameter sharing, which involves the utilization of supplementary\nparameters for decay computation; decay granularity, comparing scalar versus\nvector-based decay; and compatibility with relative positional encoding\nmethods, such as Rotary Position Embedding (RoPE). Through an extensive series\nof experiments conducted on diverse language modeling tasks, we uncovered\nseveral critical insights. Firstly, the design of the parameterization strategy\nfor decay requires meticulous consideration. Our findings indicate that\neffective configurations are typically confined to a specific range of\nparameters. Secondly, parameter sharing cannot be used arbitrarily, as it may\ncause decay values to be too large or too small, thereby significantly\nimpacting performance. Thirdly, under identical parameterization strategies,\nscalar decay generally underperforms compared to its vector-based counterpart.\nHowever, in certain scenarios with alternative parameterization strategies,\nscalar decay may unexpectedly surpass vector decay in efficacy. Lastly, our\nanalysis reveals that RoPE, a commonly employed relative positional encoding\nmethod, typically fails to provide tangible benefits to the majority of linear\nattention mechanisms."}
{"id": "2509.05291", "pdf": "https://arxiv.org/pdf/2509.05291.pdf", "abs": "https://arxiv.org/abs/2509.05291", "title": "Crosscoding Through Time: Tracking Emergence & Consolidation Of Linguistic Representations Throughout LLM Pretraining", "authors": ["Deniz Bayazit", "Aaron Mueller", "Antoine Bosselut"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) learn non-trivial abstractions during\npretraining, like detecting irregular plural noun subjects. However, it is not\nwell understood when and how specific linguistic abilities emerge as\ntraditional evaluation methods such as benchmarking fail to reveal how models\nacquire concepts and capabilities. To bridge this gap and better understand\nmodel training at the concept level, we use sparse crosscoders to discover and\nalign features across model checkpoints. Using this approach, we track the\nevolution of linguistic features during pretraining. We train crosscoders\nbetween open-sourced checkpoint triplets with significant performance and\nrepresentation shifts, and introduce a novel metric, Relative Indirect Effects\n(RelIE), to trace training stages at which individual features become causally\nimportant for task performance. We show that crosscoders can detect feature\nemergence, maintenance, and discontinuation during pretraining. Our approach is\narchitecture-agnostic and scalable, offering a promising path toward more\ninterpretable and fine-grained analysis of representation learning throughout\npretraining."}
{"id": "2506.03083", "pdf": "https://arxiv.org/pdf/2506.03083.pdf", "abs": "https://arxiv.org/abs/2506.03083", "title": "Labelling Data with Unknown References", "authors": ["Adrian de Wynter"], "categories": ["cs.DS", "cs.AI", "cs.CL"], "comment": "Extended version with LLM-based results/analysis", "summary": "An evaluator is trustworthy when there exists some agreed-upon way to measure\nits performance as a labeller. The two ways to establish trustworthiness are\neither by testing it, or by assuming the evaluator `knows' somehow the way to\nlabel the corpus. However, if labelled references (e.g., a development set) are\nunavailable, neither of these approaches work: the former requires the data,\nand the latter is an assumption, not evidence. To address this, we introduce an\nalgorithm (the `No-Data Algorithm') by which to establish trust in an evaluator\nwithout any existing references. Our algorithm works by successively posing\nchallenges to said evaluator. We show that this is sufficient to establish\ntrustworthiness w.h.p., in such a way that when the evaluator actually knows\nthe way to label the corpus, the No-Data Algorithm accepts its output; and,\nconversely, flags untrustworthy evaluators when these are unable to prove it.\nWe present formal proofs of correctness, empirical tests, and applications to\nLLMs-as-judges on low-resource languages."}
{"id": "2509.04481", "pdf": "https://arxiv.org/pdf/2509.04481.pdf", "abs": "https://arxiv.org/abs/2509.04481", "title": "Narrative-to-Scene Generation: An LLM-Driven Pipeline for 2D Game Environments", "authors": ["Yi-Chun Chen", "Arnav Jhala"], "categories": ["cs.GR", "cs.AI", "cs.CL", "cs.MM"], "comment": null, "summary": "Recent advances in large language models(LLMs) enable compelling story\ngeneration, but connecting narrative text to playable visual environments\nremains an open challenge in procedural content generation(PCG). We present a\nlightweight pipeline that transforms short narrative prompts into a sequence of\n2D tile-based game scenes, reflecting the temporal structure of stories. Given\nan LLM-generated narrative, our system identifies three key time frames,\nextracts spatial predicates in the form of \"Object-Relation-Object\" triples,\nand retrieves visual assets using affordance-aware semantic embeddings from the\nGameTileNet dataset. A layered terrain is generated using Cellular Automata,\nand objects are placed using spatial rules grounded in the predicate structure.\nWe evaluated our system in ten diverse stories, analyzing tile-object matching,\naffordance-layer alignment, and spatial constraint satisfaction across frames.\nThis prototype offers a scalable approach to narrative-driven scene generation\nand lays the foundation for future work on multi-frame continuity, symbolic\ntracking, and multi-agent coordination in story-centered PCG."}
{"id": "2509.04642", "pdf": "https://arxiv.org/pdf/2509.04642.pdf", "abs": "https://arxiv.org/abs/2509.04642", "title": "Maestro: Joint Graph & Config Optimization for Reliable AI Agents", "authors": ["Wenxiao Wang", "Priyatham Kattakinda", "Soheil Feizi"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "comment": "Technical Report by RELAI.ai", "summary": "Building reliable LLM agents requires decisions at two levels: the graph\n(which modules exist and how information flows) and the configuration of each\nnode (models, prompts, tools, control knobs). Most existing optimizers tune\nconfigurations while holding the graph fixed, leaving structural failure modes\nunaddressed. We introduce Maestro, a framework-agnostic holistic optimizer for\nLLM agents that jointly searches over graphs and configurations to maximize\nagent quality, subject to explicit rollout/token budgets. Beyond numeric\nmetrics, Maestro leverages reflective textual feedback from traces to\nprioritize edits, improving sample efficiency and targeting specific failure\nmodes. On the IFBench and HotpotQA benchmarks, Maestro consistently surpasses\nleading prompt optimizers--MIPROv2, GEPA, and GEPA+Merge--by an average of 12%,\n4.9%, and 4.86%, respectively; even when restricted to prompt-only\noptimization, it still leads by 9.65%, 2.37%, and 2.41%. Maestro achieves these\nresults with far fewer rollouts than GEPA. We further show large gains on two\napplications (interviewer & RAG agents), highlighting that joint graph &\nconfiguration search addresses structural failure modes that prompt tuning\nalone cannot fix."}
{"id": "2509.04667", "pdf": "https://arxiv.org/pdf/2509.04667.pdf", "abs": "https://arxiv.org/abs/2509.04667", "title": "DarkStream: real-time speech anonymization with low latency", "authors": ["Waris Quamer", "Ricardo Gutierrez-Osuna"], "categories": ["eess.AS", "cs.CL", "cs.LG"], "comment": "Accepted for presentation at ASRU 2025", "summary": "We propose DarkStream, a streaming speech synthesis model for real-time\nspeaker anonymization. To improve content encoding under strict latency\nconstraints, DarkStream combines a causal waveform encoder, a short lookahead\nbuffer, and transformer-based contextual layers. To further reduce inference\ntime, the model generates waveforms directly via a neural vocoder, thus\nremoving intermediate mel-spectrogram conversions. Finally, DarkStream\nanonymizes speaker identity by injecting a GAN-generated pseudo-speaker\nembedding into linguistic features from the content encoder. Evaluations show\nour model achieves strong anonymization, yielding close to 50% speaker\nverification EER (near-chance performance) on the lazy-informed attack\nscenario, while maintaining acceptable linguistic intelligibility (WER within\n9%). By balancing low-latency, robust privacy, and minimal intelligibility\ndegradation, DarkStream provides a practical solution for privacy-preserving\nreal-time speech communication."}
{"id": "2509.04731", "pdf": "https://arxiv.org/pdf/2509.04731.pdf", "abs": "https://arxiv.org/abs/2509.04731", "title": "Language-Driven Hierarchical Task Structures as Explicit World Models for Multi-Agent Learning", "authors": ["Brennen Hill"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "cs.RO", "68T05, 90C40, 91A26, 68T42, 93E35", "I.2.11; I.2.6; I.2.8; I.2.9; I.2.7"], "comment": null, "summary": "The convergence of Language models, Agent models, and World models represents\na critical frontier for artificial intelligence. While recent progress has\nfocused on scaling Language and Agent models, the development of sophisticated,\nexplicit World Models remains a key bottleneck, particularly for complex,\nlong-horizon multi-agent tasks. In domains such as robotic soccer, agents\ntrained via standard reinforcement learning in high-fidelity but\nstructurally-flat simulators often fail due to intractable exploration spaces\nand sparse rewards. This position paper argues that the next frontier in\ndeveloping capable agents lies in creating environments that possess an\nexplicit, hierarchical World Model. We contend that this is best achieved\nthrough hierarchical scaffolding, where complex goals are decomposed into\nstructured, manageable subgoals. Drawing evidence from a systematic review of\n2024 research in multi-agent soccer, we identify a clear and decisive trend\ntowards integrating symbolic and hierarchical methods with multi-agent\nreinforcement learning (MARL). These approaches implicitly or explicitly\nconstruct a task-based world model to guide agent learning. We then propose a\nparadigm shift: leveraging Large Language Models to dynamically generate this\nhierarchical scaffold, effectively using language to structure the World Model\non the fly. This language-driven world model provides an intrinsic curriculum,\ndense and meaningful learning signals, and a framework for compositional\nlearning, enabling Agent Models to acquire sophisticated, strategic behaviors\nwith far greater sample efficiency. By building environments with explicit,\nlanguage-configurable task layers, we can bridge the gap between low-level\nreactive behaviors and high-level strategic team play, creating a powerful and\ngeneralizable framework for training the next generation of intelligent agents."}
{"id": "2509.04744", "pdf": "https://arxiv.org/pdf/2509.04744.pdf", "abs": "https://arxiv.org/abs/2509.04744", "title": "WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning", "authors": ["Gagan Mundada", "Yash Vishe", "Amit Namburi", "Xin Xu", "Zachary Novack", "Julian McAuley", "Junda Wu"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated\nimpressive capabilities across various vision-language tasks. However, their\nreasoning abilities in the multimodal symbolic music domain remain largely\nunexplored. We introduce WildScore, the first in-the-wild multimodal symbolic\nmusic reasoning and analysis benchmark, designed to evaluate MLLMs' capacity to\ninterpret real-world music scores and answer complex musicological queries.\nEach instance in WildScore is sourced from genuine musical compositions and\naccompanied by authentic user-generated questions and discussions, capturing\nthe intricacies of practical music analysis. To facilitate systematic\nevaluation, we propose a systematic taxonomy, comprising both high-level and\nfine-grained musicological ontologies. Furthermore, we frame complex music\nreasoning as multiple-choice question answering, enabling controlled and\nscalable assessment of MLLMs' symbolic music understanding. Empirical\nbenchmarking of state-of-the-art MLLMs on WildScore reveals intriguing patterns\nin their visual-symbolic reasoning, uncovering both promising directions and\npersistent challenges for MLLMs in symbolic music reasoning and analysis. We\nrelease the dataset and code."}
{"id": "2509.04810", "pdf": "https://arxiv.org/pdf/2509.04810.pdf", "abs": "https://arxiv.org/abs/2509.04810", "title": "Code Review Without Borders: Evaluating Synthetic vs. Real Data for Review Recommendation", "authors": ["Yogev Cohen", "Dudi Ohayon", "Romy Somkin", "Yehudit Aperstein", "Alexander Apartsin"], "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": "4 pages, 1 figure", "summary": "Automating the decision of whether a code change requires manual review is\nvital for maintaining software quality in modern development workflows.\nHowever, the emergence of new programming languages and frameworks creates a\ncritical bottleneck: while large volumes of unlabelled code are readily\navailable, there is an insufficient amount of labelled data to train supervised\nmodels for review classification. We address this challenge by leveraging Large\nLanguage Models (LLMs) to translate code changes from well-resourced languages\ninto equivalent changes in underrepresented or emerging languages, generating\nsynthetic training data where labelled examples are scarce. We assume that\nalthough LLMs have learned the syntax and semantics of new languages from\navailable unlabelled code, they have yet to fully grasp which code changes are\nconsidered significant or review-worthy within these emerging ecosystems. To\novercome this, we use LLMs to generate synthetic change examples and train\nsupervised classifiers on them. We systematically compare the performance of\nthese classifiers against models trained on real labelled data. Our experiments\nacross multiple GitHub repositories and language pairs demonstrate that\nLLM-generated synthetic data can effectively bootstrap review recommendation\nsystems, narrowing the performance gap even in low-resource settings. This\napproach provides a scalable pathway to extend automated code review\ncapabilities to rapidly evolving technology stacks, even in the absence of\nannotated data."}
{"id": "2509.04823", "pdf": "https://arxiv.org/pdf/2509.04823.pdf", "abs": "https://arxiv.org/abs/2509.04823", "title": "Evaluating Cognitive-Behavioral Fixation via Multimodal User Viewing Patterns on Social Media", "authors": ["Yujie Wang", "Yunwei Zhao", "Jing Yang", "Han Han", "Shiguang Shan", "Jie Zhang"], "categories": ["cs.SI", "cs.CL"], "comment": null, "summary": "Digital social media platforms frequently contribute to cognitive-behavioral\nfixation, a phenomenon in which users exhibit sustained and repetitive\nengagement with narrow content domains. While cognitive-behavioral fixation has\nbeen extensively studied in psychology, methods for computationally detecting\nand evaluating such fixation remain underexplored. To address this gap, we\npropose a novel framework for assessing cognitive-behavioral fixation by\nanalyzing users' multimodal social media engagement patterns. Specifically, we\nintroduce a multimodal topic extraction module and a cognitive-behavioral\nfixation quantification module that collaboratively enable adaptive,\nhierarchical, and interpretable assessment of user behavior. Experiments on\nexisting benchmarks and a newly curated multimodal dataset demonstrate the\neffectiveness of our approach, laying the groundwork for scalable computational\nanalysis of cognitive fixation. All code in this project is publicly available\nfor research purposes at\nhttps://github.com/Liskie/cognitive-fixation-evaluation."}
{"id": "2509.04908", "pdf": "https://arxiv.org/pdf/2509.04908.pdf", "abs": "https://arxiv.org/abs/2509.04908", "title": "SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing", "authors": ["Hongyi Jing", "Jiafu Chen", "Chen Rao", "Ziqiang Dang", "Jiajie Teng", "Tianyi Chu", "Juncheng Mo", "Shuo Fang", "Huaizhong Lin", "Rui Lv", "Chenguang Ma", "Lei Zhao"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": null, "summary": "The existing Multimodal Large Language Models (MLLMs) for GUI perception have\nmade great progress. However, the following challenges still exist in prior\nmethods: 1) They model discrete coordinates based on text autoregressive\nmechanism, which results in lower grounding accuracy and slower inference\nspeed. 2) They can only locate predefined sets of elements and are not capable\nof parsing the entire interface, which hampers the broad application and\nsupport for downstream tasks. To address the above issues, we propose\nSparkUI-Parser, a novel end-to-end framework where higher localization\nprecision and fine-grained parsing capability of the entire interface are\nsimultaneously achieved. Specifically, instead of using probability-based\ndiscrete modeling, we perform continuous modeling of coordinates based on a\npre-trained Multimodal Large Language Model (MLLM) with an additional token\nrouter and coordinate decoder. This effectively mitigates the limitations\ninherent in the discrete output characteristics and the token-by-token\ngeneration process of MLLMs, consequently boosting both the accuracy and the\ninference speed. To further enhance robustness, a rejection mechanism based on\na modified Hungarian matching algorithm is introduced, which empowers the model\nto identify and reject non-existent elements, thereby reducing false positives.\nMoreover, we present ScreenParse, a rigorously constructed benchmark to\nsystematically assess structural perception capabilities of GUI models across\ndiverse scenarios. Extensive experiments demonstrate that our approach\nconsistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2,\nCAGUI-Grounding and ScreenParse benchmarks. The resources are available at\nhttps://github.com/antgroup/SparkUI-Parser."}
{"id": "2509.04926", "pdf": "https://arxiv.org/pdf/2509.04926.pdf", "abs": "https://arxiv.org/abs/2509.04926", "title": "Towards Ontology-Based Descriptions of Conversations with Qualitatively-Defined Concepts", "authors": ["Barbara Gendron", "Ga√´l Guibon", "Mathieu D'aquin"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted at TOTh 2025 (Terminology \\& Ontology: Theories and\n  applications)", "summary": "The controllability of Large Language Models (LLMs) when used as\nconversational agents is a key challenge, particularly to ensure predictable\nand user-personalized responses. This work proposes an ontology-based approach\nto formally define conversational features that are typically qualitative in\nnature. By leveraging a set of linguistic descriptors, we derive quantitative\ndefinitions for qualitatively-defined concepts, enabling their integration into\nan ontology for reasoning and consistency checking. We apply this framework to\nthe task of proficiency-level control in conversations, using CEFR language\nproficiency levels as a case study. These definitions are then formalized in\ndescription logic and incorporated into an ontology, which guides controlled\ntext generation of an LLM through fine-tuning. Experimental results demonstrate\nthat our approach provides consistent and explainable proficiency-level\ndefinitions, improving transparency in conversational AI."}
{"id": "2509.05007", "pdf": "https://arxiv.org/pdf/2509.05007.pdf", "abs": "https://arxiv.org/abs/2509.05007", "title": "Sticker-TTS: Learn to Utilize Historical Experience with a Sticker-driven Test-Time Scaling Framework", "authors": ["Jie Chen", "Jinhao Jiang", "Yingqian Min", "Zican Dong", "Shijie Wang", "Wayne Xin Zhao", "Ji-Rong Wen"], "categories": ["cs.AI", "cs.CL", "I.2.7"], "comment": "11 pages, 1 figures, 5 tables", "summary": "Large reasoning models (LRMs) have exhibited strong performance on complex\nreasoning tasks, with further gains achievable through increased computational\nbudgets at inference. However, current test-time scaling methods predominantly\nrely on redundant sampling, ignoring the historical experience utilization,\nthereby limiting computational efficiency. To overcome this limitation, we\npropose Sticker-TTS, a novel test-time scaling framework that coordinates three\ncollaborative LRMs to iteratively explore and refine solutions guided by\nhistorical attempts. At the core of our framework are distilled key\nconditions-termed stickers-which drive the extraction, refinement, and reuse of\ncritical information across multiple rounds of reasoning. To further enhance\nthe efficiency and performance of our framework, we introduce a two-stage\noptimization strategy that combines imitation learning with self-improvement,\nenabling progressive refinement. Extensive evaluations on three challenging\nmathematical reasoning benchmarks, including AIME-24, AIME-25, and OlymMATH,\ndemonstrate that Sticker-TTS consistently surpasses strong baselines, including\nself-consistency and advanced reinforcement learning approaches, under\ncomparable inference budgets. These results highlight the effectiveness of\nsticker-guided historical experience utilization. Our code and data are\navailable at https://github.com/RUCAIBox/Sticker-TTS."}
{"id": "2509.05072", "pdf": "https://arxiv.org/pdf/2509.05072.pdf", "abs": "https://arxiv.org/abs/2509.05072", "title": "Finding your MUSE: Mining Unexpected Solutions Engine", "authors": ["Nir Sweed", "Hanit Hakim", "Ben Wolfson", "Hila Lifshitz", "Dafna Shahaf"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Innovators often exhibit cognitive fixation on existing solutions or nascent\nideas, hindering the exploration of novel alternatives. This paper introduces a\nmethodology for constructing Functional Concept Graphs (FCGs), interconnected\nrepresentations of functional elements that support abstraction, problem\nreframing, and analogical inspiration. Our approach yields large-scale,\nhigh-quality FCGs with explicit abstraction relations, overcoming limitations\nof prior work. We further present MUSE, an algorithm leveraging FCGs to\ngenerate creative inspirations for a given problem. We demonstrate our method\nby computing an FCG on 500K patents, which we release for further research."}
{"id": "2509.05276", "pdf": "https://arxiv.org/pdf/2509.05276.pdf", "abs": "https://arxiv.org/abs/2509.05276", "title": "SpikingBrain Technical Report: Spiking Brain-inspired Large Models", "authors": ["Yuqi Pan", "Yupeng Feng", "Jinghao Zhuang", "Siyu Ding", "Zehao Liu", "Bohan Sun", "Yuhong Chou", "Han Xu", "Xuerui Qiu", "Anlin Deng", "Anjie Hu", "Peng Zhou", "Man Yao", "Jibin Wu", "Jian Yang", "Guoliang Sun", "Bo Xu", "Guoqi Li"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Mainstream Transformer-based large language models face major efficiency\nbottlenecks: training computation scales quadratically with sequence length,\nand inference memory grows linearly, limiting long-context processing. Building\nlarge models on non-NVIDIA platforms also poses challenges for stable and\nefficient training. To address this, we introduce SpikingBrain, a family of\nbrain-inspired models designed for efficient long-context training and\ninference. SpikingBrain leverages the MetaX GPU cluster and focuses on three\naspects: (1) Model Architecture: linear and hybrid-linear attention\narchitectures with adaptive spiking neurons; (2) Algorithmic Optimizations: an\nefficient, conversion-based training pipeline and a dedicated spike coding\nframework; (3) System Engineering: customized training frameworks, operator\nlibraries, and parallelism strategies tailored to MetaX hardware.\n  Using these techniques, we develop two models: SpikingBrain-7B, a linear LLM,\nand SpikingBrain-76B, a hybrid-linear MoE LLM. These models demonstrate the\nfeasibility of large-scale LLM development on non-NVIDIA platforms.\nSpikingBrain achieves performance comparable to open-source Transformer\nbaselines while using only about 150B tokens for continual pre-training. Our\nmodels significantly improve long-sequence training efficiency and deliver\ninference with (partially) constant memory and event-driven spiking behavior.\nFor example, SpikingBrain-7B attains over 100x speedup in Time to First Token\nfor 4M-token sequences. Training remains stable for weeks on hundreds of MetaX\nC550 GPUs, with the 7B model reaching a Model FLOPs Utilization of 23.4\npercent. The proposed spiking scheme achieves 69.15 percent sparsity, enabling\nlow-power operation. Overall, this work demonstrates the potential of\nbrain-inspired mechanisms to drive the next generation of efficient and\nscalable large model design."}
{"id": "2509.05293", "pdf": "https://arxiv.org/pdf/2509.05293.pdf", "abs": "https://arxiv.org/abs/2509.05293", "title": "Non-Termination Proving: 100 Million LoC and Beyond", "authors": ["Julien Vanegue", "Jules Villard", "Peter O'Hearn", "Azalea Raad"], "categories": ["cs.PL", "cs.CL", "cs.SE", "D.3; F.3"], "comment": "14 pages, 4 figures", "summary": "We report on our tool, Pulse Infinite, that uses proof techniques to show\nnon-termination (divergence) in large programs. Pulse Infinite works\ncompositionally and under-approximately: the former supports scale, and the\nlatter ensures soundness for proving divergence. Prior work focused on small\nbenchmarks in the tens or hundreds of lines of code (LoC), and scale limits\ntheir practicality: a single company may have tens of millions, or even\nhundreds of millions of LoC or more. We report on applying Pulse Infinite to\nover a hundred million lines of open-source and proprietary software written in\nC, C++, and Hack, identifying over 30 previously unknown issues, establishing a\nnew state of the art for detecting divergence in real-world codebases."}
{"id": "2302.01626", "pdf": "https://arxiv.org/pdf/2302.01626.pdf", "abs": "https://arxiv.org/abs/2302.01626", "title": "Modeling Sequential Sentence Relation to Improve Cross-lingual Dense Retrieval", "authors": ["Shunyu Zhang", "Yaobo Liang", "Ming Gong", "Daxin Jiang", "Nan Duan"], "categories": ["cs.CL", "cs.IR"], "comment": "Published at ICLR 2023", "summary": "Recently multi-lingual pre-trained language models (PLM) such as mBERT and\nXLM-R have achieved impressive strides in cross-lingual dense retrieval.\nDespite its successes, they are general-purpose PLM while the multilingual PLM\ntailored for cross-lingual retrieval is still unexplored. Motivated by an\nobservation that the sentences in parallel documents are approximately in the\nsame order, which is universal across languages, we propose to model this\nsequential sentence relation to facilitate cross-lingual representation\nlearning. Specifically, we propose a multilingual PLM called masked sentence\nmodel (MSM), which consists of a sentence encoder to generate the sentence\nrepresentations, and a document encoder applied to a sequence of sentence\nvectors from a document. The document encoder is shared for all languages to\nmodel the universal sequential sentence relation across languages. To train the\nmodel, we propose a masked sentence prediction task, which masks and predicts\nthe sentence vector via a hierarchical contrastive loss with sampled negatives.\nComprehensive experiments on four cross-lingual retrieval tasks show MSM\nsignificantly outperforms existing advanced pre-training models, demonstrating\nthe effectiveness and stronger cross-lingual retrieval capabilities of our\napproach. Code and model are available at https://github.com/shunyuzh/MSM."}
{"id": "2401.14295", "pdf": "https://arxiv.org/pdf/2401.14295.pdf", "abs": "https://arxiv.org/abs/2401.14295", "title": "Demystifying Chains, Trees, and Graphs of Thoughts", "authors": ["Maciej Besta", "Florim Memedi", "Zhenyu Zhang", "Robert Gerstenberger", "Guangyuan Piao", "Nils Blach", "Piotr Nyczyk", "Marcin Copik", "Grzegorz Kwa≈õniewski", "J√ºrgen M√ºller", "Lukas Gianinazzi", "Ales Kubicek", "Hubert Niewiadomski", "Aidan O'Mahony", "Onur Mutlu", "Torsten Hoefler"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The field of natural language processing (NLP) has witnessed significant\nprogress in recent years, with a notable focus on improving large language\nmodels' (LLM) performance through innovative prompting techniques. Among these,\nprompt engineering coupled with structures has emerged as a promising paradigm,\nwith designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts,\nin which the overall LLM reasoning is guided by a structure such as a graph. As\nillustrated with numerous examples, this paradigm significantly enhances the\nLLM's capability to solve numerous tasks, ranging from logical or mathematical\nreasoning to planning or creative writing. To facilitate the understanding of\nthis growing field and pave the way for future developments, we devise a\ngeneral blueprint for effective and efficient LLM reasoning schemes. For this,\nwe conduct an in-depth analysis of the prompt execution pipeline, clarifying\nand clearly defining different concepts. We then build the first taxonomy of\nstructure-enhanced LLM reasoning schemes. We focus on identifying fundamental\nclasses of harnessed structures, and we analyze the representations of these\nstructures, algorithms executed with these structures, and many others. We\nrefer to these structures as reasoning topologies, because their representation\nbecomes to a degree spatial, as they are contained within the LLM context. Our\nstudy compares existing prompting schemes using the proposed taxonomy,\ndiscussing how certain design choices lead to different patterns in performance\nand cost. We also outline theoretical underpinnings, relationships between\nprompting and other parts of the LLM ecosystem such as knowledge bases, and the\nassociated research challenges. Our work will help to advance future prompt\nengineering techniques."}
{"id": "2402.12226", "pdf": "https://arxiv.org/pdf/2402.12226.pdf", "abs": "https://arxiv.org/abs/2402.12226", "title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling", "authors": ["Jun Zhan", "Junqi Dai", "Jiasheng Ye", "Yunhua Zhou", "Dong Zhang", "Zhigeng Liu", "Xin Zhang", "Ruibin Yuan", "Ge Zhang", "Linyang Li", "Hang Yan", "Jie Fu", "Tao Gui", "Tianxiang Sun", "Yugang Jiang", "Xipeng Qiu"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "28 pages, 16 figures, under review, work in progress", "summary": "We introduce AnyGPT, an any-to-any multimodal language model that utilizes\ndiscrete representations for the unified processing of various modalities,\nincluding speech, text, images, and music. AnyGPT can be trained stably without\nany alterations to the current large language model (LLM) architecture or\ntraining paradigms. Instead, it relies exclusively on data-level preprocessing,\nfacilitating the seamless integration of new modalities into LLMs, akin to the\nincorporation of new languages. We build a multimodal text-centric dataset for\nmultimodal alignment pre-training. Utilizing generative models, we synthesize\nthe first large-scale any-to-any multimodal instruction dataset. It consists of\n108k samples of multi-turn conversations that intricately interweave various\nmodalities, thus equipping the model to handle arbitrary combinations of\nmultimodal inputs and outputs. Experimental results demonstrate that AnyGPT is\ncapable of facilitating any-to-any multimodal conversation while achieving\nperformance comparable to specialized models across all modalities, proving\nthat discrete representations can effectively and conveniently unify multiple\nmodalities within a language model. Demos are shown in\nhttps://junzhan2000.github.io/AnyGPT.github.io/"}
{"id": "2407.18416", "pdf": "https://arxiv.org/pdf/2407.18416.pdf", "abs": "https://arxiv.org/abs/2407.18416", "title": "PersonaGym: Evaluating Persona Agents and LLMs", "authors": ["Vinay Samuel", "Henry Peng Zou", "Yue Zhou", "Shreyas Chaudhari", "Ashwin Kalyan", "Tanmay Rajpurohit", "Ameet Deshpande", "Karthik Narasimhan", "Vishvak Murahari"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "EMNLP Findings 2025", "summary": "Persona agents, which are LLM agents conditioned to act according to an\nassigned persona, enable contextually rich and user aligned interactions across\ndomains like education and healthcare. However, evaluating how faithfully these\nagents adhere to their personas remains a significant challenge, particularly\nin free-form settings that demand consistency across diverse, persona-relevant\nenvironments. We introduce PersonaGym, the first dynamic evaluation framework\nfor persona agents, and PersonaScore, a human-aligned automatic metric grounded\nin decision theory that enables comprehensive large-scale evaluation. Our\nevaluation of 10 leading LLMs across 200 personas and 10,000 questions reveals\nsignificant advancement opportunities. For example, GPT-4.1 had the exact same\nPersonaScore as LLaMA-3-8b despite being a more recent and advanced closed\nsource model. Importantly, increased model size and complexity do not\nnecessarily enhance persona agent capabilities, underscoring the need for\nalgorithmic and architectural innovation toward faithful, performant persona\nagents."}
{"id": "2407.19835", "pdf": "https://arxiv.org/pdf/2407.19835.pdf", "abs": "https://arxiv.org/abs/2407.19835", "title": "ATHAR: A High-Quality and Diverse Dataset for Classical Arabic to English Translation", "authors": ["Mohammed Khalil", "Mohammed Sabry"], "categories": ["cs.CL", "cs.AI"], "comment": "ArabicNLP 2025", "summary": "Classical Arabic represents a significant era that encompasses the golden age\nof Arab culture, philosophy, and scientific literature. With a broad consensus\non the importance of translating these literatures to enrich knowledge\ndissemination across communities, the advent of large language models (LLMs)\nand translation systems offers promising tools to facilitate this goal.\nHowever, we have identified a scarcity of translation datasets in Classical\nArabic, which are often limited in scope and topics, hindering the development\nof high-quality translation systems. In response, we present the ATHAR dataset,\nwhich comprises 66,000 high-quality classical Arabic to English translation\nsamples that cover a wide array of topics including science, culture, and\nphilosophy. Furthermore, we assess the performance of current state-of-the-art\nLLMs under various settings, concluding that there is a need for such datasets\nin current systems. Our findings highlight how models can benefit from\nfine-tuning or incorporating this dataset into their pretraining pipelines. The\ndataset is publicly available on the HuggingFace Data Hub:\nhttps://huggingface.co/datasets/mohamed-khalil/ATHAR."}
{"id": "2408.02544", "pdf": "https://arxiv.org/pdf/2408.02544.pdf", "abs": "https://arxiv.org/abs/2408.02544", "title": "Caution for the Environment: Multimodal LLM Agents are Susceptible to Environmental Distractions", "authors": ["Xinbei Ma", "Yiting Wang", "Yao Yao", "Tongxin Yuan", "Aston Zhang", "Zhuosheng Zhang", "Hai Zhao"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "This paper investigates the faithfulness of multimodal large language model\n(MLLM) agents in a graphical user interface (GUI) environment, aiming to\naddress the research question of whether multimodal GUI agents can be\ndistracted by environmental context. A general scenario is proposed where both\nthe user and the agent are benign, and the environment, while not malicious,\ncontains unrelated content. A wide range of MLLMs are evaluated as GUI agents\nusing a simulated dataset, following three working patterns with different\nlevels of perception. Experimental results reveal that even the most powerful\nmodels, whether generalist agents or specialist GUI agents, are susceptible to\ndistractions. While recent studies predominantly focus on the helpfulness of\nagents, our findings first indicate that these agents are prone to\nenvironmental distractions. Furthermore, we implement an adversarial\nenvironment injection and analyze the approach to improve faithfulness, calling\nfor a collective focus on this important topic."}
{"id": "2408.13518", "pdf": "https://arxiv.org/pdf/2408.13518.pdf", "abs": "https://arxiv.org/abs/2408.13518", "title": "Selective Preference Optimization via Token-Level Reward Function Estimation", "authors": ["Kailai Yang", "Zhiwei Liu", "Qianqian Xie", "Jimin Huang", "Erxue Min", "Sophia Ananiadou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by the EMNLP 2025 main conference", "summary": "Recent advancements in large language model alignment leverage token-level\nsupervisions to perform fine-grained preference optimization. However, existing\ntoken-level alignment methods either optimize on all available tokens, which\ncan be noisy and inefficient, or perform selective training with complex and\nexpensive key token selection strategies. In this work, we propose Selective\nPreference Optimization (SePO), a novel selective alignment strategy that\ncenters on efficient key token selection. SePO proposes the first token\nselection method based on Direct Preference Optimization (DPO), which trains an\noracle model to estimate a token-level reward function on the target data. This\nmethod applies to any existing alignment datasets with response-level\nannotations and enables cost-efficient token selection with small-scale oracle\nmodels and training data. The estimated reward function is then utilized to\nscore all tokens within the target dataset, where only the key tokens are\nselected to supervise the target policy model with a reference model-free\ncontrastive objective function. Extensive experiments on three public\nevaluation benchmarks show that SePO significantly outperforms competitive\nbaseline methods by only optimizing 30% key tokens on the target dataset. SePO\napplications on weak-to-strong generalization show that weak oracle models\neffectively supervise strong policy models with up to 16.8x more parameters.\nSePO also effectively selects key tokens from out-of-distribution data to\nenhance strong policy models and alleviate the over-optimization problem."}
{"id": "2409.12929", "pdf": "https://arxiv.org/pdf/2409.12929.pdf", "abs": "https://arxiv.org/abs/2409.12929", "title": "LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning", "authors": ["Jin Jiang", "Yuchen Yan", "Yang Liu", "Jianing Wang", "Shuai Peng", "Xunliang Cai", "Yixin Cao", "Mengdi Zhang", "Liangcai Gao"], "categories": ["cs.CL"], "comment": "19 pages, ACL 2025 (Volume 1 Long Papers), pages 26200-26218", "summary": "In this paper, we propose a new data synthesis method called\n\\textbf{LogicPro}, which leverages LeetCode-style algorithm\n\\underline{Pro}blems and their corresponding \\underline{Pro}gram solutions to\nsynthesize Complex \\underline{Logic}al Reasoning data in text format. First, we\nsynthesize complex reasoning problems through source algorithm problems and\ntest cases. Then, standard answers and intermediate variable outputs are\nobtained for each problem based on standard python solutions and test cases.\nFinally, with the guidance of code intermediate variables, we synthesize the\ntext reasoning process for each reasoning problems. Through this method, we can\nsynthesize data that is difficult, scalable, effective, and comes with golden\nstandard answers and high-quality reasoning processes. As a result, with our\n540K synthesized dataset constructed solely from 2,360 algorithm problems, our\napproach \\footnote{Code and data are publicly available at\nhttps://github.com/jiangjin1999/LogicPro} achieves significant improvements in\nmultiple models for the datasets \\textit{BBH$^{27}$}, \\textit{LogicBench},\n\\textit{DROP}, \\textit{AR-LSAT}, and \\textit{GSM8K}, etc. outperforming a wide\nrange of existing reasoning datasets."}
{"id": "2411.19858", "pdf": "https://arxiv.org/pdf/2411.19858.pdf", "abs": "https://arxiv.org/abs/2411.19858", "title": "What fifty-one years of Linguistics and Artificial Intelligence research tell us about their correlation: A scientometric analysis", "authors": ["Mohammed Q. Shormani", "Yehia A. AlSohbani"], "categories": ["cs.CL", "cs-CL", "F.2.2; I.2.7"], "comment": "26 pages, 15 figures", "summary": "There is a strong correlation between linguistics and artificial intelligence\n(AI), best manifested by deep learning language models. This study provides a\nthorough scientometric analysis of this correlation, synthesizing the\nintellectual production over 51 years, from 1974 to 2024. Web of Science Core\nCollection (WoSCC) database was the data source. The data collected were\nanalyzed by two powerful software, viz., CiteSpace and VOSviewer, through which\nmapping visualizations of the intellectual landscape, trending issues and\n(re)emerging hotspots were generated. The results indicate that in the 1980s\nand 1990s, linguistics and AI (AIL) research was not robust, characterized by\nunstable publication over time. It has, however, witnessed a remarkable\nincrease of publication since then, reaching 1478 articles in 2023, and 546\narticles in January-March timespan in 2024, involving emerging issues including\nNatural language processing, Cross-sectional study, Using bidirectional encoder\nrepresentation, and Using ChatGPT and hotspots such as Novice programmer,\nPrioritization, and Artificial intelligence, addressing new horizons, new\ntopics, and launching new applications and powerful deep learning language\nmodels including ChatGPT. It concludes that linguistics and AI correlation is\nestablished at several levels, research centers, journals, and countries\nshaping AIL knowledge production and reshaping its future frontiers."}
{"id": "2501.08613", "pdf": "https://arxiv.org/pdf/2501.08613.pdf", "abs": "https://arxiv.org/abs/2501.08613", "title": "Assessing the Sensitivity and Alignment of FOL Closeness Metrics", "authors": ["Ramya Keerthy Thatikonda", "Wray Buntine", "Ehsan Shareghi"], "categories": ["cs.CL"], "comment": "EMNLP 2025", "summary": "The recent successful paradigm of solving logical reasoning problems with\ntool-augmented large language models (LLMs) leverages translation of natural\nlanguage (NL) statements into First-Order Logic~(FOL) and external theorem\nprovers. However, the correctness of FOL statements, comprising operators and\ntext, often go unverified due to the lack of a reliable evaluation metric for\ncomparing generated and ground-truth FOLs. In this paper, we conduct a\ncomprehensive study on the sensitivity of existing NL-, FOL-, and graph-based\nmetrics to capture differences between a sampled FOL and its corresponding\nground-truth. We then measure the alignment between a metric-based ranking of\nFOL outputs and a strong LLM as-a-judge. To do this, we first apply operator\nand text-based perturbations to ground-truth FOL statements to assess metric\nsensitivity. We then evaluate metric robustness by comparing the metrics\nagainst LLMs judgment. Our empirical findings highlight a clear oversensitivity\nin the n-gram metric BLEU for text perturbations. The operator perturbation\naffects the semantic graph metric Smatch++ for structural changes, and the FOL\nmetric for specific operator changes. We observe a closer alignment between\nBertScore and LLM judgement, proving the importance of semantic evaluation.\nAdditionally, we show that combining metrics enhances both robustness and\nsensitivity compared to using individual metrics."}
{"id": "2501.18724", "pdf": "https://arxiv.org/pdf/2501.18724.pdf", "abs": "https://arxiv.org/abs/2501.18724", "title": "Large Language Models with Temporal Reasoning for Longitudinal Clinical Summarization and Prediction", "authors": ["Maya Kruse", "Shiyue Hu", "Nicholas Derby", "Yifu Wu", "Samantha Stonbraker", "Bingsheng Yao", "Dakuo Wang", "Elizabeth Goldberg", "Yanjun Gao"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have shown potential in\nclinical text summarization, but their ability to handle long patient\ntrajectories with multi-modal data spread across time remains underexplored.\nThis study systematically evaluates several state-of-the-art open-source LLMs,\ntheir Retrieval Augmented Generation (RAG) variants and chain-of-thought (CoT)\nprompting on long-context clinical summarization and prediction. We examine\ntheir ability to synthesize structured and unstructured Electronic Health\nRecords (EHR) data while reasoning over temporal coherence, by re-engineering\nexisting tasks, including discharge summarization and diagnosis prediction from\ntwo publicly available EHR datasets. Our results indicate that long context\nwindows improve input integration but do not consistently enhance clinical\nreasoning, and LLMs are still struggling with temporal progression and rare\ndisease prediction. While RAG shows improvements in hallucination in some\ncases, it does not fully address these limitations. Our work fills the gap in\nlong clinical text summarization, establishing a foundation for evaluating LLMs\nwith multi-modal data and temporal reasoning."}
{"id": "2502.16487", "pdf": "https://arxiv.org/pdf/2502.16487.pdf", "abs": "https://arxiv.org/abs/2502.16487", "title": "All That Glitters is Not Novel: Plagiarism in AI Generated Research", "authors": ["Tarun Gupta", "Danish Pruthi"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (main) conference", "summary": "Automating scientific research is considered the final frontier of science.\nRecently, several papers claim autonomous research agents can generate novel\nresearch ideas. Amidst the prevailing optimism, we document a critical concern:\na considerable fraction of such research documents are smartly plagiarized.\nUnlike past efforts where experts evaluate the novelty and feasibility of\nresearch ideas, we request $13$ experts to operate under a different\nsituational logic: to identify similarities between LLM-generated research\ndocuments and existing work. Concerningly, the experts identify $24\\%$ of the\n$50$ evaluated research documents to be either paraphrased (with one-to-one\nmethodological mapping), or significantly borrowed from existing work. These\nreported instances are cross-verified by authors of the source papers. The\nremaining $76\\%$ of documents show varying degrees of similarity with existing\nwork, with only a small fraction appearing completely novel. Problematically,\nthese LLM-generated research documents do not acknowledge original sources, and\nbypass inbuilt plagiarism detectors. Lastly, through controlled experiments we\nshow that automated plagiarism detectors are inadequate at catching plagiarized\nideas from such systems. We recommend a careful assessment of LLM-generated\nresearch, and discuss the implications of our findings on academic publishing."}
{"id": "2503.21934", "pdf": "https://arxiv.org/pdf/2503.21934.pdf", "abs": "https://arxiv.org/abs/2503.21934", "title": "Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad", "authors": ["Ivo Petrov", "Jasper Dekoninck", "Lyuben Baltadzhiev", "Maria Drencheva", "Kristian Minchev", "Mislav Balunoviƒá", "Nikola Jovanoviƒá", "Martin Vechev"], "categories": ["cs.CL"], "comment": null, "summary": "Recent math benchmarks for large language models (LLMs) such as MathArena\nindicate that state-of-the-art reasoning models achieve impressive performance\non mathematical competitions like AIME, with the leading model, Gemini-2.5-Pro,\nachieving scores comparable to top human competitors. However, these benchmarks\nevaluate models solely based on final numerical answers, neglecting rigorous\nreasoning and proof generation which are essential for real-world mathematical\ntasks. To address this, we introduce a comprehensive evaluation of\nfull-solution reasoning for challenging mathematical problems. Using expert\nhuman annotators, we evaluated several state-of-the-art reasoning models on the\nsix problems from the 2025 USAMO within hours of their release. Our results\nreveal that all tested models struggled significantly: only Gemini-2.5-Pro\nachieves a non-trivial score of 25%, while all other models achieve less than\n5%. Through detailed analysis of reasoning traces, we identify the most common\nfailure modes and find several unwanted artifacts arising from the optimization\nstrategies employed during model training. Overall, our results suggest that\ncurrent LLMs are inadequate for rigorous mathematical reasoning tasks,\nhighlighting the need for substantial improvements in reasoning and proof\ngeneration capabilities."}
{"id": "2504.03352", "pdf": "https://arxiv.org/pdf/2504.03352.pdf", "abs": "https://arxiv.org/abs/2504.03352", "title": "StereoDetect: Detecting Stereotypes and Anti-stereotypes the Correct Way Using Social Psychological Underpinnings", "authors": ["Kaustubh Shivshankar Shejole", "Pushpak Bhattacharyya"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "Stereotypes are known to have very harmful effects, making their detection\ncritically important. However, current research predominantly focuses on\ndetecting and evaluating stereotypical biases, thereby leaving the study of\nstereotypes in its early stages. Our study revealed that many works have failed\nto clearly distinguish between stereotypes and stereotypical biases, which has\nsignificantly slowed progress in advancing research in this area. Stereotype\nand Anti-stereotype detection is a problem that requires social knowledge;\nhence, it is one of the most difficult areas in Responsible AI. This work\ninvestigates this task, where we propose a five-tuple definition and provide\nprecise terminologies disentangling stereotypes, anti-stereotypes,\nstereotypical bias, and general bias. We provide a conceptual framework\ngrounded in social psychology for reliable detection. We identify key\nshortcomings in existing benchmarks for this task of stereotype and\nanti-stereotype detection. To address these gaps, we developed StereoDetect, a\nwell curated, definition-aligned benchmark dataset designed for this task. We\nshow that sub-10B language models and GPT-4o frequently misclassify\nanti-stereotypes and fail to recognize neutral overgeneralizations. We\ndemonstrate StereoDetect's effectiveness through multiple qualitative and\nquantitative comparisons with existing benchmarks and models fine-tuned on\nthem. The dataset and code is available at\nhttps://github.com/KaustubhShejole/StereoDetect."}
{"id": "2504.06460", "pdf": "https://arxiv.org/pdf/2504.06460.pdf", "abs": "https://arxiv.org/abs/2504.06460", "title": "Can LLMs Simulate Personas with Reversed Performance? A Benchmark for Counterfactual Instruction Following", "authors": ["Sai Adith Senthil Kumar", "Hao Yan", "Saipavan Perepa", "Murong Yue", "Ziyu Yao"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are now increasingly widely used to simulate\npersonas in virtual environments, leveraging their instruction-following\ncapability. However, we discovered that even state-of-the-art LLMs cannot\nsimulate personas with reversed performance (e.g., student personas with low\nproficiency in educational settings), which impairs the simulation diversity\nand limits the practical applications of the simulated environments. In this\nwork, using mathematical reasoning as a representative scenario, we propose the\nfirst benchmark dataset for evaluating LLMs on simulating personas with\nreversed performance, a capability that we dub \"counterfactual instruction\nfollowing\". We evaluate both open-weight and closed-source LLMs on this task\nand find that LLMs, including the OpenAI o1 reasoning model, all struggle to\nfollow counterfactual instructions for simulating reversedly performing\npersonas. Intersectionally simulating both the performance level and the race\npopulation of a persona worsens the effect even further. These results\nhighlight the challenges of counterfactual instruction following and the need\nfor further research."}
{"id": "2504.12882", "pdf": "https://arxiv.org/pdf/2504.12882.pdf", "abs": "https://arxiv.org/abs/2504.12882", "title": "ViClaim: A Multilingual Multilabel Dataset for Automatic Claim Detection in Videos", "authors": ["Patrick Giedemann", "Pius von D√§niken", "Jan Deriu", "Alvaro Rodrigo", "Anselmo Pe√±as", "Mark Cieliebak"], "categories": ["cs.CL"], "comment": null, "summary": "The growing influence of video content as a medium for communication and\nmisinformation underscores the urgent need for effective tools to analyze\nclaims in multilingual and multi-topic settings. Existing efforts in\nmisinformation detection largely focus on written text, leaving a significant\ngap in addressing the complexity of spoken text in video transcripts. We\nintroduce ViClaim, a dataset of 1,798 annotated video transcripts across three\nlanguages (English, German, Spanish) and six topics. Each sentence in the\ntranscripts is labeled with three claim-related categories: fact-check-worthy,\nfact-non-check-worthy, or opinion. We developed a custom annotation tool to\nfacilitate the highly complex annotation process. Experiments with\nstate-of-the-art multilingual language models demonstrate strong performance in\ncross-validation (macro F1 up to 0.896) but reveal challenges in generalization\nto unseen topics, particularly for distinct domains. Our findings highlight the\ncomplexity of claim detection in video transcripts. ViClaim offers a robust\nfoundation for advancing misinformation detection in video-based communication,\naddressing a critical gap in multimodal analysis."}
{"id": "2505.17114", "pdf": "https://arxiv.org/pdf/2505.17114.pdf", "abs": "https://arxiv.org/abs/2505.17114", "title": "RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language", "authors": ["Subrata Biswas", "Mohammad Nur Hossain Khan", "Bashima Islam"], "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Multimodal question answering (QA) often requires identifying which video,\naudio, or sensor tokens are relevant to the question. Yet modality\ndisagreements are common: off-camera speech, background noise, or motion\noutside the field of view often mislead fusion models that weight all streams\nequally. We present RAVEN, a unified QA architecture whose core is QuART, a\nquery-conditioned cross-modal gating module that assigns scalar relevance\nscores to each token across modalities, enabling the model to amplify\ninformative signals and suppress distractors before fusion. RAVEN is trained\nthrough a three-stage pipeline comprising unimodal pretraining, query-aligned\nfusion, and disagreement-oriented fine-tuning -- each stage targeting a\ndistinct challenge in multi-modal reasoning: representation quality,\ncross-modal relevance, and robustness to modality mismatch. To support training\nand evaluation, we release AVS-QA, a dataset of 300K synchronized\nAudio--Video-Sensor streams paired with automatically generated question-answer\npairs. Experimental results on seven multi-modal QA benchmarks -- including\negocentric and exocentric tasks -- show that RAVEN achieves up to 14.5\\% and\n8.0\\% gains in accuracy compared to state-of-the-art multi-modal large language\nmodels, respectively. Incorporating sensor data provides an additional 16.4\\%\nboost, and the model remains robust under modality corruption, outperforming\nSOTA baselines by 50.23\\%. Our code and dataset are available at\nhttps://github.com/BASHLab/RAVEN."}
{"id": "2505.22809", "pdf": "https://arxiv.org/pdf/2505.22809.pdf", "abs": "https://arxiv.org/abs/2505.22809", "title": "First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons & Dragons Gameplay", "authors": ["Andrew Zhu", "Evan Osgood", "Chris Callison-Burch"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "9 pages, 5 figures. COLM 2025 Workshop on AI Agents", "summary": "Much work has been done on conversational LLM agents which directly assist\nhuman users with tasks. We present an alternative paradigm for interacting with\nLLM agents, which we call \"overhearing agents\". These overhearing agents do not\nactively participate in conversation -- instead, they \"listen in\" on\nhuman-to-human conversations and perform background tasks or provide\nsuggestions to assist the user. In this work, we explore the overhearing agents\nparadigm through the lens of Dungeons & Dragons gameplay. We present an\nin-depth study using large multimodal audio-language models as overhearing\nagents to assist a Dungeon Master. We perform a human evaluation to examine the\nhelpfulness of such agents and find that some large audio-language models have\nthe emergent ability to perform overhearing agent tasks using implicit audio\ncues. Finally, we release Python libraries and our project code to support\nfurther research into the overhearing agents paradigm at\nhttps://github.com/zhudotexe/overhearing_agents."}
{"id": "2506.21445", "pdf": "https://arxiv.org/pdf/2506.21445.pdf", "abs": "https://arxiv.org/abs/2506.21445", "title": "Text2Cypher Across Languages: Evaluating and Finetuning LLMs", "authors": ["Makbule Gulcin Ozsoy", "William Tai"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled natural language\ninterfaces that translate user questions into database queries, such as\nText2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database\naccessibility, most research today focuses on English, with limited evaluation\nin other languages. This paper investigates the performance of both\nfoundational and finetuned LLMs on the Text2Cypher task across multiple\nlanguages. We create and release a multilingual dataset by translating English\nquestions into Spanish and Turkish while preserving the original Cypher\nqueries, enabling fair cross-lingual comparison. Using standardized prompts and\nmetrics, we evaluate several foundational models and observe a consistent\nperformance pattern: highest on English, followed by Spanish, and lowest on\nTurkish. We attribute this to differences in training data availability and\nlinguistic features. We also examine the impact of translating task prompts\ninto Spanish and Turkish. Results show little to no change in evaluation\nmetrics, suggesting prompt translation has minor impact. Furthermore, we\nfinetune a foundational model on two datasets: one in English only, and one\nmultilingual. Finetuning on English improves overall accuracy but widens the\nperformance gap between languages. In contrast, multilingual finetuning narrows\nthe gap, resulting in more balanced performance. Our findings highlight the\nimportance for multilingual evaluation and training to build more inclusive and\nrobust query generation systems."}
{"id": "2507.14240", "pdf": "https://arxiv.org/pdf/2507.14240.pdf", "abs": "https://arxiv.org/abs/2507.14240", "title": "HuggingGraph: Understanding the Supply Chain of LLM Ecosystem", "authors": ["Mohammad Shahedur Rahman", "Peng Gao", "Yuede Ji"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) leverage deep learning architectures to process\nand predict sequences of words, enabling them to perform a wide range of\nnatural language processing tasks, such as translation, summarization, question\nanswering, and content generation. As existing LLMs are often built from base\nmodels or other pre-trained models and use external datasets, they can\ninevitably inherit vulnerabilities, biases, or malicious components that exist\nin previous models or datasets. Therefore, it is critical to understand these\ncomponents' origin and development process to detect potential risks, improve\nmodel fairness, and ensure compliance with regulatory frameworks. Motivated by\nthat, this project aims to study such relationships between models and\ndatasets, which are the central parts of the LLM supply chain. First, we design\na methodology to systematically collect LLMs' supply chain information. Then,\nwe design a new graph to model the relationships between models and datasets,\nwhich is a directed heterogeneous graph, having 402,654 nodes and 462,524\nedges. Lastly, we perform different types of analysis and make multiple\ninteresting findings."}
{"id": "2507.19081", "pdf": "https://arxiv.org/pdf/2507.19081.pdf", "abs": "https://arxiv.org/abs/2507.19081", "title": "Arg-LLaDA: Argument Summarization via Large Language Diffusion Models and Sufficiency-Aware Refinement", "authors": ["Hao Li", "Yizheng Sun", "Viktor Schlegel", "Kailai Yang", "Riza Batista-Navarro", "Goran Nenadic"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Argument summarization aims to generate concise, structured representations\nof complex, multi-perspective debates. While recent work has advanced the\nidentification and clustering of argumentative components, the generation stage\nremains underexplored. Existing approaches typically rely on single-pass\ngeneration, offering limited support for factual correction or structural\nrefinement. To address this gap, we introduce Arg-LLaDA, a novel large language\ndiffusion framework that iteratively improves summaries via sufficiency-guided\nremasking and regeneration. Our method combines a flexible masking controller\nwith a sufficiency-checking module to identify and revise unsupported,\nredundant, or incomplete spans, yielding more faithful, concise, and coherent\noutputs. Empirical results on two benchmark datasets demonstrate that Arg-LLaDA\nsurpasses state-of-the-art baselines in 7 out of 10 automatic evaluation\nmetrics. In addition, human evaluations reveal substantial improvements across\ncore dimensions, coverage, faithfulness, and conciseness, validating the\neffectiveness of our iterative, sufficiency-aware generation strategy."}
{"id": "2507.21509", "pdf": "https://arxiv.org/pdf/2507.21509.pdf", "abs": "https://arxiv.org/abs/2507.21509", "title": "Persona Vectors: Monitoring and Controlling Character Traits in Language Models", "authors": ["Runjin Chen", "Andy Arditi", "Henry Sleight", "Owain Evans", "Jack Lindsey"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models interact with users through a simulated 'Assistant'\npersona. While the Assistant is typically trained to be helpful, harmless, and\nhonest, it sometimes deviates from these ideals. In this paper, we identify\ndirections in the model's activation space-persona vectors-underlying several\ntraits, such as evil, sycophancy, and propensity to hallucinate. We confirm\nthat these vectors can be used to monitor fluctuations in the Assistant's\npersonality at deployment time. We then apply persona vectors to predict and\ncontrol personality shifts that occur during training. We find that both\nintended and unintended personality changes after finetuning are strongly\ncorrelated with shifts along the relevant persona vectors. These shifts can be\nmitigated through post-hoc intervention, or avoided in the first place with a\nnew preventative steering method. Moreover, persona vectors can be used to flag\ntraining data that will produce undesirable personality changes, both at the\ndataset level and the individual sample level. Our method for extracting\npersona vectors is automated and can be applied to any personality trait of\ninterest, given only a natural-language description."}
{"id": "2508.17450", "pdf": "https://arxiv.org/pdf/2508.17450.pdf", "abs": "https://arxiv.org/abs/2508.17450", "title": "Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability in Knowledge and Safety with DuET-PD", "authors": ["Bryan Chen Zhengyu Tan", "Daniel Wai Kit Chin", "Zhengyuan Liu", "Nancy F. Chen", "Roy Ka-Wei Lee"], "categories": ["cs.CL", "cs.CY"], "comment": "To appear at EMNLP 2025", "summary": "Large Language Models (LLMs) can struggle to balance gullibility to\nmisinformation and resistance to valid corrections in persuasive dialogues, a\ncritical challenge for reliable deployment. We introduce DuET-PD (Dual\nEvaluation for Trust in Persuasive Dialogues), a framework evaluating\nmulti-turn stance-change dynamics across dual dimensions: persuasion type\n(corrective/misleading) and domain (knowledge via MMLU-Pro, and safety via\nSALAD-Bench). We find that even a state-of-the-art model like GPT-4o achieves\nonly 27.32% accuracy in MMLU-Pro under sustained misleading persuasions.\nMoreover, results reveal a concerning trend of increasing sycophancy in newer\nopen-source models. To address this, we introduce Holistic DPO, a training\napproach balancing positive and negative persuasion examples. Unlike prompting\nor resist-only training, Holistic DPO enhances both robustness to\nmisinformation and receptiveness to corrections, improving\nLlama-3.1-8B-Instruct's accuracy under misleading persuasion in safety contexts\nfrom 4.21% to 76.54%. These contributions offer a pathway to developing more\nreliable and adaptable LLMs for multi-turn dialogue. Code is available at\nhttps://github.com/Social-AI-Studio/DuET-PD."}
{"id": "2508.20201", "pdf": "https://arxiv.org/pdf/2508.20201.pdf", "abs": "https://arxiv.org/abs/2508.20201", "title": "Social Bias in Multilingual Language Models: A Survey", "authors": ["Lance Calvin Lim Gamboa", "Yue Feng", "Mark Lee"], "categories": ["cs.CL"], "comment": "Accepted into EMNLP 2025 Main Conference", "summary": "Pretrained multilingual models exhibit the same social bias as models\nprocessing English texts. This systematic review analyzes emerging research\nthat extends bias evaluation and mitigation approaches into multilingual and\nnon-English contexts. We examine these studies with respect to linguistic\ndiversity, cultural awareness, and their choice of evaluation metrics and\nmitigation techniques. Our survey illuminates gaps in the field's dominant\nmethodological design choices (e.g., preference for certain languages, scarcity\nof multilingual mitigation experiments) while cataloging common issues\nencountered and solutions implemented in adapting bias benchmarks across\nlanguages and cultures. Drawing from the implications of our findings, we chart\ndirections for future research that can reinforce the multilingual bias\nliterature's inclusivity, cross-cultural appropriateness, and alignment with\nstate-of-the-art NLP advancements."}
{"id": "2509.00030", "pdf": "https://arxiv.org/pdf/2509.00030.pdf", "abs": "https://arxiv.org/abs/2509.00030", "title": "MultiStream-LLM: Bridging Modalities for Robust Sign Language Translation", "authors": ["Marshall Thomas", "Edward Fish", "Richard Bowden"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Despite progress in gloss-free Sign Language Translation (SLT), monolithic\nend-to-end models consistently fail on two critical components of natural\nsigning: the precise recognition of high-speed fingerspelling and the\nintegration of asynchronous non-manual cues from the face. Recent progress in\nAutomated Sign Language Translation with Large Language Models has side stepped\nthis challenge, forcing a single network to learn these simultaneously\nresulting in poor performance when tasked with translating crucial information\nsuch as names,places, and technical terms. We introduce MultiStream-LLM, a\nmodular framework designed to overcome these limitations. Our approach employs\nseparate, specialized predictors for continuous signing, fingerspelling, and\nlipreading. Each expert network first decodes its specific modality into a\nsequence of tokens. These parallel streams are then fused by a lightweight\ntransformer that resolves temporal misalignments before passing the combined\nrepresentation to a Large Language Model (LLM) for final sentence generation.\nOur method establishes a new state-of-the-art on the How2Sign benchmark with a\nBLEU-4 score of 23.5 and achieves 73.2% letter accuracy on the challenging\nChicagoFSWildPlus fingerspelling dataset. These results validate our core\nhypothesis: by isolating and solving distinct recogni tion tasks before fusion,\nour multi-expert approach provides a more powerful and effective pathway to\nrobust, high-fidelity sign language translation."}
{"id": "2509.00461", "pdf": "https://arxiv.org/pdf/2509.00461.pdf", "abs": "https://arxiv.org/abs/2509.00461", "title": "TECP: Token-Entropy Conformal Prediction for LLMs", "authors": ["Beining Xu", "Yongming Lu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Uncertainty quantification (UQ) for open-ended language generation remains a\ncritical yet underexplored challenge, especially under black-box constraints\nwhere internal model signals are inaccessible. In this paper, we introduce\nToken-Entropy Conformal Prediction (TECP), a novel framework that leverages\ntoken-level entropy as a logit-free, reference-free uncertainty measure and\nintegrates it into a split conformal prediction (CP) pipeline to construct\nprediction sets with formal coverage guarantees. Unlike existing approaches\nthat rely on semantic consistency heuristics or white-box features, TECP\ndirectly estimates epistemic uncertainty from the token entropy structure of\nsampled generations and calibrates uncertainty thresholds via CP quantiles to\nensure provable error control. Empirical evaluations across six large language\nmodels and two benchmarks (CoQA and TriviaQA) demonstrate that TECP\nconsistently achieves reliable coverage and compact prediction sets,\noutperforming prior self-consistency-based UQ methods. Our method provides a\nprincipled and efficient solution for trustworthy generation in black-box LLM\nsettings."}
{"id": "2509.04111", "pdf": "https://arxiv.org/pdf/2509.04111.pdf", "abs": "https://arxiv.org/abs/2509.04111", "title": "MultiWikiQA: A Reading Comprehension Benchmark in 300+ Languages", "authors": ["Dan Saattrup Smart"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce a new reading comprehension dataset, dubbed MultiWikiQA, which\ncovers 306 languages. The context data comes from Wikipedia articles, with\nquestions generated by an LLM and the answers appearing verbatim in the\nWikipedia articles. We conduct a crowdsourced human evaluation of the fluency\nof the generated questions across 30 of the languages, providing evidence that\nthe questions are of good quality. We evaluate 6 different language models,\nboth decoder and encoder models of varying sizes, showing that the benchmark is\nsufficiently difficult and that there is a large performance discrepancy\namongst the languages. The dataset and survey evaluations are freely available."}
{"id": "2410.18122", "pdf": "https://arxiv.org/pdf/2410.18122.pdf", "abs": "https://arxiv.org/abs/2410.18122", "title": "Yesterday's News: Benchmarking Multi-Dimensional Out-of-Distribution Generalization of Misinformation Detection Models", "authors": ["Ivo Verhoeven", "Pushkar Mishra", "Ekaterina Shutova"], "categories": ["cs.IR", "cs.CL"], "comment": "Under review", "summary": "This article introduces misinfo-general, a benchmark dataset for evaluating\nmisinformation models' ability to perform out-of-distribution generalization.\nMisinformation changes rapidly, much more quickly than moderators can annotate\nat scale, resulting in a shift between the training and inference data\ndistributions. As a result, misinformation detectors need to be able to perform\nout-of-distribution generalization, an attribute they currently lack. Our\nbenchmark uses distant labelling to enable simulating covariate shifts in\nmisinformation content. We identify time, event, topic, publisher, political\nbias, misinformation type as important axes for generalization, and we evaluate\na common class of baseline models on each. Using article metadata, we show how\nthis model fails desiderata, which is not necessarily obvious from\nclassification metrics. Finally, we analyze properties of the data to ensure\nlimited presence of modelling shortcuts. We make the dataset and accompanying\ncode publicly available: https://github.com/ioverho/misinfo-general"}
{"id": "2505.11737", "pdf": "https://arxiv.org/pdf/2505.11737.pdf", "abs": "https://arxiv.org/abs/2505.11737", "title": "TokUR: Token-Level Uncertainty Estimation for Large Language Model Reasoning", "authors": ["Tunyu Zhang", "Haizhou Shi", "Yibin Wang", "Hengyi Wang", "Xiaoxiao He", "Zhuowei Li", "Haoxian Chen", "Ligong Han", "Kai Xu", "Huan Zhang", "Dimitris Metaxas", "Hao Wang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint; Work in progress", "summary": "While Large Language Models (LLMs) have demonstrated impressive capabilities,\ntheir output quality remains inconsistent across various application scenarios,\nmaking it difficult to identify trustworthy responses, especially in complex\ntasks requiring multi-step reasoning. In this paper, we propose a Token-level\nUncertainty estimation framework for Reasoning (TokUR) to enable LLMs to\nself-assess and self-improve their generation quality in mathematical\nreasoning. Specifically, we introduce low-rank random weight perturbation to\nLLM decoding, generating predictive distributions that we use to estimate\ntoken-level uncertainties. We then aggregate these uncertainties to reflect\nsemantic uncertainty of the generated sequences. Experiments on mathematical\nreasoning datasets of varying difficulty demonstrate that our token-level\nuncertainty metrics strongly correlate with answer correctness and model\nrobustness. Additionally, we explore using uncertainty to directly enhance the\nmodel's reasoning performance through multiple generations and the particle\nfiltering algorithm. Our approach consistently outperforms existing uncertainty\nestimation methods, establishing effective uncertainty estimation as a valuable\ntool for both evaluating and improving reasoning generation in LLMs."}
{"id": "2507.05528", "pdf": "https://arxiv.org/pdf/2507.05528.pdf", "abs": "https://arxiv.org/abs/2507.05528", "title": "Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment", "authors": ["Jiahuan Pei", "Fanghua Ye", "Xin Sun", "Wentao Deng", "Koen Hindriks", "Junxiao Wang"], "categories": ["cs.AI", "cs.CL"], "comment": "14 pages, accepted by EMNLP 2025", "summary": "Large language models (LLMs) have advanced virtual educators and learners,\nbridging NLP with AI4Education. Existing work often lacks scalability and fails\nto leverage diverse, large-scale course content, with limited frameworks for\nassessing pedagogic quality. To this end, we propose WikiHowAgent, a\nmulti-agent workflow leveraging LLMs to simulate interactive teaching-learning\nconversations. It integrates teacher and learner agents, an interaction\nmanager, and an evaluator to facilitate procedural learning and assess\npedagogic quality. We introduce a dataset of 114,296 teacher-learner\nconversations grounded in 14,287 tutorials across 17 domains and 727 topics.\nOur evaluation protocol combines computational and rubric-based metrics with\nhuman judgment alignment. Results demonstrate the workflow's effectiveness in\ndiverse setups, offering insights into LLM capabilities across domains. Our\ndatasets and implementations are fully open-sourced."}
{"id": "2507.07236", "pdf": "https://arxiv.org/pdf/2507.07236.pdf", "abs": "https://arxiv.org/abs/2507.07236", "title": "Simple Yet Effective: An Information-Theoretic Approach to Multi-LLM Uncertainty Quantification", "authors": ["Maya Kruse", "Majid Afshar", "Saksham Khatwani", "Anoop Mayampurath", "Guanhua Chen", "Yanjun Gao"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Large language models (LLMs) often behave inconsistently across inputs,\nindicating uncertainty and motivating the need for its quantification in\nhigh-stakes settings. Prior work on calibration and uncertainty quantification\noften focuses on individual models, overlooking the potential of model\ndiversity. We hypothesize that LLMs make complementary predictions due to\ndifferences in training and the Zipfian nature of language, and that\naggregating their outputs leads to more reliable uncertainty estimates. To\nleverage this, we propose MUSE (Multi-LLM Uncertainty via Subset Ensembles), a\nsimple information-theoretic method that uses Jensen-Shannon Divergence to\nidentify and aggregate well-calibrated subsets of LLMs. Experiments on binary\nprediction tasks demonstrate improved calibration and predictive performance\ncompared to single-model and na\\\"ive ensemble baselines. In addition, we\nexplore using MUSE as guided signals with chain-of-thought distillation to\nfine-tune LLMs for calibration. MUSE is available\nat:https://github.com/LARK-NLP-Lab/MUSE."}
{"id": "2507.20474", "pdf": "https://arxiv.org/pdf/2507.20474.pdf", "abs": "https://arxiv.org/abs/2507.20474", "title": "MountainLion: A Multi-Modal LLM-Based Agent System for Interpretable and Adaptive Financial Trading", "authors": ["Siyi Wu", "Junqiao Wang", "Zhaoyang Guan", "Leyi Zhao", "Xinyuan Song", "Xinyu Ying", "Dexu Yu", "Jinhao Wang", "Hanlin Zhang", "Michele Pak", "Yangfan He", "Yi Xin", "Jianhui Wang", "Tianyu Shi"], "categories": ["q-fin.TR", "cs.CL", "cs.LG"], "comment": null, "summary": "Cryptocurrency trading is a challenging task requiring the integration of\nheterogeneous data from multiple modalities. Traditional deep learning and\nreinforcement learning approaches typically demand large training datasets and\nencode diverse inputs into numerical representations, often at the cost of\ninterpretability. Recent progress in large language model (LLM)-based agents\nhas demonstrated the capacity to process multi-modal data and support complex\ninvestment decision-making. Building on these advances, we present\n\\textbf{MountainLion}, a multi-modal, multi-agent system for financial trading\nthat coordinates specialized LLM-based agents to interpret financial data and\ngenerate investment strategies. MountainLion processes textual news,\ncandlestick charts, and trading signal charts to produce high-quality financial\nreports, while also enabling modification of reports and investment\nrecommendations through data-driven user interaction and question answering. A\ncentral reflection module analyzes historical trading signals and outcomes to\ncontinuously refine decision processes, and the system is capable of real-time\nreport analysis, summarization, and dynamic adjustment of investment\nstrategies. Empirical results confirm that MountainLion systematically enriches\ntechnical price triggers with contextual macroeconomic and capital flow\nsignals, providing a more interpretable, robust, and actionable investment\nframework that improves returns and strengthens investor confidence."}
{"id": "2508.01249", "pdf": "https://arxiv.org/pdf/2508.01249.pdf", "abs": "https://arxiv.org/abs/2508.01249", "title": "AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection", "authors": ["Peiran Wang", "Yang Liu", "Yunfei Lu", "Yifeng Cai", "Hongbo Chen", "Qingyou Yang", "Jie Zhang", "Jue Hong", "Ye Wu"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "Large Language Model (LLM) agents offer a powerful new paradigm for solving\nvarious problems by combining natural language reasoning with the execution of\nexternal tools. However, their dynamic and non-transparent behavior introduces\ncritical security risks, particularly in the presence of prompt injection\nattacks. In this work, we propose a novel insight that treats the agent runtime\ntraces as structured programs with analyzable semantics. Thus, we present\nAgentArmor, a program analysis framework that converts agent traces into graph\nintermediate representation-based structured program dependency representations\n(e.g., CFG, DFG, and PDG) and enforces security policies via a type system.\nAgentArmor consists of three key components: (1) a graph constructor that\nreconstructs the agent's runtime traces as graph-based intermediate\nrepresentations with control and data flow described within; (2) a property\nregistry that attaches security-relevant metadata of interacted tools \\& data,\nand (3) a type system that performs static inference and checking over the\nintermediate representation. By representing agent behavior as structured\nprograms, AgentArmor enables program analysis for sensitive data flow, trust\nboundaries, and policy violations. We evaluate AgentArmor on the AgentDojo\nbenchmark, the results show that AgentArmor can reduce the ASR to 3\\%, with the\nutility drop only 1\\%."}
{"id": "2508.20312", "pdf": "https://arxiv.org/pdf/2508.20312.pdf", "abs": "https://arxiv.org/abs/2508.20312", "title": "ELIXIR: Efficient and LIghtweight model for eXplaIning Recommendations", "authors": ["Ben Kabongo", "Vincent Guigue", "Pirmin Lemberger"], "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": "10 pages, 3 figures, 6 Tables", "summary": "Collaborative filtering drives many successful recommender systems but\nstruggles with fine-grained user-item interactions and explainability. As users\nincreasingly seek transparent recommendations, generating textual explanations\nthrough language models has become a critical research area. Existing methods\nemploy either RNNs or Transformers. However, RNN-based approaches fail to\nleverage the capabilities of pre-trained Transformer models, whereas\nTransformer-based methods often suffer from suboptimal adaptation and neglect\naspect modeling, which is crucial for personalized explanations. We propose\nELIXIR (Efficient and LIghtweight model for eXplaIning Recommendations), a\nmulti-task model combining rating prediction with personalized review\ngeneration. ELIXIR jointly learns global and aspect-specific representations of\nusers and items, optimizing overall rating, aspect-level ratings, and review\ngeneration, with personalized attention to emphasize aspect importance. Based\non a T5-small (60M) model, we demonstrate the effectiveness of our aspect-based\narchitecture in guiding text generation in a personalized context, where\nstate-of-the-art approaches exploit much larger models but fail to match user\npreferences as well. Experimental results on TripAdvisor and RateBeer\ndemonstrate that ELIXIR significantly outperforms strong baseline models,\nespecially in review generation."}
{"id": "2509.02544", "pdf": "https://arxiv.org/pdf/2509.02544.pdf", "abs": "https://arxiv.org/abs/2509.02544", "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning", "authors": ["Haoming Wang", "Haoyang Zou", "Huatong Song", "Jiazhan Feng", "Junjie Fang", "Junting Lu", "Longxiang Liu", "Qinyu Luo", "Shihao Liang", "Shijue Huang", "Wanjun Zhong", "Yining Ye", "Yujia Qin", "Yuwen Xiong", "Yuxin Song", "Zhiyong Wu", "Aoyan Li", "Bo Li", "Chen Dun", "Chong Liu", "Daoguang Zan", "Fuxing Leng", "Hanbin Wang", "Hao Yu", "Haobin Chen", "Hongyi Guo", "Jing Su", "Jingjia Huang", "Kai Shen", "Kaiyu Shi", "Lin Yan", "Peiyao Zhao", "Pengfei Liu", "Qinghao Ye", "Renjie Zheng", "Shulin Xin", "Wayne Xin Zhao", "Wen Heng", "Wenhao Huang", "Wenqian Wang", "Xiaobo Qin", "Yi Lin", "Youbin Wu", "Zehui Chen", "Zihao Wang", "Baoquan Zhong", "Xinchun Zhang", "Xujing Li", "Yuanfan Li", "Zhongkai Zhao", "Chengquan Jiang", "Faming Wu", "Haotian Zhou", "Jinlin Pang", "Li Han", "Qi Liu", "Qianli Ma", "Siyao Liu", "Songhua Cai", "Wenqi Fu", "Xin Liu", "Yaohui Wang", "Zhi Zhang", "Bo Zhou", "Guoliang Li", "Jiajun Shi", "Jiale Yang", "Jie Tang", "Li Li", "Qihua Han", "Taoran Lu", "Woyu Lin", "Xiaokang Tong", "Xinyao Li", "Yichi Zhang", "Yu Miao", "Zhengxuan Jiang", "Zili Li", "Ziyuan Zhao", "Chenxin Li", "Dehua Ma", "Feng Lin", "Ge Zhang", "Haihua Yang", "Hangyu Guo", "Hongda Zhu", "Jiaheng Liu", "Junda Du", "Kai Cai", "Kuanye Li", "Lichen Yuan", "Meilan Han", "Minchao Wang", "Shuyue Guo", "Tianhao Cheng", "Xiaobo Ma", "Xiaojun Xiao", "Xiaolong Huang", "Xinjie Chen", "Yidi Du", "Yilin Chen", "Yiwen Wang", "Zhaojian Li", "Zhenzhu Yang", "Zhiyuan Zeng", "Chaolin Jin", "Chen Li", "Hao Chen", "Haoli Chen", "Jian Chen", "Qinghao Zhao", "Guang Shi"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": null, "summary": "The development of autonomous agents for graphical user interfaces (GUIs)\npresents major challenges in artificial intelligence. While recent advances in\nnative agent models have shown promise by unifying perception, reasoning,\naction, and memory through end-to-end learning, open problems remain in data\nscalability, multi-turn reinforcement learning (RL), the limitations of\nGUI-only operation, and environment stability. In this technical report, we\npresent UI-TARS-2, a native GUI-centered agent model that addresses these\nchallenges through a systematic training methodology: a data flywheel for\nscalable data generation, a stabilized multi-turn RL framework, a hybrid GUI\nenvironment that integrates file systems and terminals, and a unified sandbox\nplatform for large-scale rollouts. Empirical evaluation demonstrates that\nUI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5.\nOn GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on\nWindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines\nsuch as Claude and OpenAI agents. In game environments, it attains a mean\nnormalized score of 59.8 across a 15-game suite-roughly 60% of human-level\nperformance-and remains competitive with frontier proprietary models (e.g.,\nOpenAI o3) on LMGame-Bench. Additionally, the model can generalize to\nlong-horizon information-seeking tasks and software engineering benchmarks,\nhighlighting its robustness across diverse agent tasks. Detailed analyses of\ntraining dynamics further provide insights into achieving stability and\nefficiency in large-scale agent RL. These results underscore UI-TARS-2's\npotential to advance the state of GUI agents and exhibit strong generalization\nto real-world interactive scenarios."}
{"id": "2509.03730", "pdf": "https://arxiv.org/pdf/2509.03730.pdf", "abs": "https://arxiv.org/abs/2509.03730", "title": "The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs", "authors": ["Pengrui Han", "Rafal Kocielnik", "Peiyang Song", "Ramit Debnath", "Dean Mobbs", "Anima Anandkumar", "R. Michael Alvarez"], "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG", "stat.ML"], "comment": "We make public all code and source data at\n  https://github.com/psychology-of-AI/Personality-Illusion for full\n  reproducibility", "summary": "Personality traits have long been studied as predictors of human behavior.\nRecent advances in Large Language Models (LLMs) suggest similar patterns may\nemerge in artificial systems, with advanced LLMs displaying consistent\nbehavioral tendencies resembling human traits like agreeableness and\nself-regulation. Understanding these patterns is crucial, yet prior work\nprimarily relied on simplified self-reports and heuristic prompting, with\nlittle behavioral validation. In this study, we systematically characterize LLM\npersonality across three dimensions: (1) the dynamic emergence and evolution of\ntrait profiles throughout training stages; (2) the predictive validity of\nself-reported traits in behavioral tasks; and (3) the impact of targeted\ninterventions, such as persona injection, on both self-reports and behavior.\nOur findings reveal that instructional alignment (e.g., RLHF, instruction\ntuning) significantly stabilizes trait expression and strengthens trait\ncorrelations in ways that mirror human data. However, these self-reported\ntraits do not reliably predict behavior, and observed associations often\ndiverge from human patterns. While persona injection successfully steers\nself-reports in the intended direction, it exerts little or inconsistent effect\non actual behavior. By distinguishing surface-level trait expression from\nbehavioral consistency, our findings challenge assumptions about LLM\npersonality and underscore the need for deeper evaluation in alignment and\ninterpretability."}
