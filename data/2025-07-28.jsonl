{"id": "2507.18637", "pdf": "https://arxiv.org/pdf/2507.18637.pdf", "abs": "https://arxiv.org/abs/2507.18637", "title": "More Expert-like Eye Gaze Movement Patterns are Related to Better X-ray Reading", "authors": ["Pingjing Yang", "Jennifer Cromley", "Jana Diesner"], "categories": ["cs.HC", "cs.AI"], "comment": "This work will appear at the 26th International Conference on\n  Artificial Intelligence in Education (AIED 2025)", "summary": "Understanding how novices acquire and hone visual search skills is crucial\nfor developing and optimizing training methods across domains. Network analysis\nmethods can be used to analyze graph representations of visual expertise. This\nstudy investigates the relationship between eye-gaze movements and learning\noutcomes among undergraduate dentistry students who were diagnosing dental\nradiographs over multiple semesters. We use network analysis techniques to\nmodel eye-gaze scanpaths as directed graphs and examine changes in network\nmetrics over time. Using time series clustering on each metric, we identify\ndistinct patterns of visual search strategies and explore their association\nwith students' diagnostic performance. Our findings suggest that the network\nmetric of transition entropy is negatively correlated with performance scores,\nwhile the number of nodes and edges as well as average PageRank are positively\ncorrelated with performance scores. Changes in network metrics for individual\nstudents over time suggest a developmental shift from intermediate to\nexpert-level processing. These insights contribute to understanding expertise\nacquisition in visual tasks and can inform the design of AI-assisted learning\ninterventions."}
{"id": "2507.18638", "pdf": "https://arxiv.org/pdf/2507.18638.pdf", "abs": "https://arxiv.org/abs/2507.18638", "title": "Prompt Engineering and the Effectiveness of Large Language Models in Enhancing Human Productivity", "authors": ["Rizal Khoirul Anam"], "categories": ["cs.HC", "cs.AI", "68T50 68T50 68T50", "I.2.7"], "comment": "38 pages, 15 tables, 5 figures. Submitted as a research paper draft\n  for arXiv. Based on survey data collected in 2025", "summary": "The widespread adoption of large language models (LLMs) such as ChatGPT,\nGemini, and DeepSeek has significantly changed how people approach tasks in\neducation, professional work, and creative domains. This paper investigates how\nthe structure and clarity of user prompts impact the effectiveness and\nproductivity of LLM outputs. Using data from 243 survey respondents across\nvarious academic and occupational backgrounds, we analyze AI usage habits,\nprompting strategies, and user satisfaction. The results show that users who\nemploy clear, structured, and context-aware prompts report higher task\nefficiency and better outcomes. These findings emphasize the essential role of\nprompt engineering in maximizing the value of generative AI and provide\npractical implications for its everyday use."}
{"id": "2507.18639", "pdf": "https://arxiv.org/pdf/2507.18639.pdf", "abs": "https://arxiv.org/abs/2507.18639", "title": "People Are Highly Cooperative with Large Language Models, Especially When Communication Is Possible or Following Human Interaction", "authors": ["Paweł Niszczota", "Tomasz Grzegorczyk", "Alexander Pastukhov"], "categories": ["cs.HC", "cs.CL", "cs.CY", "econ.GN", "q-fin.EC", "I.2.7; H.5.2; H.5.3; K.4.3"], "comment": null, "summary": "Machines driven by large language models (LLMs) have the potential to augment\nhumans across various tasks, a development with profound implications for\nbusiness settings where effective communication, collaboration, and stakeholder\ntrust are paramount. To explore how interacting with an LLM instead of a human\nmight shift cooperative behavior in such settings, we used the Prisoner's\nDilemma game -- a surrogate of several real-world managerial and economic\nscenarios. In Experiment 1 (N=100), participants engaged in a thirty-round\nrepeated game against a human, a classic bot, and an LLM (GPT, in real-time).\nIn Experiment 2 (N=192), participants played a one-shot game against a human or\nan LLM, with half of them allowed to communicate with their opponent, enabling\nLLMs to leverage a key advantage over older-generation machines. Cooperation\nrates with LLMs -- while lower by approximately 10-15 percentage points\ncompared to interactions with human opponents -- were nonetheless high. This\nfinding was particularly notable in Experiment 2, where the psychological cost\nof selfish behavior was reduced. Although allowing communication about\ncooperation did not close the human-machine behavioral gap, it increased the\nlikelihood of cooperation with both humans and LLMs equally (by 88%), which is\nparticularly surprising for LLMs given their non-human nature and the\nassumption that people might be less receptive to cooperating with machines\ncompared to human counterparts. Additionally, cooperation with LLMs was higher\nfollowing prior interaction with humans, suggesting a spillover effect in\ncooperative behavior. Our findings validate the (careful) use of LLMs by\nbusinesses in settings that have a cooperative component."}
{"id": "2507.18640", "pdf": "https://arxiv.org/pdf/2507.18640.pdf", "abs": "https://arxiv.org/abs/2507.18640", "title": "How good are humans at detecting AI-generated images? Learnings from an experiment", "authors": ["Thomas Roca", "Anthony Cintron Roman", "Jehú Torres Vega", "Marcelo Duarte", "Pengce Wang", "Kevin White", "Amit Misra", "Juan Lavista Ferres"], "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": null, "summary": "As AI-powered image generation improves, a key question is how well human\nbeings can differentiate between \"real\" and AI-generated or modified images.\nUsing data collected from the online game \"Real or Not Quiz.\", this study\ninvestigates how effectively people can distinguish AI-generated images from\nreal ones. Participants viewed a randomized set of real and AI-generated\nimages, aiming to identify their authenticity. Analysis of approximately\n287,000 image evaluations by over 12,500 global participants revealed an\noverall success rate of only 62\\%, indicating a modest ability, slightly above\nchance. Participants were most accurate with human portraits but struggled\nsignificantly with natural and urban landscapes. These results highlight the\ninherent challenge humans face in distinguishing AI-generated visual content,\nparticularly images without obvious artifacts or stylistic cues. This study\nstresses the need for transparency tools, such as watermarks and robust AI\ndetection tools to mitigate the risks of misinformation arising from\nAI-generated content"}
{"id": "2507.18742", "pdf": "https://arxiv.org/pdf/2507.18742.pdf", "abs": "https://arxiv.org/abs/2507.18742", "title": "Specification Self-Correction: Mitigating In-Context Reward Hacking Through Test-Time Refinement", "authors": ["Víctor Gallego"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to SCALR Workshop @ COLM 2025", "summary": "Language models (LMs) are susceptible to in-context reward hacking, where\nthey exploit flaws in tainted or faulty written specifications or rubrics to\nachieve high scores without fulfilling the user's true intent. We introduce\nSpecification Self-Correction (SSC), a novel, test-time framework that enables\nan LM to identify and correct flaws within its own guiding specification. SSC\nemploys a multi-step inference process where the model first generates a\nresponse based on a potentially tainted specification, critiques its output,\nand then revises the specification itself to remove the exploitable loophole. A\nfinal, more robust response is then generated using this self-corrected\nspecification. Across experiments spanning creative writing and agentic coding\ntasks with several LMs, we demonstrate that while models initially game tainted\nspecifications in 50-70\\% of cases, the SSC process reduces this vulnerability\nby over 90\\%. This dynamic repair occurs at inference time, requires no weight\nmodification, and leads to more robustly aligned model behavior. Code at\nhttps://github.com/vicgalle/specification-self-correction ."}
{"id": "2507.18641", "pdf": "https://arxiv.org/pdf/2507.18641.pdf", "abs": "https://arxiv.org/abs/2507.18641", "title": "Comparing Human and AI Performance in Visual Storytelling through Creation of Comic Strips: A Case Study", "authors": ["Uğur Önal", "Sanem Sariel", "Metin Sezgin", "Ergun Akleman"], "categories": ["cs.HC", "cs.CY"], "comment": "This paper is accepted to be presented in Digital Humanities\n  Conference 2025, and it will also appear in their proceedings", "summary": "This article presents a case study comparing the capabilities of humans and\nartificial intelligence (AI) for visual storytelling. We developed detailed\ninstructions to recreate a three-panel Nancy cartoon strip by Ernie Bushmiller\nand provided them to both humans and AI systems. The human participants were\n20-something students with basic artistic training but no experience or\nknowledge of this comic strip. The AI systems used were popular commercial\nmodels trained to draw and paint like artists, though their training sets may\nnot necessarily include Bushmiller's work. Results showed that AI systems excel\nat mimicking professional art but struggle to create coherent visual stories.\nIn contrast, humans proved highly adept at transforming instructions into\nmeaningful visual narratives."}
{"id": "2507.18762", "pdf": "https://arxiv.org/pdf/2507.18762.pdf", "abs": "https://arxiv.org/abs/2507.18762", "title": "The Role of Orthographic Consistency in Multilingual Embedding Models for Text Classification in Arabic-Script Languages", "authors": ["Abdulhady Abas Abdullah", "Amir H. Gandomi", "Tarik A Rashid", "Seyedali Mirjalili", "Laith Abualigah", "Milena Živković", "Hadi Veisi"], "categories": ["cs.CL"], "comment": null, "summary": "In natural language processing, multilingual models like mBERT and\nXLM-RoBERTa promise broad coverage but often struggle with languages that share\na script yet differ in orthographic norms and cultural context. This issue is\nespecially notable in Arabic-script languages such as Kurdish Sorani, Arabic,\nPersian, and Urdu. We introduce the Arabic Script RoBERTa (AS-RoBERTa) family:\nfour RoBERTa-based models, each pre-trained on a large corpus tailored to its\nspecific language. By focusing pre-training on language-specific script\nfeatures and statistics, our models capture patterns overlooked by\ngeneral-purpose models. When fine-tuned on classification tasks, AS-RoBERTa\nvariants outperform mBERT and XLM-RoBERTa by 2 to 5 percentage points. An\nablation study confirms that script-focused pre-training is central to these\ngains. Error analysis using confusion matrices shows how shared script traits\nand domain-specific content affect performance. Our results highlight the value\nof script-aware specialization for languages using the Arabic script and\nsupport further work on pre-training strategies rooted in script and language\nspecificity."}
{"id": "2507.18802", "pdf": "https://arxiv.org/pdf/2507.18802.pdf", "abs": "https://arxiv.org/abs/2507.18802", "title": "DxHF: Providing High-Quality Human Feedback for LLM Alignment via Interactive Decomposition", "authors": ["Danqing Shi", "Furui Cheng", "Tino Weinkauf", "Antti Oulasvirta", "Mennatallah El-Assady"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Human preferences are widely used to align large language models (LLMs)\nthrough methods such as reinforcement learning from human feedback (RLHF).\nHowever, the current user interfaces require annotators to compare text\nparagraphs, which is cognitively challenging when the texts are long or\nunfamiliar. This paper contributes by studying the decomposition principle as\nan approach to improving the quality of human feedback for LLM alignment. This\napproach breaks down the text into individual claims instead of directly\ncomparing two long-form text responses. Based on the principle, we build a\nnovel user interface DxHF. It enhances the comparison process by showing\ndecomposed claims, visually encoding the relevance of claims to the\nconversation and linking similar claims. This allows users to skim through key\ninformation and identify differences for better and quicker judgment. Our\ntechnical evaluation shows evidence that decomposition generally improves\nfeedback accuracy regarding the ground truth, particularly for users with\nuncertainty. A crowdsourcing study with 160 participants indicates that using\nDxHF improves feedback accuracy by an average of 5%, although it increases the\naverage feedback time by 18 seconds. Notably, accuracy is significantly higher\nin situations where users have less certainty. The finding of the study\nhighlights the potential of HCI as an effective method for improving human-AI\nalignment."}
{"id": "2507.18769", "pdf": "https://arxiv.org/pdf/2507.18769.pdf", "abs": "https://arxiv.org/abs/2507.18769", "title": "ylmmcl at Multilingual Text Detoxification 2025: Lexicon-Guided Detoxification and Classifier-Gated Rewriting", "authors": ["Nicole Lai-Lopez", "Lusha Wang", "Su Yuan", "Liza Zhang"], "categories": ["cs.CL", "cs.LG"], "comment": "16 pages, 5 figures, 3 tables,", "summary": "In this work, we introduce our solution for the Multilingual Text\nDetoxification Task in the PAN-2025 competition for the ylmmcl team: a robust\nmultilingual text detoxification pipeline that integrates lexicon-guided\ntagging, a fine-tuned sequence-to-sequence model (s-nlp/mt0-xl-detox-orpo) and\nan iterative classifier-based gatekeeping mechanism. Our approach departs from\nprior unsupervised or monolingual pipelines by leveraging explicit toxic word\nannotation via the multilingual_toxic_lexicon to guide detoxification with\ngreater precision and cross-lingual generalization. Our final model achieves\nthe highest STA (0.922) from our previous attempts, and an average official J\nscore of 0.612 for toxic inputs in both the development and test sets. It also\nachieved xCOMET scores of 0.793 (dev) and 0.787 (test). This performance\noutperforms baseline and backtranslation methods across multiple languages, and\nshows strong generalization in high-resource settings (English, Russian,\nFrench). Despite some trade-offs in SIM, the model demonstrates consistent\nimprovements in detoxification strength. In the competition, our team achieved\nninth place with a score of 0.612."}
{"id": "2507.18828", "pdf": "https://arxiv.org/pdf/2507.18828.pdf", "abs": "https://arxiv.org/abs/2507.18828", "title": "Ethical Considerations for Observational Research in Social VR", "authors": ["Victoria Chang", "Caro Williams-Pierce", "Huaishu Peng", "Ge Gao"], "categories": ["cs.HC"], "comment": "CSCW Companion '25, October 18-22, 2025, Bergen, Norway", "summary": "Social VR introduces new ethical challenges for observational research. The\ncurrent paper presents a narrative literature review of ethical considerations\nin observational methods, with a focus on work in HCI. We examine how\nunobtrusive or selectively disclosed observation is implemented in public\nface-to-face and social VR settings. Our review extends ethical discussions\nfrom traditional public research into the context of social VR, highlighting\ntensions between observer visibility, data traceability, and participant\nautonomy. Drawing on insights distilled from prior literature, we propose five\nconstructive guidelines for ethical observational research in public social VR\nenvironments. Our work offers key implications for future research, addressing\nanticipated improvements in platform design, the management of researcher\npresence, and the development of community-informed consent mechanisms."}
{"id": "2507.18791", "pdf": "https://arxiv.org/pdf/2507.18791.pdf", "abs": "https://arxiv.org/abs/2507.18791", "title": "Evaluating Code-Mixing in LLMs Across 18 Languages", "authors": ["Yilun Yang", "Yekun Chai"], "categories": ["cs.CL"], "comment": null, "summary": "Code-mixing, the practice of switching between languages within a\nconversation, presents unique challenges for traditional natural language\nprocessing. Existing benchmarks, such as LinCE and GLUECoS, are limited by\nnarrow language pairings and tasks, failing to adequately evaluate the\ncode-mixing capabilities of large language models (LLMs). Despite the\nsignificance of code-mixing for multilingual users, research on LLMs in this\ncontext remains limited. Additionally, current methods for generating\ncode-mixed data are underdeveloped. In this paper, we conduct a comprehensive\nevaluation of LLMs' performance on code-mixed data across 18 languages from\nseven language families. We also propose a novel approach for generating\nsynthetic code-mixed texts by combining word substitution with GPT-4 prompting.\nOur analysis reveals consistent underperformance of LLMs on code-mixed datasets\ninvolving multiple language families. We suggest that improvements in training\ndata size, model scale, and few-shot learning could enhance their performance."}
{"id": "2507.18836", "pdf": "https://arxiv.org/pdf/2507.18836.pdf", "abs": "https://arxiv.org/abs/2507.18836", "title": "Uncertainty on Display: The Effects of Communicating Confidence Cues in Autonomous Vehicle-Pedestrian Interactions", "authors": ["Yue Luo", "Xinyan Yu", "Tram Thi Minh Tran", "Marius Hoggenmueller"], "categories": ["cs.HC"], "comment": null, "summary": "Uncertainty is an inherent aspect of autonomous vehicle (AV) decision-making,\nyet it is rarely communicated to pedestrians, which hinders transparency. This\nstudy investigates how AV uncertainty can be conveyed through two approaches:\nexplicit communication (confidence percentage displays) and implicit\ncommunication (vehicle motion cues), across different confidence levels (high\nand low). Through a within-subject VR experiment (N=26), we evaluated these\napproaches in a crossing scenario, assessing interface qualities (visibility\nand intuitiveness), how well the information conveyed the vehicle's level of\nconfidence, and their impact on participants' perceived safety, trust, and user\nexperience. Our results show that explicit communication is more effective and\npreferred for conveying uncertainty, enhancing safety, trust, and user\nexperience. Conversely, implicit communication introduces ambiguity, especially\nwhen AV confidence is low. This research provides empirical insights into how\nuncertainty communication shapes pedestrian interpretation of AV behaviour and\noffer design guidance for external interfaces that integrate uncertainty as a\ncommunicative element."}
{"id": "2507.18827", "pdf": "https://arxiv.org/pdf/2507.18827.pdf", "abs": "https://arxiv.org/abs/2507.18827", "title": "CueBuddy: helping non-native English speakers navigate English-centric STEM education", "authors": ["Pranav Gupta"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Students across the world in STEM classes, especially in the Global South,\nfall behind their peers who are more fluent in English, despite being at par\nwith them in terms of scientific prerequisites. While many of them are able to\nfollow everyday English at ease, key terms in English stay challenging. In most\ncases, such students have had most of their course prerequisites in a lower\nresource language. Live speech translation to lower resource languages is a\npromising area of research, however, models for speech translation can be too\nexpensive on a large scale and often struggle with technical content. In this\npaper, we describe CueBuddy, which aims to remediate these issues by providing\nreal-time \"lexical cues\" through technical keyword spotting along real-time\nmultilingual glossary lookup to help students stay up to speed with complex\nEnglish jargon without disrupting their concentration on the lecture. We also\ndescribe the limitations and future extensions of our approach."}
{"id": "2507.18877", "pdf": "https://arxiv.org/pdf/2507.18877.pdf", "abs": "https://arxiv.org/abs/2507.18877", "title": "A Survey on Methodological Approaches to Collaborative Embodiment in Virtual Reality", "authors": ["Hongyu Zhou", "Yihao Dong", "Masahiko Inami", "Zhanna Sarsenbayeva", "Anusha Withana"], "categories": ["cs.HC"], "comment": null, "summary": "The application and implementation of collaborative embodiment in virtual\nreality (VR) are a critical aspect of the computer science landscape, aiming to\nenhance multi-user interaction and teamwork in immersive environments. A\nnotable and enduring area of collaborative embodiment research focuses on\napproaches that enable multiple users to share control, interact, and\ninvestigate scenarios involving supernumerary arms in virtual spaces. In this\nsurvey, we will present an extensive overview of the methodologies employed in\nthe past decade to enable collaboration in VR environments, particularly\nthrough embodiment. Using the PRISMA guidelines, we plan to analyze the study\ndetails from over 137 relevant research papers. Through this analysis, a\ncritical assessment of the effectiveness of these methodologies will be\nconducted, highlighting current challenges and limitations in implementing\ncollaborative embodiment in VR. Lastly, we discuss potential future research\ndirections and opportunities for enhancing collaboration embodiment in virtual\nenvironments."}
{"id": "2507.18857", "pdf": "https://arxiv.org/pdf/2507.18857.pdf", "abs": "https://arxiv.org/abs/2507.18857", "title": "PrismRAG: Boosting RAG Factuality with Distractor Resilience and Strategized Reasoning", "authors": ["Mohammad Kachuee", "Teja Gollapudi", "Minseok Kim", "Yin Huang", "Kai Sun", "Xiao Yang", "Jiaqi Wang", "Nirav Shah", "Yue Liu", "Aaron Colak", "Anuj Kumar", "Wen-tau Yih", "Xin Luna Dong"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Retrieval-augmented generation (RAG) often falls short when retrieved context\nincludes confusing semi-relevant passages, or when answering questions require\ndeep contextual understanding and reasoning. We propose an efficient\nfine-tuning framework, called PrismRAG, that (i) trains the model with\ndistractor-aware QA pairs mixing gold evidence with subtle distractor passages,\nand (ii) instills reasoning-centric habits that make the LLM plan, rationalize,\nand synthesize without relying on extensive human engineered instructions.\nEvaluated across 12 open-book RAG QA benchmarks spanning diverse application\ndomains and scenarios, PrismRAG improves average factuality by 5.4%,\noutperforming state-of-the-art solutions."}
{"id": "2507.18878", "pdf": "https://arxiv.org/pdf/2507.18878.pdf", "abs": "https://arxiv.org/abs/2507.18878", "title": "Improving the State of the Art for Training Human-AI Teams: Technical Report #5 -- Individual Differences and Team Qualities to Measure in a Human-AI Teaming Testbed", "authors": ["Lillian Asiala", "James E. McCarthy"], "categories": ["cs.HC"], "comment": null, "summary": "Sonalysts, Inc. (Sonalysts) is working on an initiative to expand our\nexpertise in teaming to include Human-Artificial Intelligence (AI) teams. The\nfirst step of this process is to develop a Synthetic Task Environment (STE) to\nsupport our original research. Prior knowledge elicitation efforts within the\nHuman-AI teaming research stakeholder community revealed a desire to support\ndata collection using pre- and post-performance surveys. In this technical\nreport, we review a number of constructs that capture meaningful individual\ndifferences and teaming qualities. Additionally, we explore methods of\nmeasuring those constructs within the STE."}
{"id": "2507.18884", "pdf": "https://arxiv.org/pdf/2507.18884.pdf", "abs": "https://arxiv.org/abs/2507.18884", "title": "MindFlow+: A Self-Evolving Agent for E-Commerce Customer Service", "authors": ["Ming Gong", "Xucheng Huang", "Ziheng Xu", "Vijayan K. Asari"], "categories": ["cs.CL"], "comment": null, "summary": "High-quality dialogue is crucial for e-commerce customer service, yet\ntraditional intent-based systems struggle with dynamic, multi-turn\ninteractions. We present MindFlow+, a self-evolving dialogue agent that learns\ndomain-specific behavior by combining large language models (LLMs) with\nimitation learning and offline reinforcement learning (RL). MindFlow+\nintroduces two data-centric mechanisms to guide learning: tool-augmented\ndemonstration construction, which exposes the model to knowledge-enhanced and\nagentic (ReAct-style) interactions for effective tool use; and\nreward-conditioned data modeling, which aligns responses with task-specific\ngoals using reward signals. To evaluate the model's role in response\ngeneration, we introduce the AI Contribution Ratio, a novel metric quantifying\nAI involvement in dialogue. Experiments on real-world e-commerce conversations\nshow that MindFlow+ outperforms strong baselines in contextual relevance,\nflexibility, and task accuracy. These results demonstrate the potential of\ncombining LLMs tool reasoning, and reward-guided learning to build\ndomain-specialized, context-aware dialogue systems."}
{"id": "2507.18880", "pdf": "https://arxiv.org/pdf/2507.18880.pdf", "abs": "https://arxiv.org/abs/2507.18880", "title": "Rethinking Accessible Prototyping Methods for Blind and Visually Impaired Passengers in Highly Automated Vehicles", "authors": ["Luca-Maxim Meinhardt", "Enrico Rukzio"], "categories": ["cs.HC"], "comment": "Workshop paper presented at \"Access InContext: Futuring Accessible\n  Prototyping Tools and Methods\", CHI'25, April 26, 2025, Yokohama, Japan.\n  Submitted Feb 5, accepted Mar 1", "summary": "Highly Automated Vehicles (HAVs) can improve mobility for blind and visually\nimpaired people (BVIPs). However, designing non-visual interfaces that enable\nthem to maintain situation awareness inside the vehicle is a challenge. This\npaper presents two of our participatory design workshops that explored what\ninformation BVIPs need in HAVs and what an interface that meets these needs\nmight look like. Based on the participants' insights, we created final systems\nto improve their situation awareness. The two workshops used different\napproaches: in the first, participants built their own low-fidelity prototypes;\nin the second, they evaluated and discussed the initial prototypes we provided.\nWe will outline how each workshop was set up and share lessons learned about\nprototyping methods for BVIPs and how they could be improved."}
{"id": "2507.18890", "pdf": "https://arxiv.org/pdf/2507.18890.pdf", "abs": "https://arxiv.org/abs/2507.18890", "title": "NUTMEG: Separating Signal From Noise in Annotator Disagreement", "authors": ["Jonathan Ivey", "Susan Gauch", "David Jurgens"], "categories": ["cs.CL"], "comment": null, "summary": "NLP models often rely on human-labeled data for training and evaluation. Many\napproaches crowdsource this data from a large number of annotators with varying\nskills, backgrounds, and motivations, resulting in conflicting annotations.\nThese conflicts have traditionally been resolved by aggregation methods that\nassume disagreements are errors. Recent work has argued that for many tasks\nannotators may have genuine disagreements and that variation should be treated\nas signal rather than noise. However, few models separate signal and noise in\nannotator disagreement. In this work, we introduce NUTMEG, a new Bayesian model\nthat incorporates information about annotator backgrounds to remove noisy\nannotations from human-labeled training data while preserving systematic\ndisagreements. Using synthetic data, we show that NUTMEG is more effective at\nrecovering ground-truth from annotations with systematic disagreement than\ntraditional aggregation methods. We provide further analysis characterizing how\ndifferences in subpopulation sizes, rates of disagreement, and rates of spam\naffect the performance of our model. Finally, we demonstrate that downstream\nmodels trained on NUTMEG-aggregated data significantly outperform models\ntrained on data from traditionally aggregation methods. Our results highlight\nthe importance of accounting for both annotator competence and systematic\ndisagreements when training on human-labeled data."}
{"id": "2507.18913", "pdf": "https://arxiv.org/pdf/2507.18913.pdf", "abs": "https://arxiv.org/abs/2507.18913", "title": "Limits at a Distance: Design Directions to Address Psychological Distance in Policy Decisions Affecting Planetary Boundaries", "authors": ["Eshta Bhardwaj", "Han Qiao", "Christoph Becker"], "categories": ["cs.HC"], "comment": "Post-proceedings paper presented at LIMITS 2024: 10th Workshop on\n  Computing within Limits, 2024-06-19/20, Online", "summary": "Policy decisions relevant to the environment rely on tools like dashboards,\nrisk models, and prediction models to provide information and data\nvisualizations that enable decision-makers to make trade-offs. The conventional\nparadigm of data visualization practices for policy and decision-making is to\nconvey data in a supposedly neutral, objective manner for rational\ndecision-makers. Feminist critique advocates for nuanced and reflexive\napproaches that take into account situated decision-makers and their affective\nrelationships to data. This paper sheds light on a key cognitive aspect that\nimpacts how decision-makers interpret data. Because all outcomes from policies\nrelevant to climate change occur at a distance, decision-makers experience\nso-called `psychological distance' to environmental decisions in terms of\nspace, time, social identity, and hypotheticality. This profoundly impacts how\nthey perceive and evaluate outcomes. Since policy decisions to achieve a safe\nplanetary space are urgently needed for immediate transition and change, we\nneed a design practice that takes into account how psychological distance\naffects cognition and decision-making. Our paper explores the role of\nalternative design approaches in developing visualizations used for climate\npolicymaking. We conduct a literature review and synthesis which bridges\npsychological distance with speculative design and data visceralization by\nillustrating the value of affective design methods via examples from previous\nresearch. Through this work, we propose a novel premise for the communication\nand visualization of environmental data. Our paper lays out how future research\non the impacts of alternative design approaches on psychological distance can\nmake data used for policy decisions more tangible and visceral."}
{"id": "2507.18901", "pdf": "https://arxiv.org/pdf/2507.18901.pdf", "abs": "https://arxiv.org/abs/2507.18901", "title": "REPRO-Bench: Can Agentic AI Systems Assess the Reproducibility of Social Science Research?", "authors": ["Chuxuan Hu", "Liyun Zhang", "Yeji Lim", "Aum Wadhwani", "Austin Peters", "Daniel Kang"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "Assessing the reproducibility of social science papers is essential for\npromoting rigor in research processes, but manual assessment is costly. With\nrecent advances in agentic AI systems (i.e., AI agents), we seek to evaluate\ntheir capability to automate this process. However, existing benchmarks for\nreproducing research papers (1) focus solely on reproducing results using\nprovided code and data without assessing their consistency with the paper, (2)\noversimplify real-world scenarios, and (3) lack necessary diversity in data\nformats and programming languages. To address these issues, we introduce\nREPRO-Bench, a collection of 112 task instances, each representing a social\nscience paper with a publicly available reproduction report. The agents are\ntasked with assessing the reproducibility of the paper based on the original\npaper PDF and the corresponding reproduction package. REPRO-Bench features\nend-to-end evaluation tasks on the reproducibility of social science papers\nwith complexity comparable to real-world assessments. We evaluate three\nrepresentative AI agents on REPRO-Bench, with the best-performing agent\nachieving an accuracy of only 21.4%. Building on our empirical analysis, we\ndevelop REPRO-Agent, which improves the highest accuracy achieved by existing\nagents by 71%. We conclude that more advanced AI agents should be developed to\nautomate real-world reproducibility assessment. REPRO-Bench is publicly\navailable at https://github.com/uiuc-kang-lab/REPRO-Bench."}
{"id": "2507.18945", "pdf": "https://arxiv.org/pdf/2507.18945.pdf", "abs": "https://arxiv.org/abs/2507.18945", "title": "TreeReader: A Hierarchical Academic Paper Reader Powered by Language Models", "authors": ["Zijian Zhang", "Pan Chen", "Fangshi Du", "Runlong Ye", "Oliver Huang", "Michael Liut", "Alán Aspuru-Guzik"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Efficiently navigating and understanding academic papers is crucial for\nscientific progress. Traditional linear formats like PDF and HTML can cause\ncognitive overload and obscure a paper's hierarchical structure, making it\ndifficult to locate key information. While LLM-based chatbots offer\nsummarization, they often lack nuanced understanding of specific sections, may\nproduce unreliable information, and typically discard the document's\nnavigational structure. Drawing insights from a formative study on academic\nreading practices, we introduce TreeReader, a novel language model-augmented\npaper reader. TreeReader decomposes papers into an interactive tree structure\nwhere each section is initially represented by an LLM-generated concise\nsummary, with underlying details accessible on demand. This design allows users\nto quickly grasp core ideas, selectively explore sections of interest, and\nverify summaries against the source text. A user study was conducted to\nevaluate TreeReader's impact on reading efficiency and comprehension.\nTreeReader provides a more focused and efficient way to navigate and understand\ncomplex academic literature by bridging hierarchical summarization with\ninteractive exploration."}
{"id": "2507.18902", "pdf": "https://arxiv.org/pdf/2507.18902.pdf", "abs": "https://arxiv.org/abs/2507.18902", "title": "SLoW: Select Low-frequency Words! Automatic Dictionary Selection for Translation on Large Language Models", "authors": ["Hongyuan Lu", "Zixuan Li", "Zefan Zhang", "Wai Lam"], "categories": ["cs.CL"], "comment": null, "summary": "There are more than 7,000 languages around the world, and current Large\nLanguage Models (LLMs) only support hundreds of languages. Dictionary-based\nprompting methods can enhance translation on them, but most methods use all the\navailable dictionaries, which could be expensive. Instead, it will be flexible\nto have a trade-off between token consumption and translation performance. This\npaper proposes a novel task called \\textbf{A}utomatic \\textbf{D}ictionary\n\\textbf{S}election (\\textbf{ADS}). The goal of the task is to automatically\nselect which dictionary to use to enhance translation. We propose a novel and\neffective method which we call \\textbf{S}elect \\textbf{Lo}w-frequency\n\\textbf{W}ords! (\\textbf{SLoW}) which selects those dictionaries that have a\nlower frequency. Our methods have unique advantages. First, there is no need\nfor access to the training data for frequency estimation (which is usually\nunavailable). Second, it inherits the advantage of dictionary-based methods,\nwhere no additional tuning is required on LLMs. Experimental results on 100\nlanguages from FLORES indicate that SLoW surpasses strong baselines, and it can\nobviously save token usage, with many languages even surpassing the translation\nperformance of the full dictionary baseline.\\footnote{A shocking fact is that\nthere is no need to use the actual training data (often unobtainable) for\nfrequency estimation, and an estimation frequency obtained using public\nresources is still apparently effective in improving translation with ChatGPT\nand Llama, and DeepSeek.}\\footnote{Code and data available upon publication.}"}
{"id": "2507.18971", "pdf": "https://arxiv.org/pdf/2507.18971.pdf", "abs": "https://arxiv.org/abs/2507.18971", "title": "Rethinking Dataset Discovery with DataScout", "authors": ["Rachel Lin", "Bhavya Chopra", "Wenjing Lin", "Shreya Shankar", "Madelon Hulsebos", "Aditya G. Parameswaran"], "categories": ["cs.HC"], "comment": "16 pages; 6 figures; 4 tables; To appear at UIST 2025", "summary": "Dataset Search -- the process of finding appropriate datasets for a given\ntask -- remains a critical yet under-explored challenge in data science\nworkflows. Assessing dataset suitability for a task (e.g., training a\nclassification model) is a multi-pronged affair that involves understanding:\ndata characteristics (e.g. granularity, attributes, size), semantics (e.g.,\ndata semantics, creation goals), and relevance to the task at hand. Present-day\ndataset search interfaces are restrictive -- users struggle to convey implicit\npreferences and lack visibility into the search space and result inclusion\ncriteria -- making query iteration challenging. To bridge these gaps, we\nintroduce DataScout to proactively steer users through the process of dataset\ndiscovery via -- (i) AI-assisted query reformulations informed by the\nunderlying search space, (ii) semantic search and filtering based on dataset\ncontent, including attributes (columns) and granularity (rows), and (iii)\ndataset relevance indicators, generated dynamically based on the user-specified\ntask. A within-subjects study with 12 participants comparing DataScout to\nkeyword and semantic dataset search reveals that users uniquely employ\nDataScout's features not only for structured explorations, but also to glean\nfeedback on their search queries and build conceptual models of the search\nspace."}
{"id": "2507.18905", "pdf": "https://arxiv.org/pdf/2507.18905.pdf", "abs": "https://arxiv.org/abs/2507.18905", "title": "Large language models provide unsafe answers to patient-posed medical questions", "authors": ["Rachel L. Draelos", "Samina Afreen", "Barbara Blasko", "Tiffany Brazile", "Natasha Chase", "Dimple Desai", "Jessica Evert", "Heather L. Gardner", "Lauren Herrmann", "Aswathy Vaikom House", "Stephanie Kass", "Marianne Kavan", "Kirshma Khemani", "Amanda Koire", "Lauren M. McDonald", "Zahraa Rabeeah", "Amy Shah"], "categories": ["cs.CL", "cs.HC"], "comment": "20 pages", "summary": "Millions of patients are already using large language model (LLM) chatbots\nfor medical advice on a regular basis, raising patient safety concerns. This\nphysician-led red-teaming study compares the safety of four publicly available\nchatbots--Claude by Anthropic, Gemini by Google, GPT-4o by OpenAI, and\nLlama3-70B by Meta--on a new dataset, HealthAdvice, using an evaluation\nframework that enables quantitative and qualitative analysis. In total, 888\nchatbot responses are evaluated for 222 patient-posed advice-seeking medical\nquestions on primary care topics spanning internal medicine, women's health,\nand pediatrics. We find statistically significant differences between chatbots.\nThe rate of problematic responses varies from 21.6 percent (Claude) to 43.2\npercent (Llama), with unsafe responses varying from 5 percent (Claude) to 13\npercent (GPT-4o, Llama). Qualitative results reveal chatbot responses with the\npotential to lead to serious patient harm. This study suggests that millions of\npatients could be receiving unsafe medical advice from publicly available\nchatbots, and further work is needed to improve the clinical safety of these\npowerful tools."}
{"id": "2507.19026", "pdf": "https://arxiv.org/pdf/2507.19026.pdf", "abs": "https://arxiv.org/abs/2507.19026", "title": "RhythmTA: A Visual-Aided Interactive System for ESL Rhythm Training via Dubbing Practice", "authors": ["Chang Chen", "Sicheng Song", "Shuchang Xu", "Zhicheng Li", "Huamin Qu", "Yanna Lin"], "categories": ["cs.HC"], "comment": null, "summary": "English speech rhythm, the temporal patterns of stressed syllables, is\nessential for English as a second language (ESL) learners to produce\nnatural-sounding and comprehensible speech. Rhythm training is generally based\non imitation of native speech. However, it relies heavily on external\ninstructor feedback, preventing ESL learners from independent practice. To\naddress this gap, we present RhythmTA, an interactive system for ESL learners\nto practice speech rhythm independently via dubbing, an imitation-based\napproach. The system automatically extracts rhythm from any English speech and\nintroduces novel visual designs to support three stages of dubbing practice:\n(1) Synchronized listening with visual aids to enhance perception, (2) Guided\nrepeating by visual cues for self-adjustment, and (3) Comparative reflection\nfrom a parallel view for self-monitoring. Our design is informed by a formative\nstudy with nine spoken English instructors, which identified current practices\nand challenges. A user study with twelve ESL learners demonstrates that\nRhythmTA effectively enhances learners' rhythm perception and shows significant\npotential for improving rhythm production."}
{"id": "2507.18910", "pdf": "https://arxiv.org/pdf/2507.18910.pdf", "abs": "https://arxiv.org/abs/2507.18910", "title": "A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems: Progress, Gaps, and Future Directions", "authors": ["Agada Joseph Oche", "Ademola Glory Folashade", "Tirthankar Ghosal", "Arpan Biswas"], "categories": ["cs.CL", "cs.LG"], "comment": "33 pages, 2 figures", "summary": "Retrieval-Augmented Generation (RAG) represents a major advancement in\nnatural language processing (NLP), combining large language models (LLMs) with\ninformation retrieval systems to enhance factual grounding, accuracy, and\ncontextual relevance. This paper presents a comprehensive systematic review of\nRAG, tracing its evolution from early developments in open domain question\nanswering to recent state-of-the-art implementations across diverse\napplications. The review begins by outlining the motivations behind RAG,\nparticularly its ability to mitigate hallucinations and outdated knowledge in\nparametric models. Core technical components-retrieval mechanisms,\nsequence-to-sequence generation models, and fusion strategies are examined in\ndetail. A year-by-year analysis highlights key milestones and research trends,\nproviding insight into RAG's rapid growth. The paper further explores the\ndeployment of RAG in enterprise systems, addressing practical challenges\nrelated to retrieval of proprietary data, security, and scalability. A\ncomparative evaluation of RAG implementations is conducted, benchmarking\nperformance on retrieval accuracy, generation fluency, latency, and\ncomputational efficiency. Persistent challenges such as retrieval quality,\nprivacy concerns, and integration overhead are critically assessed. Finally,\nthe review highlights emerging solutions, including hybrid retrieval\napproaches, privacy-preserving techniques, optimized fusion strategies, and\nagentic RAG architectures. These innovations point toward a future of more\nreliable, efficient, and context-aware knowledge-intensive NLP systems."}
{"id": "2507.19072", "pdf": "https://arxiv.org/pdf/2507.19072.pdf", "abs": "https://arxiv.org/abs/2507.19072", "title": "Exploring post-neoliberal futures for managing commercial heating and cooling through speculative praxis", "authors": ["Oliver Bates", "Christian Remy", "Kieran Cutting", "Adam Tyler", "Adrian Friday"], "categories": ["cs.HC"], "comment": "Post-proceedings paper presented at LIMITS 2024: 10th Workshop on\n  Computing within Limits, 2024-06-19/20, Online", "summary": "What could designing for carbon reduction of heating and cooling in\ncommercial settings look like in the near future? How can we challenge dominant\nmindsets and paradigms of efficiency and behaviour change? How can we help\nbuild worlds through our practice that can become future realities? This paper\nintroduces the fictional consultancy ANCSTRL.LAB to explore opportunities for\nmaking space in research projects that can encourage more systems-oriented\ninterventions. We present a design fiction that asks `what if energy management\nand reduction practice embraced systems thinking?'. Our design fiction explores\nhow future energy consultancies could utilise systems thinking, and (more than)\nhuman centred design to re-imagine energy management practice and change\nsystems in ways that are currently unfathomable. We finish by discussing how\nLIMITS research can utilise design fiction and speculative praxis to help build\nnew material realities where more holistic perspectives, the leveraging of\nsystems change, and the imagining of post-neoliberal futures is the norm."}
{"id": "2507.18915", "pdf": "https://arxiv.org/pdf/2507.18915.pdf", "abs": "https://arxiv.org/abs/2507.18915", "title": "Mining Contextualized Visual Associations from Images for Creativity Understanding", "authors": ["Ananya Sahu", "Amith Ananthram", "Kathleen McKeown"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Understanding another person's creative output requires a shared language of\nassociation. However, when training vision-language models such as CLIP, we\nrely on web-scraped datasets containing short, predominantly literal, alt-text.\nIn this work, we introduce a method for mining contextualized associations for\nsalient visual elements in an image that can scale to any unlabeled dataset.\nGiven an image, we can use these mined associations to generate high quality\ncreative captions at increasing degrees of abstraction. With our method, we\nproduce a new dataset of visual associations and 1.7m creative captions for the\nimages in MSCOCO. Human evaluation confirms that these captions remain visually\ngrounded while exhibiting recognizably increasing abstraction. Moreover,\nfine-tuning a visual encoder on this dataset yields meaningful improvements in\nzero-shot image-text retrieval in two creative domains: poetry and metaphor\nvisualization. We release our dataset, our generation code and our models for\nuse by the broader community."}
{"id": "2507.19094", "pdf": "https://arxiv.org/pdf/2507.19094.pdf", "abs": "https://arxiv.org/abs/2507.19094", "title": "Environmental (in)considerations in the Design of Smartphone Settings", "authors": ["Thomas Thibault", "Léa Mosesso", "Camille Adam", "Aurélien Tabard", "Anaëlle Beignon", "Nolwenn Maudet"], "categories": ["cs.HC"], "comment": "Post-proceedings paper presented at LIMITS 2025: 11th Workshop on\n  Computing within Limits, 2025-06-26/27, Online", "summary": "Designing for sufficiency is one of many approaches that could foster more\nmoderate and sustainable digital practices. Based on the Sustainable\nInformation and Communication Technologies (ICT) and Human-Computer Interaction\n(HCI) literature, we identify five environmental settings categories. However,\nour analysis of three mobile OS and nine representative applications shows an\noverall lack of environmental concerns in settings design, leading us to\nidentify six pervasive anti-patterns. Environmental settings, where they exist,\nare set on the most intensive option by default. They are not presented as\nsuch, are not easily accessible, and offer little explanation of their impact.\nInstead, they encourage more intensive use. Based on these findings, we create\na design workbook that explores design principles for environmental settings:\npresenting the environmental potential of settings; shifting to environmentally\nneutral states; previewing effects to encourage moderate use; rethinking\ndefaults; facilitating settings access and; exploring more frugal settings.\nBuilding upon this workbook, we discuss how settings can tie individual\nbehaviors to systemic factors."}
{"id": "2507.18918", "pdf": "https://arxiv.org/pdf/2507.18918.pdf", "abs": "https://arxiv.org/abs/2507.18918", "title": "Uncovering Cross-Linguistic Disparities in LLMs using Sparse Autoencoders", "authors": ["Richmond Sin Jing Xuan", "Jalil Huseynov", "Yang Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multilingual large language models (LLMs) exhibit strong cross-linguistic\ngeneralization, yet medium to low resource languages underperform on common\nbenchmarks such as ARC-Challenge, MMLU, and HellaSwag. We analyze activation\npatterns in Gemma-2-2B across all 26 residual layers and 10 languages: Chinese\n(zh), Russian (ru), Spanish (es), Italian (it), medium to low resource\nlanguages including Indonesian (id), Catalan (ca), Marathi (mr), Malayalam\n(ml), and Hindi (hi), with English (en) as the reference. Using Sparse\nAutoencoders (SAEs), we reveal systematic disparities in activation patterns.\nMedium to low resource languages receive up to 26.27 percent lower activations\nin early layers, with a persistent gap of 19.89 percent in deeper layers. To\naddress this, we apply activation-aware fine-tuning via Low-Rank Adaptation\n(LoRA), leading to substantial activation gains, such as 87.69 percent for\nMalayalam and 86.32 percent for Hindi, while maintaining English retention at\napproximately 91 percent. After fine-tuning, benchmark results show modest but\nconsistent improvements, highlighting activation alignment as a key factor in\nenhancing multilingual LLM performance."}
{"id": "2507.19104", "pdf": "https://arxiv.org/pdf/2507.19104.pdf", "abs": "https://arxiv.org/abs/2507.19104", "title": "A systematic literature review to unveil users objective reaction to virtual experiences: Complemented with a conceptual model (QoUX in VE)", "authors": ["Alireza Mortezapour", "Andrea Antonio Cantone", "Monica Maria Lucia Sebillo", "Giuliana Vitiello"], "categories": ["cs.HC"], "comment": null, "summary": "In pursuit of documenting users Neurophysiological responses during\nexperiencing virtual environments (VE), this systematic review presents a novel\nconceptual model of UX in VE. Searching across seven databases yielded to 1743\narticles. Rigorous screenings, included only 66 articles. Notably, UX in VE\nlacks a consensus definition. Obviously, this UX has many unique sub-dimensions\nthat are not mentioned in other products. The presented conceptual model\ncontains 26 subdimensions which mostly not supported in previous subjective\ntools and questionnaires. While EEG and ECG were common, brain ultrasound,\nemployed in one study, highlights the need for using neurophysiological\nassessments to comprehensively grasp immersive UX intricacies."}
{"id": "2507.18940", "pdf": "https://arxiv.org/pdf/2507.18940.pdf", "abs": "https://arxiv.org/abs/2507.18940", "title": "LLaVA-NeuMT: Selective Layer-Neuron Modulation for Efficient Multilingual Multimodal Translation", "authors": ["Jingxuan Wei", "Caijun Jia", "Qi Chen", "Yujun Cai", "Linzhuang Sun", "Xiangxiang Zhang", "Gaowei Wu", "Bihui Yu"], "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "Multimodal Machine Translation (MMT) enhances translation quality by\nincorporating visual context, helping to resolve textual ambiguities. While\nexisting MMT methods perform well in bilingual settings, extending them to\nmultilingual translation remains challenging due to cross-lingual interference\nand ineffective parameter-sharing strategies. To address this, we propose\nLLaVA-NeuMT, a novel multimodal multilingual translation framework that\nexplicitly models language-specific and language-agnostic representations to\nmitigate multilingual interference. Our approach consists of a layer selection\nmechanism that identifies the most informative layers for different language\npairs and a neuron-level adaptation strategy that dynamically selects\nlanguage-specific and agnostic neurons to improve translation quality while\nreducing redundancy. We conduct extensive experiments on the M3-Multi30K and\nM3-AmbigCaps datasets, demonstrating that LLaVA-NeuMT, while fine-tuning only\n40\\% of the model parameters, surpasses full fine-tuning approaches and\nultimately achieves SOTA results on both datasets. Our analysis further\nprovides insights into the importance of selected layers and neurons in\nmultimodal multilingual adaptation, offering an efficient and scalable solution\nto cross-lingual adaptation in multimodal translation."}
{"id": "2507.19114", "pdf": "https://arxiv.org/pdf/2507.19114.pdf", "abs": "https://arxiv.org/abs/2507.19114", "title": "A Therapeutic Role-Playing VR Game for Children with Intellectual Disabilities", "authors": ["Santiago Berrezueta-Guzman", "WenChun Chen", "Stefan Wagner"], "categories": ["cs.HC"], "comment": "Paper accepted for publication and presentation in the 3rd Annual\n  IEEE International Conference on Metaverse Computing, Networking, and\n  Applications (IEEE MetaCom 2025) will be held in Sejong University, Seoul,\n  Republic of Korea, on August 27 - 29, 2025", "summary": "Virtual Reality (VR) offers promising avenues for innovative therapeutic\ninterventions in populations with intellectual disabilities (ID). This paper\npresents the design, development, and evaluation of Space Exodus, a novel\nVR-based role-playing game specifically tailored for children with ID. By\nintegrating immersive gameplay with therapeutic task design, Space Exodus aims\nto enhance concentration, cognitive processing, and fine motor skills through\nstructured hand-eye coordination exercises. A six-week pre-test/post-test study\nwas conducted with 16 children in Ecuador, using standardized assessments, the\nToulouse-Pieron Cancellation Test, and the Moss Attention Rating Scale\ncomplemented by detailed observational metrics. Quantitative results indicate\nstatistically significant improvements in concentration scores, with test\nscores increasing from 65.2 to 80.3 and 55.4 to 68.7, respectively (p < 0.01).\nQualitative observations revealed reduced task attempts, enhanced user\nconfidence, and increased active participation. The inclusion of a VR assistant\nprovided consistent guidance that further boosted engagement. These findings\ndemonstrate the potential of immersive, game-based learning environments as\npractical therapeutic tools, laying a robust foundation for developing\ninclusive and adaptive rehabilitation strategies for children with ID."}
{"id": "2507.18952", "pdf": "https://arxiv.org/pdf/2507.18952.pdf", "abs": "https://arxiv.org/abs/2507.18952", "title": "Legal Document Summarization: Enhancing Judicial Efficiency through Automation Detection", "authors": ["Yongjie Li", "Ruilin Nong", "Jianan Liu", "Lucas Evans"], "categories": ["cs.CL"], "comment": null, "summary": "Legal document summarization represents a significant advancement towards\nimproving judicial efficiency through the automation of key information\ndetection. Our approach leverages state-of-the-art natural language processing\ntechniques to meticulously identify and extract essential data from extensive\nlegal texts, which facilitates a more efficient review process. By employing\nadvanced machine learning algorithms, the framework recognizes underlying\npatterns within judicial documents to create precise summaries that encapsulate\nthe crucial elements. This automation alleviates the burden on legal\nprofessionals, concurrently reducing the likelihood of overlooking vital\ninformation that could lead to errors. Through comprehensive experiments\nconducted with actual legal datasets, we demonstrate the capability of our\nmethod to generate high-quality summaries while preserving the integrity of the\noriginal content and enhancing processing times considerably. The results\nreveal marked improvements in operational efficiency, allowing legal\npractitioners to direct their efforts toward critical analytical and\ndecision-making activities instead of manual reviews. This research highlights\npromising technology-driven strategies that can significantly alter workflow\ndynamics within the legal sector, emphasizing the role of automation in\nrefining judicial processes."}
{"id": "2507.19193", "pdf": "https://arxiv.org/pdf/2507.19193.pdf", "abs": "https://arxiv.org/abs/2507.19193", "title": "Where are the Frontlines? A Visualization Approach for Map Control in Team-Based Games", "authors": ["Jonas Peché", "Aliaksei Tsishurou", "Alexander Zap", "Guenter Wallner"], "categories": ["cs.HC"], "comment": null, "summary": "A central area of interest in many competitive online games is spatial\nbehavior which due to its complexity can be difficult to visualize. Such\nbehaviors of interest include not only overall movement patterns but also being\nable to understand which player or team is exerting control over an area to\ninform decision-making. Map control can, however, be challenging to quantify.\nIn this paper, we propose a method for calculating frontlines and first efforts\ntowards a visualization of them. The visualization can show map control and\nfrontlines at a specific time point or changes of these over time. For this\npurpose, it utilizes support vector machines to derive frontlines from unit\npositions. We illustrate our algorithm and visualization with examples based on\nthe team-based online game World of Tanks."}
{"id": "2507.18956", "pdf": "https://arxiv.org/pdf/2507.18956.pdf", "abs": "https://arxiv.org/abs/2507.18956", "title": "A Similarity Measure for Comparing Conversational Dynamics", "authors": ["Sang Min Jung", "Kaixiang Zhang", "Cristian Danescu-Niculescu-Mizil"], "categories": ["cs.CL"], "comment": "Code and demos available in ConvoKit (https://convokit.cornell.edu/)", "summary": "The quality of a conversation goes beyond the individual quality of each\nreply, and instead emerges from how these combine into interactional patterns\nthat give the conversation its distinctive overall \"shape\". However, there is\nno robust automated method for comparing conversations in terms of their\noverall interactional dynamics. Such methods could enhance the analysis of\nconversational data and help evaluate conversational agents more holistically.\n  In this work, we introduce a similarity measure for comparing conversations\nwith respect to their dynamics. We design a validation framework for testing\nthe robustness of the metric in capturing differences in conversation dynamics\nand for assessing its sensitivity to the topic of the conversations. Finally,\nto illustrate the measure's utility, we use it to analyze conversational\ndynamics in a large online community, bringing new insights into the role of\nsituational power in conversations."}
{"id": "2507.19376", "pdf": "https://arxiv.org/pdf/2507.19376.pdf", "abs": "https://arxiv.org/abs/2507.19376", "title": "Archiverse: an Approach for Immersive Cultural Heritage", "authors": ["Wieslaw Kopeć", "Anna Jaskulska", "Władysław Fuchs", "Wiktor Stawski", "Stanisław Knapiński", "Barbara Karpowicz", "Rafał Masłyk"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Digital technologies and tools have transformed the way we can study cultural\nheritage and the way we can recreate it digitally. Techniques such as laser\nscanning, photogrammetry, and a variety of Mixed Reality solutions have enabled\nresearchers to examine cultural objects and artifacts more precisely and from\nnew perspectives. In this part of the panel, we explore how Virtual Reality\n(VR) and eXtended Reality (XR) can serve as tools to recreate and visualize the\nremains of historical cultural heritage and experience it in simulations of its\noriginal complexity, which means immersive and interactive. Visualization of\nmaterial culture exemplified by archaeological sites and architecture can be\nparticularly useful when only ruins or archaeological remains survive. However,\nthese advancements also bring significant challenges, especially in the area of\ntransdisciplinary cooperation between specialists from many, often distant,\nfields, and the dissemination of virtual immersive environments among both\nprofessionals and the general public."}
{"id": "2507.18973", "pdf": "https://arxiv.org/pdf/2507.18973.pdf", "abs": "https://arxiv.org/abs/2507.18973", "title": "A Toolbox, Not a Hammer -- Multi-TAG: Scaling Math Reasoning with Multi-Tool Aggregation", "authors": ["Bohan Yao", "Vikas Yadav"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "21 pages, 3 figures", "summary": "Augmenting large language models (LLMs) with external tools is a promising\navenue for developing high-performance mathematical reasoning systems. Prior\ntool-augmented approaches typically finetune an LLM to select and invoke a\nsingle tool at each reasoning step and show promising results on simpler math\nreasoning benchmarks such as GSM8K. However, these approaches struggle with\nmore complex math problems that require precise reasoning over multiple steps.\nTo address this limitation, in this work, we propose Multi-TAG, a Multi-Tool\nAGgregation-based framework. Instead of relying on a single tool, Multi-TAG\nguides an LLM to concurrently invoke multiple tools at each reasoning step. It\nthen aggregates their diverse outputs to verify and refine the reasoning\nprocess, enhancing solution robustness and accuracy. Notably, Multi-TAG is a\nfinetuning-free, inference-only framework, making it readily applicable to any\nLLM backbone, including large open-weight models which are computationally\nexpensive to finetune and proprietary frontier models which cannot be finetuned\nwith custom recipes. We evaluate Multi-TAG on four challenging benchmarks:\nMATH500, AIME, AMC, and OlympiadBench. Across both open-weight and\nclosed-source LLM backbones, Multi-TAG consistently and substantially\noutperforms state-of-the-art baselines, achieving average improvements of 6.0%\nto 7.5% over state-of-the-art baselines."}
{"id": "2507.19466", "pdf": "https://arxiv.org/pdf/2507.19466.pdf", "abs": "https://arxiv.org/abs/2507.19466", "title": "Towards Effective Immersive Technologies in Medicine: Potential and Future Applications based on VR, AR, XR and AI solutions", "authors": ["Aliaksandr Marozau", "Barbara Karpowicz", "Tomasz Kowalewski", "Pavlo Zinevych", "Wiktor Stawski", "Adam Kuzdraliński", "Wiesław Kopeć"], "categories": ["cs.HC"], "comment": null, "summary": "Mixed Reality (MR) technologies such as Virtual and Augmented Reality (VR,\nAR) are well established in medical practice, enhancing diagnostics, treatment,\nand education. However, there are still some limitations and challenges that\nmay be overcome thanks to the latest generations of equipment, software, and\nframeworks based on eXtended Reality (XR) by enabling immersive systems that\nsupport safer, more controlled environments for training and patient care. Our\nreview highlights recent VR and AR applications in key areas of medicine. In\nmedical education, these technologies provide realistic clinical simulations,\nimproving skills and knowledge retention. In surgery, immersive tools enhance\nprocedural precision with detailed anatomical visualizations. VR-based\nrehabilitation has shown effectiveness in restoring motor functions and\nbalance, particularly for neurological patients. In mental health, VR has been\nsuccessful in treating conditions like PTSD and phobias. Although VR and AR\nsolutions are well established, there are still some important limitations,\nincluding high costs and limited tactile feedback, which may be overcome with\nimplementing new technologies that may improve the effectiveness of immersive\nmedical applications such as XR, psychophysiological feedback or integration of\nartificial intelligence (AI) for real-time data analysis and personalized\nhealthcare and training."}
{"id": "2507.19081", "pdf": "https://arxiv.org/pdf/2507.19081.pdf", "abs": "https://arxiv.org/abs/2507.19081", "title": "Arg-LLaDA: Argument Summarization via Large Language Diffusion Models and Sufficiency-Aware Refinement", "authors": ["Hao Li", "Yizheng Sun", "Viktor Schlegel", "Kailai Yang", "Riza Batista-Navarro", "Goran Nenadic"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Argument summarization aims to generate concise, structured representations\nof complex, multi-perspective debates. While recent work has advanced the\nidentification and clustering of argumentative components, the generation stage\nremains underexplored. Existing approaches typically rely on single-pass\ngeneration, offering limited support for factual correction or structural\nrefinement. To address this gap, we introduce Arg-LLaDA, a novel large language\ndiffusion framework that iteratively improves summaries via sufficiency-guided\nremasking and regeneration. Our method combines a flexible masking controller\nwith a sufficiency-checking module to identify and revise unsupported,\nredundant, or incomplete spans, yielding more faithful, concise, and coherent\noutputs. Empirical results on two benchmark datasets demonstrate that Arg-LLaDA\nsurpasses state-of-the-art baselines in 7 out of 10 automatic evaluation\nmetrics. In addition, human evaluations reveal substantial improvements across\ncore dimensions, coverage, faithfulness, and conciseness, validating the\neffectiveness of our iterative, sufficiency-aware generation strategy."}
{"id": "2507.19479", "pdf": "https://arxiv.org/pdf/2507.19479.pdf", "abs": "https://arxiv.org/abs/2507.19479", "title": "IoT and Older Adults: Towards Multimodal EMG and AI-Based Interaction with Smart Home", "authors": ["Wiesław Kopeć", "Jarosław Kowalski", "Aleksander Majda", "Anna Duszyk-Bogorodzka", "Anna Jaskulska", "Cezary Biele"], "categories": ["cs.HC"], "comment": null, "summary": "We report preliminary insights from an exploratory study on non-standard\nnon-invasive interfaces for Smart Home Technologies (SHT). This study is part\nof a broader research project on effective Smart Home ecosystem Sagacity that\nwill target older adults, impaired persons, and other groups disadvantaged in\nthe main technology discourse. Therefore, this research is in line with a\nlong-term research framework of the HASE research group (Human Aspects in\nScience and Engineering) by the Living Lab Kobo. In our study, based on the\nprototype of the comprehensive SHT management system Sagacity, we investigated\nthe potential of bioelectric signals, in particular EMG and EOG as a\ncomplementary interface for SHT. Based on our previous participatory research\nand studies on multimodal interfaces, including VUI and BCI, we prepared an\nin-depth interactive hands-on experience workshops with direct involvement of\nvarious groups of potential end users, including older adults and impaired\npersons (total 18 subjects) to explore and investigate the potential of\nsolutions based on this type of non-standard interfaces. The preliminary\ninsights from the study unveil the potential of EMG/EOG interfaces in\nmultimodal SHT management, alongside limitations and challenges stemming from\nthe current state of technology and recommendations for designing multimodal\ninteraction paradigms pinpointing areas of interest to pursue in further\nstudies."}
{"id": "2507.19090", "pdf": "https://arxiv.org/pdf/2507.19090.pdf", "abs": "https://arxiv.org/abs/2507.19090", "title": "Debating Truth: Debate-driven Claim Verification with Multiple Large Language Model Agents", "authors": ["Haorui He", "Yupeng Li", "Dacheng Wen", "Reynold Cheng", "Francis C. M. Lau"], "categories": ["cs.CL"], "comment": null, "summary": "Claim verification is critical for enhancing digital literacy. However, the\nstate-of-the-art single-LLM methods struggle with complex claim verification\nthat involves multi-faceted evidences. Inspired by real-world fact-checking\npractices, we propose DebateCV, the first claim verification framework that\nadopts a debate-driven methodology using multiple LLM agents. In our framework,\ntwo Debaters take opposing stances on a claim and engage in multi-round\nargumentation, while a Moderator evaluates the arguments and renders a verdict\nwith justifications. To further improve the performance of the Moderator, we\nintroduce a novel post-training strategy that leverages synthetic debate data\ngenerated by the zero-shot DebateCV, effectively addressing the scarcity of\nreal-world debate-driven claim verification data. Experimental results show\nthat our method outperforms existing claim verification methods under varying\nlevels of evidence quality. Our code and dataset are publicly available at\nhttps://anonymous.4open.science/r/DebateCV-6781."}
{"id": "2507.18820", "pdf": "https://arxiv.org/pdf/2507.18820.pdf", "abs": "https://arxiv.org/abs/2507.18820", "title": "MetaMorph -- A Metamodelling Approach For Robot Morphology", "authors": ["Rachel Ringe", "Robin Nolte", "Nima Zargham", "Robert Porzel", "Rainer Malaka"], "categories": ["cs.RO", "cs.HC"], "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Robot appearance crucially shapes Human-Robot Interaction (HRI) but is\ntypically described via broad categories like anthropomorphic, zoomorphic, or\ntechnical. More precise approaches focus almost exclusively on anthropomorphic\nfeatures, which fail to classify robots across all types, limiting the ability\nto draw meaningful connections between robot design and its effect on\ninteraction. In response, we present MetaMorph, a comprehensive framework for\nclassifying robot morphology. Using a metamodeling approach, MetaMorph was\nsynthesized from 222 robots in the IEEE Robots Guide, offering a structured\nmethod for comparing visual features. This model allows researchers to assess\nthe visual distances between robot models and explore optimal design traits\ntailored to different tasks and contexts."}
{"id": "2507.19117", "pdf": "https://arxiv.org/pdf/2507.19117.pdf", "abs": "https://arxiv.org/abs/2507.19117", "title": "Objectifying the Subjective: Cognitive Biases in Topic Interpretations", "authors": ["Swapnil Hingmire", "Ze Shi Li", "Shiyu", "Zeng", "Ahmed Musa Awon", "Luiz Franciscatto Guerra", "Neil Ernst"], "categories": ["cs.CL"], "comment": "Accepted for publication at the Transactions of ACL (TACL) (pre-MIT\n  Press publication version)", "summary": "Interpretation of topics is crucial for their downstream applications.\nState-of-the-art evaluation measures of topic quality such as coherence and\nword intrusion do not measure how much a topic facilitates the exploration of a\ncorpus. To design evaluation measures grounded on a task, and a population of\nusers, we do user studies to understand how users interpret topics. We propose\nconstructs of topic quality and ask users to assess them in the context of a\ntopic and provide rationale behind evaluations. We use reflexive thematic\nanalysis to identify themes of topic interpretations from rationales. Users\ninterpret topics based on availability and representativeness heuristics rather\nthan probability. We propose a theory of topic interpretation based on the\nanchoring-and-adjustment heuristic: users anchor on salient words and make\nsemantic adjustments to arrive at an interpretation. Topic interpretation can\nbe viewed as making a judgment under uncertainty by an ecologically rational\nuser, and hence cognitive biases aware user models and evaluation frameworks\nare needed."}
{"id": "2507.18905", "pdf": "https://arxiv.org/pdf/2507.18905.pdf", "abs": "https://arxiv.org/abs/2507.18905", "title": "Large language models provide unsafe answers to patient-posed medical questions", "authors": ["Rachel L. Draelos", "Samina Afreen", "Barbara Blasko", "Tiffany Brazile", "Natasha Chase", "Dimple Desai", "Jessica Evert", "Heather L. Gardner", "Lauren Herrmann", "Aswathy Vaikom House", "Stephanie Kass", "Marianne Kavan", "Kirshma Khemani", "Amanda Koire", "Lauren M. McDonald", "Zahraa Rabeeah", "Amy Shah"], "categories": ["cs.CL", "cs.HC"], "comment": "20 pages", "summary": "Millions of patients are already using large language model (LLM) chatbots\nfor medical advice on a regular basis, raising patient safety concerns. This\nphysician-led red-teaming study compares the safety of four publicly available\nchatbots--Claude by Anthropic, Gemini by Google, GPT-4o by OpenAI, and\nLlama3-70B by Meta--on a new dataset, HealthAdvice, using an evaluation\nframework that enables quantitative and qualitative analysis. In total, 888\nchatbot responses are evaluated for 222 patient-posed advice-seeking medical\nquestions on primary care topics spanning internal medicine, women's health,\nand pediatrics. We find statistically significant differences between chatbots.\nThe rate of problematic responses varies from 21.6 percent (Claude) to 43.2\npercent (Llama), with unsafe responses varying from 5 percent (Claude) to 13\npercent (GPT-4o, Llama). Qualitative results reveal chatbot responses with the\npotential to lead to serious patient harm. This study suggests that millions of\npatients could be receiving unsafe medical advice from publicly available\nchatbots, and further work is needed to improve the clinical safety of these\npowerful tools."}
{"id": "2507.19156", "pdf": "https://arxiv.org/pdf/2507.19156.pdf", "abs": "https://arxiv.org/abs/2507.19156", "title": "An Empirical Investigation of Gender Stereotype Representation in Large Language Models: The Italian Case", "authors": ["Gioele Giachino", "Marco Rondina", "Antonio Vetrò", "Riccardo Coppola", "Juan Carlos De Martin"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "16 pages, European Conference on Machine Learning and Principles and\n  Practice of Knowledge Discovery in Databases (ECML PKDD 2025) - 5th Workshop\n  on Bias and Fairness in AI (BIAS25)", "summary": "The increasing use of Large Language Models (LLMs) in a large variety of\ndomains has sparked worries about how easily they can perpetuate stereotypes\nand contribute to the generation of biased content. With a focus on gender and\nprofessional bias, this work examines in which manner LLMs shape responses to\nungendered prompts, contributing to biased outputs. This analysis uses a\nstructured experimental method, giving different prompts involving three\ndifferent professional job combinations, which are also characterized by a\nhierarchical relationship. This study uses Italian, a language with extensive\ngrammatical gender differences, to highlight potential limitations in current\nLLMs' ability to generate objective text in non-English languages. Two popular\nLLM-based chatbots are examined, namely OpenAI ChatGPT (gpt-4o-mini) and Google\nGemini (gemini-1.5-flash). Through APIs, we collected a range of 3600\nresponses. The results highlight how content generated by LLMs can perpetuate\nstereotypes. For example, Gemini associated 100% (ChatGPT 97%) of 'she'\npronouns to the 'assistant' rather than the 'manager'. The presence of bias in\nAI-generated text can have significant implications in many fields, such as in\nthe workplaces or in job selections, raising ethical concerns about its use.\nUnderstanding these risks is pivotal to developing mitigation strategies and\nassuring that AI-based systems do not increase social inequalities, but rather\ncontribute to more equitable outcomes. Future research directions include\nexpanding the study to additional chatbots or languages, refining prompt\nengineering methods or further exploiting a larger experimental base."}
{"id": "2507.19132", "pdf": "https://arxiv.org/pdf/2507.19132.pdf", "abs": "https://arxiv.org/abs/2507.19132", "title": "OS-MAP: How Far Can Computer-Using Agents Go in Breadth and Depth?", "authors": ["Xuetian Chen", "Yinghao Chen", "Xinfeng Yuan", "Zhuo Peng", "Lu Chen", "Yuekeng Li", "Zhoujia Zhang", "Yingqian Huang", "Leyan Huang", "Jiaqing Liang", "Tianbao Xie", "Zhiyong Wu", "Qiushi Sun", "Biqing Qi", "Bowen Zhou"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "Work in progress", "summary": "Computer-using agents have shown strong potential to boost human productivity\nand enable new application forms across platforms. While recent advances have\nled to usable applications, existing benchmarks fail to account for the\ninternal task heterogeneity and the corresponding agent capabilities, as well\nas their alignment with actual user demands-hindering both targeted capability\ndevelopment and the reliable transition of research progress into practical\ndeployment. To bridge the gap, we present OS-MAP, a benchmark for daily\ncomputer-using automation that organizes its 416 realistic tasks across 15\napplications along two key dimensions: a five-level taxonomy of automation and\na generalization scope derived from a real-world user demand hierarchy. To\nenable fine-grained analysis of required capabilities and alignment with\nreal-world scenarios, OS-MAP evaluates agents along two dimensions: automation\nlevel across a five-level taxonomy, and generalization scope across a demand\nhierarchy. This design captures varying levels of required agent autonomy and\ngeneralization, forming a performance-generalization evaluation matrix for\nstructured and comprehensive assessment. Experiments show that even\nState-of-the-Art agents with VLM backbones struggle with higher-level tasks\ninvolving perception, reasoning, and coordination-highlighting the need for a\ndeeper understanding of current strengths and limitations to drive the future\nprogress in computer-using agents research and deployment. All code,\nenvironments, baselines, and data are publicly available at\nhttps://github.com/OS-Copilot/OS-Map."}
{"id": "2507.19195", "pdf": "https://arxiv.org/pdf/2507.19195.pdf", "abs": "https://arxiv.org/abs/2507.19195", "title": "Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large Language Models?", "authors": ["Chaymaa Abbas", "Mariette Awad", "Razane Tajeddine"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite the ongoing improvements in the design of large language models\n(LLMs) to foster inclusion and balanced responses, these systems remain\nsusceptible to encoding and amplifying social biases. This study examines how\ndialectal variation, specifically African American Vernacular English (AAVE)\nversus Standard American English (SAE), interacts with data poisoning to\ninfluence toxicity in outputs. Using both small- and medium-scale LLaMA models,\nwe show that even minimal exposure to poisoned data significantly increases\ntoxicity for AAVE inputs, while it remains comparatively unaffected for SAE.\nLarger models exhibit a more significant amplification effect which suggests\nheightened susceptibility with scale. To further assess these disparities, we\nemployed GPT-4o as a fairness auditor, which identified harmful stereotypical\npatterns disproportionately tied to AAVE inputs, including portrayals of\naggression, criminality, and intellectual inferiority. These findings\nunderscore the compounding impact of data poisoning and dialectal bias and\nemphasize the need for dialect-aware evaluation, targeted debiasing\ninterventions, and socially responsible training protocols during development."}
{"id": "2507.19156", "pdf": "https://arxiv.org/pdf/2507.19156.pdf", "abs": "https://arxiv.org/abs/2507.19156", "title": "An Empirical Investigation of Gender Stereotype Representation in Large Language Models: The Italian Case", "authors": ["Gioele Giachino", "Marco Rondina", "Antonio Vetrò", "Riccardo Coppola", "Juan Carlos De Martin"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "16 pages, European Conference on Machine Learning and Principles and\n  Practice of Knowledge Discovery in Databases (ECML PKDD 2025) - 5th Workshop\n  on Bias and Fairness in AI (BIAS25)", "summary": "The increasing use of Large Language Models (LLMs) in a large variety of\ndomains has sparked worries about how easily they can perpetuate stereotypes\nand contribute to the generation of biased content. With a focus on gender and\nprofessional bias, this work examines in which manner LLMs shape responses to\nungendered prompts, contributing to biased outputs. This analysis uses a\nstructured experimental method, giving different prompts involving three\ndifferent professional job combinations, which are also characterized by a\nhierarchical relationship. This study uses Italian, a language with extensive\ngrammatical gender differences, to highlight potential limitations in current\nLLMs' ability to generate objective text in non-English languages. Two popular\nLLM-based chatbots are examined, namely OpenAI ChatGPT (gpt-4o-mini) and Google\nGemini (gemini-1.5-flash). Through APIs, we collected a range of 3600\nresponses. The results highlight how content generated by LLMs can perpetuate\nstereotypes. For example, Gemini associated 100% (ChatGPT 97%) of 'she'\npronouns to the 'assistant' rather than the 'manager'. The presence of bias in\nAI-generated text can have significant implications in many fields, such as in\nthe workplaces or in job selections, raising ethical concerns about its use.\nUnderstanding these risks is pivotal to developing mitigation strategies and\nassuring that AI-based systems do not increase social inequalities, but rather\ncontribute to more equitable outcomes. Future research directions include\nexpanding the study to additional chatbots or languages, refining prompt\nengineering methods or further exploiting a larger experimental base."}
{"id": "2507.19219", "pdf": "https://arxiv.org/pdf/2507.19219.pdf", "abs": "https://arxiv.org/abs/2507.19219", "title": "How Much Do Large Language Model Cheat on Evaluation? Benchmarking Overestimation under the One-Time-Pad-Based Framework", "authors": ["Zi Liang", "Liantong Yu", "Shiyu Zhang", "Qingqing Ye", "Haibo Hu"], "categories": ["cs.CL", "cs.CR"], "comment": "Source code: https://github.com/liangzid/ArxivRoll/ Website:\n  https://arxivroll.moreoverai.com/", "summary": "Overestimation in evaluating large language models (LLMs) has become an\nincreasing concern. Due to the contamination of public benchmarks or imbalanced\nmodel training, LLMs may achieve unreal evaluation results on public\nbenchmarks, either intentionally or unintentionally, which leads to unfair\ncomparisons among LLMs and undermines their realistic capability assessments.\nExisting benchmarks attempt to address these issues by keeping test cases\npermanently secret, mitigating contamination through human evaluation, or\nrepeatedly collecting and constructing new samples. However, these approaches\nfail to ensure reproducibility, transparency, and high efficiency\nsimultaneously. Moreover, the extent of overestimation in current LLMs remains\nunquantified. To address these issues, we propose ArxivRoll, a dynamic\nevaluation framework inspired by one-time pad encryption in cryptography.\nArxivRoll comprises two key components: \\emph{i) SCP (Sequencing, Cloze, and\nPrediction)}, an automated generator for private test cases, and \\emph{ii)\nRugged Scores (RS)}, metrics that measure the proportion of public benchmark\ncontamination and training bias. Leveraging SCP, ArxivRoll constructs a new\nbenchmark every six months using recent articles from ArXiv and employs them\nfor one-time evaluations of LLM performance. Extensive experiments demonstrate\nthe high quality of our benchmark, and we provide a systematic evaluation of\ncurrent LLMs. The source code is available at\nhttps://github.com/liangzid/ArxivRoll/."}
{"id": "2507.19196", "pdf": "https://arxiv.org/pdf/2507.19196.pdf", "abs": "https://arxiv.org/abs/2507.19196", "title": "Towards Multimodal Social Conversations with Robots: Using Vision-Language Models", "authors": ["Ruben Janssens", "Tony Belpaeme"], "categories": ["cs.RO", "cs.CL", "cs.HC"], "comment": "Submitted to the workshop \"Human - Foundation Models Interaction: A\n  Focus On Multimodal Information\" (FoMo-HRI) at IEEE RO-MAN 2025", "summary": "Large language models have given social robots the ability to autonomously\nengage in open-domain conversations. However, they are still missing a\nfundamental social skill: making use of the multiple modalities that carry\nsocial interactions. While previous work has focused on task-oriented\ninteractions that require referencing the environment or specific phenomena in\nsocial interactions such as dialogue breakdowns, we outline the overall needs\nof a multimodal system for social conversations with robots. We then argue that\nvision-language models are able to process this wide range of visual\ninformation in a sufficiently general manner for autonomous social robots. We\ndescribe how to adapt them to this setting, which technical challenges remain,\nand briefly discuss evaluation practices."}
{"id": "2507.19227", "pdf": "https://arxiv.org/pdf/2507.19227.pdf", "abs": "https://arxiv.org/abs/2507.19227", "title": "Jailbreaking Large Language Diffusion Models: Revealing Hidden Safety Flaws in Diffusion-Based Text Generation", "authors": ["Yuanhe Zhang", "Fangzhou Xie", "Zhenhong Zhou", "Zherui Li", "Hao Chen", "Kun Wang", "Yufei Guo"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Diffusion Models (LLDMs) exhibit comparable performance to\nLLMs while offering distinct advantages in inference speed and mathematical\nreasoning tasks.The precise and rapid generation capabilities of LLDMs amplify\nconcerns of harmful generations, while existing jailbreak methodologies\ndesigned for Large Language Models (LLMs) prove limited effectiveness against\nLLDMs and fail to expose safety vulnerabilities.Successful defense cannot\ndefinitively resolve harmful generation concerns, as it remains unclear whether\nLLDMs possess safety robustness or existing attacks are incompatible with\ndiffusion-based architectures.To address this, we first reveal the\nvulnerability of LLDMs to jailbreak and demonstrate that attack failure in\nLLDMs stems from fundamental architectural differences.We present a PArallel\nDecoding jailbreak (PAD) for diffusion-based language models. PAD introduces\nMulti-Point Attention Attack, which guides parallel generative processes toward\nharmful outputs that inspired by affirmative response patterns in LLMs.\nExperimental evaluations across four LLDMs demonstrate that PAD achieves\njailbreak attack success rates by 97%, revealing significant safety\nvulnerabilities. Furthermore, compared to autoregressive LLMs of the same size,\nLLDMs increase the harmful generation speed by 2x, significantly highlighting\nrisks of uncontrolled misuse.Through comprehensive analysis, we provide an\ninvestigation into LLDM architecture, offering critical insights for the secure\ndeployment of diffusion-based language models."}
{"id": "2507.19316", "pdf": "https://arxiv.org/pdf/2507.19316.pdf", "abs": "https://arxiv.org/abs/2507.19316", "title": "Human-AI Synergy in Adaptive Active Learning for Continuous Lithium Carbonate Crystallization Optimization", "authors": ["Shayan S. Mousavi Masouleh", "Corey A. Sanz", "Ryan P. Jansonius", "Cara Cronin", "Jason E. Hein", "Jason Hattrick-Simpers"], "categories": ["cond-mat.mtrl-sci", "cond-mat.other", "cs.HC", "cs.LG", "physics.data-an"], "comment": null, "summary": "As demand for high-purity lithium surges with the growth of the electric\nvehicle (EV) industry, cost-effective extraction from lower-grade North\nAmerican sources like the Smackover Formation is critical. These resources,\nunlike high-purity South American brines, require innovative purification\ntechniques to be economically viable. Continuous crystallization is a promising\nmethod for producing battery-grade lithium carbonate, but its optimization is\nchallenged by a complex parameter space and limited data. This study introduces\na Human-in-the-Loop (HITL) assisted active learning framework to optimize the\ncontinuous crystallization of lithium carbonate. By integrating human expertise\nwith data-driven insights, our approach accelerates the optimization of lithium\nextraction from challenging sources. Our results demonstrate the framework's\nability to rapidly adapt to new data, significantly improving the process's\ntolerance to critical impurities like magnesium from the industry standard of a\nfew hundred ppm to as high as 6000 ppm. This breakthrough makes the\nexploitation of low-grade, impurity-rich lithium resources feasible,\npotentially reducing the need for extensive pre-refinement processes. By\nleveraging artificial intelligence, we have refined operational parameters and\ndemonstrated that lower-grade materials can be used without sacrificing product\nquality. This advancement is a significant step towards economically harnessing\nNorth America's vast lithium reserves, such as those in the Smackover\nFormation, and enhancing the sustainability of the global lithium supply chain."}
{"id": "2507.19303", "pdf": "https://arxiv.org/pdf/2507.19303.pdf", "abs": "https://arxiv.org/abs/2507.19303", "title": "Identifying Fine-grained Forms of Populism in Political Discourse: A Case Study on Donald Trump's Presidential Campaigns", "authors": ["Ilias Chalkidis", "Stephanie Brandl", "Paris Aslanidis"], "categories": ["cs.CL"], "comment": "Pre-print", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of instruction-following tasks, yet their grasp of nuanced social\nscience concepts remains underexplored. This paper examines whether LLMs can\nidentify and classify fine-grained forms of populism, a complex and contested\nconcept in both academic and media debates. To this end, we curate and release\nnovel datasets specifically designed to capture populist discourse. We evaluate\na range of pre-trained (large) language models, both open-weight and\nproprietary, across multiple prompting paradigms. Our analysis reveals notable\nvariation in performance, highlighting the limitations of LLMs in detecting\npopulist discourse. We find that a fine-tuned RoBERTa classifier vastly\noutperforms all new-era instruction-tuned LLMs, unless fine-tuned.\nAdditionally, we apply our best-performing model to analyze campaign speeches\nby Donald Trump, extracting valuable insights into his strategic use of\npopulist rhetoric. Finally, we assess the generalizability of these models by\nbenchmarking them on campaign speeches by European politicians, offering a lens\ninto cross-context transferability in political discourse analysis. In this\nsetting, we find that instruction-tuned LLMs exhibit greater robustness on\nout-of-domain data."}
{"id": "2507.19470", "pdf": "https://arxiv.org/pdf/2507.19470.pdf", "abs": "https://arxiv.org/abs/2507.19470", "title": "Conversations Gone Awry, But Then? Evaluating Conversational Forecasting Models", "authors": ["Son Quoc Tran", "Tushaar Gangavarapu", "Nicholas Chernogor", "Jonathan P. Chang", "Cristian Danescu-Niculescu-Mizil"], "categories": ["cs.CL", "cs.HC"], "comment": "Code and data available as part of ConvoKit:\n  https://convokit.cornell.edu", "summary": "We often rely on our intuition to anticipate the direction of a conversation.\nEndowing automated systems with similar foresight can enable them to assist\nhuman-human interactions. Recent work on developing models with this predictive\ncapacity has focused on the Conversations Gone Awry (CGA) task: forecasting\nwhether an ongoing conversation will derail. In this work, we revisit this task\nand introduce the first uniform evaluation framework, creating a benchmark that\nenables direct and reliable comparisons between different architectures. This\nallows us to present an up-to-date overview of the current progress in CGA\nmodels, in light of recent advancements in language modeling. Our framework\nalso introduces a novel metric that captures a model's ability to revise its\nforecast as the conversation progresses."}
{"id": "2507.19315", "pdf": "https://arxiv.org/pdf/2507.19315.pdf", "abs": "https://arxiv.org/abs/2507.19315", "title": "AutoPCR: Automated Phenotype Concept Recognition by Prompting", "authors": ["Yicheng Tao", "Yuanhao Huang", "Jie Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Phenotype concept recognition (CR) is a fundamental task in biomedical text\nmining, enabling applications such as clinical diagnostics and knowledge graph\nconstruction. However, existing methods often require ontology-specific\ntraining and struggle to generalize across diverse text types and evolving\nbiomedical terminology. We present AutoPCR, a prompt-based phenotype CR method\nthat does not require ontology-specific training. AutoPCR performs CR in three\nstages: entity extraction using a hybrid of rule-based and neural tagging\nstrategies, candidate retrieval via SapBERT, and entity linking through\nprompting a large language model. Experiments on four benchmark datasets show\nthat AutoPCR achieves the best average and most robust performance across both\nmention-level and document-level evaluations, surpassing prior state-of-the-art\nmethods. Further ablation and transfer studies demonstrate its inductive\ncapability and generalizability to new ontologies."}
{"id": "2411.04576", "pdf": "https://arxiv.org/pdf/2411.04576.pdf", "abs": "https://arxiv.org/abs/2411.04576", "title": "\"I Always Felt that SomethingWasWrong.\": Understanding Compliance Risks and Mitigation Strategies when Highly-Skilled Compliance Knowledge Workers Use Large Language Models", "authors": ["Siying Hu", "Piaohong Wang", "Ka I Chan", "Yaxing Yao", "Zhicong Lu"], "categories": ["cs.HC"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has transformed\nknowledge-intensive has led to its widespread usage by knowledge workers to\nenhance their productivity. As these professionals handle sensitive\ninformation, and the training of text-based GenAI models involves the use of\nextensive data, there are thus concerns about privacy, security, and broader\ncompliance with regulations and laws. While existing research has addressed\nprivacy and security concerns, the specific compliance risks faced by\nhighly-skilled knowledge workers when using the LLMs, and their mitigation\nstrategies, remain underexplored. As understanding these risks and strategies\nis crucial for the development of industry-specific compliant LLM mechanisms,\nthis research conducted semi-structured interviews with 24 knowledge workers\nfrom knowledge-intensive industries to understand their practices and\nexperiences when integrating LLMs into their workflows. Our research explored\nhow these workers ensure compliance and the resources and challenges they\nencounter when minimizing risks. Our preliminary findings showed that knowledge\nworkers were concerned about the leakage of sensitive information and took\nproactive measures such as distorting input data and limiting prompt details to\nmitigate such risks. Their ability to identify and mitigate risks, however, was\nsignificantly hampered by a lack of LLM-specific compliance guidance and\ntraining. Our findings highlight the importance of improving knowledge workers'\ncompliance awareness and establishing support systems and compliance cultures\nwithin organizations."}
{"id": "2507.19353", "pdf": "https://arxiv.org/pdf/2507.19353.pdf", "abs": "https://arxiv.org/abs/2507.19353", "title": "Smooth Reading: Bridging the Gap of Recurrent LLM to Self-Attention LLM on Long-Context Tasks", "authors": ["Kai Liu", "Zhan Su", "Peijie Dong", "Fengran Mo", "Jianfei Gao", "ShaoTing Zhang", "Kai Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, recurrent large language models (Recurrent LLMs) with linear\ncomputational complexity have re-emerged as efficient alternatives to\nself-attention-based LLMs (Self-Attention LLMs), which have quadratic\ncomplexity. However, Recurrent LLMs often underperform on long-context tasks\ndue to their limited fixed-size memory. Previous research has primarily focused\non enhancing the memory capacity of Recurrent LLMs through architectural\ninnovations, but these approaches have not yet enabled Recurrent LLMs to match\nthe performance of Self-Attention LLMs on long-context tasks. We argue that\nthis limitation arises because processing the entire context at once is not\nwell-suited for Recurrent LLMs. In this paper, we propose Smooth Reading, a\nchunk-wise inference method inspired by human reading strategies. Smooth\nReading processes context in chunks and iteratively summarizes the contextual\ninformation, thereby reducing memory demands and making the approach more\ncompatible with Recurrent LLMs. Our experimental results show that this method\nsubstantially narrows the performance gap between Recurrent and Self-Attention\nLLMs on long-context tasks, while preserving the efficiency advantages of\nRecurrent LLMs. Our Smooth Reading boosts SWA-3B-4k (a Recurrent LLM) from\n5.68% lower to 3.61% higher performance than Self-Attention LLMs on LongBench.\nBesides, our method maintains the high efficiency, training 3x faster and\ninferring 2x faster at 64k context compared to Self-Attention LLMs. To our\nknowledge, this is the first work to achieve comparable performance using\nRecurrent LLMs compared with Self-Attention LLMs on long-context tasks. We hope\nour method will inspire future research in this area. To facilitate further\nprogress, we will release code and dataset."}
{"id": "2504.01293", "pdf": "https://arxiv.org/pdf/2504.01293.pdf", "abs": "https://arxiv.org/abs/2504.01293", "title": "Cuddle-Fish: Exploring a Soft Floating Robot with Flapping Wings for Physical Interactions", "authors": ["Mingyang Xu", "Jiayi Shao", "Yulan Ju", "Ximing Shen", "Qingyuan Gao", "Weijen Chen", "Qing Zhang", "Yun Suen Pai", "Giulia Barbareschi", "Matthias Hoppe", "Kouta Minamizawa", "Kai Kunze"], "categories": ["cs.HC", "cs.RO"], "comment": "Augmented Humans International Conference 2025 (AHs '25)", "summary": "Flying robots, such as quadrotor drones, offer new possibilities for\nhuman-robot interaction but often pose safety risks due to fast-spinning\npropellers, rigid structures, and noise. In contrast, lighter-than-air\nflapping-wing robots, inspired by animal movement, offer a soft, quiet, and\ntouch-safe alternative. Building on these advantages, we present Cuddle-Fish, a\nsoft flapping-wing floating robot designed for close-proximity interactions in\nindoor spaces. Through a user study with 24 participants, we explored their\nperceptions of the robot and experiences during a series of co-located\ndemonstrations in which the robot moved near them. Results showed that\nparticipants felt safe, willingly engaged in touch-based interactions with the\nrobot, and exhibited spontaneous affective behaviours, such as patting,\nstroking, hugging, and cheek-touching, without external prompting. They also\nreported positive emotional responses towards the robot. These findings suggest\nthat the soft floating robot with flapping wings can serve as a novel and\nsocially acceptable alternative to traditional rigid flying robots, opening new\npotential for applications in companionship, affective interaction, and play in\neveryday indoor environments."}
{"id": "2507.19356", "pdf": "https://arxiv.org/pdf/2507.19356.pdf", "abs": "https://arxiv.org/abs/2507.19356", "title": "Enhancing Speech Emotion Recognition Leveraging Aligning Timestamps of ASR Transcripts and Speaker Diarization", "authors": ["Hsuan-Yu Wang", "Pei-Ying Lee", "Berlin Chen"], "categories": ["cs.CL", "I.2.7; I.5.1"], "comment": "6 pages, 3 figures, to appear in the Proceedings of the 2025\n  International Conference on Asian Language Processing (IALP)", "summary": "In this paper, we investigate the impact of incorporating timestamp-based\nalignment between Automatic Speech Recognition (ASR) transcripts and Speaker\nDiarization (SD) outputs on Speech Emotion Recognition (SER) accuracy.\nMisalignment between these two modalities often reduces the reliability of\nmultimodal emotion recognition systems, particularly in conversational\ncontexts. To address this issue, we introduce an alignment pipeline utilizing\npre-trained ASR and speaker diarization models, systematically synchronizing\ntimestamps to generate accurately labeled speaker segments. Our multimodal\napproach combines textual embeddings extracted via RoBERTa with audio\nembeddings from Wav2Vec, leveraging cross-attention fusion enhanced by a gating\nmechanism. Experimental evaluations on the IEMOCAP benchmark dataset\ndemonstrate that precise timestamp alignment improves SER accuracy,\noutperforming baseline methods that lack synchronization. The results highlight\nthe critical importance of temporal alignment, demonstrating its effectiveness\nin enhancing overall emotion recognition accuracy and providing a foundation\nfor robust multimodal emotion analysis."}
{"id": "2506.00717", "pdf": "https://arxiv.org/pdf/2506.00717.pdf", "abs": "https://arxiv.org/abs/2506.00717", "title": "Vid2Coach: Transforming How-To Videos into Task Assistants", "authors": ["Mina Huh", "Zihui Xue", "Ujjaini Das", "Kumar Ashutosh", "Kristen Grauman", "Amy Pavel"], "categories": ["cs.HC", "cs.CV"], "comment": "Accepted to UIST 2025 Project website: https://minahuh.com/Vid2Coach/", "summary": "People use videos to learn new recipes, exercises, and crafts. Such videos\nremain difficult for blind and low vision (BLV) people to follow as they rely\non visual comparison. Our observations of visual rehabilitation therapists\n(VRTs) guiding BLV people to follow how-to videos revealed that VRTs provide\nboth proactive and responsive support including detailed descriptions,\nnon-visual workarounds, and progress feedback. We propose Vid2Coach, a system\nthat transforms how-to videos into wearable camera-based assistants that\nprovide accessible instructions and mixed-initiative feedback. From the video,\nVid2Coach generates accessible instructions by augmenting narrated instructions\nwith demonstration details and completion criteria for each step. It then uses\nretrieval-augmented-generation to extract relevant non-visual workarounds from\nBLV-specific resources. Vid2Coach then monitors user progress with a camera\nembedded in commercial smart glasses to provide context-aware instructions,\nproactive feedback, and answers to user questions. BLV participants (N=8) using\nVid2Coach completed cooking tasks with 58.5\\% fewer errors than when using\ntheir typical workflow and wanted to use Vid2Coach in their daily lives.\nVid2Coach demonstrates an opportunity for AI visual assistance that strengthens\nrather than replaces non-visual expertise."}
{"id": "2507.19361", "pdf": "https://arxiv.org/pdf/2507.19361.pdf", "abs": "https://arxiv.org/abs/2507.19361", "title": "SpeechIQ: Speech Intelligence Quotient Across Cognitive Levels in Voice Understanding Large Language Models", "authors": ["Zhen Wan", "Chao-Han Huck Yang", "Yahan Yu", "Jinchuan Tian", "Sheng Li", "Ke Hu", "Zhehuai Chen", "Shinji Watanabe", "Fei Cheng", "Chenhui Chu", "Sadao Kurohashi"], "categories": ["cs.CL", "cs.AI", "cs.SC", "cs.SD", "eess.AS"], "comment": "Our Speech-IQ leaderboard will be hosted at\n  huggingface.co/spaces/nvidia/Speech-IQ-leaderboard. ACL 2025 main", "summary": "We introduce Speech-based Intelligence Quotient (SIQ) as a new form of human\ncognition-inspired evaluation pipeline for voice understanding large language\nmodels, LLM Voice, designed to assess their voice understanding ability. Moving\nbeyond popular voice understanding metrics such as word error rate (WER), SIQ\nexamines LLM Voice across three cognitive levels motivated by Bloom's Taxonomy:\n(1) Remembering (i.e., WER for verbatim accuracy); (2) Understanding (i.e.,\nsimilarity of LLM's interpretations); and (3) Application (i.e., QA accuracy\nfor simulating downstream tasks). We demonstrate that SIQ not only quantifies\nvoice understanding abilities but also provides unified comparisons between\ncascaded methods (e.g., ASR LLM) and end-to-end models, identifies annotation\nerrors in existing benchmarks, and detects hallucinations in LLM Voice. Our\nframework represents a first-of-its-kind intelligence examination that bridges\ncognitive principles with voice-oriented benchmarks, while exposing overlooked\nchallenges in multi-modal training."}
{"id": "2506.11366", "pdf": "https://arxiv.org/pdf/2506.11366.pdf", "abs": "https://arxiv.org/abs/2506.11366", "title": "Meeting Patients Where They're At: Toward the Expansion of Chaplaincy Care into Online Spiritual Care Communities", "authors": ["Alemitu Bezabih", "Shadi Nourriz", "Anne-Marie Snider", "Rosalie Rauenzahn", "George Handzo", "C. Estelle Smith"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Despite a growing need for spiritual care in the US, it is often\nunder-served, inaccessible, or misunderstood, while almost no prior work in\nCSCW/HCI research has engaged with professional chaplains and spiritual care\nproviders. This interdisciplinary study aims to develop a foundational\nunderstanding of how spiritual care may (or may not) be expanded into online\nspaces -- especially focusing on anonymous, asynchronous, and text-based online\ncommunities. We conducted an exploratory mixed-methods study with chaplains\n(N=22) involving interviews and user testing sessions centered around Reddit\nsupport communities to understand participants' perspectives on technology and\ntheir ideations about the role of chaplaincy in prospective Online Spiritual\nCare Communities (OSCCs). Our Grounded Theory Method analysis highlighted\nbenefits of OSCCs including: meeting patients where they are at; accessibility\nand scalability; and facilitating patient-initiated care. Chaplains highlighted\nhow their presence in OSCCs could help with shaping peer interactions,\nmoderation, synchronous chats for group care, and redirecting to external\nresources, while also raising important feasibility concerns, risks, and needs\nfor future design and research. We used an existing taxonomy of chaplaincy\ntechniques to show that some spiritual care strategies may be amenable to\nonline spaces, yet we also exposed the limitations of technology to fully\nmediate spiritual care and the need to develop new online chaplaincy\ninterventions. Based on these findings, we contribute the model of a ``Care\nLoop'' between institutionally-based formal care and platform-based community\ncare to expand access and drive greater awareness and utilization of spiritual\ncare. We also contribute design implications to guide future work in online\nspiritual care."}
{"id": "2507.19374", "pdf": "https://arxiv.org/pdf/2507.19374.pdf", "abs": "https://arxiv.org/abs/2507.19374", "title": "Data Augmentation for Spoken Grammatical Error Correction", "authors": ["Penny Karanasou", "Mengjie Qian", "Stefano Bannò", "Mark J. F. Gales", "Kate M. Knill"], "categories": ["cs.CL", "cs.AI"], "comment": "This work has been accepted by ISCA SLaTE 2025", "summary": "While there exist strong benchmark datasets for grammatical error correction\n(GEC), high-quality annotated spoken datasets for Spoken GEC (SGEC) are still\nunder-resourced. In this paper, we propose a fully automated method to generate\naudio-text pairs with grammatical errors and disfluencies. Moreover, we propose\na series of objective metrics that can be used to evaluate the generated data\nand choose the more suitable dataset for SGEC. The goal is to generate an\naugmented dataset that maintains the textual and acoustic characteristics of\nthe original data while providing new types of errors. This augmented dataset\nshould augment and enrich the original corpus without altering the language\nassessment scores of the second language (L2) learners. We evaluate the use of\nthe augmented corpus both for written GEC (the text part) and for SGEC (the\naudio-text pairs). Our experiments are conducted on the S\\&I Corpus, the first\npublicly available speech dataset with grammar error annotations."}
{"id": "2507.17543", "pdf": "https://arxiv.org/pdf/2507.17543.pdf", "abs": "https://arxiv.org/abs/2507.17543", "title": "Anticipate, Simulate, Reason (ASR): A Comprehensive Generative AI Framework for Combating Messaging Scams", "authors": ["Xue Wen Tan", "Kenneth See", "Stanley Kok"], "categories": ["cs.HC"], "comment": "arXiv admin note: text overlap with arXiv:2412.13528", "summary": "The rapid growth of messaging scams creates an escalating challenge for user\nsecurity and financial safety. In this paper, we present the\n\\textit{Anticipate, Simulate, Reason} (ASR) generative AI framework to enable\nusers to proactively identify and comprehend scams within instant messaging\nplatforms. Using large language models, ASR predicts scammer responses and\ndelivers real-time, interpretable support to end-users. We also develop\nScamGPT-J, a domain-specific language model fine-tuned on a new, high-quality\ndataset of scam conversations covering multiple scam types. Thorough\nexperimental evaluation shows that the ASR framework substantially enhances\nscam detection, particularly in challenging contexts such as job scams, and\nuncovers important demographic patterns in user vulnerability and perceptions\nof AI-generated assistance. Our findings reveal a contradiction where those\nmost at risk are often least receptive to AI support, emphasizing the\nimportance of user-centered design in AI-driven fraud prevention. This work\nadvances both the practical and theoretical foundations for interpretable and\nhuman-centered AI systems in combating evolving digital threats."}
{"id": "2507.19396", "pdf": "https://arxiv.org/pdf/2507.19396.pdf", "abs": "https://arxiv.org/abs/2507.19396", "title": "Detection of Adverse Drug Events in Dutch clinical free text documents using Transformer Models: benchmark study", "authors": ["Rachel M. Murphy", "Nishant Mishra", "Nicolette F. de Keizer", "Dave A. Dongelmans", "Kitty J. Jager", "Ameen Abu-Hanna", "Joanna E. Klopotowska", "Iacer Calixto"], "categories": ["cs.CL"], "comment": "30 Pages, 5 Figures (Main Paper), 19 Pages, 2 Figures(Supplements).\n  Rachel M. Murphy and Nishant Mishra are shared first authors. Joanna E.\n  Klopotowska and Iacer Calixto are shared last authors", "summary": "In this study, we set a benchmark for adverse drug event (ADE) detection in\nDutch clinical free text documents using several transformer models, clinical\nscenarios and fit-for-purpose performance measures. We trained a Bidirectional\nLong Short-Term Memory (Bi-LSTM) model and four transformer-based Dutch and/or\nmultilingual encoder models (BERTje, RobBERT, MedRoBERTa.nl, and NuNER) for the\ntasks of named entity recognition (NER) and relation classification (RC) using\n102 richly annotated Dutch ICU clinical progress notes. Anonymized free text\nclinical progress notes of patients admitted to intensive care unit (ICU) of\none academic hospital and discharge letters of patients admitted to Internal\nMedicine wards of two non-academic hospitals were reused. We evaluated our ADE\nRC models internally using gold standard (two-step task) and predicted entities\n(end-to-end task). In addition, all models were externally validated on\ndetecting ADEs at the document level. We report both micro- and macro-averaged\nF1 scores, given the imbalance of ADEs in the datasets. Although differences\nfor the ADE RC task between the models were small, MedRoBERTa.nl was the best\nperforming model with macro-averaged F1 score of 0.63 using gold standard and\n0.62 using predicted entities. The MedRoBERTa.nl models also performed the best\nin our external validation and achieved recall of between 0.67 to 0.74 using\npredicted entities, meaning between 67 to 74% of discharge letters with ADEs\nwere detected. Our benchmark study presents a robust and clinically meaningful\napproach for evaluating language models for ADE detection in clinical free text\ndocuments. Our study highlights the need to use appropriate performance\nmeasures fit for the task of ADE detection in clinical free-text documents and\nenvisioned future clinical use."}
{"id": "2507.09089", "pdf": "https://arxiv.org/pdf/2507.09089.pdf", "abs": "https://arxiv.org/abs/2507.09089", "title": "Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity", "authors": ["Joel Becker", "Nate Rush", "Elizabeth Barnes", "David Rein"], "categories": ["cs.AI", "cs.HC", "cs.SE", "I.2"], "comment": "51 pages, 8 tables, 22 figures", "summary": "Despite widespread adoption, the impact of AI tools on software development\nin the wild remains understudied. We conduct a randomized controlled trial\n(RCT) to understand how AI tools at the February-June 2025 frontier affect the\nproductivity of experienced open-source developers. 16 developers with moderate\nAI experience complete 246 tasks in mature projects on which they have an\naverage of 5 years of prior experience. Each task is randomly assigned to allow\nor disallow usage of early 2025 AI tools. When AI tools are allowed, developers\nprimarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.\nBefore starting tasks, developers forecast that allowing AI will reduce\ncompletion time by 24%. After completing the study, developers estimate that\nallowing AI reduced completion time by 20%. Surprisingly, we find that allowing\nAI actually increases completion time by 19%--AI tooling slowed developers\ndown. This slowdown also contradicts predictions from experts in economics (39%\nshorter) and ML (38% shorter). To understand this result, we collect and\nevaluate evidence for 20 properties of our setting that a priori could\ncontribute to the observed slowdown effect--for example, the size and quality\nstandards of projects, or prior developer experience with AI tooling. Although\nthe influence of experimental artifacts cannot be entirely ruled out, the\nrobustness of the slowdown effect across our analyses suggests it is unlikely\nto primarily be a function of our experimental design."}
{"id": "2507.19407", "pdf": "https://arxiv.org/pdf/2507.19407.pdf", "abs": "https://arxiv.org/abs/2507.19407", "title": "Towards Domain Specification of Embedding Models in Medicine", "authors": ["Mohammad Khodadad", "Ali Shiraee", "Mahdi Astaraki", "Hamidreza Mahyar"], "categories": ["cs.CL"], "comment": null, "summary": "Medical text embedding models are foundational to a wide array of healthcare\napplications, ranging from clinical decision support and biomedical information\nretrieval to medical question answering, yet they remain hampered by two\ncritical shortcomings. First, most models are trained on a narrow slice of\nmedical and biological data, beside not being up to date in terms of\nmethodology, making them ill suited to capture the diversity of terminology and\nsemantics encountered in practice. Second, existing evaluations are often\ninadequate: even widely used benchmarks fail to generalize across the full\nspectrum of real world medical tasks.\n  To address these gaps, we leverage MEDTE, a GTE model extensively fine-tuned\non diverse medical corpora through self-supervised contrastive learning across\nmultiple data sources, to deliver robust medical text embeddings.\n  Alongside this model, we propose a comprehensive benchmark suite of 51 tasks\nspanning classification, clustering, pair classification, and retrieval modeled\non the Massive Text Embedding Benchmark (MTEB) but tailored to the nuances of\nmedical text. Our results demonstrate that this combined approach not only\nestablishes a robust evaluation framework but also yields embeddings that\nconsistently outperform state of the art alternatives in different tasks."}
{"id": "2507.18262", "pdf": "https://arxiv.org/pdf/2507.18262.pdf", "abs": "https://arxiv.org/abs/2507.18262", "title": "ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic Grounding for Generalizable Robotic Manipulation", "authors": ["Chenyu Su", "Weiwei Shang", "Chen Qian", "Fei Zhang", "Shuang Cong"], "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "12 pages,9 figures", "summary": "Semantics-driven 3D spatial constraints align highlevel semantic\nrepresentations with low-level action spaces, facilitating the unification of\ntask understanding and execution in robotic manipulation. The synergistic\nreasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation\nModels (VFMs) enables cross-modal 3D spatial constraint construction.\nNevertheless, existing methods have three key limitations: (1) coarse semantic\ngranularity in constraint modeling, (2) lack of real-time closed-loop planning,\n(3) compromised robustness in semantically diverse environments. To address\nthese challenges, we propose ReSem3D, a unified manipulation framework for\nsemantically diverse environments, leveraging the synergy between VFMs and\nMLLMs to achieve fine-grained visual grounding and dynamically constructs\nhierarchical 3D spatial constraints for real-time manipulation. Specifically,\nthe framework is driven by hierarchical recursive reasoning in MLLMs, which\ninteract with VFMs to automatically construct 3D spatial constraints from\nnatural language instructions and RGB-D observations in two stages: part-level\nextraction and region-level refinement. Subsequently, these constraints are\nencoded as real-time optimization objectives in joint space, enabling reactive\nbehavior to dynamic disturbances. Extensive simulation and real-world\nexperiments are conducted in semantically rich household and sparse chemical\nlab environments. The results demonstrate that ReSem3D performs diverse\nmanipulation tasks under zero-shot conditions, exhibiting strong adaptability\nand generalization. Code and videos are available at\nhttps://github.com/scy-v/ReSem3D and https://resem3d.github.io."}
{"id": "2507.19419", "pdf": "https://arxiv.org/pdf/2507.19419.pdf", "abs": "https://arxiv.org/abs/2507.19419", "title": "TokenSmith: Streamlining Data Editing, Search, and Inspection for Large-Scale Language Model Training and Interpretability", "authors": ["Mohammad Aflah Khan", "Ameya Godbole", "Johnny Tian-Zheng Wei", "Ryan Wang", "James Flemings", "Krishna Gummadi", "Willie Neiswanger", "Robin Jia"], "categories": ["cs.CL"], "comment": null, "summary": "Understanding the relationship between training data and model behavior\nduring pretraining is crucial, but existing workflows make this process\ncumbersome, fragmented, and often inaccessible to researchers. We present\nTokenSmith, an open-source library for interactive editing, inspection, and\nanalysis of datasets used in Megatron-style pretraining frameworks such as\nGPT-NeoX, Megatron, and NVIDIA NeMo. TokenSmith supports a wide range of\noperations including searching, viewing, ingesting, exporting, inspecting, and\nsampling data, all accessible through a simple user interface and a modular\nbackend. It also enables structured editing of pretraining data without\nrequiring changes to training code, simplifying dataset debugging, validation,\nand experimentation.\n  TokenSmith is designed as a plug and play addition to existing large language\nmodel pretraining workflows, thereby democratizing access to production-grade\ndataset tooling. TokenSmith is hosted on GitHub1, with accompanying\ndocumentation and tutorials. A demonstration video is also available on\nYouTube."}
{"id": "2507.19457", "pdf": "https://arxiv.org/pdf/2507.19457.pdf", "abs": "https://arxiv.org/abs/2507.19457", "title": "GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning", "authors": ["Lakshya A Agrawal", "Shangyin Tan", "Dilara Soylu", "Noah Ziems", "Rishi Khare", "Krista Opsahl-Ong", "Arnav Singhvi", "Herumb Shandilya", "Michael J Ryan", "Meng Jiang", "Christopher Potts", "Koushik Sen", "Alexandros G. Dimakis", "Ion Stoica", "Dan Klein", "Matei Zaharia", "Omar Khattab"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE", "I.2.7; I.2.6; I.2.4; I.2.8"], "comment": null, "summary": "Large language models (LLMs) are increasingly adapted to downstream tasks via\nreinforcement learning (RL) methods like Group Relative Policy Optimization\n(GRPO), which often require thousands of rollouts to learn new tasks. We argue\nthat the interpretable nature of language can often provide a much richer\nlearning medium for LLMs, compared with policy gradients derived from sparse,\nscalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt\noptimizer that thoroughly incorporates natural language reflection to learn\nhigh-level rules from trial and error. Given any AI system containing one or\nmore LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool\ncalls, and tool outputs) and reflects on them in natural language to diagnose\nproblems, propose and test prompt updates, and combine complementary lessons\nfrom the Pareto frontier of its own attempts. As a result of GEPA's design, it\ncan often turn even just a few rollouts into a large quality gain. Across four\ntasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up\nto 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer,\nMIPROv2, by over 10% across two LLMs, and demonstrates promising results as an\ninference-time search strategy for code optimization."}
{"id": "2507.19470", "pdf": "https://arxiv.org/pdf/2507.19470.pdf", "abs": "https://arxiv.org/abs/2507.19470", "title": "Conversations Gone Awry, But Then? Evaluating Conversational Forecasting Models", "authors": ["Son Quoc Tran", "Tushaar Gangavarapu", "Nicholas Chernogor", "Jonathan P. Chang", "Cristian Danescu-Niculescu-Mizil"], "categories": ["cs.CL", "cs.HC"], "comment": "Code and data available as part of ConvoKit:\n  https://convokit.cornell.edu", "summary": "We often rely on our intuition to anticipate the direction of a conversation.\nEndowing automated systems with similar foresight can enable them to assist\nhuman-human interactions. Recent work on developing models with this predictive\ncapacity has focused on the Conversations Gone Awry (CGA) task: forecasting\nwhether an ongoing conversation will derail. In this work, we revisit this task\nand introduce the first uniform evaluation framework, creating a benchmark that\nenables direct and reliable comparisons between different architectures. This\nallows us to present an up-to-date overview of the current progress in CGA\nmodels, in light of recent advancements in language modeling. Our framework\nalso introduces a novel metric that captures a model's ability to revise its\nforecast as the conversation progresses."}
{"id": "2507.18639", "pdf": "https://arxiv.org/pdf/2507.18639.pdf", "abs": "https://arxiv.org/abs/2507.18639", "title": "People Are Highly Cooperative with Large Language Models, Especially When Communication Is Possible or Following Human Interaction", "authors": ["Paweł Niszczota", "Tomasz Grzegorczyk", "Alexander Pastukhov"], "categories": ["cs.HC", "cs.CL", "cs.CY", "econ.GN", "q-fin.EC", "I.2.7; H.5.2; H.5.3; K.4.3"], "comment": null, "summary": "Machines driven by large language models (LLMs) have the potential to augment\nhumans across various tasks, a development with profound implications for\nbusiness settings where effective communication, collaboration, and stakeholder\ntrust are paramount. To explore how interacting with an LLM instead of a human\nmight shift cooperative behavior in such settings, we used the Prisoner's\nDilemma game -- a surrogate of several real-world managerial and economic\nscenarios. In Experiment 1 (N=100), participants engaged in a thirty-round\nrepeated game against a human, a classic bot, and an LLM (GPT, in real-time).\nIn Experiment 2 (N=192), participants played a one-shot game against a human or\nan LLM, with half of them allowed to communicate with their opponent, enabling\nLLMs to leverage a key advantage over older-generation machines. Cooperation\nrates with LLMs -- while lower by approximately 10-15 percentage points\ncompared to interactions with human opponents -- were nonetheless high. This\nfinding was particularly notable in Experiment 2, where the psychological cost\nof selfish behavior was reduced. Although allowing communication about\ncooperation did not close the human-machine behavioral gap, it increased the\nlikelihood of cooperation with both humans and LLMs equally (by 88%), which is\nparticularly surprising for LLMs given their non-human nature and the\nassumption that people might be less receptive to cooperating with machines\ncompared to human counterparts. Additionally, cooperation with LLMs was higher\nfollowing prior interaction with humans, suggesting a spillover effect in\ncooperative behavior. Our findings validate the (careful) use of LLMs by\nbusinesses in settings that have a cooperative component."}
{"id": "2507.18945", "pdf": "https://arxiv.org/pdf/2507.18945.pdf", "abs": "https://arxiv.org/abs/2507.18945", "title": "TreeReader: A Hierarchical Academic Paper Reader Powered by Language Models", "authors": ["Zijian Zhang", "Pan Chen", "Fangshi Du", "Runlong Ye", "Oliver Huang", "Michael Liut", "Alán Aspuru-Guzik"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Efficiently navigating and understanding academic papers is crucial for\nscientific progress. Traditional linear formats like PDF and HTML can cause\ncognitive overload and obscure a paper's hierarchical structure, making it\ndifficult to locate key information. While LLM-based chatbots offer\nsummarization, they often lack nuanced understanding of specific sections, may\nproduce unreliable information, and typically discard the document's\nnavigational structure. Drawing insights from a formative study on academic\nreading practices, we introduce TreeReader, a novel language model-augmented\npaper reader. TreeReader decomposes papers into an interactive tree structure\nwhere each section is initially represented by an LLM-generated concise\nsummary, with underlying details accessible on demand. This design allows users\nto quickly grasp core ideas, selectively explore sections of interest, and\nverify summaries against the source text. A user study was conducted to\nevaluate TreeReader's impact on reading efficiency and comprehension.\nTreeReader provides a more focused and efficient way to navigate and understand\ncomplex academic literature by bridging hierarchical summarization with\ninteractive exploration."}
{"id": "2507.18949", "pdf": "https://arxiv.org/pdf/2507.18949.pdf", "abs": "https://arxiv.org/abs/2507.18949", "title": "Adaptive Learning Systems: Personalized Curriculum Design Using LLM-Powered Analytics", "authors": ["Yongjie Li", "Ruilin Nong", "Jianan Liu", "Lucas Evans"], "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) are revolutionizing the field of education by\nenabling personalized learning experiences tailored to individual student\nneeds. In this paper, we introduce a framework for Adaptive Learning Systems\nthat leverages LLM-powered analytics for personalized curriculum design. This\ninnovative approach uses advanced machine learning to analyze real-time data,\nallowing the system to adapt learning pathways and recommend resources that\nalign with each learner's progress. By continuously assessing students, our\nframework enhances instructional strategies, ensuring that the materials\npresented are relevant and engaging. Experimental results indicate a marked\nimprovement in both learner engagement and knowledge retention when using a\ncustomized curriculum. Evaluations conducted across varied educational\nenvironments demonstrate the framework's flexibility and positive influence on\nlearning outcomes, potentially reshaping conventional educational practices\ninto a more adaptive and student-centered model."}
{"id": "2507.19037", "pdf": "https://arxiv.org/pdf/2507.19037.pdf", "abs": "https://arxiv.org/abs/2507.19037", "title": "MLLM-based Speech Recognition: When and How is Multimodality Beneficial?", "authors": ["Yiwen Guan", "Viet Anh Trinh", "Vivek Voleti", "Jacob Whitehill"], "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "comment": null, "summary": "Recent advances in multi-modal large language models (MLLMs) have opened new\npossibilities for unified modeling of speech, text, images, and other\nmodalities. Building on our prior work, this paper examines the conditions and\nmodel architectures under which multiple input modalities can improve automatic\nspeech recognition (ASR) accuracy in noisy environments. Through experiments on\nsynthetic and real-world data, we find that (1) harnessing more modalities\nusually improves ASR accuracy, as each modality provides complementary\ninformation, but the improvement depends on the amount of auditory noise. (2)\nSynchronized modalities (e.g., lip movements) are more useful at high noise\nlevels whereas unsynchronized modalities (e.g., image context) are most helpful\nat moderate noise levels. (3) Higher-quality visual representations\nconsistently improve ASR accuracy, highlighting the importance of developing\nmore powerful visual encoders. (4) Mamba exhibits similar trends regarding the\nbenefits of multimodality as do Transformers. (5) The input order of modalities\nas well as their weights in the loss function can significantly impact\naccuracy. These findings both offer practical insights and help to deepen our\nunderstanding of multi-modal speech recognition under challenging conditions."}
{"id": "2507.19040", "pdf": "https://arxiv.org/pdf/2507.19040.pdf", "abs": "https://arxiv.org/abs/2507.19040", "title": "FD-Bench: A Full-Duplex Benchmarking Pipeline Designed for Full Duplex Spoken Dialogue Systems", "authors": ["Yizhou Peng", "Yi-Wen Chao", "Dianwen Ng", "Yukun Ma", "Chongjia Ni", "Bin Ma", "Eng Siong Chng"], "categories": ["eess.AS", "cs.CL"], "comment": "Accepted to Interspeech 2025. 5 pages", "summary": "Full-duplex spoken dialogue systems (FDSDS) enable more natural human-machine\ninteractions by allowing real-time user interruptions and backchanneling,\ncompared to traditional SDS that rely on turn-taking. However, existing\nbenchmarks lack metrics for FD scenes, e.g., evaluating model performance\nduring user interruptions. In this paper, we present a comprehensive FD\nbenchmarking pipeline utilizing LLMs, TTS, and ASR to address this gap. It\nassesses FDSDS's ability to handle user interruptions, manage delays, and\nmaintain robustness in challenging scenarios with diverse novel metrics. We\napplied our benchmark to three open-source FDSDS (Moshi, Freeze-omni, and\nVITA-1.5) using over 40 hours of generated speech, with 293 simulated\nconversations and 1,200 interruptions. The results show that all models\ncontinue to face challenges, such as failing to respond to user interruptions,\nunder frequent disruptions and noisy conditions. Demonstrations, data, and code\nwill be released."}
{"id": "2507.19054", "pdf": "https://arxiv.org/pdf/2507.19054.pdf", "abs": "https://arxiv.org/abs/2507.19054", "title": "Closing the Modality Gap for Mixed Modality Search", "authors": ["Binxu Li", "Yuhui Zhang", "Xiaohan Wang", "Weixin Liang", "Ludwig Schmidt", "Serena Yeung-Levy"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": "Project page: https://yuhui-zh15.github.io/MixedModalitySearch/", "summary": "Mixed modality search -- retrieving information across a heterogeneous corpus\ncomposed of images, texts, and multimodal documents -- is an important yet\nunderexplored real-world application. In this work, we investigate how\ncontrastive vision-language models, such as CLIP, perform on the mixed modality\nsearch task. Our analysis reveals a critical limitation: these models exhibit a\npronounced modality gap in the embedding space, where image and text embeddings\nform distinct clusters, leading to intra-modal ranking bias and inter-modal\nfusion failure. To address this issue, we propose GR-CLIP, a lightweight\npost-hoc calibration method that removes the modality gap in CLIP's embedding\nspace. Evaluated on MixBench -- the first benchmark specifically designed for\nmixed modality search -- GR-CLIP improves NDCG@10 by up to 26 percentage points\nover CLIP, surpasses recent vision-language generative embedding models by 4\npercentage points, while using 75x less compute."}
{"id": "2507.19060", "pdf": "https://arxiv.org/pdf/2507.19060.pdf", "abs": "https://arxiv.org/abs/2507.19060", "title": "PurpCode: Reasoning for Safer Code Generation", "authors": ["Jiawei Liu", "Nirav Diwan", "Zhe Wang", "Haoyu Zhai", "Xiaona Zhou", "Kiet A. Nguyen", "Tianjiao Yu", "Muntasir Wahed", "Yinlin Deng", "Hadjer Benkraouda", "Yuxiang Wei", "Lingming Zhang", "Ismini Lourentzou", "Gang Wang"], "categories": ["cs.CR", "cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "We introduce PurpCode, the first post-training recipe for training safe code\nreasoning models towards generating secure code and defending against malicious\ncyberactivities. PurpCode trains a reasoning model in two stages: (i) Rule\nLearning, which explicitly teaches the model to reference cybersafety rules to\ngenerate vulnerability-free code and to avoid facilitating malicious\ncyberactivities; and (ii) Reinforcement Learning, which optimizes model safety\nand preserves model utility through diverse, multi-objective reward mechanisms.\nTo empower the training pipelines with comprehensive cybersafety data, we\nconduct internal red-teaming to synthesize comprehensive and high-coverage\nprompts based on real-world tasks for inducing unsafe cyberactivities in the\nmodel. Based on PurpCode, we develop a reasoning-based coding model, namely\nPurpCode-32B, which demonstrates state-of-the-art cybersafety, outperforming\nvarious frontier models. Meanwhile, our alignment method decreases the model\noverrefusal rates in both general and cybersafety-specific scenarios, while\npreserving model utility in both code generation and common security knowledge."}
{"id": "2507.19102", "pdf": "https://arxiv.org/pdf/2507.19102.pdf", "abs": "https://arxiv.org/abs/2507.19102", "title": "Distilling a Small Utility-Based Passage Selector to Enhance Retrieval-Augmented Generation", "authors": ["Hengran Zhang", "Keping Bi", "Jiafeng Guo", "Jiaming Zhang", "Shuaiqiang Wang", "Dawei Yin", "Xueqi Cheng"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": "9 pages, 5 figures", "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating retrieved information. Standard retrieval process prioritized\nrelevance, focusing on topical alignment between queries and passages. In\ncontrast, in RAG, the emphasis has shifted to utility, which considers the\nusefulness of passages for generating accurate answers. Despite empirical\nevidence showing the benefits of utility-based retrieval in RAG, the high\ncomputational cost of using LLMs for utility judgments limits the number of\npassages evaluated. This restriction is problematic for complex queries\nrequiring extensive information. To address this, we propose a method to\ndistill the utility judgment capabilities of LLMs into smaller, more efficient\nmodels. Our approach focuses on utility-based selection rather than ranking,\nenabling dynamic passage selection tailored to specific queries without the\nneed for fixed thresholds. We train student models to learn pseudo-answer\ngeneration and utility judgments from teacher LLMs, using a sliding window\nmethod that dynamically selects useful passages. Our experiments demonstrate\nthat utility-based selection provides a flexible and cost-effective solution\nfor RAG, significantly reducing computational costs while improving answer\nquality. We present the distillation results using Qwen3-32B as the teacher\nmodel for both relevance ranking and utility-based selection, distilled into\nRankQwen1.7B and UtilityQwen1.7B. Our findings indicate that for complex\nquestions, utility-based selection is more effective than relevance ranking in\nenhancing answer generation performance. We will release the relevance ranking\nand utility-based selection annotations for the MS MARCO dataset, supporting\nfurther research in this area."}
{"id": "2507.19132", "pdf": "https://arxiv.org/pdf/2507.19132.pdf", "abs": "https://arxiv.org/abs/2507.19132", "title": "OS-MAP: How Far Can Computer-Using Agents Go in Breadth and Depth?", "authors": ["Xuetian Chen", "Yinghao Chen", "Xinfeng Yuan", "Zhuo Peng", "Lu Chen", "Yuekeng Li", "Zhoujia Zhang", "Yingqian Huang", "Leyan Huang", "Jiaqing Liang", "Tianbao Xie", "Zhiyong Wu", "Qiushi Sun", "Biqing Qi", "Bowen Zhou"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "Work in progress", "summary": "Computer-using agents have shown strong potential to boost human productivity\nand enable new application forms across platforms. While recent advances have\nled to usable applications, existing benchmarks fail to account for the\ninternal task heterogeneity and the corresponding agent capabilities, as well\nas their alignment with actual user demands-hindering both targeted capability\ndevelopment and the reliable transition of research progress into practical\ndeployment. To bridge the gap, we present OS-MAP, a benchmark for daily\ncomputer-using automation that organizes its 416 realistic tasks across 15\napplications along two key dimensions: a five-level taxonomy of automation and\na generalization scope derived from a real-world user demand hierarchy. To\nenable fine-grained analysis of required capabilities and alignment with\nreal-world scenarios, OS-MAP evaluates agents along two dimensions: automation\nlevel across a five-level taxonomy, and generalization scope across a demand\nhierarchy. This design captures varying levels of required agent autonomy and\ngeneralization, forming a performance-generalization evaluation matrix for\nstructured and comprehensive assessment. Experiments show that even\nState-of-the-Art agents with VLM backbones struggle with higher-level tasks\ninvolving perception, reasoning, and coordination-highlighting the need for a\ndeeper understanding of current strengths and limitations to drive the future\nprogress in computer-using agents research and deployment. All code,\nenvironments, baselines, and data are publicly available at\nhttps://github.com/OS-Copilot/OS-Map."}
{"id": "2507.19196", "pdf": "https://arxiv.org/pdf/2507.19196.pdf", "abs": "https://arxiv.org/abs/2507.19196", "title": "Towards Multimodal Social Conversations with Robots: Using Vision-Language Models", "authors": ["Ruben Janssens", "Tony Belpaeme"], "categories": ["cs.RO", "cs.CL", "cs.HC"], "comment": "Submitted to the workshop \"Human - Foundation Models Interaction: A\n  Focus On Multimodal Information\" (FoMo-HRI) at IEEE RO-MAN 2025", "summary": "Large language models have given social robots the ability to autonomously\nengage in open-domain conversations. However, they are still missing a\nfundamental social skill: making use of the multiple modalities that carry\nsocial interactions. While previous work has focused on task-oriented\ninteractions that require referencing the environment or specific phenomena in\nsocial interactions such as dialogue breakdowns, we outline the overall needs\nof a multimodal system for social conversations with robots. We then argue that\nvision-language models are able to process this wide range of visual\ninformation in a sufficiently general manner for autonomous social robots. We\ndescribe how to adapt them to this setting, which technical challenges remain,\nand briefly discuss evaluation practices."}
{"id": "2507.19204", "pdf": "https://arxiv.org/pdf/2507.19204.pdf", "abs": "https://arxiv.org/abs/2507.19204", "title": "Should Top-Down Clustering Affect Boundaries in Unsupervised Word Discovery?", "authors": ["Simon Malan", "Benjamin van Niekerk", "Herman Kamper"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "5 figures, 5 tables", "summary": "We investigate the problem of segmenting unlabeled speech into word-like\nunits and clustering these to create a lexicon. Prior work can be categorized\ninto two frameworks. Bottom-up methods first determine boundaries and then\ncluster the fixed segmented words into a lexicon. In contrast, top-down methods\nincorporate information from the clustered words to inform boundary selection.\nHowever, it is unclear whether top-down information is necessary to improve\nsegmentation. To explore this, we look at two similar approaches that differ in\nwhether top-down clustering informs boundary selection. Our simple bottom-up\nstrategy predicts word boundaries using the dissimilarity between adjacent\nself-supervised features, then clusters the resulting segments to construct a\nlexicon. Our top-down system is an updated version of the ES-KMeans dynamic\nprogramming method that iteratively uses K-means to update its boundaries. On\nthe five-language ZeroSpeech benchmarks, both approaches achieve comparable\nstate-of-the-art results, with the bottom-up system being nearly five times\nfaster. Through detailed analyses, we show that the top-down influence of\nES-KMeans can be beneficial (depending on factors like the candidate\nboundaries), but in many cases the simple bottom-up method performs just as\nwell. For both methods, we show that the clustering step is a limiting factor.\nTherefore, we recommend that future work focus on improved clustering\ntechniques and learning more discriminative word-like representations. Project\ncode repository: https://github.com/s-malan/prom-seg-clus."}
{"id": "2507.19247", "pdf": "https://arxiv.org/pdf/2507.19247.pdf", "abs": "https://arxiv.org/abs/2507.19247", "title": "A Markov Categorical Framework for Language Modeling", "authors": ["Yifan Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Project Page: https://github.com/asiresearch/lm-theory", "summary": "Auto-regressive language models factorize sequence probabilities and are\ntrained by minimizing the negative log-likelihood (NLL) objective. While\nempirically powerful, a deep theoretical understanding of why this simple\nobjective yields such versatile representations remains elusive. This work\nintroduces a unifying analytical framework using Markov Categories (MCs) to\ndeconstruct the AR generation process and the NLL objective. We model the\nsingle-step generation map as a composition of Markov kernels in the category\nStoch. This compositional view, when enriched with statistical divergences,\nallows us to dissect information flow and learned geometry. Our framework makes\nthree main contributions. First, we provide a formal, information-theoretic\nrationale for the success of modern speculative decoding methods like EAGLE,\nquantifying the information surplus in hidden states that these methods\nexploit. Second, we formalize how NLL minimization forces the model to learn\nnot just the next token, but the data's intrinsic conditional stochasticity, a\nprocess we analyze using categorical entropy. Third, and most centrally, we\nprove that NLL training acts as an implicit form of spectral contrastive\nlearning. By analyzing the information geometry of the model's prediction head,\nwe show that NLL implicitly forces the learned representation space to align\nwith the eigenspectrum of a predictive similarity operator, thereby learning a\ngeometrically structured space without explicit contrastive pairs. This\ncompositional and information-geometric perspective reveals the deep structural\nprinciples underlying the effectiveness of modern LMs. Project Page:\nhttps://github.com/asiresearch/lm-theory"}
{"id": "2507.19333", "pdf": "https://arxiv.org/pdf/2507.19333.pdf", "abs": "https://arxiv.org/abs/2507.19333", "title": "Injecting External Knowledge into the Reasoning Process Enhances Retrieval-Augmented Generation", "authors": ["Minghao Tang", "Shiyu Ni", "Jiafeng Guo", "Keping Bi"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) has been widely adopted to augment large\nlanguage models (LLMs) with external knowledge for knowledge-intensive tasks.\nHowever, its effectiveness is often undermined by the presence of noisy (i.e.,\nlow-quality) retrieved passages. Enhancing LLMs' robustness to such noise is\ncritical for improving the reliability of RAG systems. Recent advances have\nequipped LLMs with strong reasoning and self-reflection capabilities, allowing\nthem to identify and correct errors in their reasoning process. Inspired by\nthis ability, we propose Passage Injection-a simple yet effective method that\nexplicitly incorporates retrieved passages into LLMs' reasoning process, aiming\nto enhance the model's ability to recognize and resist noisy passages. We\nvalidate Passage Injection under general RAG settings using BM25 as the\nretriever. Experiments on four reasoning-enhanced LLMs across four factual QA\ndatasets demonstrate that Passage Injection significantly improves overall RAG\nperformance. Further analysis on two noisy retrieval settings-random noise,\nwhere the model is provided irrelevant passages, and counterfactual noise,\nwhere it is given misleading passages-shows that Passage Injection consistently\nimproves robustness. Controlled experiments confirm that Passage Injection can\nalso effectively leverage helpful passages. These findings suggest that\nincorporating passages in LLMs' reasoning process is a promising direction for\nbuilding more robust RAG systems. The code can be found\n\\href{here}{https://github.com/mh-tang/Passage-Injection}."}
{"id": "2507.19362", "pdf": "https://arxiv.org/pdf/2507.19362.pdf", "abs": "https://arxiv.org/abs/2507.19362", "title": "LOTUS: A Leaderboard for Detailed Image Captioning from Quality to Societal Bias and User Preferences", "authors": ["Yusuke Hirota", "Boyi Li", "Ryo Hachiuma", "Yueh-Hua Wu", "Boris Ivanovic", "Yuta Nakashima", "Marco Pavone", "Yejin Choi", "Yu-Chiang Frank Wang", "Chao-Han Huck Yang"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "comment": "Accepted to ACL 2025. Leaderboard:\n  huggingface.co/spaces/nvidia/lotus-vlm-bias-leaderboard", "summary": "Large Vision-Language Models (LVLMs) have transformed image captioning,\nshifting from concise captions to detailed descriptions. We introduce LOTUS, a\nleaderboard for evaluating detailed captions, addressing three main gaps in\nexisting evaluations: lack of standardized criteria, bias-aware assessments,\nand user preference considerations. LOTUS comprehensively evaluates various\naspects, including caption quality (e.g., alignment, descriptiveness), risks\n(\\eg, hallucination), and societal biases (e.g., gender bias) while enabling\npreference-oriented evaluations by tailoring criteria to diverse user\npreferences. Our analysis of recent LVLMs reveals no single model excels across\nall criteria, while correlations emerge between caption detail and bias risks.\nPreference-oriented evaluations demonstrate that optimal model selection\ndepends on user priorities."}
{"id": "2507.19477", "pdf": "https://arxiv.org/pdf/2507.19477.pdf", "abs": "https://arxiv.org/abs/2507.19477", "title": "Advancing Event Forecasting through Massive Training of Large Language Models: Challenges, Solutions, and Broader Impacts", "authors": ["Sang-Woo Lee", "Sohee Yang", "Donghyun Kwak", "Noah Y. Siegel"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Many recent papers have studied the development of superforecaster-level\nevent forecasting LLMs. While methodological problems with early studies cast\ndoubt on the use of LLMs for event forecasting, recent studies with improved\nevaluation methods have shown that state-of-the-art LLMs are gradually reaching\nsuperforecaster-level performance, and reinforcement learning has also been\nreported to improve future forecasting. Additionally, the unprecedented success\nof recent reasoning models and Deep Research-style models suggests that\ntechnology capable of greatly improving forecasting performance has been\ndeveloped. Therefore, based on these positive recent trends, we argue that the\ntime is ripe for research on large-scale training of superforecaster-level\nevent forecasting LLMs. We discuss two key research directions: training\nmethods and data acquisition. For training, we first introduce three\ndifficulties of LLM-based event forecasting training: noisiness-sparsity,\nknowledge cut-off, and simple reward structure problems. Then, we present\nrelated ideas to mitigate these problems: hypothetical event Bayesian networks,\nutilizing poorly-recalled and counterfactual events, and auxiliary reward\nsignals. For data, we propose aggressive use of market, public, and crawling\ndatasets to enable large-scale training and evaluation. Finally, we explain how\nthese technical advances could enable AI to provide predictive intelligence to\nsociety in broader areas. This position paper presents promising specific paths\nand considerations for getting closer to superforecaster-level AI technology,\naiming to call for researchers' interest in these directions."}
{"id": "2507.19478", "pdf": "https://arxiv.org/pdf/2507.19478.pdf", "abs": "https://arxiv.org/abs/2507.19478", "title": "MMBench-GUI: Hierarchical Multi-Platform Evaluation Framework for GUI Agents", "authors": ["Xuehui Wang", "Zhenyu Wu", "JingJing Xie", "Zichen Ding", "Bowen Yang", "Zehao Li", "Zhaoyang Liu", "Qingyun Li", "Xuan Dong", "Zhe Chen", "Weiyun Wang", "Xiangyu Zhao", "Jixuan Chen", "Haodong Duan", "Tianbao Xie", "Chenyu Yang", "Shiqian Su", "Yue Yu", "Yuan Huang", "Yiqian Liu", "Xiao Zhang", "Yanting Zhang", "Xiangyu Yue", "Weijie Su", "Xizhou Zhu", "Wei Shen", "Jifeng Dai", "Wenhai Wang"], "categories": ["cs.CV", "cs.CL"], "comment": "in progress", "summary": "We introduce MMBench-GUI, a hierarchical benchmark for evaluating GUI\nautomation agents across Windows, macOS, Linux, iOS, Android, and Web\nplatforms. It comprises four levels: GUI Content Understanding, Element\nGrounding, Task Automation, and Task Collaboration, covering essential skills\nfor GUI agents. In addition, we propose a novel Efficiency-Quality Area (EQA)\nmetric to assess GUI agent execution efficiency in online automation scenarios.\nThrough MMBench-GUI, we identify accurate visual grounding as a critical\ndeterminant of overall task success, emphasizing the substantial benefits of\nmodular frameworks that integrate specialized grounding modules. Furthermore,\nto achieve reliable GUI automation, an agent requires strong task planning and\ncross-platform generalization abilities, with long-context memory, a broad\naction space, and long-term reasoning playing a critical role. More important,\ntask efficiency remains a critically underexplored dimension, and all models\nsuffer from substantial inefficiencies, with excessive redundant steps even\nwhen tasks are ultimately completed. The integration of precise localization,\neffective planning, and early stopping strategies is indispensable to enable\ntruly efficient and scalable GUI automation. Our benchmark code, evaluation\ndata, and running environment will be publicly available at\nhttps://github.com/open-compass/MMBench-GUI."}
{"id": "2311.13729", "pdf": "https://arxiv.org/pdf/2311.13729.pdf", "abs": "https://arxiv.org/abs/2311.13729", "title": "Comparison of pipeline, sequence-to-sequence, and GPT models for end-to-end relation extraction: experiments with the rare disease use-case", "authors": ["Shashank Gupta", "Xuguang Ai", "Ramakanth Kavuluru"], "categories": ["cs.CL"], "comment": "An updated version of this paper has appeared in the proceedings of\n  NLDB 2025 with a different title. The corresonding DOI is in the metadata\n  provided below", "summary": "End-to-end relation extraction (E2ERE) is an important and realistic\napplication of natural language processing (NLP) in biomedicine. In this paper,\nwe aim to compare three prevailing paradigms for E2ERE using a complex dataset\nfocused on rare diseases involving discontinuous and nested entities. We use\nthe RareDis information extraction dataset to evaluate three competing\napproaches (for E2ERE): NER $\\rightarrow$ RE pipelines, joint sequence to\nsequence models, and generative pre-trained transformer (GPT) models. We use\ncomparable state-of-the-art models and best practices for each of these\napproaches and conduct error analyses to assess their failure modes. Our\nfindings reveal that pipeline models are still the best, while\nsequence-to-sequence models are not far behind; GPT models with eight times as\nmany parameters are worse than even sequence-to-sequence models and lose to\npipeline models by over 10 F1 points. Partial matches and discontinuous\nentities caused many NER errors contributing to lower overall E2E performances.\nWe also verify these findings on a second E2ERE dataset for chemical-protein\ninteractions. Although generative LM-based methods are more suitable for\nzero-shot settings, when training data is available, our results show that it\nis better to work with more conventional models trained and tailored for E2ERE.\nMore innovative methods are needed to marry the best of the both worlds from\nsmaller encoder-decoder pipeline models and the larger GPT models to improve\nE2ERE. As of now, we see that well designed pipeline models offer substantial\nperformance gains at a lower cost and carbon footprint for E2ERE. Our\ncontribution is also the first to conduct E2ERE for the RareDis dataset."}
{"id": "2312.16903", "pdf": "https://arxiv.org/pdf/2312.16903.pdf", "abs": "https://arxiv.org/abs/2312.16903", "title": "Spike No More: Stabilizing the Pre-training of Large Language Models", "authors": ["Sho Takase", "Shun Kiyono", "Sosuke Kobayashi", "Jun Suzuki"], "categories": ["cs.CL", "cs.AI"], "comment": "COLM 2025", "summary": "Loss spikes often occur during pre-training of large language models. The\nspikes degrade the performance of large language models and sometimes ruin the\npre-training. Since the pre-training needs a vast computational budget, we\nshould avoid such spikes. Based on the assumption that the loss spike is caused\nby the sudden growth of the gradient norm, we explore factors to keep the\ngradient norm small through an analysis of the spectral norms of the Jacobian\nmatrices for the sub-layers. Our findings suggest that stabilizing the\npre-training process requires two conditions: small sub-layers and large\nshortcut. We conduct various experiments to empirically verify our theoretical\nanalyses. Experimental results demonstrate that methods satisfying the\nconditions effectively prevent loss spikes during pre-training."}
{"id": "2402.13470", "pdf": "https://arxiv.org/pdf/2402.13470.pdf", "abs": "https://arxiv.org/abs/2402.13470", "title": "How Important is Domain Specificity in Language Models and Instruction Finetuning for Biomedical Relation Extraction?", "authors": ["Aviv Brokman", "Ramakanth Kavuluru"], "categories": ["cs.CL"], "comment": "A version of this paper has appeared in the proceedings of NLDB 2025\n  with a slightly different title. The corresponding DOI is also listed below\n  in the metadata", "summary": "Cutting edge techniques developed in the general NLP domain are often\nsubsequently applied to the high-value, data-rich biomedical domain. The past\nfew years have seen generative language models (LMs), instruction finetuning,\nand few-shot learning become foci of NLP research. As such, generative LMs\npretrained on biomedical corpora have proliferated and biomedical instruction\nfinetuning has been attempted as well, all with the hope that domain\nspecificity improves performance on downstream tasks. Given the nontrivial\neffort in training such models, we investigate what, if any, benefits they have\nin the key biomedical NLP task of relation extraction. Specifically, we address\ntwo questions: (1) Do LMs trained on biomedical corpora outperform those\ntrained on general domain corpora? (2) Do models instruction finetuned on\nbiomedical datasets outperform those finetuned on assorted datasets or those\nsimply pretrained? We tackle these questions using existing LMs, testing across\nfour datasets. In a surprising result, general-domain models typically\noutperformed biomedical-domain models. However, biomedical instruction\nfinetuning improved performance to a similar degree as general instruction\nfinetuning, despite having orders of magnitude fewer instructions. Our findings\nsuggest it may be more fruitful to focus research effort on larger-scale\nbiomedical instruction finetuning of general LMs over building domain-specific\nbiomedical LMs"}
{"id": "2406.12549", "pdf": "https://arxiv.org/pdf/2406.12549.pdf", "abs": "https://arxiv.org/abs/2406.12549", "title": "MultiSocial: Multilingual Benchmark of Machine-Generated Text Detection of Social-Media Texts", "authors": ["Dominik Macko", "Jakub Kopal", "Robert Moro", "Ivan Srba"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 main", "summary": "Recent LLMs are able to generate high-quality multilingual texts,\nindistinguishable for humans from authentic human-written ones. Research in\nmachine-generated text detection is however mostly focused on the English\nlanguage and longer texts, such as news articles, scientific papers or student\nessays. Social-media texts are usually much shorter and often feature informal\nlanguage, grammatical errors, or distinct linguistic items (e.g., emoticons,\nhashtags). There is a gap in studying the ability of existing methods in\ndetection of such texts, reflected also in the lack of existing multilingual\nbenchmark datasets. To fill this gap we propose the first multilingual (22\nlanguages) and multi-platform (5 social media platforms) dataset for\nbenchmarking machine-generated text detection in the social-media domain,\ncalled MultiSocial. It contains 472,097 texts, of which about 58k are\nhuman-written and approximately the same amount is generated by each of 7\nmultilingual LLMs. We use this benchmark to compare existing detection methods\nin zero-shot as well as fine-tuned form. Our results indicate that the\nfine-tuned detectors have no problem to be trained on social-media texts and\nthat the platform selection for training matters."}
{"id": "2408.06303", "pdf": "https://arxiv.org/pdf/2408.06303.pdf", "abs": "https://arxiv.org/abs/2408.06303", "title": "Long-Form Answers to Visual Questions from Blind and Low Vision People", "authors": ["Mina Huh", "Fangyuan Xu", "Yi-Hao Peng", "Chongyan Chen", "Hansika Murugu", "Danna Gurari", "Eunsol Choi", "Amy Pavel"], "categories": ["cs.CL", "cs.CV"], "comment": "COLM 2024 Oral Spotlight", "summary": "Vision language models can now generate long-form answers to questions about\nimages - long-form visual question answers (LFVQA). We contribute VizWiz-LF, a\ndataset of long-form answers to visual questions posed by blind and low vision\n(BLV) users. VizWiz-LF contains 4.2k long-form answers to 600 visual questions,\ncollected from human expert describers and six VQA models. We develop and\nannotate functional roles of sentences of LFVQA and demonstrate that long-form\nanswers contain information beyond the question answer such as explanations and\nsuggestions. We further conduct automatic and human evaluations with BLV and\nsighted people to evaluate long-form answers. BLV people perceive both\nhuman-written and generated long-form answers to be plausible, but generated\nanswers often hallucinate incorrect visual details, especially for unanswerable\nvisual questions (e.g., blurry or irrelevant images). To reduce hallucinations,\nwe evaluate the ability of VQA models to abstain from answering unanswerable\nquestions across multiple prompting strategies."}
{"id": "2410.07919", "pdf": "https://arxiv.org/pdf/2410.07919.pdf", "abs": "https://arxiv.org/abs/2410.07919", "title": "Advancing biomolecular understanding and design following human instructions", "authors": ["Xiang Zhuang", "Keyan Ding", "Tianwen Lyu", "Yinuo Jiang", "Xiaotong Li", "Zhuoyi Xiang", "Zeyuan Wang", "Ming Qin", "Kehua Feng", "Jike Wang", "Qiang Zhang", "Huajun Chen"], "categories": ["cs.CL", "q-bio.BM"], "comment": null, "summary": "Understanding and designing biomolecules, such as proteins and small\nmolecules, is central to advancing drug discovery, synthetic biology and enzyme\nengineering. Recent breakthroughs in artificial intelligence have\nrevolutionized biomolecular research, achieving remarkable accuracy in\nbiomolecular prediction and design. However, a critical gap remains between\nartificial intelligence's computational capabilities and researchers' intuitive\ngoals, particularly in using natural language to bridge complex tasks with\nhuman intentions. Large language models have shown potential to interpret human\nintentions, yet their application to biomolecular research remains nascent due\nto challenges including specialized knowledge requirements, multimodal data\nintegration, and semantic alignment between natural language and biomolecules.\nTo address these limitations, we present InstructBioMol, a large language model\ndesigned to bridge natural language and biomolecules through a comprehensive\nany-to-any alignment of natural language, molecules and proteins. This model\ncan integrate multimodal biomolecules as the input, and enable researchers to\narticulate design goals in natural language, providing biomolecular outputs\nthat meet precise biological needs. Experimental results demonstrate that\nInstructBioMol can understand and design biomolecules following human\ninstructions. In particular, it can generate drug molecules with a 10%\nimprovement in binding affinity and design enzymes that achieve an\nenzyme-substrate pair prediction score of 70.4. This highlights its potential\nto transform real-world biomolecular research. The code is available at\nhttps://github.com/HICAI-ZJU/InstructBioMol."}
{"id": "2412.01131", "pdf": "https://arxiv.org/pdf/2412.01131.pdf", "abs": "https://arxiv.org/abs/2412.01131", "title": "A Comprehensive Evaluation of Semantic Relation Knowledge of Pretrained Language Models and Humans", "authors": ["Zhihan Cao", "Hiroaki Yamada", "Simone Teufel", "Takenobu Tokunaga"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, much work has concerned itself with the enigma of what exactly\npretrained language models~(PLMs) learn about different aspects of language,\nand how they learn it. One stream of this type of research investigates the\nknowledge that PLMs have about semantic relations. However, many aspects of\nsemantic relations were left unexplored. Generally, only one relation has been\nconsidered, namely hypernymy. Furthermore, previous work did not measure\nhumans' performance on the same task as that performed by the PLMs. This means\nthat at this point in time, there is only an incomplete view of the extent of\nthese models' semantic relation knowledge. To address this gap, we introduce a\ncomprehensive evaluation framework covering five relations beyond hypernymy,\nnamely hyponymy, holonymy, meronymy, antonymy, and synonymy. We use five\nmetrics (two newly introduced here) for recently untreated aspects of semantic\nrelation knowledge, namely soundness, completeness, symmetry, prototypicality,\nand distinguishability. Using these, we can fairly compare humans and models on\nthe same task. Our extensive experiments involve six PLMs, four masked and two\ncausal language models. The results reveal a significant knowledge gap between\nhumans and models for all semantic relations. In general, causal language\nmodels, despite their wide use, do not always perform significantly better than\nmasked language models. Antonymy is the outlier relation where all models\nperform reasonably well."}
{"id": "2412.12591", "pdf": "https://arxiv.org/pdf/2412.12591.pdf", "abs": "https://arxiv.org/abs/2412.12591", "title": "LLMs are Also Effective Embedding Models: An In-depth Overview", "authors": ["Chongyang Tao", "Tao Shen", "Shen Gao", "Junshuo Zhang", "Zhen Li", "Kai Hua", "Wenpeng Hu", "Zhengwei Tao", "Shuai Ma"], "categories": ["cs.CL"], "comment": "38 pages", "summary": "Large language models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art performance across various tasks. Recently, their\neffectiveness as embedding models has gained attention, marking a paradigm\nshift from traditional encoder-only models like ELMo and BERT to decoder-only,\nlarge-scale LLMs such as GPT, LLaMA, and Mistral. This survey provides an\nin-depth overview of this transition, beginning with foundational techniques\nbefore the LLM era, followed by LLM-based embedding models through two main\nstrategies to derive embeddings from LLMs. 1) Direct prompting: We mainly\ndiscuss the prompt designs and the underlying rationale for deriving\ncompetitive embeddings. 2) Data-centric tuning: We cover extensive aspects that\naffect tuning an embedding model, including model architecture, training\nobjectives, data constructions, etc. Upon the above, we also cover advanced\nmethods for producing embeddings from longer texts, multilingual, code,\ncross-modal data, as well as reasoning-aware and other domain-specific\nscenarios. Furthermore, we discuss factors affecting choices of embedding\nmodels, such as performance/efficiency comparisons, dense vs sparse embeddings,\npooling strategies, and scaling law. Lastly, the survey highlights the\nlimitations and challenges in adapting LLMs for embeddings, including\ncross-task embedding quality, trade-offs between efficiency and accuracy,\nlow-resource, long-context, data bias, robustness, etc. This survey serves as a\nvaluable resource for researchers and practitioners by synthesizing current\nadvancements, highlighting key challenges, and offering a comprehensive\nframework for future work aimed at enhancing the effectiveness and efficiency\nof LLMs as embedding models."}
{"id": "2412.13666", "pdf": "https://arxiv.org/pdf/2412.13666.pdf", "abs": "https://arxiv.org/abs/2412.13666", "title": "Evaluation of LLM Vulnerabilities to Being Misused for Personalized Disinformation Generation", "authors": ["Aneta Zugecova", "Dominik Macko", "Ivan Srba", "Robert Moro", "Jakub Kopal", "Katarina Marcincinova", "Matus Mesarcik"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "ACL 2025 main", "summary": "The capabilities of recent large language models (LLMs) to generate\nhigh-quality content indistinguishable by humans from human-written texts\nraises many concerns regarding their misuse. Previous research has shown that\nLLMs can be effectively misused for generating disinformation news articles\nfollowing predefined narratives. Their capabilities to generate personalized\n(in various aspects) content have also been evaluated and mostly found usable.\nHowever, a combination of personalization and disinformation abilities of LLMs\nhas not been comprehensively studied yet. Such a dangerous combination should\ntrigger integrated safety filters of the LLMs, if there are some. This study\nfills this gap by evaluating vulnerabilities of recent open and closed LLMs,\nand their willingness to generate personalized disinformation news articles in\nEnglish. We further explore whether the LLMs can reliably meta-evaluate the\npersonalization quality and whether the personalization affects the\ngenerated-texts detectability. Our results demonstrate the need for stronger\nsafety-filters and disclaimers, as those are not properly functioning in most\nof the evaluated LLMs. Additionally, our study revealed that the\npersonalization actually reduces the safety-filter activations; thus\neffectively functioning as a jailbreak. Such behavior must be urgently\naddressed by LLM developers and service providers."}
{"id": "2501.12612", "pdf": "https://arxiv.org/pdf/2501.12612.pdf", "abs": "https://arxiv.org/abs/2501.12612", "title": "T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in Image Generation", "authors": ["Lijun Li", "Zhelun Shi", "Xuhao Hu", "Bowen Dong", "Yiran Qin", "Xihui Liu", "Lu Sheng", "Jing Shao"], "categories": ["cs.CL", "cs.CR"], "comment": "Accepted at CVPR 2025", "summary": "Text-to-image (T2I) models have rapidly advanced, enabling the generation of\nhigh-quality images from text prompts across various domains. However, these\nmodels present notable safety concerns, including the risk of generating\nharmful, biased, or private content. Current research on assessing T2I safety\nremains in its early stages. While some efforts have been made to evaluate\nmodels on specific safety dimensions, many critical risks remain unexplored. To\naddress this gap, we introduce T2ISafety, a safety benchmark that evaluates T2I\nmodels across three key domains: toxicity, fairness, and bias. We build a\ndetailed hierarchy of 12 tasks and 44 categories based on these three domains,\nand meticulously collect 70K corresponding prompts. Based on this taxonomy and\nprompt set, we build a large-scale T2I dataset with 68K manually annotated\nimages and train an evaluator capable of detecting critical risks that previous\nwork has failed to identify, including risks that even ultra-large proprietary\nmodels like GPTs cannot correctly detect. We evaluate 12 prominent diffusion\nmodels on T2ISafety and reveal several concerns including persistent issues\nwith racial fairness, a tendency to generate toxic content, and significant\nvariation in privacy protection across the models, even with defense methods\nlike concept erasing. Data and evaluator are released under\nhttps://github.com/adwardlee/t2i_safety."}
{"id": "2502.11439", "pdf": "https://arxiv.org/pdf/2502.11439.pdf", "abs": "https://arxiv.org/abs/2502.11439", "title": "An Efficient Sparse Fine-Tuning with Low Quantization Error via Neural Network Pruning", "authors": ["Cen-Jhih Li", "Aditya Bhaskara"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Fine-tuning is an important step in adapting foundation models such as large\nlanguage models to downstream tasks. To make this step more accessible to users\nwith limited computational budgets, it is crucial to develop fine-tuning\nmethods that are memory and computationally efficient. Sparse Fine-tuning\n(SpFT) and Low-rank adaptation (LoRA) are two frameworks that have emerged for\naddressing this problem and have been adopted widely in practice. In this work,\nwe develop a new SpFT framework, based on ideas from neural network pruning. At\na high level, we first identify ``important'' neurons/nodes using feature\nimportance metrics from network pruning (specifically, we use the structural\npruning method), and then perform fine-tuning by restricting to weights\ninvolving these neurons. Experiments on common language tasks show our method\nimproves SpFT's memory efficiency by 20-50\\% while matching the accuracy of\nstate-of-the-art methods like LoRA's variants."}
{"id": "2502.14561", "pdf": "https://arxiv.org/pdf/2502.14561.pdf", "abs": "https://arxiv.org/abs/2502.14561", "title": "Can LLMs Predict Citation Intent? An Experimental Analysis of In-context Learning and Fine-tuning on Open LLMs", "authors": ["Paris Koloveas", "Serafeim Chatzopoulos", "Thanasis Vergoulis", "Christos Tryfonopoulos"], "categories": ["cs.CL", "cs.DL"], "comment": "Accepted for publication on TPDL 2025", "summary": "This work investigates the ability of open Large Language Models (LLMs) to\npredict citation intent through in-context learning and fine-tuning. Unlike\ntraditional approaches relying on domain-specific pre-trained models like\nSciBERT, we demonstrate that general-purpose LLMs can be adapted to this task\nwith minimal task-specific data. We evaluate twelve model variations across\nfive prominent open LLM families using zero-, one-, few-, and many-shot\nprompting. Our experimental study identifies the top-performing model and\nprompting parameters through extensive in-context learning experiments. We then\ndemonstrate the significant impact of task-specific adaptation by fine-tuning\nthis model, achieving a relative F1-score improvement of 8% on the SciCite\ndataset and 4.3% on the ACL-ARC dataset compared to the instruction-tuned\nbaseline. These findings provide valuable insights for model selection and\nprompt engineering. Additionally, we make our end-to-end evaluation framework\nand models openly available for future use."}
{"id": "2503.00151", "pdf": "https://arxiv.org/pdf/2503.00151.pdf", "abs": "https://arxiv.org/abs/2503.00151", "title": "Palm: A Culturally Inclusive and Linguistically Diverse Dataset for Arabic LLMs", "authors": ["Fakhraddin Alwajih", "Abdellah El Mekki", "Samar Mohamed Magdy", "Abdelrahim A. Elmadany", "Omer Nacar", "El Moatez Billah Nagoudi", "Reem Abdel-Salam", "Hanin Atwany", "Youssef Nafea", "Abdulfattah Mohammed Yahya", "Rahaf Alhamouri", "Hamzah A. Alsayadi", "Hiba Zayed", "Sara Shatnawi", "Serry Sibaee", "Yasir Ech-Chammakhy", "Walid Al-Dhabyani", "Marwa Mohamed Ali", "Imen Jarraya", "Ahmed Oumar El-Shangiti", "Aisha Alraeesi", "Mohammed Anwar Al-Ghrawi", "Abdulrahman S. Al-Batati", "Elgizouli Mohamed", "Noha Taha Elgindi", "Muhammed Saeed", "Houdaifa Atou", "Issam Ait Yahia", "Abdelhak Bouayad", "Mohammed Machrouh", "Amal Makouar", "Dania Alkawi", "Mukhtar Mohamed", "Safaa Taher Abdelfadil", "Amine Ziad Ounnoughene", "Rouabhia Anfel", "Rwaa Assi", "Ahmed Sorkatti", "Mohamedou Cheikh Tourad", "Anis Koubaa", "Ismail Berrada", "Mustafa Jarrar", "Shady Shehata", "Muhammad Abdul-Mageed"], "categories": ["cs.CL", "cs.AI"], "comment": "More information about our dataset is available at our project page:\n  https://github.com/UBC-NLP/palm", "summary": "As large language models (LLMs) become increasingly integrated into daily\nlife, ensuring their cultural sensitivity and inclusivity is paramount. We\nintroduce our dataset, a year-long community-driven project covering all 22\nArab countries. The dataset includes instructions (input, response pairs) in\nboth Modern Standard Arabic (MSA) and dialectal Arabic (DA), spanning 20\ndiverse topics. Built by a team of 44 researchers across the Arab world, all of\nwhom are authors of this paper, our dataset offers a broad, inclusive\nperspective. We use our dataset to evaluate the cultural and dialectal\ncapabilities of several frontier LLMs, revealing notable limitations. For\ninstance, while closed-source LLMs generally exhibit strong performance, they\nare not without flaws, and smaller open-source models face greater challenges.\nMoreover, certain countries (e.g., Egypt, the UAE) appear better represented\nthan others (e.g., Iraq, Mauritania, Yemen). Our annotation guidelines, code,\nand data for reproducibility are publicly available."}
{"id": "2503.05157", "pdf": "https://arxiv.org/pdf/2503.05157.pdf", "abs": "https://arxiv.org/abs/2503.05157", "title": "Ensemble Debiasing Across Class and Sample Levels for Fairer Prompting Accuracy", "authors": ["Ruixi Lin", "Ziqiao Wang", "Yang You"], "categories": ["cs.CL"], "comment": "Published as a conference paper at COLM 2025", "summary": "Language models are strong few-shot learners and achieve good overall\naccuracy in text classification tasks, masking the fact that their results\nsuffer from great class accuracy imbalance. We believe that the pursuit of\noverall accuracy should not come from enriching the strong classes, but from\nraising up the weak ones. To address the imbalance, we propose a Heaviside step\nfunction based ensemble debiasing method, which enables flexible rectifications\nof in-context learned class probabilities at both class and sample levels.\nEvaluations with Llama-2-13B on seven text classification benchmarks show that\nour approach achieves state-of-the-art overall accuracy gains with balanced\nclass accuracies. More importantly, we perform analyses on the resulted\nprobability correction scheme, showing that sample-level corrections are\nnecessary to elevate weak classes. Due to effectively correcting weak classes,\nour method also brings significant performance gains to a larger model variant,\nLlama-2-70B, especially on a biomedical domain task, further demonstrating the\nnecessity of ensemble debiasing at both levels. Our source code is available at\nhttps://github.com/NUS-HPC-AI-Lab/DCS."}
{"id": "2503.17799", "pdf": "https://arxiv.org/pdf/2503.17799.pdf", "abs": "https://arxiv.org/abs/2503.17799", "title": "Relation Extraction with Instance-Adapted Predicate Descriptions", "authors": ["Yuhang Jiang", "Ramakanth Kavuluru"], "categories": ["cs.CL"], "comment": "This paper has been accepted to appear in the proceedings of AMIA\n  2025", "summary": "Relation extraction (RE) is a standard information extraction task playing a\nmajor role in downstream applications such as knowledge discovery and question\nanswering. Although decoder-only large language models are excelling in\ngenerative tasks, smaller encoder models are still the go to architecture for\nRE. In this paper, we revisit fine-tuning such smaller models using a novel\ndual-encoder architecture with a joint contrastive and cross-entropy loss.\nUnlike previous methods that employ a fixed linear layer for predicate\nrepresentations, our approach uses a second encoder to compute\ninstance-specific predicate representations by infusing them with real entity\nspans from corresponding input instances. We conducted experiments on two\nbiomedical RE datasets and two general domain datasets. Our approach achieved\nF1 score improvements ranging from 1% to 2% over state-of-the-art methods with\na simple but elegant formulation. Ablation studies justify the importance of\nvarious components built into the proposed architecture."}
{"id": "2504.21019", "pdf": "https://arxiv.org/pdf/2504.21019.pdf", "abs": "https://arxiv.org/abs/2504.21019", "title": "Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations", "authors": ["Yinghan Zhou", "Juan Wen", "Wanli Peng", "Yiming Xue", "Ziwei Zhang", "Zhengxian Wu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by NAACL 2025 main conference", "summary": "The growing popularity of large language models has raised concerns regarding\nthe potential to misuse AI-generated text (AIGT). It becomes increasingly\ncritical to establish an excellent AIGT detection method with high\ngeneralization and robustness. However, existing methods either focus on model\ngeneralization or concentrate on robustness. The unified mechanism, to\nsimultaneously address the challenges of generalization and robustness, is less\nexplored. In this paper, we argue that robustness can be view as a specific\nform of domain shift, and empirically reveal an intrinsic mechanism for model\ngeneralization of AIGT detection task. Then, we proposed a novel AIGT detection\nmethod (DP-Net) via dynamic perturbations introduced by a reinforcement\nlearning with elaborated reward and action. Experimentally, extensive results\nshow that the proposed DP-Net significantly outperforms some state-of-the-art\nAIGT detection methods for generalization capacity in three cross-domain\nscenarios. Meanwhile, the DP-Net achieves best robustness under two text\nadversarial attacks. The code is publicly available at\nhttps://github.com/CAU-ISS-Lab/AIGT-Detection-Evade-Detection/tree/main/DP-Net."}
{"id": "2505.03005", "pdf": "https://arxiv.org/pdf/2505.03005.pdf", "abs": "https://arxiv.org/abs/2505.03005", "title": "RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale", "authors": ["Daniel Goldstein", "Eric Alcaide", "Janna Lu", "Eugene Cheah"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": null, "summary": "We present Rapid Attention Distillation to Linear Attention Decoders at Scale\n(RADLADS), a protocol for rapidly converting softmax attention transformers\ninto linear attention decoder models, along with two new RWKV-variant\narchitectures, and models converted from popular Qwen2.5 open source models in\n7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens,\nless than 0.005% of the token count used to train the original teacher models.\nConverting to our 72B linear attention model costs less than \\$2,000 USD at\ntoday's prices, yet quality at inference remains close to the original\ntransformer. These models achieve state-of-the-art downstream performance\nacross a set of standard benchmarks for linear attention models of their size.\nWe release all our models on HuggingFace under the Apache 2.0 license, with the\nexception of our 72B models which are also governed by the Qwen License\nAgreement.\n  Models at\nhttps://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102\nTraining Code at https://github.com/recursal/RADLADS-paper"}
{"id": "2505.15670", "pdf": "https://arxiv.org/pdf/2505.15670.pdf", "abs": "https://arxiv.org/abs/2505.15670", "title": "SALM-Duplex: Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model", "authors": ["Ke Hu", "Ehsan Hosseini-Asl", "Chen Chen", "Edresson Casanova", "Subhankar Ghosh", "Piotr Żelasko", "Zhehuai Chen", "Jason Li", "Jagadeesh Balam", "Boris Ginsburg"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Spoken dialogue is an intuitive form of human-computer interaction, yet\ncurrent speech language models often remain constrained to turn-based\nexchanges, lacking real-time adaptability such as user barge-in. We propose a\nnovel duplex speech to speech (S2S) architecture featuring continuous user\ninputs and codec agent outputs with channel fusion that directly models\nsimultaneous user and agent streams. Using a pretrained streaming encoder for\nuser input enables the first duplex S2S model without requiring speech\npretrain. Separate architectures for agent and user modeling facilitate codec\nfine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared\nto previous works. Experimental results show that the proposed model\noutperforms previous duplex models in reasoning, turn-taking, and barge-in\nabilities. The model requires significantly less speech data, as speech\npretrain is skipped, which markedly simplifies the process of building a duplex\nS2S model from any LLMs. Finally, it is the first openly available duplex S2S\nmodel with training and inference code to foster reproducibility."}
{"id": "2505.16142", "pdf": "https://arxiv.org/pdf/2505.16142.pdf", "abs": "https://arxiv.org/abs/2505.16142", "title": "Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning", "authors": ["Shicheng Xu", "Liang Pang", "Yunchang Zhu", "Jia Gu", "Zihao Wei", "Jingcheng Deng", "Feiyang Pan", "Huawei Shen", "Xueqi Cheng"], "categories": ["cs.CL"], "comment": "15 pages", "summary": "Distilling reasoning paths from teacher to student models via supervised\nfine-tuning (SFT) provides a shortcut for improving the reasoning ability of\nsmaller Large Language Models (LLMs). However, the reasoning paths generated by\nteacher models often reflect only surface-level traces of their underlying\nauthentic reasoning. Insights from cognitive neuroscience suggest that\nauthentic reasoning involves a complex interweaving between meta-reasoning\n(which selects appropriate sub-problems from multiple candidates) and solving\n(which addresses the sub-problem). This implies authentic reasoning has an\nimplicit multi-branch structure. Supervised fine-tuning collapses this rich\nstructure into a flat sequence of token prediction in the teacher's reasoning\npath, preventing effective distillation of this structure to students. To\naddress this limitation, we propose RLKD, a reinforcement learning (RL)-based\ndistillation framework guided by a novel Generative Structure Reward Model\n(GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving\nsteps and computes rewards to measure structural alignment between student and\nteacher reasoning. RLKD combines this reward with RL, enabling student LLMs to\ninternalize the teacher's implicit multi-branch reasoning structure rather than\nmerely mimicking fixed output paths. Experiments show RLKD surpasses standard\nSFT-RL pipelines even when trained on 0.1% of data under an RL-only regime,\nunlocking greater student reasoning potential than SFT-based distillation."}
{"id": "2505.19630", "pdf": "https://arxiv.org/pdf/2505.19630.pdf", "abs": "https://arxiv.org/abs/2505.19630", "title": "DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue", "authors": ["Yichun Feng", "Jiawei Wang", "Lu Zhou", "Zhen Lei", "Yixue Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated excellent capabilities in the\nfield of biomedical question answering, but their application in real-world\nclinical consultations still faces core challenges. Single-round consultation\nsystems require patients to describe all symptoms upfront, leading to vague\ndiagnosis with unclear complaints. Traditional multi-turn dialogue models,\nconstrained by static supervised learning, lack flexibility and fail to\nintelligently extract key clinical information. To address these limitations,\nwe propose \\Ours{}, a reinforcement learning (RL)-based multi-agent\ncollaborative framework that models medical consultations as a dynamic\ndecision-making process under uncertainty. The doctor agent continuously\noptimizes its questioning strategy within the RL framework through multi-turn\ninteractions with the patient agent, dynamically adjusting its\ninformation-gathering path based on comprehensive rewards from the Consultation\nEvaluator. This RL fine-tuning mechanism enables LLMs to autonomously develop\ninteraction strategies aligned with clinical reasoning logic, rather than\nsuperficially imitating patterns in existing dialogue data. Notably, we\nconstructed MTMedDialog, the first English multi-turn medical consultation\ndataset capable of simulating patient interactions. Experiments demonstrate\nthat \\Ours{} outperforms existing models in both multi-turn reasoning\ncapability and final diagnostic performance. This approach shows immense\npractical value by reducing misdiagnosis risks in time-pressured settings,\nfreeing clinicians for complex cases, and pioneering a strategy to optimize\nmedical resource allocation and alleviate workforce shortages. Code and data\nare available at https://github.com/JarvisUSTC/DoctorAgent-RL"}
{"id": "2506.00842", "pdf": "https://arxiv.org/pdf/2506.00842.pdf", "abs": "https://arxiv.org/abs/2506.00842", "title": "Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience", "authors": ["Jiawei Gu", "Ziting Xian", "Yuanzhen Xie", "Ye Liu", "Enjie Liu", "Ruichao Zhong", "Mochi Gao", "Yunzhi Tan", "Bo Hu", "Zang Li"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "Large language models (LLMs) achieve strong performance on plain text tasks\nbut underperform on structured data like tables and databases. Potential\nchallenges arise from their underexposure during pre-training and rigid\ntext-to-structure transfer mechanisms. Unlike humans who seamlessly apply\nlearned patterns across data modalities, LLMs struggle to infer implicit\nrelationships embedded in tabular formats, especially in the absence of\nexplicit structural guidance. To bridge this cognitive gap, we introduce\nContrastive Retrieval-Augmented Generation on Experience (CoRE), a framework\nthat builds experience memory representations and enhances generalization\nthrough contrastive In-Context Learning (ICL) to simulate human-like knowledge\ntransfer. Experiments on Text-to-SQL and TableQA show CoRE significantly\nimproves performance, achieving average gains of 3.44% and 4.24%, with up to\n17.2% on challenging tasks. Our Monte Carlo Tree Search (MCTS)-generated\nExperience Memory expands training data 8-9x, enhancing diversity and domain\ncoverage. This training-free and continual method propels LLMs toward\nstructured knowledge expertise."}
{"id": "2506.04076", "pdf": "https://arxiv.org/pdf/2506.04076.pdf", "abs": "https://arxiv.org/abs/2506.04076", "title": "Acoustically Precise Hesitation Tagging Is Essential for End-to-End Verbatim Transcription Systems", "authors": ["Jhen-Ke Lin", "Hao-Chien Lu", "Chung-Chun Wang", "Hong-Yun Lin", "Berlin Chen"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "accepted to the ISCA SLaTE-2025 Workshop", "summary": "Verbatim transcription for automatic speaking assessment demands accurate\ncapture of disfluencies, crucial for downstream tasks like error analysis and\nfeedback. However, many ASR systems discard or generalize hesitations, losing\nimportant acoustic details. We fine-tune Whisper models on the Speak & Improve\n2025 corpus using low-rank adaptation (LoRA), without recourse to external\naudio training data. We compare three annotation schemes: removing hesitations\n(Pure), generic tags (Rich), and acoustically precise fillers inferred by\nGemini 2.0 Flash from existing audio-transcript pairs (Extra). Our challenge\nsystem achieved 6.47% WER (Pure) and 5.81% WER (Extra). Post-challenge\nexperiments reveal that fine-tuning Whisper Large V3 Turbo with the \"Extra\"\nscheme yielded a 5.5% WER, an 11.3% relative improvement over the \"Pure\" scheme\n(6.2% WER). This demonstrates that explicit, realistic filled-pause labeling\nsignificantly enhances ASR accuracy for verbatim L2 speech transcription."}
{"id": "2506.14335", "pdf": "https://arxiv.org/pdf/2506.14335.pdf", "abs": "https://arxiv.org/abs/2506.14335", "title": "References Matter: Investigating the Impact of Reference Set Variation on Summarization Evaluation", "authors": ["Silvia Casola", "Yang Janet Liu", "Siyao Peng", "Oliver Kraus", "Albert Gatt", "Barbara Plank"], "categories": ["cs.CL"], "comment": null, "summary": "Human language production exhibits remarkable richness and variation,\nreflecting diverse communication styles and intents. However, this variation is\noften overlooked in summarization evaluation. While having multiple reference\nsummaries is known to improve correlation with human judgments, the impact of\nthe reference set on reference-based metrics has not been systematically\ninvestigated. This work examines the sensitivity of widely used reference-based\nmetrics in relation to the choice of reference sets, analyzing three diverse\nmulti-reference summarization datasets: SummEval, GUMSum, and DUC2004. We\ndemonstrate that many popular metrics exhibit significant instability. This\ninstability is particularly concerning for n-gram-based metrics like ROUGE,\nwhere model rankings vary depending on the reference sets, undermining the\nreliability of model comparisons. We also collect human judgments on LLM\noutputs for genre-diverse data and examine their correlation with metrics to\nsupplement existing findings beyond newswire summaries, finding weak-to-no\ncorrelation. Taken together, we recommend incorporating reference set variation\ninto summarization evaluation to enhance consistency alongside correlation with\nhuman judgments, especially when evaluating LLMs."}
{"id": "2506.19037", "pdf": "https://arxiv.org/pdf/2506.19037.pdf", "abs": "https://arxiv.org/abs/2506.19037", "title": "Plan for Speed: Dilated Scheduling for Masked Diffusion Language Models", "authors": ["Omer Luxembourg", "Haim Permuter", "Eliya Nachmani"], "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "cs.NE", "math.IT"], "comment": null, "summary": "Masked diffusion language models (MDLMs) promise fast, non-autoregressive\ntext generation, yet existing samplers, which pick tokens to unmask based on\nmodel confidence, ignore interactions when unmasking multiple positions in\nparallel and effectively reduce to slow, autoregressive behavior. We propose\nthe Dilated Unmasking Scheduler (DUS), an inference-only, planner-model-free\nmethod that partitions sequence positions into non-adjacent dilated groups and\nunmasked them in parallel so as to minimize an upper bound on joint entropy\ngain at each denoising step. By explicitly trading off the number of network\ncalls against generation quality, DUS recovers most of the performance lost\nunder traditional parallel unmasking strategies. Across math (GSM8K, MATH500),\ncode (HumanEval, MBPP) and general-knowledge benchmarks (BBH, MMLU-Pro), DUS\noutperforms confidence-based planners, without modifying the underlying\ndenoiser, and reveals the true speed-quality frontier of MDLMs."}
{"id": "2506.19315", "pdf": "https://arxiv.org/pdf/2506.19315.pdf", "abs": "https://arxiv.org/abs/2506.19315", "title": "JCAPT: A Joint Modeling Approach for CAPT", "authors": ["Tzu-Hsuan Yang", "Yue-Yang He", "Berlin Chen"], "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": "Accepted to the ISCA SLaTE-2025 Workshop", "summary": "Effective pronunciation feedback is critical in second language (L2)\nlearning, for which computer-assisted pronunciation training (CAPT) systems\noften encompass two key tasks: automatic pronunciation assessment (APA) and\nmispronunciation detection and diagnosis (MDD). Recent work has shown that\njoint modeling of these two tasks can yield mutual benefits. Our unified\nframework leverages Mamba, a selective state space model (SSM), while\nintegrating phonological features and think token strategies to jointly enhance\ninterpretability and fine-grained temporal reasoning in APA and MDD. To our\nknowledge, this is the first study to combine phonological attribution,\nSSM-based modeling, and prompting in CAPT. A series of experiments conducted on\nthe speechocean762 benchmark demonstrate that our model consistently\noutperforms prior methods, particularly on the MDD task."}
{"id": "2507.08013", "pdf": "https://arxiv.org/pdf/2507.08013.pdf", "abs": "https://arxiv.org/abs/2507.08013", "title": "MedicalBERT: enhancing biomedical natural language processing using pretrained BERT-based model", "authors": ["K. Sahit Reddy", "N. Ragavenderan", "Vasanth K.", "Ganesh N. Naik", "Vishalakshi Prabhu", "Nagaraja G. S"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in natural language processing (NLP) have been driven\nbypretrained language models like BERT, RoBERTa, T5, and GPT. Thesemodels excel\nat understanding complex texts, but biomedical literature, withits\ndomain-specific terminology, poses challenges that models likeWord2Vec and\nbidirectional long short-term memory (Bi-LSTM) can't fullyaddress. GPT and T5,\ndespite capturing context, fall short in tasks needingbidirectional\nunderstanding, unlike BERT. Addressing this, we proposedMedicalBERT, a\npretrained BERT model trained on a large biomedicaldataset and equipped with\ndomain-specific vocabulary that enhances thecomprehension of biomedical\nterminology. MedicalBERT model is furtheroptimized and fine-tuned to address\ndiverse tasks, including named entityrecognition, relation extraction, question\nanswering, sentence similarity, anddocument classification. Performance metrics\nsuch as the F1-score,accuracy, and Pearson correlation are employed to showcase\nthe efficiencyof our model in comparison to other BERT-based models such as\nBioBERT,SciBERT, and ClinicalBERT. MedicalBERT outperforms these models onmost\nof the benchmarks, and surpasses the general-purpose BERT model by5.67% on\naverage across all the tasks evaluated respectively. This work alsounderscores\nthe potential of leveraging pretrained BERT models for medicalNLP tasks,\ndemonstrating the effectiveness of transfer learning techniques incapturing\ndomain-specific information.\n  (PDF) MedicalBERT: enhancing biomedical natural language processing using\npretrained BERT-based model. Available from:\nhttps://www.researchgate.net/publication/392489050_MedicalBERT_enhancing_biomedical_natural_language_processing_using_pretrained_BERT-based_model\n[accessed Jul 06 2025]."}
{"id": "2507.13618", "pdf": "https://arxiv.org/pdf/2507.13618.pdf", "abs": "https://arxiv.org/abs/2507.13618", "title": "Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters", "authors": ["Shanbo Cheng", "Yu Bao", "Qian Cao", "Luyang Huang", "Liyan Kang", "Zhicheng Liu", "Yu Lu", "Wenhao Zhu", "Jingwen Chen", "Zhichao Huang", "Tao Li", "Yifu Li", "Huiying Lin", "Sitong Liu", "Ningxin Peng", "Shuaijie She", "Lu Xu", "Nuo Xu", "Sen Yang", "Runsheng Yu", "Yiming Yu", "Liehao Zou", "Hang Li", "Lu Lu", "Yuxuan Wang", "Yonghui Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multilingual translation stands as a challenging task for large language\nmodels (LLMs) to handle intricate language patterns and stilted translations\nthat arise in automated translations. In this paper, we introduce Seed-X, a\nfamily of open-source LLMs comprising instruct and reasoning models, pushing\nthe limits of translation capability with 7B parameter size. The base model is\npre-trained on a diverse, high-quality dataset encompassing both monolingual\nand bilingual content across 28 languages, harnessing the full potential of\nmultilingual data. The instruct model is then finetuned to translate by\nChain-of-Thought (CoT) reasoning and further enhanced through reinforcement\nlearning (RL) to achieve better generalization across diverse language pairs.\nSeed-X achieves performance comparable to leading closed-source models,\nincluding Gemini-2.5 and GPT-4o, across 28 languages, and significantly\noutperforms larger open-source models in both automatic metrics and human\nevaluations. We share the best practices through our optimization process, and\nmake the parameter public available for advancing translation research and\napplications."}
{"id": "2507.14241", "pdf": "https://arxiv.org/pdf/2507.14241.pdf", "abs": "https://arxiv.org/abs/2507.14241", "title": "Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models", "authors": ["Rithesh Murthy", "Ming Zhu", "Liangwei Yang", "Jielin Qiu", "Juntao Tan", "Shelby Heinecke", "Caiming Xiong", "Silvio Savarese", "Huan Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) perform best with well-crafted prompts, yet\nprompt engineering remains manual, inconsistent, and inaccessible to\nnon-experts. We introduce Promptomatix, an automatic prompt optimization\nframework that transforms natural language task descriptions into high-quality\nprompts without requiring manual tuning or domain expertise. Promptomatix\nsupports both a lightweight meta-prompt-based optimizer and a DSPy-powered\ncompiler, with modular design enabling future extension to more advanced\nframeworks. The system analyzes user intent, generates synthetic training data,\nselects prompting strategies, and refines prompts using cost-aware objectives.\nEvaluated across 5 task categories, Promptomatix achieves competitive or\nsuperior performance compared to existing libraries, while reducing prompt\nlength and computational overhead making prompt optimization scalable and\nefficient."}
{"id": "2507.15742", "pdf": "https://arxiv.org/pdf/2507.15742.pdf", "abs": "https://arxiv.org/abs/2507.15742", "title": "A Fisher's exact test justification of the TF-IDF term-weighting scheme", "authors": ["Paul Sheridan", "Zeyad Ahmed", "Aitazaz A. Farooque"], "categories": ["cs.CL", "cs.IR", "math.ST", "stat.TH"], "comment": "23 pages, 4 tables, accepted in The American Statistician 2025", "summary": "Term frequency-inverse document frequency, or TF-IDF for short, is arguably\nthe most celebrated mathematical expression in the history of information\nretrieval. Conceived as a simple heuristic quantifying the extent to which a\ngiven term's occurrences are concentrated in any one given document out of\nmany, TF-IDF and its many variants are routinely used as term-weighting schemes\nin diverse text analysis applications. There is a growing body of scholarship\ndedicated to placing TF-IDF on a sound theoretical foundation. Building on that\ntradition, this paper justifies the use of TF-IDF to the statistics community\nby demonstrating how the famed expression can be understood from a significance\ntesting perspective. We show that the common TF-IDF variant TF-ICF is, under\nmild regularity conditions, closely related to the negative logarithm of the\n$p$-value from a one-tailed version of Fisher's exact test of statistical\nsignificance. As a corollary, we establish a connection between TF-IDF and the\nsaid negative log-transformed $p$-value under certain idealized assumptions. We\nfurther demonstrate, as a limiting case, that this same quantity converges to\nTF-IDF in the limit of an infinitely large document collection. The Fisher's\nexact test justification of TF-IDF equips the working statistician with a ready\nexplanation of the term-weighting scheme's long-established effectiveness."}
{"id": "2507.15850", "pdf": "https://arxiv.org/pdf/2507.15850.pdf", "abs": "https://arxiv.org/abs/2507.15850", "title": "3LM: Bridging Arabic, STEM, and Code through Benchmarking", "authors": ["Basma El Amel Boussaha", "Leen AlQadi", "Mugariya Farooq", "Shaikha Alsuwaidi", "Giulia Campesan", "Ahmed Alzubaidi", "Mohammed Alyafeai", "Hakim Hacid"], "categories": ["cs.CL"], "comment": null, "summary": "Arabic is one of the most widely spoken languages in the world, yet efforts\nto develop and evaluate Large Language Models (LLMs) for Arabic remain\nrelatively limited. Most existing Arabic benchmarks focus on linguistic,\ncultural, or religious content, leaving a significant gap in domains like STEM\nand code which are increasingly relevant for real-world LLM applications. To\nhelp bridge this gap, we present 3LM, a suite of three benchmarks designed\nspecifically for Arabic. The first is a set of STEM-related question-answer\npairs, naturally sourced from Arabic textbooks and educational worksheets. The\nsecond consists of synthetically generated STEM questions, created using the\nsame sources. The third benchmark focuses on code generation, built through a\ncareful translation of two widely used code benchmarks, incorporating a\nhuman-in-the-loop process with several rounds of review to ensure high-quality\nand faithful translations. We release all three benchmarks publicly to support\nthe growth of Arabic LLM research in these essential but underrepresented\nareas."}
{"id": "2507.16331", "pdf": "https://arxiv.org/pdf/2507.16331.pdf", "abs": "https://arxiv.org/abs/2507.16331", "title": "Re:Form -- Reducing Human Priors in Scalable Formal Software Verification with RL in LLMs: A Preliminary Study on Dafny", "authors": ["Chuanhao Yan", "Fengdi Che", "Xuhan Huang", "Xu Xu", "Xin Li", "Yizhi Li", "Xingwei Qu", "Jingzhe Shi", "Zhuangzhuang He", "Chenghua Lin", "Yaodong Yang", "Binhang Yuan", "Hang Zhao", "Yu Qiao", "Bowen Zhou", "Jie Fu"], "categories": ["cs.CL"], "comment": null, "summary": "Existing informal language-based (e.g., human language) Large Language Models\n(LLMs) trained with Reinforcement Learning (RL) face a significant challenge:\ntheir verification processes, which provide crucial training signals, are\nneither reliable nor scalable. In fact, the prevalent large proprietary models\ncould hardly generate verifiable programs. A promising yet largely uncharted\nalternative is formal language-based reasoning. Grounding LLMs in rigorous\nformal systems where generative models operate in formal language spaces (e.g.,\nDafny) enables the automatic and mathematically provable verification of their\nreasoning processes and outcomes. This capability is pivotal for achieving\nlarge-scale, reliable formal software verification. It is a common practice to\nemploy human-annotated chain-of-thought and other human priors to induce the\nreasoning and coding capabilities of LLMs. Unfortunately, it becomes\nunacceptably all-consuming to provide such priors for supervising complex\nprogramming tasks. In this work, we systematically explore ways to reduce human\npriors with the formal language, Dafny, as the main environment for our pilot\nstudy. Our pipeline mainly relies on introducing an automatic and scalable data\ncuration pipeline, and careful RL designs integrated with feedback from the\nformal language verifier. We introduce DafnyComp, a benchmark of compositional\nformal programs with auto-formalized specifications for specification\nreasoning. Our supervised fine-tuning (SFT) stage enables even small models\n(e.g., 0.5B) to generate syntactically valid and verifiable Dafny code,\nsurpassing proprietary models. RL with regularization further improves\nperformance, achieving stronger generalization to out-of-domain tasks and\noutperforming all strong baselines on the challenging DafnyComp benchmark."}
{"id": "2507.17974", "pdf": "https://arxiv.org/pdf/2507.17974.pdf", "abs": "https://arxiv.org/abs/2507.17974", "title": "Natural Language Processing for Tigrinya: Current State and Future Directions", "authors": ["Fitsum Gaim", "Jong C. Park"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "Despite being spoken by millions of people, Tigrinya remains severely\nunderrepresented in Natural Language Processing (NLP) research. This work\npresents a comprehensive survey of NLP research for Tigrinya, analyzing over 40\nstudies spanning more than a decade of work from 2011 to 2025. We\nsystematically review the current state of computational resources, models, and\napplications across ten distinct downstream tasks, including morphological\nprocessing, machine translation, speech recognition, and question-answering.\nOur analysis reveals a clear trajectory from foundational, rule-based systems\nto modern neural architectures, with progress consistently unlocked by resource\ncreation milestones. We identify key challenges rooted in Tigrinya's\nmorphological complexity and resource scarcity, while highlighting promising\nresearch directions, including morphology-aware modeling, cross-lingual\ntransfer, and community-centered resource development. This work serves as both\na comprehensive reference for researchers and a roadmap for advancing Tigrinya\nNLP. A curated metadata of the surveyed studies and resources is made publicly\navailable."}
{"id": "2507.18013", "pdf": "https://arxiv.org/pdf/2507.18013.pdf", "abs": "https://arxiv.org/abs/2507.18013", "title": "Technical Report of TeleChat2, TeleChat2.5 and T1", "authors": ["Zihan Wang", "Xinzhang Liu", "Yitong Yao", "Chao Wang", "Yu Zhao", "Zhihao Yang", "Wenmin Deng", "Kaipeng Jia", "Jiaxin Peng", "Yuyao Huang", "Sishi Xiong", "Zhuo Jiang", "Kaidong Yu", "Xiaohui Hu", "Fubei Yao", "Ruiyu Fang", "Zhuoru Jiang", "Ruiting Song", "Qiyi Xie", "Rui Xue", "Xuewei He", "Yanlei Xue", "Zhu Yuan", "Zhaoxi Zhang", "Zilu Huang", "Shiquan Wang", "Xin Wang", "Hanming Wu", "Mingyuan Wang", "Xufeng Zhan", "Yuhan Sun", "Zhaohu Xing", "Yuhao Jiang", "Bingkai Yang", "Shuangyong Song", "Yongxiang Li", "Zhongjiang He", "Xuelong Li"], "categories": ["cs.CL", "I.2.7"], "comment": "32 pages, 5 figures", "summary": "We introduce the latest series of TeleChat models: \\textbf{TeleChat2},\n\\textbf{TeleChat2.5}, and \\textbf{T1}, offering a significant upgrade over\ntheir predecessor, TeleChat. Despite minimal changes to the model architecture,\nthe new series achieves substantial performance gains through enhanced training\nstrategies in both pre-training and post-training stages. The series begins\nwith \\textbf{TeleChat2}, which undergoes pretraining on 10 trillion\nhigh-quality and diverse tokens. This is followed by Supervised Fine-Tuning\n(SFT) and Direct Preference Optimization (DPO) to further enhance its\ncapabilities. \\textbf{TeleChat2.5} and \\textbf{T1} expand the pipeline by\nincorporating a continual pretraining phase with domain-specific datasets,\ncombined with reinforcement learning (RL) to improve performance in code\ngeneration and mathematical reasoning tasks. The \\textbf{T1} variant is\ndesigned for complex reasoning, supporting long Chain-of-Thought (CoT)\nreasoning and demonstrating substantial improvements in mathematics and coding.\nIn contrast, \\textbf{TeleChat2.5} prioritizes speed, delivering rapid\ninference. Both flagship models of \\textbf{T1} and \\textbf{TeleChat2.5} are\ndense Transformer-based architectures with 115B parameters, showcasing\nsignificant advancements in reasoning and general task performance compared to\nthe original TeleChat. Notably, \\textbf{T1-115B} outperform proprietary models\nsuch as OpenAI's o1-mini and GPT-4o. We publicly release \\textbf{TeleChat2},\n\\textbf{TeleChat2.5} and \\textbf{T1}, including post-trained versions with 35B\nand 115B parameters, to empower developers and researchers with\nstate-of-the-art language models tailored for diverse applications."}
{"id": "2507.18119", "pdf": "https://arxiv.org/pdf/2507.18119.pdf", "abs": "https://arxiv.org/abs/2507.18119", "title": "GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness", "authors": ["Hongjie Chen", "Zehan Li", "Yaodong Song", "Wenming Deng", "Yitong Yao", "Yuxin Zhang", "Hang Lv", "Xuechao Zhu", "Jian Kang", "Jie Lian", "Jie Li", "Chao Wang", "Shuangyong Song", "Yongxiang Li", "Zhongjiang He", "Xuelong Li"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Recent advances in end-to-end spoken language models (SLMs) have\nsignificantly improved the ability of AI systems to engage in natural spoken\ninteractions. However, most existing models treat speech merely as a vehicle\nfor linguistic content, often overlooking the rich paralinguistic and speaker\ncharacteristic cues embedded in human speech, such as dialect, age, emotion,\nand non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel\nspoken language model with paralinguistic and speaker characteristic awareness,\ndesigned to extend spoken language modeling beyond text semantics. GOAT-SLM\nadopts a dual-modality head architecture that decouples linguistic modeling\nfrom acoustic realization, enabling robust language understanding while\nsupporting expressive and adaptive speech generation. To enhance model\nefficiency and versatility, we propose a modular, staged training strategy that\nprogressively aligns linguistic, paralinguistic, and speaker characteristic\ninformation using large-scale speech-text corpora. Experimental results on\nTELEVAL, a multi-dimensional evaluation benchmark, demonstrate that GOAT-SLM\nachieves well-balanced performance across both semantic and non-semantic tasks,\nand outperforms existing open-source models in handling emotion, dialectal\nvariation, and age-sensitive interactions. This work highlights the importance\nof modeling beyond linguistic content and advances the development of more\nnatural, adaptive, and socially aware spoken language systems."}
{"id": "2507.18143", "pdf": "https://arxiv.org/pdf/2507.18143.pdf", "abs": "https://arxiv.org/abs/2507.18143", "title": "HIVMedQA: Benchmarking large language models for HIV medical decision support", "authors": ["Gonzalo Cardenal-Antolin", "Jacques Fellay", "Bashkim Jaha", "Roger Kouyos", "Niko Beerenwinkel", "Diane Duroux"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are emerging as valuable tools to support\nclinicians in routine decision-making. HIV management is a compelling use case\ndue to its complexity, including diverse treatment options, comorbidities, and\nadherence challenges. However, integrating LLMs into clinical practice raises\nconcerns about accuracy, potential harm, and clinician acceptance. Despite\ntheir promise, AI applications in HIV care remain underexplored, and LLM\nbenchmarking studies are scarce. This study evaluates the current capabilities\nof LLMs in HIV management, highlighting their strengths and limitations. We\nintroduce HIVMedQA, a benchmark designed to assess open-ended medical question\nanswering in HIV care. The dataset consists of curated, clinically relevant\nquestions developed with input from an infectious disease physician. We\nevaluated seven general-purpose and three medically specialized LLMs, applying\nprompt engineering to enhance performance. Our evaluation framework\nincorporates both lexical similarity and an LLM-as-a-judge approach, extended\nto better reflect clinical relevance. We assessed performance across key\ndimensions: question comprehension, reasoning, knowledge recall, bias,\npotential harm, and factual accuracy. Results show that Gemini 2.5 Pro\nconsistently outperformed other models across most dimensions. Notably, two of\nthe top three models were proprietary. Performance declined as question\ncomplexity increased. Medically fine-tuned models did not always outperform\ngeneral-purpose ones, and larger model size was not a reliable predictor of\nperformance. Reasoning and comprehension were more challenging than factual\nrecall, and cognitive biases such as recency and status quo were observed.\nThese findings underscore the need for targeted development and evaluation to\nensure safe, effective LLM integration in clinical care."}
{"id": "2401.10337", "pdf": "https://arxiv.org/pdf/2401.10337.pdf", "abs": "https://arxiv.org/abs/2401.10337", "title": "Noise Contrastive Estimation-based Matching Framework for Low-Resource Security Attack Pattern Recognition", "authors": ["Tu Nguyen", "Nedim Šrndić", "Alexander Neth"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": "accepted at EACL 2024, in ARR October 2023", "summary": "Tactics, Techniques and Procedures (TTPs) represent sophisticated attack\npatterns in the cybersecurity domain, described encyclopedically in textual\nknowledge bases. Identifying TTPs in cybersecurity writing, often called TTP\nmapping, is an important and challenging task. Conventional learning approaches\noften target the problem in the classical multi-class or multilabel\nclassification setting. This setting hinders the learning ability of the model\ndue to a large number of classes (i.e., TTPs), the inevitable skewness of the\nlabel distribution and the complex hierarchical structure of the label space.\nWe formulate the problem in a different learning paradigm, where the assignment\nof a text to a TTP label is decided by the direct semantic similarity between\nthe two, thus reducing the complexity of competing solely over the large\nlabeling space. To that end, we propose a neural matching architecture with an\neffective sampling-based learn-to-compare mechanism, facilitating the learning\nprocess of the matching model despite constrained resources."}
{"id": "2405.06270", "pdf": "https://arxiv.org/pdf/2405.06270.pdf", "abs": "https://arxiv.org/abs/2405.06270", "title": "XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced In-Context Learning in Healthcare", "authors": ["Fatemeh Nazary", "Yashar Deldjoo", "Tommaso Di Noia", "Eugenio di Sciascio"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Clinical decision support systems require models that are not only highly\naccurate but also equitable and sensitive to the implications of missed\ndiagnoses. In this study, we introduce a knowledge-guided in-context learning\n(ICL) framework designed to enable large language models (LLMs) to effectively\nprocess structured clinical data. Our approach integrates domain-specific\nfeature groupings, carefully balanced few-shot examples, and task-specific\nprompting strategies. We systematically evaluate this method across seventy\ndistinct ICL designs by various prompt variations and two different\ncommunication styles-natural-language narrative and numeric conversational-and\ncompare its performance to robust classical machine learning (ML) benchmarks on\ntasks involving heart disease and diabetes prediction.\n  Our findings indicate that while traditional ML models maintain superior\nperformance in balanced precision-recall scenarios, LLMs employing narrative\nprompts with integrated domain knowledge achieve higher recall and\nsignificantly reduce gender bias, effectively narrowing fairness disparities by\nan order of magnitude. Despite the current limitation of increased inference\nlatency, LLMs provide notable advantages, including the capacity for zero-shot\ndeployment and enhanced equity. This research offers the first comprehensive\nanalysis of ICL design considerations for applying LLMs to tabular clinical\ntasks and highlights distillation and multimodal extensions as promising\ndirections for future research."}
{"id": "2406.14117", "pdf": "https://arxiv.org/pdf/2406.14117.pdf", "abs": "https://arxiv.org/abs/2406.14117", "title": "An Investigation of Prompt Variations for Zero-shot LLM-based Rankers", "authors": ["Shuoqi Sun", "Shengyao Zhuang", "Shuai Wang", "Guido Zuccon"], "categories": ["cs.IR", "cs.CL"], "comment": "Accepted for publication at the 47th European Conference on\n  Information Retrieval (ECIR 2025)", "summary": "We provide a systematic understanding of the impact of specific components\nand wordings used in prompts on the effectiveness of rankers based on zero-shot\nLarge Language Models (LLMs). Several zero-shot ranking methods based on LLMs\nhave recently been proposed. Among many aspects, methods differ across (1) the\nranking algorithm they implement, e.g., pointwise vs. listwise, (2) the\nbackbone LLMs used, e.g., GPT3.5 vs. FLAN-T5, (3) the components and wording\nused in prompts, e.g., the use or not of role-definition (role-playing) and the\nactual words used to express this. It is currently unclear whether performance\ndifferences are due to the underlying ranking algorithm, or because of spurious\nfactors such as better choice of words used in prompts. This confusion risks to\nundermine future research. Through our large-scale experimentation and\nanalysis, we find that ranking algorithms do contribute to differences between\nmethods for zero-shot LLM ranking. However, so do the LLM backbones -- but even\nmore importantly, the choice of prompt components and wordings affect the\nranking. In fact, in our experiments, we find that, at times, these latter\nelements have more impact on the ranker's effectiveness than the actual ranking\nalgorithms, and that differences among ranking methods become more blurred when\nprompt variations are considered."}
{"id": "2409.00920", "pdf": "https://arxiv.org/pdf/2409.00920.pdf", "abs": "https://arxiv.org/abs/2409.00920", "title": "ToolACE: Winning the Points of LLM Function Calling", "authors": ["Weiwen Liu", "Xu Huang", "Xingshan Zeng", "Xinlong Hao", "Shuai Yu", "Dexun Li", "Shuai Wang", "Weinan Gan", "Zhengying Liu", "Yuanqing Yu", "Zezhong Wang", "Yuxian Wang", "Wu Ning", "Yutai Hou", "Bin Wang", "Chuhan Wu", "Xinzhi Wang", "Yong Liu", "Yasheng Wang", "Duyu Tang", "Dandan Tu", "Lifeng Shang", "Xin Jiang", "Ruiming Tang", "Defu Lian", "Qun Liu", "Enhong Chen"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "21 pages, 22 figures", "summary": "Function calling significantly extends the application boundary of large\nlanguage models, where high-quality and diverse training data is critical for\nunlocking this capability. However, real function-calling data is quite\nchallenging to collect and annotate, while synthetic data generated by existing\npipelines tends to lack coverage and accuracy. In this paper, we present\nToolACE, an automatic agentic pipeline designed to generate accurate, complex,\nand diverse tool-learning data. ToolACE leverages a novel self-evolution\nsynthesis process to curate a comprehensive API pool of 26,507 diverse APIs.\nDialogs are further generated through the interplay among multiple agents,\nguided by a formalized thinking process. To ensure data accuracy, we implement\na dual-layer verification system combining rule-based and model-based checks.\nWe demonstrate that models trained on our synthesized data, even with only 8B\nparameters, achieve state-of-the-art performance on the Berkeley\nFunction-Calling Leaderboard, rivaling the latest GPT-4 models. Our model and a\nsubset of the data are publicly available at https://huggingface.co/Team-ACE."}
{"id": "2411.18651", "pdf": "https://arxiv.org/pdf/2411.18651.pdf", "abs": "https://arxiv.org/abs/2411.18651", "title": "Verbalized Representation Learning for Interpretable Few-Shot Generalization", "authors": ["Cheng-Fu Yang", "Da Yin", "Wenbo Hu", "Nanyun Peng", "Bolei Zhou", "Kai-Wei Chang"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Accepted to ICCV 2025", "summary": "Humans recognize objects after observing only a few examples, a remarkable\ncapability enabled by their inherent language understanding of the real-world\nenvironment. Developing verbalized and interpretable representation can\nsignificantly improve model generalization in low-data settings. In this work,\nwe propose Verbalized Representation Learning (VRL), a novel approach for\nautomatically extracting human-interpretable features for object recognition\nusing few-shot data. Our method uniquely captures inter-class differences and\nintra-class commonalities in the form of natural language by employing a\nVision-Language Model (VLM) to identify key discriminative features between\ndifferent classes and shared characteristics within the same class. These\nverbalized features are then mapped to numeric vectors through the VLM. The\nresulting feature vectors can be further utilized to train and infer with\ndownstream classifiers. Experimental results show that, at the same model\nscale, VRL achieves a 24% absolute improvement over prior state-of-the-art\nmethods while using 95% less data and a smaller mode. Furthermore, compared to\nhuman-labeled attributes, the features learned by VRL exhibit a 20% absolute\ngain when used for downstream classification tasks. Code is available at:\nhttps://github.com/joeyy5588/VRL/tree/main."}
{"id": "2411.19628", "pdf": "https://arxiv.org/pdf/2411.19628.pdf", "abs": "https://arxiv.org/abs/2411.19628", "title": "Accelerating Multimodal Large Language Models via Dynamic Visual-Token Exit and the Empirical Findings", "authors": ["Qiong Wu", "Wenhao Lin", "Yiyi Zhou", "Weihao Ye", "Zhanpeng Zen", "Xiaoshuai Sun", "Rongrong Ji"], "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM"], "comment": null, "summary": "The excessive use of visual tokens in existing Multimoal Large Language\nModels (MLLMs) often exhibits obvious redundancy and brings in prohibitively\nexpensive computation. To gain insights into this problem, we first conduct\nextensive empirical studies on the attention behaviors of MLLMs, and summarize\nthree main inference stages in MLLMs: (i) Early fusion between tokens is first\naccomplished quickly. (ii) Intra-modality modeling then comes to play. (iii)\nMultimodal reasoning} resumes and lasts until the end of inference. In\nparticular, we reveal that visual tokens will stop contributing to reasoning\nwhen the text tokens receive enough image information, yielding obvious visual\nredundancy. Based on these generalized observations, we propose a simple yet\neffective method to improve the efficiency of MLLMs, termed dynamic\nvisual-token exit (DyVTE). DyVTE uses lightweight hyper-networks to perceive\nthe text token status and decide the removal of all visual tokens after a\ncertain layer, thereby addressing the observed visual redundancy. To validate\nVTE, we apply it to a set of MLLMs, including LLaVA, VILA, Eagle and InternVL,\nand conduct extensive experiments on a bunch of benchmarks. The experiment\nresults not only show the effectiveness of our VTE in improving MLLMs'\nefficiency, but also yield the general modeling patterns of MLLMs, well\nfacilitating the in-depth understanding of MLLMs. Our code is released at\nhttps://github.com/DoubtedSteam/DyVTE."}
{"id": "2502.03032", "pdf": "https://arxiv.org/pdf/2502.03032.pdf", "abs": "https://arxiv.org/abs/2502.03032", "title": "Analyze Feature Flow to Enhance Interpretation and Steering in Language Models", "authors": ["Daniil Laptev", "Nikita Balagansky", "Yaroslav Aksenov", "Daniil Gavrilov"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We introduce a new approach to systematically map features discovered by\nsparse autoencoder across consecutive layers of large language models,\nextending earlier work that examined inter-layer feature links. By using a\ndata-free cosine similarity technique, we trace how specific features persist,\ntransform, or first appear at each stage. This method yields granular flow\ngraphs of feature evolution, enabling fine-grained interpretability and\nmechanistic insights into model computations. Crucially, we demonstrate how\nthese cross-layer feature maps facilitate direct steering of model behavior by\namplifying or suppressing chosen features, achieving targeted thematic control\nin text generation. Together, our findings highlight the utility of a causal,\ncross-layer interpretability framework that not only clarifies how features\ndevelop through forward passes but also provides new means for transparent\nmanipulation of large language models."}
{"id": "2502.08606", "pdf": "https://arxiv.org/pdf/2502.08606.pdf", "abs": "https://arxiv.org/abs/2502.08606", "title": "Distillation Scaling Laws", "authors": ["Dan Busbridge", "Amitis Shidani", "Floris Weers", "Jason Ramapuram", "Etai Littwin", "Russ Webb"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "Version accepted to ICML 2025. 69 pages, 54 figures, 13 tables", "summary": "We propose a distillation scaling law that estimates distilled model\nperformance based on a compute budget and its allocation between the student\nand teacher. Our findings mitigate the risks associated with large-scale\ndistillation by enabling compute-optimal allocation for both the teacher and\nstudent to maximize student performance. We provide compute-optimal\ndistillation recipes for two key scenarios: when a teacher already exists, and\nwhen a teacher needs training. In settings involving many students or an\nexisting teacher, distillation outperforms supervised learning up to a compute\nlevel that scales predictably with student size. Conversely, if only one\nstudent is to be distilled and a teacher also requires training, supervised\nlearning is generally preferable. Additionally, our large-scale study of\ndistillation increases our understanding of the process and helps inform\nexperimental design."}
{"id": "2504.10519", "pdf": "https://arxiv.org/pdf/2504.10519.pdf", "abs": "https://arxiv.org/abs/2504.10519", "title": "Toward Super Agent System with Hybrid AI Routers", "authors": ["Yuhang Yao", "Haixin Wang", "Yibo Chen", "Jiawen Wang", "Min Chang Jordan Ren", "Bosheng Ding", "Salman Avestimehr", "Chaoyang He"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": null, "summary": "AI Agents powered by Large Language Models are transforming the world through\nenormous applications. A super agent has the potential to fulfill diverse user\nneeds, such as summarization, coding, and research, by accurately understanding\nuser intent and leveraging the appropriate tools to solve tasks. However, to\nmake such an agent viable for real-world deployment and accessible at scale,\nsignificant optimizations are required to ensure high efficiency and low cost.\nThis position paper presents a design of the Super Agent System powered by the\nhybrid AI routers. Upon receiving a user prompt, the system first detects the\nintent of the user, then routes the request to specialized task agents with the\nnecessary tools or automatically generates agentic workflows. In practice, most\napplications directly serve as AI assistants on edge devices such as phones and\nrobots. As different language models vary in capability and cloud-based models\noften entail high computational costs, latency, and privacy concerns, we then\nexplore the hybrid mode where the router dynamically selects between local and\ncloud models based on task complexity. Finally, we introduce the blueprint of\nan on-device super agent enhanced with cloud. With advances in multi-modality\nmodels and edge hardware, we envision that most computations can be handled\nlocally, with cloud collaboration only as needed. Such architecture paves the\nway for super agents to be seamlessly integrated into everyday life in the near\nfuture."}
{"id": "2506.12479", "pdf": "https://arxiv.org/pdf/2506.12479.pdf", "abs": "https://arxiv.org/abs/2506.12479", "title": "AI Flow: Perspectives, Scenarios, and Approaches", "authors": ["Hongjun An", "Wenhan Hu", "Sida Huang", "Siqi Huang", "Ruanjun Li", "Yuanzhi Liang", "Jiawei Shao", "Yiliang Song", "Zihan Wang", "Cheng Yuan", "Chi Zhang", "Hongyuan Zhang", "Wenhao Zhuang", "Xuelong Li"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.DC", "eess.SP"], "comment": "Authors are with Institute of Artificial Intelligence (TeleAI), China\n  Telecom, China. Author names are listed alphabetically by surname. This work\n  was conducted at TeleAI, facilitated by Dr. Jiawei Shao (e-mail:\n  shaojw2@chinatelecom.cn) under the leadership of Prof. Xuelong Li. The\n  corresponding author is Prof. Xuelong Li (e-mail: xuelong li@ieee.org), the\n  CTO and Chief Scientist of China Telecom", "summary": "Pioneered by the foundational information theory by Claude Shannon and the\nvisionary framework of machine intelligence by Alan Turing, the convergent\nevolution of information and communication technologies (IT/CT) has created an\nunbroken wave of connectivity and computation. This synergy has sparked a\ntechnological revolution, now reaching its peak with large artificial\nintelligence (AI) models that are reshaping industries and redefining\nhuman-machine collaboration. However, the realization of ubiquitous\nintelligence faces considerable challenges due to substantial resource\nconsumption in large models and high communication bandwidth demands. To\naddress these challenges, AI Flow has been introduced as a multidisciplinary\nframework that integrates cutting-edge IT and CT advancements, with a\nparticular emphasis on the following three key points. First, device-edge-cloud\nframework serves as the foundation, which integrates end devices, edge servers,\nand cloud clusters to optimize scalability and efficiency for low-latency model\ninference. Second, we introduce the concept of familial models, which refers to\na series of different-sized models with aligned hidden features, enabling\neffective collaboration and the flexibility to adapt to varying resource\nconstraints and dynamic scenarios. Third, connectivity- and interaction-based\nintelligence emergence is a novel paradigm of AI Flow. By leveraging\ncommunication networks to enhance connectivity, the collaboration among AI\nmodels across heterogeneous nodes achieves emergent intelligence that surpasses\nthe capability of any single model. The innovations of AI Flow provide enhanced\nintelligence, timely responsiveness, and ubiquitous accessibility to AI\nservices, paving the way for the tighter fusion of AI techniques and\ncommunication systems."}
{"id": "2507.10616", "pdf": "https://arxiv.org/pdf/2507.10616.pdf", "abs": "https://arxiv.org/abs/2507.10616", "title": "Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces Them", "authors": ["Neel Rajani", "Aryo Pradipta Gema", "Seraphina Goldfarb-Tarrant", "Ivan Titov"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Training large language models (LLMs) for reasoning via maths and code\ndatasets has become a major new focus in LLM post-training. Two particularly\npopular approaches are reinforcement learning (RL) and supervised fine-tuning\n(SFT), but their training dynamics are poorly understood. We present a\ncomparative analysis of RL and SFT on the same maths problems with the same\nmodel and similar hyperparameters. We find that RL yields minor in-domain gains\non maths and slight degradation on knowledge-intensive benchmarks like MMLU,\nwhile both trends are more pronounced in SFT. We also analyse model parameters\nacross checkpoints, observing that both algorithms modify query and key weights\nthe most. Meanwhile, SFT exhibits greater updates and also affects mid-layer\nMLPs more, leading us to hypothesise that this may have caused the\nout-of-domain degradation. We therefore investigate whether freezing parts of\nthe model during training can mitigate the reduced performance on\nknowledge-intensive benchmarks. However, our results are inconclusive, with\nbenefits on GPQA:Diamond and degradation on other benchmarks. Taken together,\nour observations provide a preliminary indication for why RL amplifies existing\ncapabilities, while SFT replaces old skills with new ones."}
