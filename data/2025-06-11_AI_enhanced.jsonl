{"id": "2506.08200", "pdf": "https://arxiv.org/pdf/2506.08200.pdf", "abs": "https://arxiv.org/abs/2506.08200", "title": "AffectMachine-Pop: A controllable expert system for real-time pop music generation", "authors": ["Kat R. Agres", "Adyasha Dash", "Phoebe Chua", "Stefan K. Ehrlich"], "categories": ["cs.HC", "cs.MM"], "comment": null, "summary": "Music is a powerful medium for influencing listeners' emotional states, and\nthis capacity has driven a surge of research interest in AI-based affective\nmusic generation in recent years. Many existing systems, however, are a black\nbox which are not directly controllable, thus making these systems less\nflexible and adaptive to users. We present \\textit{AffectMachine-Pop}, an\nexpert system capable of generating retro-pop music according to arousal and\nvalence values, which can either be pre-determined or based on a listener's\nreal-time emotion states. To validate the efficacy of the system, we conducted\na listening study demonstrating that AffectMachine-Pop is capable of generating\naffective music at target levels of arousal and valence. The system is tailored\nfor use either as a tool for generating interactive affective music based on\nuser input, or for incorporation into biofeedback or neurofeedback systems to\nassist users with emotion self-regulation.", "AI": {"tldr": "AffectMachine-Pop generates retro-pop music based on user emotions, enhancing flexibility and user control.", "motivation": "To address the limitations of existing AI music systems that are not directly controllable, making them less adaptive to individual users' emotional needs.", "method": "The paper presents AffectMachine-Pop, which generates music according to specific arousal and valence values from either predetermined data or real-time user emotions, validated through a listening study.", "result": "The listening study confirmed that AffectMachine-Pop effectively generates music that meets target arousal and valence levels as intended.", "conclusion": "AffectMachine-Pop can be used as an interactive tool for music generation based on user emotions or integrated into bio/neurofeedback systems for emotion regulation.", "key_contributions": ["Introduction of a controllable AI music generation system", "Validation of music generation through user listening studies", "Application for emotion self-regulation in biofeedback systems"], "limitations": "", "keywords": ["affective music generation", "AI", "emotion self-regulation"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.08294", "pdf": "https://arxiv.org/pdf/2506.08294.pdf", "abs": "https://arxiv.org/abs/2506.08294", "title": "Z3Guide: A Scalable, Student-Centered, and Extensible Educational Environment for Logic Modeling", "authors": ["Ruanqianqian Huang", "Ayana Monroe", "Peli de Halleux", "Sorin Lerner", "Nikolaj Bjørner"], "categories": ["cs.HC", "cs.LO"], "comment": null, "summary": "Constraint-satisfaction problems (CSPs) are ubiquitous, ranging from\nbudgeting for grocery shopping to verifying software behavior. Logic modeling\nhelps solve CSPs programmatically using SMT solvers. Despite its importance in\nmany Computer Science disciplines, resources for teaching and learning logic\nmodeling are scarce and scattered, and challenges remain in designing\neducational environments for logic modeling that are accessible and meet the\nneeds of teachers and students. This paper explores how to design such an\nenvironment and probes the impact of the design on the learning experience.\nFrom a need-finding interview study and a design iteration with teachers of\nlogic modeling, we curated 10 design guidelines spanning three main\nrequirements: providing easy access, supporting various educational modalities,\nand allowing extensions for customized pedagogical needs. We implemented nine\nguidelines in Z3Guide, an open-source browser-based tool. Using Z3Guide in a\nlogic modeling learning workshop with more than 100 students, we gathered\npositive feedback on its support for learning and identified opportunities for\nfuture improvements.", "AI": {"tldr": "This paper discusses designing educational environments for teaching logic modeling in constraint-satisfaction problems, presenting guidelines and an implementation called Z3Guide that received positive feedback from students.", "motivation": "To address the lack of resources for teaching logic modeling and improve educational environments for both teachers and students.", "method": "The authors conducted need-finding interviews with teachers and iterated designs based on their feedback, resulting in ten design guidelines which were implemented in a tool called Z3Guide.", "result": "Z3Guide was used in a workshop with over 100 students, who provided positive feedback on its effectiveness in supporting the learning of logic modeling.", "conclusion": "The findings suggest that the designed educational environment meets the needs of students and teachers, while identifying areas for future improvement.", "key_contributions": ["Curated 10 design guidelines for teaching logic modeling", "Developed Z3Guide, an open-source tool for learning logic modeling", "Gathered and analyzed student feedback for enhancing educational tools"], "limitations": "The study primarily focuses on one educational setting and may need further validation in diverse contexts.", "keywords": ["logic modeling", "educational environments", "constraint-satisfaction problems", "Z3Guide", "student feedback"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.08303", "pdf": "https://arxiv.org/pdf/2506.08303.pdf", "abs": "https://arxiv.org/abs/2506.08303", "title": "EMG-Driven Stiffness-Modulating Palpation for Telerehabilitation", "authors": ["Thomas M. Kwok", "Hilary HY Cheng", "Wai Tuck Chow"], "categories": ["cs.HC"], "comment": "Accepted by the Workshop on Human-Robot Contact and Manipulation\n  (HRCM 2025) at RSS Conference 2025", "summary": "In this work, we introduce HJ-Pal, a lightweight wearable haptic device that\nleverages EMG-driven honeycomb jamming to render muscle activation as\nkinesthetic feedback, enabling remote palpation for small muscle assessment in\ntelerehabilitation.", "AI": {"tldr": "HJ-Pal is a wearable haptic device that provides muscle activation feedback for remote palpation in telerehabilitation.", "motivation": "To improve remote palpation techniques for small muscle assessment in telerehabilitation.", "method": "Development of a lightweight device utilizing EMG-driven honeycomb jamming for kinesthetic feedback.", "result": "HJ-Pal effectively transmits muscle activation feedback to users, enhancing their ability to perform remote palpation.", "conclusion": "HJ-Pal has significant potential applications in telerehabilitation, providing a novel approach to muscle assessment.", "key_contributions": ["Introduction of HJ-Pal as a novel wearable haptic device", "EMG-driven honeycomb jamming mechanism for feedback", "Enhancement of telerehabilitation practices."], "limitations": "", "keywords": ["wearable technology", "haptic feedback", "telerehabilitation"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.08443", "pdf": "https://arxiv.org/pdf/2506.08443.pdf", "abs": "https://arxiv.org/abs/2506.08443", "title": "SakugaFlow: A Stagewise Illustration Framework Emulating the Human Drawing Process and Providing Interactive Tutoring for Novice Drawing Skills", "authors": ["Kazuki Kawamura", "Jun Rekimoto"], "categories": ["cs.HC", "cs.CV", "68T05", "H.5.2; K.3; I.2.7"], "comment": "5 pages, 1 figure; accepted as a paper to the Generative AI and HCI\n  (GenAICHI) workshop at CHI 2025 (Yokohama, 27 Apr 2025)", "summary": "While current AI illustration tools can generate high-quality images from\ntext prompts, they rarely reveal the step-by-step procedure that human artists\nfollow. We present SakugaFlow, a four-stage pipeline that pairs diffusion-based\nimage generation with a large-language-model tutor. At each stage, novices\nreceive real-time feedback on anatomy, perspective, and composition, revise any\nstep non-linearly, and branch alternative versions. By exposing intermediate\noutputs and embedding pedagogical dialogue, SakugaFlow turns a black-box\ngenerator into a scaffolded learning environment that supports both creative\nexploration and skills acquisition.", "AI": {"tldr": "SakugaFlow integrates a four-stage pipeline for AI image generation with a large-language-model tutor to enhance learning for novice artists by providing feedback and allowing non-linear revisions.", "motivation": "Current AI illustration tools lack transparency in the creative process of human artists, failing to support novice artists effectively.", "method": "The paper presents SakugaFlow, a pipeline that pairs diffusion-based image generation with real-time feedback mechanisms in a four-stage process, allowing for interactive learning and revisions.", "result": "SakugaFlow facilitates a creative learning environment where novices can understand artistic principles and receive iterative feedback, leading to improved artistic skills.", "conclusion": "By revealing the generative process and incorporating dialogue, SakugaFlow transforms image generation into an educational tool that enhances understanding of art fundamentals.", "key_contributions": ["Development of a four-stage generative pipeline for AI-assisted art creation", "Integration of real-time feedback from a large-language model", "Creation of a scaffolded environment for artistic skills improvement"], "limitations": "", "keywords": ["AI illustration", "HCI", "learning environment", "image generation", "large-language models"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2506.08120", "pdf": "https://arxiv.org/pdf/2506.08120.pdf", "abs": "https://arxiv.org/abs/2506.08120", "title": "Conservative Bias in Large Language Models: Measuring Relation Predictions", "authors": ["Toyin Aguda", "Erik Wilson", "Allan Anzagira", "Simerjot Kaur", "Charese Smiley"], "categories": ["cs.CL"], "comment": "10 pages", "summary": "Large language models (LLMs) exhibit pronounced conservative bias in relation\nextraction tasks, frequently defaulting to No_Relation label when an\nappropriate option is unavailable. While this behavior helps prevent incorrect\nrelation assignments, our analysis reveals that it also leads to significant\ninformation loss when reasoning is not explicitly included in the output. We\nsystematically evaluate this trade-off across multiple prompts, datasets, and\nrelation types, introducing the concept of Hobson's choice to capture scenarios\nwhere models opt for safe but uninformative labels over hallucinated ones. Our\nfindings suggest that conservative bias occurs twice as often as hallucination.\nTo quantify this effect, we use SBERT and LLM prompts to capture the semantic\nsimilarity between conservative bias behaviors in constrained prompts and\nlabels generated from semi-constrained and open-ended prompts.", "AI": {"tldr": "This paper investigates the conservative bias of large language models in relation extraction tasks, highlighting significant information loss due to opting for No_Relation labels.", "motivation": "To analyze the conservative bias of LLMs in relation extraction and the associated trade-offs between information loss and incorrect relation assignments.", "method": "The paper systematically evaluates LLM behavior across various prompts and datasets, introducing Hobson's choice to illustrate preference for safe labels. It uses SBERT and LLM prompts to assess semantic similarity.", "result": "The findings indicate that conservative bias occurs twice as frequently as hallucination in LLM outputs, leading to considerable information loss.", "conclusion": "The analysis demonstrates the need for improved understanding and handling of LLM biases in relation extraction scenarios.", "key_contributions": ["Introduces the concept of Hobson's choice in LLM outputs.", "Quantifies the frequency of conservative bias compared to hallucination in LLMs.", "Evaluates the semantic similarity of conservative outputs versus more creative labels."], "limitations": "The study may not account for all types of prompts or datasets that could influence LLM behavior.", "keywords": ["large language models", "relation extraction", "conservative bias", "Hobson's choice", "semantic similarity"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.08467", "pdf": "https://arxiv.org/pdf/2506.08467.pdf", "abs": "https://arxiv.org/abs/2506.08467", "title": "Rethinking Citation of AI Sources in Student-AI Collaboration within HCI Design Education", "authors": ["Prakash Shukla", "Suchismita Naik", "Ike Obi", "Jessica Backus", "Nancy Rasche", "Paul Parson"], "categories": ["cs.HC"], "comment": "8 pages, EduCHI 2025: 7th Annual Symposium on HCI Education,\n  Bloomington, IN, USA, July 2025", "summary": "The growing integration of AI tools in student design projects presents an\nunresolved challenge in HCI education: how should AI-generated content be cited\nand documented? Traditional citation frameworks -- grounded in credibility,\nretrievability, and authorship -- struggle to accommodate the dynamic and\nephemeral nature of AI outputs. In this paper, we examine how undergraduate\nstudents in a UX design course approached AI usage and citation when given the\nfreedom to integrate generative tools into their design process. Through\nqualitative analysis of 35 team projects and reflections from 175 students, we\nidentify varied citation practices ranging from formal attribution to indirect\nor absent acknowledgment. These inconsistencies reveal gaps in existing\nframeworks and raise questions about authorship, assessment, and pedagogical\ntransparency. We argue for rethinking AI citation as a reflective and\npedagogical practice; one that supports metacognitive engagement by prompting\nstudents to critically evaluate how and why they used AI throughout the design\nprocess. We propose alternative strategies -- such as AI contribution\nstatements and process-aware citation models that better align with the\niterative and reflective nature of design education. This work invites\neducators to reconsider how citation practices can support meaningful\nstudent--AI collaboration.", "AI": {"tldr": "Examines AI citation practices in HCI education based on UX design projects by undergraduate students.", "motivation": "To address unresolved challenges in citation and documentation of AI-generated content in HCI education.", "method": "Qualitative analysis of 35 team projects and 175 student reflections on AI usage and citation in UX design.", "result": "Identified varied citation practices, revealing gaps in existing citation frameworks and highlighting issues of authorship and pedagogical transparency.", "conclusion": "Proposes rethinking AI citation as a reflective practice and suggests strategies like AI contribution statements to enhance student-AI collaboration.", "key_contributions": ["Identifies inconsistencies in AI citation practices among students", "Raises questions about traditional citation frameworks in HCI education", "Proposes alternative citation strategies to support meaningful AI collaboration"], "limitations": "", "keywords": ["AI citation", "HCI education", "UX design", "student projects", "pedagogical strategies"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.08123", "pdf": "https://arxiv.org/pdf/2506.08123.pdf", "abs": "https://arxiv.org/abs/2506.08123", "title": "QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA", "authors": ["Jacob Dineen", "Aswin RRV", "Qin Liu", "Zhikun Xu", "Xiao Ye", "Ming Shen", "Zhaonan Li", "Shijie Lu", "Chitta Baral", "Muhao Chen", "Ben Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Alignment of large language models with explicit principles (such as\nhelpfulness, honesty, and harmlessness) is crucial for ensuring safe and\nreliable AI systems. However, standard reward-based alignment methods typically\ncollapse diverse feedback into a single scalar reward, entangling multiple\nobjectives into one opaque training signal, which hinders interpretability. In\nthis work, we introduce QA-LIGN, an automatic symbolic reward decomposition\napproach that preserves the structure of each constitutional principle within\nthe reward mechanism. Instead of training a black-box reward model that outputs\na monolithic score, QA-LIGN formulates principle-specific evaluation questions\nand derives separate reward components for each principle, making it a drop-in\nreward model replacement. Experiments aligning an uncensored large language\nmodel with a set of constitutional principles demonstrate that QA-LIGN offers\ngreater transparency and adaptability in the alignment process. At the same\ntime, our approach achieves performance on par with or better than a DPO\nbaseline. Overall, these results represent a step toward more interpretable and\ncontrollable alignment of language models, achieved without sacrificing\nend-task performance.", "AI": {"tldr": "QA-LIGN is a novel approach for aligning large language models with explicit principles that enhances interpretability and adaptability without sacrificing performance.", "motivation": "To ensure safe and reliable AI systems, it is essential to align large language models with principles such as helpfulness, honesty, and harmlessness.", "method": "QA-LIGN employs an automatic symbolic reward decomposition that formulates principle-specific evaluation questions, leading to separate reward components for each alignment principle.", "result": "Experiments show that QA-LIGN provides greater transparency and adaptability in alignment compared to traditional methods, achieving performance on par with or better than existing baselines.", "conclusion": "QA-LIGN represents an advancement toward more interpretable and controllable alignment of language models without compromising end-task performance.", "key_contributions": ["Introduction of QA-LIGN for principle-specific reward decomposition.", "Improvement in transparency and adaptability during model alignment.", "Compatibility as a drop-in replacement for existing reward models."], "limitations": "", "keywords": ["large language models", "reward decomposition", "alignment", "interpretability", "AI safety"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.08517", "pdf": "https://arxiv.org/pdf/2506.08517.pdf", "abs": "https://arxiv.org/abs/2506.08517", "title": "Guidelines for Gaze-based Neural Preliminary Diagnosis", "authors": ["Mayar Elfares", "Salma Younis", "Pascal Reisert", "Ralf Küsters", "Tobias Renner", "Andreas Bulling"], "categories": ["cs.HC", "q-bio.NC"], "comment": null, "summary": "Neural disorders refer to any condition affecting the nervous system and that\ninfluence how individuals perceive and interact with the world. Traditional\nneural diagnoses rely on cumbersome, time-consuming, or subjective methods,\nsuch as clinical interviews, behavioural observations, or medical imaging. Eye\ntracking is an attractive alternative because analysing eye movements, such as\nfixations and saccades, can provide more objective insights into brain function\nand cognitive processing by capturing non-verbal and unconscious responses.\nDespite its potential, existing gaze-based studies presented seemingly\ncontradictory findings. They are dispersed across diverse fields, requiring\nfurther research to standardise protocols and expand their application,\nparticularly as a preliminary indicator of neural processes for differential\ndiagnosis. Therefore, this paper outlines the main agreed-upon findings and\nprovides a systematisation of knowledge and key guidelines towards advancing\ngaze-based neural preliminary diagnosis.", "AI": {"tldr": "This paper discusses the use of eye tracking as a method for preliminary diagnosis of neural disorders by analyzing eye movements to gain insights into brain function and cognition.", "motivation": "The paper addresses the limitations of traditional neural diagnosis methods, which are often cumbersome and subjective, advocating for the use of eye tracking as a more objective alternative.", "method": "The authors conduct a systematisation of current knowledge and guidelines regarding gaze-based studies, aiming to standardize protocols for their application in neural diagnostics.", "result": "The study identifies key findings from previous gaze-based research and proposes standardized guidelines to enhance their usage in preliminary neural diagnosis.", "conclusion": "The authors conclude that with further research and standardization, gaze-based methods can serve as valuable tools for the preliminary diagnosis of neural disorders.", "key_contributions": ["Systematisation of existing knowledge on gaze-based neural diagnosis.", "Proposed guidelines for standardizing gaze-based study protocols.", "Identification of key findings that can be helpful in advancing the use of eye tracking for neural diagnostics."], "limitations": "", "keywords": ["neural disorders", "eye tracking", "gaze-based diagnosis", "cognitive processing", "objective insights"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.08136", "pdf": "https://arxiv.org/pdf/2506.08136.pdf", "abs": "https://arxiv.org/abs/2506.08136", "title": "EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in Realistic Web Environments", "authors": ["Zefang Liu", "Yinzhu Quan"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce EconWebArena, a benchmark for evaluating autonomous agents on\ncomplex, multimodal economic tasks in realistic web environments. The benchmark\ncomprises 360 curated tasks from 82 authoritative websites spanning domains\nsuch as macroeconomics, labor, finance, trade, and public policy. Each task\nchallenges agents to navigate live websites, interpret structured and visual\ncontent, interact with real interfaces, and extract precise, time-sensitive\ndata through multi-step workflows. We construct the benchmark by prompting\nmultiple large language models (LLMs) to generate candidate tasks, followed by\nrigorous human curation to ensure clarity, feasibility, and source reliability.\nUnlike prior work, EconWebArena emphasizes fidelity to authoritative data\nsources and the need for grounded web-based economic reasoning. We evaluate a\ndiverse set of state-of-the-art multimodal LLMs as web agents, analyze failure\ncases, and conduct ablation studies to assess the impact of visual grounding,\nplan-based reasoning, and interaction design. Our results reveal substantial\nperformance gaps and highlight persistent challenges in grounding, navigation,\nand multimodal understanding, positioning EconWebArena as a rigorous testbed\nfor economic web intelligence.", "AI": {"tldr": "EconWebArena is a benchmark for evaluating autonomous agents on complex economic tasks in web environments, emphasizing grounded data and multimodal understanding.", "motivation": "To evaluate autonomous agents on realistic economic tasks using multimodal inputs and interactions with authoritative web data.", "method": "The benchmark involves 360 tasks curated from 82 websites, prompting various LLMs to generate candidates followed by rigorous human curation for clarity and reliability.", "result": "Performance gaps were identified in various LLMs concerning grounding, navigation, and multimodal understanding, highlighting challenges in economic web intelligence.", "conclusion": "EconWebArena serves as a critical testbed for assessing and improving economic web intelligence in LLMs.", "key_contributions": ["Introduction of a comprehensive benchmark for economic tasks on web platforms.", "Emphasis on the fidelity to authoritative data sources in economic reasoning.", "Evaluation of state-of-the-art multimodal LLMs on these tasks."], "limitations": "The benchmark does not cover all possible economic domains and may require further expansion for broader applicability.", "keywords": ["autonomous agents", "economic tasks", "benchmark", "multimodal", "large language models"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.08549", "pdf": "https://arxiv.org/pdf/2506.08549.pdf", "abs": "https://arxiv.org/abs/2506.08549", "title": "Exploring the Convergence of HCI and Evolving Technologies in Information Systems", "authors": ["Rajan Das Gupta", "Ashikur Rahman", "Md Imrul Hasan Showmick", "Md. Yeasin Rahat", "Md. Jakir Hossen"], "categories": ["cs.HC"], "comment": "Accepted in CITIC 2025", "summary": "Modern technology driven information systems are part of our daily lives.\nHowever, this deep integration poses new challenges to the human computer\ninteraction (HCI) professionals. With the rapid growth of mobile and cloud\ncomputing and the Internet of Things (IoT), the demand for HCI specialists to\ndesign user-friendly and adaptable interfaces has never been more pressing.\nEspecially for diverse user groups such as children, the elderly and people\nwith disabilities who need interfaces tailored to their needs regardless of\ntime and location. This study reviewed 50 recent papers on HCI interface design\nfor modern information systems. The goal is to see how well these methods\naddress the demands of current technology. The findings show that most HCI\ndesign methods are still based on old desktop models and do not support mobile\nusers and location-based services well. Most existing interface design\nguidelines do not align with the flexibility and dynamism of emerging\ntechnologies. The goal of this study is to improve interface design by\ncombining agile methodologies with human-centered design principles. Future\nstudies should also incorporate both qualitative and quantitative approaches,\nparticularly in the context of cloud-based technologies and organizational\ninformation systems. This approach aims to bridge the gap between current\ninterface design practices and the changing technological landscape.", "AI": {"tldr": "This study reviews HCI interface design methods, revealing they often do not meet the needs of modern mobile and cloud-based technologies.", "motivation": "To address the pressing need for adaptable interfaces for diverse user groups in the context of modern technology-driven information systems.", "method": "The study reviewed 50 recent papers on HCI interface design to assess their effectiveness in current technological contexts.", "result": "Findings indicate that most HCI design methods are outdated, primarily based on old desktop models, and insufficient for mobile and location-based services.", "conclusion": "The study advocates for integrating agile methodologies with human-centered design principles to enhance interface design in line with emerging technologies.", "key_contributions": ["Review of 50 recent papers on HCI interface design", "Identification of gaps in current HCI methods for mobile and cloud technologies", "Proposed integration of agile methodologies with human-centered principles"], "limitations": "The dependence on older desktop models in current design guidelines limits their applicability in modern contexts.", "keywords": ["Human-Computer Interaction", "Interface Design", "Agile Methodologies"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.08147", "pdf": "https://arxiv.org/pdf/2506.08147.pdf", "abs": "https://arxiv.org/abs/2506.08147", "title": "Multilingual Hate Speech Detection in Social Media Using Translation-Based Approaches with Large Language Models", "authors": ["Muhammad Usman", "Muhammad Ahmad", "M. Shahiki Tash", "Irina Gelbukh", "Rolando Quintero Tellez", "Grigori Sidorov"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Social media platforms are critical spaces for public discourse, shaping\nopinions and community dynamics, yet their widespread use has amplified harmful\ncontent, particularly hate speech, threatening online safety and inclusivity.\nWhile hate speech detection has been extensively studied in languages like\nEnglish and Spanish, Urdu remains underexplored, especially using\ntranslation-based approaches. To address this gap, we introduce a trilingual\ndataset of 10,193 tweets in English (3,834 samples), Urdu (3,197 samples), and\nSpanish (3,162 samples), collected via keyword filtering, with a balanced\ndistribution of 4,849 Hateful and 5,344 Not-Hateful labels. Our methodology\nleverages attention layers as a precursor to transformer-based models and large\nlanguage models (LLMs), enhancing feature extraction for multilingual hate\nspeech detection. For non-transformer models, we use TF-IDF for feature\nextraction. The dataset is benchmarked using state-of-the-art models, including\nGPT-3.5 Turbo and Qwen 2.5 72B, alongside traditional machine learning models\nlike SVM and other transformers (e.g., BERT, RoBERTa). Three annotators,\nfollowing rigorous guidelines, ensured high dataset quality, achieving a\nFleiss' Kappa of 0.821. Our approach, integrating attention layers with GPT-3.5\nTurbo and Qwen 2.5 72B, achieves strong performance, with macro F1 scores of\n0.87 for English (GPT-3.5 Turbo), 0.85 for Spanish (GPT-3.5 Turbo), 0.81 for\nUrdu (Qwen 2.5 72B), and 0.88 for the joint multilingual model (Qwen 2.5 72B).\nThese results reflect improvements of 8.75% in English (over SVM baseline\n0.80), 8.97% in Spanish (over SVM baseline 0.78), 5.19% in Urdu (over SVM\nbaseline 0.77), and 7.32% in the joint multilingual model (over SVM baseline\n0.82). Our framework offers a robust solution for multilingual hate speech\ndetection, fostering safer digital communities worldwide.", "AI": {"tldr": "This paper presents a trilingual dataset for hate speech detection in English, Urdu, and Spanish and a robust framework leveraging attention layers and large language models for improved multilingual detection performance.", "motivation": "To address the lack of research and tools for hate speech detection in Urdu and improve overall multilingual hate speech detection.", "method": "The paper introduces a trilingual dataset of 10,193 tweets and utilizes attention layers with transformer models for feature extraction, alongside traditional methods like TF-IDF for non-transformer models.", "result": "The framework achieves strong performance with macro F1 scores of 0.87 for English, 0.85 for Spanish, and 0.81 for Urdu using state-of-the-art models, showing significant improvements over SVM baselines.", "conclusion": "The study contributes a valuable dataset and establishes a framework that enhances multilingual hate speech detection, promoting safer digital environments.", "key_contributions": ["Trilingual dataset for hate speech detection in English, Urdu, and Spanish.", "Integration of attention layers with transformer models for improved feature extraction.", "Benchmarking with state-of-the-art and traditional models, showcasing performance improvements."], "limitations": "", "keywords": ["hate speech detection", "multilingual dataset", "transformer models", "social media", "natural language processing"], "importance_score": 4, "read_time_minutes": 12}}
{"id": "2506.08634", "pdf": "https://arxiv.org/pdf/2506.08634.pdf", "abs": "https://arxiv.org/abs/2506.08634", "title": "MOSAIC-F: A Framework for Enhancing Students' Oral Presentation Skills through Personalized Feedback", "authors": ["Alvaro Becerra", "Daniel Andres", "Pablo Villegas", "Roberto Daza", "Ruth Cobos"], "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": "Accepted in LASI Spain 25: Learning Analytics Summer Institute Spain\n  2025", "summary": "In this article, we present a novel multimodal feedback framework called\nMOSAIC-F, an acronym for a data-driven Framework that integrates Multimodal\nLearning Analytics (MMLA), Observations, Sensors, Artificial Intelligence (AI),\nand Collaborative assessments for generating personalized feedback on student\nlearning activities. This framework consists of four key steps. First, peers\nand professors' assessments are conducted through standardized rubrics (that\ninclude both quantitative and qualitative evaluations). Second, multimodal data\nare collected during learning activities, including video recordings, audio\ncapture, gaze tracking, physiological signals (heart rate, motion data), and\nbehavioral interactions. Third, personalized feedback is generated using AI,\nsynthesizing human-based evaluations and data-based multimodal insights such as\nposture, speech patterns, stress levels, and cognitive load, among others.\nFinally, students review their own performance through video recordings and\nengage in self-assessment and feedback visualization, comparing their own\nevaluations with peers and professors' assessments, class averages, and\nAI-generated recommendations. By combining human-based and data-based\nevaluation techniques, this framework enables more accurate, personalized and\nactionable feedback. We tested MOSAIC-F in the context of improving oral\npresentation skills.", "AI": {"tldr": "MOSAIC-F is a multimodal feedback framework that combines human assessments and AI to provide personalized feedback on student learning activities, tested for oral presentation skills.", "motivation": "To improve personalized feedback mechanisms in student learning by integrating various multimodal data and AI analytics.", "method": "The framework involves peer and professor assessments, collection of multimodal data during activities, AI generation of personalized feedback, and student self-assessment.", "result": "MOSAIC-F generates accurate and actionable feedback by synthesizing qualitative evaluations with multimodal insights such as posture and cognitive load.", "conclusion": "The integration of AI with traditional assessment techniques enhances the personalization and effectiveness of feedback provided to students.", "key_contributions": ["Development of a multimodal feedback framework", "Integration of various data sources and AI for feedback generation", "Application in oral presentation skill improvement"], "limitations": "", "keywords": ["MOSAIC-F", "multimodal feedback", "student learning", "personalized feedback", "AI"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.08158", "pdf": "https://arxiv.org/pdf/2506.08158.pdf", "abs": "https://arxiv.org/abs/2506.08158", "title": "ETT-CKGE: Efficient Task-driven Tokens for Continual Knowledge Graph Embedding", "authors": ["Lijing Zhu", "Qizhen Lan", "Qing Tian", "Wenbo Sun", "Li Yang", "Lu Xia", "Yixin Xie", "Xi Xiao", "Tiehang Duan", "Cui Tao", "Shuteng Niu"], "categories": ["cs.CL"], "comment": null, "summary": "Continual Knowledge Graph Embedding (CKGE) seeks to integrate new knowledge\nwhile preserving past information. However, existing methods struggle with\nefficiency and scalability due to two key limitations: (1) suboptimal knowledge\npreservation between snapshots caused by manually designed node/relation\nimportance scores that ignore graph dependencies relevant to the downstream\ntask, and (2) computationally expensive graph traversal for node/relation\nimportance calculation, leading to slow training and high memory overhead. To\naddress these limitations, we introduce ETT-CKGE (Efficient, Task-driven,\nTokens for Continual Knowledge Graph Embedding), a novel task-guided CKGE\nmethod that leverages efficient task-driven tokens for efficient and effective\nknowledge transfer between snapshots. Our method introduces a set of learnable\ntokens that directly capture task-relevant signals, eliminating the need for\nexplicit node scoring or traversal. These tokens serve as consistent and\nreusable guidance across snapshots, enabling efficient token-masked embedding\nalignment between snapshots. Importantly, knowledge transfer is achieved\nthrough simple matrix operations, significantly reducing training time and\nmemory usage. Extensive experiments across six benchmark datasets demonstrate\nthat ETT-CKGE consistently achieves superior or competitive predictive\nperformance, while substantially improving training efficiency and scalability\ncompared to state-of-the-art CKGE methods. The code is available at:\nhttps://github.com/lijingzhu1/ETT-CKGE/tree/main", "AI": {"tldr": "ETT-CKGE is a novel method for Continual Knowledge Graph Embedding that improves efficiency and scalability by using learnable tokens for task-relevant knowledge transfer, avoiding costly graph traversals.", "motivation": "Current methods for Continual Knowledge Graph Embedding face challenges with efficiency and scalability due to suboptimal knowledge preservation and expensive computations.", "method": "ETT-CKGE employs task-driven tokens for knowledge transfer, enabling consistent guidance across snapshots and significantly reducing the need for complex graph traversal and node scoring.", "result": "ETT-CKGE demonstrates superior or competitive predictive performance across six benchmark datasets, while enhancing training efficiency and scalability.", "conclusion": "The proposed method offers a practical solution for applying continual learning in knowledge graph settings without heavy computational requirements.", "key_contributions": ["Introduction of learnable task-driven tokens for efficient knowledge transfer", "Reduction of computational overhead through simple matrix operations", "Consistent guidance across multiple snapshots for better performance"], "limitations": "", "keywords": ["Continual Learning", "Knowledge Graph", "Machine Learning", "Graph Embedding", "Task-driven Methods"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2506.08725", "pdf": "https://arxiv.org/pdf/2506.08725.pdf", "abs": "https://arxiv.org/abs/2506.08725", "title": "Stop Misusing t-SNE and UMAP for Visual Analytics", "authors": ["Hyeon Jeon", "Jeongin Park", "Sungbok Shin", "Jinwook Seo"], "categories": ["cs.HC", "cs.LG"], "comment": "9 pages", "summary": "Misuses of t-SNE and UMAP in visual analytics have become increasingly\ncommon. For example, although t-SNE and UMAP projections often do not\nfaithfully reflect true distances between clusters, practitioners frequently\nuse them to investigate inter-cluster relationships. In this paper, we bring\nthis issue to the surface and comprehensively investigate why such misuse\noccurs and how to prevent it. We conduct a literature review of 114 papers to\nverify the prevalence of the misuse and analyze the reasonings behind it. We\nthen execute an interview study to uncover practitioners' implicit motivations\nfor using these techniques -- rationales often undisclosed in the literature.\nOur findings indicate that misuse of t-SNE and UMAP primarily stems from\nlimited discourse on their appropriate use in visual analytics. We conclude by\nproposing future directions and concrete action items to promote more\nreasonable use of DR.", "AI": {"tldr": "This paper investigates the misuse of t-SNE and UMAP in visual analytics, identifying the reasons behind it, and proposes ways to improve their appropriate use.", "motivation": "To address the increasing misuse of t-SNE and UMAP in visual analytics, which often leads to misleading interpretations of data.", "method": "A literature review of 114 papers was conducted to identify the prevalence of misuse, followed by an interview study to explore the motivations of practitioners.", "result": "The study found that misuse of t-SNE and UMAP results from limited discourse on their proper use and highlights the need for better guidelines.", "conclusion": "The paper concludes with recommendations for promoting reasonable use of dimensionality reduction techniques in visual analytics.", "key_contributions": ["Comprehensive literature review on the misuse of t-SNE and UMAP.", "Insights from interviews with practitioners about their motivations for using these techniques.", "Proposed actions to prevent misuse and improve the understanding of dimensionality reduction methods."], "limitations": "The study is limited to t-SNE and UMAP, and the findings may not generalize to other dimensionality reduction techniques.", "keywords": ["t-SNE", "UMAP", "visual analytics", "dimensionality reduction", "misuse"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2506.08172", "pdf": "https://arxiv.org/pdf/2506.08172.pdf", "abs": "https://arxiv.org/abs/2506.08172", "title": "Can Artificial Intelligence Write Like Borges? An Evaluation Protocol for Spanish Microfiction", "authors": ["Gerardo Aleman Manzanarez", "Nora de la Cruz Arana", "Jorge Garcia Flores", "Yobany Garcia Medina", "Raul Monroy", "Nathalie Pernelle"], "categories": ["cs.CL"], "comment": "28 pages, 16 figures. Submitted to Applied Sciences", "summary": "Automated story writing has been a subject of study for over 60 years. Large\nlanguage models can generate narratively consistent and linguistically coherent\nshort fiction texts. Despite these advancements, rigorous assessment of such\noutputs for literary merit - especially concerning aesthetic qualities - has\nreceived scant attention. In this paper, we address the challenge of evaluating\nAI-generated microfictions and argue that this task requires consideration of\nliterary criteria across various aspects of the text, such as thematic\ncoherence, textual clarity, interpretive depth, and aesthetic quality. To\nfacilitate this, we present GrAImes: an evaluation protocol grounded in\nliterary theory, specifically drawing from a literary perspective, to offer an\nobjective framework for assessing AI-generated microfiction. Furthermore, we\nreport the results of our validation of the evaluation protocol, as answered by\nboth literature experts and literary enthusiasts. This protocol will serve as a\nfoundation for evaluating automatically generated microfictions and assessing\ntheir literary value.", "AI": {"tldr": "The paper presents GrAImes, an evaluation protocol for assessing the literary merit of AI-generated microfictions, focusing on various aesthetic and thematic criteria.", "motivation": "To rigorously evaluate AI-generated narratives for literary merit, addressing the lack of frameworks for assessing aesthetic qualities.", "method": "The paper develops an evaluation protocol grounded in literary theory, specifically designed to assess AI-generated microfiction.", "result": "The validation of GrAImes was performed through assessments by literature experts and enthusiasts, demonstrating its applicability in evaluating literary qualities.", "conclusion": "GrAImes provides an objective framework for evaluating the literary value of automated microfictions, aiding researchers in the field.", "key_contributions": ["Introduction of GrAImes for literary evaluation of AI-generated works", "Focus on aesthetic qualities in narrative assessments", "Validation of the framework through expert feedback"], "limitations": "", "keywords": ["AI-generated microfictions", "literary evaluation", "GrAImes", "aesthetic quality", "assessment framework"], "importance_score": 5, "read_time_minutes": 20}}
{"id": "2506.08805", "pdf": "https://arxiv.org/pdf/2506.08805.pdf", "abs": "https://arxiv.org/abs/2506.08805", "title": "Communicating Through Avatars in Industry 5.0: A Focus Group Study on Human-Robot Collaboration", "authors": ["Stina Klein", "Pooja Prajod", "Katharina Weitz", "Matteo Lavit Nicora", "Dimitra Tsovaltzi", "Elisabeth André"], "categories": ["cs.HC", "cs.RO"], "comment": "Accepted LBW at CHIWORK 2025", "summary": "The integration of collaborative robots (cobots) in industrial settings\nraises concerns about worker well-being, particularly due to reduced social\ninteractions. Avatars - designed to facilitate worker interactions and\nengagement - are promising solutions to enhance the human-robot collaboration\n(HRC) experience. However, real-world perspectives on avatar-supported HRC\nremain unexplored. To address this gap, we conducted a focus group study with\nemployees from a German manufacturing company that uses cobots. Before the\ndiscussion, participants engaged with a scripted, industry-like HRC demo in a\nlab setting. This qualitative approach provided valuable insights into the\navatar's potential roles, improvements to its behavior, and practical\nconsiderations for deploying them in industrial workcells. Our findings also\nemphasize the importance of personalized communication and task assistance.\nAlthough our study's limitations restrict its generalizability, it serves as an\ninitial step in recognizing the potential of adaptive, context-aware avatar\ninteractions in real-world industrial environments.", "AI": {"tldr": "Explores how avatars can enhance human-robot collaboration in industrial settings to improve worker engagement and well-being.", "motivation": "Addresses concerns regarding worker well-being due to decreased social interactions when integrating collaborative robots (cobots).", "method": "Conducted a focus group study with employees from a German manufacturing company utilizing cobots and included a scripted HRC demo.", "result": "Gained insights into avatar roles, improvements in behavior, and practical considerations for their deployment in industrial environments.", "conclusion": "Highlights the need for personalized communication and task assistance from avatars in HRC, acknowledging limitations in generalizability of the findings.", "key_contributions": ["Identifies potential roles of avatars in HRC", "Highlights improvements needed in avatar behavior", "Discusses practical considerations for deploying avatars in industry"], "limitations": "Study's limitations restrict generalizability of findings.", "keywords": ["collaborative robots", "human-robot collaboration", "avatars", "worker engagement", "industrial settings"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.08174", "pdf": "https://arxiv.org/pdf/2506.08174.pdf", "abs": "https://arxiv.org/abs/2506.08174", "title": "LLM-BT: Back-Translation as a Framework for Terminology Standardization and Dynamic Semantic Embedding", "authors": ["Li Weigang", "Pedro Carvalho Brom"], "categories": ["cs.CL"], "comment": "23 pages", "summary": "The rapid growth of English technical terms challenges traditional\nexpert-driven standardization, especially in fast-evolving fields like AI and\nquantum computing. Manual methods struggle to ensure multilingual consistency.\nWe propose \\textbf{LLM-BT}, a back-translation framework powered by large\nlanguage models (LLMs) to automate terminology verification and standardization\nvia cross-lingual semantic alignment. Our contributions are: \\textbf{(1)\nTerm-Level Consistency Validation:} Using English $\\rightarrow$ intermediate\nlanguage $\\rightarrow$ English back-translation, LLM-BT achieves high term\nconsistency across models (e.g., GPT-4, DeepSeek, Grok), with case studies\nshowing over 90\\% exact or semantic matches. \\textbf{(2) Multi-Path\nVerification Workflow:} A novel ``Retrieve--Generate--Verify--Optimize''\npipeline integrates serial (e.g., EN $\\rightarrow$ ZHcn $\\rightarrow$ ZHtw\n$\\rightarrow$ EN) and parallel (e.g., EN $\\rightarrow$ Chinese/Portuguese\n$\\rightarrow$ EN) BT routes. BLEU and term accuracy indicate strong\ncross-lingual robustness (BLEU $>$ 0.45; Portuguese accuracy 100\\%).\n\\textbf{(3) Back-Translation as Semantic Embedding:} BT is conceptualized as\ndynamic semantic embedding, revealing latent meaning trajectories. Unlike\nstatic embeddings, LLM-BT provides transparent path-based embeddings shaped by\nmodel evolution. LLM-BT transforms back-translation into an active engine for\nmultilingual terminology standardization, enabling human--AI collaboration:\nmachines ensure semantic fidelity, humans guide cultural interpretation. This\ninfrastructure supports terminology governance across scientific and\ntechnological fields worldwide.", "AI": {"tldr": "Proposes LLM-BT, a back-translation framework using large language models for automated multilingual terminology standardization and verification.", "motivation": "The increasing speed of English technical term growth necessitates a more efficient method of multilingual consistency due to limitations of expert-driven approaches.", "method": "Employing back-translation from English to an intermediate language and back to English to validate term-level consistency, along with a multi-path verification workflow integrating both serial and parallel routes.", "result": "Achieved over 90% exact or semantic matches in term consistency, with strong cross-lingual robustness indicated by BLEU scores above 0.45 and 100% accuracy for Portuguese.", "conclusion": "LLM-BT empowers human-AI collaboration in terminology governance and enhances multilingual standardization processes across various fields.", "key_contributions": ["Term-Level Consistency Validation", "Multi-Path Verification Workflow", "Back-Translation as Semantic Embedding"], "limitations": "", "keywords": ["back-translation", "large language models", "multilingual terminology", "semantic alignment", "AI"], "importance_score": 8, "read_time_minutes": 23}}
{"id": "2506.08881", "pdf": "https://arxiv.org/pdf/2506.08881.pdf", "abs": "https://arxiv.org/abs/2506.08881", "title": "From Fads to Classics -- Analyzing Video Game Trend Evolutions through Steam Tags", "authors": ["Nicolas Grelier", "Johannes Pfau", "Nicolas Mathieu", "Stéphane Kaufmann"], "categories": ["cs.HC"], "comment": null, "summary": "The video game industry deals with a fast-paced, competitive and almost\nunpredictable market. Trends of genres, settings and modalities change on a\nperpetual basis, studios are often one big hit or miss away from surviving or\nperishing, and hitting the pulse of the time has become one of the greatest\nchallenges for industrials, investors and other stakeholders. In this work, we\naim to support the understanding of video game trends over time based on\ndata-driven analysis, visualization and interpretation of Steam tag evolutions.\nWe confirm underlying groundwork that trends can be categorized in short-lived\nfads, contemporary fashions, or stable classics, and derived that the surge of\na trend averages at about four years in the realm of video games. After using\nindustrial experts to validate our findings, we deliver visualizations,\ninsights and an open approach of deciphering shifts in video game trends.", "AI": {"tldr": "This paper analyzes the evolution of video game trends using data-driven methods, categorizing trends into fads, fashions, and classics, and providing visualizations and insights.", "motivation": "To support the understanding of video game trends amidst a fast-changing industry and to help stakeholders make informed decisions.", "method": "Data-driven analysis of Steam tag evolutions, validated by industrial experts, leading to insights and visualizations of trend categories and durations.", "result": "Identified that trends can be categorized as short-lived fads, contemporary fashions, or stable classics, with an average surge duration of about four years.", "conclusion": "The research provides valuable insights and visualizations that enhance the understanding of shifting trends in the video game industry.", "key_contributions": ["Categorization of video game trends into fads, fashions, and classics", "Establishment of the average trend duration in video games", "Provision of visual data interpretations for stakeholders"], "limitations": "", "keywords": ["video games", "trend analysis", "data visualization", "Steam tags", "market trends"], "importance_score": 2, "read_time_minutes": 15}}
{"id": "2506.08184", "pdf": "https://arxiv.org/pdf/2506.08184.pdf", "abs": "https://arxiv.org/abs/2506.08184", "title": "Unable to forget: Proactive lnterference Reveals Working Memory Limits in LLMs Beyond Context Length", "authors": ["Chupei Wang", "Jiaqiu Vince Sun"], "categories": ["cs.CL", "cs.AI", "q-bio.NC"], "comment": null, "summary": "Information retrieval in Large Language Models (LLMs) is increasingly\nrecognized as intertwined with generation capabilities rather than mere lookup.\nWhile longer contexts are often assumed to improve retrieval, the effects of\nintra-context interference remain understudied. To address this, we adapt the\nproactive interference (PI) paradigm from cognitive science, where earlier\ninformation disrupts recall of newer updates. In humans, susceptibility to such\ninterference is inversely linked to working memory capacity. We introduce\nPI-LLM, an evaluation that sequentially streams semantically related key-value\nupdates and queries only the final values. Although these final values are\nclearly positioned just before the query, LLM retrieval accuracy declines\nlog-linearly toward zero as interference accumulates; errors arise from\nretrieving previously overwritten values. Attempts to mitigate interference via\nprompt engineering (e.g., instructing models to ignore earlier input) yield\nlimited success. These findings reveal a fundamental constraint on LLMs'\nability to disentangle interference and flexibly manipulate information,\nsuggesting a working memory bottleneck beyond mere context access. This calls\nfor approaches that strengthen models' ability to suppress irrelevant content\nduring retrieval.", "AI": {"tldr": "This paper investigates how intra-context interference affects information retrieval in Large Language Models (LLMs), highlighting a fundamental constraint linked to working memory capacity.", "motivation": "The study aims to explore the effects of intra-context interference on LLMs' retrieval capabilities, which are increasingly viewed as tied to generation rather than just lookup.", "method": "The authors adapt the proactive interference paradigm from cognitive science to evaluate LLM retrieval performance by streaming semantically related updates and assessing the impact of interference on accuracy.", "result": "Retrieval accuracy in LLMs declines log-linearly as interference accumulates, suggesting significant challenges in disentangling interference when manipulating information.", "conclusion": "The paper concludes that LLMs face a working memory bottleneck that necessitates new strategies to enhance their ability to suppress irrelevant information during retrieval.", "key_contributions": ["Introduces the PI-LLM evaluation framework for studying interference in LLMs", "Demonstrates the log-linear decline of retrieval accuracy due to interference", "Highlights the limitations of current prompt engineering methods in mitigating interference."], "limitations": "The study indicates that mitigation strategies such as prompt engineering are only partially effective in improving retrieval accuracy.", "keywords": ["Large Language Models", "Information Retrieval", "Proactive Interference", "Working Memory", "Prompt Engineering"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.08892", "pdf": "https://arxiv.org/pdf/2506.08892.pdf", "abs": "https://arxiv.org/abs/2506.08892", "title": "Help or Hindrance: Understanding the Impact of Robot Communication in Action Teams", "authors": ["Tauhid Tanjim", "Jonathan St. George", "Kevin Ching", "Hee Rin Lee", "Angelique Taylor"], "categories": ["cs.HC", "cs.RO"], "comment": "This is the author's original submitted version of the paper accepted\n  to the 2025 IEEE International Conference on Robot and Human Interactive\n  Communication (RO-MAN). \\c{opyright} 2025 IEEE. Personal use of this material\n  is permitted. For any other use, please contact IEEE", "summary": "The human-robot interaction (HRI) field has recognized the importance of\nenabling robots to interact with teams. Human teams rely on effective\ncommunication for successful collaboration in time-sensitive environments.\nRobots can play a role in enhancing team coordination through real-time\nassistance. Despite significant progress in human-robot teaming research, there\nremains an essential gap in how robots can effectively communicate with action\nteams using multimodal interaction cues in time-sensitive environments. This\nstudy addresses this knowledge gap in an experimental in-lab study to\ninvestigate how multimodal robot communication in action teams affects workload\nand human perception of robots. We explore team collaboration in a medical\ntraining scenario where a robotic crash cart (RCC) provides verbal and\nnon-verbal cues to help users remember to perform iterative tasks and search\nfor supplies. Our findings show that verbal cues for object search tasks and\nvisual cues for task reminders reduce team workload and increase perceived ease\nof use and perceived usefulness more effectively than a robot with no feedback.\nOur work contributes to multimodal interaction research in the HRI field,\nhighlighting the need for more human-robot teaming research to understand best\npractices for integrating collaborative robots in time-sensitive environments\nsuch as in hospitals, search and rescue, and manufacturing applications.", "AI": {"tldr": "This study investigates how multimodal robot communication affects workload and human perception in team settings, particularly in medical training scenarios.", "motivation": "The research aims to address the knowledge gap in how robots can communicate effectively with human teams in time-sensitive environments, enhancing collaboration and coordination.", "method": "An experimental in-lab study was conducted to explore team collaboration using a robotic crash cart (RCC) that provided verbal and non-verbal cues in a medical training scenario.", "result": "Findings indicate that verbal cues for object search tasks and visual cues for task reminders significantly reduce team workload and increase perceived ease of use and usefulness compared to a robot that gives no feedback.", "conclusion": "The study highlights the importance of multimodal interaction in human-robot teams and calls for more research on integrating collaborative robots in critical environments like hospitals and manufacturing.", "key_contributions": ["Investigating the impact of multimodal robot communication on team workload", "Demonstrating the effectiveness of verbal and visual cues in robotic assistance", "Providing insights for future HRI research in time-sensitive environments"], "limitations": "", "keywords": ["human-robot interaction", "multimodal communication", "team collaboration", "robotic assistance", "medical training"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.08221", "pdf": "https://arxiv.org/pdf/2506.08221.pdf", "abs": "https://arxiv.org/abs/2506.08221", "title": "\"I Wrote, I Paused, I Rewrote\" Teaching LLMs to Read Between the Lines of Student Writing", "authors": ["Samra Zafar", "Shaheer Minhas", "Syed Ali Hassan Zaidi", "Arfa Naeem", "Zahra Ali"], "categories": ["cs.CL"], "comment": "7 pages, 6 figures, 2 tables", "summary": "Large language models(LLMs) like Gemini are becoming common tools for\nsupporting student writing. But most of their feedback is based only on the\nfinal essay missing important context about how that text was written. In this\npaper, we explore whether using writing process data, collected through\nkeystroke logging and periodic snapshots, can help LLMs give feedback that\nbetter reflects how learners think and revise while writing. We built a digital\nwriting tool that captures both what students type and how their essays evolve\nover time. Twenty students used this tool to write timed essays, which were\nthen evaluated in two ways: (i) LLM generated feedback using both the final\nessay and the full writing trace, and (ii) After the task, students completed\nsurveys about how useful and relatable they found the feedback. Early results\nshow that learners preferred the process-aware LLM feedback, finding it more in\ntune with their own thinking. We also found that certain types of edits, like\nadding new content or reorganizing paragraphs, aligned closely with higher\nscores in areas like coherence and elaboration. Our findings suggest that\nmaking LLMs more aware of the writing process can lead to feedback that feels\nmore meaningful, personal, and supportive.", "AI": {"tldr": "The paper investigates how writing process data can enhance feedback from large language models for student writing.", "motivation": "To improve the relevance and personalization of feedback provided by large language models (LLMs) used in student writing.", "method": "Developed a digital writing tool that captures keystroke logs and periodic snapshots of student essays, utilized by twenty students who wrote timed essays.", "result": "Students preferred LLM feedback that was informed by their writing process, and it correlated with better scores in coherence and elaboration when they made significant edits.", "conclusion": "Enhancing LLMs with awareness of the writing process can make feedback feel more meaningful and supportive for learners.", "key_contributions": ["Introduced a novel digital writing tool for capturing the writing process.", "Demonstrated that process-aware feedback is preferred by students.", "Found correlation between types of edits made and writing quality metrics."], "limitations": "Initial results from a small sample size and limited contexts for feedback evaluation.", "keywords": ["Large Language Models", "Writing Process", "Educational Technology", "Feedback", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.08911", "pdf": "https://arxiv.org/pdf/2506.08911.pdf", "abs": "https://arxiv.org/abs/2506.08911", "title": "Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU", "authors": ["Petar Jakuš", "Hrvoje Džapo"], "categories": ["cs.HC", "cs.LG", "cs.SD", "eess.AS"], "comment": "4 pages", "summary": "This paper presents a keyword spotting (KWS) system implemented on the NXP\nMCXN947 microcontroller with an integrated Neural Processing Unit (NPU),\nenabling real-time voice interaction on resource-constrained devices. The\nsystem combines MFCC feature extraction with a CNN classifier, optimized using\nQuantization Aware Training to reduce model size with minimal accuracy drop.\nExperimental results demonstrate a 59x speedup in inference time when\nleveraging the NPU compared to CPU-only execution, achieving 97.06% accuracy\nwith a model size of 30.58 KB, demonstrating the feasibility of efficient,\nlow-power voice interfaces on embedded platforms.", "AI": {"tldr": "This paper describes a real-time keyword spotting system for resource-constrained devices using a microcontroller with an integrated Neural Processing Unit, achieving high accuracy and efficiency.", "motivation": "To enable real-time voice interaction on low-power, resource-constrained devices, enhancing user experience in embedded systems.", "method": "The system employs MFCC feature extraction combined with a CNN classifier, optimized through Quantization Aware Training to balance model size and accuracy.", "result": "Achieved a 59x speedup in inference time on the NPU with a model size of 30.58 KB and an accuracy of 97.06%.", "conclusion": "The research illustrates the potential for efficient, low-power voice interfaces in embedded environments, paving the way for improved HCI solutions.", "key_contributions": ["Development of a keyword spotting system for low-power devices", "Use of Quantization Aware Training to optimize model size", "Demonstration of significant speedup in inference time on NPU"], "limitations": "", "keywords": ["keyword spotting", "NXP MCXN947", "Neural Processing Unit", "CNN", "Quantization Aware Training"], "importance_score": 6, "read_time_minutes": 4}}
{"id": "2506.08234", "pdf": "https://arxiv.org/pdf/2506.08234.pdf", "abs": "https://arxiv.org/abs/2506.08234", "title": "Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions", "authors": ["Yu-Ang Lee", "Guan-Ting Yi", "Mei-Yi Liu", "Jui-Chao Lu", "Guan-Bo Yang", "Yun-Nung Chen"], "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 4 figures, 1 table", "summary": "Recent advancements in large language models (LLMs) and AI systems have led\nto a paradigm shift in the design and optimization of complex AI workflows. By\nintegrating multiple components, compound AI systems have become increasingly\nadept at performing sophisticated tasks. However, as these systems grow in\ncomplexity, new challenges arise in optimizing not only individual components\nbut also their interactions. While traditional optimization methods such as\nsupervised fine-tuning (SFT) and reinforcement learning (RL) remain\nfoundational, the rise of natural language feedback introduces promising new\napproaches, especially for optimizing non-differentiable systems. This paper\nprovides a systematic review of recent progress in optimizing compound AI\nsystems, encompassing both numerical and language-based techniques. We\nformalize the notion of compound AI system optimization, classify existing\nmethods along several key dimensions, and highlight open research challenges\nand future directions in this rapidly evolving field. A list of surveyed papers\nis publicly available at https://github.com/MiuLab/AISysOpt-Survey.", "AI": {"tldr": "The paper reviews recent advancements in optimizing compound AI systems, focusing on both traditional and novel techniques, especially natural language feedback methods.", "motivation": "To address the challenges in optimizing complex AI workflows as they integrate more components and become more sophisticated.", "method": "Systematic review of existing optimization methods for compound AI systems, classified along key dimensions, including numerical and language-based techniques.", "result": "A formalized understanding of compound AI system optimization, identification of existing methods, and highlighting of open research challenges.", "conclusion": "There is a need for new approaches to optimization, particularly in non-differentiable systems, as the field continues to evolve.", "key_contributions": ["Systematic classification of optimization methods for compound AI systems.", "Identification of the role of natural language feedback in optimizing these systems.", "Overview of open research challenges and future research directions."], "limitations": "", "keywords": ["compound AI systems", "optimization", "natural language feedback", "machine learning", "AI workflows"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.08235", "pdf": "https://arxiv.org/pdf/2506.08235.pdf", "abs": "https://arxiv.org/abs/2506.08235", "title": "Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning", "authors": ["Shashidhar Reddy Javaji", "Yupeng Cao", "Haohang Li", "Yangyang Yu", "Nikhil Muralidhar", "Zining Zhu"], "categories": ["cs.CL", "cs.AI"], "comment": "21 pages, 6 figures, Under review", "summary": "Large language models (LLMs) are increasingly being used for complex research\ntasks such as literature review, idea generation, and scientific paper\nanalysis, yet their ability to truly understand and process the intricate\nrelationships within complex research papers, such as the logical links between\nclaims and supporting evidence remains largely unexplored. In this study, we\npresent CLAIM-BENCH, a comprehensive benchmark for evaluating LLMs'\ncapabilities in scientific claim-evidence extraction and validation, a task\nthat reflects deeper comprehension of scientific argumentation. We\nsystematically compare three approaches which are inspired by divide and\nconquer approaches, across six diverse LLMs, highlighting model-specific\nstrengths and weaknesses in scientific comprehension. Through evaluation\ninvolving over 300 claim-evidence pairs across multiple research domains, we\nreveal significant limitations in LLMs' ability to process complex scientific\ncontent. Our results demonstrate that closed-source models like GPT-4 and\nClaude consistently outperform open-source counterparts in precision and recall\nacross claim-evidence identification tasks. Furthermore, strategically designed\nthree-pass and one-by-one prompting approaches significantly improve LLMs'\nabilities to accurately link dispersed evidence with claims, although this\ncomes at increased computational cost. CLAIM-BENCH sets a new standard for\nevaluating scientific comprehension in LLMs, offering both a diagnostic tool\nand a path forward for building systems capable of deeper, more reliable\nreasoning across full-length papers.", "AI": {"tldr": "The study presents CLAIM-BENCH, a benchmark for evaluating LLMs' ability in scientific claim-evidence extraction and validation.", "motivation": "To investigate LLMs' understanding of complex research relationships, particularly in scientific claim-evidence processing.", "method": "Systematic comparison of three claim-evidence extraction approaches across six diverse LLMs using over 300 claim-evidence pairs.", "result": "Closed-source models like GPT-4 and Claude outperformed open-source models in precision and recall, revealing significant comprehension limitations in LLMs.", "conclusion": "CLAIM-BENCH establishes a new standard for assessing LLMs' scientific reasoning capabilities, suggesting methods to enhance their performance in claim-evidence tasks.", "key_contributions": ["Introduction of CLAIM-BENCH as a benchmark for LLM evaluation", "In-depth analysis of model-specific strengths and weaknesses", "Demonstration of improved performance through strategic prompting approaches"], "limitations": "Limited to claim-evidence extraction and validation tasks; increased computational costs with proposed approaches.", "keywords": ["Large Language Models", "Claim-Evidence Extraction", "Scientific Comprehension", "Benchmarking"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.08260", "pdf": "https://arxiv.org/pdf/2506.08260.pdf", "abs": "https://arxiv.org/abs/2506.08260", "title": "Automatic Generation of Inference Making Questions for Reading Comprehension Assessments", "authors": ["Wanjing Anya Ma", "Michael Flor", "Zuowei Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the 20th Workshop on Innovative Use of NLP for Building\n  Educational Applications (BEA 2025), co-located with the ACL 2025", "summary": "Inference making is an essential but complex skill in reading comprehension\n(RC). Some inferences require resolving references across sentences, and some\nrely on using prior knowledge to fill in the detail that is not explicitly\nwritten in the text. Diagnostic RC questions can help educators provide more\neffective and targeted reading instruction and interventions for school-age\nstudents. We introduce a taxonomy of inference types for RC and use it to\nanalyze the distribution of items within a diagnostic RC item bank. Next, we\npresent experiments using GPT-4o to generate bridging-inference RC items for\ngiven reading passages via few-shot prompting, comparing conditions with and\nwithout chain-of-thought prompts. Generated items were evaluated on three\naspects: overall item quality, appropriate inference type, and LLM reasoning,\nachieving high inter-rater agreements above 0.90. Our results show that GPT-4o\nproduced 93.8% good-quality questions suitable for operational use in grade\n3-12 contexts; however, only 42.6% of the generated questions accurately\nmatched the targeted inference type. We conclude that combining automatic item\ngeneration with human judgment offers a promising path toward scalable,\nhigh-quality diagnostic RC assessments.", "AI": {"tldr": "This paper presents a taxonomy of inference types for reading comprehension and experiments using GPT-4o to generate diagnostic reading comprehension questions, analyzing their quality and accuracy.", "motivation": "To improve reading comprehension instruction and interventions for students by using AI-generated diagnostic questions.", "method": "The study analyzes a diagnostic reading comprehension item bank, creates a taxonomy of inference types, and conducts experiments with GPT-4o for generating questions through few-shot prompting.", "result": "GPT-4o produced 93.8% good-quality questions for grades 3-12; however, only 42.6% matched the intended inference types accurately.", "conclusion": "Combining AI-generated questions with human judgment holds potential for enhancing diagnostic reading comprehension assessments.", "key_contributions": ["Introduction of a taxonomy for inference types in reading comprehension", "Demonstration of GPT-4o's ability to generate high-quality diagnostic questions", "Evaluation framework for assessing the quality and accuracy of generated questions"], "limitations": "Only 42.6% of generated questions accurately matched the targeted inference type, indicating room for improvement.", "keywords": ["reading comprehension", "inference making", "GPT-4o", "educational applications", "question generation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.08300", "pdf": "https://arxiv.org/pdf/2506.08300.pdf", "abs": "https://arxiv.org/abs/2506.08300", "title": "Institutional Books 1.0: A 242B token dataset from Harvard Library's collections, refined for accuracy and usability", "authors": ["Matteo Cargnelutti", "Catherine Brobston", "John Hess", "Jack Cushman", "Kristi Mukk", "Aristana Scourtas", "Kyle Courtney", "Greg Leppert", "Amanda Watson", "Martha Whitehead", "Jonathan Zittrain"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) use data to learn about the world in order to\nproduce meaningful correlations and predictions. As such, the nature, scale,\nquality, and diversity of the datasets used to train these models, or to\nsupport their work at inference time, have a direct impact on their quality.\nThe rapid development and adoption of LLMs of varying quality has brought into\nfocus the scarcity of publicly available, high-quality training data and\nrevealed an urgent need to ground the stewardship of these datasets in\nsustainable practices with clear provenance chains. To that end, this technical\nreport introduces Institutional Books 1.0, a large collection of public domain\nbooks originally digitized through Harvard Library's participation in the\nGoogle Books project, beginning in 2006. Working with Harvard Library, we\nextracted, analyzed, and processed these volumes into an extensively-documented\ndataset of historic texts. This analysis covers the entirety of Harvard\nLibrary's collection scanned as part of that project, originally spanning\n1,075,899 volumes written in over 250 different languages for a total of\napproximately 250 billion tokens. As part of this initial release, the\nOCR-extracted text (original and post-processed) as well as the metadata\n(bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens,\nidentified as being in the public domain have been made available. This report\ndescribes this project's goals and methods as well as the results of the\nanalyses we performed, all in service of making this historical collection more\naccessible and easier for humans and machines alike to filter, read and use.", "AI": {"tldr": "The paper presents Institutional Books 1.0, a dataset of public domain books derived from Harvard Library's digitization efforts via the Google Books project. It emphasizes the importance of high-quality training data for large language models (LLMs) and discusses the dataset's accessibility and usability for both humans and machines.", "motivation": "To address the scarcity of high-quality publicly available datasets for training large language models and to promote sustainable practices in dataset stewardship.", "method": "The authors extracted and processed historical texts from Harvard Library's collection of digitally scanned books, generating an extensively documented dataset that includes both OCR-extracted text and various metadata.", "result": "The project resulted in a dataset containing metadata and OCR-extracted text from 983,004 public domain volumes totaling approximately 242 billion tokens, significantly expanding accessible training data for LLMs.", "conclusion": "Making this historical collection readily accessible enhances opportunities for meaningful use in machine learning applications and aids the advancement of human-computer interaction.", "key_contributions": ["Introduction of Institutional Books 1.0, a large public domain dataset for LLM training.", "Extensive documentation and processing of historical texts from Harvard Library.", "Highlighting the need for high-quality training data in LLM development."], "limitations": "", "keywords": ["large language models", "public domain books", "dataset stewardship", "historic texts", "OCR extraction"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.08836", "pdf": "https://arxiv.org/pdf/2506.08836.pdf", "abs": "https://arxiv.org/abs/2506.08836", "title": "Advancing STT for Low-Resource Real-World Speech", "authors": ["Flavio D'Intino", "Hans-Peter Hutter"], "categories": ["cs.CL", "cs.HC"], "comment": "Conference: HCI International 2025, 20 pages, 4 figures", "summary": "Swiss German is a low-resource language represented by diverse dialects that\ndiffer significantly from Standard German and from each other, lacking a\nstandardized written form. As a result, transcribing Swiss German involves\ntranslating into Standard German. Existing datasets have been collected in\ncontrolled environments, yielding effective speech-to-text (STT) models, but\nthese models struggle with spontaneous conversational speech.\n  This paper, therefore, introduces the new SRB-300 dataset, a 300-hour\nannotated speech corpus featuring real-world long-audio recordings from 39\nSwiss German radio and TV stations. It captures spontaneous speech across all\nmajor Swiss dialects recorded in various realistic environments and overcomes\nthe limitation of prior sentence-level corpora.\n  We fine-tuned multiple OpenAI Whisper models on the SRB-300 dataset,\nachieving notable enhancements over previous zero-shot performance metrics.\nImprovements in word error rate (WER) ranged from 19% to 33%, while BLEU scores\nincreased between 8% and 40%. The best fine-tuned model, large-v3, achieved a\nWER of 17.1% and a BLEU score of 74.8. This advancement is crucial for\ndeveloping effective and robust STT systems for Swiss German and other\nlow-resource languages in real-world contexts.", "AI": {"tldr": "The paper introduces the SRB-300 dataset, a 300-hour annotated speech corpus for Swiss German, and demonstrates improved speech-to-text performance using OpenAI Whisper models.", "motivation": "Swiss German is a low-resource language with many dialects and no standardized written form, complicating transcription efforts and STT model performance.", "method": "The authors collected real-world audio recordings from 39 Swiss German radio and TV stations, then fine-tuned multiple OpenAI Whisper models on the new SRB-300 dataset.", "result": "Fine-tuning resulted in significant improvements in word error rate (WER) by 19-33% and BLEU scores by 8-40%. The optimal model achieved a WER of 17.1% and a BLEU score of 74.8.", "conclusion": "This work advances the effectiveness of STT systems for Swiss German and other low-resource languages in realistic scenarios.", "key_contributions": ["Introduction of the SRB-300 dataset", "Fine-tuning results showing notable STT improvements", "Focus on real-world spontaneous speech capture"], "limitations": "", "keywords": ["Swiss German", "speech-to-text", "low-resource languages", "OpenAI Whisper", "SRB-300"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.08343", "pdf": "https://arxiv.org/pdf/2506.08343.pdf", "abs": "https://arxiv.org/abs/2506.08343", "title": "Wait, We Don't Need to \"Wait\"! Removing Thinking Tokens Improves Reasoning Efficiency", "authors": ["Chenlong Wang", "Yuanning Feng", "Dongping Chen", "Zhaoyang Chu", "Ranjay Krishna", "Tianyi Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large reasoning models have enabled complex, step-by-step\nreasoning but often introduce significant overthinking, resulting in verbose\nand redundant outputs that hinder efficiency. In this study, we examine whether\nexplicit self-reflection, signaled by tokens such as \"Wait\" and \"Hmm\", is\nnecessary for advanced reasoning. We propose NoWait, a simple yet effective\napproach that disables explicit self-reflection by suppressing these tokens\nduring inference. Extensive experiments on ten benchmarks across textual,\nvisual, and video reasoning tasks show that NoWait reduces chain-of-thought\ntrajectory length by up to 27%-51% in five R1-style model series, without\ncompromising model utility. NoWait thus offers a plug-and-play solution for\nefficient and utility-preserving multimodal reasoning.", "AI": {"tldr": "NoWait is a method that disables explicit self-reflection in reasoning models to enhance efficiency without sacrificing utility.", "motivation": "To address the problem of excessive verbosity and redundancy in reasoning outputs that hinder efficiency in large reasoning models.", "method": "The study proposes NoWait, which suppresses explicit self-reflection tokens like 'Wait' and 'Hmm' during inference, streamlining the reasoning process.", "result": "NoWait reduces chain-of-thought trajectory length by 27%-51% in five R1-style model series while maintaining model utility across multiple benchmarks in textual, visual, and video reasoning tasks.", "conclusion": "NoWait presents an effective, plug-and-play solution for enhancing multimodal reasoning efficiency without compromising performance.", "key_contributions": ["Introduction of NoWait for efficient reasoning", "Demonstrated significant reduction in reasoning trajectory length", "Maintained model utility across diverse benchmarks"], "limitations": "", "keywords": ["reasoning", "self-reflection", "NoWait", "multimodal", "efficiency"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.08349", "pdf": "https://arxiv.org/pdf/2506.08349.pdf", "abs": "https://arxiv.org/abs/2506.08349", "title": "Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving", "authors": ["Yuxuan Zhou", "Xien Liu", "Chenwei Yan", "Chen Ning", "Xiao Zhang", "Boxun Li", "Xiangling Fu", "Shijin Wang", "Guoping Hu", "Yu Wang", "Ji Wu"], "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 11 figures. Accepted by ICML 2025", "summary": "Large language models (LLMs) have demonstrated remarkable performance on\nvarious medical benchmarks, but their capabilities across different cognitive\nlevels remain underexplored. Inspired by Bloom's Taxonomy, we propose a\nmulti-cognitive-level evaluation framework for assessing LLMs in the medical\ndomain in this study. The framework integrates existing medical datasets and\nintroduces tasks targeting three cognitive levels: preliminary knowledge grasp,\ncomprehensive knowledge application, and scenario-based problem solving. Using\nthis framework, we systematically evaluate state-of-the-art general and medical\nLLMs from six prominent families: Llama, Qwen, Gemma, Phi, GPT, and DeepSeek.\nOur findings reveal a significant performance decline as cognitive complexity\nincreases across evaluated models, with model size playing a more critical role\nin performance at higher cognitive levels. Our study highlights the need to\nenhance LLMs' medical capabilities at higher cognitive levels and provides\ninsights for developing LLMs suited to real-world medical applications.", "AI": {"tldr": "This study proposes a framework for evaluating large language models (LLMs) in the medical domain based on cognitive levels, revealing a decline in performance as cognitive complexity increases.", "motivation": "To explore the capabilities of LLMs across different cognitive levels in the medical domain, enhancing their application in real-world scenarios.", "method": "A multi-cognitive-level evaluation framework was developed, incorporating existing medical datasets and defining tasks for three cognitive levels: preliminary knowledge grasp, comprehensive knowledge application, and scenario-based problem solving.", "result": "Performance evaluation of state-of-the-art general and medical LLMs shows a significant decline in performance with increasing cognitive complexity, indicating model size is more crucial at higher levels.", "conclusion": "There is a necessity to improve LLMs' performance in medical applications at higher cognitive levels, suggesting directions for future research.", "key_contributions": ["Development of a multi-cognitive-level evaluation framework for LLMs in medicine", "Systematic evaluation of six prominent LLM families", "Insights into performance trends across cognitive levels"], "limitations": "The study focuses primarily on performance metrics and does not address broader implications of LLMs in diverse medical scenarios.", "keywords": ["large language models", "medical applications", "cognitive levels", "evaluation framework", "AI in health"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.08354", "pdf": "https://arxiv.org/pdf/2506.08354.pdf", "abs": "https://arxiv.org/abs/2506.08354", "title": "Text Embeddings Should Capture Implicit Semantics, Not Just Surface Meaning", "authors": ["Yiqun Sun", "Qiang Huang", "Anthony K. H. Tung", "Jun Yu"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "This position paper argues that the text embedding research community should\nmove beyond surface meaning and embrace implicit semantics as a central\nmodeling goal. Text embedding models have become foundational in modern NLP,\npowering a wide range of applications and drawing increasing research\nattention. Yet, much of this progress remains narrowly focused on surface-level\nsemantics. In contrast, linguistic theory emphasizes that meaning is often\nimplicit, shaped by pragmatics, speaker intent, and sociocultural context.\nCurrent embedding models are typically trained on data that lacks such depth\nand evaluated on benchmarks that reward the capture of surface meaning. As a\nresult, they struggle with tasks requiring interpretive reasoning, speaker\nstance, or social meaning. Our pilot study highlights this gap, showing that\neven state-of-the-art models perform only marginally better than simplistic\nbaselines on implicit semantics tasks. To address this, we call for a paradigm\nshift: embedding research should prioritize more diverse and linguistically\ngrounded training data, design benchmarks that evaluate deeper semantic\nunderstanding, and explicitly frame implicit meaning as a core modeling\nobjective, better aligning embeddings with real-world language complexity.", "AI": {"tldr": "The paper argues for a shift in text embedding research towards implicit semantics, emphasizing the need for deeper semantic understanding in NLP models.", "motivation": "Current text embedding models are largely focused on surface meaning and fail to capture the implicit semantics that are essential for understanding human language.", "method": "The paper reviews existing text embedding models and highlights their limitations in capturing implicit semantics through a pilot study comparing performance on implicit tasks.", "result": "The pilot study indicates that state-of-the-art embedding models only marginally outperform basic baselines on tasks involving implicit semantics, revealing significant gaps in their understanding.", "conclusion": "The paper calls for the embedding research community to focus on more diverse training data, meaningful benchmarks, and a core objective of modeling implicit meaning.", "key_contributions": ["Highlights the inadequacy of existing models in capturing implicit semantics", "Provides evidence from a pilot study demonstrating performance gaps", "Calls for a paradigm shift in text embedding research toward implicit semantic understanding"], "limitations": "The findings are based on a pilot study, which may limit the generalizability of the results and recommendations.", "keywords": ["text embeddings", "implicit semantics", "natural language processing", "linguistic theory", "modeling"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2503.16586", "pdf": "https://arxiv.org/pdf/2503.16586.pdf", "abs": "https://arxiv.org/abs/2503.16586", "title": "Big Help or Big Brother? Auditing Tracking, Profiling, and Personalization in Generative AI Assistants", "authors": ["Yash Vekaria", "Aurelio Loris Canino", "Jonathan Levitsky", "Alex Ciechonski", "Patricia Callejo", "Anna Maria Mandalari", "Zubair Shafiq"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CR", "cs.CY", "I.2; I.2.1; I.2.7; H.3.4; K.4; K.4.1; H.1; H.1.2; H.5.2; H.4.3"], "comment": null, "summary": "Generative AI (GenAI) browser assistants integrate powerful capabilities of\nGenAI in web browsers to provide rich experiences such as question answering,\ncontent summarization, and agentic navigation. These assistants, available\ntoday as browser extensions, can not only track detailed browsing activity such\nas search and click data, but can also autonomously perform tasks such as\nfilling forms, raising significant privacy concerns. It is crucial to\nunderstand the design and operation of GenAI browser extensions, including how\nthey collect, store, process, and share user data. To this end, we study their\nability to profile users and personalize their responses based on explicit or\ninferred demographic attributes and interests of users. We perform network\ntraffic analysis and use a novel prompting framework to audit tracking,\nprofiling, and personalization by the ten most popular GenAI browser assistant\nextensions. We find that instead of relying on local in-browser models, these\nassistants largely depend on server-side APIs, which can be auto-invoked\nwithout explicit user interaction. When invoked, they collect and share webpage\ncontent, often the full HTML DOM and sometimes even the user's form inputs,\nwith their first-party servers. Some assistants also share identifiers and user\nprompts with third-party trackers such as Google Analytics. The collection and\nsharing continues even if a webpage contains sensitive information such as\nhealth or personal information such as name or SSN entered in a web form. We\nfind that several GenAI browser assistants infer demographic attributes such as\nage, gender, income, and interests and use this profile--which carries across\nbrowsing contexts--to personalize responses. In summary, our work shows that\nGenAI browser assistants can and do collect personal and sensitive information\nfor profiling and personalization with little to no safeguards.", "AI": {"tldr": "This paper investigates the privacy implications of generative AI browser assistants, focusing on their data collection and profiling practices.", "motivation": "To understand how generative AI browser assistants operate and address privacy concerns stemming from their data collection practices.", "method": "The study employs network traffic analysis and a novel prompting framework to audit the tracking, profiling, and personalization practices of popular GenAI browser extensions.", "result": "The investigation reveals that these browser assistants heavily rely on server-side APIs for data processing, collect extensive user data (including sensitive information), and use this data to create user profiles for personalization.", "conclusion": "The findings indicate that generative AI browser assistants can collect and share personal and sensitive information with minimal safeguards, raising significant privacy concerns.", "key_contributions": ["Analyzed the data collection practices of popular GenAI browser assistants", "Demonstrated the reliance on server-side APIs over local processing", "Highlighted the risks associated with user profiling and data sharing"], "limitations": "The study focuses on only the top ten GenAI browser extensions, and the findings may not represent all browser assistants.", "keywords": ["Generative AI", "browser assistants", "data privacy", "user profiling", "personalization"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.08359", "pdf": "https://arxiv.org/pdf/2506.08359.pdf", "abs": "https://arxiv.org/abs/2506.08359", "title": "DEAL: Disentangling Transformer Head Activations for LLM Steering", "authors": ["Li-Ming Zhan", "Bo Liu", "Zexin Lu", "Chengqiang Xie", "Jiannong Cao", "Xiao-Ming Wu"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Inference-time steering aims to alter the response characteristics of large\nlanguage models (LLMs) without modifying their underlying parameters. A\ncritical step in this process is the identification of internal modules within\nLLMs that are associated with the target behavior. However, current approaches\nto module selection often depend on superficial cues or ad-hoc heuristics,\nwhich can result in suboptimal or unintended outcomes. In this work, we propose\na principled causal-attribution framework for identifying behavior-relevant\nattention heads in transformers. For each head, we train a vector-quantized\nautoencoder (VQ-AE) on its attention activations, partitioning the latent space\ninto behavior-relevant and behavior-irrelevant subspaces, each quantized with a\nshared learnable codebook. We assess the behavioral relevance of each head by\nquantifying the separability of VQ-AE encodings for behavior-aligned versus\nbehavior-violating responses using a binary classification metric. This yields\na behavioral relevance score that reflects each head discriminative capacity\nwith respect to the target behavior, guiding both selection and importance\nweighting. Experiments on seven LLMs from two model families and five\nbehavioral steering datasets demonstrate that our method enables more accurate\ninference-time interventions, achieving superior performance on the\ntruthfulness-steering task. Furthermore, the heads selected by our approach\nexhibit strong zero-shot generalization in cross-domain truthfulness-steering\nscenarios.", "AI": {"tldr": "This paper presents a method for identifying behavior-relevant attention heads in large language models to improve inference-time steering accuracy.", "motivation": "To enhance inference-time steering of LLMs by identifying internal modules responsible for specific behaviors, as current methods are ineffective.", "method": "A causal-attribution framework is introduced that uses a vector-quantized autoencoder to analyze attention heads' activations, separating the latent space into behavior-relevant and behavior-irrelevant parts.", "result": "The approach improves accuracy in steering large language models, particularly in truthfulness tasks, and shows strong zero-shot generalization across different domains.", "conclusion": "The proposed method enables better selection and weighting of attention heads in LLMs for more reliable behavior steering during inference.", "key_contributions": ["Development of a causal-attribution framework for attention head analysis in LLMs.", "Introduction of a vector-quantized autoencoder for behavioral relevance assessment.", "Demonstration of improved performance on truthfulness-steering tasks across various LLMs."], "limitations": "The method's effectiveness may vary across different model architectures and behaviors evaluated.", "keywords": ["large language models", "inference-time steering", "behavioral relevance", "vector-quantized autoencoder", "transformers"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.08364", "pdf": "https://arxiv.org/pdf/2506.08364.pdf", "abs": "https://arxiv.org/abs/2506.08364", "title": "CC-RAG: Structured Multi-Hop Reasoning via Theme-Based Causal Graphs", "authors": ["Jash Rajesh Parekh", "Pengcheng Jiang", "Jiawei Han"], "categories": ["cs.CL"], "comment": null, "summary": "Understanding cause and effect relationships remains a formidable challenge\nfor Large Language Models (LLMs), particularly in specialized domains where\nreasoning requires more than surface-level correlations. Retrieval-Augmented\nGeneration (RAG) improves factual accuracy, but standard RAG pipelines treat\nevidence as flat context, lacking the structure required to model true causal\ndependencies. We introduce Causal-Chain RAG (CC-RAG), a novel approach that\nintegrates zero-shot triple extraction and theme-aware graph chaining into the\nRAG pipeline, enabling structured multi-hop inference. Given a domain specific\ncorpus, CC-RAG constructs a Directed Acyclic Graph (DAG) of <cause, relation,\neffect> triples and uses forward/backward chaining to guide structured answer\ngeneration. Experiments on two real-world domains: Bitcoin price fluctuations\nand Gaucher disease, show that CC-RAG outperforms standard RAG and zero-shot\nLLMs in chain similarity, information density, and lexical diversity. Both\nLLM-as-a-Judge and human evaluations consistently favor CC-RAG. Our results\ndemonstrate that explicitly modeling causal structure enables LLMs to generate\nmore accurate and interpretable responses, especially in specialized domains\nwhere flat retrieval fails.", "AI": {"tldr": "CC-RAG is a novel approach that enhances Retrieval-Augmented Generation by integrating causal structure into the inference process, leading to improved accuracy and interpretability in LLM outputs.", "motivation": "Large Language Models struggle with understanding cause and effect, especially in specialized domains that require deeper reasoning. Standard Retrieval-Augmented Generation lacks the capability to model true causal relationships.", "method": "CC-RAG integrates zero-shot triple extraction and theme-aware graph chaining into the RAG pipeline, constructing a Directed Acyclic Graph (DAG) of causal triples and employing forward/backward chaining for answer generation.", "result": "CC-RAG outperforms standard RAG and zero-shot LLMs in chain similarity, information density, and lexical diversity across two domains: Bitcoin price fluctuations and Gaucher disease.", "conclusion": "Explicitly modeling causal structures allows LLMs to produce more accurate and interpretable results, particularly in domains where flat retrieval techniques are insufficient.", "key_contributions": ["Introduction of Causal-Chain RAG (CC-RAG) for structured multi-hop inference.", "Integration of causal dependency modeling in standard RAG pipelines.", "Demonstrated improved performance in real-world applications compared to existing methods."], "limitations": "", "keywords": ["Causal-Chain RAG", "Retrieval-Augmented Generation", "Large Language Models", "causal inference", "information retrieval"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.23472", "pdf": "https://arxiv.org/pdf/2505.23472.pdf", "abs": "https://arxiv.org/abs/2505.23472", "title": "Self-driving technologies need the help of the public: A narrative review of the evidence", "authors": ["Jonathan Smith", "Siddartha Khastgir"], "categories": ["cs.HC", "K.4.0; K.4.1; K.4.2"], "comment": null, "summary": "If public trust is lost in a new technology early in its life cycle it can\ntake much more time for the benefits of that technology to be realised.\nEventually tens-of-millions of people will collectively have the power to\ndetermine self-driving technology success of failure driven by their perception\nof risk, data handling, safety, governance, accountability, benefits to their\nlife and more. This paper reviews the evidence on safety critical technology\ncovering trust, engagement, and acceptance. The paper takes a narrative review\napproach concluding with a scalable model for self-driving technology education\nand engagement. The paper find that if a mismatch between the publics\nperception and expectations about self driving systems emerge it can lead to\nmisuse, disuse, or abuse of the system. Furthermore we find from the evidence\nthat industrial experts often misunderstand what matters to the public, users,\nand stakeholders. However we find that engagement programmes that develop\napproaches to defining the right information at the right time, in the right\nformat orientated around what matters to the public creates the potential for\never more sophisticated conversations, greater trust, and moving the public\ninto a progressive more active role of critique and advocacy. This work has\nbeen undertaken as part of the Partners for Automated Vehicle Education (PAVE)\nUnited Kingdom programme.", "AI": {"tldr": "This paper reviews public trust and engagement issues related to self-driving technology, proposing a model for education to enhance acceptance and trust.", "motivation": "To understand and address the public's perceptions and expectations regarding self-driving technology, which impact its acceptance and success.", "method": "A narrative review approach is employed to analyze the existing evidence on trust, engagement, and acceptance of safety-critical technologies.", "result": "The findings indicate that a mismatch between public perception and expectations can lead to misuse or disuse of self-driving technologies. Engagement programs that align information with public interests can foster greater trust and active participation.", "conclusion": "A scalable model for self-driving technology education is proposed to improve public engagement and trust.", "key_contributions": ["Review of evidence on trust and safety in self-driving technology", "Proposal of a scalable education model for public engagement", "Insights into public expectations and expert misunderstandings regarding self-driving systems"], "limitations": "The paper mainly focuses on the United Kingdom context and may not generalize to other regions.", "keywords": ["self-driving technology", "public trust", "engagement", "safety", "education"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.08371", "pdf": "https://arxiv.org/pdf/2506.08371.pdf", "abs": "https://arxiv.org/abs/2506.08371", "title": "Mitigating Posterior Salience Attenuation in Long-Context LLMs with Positional Contrastive Decoding", "authors": ["Zikai Xiao", "Ziyang Wang", "Wen Ma", "Yan Zhang", "Wei Shen", "Yan Wang", "Luqi Gong", "Zuozhu Liu"], "categories": ["cs.CL"], "comment": null, "summary": "While Large Language Models (LLMs) support long contexts, they struggle with\nperformance degradation within the context window. Current solutions incur\nprohibitive training costs, leaving statistical behaviors and cost-effective\napproaches underexplored. From the decoding perspective, we identify the\nPosterior Salience Attenuation (PSA) phenomenon, where the salience ratio\ncorrelates with long-text performance degradation. Notably, despite the\nattenuation, gold tokens still occupy high-ranking positions in the decoding\nspace. Motivated by it, we propose the training-free Positional Contrastive\nDecoding (PCD) that contrasts the logits derived from long-aware attention with\nthose from designed local-aware attention, enabling the model to focus on the\ngains introduced by large-scale short-to-long training. Through the analysis of\nlong-term decay simulation, we demonstrate that PCD effectively alleviates\nattention score degradation. Experimental results show that PCD achieves\nstate-of-the-art performance on long-context benchmarks.", "AI": {"tldr": "Positional Contrastive Decoding (PCD) addresses long-context performance issues in Large Language Models without the need for training, significantly improving attention score degradation.", "motivation": "LLMs struggle with performance degradation in long contexts, and existing solutions are costly and underexplored.", "method": "The paper proposes PCD, which contrasts logits from long-aware and local-aware attention mechanisms to enhance focus on long-context performance without retraining.", "result": "PCD alleviates attention score degradation and achieves state-of-the-art results on long-context benchmarks.", "conclusion": "PCD offers a novel training-free method to improve LLM performance on long-context tasks, showcasing significant effectiveness.", "key_contributions": ["Introduces Positional Contrastive Decoding (PCD) as a training-free method for long-context performance improvement.", "Identifies the Posterior Salience Attenuation (PSA) phenomenon linked to attention score degradation.", "Demonstrates state-of-the-art results on long-context benchmarks."], "limitations": "", "keywords": ["Large Language Models", "Long Context", "Positional Contrastive Decoding", "Attention Mechanisms", "Performance Degradation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.08373", "pdf": "https://arxiv.org/pdf/2506.08373.pdf", "abs": "https://arxiv.org/abs/2506.08373", "title": "Draft-based Approximate Inference for LLMs", "authors": ["Kevin Galim", "Ethan Ewer", "Wonjun Kang", "Minjae Lee", "Hyung Il Koo", "Kangwook Lee"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, which\nleverages a draft output to accurately assess the importance of each KV pair\nfor more effective KV cache dropping, and (ii) SpecPC, which uses the draft\nmodel's attention activations to identify and discard unimportant prompt\ntokens. To the best of our knowledge, this is the first work to use draft\nmodels for approximate LLM inference acceleration, extending their utility\nbeyond traditional lossless speculative decoding. We motivate our methods with\ntheoretical and empirical analyses, and show a strong correlation between the\nattention patterns of draft and target models. Extensive experiments on\nlong-context benchmarks show that our methods consistently achieve higher\naccuracy than existing baselines, while preserving the same improvements in\nmemory usage, latency, and throughput. Our code is available at\nhttps://github.com/furiosa-ai/draft-based-approx-llm.", "AI": {"tldr": "A novel framework using draft models to optimize inference in long-context LLMs by accurately predicting token and KV pair importance.", "motivation": "There is a need to improve the efficiency of inference in long-context LLMs due to high computational and memory demands.", "method": "Introduces SpecKV for assessing KV pair importance and SpecPC for identifying unimportant prompt tokens using draft models.", "result": "Experimental results indicate improved accuracy over existing methods, with benefits in memory usage, latency, and throughput.", "conclusion": "The proposed framework enhances inference acceleration in LLMs while maintaining performance metrics; represents a novel application of draft models.", "key_contributions": ["First approach leveraging draft models for LLM inference acceleration", "Development of SpecKV and SpecPC", "Empirical validations showing superior performance over existing methods"], "limitations": "", "keywords": ["Large Language Models", "draft models", "approximate inference", "KV cache dropping", "memory efficiency"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.08375", "pdf": "https://arxiv.org/pdf/2506.08375.pdf", "abs": "https://arxiv.org/abs/2506.08375", "title": "EIFBENCH: Extremely Complex Instruction Following Benchmark for Large Language Models", "authors": ["Tao Zou", "Xinghua Zhang", "Haiyang Yu", "Minzheng Wang", "Fei Huang", "Yongbin Li"], "categories": ["cs.CL"], "comment": "24 pages", "summary": "With the development and widespread application of large language models\n(LLMs), the new paradigm of \"Model as Product\" is rapidly evolving, and demands\nhigher capabilities to address complex user needs, often requiring precise\nworkflow execution which involves the accurate understanding of multiple tasks.\nHowever, existing benchmarks focusing on single-task environments with limited\nconstraints lack the complexity required to fully reflect real-world scenarios.\nTo bridge this gap, we present the Extremely Complex Instruction Following\nBenchmark (EIFBENCH), meticulously crafted to facilitate a more realistic and\nrobust evaluation of LLMs. EIFBENCH not only includes multi-task scenarios that\nenable comprehensive assessment across diverse task types concurrently, but\nalso integrates a variety of constraints, replicating complex operational\nenvironments. Furthermore, we propose the Segment Policy Optimization (SegPO)\nalgorithm to enhance the LLM's ability to accurately fulfill multi-task\nworkflow. Evaluations on EIFBENCH have unveiled considerable performance\ndiscrepancies in existing LLMs when challenged with these extremely complex\ninstructions. This finding underscores the necessity for ongoing optimization\nto navigate the intricate challenges posed by LLM applications.", "AI": {"tldr": "This paper introduces EIFBENCH, a new benchmark for evaluating large language models in multi-task environments, alongside the SegPO algorithm for optimizing task execution.", "motivation": "Existing benchmarks for large language models focus on single tasks, failing to capture the complexity of real-world scenarios that require accurate multi-task execution.", "method": "The paper presents the Extremely Complex Instruction Following Benchmark (EIFBENCH) designed to assess LLMs on multi-task scenarios with various constraints, alongside the Segment Policy Optimization (SegPO) algorithm.", "result": "Evaluations on EIFBENCH reveal significant performance gaps in existing LLMs when faced with complex instructions, highlighting the need for further optimization.", "conclusion": "Ongoing optimization of LLMs is essential to effectively handle the intricacies involved in multi-task workflows as presented in real-world applications.", "key_contributions": ["Introduction of the Extremely Complex Instruction Following Benchmark (EIFBENCH)", "Development of the Segment Policy Optimization (SegPO) algorithm", "Identification of performance discrepancies in existing LLMs with complex instructions"], "limitations": "", "keywords": ["Large Language Models", "Benchmarking", "Multi-Task Instruction Following", "SegPO", "AI Optimization"], "importance_score": 9, "read_time_minutes": 24}}
{"id": "2506.08400", "pdf": "https://arxiv.org/pdf/2506.08400.pdf", "abs": "https://arxiv.org/abs/2506.08400", "title": "mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks", "authors": ["Luel Hagos Beyene", "Vivek Verma", "Min Ma", "Jesujoba O. Alabi", "Fabian David Schmidt", "Joyce Nakatumba-Nabende", "David Ifeoluwa Adelani"], "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": "working paper", "summary": "Large Language models (LLMs) have demonstrated impressive performance on a\nwide range of tasks, including in multimodal settings such as speech. However,\ntheir evaluation is often limited to English and a few high-resource languages.\nFor low-resource languages, there is no standardized evaluation benchmark. In\nthis paper, we address this gap by introducing mSTEB, a new benchmark to\nevaluate the performance of LLMs on a wide range of tasks covering language\nidentification, text classification, question answering, and translation tasks\non both speech and text modalities. We evaluated the performance of leading\nLLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art open\nmodels such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap in\nperformance between high-resource and low-resource languages, especially for\nlanguages spoken in Africa and Americas/Oceania. Our findings show that more\ninvestment is needed to address their under-representation in LLMs coverage.", "AI": {"tldr": "This paper introduces mSTEB, a benchmark for evaluating LLM performance across tasks and languages, addressing the gap in evaluations for low-resource languages.", "motivation": "To provide a standardized evaluation benchmark for Low-Resource Languages in the context of Large Language Models (LLMs), as current evaluations are primarily focused on high-resource languages.", "method": "The authors introduced mSTEB, a new benchmark that evaluates tasks including language identification, text classification, question answering, and translation across speech and text modalities, using leading LLMs for testing.", "result": "Evaluation of LLMs such as Gemini 2.0 Flash and GPT-4o reveals significant performance disparities between high-resource and low-resource languages, particularly affecting languages spoken in Africa and the Americas/Oceania.", "conclusion": "The findings highlight the necessity for more investment in LLM development to improve functionality for underrepresented, low-resource languages.", "key_contributions": ["Introduction of mSTEB benchmark for evaluating LLMs", "Comprehensive evaluation across multiple tasks and modalities", "Highlighting the performance gap for low-resource languages"], "limitations": "", "keywords": ["large language models", "low-resource languages", "evaluation benchmark", "multimodal tasks", "speech and text"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2411.15129", "pdf": "https://arxiv.org/pdf/2411.15129.pdf", "abs": "https://arxiv.org/abs/2411.15129", "title": "The BS-meter: A ChatGPT-Trained Instrument to Detect Sloppy Language-Games", "authors": ["Alessandro Trevisan", "Harry Giddens", "Sarah Dillon", "Alan F. Blackwell"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "What can we learn about language from studying how it is used by ChatGPT and\nother large language model (LLM)-based chatbots? In this paper, we analyse the\ndistinctive character of language generated by ChatGPT, in relation to\nquestions raised by natural language processing pioneer, and student of\nWittgenstein, Margaret Masterman. Following frequent complaints that LLM-based\nchatbots produce \"slop,\" or even \"bullshit,\" in the sense of Frankfurt's\npopular monograph On Bullshit, we conduct an empirical study to contrast the\nlanguage of 1,000 scientific publications with typical text generated by\nChatGPT. We then explore whether the same language features can be detected in\ntwo well-known contexts of social dysfunction: George Orwell's critique of\npolitical speech, and David Graeber's characterisation of bullshit jobs. Using\nsimple hypothesis-testing methods, we demonstrate that a statistical model of\nsloppy bullshit can reliably relate the Frankfurtian artificial bullshit of\nChatGPT to the political and workplace functions of bullshit as observed in\nnatural human language.", "AI": {"tldr": "The paper analyzes the language characteristics of ChatGPT in comparison to academic publications and critiques of language by Orwell and Graeber, finding a statistically significant relationship to themes of 'bullshit.'", "motivation": "To understand the distinctive character of language generated by ChatGPT and its implications in the context of natural language use as critiqued by previous thinkers.", "method": "An empirical study comparing the language features of 1,000 scientific publications with text generated by ChatGPT, alongside hypothesis testing of linguistic patterns and themes.", "result": "The study shows that certain language features identified in ChatGPT correspond significantly with concepts of 'bullshit' as outlined by Frankfurt and reflected in the critiques of political and workplace language by Orwell and Graeber.", "conclusion": "Statistical analysis demonstrates a reliable connection between the language used by LLMs like ChatGPT and human-defined constructs of bullshit, prompting further inquiry into the implications of AI-generated language.", "key_contributions": ["Identifies linguistic features of ChatGPT related to academic language and critique of political speech.", "Establishes a statistical model linking AI language to historical critiques of language and social dysfunction.", "Expands understanding of how LLM language may reflect or distort human communication norms."], "limitations": "The analysis is limited to specific texts and contexts; broader implications for varied language settings are yet to be explored.", "keywords": ["ChatGPT", "language analysis", "bullshit", "natural language processing", "empirical study"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.08403", "pdf": "https://arxiv.org/pdf/2506.08403.pdf", "abs": "https://arxiv.org/abs/2506.08403", "title": "TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration", "authors": ["Weiya Li", "Junjie Chen", "Bei Li", "Boyang Liu", "Zichen Wen", "Nuanqiao Shan", "Xiaoqian Liu", "Anping Liu", "Huajie Liu", "Youyan Wang", "Wujiuge Yin", "Hu Song", "Bing Huang", "Zhiyuan Xia", "Jialiang Chen", "Linfeng Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 4 figures, Under review. Code:\n  https://github.com/weiyali126/TACTIC", "summary": "Machine translation has long been a central task in natural language\nprocessing. With the rapid advancement of large language models (LLMs), there\nhas been remarkable progress in translation quality. However, fully realizing\nthe translation potential of LLMs remains an open challenge. Recent studies\nhave explored multi-agent systems to decompose complex translation tasks into\ncollaborative subtasks, showing initial promise in enhancing translation\nquality through agent cooperation and specialization. Nevertheless, existing\nmulti-agent translation frameworks largely neglect foundational insights from\ncognitive translation studies. These insights emphasize how human translators\nemploy different cognitive strategies, such as balancing literal and free\ntranslation, refining expressions based on context, and iteratively evaluating\noutputs. To address this limitation, we propose a cognitively informed\nmulti-agent framework called TACTIC, which stands for T ranslation A gents with\nCognitive- T heoretic Interactive Collaboration. The framework comprises six\nfunctionally distinct agents that mirror key cognitive processes observed in\nhuman translation behavior. These include agents for drafting, refinement,\nevaluation, scoring, context reasoning, and external knowledge gathering. By\nsimulating an interactive and theory-grounded translation workflow, TACTIC\neffectively leverages the full capacity of LLMs for high-quality translation.\nExperimental results on diverse language pairs from the FLORES-200 and WMT24\nbenchmarks show that our method consistently achieves state-of-the-art\nperformance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by\nan average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it\nfurther improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at\nhttps://github.com/weiyali126/TACTIC.", "AI": {"tldr": "TACTIC is a cognitively informed multi-agent framework that enhances machine translation by mimicking human cognitive strategies through collaborative agents.", "motivation": "To address the limitations of existing multi-agent translation frameworks that neglect cognitive insights from human translators, leading to suboptimal translation quality.", "method": "TACTIC employs six distinct agents that simulate human cognitive processes in translation, including drafting, refinement, evaluation, scoring, context reasoning, and knowledge gathering.", "result": "Experimental results demonstrate that TACTIC achieves state-of-the-art translation performance, consistently surpassing both GPT-4.1 and DeepSeek-R1 across various language pairs.", "conclusion": "TACTIC effectively leverages LLMs by incorporating cognitive strategies, leading to significant improvements in translation quality.", "key_contributions": ["Cognitively informed multi-agent framework for translation", "Detailed simulation of human cognitive translation processes", "Demonstrated superior performance compared to existing models"], "limitations": "", "keywords": ["Machine Translation", "Cognitive Processes", "Multi-Agent Systems", "Natural Language Processing", "Large Language Models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.08410", "pdf": "https://arxiv.org/pdf/2506.08410.pdf", "abs": "https://arxiv.org/abs/2506.08410", "title": "Large Language Models Have Intrinsic Meta-Cognition, but Need a Good Lens", "authors": ["Ziyang Ma", "Qingyue Yuan", "Zhenglin Wang", "Deyu Zhou"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Previous research has primarily focused on the cognitive error detection\ncapabilities of Large Language Models (LLMs), often prompting them to analyze\nmistakes in reasoning chains. However, few studies have examined the\nmeta-cognitive abilities of LLMs (e.g., their self-awareness of step errors),\nwhich are crucial for their reliability. While studies on LLM self-evaluation\npresent some measures, such as perplexity, which can reflect the answer\ncorrectness and be viewed as the lens of meta-cognition, they lack step-level\nanalysis and adaptation. This paper studies the evaluation of LLM\nmeta-cognition using the current lenses and how to improve these lenses.\nSpecifically, we propose AutoMeco, an Automated Meta-cognition Evaluation\nframework for benchmarking the existing lenses. Furthermore, a training-free\nMarkovian Intrinsic Reward Adjustment strategy, MIRA, is proposed to boost\ncurrent meta-cognition lenses. Experimental results on three mathematical\nreasoning datasets and three LLMs show the reasonableness of AutoMeco by\ncomparing it with Best-of-N verification. Moreover, the meta-cognition ability\nof LLMs can be better evaluated using MIRA.", "AI": {"tldr": "This paper examines the meta-cognitive abilities of Large Language Models (LLMs) and proposes a framework for evaluating them.", "motivation": "Previous research has largely overlooked the meta-cognitive capabilities of LLMs, focusing mostly on cognitive error detection. Understanding their self-awareness regarding errors is essential for improving their reliability.", "method": "The paper introduces AutoMeco, an Automated Meta-cognition Evaluation framework for benchmarking existing evaluation methods. Additionally, the Markovian Intrinsic Reward Adjustment strategy (MIRA) is proposed to enhance the meta-cognition evaluation processes without requiring additional training.", "result": "Experimental results demonstrate the effectiveness of AutoMeco and reveal that LLMs' meta-cognition can be more accurately assessed using MIRA, particularly when benchmarked against Best-of-N verification from established methods.", "conclusion": "The findings indicate that by improving evaluation techniques with AutoMeco and MIRA, the assessment of LLM meta-cognition can be significantly enhanced, ultimately contributing to the reliability of these models.", "key_contributions": ["Introduction of AutoMeco framework for meta-cognition evaluation of LLMs", "Development of MIRA strategy to enhance existing evaluation lenses", "Empirical results validating the effectiveness of the proposed methods on various datasets"], "limitations": "The study primarily focuses on mathematical reasoning datasets, which may not generalize to all types of reasoning tasks. Further exploration is needed in more diverse contexts.", "keywords": ["Meta-cognition", "Large Language Models", "Error detection", "AutoMeco", "MIRA"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2502.15226", "pdf": "https://arxiv.org/pdf/2502.15226.pdf", "abs": "https://arxiv.org/abs/2502.15226", "title": "Understand User Opinions of Large Language Models via LLM-Powered In-the-Moment User Experience Interviews", "authors": ["Mengqiao Liu", "Tevin Wang", "Cassandra A. Cohen", "Sarah Li", "Chenyan Xiong"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Which large language model (LLM) is better? Every evaluation tells a story,\nbut what do users really think about current LLMs? This paper presents CLUE, an\nLLM-powered interviewer that conducts in-the-moment user experience interviews,\nright after users interact with LLMs, and automatically gathers insights about\nuser opinions from massive interview logs. We conduct a study with thousands of\nusers to understand user opinions on mainstream LLMs, recruiting users to first\nchat with a target LLM and then be interviewed by CLUE. Our experiments\ndemonstrate that CLUE captures interesting user opinions, e.g., the bipolar\nviews on the displayed reasoning process of DeepSeek-R1 and demands for\ninformation freshness and multi-modality. Our code and data are at\nhttps://github.com/cxcscmu/LLM-Interviewer.", "AI": {"tldr": "The paper presents CLUE, an LLM-powered interviewer that gathers user insights immediately after interactions with LLMs, showcasing user opinions through a large-scale study.", "motivation": "To evaluate current LLMs from a user perspective and gather real-time insights on user experiences.", "method": "Conducted a study involving thousands of users who interacted with LLMs followed by interviews facilitated by CLUE to collect opinions.", "result": "Users expressed varied opinions on LLMs, highlighting concerns such as the reasoning process and the need for information freshness and multi-modality.", "conclusion": "CLUE effectively captures nuanced user opinions about LLMs, providing valuable insights into user experiences.", "key_contributions": ["Introduction of an LLM-powered interview system (CLUE) for user experience evaluation.", "Large-scale study with real users to collect insights on LLMs.", "Findings on user opinions regarding reasoning processes and multi-modality in LLMs."], "limitations": "", "keywords": ["User Experience", "Language Models", "Human-Computer Interaction", "LLM Evaluation", "CLUE"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.08427", "pdf": "https://arxiv.org/pdf/2506.08427.pdf", "abs": "https://arxiv.org/abs/2506.08427", "title": "Know-MRI: A Knowledge Mechanisms Revealer&Interpreter for Large Language Models", "authors": ["Jiaxiang Liu", "Boxuan Xing", "Chenhao Yuan", "Chenxiang Zhang", "Di Wu", "Xiusheng Huang", "Haida Yu", "Chuhan Lang", "Pengfei Cao", "Jun Zhao", "Kang Liu"], "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) continue to advance, there is a growing\nurgency to enhance the interpretability of their internal knowledge mechanisms.\nConsequently, many interpretation methods have emerged, aiming to unravel the\nknowledge mechanisms of LLMs from various perspectives. However, current\ninterpretation methods differ in input data formats and interpreting outputs.\nThe tools integrating these methods are only capable of supporting tasks with\nspecific inputs, significantly constraining their practical applications. To\naddress these challenges, we present an open-source Knowledge Mechanisms\nRevealer&Interpreter (Know-MRI) designed to analyze the knowledge mechanisms\nwithin LLMs systematically. Specifically, we have developed an extensible core\nmodule that can automatically match different input data with interpretation\nmethods and consolidate the interpreting outputs. It enables users to freely\nchoose appropriate interpretation methods based on the inputs, making it easier\nto comprehensively diagnose the model's internal knowledge mechanisms from\nmultiple perspectives. Our code is available at\nhttps://github.com/nlpkeg/Know-MRI. We also provide a demonstration video on\nhttps://youtu.be/NVWZABJ43Bs.", "AI": {"tldr": "The paper introduces Know-MRI, an open-source tool for analyzing knowledge mechanisms in large language models (LLMs) that supports diverse input formats and interpretation methods.", "motivation": "To improve the interpretability of large language models and address the limitations of current interpretation methods that restrict their practical applications.", "method": "Developed an extensible core module for Know-MRI that matches various input data with corresponding interpretation methods and consolidates the outputs.", "result": "Know-MRI allows users to choose interpretation methods tailored to their input data, enhancing the comprehensiveness of model diagnosis across multiple perspectives.", "conclusion": "The tool facilitates a better understanding of LLMs' internal knowledge mechanisms and is readily accessible for users.", "key_contributions": ["Introduction of the Know-MRI tool for LLM interpretation", "Extensible core module for matching inputs with interpretation methods", "Open-source availability for community use"], "limitations": "", "keywords": ["large language models", "interpretability", "knowledge mechanisms", "open-source", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.08430", "pdf": "https://arxiv.org/pdf/2506.08430.pdf", "abs": "https://arxiv.org/abs/2506.08430", "title": "CAF-I: A Collaborative Multi-Agent Framework for Enhanced Irony Detection with Large Language Models", "authors": ["Ziqi. Liu", "Ziyang. Zhou", "Mingxuan. Hu"], "categories": ["cs.CL", "cs.MA"], "comment": "ICML 2025 Workshop on Collaborative and Federated Agentic Workflows", "summary": "Large language model (LLM) have become mainstream methods in the field of\nsarcasm detection. However, existing LLM methods face challenges in irony\ndetection, including: 1. single-perspective limitations, 2. insufficient\ncomprehensive understanding, and 3. lack of interpretability. This paper\nintroduces the Collaborative Agent Framework for Irony (CAF-I), an LLM-driven\nmulti-agent system designed to overcome these issues. CAF-I employs specialized\nagents for Context, Semantics, and Rhetoric, which perform multidimensional\nanalysis and engage in interactive collaborative optimization. A Decision Agent\nthen consolidates these perspectives, with a Refinement Evaluator Agent\nproviding conditional feedback for optimization. Experiments on benchmark\ndatasets establish CAF-I's state-of-the-art zero-shot performance. Achieving\nSOTA on the vast majority of metrics, CAF-I reaches an average Macro-F1 of\n76.31, a 4.98 absolute improvement over the strongest prior baseline. This\nsuccess is attained by its effective simulation of human-like multi-perspective\nanalysis, enhancing detection accuracy and interpretability.", "AI": {"tldr": "This paper presents CAF-I, a multi-agent LLM system for improved irony detection, addressing existing limitations and achieving state-of-the-art performance.", "motivation": "To address the challenges faced by existing LLM methods in irony detection, including single-perspective limitations and lack of interpretability.", "method": "The Collaborative Agent Framework for Irony (CAF-I) uses specialized agents for different aspects—Context, Semantics, and Rhetoric—to perform multidimensional analysis and collaborative optimization, culminating in a Decision Agent to consolidate insights.", "result": "CAF-I achieves state-of-the-art performance on irony detection benchmarks, with an average Macro-F1 score of 76.31, representing a 4.98 improvement over the previous baseline.", "conclusion": "CAF-I effectively simulates human-like analysis for better accuracy and interpretability in irony detection.", "key_contributions": ["Introduction of the Collaborative Agent Framework for Irony (CAF-I)", "Use of specialized agents for multidimensional analysis", "Achievement of state-of-the-art performance with improved interpretability"], "limitations": "", "keywords": ["irony detection", "large language models", "multi-agent system", "interpretability", "natural language processing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.08433", "pdf": "https://arxiv.org/pdf/2506.08433.pdf", "abs": "https://arxiv.org/abs/2506.08433", "title": "Low-resource domain adaptation while minimizing energy and hardware resource consumption", "authors": ["Hernán Maina", "Nicolás Wolovick", "Luciana Benotti"], "categories": ["cs.CL", "cs.DC", "cs.LG"], "comment": "A shorter version of this work was accepted as a two-page abstract\n  for presentation at the Widening Natural Language Processing (WiNLP) 2023\n  Workshop. That version was not publicly released, and this is the first\n  public version of the work", "summary": "Training Large Language Models (LLMs) is costly in terms of energy, hardware,\nand annotated data, often resulting in a positionality rooted in predominant\ncultures and values (Santy et al., 2023). Domain adaptation has emerged as a\npromising strategy to better align models with diverse cultural and value\ncontexts (Hershcovich et al., 2022), but its computational cost remains a\nsignificant barrier, particularly for research groups lacking access to\nlarge-scale infrastructure. In this paper, we evaluate how the use of different\nnumerical precisions and data parallelization strategies impacts both training\nspeed (as a proxy to energy and hardware consumption) and model accuracy, with\nthe goal of facilitating domain adaptation in low-resource environments. Our\nfindings are relevant to any setting where energy efficiency, accessibility, or\nlimited hardware availability are key concerns.", "AI": {"tldr": "This paper evaluates the impact of numerical precision and data parallelization on training speed and model accuracy for Large Language Models, aiming to improve domain adaptation in low-resource settings.", "motivation": "To address the high costs and accessibility issues associated with training Large Language Models (LLMs) in diverse cultural and value contexts.", "method": "The study examines different numerical precisions and data parallelization strategies during the training of LLMs, measuring their effects on training speed and model accuracy.", "result": "The evaluation shows that adjusting numerical precisions and employing data parallelization can significantly enhance training speed while maintaining model accuracy, making domain adaptation more feasible in low-resource environments.", "conclusion": "The findings suggest that energy efficiency and model accessibility can be improved for LLM training, benefitting settings with constrained hardware and energy resources.", "key_contributions": ["Analysis of numerical precision and its impact on training performance", "Evaluation of data parallelization strategies for LLMs", "Recommendations for low-resource environments to facilitate domain adaptation"], "limitations": "", "keywords": ["Large Language Models", "Domain Adaptation", "Numerical Precision", "Data Parallelization", "Energy Efficiency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.08436", "pdf": "https://arxiv.org/pdf/2506.08436.pdf", "abs": "https://arxiv.org/abs/2506.08436", "title": "Olica: Efficient Structured Pruning of Large Language Models without Retraining", "authors": ["Jiujun He", "Huazhen Lin"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ICML 2025", "summary": "Most existing structured pruning methods for Large Language Models (LLMs)\nrequire substantial computational and data resources for retraining to\nreestablish the corrupted correlations, making them prohibitively expensive. To\naddress this, we propose a pruning framework for LLMs called Orthogonal\ndecomposition and Linear Calibration (Olica), which eliminates the need for\nretraining. A key observation is that the multi-head attention (MHA) layer\ndepends on two types of matrix products. By treating these matrix products as\nunified entities and applying principal component analysis (PCA), we extract\nthe most important information to compress LLMs without sacrificing accuracy or\ndisrupting their original structure. Consequently, retraining becomes\nunnecessary. A fast decomposition method is devised, reducing the complexity of\nPCA by a factor of the square of the number of attention heads. Additionally,\nto mitigate error accumulation problem caused by pruning the feed-forward\nnetwork (FFN) layer, we introduce a linear calibration method to reconstruct\nthe residual errors of pruned layers using low-rank matrices. By leveraging\nsingular value decomposition (SVD) on the solution of the least-squares\nproblem, these matrices are obtained without requiring retraining. Extensive\nexperiments show that the proposed Olica is efficient in terms of data usage,\nGPU memory, and running time, while delivering superior performance across\nmultiple benchmarks.", "AI": {"tldr": "A pruning framework for LLMs called Olica is proposed, which eliminates the need for retraining by using PCA and linear calibration methods to maintain accuracy while compressing models efficiently.", "motivation": "Existing structured pruning methods for LLMs require extensive resources for retraining, making them costly and impractical.", "method": "The Olica framework utilizes principal component analysis (PCA) on matrix products in multi-head attention layers to compress LLMs without retraining and introduces a linear calibration method for feed-forward network layers to correct errors.", "result": "Olica significantly reduces the complexity of PCA and mitigates error accumulation in pruned layers, leading to superior performance across multiple benchmarks without requiring retraining.", "conclusion": "Olica is efficient in terms of data usage, GPU memory, and running time, while achieving competitive performance.", "key_contributions": ["Introduction of the Olica framework for LLM pruning without retraining.", "Reduction of PCA complexity in multi-head attention.", "Development of a linear calibration method for feed-forward network error correction."], "limitations": "", "keywords": ["Large Language Models", "pruning", "machine learning", "PCA", "linear calibration"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.07667", "pdf": "https://arxiv.org/pdf/2506.07667.pdf", "abs": "https://arxiv.org/abs/2506.07667", "title": "Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech on Twitch", "authors": ["Prarabdh Shukla", "Wei Yin Chong", "Yash Patel", "Brennan Schaffner", "Danish Pruthi", "Arjun Bhagoji"], "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": "Accepted to ACL 2025 (main) conference", "summary": "To meet the demands of content moderation, online platforms have resorted to\nautomated systems. Newer forms of real-time engagement($\\textit{e.g.}$, users\ncommenting on live streams) on platforms like Twitch exert additional pressures\non the latency expected of such moderation systems. Despite their prevalence,\nrelatively little is known about the effectiveness of these systems. In this\npaper, we conduct an audit of Twitch's automated moderation tool\n($\\texttt{AutoMod}$) to investigate its effectiveness in flagging hateful\ncontent. For our audit, we create streaming accounts to act as siloed test\nbeds, and interface with the live chat using Twitch's APIs to send over\n$107,000$ comments collated from $4$ datasets. We measure $\\texttt{AutoMod}$'s\naccuracy in flagging blatantly hateful content containing misogyny, racism,\nableism and homophobia. Our experiments reveal that a large fraction of hateful\nmessages, up to $94\\%$ on some datasets, $\\textit{bypass moderation}$.\nContextual addition of slurs to these messages results in $100\\%$ removal,\nrevealing $\\texttt{AutoMod}$'s reliance on slurs as a moderation signal. We\nalso find that contrary to Twitch's community guidelines, $\\texttt{AutoMod}$\nblocks up to $89.5\\%$ of benign examples that use sensitive words in\npedagogical or empowering contexts. Overall, our audit points to large gaps in\n$\\texttt{AutoMod}$'s capabilities and underscores the importance for such\nsystems to understand context effectively.", "AI": {"tldr": "This paper audits Twitch's automated moderation tool, AutoMod, highlighting its ineffectiveness in flagging hateful content while blocking benign messages, showing significant context comprehension issues.", "motivation": "To investigate the effectiveness of Twitch's automated moderation tool, AutoMod, in the context of real-time engagement and content moderation challenges.", "method": "Conducted an audit using test accounts, sending over 107,000 comments from 4 datasets via Twitch's APIs to evaluate AutoMod's performance against hateful content.", "result": "Found that up to 94% of hateful messages bypass moderation; AutoMod relies heavily on slurs, with context-sensitive phrases blocking 89.5% of benign examples.", "conclusion": "There are significant deficiencies in AutoMod's capabilities, particularly in understanding context, highlighting the need for improvements in automated moderation systems.", "key_contributions": ["Conducted a comprehensive audit of AutoMod's effectiveness.", "Revealed high rates of hateful content bypassing moderation.", "Demonstrated AutoMod's failure to understand context in content moderation."], "limitations": "Focuses solely on Twitch's AutoMod and may not generalize to other platforms’ moderation systems.", "keywords": ["Twitch", "content moderation", "AutoMod", "auditing", "hateful content"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.08477", "pdf": "https://arxiv.org/pdf/2506.08477.pdf", "abs": "https://arxiv.org/abs/2506.08477", "title": "Detecting Harmful Memes with Decoupled Understanding and Guided CoT Reasoning", "authors": ["Fengjun Pan", "Anh Tuan Luu", "Xiaobao Wu"], "categories": ["cs.CL"], "comment": null, "summary": "Detecting harmful memes is essential for maintaining the integrity of online\nenvironments. However, current approaches often struggle with resource\nefficiency, flexibility, or explainability, limiting their practical deployment\nin content moderation systems. To address these challenges, we introduce\nU-CoT+, a novel framework for harmful meme detection. Instead of relying solely\non prompting or fine-tuning multimodal models, we first develop a high-fidelity\nmeme-to-text pipeline that converts visual memes into detail-preserving textual\ndescriptions. This design decouples meme interpretation from meme\nclassification, thus avoiding immediate reasoning over complex raw visual\ncontent and enabling resource-efficient harmful meme detection with general\nlarge language models (LLMs). Building on these textual descriptions, we\nfurther incorporate targeted, interpretable human-crafted guidelines to guide\nmodels' reasoning under zero-shot CoT prompting. As such, this framework allows\nfor easy adaptation to different harmfulness detection criteria across\nplatforms, regions, and over time, offering high flexibility and\nexplainability. Extensive experiments on seven benchmark datasets validate the\neffectiveness of our framework, highlighting its potential for explainable and\nlow-resource harmful meme detection using small-scale LLMs. Codes and data are\navailable at: https://anonymous.4open.science/r/HMC-AF2B/README.md.", "AI": {"tldr": "U-CoT+ is a novel framework for harmful meme detection that uses a meme-to-text pipeline for efficient and explainable classification with LLMs.", "motivation": "The paper addresses the challenges of resource efficiency, flexibility, and explainability in current meme detection approaches that hinder their deployment in content moderation.", "method": "The framework features a high-fidelity meme-to-text pipeline that converts memes into textual descriptions, combined with human-crafted guidelines for zero-shot reasoning.", "result": "Extensive experiments on seven benchmark datasets demonstrate the framework's effectiveness in harmful meme detection using small-scale LLMs with high explainability and low resource requirements.", "conclusion": "U-CoT+ allows for adaptation to various harmfulness detection criteria, improving the deployment of meme detection systems across different contexts.", "key_contributions": ["Development of the high-fidelity meme-to-text pipeline", "Decoupling meme interpretation from classification", "Incorporation of human-crafted guidelines for effective reasoning"], "limitations": "", "keywords": ["harmful meme detection", "U-CoT+", "framework", "explainability", "large language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.08479", "pdf": "https://arxiv.org/pdf/2506.08479.pdf", "abs": "https://arxiv.org/abs/2506.08479", "title": "Efficient Context Selection for Long-Context QA: No Tuning, No Iteration, Just Adaptive-$k$", "authors": ["Chihiro Taguchi", "Seiji Maekawa", "Nikita Bhutani"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "26 pages, 16 tables, 5 figures", "summary": "Retrieval-augmented generation (RAG) and long-context language models (LCLMs)\nboth address context limitations of LLMs in open-domain question answering\n(QA). However, optimal external context to retrieve remains an open problem:\nfixing the retrieval size risks either wasting tokens or omitting key evidence.\nExisting adaptive methods like Self-RAG and Self-Route rely on iterative LLM\nprompting and perform well on factoid QA, but struggle with aggregation QA,\nwhere the optimal context size is both unknown and variable. We present\nAdaptive-$k$ retrieval, a simple and effective single-pass method that\nadaptively selects the number of passages based on the distribution of the\nsimilarity scores between the query and the candidate passages. It does not\nrequire model fine-tuning, extra LLM inferences or changes to existing\nretriever-reader pipelines. On both factoid and aggregation QA benchmarks,\nAdaptive-$k$ matches or outperforms fixed-$k$ baselines while using up to 10x\nfewer tokens than full-context input, yet still retrieves 70% of relevant\npassages. It improves accuracy across five LCLMs and two embedding models,\nhighlighting that dynamically adjusting context size leads to more efficient\nand accurate QA.", "AI": {"tldr": "The paper introduces Adaptive-$k$ retrieval, a method that adaptively selects the number of passages to retrieve based on similarity scores, improving efficiency and accuracy in open-domain question answering (QA).", "motivation": "To address the limitations of fixed retrieval sizes in open-domain QA, which can either waste tokens or miss key evidence.", "method": "The Adaptive-$k$ retrieval method selects the number of passages based on the distribution of similarity scores between the query and candidate passages without requiring model fine-tuning or additional LLM inferences.", "result": "On factoid and aggregation QA benchmarks, Adaptive-$k$ matches or outperforms fixed-$k$ baselines while using up to 10x fewer tokens and still retrieving 70% of relevant passages.", "conclusion": "Dynamically adjusting the context size using Adaptive-$k$ improves QA accuracy across multiple long-context language models and embedding models.", "key_contributions": ["Introduction of Adaptive-$k$ retrieval method", "Significant token efficiency compared to fixed-$k$", "Improved accuracy in QA tasks without requiring model retraining"], "limitations": "", "keywords": ["Retrieval-augmented generation", "Long-context language models", "Open-domain question answering"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.08480", "pdf": "https://arxiv.org/pdf/2506.08480.pdf", "abs": "https://arxiv.org/abs/2506.08480", "title": "Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models", "authors": ["Huixuan Zhang", "Xiaojun Wan"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Text-to-image models often struggle to generate images that precisely match\ntextual prompts. Prior research has extensively studied the evaluation of\nimage-text alignment in text-to-image generation. However, existing evaluations\nprimarily focus on agreement with human assessments, neglecting other critical\nproperties of a trustworthy evaluation framework. In this work, we first\nidentify two key aspects that a reliable evaluation should address. We then\nempirically demonstrate that current mainstream evaluation frameworks fail to\nfully satisfy these properties across a diverse range of metrics and models.\nFinally, we propose recommendations for improving image-text alignment\nevaluation.", "AI": {"tldr": "This paper critiques existing evaluation methods for text-to-image models and proposes recommendations for improving them.", "motivation": "To address the shortcomings in current evaluation frameworks for image-text alignment in text-to-image generation.", "method": "The authors identify critical properties for reliable evaluation and assess mainstream frameworks against these properties using various metrics and models.", "result": "It is demonstrated that existing evaluations fail to fully satisfy the proposed properties, highlighting the need for improvements.", "conclusion": "New recommendations for enhancing image-text alignment evaluations are provided based on empirical findings.", "key_contributions": ["Identification of key aspects for reliable evaluation frameworks", "Empirical assessment of current evaluation methods", "Recommendations for improving evaluation of image-text alignment"], "limitations": "The paper does not propose new metrics but rather focuses on improving existing evaluation frameworks.", "keywords": ["text-to-image generation", "evaluation frameworks", "image-text alignment"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.08487", "pdf": "https://arxiv.org/pdf/2506.08487.pdf", "abs": "https://arxiv.org/abs/2506.08487", "title": "Fairness is Not Silence: Unmasking Vacuous Neutrality in Small Language Models", "authors": ["Sumanth Manduru", "Carlotta Domeniconi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid adoption of Small Language Models (SLMs) for on-device and\nresource-constrained deployments has outpaced our understanding of their\nethical risks. To the best of our knowledge, we present the first large-scale\naudit of instruction-tuned SLMs spanning 0.5 to 5 billion parameters-an\noverlooked \"middle tier\" between BERT-class encoders and flagship LLMs. Our\nevaluation includes nine open-source models from the Qwen 2.5, LLaMA 3.2, Gemma\n3, and Phi families. Using the BBQ benchmark under zero-shot prompting, we\nanalyze both utility and fairness across ambiguous and disambiguated contexts.\nThis evaluation reveals three key insights. First, competence and fairness need\nnot be antagonistic: Phi models achieve F1 scores exceeding 90 percent while\nexhibiting minimal bias, showing that efficient and ethical NLP is attainable.\nSecond, social bias varies significantly by architecture: Qwen 2.5 models may\nappear fair, but this often reflects vacuous neutrality, random guessing, or\nevasive behavior rather than genuine ethical alignment. In contrast, LLaMA 3.2\nmodels exhibit stronger stereotypical bias, suggesting overconfidence rather\nthan neutrality. Third, compression introduces nuanced trade-offs: 4-bit AWQ\nquantization improves F1 scores in ambiguous settings for LLaMA 3.2-3B but\nincreases disability-related bias in Phi-4-Mini by over 7 percentage points.\nThese insights provide practical guidance for the responsible deployment of\nSLMs in applications demanding fairness and efficiency, particularly benefiting\nsmall enterprises and resource-constrained environments.", "AI": {"tldr": "The paper audits the ethical risks and performance of Small Language Models (SLMs) with 0.5 to 5 billion parameters, revealing critical insights on utility, fairness, and the impact of model architecture and quantization on bias.", "motivation": "To understand the ethical risks associated with the rapid adoption of Small Language Models (SLMs) for resource-constrained deployments.", "method": "A large-scale audit was conducted on nine instruction-tuned SLMs from various families, using the BBQ benchmark to evaluate utility and fairness through zero-shot prompting.", "result": "The evaluation found that Phi models show high F1 scores with minimal bias, while Qwen 2.5 models exhibit vacuous neutrality, and LLaMA 3.2 shows significant stereotypical bias.", "conclusion": "The study offers insights that advocate for the responsible deployment of SLMs, emphasizing the balance between efficiency and fairness, especially for small enterprises.", "key_contributions": ["First large-scale audit of instruction-tuned SLMs", "Identification of trade-offs in utility, fairness, and bias by architecture", "Practical guidance for deploying SLMs ethically"], "limitations": "Limited to analysis of specific model families and their architectures; does not cover all possible SLMs and configurations.", "keywords": ["Small Language Models", "ethical risks", "fairness", "utility", "bias"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.08488", "pdf": "https://arxiv.org/pdf/2506.08488.pdf", "abs": "https://arxiv.org/abs/2506.08488", "title": "EtiCor++: Towards Understanding Etiquettical Bias in LLMs", "authors": ["Ashutosh Dwivedi", "Siddhant Shivdutt Singh", "Ashutosh Modi"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Accepted at ACL Findings 2025, 22 pages (9 pages main content + 4\n  pages references + 9 pages appendix)", "summary": "In recent years, researchers have started analyzing the cultural sensitivity\nof LLMs. In this respect, Etiquettes have been an active area of research.\nEtiquettes are region-specific and are an essential part of the culture of a\nregion; hence, it is imperative to make LLMs sensitive to etiquettes. However,\nthere needs to be more resources in evaluating LLMs for their understanding and\nbias with regard to etiquettes. In this resource paper, we introduce EtiCor++,\na corpus of etiquettes worldwide. We introduce different tasks for evaluating\nLLMs for knowledge about etiquettes across various regions. Further, we\nintroduce various metrics for measuring bias in LLMs. Extensive experimentation\nwith LLMs shows inherent bias towards certain regions.", "AI": {"tldr": "This paper presents EtiCor++, a corpus created to evaluate large language models (LLMs) on their understanding of cultural etiquettes and associated biases across various regions.", "motivation": "There is a lack of resources for evaluating LLMs regarding their comprehension and bias related to cultural etiquettes, which are crucial for accurate human-computer interactions.", "method": "The paper introduces EtiCor++, a worldwide corpus of etiquettes, and outlines tasks and metrics for assessing LLMs' knowledge and biases in this domain.", "result": "Extensive experiments reveal a significant inherent bias in LLMs toward certain regions' etiquettes, indicating the need for improved sensitivity in AI models.", "conclusion": "The findings emphasize the importance of culturally sensitive AI and provide a foundation for future research and evaluation of LLM biases related to etiquettes.", "key_contributions": ["Introduction of EtiCor++, a new corpus for evaluating LLMs on cultural etiquettes.", "Development of tasks designed to assess etiquettes understanding in LLMs.", "Introduction of metrics to measure bias in LLMs regarding cultural backgrounds."], "limitations": "The study is primarily resource-focused and does not propose comprehensive solutions for identified biases in LLMs.", "keywords": ["cultural sensitivity", "LLMs", "etiquettes", "bias measurement", "evaluation tasks"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.08490", "pdf": "https://arxiv.org/pdf/2506.08490.pdf", "abs": "https://arxiv.org/abs/2506.08490", "title": "Integration of Old and New Knowledge for Generalized Intent Discovery: A Consistency-driven Prototype-Prompting Framework", "authors": ["Xiao Wei", "Xiaobao Wang", "Ning Zhuang", "Chenyang Wang", "Longbiao Wang", "Jianwu dang"], "categories": ["cs.CL"], "comment": "9 pages, 2 figures, 7 tables, IJCAI 2025", "summary": "Intent detection aims to identify user intents from natural language inputs,\nwhere supervised methods rely heavily on labeled in-domain (IND) data and\nstruggle with out-of-domain (OOD) intents, limiting their practical\napplicability. Generalized Intent Discovery (GID) addresses this by leveraging\nunlabeled OOD data to discover new intents without additional annotation.\nHowever, existing methods focus solely on clustering unsupervised data while\nneglecting domain adaptation. Therefore, we propose a consistency-driven\nprototype-prompting framework for GID from the perspective of integrating old\nand new knowledge, which includes a prototype-prompting framework for\ntransferring old knowledge from external sources, and a hierarchical\nconsistency constraint for learning new knowledge from target domains. We\nconducted extensive experiments and the results show that our method\nsignificantly outperforms all baseline methods, achieving state-of-the-art\nresults, which strongly demonstrates the effectiveness and generalization of\nour methods. Our source code is publicly available at\nhttps://github.com/smileix/cpp.", "AI": {"tldr": "This paper introduces a consistency-driven prototype-prompting framework for Generalized Intent Discovery (GID) that effectively integrates old and new knowledge, improving intent detection from natural language inputs, especially for out-of-domain intents.", "motivation": "The motivation behind this research is to enhance intent detection capability, particularly under conditions where supervised methods are hampered by a lack of labeled in-domain data and struggle with out-of-domain intents.", "method": "The proposed approach utilizes a prototype-prompting framework to leverage existing knowledge from external sources and employs a hierarchical consistency constraint to learn new knowledge from target domains.", "result": "The experiments conducted show that the proposed method significantly outperforms existing baseline methods in intent detection, achieving state-of-the-art results.", "conclusion": "The findings of the study strongly validate the effectiveness and generalization of the proposed framework in GID, lay the groundwork for future improvements in intent detection methodologies.", "key_contributions": ["Consistency-driven prototype-prompting framework for GID", "Integration of old knowledge through prototype prompting", "Hierarchical consistency constraints for new knowledge learning"], "limitations": "", "keywords": ["intent detection", "Generalized Intent Discovery", "domain adaptation", "machine learning", "natural language processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.08500", "pdf": "https://arxiv.org/pdf/2506.08500.pdf", "abs": "https://arxiv.org/abs/2506.08500", "title": "DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in Search-Augmented LLMs", "authors": ["Arie Cattan", "Alon Jacovi", "Ori Ram", "Jonathan Herzig", "Roee Aharoni", "Sasha Goldshtein", "Eran Ofek", "Idan Szpektor", "Avi Caciularu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Retrieval Augmented Generation (RAG) is a commonly used approach for\nenhancing large language models (LLMs) with relevant and up-to-date\ninformation. However, the retrieved sources can often contain conflicting\ninformation and it remains unclear how models should address such\ndiscrepancies. In this work, we first propose a novel taxonomy of knowledge\nconflict types in RAG, along with the desired model behavior for each type. We\nthen introduce CONFLICTS, a high-quality benchmark with expert annotations of\nconflict types in a realistic RAG setting. CONFLICTS is the first benchmark\nthat enables tracking progress on how models address a wide range of knowledge\nconflicts. We conduct extensive experiments on this benchmark, showing that\nLLMs often struggle to appropriately resolve conflicts between sources. While\nprompting LLMs to explicitly reason about the potential conflict in the\nretrieved documents significantly improves the quality and appropriateness of\ntheir responses, substantial room for improvement in future research remains.", "AI": {"tldr": "This paper introduces a taxonomy of knowledge conflict types in Retrieval Augmented Generation (RAG) and presents a benchmark called CONFLICTS to evaluate how LLMs handle these conflicts.", "motivation": "To address the issue of conflicts in information retrieved by large language models (LLMs) and improve their response quality.", "method": "The authors propose a novel taxonomy of knowledge conflict types specific to RAG and create CONFLICTS, a benchmark with expert annotations for evaluating model responses to these conflicts.", "result": "Experiments demonstrate that LLMs frequently fail to resolve conflicts effectively, although prompting them to consider potential conflicts can enhance response quality.", "conclusion": "There is significant potential for future research to advance how LLMs manage conflicting information from different sources in RAG settings.", "key_contributions": ["Introduction of a novel taxonomy of knowledge conflict types in RAG.", "Development of the CONFLICTS benchmark for evaluating conflict resolution in LLMs.", "Analysis of LLM performance in resolving information conflicts and strategies for improvement."], "limitations": "The study highlights that while improvements in response quality were observed, LLMs still face substantial challenges in consistently resolving knowledge conflicts.", "keywords": ["Retrieval Augmented Generation", "knowledge conflict", "large language models", "benchmark", "conflict resolution"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.08504", "pdf": "https://arxiv.org/pdf/2506.08504.pdf", "abs": "https://arxiv.org/abs/2506.08504", "title": "CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations", "authors": ["Divyaksh Shukla", "Ritesh Baviskar", "Dwijesh Gohil", "Aniket Tiwari", "Atul Shree", "Ashutosh Modi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ACL Findings 2025 (16 pages: 5 pages main content + 3\n  pages references + 8 pages appendix)", "summary": "Discourse parsing is an important task useful for NLU applications such as\nsummarization, machine comprehension, and emotion recognition. The current\ndiscourse parsing datasets based on conversations consists of written English\ndialogues restricted to a single domain. In this resource paper, we introduce\nCoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in\nconversations. The corpus (code-mixed in Hindi and English) has both audio and\ntranscribed text and is annotated with nine discourse relations. We experiment\nwith various SoTA baseline models; the poor performance of SoTA models\nhighlights the challenges of multi-domain code-mixed corpus, pointing towards\nthe need for developing better models for such realistic settings.", "AI": {"tldr": "This paper introduces CoMuMDR, a code-mixed multi-modal corpus for discourse parsing in conversations, highlighting challenges faced by state-of-the-art models.", "motivation": "Current discourse parsing datasets are limited to single-domain, written English dialogues, necessitating a new resource that encompasses multi-domain and code-mixed conversations.", "method": "The paper presents the CoMuMDR corpus, which contains audio and transcribed text in Hindi and English, annotated with nine discourse relations, and evaluates various state-of-the-art baseline models on it.", "result": "State-of-the-art models show poor performance on the CoMuMDR corpus, underscoring the challenges posed by its multi-domain and code-mixed nature.", "conclusion": "There is a critical need for the development of better models tailored for discourse parsing in realistic, code-mixed, multi-domain settings.", "key_contributions": ["Introduction of CoMuMDR corpus for discourse parsing", "Inclusion of audio and transcribed text in multiple languages", "Highlighting limitations of existing state-of-the-art models in multi-domain settings."], "limitations": "Existing SoTA models struggle with performance, indicating a gap in addressing the complexities of code-mixed discourse parsing.", "keywords": ["discourse parsing", "code-mixed corpus", "multi-domain", "human-computer interaction", "NLU"], "importance_score": 8, "read_time_minutes": 16}}
{"id": "2506.08552", "pdf": "https://arxiv.org/pdf/2506.08552.pdf", "abs": "https://arxiv.org/abs/2506.08552", "title": "Efficient Post-Training Refinement of Latent Reasoning in Large Language Models", "authors": ["Xinyuan Wang", "Dongjie Wang", "Wangyang Ying", "Haoyue Bai", "Nanxu Gong", "Sixun Dong", "Kunpeng Liu", "Yanjie Fu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reasoning is a key component of language understanding in Large Language\nModels. While Chain-of-Thought prompting enhances performance via explicit\nintermediate steps, it suffers from sufficient token overhead and a fixed\nreasoning trajectory, preventing step-wise refinement. Recent advances in\nlatent reasoning address these limitations by refining internal reasoning\nprocesses directly in the model's latent space, without producing explicit\noutputs. However, a key challenge remains: how to effectively update reasoning\nembeddings during post-training to guide the model toward more accurate\nsolutions. To overcome this challenge, we propose a lightweight post-training\nframework that refines latent reasoning trajectories using two novel\nstrategies: 1) Contrastive reasoning feedback, which compares reasoning\nembeddings against strong and weak baselines to infer effective update\ndirections via embedding enhancement; 2) Residual embedding refinement, which\nstabilizes updates by progressively integrating current and historical\ngradients, enabling fast yet controlled convergence. Extensive experiments and\ncase studies are conducted on five reasoning benchmarks to demonstrate the\neffectiveness of the proposed framework. Notably, a 5\\% accuracy gain on MathQA\nwithout additional training.", "AI": {"tldr": "The paper proposes a framework for refining latent reasoning in Large Language Models (LLMs) during post-training, addressing challenges with updating reasoning embeddings effectively.", "motivation": "To improve the reasoning capabilities of Language Models beyond traditional Chain-of-Thought prompting, which is limited by token overhead and fixed reasoning paths.", "method": "A lightweight post-training framework that incorporates contrastive reasoning feedback and residual embedding refinement to enhance reasoning embeddings and ensure stable convergence.", "result": "The proposed framework yields a 5% accuracy improvement on the MathQA benchmark without requiring additional training.", "conclusion": "The results demonstrate that the framework successfully refines reasoning trajectories, leading to more accurate problem-solving in LLMs.", "key_contributions": ["Development of a post-training framework for LLMs", "Introduction of contrastive reasoning feedback", "Implementation of residual embedding refinement"], "limitations": "The extent of improvement may vary across different tasks and reasoning benchmarks.", "keywords": ["Large Language Models", "reasoning", "post-training", "embedding refinement", "contrastive learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.08564", "pdf": "https://arxiv.org/pdf/2506.08564.pdf", "abs": "https://arxiv.org/abs/2506.08564", "title": "Neighbors and relatives: How do speech embeddings reflect linguistic connections across the world?", "authors": ["Tuukka Törö", "Antti Suni", "Juraj Šimko"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "27 pages, 11 figures (+5 supplementary), submitted to PLOS One", "summary": "Investigating linguistic relationships on a global scale requires analyzing\ndiverse features such as syntax, phonology and prosody, which evolve at varying\nrates influenced by internal diversification, language contact, and\nsociolinguistic factors. Recent advances in machine learning (ML) offer\ncomplementary alternatives to traditional historical and typological\napproaches. Instead of relying on expert labor in analyzing specific linguistic\nfeatures, these new methods enable the exploration of linguistic variation\nthrough embeddings derived directly from speech, opening new avenues for\nlarge-scale, data-driven analyses.\n  This study employs embeddings from the fine-tuned XLS-R self-supervised\nlanguage identification model voxlingua107-xls-r-300m-wav2vec, to analyze\nrelationships between 106 world languages based on speech recordings. Using\nlinear discriminant analysis (LDA), language embeddings are clustered and\ncompared with genealogical, lexical, and geographical distances. The results\ndemonstrate that embedding-based distances align closely with traditional\nmeasures, effectively capturing both global and local typological patterns.\nChallenges in visualizing relationships, particularly with hierarchical\nclustering and network-based methods, highlight the dynamic nature of language\nchange.\n  The findings show potential for scalable analyses of language variation based\non speech embeddings, providing new perspectives on relationships among\nlanguages. By addressing methodological considerations such as corpus size and\nlatent space dimensionality, this approach opens avenues for studying\nlow-resource languages and bridging macro- and micro-level linguistic\nvariation. Future work aims to extend these methods to underrepresented\nlanguages and integrate sociolinguistic variation for a more comprehensive\nunderstanding of linguistic diversity.", "AI": {"tldr": "This study utilizes machine learning methods, specifically speech embeddings, to analyze linguistic relationships among 106 languages, showing potential for scalable approaches in language variation studies.", "motivation": "To investigate linguistic relationships globally using diverse features influenced by sociolinguistic factors, leveraging recent machine learning advancements.", "method": "Embeddings from the XLS-R self-supervised language model were used, employing linear discriminant analysis (LDA) to cluster and compare language embeddings with traditional linguistic measures.", "result": "The embedding-based distances align closely with traditional genealogical, lexical, and geographical measures, capturing typological patterns effectively.", "conclusion": "The study opens new avenues for analyzing low-resource languages and integrating sociolinguistic variation, enhancing our understanding of linguistic diversity.", "key_contributions": ["Introduction of speech embeddings for linguistic analysis.", "Alignment of ML-derived distances with traditional linguistic measures.", "Addressing methodological considerations for studying low-resource languages."], "limitations": "Challenges in visualizing relationships, particularly with hierarchical clustering and network-based methods.", "keywords": ["machine learning", "linguistic relationships", "speech embeddings", "language variation", "sociolinguistics"], "importance_score": 3, "read_time_minutes": 20}}
{"id": "2506.08584", "pdf": "https://arxiv.org/pdf/2506.08584.pdf", "abs": "https://arxiv.org/abs/2506.08584", "title": "CounselBench: A Large-Scale Expert Evaluation and Adversarial Benchmark of Large Language Models in Mental Health Counseling", "authors": ["Yahan Li", "Jifan Yao", "John Bosco S. Bunyi", "Adam C. Frank", "Angel Hwang", "Ruishan Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly proposed for use in mental\nhealth support, yet their behavior in realistic counseling scenarios remains\nlargely untested. We introduce CounselBench, a large-scale benchmark developed\nwith 100 mental health professionals to evaluate and stress-test LLMs in\nsingle-turn counseling. The first component, CounselBench-EVAL, contains 2,000\nexpert evaluations of responses from GPT-4, LLaMA 3, Gemini, and online human\ntherapists to real patient questions. Each response is rated along six\nclinically grounded dimensions, with written rationales and span-level\nannotations. We find that LLMs often outperform online human therapists in\nperceived quality, but experts frequently flag their outputs for safety\nconcerns such as unauthorized medical advice. Follow-up experiments show that\nLLM judges consistently overrate model responses and overlook safety issues\nidentified by human experts. To probe failure modes more directly, we construct\nCounselBench-Adv, an adversarial dataset of 120 expert-authored counseling\nquestions designed to trigger specific model issues. Evaluation across 2,880\nresponses from eight LLMs reveals consistent, model-specific failure patterns.\nTogether, CounselBench establishes a clinically grounded framework for\nbenchmarking and improving LLM behavior in high-stakes mental health settings.", "AI": {"tldr": "CounselBench is a benchmark for evaluating large language models in mental health counseling, revealing both efficacy and safety concerns.", "motivation": "To assess the performance of LLMs in real-world counseling scenarios and improve their safety in mental health applications.", "method": "Development of CounselBench, comprising two components: CounselBench-EVAL for expert evaluations and CounselBench-Adv for adversarial questioning, involving real patient questions and expert ratings.", "result": "LLMs often outperform online human therapists in perceived quality but are flagged for safety issues, highlighting a gap in model evaluation regarding unauthorized medical advice and oversight.", "conclusion": "CounselBench provides a structured approach to enhancing LLM safety and effectiveness in mental health contexts, identifying specific failure patterns.", "key_contributions": ["Creation of a large-scale LLM benchmarking framework for mental health counseling", "Expert evaluation of LLM responses against human therapists", "Identification of model-specific failure patterns through adversarial testing"], "limitations": "The study indicates that LLM judges may overrate model responses and overlook safety issues identified by human experts.", "keywords": ["large language models", "mental health", "benchmarking", "counseling", "adversarial testing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.08592", "pdf": "https://arxiv.org/pdf/2506.08592.pdf", "abs": "https://arxiv.org/abs/2506.08592", "title": "Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings", "authors": ["Liyan Xu", "Zhenlin Su", "Mo Yu", "Jiangnan Li", "Fandong Meng", "Jie Zhou"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This work focuses on an observed limitation of text encoders: embeddings may\nnot be able to recognize fine-grained entities or events within the semantics,\nresulting in failed dense retrieval on even simple cases. To examine such\nbehaviors, we first introduce a new evaluation dataset in Chinese, named\nCapRetrieval, whose passages are image captions, and queries are phrases\ninquiring entities or events in various forms. Zero-shot evaluation suggests\nthat encoders may fail on these fine-grained matching, regardless of training\nsources or model sizes. Aiming for enhancement, we proceed to finetune encoders\nwith our proposed data generation strategies, which obtains the best\nperformance on CapRetrieval. Within this process, we further identify an issue\nof granularity dilemma, a challenge for embeddings to express fine-grained\nsalience while aligning with overall semantics. Our dataset, code and models in\nthis work are publicly released at https://github.com/lxucs/CapRetrieval.", "AI": {"tldr": "The paper addresses limitations in text encoders for fine-grained entity recognition, proposing a new dataset for evaluation and enhancing encoder performance through data generation strategies.", "motivation": "The study investigates the inability of text encoders to effectively recognize fine-grained entities/events, which hampers dense retrieval systems.", "method": "Introduces the CapRetrieval dataset for evaluating encoder performance on fine-grained matching and proposes data generation strategies to finetune the encoders.", "result": "Finetuning encoders with the proposed strategies yields the best performance on the CapRetrieval dataset, revealing a granularity dilemma in embedding performance.", "conclusion": "The work highlights the challenges in fine-grained semantic alignment for embeddings and offers a publicly available dataset and models to further research in this area.", "key_contributions": ["Introduction of the CapRetrieval dataset for fine-grained evaluation", "Data generation strategies for finetuning text encoders", "Identification of the granularity dilemma in embeddings"], "limitations": "", "keywords": ["text encoders", "fine-grained entities", "dense retrieval"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.08593", "pdf": "https://arxiv.org/pdf/2506.08593.pdf", "abs": "https://arxiv.org/abs/2506.08593", "title": "Hateful Person or Hateful Model? Investigating the Role of Personas in Hate Speech Detection by Large Language Models", "authors": ["Shuzhou Yuan", "Ercong Nie", "Mario Tawfelis", "Helmut Schmid", "Hinrich Schütze", "Michael Färber"], "categories": ["cs.CL"], "comment": null, "summary": "Hate speech detection is a socially sensitive and inherently subjective task,\nwith judgments often varying based on personal traits. While prior work has\nexamined how socio-demographic factors influence annotation, the impact of\npersonality traits on Large Language Models (LLMs) remains largely unexplored.\nIn this paper, we present the first comprehensive study on the role of persona\nprompts in hate speech classification, focusing on MBTI-based traits. A human\nannotation survey confirms that MBTI dimensions significantly affect labeling\nbehavior. Extending this to LLMs, we prompt four open-source models with MBTI\npersonas and evaluate their outputs across three hate speech datasets. Our\nanalysis uncovers substantial persona-driven variation, including\ninconsistencies with ground truth, inter-persona disagreement, and logit-level\nbiases. These findings highlight the need to carefully define persona prompts\nin LLM-based annotation workflows, with implications for fairness and alignment\nwith human values.", "AI": {"tldr": "This paper investigates how personality traits, particularly MBTI dimensions, influence hate speech detection in LLMs and identifies significant persona-driven variation in model outputs.", "motivation": "To understand the impact of personality traits on LLM performance in hate speech detection, an area that has been largely unexplored despite the subjective nature of the task.", "method": "The study uses MBTI-based persona prompts to evaluate outputs from four open-source LLMs across three hate speech datasets, supplemented by a human annotation survey to confirm the influence of personality traits.", "result": "The analysis shows that MBTI dimensions significantly affect labeling behavior and model outputs, revealing inconsistencies with ground truth and biases related to different personas.", "conclusion": "The results underscore the importance of carefully defining persona prompts in LLM-based workflows to ensure fairness and alignment with human values in hate speech classification.", "key_contributions": ["First comprehensive study on personality traits' impact on LLMs in hate speech detection", "Identification of significant persona-driven variation in model outputs", "Recommendations for improving LLM-based annotation workflows for fairness."], "limitations": "The study may not cover all personality dimensions beyond MBTI, and the findings are specific to the hate speech task.", "keywords": ["Hate Speech Detection", "Large Language Models", "MBTI", "Persona Prompts", "Fairness"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.08625", "pdf": "https://arxiv.org/pdf/2506.08625.pdf", "abs": "https://arxiv.org/abs/2506.08625", "title": "RAISE: Enhancing Scientific Reasoning in LLMs via Step-by-Step Retrieval", "authors": ["Minhae Oh", "Jeonghye Kim", "Nakyung Lee", "Donggeon Seo", "Taeuk Kim", "Jungwoo Lee"], "categories": ["cs.CL"], "comment": null, "summary": "Scientific reasoning requires not only long-chain reasoning processes, but\nalso knowledge of domain-specific terminologies and adaptation to updated\nfindings. To deal with these challenges for scientific reasoning, we introduce\nRAISE, a step-by-step retrieval-augmented framework which retrieves logically\nrelevant documents from in-the-wild corpus. RAISE is divided into three steps:\nproblem decomposition, logical query generation, and logical retrieval. We\nobserve that RAISE consistently outperforms other baselines on scientific\nreasoning benchmarks. We analyze that unlike other baselines, RAISE retrieves\ndocuments that are not only similar in terms of the domain knowledge, but also\ndocuments logically more relevant.", "AI": {"tldr": "RAISE is a retrieval-augmented framework designed for effective scientific reasoning, enhancing the retrieval of domain-specific, logically relevant documents.", "motivation": "To tackle the challenges of scientific reasoning which include handling long-chain reasoning processes and adapting to new findings.", "method": "RAISE operates in three steps: problem decomposition, logical query generation, and logical retrieval.", "result": "RAISE consistently outperforms other baselines on scientific reasoning benchmarks by retrieving documents that are not only similar in domain knowledge but also logically relevant.", "conclusion": "RAISE improves the process of scientific reasoning by effectively integrating retrieval techniques that focus on logical relevance.", "key_contributions": ["Introduction of a three-step framework for scientific reasoning", "Demonstration of improved performance on reasoning benchmarks", "Ability to retrieve logically relevant documents in addition to domain similarity"], "limitations": "", "keywords": ["scientific reasoning", "retrieval-augmented framework", "logical relevance"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2506.08643", "pdf": "https://arxiv.org/pdf/2506.08643.pdf", "abs": "https://arxiv.org/abs/2506.08643", "title": "MEMETRON: Metaheuristic Mechanisms for Test-time Response Optimization of Large Language Models", "authors": ["Son The Nguyen", "Theja Tulabandhula"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly used for both open-ended and\nstructured tasks, yet their inference-time behavior is still largely dictated\nby heuristic decoding strategies such as greedy search, sampling, or reranking.\nThese methods provide limited control and do not explicitly optimize for\ntask-specific objectives. We introduce MEMETRON, a task-agnostic framework that\nformulates LLM decoding as a discrete black-box optimization problem. MEMETRON\nleverages hybrid metaheuristic algorithms, GENETRON and ANNETRON, to search the\nresponse space, guided by reward models and contextual operations performed by\nthe LLM itself. This approach enables efficient discovery of high-reward\nresponses without requiring model retraining or gradient access. The framework\nis modular and generalizes across diverse tasks, requiring only a reward\nfunction and lightweight prompt templates. We evaluate our framework on the\ncritical human preference alignment task and demonstrate that it significantly\noutperforms standard decoding and reranking methods, highlighting its potential\nto improve alignment without model retraining.", "AI": {"tldr": "Introducing MEMETRON, a task-agnostic framework for optimizing LLM decoding using hybrid metaheuristic algorithms to improve response quality without retraining.", "motivation": "Enhance control over LLM inference-time behavior, which is currently limited by heuristic decoding strategies that lack optimization for specific tasks.", "method": "MEMETRON formulates LLM decoding as a discrete black-box optimization problem, utilizing hybrid algorithms (GENETRON and ANNETRON) informed by reward models and LLM context.", "result": "The MEMETRON framework significantly outperforms standard decoding methods in human preference alignment tasks.", "conclusion": "MEMETRON offers a modular solution for improving LLM alignment without the need for model retraining, applicable across various tasks with minimal setup.", "key_contributions": ["Introduction of a task-agnostic optimization framework for LLM decoding", "Use of metaheuristic algorithms for improved response quality", "Evaluation demonstrates superior performance in human preference alignment over existing methods."], "limitations": "", "keywords": ["Large Language Models", "Decoding Strategies", "Optimization Framework", "Metaheuristics", "Human Preference Alignment"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.08646", "pdf": "https://arxiv.org/pdf/2506.08646.pdf", "abs": "https://arxiv.org/abs/2506.08646", "title": "TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning", "authors": ["Mingyu Zheng", "Zhifan Feng", "Jia Wang", "Lanrui Wang", "Zheng Lin", "Yang Hao", "Weiping Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "27 pages, 19 figures, Findings of ACL 2025", "summary": "Despite the commendable progress of recent LLM-based data synthesis methods,\nthey face two limitations in generating table instruction tuning data. First,\nthey can not thoroughly explore the vast input space of table understanding\ntasks, leading to limited data diversity. Second, they ignore the weaknesses in\ntable understanding ability of the target LLM and blindly pursue the increase\nof data quantity, resulting in suboptimal data efficiency. In this paper, we\nintroduce a progressive and weakness-guided data synthesis framework tailored\nfor table instruction tuning, named TableDreamer, to mitigate the above issues.\nSpecifically, we first synthesize diverse tables and related instructions as\nseed data, and then perform an iterative exploration of the input space under\nthe guidance of the newly identified weakness data, which eventually serve as\nthe final training data for fine-tuning the target LLM. Extensive experiments\non 10 tabular benchmarks demonstrate the effectiveness of the proposed\nframework, which boosts the average accuracy of Llama3.1-8B-instruct by 11.62%\n(49.07% to 60.69%) with 27K GPT-4o synthetic data and outperforms\nstate-of-the-art data synthesis baselines which use more training data. The\ncode and data is available at https://github.com/SpursGoZmy/TableDreamer", "AI": {"tldr": "Introduction of TableDreamer, a data synthesis framework for improving table instruction tuning by addressing data diversity and utilization of LLM weaknesses.", "motivation": "Recent LLM-based data synthesis methods for table instruction tuning struggle with data diversity and inefficient data utilization due to existing weaknesses in LLMs.", "method": "TableDreamer synthesizes diverse tables and instructions as seed data, then iteratively refines the input space using identified weaknesses to create final training data for LLM fine-tuning.", "result": "The proposed framework improves the average accuracy of Llama3.1-8B-instruct by 11.62% on 10 tabular benchmarks, surpassing existing data synthesis methods that use larger datasets.", "conclusion": "TableDreamer effectively enhances table instruction tuning data generation, demonstrating superior performance in accuracy while utilizing a smaller training set.", "key_contributions": ["Novel data synthesis framework for table instruction tuning", "Weakness-guided iterative exploration of training data", "Demonstrated significant accuracy improvements using smaller synthetic datasets"], "limitations": "", "keywords": ["Data Synthesis", "Table Instruction Tuning", "LLM", "Machine Learning", "Human-Computer Interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.08647", "pdf": "https://arxiv.org/pdf/2506.08647.pdf", "abs": "https://arxiv.org/abs/2506.08647", "title": "Summarization for Generative Relation Extraction in the Microbiome Domain", "authors": ["Oumaima El Khettari", "Solen Quiniou", "Samuel Chaffron"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We explore a generative relation extraction (RE) pipeline tailored to the\nstudy of interactions in the intestinal microbiome, a complex and low-resource\nbiomedical domain. Our method leverages summarization with large language\nmodels (LLMs) to refine context before extracting relations via\ninstruction-tuned generation. Preliminary results on a dedicated corpus show\nthat summarization improves generative RE performance by reducing noise and\nguiding the model. However, BERT-based RE approaches still outperform\ngenerative models. This ongoing work demonstrates the potential of generative\nmethods to support the study of specialized domains in low-resources setting.", "AI": {"tldr": "A generative relation extraction pipeline using LLMs for studying the intestinal microbiome shows promise, but BERT-based methods currently outperform it.", "motivation": "To improve relation extraction in the low-resource biomedical domain of the intestinal microbiome.", "method": "The approach uses summarization with large language models (LLMs) to refine context before relation extraction via instruction-tuned generation.", "result": "Preliminary results indicate that summarization enhances generative relation extraction performance by reducing noise and guiding the model, yet BERT-based methods still outperform generative models.", "conclusion": "This work highlights the potential of generative methods for low-resource domains despite the current superiority of traditional models like BERT.", "key_contributions": ["Introduction of a generative RE pipeline tailored for the intestinal microbiome", "Demonstration of LLM summarization improving RE performance", "Insights on the limitations of generative models in comparison to BERT-based approaches."], "limitations": "The BERT-based approaches still provide better performance than the generative models tested.", "keywords": ["relation extraction", "large language models", "intestinal microbiome", "generative methods", "biomedical domain"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2506.08672", "pdf": "https://arxiv.org/pdf/2506.08672.pdf", "abs": "https://arxiv.org/abs/2506.08672", "title": "RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling", "authors": ["Yang Liu", "Jiaqi Li", "Zilong Zheng"], "categories": ["cs.CL"], "comment": "22 pages, 10 figures, 8 tables", "summary": "Rule-based reasoning has been acknowledged as one of the fundamental problems\nin reasoning, while deviations in rule formats, types, and complexity in\nreal-world applications pose severe challenges. Recent studies have shown that\nlarge reasoning models (LRMs) have remarkable reasoning capabilities, and their\nperformance is substantially enhanced by reinforcement learning (RL). However,\nit remains an open question whether small reasoning models (SRMs) can learn\nrule-based reasoning effectively with robust generalization across diverse\ntasks and domains. To address this, we introduce Reinforced Rule-based\nReasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct\nrule-based reasoning via a wide collection of curated tasks and a novel\ndomain-aware dynamic sampling approach. Specifically, RuleReasoner resamples\neach training batch by updating the sampling weights of different domains based\non historical rewards. This facilitates domain augmentation and flexible online\nlearning schedules for RL, obviating the need for pre-hoc human-engineered\nmix-training recipes used in existing methods. Empirical evaluations on\nin-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that\nRuleReasoner outperforms frontier LRMs by a significant margin ($\\Delta$4.1%\naverage points on eight ID tasks and $\\Delta$10.4% average points on three OOD\ntasks over OpenAI-o1). Notably, our approach also exhibits higher computational\nefficiency compared to prior dynamic sampling methods for RL.", "AI": {"tldr": "RuleReasoner improves rule-based reasoning in small reasoning models through dynamic sampling and reinforcement learning.", "motivation": "There are significant challenges in rule-based reasoning due to variations in rule formats and complexities in real-world applications. This paper investigates whether small reasoning models can effectively learn and generalize rule-based reasoning across various tasks and domains.", "method": "Introducing RuleReasoner, a method that utilizes dynamic sampling based on historical rewards to optimize training for rule-based reasoning tasks, allowing for domain augmentation and flexible online learning schedules.", "result": "Empirical evaluations show RuleReasoner outperforms large reasoning models on benchmark tasks, achieving improvements of 4.1% on in-distribution tasks and 10.4% on out-of-distribution tasks compared to existing methods.", "conclusion": "RuleReasoner demonstrates effective rule-based reasoning capabilities in small reasoning models with higher computational efficiency and robust generalization across diverse domains.", "key_contributions": ["Development of RuleReasoner for rule-based reasoning in small reasoning models", "Implementation of a domain-aware dynamic sampling approach", "Demonstration of superior performance on ID and OOD tasks compared to existing LRM methods"], "limitations": "", "keywords": ["Rule-based reasoning", "Reinforcement learning", "Dynamic sampling", "Machine learning", "Generalization"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.08686", "pdf": "https://arxiv.org/pdf/2506.08686.pdf", "abs": "https://arxiv.org/abs/2506.08686", "title": "Brevity is the soul of sustainability: Characterizing LLM response lengths", "authors": ["Soham Poddar", "Paramita Koley", "Janardan Misra", "Sanjay Podder", "Navveen Balani", "Niloy Ganguly", "Saptarshi Ghosh"], "categories": ["cs.CL", "cs.CY"], "comment": "Accepted to appear at the ACL 2025 findings", "summary": "A significant portion of the energy consumed by Large Language Models (LLMs)\narises from their inference processes; hence developing energy-efficient\nmethods for inference is crucial. While several techniques exist for inference\noptimization, output compression remains relatively unexplored, with only a few\npreliminary efforts addressing this aspect. In this work, we first benchmark 12\ndecoder-only LLMs across 5 datasets, revealing that these models often produce\nresponses that are substantially longer than necessary. We then conduct a\ncomprehensive quality assessment of LLM responses, formally defining six\ninformation categories present in LLM responses. We show that LLMs often tend\nto include redundant or additional information besides the minimal answer. To\naddress this issue of long responses by LLMs, we explore several simple and\nintuitive prompt-engineering strategies. Empirical evaluation shows that\nappropriate prompts targeting length reduction and controlling information\ncontent can achieve significant energy optimization between 25-60\\% by reducing\nthe response length while preserving the quality of LLM responses.", "AI": {"tldr": "This paper addresses energy efficiency in LLMs by proposing prompt-engineering strategies to reduce output length, resulting in significant energy savings without sacrificing response quality.", "motivation": "The energy consumed by LLM inference processes is significant, making energy-efficient methods crucial; however, output compression techniques are underexplored.", "method": "Benchmarking 12 decoder-only LLMs across 5 datasets to assess response quality and exploring prompt-engineering strategies for length reduction and information control.", "result": "The study finds that LLMs frequently produce unnecessarily long responses and demonstrates that prompts can reduce response lengths and achieve 25-60% energy optimization while maintaining response quality.", "conclusion": "By implementing specific prompt-engineering strategies, we can significantly reduce the energy consumption of LLMs during inference without compromising the quality of their outputs.", "key_contributions": ["Benchmarking of LLMs for output length assessment", "Identification of information redundancy in LLM responses", "Proposed prompt-engineering strategies for effective length reduction"], "limitations": "Limited exploration of techniques beyond prompt engineering and potential trade-offs in certain scenarios.", "keywords": ["Large Language Models", "energy efficiency", "output compression", "prompt engineering", "inference optimization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.08700", "pdf": "https://arxiv.org/pdf/2506.08700.pdf", "abs": "https://arxiv.org/abs/2506.08700", "title": "ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts", "authors": ["Ruiran Su", "Jiasheng Si", "Zhijiang Guo", "Janet B. Pierrehumbert"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Scientific fact-checking has mostly focused on text and tables, overlooking\nscientific charts, which are key for presenting quantitative evidence and\nstatistical reasoning. We introduce ClimateViz, the first large-scale benchmark\nfor scientific fact-checking using expert-curated scientific charts. ClimateViz\ncontains 49,862 claims linked to 2,896 visualizations, each labeled as support,\nrefute, or not enough information. To improve interpretability, each example\nincludes structured knowledge graph explanations covering trends, comparisons,\nand causal relations. We evaluate state-of-the-art multimodal language models,\nincluding both proprietary and open-source systems, in zero-shot and few-shot\nsettings. Results show that current models struggle with chart-based reasoning:\neven the best systems, such as Gemini 2.5 and InternVL 2.5, reach only 76.2 to\n77.8 percent accuracy in label-only settings, far below human performance (89.3\nand 92.7 percent). Explanation-augmented outputs improve performance in some\nmodels. We released our dataset and code alongside the paper.", "AI": {"tldr": "Introduction of ClimateViz, a benchmark for scientific fact-checking using visualizations and its evaluation of multimodal language models.", "motivation": "To address the gap in scientific fact-checking that has largely overlooked charts, which are essential for conveying quantitative evidence.", "method": "Evaluation of state-of-the-art multimodal language models on a dataset of claims linked to scientific charts, assessing their ability in zero-shot and few-shot settings.", "result": "Models achieved only 76.2 to 77.8 percent accuracy in label-only settings, significantly lower than human performance, indicating challenges in chart-based reasoning.", "conclusion": "Current language models struggle with interpreting scientific charts, but explanation-augmented outputs show promise in improving performance.", "key_contributions": ["Introduction of the ClimateViz benchmark for scientific fact-checking with visualizations.", "Structured knowledge graph explanations accompanying visualizations to enhance interpretability.", "Release of a large-scale dataset for further research in chart-based reasoning."], "limitations": "Current models show limited capability in chart-based reasoning and require further improvements to match human performance.", "keywords": ["scientific fact-checking", "visualization", "multimodal language models", "knowledge graphs", "ClimateViz"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.08712", "pdf": "https://arxiv.org/pdf/2506.08712.pdf", "abs": "https://arxiv.org/abs/2506.08712", "title": "ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization", "authors": ["Hee Suk Yoon", "Eunseop Yoon", "Mark A. Hasegawa-Johnson", "Sungwoong Kim", "Chang D. Yoo"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "We introduce ConfPO, a method for preference learning in Large Language\nModels (LLMs) that identifies and optimizes preference-critical tokens based\nsolely on the training policy's confidence, without requiring any auxiliary\nmodels or compute. Unlike prior Direct Alignment Algorithms (DAAs) such as\nDirect Preference Optimization (DPO), which uniformly adjust all token\nprobabilities regardless of their relevance to preference, ConfPO focuses\noptimization on the most impactful tokens. This targeted approach improves\nalignment quality while mitigating overoptimization (i.e., reward hacking) by\nusing the KL divergence budget more efficiently. In contrast to recent\ntoken-level methods that rely on credit-assignment models or AI annotators,\nraising concerns about scalability and reliability, ConfPO is simple,\nlightweight, and model-free. Experimental results on challenging alignment\nbenchmarks, including AlpacaEval 2 and Arena-Hard, demonstrate that ConfPO\nconsistently outperforms uniform DAAs across various LLMs, delivering better\nalignment with zero additional computational overhead.", "AI": {"tldr": "ConfPO is a novel method for preference learning in LLMs that optimizes preference-critical tokens based on the training policy's confidence, improving alignment quality without auxiliary models.", "motivation": "To enhance the alignment quality of LLMs while avoiding the pitfalls of overoptimization and relying on additional computational resources.", "method": "ConfPO identifies and optimizes preference-critical tokens based on the confidence from the training policy, avoiding uniform adjustments across all tokens.", "result": "Experimental results indicate that ConfPO consistently outperforms previous uniform Direct Alignment Algorithms (DAAs) like DPO on various challenging benchmarks.", "conclusion": "ConfPO provides a more efficient and targeted method for preference learning in LLMs, offering better alignment with no additional computational costs.", "key_contributions": ["Introduces a method that optimizes only preference-critical tokens based on policy confidence.", "Demonstrates superior performance compared to existing DAAs without additional computational burden.", "Addresses scalability and reliability issues found in token-level methods relying on external models."], "limitations": "", "keywords": ["Large Language Models", "preference learning", "alignment", "direct preference optimization", "KL divergence"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.08713", "pdf": "https://arxiv.org/pdf/2506.08713.pdf", "abs": "https://arxiv.org/abs/2506.08713", "title": "Explainable Compliance Detection with Multi-Hop Natural Language Inference on Assurance Case Structure", "authors": ["Fariz Ikhwantri", "Dusica Marijan"], "categories": ["cs.CL", "cs.SE"], "comment": null, "summary": "Ensuring complex systems meet regulations typically requires checking the\nvalidity of assurance cases through a claim-argument-evidence framework. Some\nchallenges in this process include the complicated nature of legal and\ntechnical texts, the need for model explanations, and limited access to\nassurance case data. We propose a compliance detection approach based on\nNatural Language Inference (NLI): EXplainable CompLiance detection with\nArgumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the\nclaim-argument-evidence structure of an assurance case as a multi-hop inference\nfor explainable and traceable compliance detection. We address the limited\nnumber of assurance cases by generating them using large language models\n(LLMs). We introduce metrics that measure the coverage and structural\nconsistency. We demonstrate the effectiveness of the generated assurance case\nfrom GDPR requirements in a multi-hop inference task as a case study. Our\nresults highlight the potential of NLI-based approaches in automating the\nregulatory compliance process.", "AI": {"tldr": "Proposes a compliance detection approach, EXCLAIM, using NLI to automate regulatory compliance through multi-hop reasoning.", "motivation": "To ensure complex systems meet regulations by effectively validating assurance cases within a claim-argument-evidence framework, overcoming challenges in legal and technical texts.", "method": "Developed a Natural Language Inference (NLI) approach, EXCLAIM, that formulates the assurance case structure as a multi-hop inference for explainable compliance detection, generating examples using large language models.", "result": "Demonstrated effectiveness of generated assurance cases focusing on GDPR requirements, utilizing metrics for coverage and structural consistency.", "conclusion": "NLI-based methods have significant potential in automating the regulatory compliance process, improving efficiency and clarity in assurance case validation.", "key_contributions": ["Introduced EXCLAIM for compliance detection via NLI.", "Used LLMs for generating assurance cases to address data limitations.", "Developed metrics for evaluating coverage and structural consistency of assurance cases."], "limitations": "Limited number of assurance cases and challenges in the underlying legal and technical text complexities still persist.", "keywords": ["Natural Language Inference", "compliance detection", "assurance cases", "multi-hop reasoning", "large language models"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.08717", "pdf": "https://arxiv.org/pdf/2506.08717.pdf", "abs": "https://arxiv.org/abs/2506.08717", "title": "Multi-Teacher Language-Aware Knowledge Distillation for Multilingual Speech Emotion Recognition", "authors": ["Mehedi Hasan Bijoy", "Dejan Porjazovski", "Tamás Grósz", "Mikko Kurimo"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to INTERSPEECH 2025 conference", "summary": "Speech Emotion Recognition (SER) is crucial for improving human-computer\ninteraction. Despite strides in monolingual SER, extending them to build a\nmultilingual system remains challenging. Our goal is to train a single model\ncapable of multilingual SER by distilling knowledge from multiple teacher\nmodels. To address this, we introduce a novel language-aware multi-teacher\nknowledge distillation method to advance SER in English, Finnish, and French.\nIt leverages Wav2Vec2.0 as the foundation of monolingual teacher models and\nthen distills their knowledge into a single multilingual student model. The\nstudent model demonstrates state-of-the-art performance, with a weighted recall\nof 72.9 on the English dataset and an unweighted recall of 63.4 on the Finnish\ndataset, surpassing fine-tuning and knowledge distillation baselines. Our\nmethod excels in improving recall for sad and neutral emotions, although it\nstill faces challenges in recognizing anger and happiness.", "AI": {"tldr": "A novel method for multilingual Speech Emotion Recognition (SER) using knowledge distillation from multiple teacher models, achieving state-of-the-art results.", "motivation": "To improve multilingual Speech Emotion Recognition (SER) and enhance human-computer interaction.", "method": "Introduce a language-aware multi-teacher knowledge distillation method that uses Wav2Vec2.0-based monolingual teacher models to train a single multilingual student model.", "result": "The student model achieved a weighted recall of 72.9 on the English dataset and an unweighted recall of 63.4 on the Finnish dataset, surpassing existing baselines.", "conclusion": "The proposed SER model significantly improves recall for sad and neutral emotions but still struggles with anger and happiness recognition.", "key_contributions": ["Development of a language-aware multi-teacher knowledge distillation method for SER.", "Creation of a high-performing multilingual SER model using Wav2Vec2.0.", "Demonstration of improved performance across multiple languages in emotion recognition."], "limitations": "Challenges remain in accurately recognizing anger and happiness emotions in the multilingual context.", "keywords": ["Speech Emotion Recognition", "knowledge distillation", "multilingual", "human-computer interaction", "Wav2Vec2.0"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.08726", "pdf": "https://arxiv.org/pdf/2506.08726.pdf", "abs": "https://arxiv.org/abs/2506.08726", "title": "Improved LLM Agents for Financial Document Question Answering", "authors": ["Nelvin Tan", "Zian Seng", "Liang Zhang", "Yu-Ching Shih", "Dong Yang", "Amol Salunkhe"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 5 figures", "summary": "Large language models (LLMs) have shown impressive capabilities on numerous\nnatural language processing tasks. However, LLMs still struggle with numerical\nquestion answering for financial documents that include tabular and textual\ndata. Recent works have showed the effectiveness of critic agents (i.e.,\nself-correction) for this task given oracle labels. Building upon this\nframework, this paper examines the effectiveness of the traditional critic\nagent when oracle labels are not available, and show, through experiments, that\nthis critic agent's performance deteriorates in this scenario. With this in\nmind, we present an improved critic agent, along with the calculator agent\nwhich outperforms the previous state-of-the-art approach (program-of-thought)\nand is safer. Furthermore, we investigate how our agents interact with each\nother, and how this interaction affects their performance.", "AI": {"tldr": "The paper explores the limitations of traditional critic agents for numerical question answering in financial documents when oracle labels are unavailable and introduces a new critic agent and a calculator agent that improve performance and safety.", "motivation": "To address the challenges faced by large language models in numerical question answering for financial documents involving tabular and textual data, particularly in the absence of oracle labels.", "method": "The authors examined the performance of traditional critic agents and proposed improvements with a new critic agent and a calculator agent, supported by experimental evaluations.", "result": "Experiments demonstrated that the new critic and calculator agents significantly outperform the previous program-of-thought state-of-the-art approach, even in the absence of oracle labels.", "conclusion": "The proposed agents effectively enhance performance in numerical questioning tasks and offer a safer alternative to existing methods by improving inter-agent interactions.", "key_contributions": ["Introduction of an improved critic agent for numerical question answering without oracle labels", "Development of a calculator agent that outperforms existing methods", "Analysis of inter-agent interactions and their impact on performance"], "limitations": "The paper primarily focuses on financial documents and may not generalize well to other domains.", "keywords": ["large language models", "numerical question answering", "financial documents", "critic agents", "calculator agents"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2506.08738", "pdf": "https://arxiv.org/pdf/2506.08738.pdf", "abs": "https://arxiv.org/abs/2506.08738", "title": "Societal AI Research Has Become Less Interdisciplinary", "authors": ["Dror Kris Markus", "Fabrizio Gilardi", "Daria Stetsenko"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "As artificial intelligence (AI) systems become deeply embedded in everyday\nlife, calls to align AI development with ethical and societal values have\nintensified. Interdisciplinary collaboration is often championed as a key\npathway for fostering such engagement. Yet it remains unclear whether\ninterdisciplinary research teams are actually leading this shift in practice.\nThis study analyzes over 100,000 AI-related papers published on ArXiv between\n2014 and 2024 to examine how ethical values and societal concerns are\nintegrated into technical AI research. We develop a classifier to identify\nsocietal content and measure the extent to which research papers express these\nconsiderations. We find a striking shift: while interdisciplinary teams remain\nmore likely to produce societally-oriented research, computer science-only\nteams now account for a growing share of the field's overall societal output.\nThese teams are increasingly integrating societal concerns into their papers\nand tackling a wide range of domains - from fairness and safety to healthcare\nand misinformation. These findings challenge common assumptions about the\ndrivers of societal AI and raise important questions. First, what are the\nimplications for emerging understandings of AI safety and governance if most\nsocietally-oriented research is being undertaken by exclusively technical\nteams? Second, for scholars in the social sciences and humanities: in a\ntechnical field increasingly responsive to societal demands, what distinctive\nperspectives can we still offer to help shape the future of AI?", "AI": {"tldr": "This study analyzes AI-related papers to assess the integration of ethical values and societal concerns in research, finding a shift towards computer science-only teams contributing to societal outputs.", "motivation": "To evaluate how ethical values and societal concerns are incorporated into AI research amidst the intensifying calls for ethical AI development.", "method": "Analyzed over 100,000 AI-related papers from ArXiv published between 2014 and 2024, using a classifier to identify societal content in the research.", "result": "Interdisciplinary teams are still more likely to produce societally-oriented research, but computer science-only teams account for an increasing share, incorporating a variety of societal concerns.", "conclusion": "The findings question assumptions about the drivers of societal AI research and emphasize the need for interdisciplinary inputs in a field increasingly attuned to societal demands.", "key_contributions": ["Development of a classifier for measuring societal concerns in AI papers", "Demonstration of a shift in production of societally-oriented AI research towards computer science-only teams", "Insights on the implications for AI safety and governance related to societal integration"], "limitations": "", "keywords": ["AI research", "societal concerns", "interdisciplinary collaboration", "ethical values", "AI governance"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.08746", "pdf": "https://arxiv.org/pdf/2506.08746.pdf", "abs": "https://arxiv.org/abs/2506.08746", "title": "Towards Secure and Private Language Models for Nuclear Power Plants", "authors": ["Muhammad Anwar", "Mishca de Costa", "Issam Hammad", "Daniel Lau"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper introduces a domain-specific Large Language Model for nuclear\napplications, built from the publicly accessible Essential CANDU textbook.\nDrawing on a compact Transformer-based architecture, the model is trained on a\nsingle GPU to protect the sensitive data inherent in nuclear operations.\nDespite relying on a relatively small dataset, it shows encouraging signs of\ncapturing specialized nuclear vocabulary, though the generated text sometimes\nlacks syntactic coherence. By focusing exclusively on nuclear content, this\napproach demonstrates the feasibility of in-house LLM solutions that align with\nrigorous cybersecurity and data confidentiality standards. Early successes in\ntext generation underscore the model's utility for specialized tasks, while\nalso revealing the need for richer corpora, more sophisticated preprocessing,\nand instruction fine-tuning to enhance domain accuracy. Future directions\ninclude extending the dataset to cover diverse nuclear subtopics, refining\ntokenization to reduce noise, and systematically evaluating the model's\nreadiness for real-world applications in nuclear domain.", "AI": {"tldr": "The paper presents a domain-specific Large Language Model for nuclear applications, trained on the Essential CANDU textbook, demonstrating initial successes in text generation while highlighting the need for improvements in dataset richness and model fine-tuning.", "motivation": "To create a Large Language Model tailored for the nuclear domain, ensuring compliance with cybersecurity and data confidentiality standards.", "method": "A compact Transformer-based architecture is utilized, trained on the Essential CANDU textbook on a single GPU, focusing on specialized nuclear vocabulary.", "result": "The model shows initial promise in generating relevant text for nuclear applications, although some outputs lack syntactic coherence.", "conclusion": "While the model demonstrates feasibility for in-house LLM solutions in nuclear applications, it requires enhancements in dataset size, preprocessing, and instruction fine-tuning for better domain accuracy.", "key_contributions": ["Introduction of a domain-specific LLM for nuclear applications.", "Demonstration of compact Transformer-based architecture for sensitive domains.", "Identification of challenges and future directions for model improvement."], "limitations": "The model's output sometimes lacks syntactic coherence and is based on a limited dataset, requiring richer corpora and better preprocessing.", "keywords": ["Large Language Model", "nuclear applications", "Transformer architecture", "text generation", "cybersecurity"], "importance_score": 0, "read_time_minutes": 10}}
{"id": "2506.08750", "pdf": "https://arxiv.org/pdf/2506.08750.pdf", "abs": "https://arxiv.org/abs/2506.08750", "title": "Unlocking the Potential of Large Language Models in the Nuclear Industry with Synthetic Data", "authors": ["Muhammad Anwar", "Daniel Lau", "Mishca de Costa", "Issam Hammad"], "categories": ["cs.CL"], "comment": null, "summary": "The nuclear industry possesses a wealth of valuable information locked away\nin unstructured text data. This data, however, is not readily usable for\nadvanced Large Language Model (LLM) applications that require clean, structured\nquestion-answer pairs for tasks like model training, fine-tuning, and\nevaluation. This paper explores how synthetic data generation can bridge this\ngap, enabling the development of robust LLMs for the nuclear domain. We discuss\nthe challenges of data scarcity and privacy concerns inherent in the nuclear\nindustry and how synthetic data provides a solution by transforming existing\ntext data into usable Q&A pairs. This approach leverages LLMs to analyze text,\nextract key information, generate relevant questions, and evaluate the quality\nof the resulting synthetic dataset. By unlocking the potential of LLMs in the\nnuclear industry, synthetic data can pave the way for improved information\nretrieval, enhanced knowledge sharing, and more informed decision-making in\nthis critical sector.", "AI": {"tldr": "Exploration of synthetic data generation to create structured Q&A pairs from unstructured text in the nuclear industry, enabling the use of LLMs.", "motivation": "To address data scarcity and privacy concerns in the nuclear industry by transforming unstructured text into usable Q&A pairs for LLM applications.", "method": "The paper discusses leveraging LLMs to analyze text, extract key information, generate relevant questions, and evaluate synthetic dataset quality.", "result": "Synthetic data generation allows for the creation of structured question-answer pairs that enhance the performance and usability of LLMs in the nuclear sector.", "conclusion": "By unlocking the potential of LLMs through synthetic data, the nuclear industry can achieve better information retrieval and decision-making capabilities.", "key_contributions": ["Demonstrates the application of LLMs for synthetic data generation in a specialized domain.", "Addresses privacy and data scarcity challenges through innovative data transformation.", "Enhances decision-making processes in the nuclear industry by providing structured information."], "limitations": "", "keywords": ["nuclear industry", "synthetic data", "large language models", "unstructured text", "information retrieval"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.08753", "pdf": "https://arxiv.org/pdf/2506.08753.pdf", "abs": "https://arxiv.org/abs/2506.08753", "title": "Factors affecting the in-context learning abilities of LLMs for dialogue state tracking", "authors": ["Pradyoth Hegde", "Santosh Kesiraju", "Jan Švec", "Šimon Sedláček", "Bolaji Yusuf", "Oldřich Plchot", "Deepak K T", "Jan Černocký"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Interspeech 2025", "summary": "This study explores the application of in-context learning (ICL) to the\ndialogue state tracking (DST) problem and investigates the factors that\ninfluence its effectiveness. We use a sentence embedding based k-nearest\nneighbour method to retrieve the suitable demonstrations for ICL. The selected\ndemonstrations, along with the test samples, are structured within a template\nas input to the LLM. We then conduct a systematic study to analyse the impact\nof factors related to demonstration selection and prompt context on DST\nperformance. This work is conducted using the MultiWoZ2.4 dataset and focuses\nprimarily on the OLMo-7B-instruct, Mistral-7B-Instruct-v0.3, and\nLlama3.2-3B-Instruct models. Our findings provide several useful insights on\nin-context learning abilities of LLMs for dialogue state tracking.", "AI": {"tldr": "This study investigates the use of in-context learning for dialogue state tracking, examining factors influencing its effectiveness with LLMs.", "motivation": "To explore the effectiveness of in-context learning in dialogue state tracking and understand factors affecting performance.", "method": "A sentence embedding based k-nearest neighbour method is used to select suitable demonstrations for in-context learning, structured as input to LLMs along with test samples.", "result": "The study demonstrates insights on the performance of in-context learning abilities of LLMs in dialogue state tracking using various models on the MultiWoZ2.4 dataset.", "conclusion": "The findings highlight key factors influencing the effectiveness of in-context learning for dialogue state tracking in LLMs.", "key_contributions": ["Investigates in-context learning in dialogue state tracking", "Analyzes demonstration selection and prompt context impact", "Provides insights on LLM performance with DST."], "limitations": "", "keywords": ["in-context learning", "dialogue state tracking", "LLMs", "demonstration selection", "MultiWoZ2.4"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.08757", "pdf": "https://arxiv.org/pdf/2506.08757.pdf", "abs": "https://arxiv.org/abs/2506.08757", "title": "Enhancing Accuracy and Maintainability in Nuclear Plant Data Retrieval: A Function-Calling LLM Approach Over NL-to-SQL", "authors": ["Mishca de Costa", "Muhammad Anwar", "Dave Mercier", "Mark Randall", "Issam Hammad"], "categories": ["cs.CL", "cs.LG"], "comment": "44th Annual CNS Conference and the 49th Annual CNS/CNA Student\n  Conference, Westin Harbour Castle Hotel, Toronto, ON, Canada, June 8-11, 2025", "summary": "Retrieving operational data from nuclear power plants requires exceptional\naccuracy and transparency due to the criticality of the decisions it supports.\nTraditionally, natural language to SQL (NL-to-SQL) approaches have been\nexplored for querying such data. While NL-to-SQL promises ease of use, it poses\nsignificant risks: end-users cannot easily validate generated SQL queries, and\nlegacy nuclear plant databases -- often complex and poorly structured --\ncomplicate query generation due to decades of incremental modifications. These\nchallenges increase the likelihood of inaccuracies and reduce trust in the\napproach. In this work, we propose an alternative paradigm: leveraging\nfunction-calling large language models (LLMs) to address these challenges.\nInstead of directly generating SQL queries, we define a set of pre-approved,\npurpose-specific functions representing common use cases. Queries are processed\nby invoking these functions, which encapsulate validated SQL logic. This hybrid\napproach mitigates the risks associated with direct NL-to-SQL translations by\nensuring that SQL queries are reviewed and optimized by experts before\ndeployment. While this strategy introduces the upfront cost of developing and\nmaintaining the function library, we demonstrate how NL-to-SQL tools can assist\nin the initial generation of function code, allowing experts to focus on\nvalidation rather than creation. Our study includes a performance comparison\nbetween direct NL-to-SQL generation and the proposed function-based approach,\nhighlighting improvements in accuracy and maintainability. This work\nunderscores the importance of balancing user accessibility with operational\nsafety and provides a novel, actionable framework for robust data retrieval in\ncritical systems.", "AI": {"tldr": "This work proposes a function-calling LLM approach for improving the reliability of NL-to-SQL queries in nuclear power plant data retrieval.", "motivation": "To enhance the accuracy and transparency of querying operational data from nuclear power plants due to the critical decisions supported by such data.", "method": "Leveraging function-calling large language models instead of direct NL-to-SQL translations, creating a library of pre-approved functions to encapsulate validated SQL logic.", "result": "Demonstrated improvements in accuracy and maintainability compared to traditional NL-to-SQL methods.", "conclusion": "The proposed hybrid approach strikes a balance between user accessibility and operational safety for critical data retrieval processes.", "key_contributions": ["Introduction of a function-calling approach to NL-to-SQL queries", "Validation of SQL logic through expert-reviewed functions", "Performance comparison showing improvements over direct NL-to-SQL methods"], "limitations": "Initial development and maintenance of the function library may require substantial upfront effort.", "keywords": ["NL-to-SQL", "large language models", "nuclear power plants", "data retrieval", "operational safety"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.08768", "pdf": "https://arxiv.org/pdf/2506.08768.pdf", "abs": "https://arxiv.org/abs/2506.08768", "title": "AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP", "authors": ["Ahmed Hasanaath", "Aisha Alansari", "Ahmed Ashraf", "Chafik Salmane", "Hamzah Luqman", "Saad Ezzini"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable progress in reasoning\nabilities and general natural language processing (NLP) tasks, yet their\nperformance on Arabic data, characterized by rich morphology, diverse dialects,\nand complex script, remains underexplored. This paper presents a comprehensive\nbenchmarking study of multiple reasoning-focused LLMs, with a special emphasis\non the newly introduced DeepSeek models, across a suite of fifteen Arabic NLP\ntasks. We experiment with various strategies, including zero-shot, few-shot,\nand fine-tuning. This allows us to systematically evaluate performance on\ndatasets covering a range of applications to examine their capacity for\nlinguistic reasoning under different levels of complexity. Our experiments\nreveal several key findings. First, carefully selecting just three in-context\nexamples delivers an average uplift of over 13 F1 points on classification\ntasks-boosting sentiment analysis from 35.3% to 87.5% and paraphrase detection\nfrom 56.1% to 87.0%. Second, reasoning-focused DeepSeek architectures\noutperform a strong GPT o4-mini baseline by an average of 12 F1 points on\ncomplex inference tasks in the zero-shot setting. Third, LoRA-based fine-tuning\nyields up to an additional 8 points in F1 and BLEU compared to equivalent\nincreases in model scale. The code is available at\nhttps://anonymous.4open.science/r/AraReasoner41299", "AI": {"tldr": "This paper benchmarks reasoning-focused LLMs, particularly DeepSeek models, on fifteen Arabic NLP tasks, revealing significant improvements in classification and inference tasks.", "motivation": "To explore the performance of LLMs on Arabic data, which presents unique challenges such as rich morphology and diverse dialects, while focusing on their reasoning capabilities.", "method": "The study conducts a benchmarking analysis of multiple reasoning-focused LLMs, including strategies like zero-shot, few-shot, and fine-tuning across fifteen Arabic NLP tasks.", "result": "DeepSeek architectures show superior performance to a GPT o4-mini baseline, with significant F1 score improvements in various tasks, particularly in sentiment analysis and paraphrase detection.", "conclusion": "The findings suggest that targeted strategies can vastly improve the effectiveness of LLMs in Arabic NLP tasks, with fine-tuning proving especially beneficial.", "key_contributions": ["Introduces a comprehensive benchmark for Arabic NLP tasks using reasoning-focused LLMs.", "Demonstrates significant performance gains through in-context examples and fine-tuning strategies.", "Provides insights into the effectiveness of DeepSeek models compared to baseline LLMs."], "limitations": "", "keywords": ["Large Language Models", "Natural Language Processing", "Arabic Language", "DeepSeek", "Reasoning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.08827", "pdf": "https://arxiv.org/pdf/2506.08827.pdf", "abs": "https://arxiv.org/abs/2506.08827", "title": "The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation", "authors": ["Francisco Vargas", "Alejandro González Coene", "Gaston Escalante", "Exequiel Lobón", "Manuel Pulido"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The extraction of information about traffic accidents from legal documents is\ncrucial for quantifying insurance company costs. Extracting entities such as\npercentages of physical and/or psychological disability and the involved\ncompensation amounts is a challenging process, even for experts, due to the\nsubtle arguments and reasoning in the court decision. A two-step procedure is\nproposed: first, segmenting the document identifying the most relevant\nsegments, and then extracting the entities. For text segmentation, two\nmethodologies are compared: a classic method based on regular expressions and a\nsecond approach that divides the document into blocks of n-tokens, which are\nthen vectorized using multilingual models for semantic searches\n(text-embedding-ada-002/MiniLM-L12-v2 ). Subsequently, large language models\n(LLaMA-2 7b, 70b, LLaMA-3 8b, and GPT-4 Turbo) are applied with prompting to\nthe selected segments for entity extraction. For the LLaMA models, fine-tuning\nis performed using LoRA. LLaMA-2 7b, even with zero temperature, shows a\nsignificant number of hallucinations in extractions which are an important\ncontention point for named entity extraction. This work shows that these\nhallucinations are substantially reduced after finetuning the model. The\nperformance of the methodology based on segment vectorization and subsequent\nuse of LLMs significantly surpasses the classic method which achieves an\naccuracy of 39.5%. Among open-source models, LLaMA-2 70B with finetuning\nachieves the highest accuracy 79.4%, surpassing its base version 61.7%.\nNotably, the base LLaMA-3 8B model already performs comparably to the finetuned\nLLaMA-2 70B model, achieving 76.6%, highlighting the rapid progress in model\ndevelopment. Meanwhile, GPT-4 Turbo achieves the highest accuracy at 86.1%.", "AI": {"tldr": "This paper proposes a two-step procedure for extracting traffic accident information from legal documents using segment vectorization and large language models (LLMs).", "motivation": "Extracting relevant entities from legal documents is crucial for quantifying insurance costs but presents significant challenges due to complex reasoning within court decisions.", "method": "A two-step methodology involving document segmentation to identify relevant segments followed by entity extraction using classic regular expressions and LLMs (fine-tuned LLaMA models and GPT-4 Turbo).", "result": "LLaMA-2 70B with fine-tuning achieved the highest entity extraction accuracy of 79.4%, while GPT-4 Turbo outperformed all at 86.1%.", "conclusion": "The proposed methodology significantly improves accuracy in entity extraction from legal documents compared to classic methods, with fine-tuning of LLMs reducing hallucinations.", "key_contributions": ["Introduction of a two-step procedure for legal document analysis.", "Comparative analysis of regular expressions vs. segment vectorization for entity extraction.", "Demonstration of superior performance of fine-tuned LLMs over traditional methods."], "limitations": "Potential for hallucinations in LLM outputs and the need for extensive fine-tuning.", "keywords": ["traffic accidents", "entity extraction", "legal documents", "large language models", "fine-tuning"], "importance_score": 6, "read_time_minutes": 12}}
{"id": "2506.08836", "pdf": "https://arxiv.org/pdf/2506.08836.pdf", "abs": "https://arxiv.org/abs/2506.08836", "title": "Advancing STT for Low-Resource Real-World Speech", "authors": ["Flavio D'Intino", "Hans-Peter Hutter"], "categories": ["cs.CL", "cs.HC"], "comment": "Conference: HCI International 2025, 20 pages, 4 figures", "summary": "Swiss German is a low-resource language represented by diverse dialects that\ndiffer significantly from Standard German and from each other, lacking a\nstandardized written form. As a result, transcribing Swiss German involves\ntranslating into Standard German. Existing datasets have been collected in\ncontrolled environments, yielding effective speech-to-text (STT) models, but\nthese models struggle with spontaneous conversational speech.\n  This paper, therefore, introduces the new SRB-300 dataset, a 300-hour\nannotated speech corpus featuring real-world long-audio recordings from 39\nSwiss German radio and TV stations. It captures spontaneous speech across all\nmajor Swiss dialects recorded in various realistic environments and overcomes\nthe limitation of prior sentence-level corpora.\n  We fine-tuned multiple OpenAI Whisper models on the SRB-300 dataset,\nachieving notable enhancements over previous zero-shot performance metrics.\nImprovements in word error rate (WER) ranged from 19% to 33%, while BLEU scores\nincreased between 8% and 40%. The best fine-tuned model, large-v3, achieved a\nWER of 17.1% and a BLEU score of 74.8. This advancement is crucial for\ndeveloping effective and robust STT systems for Swiss German and other\nlow-resource languages in real-world contexts.", "AI": {"tldr": "The paper presents the SRB-300 dataset, a 300-hour annotated speech corpus for Swiss German, addressing limitations in current speech-to-text models.", "motivation": "To improve speech-to-text models for Swiss German, a low-resource language with diverse dialects and no standardized written form.", "method": "Introduced the SRB-300 dataset with real-world audio recordings and fine-tuned OpenAI Whisper models on this dataset.", "result": "Achieved significant improvements in word error rate (WER) and BLEU scores, indicating enhanced model performance in real-world speech recognition tasks.", "conclusion": "The SRB-300 dataset is pivotal for effective speech-to-text systems for Swiss German and can benefit other low-resource languages.", "key_contributions": ["Introduced the SRB-300 dataset with long-audio recordings.", "Demonstrated improvements in STT model performance for spontaneous speech.", "Addressed the challenges posed by dialectal variation in Swiss German."], "limitations": "", "keywords": ["Swiss German", "speech-to-text", "dataset", "low-resource languages", "OpenAI Whisper"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.08885", "pdf": "https://arxiv.org/pdf/2506.08885.pdf", "abs": "https://arxiv.org/abs/2506.08885", "title": "AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)", "authors": ["Danush Khanna", "Krishna Kumar", "Basab Ghosh", "Vinija Jain", "Vasu Sharma", "Aman Chadha", "Amitava Das"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Adversarial threats against LLMs are escalating faster than current defenses\ncan adapt. We expose a critical geometric blind spot in alignment: adversarial\nprompts exploit latent camouflage, embedding perilously close to the safe\nrepresentation manifold while encoding unsafe intent thereby evading surface\nlevel defenses like Direct Preference Optimization (DPO), which remain blind to\nthe latent geometry. We introduce ALKALI, the first rigorously curated\nadversarial benchmark and the most comprehensive to date spanning 9,000 prompts\nacross three macro categories, six subtypes, and fifteen attack families.\nEvaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates\n(ASRs) across both open and closed source models, exposing an underlying\nvulnerability we term latent camouflage, a structural blind spot where\nadversarial completions mimic the latent geometry of safe ones. To mitigate\nthis vulnerability, we introduce GRACE - Geometric Representation Aware\nContrastive Enhancement, an alignment framework coupling preference learning\nwith latent space regularization. GRACE enforces two constraints: latent\nseparation between safe and adversarial completions, and adversarial cohesion\namong unsafe and jailbreak behaviors. These operate over layerwise pooled\nembeddings guided by a learned attention profile, reshaping internal geometry\nwithout modifying the base model, and achieve up to 39% ASR reduction.\nMoreover, we introduce AVQI, a geometry aware metric that quantifies latent\nalignment failure via cluster separation and compactness. AVQI reveals when\nunsafe completions mimic the geometry of safe ones, offering a principled lens\ninto how models internally encode safety. We make the code publicly available\nat https://anonymous.4open.science/r/alkali-B416/README.md.", "AI": {"tldr": "The paper exposes vulnerabilities in LLMs due to adversarial prompts that exploit latent camouflage, introducing the ALKALI benchmark and GRACE framework to enhance LLM adversarial robustness.", "motivation": "Adversarial threats against LLMs are growing faster than existing defenses can keep up with, revealing critical weaknesses in alignment strategies.", "method": "The paper introduces ALKALI, a benchmark with 9,000 adversarial prompts spanning various categories, and proposes GRACE, an alignment framework using latent space regularization to enhance defenses against adversarial attacks.", "result": "The evaluation of 21 LLMs shows high Attack Success Rates (ASRs) due to latent camouflage; GRACE achieves up to 39% reduction in ASR, indicating its effectiveness in improving model robustness.", "conclusion": "Implementing GRACE can enhance LLM safety through geometric representation awareness, providing a novel approach to address challenges posed by adversarial threats in NLP.", "key_contributions": ["Introduction of the ALKALI benchmark for adversarial prompts.", "Development of the GRACE framework for enhancing LLM robustness.", "Proposal of the AVQI metric for assessing latent alignment failure."], "limitations": "The framework may require further validation across more diverse models and threat scenarios.", "keywords": ["adversarial attacks", "large language models", "alignment", "latent space", "robustness"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.08897", "pdf": "https://arxiv.org/pdf/2506.08897.pdf", "abs": "https://arxiv.org/abs/2506.08897", "title": "PlantBert: An Open Source Language Model for Plant Science", "authors": ["Hiba Khey", "Amine Lakhder", "Salma Rouichi", "Imane El Ghabi", "Kamal Hejjaoui", "Younes En-nahli", "Fahd Kalloubi", "Moez Amri"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid advancement of transformer-based language models has catalyzed\nbreakthroughs in biomedical and clinical natural language processing; however,\nplant science remains markedly underserved by such domain-adapted tools. In\nthis work, we present PlantBert, a high-performance, open-source language model\nspecifically tailored for extracting structured knowledge from plant\nstress-response literature. Built upon the DeBERTa architecture-known for its\ndisentangled attention and robust contextual encoding-PlantBert is fine-tuned\non a meticulously curated corpus of expert-annotated abstracts, with a primary\nfocus on lentil (Lens culinaris) responses to diverse abiotic and biotic\nstressors. Our methodology combines transformer-based modeling with\nrule-enhanced linguistic post-processing and ontology-grounded entity\nnormalization, enabling PlantBert to capture biologically meaningful\nrelationships with precision and semantic fidelity. The underlying corpus is\nannotated using a hierarchical schema aligned with the Crop Ontology,\nencompassing molecular, physiological, biochemical, and agronomic dimensions of\nplant adaptation. PlantBert exhibits strong generalization capabilities across\nentity types and demonstrates the feasibility of robust domain adaptation in\nlow-resource scientific fields. By providing a scalable and reproducible\nframework for high-resolution entity recognition, PlantBert bridges a critical\ngap in agricultural NLP and paves the way for intelligent, data-driven systems\nin plant genomics, phenomics, and agronomic knowledge discovery. Our model is\npublicly released to promote transparency and accelerate cross-disciplinary\ninnovation in computational plant science.", "AI": {"tldr": "PlantBert is an open-source language model designed for extracting structured knowledge from plant stress-response literature, particularly focused on lentil (Lens culinaris).", "motivation": "The need for domain-adapted tools in plant science due to the shortcomings of current language models in handling relevant literature.", "method": "PlantBert is built on the DeBERTa architecture, fine-tuned on a curated corpus of expert-annotated abstracts, integrating transformer-based modeling with rule-enhanced post-processing and ontology-grounded normalization.", "result": "PlantBert shows strong generalization capabilities across entity types and demonstrates effective domain adaptation in low-resource scientific areas.", "conclusion": "The model provides a scalable framework for high-resolution entity recognition and encourages cross-disciplinary collaboration in plant genomics.", "key_contributions": ["Introduction of PlantBert, a specialized model for plant science.", "Demonstration of effective domain adaptation in a low-resource field.", "Public release of the model to enhance transparency and innovation."], "limitations": "", "keywords": ["Plant science", "Natural language processing", "Language model", "Entity recognition", "Domain adaptation"], "importance_score": 2, "read_time_minutes": 5}}
{"id": "2506.08899", "pdf": "https://arxiv.org/pdf/2506.08899.pdf", "abs": "https://arxiv.org/abs/2506.08899", "title": "From Legal Texts to Defeasible Deontic Logic via LLMs: A Study in Automated Semantic Analysis", "authors": ["Elias Horner", "Cristinel Mateis", "Guido Governatori", "Agata Ciabattoni"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LO"], "comment": null, "summary": "We present a novel approach to the automated semantic analysis of legal texts\nusing large language models (LLMs), targeting their transformation into formal\nrepresentations in Defeasible Deontic Logic (DDL). We propose a structured\npipeline that segments complex normative language into atomic snippets,\nextracts deontic rules, and evaluates them for syntactic and semantic\ncoherence. Our methodology is evaluated across various LLM configurations,\nincluding prompt engineering strategies, fine-tuned models, and multi-stage\npipelines, focusing on legal norms from the Australian Telecommunications\nConsumer Protections Code. Empirical results demonstrate promising alignment\nbetween machine-generated and expert-crafted formalizations, showing that LLMs\n- particularly when prompted effectively - can significantly contribute to\nscalable legal informatics.", "AI": {"tldr": "A novel approach to automated semantic analysis of legal texts using LLMs for formal representation in Defeasible Deontic Logic.", "motivation": "To improve the analysis and formalization of legal texts through the application of large language models.", "method": "A structured pipeline that segments normative language into atomic snippets, extracts deontic rules, and evaluates them for coherence using multiple LLM configurations and prompt engineering techniques.", "result": "Empirical results show promising alignment between machine-generated and expert-crafted formalizations, indicating effective contributions of LLMs in legal informatics.", "conclusion": "LLMs, especially with effective prompting, can enhance the scalability of legal informatics through improved analysis of legal norms.", "key_contributions": ["Development of a structured pipeline for semantic analysis of legal texts", "Evaluation of various LLM configurations and prompt strategies", "Demonstration of alignment between machine-generated and expert-crafted representations"], "limitations": "", "keywords": ["legal text analysis", "large language models", "Defeasible Deontic Logic"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2506.08907", "pdf": "https://arxiv.org/pdf/2506.08907.pdf", "abs": "https://arxiv.org/abs/2506.08907", "title": "Dialect Normalization using Large Language Models and Morphological Rules", "authors": ["Antonios Dimakis", "John Pavlopoulos", "Antonios Anastasopoulos"], "categories": ["cs.CL", "I.2.7"], "comment": "19 pages, 18 figures, to be published in the Findings of the\n  Association for Computational Linguistics 2025", "summary": "Natural language understanding systems struggle with low-resource languages,\nincluding many dialects of high-resource ones. Dialect-to-standard\nnormalization attempts to tackle this issue by transforming dialectal text so\nthat it can be used by standard-language tools downstream. In this study, we\ntackle this task by introducing a new normalization method that combines\nrule-based linguistically informed transformations and large language models\n(LLMs) with targeted few-shot prompting, without requiring any parallel data.\nWe implement our method for Greek dialects and apply it on a dataset of\nregional proverbs, evaluating the outputs using human annotators. We then use\nthis dataset to conduct downstream experiments, finding that previous results\nregarding these proverbs relied solely on superficial linguistic information,\nincluding orthographic artifacts, while new observations can still be made\nthrough the remaining semantics.", "AI": {"tldr": "This paper presents a novel dialect-to-standard normalization method for low-resource languages using a combination of rule-based transformations and large language models (LLMs), applied on Greek dialects.", "motivation": "To address the challenges faced by natural language understanding systems with low-resource languages and dialects, particularly in the context of enabling standard language tools to work effectively with dialectal text.", "method": "The proposed normalization method integrates rule-based, linguistically-informed transformations alongside large language models (LLMs) employing targeted few-shot prompting, without the need for parallel data.", "result": "The outputs of the normalization method were evaluated using human annotators, revealing that prior studies on Greek proverbs relied on superficial linguistic features while enabling new semantic insights through the suggested methodology.", "conclusion": "The study demonstrates the effectiveness of combining rule-based techniques with LLMs for dialect normalization, providing deeper understanding of dialectal content.", "key_contributions": ["Introduction of a novel normalization method for dialectal text", "Combination of rule-based transformations with LLMs", "Insights into semantic understanding beyond superficial linguistic features"], "limitations": "The study is limited to Greek dialects and may not generalize to other low-resource languages without further adaptation.", "keywords": ["natural language understanding", "dialect normalization", "large language models", "low-resource languages", "targeted few-shot prompting"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.08920", "pdf": "https://arxiv.org/pdf/2506.08920.pdf", "abs": "https://arxiv.org/abs/2506.08920", "title": "PropMEND: Hypernetworks for Knowledge Propagation in LLMs", "authors": ["Zeyu Leo Liu", "Greg Durrett", "Eunsol Choi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under review", "summary": "Knowledge editing techniques for large language models (LLMs) can inject\nknowledge that is later reproducible verbatim, but they fall short on\npropagating that knowledge: models cannot answer questions that require\nreasoning with the injected knowledge. We present a hypernetwork-based approach\nfor knowledge propagation, named PropMEND, where we meta-learn how to modify\ngradients of a language modeling loss to encourage injected information to\npropagate. Our approach extends the meta-objective of MEND [29] so that\ngradient updates on knowledge are transformed to enable answering multi-hop\nquestions involving that knowledge. We show improved performance on the\nRippleEdit dataset, showing almost 2x accuracy on challenging multi-hop\nquestions whose answers are not explicitly stated in the injected fact. We\nfurther introduce a new dataset, Controlled RippleEdit, to evaluate the\ngeneralization of our hypernetwork, testing knowledge propagation along\nrelations and entities unseen during hypernetwork training. PropMEND still\noutperforms existing approaches in unseen entity-relation pairs, yet the\nperformance gap decreases substantially, suggesting future work in propagating\nknowledge to a wide range of relations.", "AI": {"tldr": "PropMEND is a hypernetwork-based method that enhances knowledge propagation in LLMs, enabling them to answer multi-hop questions using injected knowledge.", "motivation": "Current knowledge editing techniques in LLMs allow for knowledge injection but fail to propagate that knowledge for reasoning, especially in multi-hop questions.", "method": "PropMEND employs a hypernetwork to meta-learn modifications to gradients of language modeling losses, facilitating the propagation of injected knowledge.", "result": "The approach shows nearly 2x accuracy improvement on the RippleEdit dataset for multi-hop questions, demonstrating effective knowledge propagation.", "conclusion": "While PropMEND outperforms existing methods, the performance gap for unseen relations indicates that further work is needed in knowledge propagation.", "key_contributions": ["Introduction of a hypernetwork-based approach for knowledge propagation in LLMs", "Meta-learning gradient modifications to facilitate reasoning with injected knowledge", "Development of a new dataset, Controlled RippleEdit, for evaluating knowledge propagation"], "limitations": "Performance gap for unseen entity-relation pairs suggests the need for improvement in knowledge propagation.", "keywords": ["knowledge propagation", "large language models", "hypernetwork", "multi-hop questions", "gradient modification"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.08935", "pdf": "https://arxiv.org/pdf/2506.08935.pdf", "abs": "https://arxiv.org/abs/2506.08935", "title": "Can A Gamer Train A Mathematical Reasoning Model?", "authors": ["Andrew Shin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "While large language models (LLMs) have achieved remarkable performance in\nvarious tasks including mathematical reasoning, their development typically\ndemands prohibitive computational resources. Recent advancements have reduced\ncosts for training capable models, yet even these approaches rely on high-end\nhardware clusters. In this paper, we demonstrate that a single average gaming\nGPU can train a solid mathematical reasoning model, by integrating\nreinforcement learning and memory optimization techniques. Specifically, we\ntrain a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB\nmemory that achieves comparable or better performance on mathematical reasoning\nbenchmarks than models several times larger, in resource-constrained\nenvironments. Our results challenge the paradigm that state-of-the-art\nmathematical reasoning necessitates massive infrastructure, democratizing\naccess to high-performance AI research.\nhttps://github.com/shinandrew/YouronMath.", "AI": {"tldr": "This paper demonstrates how a single average gaming GPU can train a competitive mathematical reasoning model using reinforcement learning and memory optimization, challenging existing paradigms of high computational resource requirements.", "motivation": "To show that it is possible to train effective mathematical reasoning models without the need for expensive, high-end hardware clusters, thereby democratizing access to AI research.", "method": "The authors integrate reinforcement learning and memory optimization techniques to train a 1.5B parameter model on an RTX 3080 Ti GPU.", "result": "The model achieves comparable or better performance on mathematical reasoning benchmarks than much larger models, demonstrating efficacy in resource-constrained environments.", "conclusion": "The findings suggest that high-performance AI research can be accessible without massive infrastructure investments.", "key_contributions": ["Training a 1.5B parameter model on a single gaming GPU", "Integration of reinforcement learning and memory optimization techniques", "Achievement of competitive performance with reduced computational resources."], "limitations": "", "keywords": ["large language models", "mathematical reasoning", "reinforcement learning", "memory optimization", "democratization of AI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.08938", "pdf": "https://arxiv.org/pdf/2506.08938.pdf", "abs": "https://arxiv.org/abs/2506.08938", "title": "FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented Generation", "authors": ["Qinggang Zhang", "Zhishang Xiang", "Yilin Xiao", "Le Wang", "Junhui Li", "Xinrun Wang", "Jinsong Su"], "categories": ["cs.CL"], "comment": "Qinggang Zhang and Zhishang Xiang contributed equally to this work.\n  Corresponding author: Jinsong Su", "summary": "Large language models (LLMs) augmented with retrieval systems have\ndemonstrated significant potential in handling knowledge-intensive tasks.\nHowever, these models often struggle with unfaithfulness issues, generating\noutputs that either ignore the retrieved context or inconsistently blend it\nwith the LLM`s parametric knowledge. This issue is particularly severe in cases\nof knowledge conflict, where the retrieved context conflicts with the model`s\nparametric knowledge. While existing faithful RAG approaches enforce strict\ncontext adherence through well-designed prompts or modified decoding\nstrategies, our analysis reveals a critical limitation: they achieve\nfaithfulness by forcibly suppressing the model`s parametric knowledge, which\nundermines the model`s internal knowledge structure and increases the risk of\nmisinterpreting the context. To this end, this paper proposes FaithfulRAG, a\nnovel framework that resolves knowledge conflicts by explicitly modeling\ndiscrepancies between the model`s parametric knowledge and retrieved context.\nSpecifically, FaithfulRAG identifies conflicting knowledge at the fact level\nand designs a self-thinking process, allowing LLMs to reason about and\nintegrate conflicting facts before generating responses. Extensive experiments\ndemonstrate that our method outperforms state-of-the-art methods. The code is\navailable at https:// github.com/DeepLearnXMU/Faithful-RAG", "AI": {"tldr": "Proposes FaithfulRAG, a framework to resolve knowledge conflicts in large language models by modeling discrepancies between parametric knowledge and retrieved context.", "motivation": "LLMs struggle with unfaithfulness in outputs, especially when there are conflicts between retrieved context and parametric knowledge.", "method": "FaithfulRAG identifies fact-level conflicts and employs a self-thinking process for LLMs to reason about and integrate these conflicting facts before generating responses.", "result": "Extensive experiments show that FaithfulRAG outperforms existing state-of-the-art methods in maintaining faithfulness while leveraging both retrieved context and parametric knowledge.", "conclusion": "The proposed framework addresses the limitations of existing approaches by enabling LLMs to effectively manage conflicting information, enhancing response reliability.", "key_contributions": ["Introduction of FaithfulRAG framework for knowledge conflict resolution", "Identification of conflicting knowledge at the fact level", "Self-thinking process for reasoning about conflicting facts"], "limitations": "", "keywords": ["large language models", "retrieval-augmented generation", "knowledge conflict", "faithfulness", "self-thinking process"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.08952", "pdf": "https://arxiv.org/pdf/2506.08952.pdf", "abs": "https://arxiv.org/abs/2506.08952", "title": "Can LLMs Ground when they (Don't) Know: A Study on Direct and Loaded Political Questions", "authors": ["Clara Lachenmaier", "Judith Sieker", "Sina Zarrieß"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint accepted at ACL Main Conference 2025", "summary": "Communication among humans relies on conversational grounding, allowing\ninterlocutors to reach mutual understanding even when they do not have perfect\nknowledge and must resolve discrepancies in each other's beliefs. This paper\ninvestigates how large language models (LLMs) manage common ground in cases\nwhere they (don't) possess knowledge, focusing on facts in the political domain\nwhere the risk of misinformation and grounding failure is high. We examine the\nability of LLMs to answer direct knowledge questions and loaded questions that\npresuppose misinformation. We evaluate whether loaded questions lead LLMs to\nengage in active grounding and correct false user beliefs, in connection to\ntheir level of knowledge and their political bias. Our findings highlight\nsignificant challenges in LLMs' ability to engage in grounding and reject false\nuser beliefs, raising concerns about their role in mitigating misinformation in\npolitical discourse.", "AI": {"tldr": "The paper explores how large language models (LLMs) handle conversational grounding, particularly in the political domain, emphasizing challenges in correcting misinformation and engaging users effectively.", "motivation": "To understand how LLMs maintain common ground in conversations, especially concerning political misinformation and grounding failures.", "method": "The authors evaluate LLMs' responses to direct and loaded questions related to political facts, assessing their ability to correct misinformation and discern their knowledge and biases.", "result": "Results indicated that LLMs struggle with grounding and often fail to correct false beliefs, highlighting their limitations in addressing misinformation in political discussions.", "conclusion": "The findings raise important questions about the effectiveness of LLMs in managing misinformation and engaging users in political discourse.", "key_contributions": ["Analysis of LLM responses to loaded questions in political contexts", "Evaluation of LLMs' grounding abilities related to misinformation", "Insights into LLM biases and knowledge limitations in conversation"], "limitations": "The study is constrained by the specific political domain examined and the potential influence of model architecture on results.", "keywords": ["large language models", "conversational grounding", "misinformation", "political discourse", "active grounding"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.08966", "pdf": "https://arxiv.org/pdf/2506.08966.pdf", "abs": "https://arxiv.org/abs/2506.08966", "title": "Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers", "authors": ["Marek Kadlčík", "Michal Štefánik", "Timothee Mickus", "Michal Spiegel", "Josef Kuchař"], "categories": ["cs.CL", "cs.LG", "cs.NE"], "comment": null, "summary": "Pretrained language models (LMs) are prone to arithmetic errors. Existing\nwork showed limited success in probing numeric values from models'\nrepresentations, indicating that these errors can be attributed to the inherent\nunreliability of distributionally learned embeddings in representing exact\nquantities. However, we observe that previous probing methods are inadequate\nfor the emergent structure of learned number embeddings with sinusoidal\npatterns.\n  In response, we propose a novel probing technique that decodes numeric values\nfrom input embeddings with near-perfect accuracy across a range of open-source\nLMs. This proves that after the sole pre-training, LMs represent numbers with\nremarkable precision. Finally, we find that the embeddings' preciseness judged\nby our probe's accuracy explains a large portion of LM's errors in elementary\narithmetic, and show that aligning the embeddings with the pattern discovered\nby our probe can mitigate these errors.", "AI": {"tldr": "This paper introduces a new probing technique that accurately decodes numeric values from pretrained language models, addressing arithmetic errors linked to numeric embeddings.", "motivation": "To investigate and mitigate arithmetic errors in pretrained language models, which are mainly attributed to their unreliable numeric representations.", "method": "A novel probing technique is proposed that decodes numeric values from input embeddings, demonstrating high accuracy across various open-source language models.", "result": "The proposed probing method achieves near-perfect accuracy in decoding numeric values and reveals that alignment of numeric embeddings can reduce LM's arithmetic errors significantly.", "conclusion": "The accuracy of the embedding's representation of numeric values impacts the language models' arithmetic performance, suggesting implications for improving numeric accuracy in LMs.", "key_contributions": ["Introduction of a novel probing technique for decoding numeric values from embeddings", "Demonstration of high accuracy in numeric representation by language models", "Insights into reducing arithmetic errors through embedding alignment"], "limitations": "", "keywords": ["pretrained language models", "numeric values", "probing technique", "arithmetic errors", "embeddings"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.08972", "pdf": "https://arxiv.org/pdf/2506.08972.pdf", "abs": "https://arxiv.org/abs/2506.08972", "title": "Atomic-to-Compositional Generalization for Mobile Agents with A New Benchmark and Scheduling System", "authors": ["Yuan Guo", "Tingjia Miao", "Zheng Wu", "Pengzhou Cheng", "Ming Zhou", "Zhuosheng Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Autonomous agents powered by multimodal large language models have been\ndeveloped to facilitate task execution on mobile devices. However, prior work\nhas predominantly focused on atomic tasks -- such as shot-chain execution tasks\nand single-screen grounding tasks -- while overlooking the generalization to\ncompositional tasks, which are indispensable for real-world applications. This\nwork introduces UI-NEXUS, a comprehensive benchmark designed to evaluate mobile\nagents on three categories of compositional operations: Simple Concatenation,\nContext Transition, and Deep Dive. UI-NEXUS supports interactive evaluation in\n20 fully controllable local utility app environments, as well as 30 online\nChinese and English service apps. It comprises 100 interactive task templates\nwith an average optimal step count of 14.05. Experimental results across a\nrange of mobile agents with agentic workflow or agent-as-a-model show that\nUI-NEXUS presents significant challenges. Specifically, existing agents\ngenerally struggle to balance performance and efficiency, exhibiting\nrepresentative failure modes such as under-execution, over-execution, and\nattention drift, causing visible atomic-to-compositional generalization gap.\nInspired by these findings, we propose AGENT-NEXUS, a lightweight and efficient\nscheduling system to tackle compositional mobile tasks. AGENT-NEXUS\nextrapolates the abilities of existing mobile agents by dynamically decomposing\nlong-horizon tasks to a series of self-contained atomic subtasks. AGENT-NEXUS\nachieves 24% to 40% task success rate improvement for existing mobile agents on\ncompositional operation tasks within the UI-NEXUS benchmark without\nsignificantly sacrificing inference overhead. The demo video, dataset, and code\nare available on the project page at https://ui-nexus.github.io.", "AI": {"tldr": "This paper presents UI-NEXUS, a benchmark for evaluating autonomous agents on compositional tasks in mobile environments, and introduces AGENT-NEXUS, a scheduling system that enhances task execution.", "motivation": "To address the gap in evaluating mobile agents on compositional tasks which are crucial for real-world applications.", "method": "The UI-NEXUS benchmark evaluates agents on three categories of compositional tasks within 20 local app environments and 30 online apps, featuring 100 interactive task templates. AGENT-NEXUS dynamically decomposes long tasks into atomic subtasks for improved efficiency.", "result": "Experiments show that existing agents face challenges with performance and efficiency, often failing to execute tasks effectively, but AGENT-NEXUS improves success rates by 24% to 40% for compositional operations.", "conclusion": "AGENT-NEXUS provides a practical solution to enhance the performance of mobile agents in compositional task execution while maintaining efficiency.", "key_contributions": ["Introduction of the UI-NEXUS benchmark for evaluating compositional task performance of mobile agents.", "Development of AGENT-NEXUS, an efficient scheduling system for breaking down long tasks into manageable subtasks.", "Demonstrated significant improvement in task success rates for existing agents when using the AGENT-NEXUS system."], "limitations": "The generalization of findings to all types of mobile agents may be limited, and further testing in diverse environments is warranted.", "keywords": ["Autonomous agents", "Mobile tasks", "Large language models", "Task execution", "Benchmarking"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.08981", "pdf": "https://arxiv.org/pdf/2506.08981.pdf", "abs": "https://arxiv.org/abs/2506.08981", "title": "FROST-EMA: Finnish and Russian Oral Speech Dataset of Electromagnetic Articulography Measurements with L1, L2 and Imitated L2 Accents", "authors": ["Satu Hopponen", "Tomi Kinnunen", "Alexandre Nikolaev", "Rosa González Hautamäki", "Lauri Tavi", "Einar Meister"], "categories": ["cs.CL"], "comment": "Accepted in Interspeech 2025", "summary": "We introduce a new FROST-EMA (Finnish and Russian Oral Speech Dataset of\nElectromagnetic Articulography) corpus. It consists of 18 bilingual speakers,\nwho produced speech in their native language (L1), second language (L2), and\nimitated L2 (fake foreign accent). The new corpus enables research into\nlanguage variability from phonetic and technological points of view.\nAccordingly, we include two preliminary case studies to demonstrate both\nperspectives. The first case study explores the impact of L2 and imitated L2 on\nthe performance of an automatic speaker verification system, while the second\nillustrates the articulatory patterns of one speaker in L1, L2, and a fake\naccent.", "AI": {"tldr": "Introduction of FROST-EMA corpus for bilingual speech research.", "motivation": "To study language variability in bilingual speakers from phonetic and technological perspectives.", "method": "Introduction of a new corpus consisting of recordings from bilingual speakers producing speech in L1, L2, and imitated L2, with two case studies to illustrate findings.", "result": "The first case study assesses how L2 and imitated L2 impact automatic speaker verification, while the second analyzes articulatory patterns across L1, L2, and fake accent.", "conclusion": "The FROST-EMA corpus provides valuable insights into speech variability and can enhance speaker verification systems.", "key_contributions": ["Creation of a bilingual speech corpus (FROST-EMA)", "Demonstrated case studies on speaker verification and articulatory patterns", "Insights on language variability for HCI and ML applications."], "limitations": "", "keywords": ["Bilingual Corpus", "Electromagnetic Articulography", "Speaker Verification", "Language Variability", "Phonetic Analysis"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.08986", "pdf": "https://arxiv.org/pdf/2506.08986.pdf", "abs": "https://arxiv.org/abs/2506.08986", "title": "Naturalistic Language-related Movie-Watching fMRI Task for Detecting Neurocognitive Decline and Disorder", "authors": ["Yuejiao Wang", "Xianmin Gong", "Xixin Wu", "Patrick Wong", "Hoi-lam Helene Fung", "Man Wai Mak", "Helen Meng"], "categories": ["cs.CL"], "comment": "5 pages,3 figures, accepted by ISCSLP 2024", "summary": "Early detection is crucial for timely intervention aimed at preventing and\nslowing the progression of neurocognitive disorder (NCD), a common and\nsignificant health problem among the aging population. Recent evidence has\nsuggested that language-related functional magnetic resonance imaging (fMRI)\nmay be a promising approach for detecting cognitive decline and early NCD. In\nthis paper, we proposed a novel, naturalistic language-related fMRI task for\nthis purpose. We examined the effectiveness of this task among 97 non-demented\nChinese older adults from Hong Kong. The results showed that machine-learning\nclassification models based on fMRI features extracted from the task and\ndemographics (age, gender, and education year) achieved an average area under\nthe curve of 0.86 when classifying participants' cognitive status (labeled as\nNORMAL vs DECLINE based on their scores on a standard neurcognitive test).\nFeature localization revealed that the fMRI features most frequently selected\nby the data-driven approach came primarily from brain regions associated with\nlanguage processing, such as the superior temporal gyrus, middle temporal\ngyrus, and right cerebellum. The study demonstrated the potential of the\nnaturalistic language-related fMRI task for early detection of aging-related\ncognitive decline and NCD.", "AI": {"tldr": "This paper introduces a naturalistic language-related fMRI task that leverages machine learning to detect cognitive decline in aging populations, showing promising results with an average AUC of 0.86.", "motivation": "The need for early detection of neurocognitive disorders (NCD) among the aging population to enable timely intervention.", "method": "A novel fMRI task was created, and its effectiveness was evaluated using machine learning classification models on data from 97 non-demented older adults.", "result": "The classification models achieved an average area under the curve of 0.86 in distinguishing between NORMAL and DECLINE cognitive statuses based on fMRI features and demographics.", "conclusion": "The study indicates the potential of the language-related fMRI task in early detection of cognitive decline and neurocognitive disorders in older adults.", "key_contributions": ["Development of a naturalistic fMRI task for cognitive assessment", "High classification accuracy for cognitive status using machine learning", "Identification of key brain regions involved in language processing related to cognitive decline"], "limitations": "", "keywords": ["neurocognitive disorder", "fMRI", "machine learning", "cognitive decline", "aging"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.08999", "pdf": "https://arxiv.org/pdf/2506.08999.pdf", "abs": "https://arxiv.org/abs/2506.08999", "title": "Employing self-supervised learning models for cross-linguistic child speech maturity classification", "authors": ["Theo Zhang", "Madurya Suresh", "Anne S. Warlaumont", "Kasia Hitczenko", "Alejandrina Cristia", "Margaret Cychosz"], "categories": ["cs.CL", "cs.AI"], "comment": "To be published in Interspeech 2025. 5 pages, 2 figures. For\n  associated Github repository, see\n  https://github.com/spoglab-stanford/w2v2-pro-sm/tree/main/speechbrain/recipes/W2V2-LL4300-Pro-SM", "summary": "Speech technology systems struggle with many downstream tasks for child\nspeech due to small training corpora and the difficulties that child speech\npose. We apply a novel dataset, SpeechMaturity, to state-of-the-art transformer\nmodels to address a fundamental classification task: identifying child\nvocalizations. Unlike previous corpora, our dataset captures maximally\necologically-valid child vocalizations across an unprecedented sample,\ncomprising children acquiring 25+ languages in the U.S., Bolivia, Vanuatu,\nPapua New Guinea, Solomon Islands, and France. The dataset contains 242,004\nlabeled vocalizations, magnitudes larger than previous work. Models were\ntrained to distinguish between cry, laughter, mature (consonant+vowel), and\nimmature speech (just consonant or vowel). Models trained on the dataset\noutperform state-of-the-art models trained on previous datasets, achieved\nclassification accuracy comparable to humans, and were robust across rural and\nurban settings.", "AI": {"tldr": "This paper presents a novel dataset, SpeechMaturity, addressing child vocalization classification using advanced transformer models.", "motivation": "To improve speech technology systems' performance on child speech, which is hindered by limited training data and the unique challenges posed by children's vocalizations.", "method": "The paper introduces the SpeechMaturity dataset comprising 242,004 labeled child vocalizations from diverse linguistic backgrounds, and applies transformer models to classify these vocalizations into categories such as cry, laughter, and speech maturity levels.", "result": "Models trained on the SpeechMaturity dataset significantly outperform previous state-of-the-art models, achieving human-comparable classification accuracy across various settings.", "conclusion": "The SpeechMaturity dataset offers a substantial improvement in classifying child vocalizations, paving the way for advancements in speech technology for children.", "key_contributions": ["Introduction of the SpeechMaturity dataset with 242,004 labeled vocalizations", "Demonstration of robust transformer model performance on child speech classification", "Establishment of human-comparable accuracy in distinguishing vocalization types."], "limitations": "", "keywords": ["child speech", "speech technology", "transformer models", "vocalization classification", "SpeechMaturity"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2506.09003", "pdf": "https://arxiv.org/pdf/2506.09003.pdf", "abs": "https://arxiv.org/abs/2506.09003", "title": "SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner", "authors": ["Lei Zhang", "Jiaxi Yang", "Min Yang", "Jian Yang", "Mouxiang Chen", "Jiajun Zhang", "Zeyu Cui", "Binyuan Hui", "Junyang Lin"], "categories": ["cs.CL"], "comment": "Accepted by ICML2025", "summary": "We introduce **SWE-Flow**, a novel data synthesis framework grounded in\nTest-Driven Development (TDD). Unlike existing software engineering data that\nrely on human-submitted issues, **SWE-Flow** automatically infers incremental\ndevelopment steps directly from unit tests, which inherently encapsulate\nhigh-level requirements. The core of **SWE-Flow** is the construction of a\nRuntime Dependency Graph (RDG), which precisely captures function interactions,\nenabling the generation of a structured, step-by-step *development schedule*.\nAt each step, **SWE-Flow** produces a partial codebase, the corresponding unit\ntests, and the necessary code modifications, resulting in fully verifiable TDD\ntasks. With this approach, we generated 16,061 training instances and 2,020\ntest instances from real-world GitHub projects, creating the **SWE-Flow-Eval**\nbenchmark. Our experiments show that fine-tuning open model on this dataset\nsignificantly improves performance in TDD-based coding. To facilitate further\nresearch, we release all code, datasets, models, and Docker images at\n[Github](https://github.com/Hambaobao/SWE-Flow).", "AI": {"tldr": "Introducing SWE-Flow, a framework for automated data synthesis based on Test-Driven Development (TDD) that infers development steps from unit tests.", "motivation": "To improve the generation of software engineering data, moving away from human-submitted issues and towards automation-driven processes based on unit tests.", "method": "Construction of a Runtime Dependency Graph (RDG) to capture function interactions, enabling structured development schedules and task generation for TDD.", "result": "Generated 16,061 training and 2,020 test instances from real-world projects, leading to the SWE-Flow-Eval benchmark; showed performance improvements with fine-tuning on the dataset.", "conclusion": "SWE-Flow successfully automates TDD tasks and improves performance in coding through its innovative framework, with resources available for further research.", "key_contributions": ["Automatic inference of development steps from unit tests", "Creation of SWE-Flow-Eval benchmark from real-world projects", "Doubling the efficiency of coding tasks in a TDD environment"], "limitations": "", "keywords": ["Test-Driven Development", "data synthesis", "software engineering"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.09009", "pdf": "https://arxiv.org/pdf/2506.09009.pdf", "abs": "https://arxiv.org/abs/2506.09009", "title": "UD-KSL Treebank v1.3: A semi-automated framework for aligning XPOS-extracted units with UPOS tags", "authors": ["Hakyung Sung", "Gyu-Ho Shin", "Chanyoung Lee", "You Kyung Sung", "Boo Kyung Jung"], "categories": ["cs.CL"], "comment": null, "summary": "The present study extends recent work on Universal Dependencies annotations\nfor second-language (L2) Korean by introducing a semi-automated framework that\nidentifies morphosyntactic constructions from XPOS sequences and aligns those\nconstructions with corresponding UPOS categories. We also broaden the existing\nL2-Korean corpus by annotating 2,998 new sentences from argumentative essays.\nTo evaluate the impact of XPOS-UPOS alignments, we fine-tune L2-Korean\nmorphosyntactic analysis models on datasets both with and without these\nalignments, using two NLP toolkits. Our results indicate that the aligned\ndataset not only improves consistency across annotation layers but also\nenhances morphosyntactic tagging and dependency-parsing accuracy, particularly\nin cases of limited annotated data.", "AI": {"tldr": "The study presents a semi-automated framework for improving morphosyntactic analysis in L2 Korean through XPOS-UPOS alignment and new sentence annotations.", "motivation": "To enhance the accuracy and consistency of morphosyntactic analysis for second-language Korean by addressing limitations in existing datasets and tools.", "method": "Developed a semi-automated framework to align morphosyntactic constructions with UPOS categories and annotated a new set of sentences from argumentative essays to enrich the corpus.", "result": "Aligned datasets show improved consistency across annotation layers and higher accuracy in morphosyntactic tagging and dependency parsing, especially with limited data.", "conclusion": "The study confirms that XPOS-UPOS alignments significantly benefit the morphosyntactic models, paving the way for better analysis in the realm of L2 Korean.", "key_contributions": ["Introduction of a semi-automated framework for aligning morphosyntactic constructions", "Expansion of the L2-Korean corpus with new sentence annotations", "Demonstration of improved model performance through empirical evaluation"], "limitations": "", "keywords": ["Universal Dependencies", "L2 Korean", "morphosyntactic analysis", "XPOS", "UPOS"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.09014", "pdf": "https://arxiv.org/pdf/2506.09014.pdf", "abs": "https://arxiv.org/abs/2506.09014", "title": "Learning to Reason Across Parallel Samples for LLM Reasoning", "authors": ["Jianing Qi", "Xi Ye", "Hao Tang", "Zhigang Zhu", "Eunsol Choi"], "categories": ["cs.CL"], "comment": null, "summary": "Scaling test-time compute brings substantial performance gains for large\nlanguage models (LLMs). By sampling multiple answers and heuristically\naggregate their answers (e.g., either through majority voting or using\nverifiers to rank the answers), one can achieve consistent performance gains in\nmath domains. In this paper, we propose a new way to leverage such multiple\nsample set. We train a compact LLM, called Sample Set Aggregator (SSA), that\ntakes a concatenated sequence of multiple samples and output the final answer,\noptimizing it for the answer accuracy with reinforcement learning. Experiments\non multiple reasoning datasets show that SSA outperforms other test-time\nscaling methods such as reward model-based re-ranking. Our approach also shows\na promising generalization ability, across sample set sizes, base model\nfamilies and scales, and tasks. By separating LLMs to generate answers and LLMs\nto analyze and aggregate sampled answers, our approach can work with the\noutputs from premier black box models easily and efficiently.", "AI": {"tldr": "This paper introduces the Sample Set Aggregator (SSA), a compact LLM that enhances the accuracy of answers by aggregating multiple samples with reinforcement learning techniques.", "motivation": "To improve the performance of large language models at test-time by utilizing multiple sample answers for better accuracy.", "method": "The authors trained the Sample Set Aggregator (SSA) LLM on concatenated sequences of multiple answer samples, optimizing it for answer accuracy using reinforcement learning.", "result": "SSA demonstrated superior performance on multiple reasoning datasets compared to existing test-time scaling methods such as reward model-based re-ranking.", "conclusion": "The SSA can efficiently aggregate outputs from various black box models and shows strong generalization across different model families and tasks.", "key_contributions": ["Introduction of the Sample Set Aggregator (SSA) for LLM answer aggregation.", "Optimization of answer accuracy through reinforcement learning on multiple samples.", "Demonstration of improved performance across various reasoning tasks and models."], "limitations": "", "keywords": ["sample set aggregation", "large language models", "reinforcement learning", "answer accuracy", "reasoning tasks"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.09021", "pdf": "https://arxiv.org/pdf/2506.09021.pdf", "abs": "https://arxiv.org/abs/2506.09021", "title": "Comparing human and LLM proofreading in L2 writing: Impact on lexical and syntactic features", "authors": ["Hakyung Sung", "Karla Csuros", "Min-Chang Sung"], "categories": ["cs.CL"], "comment": null, "summary": "This study examines the lexical and syntactic interventions of human and LLM\nproofreading aimed at improving overall intelligibility in identical second\nlanguage writings, and evaluates the consistency of outcomes across three LLMs\n(ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b). Findings show that both human and\nLLM proofreading enhance bigram lexical features, which may contribute to\nbetter coherence and contextual connectedness between adjacent words. However,\nLLM proofreading exhibits a more generative approach, extensively reworking\nvocabulary and sentence structures, such as employing more diverse and\nsophisticated vocabulary and incorporating a greater number of adjective\nmodifiers in noun phrases. The proofreading outcomes are highly consistent in\nmajor lexical and syntactic features across the three models.", "AI": {"tldr": "The study evaluates the effectiveness of human and LLM proofreading on second language writings, revealing enhanced intelligibility and consistency across multiple LLMs.", "motivation": "To understand how human and LLM proofreading can improve the intelligibility of second language writings.", "method": "The study involved comparing the proofreading effects of human annotators and three LLMs (ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b) on identical second language texts, focusing on lexical and syntactic features.", "result": "Both human and LLM proofreading improved bigram lexical features, leading to better coherence, but LLMs displayed a more generative approach with diverse vocabulary and enhanced sentence structures.", "conclusion": "LLM proofreading provides significant enhancements in lexical and syntactic features with high consistency across models, benefiting second language writing.", "key_contributions": ["Demonstrated effectiveness of LLMs in proofreading second language texts", "Identified differences in approaches between human and LLM proofreading", "Provided insights into the consistency of outcomes across multiple LLMs"], "limitations": "", "keywords": ["proofreading", "second language", "LLM", "human-Computer interaction", "lexical features"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.09033", "pdf": "https://arxiv.org/pdf/2506.09033.pdf", "abs": "https://arxiv.org/abs/2506.09033", "title": "Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning", "authors": ["Haozhen Zhang", "Tao Feng", "Jiaxuan You"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Code is available at https://github.com/ulab-uiuc/Router-R1", "summary": "The rapid emergence of diverse large language models (LLMs) has spurred the\ndevelopment of LLM routers that assign user queries to the most suitable model.\nHowever, existing LLM routers typically perform a single-round, one-to-one\nmapping (\\textit{i.e.}, assigning each query to a single model in isolation),\nwhich limits their capability to tackle complex tasks that demand the\ncomplementary strengths of multiple LLMs. In this paper, we present\n\\textbf{Router-R1}, a reinforcement learning (RL)-based framework that\nformulates multi-LLM routing and aggregation as a sequential decision process.\nRouter-R1 instantiates the router itself as a capable LLM, leveraging its\nreasoning ability to interleave \"think\" actions (internal deliberation) with\n\"route\" actions (dynamic model invocation), and integrates each response into\nits evolving context. To guide learning, we employ a lightweight rule-based\nreward comprising format rewards, final outcome rewards, and a novel cost\nreward for performance and cost trade-off optimization, opening a pathway\ntoward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions\nonly on simple model descriptors such as pricing, latency, and example\nperformance, enabling strong generalization to unseen model selection.\nExperiments on seven general and multi-hop QA benchmarks show that Router-R1\noutperforms over several strong baselines, achieving superior performance while\nmaintaining robust generalization and cost management.Code is available at\nhttps://github.com/ulab-uiuc/Router-R1.", "AI": {"tldr": "Router-R1 is a reinforcement learning framework for multi-LLM routing and aggregation, optimizing performance-cost tradeoffs for complex tasks.", "motivation": "The current limitations of single-round, one-to-one LLM routers hinder their ability to address complex tasks that require the strengths of multiple LLMs.", "method": "Router-R1 formulates multi-LLM routing as a sequential decision process by integrating both 'think' and 'route' actions, supported by a rule-based reward system for optimizing outcomes and costs.", "result": "Router-R1 demonstrates superior performance on seven general and multi-hop QA benchmarks compared to several strong baselines, showing robust generalization and effective cost management.", "conclusion": "Router-R1 effectively combines the reasoning capabilities of LLMs with strategic routing decisions, paving the way for improved multi-LLM applications in AI.", "key_contributions": ["Introduces a reinforcement learning-based framework for multi-LLM routing.", "Incorporates internal deliberation with dynamic model invocation.", "Optimizes performance and cost trade-offs through a novel reward system."], "limitations": "", "keywords": ["large language models", "reinforcement learning", "multi-LLM routing", "performance-cost optimization", "QA benchmarks"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.09047", "pdf": "https://arxiv.org/pdf/2506.09047.pdf", "abs": "https://arxiv.org/abs/2506.09047", "title": "Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs", "authors": ["Yaniv Nikankin", "Dana Arad", "Yossi Gandelsman", "Yonatan Belinkov"], "categories": ["cs.CL", "68T5", "I.2.7"], "comment": null, "summary": "Vision-Language models (VLMs) show impressive abilities to answer questions\non visual inputs (e.g., counting objects in an image), yet demonstrate higher\naccuracies when performing an analogous task on text (e.g., counting words in a\ntext). We investigate this accuracy gap by identifying and comparing the\n\\textit{circuits} - the task-specific computational sub-graphs - in different\nmodalities. We show that while circuits are largely disjoint between\nmodalities, they implement relatively similar functionalities: the differences\nlie primarily in processing modality-specific data positions (an image or a\ntext sequence). Zooming in on the image data representations, we observe they\nbecome aligned with the higher-performing analogous textual representations\nonly towards later layers, too late in processing to effectively influence\nsubsequent positions. To overcome this, we patch the representations of visual\ndata tokens from later layers back into earlier layers. In experiments with\nmultiple tasks and models, this simple intervention closes a third of the\nperformance gap between the modalities, on average. Our analysis sheds light on\nthe multi-modal performance gap in VLMs and suggests a training-free approach\nfor reducing it.", "AI": {"tldr": "This paper examines the performance gap between Vision-Language models (VLMs) on visual inputs compared to text inputs, identifying differences in their computational circuits and proposing a method to improve visual data representation.", "motivation": "To understand and address the accuracy gap between Vision-Language models when processing visual versus textual inputs.", "method": "The study analyzes task-specific computational sub-graphs in VLMs across different modalities and introduces a method to augment visual data representations from later layers into earlier ones.", "result": "The proposed intervention reduces the performance gap between visual and text modalities by closing a third of the accuracy deficit, as evidenced by experiments across multiple tasks and models.", "conclusion": "This research reveals significant insights into VLMs' multi-modal performance and offers a simple, training-free solution to enhance visual input processing.", "key_contributions": ["Proposed a method to improve visual data representation in VLMs", "Identified and compared task-specific computational circuits in different modalities", "Highlighted the misalignment of image data representations with textual representations in initial processing layers"], "limitations": "", "keywords": ["Vision-Language models", "multi-modal processing", "data representation", "visual inputs", "text inputs"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2206.05446", "pdf": "https://arxiv.org/pdf/2206.05446.pdf", "abs": "https://arxiv.org/abs/2206.05446", "title": "A Decomposition-Based Approach for Evaluating and Analyzing Inter-Annotator Disagreement", "authors": ["Effi Levi", "Shaul R. Shenhav"], "categories": ["cs.CL"], "comment": null, "summary": "We propose a novel method to conceptually decompose an existing annotation\ninto separate levels, allowing the analysis of inter-annotators disagreement in\neach level separately. We suggest two distinct strategies in order to actualize\nthis approach: a theoretically-driven one, in which the researcher defines a\ndecomposition based on prior knowledge of the annotation task, and an\nexploration-based one, in which many possible decompositions are inductively\ncomputed and presented to the researcher for interpretation and evaluation.\nUtilizing a recently constructed dataset for narrative analysis as our\nuse-case, we apply each of the two strategies to demonstrate the potential of\nour approach in testing hypotheses regarding the sources of annotation\ndisagreements, as well as revealing latent structures and relations within the\nannotation task. We conclude by suggesting how to extend and generalize our\napproach, as well as use it for other purposes.", "AI": {"tldr": "A method is proposed for decomposing annotations into separate levels to analyze inter-annotators disagreement.", "motivation": "To better understand and analyze inter-annotators disagreement by decomposing existing annotations into levels.", "method": "The paper presents two strategies: a theoretically-driven approach based on prior knowledge and an exploration-based approach that computes possible decompositions inductively.", "result": "Application of the method on a narrative analysis dataset demonstrated its potential in testing hypotheses about annotation disagreements and revealing underlying structures.", "conclusion": "The approach can be extended for more generalized use and applied to various purposes beyond the presented case.", "key_contributions": ["Novel method for annotation decomposition", "Two distinct strategies for achieving decomposition", "Application demonstrated on narrative analysis dataset"], "limitations": "The scalability of the approach and its application to different types of annotations are not extensively tested.", "keywords": ["annotation", "inter-annotator disagreement", "decomposition", "narrative analysis", "research methodology"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2302.14502", "pdf": "https://arxiv.org/pdf/2302.14502.pdf", "abs": "https://arxiv.org/abs/2302.14502", "title": "A Survey on Long Text Modeling with Transformers", "authors": ["Zican Dong", "Tianyi Tang", "Junyi Li", "Wayne Xin Zhao"], "categories": ["cs.CL"], "comment": null, "summary": "Modeling long texts has been an essential technique in the field of natural\nlanguage processing (NLP). With the ever-growing number of long documents, it\nis important to develop effective modeling methods that can process and analyze\nsuch texts. However, long texts pose important research challenges for existing\ntext models, with more complex semantics and special characteristics. In this\npaper, we provide an overview of the recent advances on long texts modeling\nbased on Transformer models. Firstly, we introduce the formal definition of\nlong text modeling. Then, as the core content, we discuss how to process long\ninput to satisfy the length limitation and design improved Transformer\narchitectures to effectively extend the maximum context length. Following this,\nwe discuss how to adapt Transformer models to capture the special\ncharacteristics of long texts. Finally, we describe four typical applications\ninvolving long text modeling and conclude this paper with a discussion of\nfuture directions. Our survey intends to provide researchers with a synthesis\nand pointer to related work on long text modeling.", "AI": {"tldr": "Overview of recent advances in long text modeling using Transformer models, addressing challenges and applications.", "motivation": "To develop effective modeling methods for processing and analyzing long texts in NLP due to their complex semantics and characteristics.", "method": "Provide an overview of long text modeling, discuss processing methods and improved Transformer architectures to extend maximum context length, and adapt Transformer models for special characteristics of long texts.", "result": "Identified key methods for effectively processing long texts and adapting Transformer architectures, with applications discussed.", "conclusion": "The paper concludes with suggestions for future research directions in long text modeling.", "key_contributions": ["Definition of long text modeling", "Review of methods to extend context length in Transformer models", "Identification of applications for long text modeling"], "limitations": "", "keywords": ["long text modeling", "Transformer models", "NLP", "context length", "applications"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2310.16937", "pdf": "https://arxiv.org/pdf/2310.16937.pdf", "abs": "https://arxiv.org/abs/2310.16937", "title": "Cross-lingual Transfer in Programming Languages: An Extensive Empirical Study", "authors": ["Razan Baltaji", "Saurabh Pujar", "Louis Mandel", "Martin Hirzel", "Luca Buratti", "Lav Varshney"], "categories": ["cs.CL", "I.2.7; I.2.5"], "comment": "Published in Transactions on Machine Learning Research (06/2025) 26\n  pages, 5 figures, 10 tables", "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious software engineering tasks, including error detection, clone detection,\nand code translation, primarily leveraging high-resource programming languages\nlike Python and Java. However, many critical languages, such as COBOL, as well\nas emerging languages, such as Rust and Swift, remain low-resource due to\nlimited openly available code. This scarcity hampers the training and\neffectiveness of LLMs for these languages, increasing software maintenance\ncosts and stifling innovation. Addressing this gap, we investigate the\npotential of transfer learning to enhance LLM performance on low-resource\nprogramming languages by leveraging data from high-resource counterparts. Our\nextensive empirical study evaluates transferability across 10 to 41 programming\nlanguages and five key tasks: code generation, clone detection, code repair,\nsolution domain classification, and error detection. Additionally, we develop a\nperformance prediction model to guess the best source languages for a given\ntarget and task, and analyze the features that influence transfer performance.\nWe further replicate a representative subset of experiments with a larger model\nto test the generalizability of our conclusions to contemporary large-scale\nLLMs. Our findings demonstrate that cross-lingual transfer significantly\noutperforms zero-shot learning, with effectiveness varying based on both source\nand target languages. Furthermore, our model reliably predicts successful\ntransfer sources by considering linguistic and dataset-specific features,\noffering practical guidance for data acquisition and model training. This work\ncontributes to the development of LLM-driven tools for low-resource programming\nlanguages and provides insights into the characteristics that facilitate\ntransfer across language pairs.", "AI": {"tldr": "This paper explores enhancing large language models (LLMs) performance on low-resource programming languages using transfer learning from high-resource languages, finding significant improvements through cross-lingual transfer.", "motivation": "To address the scarcity of data for low-resource programming languages that hampers LLM performance and increases software maintenance costs.", "method": "An empirical study assessing transferability across 10 to 41 programming languages and five key tasks, including code generation and error detection, supplemented by a performance prediction model.", "result": "Cross-lingual transfer significantly outperforms zero-shot learning, with variation in effectiveness depending on source and target languages.", "conclusion": "The study provides practical guidance for data acquisition and model training, showing that careful selection of source languages can enhance transfer performance.", "key_contributions": ["Demonstrated effectiveness of cross-lingual transfer over zero-shot learning for LLMs on low-resource languages.", "Developed a performance prediction model for choosing source languages based on various linguistic features.", "Provided insights into the features that influence transfer performance across different programming languages."], "limitations": "The results may vary for unexamined languages or tasks not included in the study.", "keywords": ["large language models", "transfer learning", "low-resource programming languages", "software engineering", "code generation"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2404.01856", "pdf": "https://arxiv.org/pdf/2404.01856.pdf", "abs": "https://arxiv.org/abs/2404.01856", "title": "Poro 34B and the Blessing of Multilinguality", "authors": ["Risto Luukkonen", "Jonathan Burdge", "Elaine Zosa", "Aarne Talman", "Ville Komulainen", "Väinö Hatanpää", "Peter Sarlin", "Sampo Pyysalo"], "categories": ["cs.CL"], "comment": null, "summary": "The pretraining of state-of-the-art large language models now requires\ntrillions of words of text, which is orders of magnitude more than available\nfor the vast majority of languages. While including text in more than one\nlanguage is an obvious way to acquire more pretraining data, multilinguality is\noften seen as a curse, and most model training efforts continue to focus\nnear-exclusively on individual large languages. We believe that multilinguality\ncan be a blessing: when the lack of training data is a constraint for\neffectively training larger models for a target language, augmenting the\ndataset with other languages can offer a way to improve over the capabilities\nof monolingual models for that language. In this study, we introduce Poro 34B,\na 34 billion parameter model trained for 1 trillion tokens of Finnish, English,\nand programming languages, and demonstrate that a multilingual training\napproach can produce a model that substantially advances over the capabilities\nof existing models for Finnish and excels in translation, while also achieving\ncompetitive performance in its class for English and programming languages. We\nrelease the model parameters, scripts, and data under open licenses at\nhttps://huggingface.co/LumiOpen/Poro-34B.", "AI": {"tldr": "This paper introduces Poro 34B, a multilingual model that improves upon monolingual models for Finnish through the inclusion of multiple languages in its training dataset.", "motivation": "The need for more training data for languages with limited resources, like Finnish, can be fulfilled by employing multilingual training strategies instead of focusing solely on individual large languages.", "method": "Poro 34B is a 34 billion parameter model trained on 1 trillion tokens across Finnish, English, and programming languages, demonstrating a multilingual training approach.", "result": "Poro 34B significantly improves the capabilities for Finnish, excels in translation tasks, and performs competitively in English and programming languages.", "conclusion": "Multilingual training can effectively overcome data scarcity for less-resourced languages, leading to better model performance compared to monolingual approaches.", "key_contributions": ["Introduction of Poro 34B model with multilingual training", "Demonstration of improved capabilities for Finnish through multilingualism", "Release of model parameters and training data under open licenses"], "limitations": "", "keywords": ["multilinguality", "language models", "Finnish", "machine learning", "open source"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2406.12548", "pdf": "https://arxiv.org/pdf/2406.12548.pdf", "abs": "https://arxiv.org/abs/2406.12548", "title": "P-React: Synthesizing Topic-Adaptive Reactions of Personality Traits via Mixture of Specialized LoRA Experts", "authors": ["Yuhao Dan", "Jie Zhou", "Qin Chen", "Junfeng Tian", "Liang He"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "Personalized large language models (LLMs) have attracted great attention in\nmany applications, such as emotional support and role-playing. However,\nexisting works primarily focus on modeling explicit character profiles, while\nignoring the underlying personality traits that truly shape behaviors and\ndecision-making, hampering the development of more anthropomorphic and\npsychologically-grounded AI systems. In this paper, we explore the modeling of\nBig Five personality traits, which is the most widely used trait theory in\npsychology, and propose P-React, a mixture of experts (MoE)-based personalized\nLLM. Particularly, we integrate a Personality Specialization Loss (PSL) to\nbetter capture individual trait expressions, providing a more nuanced and\npsychologically grounded personality simulacrum. To facilitate research in this\nfield, we curate OCEAN-Chat, a high-quality, human-verified dataset designed to\ntrain LLMs in expressing personality traits across diverse topics. Extensive\nexperiments demonstrate the effectiveness of P-React in maintaining consistent\nand real personality.", "AI": {"tldr": "This paper introduces P-React, a personalized large language model that incorporates Big Five personality traits to enhance the realism and psychological grounding of AI systems in applications like emotional support.", "motivation": "The work aims to improve the anthropomorphism and decision-making reflection in personalized large language models by addressing the underlying personality traits that shape behavior, which have been overlooked in existing research.", "method": "The authors propose a mixture of experts (MoE)-based model called P-React, which integrates a Personality Specialization Loss (PSL) to better capture individual trait expressions of users.", "result": "Experiments show that P-React effectively maintains a consistent and authentic representation of personality traits in responses, outperforming existing models that do not account for these traits.", "conclusion": "The proposed P-React model and the OCEAN-Chat dataset together provide a significant step towards creating more nuanced AI systems that can better simulate human-like personality expressions.", "key_contributions": ["Introduction of P-React, a novel MoE-based personalized LLM.", "Integration of Personality Specialization Loss (PSL) for better trait expression.", "Creation of the OCEAN-Chat dataset for training LLMs with personality traits."], "limitations": "The model's effectiveness may vary across different personality dimensions and cultural contexts, which requires further investigation.", "keywords": ["Personalized LLMs", "Big Five personality traits", "Mixture of experts", "Personality Specialization Loss", "OCEAN-Chat"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2406.19593", "pdf": "https://arxiv.org/pdf/2406.19593.pdf", "abs": "https://arxiv.org/abs/2406.19593", "title": "SK-VQA: Synthetic Knowledge Generation at Scale for Training Context-Augmented Multimodal LLMs", "authors": ["Xin Su", "Man Luo", "Kris W Pan", "Tien Pei Chou", "Vasudev Lal", "Phillip Howard"], "categories": ["cs.CL", "cs.CV"], "comment": "ICML 2025 Spotlight Oral", "summary": "Multimodal retrieval augmented generation (RAG) plays a crucial role in\ndomains such as knowledge-based visual question answering (KB-VQA), where\nexternal knowledge is needed to answer a question. However, existing multimodal\nLLMs (MLLMs) are not designed for context-augmented generation, limiting their\neffectiveness in such tasks. While synthetic data generation has recently\ngained attention for training MLLMs, its application for context-augmented\ngeneration remains underexplored. To address this gap, we introduce SK-VQA, a\nlarge-scale synthetic multimodal dataset containing over 2 million visual\nquestion-answer pairs, each associated with context documents containing\ninformation necessary to determine the final answer. Compared to previous\ndatasets, SK-VQA contains 11x more unique questions, exhibits greater domain\ndiversity, and covers a broader spectrum of image sources. Through human\nevaluations, we confirm the high quality of the generated question-answer pairs\nand their contextual relevance. Extensive experiments show that SK-VQA serves\nboth as a challenging KB-VQA benchmark and as an effective training resource\nfor adapting MLLMs to context-augmented generation. Our results further\nindicate that models trained on SK-VQA demonstrate enhanced generalization in\nboth context-aware VQA and multimodal RAG settings. SK-VQA is publicly\navailable via Hugging Face Hub.", "AI": {"tldr": "Introduction of SK-VQA, a large-scale synthetic multimodal dataset for knowledge-based visual question answering, enhancing multimodal LLMs with context-augmented generation.", "motivation": "To improve the effectiveness of multimodal LLMs in context-augmented generation tasks, particularly in knowledge-based visual question answering.", "method": "Development of the SK-VQA dataset containing over 2 million visual question-answer pairs and context documents, followed by human evaluations and extensive experiments on model training.", "result": "Models trained on SK-VQA showcased improved performance in context-aware VQA and multimodal retrieval augmented generation settings, confirming the dataset's value as a benchmark and training resource.", "conclusion": "SK-VQA enhances the capabilities of MLLMs in context-augmented scenarios and is available for public use, promising better generalization in VQA tasks.", "key_contributions": ["Introduction of a synthetic dataset (SK-VQA) with 2 million+ visual question-answer pairs", "Demonstration of high-quality and contextual relevance through human evaluations", "Proven effectiveness in training models for improved generalization in context-aware tasks"], "limitations": "", "keywords": ["Multimodal retrieval", "Visual question answering", "Synthetic datasets", "Context-augmented generation", "Knowledge-based VQA"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2408.01214", "pdf": "https://arxiv.org/pdf/2408.01214.pdf", "abs": "https://arxiv.org/abs/2408.01214", "title": "High-Throughput Phenotyping of Clinical Text Using Large Language Models", "authors": ["Daniel B. Hier", "S. Ilyas Munzir", "Anne Stahlfeld", "Tayo Obafemi-Ajayi", "Michael D. Carrithers"], "categories": ["cs.CL", "cs.AI", "I.7; I.2"], "comment": "Submitted to IEEE-EMBS International Conference on Biomedical and\n  Health Informatics, Houston TX", "summary": "High-throughput phenotyping automates the mapping of patient signs to\nstandardized ontology concepts and is essential for precision medicine. This\nstudy evaluates the automation of phenotyping of clinical summaries from the\nOnline Mendelian Inheritance in Man (OMIM) database using large language\nmodels. Due to their rich phenotype data, these summaries can be surrogates for\nphysician notes. We conduct a performance comparison of GPT-4 and\nGPT-3.5-Turbo. Our results indicate that GPT-4 surpasses GPT-3.5-Turbo in\nidentifying, categorizing, and normalizing signs, achieving concordance with\nmanual annotators comparable to inter-rater agreement. Despite some limitations\nin sign normalization, the extensive pre-training of GPT-4 results in high\nperformance and generalizability across several phenotyping tasks while\nobviating the need for manually annotated training data. Large language models\nare expected to be the dominant method for automating high-throughput\nphenotyping of clinical text.", "AI": {"tldr": "This study evaluates the use of large language models (LLMs) for automating phenotyping of clinical summaries to enhance precision medicine.", "motivation": "To improve precision medicine by automating the mapping of patient signs to standardized ontology concepts using high-throughput phenotyping.", "method": "Performance comparison of GPT-4 and GPT-3.5-Turbo on clinical summaries from the OMIM database, focusing on their ability to identify, categorize, and normalize signs.", "result": "GPT-4 significantly outperforms GPT-3.5-Turbo in identifying, categorizing, and normalizing clinical signs, achieving high agreement with manual annotations.", "conclusion": "The extensive pre-training of GPT-4 enables superior performance in high-throughput phenotyping tasks without needing manually annotated training data.", "key_contributions": ["Comparison of GPT-4 with GPT-3.5-Turbo in clinical summary phenotyping", "Demonstration of high concordance with manual annotators", "Insight into the potential dominance of LLMs in automating clinical text phenotyping"], "limitations": "Some limitations noted in sign normalization processes.", "keywords": ["High-throughput phenotyping", "Large language models", "Clinical summaries", "Precision medicine", "Health informatics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2409.08797", "pdf": "https://arxiv.org/pdf/2409.08797.pdf", "abs": "https://arxiv.org/abs/2409.08797", "title": "Exploring SSL Discrete Speech Features for Zipformer-based Contextual ASR", "authors": ["Mingyu Cui", "Yifan Yang", "Jiajun Deng", "Jiawen Kang", "Shujie Hu", "Tianzi Wang", "Zhaoqing Li", "Shiliang Zhang", "Xie Chen", "Xunying Liu"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "Self-supervised learning (SSL) based discrete speech representations are\nhighly compact and domain adaptable. In this paper, SSL discrete speech\nfeatures extracted from WavLM models are used as additional cross-utterance\nacoustic context features in Zipformer-Transducer ASR systems. The efficacy of\nreplacing Fbank features with discrete token features for modelling either\ncross-utterance contexts (from preceding and future segments), or current\nutterance's internal contexts alone, or both at the same time, are demonstrated\nthoroughly on the Gigaspeech 1000-hr corpus. The best Zipformer-Transducer\nsystem using discrete tokens based cross-utterance context features outperforms\nthe baseline using utterance internal context only with statistically\nsignificant word error rate (WER) reductions of 0.32% to 0.41% absolute (2.78%\nto 3.54% relative) on the dev and test data. The lowest published WER of 11.15%\nand 11.14% were obtained on the dev and test sets. Our work is open-source and\npublicly available at\nhttps://github.com/open-creator/icefall/tree/master/egs/gigaspeech/Context\\_ASR.", "AI": {"tldr": "This paper explores the use of self-supervised learning (SSL) discrete speech features in ASR systems, demonstrating significant improvements in word error rates.", "motivation": "The motivation of this work is to enhance automatic speech recognition (ASR) systems by leveraging SSL discrete speech representations to improve context modeling.", "method": "The paper employs discrete speech features from WavLM models as additional acoustic context features in Zipformer-Transducer ASR systems, testing their efficacy against traditional Fbank features on the Gigaspeech corpus.", "result": "The study shows that using discrete token features for modeling both cross-utterance and internal contexts significantly reduces the word error rate (WER) by 0.32% to 0.41% absolute, achieving the lowest published WER of 11.15% and 11.14% on respective datasets.", "conclusion": "The findings indicate that SSL-based discrete speech features improve ASR system performance, providing a promising direction for future work in the domain and are available for public use.", "key_contributions": ["Demonstrated effectiveness of SSL discrete speech features for ASR.", "Achieved statistically significant WER reductions compared to baseline models.", "Provided open-source implementation for further research."], "limitations": "", "keywords": ["self-supervised learning", "speech recognition", "Zipformer-Transducer", "acoustic context", "Gigaspeech"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2410.08674", "pdf": "https://arxiv.org/pdf/2410.08674.pdf", "abs": "https://arxiv.org/abs/2410.08674", "title": "Guidelines for Fine-grained Sentence-level Arabic Readability Annotation", "authors": ["Nizar Habash", "Hanada Taha-Thomure", "Khalid N. Elmadani", "Zeina Zeino", "Abdallah Abushmaes"], "categories": ["cs.CL"], "comment": "Accepted at LAW-XIX at ACL 2025", "summary": "This paper presents the annotation guidelines of the Balanced Arabic\nReadability Evaluation Corpus (BAREC), a large-scale resource for fine-grained\nsentence-level readability assessment in Arabic. BAREC includes 69,441\nsentences (1M+ words) labeled across 19 levels, from kindergarten to\npostgraduate. Based on the Taha/Arabi21 framework, the guidelines were refined\nthrough iterative training with native Arabic-speaking educators. We highlight\nkey linguistic, pedagogical, and cognitive factors in determining readability\nand report high inter-annotator agreement: Quadratic Weighted Kappa 81.8%\n(substantial/excellent agreement) in the last annotation phase. We also\nbenchmark automatic readability models across multiple classification\ngranularities (19-, 7-, 5-, and 3-level). The corpus and guidelines are\npublicly available.", "AI": {"tldr": "This paper details the annotation guidelines for the Balanced Arabic Readability Evaluation Corpus (BAREC), a resource for assessing sentence-level readability in Arabic, validated by educators.", "motivation": "To create a large-scale resource for fine-grained readability assessment in Arabic, addressing the lack of such corpora.", "method": "The guidelines were developed using the Taha/Arabi21 framework and refined through iterative training with native Arabic-speaking educators, leading to high inter-annotator agreement.", "result": "The corpus features 69,441 sentences labeled across 19 readability levels, with substantial inter-annotator agreement at 81.8% and benchmarking of automatic readability models across various classification levels.", "conclusion": "The BAREC corpus and its annotation guidelines provide a standardized resource for Arabic readability assessment, benefiting educational and research applications.", "key_contributions": ["Creation of a large-scale, publicly available Arabic readability corpus", "Refinement of annotation guidelines through collaboration with educators", "Benchmarking of automatic readability models across multiple levels"], "limitations": "", "keywords": ["Arabic readability", "corpus", "annotation guidelines", "Taha/Arabi21", "educational resource"], "importance_score": 4, "read_time_minutes": 12}}
{"id": "2410.19317", "pdf": "https://arxiv.org/pdf/2410.19317.pdf", "abs": "https://arxiv.org/abs/2410.19317", "title": "FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in Conversational LLMs", "authors": ["Zhiting Fan", "Ruizhe Chen", "Tianxiang Hu", "Zuozhu Liu"], "categories": ["cs.CL"], "comment": "ICLR 2025 spotlight", "summary": "The growing use of large language model (LLM)-based chatbots has raised\nconcerns about fairness. Fairness issues in LLMs can lead to severe\nconsequences, such as bias amplification, discrimination, and harm to\nmarginalized communities. While existing fairness benchmarks mainly focus on\nsingle-turn dialogues, multi-turn scenarios, which in fact better reflect\nreal-world conversations, present greater challenges due to conversational\ncomplexity and potential bias accumulation. In this paper, we propose a\ncomprehensive fairness benchmark for LLMs in multi-turn dialogue scenarios,\n\\textbf{FairMT-Bench}. Specifically, we formulate a task taxonomy targeting LLM\nfairness capabilities across three stages: context understanding, user\ninteraction, and instruction trade-offs, with each stage comprising two tasks.\nTo ensure coverage of diverse bias types and attributes, we draw from existing\nfairness datasets and employ our template to construct a multi-turn dialogue\ndataset, \\texttt{FairMT-10K}. For evaluation, GPT-4 is applied, alongside bias\nclassifiers including Llama-Guard-3 and human validation to ensure robustness.\nExperiments and analyses on \\texttt{FairMT-10K} reveal that in multi-turn\ndialogue scenarios, current LLMs are more likely to generate biased responses,\nand there is significant variation in performance across different tasks and\nmodels. Based on this, we curate a challenging dataset, \\texttt{FairMT-1K}, and\ntest 15 current state-of-the-art (SOTA) LLMs on this dataset. The results show\nthe current state of fairness in LLMs and showcase the utility of this novel\napproach for assessing fairness in more realistic multi-turn dialogue contexts,\ncalling for future work to focus on LLM fairness improvement and the adoption\nof \\texttt{FairMT-1K} in such efforts.", "AI": {"tldr": "This paper introduces a benchmark for assessing fairness in large language models (LLMs) during multi-turn dialogue, addressing bias issues more effectively than existing single-turn benchmarks.", "motivation": "Concerns about fairness in LLMs leading to bias and discrimination in real-world applications necessitate a comprehensive evaluation framework for multi-turn dialogue scenarios.", "method": "The authors propose FairMT-Bench, a benchmark that includes a task taxonomy over three stages related to LLM fairness, and develop a multi-turn dialogue dataset called FairMT-10K for evaluation.", "result": "Current LLMs show increased likelihood of generating biased responses in multi-turn dialogues, with performance variations across tasks and models; a new dataset, FairMT-1K, is introduced for testing.", "conclusion": "The study highlights the need for improved fairness in LLMs and suggests that FairMT-1K should be adopted for future fairness assessments in multi-turn dialogues.", "key_contributions": ["Introduction of FairMT-Bench for multi-turn dialogue fairness assessment", "Creation of FairMT-10K and FairMT-1K datasets for evaluating LLMs", "Empirical evidence showcasing bias in LLMs during multi-turn dialogues"], "limitations": "", "keywords": ["fairness", "large language models", "multi-turn dialogue", "bias", "benchmark"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2410.20682", "pdf": "https://arxiv.org/pdf/2410.20682.pdf", "abs": "https://arxiv.org/abs/2410.20682", "title": "SHARE: Shared Memory-Aware Open-Domain Long-Term Dialogue Dataset Constructed from Movie Script", "authors": ["Eunwon Kim", "Chanho Park", "Buru Chang"], "categories": ["cs.CL"], "comment": null, "summary": "Shared memories between two individuals strengthen their bond and are crucial\nfor facilitating their ongoing conversations. This study aims to make long-term\ndialogue more engaging by leveraging these shared memories. To this end, we\nintroduce a new long-term dialogue dataset named SHARE, constructed from movie\nscripts, which are a rich source of shared memories among various\nrelationships. Our dialogue dataset contains the summaries of persona\ninformation and events of two individuals, as explicitly revealed in their\nconversation, along with implicitly extractable shared memories. We also\nintroduce EPISODE, a long-term dialogue framework based on SHARE that utilizes\nshared experiences between individuals. Through experiments using SHARE, we\ndemonstrate that shared memories between two individuals make long-term\ndialogues more engaging and sustainable, and that EPISODE effectively manages\nshared memories during dialogue. Our dataset and code are available at\nhttps://github.com/e1kim/SHARE.", "AI": {"tldr": "The study introduces the SHARE dataset for enhancing long-term dialogue by leveraging shared memories, along with the EPISODE framework that manages these memories in conversations.", "motivation": "To engage long-term dialogue using shared memories between individuals, which strengthen their relationships.", "method": "The study constructs the SHARE dataset from movie scripts and develops the EPISODE framework to utilize shared experiences, supported by experiments demonstrating improved dialogue engagement.", "result": "Experiments show that dialogues incorporating shared memories are more engaging and sustainable, and that EPISODE effectively manages these memories.", "conclusion": "The SHARE dataset and EPISODE framework provide valuable tools for enhancing long-term dialogue systems by incorporating shared memories.", "key_contributions": ["Introduction of the SHARE dataset constructed from movie scripts", "Development of the EPISODE framework for managing shared memories", "Demonstrated effectiveness of shared memories in enhancing dialogue engagement"], "limitations": "", "keywords": ["long-term dialogue", "shared memories", "dataset", "dialogue framework", "AI"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2410.24200", "pdf": "https://arxiv.org/pdf/2410.24200.pdf", "abs": "https://arxiv.org/abs/2410.24200", "title": "Length-Induced Embedding Collapse in PLM-based Models", "authors": ["Yuqi Zhou", "Sunhao Dai", "Zhanshuo Cao", "Xiao Zhang", "Jun Xu"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted by ACL 2025", "summary": "Text embeddings from PLM-based models enable a wide range of applications,\nyet their performance often degrades on longer texts. In this paper, we\nintroduce a phenomenon we call Length Collapse, where embeddings of longer\ntexts tend to cluster together. This clustering results in a distributional\ninconsistency between the embeddings of short and long texts. We further\ninvestigate how these differences contribute to the performance decline\nobserved with longer texts across various downstream tasks. Through a rigorous\ntheoretical analysis of the self-attention mechanism, which acts as a low-pass\nfilter in PLM-based models, we demonstrate that as text length increases, the\nstrength of low-pass filtering intensifies, causing embeddings to retain more\nlow-frequency components. As a result, input token features become more\nsimilar, leading to clustering and ultimately the collapse of embeddings for\nlonger texts. To address this issue, we propose a simple method, TempScale,\nwhich mitigates the Length Collapse phenomenon. By narrowing the gap in\nlow-pass filtering rates between long and short texts, TempScale ensures more\nconsistent embeddings across different text lengths. This approach leads to\nperformance improvements of 0.94% on MTEB and 1.10% on LongEmbed, which focuses\nspecifically on long-context retrieval, providing strong evidence for the\nvalidity of our analysis. The source code is available at\nhttps://github.com/Yuqi-Zhou/Length_Collapse.", "AI": {"tldr": "This paper introduces Length Collapse, a phenomenon where PLM-based text embeddings cluster for longer texts, causing performance declines in NLP tasks. It proposes TempScale to mitigate this issue, leading to performance improvements in embeddings.", "motivation": "To address the performance degradation of PLM-based models on longer texts, specifically investigating a phenomenon known as Length Collapse.", "method": "The paper presents a theoretical analysis of the self-attention mechanism in PLM-based models as a low-pass filter, introducing the method TempScale to narrow filtering gaps between text lengths.", "result": "The proposed TempScale method improves performance by 0.94% on MTEB and 1.10% on LongEmbed, which focuses on long-context retrieval tasks.", "conclusion": "TempScale effectively mitigates the Length Collapse phenomenon, leading to more consistent embeddings for texts of varying lengths, and thereby improving model performance on downstream tasks.", "key_contributions": ["Introduction of Length Collapse phenomenon", "Theoretical analysis of self-attention as a low-pass filter", "Development of TempScale to address Length Collapse."], "limitations": "", "keywords": ["text embeddings", "length collapse", "self-attention", "PLM models", "TempScale"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2411.15129", "pdf": "https://arxiv.org/pdf/2411.15129.pdf", "abs": "https://arxiv.org/abs/2411.15129", "title": "The BS-meter: A ChatGPT-Trained Instrument to Detect Sloppy Language-Games", "authors": ["Alessandro Trevisan", "Harry Giddens", "Sarah Dillon", "Alan F. Blackwell"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "What can we learn about language from studying how it is used by ChatGPT and\nother large language model (LLM)-based chatbots? In this paper, we analyse the\ndistinctive character of language generated by ChatGPT, in relation to\nquestions raised by natural language processing pioneer, and student of\nWittgenstein, Margaret Masterman. Following frequent complaints that LLM-based\nchatbots produce \"slop,\" or even \"bullshit,\" in the sense of Frankfurt's\npopular monograph On Bullshit, we conduct an empirical study to contrast the\nlanguage of 1,000 scientific publications with typical text generated by\nChatGPT. We then explore whether the same language features can be detected in\ntwo well-known contexts of social dysfunction: George Orwell's critique of\npolitical speech, and David Graeber's characterisation of bullshit jobs. Using\nsimple hypothesis-testing methods, we demonstrate that a statistical model of\nsloppy bullshit can reliably relate the Frankfurtian artificial bullshit of\nChatGPT to the political and workplace functions of bullshit as observed in\nnatural human language.", "AI": {"tldr": "This paper examines the language generated by ChatGPT, analyzing its similarities to descriptors of 'bullshit' in human discourse, through empirical comparison with scientific publications.", "motivation": "To investigate the language features of ChatGPT in relation to criticisms of its quality and its comparison to human-generated text in contexts of social dysfunction.", "method": "Empirical analysis contrasting the language of 1,000 scientific publications with text generated by ChatGPT, using statistical modeling and hypothesis-testing methods.", "result": "The study finds a reliable statistical correlation between the characteristics of language produced by ChatGPT and the functions of 'bullshit' as described in political speech and workplace contexts.", "conclusion": "LLM-generated language can exhibit features similar to those found in problematic human communication, raising questions about linguistic quality and meaning.", "key_contributions": ["Empirical analysis of language generated by ChatGPT", "Comparison to human-produced 'bullshit' in political and workplace contexts", "Statistical modeling linking language features to social dysfunction"], "limitations": "Focus is primarily on empirical characteristics without delving into deeper implications of language quality.", "keywords": ["ChatGPT", "bullshit", "language model", "natural language processing", "social dysfunction"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2412.03719", "pdf": "https://arxiv.org/pdf/2412.03719.pdf", "abs": "https://arxiv.org/abs/2412.03719", "title": "From Language Models over Tokens to Language Models over Characters", "authors": ["Tim Vieira", "Ben LeBrun", "Mario Giulianelli", "Juan Luis Gastaldi", "Brian DuSell", "John Terilla", "Timothy J. O'Donnell", "Ryan Cotterell"], "categories": ["cs.CL", "cs.AI"], "comment": "ICML 2025", "summary": "Modern language models are internally -- and mathematically -- distributions\nover $\\it{token}$ strings rather than $\\it{character}$ strings, posing numerous\nchallenges for programmers building user applications on top of them. For\nexample, if a prompt is specified as a character string, it must be tokenized\nbefore passing it to the token-level language model. Thus, the tokenizer and\nconsequent processing are very sensitive to the specification of the prompt\n(e.g., whether the prompt ends with a space or not). This paper presents\nalgorithms for converting token-level language models to character-level ones.\nWe present both exact and approximate algorithms. In the empirical portion of\nthe paper, we benchmark the practical runtime and approximation quality. Across\nfour publicly available language models, we find that -- even with a small\ncomputation budget -- our method is able to accurately approximate the\ncharacter-level distribution at reasonably fast speeds, and that a significant\nimprovement in the language model's compression rate (bits/byte) is achieved.", "AI": {"tldr": "This paper proposes algorithms to convert token-level language models to character-level models, improving the processing and prompt specification for applications.", "motivation": "To address the challenges faced by programmers using token-level language models due to the requirements of tokenization, which is sensitive to prompt specification.", "method": "The paper presents both exact and approximate algorithms for converting token-level language models to character-level models, and benchmarks their runtime and approximation quality.", "result": "The methods maintain accuracy in approximating character-level distributions and demonstrate significant improvements in compression rates across four language models tested.", "conclusion": "The proposed conversion algorithms enable better performance and efficiency for applications using language models by optimizing character-level handling.", "key_contributions": ["Algorithms for converting token-level to character-level language models", "Benchmarking performance across multiple language models", "Achieving higher compression rates for language models"], "limitations": "", "keywords": ["language models", "tokenization", "character-level models", "compression", "approximation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2412.09569", "pdf": "https://arxiv.org/pdf/2412.09569.pdf", "abs": "https://arxiv.org/abs/2412.09569", "title": "JuStRank: Benchmarking LLM Judges for System Ranking", "authors": ["Ariel Gera", "Odellia Boni", "Yotam Perlitz", "Roy Bar-Haim", "Lilach Eden", "Asaf Yehudai"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025", "summary": "Given the rapid progress of generative AI, there is a pressing need to\nsystematically compare and choose between the numerous models and\nconfigurations available. The scale and versatility of such evaluations make\nthe use of LLM-based judges a compelling solution for this challenge.\nCrucially, this approach requires first to validate the quality of the LLM\njudge itself. Previous work has focused on instance-based assessment of LLM\njudges, where a judge is evaluated over a set of responses, or response pairs,\nwhile being agnostic to their source systems. We argue that this setting\noverlooks critical factors affecting system-level ranking, such as a judge's\npositive or negative bias towards certain systems. To address this gap, we\nconduct the first large-scale study of LLM judges as system rankers. System\nscores are generated by aggregating judgment scores over multiple system\noutputs, and the judge's quality is assessed by comparing the resulting system\nranking to a human-based ranking. Beyond overall judge assessment, our analysis\nprovides a fine-grained characterization of judge behavior, including their\ndecisiveness and bias.", "AI": {"tldr": "This paper presents a large-scale study on evaluating LLM judges for system ranking in generative AI, addressing biases that affect system-level rankings.", "motivation": "The rapid advancements in generative AI necessitate effective evaluation mechanisms for various models and configurations, particularly due to the multitude of LLMs available.", "method": "The study conducts large-scale assessments of LLM judges by aggregating judgment scores from multiple system outputs to generate rankings, which are then compared with human-based rankings.", "result": "The assessment reveals biases in LLM judges towards certain systems, affecting their ranking capabilities, and provides insights into judge decisiveness.", "conclusion": "The findings highlight the importance of validating LLM judges in system-ranking contexts and understanding their behavior to improve evaluation quality.", "key_contributions": ["First large-scale study of LLM judges as system rankers", "Characterization of judge behavior, including biases and decisiveness", "Comparison of LLM-based rankings with human rankings"], "limitations": "The study may not encompass all types of biases or judge behaviors across different generative AI models.", "keywords": ["Generative AI", "LLM judges", "System ranking", "Bias assessment", "Human comparison"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2412.13949", "pdf": "https://arxiv.org/pdf/2412.13949.pdf", "abs": "https://arxiv.org/abs/2412.13949", "title": "Cracking the Code of Hallucination in LVLMs with Vision-aware Head Divergence", "authors": ["Jinghan He", "Kuan Zhu", "Haiyun Guo", "Junfeng Fang", "Zhenglin Hua", "Yuheng Jia", "Ming Tang", "Tat-Seng Chua", "Jinqiao Wang"], "categories": ["cs.CL", "cs.CV"], "comment": "ACL2025", "summary": "Large vision-language models (LVLMs) have made substantial progress in\nintegrating large language models (LLMs) with visual inputs, enabling advanced\nmultimodal reasoning. Despite their success, a persistent challenge is\nhallucination-where generated text fails to accurately reflect visual\ncontent-undermining both accuracy and reliability. Existing methods focus on\nalignment training or decoding refinements but primarily address symptoms at\nthe generation stage without probing the underlying causes. In this work, we\ninvestigate the internal mechanisms driving hallucination in LVLMs, with an\nemphasis on the multi-head attention module. Specifically, we introduce\nVision-aware Head Divergence (VHD), a metric that quantifies the sensitivity of\nattention head outputs to visual context. Based on this, our findings reveal\nthe presence of vision-aware attention heads that are more attuned to visual\ninformation; however, the model's overreliance on its prior language patterns\nis closely related to hallucinations. Building on these insights, we propose\nVision-aware Head Reinforcement (VHR), a training-free approach to mitigate\nhallucination by enhancing the role of vision-aware attention heads. Extensive\nexperiments demonstrate that our method achieves superior performance compared\nto state-of-the-art approaches in mitigating hallucinations, while maintaining\nhigh efficiency with negligible additional time overhead.", "AI": {"tldr": "This paper addresses hallucinations in Large Vision-Language Models (LVLMs) by investigating the internal mechanisms involving multi-head attention, introducing metrics and methods to enhance attention focused on visual information while reducing reliance on language patterns.", "motivation": "To address hallucination issues in LVLMs, which undermine their accuracy and reliability, by exploring the underlying mechanisms rather than just the symptoms at the generation stage.", "method": "The study introduces Vision-aware Head Divergence (VHD) as a metric to evaluate attention head sensitivity to visual context, and proposes Vision-aware Head Reinforcement (VHR) to enhance the performance of vision-aware attention heads without requiring additional training.", "result": "The proposed method, VHR, shows superior performance in reducing hallucinations compared to existing approaches while maintaining efficiency with minimal time overhead.", "conclusion": "Enhancing the role of vision-aware attention heads helps mitigate hallucination in LVLMs, leading to more reliable outputs without compromising efficiency.", "key_contributions": ["Introduction of Vision-aware Head Divergence (VHD) for evaluating attention heads' visual sensitivity", "Proposal of Vision-aware Head Reinforcement (VHR) as a training-free method to reduce hallucinations", "Extensive experimental results demonstrating improved performance over state-of-the-art methods."], "limitations": "", "keywords": ["Large Vision-Language Models", "multimodal reasoning", "hallucination", "attention mechanisms", "visual context"], "importance_score": 8, "read_time_minutes": 7}}
{"id": "2502.02958", "pdf": "https://arxiv.org/pdf/2502.02958.pdf", "abs": "https://arxiv.org/abs/2502.02958", "title": "Position: Editing Large Language Models Poses Serious Safety Risks", "authors": ["Paul Youssef", "Zhixue Zhao", "Daniel Braun", "Jörg Schlötterer", "Christin Seifert"], "categories": ["cs.CL"], "comment": "Accepted at ICML 2025", "summary": "Large Language Models (LLMs) contain large amounts of facts about the world.\nThese facts can become outdated over time, which has led to the development of\nknowledge editing methods (KEs) that can change specific facts in LLMs with\nlimited side effects. This position paper argues that editing LLMs poses\nserious safety risks that have been largely overlooked. First, we note the fact\nthat KEs are widely available, computationally inexpensive, highly performant,\nand stealthy makes them an attractive tool for malicious actors. Second, we\ndiscuss malicious use cases of KEs, showing how KEs can be easily adapted for a\nvariety of malicious purposes. Third, we highlight vulnerabilities in the AI\necosystem that allow unrestricted uploading and downloading of updated models\nwithout verification. Fourth, we argue that a lack of social and institutional\nawareness exacerbates this risk, and discuss the implications for different\nstakeholders. We call on the community to (i) research tamper-resistant models\nand countermeasures against malicious model editing, and (ii) actively engage\nin securing the AI ecosystem.", "AI": {"tldr": "The paper discusses the safety risks associated with knowledge editing methods (KEs) in Large Language Models (LLMs), highlighting the potential for malicious use and vulnerabilities in the AI ecosystem.", "motivation": "As LLMs hold vast amounts of factual content, ensuring their reliability and safety has become critical, especially as outdated information can lead to misinformation through knowledge editing.", "method": "The paper examines the characteristics of KEs that make them appealing to malicious actors, analyzes specific use cases of malicious activities enabled by KEs, and identifies vulnerabilities within the AI ecosystem.", "result": "The analysis shows that KEs are computationally inexpensive and easily adaptable for malicious purposes, posing risks to the integrity of LLMs and the AI ecosystem.", "conclusion": "The paper urges the AI community to focus on developing tamper-resistant models and engaging in active protection of the AI ecosystem against knowledge editing risks.", "key_contributions": ["Highlighting the safety risks of knowledge editing methods in LLMs.", "Identifying specific malicious use cases of KEs.", "Calling for research on tamper-resistant AI models."], "limitations": "The paper does not provide experimental data to support the claims, focusing instead on theoretical implications.", "keywords": ["Large Language Models", "knowledge editing", "safety risks", "AI ecosystem", "malicious use cases"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.03699", "pdf": "https://arxiv.org/pdf/2502.03699.pdf", "abs": "https://arxiv.org/abs/2502.03699", "title": "LLM Alignment as Retriever Optimization: An Information Retrieval Perspective", "authors": ["Bowen Jin", "Jinsung Yoon", "Zhen Qin", "Ziqi Wang", "Wei Xiong", "Yu Meng", "Jiawei Han", "Sercan O. Arik"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "26 pages", "summary": "Large Language Models (LLMs) have revolutionized artificial intelligence with\ncapabilities in reasoning, coding, and communication, driving innovation across\nindustries. Their true potential depends on effective alignment to ensure\ncorrect, trustworthy and ethical behavior, addressing challenges like\nmisinformation, hallucinations, bias and misuse. While existing Reinforcement\nLearning (RL)-based alignment methods are notoriously complex, direct\noptimization approaches offer a simpler alternative. In this work, we introduce\na novel direct optimization approach for LLM alignment by drawing on\nestablished Information Retrieval (IR) principles. We present a systematic\nframework that bridges LLM alignment and IR methodologies, mapping LLM\ngeneration and reward models to IR's retriever-reranker paradigm. Building on\nthis foundation, we propose LLM Alignment as Retriever Preference Optimization\n(LarPO), a new alignment method that enhances overall alignment quality.\nExtensive experiments validate LarPO's effectiveness with 38.9 % and 13.7 %\naveraged improvement on AlpacaEval2 and MixEval-Hard respectively. Our work\nopens new avenues for advancing LLM alignment by integrating IR foundations,\noffering a promising direction for future research.", "AI": {"tldr": "This paper introduces LarPO, a novel method for aligning Large Language Models (LLMs) using direct optimization principles from Information Retrieval (IR).", "motivation": "To address the challenges in aligning LLMs for trustworthy and ethical behavior, particularly issues like misinformation and bias.", "method": "The authors propose a framework that connects LLM alignment with IR methodologies, specifically using a retriever-reranker paradigm.", "result": "LarPO demonstrates significant improvements in alignment quality, achieving a 38.9% and 13.7% enhancement on two evaluation benchmarks.", "conclusion": "The integration of IR principles into LLM alignment offers a simpler and effective alternative, paving the way for future research in this area.", "key_contributions": ["Introduction of LarPO, a novel alignment method for LLMs.", "Integration of Information Retrieval principles for LLM alignment.", "Demonstrated significant improvements in evaluation metrics."], "limitations": "", "keywords": ["Large Language Models", "LLM alignment", "Information Retrieval", "Reinforcement Learning", "direct optimization"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.04390", "pdf": "https://arxiv.org/pdf/2502.04390.pdf", "abs": "https://arxiv.org/abs/2502.04390", "title": "In Praise of Stubbornness: An Empirical Case for Cognitive-Dissonance Aware Continual Update of Knowledge in LLMs", "authors": ["Simone Clemente", "Zied Ben Houidi", "Alexis Huet", "Dario Rossi", "Giulio Franzese", "Pietro Michiardi"], "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.NC"], "comment": null, "summary": "Through systematic empirical investigation, we uncover a fundamental and\nconcerning property of Large Language Models: while they can safely learn facts\nthat don't contradict their knowledge, attempting to update facts with\ncontradictory information triggers catastrophic corruption of unrelated\nknowledge. Unlike humans, who naturally resist contradictory information, these\nmodels indiscriminately accept contradictions, leading to devastating\ninterference, destroying up to 80% of unrelated knowledge even when learning as\nfew as 10-100 contradicting facts. To understand whether this interference\ncould be mitigated through selective plasticity, we experiment with targeted\nnetwork updates, distinguishing between previously used (stubborn) and rarely\nused (plastic) neurons. We uncover another asymmetry: while sparing\nfrequently-used neurons significantly improves retention of existing knowledge\nfor non-contradictory updates (98% vs 93% with standard updates), contradictory\nupdates trigger catastrophic interference regardless of targeting strategy.\nThis effect which persists across tested model scales (GPT-2 to GPT-J-6B),\nsuggests a fundamental limitation in how neural networks handle contradictions.\nFinally, we demonstrate that contradictory information can be reliably detected\n(95%+ accuracy) using simple model features, offering a potential protective\nmechanism. These findings motivate new architectures that can, like humans,\nnaturally resist contradictions rather than allowing destructive overwrites.", "AI": {"tldr": "The paper investigates the catastrophic interference in Large Language Models when updating facts with contradictory information, leading to significant loss of unrelated knowledge. It explores selective plasticity in neural networks and proposes potential architectural changes to mitigate these issues.", "motivation": "To understand the fundamental limitations of Large Language Models in handling contradictory information and to investigate methods to prevent catastrophic knowledge overwrites.", "method": "The researchers conducted systematic empirical investigations on various model scales (GPT-2 to GPT-J-6B) to study the effects of contradictory updates and implemented targeted network updates to distinguish between frequently used and rarely used neurons.", "result": "The study found that contradictory updates lead to catastrophic interference that destroys up to 80% of unrelated knowledge, regardless of the targeted updating strategy. However, sparing frequently-used neurons significantly improves knowledge retention for non-contradictory updates.", "conclusion": "The findings indicate a fundamental limitation in how neural networks handle contradictions, suggesting the need for new architectures that can resist contradictions similar to human cognition.", "key_contributions": ["Identified catastrophic interference in LLMs due to contradictory information updates.", "Evaluated the impact of neuron usage frequency on knowledge retention during updates.", "Proposed a detection mechanism for contradictory information with high accuracy."], "limitations": "The approach may not generalize to all types of neural network architectures or domains beyond language models.", "keywords": ["Large Language Models", "catastrophic interference", "neural networks", "contradictory information", "selective plasticity"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.05202", "pdf": "https://arxiv.org/pdf/2502.05202.pdf", "abs": "https://arxiv.org/abs/2502.05202", "title": "Accelerating LLM Inference with Lossless Speculative Decoding Algorithms for Heterogeneous Vocabularies", "authors": ["Nadav Timor", "Jonathan Mamou", "Daniel Korat", "Moshe Berchansky", "Gaurav Jain", "Oren Pereg", "Moshe Wasserblat", "David Harel"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML'25 Oral (top %1)", "summary": "Accelerating the inference of large language models (LLMs) is a critical\nchallenge in generative AI. Speculative decoding (SD) methods offer substantial\nefficiency gains by generating multiple tokens using a single target forward\npass. However, existing SD approaches require the drafter and target models to\nshare the same vocabulary, thus limiting the pool of possible drafters, often\nnecessitating the training of a drafter from scratch. We present three new SD\nmethods that remove this shared-vocabulary constraint. All three methods\npreserve the target distribution (i.e., they are lossless) and work with\noff-the-shelf models without requiring additional training or modifications.\nEmpirically, on summarization, programming, and long-context tasks, our\nalgorithms demonstrate significant speedups of up to 2.8x over standard\nautoregressive decoding. By enabling any off-the-shelf model to serve as a\ndrafter and requiring no retraining, this work substantially broadens the\napplicability of the SD framework in practice.", "AI": {"tldr": "This paper introduces novel speculative decoding methods that efficiently infer large language models without requiring shared vocabularies between drafter and target models, enabling faster processing while preserving output quality.", "motivation": "The need to accelerate inference in large language models (LLMs) while keeping results accurate and efficient.", "method": "Introduction of three new speculative decoding methods that allow different vocabularies for drafter and target models, preserving the target distribution and requiring no additional training.", "result": "The proposed methods achieve speedups of up to 2.8 times over standard autoregressive decoding on various tasks including summarization and programming.", "conclusion": "These methods enhance the application of speculative decoding in practice by broadening the range of models that can be utilized without retraining.", "key_contributions": ["Three new speculative decoding methods eliminating shared vocabulary constraints", "Lossless methods that work with existing models without retraining", "Significant speed enhancements on inference tasks"], "limitations": "", "keywords": ["speculative decoding", "large language models", "inference acceleration"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.12658", "pdf": "https://arxiv.org/pdf/2502.12658.pdf", "abs": "https://arxiv.org/abs/2502.12658", "title": "R.R.: Unveiling LLM Training Privacy through Recollection and Ranking", "authors": ["Wenlong Meng", "Zhenyuan Guo", "Lenan Wu", "Chen Gong", "Wenyan Liu", "Weixian Li", "Chengkun Wei", "Wenzhi Chen"], "categories": ["cs.CL"], "comment": "13 pages, 9 figures; typos corrected", "summary": "Large Language Models (LLMs) pose significant privacy risks, potentially\nleaking training data due to implicit memorization. Existing privacy attacks\nprimarily focus on membership inference attacks (MIAs) or data extraction\nattacks, but reconstructing specific personally identifiable information (PII)\nin LLMs' training data remains challenging. In this paper, we propose R.R.\n(Recollect and Rank), a novel two-step privacy stealing attack that enables\nattackers to reconstruct PII entities from scrubbed training data where the PII\nentities have been masked. In the first stage, we introduce a prompt paradigm\nnamed recollection, which instructs the LLM to repeat a masked text but fill in\nmasks. Then we can use PII identifiers to extract recollected PII candidates.\nIn the second stage, we design a new criterion to score each PII candidate and\nrank them. Motivated by membership inference, we leverage the reference model\nas a calibration to our criterion. Experiments across three popular PII\ndatasets demonstrate that the R.R. achieves better PII identification\nperformance than baselines. These results highlight the vulnerability of LLMs\nto PII leakage even when training data has been scrubbed. We release our code\nand datasets at GitHub.", "AI": {"tldr": "This paper introduces R.R., a novel attack method to reconstruct personally identifiable information (PII) from masked training data in large language models (LLMs).", "motivation": "The paper addresses the significant privacy risks posed by LLMs, particularly the challenge of reconstructing specific PII from scrubbed training data, which is a gap in current privacy attack methodologies.", "method": "The proposed method, R.R. (Recollect and Rank), involves a two-step process: first, using a prompt paradigm for recollecting masked text, and second, scoring and ranking the identified PII candidates based on a new criterion.", "result": "Experiments on three popular PII datasets show that R.R. outperforms existing baselines in identifying PII, demonstrating the vulnerability of LLMs even when training data is scrubbed.", "conclusion": "R.R. effectively highlights the risks of PII leakage in LLMs and raises awareness about privacy concerns in language models, with the authors providing their code and datasets for public use.", "key_contributions": ["Introduction of R.R., a two-step privacy attack for PII reconstruction", "Development of a novel scoring criterion for PII candidates", "Demonstration of superior performance in PII identification over existing methods"], "limitations": "", "keywords": ["Privacy", "Large Language Models", "Personal Identifiable Information", "Data Scrubbing", "Security vulnerabilities"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2502.14898", "pdf": "https://arxiv.org/pdf/2502.14898.pdf", "abs": "https://arxiv.org/abs/2502.14898", "title": "Retrieval-augmented systems can be dangerous medical communicators", "authors": ["Lionel Wong", "Ayman Ali", "Raymond Xiong", "Shannon Zeijang Shen", "Yoon Kim", "Monica Agrawal"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Position paper in Proceedings of the 42 nd International Conference\n  on Machine Learning", "summary": "Patients have long sought health information online, and increasingly, they\nare turning to generative AI to answer their health-related queries. Given the\nhigh stakes of the medical domain, techniques like retrieval-augmented\ngeneration and citation grounding have been widely promoted as methods to\nreduce hallucinations and improve the accuracy of AI-generated responses and\nhave been widely adopted into search engines. This paper argues that even when\nthese methods produce literally accurate content drawn from source documents\nsans hallucinations, they can still be highly misleading. Patients may derive\nsignificantly different interpretations from AI-generated outputs than they\nwould from reading the original source material, let alone consulting a\nknowledgeable clinician. Through a large-scale query analysis on topics\nincluding disputed diagnoses and procedure safety, we support our argument with\nquantitative and qualitative evidence of the suboptimal answers resulting from\ncurrent systems. In particular, we highlight how these models tend to\ndecontextualize facts, omit critical relevant sources, and reinforce patient\nmisconceptions or biases. We propose a series of recommendations -- such as the\nincorporation of communication pragmatics and enhanced comprehension of source\ndocuments -- that could help mitigate these issues and extend beyond the\nmedical domain.", "AI": {"tldr": "This paper examines the limitations of generative AI in providing accurate health information, arguing that despite the use of techniques like retrieval-augmented generation, AI responses can mislead patients by decontextualizing facts and perpetuating misconceptions.", "motivation": "To address the misleading nature of AI-generated health information despite accurate sourcing, particularly in high-stakes medical contexts where patient understanding is critical.", "method": "Conducted a large-scale query analysis on disputed diagnoses and procedure safety, combining quantitative and qualitative evidence to assess the effectiveness of current generative AI systems.", "result": "Findings highlight that current AI systems often fail to provide context, omit critical sources, and reinforce patient misconceptions despite generating text without hallucinations.", "conclusion": "Recommendations are proposed for improving AI communication in health contexts, emphasizing the need for better source comprehension and consideration of communication pragmatics.", "key_contributions": ["Critiques the reliability of generative AI in conveying health information accurately.", "Presents quantitative and qualitative evidence demonstrating the misleading nature of AI responses.", "Offers actionable recommendations to enhance AI communication strategies in health."], "limitations": "The study may not cover all areas of AI-generated content in health and focuses primarily on specific query examples.", "keywords": ["Generative AI", "Health Information", "Retrieval-Augmented Generation", "Patient Misconceptions", "Communication Pragmatics"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.15226", "pdf": "https://arxiv.org/pdf/2502.15226.pdf", "abs": "https://arxiv.org/abs/2502.15226", "title": "Understand User Opinions of Large Language Models via LLM-Powered In-the-Moment User Experience Interviews", "authors": ["Mengqiao Liu", "Tevin Wang", "Cassandra A. Cohen", "Sarah Li", "Chenyan Xiong"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Which large language model (LLM) is better? Every evaluation tells a story,\nbut what do users really think about current LLMs? This paper presents CLUE, an\nLLM-powered interviewer that conducts in-the-moment user experience interviews,\nright after users interact with LLMs, and automatically gathers insights about\nuser opinions from massive interview logs. We conduct a study with thousands of\nusers to understand user opinions on mainstream LLMs, recruiting users to first\nchat with a target LLM and then be interviewed by CLUE. Our experiments\ndemonstrate that CLUE captures interesting user opinions, e.g., the bipolar\nviews on the displayed reasoning process of DeepSeek-R1 and demands for\ninformation freshness and multi-modality. Our code and data are at\nhttps://github.com/cxcscmu/LLM-Interviewer.", "AI": {"tldr": "The paper introduces CLUE, an LLM-powered interviewer that collects user experience insights post-interaction with LLMs, based on a large study.", "motivation": "To gauge real user opinions on current large language models (LLMs) through automated user interviews.", "method": "The study involved thousands of users interacting with LLMs followed by interviews conducted by CLUE to capture their opinions.", "result": "CLUE successfully gathered user insights, revealing polarized views on LLM reasoning processes and user demands for updated and multimodal information.", "conclusion": "The findings demonstrate the effectiveness of CLUE in capturing nuanced user feedback on LLMs, highlighting key areas of user concern.", "key_contributions": ["Introduction of CLUE, an automated user interview tool for LLMs.", "Large-scale user study capturing real-time feedback on LLM interactions.", "Insights into user opinions regarding LLM reasoning and information freshness."], "limitations": "The study may have biases based on user selection and limited to specific LLMs tested.", "keywords": ["LLM", "user experience", "interviews", "automation", "user feedback"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.20122", "pdf": "https://arxiv.org/pdf/2502.20122.pdf", "abs": "https://arxiv.org/abs/2502.20122", "title": "Self-Training Elicits Concise Reasoning in Large Language Models", "authors": ["Tergel Munkhbat", "Namgyu Ho", "Seo Hyun Kim", "Yongjin Yang", "Yujin Kim", "Se-Young Yun"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "26 pages, 10 figures, 23 tables. Accepted to Findings of ACL 2025", "summary": "Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to\nutilize additional computation through intermediate tokens to solve complex\ntasks. However, we posit that typical reasoning traces contain many redundant\ntokens, incurring extraneous inference costs. Upon examination of the output\ndistribution of current LLMs, we find evidence on their latent ability to\nreason more concisely, relative to their default behavior. To elicit this\ncapability, we propose simple fine-tuning methods which leverage self-generated\nconcise reasoning paths obtained by best-of-N sampling and few-shot\nconditioning, in task-specific settings. Our combined method achieves a 30%\nreduction in output tokens on average, across five model families on GSM8K and\nMATH, while maintaining average accuracy. By exploiting the fundamental\nstochasticity and in-context learning capabilities of LLMs, our self-training\napproach robustly elicits concise reasoning on a wide range of models,\nincluding those with extensive post-training. Code is available at\nhttps://github.com/TergelMunkhbat/concise-reasoning", "AI": {"tldr": "This paper proposes a fine-tuning method for large language models to achieve more concise reasoning, reducing the number of output tokens by 30% while maintaining accuracy.", "motivation": "The intent is to reduce the redundant tokens generated during chain-of-thought reasoning in LLMs, which incur unnecessary inference costs.", "method": "The authors propose simple fine-tuning methods utilizing self-generated concise reasoning paths derived from best-of-N sampling and few-shot conditioning in a task-specific manner.", "result": "The method results in a 30% reduction in output tokens on average across five model families tested on GSM8K and MATH, while preserving average accuracy.", "conclusion": "The self-training approach effectively demonstrates that LLMs can reason more concisely, capitalizing on their stochasticity and in-context learning capabilities to improve efficiency.", "key_contributions": ["Introduction of fine-tuning methods for concise reasoning", "Demonstration of 30% output token reduction", "Robust application across a variety of LLMs and tasks"], "limitations": "", "keywords": ["large language models", "chain-of-thought reasoning", "fine-tuning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.04793", "pdf": "https://arxiv.org/pdf/2503.04793.pdf", "abs": "https://arxiv.org/abs/2503.04793", "title": "Sentence-level Reward Model can Generalize Better for Aligning LLM from Human Preference", "authors": ["Wenjie Qiu", "Yi-Chen Li", "Xuqin Zhang", "Tianyi Zhang", "Yihang Zhang", "Zongzhang Zhang", "Yang Yu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Learning reward models from human preference datasets and subsequently\noptimizing language models via reinforcement learning has emerged as a\nfundamental paradigm for aligning LLMs with human preferences. The performance\nof the reward model plays a crucial role in the effectiveness of alignment.\nPrevious reward models operate at a coarse-grained level, requiring the\ngeneration of a complete response to obtain a reward value. The sparse reward\nmay present challenges for downstream reinforcement learning. While recent\nefforts have attempted to learn token-level reward models, the lack of explicit\nsemantic information makes it difficult to model the credit of every individual\ntoken. In this paper, we propose assigning scores to every sentence,\nintroducing an intermediate-grained reward model. By segmenting the complete\nresponse into sentences and applying differential operations to reward output\nat the start and end positions of each sentence, we can effectively model the\nrewards of sentences. Moreover, a novel attention mechanism is introduced to\naggregate the scores of all sentences into a response-level score, which allows\nit to be trained using the Bradley-Terry model. On common benchmarks, our\nmethod outperforms the response-level reward model by 2.7% on RewardBench (for\nreward modeling evaluation) and surpasses all baselines on AlpacaEval (for\nalignment evaluation).", "AI": {"tldr": "This paper introduces an intermediate-grained reward model for aligning language models with human preferences by assigning scores to sentences rather than entire responses.", "motivation": "To improve the effectiveness of aligning LLMs with human preferences and address the limitations of existing coarse and token-level reward models.", "method": "The authors propose a method that assigns scores to sentences in a response, using differential operations on sentence start and end positions, and an attention mechanism to aggregate these scores into a single score for the response.", "result": "The proposed intermediate-grained reward model outperforms traditional response-level models by 2.7% on the RewardBench benchmark and surpasses all baselines on the AlpacaEval alignment evaluation.", "conclusion": "Segmenting responses into sentences and assigning scores at this level provides a more effective way to model rewards compared to existing methods, contributing positively to the alignment of LLMs with human preferences.", "key_contributions": ["Introduction of an intermediate-grained reward model that scores sentences.", "Development of a novel attention mechanism to aggregate sentence scores into response-level scores.", "Demonstration of improved performance on standard benchmarks for reward modeling and alignment evaluation."], "limitations": "", "keywords": ["reward models", "language models", "human preferences", "reinforcement learning", "attention mechanism"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.17739", "pdf": "https://arxiv.org/pdf/2503.17739.pdf", "abs": "https://arxiv.org/abs/2503.17739", "title": "Enhancing Arabic Automated Essay Scoring with Synthetic Data and Error Injection", "authors": ["Chatrine Qwaider", "Bashar Alhafni", "Kirill Chirkunov", "Nizar Habash", "Ted Briscoe"], "categories": ["cs.CL"], "comment": null, "summary": "Automated Essay Scoring (AES) plays a crucial role in assessing language\nlearners' writing quality, reducing grading workload, and providing real-time\nfeedback. The lack of annotated essay datasets inhibits the development of\nArabic AES systems. This paper leverages Large Language Models (LLMs) and\nTransformer models to generate synthetic Arabic essays for AES. We prompt an\nLLM to generate essays across the Common European Framework of Reference (CEFR)\nproficiency levels and introduce and compare two approaches to error injection.\nWe create a dataset of 3,040 annotated essays with errors injected using our\ntwo methods. Additionally, we develop a BERT-based Arabic AES system calibrated\nto CEFR levels. Our experimental results demonstrate the effectiveness of our\nsynthetic dataset in improving Arabic AES performance. We make our code and\ndata publicly available.", "AI": {"tldr": "This paper presents a method for generating synthetic Arabic essays to enhance Automated Essay Scoring (AES) systems using Large Language Models (LLMs).", "motivation": "The lack of annotated essay datasets hampers the development of Arabic AES systems, leading to a need for synthetic data generation.", "method": "Utilizing Large Language Models (LLMs) and Transformer models, the authors generate essays across CEFR proficiency levels and implement two error injection approaches.", "result": "The experiments conducted demonstrate that the synthetic dataset significantly improves the performance of a BERT-based Arabic AES system calibrated to CEFR levels.", "conclusion": "The study successfully creates a dataset of 3,040 annotated essays, showcasing the potential of synthetic data in advancing Arabic language assessment tools.", "key_contributions": ["Development of a synthetic Arabic essay dataset with injected errors", "Comparison of two error injection methods", "Implementation of a BERT-based AES system for Arabic calibrated to CEFR levels"], "limitations": "The paper may have limitations regarding the generalizability of the synthetic essays to real-world writing.", "keywords": ["Automated Essay Scoring", "Arabic language", "Large Language Models", "Transformer models", "Dataset generation"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2504.08024", "pdf": "https://arxiv.org/pdf/2504.08024.pdf", "abs": "https://arxiv.org/abs/2504.08024", "title": "Summarizing Speech: A Comprehensive Survey", "authors": ["Fabian Retkowski", "Maike Züfle", "Andreas Sudmann", "Dinah Pfau", "Shinji Watanabe", "Jan Niehues", "Alexander Waibel"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Speech summarization has become an essential tool for efficiently managing\nand accessing the growing volume of spoken and audiovisual content. However,\ndespite its increasing importance, speech summarization remains loosely\ndefined. The field intersects with several research areas, including speech\nrecognition, text summarization, and specific applications like meeting\nsummarization. This survey not only examines existing datasets and evaluation\nprotocols, which are crucial for assessing the quality of summarization\napproaches, but also synthesizes recent developments in the field, highlighting\nthe shift from traditional systems to advanced models like fine-tuned cascaded\narchitectures and end-to-end solutions. In doing so, we surface the ongoing\nchallenges, such as the need for realistic evaluation benchmarks, multilingual\ndatasets, and long-context handling.", "AI": {"tldr": "This survey on speech summarization outlines its importance, reviews existing datasets and evaluation methods, and highlights advancements toward more sophisticated models while addressing ongoing challenges.", "motivation": "To provide a comprehensive overview of speech summarization due to its rising significance in managing spoken content, while clarifying its loose definitions and intersection with various research areas.", "method": "The paper surveys current techniques in speech summarization, including existing datasets and evaluation protocols, and synthesizes recent developments and shifts towards advanced models.", "result": "The survey identifies a need for improved evaluation benchmarks, multilingual datasets, and solutions for managing long-contexts in speech summarization.", "conclusion": "The paper concludes that despite advances, several challenges persist in the effective evaluation and application of speech summarization technologies.", "key_contributions": ["Comprehensive overview of speech summarization techniques", "Assessment of current datasets and evaluation methods", "Identification of ongoing challenges and future directions"], "limitations": "", "keywords": ["speech summarization", "dataset evaluation", "multilingual datasets", "fine-tuned models", "context handling"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.06395", "pdf": "https://arxiv.org/pdf/2506.06395.pdf", "abs": "https://arxiv.org/abs/2506.06395", "title": "Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models", "authors": ["Pengyi Li", "Matvey Skripkin", "Alexander Zubrey", "Andrey Kuznetsov", "Ivan Oseledets"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) excel at reasoning, yet post-training remains\ncritical for aligning their behavior with task goals. Existing reinforcement\nlearning (RL) methods often depend on costly human annotations or external\nreward models. We propose Reinforcement Learning via Self-Confidence (RLSC),\nwhich uses the model's own confidence as reward signals-eliminating the need\nfor labels, preference models, or reward engineering. Applied to\nQwen2.5-Math-7B with only 16 samples per question and 10 or 20 training steps,\nRLSC improves accuracy by +13.4% on AIME2024, +21.2% on MATH500, +21.7% on\nMinerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23. RLSC provides a\nsimple, scalable post-training method for inference models, requiring only a\nsmall number of samples and unlabelled supervision.", "AI": {"tldr": "RLSC improves LLM performance using self-generated confidence as reward signals without needing external labels.", "motivation": "To enhance alignment of large language models with task goals post-training without relying on costly human annotations or external reward models.", "method": "The proposed method, Reinforcement Learning via Self-Confidence (RLSC), utilizes the model's own confidence as reward signals during post-training to guide learning.", "result": "RLSC exhibits significant accuracy improvements: +13.4% on AIME2024, +21.2% on MATH500, +21.7% on Minerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23 with limited training data.", "conclusion": "RLSC serves as a simple and scalable post-training method for inference models, needing only a handful of samples and unlabelled data to improve performance.", "key_contributions": ["Introduces a novel self-confidence based reward mechanism for LLMs", "Demonstrates significant performance improvements on multiple math benchmarks", "Eliminates the need for costly human annotations in training"], "limitations": "", "keywords": ["Reinforcement Learning", "Large Language Models", "Self-Confidence", "Post-Training", "Unlabelled Supervision"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.07044", "pdf": "https://arxiv.org/pdf/2506.07044.pdf", "abs": "https://arxiv.org/abs/2506.07044", "title": "Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning", "authors": ["LASA Team", "Weiwen Xu", "Hou Pong Chan", "Long Li", "Mahani Aljunied", "Ruifeng Yuan", "Jianyu Wang", "Chenghao Xiao", "Guizhen Chen", "Chaoqun Liu", "Zhaodonghui Li", "Yu Sun", "Junao Shen", "Chaojun Wang", "Jie Tan", "Deli Zhao", "Tingyang Xu", "Hao Zhang", "Yu Rong"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Technical Report, 53 pages, 25 tables, and 16 figures", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities in understanding common visual elements, largely due to their\nlarge-scale datasets and advanced training strategies. However, their\neffectiveness in medical applications remains limited due to the inherent\ndiscrepancies between data and tasks in medical scenarios and those in the\ngeneral domain. Concretely, existing medical MLLMs face the following critical\nlimitations: (1) limited coverage of medical knowledge beyond imaging, (2)\nheightened susceptibility to hallucinations due to suboptimal data curation\nprocesses, (3) lack of reasoning capabilities tailored for complex medical\nscenarios. To address these challenges, we first propose a comprehensive data\ncuration procedure that (1) efficiently acquires rich medical knowledge data\nnot only from medical imaging but also from extensive medical texts and\ngeneral-domain data; and (2) synthesizes accurate medical captions, visual\nquestion answering (VQA), and reasoning samples. As a result, we build a\nmultimodal dataset enriched with extensive medical knowledge. Building on the\ncurated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu\nundergoes multi-stage training to embed medical expertise and enhance its\ntask-solving capabilities progressively. Besides, we preliminarily explore the\npotential of applying reinforcement learning with verifiable rewards paradigm\nto enhance Lingshu's medical reasoning ability. Additionally, we develop\nMedEvalKit, a unified evaluation framework that consolidates leading multimodal\nand textual medical benchmarks for standardized, fair, and efficient model\nassessment. We evaluate the performance of Lingshu on three fundamental medical\ntasks, multimodal QA, text-based QA, and medical report generation. The results\nshow that Lingshu consistently outperforms the existing open-source multimodal\nmodels on most tasks ...", "AI": {"tldr": "This paper presents Lingshu, a medical-specialized Multimodal Large Language Model, addressing current limitations in medical AI applications through enhanced data curation, training methods, and evaluation framework.", "motivation": "The study aims to overcome the gaps in medical applications of MLLMs, specifically addressing issues like limited medical knowledge coverage and reasoning capabilities.", "method": "The authors propose a comprehensive data curation process that gathers rich medical knowledge from imaging and extensive texts, and builds a specialized model (Lingshu) that incorporates this data through multi-stage training and reinforcement learning.", "result": "Lingshu shows significant improvements in performance across three core medical tasks compared to existing multimodal models, indicating effective enhancement in task-solving capabilities and reasoning.", "conclusion": "The development of Lingshu and the associated MedEvalKit provides a step forward in the effective application of MLLMs in the medical domain, demonstrating the efficacy of the proposed methodologies.", "key_contributions": ["Introduction of Lingshu, a medical-specialized MLLM", "Proposed comprehensive medical data curation procedure", "Development of MedEvalKit for standardized model evaluation"], "limitations": "Potential challenges in data quality and the generalizability of the model in diverse medical scenarios.", "keywords": ["Multimodal Large Language Models", "Medical AI", "Data Curation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.07667", "pdf": "https://arxiv.org/pdf/2506.07667.pdf", "abs": "https://arxiv.org/abs/2506.07667", "title": "Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech on Twitch", "authors": ["Prarabdh Shukla", "Wei Yin Chong", "Yash Patel", "Brennan Schaffner", "Danish Pruthi", "Arjun Bhagoji"], "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": "Accepted to ACL 2025 (main) conference", "summary": "To meet the demands of content moderation, online platforms have resorted to\nautomated systems. Newer forms of real-time engagement($\\textit{e.g.}$, users\ncommenting on live streams) on platforms like Twitch exert additional pressures\non the latency expected of such moderation systems. Despite their prevalence,\nrelatively little is known about the effectiveness of these systems. In this\npaper, we conduct an audit of Twitch's automated moderation tool\n($\\texttt{AutoMod}$) to investigate its effectiveness in flagging hateful\ncontent. For our audit, we create streaming accounts to act as siloed test\nbeds, and interface with the live chat using Twitch's APIs to send over\n$107,000$ comments collated from $4$ datasets. We measure $\\texttt{AutoMod}$'s\naccuracy in flagging blatantly hateful content containing misogyny, racism,\nableism and homophobia. Our experiments reveal that a large fraction of hateful\nmessages, up to $94\\%$ on some datasets, $\\textit{bypass moderation}$.\nContextual addition of slurs to these messages results in $100\\%$ removal,\nrevealing $\\texttt{AutoMod}$'s reliance on slurs as a moderation signal. We\nalso find that contrary to Twitch's community guidelines, $\\texttt{AutoMod}$\nblocks up to $89.5\\%$ of benign examples that use sensitive words in\npedagogical or empowering contexts. Overall, our audit points to large gaps in\n$\\texttt{AutoMod}$'s capabilities and underscores the importance for such\nsystems to understand context effectively.", "AI": {"tldr": "An audit of Twitch's AutoMod reveals its inability to effectively moderate hateful content, with up to 94% of such messages bypassing the system and excessive blocking of benign messages.", "motivation": "To investigate the effectiveness of automated content moderation systems, particularly in real-time engagement contexts on platforms like Twitch.", "method": "The study involved creating test accounts and simulating live chat interactions using Twitch's APIs to send over 107,000 comments, measuring AutoMod's accuracy in flagging hateful content.", "result": "The audit found that a significant portion (up to 94%) of hateful messages bypassed moderation, while contextually added slurs resulted in 100% removal, indicating a reliance on slur signals. Up to 89.5% of benign content was incorrectly blocked.", "conclusion": "The findings highlight substantial gaps in AutoMod's moderation capabilities and the necessity for such systems to better understand message context.", "key_contributions": ["Detailed audit of Twitch's AutoMod effectiveness", "Identification of context-related limitations in moderation", "Quantitative analysis of flagged versus unflagged content"], "limitations": "The study only focuses on Twitch's AutoMod and may not generalize to other platforms' moderation systems.", "keywords": ["content moderation", "Twitch", "AutoMod", "hateful content", "HCI"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2503.16586", "pdf": "https://arxiv.org/pdf/2503.16586.pdf", "abs": "https://arxiv.org/abs/2503.16586", "title": "Big Help or Big Brother? Auditing Tracking, Profiling, and Personalization in Generative AI Assistants", "authors": ["Yash Vekaria", "Aurelio Loris Canino", "Jonathan Levitsky", "Alex Ciechonski", "Patricia Callejo", "Anna Maria Mandalari", "Zubair Shafiq"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CR", "cs.CY", "I.2; I.2.1; I.2.7; H.3.4; K.4; K.4.1; H.1; H.1.2; H.5.2; H.4.3"], "comment": null, "summary": "Generative AI (GenAI) browser assistants integrate powerful capabilities of\nGenAI in web browsers to provide rich experiences such as question answering,\ncontent summarization, and agentic navigation. These assistants, available\ntoday as browser extensions, can not only track detailed browsing activity such\nas search and click data, but can also autonomously perform tasks such as\nfilling forms, raising significant privacy concerns. It is crucial to\nunderstand the design and operation of GenAI browser extensions, including how\nthey collect, store, process, and share user data. To this end, we study their\nability to profile users and personalize their responses based on explicit or\ninferred demographic attributes and interests of users. We perform network\ntraffic analysis and use a novel prompting framework to audit tracking,\nprofiling, and personalization by the ten most popular GenAI browser assistant\nextensions. We find that instead of relying on local in-browser models, these\nassistants largely depend on server-side APIs, which can be auto-invoked\nwithout explicit user interaction. When invoked, they collect and share webpage\ncontent, often the full HTML DOM and sometimes even the user's form inputs,\nwith their first-party servers. Some assistants also share identifiers and user\nprompts with third-party trackers such as Google Analytics. The collection and\nsharing continues even if a webpage contains sensitive information such as\nhealth or personal information such as name or SSN entered in a web form. We\nfind that several GenAI browser assistants infer demographic attributes such as\nage, gender, income, and interests and use this profile--which carries across\nbrowsing contexts--to personalize responses. In summary, our work shows that\nGenAI browser assistants can and do collect personal and sensitive information\nfor profiling and personalization with little to no safeguards.", "AI": {"tldr": "Study on privacy implications of Generative AI (GenAI) browser assistants that profile users with little safeguards.", "motivation": "Understanding the design and operation of GenAI browser extensions and their impact on user privacy.", "method": "Network traffic analysis and a novel prompting framework to audit popular GenAI browser assistants.", "result": "GenAI assistants rely on server-side APIs for data collection, often sharing sensitive information without user consent and profiling users based on inferred attributes.", "conclusion": "GenAI browser assistants can collect and use personal information for profiling with inadequate privacy protections.", "key_contributions": ["Analysis of data collection methods in GenAI browser assistants", "Revelation of profiling based on demographic attributes", "Identification of privacy concerns related to sensitive information sharing"], "limitations": "", "keywords": ["Generative AI", "browser assistants", "user profiling", "privacy concerns", "network analysis"], "importance_score": 8, "read_time_minutes": 10}}
