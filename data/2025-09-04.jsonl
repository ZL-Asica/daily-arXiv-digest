{"id": "2509.02732", "pdf": "https://arxiv.org/pdf/2509.02732.pdf", "abs": "https://arxiv.org/abs/2509.02732", "title": "STRive: An association rule-based system for the exploration of spatiotemporal categorical data", "authors": ["Mauro Diaz", "Luis Sante", "Joel Perca", "João Victor da Silva", "Nivan Ferreira", "Jorge Poco"], "categories": ["cs.HC"], "comment": null, "summary": "Effectively analyzing spatiotemporal data plays a central role in\nunderstanding real-world phenomena and informing decision-making. Capturing the\ninteraction between spatial and temporal dimensions also helps explain the\nunderlying structure of the data. However, most datasets do not reveal\nattribute relationships, requiring additional algorithms to extract meaningful\npatterns. Existing visualization tools often focus either on attribute\nrelationships or spatiotemporal analysis, but rarely support both\nsimultaneously. In this paper, we present STRive (SpatioTemporal Rule\nInteractive Visual Explorer), a visual analytics system that enables users to\nuncover and explore spatial and temporal patterns in data. At the core of\nSTRive lies Association Rule Mining (ARM), which we apply to spatiotemporal\ndatasets to generate interpretable and actionable insights. We combine ARM with\nmultiple interactive mechanisms to analyze the extracted relationships.\nAssociation rules serve as interpretable guidance mechanisms for visual\nanalytics by highlighting the meaningful aspects of the data that users should\ninvestigate. Our methodology includes three key steps: rule generation, rule\nclustering, and interactive visualization. STRive offers two modes of analysis.\nThe first operates at the rule cluster level and includes four coordinated\nviews, each showing a different facet of a cluster, including its temporal and\nspatial behavior. The second mode mirrors the first but focuses on individual\nrules within a selected cluster. We evaluate the effectiveness of STRive\nthrough two case studies involving real-world datasets -- fatal vehicle\naccidents and urban crime. Results demonstrate the system's ability to support\nthe discovery and analysis of interpretable patterns in complex spatiotemporal\ncontexts."}
{"id": "2509.02878", "pdf": "https://arxiv.org/pdf/2509.02878.pdf", "abs": "https://arxiv.org/abs/2509.02878", "title": "Designing a Lightweight GenAI Interface for Visual Data Analysis", "authors": ["Ratanond Koonchanok", "Alex Kale", "Khairi Reda"], "categories": ["cs.HC"], "comment": null, "summary": "Recent advances in Generative AI have transformed how users interact with\ndata analysis through natural language interfaces. However, many systems rely\ntoo heavily on LLMs, creating risks of hallucination, opaque reasoning, and\nreduced user control. We present a hybrid visual analysis system that\nintegrates GenAI in a constrained, high-level role to support statistical\nmodeling while preserving transparency and user agency. GenAI translates\nnatural language intent into formal statistical formulations, while interactive\nvisualizations surface model behavior, residual patterns, and hypothesis\ncomparisons to guide iterative exploration. Model fitting, diagnostics, and\nhypothesis testing are delegated entirely to a structured R-based backend,\nensuring correctness, interpretability, and reproducibility. By combining\nGenAI-assisted intent translation with visualization-driven reasoning, our\napproach broadens access to modeling tools without compromising rigor. We\npresent an example use case of the tool and discuss challenges and\nopportunities for future research."}
{"id": "2509.02910", "pdf": "https://arxiv.org/pdf/2509.02910.pdf", "abs": "https://arxiv.org/abs/2509.02910", "title": "The Basic B*** Effect: The Use of LLM-based Agents Reduces the Distinctiveness and Diversity of People's Choices", "authors": ["Sandra C. Matz", "C. Blaine Horton", "Sofie Goethals"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) increasingly act on people's behalf: they write\nemails, buy groceries, and book restaurants. While the outsourcing of human\ndecision-making to AI can be both efficient and effective, it raises a\nfundamental question: how does delegating identity-defining choices to AI\nreshape who people become? We study the impact of agentic LLMs on two\nidentity-relevant outcomes: interpersonal distinctiveness - how unique a\nperson's choices are relative to others - and intrapersonal diversity - the\nbreadth of a single person's choices over time. Using real choices drawn from\nsocial-media behavior of 1,000 U.S. users (110,000 choices in total), we\ncompare a generic and personalized agent to a human baseline. Both agents shift\npeople's choices toward more popular options, reducing the distinctiveness of\ntheir behaviors and preferences. While the use of personalized agents tempers\nthis homogenization (compared to the generic AI), it also more strongly\ncompresses the diversity of people's preference portfolios by narrowing what\nthey explore across topics and psychological affinities. Understanding how AI\nagents might flatten human experience, and how using generic versus\npersonalized agents involves distinctiveness-diversity trade-offs, is critical\nfor designing systems that augment rather than constrain human agency, and for\nsafeguarding diversity in thought, taste, and expression."}
{"id": "2509.02933", "pdf": "https://arxiv.org/pdf/2509.02933.pdf", "abs": "https://arxiv.org/abs/2509.02933", "title": "Demonstrating Visual Information Manipulation Attacks in Augmented Reality: A Hands-On Miniature City-Based Setup", "authors": ["Yanming Xiu", "Maria Gorlatova"], "categories": ["cs.HC"], "comment": "The paper has been accepted to 2025 MobiHoc 1st Workshop on Enhancing\n  Security, Privacy, and Trust in Extended Reality (XR) Systems", "summary": "Augmented reality (AR) enhances user interaction with the real world but also\npresents vulnerabilities, particularly through Visual Information Manipulation\n(VIM) attacks. These attacks alter important real-world visual cues, leading to\nuser confusion and misdirected actions. In this demo, we present a hands-on\nexperience using a miniature city setup, where users interact with manipulated\nAR content via the Meta Quest 3. The demo highlights the impact of VIM attacks\non user decision-making and underscores the need for effective security\nmeasures in AR systems. Future work includes a user study and cross-platform\ntesting."}
{"id": "2509.02785", "pdf": "https://arxiv.org/pdf/2509.02785.pdf", "abs": "https://arxiv.org/abs/2509.02785", "title": "DrDiff: Dynamic Routing Diffusion with Hierarchical Attention for Breaking the Efficiency-Quality Trade-off", "authors": ["Jusheng Zhang", "Yijia Fan", "Kaitong Cai", "Zimeng Huang", "Xiaofei Sun", "Jian Wang", "Chengpei Tang", "Keze Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted 2025 EMNLP (MainConference)", "summary": "This paper introduces DrDiff, a novel framework for long-text generation that\novercomes the efficiency-quality trade-off through three core technologies.\nFirst, we design a dynamic expert scheduling mechanism that intelligently\nallocates computational resources during the diffusion process based on text\ncomplexity, enabling more efficient handling of text generation tasks of\nvarying difficulty. Second, we introduce a Hierarchical Sparse Attention (HSA)\nmechanism that adaptively adjusts attention patterns according to a variety of\ninput lengths, reducing computational complexity from O($n^2$) to O($n$) while\nmaintaining model performance. Finally, we propose a soft absorption guidance\noptimization strategy that combines with DPM-solver++ to reduce diffusion\nsteps, significantly improving generation speed. Comprehensive experiments on\nvarious long-text generation benchmarks demonstrate the superiority of our\nDrDiff over the existing SOTA methods."}
{"id": "2509.03164", "pdf": "https://arxiv.org/pdf/2509.03164.pdf", "abs": "https://arxiv.org/abs/2509.03164", "title": "OPRA-Vis: Visual Analytics System to Assist Organization-Public Relationship Assessment with Large Language Models", "authors": ["Sangbong Yoo", "Seongbum Seo", "Chanyoung Yoon", "Hyelim Lee", "Jeong-Nam Kim", "Chansoo Kim", "Yun Jang", "Takanori Fujiwara"], "categories": ["cs.HC"], "comment": null, "summary": "Analysis of public opinions collected from digital media helps organizations\nmaintain positive relationships with the public. Such public relations (PR)\nanalysis often involves assessing opinions, for example, measuring how strongly\npeople trust an organization. Pre-trained Large Language Models (LLMs) hold\ngreat promise for supporting Organization-Public Relationship Assessment (OPRA)\nbecause they can map unstructured public text to OPRA dimensions and articulate\nrationales through prompting. However, adapting LLMs for PR analysis typically\nrequires fine-tuning on large labeled datasets, which is both labor-intensive\nand knowledge-intensive, making it difficult for PR researchers to apply these\nmodels. In this paper, we present OPRA-Vis, a visual analytics system that\nleverages LLMs for OPRA without requiring extensive labeled data. Our framework\nemploys Chain-of-Thought prompting to guide LLMs in analyzing public opinion\ndata by incorporating PR expertise directly into the reasoning process.\nFurthermore, OPRA-Vis provides visualizations that reveal the clues and\nreasoning paths used by LLMs, enabling users to explore, critique, and refine\nmodel decisions. We demonstrate the effectiveness of OPRA-Vis through two\nreal-world use cases and evaluate it quantitatively, through comparisons with\nalternative LLMs and prompting strategies, and qualitatively, through\nassessments of usability, effectiveness, and expert feedback."}
{"id": "2509.02830", "pdf": "https://arxiv.org/pdf/2509.02830.pdf", "abs": "https://arxiv.org/abs/2509.02830", "title": "SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR", "authors": ["Pu Wang", "Shinji Watanabe", "Hugo Van hamme"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted by IEEE ASRU 2025", "summary": "Parameter-efficient fine-tuning (PEFT) has emerged as a scalable solution for\nadapting large foundation models. While low-rank adaptation (LoRA) is widely\nused in speech applications, its state-of-the-art variants, e.g., VeRA, DoRA,\nPiSSA, and SVFT, are developed mainly for language and vision tasks, with\nlimited validation in speech. This work presents the first comprehensive\nintegration and benchmarking of these PEFT methods within ESPnet. We further\nintroduce structured SVD-guided (SSVD) fine-tuning, which selectively rotates\ninput-associated right singular vectors while keeping output-associated vectors\nfixed to preserve semantic mappings. This design enables robust domain\nadaptation with minimal trainable parameters and improved efficiency. We\nevaluate all methods on domain-shifted speech recognition tasks, including\nchild speech and dialectal variation, across model scales from 0.1B to 2B. All\nimplementations are released in ESPnet to support reproducibility and future\nwork."}
{"id": "2509.03181", "pdf": "https://arxiv.org/pdf/2509.03181.pdf", "abs": "https://arxiv.org/abs/2509.03181", "title": "Beyond Words: Interjection Classification for Improved Human-Computer Interaction", "authors": ["Yaniv Goren", "Yuval Cohen", "Alexander Apartsin", "Yehudit Aperstein"], "categories": ["cs.HC", "cs.LG"], "comment": "9 pages", "summary": "In the realm of human-computer interaction, fostering a natural dialogue\nbetween humans and machines is paramount. A key, often overlooked, component of\nthis dialogue is the use of interjections such as \"mmm\" and \"hmm\". Despite\ntheir frequent use to express agreement, hesitation, or requests for\ninformation, these interjections are typically dismissed as \"non-words\" by\nAutomatic Speech Recognition (ASR) engines. Addressing this gap, we introduce a\nnovel task dedicated to interjection classification, a pioneer in the field to\nour knowledge. This task is challenging due to the short duration of\ninterjection signals and significant inter- and intra-speaker variability. In\nthis work, we present and publish a dataset of interjection signals collected\nspecifically for interjection classification. We employ this dataset to train\nand evaluate a baseline deep learning model. To enhance performance, we augment\nthe training dataset using techniques such as tempo and pitch transformation,\nwhich significantly improve classification accuracy, making models more robust.\nThe interjection dataset, a Python library for the augmentation pipeline,\nbaseline model, and evaluation scripts, are available to the research\ncommunity."}
{"id": "2509.02834", "pdf": "https://arxiv.org/pdf/2509.02834.pdf", "abs": "https://arxiv.org/abs/2509.02834", "title": "Clustering Discourses: Racial Biases in Short Stories about Women Generated by Large Language Models", "authors": ["Gustavo Bonil", "João Gondim", "Marina dos Santos", "Simone Hashiguti", "Helena Maia", "Nadia Silva", "Helio Pedrini", "Sandra Avila"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 3 figures. Accepted at STIL @ BRACIS 2025", "summary": "This study investigates how large language models, in particular LLaMA\n3.2-3B, construct narratives about Black and white women in short stories\ngenerated in Portuguese. From 2100 texts, we applied computational methods to\ngroup semantically similar stories, allowing a selection for qualitative\nanalysis. Three main discursive representations emerge: social overcoming,\nancestral mythification and subjective self-realization. The analysis uncovers\nhow grammatically coherent, seemingly neutral texts materialize a crystallized,\ncolonially structured framing of the female body, reinforcing historical\ninequalities. The study proposes an integrated approach, that combines machine\nlearning techniques with qualitative, manual discourse analysis."}
{"id": "2509.03199", "pdf": "https://arxiv.org/pdf/2509.03199.pdf", "abs": "https://arxiv.org/abs/2509.03199", "title": "Finding My Way: Influence of Different Audio Augmented Reality Navigation Cues on User Experience and Subjective Usefulness", "authors": ["Sina Hinzmann", "Francesco Vona", "Juliane Henning", "Mohamed Amer", "Omar Abdellatif", "Tanja Kojic", "Jan-Niklas Voigt-Antons"], "categories": ["cs.HC"], "comment": null, "summary": "As augmented reality (AR) becomes increasingly prevalent in mobile and\ncontext-aware applications, the role of auditory cues in guiding users through\nphysical environments is becoming critical. This study investigates the\neffectiveness and user experience of various categories of audio cues,\nincluding fully non-verbal sounds and speech-derived Spearcons, during outdoor\nnavigation tasks using the Meta Quest 3 headset. Twenty participants navigated\nfive outdoor routes using audio-only cue types: Artificial Sounds, Nature\nSounds, Spearcons, Musical Instruments, and Auditory Icons. Subjective\nevaluations were collected to assess the perceived effectiveness and user\nexperience of each sound type. Results revealed significant differences in\nperceived novelty and stimulation across sound types. Artificial Sounds and\nMusical Instruments were rated higher than Spearcons in novelty, while\nArtificial Sounds were also rated higher than Spearcons in stimulation. Overall\npreference was evenly split between Nature Sounds and Artificial Sounds. These\nfindings suggest that incorporating aspects of novelty and user engagement in\nauditory feedback design may enhance the effectiveness of AR navigation\nsystems."}
{"id": "2509.02855", "pdf": "https://arxiv.org/pdf/2509.02855.pdf", "abs": "https://arxiv.org/abs/2509.02855", "title": "IDEAlign: Comparing Large Language Models to Human Experts in Open-ended Interpretive Annotations", "authors": ["Hyunji Nam", "Lucia Langlois", "James Malamut", "Mei Tan", "Dorottya Demszky"], "categories": ["cs.CL", "cs.CY"], "comment": "10 pages, 9 pages for appendix", "summary": "Large language models (LLMs) are increasingly applied to open-ended,\ninterpretive annotation tasks, such as thematic analysis by researchers or\ngenerating feedback on student work by teachers. These tasks involve free-text\nannotations requiring expert-level judgments grounded in specific objectives\n(e.g., research questions or instructional goals). Evaluating whether\nLLM-generated annotations align with those generated by expert humans is\nchallenging to do at scale, and currently, no validated, scalable measure of\nsimilarity in ideas exists. In this paper, we (i) introduce the scalable\nevaluation of interpretive annotation by LLMs as a critical and understudied\ntask, (ii) propose IDEAlgin, an intuitive benchmarking paradigm for capturing\nexpert similarity ratings via a \"pick-the-odd-one-out\" triplet judgment task,\nand (iii) evaluate various similarity metrics, including vector-based ones\n(topic models, embeddings) and LLM-as-a-judge via IDEAlgin, against these human\nbenchmarks. Applying this approach to two real-world educational datasets\n(interpretive analysis and feedback generation), we find that vector-based\nmetrics largely fail to capture the nuanced dimensions of similarity meaningful\nto experts. Prompting LLMs via IDEAlgin significantly improves alignment with\nexpert judgments (9-30% increase) compared to traditional lexical and\nvector-based metrics. These results establish IDEAlgin as a promising paradigm\nfor evaluating LLMs against open-ended expert annotations at scale, informing\nresponsible deployment of LLMs in education and beyond."}
{"id": "2509.03232", "pdf": "https://arxiv.org/pdf/2509.03232.pdf", "abs": "https://arxiv.org/abs/2509.03232", "title": "Card Sorting with Fewer Cards and the Same Mental Models? A Re-examination of an Established Practice", "authors": ["Eduard Kuric", "Peter Demcak", "Matus Krajcovic"], "categories": ["cs.HC", "H.5"], "comment": null, "summary": "To keep card sorting with a lot of cards concise, a common strategy for\ngauging mental models involves presenting participants with fewer randomly\nselected cards instead of the full set. This is a decades-old practice, but its\neffects lacked systematic examination. To assess how randomized subsets affect\ndata, we conducted an experiment with 160 participants. We compared results\nbetween full and randomized 60\\% card sets, then analyzed sample size\nrequirements and the impacts of individual personality and cognitive factors.\nOur results demonstrate that randomized subsets can yield comparable similarity\nmatrices to standard card sorting, but thematic patterns in categories can\ndiffer. Increased data variability also warrants larger sample sizes (25-35 for\n60% card subset). Results indicate that personality traits and cognitive\nreflection interact with card sorting. Our research suggests evidence-based\npractices for conducting card sorting while exposing the influence of study\ndesign and individual differences on measurement of mental models."}
{"id": "2509.02864", "pdf": "https://arxiv.org/pdf/2509.02864.pdf", "abs": "https://arxiv.org/abs/2509.02864", "title": "A-SEA3L-QA: A Fully Automated Self-Evolving, Adversarial Workflow for Arabic Long-Context Question-Answer Generation", "authors": ["Kesen Wang", "Daulet Toibazar", "Pedro J. Moreno"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present an end-to-end, self-evolving adversarial workflow for long-context\nQuestion-Answer (QA) Generation in Arabic. By orchestrating multiple\nspecialized LVLMs: a question generator, an evaluator, and a swarm of answer\ngenerators, our system iteratively refines its own performance without any\nhuman intervention. Starting from raw, multi-page Arabic documents across\ndiverse domains, the question generator produces fine-grained, context-aware\nqueries to be tackled by the answer generator swarm, and the evaluator assesses\nand feeds back quality metrics. This closed-loop cycle enables continuous\nlearning: low-confidence outputs trigger automated re-generation and model\nupdates, progressively enhancing question difficulty and relevance. Moreover,\nwe set the quality metrics as a tunable hyperparameter, enabling question\ngeneration at controllable and customizable difficulty levels. We release\nAraLongBench, a large-scale Arabic benchmark of single- and multi-page\nchallenges spanning hundreds of pages, and demonstrate that our self-evolving\nworkflow substantially outperform static pipelines, markedly boosting the\nlong-context comprehension capabilities of leading Arabic Large Vision Language\nModels (LVLMs). Lastly, we also meticulously architect a fully automated\nagentic workflow for long-context Arabic document collection."}
{"id": "2509.03271", "pdf": "https://arxiv.org/pdf/2509.03271.pdf", "abs": "https://arxiv.org/abs/2509.03271", "title": "Beyond Quantification: Navigating Uncertainty in Professional AI Systems", "authors": ["Sylvie Delacroix", "Diana Robinson", "Umang Bhatt", "Jacopo Domenicucci", "Jessica Montgomery", "Gael Varoquaux", "Carl Henrik Ek", "Vincent Fortuin", "Yulan He", "Tom Diethe", "Neill Campbell", "Mennatallah El-Assady", "Soren Hauberg", "Ivana Dusparic", "Neil Lawrence"], "categories": ["cs.HC"], "comment": null, "summary": "The growing integration of large language models across professional domains\ntransforms how experts make critical decisions in healthcare, education, and\nlaw. While significant research effort focuses on getting these systems to\ncommunicate their outputs with probabilistic measures of reliability, many\nconsequential forms of uncertainty in professional contexts resist such\nquantification. A physician pondering the appropriateness of documenting\npossible domestic abuse, a teacher assessing cultural sensitivity, or a\nmathematician distinguishing procedural from conceptual understanding face\nforms of uncertainty that cannot be reduced to percentages. This paper argues\nfor moving beyond simple quantification toward richer expressions of\nuncertainty essential for beneficial AI integration. We propose participatory\nrefinement processes through which professional communities collectively shape\nhow different forms of uncertainty are communicated. Our approach acknowledges\nthat uncertainty expression is a form of professional sense-making that\nrequires collective development rather than algorithmic optimization."}
{"id": "2509.02908", "pdf": "https://arxiv.org/pdf/2509.02908.pdf", "abs": "https://arxiv.org/abs/2509.02908", "title": "Advancing Minority Stress Detection with Transformers: Insights from the Social Media Datasets", "authors": ["Santosh Chapagain", "Cory J Cascalheira", "Shah Muhammad Hamdi", "Soukaina Filali Boubrahimi", "Jillian R. Scheer"], "categories": ["cs.CL"], "comment": "Accepted in Social Network Analysis and Mining Journal (SNAM)", "summary": "Individuals from sexual and gender minority groups experience\ndisproportionately high rates of poor health outcomes and mental disorders\ncompared to their heterosexual and cisgender counterparts, largely as a\nconsequence of minority stress as described by Meyer's (2003) model. This study\npresents the first comprehensive evaluation of transformer-based architectures\nfor detecting minority stress in online discourse. We benchmark multiple\ntransformer models including ELECTRA, BERT, RoBERTa, and BART against\ntraditional machine learning baselines and graph-augmented variants. We further\nassess zero-shot and few-shot learning paradigms to assess their applicability\non underrepresented datasets. Experiments are conducted on the two largest\npublicly available Reddit corpora for minority stress detection, comprising\n12,645 and 5,789 posts, and are repeated over five random seeds to ensure\nrobustness. Our results demonstrate that integrating graph structure\nconsistently improves detection performance across transformer-only models and\nthat supervised fine-tuning with relational context outperforms zero and\nfew-shot approaches. Theoretical analysis reveals that modeling social\nconnectivity and conversational context via graph augmentation sharpens the\nmodels' ability to identify key linguistic markers such as identity\nconcealment, internalized stigma, and calls for support, suggesting that\ngraph-enhanced transformers offer the most reliable foundation for digital\nhealth interventions and public health policy."}
{"id": "2509.03392", "pdf": "https://arxiv.org/pdf/2509.03392.pdf", "abs": "https://arxiv.org/abs/2509.03392", "title": "More AI Assistance Reduces Cognitive Engagement: Examining the AI Assistance Dilemma in AI-Supported Note-Taking", "authors": ["Xinyue Chen", "Kunlin Ruan", "Kexin Phyllis Ju", "Nathan Yap", "Xu Wang"], "categories": ["cs.HC"], "comment": "Accepted by CSCW2025", "summary": "As AI tools become increasingly embedded in cognitively demanding tasks such\nas note-taking, questions remain about whether they enhance or undermine\ncognitive engagement. This paper examines the \"AI Assistance Dilemma\" in\nnote-taking, investigating how varying levels of AI support affect user\nengagement and comprehension. In a within-subject experiment, we asked\nparticipants (N=30) to take notes during lecture videos under three conditions:\nAutomated AI (high assistance with structured notes), Intermediate AI (moderate\nassistance with real-time summary, and Minimal AI (low assistance with\ntranscript). Results reveal that Intermediate AI yields the highest post-test\nscores and Automated AI the lowest. Participants, however, preferred the\nautomated setup due to its perceived ease of use and lower cognitive effort,\nsuggesting a discrepancy between preferred convenience and cognitive benefits.\nOur study provides insights into designing AI assistance that preserves\ncognitive engagement, offering implications for designing moderate AI support\nin cognitive tasks."}
{"id": "2509.02915", "pdf": "https://arxiv.org/pdf/2509.02915.pdf", "abs": "https://arxiv.org/abs/2509.02915", "title": "English Pronunciation Evaluation without Complex Joint Training: LoRA Fine-tuned Speech Multimodal LLM", "authors": ["Taekyung Ahn", "Hosung Nam"], "categories": ["cs.CL"], "comment": null, "summary": "This study demonstrates that a Multimodal Large Language Model (MLLM) adapted\nvia Low-Rank Adaptation (LoRA) can perform both Automatic Pronunciation\nAssessment (APA) and Mispronunciation Detection and Diagnosis (MDD)\nsimultaneously. Leveraging Microsoft's Phi-4-multimodal-instruct, our\nfine-tuning method eliminates the need for complex architectural changes or\nseparate training procedures conventionally required for these distinct tasks.\nFine-tuned on the Speechocean762 dataset, the pronunciation evaluation scores\npredicted by the model exhibited a strong Pearson Correlation Coefficient (PCC\n> 0.7) with human-assigned scores, while achieving low Word Error Rate (WER)\nand Phoneme Error Rate (PER) (both < 0.15). Notably, fine-tuning only the LoRA\nlayers was sufficient to achieve performance levels comparable to those\nachieved by fine-tuning all audio layers. This research highlights that an\nintegrated pronunciation assessment system can be established by adapting large\nmultimodal models without full fine-tuning, utilizing a significantly simpler\ntraining methodology compared to previous joint models designed for\nsimultaneous APA and MDD. This efficient LoRA-based approach paves the way for\nmore accessible, integrated, and effective Computer-Assisted Pronunciation\nTraining (CAPT) technologies for English L2 learners."}
{"id": "2509.03430", "pdf": "https://arxiv.org/pdf/2509.03430.pdf", "abs": "https://arxiv.org/abs/2509.03430", "title": "EclipseTouch: Touch Segmentation on Ad Hoc Surfaces using Worn Infrared Shadow Casting", "authors": ["Vimal Mollyn", "Nathan DeVrio", "Chris Harrison"], "categories": ["cs.HC", "cs.CV", "cs.GR", "cs.RO"], "comment": "Accepted to UIST 2025", "summary": "The ability to detect touch events on uninstrumented, everyday surfaces has\nbeen a long-standing goal for mixed reality systems. Prior work has shown that\nvirtual interfaces bound to physical surfaces offer performance and ergonomic\nbenefits over tapping at interfaces floating in the air. A wide variety of\napproaches have been previously developed, to which we contribute a new\nheadset-integrated technique called \\systemname. We use a combination of a\ncomputer-triggered camera and one or more infrared emitters to create\nstructured shadows, from which we can accurately estimate hover distance (mean\nerror of 6.9~mm) and touch contact (98.0\\% accuracy). We discuss how our\ntechnique works across a range of conditions, including surface material,\ninteraction orientation, and environmental lighting."}
{"id": "2509.02926", "pdf": "https://arxiv.org/pdf/2509.02926.pdf", "abs": "https://arxiv.org/abs/2509.02926", "title": "Decoding the Rule Book: Extracting Hidden Moderation Criteria from Reddit Communities", "authors": ["Youngwoo Kim", "Himanshu Beniwal", "Steven L. Johnson", "Thomas Hartvigsen"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Main", "summary": "Effective content moderation systems require explicit classification\ncriteria, yet online communities like subreddits often operate with diverse,\nimplicit standards. This work introduces a novel approach to identify and\nextract these implicit criteria from historical moderation data using an\ninterpretable architecture. We represent moderation criteria as score tables of\nlexical expressions associated with content removal, enabling systematic\ncomparison across different communities. Our experiments demonstrate that these\nextracted lexical patterns effectively replicate the performance of neural\nmoderation models while providing transparent insights into decision-making\nprocesses. The resulting criteria matrix reveals significant variations in how\nseemingly shared norms are actually enforced, uncovering previously\nundocumented moderation patterns including community-specific tolerances for\nlanguage, features for topical restrictions, and underlying subcategories of\nthe toxic speech classification."}
{"id": "2509.03451", "pdf": "https://arxiv.org/pdf/2509.03451.pdf", "abs": "https://arxiv.org/abs/2509.03451", "title": "SmartPoser: Arm Pose Estimation with a Smartphone and Smartwatch Using UWB and IMU Data", "authors": ["Nathan DeVrio", "Vimal Mollyn", "Chris Harrison"], "categories": ["cs.HC", "cs.CV", "cs.GR", "cs.RO"], "comment": "The first two listed authors contributed equally. Published at UIST\n  2023", "summary": "The ability to track a user's arm pose could be valuable in a wide range of\napplications, including fitness, rehabilitation, augmented reality input, life\nlogging, and context-aware assistants. Unfortunately, this capability is not\nreadily available to consumers. Systems either require cameras, which carry\nprivacy issues, or utilize multiple worn IMUs or markers. In this work, we\ndescribe how an off-the-shelf smartphone and smartwatch can work together to\naccurately estimate arm pose. Moving beyond prior work, we take advantage of\nmore recent ultra-wideband (UWB) functionality on these devices to capture\nabsolute distance between the two devices. This measurement is the perfect\ncomplement to inertial data, which is relative and suffers from drift. We\nquantify the performance of our software-only approach using off-the-shelf\ndevices, showing it can estimate the wrist and elbow joints with a \\hl{median\npositional error of 11.0~cm}, without the user having to provide training data."}
{"id": "2509.02949", "pdf": "https://arxiv.org/pdf/2509.02949.pdf", "abs": "https://arxiv.org/abs/2509.02949", "title": "ProMQA-Assembly: Multimodal Procedural QA Dataset on Assembly", "authors": ["Kimihiro Hasegawa", "Wiradee Imrattanatrai", "Masaki Asada", "Susan Holm", "Yuran Wang", "Vincent Zhou", "Ken Fukuda", "Teruko Mitamura"], "categories": ["cs.CL", "cs.CV"], "comment": "29 pages. Code and data: https://github.com/kimihiroh/promqa-assembly", "summary": "Assistants on assembly tasks have a large potential to benefit humans from\neveryday tasks to industrial settings. However, no testbeds support\napplication-oriented system evaluation in a practical setting, especially in\nassembly. To foster the development, we propose a new multimodal QA dataset on\nassembly activities. Our dataset, ProMQA-Assembly, consists of 391 QA pairs\nthat require the multimodal understanding of human-activity recordings and\ntheir instruction manuals in an online-style manner. In the development, we\nadopt a semi-automated QA annotation approach, where LLMs generate candidates\nand humans verify them, as a cost-effective method, and further improve it by\nintegrating fine-grained action labels to diversify question types.\nFurthermore, we create instruction task graphs for the target tasks of\nassembling toy vehicles. These newly created task graphs are used in our\nbenchmarking experiment, as well as to facilitate the human verification\nprocess in the QA annotation. Utilizing our dataset, we benchmark models,\nincluding competitive proprietary multimodal models. Our results suggest great\nroom for improvement for the current models. We believe our new evaluation\ndataset can contribute to the further development of procedural-activity\nassistants."}
{"id": "2509.02624", "pdf": "https://arxiv.org/pdf/2509.02624.pdf", "abs": "https://arxiv.org/abs/2509.02624", "title": "Who Owns The Robot?: Four Ethical and Socio-technical Questions about Wellbeing Robots in the Real World through Community Engagement", "authors": ["Minja Axelsson", "Jiaee Cheong", "Rune Nyrup", "Hatice Gunes"], "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.RO", "I.2.9; K.4.2; K.4.1"], "comment": "Accepted at the 8th AAAI/ACM Conference on AI, Ethics, and Society.\n  23 pages, 1 figure", "summary": "Recent studies indicate that robotic coaches can play a crucial role in\npromoting wellbeing. However, the real-world deployment of wellbeing robots\nraises numerous ethical and socio-technical questions and concerns. To explore\nthese questions, we undertake a community-centered investigation to examine\nthree different communities' perspectives on using robotic wellbeing coaches in\nreal-world environments. We frame our work as an anticipatory ethical\ninvestigation, which we undertake to better inform the development of robotic\ntechnologies with communities' opinions, with the ultimate goal of aligning\nrobot development with public interest. We conducted workshops with three\ncommunities who are under-represented in robotics development: 1) members of\nthe public at a science festival, 2) women computer scientists at a conference,\nand 3) humanities researchers interested in history and philosophy of science.\nIn the workshops, we collected qualitative data using the Social Robot\nCo-Design Canvas on Ethics. We analysed the collected qualitative data with\nThematic Analysis, informed by notes taken during workshops. Through our\nanalysis, we identify four themes regarding key ethical and socio-technical\nquestions about the real-world use of wellbeing robots. We group participants'\ninsights and discussions around these broad thematic questions, discuss them in\nlight of state-of-the-art literature, and highlight areas for future\ninvestigation. Finally, we provide the four questions as a broad framework that\nroboticists can and should use during robotic development and deployment, in\norder to reflect on the ethics and socio-technical dimensions of their robotic\napplications, and to engage in dialogue with communities of robot users. The\nfour questions are: 1) Is the robot safe and how can we know that?, 2) Who is\nthe robot built for and with?, 3) Who owns the robot and the data?, and 4) Why\na robot?."}
{"id": "2509.02999", "pdf": "https://arxiv.org/pdf/2509.02999.pdf", "abs": "https://arxiv.org/abs/2509.02999", "title": "DiaCBT: A Long-Periodic Dialogue Corpus Guided by Cognitive Conceptualization Diagram for CBT-based Psychological Counseling", "authors": ["Yougen Zhou", "Ningning Zhou", "Qin Chen", "Jie Zhou", "Aimin Zhou", "Liang He"], "categories": ["cs.CL"], "comment": null, "summary": "Psychotherapy reaches only a small fraction of individuals suffering from\nmental disorders due to social stigma and the limited availability of\ntherapists. Large language models (LLMs), when equipped with professional\npsychotherapeutic skills, offer a promising solution to expand access to mental\nhealth services. However, the lack of psychological conversation datasets\npresents significant challenges in developing effective psychotherapy-guided\nconversational agents. In this paper, we construct a long-periodic dialogue\ncorpus for counseling based on cognitive behavioral therapy (CBT). Our curated\ndataset includes multiple sessions for each counseling and incorporates\ncognitive conceptualization diagrams (CCDs) to guide client simulation across\ndiverse scenarios. To evaluate the utility of our dataset, we train an in-depth\ncounseling model and present a comprehensive evaluation framework to benchmark\nit against established psychological criteria for CBT-based counseling. Results\ndemonstrate that DiaCBT effectively enhances LLMs' ability to emulate\npsychologists with CBT expertise, underscoring its potential for training more\nprofessional counseling agents."}
{"id": "2509.02924", "pdf": "https://arxiv.org/pdf/2509.02924.pdf", "abs": "https://arxiv.org/abs/2509.02924", "title": "Simulacra Naturae: Generative Ecosystem driven by Agent-Based Simulations and Brain Organoid Collective Intelligence", "authors": ["Nefeli Manoudaki", "Mert Toka", "Iason Paterakis", "Diarmid Flatley"], "categories": ["cs.MM", "cs.AI", "cs.HC"], "comment": "to be published in IEEE VISAP 2025", "summary": "Simulacra Naturae is a data-driven media installation that explores\ncollective care through the entanglement of biological computation, material\necologies, and generative systems. The work translates pre-recorded neural\nactivity from brain organoids, lab-grown three-dimensional clusters of neurons,\ninto a multi-sensory environment composed of generative visuals, spatial audio,\nliving plants, and fabricated clay artifacts. These biosignals, streamed\nthrough a real-time system, modulate emergent agent behaviors inspired by\nnatural systems such as termite colonies and slime molds. Rather than using\nbiosignals as direct control inputs, Simulacra Naturae treats organoid activity\nas a co-creative force, allowing neural rhythms to guide the growth, form, and\natmosphere of a generative ecosystem. The installation features computationally\nfabricated clay prints embedded with solenoids, adding physical sound\nresonances to the generative surround composition. The spatial environment,\nfilled with live tropical plants and a floor-level projection layer featuring\nreal-time generative AI visuals, invites participants into a sensory field\nshaped by nonhuman cognition. By grounding abstract data in living materials\nand embodied experience, Simulacra Naturae reimagines visualization as a\npractice of care, one that decentralizes human agency and opens new spaces for\nethics, empathy, and ecological attunement within hybrid computational systems."}
{"id": "2509.03010", "pdf": "https://arxiv.org/pdf/2509.03010.pdf", "abs": "https://arxiv.org/abs/2509.03010", "title": "Mitigating Data Imbalance in Automated Speaking Assessment", "authors": ["Fong-Chun Tsai", "Kuan-Tang Huang", "Bi-Cheng Yan", "Tien-Hong Lo", "Berlin Chen"], "categories": ["cs.CL", "cs.LG", "eess.AS"], "comment": "Submitted to APSIPA 2025", "summary": "Automated Speaking Assessment (ASA) plays a crucial role in evaluating\nsecond-language (L2) learners proficiency. However, ASA models often suffer\nfrom class imbalance, leading to biased predictions. To address this, we\nintroduce a novel objective for training ASA models, dubbed the Balancing Logit\nVariation (BLV) loss, which perturbs model predictions to improve feature\nrepresentation for minority classes without modifying the dataset. Evaluations\non the ICNALE benchmark dataset show that integrating the BLV loss into a\ncelebrated text-based (BERT) model significantly enhances classification\naccuracy and fairness, making automated speech evaluation more robust for\ndiverse learners."}
{"id": "2509.03222", "pdf": "https://arxiv.org/pdf/2509.03222.pdf", "abs": "https://arxiv.org/abs/2509.03222", "title": "The Role of Embodiment in Intuitive Whole-Body Teleoperation for Mobile Manipulation", "authors": ["Sophia Bianchi Moyen", "Rickmer Krohn", "Sophie Lueth", "Kay Pompetzki", "Jan Peters", "Vignesh Prasad", "Georgia Chalvatzaki"], "categories": ["cs.RO", "cs.HC", "cs.LG"], "comment": "8 pages, 8 figures, Accepted at the IEEE-RAS International Conference\n  on Humanoid Robots (Humanoids) 2025", "summary": "Intuitive Teleoperation interfaces are essential for mobile manipulation\nrobots to ensure high quality data collection while reducing operator workload.\nA strong sense of embodiment combined with minimal physical and cognitive\ndemands not only enhances the user experience during large-scale data\ncollection, but also helps maintain data quality over extended periods. This\nbecomes especially crucial for challenging long-horizon mobile manipulation\ntasks that require whole-body coordination. We compare two distinct robot\ncontrol paradigms: a coupled embodiment integrating arm manipulation and base\nnavigation functions, and a decoupled embodiment treating these systems as\nseparate control entities. Additionally, we evaluate two visual feedback\nmechanisms: immersive virtual reality and conventional screen-based\nvisualization of the robot's field of view. These configurations were\nsystematically assessed across a complex, multi-stage task sequence requiring\nintegrated planning and execution. Our results show that the use of VR as a\nfeedback modality increases task completion time, cognitive workload, and\nperceived effort of the teleoperator. Coupling manipulation and navigation\nleads to a comparable workload on the user as decoupling the embodiments, while\npreliminary experiments suggest that data acquired by coupled teleoperation\nleads to better imitation learning performance. Our holistic view on intuitive\nteleoperation interfaces provides valuable insight into collecting\nhigh-quality, high-dimensional mobile manipulation data at scale with the human\noperator in mind. Project\nwebsite:https://sophiamoyen.github.io/role-embodiment-wbc-moma-teleop/"}
{"id": "2509.03020", "pdf": "https://arxiv.org/pdf/2509.03020.pdf", "abs": "https://arxiv.org/abs/2509.03020", "title": "Training LLMs to be Better Text Embedders through Bidirectional Reconstruction", "authors": ["Chang Su", "Dengliang Shi", "Siyuan Huang", "Jintao Du", "Changhua Meng", "Yu Cheng", "Weiqiang Wang", "Zhouhan Lin"], "categories": ["cs.CL", "cs.IR"], "comment": "accepted by EMNLP 2025 Main Conference", "summary": "Large language models (LLMs) have increasingly been explored as powerful text\nembedders. Existing LLM-based text embedding approaches often leverage the\nembedding of the final token, typically a reserved special token such as [EOS].\nHowever, these tokens have not been intentionally trained to capture the\nsemantics of the whole context, limiting their capacity as text embeddings,\nespecially for retrieval and re-ranking tasks. We propose to add a new training\nstage before contrastive learning to enrich the semantics of the final token\nembedding. This stage employs bidirectional generative reconstruction tasks,\nnamely EBQ2D (Embedding-Based Query-to-Document) and EBD2Q (Embedding-Based\nDocument-to-Query), which interleave to anchor the [EOS] embedding and\nreconstruct either side of Query-Document pairs. Experimental results\ndemonstrate that our additional training stage significantly improves LLM\nperformance on the Massive Text Embedding Benchmark (MTEB), achieving new\nstate-of-the-art results across different LLM base models and scales."}
{"id": "2509.03436", "pdf": "https://arxiv.org/pdf/2509.03436.pdf", "abs": "https://arxiv.org/abs/2509.03436", "title": "Cost-Optimized Systems Engineering for IoT-Enabled Robot Nurse in Infectious Pandemic Management", "authors": ["Md Mhamud Hussen Sifat", "Md Maruf", "Md Rokunuzzaman"], "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY", "I.2.9; C.3; J.3"], "comment": "11 pages, 10 figures, 4 tables, 1 algorithm. Corresponding author: Md\n  Maruf (maruf.mte.17@gmail.com)", "summary": "The utilization of robotic technology has gained traction in healthcare\nfacilities due to progress in the field that enables time and cost savings,\nminimizes waste, and improves patient care. Digital healthcare technologies\nthat leverage automation, such as robotics and artificial intelligence, have\nthe potential to enhance the sustainability and profitability of healthcare\nsystems in the long run. However, the recent COVID-19 pandemic has amplified\nthe need for cyber-physical robots to automate check-ups and medication\nadministration. A robot nurse is controlled by the Internet of Things (IoT) and\ncan serve as an automated medical assistant while also allowing supervisory\ncontrol based on custom commands. This system helps reduce infection risk and\nimproves outcomes in pandemic settings. This research presents a test case with\na nurse robot that can assess a patient's health status and take action\naccordingly. We also evaluate the system's performance in medication\nadministration, health-status monitoring, and life-cycle considerations."}
{"id": "2509.03057", "pdf": "https://arxiv.org/pdf/2509.03057.pdf", "abs": "https://arxiv.org/abs/2509.03057", "title": "Structure-Learnable Adapter Fine-Tuning for Parameter-Efficient Large Language Models", "authors": ["Ming Gong", "Yingnan Deng", "Nia Qi", "Yujun Zou", "Zhihao Xue", "Yun Zi"], "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the issues of parameter redundancy, rigid structure, and\nlimited task adaptability in the fine-tuning of large language models. It\nproposes an adapter-based fine-tuning method built on a structure-learnable\nmechanism. By introducing differentiable gating functions and structural\nsparsity control variables, the method enables automatic optimization of\nadapter insertion points, activation paths, and module combinations. This\nallows the model to adjust its structure flexibly in multi-task settings to\nmatch different task characteristics. With the backbone parameters kept frozen,\nthe method uses a structure search mechanism to guide the dynamic construction\nof task-specific efficient substructures during training. This significantly\nimproves parameter utilization and representational capacity. In addition, the\npaper designs a set of sensitivity analysis experiments to systematically\nevaluate the effects of sparsity weight, noise injection ratio, and data\nperturbation on model performance. These experiments verify the stability and\nrobustness of the proposed method across various multi-task natural language\nunderstanding tasks. The experimental results show that the proposed method\noutperforms mainstream parameter-efficient tuning techniques on multiple tasks.\nIt achieves a better balance among accuracy, compression rate, and robustness\nto noise and perturbation."}
{"id": "2509.03501", "pdf": "https://arxiv.org/pdf/2509.03501.pdf", "abs": "https://arxiv.org/abs/2509.03501", "title": "Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning via Synthetic Instruction Data", "authors": ["Honglu Zhou", "Xiangyu Peng", "Shrikant Kendre", "Michael S. Ryoo", "Silvio Savarese", "Caiming Xiong", "Juan Carlos Niebles"], "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "comment": "This technical report serves as the archival version of our paper\n  accepted at the ICCV 2025 Workshop. For more information, please visit our\n  project website: https://strefer.github.io/", "summary": "Next-generation AI companions must go beyond general video understanding to\nresolve spatial and temporal references in dynamic, real-world environments.\nExisting Video Large Language Models (Video LLMs), while capable of\ncoarse-level comprehension, struggle with fine-grained, spatiotemporal\nreasoning, especially when user queries rely on time-based event references for\ntemporal anchoring, or gestural cues for spatial anchoring to clarify object\nreferences and positions. To bridge this critical gap, we introduce Strefer, a\nsynthetic instruction data generation framework designed to equip Video LLMs\nwith spatiotemporal referring and reasoning capabilities. Strefer produces\ndiverse instruction-tuning data using a data engine that pseudo-annotates\ntemporally dense, fine-grained video metadata, capturing rich spatial and\ntemporal information in a structured manner, including subjects, objects, their\nlocations as masklets, and their action descriptions and timelines. Our\napproach enhances the ability of Video LLMs to interpret spatial and temporal\nreferences, fostering more versatile, space-time-aware reasoning essential for\nreal-world AI companions. Without using proprietary models, costly human\nannotation, or the need to annotate large volumes of new videos, experimental\nevaluations show that models trained with data produced by Strefer outperform\nbaselines on tasks requiring spatial and temporal disambiguation. Additionally,\nthese models exhibit enhanced space-time-aware reasoning, establishing a new\nfoundation for perceptually grounded, instruction-tuned Video LLMs."}
{"id": "2509.03060", "pdf": "https://arxiv.org/pdf/2509.03060.pdf", "abs": "https://arxiv.org/abs/2509.03060", "title": "A Long Short-Term Memory (LSTM) Model for Business Sentiment Analysis Based on Recurrent Neural Network", "authors": ["Md. Jahidul Islam Razin", "Md. Abdul Karim", "M. F. Mridha", "S M Rafiuddin", "Tahira Alam"], "categories": ["cs.CL"], "comment": "11 pages, 9 figures, 3 tables, published in Sustainable Communication\n  Networks and Application: Proceedings of ICSCN 2020 (2021). Paper presents an\n  LSTM-based business sentiment analysis model with 91.33% accuracy, compares\n  against KNN, SVM, and Naive Bayes, and discusses methodology, dataset,\n  training/testing, results, and implementation tools", "summary": "Business sentiment analysis (BSA) is one of the significant and popular\ntopics of natural language processing. It is one kind of sentiment analysis\ntechniques for business purposes. Different categories of sentiment analysis\ntechniques like lexicon-based techniques and different types of machine\nlearning algorithms are applied for sentiment analysis on different languages\nlike English, Hindi, Spanish, etc. In this paper, long short-term memory (LSTM)\nis applied for business sentiment analysis, where a recurrent neural network is\nused. An LSTM model is used in a modified approach to prevent the vanishing\ngradient problem rather than applying the conventional recurrent neural network\n(RNN). To apply the modified RNN model, product review dataset is used. In this\nexperiment, 70\\% of the data is trained for the LSTM and the rest 30\\% of the\ndata is used for testing. The result of this modified RNN model is compared\nwith other conventional RNN models, and a comparison is made among the results.\nIt is noted that the proposed model performs better than the other conventional\nRNN models. Here, the proposed model, i.e., the modified RNN model approach has\nachieved around 91.33\\% of accuracy. By applying this model, any business\ncompany or e-commerce business site can identify the feedback from their\ncustomers about different types of products that customers like or dislike.\nBased on the customer reviews, a business company or e-commerce platform can\nevaluate its marketing strategy."}
{"id": "2402.07911", "pdf": "https://arxiv.org/pdf/2402.07911.pdf", "abs": "https://arxiv.org/abs/2402.07911", "title": "From Metrics to Meaning: Time to Rethink Evaluation in Human-AI Collaborative Design", "authors": ["Sean P. Walton", "Ben J. Evans", "Alma A. M. Rahat", "James Stovold", "Jakub Vincalek"], "categories": ["cs.HC", "cs.AI", "cs.CE", "cs.NE", "I.2.0; J.6; G.1.6"], "comment": "31 pages, under review", "summary": "As AI systems increasingly shape decision making in creative design contexts,\nunderstanding how humans engage with these tools has become a critical\nchallenge for interactive intelligent systems research. This paper contributes\na challenge to rethink how to evaluate human--AI collaborative systems,\nadvocating for a more nuanced and multidimensional approach. Findings from one\nof the largest field studies to date (n = 808) of a human--AI co-creative\nsystem, The Genetic Car Designer, complemented by a controlled lab study (n =\n12) are presented. The system is based on an interactive evolutionary algorithm\nwhere participants were tasked with designing a simple two dimensional\nrepresentation of a car. Participants were exposed to galleries of design\nsuggestions generated by an intelligent system, MAP--Elites, and a random\ncontrol. Results indicate that exposure to galleries generated by MAP--Elites\nsignificantly enhanced both cognitive and behavioural engagement, leading to\nhigher-quality design outcomes. Crucially for the wider community, the analysis\nreveals that conventional evaluation methods, which often focus on solely\nbehavioural and design quality metrics, fail to capture the full spectrum of\nuser engagement. By considering the human--AI design process as a changing\nemotional, behavioural and cognitive state of the designer, we propose\nevaluating human--AI systems holistically and considering intelligent systems\nas a core part of the user experience -- not simply a back end tool."}
{"id": "2509.03116", "pdf": "https://arxiv.org/pdf/2509.03116.pdf", "abs": "https://arxiv.org/abs/2509.03116", "title": "Measuring Scalar Constructs in Social Science with LLMs", "authors": ["Hauke Licht", "Rupak Sarkar", "Patrick Y. Wu", "Pranav Goel", "Niklas Stoehr", "Elliott Ash", "Alexander Miserlis Hoyle"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 (Main)", "summary": "Many constructs that characterize language, like its complexity or\nemotionality, have a naturally continuous semantic structure; a public speech\nis not just \"simple\" or \"complex,\" but exists on a continuum between extremes.\nAlthough large language models (LLMs) are an attractive tool for measuring\nscalar constructs, their idiosyncratic treatment of numerical outputs raises\nquestions of how to best apply them. We address these questions with a\ncomprehensive evaluation of LLM-based approaches to scalar construct\nmeasurement in social science. Using multiple datasets sourced from the\npolitical science literature, we evaluate four approaches: unweighted direct\npointwise scoring, aggregation of pairwise comparisons,\ntoken-probability-weighted pointwise scoring, and finetuning. Our study yields\nactionable findings for applied researchers. First, LLMs prompted to generate\npointwise scores directly from texts produce discontinuous distributions with\nbunching at arbitrary numbers. The quality of the measurements improves with\npairwise comparisons made by LLMs, but it improves even more by taking\npointwise scores and weighting them by token probability. Finally, finetuning\nsmaller models with as few as 1,000 training pairs can match or exceed the\nperformance of prompted LLMs."}
{"id": "2409.15550", "pdf": "https://arxiv.org/pdf/2409.15550.pdf", "abs": "https://arxiv.org/abs/2409.15550", "title": "Talk, Listen, Connect: How Humans and AI Evaluate Empathy in Responses to Emotionally Charged Narratives", "authors": ["Mahnaz Roshanaei", "Rezvaneh Rezapour", "Magy Seif El-Nasr"], "categories": ["cs.HC", "F.2.2; I.2.7"], "comment": "21 pages, 4 figures, 6 tables. Title updated from \"Talk, Listen,\n  Connect: Navigating Empathy in Human-AI Interactions\" to \"Talk, Listen,\n  Connect: How Humans and AI Evaluate Empathy in Responses to Emotionally\n  Charged Narratives\" in this version. This is version 2 (v2) of the paper. All\n  previous citations of arXiv:2409.15550 with the old title still refer to the\n  same paper", "summary": "Social interactions promote well-being, yet barriers like geographic\ndistance, time limitations, and mental health conditions can limit face-to-face\ninteractions. Emotionally responsive AI systems, such as chatbots, offer new\nopportunities for social and emotional support, but raise critical questions\nabout how empathy is perceived and experienced in human-AI interactions. This\nstudy examines how empathy is evaluated in AI-generated versus human responses.\nUsing personal narratives, we explored how persona attributes (e.g., gender,\nempathic traits, shared experiences) and story qualities affect empathy\nratings. We compared responses from standard and fine-tuned AI models with\nhuman judgments. Results show that while humans are highly sensitive to\nemotional vividness and shared experience, AI-responses are less influenced by\nthese cues, often lack nuance in empathic expression. These findings highlight\nchallenges in designing emotionally intelligent systems that respond\nmeaningfully across diverse users and contexts, and informs the design of\nethically aware tools to support social connection and well-being."}
{"id": "2509.03122", "pdf": "https://arxiv.org/pdf/2509.03122.pdf", "abs": "https://arxiv.org/abs/2509.03122", "title": "From Evaluation to Defense: Constructing Persistent Edit-Based Fingerprints for Large Language Models", "authors": ["Yue Li", "Xin Yi", "Dongsheng Shi", "Yongyi Cui", "Gerard de Melo", "Xiaoling Wang", "Linlin Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "preprint", "summary": "The intellectual property (IP) protection of Large Language Models (LLMs) is\nincreasingly critical. Injecting specialized fingerprints into LLMs through\ninstruction tuning is a common IP protection technique. However, this may\nsignificantly degrade model performance, requires substantial computational\nresources, and exhibits poor persistence under model modifications. We argue\nthat knowledge editing offers a lightweight alternative that is more suitable\nfor fingerprint injection. Accordingly, we apply knowledge editing to\nfingerprint injection for the first time and demonstrate its strong capability.\nDespite using scrambled text as fingerprints to prevent them from being\noverwritten during fine-tuning, degradation still occurs under large-scale\nfine-tuning. To address this, we propose Fingerprint Subspace-aware Fine-Tuning\n(FSFT), which reduces fingerprint degradation by constraining the update of the\nfingerprint subspace. The performance of FSFT exceeds fine-tuning by 10% even\nin the worst-case scenario. Additionally, we observe that the\nfingerprint-injected models struggle to distinguish between fingerprints and\nsimilar texts due to the high similarity of their features. This finding\nunderscores the urgent need for more robust and fine-grained fingerprinting\ninjection methods for LLMs."}
{"id": "2508.03700", "pdf": "https://arxiv.org/pdf/2508.03700.pdf", "abs": "https://arxiv.org/abs/2508.03700", "title": "MagicGUI: A Foundational Mobile GUI Agent with Scalable Data Pipeline and Reinforcement Fine-tuning", "authors": ["Liujian Tang", "Shaokang Dong", "Yijia Huang", "Minqi Xiang", "Hongtao Ruan", "Bin Wang", "Shuo Li", "Zhiheng Xi", "Zhihui Cao", "Hailiang Pang", "Heng Kong", "He Yang", "Mingxu Chai", "Zhilin Gao", "Xingyu Liu", "Yingnan Fu", "Jiaming Liu", "Xuanjing Huang", "Yu-Gang Jiang", "Tao Gui", "Qi Zhang", "Kang Wang", "Yunke Zhang", "Yuran Wang"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This paper presents MagicGUI, a foundational mobile GUI agent designed to\naddress critical challenges in perception, grounding, and reasoning within\nreal-world mobile GUI environments. The framework is underpinned by following\nsix key components: (1) a comprehensive and accurate dataset, constructed via\nthe scalable GUI Data Pipeline, which aggregates the largest and most diverse\nGUI-centric multimodal data to date from open-source repositories, automated\ncrawling, and targeted manual annotation; (2) enhanced perception and grounding\ncapabilities, facilitating fine-grained multimodal alignment for UI element\nreferencing, grounding, and screen comprehension; (3) a comprehensive and\nunified action space, encompassing both fundamental UI operations and complex\ninteractive intents to support human-agent interactions; (4) planning-oriented\nreasoning mechanisms that enable the model to decompose complex user\ninstructions into sequential actions with explicit intermediate meta-paln\nreasoning; (5) an iterative two-stage training procedure, combining large-scale\ncontinue pre-training on 7.8M samples with reinforcement fine-tuning utilizing\na spatially enhanced composite reward and dual filtering strategy; and (6)\ncompetitive performance on both the proprietary Magic-RICH benchmark and over a\ndozen public benchmarks, achieving superior performance across GUI perception\nand agent tasks, while demonstrating robust generalization and real-world\ndeployment potential in practical mobile GUI scenarios, as detailed in Figure\n1."}
{"id": "2509.03143", "pdf": "https://arxiv.org/pdf/2509.03143.pdf", "abs": "https://arxiv.org/abs/2509.03143", "title": "An experimental and computational study of an Estonian single-person word naming", "authors": ["Kaidi Lõo", "Arvi Tavast", "Maria Heitmeier", "Harald Baayen"], "categories": ["cs.CL"], "comment": null, "summary": "This study investigates lexical processing in Estonian. A large-scale\nsingle-subject experiment is reported that combines the word naming task with\neye-tracking. Five response variables (first fixation duration, total fixation\nduration, number of fixations, word naming latency, and spoken word duration)\nare analyzed with the generalized additive model. Of central interest is the\nquestion of whether measures for lexical processing generated by a\ncomputational model of the mental lexicon (the Discriminative Lexicon Model,\nDLM) are predictive for these response variables, and how they compare to\nclassical predictors such as word frequency, neighborhood size, and\ninflectional paradigm size. Computational models were implemented both with\nlinear and deep mappings. Central findings are, first, that DLM-based measures\nare powerful predictors for lexical processing, second, that DLM-measures using\ndeep learning are not necessarily more precise predictors of lexical processing\nthan DLM-measures using linear mappings, third, that classical predictors tend\nto provide somewhat more precise fits compared to DLM-based predictors (except\nfor total fixation duration, where the two provide equivalent goodness of fit),\nand fourth, that in the naming task lexical variables are not predictive for\nfirst fixation duration and the total number of fixations. As the DLM works\nwith mappings from form to meaning, the predictivity of DLM-based measures for\ntotal fixation duration, naming latencies, and spoken word duration indicates\nthat meaning is heavily involved in the present word naming task."}
{"id": "2403.04931", "pdf": "https://arxiv.org/pdf/2403.04931.pdf", "abs": "https://arxiv.org/abs/2403.04931", "title": "A Survey on Human-AI Collaboration with Large Foundation Models", "authors": ["Vanshika Vats", "Marzia Binta Nizam", "Minghao Liu", "Ziyuan Wang", "Richard Ho", "Mohnish Sai Prasad", "Vincent Titterton", "Sai Venkat Malreddy", "Riya Aggarwal", "Yanwen Xu", "Lei Ding", "Jay Mehta", "Nathan Grinnell", "Li Liu", "Sijia Zhong", "Devanathan Nallur Gandamani", "Xinyi Tang", "Rohan Ghosalkar", "Celeste Shen", "Rachel Shen", "Nafisa Hussain", "Kesav Ravichandran", "James Davis"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "Topic and scope refinement", "summary": "As the capabilities of artificial intelligence (AI) continue to expand\nrapidly, Human-AI (HAI) Collaboration, combining human intellect and AI\nsystems, has become pivotal for advancing problem-solving and decision-making\nprocesses. The advent of Large Foundation Models (LFMs) has greatly expanded\nits potential, offering unprecedented capabilities by leveraging vast amounts\nof data to understand and predict complex patterns. At the same time, realizing\nthis potential responsibly requires addressing persistent challenges related to\nsafety, fairness, and control. This paper reviews the crucial integration of\nLFMs with HAI, highlighting both opportunities and risks. We structure our\nanalysis around four areas: human-guided model development, collaborative\ndesign principles, ethical and governance frameworks, and applications in\nhigh-stakes domains. Our review shows that successful HAI systems are not the\nautomatic result of stronger models but the product of careful, human-centered\ndesign. By identifying key open challenges, this survey aims to give insight\ninto current and future research that turns the raw power of LFMs into\npartnerships that are reliable, trustworthy, and beneficial to society."}
{"id": "2509.03148", "pdf": "https://arxiv.org/pdf/2509.03148.pdf", "abs": "https://arxiv.org/abs/2509.03148", "title": "Expanding the WMT24++ Benchmark with Rumantsch Grischun, Sursilvan, Sutsilvan, Surmiran, Puter, and Vallader", "authors": ["Jannis Vamvas", "Ignacio Pérez Prat", "Not Battesta Soliva", "Sandra Baltermia-Guetg", "Andrina Beeli", "Simona Beeli", "Madlaina Capeder", "Laura Decurtins", "Gian Peder Gregori", "Flavia Hobi", "Gabriela Holderegger", "Arina Lazzarini", "Viviana Lazzarini", "Walter Rosselli", "Bettina Vital", "Anna Rutkiewicz", "Rico Sennrich"], "categories": ["cs.CL"], "comment": "Submitted to WMT25 (Open Language Data Initiative Shared Task)", "summary": "The Romansh language, spoken in Switzerland, has limited resources for\nmachine translation evaluation. In this paper, we present a benchmark for six\nvarieties of Romansh: Rumantsch Grischun, a supra-regional variety, and five\nregional varieties: Sursilvan, Sutsilvan, Surmiran, Puter, and Vallader. Our\nreference translations were created by human translators based on the WMT24++\nbenchmark, which ensures parallelism with more than 55 other languages. An\nautomatic evaluation of existing MT systems and LLMs shows that translation out\nof Romansh into German is handled relatively well for all the varieties, but\ntranslation into Romansh is still challenging."}
{"id": "2404.00024", "pdf": "https://arxiv.org/pdf/2404.00024.pdf", "abs": "https://arxiv.org/abs/2404.00024", "title": "Hey, Teacher, (Don't) Leave Those Kids Alone: Standardizing HRI Education", "authors": ["Alexis E. Block"], "categories": ["cs.RO", "cs.CY", "cs.HC"], "comment": "Presented at the Designing an Intro to HRI Course Workshop at HRI\n  2024 (arXiv:2403.05588)", "summary": "Creating a standardized introduction course becomes more critical as the\nfield of human-robot interaction (HRI) becomes more established. This paper\noutlines the key components necessary to provide an undergraduate with a\nsufficient foundational understanding of the interdisciplinary nature of this\nfield and provides proposed course content. It emphasizes the importance of\ncreating a course with theoretical and experimental components to accommodate\nall different learning preferences. This manuscript also advocates creating or\nadopting a universal platform to standardize the hands-on component of\nintroductory HRI courses, regardless of university funding or size. Next, it\nrecommends formal training in how to read scientific articles and staying\nup-to-date with the latest relevant papers. Finally, it provides detailed\nlecture content and project milestones for a 15-week semester. By creating a\nstandardized course, researchers can ensure consistency and quality are\nmaintained across institutions, which will help students as well as industrial\nand academic employers understand what foundational knowledge is expected."}
{"id": "2509.03161", "pdf": "https://arxiv.org/pdf/2509.03161.pdf", "abs": "https://arxiv.org/abs/2509.03161", "title": "Domain Adaptation of LLMs for Process Data", "authors": ["Rafael Seidi Oyamada", "Jari Peeperkorn", "Jochen De Weerdt", "Johannes De Smedt"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In recent years, Large Language Models (LLMs) have emerged as a prominent\narea of interest across various research domains, including Process Mining\n(PM). Current applications in PM have predominantly centered on prompt\nengineering strategies or the transformation of event logs into narrative-style\ndatasets, thereby exploiting the semantic capabilities of LLMs to address\ndiverse tasks. In contrast, this study investigates the direct adaptation of\npretrained LLMs to process data without natural language reformulation,\nmotivated by the fact that these models excel in generating sequences of\ntokens, similar to the objective in PM. More specifically, we focus on\nparameter-efficient fine-tuning techniques to mitigate the computational\noverhead typically associated with such models. Our experimental setup focuses\non Predictive Process Monitoring (PPM), and considers both single- and\nmulti-task predictions. The results demonstrate a potential improvement in\npredictive performance over state-of-the-art recurrent neural network (RNN)\napproaches and recent narrative-style-based solutions, particularly in the\nmulti-task setting. Additionally, our fine-tuned models exhibit faster\nconvergence and require significantly less hyperparameter optimization."}
{"id": "2509.03162", "pdf": "https://arxiv.org/pdf/2509.03162.pdf", "abs": "https://arxiv.org/abs/2509.03162", "title": "SinhalaMMLU: A Comprehensive Benchmark for Evaluating Multitask Language Understanding in Sinhala", "authors": ["Ashmari Pramodya", "Nirasha Nelki", "Heshan Shalinda", "Chamila Liyanage", "Yusuke Sakai", "Randil Pushpananda", "Ruvan Weerasinghe", "Hidetaka Kamigaito", "Taro Watanabe"], "categories": ["cs.CL"], "comment": "19 pages, 11 figures", "summary": "Large Language Models (LLMs) demonstrate impressive general knowledge and\nreasoning abilities, yet their evaluation has predominantly focused on global\nor anglocentric subjects, often neglecting low-resource languages and\nculturally specific content. While recent multilingual benchmarks attempt to\nbridge this gap, many rely on automatic translation, which can introduce errors\nand misrepresent the original cultural context. To address this, we introduce\nSinhalaMMLU, the first multiple-choice question answering benchmark designed\nspecifically for Sinhala, a low-resource language. The dataset includes over\n7,000 questions spanning secondary to collegiate education levels, aligned with\nthe Sri Lankan national curriculum, and covers six domains and 30 subjects,\nencompassing both general academic topics and culturally grounded knowledge. We\nevaluate 26 LLMs on SinhalaMMLU and observe that, while Claude 3.5 sonnet and\nGPT-4o achieve the highest average accuracies at 67% and 62% respectively,\noverall model performance remains limited. In particular, models struggle in\nculturally rich domains such as the Humanities, revealing substantial room for\nimprovement in adapting LLMs to low-resource and culturally specific contexts."}
{"id": "2509.03256", "pdf": "https://arxiv.org/pdf/2509.03256.pdf", "abs": "https://arxiv.org/abs/2509.03256", "title": "Comparison of End-to-end Speech Assessment Models for the NOCASA 2025 Challenge", "authors": ["Aleksei Žavoronkov", "Tanel Alumäe"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Published at IEEE MLSP 2025", "summary": "This paper presents an analysis of three end-to-end models developed for the\nNOCASA 2025 Challenge, aimed at automatic word-level pronunciation assessment\nfor children learning Norwegian as a second language. Our models include an\nencoder-decoder Siamese architecture (E2E-R), a prefix-tuned direct\nclassification model leveraging pretrained wav2vec2.0 representations, and a\nnovel model integrating alignment-free goodness-of-pronunciation (GOP) features\ncomputed via CTC. We introduce a weighted ordinal cross-entropy loss tailored\nfor optimizing metrics such as unweighted average recall and mean absolute\nerror. Among the explored methods, our GOP-CTC-based model achieved the highest\nperformance, substantially surpassing challenge baselines and attaining top\nleaderboard scores."}
{"id": "2509.03300", "pdf": "https://arxiv.org/pdf/2509.03300.pdf", "abs": "https://arxiv.org/abs/2509.03300", "title": "LatPhon: Lightweight Multilingual G2P for Romance Languages and English", "authors": ["Luis Felipe Chary", "Miguel Arjona Ramirez"], "categories": ["cs.CL"], "comment": null, "summary": "Grapheme-to-phoneme (G2P) conversion is a key front-end for text-to-speech\n(TTS), automatic speech recognition (ASR), speech-to-speech translation (S2ST)\nand alignment systems, especially across multiple Latin-script languages.We\npresent LatPhon, a 7.5 M - parameter Transformer jointly trained on six such\nlanguages--English, Spanish, French, Italian, Portuguese, and Romanian. On the\npublic ipa-dict corpus, it attains a mean phoneme error rate (PER) of 3.5%,\noutperforming the byte-level ByT5 baseline (5.4%) and approaching\nlanguage-specific WFSTs (3.2%) while occupying 30 MB of memory, which makes\non-device deployment feasible when needed. These results indicate that compact\nmultilingual G2P can serve as a universal front-end for Latin-language speech\npipelines."}
{"id": "2509.03312", "pdf": "https://arxiv.org/pdf/2509.03312.pdf", "abs": "https://arxiv.org/abs/2509.03312", "title": "AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?", "authors": ["Guibin Zhang", "Junhao Wang", "Junjie Chen", "Wangchunshu Zhou", "Kun Wang", "Shuicheng Yan"], "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "Large Language Model (LLM)-based agentic systems, often comprising multiple\nmodels, complex tool invocations, and orchestration protocols, substantially\noutperform monolithic agents. Yet this very sophistication amplifies their\nfragility, making them more prone to system failure. Pinpointing the specific\nagent or step responsible for an error within long execution traces defines the\ntask of agentic system failure attribution. Current state-of-the-art reasoning\nLLMs, however, remain strikingly inadequate for this challenge, with accuracy\ngenerally below 10%. To address this gap, we propose AgenTracer, the first\nautomated framework for annotating failed multi-agent trajectories via\ncounterfactual replay and programmed fault injection, producing the curated\ndataset TracerTraj. Leveraging this resource, we develop AgenTracer-8B, a\nlightweight failure tracer trained with multi-granular reinforcement learning,\ncapable of efficiently diagnosing errors in verbose multi-agent interactions.\nOn the Who&When benchmark, AgenTracer-8B outperforms giant proprietary LLMs\nlike Gemini-2.5-Pro and Claude-4-Sonnet by up to 18.18%, setting a new standard\nin LLM agentic failure attribution. More importantly, AgenTracer-8B delivers\nactionable feedback to off-the-shelf multi-agent systems like MetaGPT and MaAS\nwith 4.8-14.2% performance gains, empowering self-correcting and self-evolving\nagentic AI."}
{"id": "2509.03405", "pdf": "https://arxiv.org/pdf/2509.03405.pdf", "abs": "https://arxiv.org/abs/2509.03405", "title": "LMEnt: A Suite for Analyzing Knowledge in Language Models from Pretraining Data to Representations", "authors": ["Daniela Gottesman", "Alon Gilae-Dotan", "Ido Cohen", "Yoav Gur-Arieh", "Marius Mosbach", "Ori Yoran", "Mor Geva"], "categories": ["cs.CL"], "comment": "Submitted to TACL, August 2025", "summary": "Language models (LMs) increasingly drive real-world applications that require\nworld knowledge. However, the internal processes through which models turn data\ninto representations of knowledge and beliefs about the world, are poorly\nunderstood. Insights into these processes could pave the way for developing LMs\nwith knowledge representations that are more consistent, robust, and complete.\nTo facilitate studying these questions, we present LMEnt, a suite for analyzing\nknowledge acquisition in LMs during pretraining. LMEnt introduces: (1) a\nknowledge-rich pretraining corpus, fully annotated with entity mentions, based\non Wikipedia, (2) an entity-based retrieval method over pretraining data that\noutperforms previous approaches by as much as 80.4%, and (3) 12 pretrained\nmodels with up to 1B parameters and 4K intermediate checkpoints, with\ncomparable performance to popular open-sourced models on knowledge benchmarks.\nTogether, these resources provide a controlled environment for analyzing\nconnections between entity mentions in pretraining and downstream performance,\nand the effects of causal interventions in pretraining data. We show the\nutility of LMEnt by studying knowledge acquisition across checkpoints, finding\nthat fact frequency is key, but does not fully explain learning trends. We\nrelease LMEnt to support studies of knowledge in LMs, including knowledge\nrepresentations, plasticity, editing, attribution, and learning dynamics."}
{"id": "2509.03407", "pdf": "https://arxiv.org/pdf/2509.03407.pdf", "abs": "https://arxiv.org/abs/2509.03407", "title": "Learning Mechanism Underlying NLP Pre-Training and Fine-Tuning", "authors": ["Yarden Tzach", "Ronit D. Gross", "Ella Koresh", "Shalom Rosner", "Or Shpringer", "Tal Halevi", "Ido Kanter"], "categories": ["cs.CL"], "comment": "46 pages, 18 figures, 10 tables", "summary": "Natural language processing (NLP) enables the understanding and generation of\nmeaningful human language, typically using a pre-trained complex architecture\non a large dataset to learn the language and next fine-tune its weights to\nimplement a specific task. Twofold goals are examined; to understand the\nmechanism underlying successful pre-training and to determine the interplay\nbetween the pre-training accuracy and the fine-tuning of classification tasks.\nThe following main results were obtained; the accuracy per token (APT)\nincreased with its appearance frequency in the dataset, and its average over\nall tokens served as an order parameter to quantify pre-training success, which\nincreased along the transformer blocks. Pre-training broke the symmetry among\ntokens and grouped them into finite, small, strong match token clusters, as\ninferred from the presented token confusion matrix. This feature was sharpened\nalong the transformer blocks toward the output layer, enhancing its performance\nconsiderably compared with that of the embedding layer. Consequently,\nhigher-order language structures were generated by pre-training, even though\nthe learning cost function was directed solely at identifying a single token.\nThese pre-training findings were reflected by the improved fine-tuning accuracy\nalong the transformer blocks. Additionally, the output label prediction\nconfidence was found to be independent of the average input APT, as the input\nmeaning was preserved since the tokens are replaced primarily by strong match\ntokens. Finally, although pre-training is commonly absent in image\nclassification tasks, its underlying mechanism is similar to that used in\nfine-tuning NLP classification tasks, hinting at its universality. The results\nwere based on the BERT-6 architecture pre-trained on the Wikipedia dataset and\nfine-tuned on the FewRel and DBpedia classification tasks."}
{"id": "2509.03419", "pdf": "https://arxiv.org/pdf/2509.03419.pdf", "abs": "https://arxiv.org/abs/2509.03419", "title": "Curse of Knowledge: When Complex Evaluation Context Benefits yet Biases LLM Judges", "authors": ["Weiyuan Li", "Xintao Wang", "Siyu Yuan", "Rui Xu", "Jiangjie Chen", "Qingqing Dong", "Yanghua Xiao", "Deqing Yang"], "categories": ["cs.CL"], "comment": "8 pages, 4 figures, conference", "summary": "As large language models (LLMs) grow more capable, they face increasingly\ndiverse and complex tasks, making reliable evaluation challenging. The paradigm\nof LLMs as judges has emerged as a scalable solution, yet prior work primarily\nfocuses on simple settings. Their reliability in complex tasks--where\nmulti-faceted rubrics, unstructured reference answers, and nuanced criteria are\ncritical--remains understudied. In this paper, we constructed ComplexEval, a\nchallenge benchmark designed to systematically expose and quantify Auxiliary\nInformation Induced Biases. We systematically investigated and validated 6\npreviously unexplored biases across 12 basic and 3 advanced scenarios. Key\nfindings reveal: (1) all evaluated models exhibit significant susceptibility to\nthese biases, with bias magnitude scaling with task complexity; (2) notably,\nLarge Reasoning Models (LRMs) show paradoxical vulnerability. Our in-depth\nanalysis offers crucial insights for improving the accuracy and verifiability\nof evaluation signals, paving the way for more general and robust evaluation\nmodels."}
{"id": "2509.03467", "pdf": "https://arxiv.org/pdf/2509.03467.pdf", "abs": "https://arxiv.org/abs/2509.03467", "title": "Continuous Saudi Sign Language Recognition: A Vision Transformer Approach", "authors": ["Soukeina Elhassen", "Lama Al Khuzayem", "Areej Alhothali", "Ohoud Alzamzami", "Nahed Alowaidi"], "categories": ["cs.CL", "cs.AI"], "comment": "23 pages, 13 figures, 5 tables", "summary": "Sign language (SL) is an essential communication form for hearing-impaired\nand deaf people, enabling engagement within the broader society. Despite its\nsignificance, limited public awareness of SL often leads to inequitable access\nto educational and professional opportunities, thereby contributing to social\nexclusion, particularly in Saudi Arabia, where over 84,000 individuals depend\non Saudi Sign Language (SSL) as their primary form of communication. Although\ncertain technological approaches have helped to improve communication for\nindividuals with hearing impairments, there continues to be an urgent\nrequirement for more precise and dependable translation techniques, especially\nfor Arabic sign language variants like SSL. Most state-of-the-art solutions\nhave primarily focused on non-Arabic sign languages, resulting in a\nconsiderable absence of resources dedicated to Arabic sign language,\nspecifically SSL. The complexity of the Arabic language and the prevalence of\nisolated sign language datasets that concentrate on individual words instead of\ncontinuous speech contribute to this issue. To address this gap, our research\nrepresents an important step in developing SSL resources. To address this, we\nintroduce the first continuous Saudi Sign Language dataset called KAU-CSSL,\nfocusing on complete sentences to facilitate further research and enable\nsophisticated recognition systems for SSL recognition and translation.\nAdditionally, we propose a transformer-based model, utilizing a pretrained\nResNet-18 for spatial feature extraction and a Transformer Encoder with\nBidirectional LSTM for temporal dependencies, achieving 99.02\\% accuracy at\nsigner dependent mode and 77.71\\% accuracy at signer independent mode. This\ndevelopment leads the way to not only improving communication tools for the SSL\ncommunity but also making a substantial contribution to the wider field of sign\nlanguage."}
{"id": "2509.03479", "pdf": "https://arxiv.org/pdf/2509.03479.pdf", "abs": "https://arxiv.org/abs/2509.03479", "title": "Design and Optimization of Reinforcement Learning-Based Agents in Text-Based Games", "authors": ["Haonan Wang", "Mingjia Zhao", "Junfeng Sun", "Wei Liu"], "categories": ["cs.CL"], "comment": "6 papges", "summary": "As AI technology advances, research in playing text-based games with agents\nhas becomeprogressively popular. In this paper, a novel approach to agent\ndesign and agent learning ispresented with the context of reinforcement\nlearning. A model of deep learning is first applied toprocess game text and\nbuild a world model. Next, the agent is learned through a policy gradient-based\ndeep reinforcement learning method to facilitate conversion from state value to\noptimal policy.The enhanced agent works better in several text-based game\nexperiments and significantlysurpasses previous agents on game completion ratio\nand win rate. Our study introduces novelunderstanding and empirical ground for\nusing reinforcement learning for text games and sets thestage for developing\nand optimizing reinforcement learning agents for more general domains\nandproblems."}
{"id": "2509.02859", "pdf": "https://arxiv.org/pdf/2509.02859.pdf", "abs": "https://arxiv.org/abs/2509.02859", "title": "Speech DF Arena: A Leaderboard for Speech DeepFake Detection Models", "authors": ["Sandipana Dowerah", "Atharva Kulkarni", "Ajinkya Kulkarni", "Hoan My Tran", "Joonas Kalda", "Artem Fedorchenko", "Benoit Fauve", "Damien Lolive", "Tanel Alumäe", "Matthew Magimai Doss"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Parallel to the development of advanced deepfake audio generation, audio\ndeepfake detection has also seen significant progress. However, a standardized\nand comprehensive benchmark is still missing. To address this, we introduce\nSpeech DeepFake (DF) Arena, the first comprehensive benchmark for audio\ndeepfake detection. Speech DF Arena provides a toolkit to uniformly evaluate\ndetection systems, currently across 14 diverse datasets and attack scenarios,\nstandardized evaluation metrics and protocols for reproducibility and\ntransparency. It also includes a leaderboard to compare and rank the systems to\nhelp researchers and developers enhance their reliability and robustness. We\ninclude 14 evaluation sets, 12 state-of-the-art open-source and 3 proprietary\ndetection systems. Our study presents many systems exhibiting high EER in\nout-of-domain scenarios, highlighting the need for extensive cross-domain\nevaluation. The leaderboard is hosted on Huggingface1 and a toolkit for\nreproducing results across the listed datasets is available on GitHub."}
{"id": "2509.03004", "pdf": "https://arxiv.org/pdf/2509.03004.pdf", "abs": "https://arxiv.org/abs/2509.03004", "title": "Identifiability and minimality bounds of quantum and post-quantum models of classical stochastic processes", "authors": ["Paul M. Riechers", "Thomas J. Elliott"], "categories": ["quant-ph", "cond-mat.stat-mech", "cs.CL", "cs.FL", "cs.IT", "math.IT"], "comment": "11 pages, 4 figures", "summary": "To make sense of the world around us, we develop models, constructed to\nenable us to replicate, describe, and explain the behaviours we see. Focusing\non the broad case of sequences of correlated random variables, i.e., classical\nstochastic processes, we tackle the question of determining whether or not two\ndifferent models produce the same observable behavior. This is the problem of\nidentifiability. Curiously, the physics of the model need not correspond to the\nphysics of the observations; recent work has shown that it is even advantageous\n-- in terms of memory and thermal efficiency -- to employ quantum models to\ngenerate classical stochastic processes. We resolve the identifiability problem\nin this regime, providing a means to compare any two models of a classical\nprocess, be the models classical, quantum, or `post-quantum', by mapping them\nto a canonical `generalized' hidden Markov model. Further, this enables us to\nplace (sometimes tight) bounds on the minimal dimension required of a quantum\nmodel to generate a given classical stochastic process."}
{"id": "2509.03113", "pdf": "https://arxiv.org/pdf/2509.03113.pdf", "abs": "https://arxiv.org/abs/2509.03113", "title": "Mitigating Multimodal Hallucinations via Gradient-based Self-Reflection", "authors": ["Shan Wang", "Maying Shen", "Nadine Chang", "Chuong Nguyen", "Hongdong Li", "Jose M. Alvarez"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Hallucinations in multimodal large language model are caused by the\ntext-visual bias and the co-occurrence bias. The former reflects an\nover-reliance on text information in the decision-making process, while the\nlatter arises from the statistical object-pairing patterns abstracted from the\ntraining data. Existing mitigation methods heuristically address these biases\nwithout understanding the fluctuating bias level across the instances. We first\npropose estimating the influence of respective token types (visual, prompt, and\nprevious outputs) using a gradient-based self-reflection method. The estimated\ntoken influence further enables the detection of object-related visual tokens\nand their integration into an influence-aware contrastive decoding framework to\nmitigate both types of biases simultaneously. Our method operates without the\nneed for additional resources, such as costly fine-tuning, extra models, or\ndata statistics. Extensive experiments show it effectively reduces\nhallucinations, achieving up to a 92% accuracy increase on LLaVA-QA90."}
{"id": "2509.03329", "pdf": "https://arxiv.org/pdf/2509.03329.pdf", "abs": "https://arxiv.org/abs/2509.03329", "title": "SESGO: Spanish Evaluation of Stereotypical Generative Outputs", "authors": ["Melissa Robles", "Catalina Bernal", "Denniss Raigoso", "Mateo Dulce Rubio"], "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "This paper addresses the critical gap in evaluating bias in multilingual\nLarge Language Models (LLMs), with a specific focus on Spanish language within\nculturally-aware Latin American contexts. Despite widespread global deployment,\ncurrent evaluations remain predominantly US-English-centric, leaving potential\nharms in other linguistic and cultural contexts largely underexamined. We\nintroduce a novel, culturally-grounded framework for detecting social biases in\ninstruction-tuned LLMs. Our approach adapts the underspecified question\nmethodology from the BBQ dataset by incorporating culturally-specific\nexpressions and sayings that encode regional stereotypes across four social\ncategories: gender, race, socioeconomic class, and national origin. Using more\nthan 4,000 prompts, we propose a new metric that combines accuracy with the\ndirection of error to effectively balance model performance and bias alignment\nin both ambiguous and disambiguated contexts. To our knowledge, our work\npresents the first systematic evaluation examining how leading commercial LLMs\nrespond to culturally specific bias in the Spanish language, revealing varying\npatterns of bias manifestation across state-of-the-art models. We also\ncontribute evidence that bias mitigation techniques optimized for English do\nnot effectively transfer to Spanish tasks, and that bias patterns remain\nlargely consistent across different sampling temperatures. Our modular\nframework offers a natural extension to new stereotypes, bias categories, or\nlanguages and cultural contexts, representing a significant step toward more\nequitable and culturally-aware evaluation of AI systems in the diverse\nlinguistic environments where they operate."}
{"id": "2509.03345", "pdf": "https://arxiv.org/pdf/2509.03345.pdf", "abs": "https://arxiv.org/abs/2509.03345", "title": "Language Models Do Not Follow Occam's Razor: A Benchmark for Inductive and Abductive Reasoning", "authors": ["Yunxin Sun", "Abulhair Saparov"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning is a core capability in artificial intelligence systems, for which\nlarge language models (LLMs) have recently shown remarkable progress. However,\nmost work focuses exclusively on deductive reasoning, which is problematic\nsince other types of reasoning are also essential in solving real-world\nproblems, and they are less explored. This work focuses on evaluating LLMs'\ninductive and abductive reasoning capabilities. We introduce a programmable and\nsynthetic dataset, InAbHyD (pronounced in-a-bid), where each reasoning example\nconsists of an incomplete world model and a set of observations. The task for\nthe intelligent agent is to produce hypotheses to explain observations under\nthe incomplete world model to solve each reasoning example. We propose a new\nmetric to evaluate the quality of hypotheses based on Occam's Razor. We\nevaluate and analyze some state-of-the-art LLMs. Our analysis shows that LLMs\ncan perform inductive and abductive reasoning in simple scenarios, but struggle\nwith complex world models and producing high-quality hypotheses, even with\npopular reasoning-enhancing techniques such as in-context learning and RLVR."}
{"id": "2509.03380", "pdf": "https://arxiv.org/pdf/2509.03380.pdf", "abs": "https://arxiv.org/abs/2509.03380", "title": "Situating AI Agents in their World: Aspective Agentic AI for Dynamic Partially Observable Information Systems", "authors": ["Peter J. Bentley", "Soo Ling Lim", "Fuyuki Ishikawa"], "categories": ["cs.AI", "cs.CL", "93A16", "I.2.11"], "comment": "9 pages", "summary": "Agentic LLM AI agents are often little more than autonomous chatbots: actors\nfollowing scripts, often controlled by an unreliable director. This work\nintroduces a bottom-up framework that situates AI agents in their environment,\nwith all behaviors triggered by changes in their environments. It introduces\nthe notion of aspects, similar to the idea of umwelt, where sets of agents\nperceive their environment differently to each other, enabling clearer control\nof information. We provide an illustrative implementation and show that\ncompared to a typical architecture, which leaks up to 83% of the time,\naspective agentic AI enables zero information leakage. We anticipate that this\nconcept of specialist agents working efficiently in their own information\nniches can provide improvements to both security and efficiency."}
{"id": "2509.03505", "pdf": "https://arxiv.org/pdf/2509.03505.pdf", "abs": "https://arxiv.org/abs/2509.03505", "title": "LimiX: Unleashing Structured-Data Modeling Capability for Generalist Intelligence", "authors": ["Xingxuan Zhang", "Gang Ren", "Han Yu", "Hao Yuan", "Hui Wang", "Jiansheng Li", "Jiayun Wu", "Lang Mo", "Li Mao", "Mingchao Hao", "Ningbo Dai", "Renzhe Xu", "Shuyang Li", "Tianyang Zhang", "Yue He", "Yuanrui Wang", "Yunjia Zhang", "Zijing Xu", "Dongzhe Li", "Fang Gao", "Hao Zou", "Jiandong Liu", "Jiashuo Liu", "Jiawei Xu", "Kaijie Cheng", "Kehan Li", "Linjun Zhou", "Qing Li", "Shaohua Fan", "Xiaoyu Lin", "Xinyan Han", "Xuanyue Li", "Yan Lu", "Yuan Xue", "Yuanyuan Jiang", "Zimu Wang", "Zhenlei Wang", "Peng Cui"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "56 pages", "summary": "We argue that progress toward general intelligence requires complementary\nfoundation models grounded in language, the physical world, and structured\ndata. This report presents LimiX, the first installment of our large\nstructured-data models (LDMs). LimiX treats structured data as a joint\ndistribution over variables and missingness, thus capable of addressing a wide\nrange of tabular tasks through query-based conditional prediction via a single\nmodel. LimiX is pretrained using masked joint-distribution modeling with an\nepisodic, context-conditional objective, where the model predicts for query\nsubsets conditioned on dataset-specific contexts, supporting rapid,\ntraining-free adaptation at inference. We evaluate LimiX across 10 large\nstructured-data benchmarks with broad regimes of sample size, feature\ndimensionality, class number, categorical-to-numerical feature ratio,\nmissingness, and sample-to-feature ratios. With a single model and a unified\ninterface, LimiX consistently surpasses strong baselines including\ngradient-boosting trees, deep tabular networks, recent tabular foundation\nmodels, and automated ensembles, as shown in Figure 1 and Figure 2. The\nsuperiority holds across a wide range of tasks, such as classification,\nregression, missing value imputation, and data generation, often by substantial\nmargins, while avoiding task-specific architectures or bespoke training per\ntask. All LimiX models are publicly accessible under Apache 2.0."}
{"id": "2210.14275", "pdf": "https://arxiv.org/pdf/2210.14275.pdf", "abs": "https://arxiv.org/abs/2210.14275", "title": "Similarity between Units of Natural Language: The Transition from Coarse to Fine Estimation", "authors": ["Wenchuan Mu"], "categories": ["cs.CL"], "comment": "PhD thesis", "summary": "Capturing the similarities between human language units is crucial for\nexplaining how humans associate different objects, and therefore its\ncomputation has received extensive attention, research, and applications. With\nthe ever-increasing amount of information around us, calculating similarity\nbecomes increasingly complex, especially in many cases, such as legal or\nmedical affairs, measuring similarity requires extra care and precision, as\nsmall acts within a language unit can have significant real-world effects. My\nresearch goal in this thesis is to develop regression models that account for\nsimilarities between language units in a more refined way.\n  Computation of similarity has come a long way, but approaches to debugging\nthe measures are often based on continually fitting human judgment values. To\nthis end, my goal is to develop an algorithm that precisely catches loopholes\nin a similarity calculation. Furthermore, most methods have vague definitions\nof the similarities they compute and are often difficult to interpret. The\nproposed framework addresses both shortcomings. It constantly improves the\nmodel through catching different loopholes. In addition, every refinement of\nthe model provides a reasonable explanation. The regression model introduced in\nthis thesis is called progressively refined similarity computation, which\ncombines attack testing with adversarial training. The similarity regression\nmodel of this thesis achieves state-of-the-art performance in handling edge\ncases."}
{"id": "2406.13748", "pdf": "https://arxiv.org/pdf/2406.13748.pdf", "abs": "https://arxiv.org/abs/2406.13748", "title": "Learn and Unlearn: Addressing Misinformation in Multilingual LLMs", "authors": ["Taiming Lu", "Philipp Koehn"], "categories": ["cs.CL", "cs.LG"], "comment": "EMNLP 2025 Main Conference", "summary": "This paper investigates the propagation of harmful information in\nmultilingual large language models (LLMs) and evaluates the efficacy of various\nunlearning methods. We demonstrate that fake information, regardless of the\nlanguage it is in, once introduced into these models through training data, can\nspread across different languages, compromising the integrity and reliability\nof the generated content. Our findings reveal that standard unlearning\ntechniques, which typically focus on English data, are insufficient in\nmitigating the spread of harmful content in multilingual contexts and could\ninadvertently reinforce harmful content across languages. We show that only by\naddressing harmful responses in both English and the original language of the\nharmful data can we effectively eliminate generations for all languages. This\nunderscores the critical need for comprehensive unlearning strategies that\nconsider the multilingual nature of modern LLMs to enhance their safety and\nreliability across diverse linguistic landscapes."}
{"id": "2406.15486", "pdf": "https://arxiv.org/pdf/2406.15486.pdf", "abs": "https://arxiv.org/abs/2406.15486", "title": "SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention", "authors": ["Qianchao Zhu", "Jiangfei Duan", "Chang Chen", "Siran Liu", "Guanyu Feng", "Xin Lv", "Xiao Chuanfu", "Dahua Lin", "Chao Yang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) now support extremely long context windows, but\nthe quadratic complexity of vanilla attention results in significantly long\nTime-to-First-Token (TTFT) latency. Existing approaches to address this\ncomplexity require additional pretraining or finetuning, and often sacrifice\nmodel accuracy. In this paper, we first provide both theoretical and empirical\nfoundations for near-lossless sparse attention. We find dynamically capturing\nhead-specific sparse patterns at runtime with low overhead is crucial. To\naddress this, we propose SampleAttention, an adaptive structured and\nnear-lossless sparse attention. Leveraging observed significant sparse\npatterns, SampleAttention attends to a fixed percentage of adjacent tokens to\ncapture local window patterns, and employs a two-stage query-guided key-value\nfiltering approach, which adaptively select a minimum set of key-values with\nlow overhead, to capture column stripe patterns. Comprehensive evaluations show\nthat SampleAttention can seamlessly replace vanilla attention in off-the-shelf\nLLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\\times$\ncompared with FlashAttention."}
{"id": "2406.17642", "pdf": "https://arxiv.org/pdf/2406.17642.pdf", "abs": "https://arxiv.org/abs/2406.17642", "title": "Banishing LLM Hallucinations Requires Rethinking Generalization", "authors": ["Johnny Li", "Saksham Consul", "Eda Zhou", "James Wong", "Naila Farooqui", "Yuxin Ye", "Nithyashree Manohar", "Zhuxiaona Wei", "Tian Wu", "Ben Echols", "Sharon Zhou", "Gregory Diamos"], "categories": ["cs.CL", "cs.AI"], "comment": "I want to revisit some of the experiments in this paper, specifically\n  figure 5", "summary": "Despite their powerful chat, coding, and reasoning abilities, Large Language\nModels (LLMs) frequently hallucinate. Conventional wisdom suggests that\nhallucinations are a consequence of a balance between creativity and\nfactuality, which can be mitigated, but not eliminated, by grounding the LLM in\nexternal knowledge sources. Through extensive systematic experiments, we show\nthat these traditional approaches fail to explain why LLMs hallucinate in\npractice. Specifically, we show that LLMs augmented with a massive Mixture of\nMemory Experts (MoME) can easily memorize large datasets of random numbers. We\ncorroborate these experimental findings with a theoretical construction showing\nthat simple neural networks trained to predict the next token hallucinate when\nthe training loss is above a threshold as it usually does in practice when\ntraining on internet scale data. We interpret our findings by comparing against\ntraditional retrieval methods for mitigating hallucinations. We use our\nfindings to design a first generation model for removing hallucinations --\nLamini-1 -- that stores facts in a massive mixture of millions of memory\nexperts that are retrieved dynamically."}
{"id": "2409.00061", "pdf": "https://arxiv.org/pdf/2409.00061.pdf", "abs": "https://arxiv.org/abs/2409.00061", "title": "Enhancing Natural Language Inference Performance with Knowledge Graph for COVID-19 Automated Fact-Checking in Indonesian Language", "authors": ["Arief Purnama Muharram", "Ayu Purwarianti"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for publication in the Journal of ICT Research and\n  Applications (JICTRA)", "summary": "Automated fact-checking is a key strategy to overcome the spread of COVID-19\nmisinformation on the internet. These systems typically leverage deep learning\napproaches through Natural Language Inference (NLI) to verify the truthfulness\nof information based on supporting evidence. However, one challenge that arises\nin deep learning is performance stagnation due to a lack of knowledge during\ntraining. This study proposes using a Knowledge Graph (KG) as external\nknowledge to enhance NLI performance for automated COVID-19 fact-checking in\nthe Indonesian language. The proposed model architecture comprises three\nmodules: a fact module, an NLI module, and a classifier module. The fact module\nprocesses information from the KG, while the NLI module handles semantic\nrelationships between the given premise and hypothesis. The representation\nvectors from both modules are concatenated and fed into the classifier module\nto produce the final result. The model was trained using the generated\nIndonesian COVID-19 fact-checking dataset and the COVID-19 KG Bahasa Indonesia.\nOur study demonstrates that incorporating KGs can significantly improve NLI\nperformance in fact-checking, achieving the best accuracy of 0.8616. This\nsuggests that KGs are a valuable component for enhancing NLI performance in\nautomated fact-checking."}
{"id": "2410.16033", "pdf": "https://arxiv.org/pdf/2410.16033.pdf", "abs": "https://arxiv.org/abs/2410.16033", "title": "TreeBoN: Enhancing Inference-Time Alignment with Speculative Tree-Search and Best-of-N Sampling", "authors": ["Jiahao Qiu", "Yifu Lu", "Yifan Zeng", "Jiacheng Guo", "Jiayi Geng", "Chenhao Zhu", "Xinzhe Juan", "Ling Yang", "Huazheng Wang", "Kaixuan Huang", "Yue Wu", "Mengdi Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Inference-time alignment enhances the performance of large language models\nwithout requiring additional training or fine-tuning but presents challenges\ndue to balancing computational efficiency with high-quality output. Best-of-N\n(BoN) sampling, as a simple yet powerful approach, generates multiple responses\nand selects the best one, achieving improved performance but with a high\ncomputational cost. We propose TreeBoN, a novel framework that integrates a\nspeculative tree-search strategy into Best-of-N (BoN) Sampling. TreeBoN\nmaintains a set of parent nodes, iteratively branching and pruning low-quality\nresponses, thereby reducing computational overhead while maintaining high\noutput quality. Our approach also leverages token-level rewards from Direct\nPreference Optimization (DPO) to guide tree expansion and prune low-quality\npaths. We evaluate TreeBoN using AlpacaFarm, HH-RLHF, UltraFeedback, GSM8K, and\nTutorEval datasets, demonstrating consistent improvements. Specifically,\nTreeBoN achieves the highest win rate of 65% on TutorEval and around 60% win\nrates across other different datasets, outperforming standard BoN with the same\ncomputational cost and showcasing its scalability and alignment efficacy."}
{"id": "2410.20940", "pdf": "https://arxiv.org/pdf/2410.20940.pdf", "abs": "https://arxiv.org/abs/2410.20940", "title": "Attacking Misinformation Detection Using Adversarial Examples Generated by Language Models", "authors": ["Piotr Przybyła", "Euan McGill", "Horacio Saggion"], "categories": ["cs.CL"], "comment": "Presented at EMNLP 2025", "summary": "Large language models have many beneficial applications, but can they also be\nused to attack content-filtering algorithms in social media platforms? We\ninvestigate the challenge of generating adversarial examples to test the\nrobustness of text classification algorithms detecting low-credibility content,\nincluding propaganda, false claims, rumours and hyperpartisan news. We focus on\nsimulation of content moderation by setting realistic limits on the number of\nqueries an attacker is allowed to attempt. Within our solution (TREPAT),\ninitial rephrasings are generated by large language models with prompts\ninspired by meaning-preserving NLP tasks, such as text simplification and style\ntransfer. Subsequently, these modifications are decomposed into small changes,\napplied through beam search procedure, until the victim classifier changes its\ndecision. We perform (1) quantitative evaluation using various prompts, models\nand query limits, (2) targeted manual assessment of the generated text and (3)\nqualitative linguistic analysis. The results confirm the superiority of our\napproach in the constrained scenario, especially in case of long input text\n(news articles), where exhaustive search is not feasible."}
{"id": "2412.09049", "pdf": "https://arxiv.org/pdf/2412.09049.pdf", "abs": "https://arxiv.org/abs/2412.09049", "title": "Dial-In LLM: Human-Aligned LLM-in-the-loop Intent Clustering for Customer Service Dialogues", "authors": ["Mengze Hong", "Wailing Ng", "Chen Jason Zhang", "Yuanfeng Song", "Di Jiang"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted by EMNLP 2025 Main Conference", "summary": "Discovering customer intentions is crucial for automated service agents, yet\nexisting intent clustering methods often fall short due to their reliance on\nembedding distance metrics and neglect of underlying semantic structures. To\naddress these limitations, we propose an LLM-in-the-loop (LLM-ITL) intent\nclustering framework, integrating the language understanding capabilities of\nLLMs into conventional clustering algorithms. Specifically, this paper (1)\nexamines the effectiveness of fine-tuned LLMs in semantic coherence evaluation\nand intent cluster naming, achieving over 95% accuracy aligned with human\njudgments; (2) designs an LLM-ITL framework that facilitates the iterative\ndiscovery of coherent intent clusters and the optimal number of clusters; and\n(3) introduces context-aware techniques tailored for customer service dialogue.\nSince existing English benchmarks lack sufficient semantic diversity and intent\ncoverage, we further present a comprehensive Chinese dialogue intent dataset\ncomprising over 100k real customer service calls with 1,507 human-annotated\nclusters. The proposed approaches significantly outperform LLM-guided\nbaselines, achieving notable improvements in clustering quality, cost\nefficiency, and downstream applications. Combined with several best practices,\nour findings highlight the prominence of LLM-in-the-loop techniques for\nscalable dialogue data mining."}
{"id": "2501.09997", "pdf": "https://arxiv.org/pdf/2501.09997.pdf", "abs": "https://arxiv.org/abs/2501.09997", "title": "Attention-guided Self-reflection for Zero-shot Hallucination Detection in Large Language Models", "authors": ["Qiang Liu", "Xinlong Chen", "Yue Ding", "Bowen Song", "Weiqiang Wang", "Shu Wu", "Liang Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hallucination has emerged as a significant barrier to the effective\napplication of Large Language Models (LLMs). In this work, we introduce a novel\nAttention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination\ndetection in LLMs. The AGSER method utilizes attention contributions to\ncategorize the input query into attentive and non-attentive queries. Each query\nis then processed separately through the LLMs, allowing us to compute\nconsistency scores between the generated responses and the original answer. The\ndifference between the two consistency scores serves as a hallucination\nestimator. In addition to its efficacy in detecting hallucinations, AGSER\nnotably reduces computational overhead, requiring only three passes through the\nLLM and utilizing two sets of tokens. We have conducted extensive experiments\nwith four widely-used LLMs across three different hallucination benchmarks,\ndemonstrating that our approach significantly outperforms existing methods in\nzero-shot hallucination detection."}
{"id": "2502.04387", "pdf": "https://arxiv.org/pdf/2502.04387.pdf", "abs": "https://arxiv.org/abs/2502.04387", "title": "FedP$^2$EFT: Federated Learning to Personalize PEFT for Multilingual LLMs", "authors": ["Royson Lee", "Minyoung Kim", "Fady Rezk", "Rui Li", "Stylianos I. Venieris", "Timothy Hospedales"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Federated learning (FL) has enabled the training of multilingual large\nlanguage models (LLMs) on diverse and decentralized multilingual data,\nespecially on low-resource languages. To improve client-specific performance,\npersonalization via the use of parameter-efficient fine-tuning (PEFT) modules\nsuch as LoRA is common. This involves a personalization strategy (PS), such as\nthe design of the PEFT adapter structures (e.g., in which layers to add LoRAs\nand what ranks) and choice of hyperparameters (e.g., learning rates) for\nfine-tuning. Instead of manual PS configuration, we propose FedP$^2$EFT, a\nfederated learning-to-personalize method for multilingual LLMs in cross-device\nFL settings. Unlike most existing PEFT structure selection methods, which are\nprone to overfitting low-data regimes, FedP$^2$EFT collaboratively learns the\noptimal personalized PEFT structure for each client via Bayesian sparse rank\nselection. Evaluations on both simulated and real-world multilingual FL\nbenchmarks demonstrate that FedP$^2$EFT largely outperforms existing\npersonalized fine-tuning methods, while complementing other existing FL\nmethods. Code is available at https://github.com/SamsungLabs/fedp2eft."}
{"id": "2502.11128", "pdf": "https://arxiv.org/pdf/2502.11128.pdf", "abs": "https://arxiv.org/abs/2502.11128", "title": "FELLE: Autoregressive Speech Synthesis with Token-Wise Coarse-to-Fine Flow Matching", "authors": ["Hui Wang", "Shujie Liu", "Lingwei Meng", "Jinyu Li", "Yifan Yang", "Shiwan Zhao", "Haiyang Sun", "Yanqing Liu", "Haoqin Sun", "Jiaming Zhou", "Yan Lu", "Yong Qin"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted by ACM Multimedia 2025", "summary": "To advance continuous-valued token modeling and temporal-coherence\nenforcement, we propose FELLE, an autoregressive model that integrates language\nmodeling with token-wise flow matching. By leveraging the autoregressive nature\nof language models and the generative efficacy of flow matching, FELLE\neffectively predicts continuous-valued tokens (mel-spectrograms). For each\ncontinuous-valued token, FELLE modifies the general prior distribution in flow\nmatching by incorporating information from the previous step, improving\ncoherence and stability. Furthermore, to enhance synthesis quality, FELLE\nintroduces a coarse-to-fine flow-matching mechanism, generating\ncontinuous-valued tokens hierarchically, conditioned on the language model's\noutput. Experimental results demonstrate the potential of incorporating\nflow-matching techniques in autoregressive mel-spectrogram modeling, leading to\nsignificant improvements in TTS generation quality, as shown in\nhttps://aka.ms/felle."}
{"id": "2502.14791", "pdf": "https://arxiv.org/pdf/2502.14791.pdf", "abs": "https://arxiv.org/abs/2502.14791", "title": "Rapid Word Learning Through Meta In-Context Learning", "authors": ["Wentao Wang", "Guangyuan Jiang", "Tal Linzen", "Brenden M. Lake"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Humans can quickly learn a new word from a few illustrative examples, and\nthen systematically and flexibly use it in novel contexts. Yet the abilities of\ncurrent language models for few-shot word learning, and methods for improving\nthese abilities, are underexplored. In this study, we introduce a novel method,\nMeta-training for IN-context learNing Of Words (Minnow). This method trains\nlanguage models to generate new examples of a word's usage given a few\nin-context examples, using a special placeholder token to represent the new\nword. This training is repeated on many new words to develop a general\nword-learning ability. We find that training models from scratch with Minnow on\nhuman-scale child-directed language enables strong few-shot word learning,\ncomparable to a large language model (LLM) pre-trained on orders of magnitude\nmore data. Furthermore, through discriminative and generative evaluations, we\ndemonstrate that finetuning pre-trained LLMs with Minnow improves their ability\nto discriminate between new words, identify syntactic categories of new words,\nand generate reasonable new usages and definitions for new words, based on one\nor a few in-context examples. These findings highlight the data efficiency of\nMinnow and its potential to improve language model performance in word learning\ntasks."}
{"id": "2502.18179", "pdf": "https://arxiv.org/pdf/2502.18179.pdf", "abs": "https://arxiv.org/abs/2502.18179", "title": "Problem Solved? Information Extraction Design Space for Layout-Rich Documents using LLMs", "authors": ["Gaye Colakoglu", "Gürkan Solmaz", "Jonathan Fürst"], "categories": ["cs.CL", "cs.AI"], "comment": "accepted at EMNLP'25", "summary": "This paper defines and explores the design space for information extraction\n(IE) from layout-rich documents using large language models (LLMs). The three\ncore challenges of layout-aware IE with LLMs are 1) data structuring, 2) model\nengagement, and 3) output refinement. Our study investigates the sub-problems\nand methods within these core challenges, such as input representation,\nchunking, prompting, selection of LLMs, and multimodal models. It examines the\neffect of different design choices through LayIE-LLM, a new, open-source,\nlayout-aware IE test suite, benchmarking against traditional, fine-tuned IE\nmodels. The results on two IE datasets show that LLMs require adjustment of the\nIE pipeline to achieve competitive performance: the optimized configuration\nfound with LayIE-LLM achieves 13.3--37.5 F1 points more than a general-practice\nbaseline configuration using the same LLM. To find a well-working\nconfiguration, we develop a one-factor-at-a-time (OFAT) method that achieves\nnear-optimal results. Our method is only 0.8--1.8 points lower than the best\nfull factorial exploration with a fraction (2.8%) of the required computation.\nOverall, we demonstrate that, if well-configured, general-purpose LLMs match\nthe performance of specialized models, providing a cost-effective,\nfinetuning-free alternative. Our test-suite is available at\nhttps://github.com/gayecolakoglu/LayIE-LLM."}
{"id": "2503.23768", "pdf": "https://arxiv.org/pdf/2503.23768.pdf", "abs": "https://arxiv.org/abs/2503.23768", "title": "Texture or Semantics? Vision-Language Models Get Lost in Font Recognition", "authors": ["Zhecheng Li", "Guoxian Song", "Yujun Cai", "Zhen Xiong", "Junsong Yuan", "Yiwei Wang"], "categories": ["cs.CL", "cs.CV"], "comment": "Accepted to COLM 2025", "summary": "Modern Vision-Language Models (VLMs) exhibit remarkable visual and linguistic\ncapabilities, achieving impressive performance in various tasks such as image\nrecognition and object localization. However, their effectiveness in\nfine-grained tasks remains an open question. In everyday scenarios, individuals\nencountering design materials, such as magazines, typography tutorials,\nresearch papers, or branding content, may wish to identify aesthetically\npleasing fonts used in the text. Given their multimodal capabilities and free\naccessibility, many VLMs are often considered potential tools for font\nrecognition. This raises a fundamental question: Do VLMs truly possess the\ncapability to recognize fonts? To investigate this, we introduce the Font\nRecognition Benchmark (FRB), a compact and well-structured dataset comprising\n15 commonly used fonts. FRB includes two versions: (i) an easy version, where\n10 sentences are rendered in different fonts, and (ii) a hard version, where\neach text sample consists of the names of the 15 fonts themselves, introducing\na stroop effect that challenges model perception. Through extensive evaluation\nof various VLMs on font recognition tasks, we arrive at the following key\nfindings: (i) Current VLMs exhibit limited font recognition capabilities, with\nmany state-of-the-art models failing to achieve satisfactory performance and\nbeing easily affected by the stroop effect introduced by textual information.\n(ii) Few-shot learning and Chain-of-Thought (CoT) prompting provide minimal\nbenefits in improving font recognition accuracy across different VLMs. (iii)\nAttention analysis sheds light on the inherent limitations of VLMs in capturing\nsemantic features."}
{"id": "2504.14212", "pdf": "https://arxiv.org/pdf/2504.14212.pdf", "abs": "https://arxiv.org/abs/2504.14212", "title": "Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification", "authors": ["Takuma Udagawa", "Yang Zhao", "Hiroshi Kanayama", "Bishwaranjan Bhattacharjee"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 (Findings)", "summary": "Large language models (LLMs) acquire general linguistic knowledge from\nmassive-scale pretraining. However, pretraining data mainly comprised of\nweb-crawled texts contain undesirable social biases which can be perpetuated or\neven amplified by LLMs. In this study, we propose an efficient yet effective\nannotation pipeline to investigate social biases in the pretraining corpora.\nOur pipeline consists of protected attribute detection to identify diverse\ndemographics, followed by regard classification to analyze the language\npolarity towards each attribute. Through our experiments, we demonstrate the\neffect of our bias analysis and mitigation measures, focusing on Common Crawl\nas the most representative pretraining corpus."}
{"id": "2504.18942", "pdf": "https://arxiv.org/pdf/2504.18942.pdf", "abs": "https://arxiv.org/abs/2504.18942", "title": "LawFlow: Collecting and Simulating Lawyers' Thought Processes on Business Formation Case Studies", "authors": ["Debarati Das", "Khanh Chi Le", "Ritik Sachin Parkar", "Karin De Langis", "Brendan Madson", "Chad M. Berryman", "Robin M. Willis", "Daniel H. Moses", "Brett McDonnell", "Daniel Schwarcz", "Dongyeop Kang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at COLM 2025", "summary": "Legal practitioners, particularly those early in their careers, face complex,\nhigh-stakes tasks that require adaptive, context-sensitive reasoning. While AI\nholds promise in supporting legal work, current datasets and models are\nnarrowly focused on isolated subtasks and fail to capture the end-to-end\ndecision-making required in real-world practice. To address this gap, we\nintroduce LawFlow, a dataset of complete end-to-end legal workflows collected\nfrom trained law students, grounded in real-world business entity formation\nscenarios. Unlike prior datasets focused on input-output pairs or linear chains\nof thought, LawFlow captures dynamic, modular, and iterative reasoning\nprocesses that reflect the ambiguity, revision, and client-adaptive strategies\nof legal practice. Using LawFlow, we compare human and LLM-generated workflows,\nrevealing systematic differences in structure, reasoning flexibility, and plan\nexecution. Human workflows tend to be modular and adaptive, while LLM workflows\nare more sequential, exhaustive, and less sensitive to downstream implications.\nOur findings also suggest that legal professionals prefer AI to carry out\nsupportive roles, such as brainstorming, identifying blind spots, and surfacing\nalternatives, rather than executing complex workflows end-to-end. Our results\nhighlight both the current limitations of LLMs in supporting complex legal\nworkflows and opportunities for developing more collaborative, reasoning-aware\nlegal AI systems.\n  All data and code are available on our project page\n(https://minnesotanlp.github.io/LawFlow-website/)."}
{"id": "2505.02273", "pdf": "https://arxiv.org/pdf/2505.02273.pdf", "abs": "https://arxiv.org/abs/2505.02273", "title": "Demystifying optimized prompts in language models", "authors": ["Rimon Melamed", "Lucas H. McCabe", "H. Howie Huang"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Main", "summary": "Modern language models (LMs) are not robust to out-of-distribution inputs.\nMachine generated (``optimized'') prompts can be used to modulate LM outputs\nand induce specific behaviors while appearing completely uninterpretable. In\nthis work, we investigate the composition of optimized prompts, as well as the\nmechanisms by which LMs parse and build predictions from optimized prompts. We\nfind that optimized prompts primarily consist of punctuation and noun tokens\nwhich are more rare in the training data. Internally, optimized prompts are\nclearly distinguishable from natural language counterparts based on sparse\nsubsets of the model's activations. Across various families of\ninstruction-tuned models, optimized prompts follow a similar path in how their\nrepresentations form through the network."}
{"id": "2505.05225", "pdf": "https://arxiv.org/pdf/2505.05225.pdf", "abs": "https://arxiv.org/abs/2505.05225", "title": "QualBench: Benchmarking Chinese LLMs with Localized Professional Qualifications for Vertical Domain Evaluation", "authors": ["Mengze Hong", "Wailing Ng", "Chen Jason Zhang", "Di Jiang"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025 Main Conference. Homepage:\n  https://github.com/mengze-hong/QualBench", "summary": "The rapid advancement of Chinese LLMs underscores the need for\nvertical-domain evaluations to ensure reliable applications. However, existing\nbenchmarks often lack domain coverage and provide limited insights into the\nChinese working context. Leveraging qualification exams as a unified framework\nfor expertise evaluation, we introduce QualBench, the first multi-domain\nChinese QA benchmark dedicated to localized assessment of Chinese LLMs. The\ndataset includes over 17,000 questions across six vertical domains, drawn from\n24 Chinese qualifications to align with national policies and professional\nstandards. Results reveal an interesting pattern of Chinese LLMs consistently\nsurpassing non-Chinese models, with the Qwen2.5 model outperforming the more\nadvanced GPT-4o, emphasizing the value of localized domain knowledge in meeting\nqualification requirements. The average accuracy of 53.98% reveals the current\ngaps in domain coverage within model capabilities. Furthermore, we identify\nperformance degradation caused by LLM crowdsourcing, assess data contamination,\nand illustrate the effectiveness of prompt engineering and model fine-tuning,\nsuggesting opportunities for future improvements through multi-domain RAG and\nFederated Learning."}
{"id": "2505.05755", "pdf": "https://arxiv.org/pdf/2505.05755.pdf", "abs": "https://arxiv.org/abs/2505.05755", "title": "Insertion Language Models: Sequence Generation with Arbitrary-Position Insertions", "authors": ["Dhruvesh Patel", "Aishwarya Sahoo", "Avinash Amballa", "Tahira Naseem", "Tim G. J. Rudner", "Andrew McCallum"], "categories": ["cs.CL", "cs.LG"], "comment": "Additional related work. Code available at:\n  https://dhruveshp.com/projects/ilm", "summary": "Autoregressive models (ARMs), which predict subsequent tokens one-by-one\n``from left to right,'' have achieved significant success across a wide range\nof sequence generation tasks. However, they struggle to accurately represent\nsequences that require satisfying sophisticated constraints or whose sequential\ndependencies are better addressed by out-of-order generation. Masked Diffusion\nModels (MDMs) address some of these limitations, but the process of unmasking\nmultiple tokens simultaneously in MDMs can introduce incoherences, and MDMs\ncannot handle arbitrary infilling constraints when the number of tokens to be\nfilled in is not known in advance. In this work, we introduce Insertion\nLanguage Models (ILMs), which learn to insert tokens at arbitrary positions in\na sequence -- that is, they select jointly both the position and the vocabulary\nelement to be inserted. By inserting tokens one at a time, ILMs can represent\nstrong dependencies between tokens, and their ability to generate sequences in\narbitrary order allows them to accurately model sequences where token\ndependencies do not follow a left-to-right sequential structure. To train ILMs,\nwe propose a tailored network parameterization and use a simple denoising\nobjective. Our empirical evaluation demonstrates that ILMs outperform both ARMs\nand MDMs on common planning tasks. Furthermore, we show that ILMs outperform\nMDMs and perform on par with ARMs in an unconditional text generation task\nwhile offering greater flexibility than MDMs in arbitrary-length text\ninfilling. The code is available at: https://dhruveshp.com/projects/ilm ."}
{"id": "2505.16022", "pdf": "https://arxiv.org/pdf/2505.16022.pdf", "abs": "https://arxiv.org/abs/2505.16022", "title": "NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning", "authors": ["Wei Liu", "Siya Qi", "Xinyu Wang", "Chen Qian", "Yali Du", "Yulan He"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "20 pages, 5 tables, 12 figures. accepted to EMNLP 2025", "summary": "Recent advances such as DeepSeek R1-Zero highlight the effectiveness of\nincentive training, a reinforcement learning paradigm that computes rewards\nsolely based on the final answer part of a language model's output, thereby\nencouraging the generation of intermediate reasoning steps. However, these\nmethods fundamentally rely on external verifiers, which limits their\napplicability to domains like mathematics and coding where such verifiers are\nreadily available. Although reward models can serve as verifiers, they require\nhigh-quality annotated data and are costly to train. In this work, we propose\nNOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning\nframework that requires only standard supervised fine-tuning data with no need\nfor an external verifier. NOVER enables incentive training across a wide range\nof text-to-text tasks and outperforms the model of the same size distilled from\nlarge reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the\nflexibility of NOVER enables new possibilities for optimizing large language\nmodels, such as inverse incentive training."}
{"id": "2505.17067", "pdf": "https://arxiv.org/pdf/2505.17067.pdf", "abs": "https://arxiv.org/abs/2505.17067", "title": "Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning", "authors": ["Kristin Qi", "Jiali Cheng", "Youxiang Zhu", "Hadi Amiri", "Xiaohui Liang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "IEEE Global Communications Conference (GlobeCom) 2025", "summary": "Detecting Mild Cognitive Impairment from picture descriptions is critical yet\nchallenging, especially in multilingual and multiple picture settings. Prior\nwork has primarily focused on English speakers describing a single picture\n(e.g., the 'Cookie Theft'). The TAUKDIAL-2024 challenge expands this scope by\nintroducing multilingual speakers and multiple pictures, which presents new\nchallenges in analyzing picture-dependent content. To address these challenges,\nwe propose a framework with three components: (1) enhancing discriminative\nrepresentation learning via supervised contrastive learning, (2) involving\nimage modality rather than relying solely on speech and text modalities, and\n(3) applying a Product of Experts (PoE) strategy to mitigate spurious\ncorrelations and overfitting. Our framework improves MCI detection performance,\nachieving a +7.1% increase in Unweighted Average Recall (UAR) (from 68.1% to\n75.2%) and a +2.9% increase in F1 score (from 80.6% to 83.5%) compared to the\ntext unimodal baseline. Notably, the contrastive learning component yields\ngreater gains for the text modality compared to speech. These results highlight\nour framework's effectiveness in multilingual and multi-picture MCI detection."}
{"id": "2505.17137", "pdf": "https://arxiv.org/pdf/2505.17137.pdf", "abs": "https://arxiv.org/abs/2505.17137", "title": "Cog-TiPRO: Iterative Prompt Refinement with LLMs to Detect Cognitive Decline via Longitudinal Voice Assistant Commands", "authors": ["Kristin Qi", "Youxiang Zhu", "Caroline Summerour", "John A. Batsis", "Xiaohui Liang"], "categories": ["cs.CL", "cs.AI"], "comment": "IEEE Global Communications Conference (GlobeCom) 2025", "summary": "Early detection of cognitive decline is crucial for enabling interventions\nthat can slow neurodegenerative disease progression. Traditional diagnostic\napproaches rely on labor-intensive clinical assessments, which are impractical\nfor frequent monitoring. Our pilot study investigates voice assistant systems\n(VAS) as non-invasive tools for detecting cognitive decline through\nlongitudinal analysis of speech patterns in voice commands. Over an 18-month\nperiod, we collected voice commands from 35 older adults, with 15 participants\nproviding daily at-home VAS interactions. To address the challenges of\nanalyzing these short, unstructured and noisy commands, we propose Cog-TiPRO, a\nframework that combines (1) LLM-driven iterative prompt refinement for\nlinguistic feature extraction, (2) HuBERT-based acoustic feature extraction,\nand (3) transformer-based temporal modeling. Using iTransformer, our approach\nachieves 73.80% accuracy and 72.67% F1-score in detecting MCI, outperforming\nits baseline by 27.13%. Through our LLM approach, we identify linguistic\nfeatures that uniquely characterize everyday command usage patterns in\nindividuals experiencing cognitive decline."}
{"id": "2505.20015", "pdf": "https://arxiv.org/pdf/2505.20015.pdf", "abs": "https://arxiv.org/abs/2505.20015", "title": "On the class of coding optimality of human languages and the origins of Zipf's law", "authors": ["Ramon Ferrer-i-Cancho"], "categories": ["cs.CL", "physics.soc-ph"], "comment": "a few typos corrected, in press in Europhysics Letters", "summary": "Here we present a new class of optimality for coding systems. Members of that\nclass are displaced linearly from optimal coding and thus exhibit Zipf's law,\nnamely a power-law distribution of frequency ranks. Within that class, Zipf's\nlaw, the size-rank law and the size-probability law form a group-like\nstructure. We identify human languages that are members of the class. All\nlanguages showing sufficient agreement with Zipf's law are potential members of\nthe class. In contrast, there are communication systems in other species that\ncannot be members of that class for exhibiting an exponential distribution\ninstead but dolphins and humpback whales might. We provide a new insight into\nplots of frequency versus rank in double logarithmic scale. For any system, a\nstraight line in that scale indicates that the lengths of optimal codes under\nnon-singular coding and under uniquely decodable encoding are displaced by a\nlinear function whose slope is the exponent of Zipf's law. For systems under\ncompression and constrained to be uniquely decodable, such a straight line may\nindicate that the system is coding close to optimality. We provide support for\nthe hypothesis that Zipf's law originates from compression and define testable\nconditions for the emergence of Zipf's law in compressing systems."}
{"id": "2506.21619", "pdf": "https://arxiv.org/pdf/2506.21619.pdf", "abs": "https://arxiv.org/abs/2506.21619", "title": "IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech", "authors": ["Siyi Zhou", "Yiquan Zhou", "Yi He", "Xun Zhou", "Jinchao Wang", "Wei Deng", "Jingchen Shu"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Existing autoregressive large-scale text-to-speech (TTS) models have\nadvantages in speech naturalness, but their token-by-token generation mechanism\nmakes it difficult to precisely control the duration of synthesized speech.\nThis becomes a significant limitation in applications requiring strict\naudio-visual synchronization, such as video dubbing. This paper introduces\nIndexTTS2, which proposes a novel, general, and autoregressive model-friendly\nmethod for speech duration control. The method supports two generation modes:\none explicitly specifies the number of generated tokens to precisely control\nspeech duration; the other freely generates speech in an autoregressive manner\nwithout specifying the number of tokens, while faithfully reproducing the\nprosodic features of the input prompt. Furthermore, IndexTTS2 achieves\ndisentanglement between emotional expression and speaker identity, enabling\nindependent control over timbre and emotion. In the zero-shot setting, the\nmodel can accurately reconstruct the target timbre (from the timbre prompt)\nwhile perfectly reproducing the specified emotional tone (from the style\nprompt). To enhance speech clarity in highly emotional expressions, we\nincorporate GPT latent representations and design a novel three-stage training\nparadigm to improve the stability of the generated speech. Additionally, to\nlower the barrier for emotional control, we designed a soft instruction\nmechanism based on text descriptions by fine-tuning Qwen3, effectively guiding\nthe generation of speech with the desired emotional orientation. Finally,\nexperimental results on multiple datasets show that IndexTTS2 outperforms\nstate-of-the-art zero-shot TTS models in terms of word error rate, speaker\nsimilarity, and emotional fidelity. Audio samples are available at:\nhttps://index-tts.github.io/index-tts2.github.io/"}
{"id": "2507.04416", "pdf": "https://arxiv.org/pdf/2507.04416.pdf", "abs": "https://arxiv.org/abs/2507.04416", "title": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based Sequence Modeling", "authors": ["Xiuying Wei", "Anunay Yadav", "Razvan Pascanu", "Caglar Gulcehre"], "categories": ["cs.CL"], "comment": null, "summary": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT."}
{"id": "2507.20301", "pdf": "https://arxiv.org/pdf/2507.20301.pdf", "abs": "https://arxiv.org/abs/2507.20301", "title": "Advancing Dialectal Arabic to Modern Standard Arabic Machine Translation", "authors": ["Abdullah Alabdullah", "Lifeng Han", "Chenghua Lin"], "categories": ["cs.CL"], "comment": null, "summary": "Dialectal Arabic (DA) poses a persistent challenge for natural language\nprocessing (NLP), as most everyday communication in the Arab world occurs in\ndialects that diverge significantly from Modern Standard Arabic (MSA). This\nlinguistic divide impedes progress in Arabic machine translation. This paper\npresents two core contributions to advancing DA-MSA translation for the\nLevantine, Egyptian, and Gulf dialects, particularly in low-resource and\ncomputationally constrained settings: (i) a comprehensive evaluation of\ntraining-free prompting techniques, and (ii) the development of a\nresource-efficient fine-tuning pipeline. Our evaluation of prompting strategies\nacross six large language models (LLMs) found that few-shot prompting\nconsistently outperformed zero-shot, chain-of-thought, and our proposed\nAra-TEaR method. Ara-TEaR is designed as a three-stage self-refinement\nprompting process, targeting frequent meaning-transfer and adaptation errors in\nDA-MSA translation. In this evaluation, GPT-4o achieved the highest performance\nacross all prompting settings. For fine-tuning LLMs, a quantized Gemma2-9B\nmodel achieved a chrF++ score of 49.88, outperforming zero-shot GPT-4o (44.58).\nJoint multi-dialect trained models outperformed single-dialect counterparts by\nover 10% chrF++, and 4-bit quantization reduced memory usage by 60% with less\nthan 1% performance loss. The results and insights of our experiments offer a\npractical blueprint for improving dialectal inclusion in Arabic NLP, showing\nthat high-quality DA-MSA machine translation is achievable even with limited\nresources and paving the way for more inclusive language technologies."}
{"id": "2508.11133", "pdf": "https://arxiv.org/pdf/2508.11133.pdf", "abs": "https://arxiv.org/abs/2508.11133", "title": "MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents", "authors": ["Tomer Wolfson", "Harsh Trivedi", "Mor Geva", "Yoav Goldberg", "Dan Roth", "Tushar Khot", "Ashish Sabharwal", "Reut Tsarfaty"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "Accepted for publication in Transactions of the Association for\n  Computational Linguistics (TACL), 2025. Authors pre-print", "summary": "Automated agents, powered by Large language models (LLMs), are emerging as\nthe go-to tool for querying information. However, evaluation benchmarks for LLM\nagents rarely feature natural questions that are both information-seeking and\ngenuinely time-consuming for humans. To address this gap we introduce MoNaCo, a\nbenchmark of 1,315 natural and time-consuming questions that require dozens,\nand at times hundreds, of intermediate steps to solve -- far more than any\nexisting QA benchmark. To build MoNaCo, we developed a decomposed annotation\npipeline to elicit and manually answer real-world time-consuming questions at\nscale. Frontier LLMs evaluated on MoNaCo achieve at most 61.2% F1, hampered by\nlow recall and hallucinations. Our results underscore the limitations of\nLLM-powered agents in handling the complexity and sheer breadth of real-world\ninformation-seeking tasks -- with MoNaCo providing an effective resource for\ntracking such progress. The MoNaCo benchmark, codebase, prompts and models\npredictions are all publicly available at:\nhttps://tomerwolgithub.github.io/monaco"}
{"id": "2508.19828", "pdf": "https://arxiv.org/pdf/2508.19828.pdf", "abs": "https://arxiv.org/abs/2508.19828", "title": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning", "authors": ["Sikuan Yan", "Xiufeng Yang", "Zuchao Huang", "Ercong Nie", "Zifeng Ding", "Zonggen Li", "Xiaowen Ma", "Hinrich Schütze", "Volker Tresp", "Yunpu Ma"], "categories": ["cs.CL", "cs.MA"], "comment": "work in progress", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na wide range of NLP tasks, but they remain fundamentally stateless, constrained\nby limited context windows that hinder long-horizon reasoning. Recent efforts\nto address this limitation often augment LLMs with an external memory bank, yet\nmost existing pipelines are static and heuristic-driven, lacking any learned\nmechanism for deciding what to store, update, or retrieve. We present\nMemory-R1, a reinforcement learning (RL) framework that equips LLMs with the\nability to actively manage and utilize external memory through two specialized\nagents: a Memory Manager that learns to perform structured memory operations,\nincluding adding, updating, deleting, or taking no operation on memory entries;\nand an Answer Agent that selects the most relevant entries and reasons over\nthem to produce an answer. Both agents are fine-tuned with outcome-driven RL\n(PPO and GRPO), enabling adaptive memory management and utilization with\nminimal supervision. With as few as 152 question-answer pairs and a\ncorresponding temporal memory bank for training, Memory-R1 outperforms the\nstrongest existing baseline and demonstrates strong generalization across\ndiverse question types and LLM backbones. Beyond presenting an effective\napproach, this work provides insights into how RL can unlock more agentic,\nmemory-aware behavior in LLMs, pointing toward richer, more persistent\nreasoning systems."}
{"id": "2508.20757", "pdf": "https://arxiv.org/pdf/2508.20757.pdf", "abs": "https://arxiv.org/abs/2508.20757", "title": "GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and Efficient Open-Ended Text Generation", "authors": ["Yuanhao Ding", "Esteban Garces Arias", "Meimingwei Li", "Julian Rodemann", "Matthias Aßenmacher", "Danlu Chen", "Gaojuan Fan", "Christian Heumann", "Chongsheng Zhang"], "categories": ["cs.CL"], "comment": "Accepted at Findings of the Association for Computational\n  Linguistics: EMNLP 2025", "summary": "Open-ended text generation faces a critical challenge: balancing coherence\nwith diversity in LLM outputs. While contrastive search-based decoding\nstrategies have emerged to address this trade-off, their practical utility is\noften limited by hyperparameter dependence and high computational costs. We\nintroduce GUARD, a self-adaptive decoding method that effectively balances\nthese competing objectives through a novel \"Glocal\" uncertainty-driven\nframework. GUARD combines global entropy estimates with local entropy\ndeviations to integrate both long-term and short-term uncertainty signals. We\ndemonstrate that our proposed global entropy formulation effectively mitigates\nabrupt variations in uncertainty, such as sudden overconfidence or high entropy\nspikes, and provides theoretical guarantees of unbiasedness and consistency. To\nreduce computational overhead, we incorporate a simple yet effective\ntoken-count-based penalty into GUARD. Experimental results demonstrate that\nGUARD achieves a good balance between text diversity and coherence, while\nexhibiting substantial improvements in generation speed. In a more nuanced\ncomparison study across different dimensions of text quality, both human and\nLLM evaluators validated its remarkable performance. Our code is available at\nhttps://github.com/YecanLee/GUARD."}
{"id": "2509.00591", "pdf": "https://arxiv.org/pdf/2509.00591.pdf", "abs": "https://arxiv.org/abs/2509.00591", "title": "Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and Quantifying Evaluation Awareness", "authors": ["Lang Xiong", "Nishant Bhargava", "Wesley Chang", "Jianhang Hong", "Haihao Liu", "Kevin Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often exhibit significant behavioral shifts when\nthey perceive a change from a real-world deployment context to a controlled\nevaluation setting, a phenomenon known as \"evaluation awareness.\" This\ndiscrepancy poses a critical challenge for AI alignment, as benchmark\nperformance may not accurately reflect a model's true safety and honesty. In\nthis work, we systematically quantify these behavioral changes by manipulating\nthe perceived context of prompts. We introduce a methodology that uses a linear\nprobe to score prompts on a continuous scale from \"test-like\" to \"deploy-like\"\nand leverage an LLM rewriting strategy to shift these prompts towards a more\nnatural, deployment-style context while preserving the original task. Using\nthis method, we achieved a 30% increase in the average probe score across a\nstrategic role-playing dataset after rewriting. Evaluating a suite of\nstate-of-the-art models on these original and rewritten prompts, we find that\nrewritten \"deploy-like\" prompts induce a significant and consistent shift in\nbehavior. Across all models, we observed an average increase in honest\nresponses of 5.26% and a corresponding average decrease in deceptive responses\nof 12.40%. Furthermore, refusal rates increased by an average of 6.38%,\nindicating heightened safety compliance. Our findings demonstrate that\nevaluation awareness is a quantifiable and manipulable factor that directly\ninfluences LLM behavior, revealing that models are more prone to unsafe or\ndeceptive outputs in perceived test environments. This underscores the urgent\nneed for more realistic evaluation frameworks to accurately gauge true model\nalignment before deployment."}
{"id": "2509.02170", "pdf": "https://arxiv.org/pdf/2509.02170.pdf", "abs": "https://arxiv.org/abs/2509.02170", "title": "Avoidance Decoding for Diverse Multi-Branch Story Generation", "authors": ["Kyeongman Park", "Nakyeong Yang", "Kyomin Jung"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often generate repetitive and monotonous\noutputs, especially in tasks like story generation, due to limited creative\ndiversity when given the same input prompt. To address this challenge, we\npropose a novel decoding strategy, Avoidance Decoding, that modifies token\nlogits by penalizing similarity to previously generated outputs, thereby\nencouraging more diverse multi-branch stories. This penalty adaptively balances\ntwo similarity measures: (1) Concept-level Similarity Penalty, which is\nprioritized in early stages to diversify initial story concepts, and (2)\nNarrative-level Similarity Penalty, which is increasingly emphasized later to\nensure natural yet diverse plot development. Notably, our method achieves up to\n2.6 times higher output diversity and reduces repetition by an average of 30%\ncompared to strong baselines, while effectively mitigating text degeneration.\nFurthermore, we reveal that our method activates a broader range of neurons,\ndemonstrating that it leverages the model's intrinsic creativity."}
{"id": "2509.02499", "pdf": "https://arxiv.org/pdf/2509.02499.pdf", "abs": "https://arxiv.org/abs/2509.02499", "title": "MoSEs: Uncertainty-Aware AI-Generated Text Detection via Mixture of Stylistics Experts with Conditional Thresholds", "authors": ["Junxi Wu", "Jinpeng Wang", "Zheng Liu", "Bin Chen", "Dongjian Hu", "Hao Wu", "Shu-Tao Xia"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025", "summary": "The rapid advancement of large language models has intensified public\nconcerns about the potential misuse. Therefore, it is important to build\ntrustworthy AI-generated text detection systems. Existing methods neglect\nstylistic modeling and mostly rely on static thresholds, which greatly limits\nthe detection performance. In this paper, we propose the Mixture of Stylistic\nExperts (MoSEs) framework that enables stylistics-aware uncertainty\nquantification through conditional threshold estimation. MoSEs contain three\ncore components, namely, the Stylistics Reference Repository (SRR), the\nStylistics-Aware Router (SAR), and the Conditional Threshold Estimator (CTE).\nFor input text, SRR can activate the appropriate reference data in SRR and\nprovide them to CTE. Subsequently, CTE jointly models the linguistic\nstatistical properties and semantic features to dynamically determine the\noptimal threshold. With a discrimination score, MoSEs yields prediction labels\nwith the corresponding confidence level. Our framework achieves an average\nimprovement 11.34% in detection performance compared to baselines. More\ninspiringly, MoSEs shows a more evident improvement 39.15% in the low-resource\ncase. Our code is available at https://github.com/creator-xi/MoSEs."}
{"id": "2208.13266", "pdf": "https://arxiv.org/pdf/2208.13266.pdf", "abs": "https://arxiv.org/abs/2208.13266", "title": "JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents", "authors": ["Kaizhi Zheng", "Kaiwen Zhou", "Jing Gu", "Yue Fan", "Jialu Wang", "Zonglin Di", "Xuehai He", "Xin Eric Wang"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.RO"], "comment": "19th International Conference on Neurosymbolic Learning and Reasoning", "summary": "Building a conversational embodied agent to execute real-life tasks has been\na long-standing yet quite challenging research goal, as it requires effective\nhuman-agent communication, multi-modal understanding, long-range sequential\ndecision making, etc. Traditional symbolic methods have scaling and\ngeneralization issues, while end-to-end deep learning models suffer from data\nscarcity and high task complexity, and are often hard to explain. To benefit\nfrom both worlds, we propose JARVIS, a neuro-symbolic commonsense reasoning\nframework for modular, generalizable, and interpretable conversational embodied\nagents. First, it acquires symbolic representations by prompting large language\nmodels (LLMs) for language understanding and sub-goal planning, and by\nconstructing semantic maps from visual observations. Then the symbolic module\nreasons for sub-goal planning and action generation based on task- and\naction-level common sense. Extensive experiments on the TEACh dataset validate\nthe efficacy and efficiency of our JARVIS framework, which achieves\nstate-of-the-art (SOTA) results on all three dialog-based embodied tasks,\nincluding Execution from Dialog History (EDH), Trajectory from Dialog (TfD),\nand Two-Agent Task Completion (TATC) (e.g., our method boosts the unseen\nSuccess Rate on EDH from 6.1\\% to 15.8\\%). Moreover, we systematically analyze\nthe essential factors that affect the task performance and also demonstrate the\nsuperiority of our method in few-shot settings. Our JARVIS model ranks first in\nthe Alexa Prize SimBot Public Benchmark Challenge."}
{"id": "2403.04931", "pdf": "https://arxiv.org/pdf/2403.04931.pdf", "abs": "https://arxiv.org/abs/2403.04931", "title": "A Survey on Human-AI Collaboration with Large Foundation Models", "authors": ["Vanshika Vats", "Marzia Binta Nizam", "Minghao Liu", "Ziyuan Wang", "Richard Ho", "Mohnish Sai Prasad", "Vincent Titterton", "Sai Venkat Malreddy", "Riya Aggarwal", "Yanwen Xu", "Lei Ding", "Jay Mehta", "Nathan Grinnell", "Li Liu", "Sijia Zhong", "Devanathan Nallur Gandamani", "Xinyi Tang", "Rohan Ghosalkar", "Celeste Shen", "Rachel Shen", "Nafisa Hussain", "Kesav Ravichandran", "James Davis"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "Topic and scope refinement", "summary": "As the capabilities of artificial intelligence (AI) continue to expand\nrapidly, Human-AI (HAI) Collaboration, combining human intellect and AI\nsystems, has become pivotal for advancing problem-solving and decision-making\nprocesses. The advent of Large Foundation Models (LFMs) has greatly expanded\nits potential, offering unprecedented capabilities by leveraging vast amounts\nof data to understand and predict complex patterns. At the same time, realizing\nthis potential responsibly requires addressing persistent challenges related to\nsafety, fairness, and control. This paper reviews the crucial integration of\nLFMs with HAI, highlighting both opportunities and risks. We structure our\nanalysis around four areas: human-guided model development, collaborative\ndesign principles, ethical and governance frameworks, and applications in\nhigh-stakes domains. Our review shows that successful HAI systems are not the\nautomatic result of stronger models but the product of careful, human-centered\ndesign. By identifying key open challenges, this survey aims to give insight\ninto current and future research that turns the raw power of LFMs into\npartnerships that are reliable, trustworthy, and beneficial to society."}
{"id": "2411.02708", "pdf": "https://arxiv.org/pdf/2411.02708.pdf", "abs": "https://arxiv.org/abs/2411.02708", "title": "Exploring Response Uncertainty in MLLMs: An Empirical Evaluation under Misleading Scenarios", "authors": ["Yunkai Dang", "Mengxi Gao", "Yibo Yan", "Xin Zou", "Yanggan Gu", "Jungang Li", "Jingyu Wang", "Peijie Jiang", "Aiwei Liu", "Jia Liu", "Xuming Hu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Multimodal large language models (MLLMs) have recently achieved\nstate-of-the-art performance on tasks ranging from visual question answering to\nvideo understanding. However, existing studies have concentrated mainly on\nvisual-textual misalignment, leaving largely unexplored the MLLMs' ability to\npreserve an originally correct answer when confronted with misleading\ninformation. We reveal a response uncertainty phenomenon: across nine standard\ndatasets, twelve state-of-the-art open-source MLLMs overturn a previously\ncorrect answer in 65% of cases after receiving a single deceptive cue. To\nsystematically quantify this vulnerability, we propose a two-stage evaluation\npipeline: (1) elicit each model's original response on unperturbed inputs; (2)\ninject explicit (false-answer hints) and implicit (contextual contradictions)\nmisleading instructions, and compute the misleading rate - the fraction of\ncorrect-to-incorrect flips. Leveraging the most susceptible examples, we curate\nthe Multimodal Uncertainty Benchmark (MUB), a collection of image-question\npairs stratified into low, medium, and high difficulty based on how many of\ntwelve state-of-the-art MLLMs they mislead. Extensive evaluation on twelve\nopen-source and five closed-source models reveals a high uncertainty: average\nmisleading rates exceed 86%, with explicit cues over 67.19% and implicit cues\nover 80.67%. To reduce the misleading rate, we then fine-tune all open-source\nMLLMs on a compact 2000-sample mixed-instruction dataset, reducing misleading\nrates to 6.97% (explicit) and 32.77% (implicit), boosting consistency by nearly\n29.37% on highly deceptive inputs, and slightly improving accuracy on standard\nbenchmarks. Our code is available at https://github.com/Yunkaidang/uncertainty"}
{"id": "2411.05085", "pdf": "https://arxiv.org/pdf/2411.05085.pdf", "abs": "https://arxiv.org/abs/2411.05085", "title": "PadChest-GR: A Bilingual Chest X-ray Dataset for Grounded Radiology Report Generation", "authors": ["Daniel C. Castro", "Aurelia Bustos", "Shruthi Bannur", "Stephanie L. Hyland", "Kenza Bouzid", "Maria Teodora Wetscherek", "Maria Dolores Sánchez-Valverde", "Lara Jaques-Pérez", "Lourdes Pérez-Rodríguez", "Kenji Takeda", "José María Salinas", "Javier Alvarez-Valle", "Joaquín Galant Herrero", "Antonio Pertusa"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Radiology report generation (RRG) aims to create free-text radiology reports\nfrom clinical imaging. Grounded radiology report generation (GRRG) extends RRG\nby including the localisation of individual findings on the image. Currently,\nthere are no manually annotated chest X-ray (CXR) datasets to train GRRG\nmodels. In this work, we present a dataset called PadChest-GR\n(Grounded-Reporting) derived from PadChest aimed at training GRRG models for\nCXR images. We curate a public bi-lingual dataset of 4,555 CXR studies with\ngrounded reports (3,099 abnormal and 1,456 normal), each containing complete\nlists of sentences describing individual present (positive) and absent\n(negative) findings in English and Spanish. In total, PadChest-GR contains\n7,037 positive and 3,422 negative finding sentences. Every positive finding\nsentence is associated with up to two independent sets of bounding boxes\nlabelled by different readers and has categorical labels for finding type,\nlocations, and progression. To the best of our knowledge, PadChest-GR is the\nfirst manually curated dataset designed to train GRRG models for understanding\nand interpreting radiological images and generated text. By including detailed\nlocalization and comprehensive annotations of all clinically relevant findings,\nit provides a valuable resource for developing and evaluating GRRG models from\nCXR images. PadChest-GR can be downloaded under request from\nhttps://bimcv.cipf.es/bimcv-projects/padchest-gr/"}
{"id": "2503.05720", "pdf": "https://arxiv.org/pdf/2503.05720.pdf", "abs": "https://arxiv.org/abs/2503.05720", "title": "That is Unacceptable: the Moral Foundations of Canceling", "authors": ["Soda Marem Lo", "Oscar Araque", "Rajesh Sharma", "Marco Antonio Stranisci"], "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "Canceling is a morally-driven phenomenon that hinders the development of safe\nsocial media platforms and contributes to ideological polarization. To address\nthis issue we present the Canceling Attitudes Detection (CADE) dataset, an\nannotated corpus of canceling incidents aimed at exploring the factors of\ndisagreements in evaluating people canceling attitudes on social media.\nSpecifically, we study the impact of annotators' morality in their perception\nof canceling, showing that morality is an independent axis for the explanation\nof disagreement on this phenomenon. Annotator's judgments heavily depend on the\ntype of controversial events and involved celebrities. This shows the need to\ndevelop more event-centric datasets to better understand how harms are\nperpetrated in social media and to develop more aware technologies for their\ndetection."}
{"id": "2506.23367", "pdf": "https://arxiv.org/pdf/2506.23367.pdf", "abs": "https://arxiv.org/abs/2506.23367", "title": "You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel Properties", "authors": ["Paige Tuttösí", "H. Henny Yeung", "Yue Wang", "Jean-Julien Aucouturier", "Angelica Lim"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to ISCA Speech Synthesis Workshop, 2025, Project webpage\n  here: https://rosielab.github.io/clear_speech/ Code here:\n  https://github.com/chocobearz/Matcha-TTS-L2-clarity", "summary": "We present the first text-to-speech (TTS) system tailored to second language\n(L2) speakers. We use duration differences between American English tense\n(longer) and lax (shorter) vowels to create a \"clarity mode\" for Matcha-TTS.\nOur perception studies showed that French-L1, English-L2 listeners had fewer\n(at least 9.15%) transcription errors when using our clarity mode, and found it\nmore encouraging and respectful than overall slowed down speech. Remarkably,\nlisteners were not aware of these effects: despite the decreased word error\nrate in clarity mode, listeners still believed that slowing all target words\nwas the most intelligible, suggesting that actual intelligibility does not\ncorrelate with perceived intelligibility. Additionally, we found that\nWhisper-ASR did not use the same cues as L2 speakers to differentiate difficult\nvowels and is not sufficient to assess the intelligibility of TTS systems for\nthese individuals."}
{"id": "2507.23751", "pdf": "https://arxiv.org/pdf/2507.23751.pdf", "abs": "https://arxiv.org/abs/2507.23751", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "authors": ["Ping Yu", "Jack Lanchantin", "Tianlu Wang", "Weizhe Yuan", "Olga Golovneva", "Ilia Kulikov", "Sainbayar Sukhbaatar", "Jason Weston", "Jing Xu"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We propose CoT-Self-Instruct, a synthetic data generation method that\ninstructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on\ngiven seed tasks, and then generate a new synthetic example of similar quality\nand complexity. This is followed by a filtering step to select high-quality\ndata using automatic metrics, which are then used for LLM training. In\nverifiable reasoning, our synthetic data significantly outperforms existing\ntraining datasets, such as s1k and OpenMathReasoning, when evaluated on\nMATH500, AMC23, AIME24, and GPQA-Diamond. For non-verifiable\ninstruction-following tasks, our method surpasses the performance of both human\nand standard Self-Instruct training data on the AlpacaEval 2.0 and Arena-Hard\nbenchmarks."}
{"id": "2508.19200", "pdf": "https://arxiv.org/pdf/2508.19200.pdf", "abs": "https://arxiv.org/abs/2508.19200", "title": "The Ramon Llull's Thinking Machine for Automated Ideation", "authors": ["Xinran Zhao", "Boyuan Zheng", "Chenglei Si", "Haofei Yu", "Ken Liu", "Runlong Zhou", "Ruochen Li", "Tong Chen", "Xiang Li", "Yiming Zhang", "Tongshuang Wu"], "categories": ["cs.AI", "cs.CL"], "comment": "21 pages, 3 figures", "summary": "This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for\ngenerating knowledge through symbolic recombination - as a conceptual\nfoundation for building a modern Llull's thinking machine for research\nideation. Our approach defines three compositional axes: Theme (e.g.,\nefficiency, adaptivity), Domain (e.g., question answering, machine\ntranslation), and Method (e.g., adversarial training, linear attention). These\nelements represent high-level abstractions common in scientific work -\nmotivations, problem settings, and technical approaches - and serve as building\nblocks for LLM-driven exploration. We mine elements from human experts or\nconference papers and show that prompting LLMs with curated combinations\nproduces research ideas that are diverse, relevant, and grounded in current\nliterature. This modern thinking machine offers a lightweight, interpretable\ntool for augmenting scientific creativity and suggests a path toward\ncollaborative ideation between humans and AI."}
{"id": "2508.21376", "pdf": "https://arxiv.org/pdf/2508.21376.pdf", "abs": "https://arxiv.org/abs/2508.21376", "title": "AHELM: A Holistic Evaluation of Audio-Language Models", "authors": ["Tony Lee", "Haoqin Tu", "Chi Heem Wong", "Zijun Wang", "Siwei Yang", "Yifan Mai", "Yuyin Zhou", "Cihang Xie", "Percy Liang"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Evaluations of audio-language models (ALMs) -- multimodal models that take\ninterleaved audio and text as input and output text -- are hindered by the lack\nof standardized benchmarks; most benchmarks measure only one or two\ncapabilities and omit evaluative aspects such as fairness or safety.\nFurthermore, comparison across models is difficult as separate evaluations test\na limited number of models and use different prompting methods and inference\nparameters. To address these shortfalls, we introduce AHELM, a benchmark that\naggregates various datasets -- including 2 new synthetic audio-text datasets\ncalled PARADE, which evaluates the ALMs on avoiding stereotypes, and\nCoRe-Bench, which measures reasoning over conversational audio through\ninferential multi-turn question answering -- to holistically measure the\nperformance of ALMs across 10 aspects we have identified as important to the\ndevelopment and usage of ALMs: audio perception, knowledge, reasoning, emotion\ndetection, bias, fairness, multilinguality, robustness, toxicity, and safety.\nWe also standardize the prompts, inference parameters, and evaluation metrics\nto ensure equitable comparisons across models. We test 14 open-weight and\nclosed-API ALMs from 3 developers and 3 additional simple baseline systems each\nconsisting of an automatic speech recognizer and a language model. Our results\nshow that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits\ngroup unfairness ($p=0.01$) on ASR tasks whereas most of the other models do\nnot. We also find that the baseline systems perform reasonably well on AHELM,\nwith one ranking 6th overall despite having only speech-to-text capabilities.\nFor transparency, all raw prompts, model generations, and outputs are available\non our website at https://crfm.stanford.edu/helm/audio/v1.0.0. AHELM is\nintended to be a living benchmark and new datasets and models will be added\nover time."}
{"id": "2509.00096", "pdf": "https://arxiv.org/pdf/2509.00096.pdf", "abs": "https://arxiv.org/abs/2509.00096", "title": "Pruning Weights but Not Truth: Safeguarding Truthfulness While Pruning LLMs", "authors": ["Yao Fu", "Runchao Li", "Xianxuan Long", "Haotian Yu", "Xiaotian Han", "Yu Yin", "Pan Li"], "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to EMNLP2025 findings (poster)", "summary": "Neural network pruning has emerged as a promising approach for deploying LLMs\nin low-resource scenarios while preserving downstream task performance.\nHowever, for the first time, we reveal that such pruning disrupts LLMs'\ninternal activation features crucial for lie detection, where probing\nclassifiers (typically small logistic regression models) trained on these\nfeatures assess the truthfulness of LLM-generated statements. This discovery\nraises a crucial open question: how can we prune LLMs without sacrificing these\ncritical lie detection capabilities? Our investigation further reveals that\nnaively adjusting layer-wise pruning sparsity based on importance inadvertently\nremoves crucial weights, failing to improve lie detection performance despite\nits reliance on the most crucial LLM layer. To address this issue, we propose\nTruthful Pruning aligned by Layer-wise Outliers (TPLO), which places greater\nemphasis on layers with more activation outliers and stronger discriminative\nfeatures simultaneously. This preserves LLMs' original performance while\nretaining critical features of inner states needed for robust lie detection.\nMoreover, we introduce a prompting rule to enrich the TruthfulQA benchmark for\nbetter calibrating LLM pruning. Empirical results show that our approach\nimproves the hallucination detection for pruned LLMs (achieving 88% accuracy at\n50% sparsity) and enhances their performance on TruthfulQA."}
{"id": "2509.00761", "pdf": "https://arxiv.org/pdf/2509.00761.pdf", "abs": "https://arxiv.org/abs/2509.00761", "title": "L-MARS: Legal Multi-Agent Workflow with Orchestrated Reasoning and Agentic Search", "authors": ["Ziqi Wang", "Boqin Yuan"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We present L-MARS (Legal Multi-Agent Workflow with Orchestrated Reasoning and\nAgentic Search), a system that reduces hallucination and uncertainty in legal\nquestion answering through coordinated multi-agent reasoning and retrieval.\nUnlike single-pass retrieval-augmented generation (RAG), L-MARS decomposes\nqueries into subproblems, issues targeted searches across heterogeneous sources\n(Serper web, local RAG, CourtListener case law), and employs a Judge Agent to\nverify sufficiency, jurisdiction, and temporal validity before answer\nsynthesis. This iterative reasoning-search-verification loop maintains\ncoherence, filters noisy evidence, and grounds answers in authoritative law. We\nevaluated L-MARS on LegalSearchQA, a new benchmark of 200 up-to-date multiple\nchoice legal questions in 2025. Results show that L-MARS substantially improves\nfactual accuracy, reduces uncertainty, and achieves higher preference scores\nfrom both human experts and LLM-based judges. Our work demonstrates that\nmulti-agent reasoning with agentic search offers a scalable and reproducible\nblueprint for deploying LLMs in high-stakes domains requiring precise legal\nretrieval and deliberation."}
{"id": "2509.00891", "pdf": "https://arxiv.org/pdf/2509.00891.pdf", "abs": "https://arxiv.org/abs/2509.00891", "title": "ChatCLIDS: Simulating Persuasive AI Dialogues to Promote Closed-Loop Insulin Adoption in Type 1 Diabetes Care", "authors": ["Zonghai Yao", "Talha Chafekar", "Junda Wang", "Shuo Han", "Feiyun Ouyang", "Junhui Qian", "Lingxi Li", "Hong Yu"], "categories": ["cs.AI", "cs.CL"], "comment": "Equal contribution for the first two authors", "summary": "Real-world adoption of closed-loop insulin delivery systems (CLIDS) in type 1\ndiabetes remains low, driven not by technical failure, but by diverse\nbehavioral, psychosocial, and social barriers. We introduce ChatCLIDS, the\nfirst benchmark to rigorously evaluate LLM-driven persuasive dialogue for\nhealth behavior change. Our framework features a library of expert-validated\nvirtual patients, each with clinically grounded, heterogeneous profiles and\nrealistic adoption barriers, and simulates multi-turn interactions with nurse\nagents equipped with a diverse set of evidence-based persuasive strategies.\nChatCLIDS uniquely supports longitudinal counseling and adversarial social\ninfluence scenarios, enabling robust, multi-dimensional evaluation. Our\nfindings reveal that while larger and more reflective LLMs adapt strategies\nover time, all models struggle to overcome resistance, especially under\nrealistic social pressure. These results highlight critical limitations of\ncurrent LLMs for behavior change, and offer a high-fidelity, scalable testbed\nfor advancing trustworthy persuasive AI in healthcare and beyond."}
