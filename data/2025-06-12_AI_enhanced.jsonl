{"id": "2506.09089", "pdf": "https://arxiv.org/pdf/2506.09089.pdf", "abs": "https://arxiv.org/abs/2506.09089", "title": "Designing conflict-based communicative tasks in Teaching Chinese as a Foreign Language with ChatGPT", "authors": ["Xia Li"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "in French language", "summary": "In developing the teaching program for a course in Oral Expression in\nTeaching Chinese as a Foreign Language at the university level, the teacher\ndesigns communicative tasks based on conflicts to encourage learners to engage\nin interactive dynamics and develop their oral interaction skills. During the\ndesign of these tasks, the teacher uses ChatGPT to assist in finalizing the\nprogram. This article aims to present the key characteristics of the\ninteractions between the teacher and ChatGPT during this program development\nprocess, as well as to examine the use of ChatGPT and its impacts in this\nspecific context.", "AI": {"tldr": "This paper explores the integration of ChatGPT in developing oral expression tasks for teaching Chinese as a foreign language, examining the dynamics of teacher-ChatGPT interactions.", "motivation": "To enhance the teaching of oral expression in Chinese by using communicative tasks that encourage learner engagement.", "method": "The paper analyzes the interactions between a teacher and ChatGPT while designing a course program, focusing on specific communicative tasks based on conflicts.", "result": "The study highlights how ChatGPT can assist teachers in finalizing teaching programs and the nature of interactions that take place during this process.", "conclusion": "Integrating ChatGPT into course design can significantly impact the development of teaching strategies and learner participation.", "key_contributions": ["Examination of teacher-ChatGPT interaction dynamics.", "Insights into using AI in language teaching contexts.", "Impact assessment of AI tools on course program development."], "limitations": "", "keywords": ["ChatGPT", "language teaching", "oral expression", "interactivity", "AI in education"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2506.09153", "pdf": "https://arxiv.org/pdf/2506.09153.pdf", "abs": "https://arxiv.org/abs/2506.09153", "title": "Real-Time Confidence Detection through Facial Expressions and Hand Gestures", "authors": ["Tanjil Hasan Sakib", "Samia Jahan Mojumder", "Rajan Das Gupta", "Md Imrul Hasan Showmick", "Md. Yeasin Rahat", "Md. Jakir Hossen"], "categories": ["cs.HC"], "comment": "Accepted in MECON 2025", "summary": "Real-time face orientation recognition is a cutting-edge technology meant to\ntrack and analyze facial movements in virtual environments such as online\ninterviews, remote meetings, and virtual classrooms. As the demand for virtual\ninteractions grows, it becomes increasingly important to measure participant\nengagement, attention, and overall interaction. This research presents a novel\nsolution that leverages the Media Pipe Face Mesh framework to identify facial\nlandmarks and extract geometric data for calculating Euler angles, which\ndetermine head orientation in real time. The system tracks 3D facial landmarks\nand uses this data to compute head movements with a focus on accuracy and\nresponsiveness. By studying Euler angles, the system can identify a user's head\norientation with an accuracy of 90\\%, even at a distance of up to four feet.\nThis capability offers significant enhancements for monitoring user\ninteraction, allowing for more immersive and interactive virtual ex-periences.\nThe proposed method shows its reliability in evaluating participant\nattentiveness during online assessments and meetings. Its application goes\nbeyond engagement analysis, potentially providing a means for improving the\nquality of virtual communication, fostering better understanding between\nparticipants, and ensuring a higher level of interaction in digital spaces.\nThis study offers a basis for future developments in enhancing virtual user\nexperiences by integrating real-time facial tracking technologies, paving the\nway for more adaptive and interactive web-based platform.", "AI": {"tldr": "This paper presents a novel real-time face orientation recognition system utilizing the Media Pipe Face Mesh framework to enhance engagement in virtual environments.", "motivation": "The growing demand for virtual interactions necessitates effective measures of participant engagement, attention, and interaction.", "method": "The system employs the Media Pipe Face Mesh framework to track 3D facial landmarks and calculates Euler angles to determine real-time head orientation with high accuracy.", "result": "The system achieves 90% accuracy in identifying head orientation at distances of up to four feet, significantly improving the monitoring of user interaction in virtual settings.", "conclusion": "The proposed method enhances the quality of virtual communication, fostering better understanding and interaction among participants in digital spaces.", "key_contributions": ["First implementation of real-time head orientation detection using Euler angles in virtual environments.", "Integration of Media Pipe Face Mesh for accurate facial landmark tracking.", "Potential applications for improving user engagement in online assessments and meetings."], "limitations": "", "keywords": ["face orientation recognition", "virtual environments", "media pipe", "head tracking", "user engagement"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2506.09212", "pdf": "https://arxiv.org/pdf/2506.09212.pdf", "abs": "https://arxiv.org/abs/2506.09212", "title": "Show Me Your Best Side: Characteristics of User-Preferred Perspectives for 3D Graph Drawings", "authors": ["Lucas Joos", "Gavin J. Mooney", "Maximilian T. Fischer", "Daniel A. Keim", "Falk Schreiber", "Helen C. Purchase", "Karsten Klein"], "categories": ["cs.HC"], "comment": null, "summary": "The visual analysis of graphs in 3D has become increasingly popular,\naccelerated by the rise of immersive technology, such as augmented and virtual\nreality. Unlike 2D drawings, 3D graph layouts are highly viewpoint-dependent,\nmaking perspective selection critical for revealing structural and relational\npatterns. Despite its importance, there is limited empirical evidence guiding\nwhat constitutes an effective or preferred viewpoint from the user's\nperspective. In this paper, we present a systematic investigation into\nuser-preferred viewpoints in 3D graph visualisations. We conducted a controlled\nstudy with 23 participants in a virtual reality environment, where users\nselected their most and least preferred viewpoints for 36 different graphs\nvarying in size and layout. From this data, enriched by qualitative feedback,\nwe distil common strategies underlying viewpoint choice. We further analyse the\nalignment of user preferences with classical 2D aesthetic criteria (e.g.,\nCrossings), 3D-specific measures (e.g., Node-Node Occlusion), and introduce a\nnovel measure capturing the perceivability of a graph's principal axes\n(Isometric Viewpoint Deviation). Our data-driven analysis indicates that\nStress, Crossings, Gabriel Ratio, Edge-Node Overlap, and Isometric Viewpoint\nDeviation are key indicators of viewpoint preference. Beyond our findings, we\ncontribute a publicly available dataset consisting of the graphs and computed\naesthetic measures, supporting further research and the development of\nviewpoint evaluation measures for 3D graph drawing.", "AI": {"tldr": "The paper investigates user-preferred viewpoints in 3D graph visualizations through a controlled study in a virtual reality environment, revealing key factors influencing viewpoint preference.", "motivation": "The importance of viewpoint selection in 3D graph visualizations and the lack of empirical evidence on user-preferred perspectives.", "method": "A controlled study with 23 participants in a VR environment where users selected preferred and least preferred viewpoints for various graph layouts.", "result": "User preferences correlated with classical 2D aesthetic criteria and new measures, leading to the identification of key indicators influencing viewpoint choice.", "conclusion": "The findings contribute to understanding viewpoint preference in 3D graph visualizations and provide a dataset for further research.", "key_contributions": ["Systematic investigation of user-preferred viewpoints in 3D graphs", "Introduction of new measures for viewpoint evaluation", "Publicly available dataset for future research"], "limitations": "", "keywords": ["3D visualization", "User experience", "Virtual reality", "Graph aesthetics"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.09216", "pdf": "https://arxiv.org/pdf/2506.09216.pdf", "abs": "https://arxiv.org/abs/2506.09216", "title": "\"How do you even know that stuff?\": Barriers to expertise sharing among spreadsheet users", "authors": ["Qing", "Xia", "Advait Sarkar", "Duncan Brumby", "Anna Cox"], "categories": ["cs.HC", "cs.CY", "H.5"], "comment": "Accepted at CSCW 2025", "summary": "Spreadsheet collaboration provides valuable opportunities for learning and\nexpertise sharing between colleagues. Sharing expertise is essential for the\nretention of important technical skillsets within organisations, but previous\nstudies suggest that spreadsheet experts often fail to disseminate their\nknowledge to others. We suggest that social norms and beliefs surrounding the\nvalue of spreadsheet use significantly influence user engagement in sharing\nbehaviours. To explore this, we conducted 31 semi-structured interviews with\nprofessional spreadsheet users from two separate samples. We found that\nspreadsheet providers face challenges in adapting highly personalised\nstrategies to often subjective standards and evaluating the appropriate social\ntiming of sharing. In addition, conflicted self-evaluations of one's\nspreadsheet expertise, dismissive normative beliefs about the value of this\nknowledge, and concerns about the potential disruptions associated with\ncollaboration can further deter sharing. We suggest these observations reflect\nthe challenges of long-term learning in feature-rich software designed\nprimarily with initial learnability in mind. We therefore provide implications\nfor design to navigate this tension. Overall, our findings demonstrate how the\ncomplex interaction between technology design and social dynamics can shape\ncollaborative learning behaviours in the context of feature-rich software.", "AI": {"tldr": "This paper explores how social norms and beliefs affect knowledge sharing among spreadsheet users, revealing challenges in adapting personalized strategies and self-evaluations of expertise.", "motivation": "The study investigates the under-explored area of knowledge sharing in spreadsheet collaboration, which is crucial for retaining technical skillsets in organizations.", "method": "The research involved conducting 31 semi-structured interviews with professional spreadsheet users from two different samples to understand their collaboration behaviors.", "result": "Findings indicate that users struggle with sharing strategies due to personalized approaches, conflicted self-assessments of their skills, and dismissive views on the importance of collaborating, leading to a decrease in knowledge dissemination.", "conclusion": "The results highlight the interplay between technology design and social factors in promoting collaborative learning, suggesting that current software does not adequately support the long-term sharing of knowledge.", "key_contributions": ["Identified social norms affecting sharing behaviors in spreadsheet use.", "Highlighted challenges in adapting personalized sharing strategies and self-evaluation.", "Provided design implications to enhance collaborative learning in feature-rich software."], "limitations": "", "keywords": ["spreadsheet collaboration", "social norms", "knowledge sharing", "user engagement", "collaborative learning"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2506.09147", "pdf": "https://arxiv.org/pdf/2506.09147.pdf", "abs": "https://arxiv.org/abs/2506.09147", "title": "LLM-as-a-qualitative-judge: automating error analysis in natural language generation", "authors": ["Nadezhda Chirkova", "Tunde Oluwaseyi Ajayi", "Seth Aycock", "Zain Muhammad Mujahid", "Vladana Perlić", "Ekaterina Borisova", "Markarit Vartampetian"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Prompting large language models (LLMs) to evaluate generated text, known as\nLLM-as-a-judge, has become a standard evaluation approach in natural language\ngeneration (NLG), but is primarily used as a quantitative tool, i.e. with\nnumerical scores as main outputs. In this work, we propose\nLLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main\noutput being a structured report of common issue types in the NLG system\noutputs. Our approach is targeted at providing developers with meaningful\ninsights on what improvements can be done to a given NLG system and consists of\ntwo main steps, namely open-ended per-instance issue analysis and clustering of\nthe discovered issues using an intuitive cumulative algorithm. We also\nintroduce a strategy for evaluating the proposed approach, coupled with ~300\nannotations of issues in instances from 12 NLG datasets. Our results show that\nLLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3\ncases and is capable of producing error type reports resembling the reports\ncomposed by human annotators. Our code and data are publicly available at\nhttps://github.com/tunde-ajayi/llm-as-a-qualitative-judge.", "AI": {"tldr": "The paper proposes an LLM-based evaluation method for natural language generation that provides qualitative insights into generated texts rather than just quantitative scores.", "motivation": "To improve the evaluation of natural language generation systems by providing developers with structured qualitative reports on issues found in generated texts.", "method": "The proposed method involves two steps: open-ended issue analysis for each instance and clustering the identified issues using a cumulative algorithm, alongside a strategy for evaluating the approach.", "result": "The approach successfully identifies instance-specific issues in approximately two-thirds of cases and produces issue reports similar to those generated by human annotators.", "conclusion": "The developed method offers a new way for developers to gain meaningful insights into the performance of NLG systems, fostering improvement through qualitative feedback.", "key_contributions": ["Introduced LLM-as-a-qualitative-judge for NLG evaluation", "Developed an intuitive cumulative algorithm for issue clustering", "Provided an evaluation strategy with annotated instances from multiple datasets."], "limitations": "", "keywords": ["large language models", "natural language generation", "qualitative evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.09220", "pdf": "https://arxiv.org/pdf/2506.09220.pdf", "abs": "https://arxiv.org/abs/2506.09220", "title": "Beyond the Hype: Mapping Uncertainty and Gratification in AI Assistant Use", "authors": ["Karen Joy", "Tawfiq Ammari", "Alyssa Sheehan"], "categories": ["cs.HC"], "comment": null, "summary": "This paper examines the gap between the promises and real-world performance\nof emerging AI personal assistants. Drawing on interviews with early adopters\nof devices like Rabbit R1 and Humane AI Pin, as well as services like Ohai and\nDocus, we map user experiences through the lens of Uses and Gratifications and\nUncertainty Reduction Theory. We identify three core types of user uncertainty,\nfunctional, interactional, and social, and explore how each disrupts different\nuser gratifications. We show that while marketing hype fuels initial adoption,\nunmet expectations often result in frustration or abandonment. Our findings\nhighlight the importance of transparency, task-specific design, and user\ncontrol over contextual memory and personalization. We provide design and\npolicy recommendations, including user-facing explainability tools and calls\nfor regulatory benchmarks such as CI Bench, to guide ethical and interpretable\nAI integration. Our study offers actionable insights for creating more usable,\ntrustworthy, and socially aligned AI assistants.", "AI": {"tldr": "The paper analyzes user experiences with emerging AI personal assistants, focusing on unmet expectations and the types of user uncertainty they face.", "motivation": "To bridge the gap between the expectations set by marketing and the actual performance of AI personal assistants by understanding user experiences.", "method": "Conducted interviews with early adopters of various AI personal assistants and analyzed their experiences using Uses and Gratifications and Uncertainty Reduction Theory.", "result": "Identified three types of user uncertainty (functional, interactional, and social) that lead to unmet expectations and subsequently frustration or abandonment of AI assistants.", "conclusion": "Recommendations for improving AI personal assistants include enhancing transparency, user control, and design that aligns with user needs and expectations.", "key_contributions": ["Identified core types of user uncertainty in AI assistant usage.", "Proposed design and policy recommendations for ethical AI integration.", "Highlighted the importance of user control over contextual memory."], "limitations": "", "keywords": ["AI personal assistants", "user experience", "Uncertainty Reduction Theory"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.09175", "pdf": "https://arxiv.org/pdf/2506.09175.pdf", "abs": "https://arxiv.org/abs/2506.09175", "title": "PHRASED: Phrase Dictionary Biasing for Speech Translation", "authors": ["Peidong Wang", "Jian Xue", "Rui Zhao", "Junkun Chen", "Aswin Shanmugam Subramanian", "Jinyu Li"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Phrases are essential to understand the core concepts in conversations.\nHowever, due to their rare occurrence in training data, correct translation of\nphrases is challenging in speech translation tasks. In this paper, we propose a\nphrase dictionary biasing method to leverage pairs of phrases mapping from the\nsource language to the target language. We apply the phrase dictionary biasing\nmethod to two types of widely adopted models, a transducer-based streaming\nspeech translation model and a multimodal large language model. Experimental\nresults show that the phrase dictionary biasing method outperforms phrase list\nbiasing by 21% relatively for the streaming speech translation model. In\naddition, phrase dictionary biasing enables multimodal large language models to\nuse external phrase information, achieving 85% relative improvement in phrase\nrecall.", "AI": {"tldr": "The paper introduces a phrase dictionary biasing method that enhances speech translation by utilizing pairs of phrases from different languages, leading to significant improvements in translation performance.", "motivation": "Correct translation of phrases is challenging due to their infrequency in training data, which hampers speech translation tasks.", "method": "The authors propose a phrase dictionary biasing method applied to a transducer-based streaming speech translation model and a multimodal large language model.", "result": "The proposed method shows a 21% relative improvement over traditional phrase list biasing for the streaming speech translation model and achieves an 85% relative improvement in phrase recall for multimodal large language models.", "conclusion": "The phrase dictionary biasing method significantly improves translation quality in speech tasks by effectively utilizing phrase mapping.", "key_contributions": ["Introduction of a phrase dictionary biasing method for speech translation", "Demonstrated substantial improvements in both transducer-based models and multimodal LLMs", "Enhanced phrase recall utilizing external phrase information"], "limitations": "", "keywords": ["speech translation", "phrase dictionary", "multimodal large language models", "translation biasing", "phrase recall"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2506.09236", "pdf": "https://arxiv.org/pdf/2506.09236.pdf", "abs": "https://arxiv.org/abs/2506.09236", "title": "Augmented Reality User Interfaces for First Responders: A Scoping Literature Review", "authors": ["Erin Argo", "Tanim Ahmed", "Sarah Gable", "Callie Hampton", "Jeronimo Grandi", "Regis Kopper"], "categories": ["cs.HC"], "comment": "19 pages, 4 figures, 8 tables", "summary": "During the past decade, there has been a significant increase in research\nfocused on integrating AR User Interfaces into public safety applications,\nparticularly for first responders in the domains of Emergency Medical Services,\nFirefighting, and Law Enforcement. This paper presents the results of a scoping\nreview involving the application of AR user interfaces in the public safety\ndomain and applies an established systematic review methodology to provide a\ncomprehensive analysis of the current research landscape, identifying key\ntrends, challenges, and gaps in the literature. This review includes\npeer-reviewed publications indexed by the major scientific databases up to\nApril 2025. A basic keyword search retrieved 1,751 papers, of which 90 were\ndeemed relevant for this review. An in-depth analysis of the literature allowed\nthe development of a faceted taxonomy that categorizes AR user interfaces for\npublic safety. This classification lays a solid foundation for future research,\nwhile also highlighting key design considerations, challenges, and gaps in the\nliterature. This review serves as a valuable resource for researchers and\ndevelopers, offering insights that can drive further advances in the field.", "AI": {"tldr": "This paper reviews the application of AR user interfaces in public safety, analyzing current research and identifying trends, challenges, and gaps.", "motivation": "To explore the integration of AR user interfaces in public safety applications for first responders and to provide a comprehensive analysis of the current research landscape.", "method": "A scoping review using a systematic review methodology, analyzing 1,751 publications to identify 90 relevant works, followed by an in-depth literature analysis to create a taxonomy.", "result": "Development of a faceted taxonomy categorizing AR user interfaces for public safety and identification of design considerations, challenges, and literature gaps.", "conclusion": "The review serves as a resource for researchers and developers, offering insights to drive future advances in AR applications in public safety.", "key_contributions": ["Comprehensive taxonomy of AR user interfaces for public safety", "Identification of key challenges and gaps in the research", "Insights for future research directions"], "limitations": "Limited to publications indexed until April 2025, may not cover emerging trends post-review.", "keywords": ["Augmented Reality", "User Interfaces", "Public Safety", "Emergency Services", "First Responders"], "importance_score": 4, "read_time_minutes": 20}}
{"id": "2506.09218", "pdf": "https://arxiv.org/pdf/2506.09218.pdf", "abs": "https://arxiv.org/abs/2506.09218", "title": "A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs", "authors": ["Bruno Ferenc Šegedin"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "The ability of deep neural networks (DNNs) to represent phonotactic\ngeneralizations derived from lexical learning remains an open question. This\nstudy (1) investigates the lexically-invariant generalization capacity of\ngenerative convolutional neural networks (CNNs) trained on raw audio waveforms\nof lexical items and (2) explores the consequences of shrinking the\nfully-connected layer (FC) bottleneck from 1024 channels to 8 before training.\nUltimately, a novel technique for probing a model's lexically-independent\ngeneralizations is proposed that works only under the narrow FC bottleneck:\ngenerating audio outputs by bypassing the FC and inputting randomized feature\nmaps into the convolutional block. These outputs are equally biased by a\nphonotactic restriction in training as are outputs generated with the FC. This\nresult shows that the convolutional layers can dynamically generalize phonetic\ndependencies beyond lexically-constrained configurations learned by the FC.", "AI": {"tldr": "This study examines the ability of convolutional neural networks to generalize phonotactic rules from raw audio waveforms, particularly focusing on the impact of reducing the fully-connected layer size.", "motivation": "To investigate how deep neural networks can learn and generalize phonotactic structures while being trained on lexical items.", "method": "The paper employs generative convolutional neural networks trained on raw audio, testing the effects of a dramatically reduced fully-connected layer size on the models' generalization abilities.", "result": "The novel probing technique revealed that convolutional layers can achieve phonetic generalizations independent of the fully-connected layer constraints, showing similar biases in outputs whether processed through FC or not.", "conclusion": "The findings suggest that the architecture of neural networks can influence their capacity for phonotactic generalization, with implications for understanding lexical learning in DNNs.", "key_contributions": ["Proposed a novel technique for probing lexically-independent generalizations", "Investigated the impact of reducing fully-connected layer size on phonotactic generalization", "Demonstrated that convolutional layers can dynamically generalize phonetic dependencies."], "limitations": "", "keywords": ["deep neural networks", "phonotactic generalization", "convolutional neural networks", "lexical learning", "audio processing"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.09292", "pdf": "https://arxiv.org/pdf/2506.09292.pdf", "abs": "https://arxiv.org/abs/2506.09292", "title": "AI Tutors vs. Tenacious Myths: Evidence from Personalised Dialogue Interventions in Education", "authors": ["Brooklyn J. Corbett", "Jason M. Tangen"], "categories": ["cs.HC"], "comment": "Originally posted as https://doi.org/10.31234/osf.io/x4wqh_v1", "summary": "Misconceptions in psychology and education persist despite clear\ncontradictory evidence, resisting traditional correction methods. This study\ninvestigated whether personalised AI dialogue could effectively correct these\nstubborn beliefs. In a preregistered experiment (N = 375), participants holding\nstrong psychology misconceptions engaged in one of three interventions: (1)\npersonalised AI dialogue targeting their specific misconception, (2) generic\ntextbook-style refutation, or (3) neutral AI dialogue (control). Results showed\nthat personalised AI dialogue produced significantly larger immediate belief\nreductions compared to both textbook reading and neutral dialogue. This\nadvantage persisted at 10-day follow-up but diminished by 2 months, where AI\ndialogue and textbook conditions converged while both remained superior to\ncontrol. Both AI conditions generated significantly higher engagement and\nconfidence than textbook reading, demonstrating the motivational benefits of\nconversational interaction. These findings demonstrate that AI dialogue can\naccelerate initial belief correction through personalised, interactive\nengagement that disrupts the cognitive processes maintaining misconceptions.\nHowever, the convergence of effects over time suggests brief interventions\nrequire reinforcement for lasting change. Future applications should integrate\nAI tutoring into structured educational programs with spaced reinforcement to\nsustain the initial advantages of personalised dialogue.", "AI": {"tldr": "The study explores the effectiveness of personalised AI dialogue in correcting psychology misconceptions and finds it significantly reduces belief in false ideas compared to traditional methods.", "motivation": "Misconceptions in psychology and education persist despite evidence against them, creating a need for innovative correction methods.", "method": "A preregistered experiment with 375 participants tested three interventions: personalised AI dialogue, generic textbook refutation, and neutral AI dialogue.", "result": "Personalised AI dialogue significantly reduced misconceptions immediately and maintained engagement, but effects lessened at 2 months without reinforcement.", "conclusion": "AI dialogue can effectively correct misconceptions but requires ongoing intervention for lasting change; future methods should include spaced reinforcement in educational frameworks.", "key_contributions": ["Demonstrated the effectiveness of personalised AI dialogue in belief correction.", "Showed the motivational benefits of conversational interaction compared to traditional methods.", "Highlighted the need for reinforcement to sustain the effectiveness of interventions over time."], "limitations": "The effects of personalised dialogue diminished over time, indicating that additional support is necessary for long-lasting change.", "keywords": ["AI dialogue", "misconceptions", "educational intervention", "personalisation", "belief correction"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.09251", "pdf": "https://arxiv.org/pdf/2506.09251.pdf", "abs": "https://arxiv.org/abs/2506.09251", "title": "Extrapolation by Association: Length Generalization Transfer in Transformers", "authors": ["Ziyang Cai", "Nayoung Lee", "Avi Schwarzschild", "Samet Oymak", "Dimitris Papailiopoulos"], "categories": ["cs.CL", "cs.AI"], "comment": "23 pages, 20 figures", "summary": "Transformer language models have demonstrated impressive generalization\ncapabilities in natural language domains, yet we lack a fine-grained\nunderstanding of how such generalization arises. In this paper, we investigate\nlength generalization--the ability to extrapolate from shorter to longer\ninputs--through the lens of \\textit{task association}. We find that length\ngeneralization can be \\textit{transferred} across related tasks. That is,\ntraining a model with a longer and related auxiliary task can lead it to\ngeneralize to unseen and longer inputs from some other target task. We\ndemonstrate this length generalization transfer across diverse algorithmic\ntasks, including arithmetic operations, string transformations, and maze\nnavigation. Our results show that transformer models can inherit generalization\ncapabilities from similar tasks when trained jointly. Moreover, we observe\nsimilar transfer effects in pretrained language models, suggesting that\npretraining equips models with reusable computational scaffolding that\nfacilitates extrapolation in downstream settings. Finally, we provide initial\nmechanistic evidence that length generalization transfer correlates with the\nre-use of the same attention heads between the tasks. Together, our findings\ndeepen our understanding of how transformers generalize to out-of-distribution\ninputs and highlight the compositional reuse of inductive structure across\ntasks.", "AI": {"tldr": "This paper explores length generalization in transformer models, showing that models can transfer generalization capabilities from related auxiliary tasks to target tasks, enhancing their ability to extrapolate to longer inputs.", "motivation": "The study aims to understand the mechanisms behind generalization in transformer language models, specifically focusing on length generalization and its transferability across tasks.", "method": "The authors examined length generalization through tasks that involve arithmetic operations, string transformations, and maze navigation, assessing the transfer of generalization from auxiliary to target tasks.", "result": "The paper demonstrates that training models on longer auxiliary tasks enhances their performance on unseen longer inputs in related target tasks, with similar effects noted in pretrained models where attention heads are reused.", "conclusion": "The findings indicate that transformer models can leverage learned generalization from related tasks, emphasizing the importance of compositionality and inductive structure in task performance beyond training data.", "key_contributions": ["Investigates length generalization transfer across tasks", "Shows the influence of task association on model generalization", "Provides evidence linking attention head reuse to generalization capabilities"], "limitations": "", "keywords": ["Length Generalization", "Transformer Models", "Task Association", "Natural Language Processing", "Attention Mechanisms"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.09354", "pdf": "https://arxiv.org/pdf/2506.09354.pdf", "abs": "https://arxiv.org/abs/2506.09354", "title": "\"Is This Really a Human Peer Supporter?\": Misalignments Between Peer Supporters and Experts in LLM-Supported Interactions", "authors": ["Kellie Yu Hui Sim", "Roy Ka-Wei Lee", "Kenny Tsu Wei Choo"], "categories": ["cs.HC", "cs.AI", "H.5.0"], "comment": null, "summary": "Mental health is a growing global concern, prompting interest in AI-driven\nsolutions to expand access to psychosocial support. Peer support, grounded in\nlived experience, offers a valuable complement to professional care. However,\nvariability in training, effectiveness, and definitions raises concerns about\nquality, consistency, and safety. Large Language Models (LLMs) present new\nopportunities to enhance peer support interactions, particularly in real-time,\ntext-based interactions. We present and evaluate an AI-supported system with an\nLLM-simulated distressed client, context-sensitive LLM-generated suggestions,\nand real-time emotion visualisations. 2 mixed-methods studies with 12 peer\nsupporters and 5 mental health professionals (i.e., experts) examined the\nsystem's effectiveness and implications for practice. Both groups recognised\nits potential to enhance training and improve interaction quality. However, we\nfound a key tension emerged: while peer supporters engaged meaningfully,\nexperts consistently flagged critical issues in peer supporter responses, such\nas missed distress cues and premature advice-giving. This misalignment\nhighlights potential limitations in current peer support training, especially\nin emotionally charged contexts where safety and fidelity to best practices are\nessential. Our findings underscore the need for standardised, psychologically\ngrounded training, especially as peer support scales globally. They also\ndemonstrate how LLM-supported systems can scaffold this development--if\ndesigned with care and guided by expert oversight. This work contributes to\nemerging conversations on responsible AI integration in mental health and the\nevolving role of LLMs in augmenting peer-delivered care.", "AI": {"tldr": "This paper presents an AI-supported system that enhances peer support interactions for mental health through LLMs, evaluates its effectiveness, and emphasizes the need for better training standards in peer support.", "motivation": "Mental health is a critical global issue, and AI-driven solutions can improve access to psychosocial support. Peer support offers valuable assistance but faces quality and safety challenges.", "method": "Two mixed-methods studies were conducted involving 12 peer supporters and 5 mental health professionals to evaluate the AI system's effectiveness and implications for practice.", "result": "Both peer supporters and mental health experts recognized the system's potential to enhance training and improve interaction quality, though experts noted critical issues in peer responses.", "conclusion": "The findings indicate a need for standardized training in peer support that aligns with best practices, highlighting LLMs' role in supporting this development under expert guidance.", "key_contributions": ["Development of an AI-supported peer support system leveraging LLMs", "Insights from evaluating the system's effectiveness in real-time interactions", "Identification of training gaps in current peer support practices"], "limitations": "Misalignment in responses between peer supporters and mental health experts, especially concerning distress cues and advice-giving.", "keywords": ["AI in mental health", "peer support", "Large Language Models", "psychosocial support", "training standards"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.09259", "pdf": "https://arxiv.org/pdf/2506.09259.pdf", "abs": "https://arxiv.org/abs/2506.09259", "title": "Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat", "authors": ["Zhuofang Li", "Rafal Kocielnik", "Fereshteh Soltani", "Penphob", "Boonyarungsrit", "Animashree Anandkumar", "R. Michael Alvarez"], "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; K.4"], "comment": null, "summary": "Millions of players engage daily in competitive online games, communicating\nthrough in-game chat. Prior research has focused on detecting relatively small\nvolumes of toxic content using various Natural Language Processing (NLP)\ntechniques for the purpose of moderation. However, recent studies emphasize the\nimportance of detecting prosocial communication, which can be as crucial as\nidentifying toxic interactions. Recognizing prosocial behavior allows for its\nanalysis, rewarding, and promotion. Unlike toxicity, there are limited\ndatasets, models, and resources for identifying prosocial behaviors in\ngame-chat text. In this work, we employed unsupervised discovery combined with\ngame domain expert collaboration to identify and categorize prosocial player\nbehaviors from game chat. We further propose a novel Self-Anchored Attention\nModel (SAAM) which gives 7.9% improvement compared to the best existing\ntechnique. The approach utilizes the entire training set as \"anchors\" to help\nimprove model performance under the scarcity of training data. This approach\nled to the development of the first automated system for classifying prosocial\nbehaviors in in-game chats, particularly given the low-resource settings where\nlarge-scale labeled data is not available. Our methodology was applied to one\nof the most popular online gaming titles - Call of Duty(R): Modern\nWarfare(R)II, showcasing its effectiveness. This research is novel in applying\nNLP techniques to discover and classify prosocial behaviors in player in-game\nchat communication. It can help shift the focus of moderation from solely\npenalizing toxicity to actively encouraging positive interactions on online\nplatforms.", "AI": {"tldr": "This research proposes a novel approach using unsupervised discovery and a Self-Anchored Attention Model (SAAM) to classify prosocial behaviors in online game chat, improving model performance in low-resource settings.", "motivation": "To address the lack of resources and focus on identifying prosocial behaviors in online gaming communications, which is as important as detecting toxicity.", "method": "The study employs unsupervised discovery methods along with expert collaboration to identify and categorize prosocial behaviors from game chat, leveraging the Self-Anchored Attention Model for improved classification.", "result": "The proposed SAAM achieved a 7.9% improvement in performance compared to existing methods and established the first automated system for classifying prosocial interactions in game-chat text.", "conclusion": "The research highlights the feasibility of identifying prosocial communication in online gaming, suggesting a shift in moderation focus towards encouraging positive interactions.", "key_contributions": ["Development of the first automated system for identifying prosocial behaviors in game chat", "Introduction of the Self-Anchored Attention Model (SAAM)", "Demonstration of effective classification in low-resource settings"], "limitations": "", "keywords": ["prosocial behavior", "Natural Language Processing", "game chat", "Self-Anchored Attention Model", "toxicity detection"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.09362", "pdf": "https://arxiv.org/pdf/2506.09362.pdf", "abs": "https://arxiv.org/abs/2506.09362", "title": "\"I Said Things I Needed to Hear Myself\": Peer Support as an Emotional, Organisational, and Sociotechnical Practice in Singapore", "authors": ["Kellie Yu Hui Sim", "Kenny Tsu Wei Choo"], "categories": ["cs.HC", "cs.AI", "H.5.0"], "comment": null, "summary": "Peer support plays a vital role in expanding access to mental health care by\nproviding empathetic, community-based support outside formal clinical systems.\nAs digital platforms increasingly mediate such support, the design and impact\nof these technologies remain under-examined, particularly in Asian contexts.\nThis paper presents findings from an interview study with 20 peer supporters in\nSingapore, who operate across diverse online, offline, and hybrid environments.\nThrough a thematic analysis, we unpack how participants start, conduct, and\nsustain peer support, highlighting their motivations, emotional labour, and the\nsociocultural dimensions shaping their practices. Building on this grounded\nunderstanding, we surface design directions for culturally responsive digital\ntools that scaffold rather than supplant relational care. Drawing insights from\nqualitative accounts, we offer a situated perspective on how AI might\nresponsibly augment peer support. This research contributes to human-centred\ncomputing by articulating the lived realities of peer supporters and proposing\ndesign implications for trustworthy and context-sensitive AI in mental health.", "AI": {"tldr": "This paper explores digital peer support for mental health in Singapore, highlighting the practices and needs of peer supporters and proposing design implications for AI-enhanced tools.", "motivation": "To explore the role of digital platforms in peer support for mental health and provide design implications for culturally responsive technologies in Asian contexts.", "method": "An interview study with 20 peer supporters in Singapore, utilizing thematic analysis to understand their practices and motivations.", "result": "The study reveals the emotional labor and sociocultural factors affecting peer support practices, and offers design directions for AI tools that complement relational care.", "conclusion": "The research underscores the importance of culturally responsive design in digital mental health supports, emphasizing trust and context sensitivity in AI applications.", "key_contributions": ["Insights into the lived realities of peer supporters in Singapore.", "Proposed design directions for AI-enhanced peer support tools.", "Thematic analysis of peer support practices and motivations."], "limitations": "", "keywords": ["peer support", "mental health", "digital platforms", "AI", "cultural responsiveness"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.09277", "pdf": "https://arxiv.org/pdf/2506.09277.pdf", "abs": "https://arxiv.org/abs/2506.09277", "title": "Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models", "authors": ["Milan Bhan", "Jean-Noel Vittaut", "Nicolas Chesneau", "Sarath Chandar", "Marie-Jeanne Lesot"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLM) have demonstrated the capability of generating\nfree text self Natural Language Explanation (self-NLE) to justify their\nanswers. Despite their logical appearance, self-NLE do not necessarily reflect\nthe LLM actual decision-making process, making such explanations unfaithful.\nWhile existing methods for measuring self-NLE faithfulness mostly rely on\nbehavioral tests or computational block identification, none of them examines\nthe neural activity underlying the model's reasoning. This work introduces a\nnovel flexible framework for quantitatively measuring the faithfulness of\nLLM-generated self-NLE by directly comparing the latter with interpretations of\nthe model's internal hidden states. The proposed framework is versatile and\nprovides deep insights into self-NLE faithfulness by establishing a direct\nconnection between self-NLE and model reasoning. This approach advances the\nunderstanding of self-NLE faithfulness and provides building blocks for\ngenerating more faithful self-NLE.", "AI": {"tldr": "This paper introduces a framework to measure the faithfulness of Large Language Model-generated explanations by comparing them with the model's internal reasoning processes.", "motivation": "To address the unfaithfulness of self-Natural Language Explanations generated by LLMs, which do not reliably reflect their decision-making processes.", "method": "The paper presents a novel flexible framework that quantitatively measures the faithfulness of self-NLE by comparing it with interpretations of the model's internal hidden states.", "result": "The framework provides insights into self-NLE faithfulness and establishes a connection between generated explanations and model reasoning.", "conclusion": "This work enhances the understanding of self-NLE faithfulness and offers foundational elements for more faithful explanation generation.", "key_contributions": ["Introduction of a novel framework for measuring self-NLE faithfulness", "Direct comparison between self-NLE and neural interpretations", "Insights into the model's reasoning process"], "limitations": "", "keywords": ["Large Language Models", "Natural Language Explanation", "faithfulness", "neural activity", "explanation generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.09696", "pdf": "https://arxiv.org/pdf/2506.09696.pdf", "abs": "https://arxiv.org/abs/2506.09696", "title": "Patterns of Patterns III", "authors": ["Joseph Corneli", "Charles J. Danoff", "Raymond S. Puzio", "Sridevi Ayloo", "Serge Belich", "Mary Tedeschi"], "categories": ["cs.HC"], "comment": "18 pages; submitted to Pattern Languages of Programs 2025", "summary": "Building on earlier installments, this paper re-examines the PLACARD pattern.\nWe report on a series of workshops where PLACARD was used to scaffold\ncollaborative reflection, speculative inquiry, and stimulate design pattern\ngeneration. These accounts are enriched by a comparison case: virtual workshops\ncarried out with simple AI-based chatbots. We discuss limitations and lessons\nlearned from both the human and multi-agent settings. We conclude by outlining\na future development strategy at the intersection of AI agents, design\npatterns, and institutional governance.", "AI": {"tldr": "This paper re-examines the PLACARD pattern through workshops and AI chatbots to enhance collaborative design and reflection.", "motivation": "To explore effective methods for supporting collaborative design and reflection using the PLACARD pattern and AI agents.", "method": "The research involved organizing workshops to utilize the PLACARD pattern and compare outcomes with those from virtual workshops featuring AI-based chatbots.", "result": "Findings reveal insights into collaborative reflection and design pattern generation, highlighting the effectiveness of PLACARD and the potential of AI chatbots.", "conclusion": "A future development strategy is proposed that merges AI agents with design patterns and governance.", "key_contributions": ["Re-evaluates the PLACARD pattern in collaborative design contexts.", "Compares human-facilitated workshops with AI chatbot-led sessions.", "Identifies limitations and offers insights for future AI and design integration."], "limitations": "The study primarily reflects on workshops and may not encompass broader applications beyond the specific settings.", "keywords": ["PLACARD pattern", "collaborative design", "AI agents", "institutional governance"], "importance_score": 6, "read_time_minutes": 18}}
{"id": "2506.09301", "pdf": "https://arxiv.org/pdf/2506.09301.pdf", "abs": "https://arxiv.org/abs/2506.09301", "title": "$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding", "authors": ["Cesare Spinoso-Di Piano", "David Austin", "Pablo Piantanida", "Jackie Chi Kit Cheung"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in\nhuman communication, resulting in utterances where the literal and the intended\nmeanings do not match. The Rational Speech Act (RSA) framework, which\nexplicitly models speaker intentions, is the most widespread theory of\nprobabilistic pragmatics, but existing implementations are either unable to\naccount for figurative expressions or require modeling the implicit motivations\nfor using figurative language (e.g., to express joy or annoyance) in a\nsetting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware\nRSA $(RSA)^2$ framework which models figurative language use by considering a\nspeaker's employed rhetorical strategy. We show that $(RSA)^2$ enables\nhuman-compatible interpretations of non-literal utterances without modeling a\nspeaker's motivations for being non-literal. Combined with LLMs, it achieves\nstate-of-the-art performance on the ironic split of PragMega+, a new irony\ninterpretation dataset introduced in this study.", "AI": {"tldr": "This paper introduces the Rhetorical-Strategy-Aware Rational Speech Act framework to enhance understanding of figurative language by modeling speakers' rhetorical strategies without relying on their implicit motivations.", "motivation": "To improve how figurative language, such as irony, is interpreted in communication, leveraging a probabilistic framework that is compatible with human understanding.", "method": "Introducing the Rhetorical-Strategy-Aware RSA framework, denoted as $(RSA)^2$, which focuses on the rhetorical strategies used by speakers rather than their implicit motivations.", "result": "$(RSA)^2$ achieves state-of-the-art results on the PragMega+ dataset specifically for irony interpretation when combined with large language models.", "conclusion": "The proposed framework allows for more accurate and human-compatible interpretations of figurative language while simplifying the modeling process by not needing to understand the speaker's motivations.", "key_contributions": ["Development of the $(RSA)^2$ framework for interpreting figurative language.", "Demonstrated success on the PragMega+ dataset for irony interpretation.", "Compatibility with large language models for enhanced performance."], "limitations": "", "keywords": ["figurative language", "rhetorical strategies", "Rational Speech Act", "irony interpretation", "large language models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.09801", "pdf": "https://arxiv.org/pdf/2506.09801.pdf", "abs": "https://arxiv.org/abs/2506.09801", "title": "Investigating the Perception of Translational Shape-Changing Haptic Interfaces", "authors": ["Qihan Yang", "Xin Zhou", "Adam J. Spiers"], "categories": ["cs.HC"], "comment": "7 pages, 8 figures. Accepted version to appear in: Proceedings of the\n  IEEE World Haptics Conference (WHC), 2025", "summary": "Shape-changing haptic interfaces (SCHIs) are a promising and emerging field.\nHowever, compared to more established stimulus modalities, such as vibration,\nthere is sparse literature on the perception of dynamic shapes. Furthermore,\nthe influence of properties such as grasp types and displacement\nmagnitude/direction has not been formally evaluated. This work attempts to\ninitiate a formal perceptual evaluation of SCHIs via a psychophysical user\nstudy involving a 1-DOF translational shape-changing interface that can move\nits body with 1.25-micrometer resolution. Participants completed a Method of\nConstant Stimulus study while holding the device with three different grasps.\nStimuli direction occurred both toward and away from the thumb, while the\nstandard stimuli varied between small (0.48 mm) and large (6 mm). Our results\nindicate that translational SCHIs should maximize the translation magnitude\nrather than the number of fingers in contact. We also demonstrated how to apply\nour findings to real-world applications via a simple 'paddle game', where we\ncompared conventional linear mapping with non-linear mapping derived from our\nperceptual experiment outcomes between the device position and its represented\nvalue. Results indicate that the non-linear mapping was more effective, with\nimproved error distribution. We hope this work inspires further formal\nperceptual investigation into other SCHI morphologies.", "AI": {"tldr": "This paper investigates the perception of shape-changing haptic interfaces (SCHIs) through a psychophysical user study, evaluating the effects of grasp types and displacement on user experience.", "motivation": "To formally evaluate the perception of dynamic shapes in SCHIs, an area with limited existing literature compared to established modalities like vibration.", "method": "A psychophysical user study with a 1-DOF translational shape-changing interface, where participants completed a Method of Constant Stimulus study while using three different grasps.", "result": "Results suggest maximizing translation magnitude over the number of fingers in contact. The non-linear mapping derived from the study improved user experience in a practical application compared to conventional linear mapping.", "conclusion": "The study's findings can inform the design of more effective shape-changing haptic interfaces and encourage further research into other SCHI morphologies.", "key_contributions": ["Initiation of formal evaluation on dynamic shape perception in SCHIs", "Comparison of grasp types and displacement effects on user perception", "Demonstration of improved user interaction through non-linear mapping in applications"], "limitations": "", "keywords": ["shape-changing haptic interfaces", "perception evaluation", "psychophysical study", "human-computer interaction"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.09315", "pdf": "https://arxiv.org/pdf/2506.09315.pdf", "abs": "https://arxiv.org/abs/2506.09315", "title": "Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models", "authors": ["Yao Xiao", "Heidi Christensen", "Stefan Goetze"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "To be published in the proceedings of Interspeech 2025", "summary": "Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive\ndecline that commonly impacts language ability. This work extends the paired\nperplexity approach to detecting AD by using a recent large language model\n(LLM), the instruction-following version of Mistral-7B. We improve accuracy by\nan average of 3.33% over the best current paired perplexity method and by 6.35%\nover the top-ranked method from the ADReSS 2020 challenge benchmark. Our\nfurther analysis demonstrates that the proposed approach can effectively detect\nAD with a clear and interpretable decision boundary in contrast to other\nmethods that suffer from opaque decision-making processes. Finally, by\nprompting the fine-tuned LLMs and comparing the model-generated responses to\nhuman responses, we illustrate that the LLMs have learned the special language\npatterns of AD speakers, which opens up possibilities for novel methods of\nmodel interpretation and data augmentation.", "AI": {"tldr": "This paper improves Alzheimer's detection using a fine-tuned LLM, Mistral-7B, achieving better accuracy and interpretability than existing methods.", "motivation": "The study aims to enhance detection methods for Alzheimer's dementia by leveraging large language models to analyze language patterns in patients.", "method": "The authors utilize the instruction-following version of the Mistral-7B LLM to extend the paired perplexity approach for AD detection.", "result": "The proposed method shows improvements in accuracy by 3.33% over current paired perplexity approaches and 6.35% over top-ranked methods from the ADReSS 2020 challenge.", "conclusion": "The findings suggest that the fine-tuned LLM not only enhances AD detection accuracy but also offers interpretability, which can inform future techniques in model transparency and data augmentation.", "key_contributions": ["Improved accuracy in AD detection using Mistral-7B LLM", "Clear and interpretable decision boundary for language analysis", "Demonstration of LLM understanding specific language patterns of AD speakers"], "limitations": "", "keywords": ["Alzheimer's dementia", "large language models", "paired perplexity", "language analysis", "model interpretability"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.09968", "pdf": "https://arxiv.org/pdf/2506.09968.pdf", "abs": "https://arxiv.org/abs/2506.09968", "title": "SRLAgent: Enhancing Self-Regulated Learning Skills through Gamification and LLM Assistance", "authors": ["Wentao Ge", "Yuqing Sun", "Ziyan Wang", "Haoyue Zheng", "Weiyang He", "Piaohong Wang", "Qianyu Zhu", "Benyou Wang"], "categories": ["cs.HC", "I.2.1; I.2.6"], "comment": "14 pages", "summary": "Self-regulated learning (SRL) is crucial for college students navigating\nincreased academic demands and independence. Insufficient SRL skills can lead\nto disorganized study habits, low motivation, and poor time management,\nundermining learners ability to thrive in challenging environments. Through a\nformative study involving 59 college students, we identified key challenges\nstudents face in developing SRL skills, including difficulties with\ngoal-setting, time management, and reflective learning. To address these\nchallenges, we introduce SRLAgent, an LLM-assisted system that fosters SRL\nskills through gamification and adaptive support from large language models\n(LLMs). Grounded in Zimmermans three-phase SRL framework, SRLAgent enables\nstudents to engage in goal-setting, strategy execution, and self-reflection\nwithin an interactive game-based environment. The system offers real-time\nfeedback and scaffolding powered by LLMs to support students independent study\nefforts. We evaluated SRLAgent using a between-subjects design, comparing it to\na baseline system (SRL without Agent features) and a traditional multimedia\nlearning condition. Results showed significant improvements in SRL skills\nwithin the SRLAgent group (p < .001, Cohens d = 0.234) and higher engagement\ncompared to the baselines. This work highlights the value of embedding SRL\nscaffolding and real-time AI support within gamified environments, offering\ndesign implications for educational technologies that aim to promote deeper\nlearning and metacognitive skill development.", "AI": {"tldr": "This paper presents SRLAgent, an LLM-assisted system designed to enhance self-regulated learning (SRL) skills in college students through gamification and real-time feedback.", "motivation": "Address the challenges faced by college students in developing self-regulated learning skills, which are essential for academic success.", "method": "A formative study involving 59 college students was conducted to identify challenges in SRL, leading to the development and evaluation of SRLAgent against baseline systems.", "result": "Significant improvements in SRL skills and higher engagement were observed in students using SRLAgent compared to traditional approaches.", "conclusion": "Integrating LLMs into gamified learning environments can effectively support the development of SRL skills and provide valuable design implications for educational technologies.", "key_contributions": ["Development of SRLAgent, an innovative LLM-assisted learning tool", "Demonstrated significant improvements in SRL capabilities among users", "Provided insights into gamification and AI support in educational contexts"], "limitations": "", "keywords": ["self-regulated learning", "LLM-assisted", "gamification", "educational technology", "metacognitive skills"], "importance_score": 7, "read_time_minutes": 14}}
{"id": "2506.09329", "pdf": "https://arxiv.org/pdf/2506.09329.pdf", "abs": "https://arxiv.org/abs/2506.09329", "title": "Towards Efficient and Effective Alignment of Large Language Models", "authors": ["Yuxin Jiang"], "categories": ["cs.CL"], "comment": "PhD thesis", "summary": "Large language models (LLMs) exhibit remarkable capabilities across diverse\ntasks, yet aligning them efficiently and effectively with human expectations\nremains a critical challenge. This thesis advances LLM alignment by introducing\nnovel methodologies in data collection, training, and evaluation. We first\naddress alignment data collection. Existing approaches rely heavily on manually\ncurated datasets or proprietary models. To overcome these limitations, we\npropose Lion, an adversarial distillation framework that iteratively refines\ntraining data by identifying and generating challenging instructions, enabling\nstate-of-the-art zero-shot reasoning. Additionally, we introduce Web\nReconstruction (WebR), a fully automated framework that synthesizes\ninstruction-tuning data directly from raw web documents, significantly\nimproving data diversity and scalability over existing synthetic data methods.\nNext, we enhance alignment training through novel optimization techniques. We\ndevelop Learning to Edit (LTE), a framework that enables LLMs to efficiently\nintegrate new knowledge while preserving existing information. LTE leverages\nmeta-learning to improve both real-time and batch knowledge updates.\nFurthermore, we introduce Bridging and Modeling Correlations (BMC), a\nrefinement of Direct Preference Optimization (DPO) that explicitly captures\ntoken-level correlations in preference data, leading to superior alignment\nacross QA and mathematical reasoning tasks. Finally, we tackle the challenge of\nevaluating alignment. Existing benchmarks emphasize response quality but\noverlook adherence to specific constraints. To bridge this gap, we introduce\nFollowBench, a multi-level, fine-grained benchmark assessing LLMs' ability to\nfollow complex constraints across diverse instruction types. Our results expose\nkey weaknesses in current models' constraint adherence, offering insights for\nfuture improvements.", "AI": {"tldr": "This thesis focuses on advancing alignment methodologies for large language models via novel data collection, training, and evaluation techniques.", "motivation": "Aligning large language models (LLMs) with human expectations remains a critical challenge, necessitating improved methodologies for data collection, training, and evaluation.", "method": "The thesis presents Lion, an adversarial distillation framework for refining training data, Web Reconstruction for automated instruction-tuning data synthesis, Learning to Edit for knowledge integration, and Bridging and Modeling Correlations for enhancing preference data alignment, culminating in FollowBench for evaluating constraint adherence.", "result": "The methods proposed significantly improve data diversity, scalability, and alignment of LLMs in reasoning tasks, exposing weaknesses in current evaluation benchmarks.", "conclusion": "The advancements made in LLM alignment can guide future improvements and better model evaluations in line with human expectations.", "key_contributions": ["Introduction of Lion for dynamic training data refinement", "Development of Learning to Edit for knowledge integration", "Creation of FollowBench for assessing adherence to complex constraints"], "limitations": "", "keywords": ["Large language models", "LLM alignment", "Data collection", "Training methodologies", "Evaluation frameworks"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.09331", "pdf": "https://arxiv.org/pdf/2506.09331.pdf", "abs": "https://arxiv.org/abs/2506.09331", "title": "Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation", "authors": ["Arjun Vaithilingam Sudhakar"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "arXiv admin note: substantial text overlap with arXiv:2311.07687", "summary": "Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot\ngeneralization capabilities across complex natural language tasks, enabling\ntheir widespread use as virtual assistants for diverse applications such as\ntranslation and summarization. Despite being trained solely on large corpora of\ntext without explicit supervision on author intent, LLMs appear to infer the\nunderlying meaning of textual interactions. This raises a fundamental question:\ncan LLMs model and reason about the intentions of others, i.e., do they possess\na form of theory of mind? Understanding other's intentions is crucial for\neffective collaboration, which underpins human societal success and is\nessential for cooperative interactions among multiple agents, including humans\nand autonomous systems. In this work, we investigate the theory of mind in LLMs\nthrough the lens of cooperative multi-agent reinforcement learning (MARL),\nwhere agents learn to collaborate via repeated interactions, mirroring human\nsocial reasoning. Our approach aims to enhance artificial agent's ability to\nadapt and cooperate with both artificial and human partners. By leveraging\nLLM-based agents capable of natural language interaction, we move towards\ncreating hybrid human-AI systems that can foster seamless collaboration, with\nbroad implications for the future of human-artificial interaction.", "AI": {"tldr": "This paper investigates if Large Language Models (LLMs) can model and reason about the intentions of others, akin to a theory of mind, through cooperative multi-agent reinforcement learning, aiming to enhance collaboration between artificial agents and humans.", "motivation": "To explore if LLMs possess a form of theory of mind, which is crucial for effective collaboration in human-AI interaction.", "method": "The study utilizes cooperative multi-agent reinforcement learning (MARL) where agents learn to collaborate through repeated interactions, simulating human social reasoning.", "result": "The approach seeks to improve artificial agents' adaptability and cooperation with both human and artificial partners, leveraging LLM-based agents for natural language interaction.", "conclusion": "The research aims to create hybrid human-AI systems that facilitate seamless collaboration, impacting the future of human-artificial interaction.", "key_contributions": ["Investigation of theory of mind in LLMs", "Framework for cooperative multi-agent reinforcement learning", "Enhancement of human-AI collaboration capabilities"], "limitations": "", "keywords": ["Large Language Models", "Theory of Mind", "Multi-Agent Reinforcement Learning", "Human-AI Collaboration", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.09340", "pdf": "https://arxiv.org/pdf/2506.09340.pdf", "abs": "https://arxiv.org/abs/2506.09340", "title": "RePO: Replay-Enhanced Policy Optimization", "authors": ["Siheng Li", "Zhanhui Zhou", "Wai Lam", "Chao Yang", "Chaochao Lu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project Page: https://github.com/SihengLi99/RePO", "summary": "Reinforcement learning (RL) is vital for optimizing large language models\n(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages\nusing multiple on-policy outputs per prompt, leading to high computational\ncosts and low data efficiency. To address this, we introduce Replay-Enhanced\nPolicy Optimization (RePO), which leverages diverse replay strategies to\nretrieve off-policy samples from a replay buffer, allowing policy optimization\nbased on a broader and more diverse set of samples for each prompt. Experiments\non five LLMs across seven mathematical reasoning benchmarks demonstrate that\nRePO achieves absolute average performance gains of $18.4$ and $4.1$ points for\nQwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further\nanalysis indicates that RePO increases computational cost by $15\\%$ while\nraising the number of effective optimization steps by $48\\%$ for Qwen3-1.7B,\nwith both on-policy and off-policy sample numbers set to $8$. The repository\ncan be accessed at https://github.com/SihengLi99/RePO.", "AI": {"tldr": "Introducing Replay-Enhanced Policy Optimization (RePO) to improve efficiency in training large language models by utilizing diverse replay strategies for better policy optimization.", "motivation": "To enhance data efficiency and reduce computational costs in the reinforcement learning (RL) optimization of large language models (LLMs).", "method": "RePO employs diverse replay strategies to retrieve off-policy samples from a replay buffer, optimizing policy based on a larger variety of samples.", "result": "RePO shows significant performance improvements of 18.4 and 4.1 points over the previous Group Relative Policy Optimization (GRPO) method on different LLMs, while increasing effective optimization steps by 48%.", "conclusion": "RePO optimizes training in LLMs by balancing computational costs and data efficiency, demonstrating better results across various benchmarks.", "key_contributions": ["Introduces Replay-Enhanced Policy Optimization (RePO)", "Achieves significant performance gains over existing methods", "Enhances policy optimization utilizing off-policy samples"], "limitations": "", "keywords": ["Reinforcement Learning", "Large Language Models", "Policy Optimization"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.09342", "pdf": "https://arxiv.org/pdf/2506.09342.pdf", "abs": "https://arxiv.org/abs/2506.09342", "title": "Latent Multi-Head Attention for Small Language Models", "authors": ["Sushant Mehta", "Raj Dandekar", "Rajat Dandekar", "Sreedath Panat"], "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 1 figure. 5 tables", "summary": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance.", "AI": {"tldr": "This paper study examines latent multi-head attention (MLA) in small language models, demonstrating significant efficiency improvements in terms of memory usage and inference speed with minimal loss in quality.", "motivation": "To explore the efficiency-quality trade-offs of latent multi-head attention in small language models, aiming for improved performance in memory-constrained environments.", "method": "The authors trained 30M-parameter GPT models on 100,000 synthetic stories and compared three architectural variants: standard multi-head attention, MLA, and MLA with rotary positional embeddings (MLA+RoPE).", "result": "MLA+RoPE with half-rank latent dimensions achieves a 45% reduction in KV-cache memory with only a 0.3% increase in validation loss, showcasing a Pareto improvement. It offers a 1.4 times speedup in inference while maintaining memory efficiency.", "conclusion": "The study concludes that MLA+RoPE is superior for small models, with RoPE being crucial for enhancing its performance compared to standard attention.", "key_contributions": ["First comprehensive study on latent multi-head attention for small language models.", "Demonstrated significant memory savings and speed improvements with MLA+RoPE.", "Insights on the importance of rotary positional embeddings in achieving better model performance."], "limitations": "", "keywords": ["latent multi-head attention", "small language models", "rotary positional embeddings", "efficiency quality trade-offs", "GPT models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.09349", "pdf": "https://arxiv.org/pdf/2506.09349.pdf", "abs": "https://arxiv.org/abs/2506.09349", "title": "OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution Speech Representations and Contrastive Alignment", "authors": ["Chao-Hong Tan", "Qian Chen", "Wen Wang", "Chong Deng", "Qinglin Zhang", "Luyao Cheng", "Hai Yu", "Xin Zhang", "Xiang Lv", "Tianyu Zhao", "Chong Zhang", "Yukun Ma", "Yafeng Chen", "Hui Wang", "Jiaqing Liu", "Jieping Ye"], "categories": ["cs.CL"], "comment": null, "summary": "Recent studies on end-to-end speech generation with large language models\n(LLMs) have attracted significant community attention, with multiple works\nextending text-based LLMs to generate discrete speech tokens. Existing\napproaches primarily fall into two categories: (1) Methods that generate\ndiscrete speech tokens independently without incorporating them into the LLM's\nautoregressive process, resulting in text generation being unaware of\nconcurrent speech synthesis. (2) Models that generate interleaved or parallel\nspeech-text tokens through joint autoregressive modeling, enabling mutual\nmodality awareness during generation. This paper presents OmniDRCA, a parallel\nspeech-text foundation model based on joint autoregressive modeling, featuring\ndual-resolution speech representations and contrastive cross-modal alignment.\nOur approach processes speech and text representations in parallel while\nenhancing audio comprehension through contrastive alignment. Experimental\nresults on Spoken Question Answering benchmarks demonstrate that OmniDRCA\nestablishes new state-of-the-art (SOTA) performance among parallel joint\nspeech-text modeling based foundation models, and achieves competitive\nperformance compared to interleaved models. Additionally, we explore the\npotential of extending the framework to full-duplex conversational scenarios.", "AI": {"tldr": "The paper introduces OmniDRCA, a parallel speech-text foundation model that enhances audio comprehension and sets new state-of-the-art performance in speech-text modeling.", "motivation": "With growing interest in end-to-end speech generation using large language models, there is a need for models that effectively integrate speech and text modalities.", "method": "OmniDRCA uses joint autoregressive modeling to generate discrete speech tokens in parallel with text tokens, incorporating dual-resolution speech representations and contrastive cross-modal alignment for better comprehension.", "result": "OmniDRCA achieves state-of-the-art performance on Spoken Question Answering benchmarks and demonstrates competitive results compared to existing interleaved models.", "conclusion": "The results indicate that OmniDRCA can effectively model speech-text interactions and has implications for full-duplex conversational applications.", "key_contributions": ["Introduction of OmniDRCA model for parallel speech-text generation", "Achieving SOTA performance on benchmarks for speech-text modeling", "Methodology integrating dual-resolution speech representations and contrastive alignment"], "limitations": "", "keywords": ["speech generation", "large language models", "joint autoregressive modeling", "parallel processing", "speech-text modeling"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2506.09351", "pdf": "https://arxiv.org/pdf/2506.09351.pdf", "abs": "https://arxiv.org/abs/2506.09351", "title": "DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts", "authors": ["Yuchen Feng", "Bowen Shen", "Naibin Gu", "Jiaxuan Zhao", "Peng Fu", "Zheng Lin", "Weiping Wang"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture\nachieve high cost-efficiency by selectively activating a subset of the\nparameters. Despite the inference efficiency of MoE LLMs, the training of\nextensive experts from scratch incurs substantial overhead, whereas\nreconstructing a dense LLM into an MoE LLM significantly reduces the training\nbudget. However, existing reconstruction methods often overlook the diversity\namong experts, leading to potential redundancy. In this paper, we come up with\nthe observation that a specific LLM exhibits notable diversity after being\npruned on different calibration datasets, based on which we present a\nDiversity-Enhanced reconstruction method named DIVE. The recipe of DIVE\nincludes domain affinity mining, pruning-based expert reconstruction, and\nefficient retraining. Specifically, the reconstruction includes pruning and\nreassembly of the feed-forward network (FFN) module. After reconstruction, we\nefficiently retrain the model on routers, experts and normalization modules. We\nimplement DIVE on Llama-style LLMs with open-source training corpora.\nExperiments show that DIVE achieves training efficiency with minimal accuracy\ntrade-offs, outperforming existing pruning and MoE reconstruction methods with\nthe same number of activated parameters.", "AI": {"tldr": "This paper introduces DIVE, a Diversity-Enhanced reconstruction method for Mixture-of-Experts LLMs, which improves training efficiency and model diversity without significant accuracy loss.", "motivation": "Existing reconstruction methods for transforming dense LLMs into MoE architectures overlook the diversity of experts, which can lead to redundancy and inefficiency.", "method": "DIVE employs domain affinity mining, pruning-based expert reconstruction, and efficient retraining of routers, experts, and normalization modules in Llama-style LLMs.", "result": "DIVE achieves improved training efficiency with minimal accuracy trade-offs, outperforming existing approaches for the same number of activated parameters.", "conclusion": "The DIVE method enhances the diversity among experts in MoE LLMs and allows for more efficient training processes.", "key_contributions": ["Introduces DIVE for enhancing expert diversity in MoE LLMs", "Implements domain affinity mining and pruning-based reconstruction", "Demonstrates improved efficiency with minimal accuracy loss compared to previous methods."], "limitations": "", "keywords": ["Large Language Models", "Mixture-of-Experts", "Diversity Enhancement", "Model Reconstruction", "Training Efficiency"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.09359", "pdf": "https://arxiv.org/pdf/2506.09359.pdf", "abs": "https://arxiv.org/abs/2506.09359", "title": "Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL", "authors": ["Qingyun Zeng", "Simin Ma", "Arash Niknafs", "Ashish Basran", "Carol Szabo"], "categories": ["cs.CL"], "comment": "8 pages", "summary": "The rise of Large Language Models (LLMs) has significantly advanced\nText-to-SQL (NL2SQL) systems, yet evaluating the semantic equivalence of\ngenerated SQL remains a challenge, especially given ambiguous user queries and\nmultiple valid SQL interpretations. This paper explores using LLMs to assess\nboth semantic and a more practical \"weak\" semantic equivalence. We analyze\ncommon patterns of SQL equivalence and inequivalence, discuss challenges in\nLLM-based evaluation.", "AI": {"tldr": "This paper investigates the use of Large Language Models (LLMs) to evaluate semantic equivalence in Text-to-SQL systems, addressing challenges related to ambiguous queries.", "motivation": "To address the challenges in evaluating semantic equivalence of SQL generated from ambiguous user queries in NL2SQL systems.", "method": "The paper analyzes patterns of SQL equivalence and inequivalence, applying LLMs to evaluate both semantic and weak semantic equivalence.", "result": "The analysis reveals common patterns and highlights the challenges faced in effectively using LLMs for evaluation.", "conclusion": "LLMs show promise in evaluating SQL equivalence, but ambiguities in user queries pose significant challenges.", "key_contributions": ["Use of LLMs for semantic and weak semantic equivalence evaluation in SQL generation", "Identification of common patterns in SQL equivalence", "Discussion of challenges in LLM-based evaluation methods"], "limitations": "", "keywords": ["Large Language Models", "Text-to-SQL", "semantic equivalence", "SQL evaluation", "NL2SQL"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2409.13058", "pdf": "https://arxiv.org/pdf/2409.13058.pdf", "abs": "https://arxiv.org/abs/2409.13058", "title": "Mixed Reality Tele-Ultrasound over 750 km: A Feasibility Study", "authors": ["Ryan Yeung", "David Black", "Patrick B. Chen", "Victoria Lessoway", "Janice Reid", "Sergio Rangel-Suarez", "Silvia D. Chang", "Septimiu E. Salcudean"], "categories": ["cs.HC", "cs.RO"], "comment": "8 pages, 11 figures", "summary": "To address the lack of access to ultrasound in remote communities, previous\nwork introduced human teleoperation, a mixed reality and haptics-based\ntele-ultrasound system. In this approach, a novice takes the role of a\ncognitive robot controlled remotely by an expert through mixed reality. In this\nmanuscript we summarize new developments to this system and describe a\nfeasibility study assessing its use for long-distance remote abdominal\nultrasound examinations. To provide simple but effective haptic feedback, we\nused an ellipsoid model of the patient with its parameters calibrated using our\nsystem's position and force sensors. We tested the system in Skidegate, Haida\nGwaii, Canada, with the experts positioned 754 km away in Vancouver, Canada. We\nperformed 11 total scans with 10 novices and 2 sonographers. The sonographers\nwere tasked with acquiring 5 target images in the epigastric region. The image\nacquisition quality was assessed by 2 radiologists. We collected alignment data\nand the novices completed task load and usability questionnaires. Both the\nnovices and sonographers provided written and verbal feedback to inform future\ndesign iterations. 92% of the acquired images had sufficient quality for\ninterpretation by both radiologists. The mean task load reported by the novices\nwas below reference values reported in literature and the usability was\nunanimously positive. No correlation was found between image quality and the\nfollower's alignment error with the virtual transducer. Overall, we show that\nhuman teleoperation enables sonographers to perform remote abdominal ultrasound\nimaging with high performance, even across large distances and with novice\nfollowers. Future work will compare human teleoperation to conventional,\nrobotic and tele-mentored ultrasound.", "AI": {"tldr": "This paper presents advancements in a mixed reality tele-ultrasound system that enables remote novice operators to perform abdominal ultrasound scans under expert supervision, demonstrating high image quality and positive user feedback.", "motivation": "To improve access to ultrasound in remote areas through teleoperation, leveraging mixed reality and haptic technologies for effective training and execution of ultrasound examinations.", "method": "A mixed reality tele-ultrasound system was assessed in a feasibility study involving 10 novices and 2 sonographers, with remote experts guiding the process. The system included haptic feedback and was tested over a distance of 754 km.", "result": "92% of the ultrasound images obtained had sufficient quality for interpretation by radiologists, with novices reporting low task load and high usability. No correlation was found between image quality and alignment error of the virtual transducer.", "conclusion": "Human teleoperation can effectively enable novice sonographers to perform abdominal ultrasound imaging at a distance, presenting a viable solution for remote healthcare delivery.", "key_contributions": ["Development of a mixed reality and haptics-based tele-ultrasound system for remote operation", "Successful feasibility study demonstrating high image acquisition quality from novice operators", "User feedback indicating positive usability and low task load for the teleoperation system."], "limitations": "The study was limited to a small number of scans and a specific geographical location, which may affect generalizability.", "keywords": ["tele-ultrasound", "mixed reality", "haptic feedback", "remote healthcare", "feasibility study"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2506.09367", "pdf": "https://arxiv.org/pdf/2506.09367.pdf", "abs": "https://arxiv.org/abs/2506.09367", "title": "COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content", "authors": ["Zhengyuan Liu", "Stella Xin Yin", "Dion Hoe-Lian Goh", "Nancy F. Chen"], "categories": ["cs.CL", "cs.AI"], "comment": "BEA 2025", "summary": "While Generative AI has demonstrated strong potential and versatility in\ncontent generation, its application to educational contexts presents several\nchallenges. Models often fail to align with curriculum standards and maintain\ngrade-appropriate reading levels consistently. Furthermore, STEM education\nposes additional challenges in balancing scientific explanations with everyday\nlanguage when introducing complex and abstract ideas and phenomena to younger\nstudents. In this work, we propose COGENT, a curriculum-oriented framework for\ngenerating grade-appropriate educational content. We incorporate three\ncurriculum components (science concepts, core ideas, and learning objectives),\ncontrol readability through length, vocabulary, and sentence complexity, and\nadopt a ``wonder-based'' approach to increase student engagement and interest.\nWe conduct a multi-dimensional evaluation via both LLM-as-a-judge and human\nexpert analysis. Experimental results show that COGENT consistently produces\ngrade-appropriate passages that are comparable or superior to human references.\nOur work establishes a viable approach for scaling adaptive and high-quality\nlearning resources.", "AI": {"tldr": "COGENT is a framework for generating curriculum-oriented, grade-appropriate educational content using Generative AI, focusing on STEM education challenges.", "motivation": "Address the challenges of applying Generative AI in educational contexts, particularly the misalignment with curriculum standards and readability issues in STEM education.", "method": "COGENT incorporates curriculum components, controls readability through various factors, and uses a wonder-based approach to enhance student engagement.", "result": "COGENT produces grade-appropriate educational content that is comparable or superior to human references according to both LLMs and human expert evaluations.", "conclusion": "COGENT establishes a scalable approach for creating adaptive, high-quality learning resources.", "key_contributions": ["Development of a curriculum-oriented framework for AI-generated educational content", "Integration of curriculum components for enhanced content alignment", "Demonstration of effectiveness through multi-dimensional evaluations"], "limitations": "", "keywords": ["Generative AI", "education", "curriculum development", "STEM", "readability"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.09375", "pdf": "https://arxiv.org/pdf/2506.09375.pdf", "abs": "https://arxiv.org/abs/2506.09375", "title": "CoLMbo: Speaker Language Model for Descriptive Profiling", "authors": ["Massa Baali", "Shuo Han", "Syed Abdul Hannan", "Purusottam Samal", "Karanveer Singh", "Soham Deshmukh", "Rita Singh", "Bhiksha Raj"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Speaker recognition systems are often limited to classification tasks and\nstruggle to generate detailed speaker characteristics or provide context-rich\ndescriptions. These models primarily extract embeddings for speaker\nidentification but fail to capture demographic attributes such as dialect,\ngender, and age in a structured manner. This paper introduces CoLMbo, a Speaker\nLanguage Model (SLM) that addresses these limitations by integrating a speaker\nencoder with prompt-based conditioning. This allows for the creation of\ndetailed captions based on speaker embeddings. CoLMbo utilizes user-defined\nprompts to adapt dynamically to new speaker characteristics and provides\ncustomized descriptions, including regional dialect variations and age-related\ntraits. This innovative approach not only enhances traditional speaker\nprofiling but also excels in zero-shot scenarios across diverse datasets,\nmarking a significant advancement in the field of speaker recognition.", "AI": {"tldr": "CoLMbo introduces a novel Speaker Language Model that enhances speaker recognition by generating rich contextual descriptions and capturing demographic attributes based on speaker embeddings and user-defined prompts.", "motivation": "To overcome limitations of traditional speaker recognition systems that only classify speakers without generating detailed characteristics or contextual descriptions.", "method": "Integration of a speaker encoder with prompt-based conditioning allows the system to create detailed captions from speaker embeddings.", "result": "CoLMbo enhances speaker profiling and performs well in zero-shot scenarios across diverse datasets by providing customized descriptions of speakers, including dialect variations and demographic traits.", "conclusion": "This approach represents a significant advancement in speaker recognition technology, paving the way for more nuanced understanding of speakers based on their embeddings.", "key_contributions": ["Introduction of CoLMbo, a Speaker Language Model", "Dynamic adaptation to new speaker characteristics through prompt-based conditioning", "Enhanced profiling capabilities including demographic attributes such as dialect, gender, and age."], "limitations": "", "keywords": ["Speaker recognition", "Speaker Language Model", "Demographic attributes", "Prompt-based conditioning", "Zero-shot learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2412.06085", "pdf": "https://arxiv.org/pdf/2412.06085.pdf", "abs": "https://arxiv.org/abs/2412.06085", "title": "From Simple Sensors to Complex Context: Insights for HabiTech", "authors": ["Albrecht Kurze", "Karola Köpferl"], "categories": ["cs.HC"], "comment": "CHI24 Extended Abstracts, Workshop HabiTech, May 11, 2024, Honolulu,\n  HI, USA 2024. 4 pages, 5 figures", "summary": "We relate our previous as well as ongoing research in the domain of smart\nhomes to the concept of HabiTech. HabiTech can benefit from existing approaches\nand findings in a broader context of whole buildings or communities within.\nAlong with data comes context of data capture and data interpretation in\ndifferent dimensions (spatial, temporal, social). For defining what is\n'community' proximity plays a crucial role in context, both spatially as well\nas socially. A participatory approach for research in living in sensing\nenvironments is promising to address complexity as well as interests of\ndifferent stakeholders. Often it is the complex context that makes even simple\nsensor data sensitive, i.e. in terms of privacy. When it comes to handle shared\ndata then concepts from the physical world for shared spaces might be related\nback to the data domain.", "AI": {"tldr": "This paper discusses the concept of HabiTech in smart homes, emphasizing the importance of context in data capture and interpretation, and suggesting a participatory approach to address stakeholder interests and privacy concerns.", "motivation": "The paper aims to explore how smart homes can leverage HabiTech and existing research to enhance community living through better data context and stakeholder involvement.", "method": "The paper adopts a participatory research approach, examining the role of proximity in defining communities and addressing the complexities of interpreting sensor data in smart environments.", "result": "Findings indicate that understanding both spatial and social dimensions of community is critical for effective data interpretation in smart home contexts, while participatory approaches help in addressing the privacy concerns of shared data.", "conclusion": "The paper concludes that integrating HabiTech with community-focused strategies can improve the management of sensor data in smart homes, balancing stakeholder interests and privacy.", "key_contributions": ["Introduces the concept of HabiTech in relation to smart housing.", " highlights the social and spatial dimensions of community for data context.", "Discusses privacy issues in shared data management."], "limitations": "The study may require further empirical validation and examines the challenges of diverse stakeholder engagement.", "keywords": ["HabiTech", "smart homes", "participatory research", "sensor data", "community"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.09381", "pdf": "https://arxiv.org/pdf/2506.09381.pdf", "abs": "https://arxiv.org/abs/2506.09381", "title": "Binary classification for perceived quality of headlines and links on worldwide news websites, 2018-2024", "authors": ["Austin McCutcheon", "Thiago E. A. de Oliveira", "Aleksandr Zheleznov", "Chris Brogly"], "categories": ["cs.CL"], "comment": null, "summary": "The proliferation of online news enables potential widespread publication of\nperceived low-quality news headlines/links. As a result, we investigated\nwhether it was possible to automatically distinguish perceived lower-quality\nnews headlines/links from perceived higher-quality headlines/links. We\nevaluated twelve machine learning models on a binary, balanced dataset of\n57,544,214 worldwide news website links/headings from 2018-2024 (28,772,107 per\nclass) with 115 extracted linguistic features. Binary labels for each text were\nderived from scores based on expert consensus regarding the respective news\ndomain quality. Traditional ensemble methods, particularly the bagging\nclassifier, had strong performance (88.1% accuracy, 88.3% F1, 80/20 train/test\nsplit). Fine-tuned DistilBERT achieved the highest accuracy (90.3%, 80/20\ntrain/test split) but required more training time. The results suggest that\nboth NLP features with traditional classifiers and deep learning models can\neffectively differentiate perceived news headline/link quality, with some\ntrade-off between predictive performance and train time.", "AI": {"tldr": "The paper investigates distinguishing between perceived lower-quality and higher-quality news headlines using machine learning.", "motivation": "To address the challenge of automated identification of news quality in the context of the vast amount of online news.", "method": "Evaluated twelve machine learning models on a balanced dataset of over 57 million news website links/headings, employing 115 linguistic features and expert-derived binary quality labels.", "result": "The bagging classifier achieved 88.1% accuracy while fine-tuned DistilBERT reached 90.3% accuracy, highlighting the effective use of both traditional classifiers and deep learning approaches.", "conclusion": "NLP features combined with traditional classifiers and deep learning models can successfully differentiate news quality, with trade-offs in performance and training time.", "key_contributions": ["Introduction of a dataset with over 57 million news links for quality classification", "Comparison of traditional ensemble methods and deep learning models for this task", "Demonstration of the importance of both linguistic features and model choice in predicting news quality"], "limitations": "Potential biases in expert consensus for quality labeling and the need for extensive training time for deep learning models.", "keywords": ["news quality", "machine learning", "NLP", "DistilBERT", "ensemble methods"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2502.09101", "pdf": "https://arxiv.org/pdf/2502.09101.pdf", "abs": "https://arxiv.org/abs/2502.09101", "title": "Bridging the Gap Between LLMs and Human Intentions: Progresses and Challenges in Instruction Understanding, Intention Reasoning, and Reliable Generation", "authors": ["Zongyu Chang", "Feihong Lu", "Ziqin Zhu", "Qian Li", "Cheng Ji", "Zhuo Chen", "Hao Peng", "Yang Liu", "Ruifeng Xu", "Yangqiu Song", "Shangguang Wang", "Jianxin Li"], "categories": ["cs.HC"], "comment": "19 pages, 11 figures", "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\nunderstanding and generation. However, when interacting with human instructions\nin real-world scenarios, LLMs still face significant challenges, particularly\nin accurately capturing and comprehending human instructions and intentions.\nThis paper focuses on three challenges in LLM-based text generation tasks:\ninstruction understanding, intention reasoning, and Reliable Dialog Generation.\nRegarding human complex instruction, LLMs have deficiencies in understanding\nlong contexts and instructions in multi-round conversations. For intention\nreasoning, LLMs may have inconsistent command reasoning, difficulty reasoning\nabout commands containing incorrect information, difficulty understanding user\nambiguous language commands, and a weak understanding of user intention in\ncommands. Besides, In terms of Reliable Dialog Generation, LLMs may have\nunstable generated content and unethical generation. To this end, we classify\nand analyze the performance of LLMs in challenging scenarios and conduct a\ncomprehensive evaluation of existing solutions. Furthermore, we introduce\nbenchmarks and categorize them based on the aforementioned three core\nchallenges. Finally, we explore potential directions for future research to\nenhance the reliability and adaptability of LLMs in real-world applications.", "AI": {"tldr": "This paper discusses significant challenges faced by large language models (LLMs) in understanding and generating text according to human instructions, focusing on instruction understanding, intention reasoning, and reliable dialog generation.", "motivation": "To address the limitations of LLMs in comprehending human instructions and improving their reliability in real-world applications.", "method": "The paper classifies and analyzes the performance of LLMs in complex instruction scenarios, conducting a comprehensive evaluation of current solutions and introducing benchmarks based on core challenges identified.", "result": "The evaluation reveals deficiencies in LLMs regarding understanding long contexts, intention reasoning, and generating reliable dialog, highlighting the need for improved models.", "conclusion": "The paper calls for targeted research on enhancing LLMs' capabilities in real-world settings, particularly around instruction comprehension and dialog reliability.", "key_contributions": ["Identification of challenges in LLMs related to instruction understanding, intention reasoning, and dialog generation.", "Classification and analysis of LLM performance in challenging scenarios.", "Introduction of benchmarks for future research direction."], "limitations": "", "keywords": ["large language models", "instruction understanding", "intention reasoning", "dialog generation", "benchmarking"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.09391", "pdf": "https://arxiv.org/pdf/2506.09391.pdf", "abs": "https://arxiv.org/abs/2506.09391", "title": "Comparing human and LLM politeness strategies in free production", "authors": ["Haoran Zhao", "Robert D. Hawkins"], "categories": ["cs.CL"], "comment": "25 pages, 5 figures", "summary": "Polite speech poses a fundamental alignment challenge for large language\nmodels (LLMs). Humans deploy a rich repertoire of linguistic strategies to\nbalance informational and social goals -- from positive approaches that build\nrapport (compliments, expressions of interest) to negative strategies that\nminimize imposition (hedging, indirectness). We investigate whether LLMs employ\na similarly context-sensitive repertoire by comparing human and LLM responses\nin both constrained and open-ended production tasks. We find that larger models\n($\\ge$70B parameters) successfully replicate key preferences from the\ncomputational pragmatics literature, and human evaluators surprisingly prefer\nLLM-generated responses in open-ended contexts. However, further linguistic\nanalyses reveal that models disproportionately rely on negative politeness\nstrategies even in positive contexts, potentially leading to\nmisinterpretations. While modern LLMs demonstrate an impressive handle on\npoliteness strategies, these subtle differences raise important questions about\npragmatic alignment in AI systems.", "AI": {"tldr": "This paper investigates how large language models (LLMs) handle polite speech, comparing their performance to human responses.", "motivation": "To understand the alignment challenge in LLMs regarding the use of polite speech strategies that balance social and informational goals.", "method": "The study compares responses generated by LLMs (≥70B parameters) and human participants in both constrained and open-ended tasks.", "result": "LLMs replicate key preferences seen in human responses but tend to over-rely on negative politeness strategies, even in contexts where positive responses are more appropriate.", "conclusion": "Although LLMs exhibit proficient politeness strategies, their tendencies may lead to misinterpretations, highlighting challenges in pragmatic alignment for AI systems.", "key_contributions": ["Demonstration of LLMs' capability to emulate human-like polite speech strategies.", "Identification of the over-reliance on negative politeness by LLMs in inappropriate contexts.", "Insights into the implications of conversational alignment challenges for AI development."], "limitations": "Focuses primarily on larger models and may not generalize to smaller LLMs or other AI systems.", "keywords": ["politeness", "large language models", "pragmatics", "human evaluation", "AI alignment"], "importance_score": 8, "read_time_minutes": 25}}
{"id": "2506.09393", "pdf": "https://arxiv.org/pdf/2506.09393.pdf", "abs": "https://arxiv.org/abs/2506.09393", "title": "A Hierarchical Probabilistic Framework for Incremental Knowledge Tracing in Classroom Settings", "authors": ["Xinyi Gao", "Qiucheng Wu", "Yang Zhang", "Xuechen Liu", "Kaizhi Qian", "Ying Xu", "Shiyu Chang"], "categories": ["cs.CL"], "comment": "24 pages, 4 figures", "summary": "Knowledge tracing (KT) aims to estimate a student's evolving knowledge state\nand predict their performance on new exercises based on performance history.\nMany realistic classroom settings for KT are typically low-resource in data and\nrequire online updates as students' exercise history grows, which creates\nsignificant challenges for existing KT approaches. To restore strong\nperformance under low-resource conditions, we revisit the hierarchical\nknowledge concept (KC) information, which is typically available in many\nclassroom settings and can provide strong prior when data are sparse. We\ntherefore propose Knowledge-Tree-based Knowledge Tracing (KT$^2$), a\nprobabilistic KT framework that models student understanding over a\ntree-structured hierarchy of knowledge concepts using a Hidden Markov Tree\nModel. KT$^2$ estimates student mastery via an EM algorithm and supports\npersonalized prediction through an incremental update mechanism as new\nresponses arrive. Our experiments show that KT$^2$ consistently outperforms\nstrong baselines in realistic online, low-resource settings.", "AI": {"tldr": "A probabilistic framework, KT$^2$, for modeling student knowledge states in low-resource settings using hierarchical knowledge concepts and a Hidden Markov Tree Model.", "motivation": "To improve knowledge tracing in low-resource classroom settings that require real-time updates as student exercise history grows.", "method": "KT$^2$ models student understanding over a tree-structured hierarchy of knowledge concepts and employs an EM algorithm for estimating mastery while enabling personalized predictions through incremental updates.", "result": "KT$^2$ demonstrates consistent performance improvement over strong baselines in online, low-resource environments.", "conclusion": "The framework effectively utilizes available hierarchical knowledge concept information to enhance knowledge tracing performance under data-scarce conditions.", "key_contributions": ["Introduction of KT$^2$ framework for low-resource knowledge tracing", "Use of Hidden Markov Tree Model to represent knowledge concepts", "Implementation of incremental update mechanism for personalized predictions"], "limitations": "", "keywords": ["Knowledge Tracing", "Probabilistic Models", "Hidden Markov Models", "Educational Technology", "Personalized Learning"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2506.09408", "pdf": "https://arxiv.org/pdf/2506.09408.pdf", "abs": "https://arxiv.org/abs/2506.09408", "title": "Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models", "authors": ["Jui-Ming Yao", "Hao-Yuan Chen", "Zi-Xian Tang", "Bing-Jia Tan", "Sheng-Wei Peng", "Bing-Cheng Xie", "Shun-Feng Su"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive performance on\nmultiple-choice question answering (MCQA) benchmarks, yet they remain highly\nvulnerable to minor input perturbations. In this paper, we introduce and\nevaluate Token Constraint Decoding (TCD). This simple yet effective\ninference-time algorithm enforces alignment between token-level predictions to\nenhance robustness in noisy settings. Through extensive experiments on\nCommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired\nwith prompt engineering (PE) fixes, significantly restores performance degraded\nby input noise, yielding up to +39\\% absolute gains for weaker models like\nGemma3 1B. Penalty sweep analyses further reveal that TCD implicitly\nregularizes overconfident outputs, with different models requiring distinct\npenalty schedules to maximize resilience. Our findings establish TCD as a\npractical, model-agnostic approach for improving reasoning stability under\nreal-world imperfections and pave the way for more reliable deployment of LLMs\nin safety-critical or user-facing applications.", "AI": {"tldr": "This paper presents Token Constraint Decoding (TCD), an algorithm designed to enhance the robustness of Large Language Models (LLMs) against minor input perturbations in multiple-choice question answering tasks.", "motivation": "To improve the performance of Large Language Models on multiple-choice question answering benchmarks while enhancing their resilience to input noise.", "method": "Introducing Token Constraint Decoding (TCD), an inference-time algorithm that enforces alignment between token-level predictions to improve robustness.", "result": "TCD significantly restores performance degraded by input noise, achieving up to +39% absolute gains for weaker models when paired with prompt engineering fixes. It also regularizes overconfident outputs through penalty sweep analyses tailored to each model.", "conclusion": "TCD is established as a practical, model-agnostic approach for enhancing reasoning stability under real-world conditions, paving the way for safer deployment of LLMs in critical applications.", "key_contributions": ["Introduction of Token Constraint Decoding (TCD) for LLMs", "Demonstrated performance improvements in noisy settings for MCQA", "Insights into penalty schedules required for different models to maximize resilience."], "limitations": "", "keywords": ["Large Language Models", "Token Constraint Decoding", "input noise", "Robustness", "Multiple-choice question answering"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.09414", "pdf": "https://arxiv.org/pdf/2506.09414.pdf", "abs": "https://arxiv.org/abs/2506.09414", "title": "PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering", "authors": ["Xiujun Zhou", "Pingjian Zhang", "Deyou Tang"], "categories": ["cs.CL", "cs.IR"], "comment": "13 pages, 7 figures, 5 tables", "summary": "Knowledge Graph Question Answering (KGQA) is a crucial task in natural\nlanguage processing that requires reasoning over knowledge graphs (KGs) to\nanswer natural language questions. Recent methods utilizing large language\nmodels (LLMs) have shown remarkable semantic parsing capabilities but are\nlimited by the scarcity of diverse annotated data and multi-hop reasoning\nsamples. Traditional data augmentation approaches are focus mainly on\nsingle-hop questions and prone to semantic distortion, while LLM-based methods\nprimarily address semantic distortion but usually neglect multi-hop reasoning,\nthus limiting data diversity. The scarcity of multi-hop samples further weakens\nmodels' generalization. To address these issues, we propose PGDA-KGQA, a\nprompt-guided generative framework with multiple data augmentation strategies\nfor KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by\ncrafting meticulously engineered prompts that integrate the provided textual\ncontent, it leverages LLMs to generate large-scale (question, logical form)\npairs for model training. Specifically, PGDA-KGQA enriches its training set by:\n(1) generating single-hop pseudo questions to improve the alignment of question\nsemantics with KG relations; (2) applying semantic-preserving question\nrewriting to improve robustness against linguistic variations; (3) employing\nanswer-guided reverse path exploration to create realistic multi-hop questions.\nBy adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA\nutilizes the augmented data to enhance the accuracy of logical form generation\nand thus improve answer retrieval performance. Experiments demonstrate that\noutperforms state-of-the-art methods on standard KGQA datasets, achieving\nimprovements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by\n1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.", "AI": {"tldr": "The paper presents PGDA-KGQA, a novel prompt-guided generative framework for enhancing Knowledge Graph Question Answering (KGQA) through various data augmentation strategies.", "motivation": "The paper addresses the limitations faced by existing KGQA methods in terms of data scarcity, particularly for multi-hop reasoning and diverse annotated data.", "method": "PGDA-KGQA uses a unified prompt-design paradigm to generate (question, logical form) pairs by crafting engineered prompts. The framework includes generating single-hop pseudo questions, semantic-preserving question rewriting, and answer-guided reverse path exploration.", "result": "Experiments demonstrate that PGDA-KGQA outperforms state-of-the-art methods on standard KGQA datasets, notably improving performance metrics such as F1, Hits@1, and Accuracy.", "conclusion": "The proposed framework significantly enhances the accuracy of logical form generation and answer retrieval performance in KGQA tasks.", "key_contributions": ["Proposing a novel prompt-guided framework for KGQA", "Integrating multiple data augmentation strategies", "Demonstrating robust performance improvements over existing methods"], "limitations": "", "keywords": ["Knowledge Graphs", "Question Answering", "Data Augmentation", "Large Language Models", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 13}}
{"id": "2506.09424", "pdf": "https://arxiv.org/pdf/2506.09424.pdf", "abs": "https://arxiv.org/abs/2506.09424", "title": "Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings", "authors": ["Md Messal Monem Miah", "Adrita Anika", "Xi Shi", "Ruihong Huang"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Detecting deception in an increasingly digital world is both a critical and\nchallenging task. In this study, we present a comprehensive evaluation of the\nautomated deception detection capabilities of Large Language Models (LLMs) and\nLarge Multimodal Models (LMMs) across diverse domains. We assess the\nperformance of both open-source and commercial LLMs on three distinct datasets:\nreal life trial interviews (RLTD), instructed deception in interpersonal\nscenarios (MU3D), and deceptive reviews (OpSpam). We systematically analyze the\neffectiveness of different experimental setups for deception detection,\nincluding zero-shot and few-shot approaches with random or similarity-based\nin-context example selection. Our results show that fine-tuned LLMs achieve\nstate-of-the-art performance on textual deception detection tasks, while LMMs\nstruggle to fully leverage cross-modal cues. Additionally, we analyze the\nimpact of auxiliary features, such as non-verbal gestures and video summaries,\nand examine the effectiveness of different prompting strategies, including\ndirect label generation and chain-of-thought reasoning. Our findings provide\nkey insights into how LLMs process and interpret deceptive cues across\nmodalities, highlighting their potential and limitations in real-world\ndeception detection applications.", "AI": {"tldr": "This study evaluates the automated deception detection capabilities of Large Language Models (LLMs) and Large Multimodal Models (LMMs) and analyzes their performance on various datasets.", "motivation": "To assess the effectiveness of automated deception detection in an increasingly digital world as deception techniques evolve.", "method": "The performance of open-source and commercial LLMs was tested on three datasets using different experimental setups, including zero-shot and few-shot approaches with various example selection strategies.", "result": "Fine-tuned LLMs achieved state-of-the-art performance in textual deception detection, whereas LMMs showed limitations in utilizing cross-modal cues; also, auxiliary features and prompting strategies were examined.", "conclusion": "LLMs have potential in detecting deception, but their effectiveness varies across modalities, and LMMs require further development.", "key_contributions": ["Comprehensive evaluation of LLMs and LMMs for deception detection.", "Analysis of the performance across different datasets and setups.", "Insights into the effectiveness of prompting strategies and auxiliary features."], "limitations": "LMMs struggle with cross-modal cues; results may vary with different model architectures or datasets.", "keywords": ["deception detection", "Large Language Models", "multimodal models", "machine learning", "natural language processing"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2506.09428", "pdf": "https://arxiv.org/pdf/2506.09428.pdf", "abs": "https://arxiv.org/abs/2506.09428", "title": "Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting", "authors": ["Fei Ding", "Baiqiao Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)'\ninstruction-following capabilities and domain-specific task adaptability, often\ndiminishes their general capabilities. Moreover, due to the inaccessibility of\noriginal pre-training data, catastrophic forgetting tends to be exacerbated\nwhen third-party practitioners implement SFT on open-sourced models. To address\nthis challenge, we propose a novel, more cost-effective SFT method which could\neffectively reduce the risk of catastrophic forgetting without access to\noriginal SFT data. Our approach begins by reconstructing the likely SFT\ninstruction distribution of the base model, followed by a multi-model screening\nprocess to select optimal data, which is then mixed with new data for SFT.\nExperimental results demonstrate that our method preserves generalization\ncapabilities in general domains while improving task-specific performance.", "AI": {"tldr": "This paper presents a new method for Supervised Fine-Tuning (SFT) that minimizes catastrophic forgetting in large language models without needing access to the original training data.", "motivation": "To enhance instruction-following and task adaptability in LLMs while mitigating the issue of catastrophic forgetting during SFT.", "method": "The proposed method involves reconstructing the instruction distribution of the base model and selecting optimal data through a multi-model screening process, which is then mixed with new data for SFT.", "result": "Experiments show that this method preserves generalization capabilities across general domains while also enhancing task-specific performance.", "conclusion": "The novel SFT method effectively balances the need for adaptability in task-specific scenarios with the retention of general performance in LLMs.", "key_contributions": ["Novel SFT method reduces catastrophic forgetting", "Does not require access to original SFT data", "Balances task-specific performance with generalization capabilities"], "limitations": "Limited testing on diverse datasets could affect generalizability of results.", "keywords": ["Supervised Fine-Tuning", "Large Language Models", "Catastrophic Forgetting"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.09440", "pdf": "https://arxiv.org/pdf/2506.09440.pdf", "abs": "https://arxiv.org/abs/2506.09440", "title": "GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture", "authors": ["GigaChat team", "Mamedov Valentin", "Evgenii Kosarev", "Gregory Leleytner", "Ilya Shchuckin", "Valeriy Berezovskiy", "Daniil Smirnov", "Dmitry Kozlov", "Sergei Averkiev", "Lukyanenko Ivan", "Aleksandr Proshunin", "Ainur Israfilova", "Ivan Baskov", "Artem Chervyakov", "Emil Shakirov", "Mikhail Kolesov", "Daria Khomich", "Darya Latortseva", "Sergei Porkhun", "Yury Fedorov", "Oleg Kutuzov", "Polina Kudriavtseva", "Sofiia Soldatova", "Kolodin Egor", "Stanislav Pyatkin", "Dzmitry Menshykh", "Grafov Sergei", "Eldar Damirov", "Karlov Vladimir", "Ruslan Gaitukiev", "Arkadiy Shatenov", "Alena Fenogenova", "Nikita Savushkin", "Fedor Minkin"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL-2025 System Demo", "summary": "Generative large language models (LLMs) have become crucial for modern NLP\nresearch and applications across various languages. However, the development of\nfoundational models specifically tailored to the Russian language has been\nlimited, primarily due to the significant computational resources required.\nThis paper introduces the GigaChat family of Russian LLMs, available in various\nsizes, including base models and instruction-tuned versions. We provide a\ndetailed report on the model architecture, pre-training process, and\nexperiments to guide design choices. In addition, we evaluate their performance\non Russian and English benchmarks and compare GigaChat with multilingual\nanalogs. The paper presents a system demonstration of the top-performing models\naccessible via an API, a Telegram bot, and a Web interface. Furthermore, we\nhave released three open GigaChat models in open-source\n(https://huggingface.co/ai-sage), aiming to expand NLP research opportunities\nand support the development of industrial solutions for the Russian language.", "AI": {"tldr": "This paper presents the GigaChat family of large language models designed for the Russian language, detailing their architecture, pre-training, performance evaluations, and offering system demonstrations via various interfaces.", "motivation": "To address the lack of Russian-specific foundational models in NLP due to high computational demands and to support industrial solutions for the Russian language.", "method": "Introduction of the GigaChat LLMs with varying sizes, including models optimized for instruction. The architecture and pre-training processes are detailed, along with performance evaluations on Russian and English benchmarks.", "result": "GigaChat models have been evaluated and compared with multilingual counterparts, demonstrating robust performance and offering open-source access to three models.", "conclusion": "The release of GigaChat models is expected to enhance Russian NLP research and facilitate the development of applicable solutions, supported by accessible APIs and user interfaces.", "key_contributions": ["Introduction of GigaChat LLMs specifically tailored for the Russian language", "Open-source availability of three GigaChat models", "Comprehensive performance evaluation against multilingual models"], "limitations": "Limited computational resources may hinder further improvements and expansion of these models.", "keywords": ["Generative LLMs", "Russian language", "NLP", "GigaChat", "Open-source"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2506.09450", "pdf": "https://arxiv.org/pdf/2506.09450.pdf", "abs": "https://arxiv.org/abs/2506.09450", "title": "UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs", "authors": ["Prameshwar Thiyagarajan", "Vaishnavi Parimi", "Shamant Sai", "Soumil Garg", "Zhangir Meirbek", "Nitin Yarlagadda", "Kevin Zhu", "Chris Kim"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Conference of the North American Chapter of the\n  Association for Computational Linguistics, Student Research Workshop 2025\n  (NAACL SRW 2025)", "summary": "Theory of Mind (ToM), the ability to understand the mental states of oneself\nand others, remains a challenging area for large language models (LLMs), which\noften fail to predict human mental states accurately. In this paper, we\nintroduce UniToMBench, a unified benchmark that integrates the strengths of\nSimToM and TOMBENCH to systematically improve and assess ToM capabilities in\nLLMs by integrating multi-interaction task designs and evolving story\nscenarios. Supported by a custom dataset of over 1,000 hand-written scenarios,\nUniToMBench combines perspective-taking techniques with diverse evaluation\nmetrics to better stimulate social cognition in LLMs. Through evaluation, we\nobserve that while models like GPT-4o and GPT-4o Mini show consistently high\naccuracy in tasks involving emotional and belief-related scenarios, with\nresults usually above 80%, there is significant variability in their\nperformance across knowledge-based tasks. These results highlight both the\nstrengths and limitations of current LLMs in ToM-related tasks, underscoring\nthe value of UniToMBench as a comprehensive tool for future development. Our\ncode is publicly available here:\nhttps://github.com/Shamant/unifiedtombenchmark.", "AI": {"tldr": "UniToMBench is a benchmark designed to improve and evaluate the Theory of Mind capabilities in large language models (LLMs), demonstrating both their strengths and limitations through various task evaluations.", "motivation": "To address the challenges LLMs face in accurately predicting human mental states, this work introduces a unified benchmark to systematically assess and enhance ToM capabilities in LLMs.", "method": "The paper presents UniToMBench, which combines multi-interaction task designs with evolving story scenarios, evaluated using a custom dataset of over 1,000 scenarios. It employs perspective-taking techniques and a variety of metrics for assessment.", "result": "Evaluation shows LLMs like GPT-4o perform with high accuracy (above 80%) in emotional and belief-related tasks but demonstrate notable variability in knowledge-based tasks.", "conclusion": "UniToMBench serves as a comprehensive evaluation tool that highlights the strengths and limitations of LLMs in Theory of Mind tasks, paving the way for future improvements.", "key_contributions": ["Introduction of UniToMBench for assessing ToM in LLMs", "Integration of multi-interaction task designs", "Development of a custom dataset with over 1,000 scenarios"], "limitations": "The variability in LLM performance across knowledge-based tasks suggests further research is needed to enhance capabilities in this area.", "keywords": ["Theory of Mind", "Large Language Models", "Benchmark", "Social Cognition", "Evaluation Metrics"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.09457", "pdf": "https://arxiv.org/pdf/2506.09457.pdf", "abs": "https://arxiv.org/abs/2506.09457", "title": "Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms", "authors": ["Zeguan Xiao", "Yun Chen", "Guanhua Chen"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization\n(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient\nalternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms\nfor aligning large language models (LLMs) with human preferences. However, DAAs\nsuffer from a fundamental limitation we identify as the \"reward-generation gap\"\n-- a misalignment between optimization objectives during training and actual\ngeneration performance during inference. In this paper, we find a contributor\nto the reward-generation gap is the mismatch between the inherent importance of\nprefix tokens during the LLM generation process and how this importance is\nreflected in the implicit reward functions of DAAs. To bridge the gap, we\nintroduce a simple yet effective approach called Prefix-Oriented Equal-length\nTraining (POET), which truncates both preferred and dispreferred responses to\nmatch the shorter one's length. Training with POET, where both responses in\neach sample are truncated to equal length, resulting in diverse truncated\nlengths across samples, the optimization of DAAs objective is implicitly\nconstrained to converge across all positions, thus paying more attention to\nprefix tokens than the standard DAAs. We conduct experiments with DPO and\nSimPO, two representative DAAs, demonstrating that POET improves over their\nstandard implementations, achieving up to 15.6 points in AlpacaEval 2 and\noverall improvements across downstream tasks. Our results highlight the\nimportance of addressing the misalignment between reward optimization and\ngeneration performance in DAAs.", "AI": {"tldr": "The paper presents Prefix-Oriented Equal-length Training (POET) to address the reward-generation gap in Direct Alignment Algorithms for optimizing large language models.", "motivation": "Direct Alignment Algorithms (DAAs) like DPO and SimPO face a challenge known as the 'reward-generation gap', where training objectives do not align with generation performance.", "method": "The authors introduce Prefix-Oriented Equal-length Training (POET), which truncates both preferred and dispreferred responses to equal lengths, thus enhancing the optimization process across all generation positions.", "result": "Experiments demonstrate that POET significantly improves performance of DAAs, resulting in up to 15.6 points improvement in AlpacaEval 2 and better outcomes on downstream tasks.", "conclusion": "Addressing the reward-generation gap can improve DAAs' alignment with human preferences during language model training.", "key_contributions": ["Introduction of Prefix-Oriented Equal-length Training (POET)", "Empirical evidence that POET remedies the reward-generation gap", "Demonstrated performance improvements on standard alignment tasks"], "limitations": "", "keywords": ["Large Language Models", "Direct Preference Optimization", "Human Feedback", "Training Algorithms"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.09495", "pdf": "https://arxiv.org/pdf/2506.09495.pdf", "abs": "https://arxiv.org/abs/2506.09495", "title": "Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers", "authors": ["Ilanit Sobol", "Shir Lissak", "Refael Tikochinski", "Tal Nakash", "Anat Brunstein Klomek", "Eyal Fruchter", "Roi Reichart"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Suicide remains a leading cause of death in Western countries, underscoring\nthe need for new research approaches. As social media becomes central to daily\nlife, digital footprints offer valuable insight into suicidal behavior.\nFocusing on individuals who attempted suicide while uploading videos to their\nchannels, we investigate: How do suicidal behaviors manifest on YouTube, and\nhow do they differ from expert knowledge? We applied complementary approaches:\ncomputational bottom-up, hybrid, and expert-driven top-down, on a novel\nlongitudinal dataset of 181 YouTube channels from individuals with\nlife-threatening attempts, alongside 134 control channels. In the bottom-up\napproach, we applied LLM-based topic modeling to identify behavioral\nindicators. Of 166 topics, five were associated with suicide-attempt, with two\nalso showing temporal attempt-related changes ($p<.01$) - Mental Health\nStruggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach,\na clinical expert reviewed LLM-derived topics and flagged 19 as\nsuicide-related. However, none showed significant attempt-related temporal\neffects beyond those identified bottom-up. Notably, YouTube Engagement, a\nplatform-specific indicator, was not flagged by the expert, underscoring the\nvalue of bottom-up discovery. In the top-down approach, psychological\nassessment of suicide attempt narratives revealed that the only significant\ndifference between individuals who attempted before and those attempted during\ntheir upload period was the motivation to share this experience: the former\naimed to Help Others ($\\beta=-1.69$, $p<.01$), while the latter framed it as\npart of their Personal Recovery ($\\beta=1.08$, $p<.01$). By integrating these\napproaches, we offer a nuanced understanding of suicidality, bridging digital\nbehavior and clinical insights.\n  * Within-group changes in relation to the suicide attempt.", "AI": {"tldr": "This study examines how suicidal behaviors manifest on YouTube by analyzing videos from individuals who attempted suicide, using a mix of computational methods and expert insights.", "motivation": "To explore the manifestation of suicidal behaviors on social media, particularly YouTube, and to compare these behavioral indicators with expert knowledge.", "method": "A longitudinal analysis of 181 YouTube channels from individuals with life-threatening attempts and 134 control channels, employing LLM-based topic modeling, hybrid expert review, and psychological assessment.", "result": "Identified five topics associated with suicide attempts, with notable changes over time in 'Mental Health Struggles' and 'YouTube Engagement'. Experts confirmed some topics but missed key platform-specific indicators like YouTube Engagement, revealing the importance of computational approaches.", "conclusion": "The integration of computational and expert-driven methods provides a nuanced understanding of suicidality, highlighting discrepancies between social media behavior and clinical insights.", "key_contributions": ["Identified novel behavioral indicators of suicidality from social media data.", "Demonstrated the value of LLM-based topic modeling in mental health research.", "Showed different motivations for sharing experiences around suicide attempts on social media."], "limitations": "The study focuses on a specific platform (YouTube) and may not generalize to other social media channels.", "keywords": ["suicidality", "YouTube", "topic modeling", "mental health", "behavioral indicators"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2506.09501", "pdf": "https://arxiv.org/pdf/2506.09501.pdf", "abs": "https://arxiv.org/abs/2506.09501", "title": "Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning", "authors": ["Jiayi Yuan", "Hao Li", "Xinheng Ding", "Wenya Xie", "Yu-Jhe Li", "Wentian Zhao", "Kun Wan", "Jing Shi", "Xia Hu", "Zirui Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are now integral across various domains and have\ndemonstrated impressive performance. Progress, however, rests on the premise\nthat benchmark scores are both accurate and reproducible. We demonstrate that\nthe reproducibility of LLM performance is fragile: changing system\nconfiguration such as evaluation batch size, GPU count, and GPU version can\nintroduce significant difference in the generated responses. This issue is\nespecially pronounced in reasoning models, where minor rounding differences in\nearly tokens can cascade into divergent chains of thought, ultimately affecting\naccuracy. For instance, under bfloat16 precision with greedy decoding, a\nreasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation\nin accuracy and 9,000 tokens difference in response length due to differences\nin GPU count, type, and evaluation batch size. We trace the root cause of this\nvariability to the non-associative nature of floating-point arithmetic under\nlimited numerical precision. This work presents the first systematic\ninvestigation into how numerical precision affects reproducibility in LLM\ninference. Through carefully controlled experiments across various hardware,\nsoftware, and precision settings, we quantify when and how model outputs\ndiverge. Our analysis reveals that floating-point precision -- while critical\nfor reproducibility -- is often neglected in evaluation practices. Inspired by\nthis, we develop a lightweight inference pipeline, dubbed LayerCast, that\nstores weights in 16-bit precision but performs all computations in FP32,\nbalancing memory efficiency with numerical stability. Code is available at\nhttps://github.com/nanomaoli/llm_reproducibility.", "AI": {"tldr": "This paper explores the fragility of LLM performance reproducibility due to changes in system configuration affecting numerical precision.", "motivation": "To understand how changes in hardware and software configurations impact the reproducibility and accuracy of LLM responses during evaluation.", "method": "Conducted controlled experiments testing various configurations like GPU type, count, and evaluation batch size to measure their impact on output divergence in LLMs.", "result": "Found that small changes in configuration can lead to significant variability in accuracy (up to 9%) and response length (up to 9,000 tokens).", "conclusion": "The research underscores the importance of numerical precision and offers LayerCast, an inference pipeline that maintains stability while optimizing memory usage.", "key_contributions": ["Systematic analysis of floating-point precision impacts on LLM reproducibility", "Quantification of response variability based on configuration changes", "Development of LayerCast for improved inference with better numerical stability"], "limitations": "The experiments are limited to specific model versions and may not generalize across all LLMs.", "keywords": ["Large Language Models", "reproducibility", "numerical precision", "inference pipeline", "LayerCast"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.09507", "pdf": "https://arxiv.org/pdf/2506.09507.pdf", "abs": "https://arxiv.org/abs/2506.09507", "title": "TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding", "authors": ["Bingheng Wu", "Jingze Shi", "Yifan Wu", "Nan Tang", "Yuyu Luo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Transformers exhibit proficiency in capturing long-range dependencies,\nwhereas State Space Models (SSMs) facilitate linear-time sequence modeling.\nNotwithstanding their synergistic potential, the integration of these\narchitectures presents a significant challenge, primarily attributable to a\nfundamental incongruity in their respective positional encoding mechanisms:\nTransformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs\nleverage implicit positional representations via convolutions. This divergence\noften precipitates discontinuities and suboptimal performance. To address this\nimpediment, we propose a unified rotary position embedding (\\textbf{\\ourRoPE})\nmethodology, thereby establishing a consistent positional encoding framework\nfor both self-attention and state-space components. Using this \\ourRoPE, we\nintroduce \\textbf{\\model}, a hybrid architecture that coherently integrates the\nTransformer and SSM layers under this unified positional encoding scheme. At a\n4K sequence length, \\model exhibits training and inference speeds that are\n\\textbf{42.3\\% and 29.5\\% faster}, respectively, relative to standard\nTransformer models. It also delivers higher accuracy: under comparable\nsettings, it surpasses a Transformer baseline by over 4\\% on language modeling\nbenchmarks. \\model furthermore scales more effectively: \\model-1.3B gains\n\\textbf{7.22\\%} in average accuracy over its 320M version (versus about 6\\%\ngains for equivalent Transformers or SSMs). Our results show that unified\npositional encoding resolves positional incompatibility in hybrid models,\nenabling efficient, high-performance long-context modeling.", "AI": {"tldr": "This paper proposes \textbf{ourRoPE}, a unified positional encoding method to integrate Transformers and State Space Models (SSMs), leading to a hybrid architecture called \textbf{model} that improves training speed and accuracy for long-sequence tasks.", "motivation": "The integration of Transformers and State Space Models is hindered by differing positional encoding methods, affecting performance.", "method": "The authors introduce a unified rotary position embedding method, \textbf{ourRoPE}, to create a hybrid architecture named \textbf{model} which combines Transformer and SSM layers.", "result": "The \textbf{model} exhibits 42.3% faster training and 29.5% faster inference than standard Transformer models and surpasses a Transformer baseline by over 4% in accuracy on language modeling benchmarks.", "conclusion": "The proposed unified positional encoding approach effectively resolves compatibility issues in hybrid models, allowing for enhanced long-context modeling.", "key_contributions": ["Unified positional encoding that bridges Transformers and SSMs", "A hybrid architecture (\textbf{model}) enhancing speed and accuracy", "Demonstrated superior scaling capabilities for larger models"], "limitations": "", "keywords": ["Transformers", "State Space Models", "Positional Encoding", "Hybrid Architecture", "Machine Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.09513", "pdf": "https://arxiv.org/pdf/2506.09513.pdf", "abs": "https://arxiv.org/abs/2506.09513", "title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning", "authors": ["Yu Sun", "Xingyu Qian", "Weiwen Xu", "Hao Zhang", "Chenghao Xiao", "Long Li", "Yu Rong", "Wenbing Huang", "Qifeng Bai", "Tingyang Xu"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "24 pages, 6 figures, 7 tables", "summary": "Though reasoning-based large language models (LLMs) have excelled in\nmathematics and programming, their capabilities in knowledge-intensive medical\nquestion answering remain underexplored. To address this, we introduce\nReasonMed, the largest medical reasoning dataset, comprising 370k high-quality\nexamples distilled from 1.7 million initial reasoning paths generated by\nvarious LLMs. ReasonMed is constructed through a \\textit{multi-agent\nverification and refinement process}, where we design an \\textit{Error Refiner}\nto enhance the reasoning paths by identifying and correcting error-prone steps\nflagged by a verifier. Leveraging ReasonMed, we systematically investigate best\npractices for training medical reasoning models and find that combining\ndetailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields\nthe most effective fine-tuning strategy. Based on this strategy, we train\nReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the\nprior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.", "AI": {"tldr": "ReasonMed is the largest medical reasoning dataset, designed to enhance reasoning-based LLMs' performance in medical question answering.", "motivation": "To improve knowledge-intensive medical question answering using reasoning-based LLMs, as their capabilities in this area have not been fully explored.", "method": "Introduction of ReasonMed, a dataset containing 370k high-quality examples, constructed through a multi-agent verification and refinement process with an Error Refiner.", "result": "ReasonMed-7B, a model trained on this dataset, outperforms the previous best sub-10B models and even exceeds the performance of LLaMA3.1-70B on PubMedQA.", "conclusion": "Combining detailed Chain-of-Thought reasoning with concise answer summaries is the most effective strategy for training medical reasoning models, setting new performance benchmarks.", "key_contributions": ["Introduction of the ReasonMed dataset", "Development of a multi-agent verification and refinement process", "Establishment of a new benchmark for sub-10B models in medical reasoning"], "limitations": "", "keywords": ["medical reasoning", "large language models", "dataset", "Chain-of-Thought", "benchmark"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.09542", "pdf": "https://arxiv.org/pdf/2506.09542.pdf", "abs": "https://arxiv.org/abs/2506.09542", "title": "KG-Infused RAG: Augmenting Corpus-Based RAG with External Knowledge Graphs", "authors": ["Dingjun Wu", "Yukun Yan", "Zhenghao Liu", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) improves factual accuracy by grounding\nresponses in external knowledge. However, existing methods typically rely on a\nsingle source, either unstructured text or structured knowledge. Moreover, they\nlack cognitively inspired mechanisms for activating relevant knowledge. To\naddress these issues, we propose KG-Infused RAG, a framework that integrates\nKGs into RAG systems to implement spreading activation, a cognitive process\nthat enables concept association and inference. KG-Infused RAG retrieves KG\nfacts, expands the query accordingly, and enhances generation by combining\ncorpus passages with structured facts, enabling interpretable, multi-source\nretrieval grounded in semantic structure. We further improve KG-Infused RAG via\npreference learning on sampled key stages in the pipeline. Experiments on five\nQA benchmarks show that KG-Infused RAG consistently outperforms vanilla RAG (by\n3.8% to 13.8%). Additionally, when integrated into Self-RAG, KG-Infused RAG\nbrings further performance gains, demonstrating its effectiveness and\nversatility as a plug-and-play enhancement module for corpus-based RAG methods.", "AI": {"tldr": "The paper introduces KG-Infused RAG, a framework that enhances Retrieval-Augmented Generation (RAG) by integrating knowledge graphs (KGs) to improve factual accuracy and multi-source retrieval.", "motivation": "To improve the factual accuracy and relevance of responses generated by RAG systems, which traditionally rely on either unstructured text or structured knowledge without cognitive mechanisms for activating relevant information.", "method": "KG-Infused RAG integrates knowledge graphs into the RAG framework to implement spreading activation for better concept association and inference, enhancing query expansion and generation processes.", "result": "Experiments demonstrate that KG-Infused RAG consistently outperforms traditional RAG methods across five QA benchmarks by significant margins.", "conclusion": "KG-Infused RAG proves effective as a plug-and-play enhancement for corpus-based RAG methods, offering improved performance and interpretability in multi-source retrieval tasks.", "key_contributions": ["Introduction of a knowledge graph integration into RAG systems.", "Implementation of spreading activation for cognitive-inspired retrieval.", "Demonstration of performance gains in QA benchmarks over vanilla RAG."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Knowledge Graphs", "Spreading Activation", "Natural Language Processing", "Question Answering"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.09556", "pdf": "https://arxiv.org/pdf/2506.09556.pdf", "abs": "https://arxiv.org/abs/2506.09556", "title": "MEDUSA: A Multimodal Deep Fusion Multi-Stage Training Framework for Speech Emotion Recognition in Naturalistic Conditions", "authors": ["Georgios Chatzichristodoulou", "Despoina Kosmopoulou", "Antonios Kritikos", "Anastasia Poulopoulou", "Efthymios Georgiou", "Athanasios Katsamanis", "Vassilis Katsouros", "Alexandros Potamianos"], "categories": ["cs.CL"], "comment": "Accepted at Interspeech 2025", "summary": "SER is a challenging task due to the subjective nature of human emotions and\ntheir uneven representation under naturalistic conditions. We propose MEDUSA, a\nmultimodal framework with a four-stage training pipeline, which effectively\nhandles class imbalance and emotion ambiguity. The first two stages train an\nensemble of classifiers that utilize DeepSER, a novel extension of a deep\ncross-modal transformer fusion mechanism from pretrained self-supervised\nacoustic and linguistic representations. Manifold MixUp is employed for further\nregularization. The last two stages optimize a trainable meta-classifier that\ncombines the ensemble predictions. Our training approach incorporates human\nannotation scores as soft targets, coupled with balanced data sampling and\nmultitask learning. MEDUSA ranked 1st in Task 1: Categorical Emotion\nRecognition in the Interspeech 2025: Speech Emotion Recognition in Naturalistic\nConditions Challenge.", "AI": {"tldr": "MEDUSA is a multimodal framework designed for emotion recognition that addresses class imbalance and emotion ambiguity, including a four-stage training pipeline and human annotation integration.", "motivation": "To improve the performance of emotion recognition systems in naturalistic conditions where human emotions are subjectively interpreted and unevenly represented.", "method": "The framework consists of a four-stage training process using an ensemble of classifiers, where DeepSER leverages pretrained representations. Manifold MixUp regularizes the model, and a meta-classifier merges ensemble predictions while incorporating human annotations as soft targets.", "result": "MEDUSA achieved first place in the Categorical Emotion Recognition task at the Interspeech 2025 challenge, demonstrating its effectiveness in handling emotionally ambiguous data.", "conclusion": "The proposed framework outperforms existing methods in recognizing emotions from speech under challenging conditions, suggesting its potential for improving HCI applications related to emotion understanding.", "key_contributions": ["Introduced the MEDUSA framework for multimodal emotion recognition.", "Utilized DeepSER as a novel approach combining acoustic and linguistic representations.", "Incorporated human annotation scores and balanced data sampling for improved performance."], "limitations": "", "keywords": ["emotion recognition", "multimodal framework", "DeepSER", "naturalistic conditions", "machine learning"], "importance_score": 7, "read_time_minutes": 6}}
{"id": "2506.09558", "pdf": "https://arxiv.org/pdf/2506.09558.pdf", "abs": "https://arxiv.org/abs/2506.09558", "title": "Gender Bias in English-to-Greek Machine Translation", "authors": ["Eleni Gkovedarou", "Joke Daems", "Luna De Bruyne"], "categories": ["cs.CL"], "comment": "Accepted at GITT 2025 (MT Summit)", "summary": "As the demand for inclusive language increases, concern has grown over the\nsusceptibility of machine translation (MT) systems to reinforce gender\nstereotypes. This study investigates gender bias in two commercial MT systems,\nGoogle Translate and DeepL, focusing on the understudied English-to-Greek\nlanguage pair. We address three aspects of gender bias: i) male bias, ii)\noccupational stereotyping, and iii) errors in anti-stereotypical translations.\nAdditionally, we explore the potential of prompted GPT-4o as a bias mitigation\ntool that provides both gender-explicit and gender-neutral alternatives when\nnecessary. To achieve this, we introduce GendEL, a manually crafted bilingual\ndataset of 240 gender-ambiguous and unambiguous sentences that feature\nstereotypical occupational nouns and adjectives. We find persistent gender bias\nin translations by both MT systems; while they perform well in cases where\ngender is explicitly defined, with DeepL outperforming both Google Translate\nand GPT-4o in feminine gender-unambiguous sentences, they are far from\nproducing gender-inclusive or neutral translations when the gender is\nunspecified. GPT-4o shows promise, generating appropriate gendered and neutral\nalternatives for most ambiguous cases, though residual biases remain evident.", "AI": {"tldr": "This study examines gender bias in machine translation systems Google Translate and DeepL within English-to-Greek translations, and evaluates GPT-4o as a potential bias mitigation tool.", "motivation": "To investigate how machine translation systems reinforce gender stereotypes and to explore alternatives for gender-inclusive translations.", "method": "The study analyzes gender bias by assessing male bias, occupational stereotyping, and errors in anti-stereotypical translations, using the newly developed GendEL bilingual dataset of 240 sentences.", "result": "Findings reveal persistent gender bias in both Google Translate and DeepL, with DeepL being more effective in translating unambiguous feminine sentences. GPT-4o shows potential in providing appropriate gendered and neutral alternatives, yet biases still persist.", "conclusion": "While commercial MT systems exhibit persistent gender biases, GPT-4o offers promising results in generating gender-inclusive translations, highlighting the need for further development in this area.", "key_contributions": ["Introduction of GendEL dataset for gender bias analysis", "Analysis of gender bias in commercial MT systems for a less studied language pair", "Evaluation of GPT-4o's effectiveness in bias mitigation"], "limitations": "The study focuses on a specific language pair and may not generalize to other languages or MT systems.", "keywords": ["gender bias", "machine translation", "GPT-4o", "HCI", "gender inclusivity"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.09560", "pdf": "https://arxiv.org/pdf/2506.09560.pdf", "abs": "https://arxiv.org/abs/2506.09560", "title": "Towards Open Foundation Language Model and Corpus for Macedonian: A Low-Resource Language", "authors": ["Stefan Krsteski", "Matea Tashkovska", "Borjan Sazdov", "Hristijan Gjoreski", "Branislav Gerazov"], "categories": ["cs.CL"], "comment": "Camera-ready version accepted at SlavNLP-2025@ACL", "summary": "The increase in technological adoption worldwide comes with demands for novel\ntools to be used by the general population. Large Language Models (LLMs)\nprovide a great opportunity in this respect, but their capabilities remain\nlimited for low-resource languages, restricting applications in countries where\nsuch languages are spoken. We create several resources to facilitate the\nadoption of LLMs and to support research advancements for Macedonian. We\ncollect the largest Macedonian corpus to date, consisting of 40GB of textual\ndata and totaling 3.5B words. To support conversational applications, we\ncollect a 106k-instance instruction dataset, carefully built to be culturally\ngrounded. For evaluation, we construct a Macedonian evaluation suite covering\nseven benchmarks. Finally, we train domestic-yak, a state-of-the-art\n8B-parameter model, on our curated datasets and evaluate it against eight\nbaseline models using the newly constructed benchmark suite. Our model\noutperforms all existing models in the 8B parameter range across all\nbenchmarks, and achieves performance comparable to models up to 10x larger.\nFurthermore, a qualitative analysis with native speakers reveals that our model\nis preferred over larger counterparts, receiving higher ratings for grammatical\ncorrectness and cultural appropriateness. All datasets, code, and model weights\nare openly released, setting a foundation for advancing LLMs in similarly\nunderrepresented languages. These resources are publicly available at\ngithub.com/LVSTCK for source code, and at huggingface.co/LVSTCK for pretrained\nmodel weights and data.", "AI": {"tldr": "This paper presents novel resources and a state-of-the-art 8B-parameter model for Large Language Models (LLMs) tailored for the Macedonian language, achieving significant performance improvements over existing models.", "motivation": "To support low-resource languages like Macedonian in leveraging Large Language Models (LLMs) for various applications.", "method": "The paper details the collection of a large Macedonian corpus, the creation of a culturally grounded instruction dataset, construction of an evaluation suite, and training of a robust 8B-parameter LLM called domestic-yak.", "result": "The domestic-yak model outperforms all existing 8B models across various benchmarks and matches performance of models up to 10x its size. It is preferred by native speakers for grammatical correctness and cultural relevance.", "conclusion": "The resources including datasets, models, and code are openly released to facilitate future research and development in underrepresented languages, specifically Macedonian.", "key_contributions": ["Creation of the largest Macedonian text corpus (40GB, 3.5B words).", "Development of a culturally grounded instruction dataset with 106k instances.", "Training and evaluation of the domestic-yak model, outperforming existing models."], "limitations": "", "keywords": ["Large Language Models", "Macedonian", "Natural Language Processing", "Culturally Grounded Datasets", "LLM Evaluation"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.09566", "pdf": "https://arxiv.org/pdf/2506.09566.pdf", "abs": "https://arxiv.org/abs/2506.09566", "title": "From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies", "authors": ["Blaž Škrlj", "Boshko Koloski", "Senja Pollak", "Nada Lavrač"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "To-appear as a book chapter", "summary": "Integrating structured knowledge from Knowledge Graphs (KGs) into Large\nLanguage Models (LLMs) enhances factual grounding and reasoning capabilities.\nThis survey paper systematically examines the synergy between KGs and LLMs,\ncategorizing existing approaches into two main groups: KG-enhanced LLMs, which\nimprove reasoning, reduce hallucinations, and enable complex question\nanswering; and LLM-augmented KGs, which facilitate KG construction, completion,\nand querying. Through comprehensive analysis, we identify critical gaps and\nhighlight the mutual benefits of structured knowledge integration. Compared to\nexisting surveys, our study uniquely emphasizes scalability, computational\nefficiency, and data quality. Finally, we propose future research directions,\nincluding neuro-symbolic integration, dynamic KG updating, data reliability,\nand ethical considerations, paving the way for intelligent systems capable of\nmanaging more complex real-world knowledge tasks.", "AI": {"tldr": "This survey examines the integration of Knowledge Graphs into Large Language Models, categorizing approaches and identifying gaps in research.", "motivation": "To enhance factual grounding and reasoning capabilities of LLMs through integration with structured knowledge from KGs.", "method": "Systematic examination and categorization of existing approaches related to KG-enhanced LLMs and LLM-augmented KGs.", "result": "Identification of critical gaps in the synergy between KGs and LLMs, and emphasis on scalability, computational efficiency, and data quality.", "conclusion": "Proposes future research directions like neuro-symbolic integration and ethical considerations for complex real-world knowledge tasks.", "key_contributions": ["Unique emphasis on scalability and computational efficiency", "Identification of mutual benefits between KGs and LLMs", "Proposed future research directions for intelligent systems."], "limitations": "", "keywords": ["Knowledge Graphs", "Large Language Models", "factual grounding", "reasoning capabilities", "neuro-symbolic integration"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.09591", "pdf": "https://arxiv.org/pdf/2506.09591.pdf", "abs": "https://arxiv.org/abs/2506.09591", "title": "Memorization in Language Models through the Lens of Intrinsic Dimension", "authors": ["Stefan Arnold"], "categories": ["cs.CL"], "comment": null, "summary": "Language Models (LMs) are prone to memorizing parts of their data during\ntraining and unintentionally emitting them at generation time, raising concerns\nabout privacy leakage and disclosure of intellectual property. While previous\nresearch has identified properties such as context length, parameter size, and\nduplication frequency, as key drivers of unintended memorization, little is\nknown about how the latent structure modulates this rate of memorization. We\ninvestigate the role of Intrinsic Dimension (ID), a geometric proxy for the\nstructural complexity of a sequence in latent space, in modulating\nmemorization. Our findings suggest that ID acts as a suppressive signal for\nmemorization: compared to low-ID sequences, high-ID sequences are less likely\nto be memorized, particularly in overparameterized models and under sparse\nexposure. These findings highlight the interaction between scale, exposure, and\ncomplexity in shaping memorization.", "AI": {"tldr": "This paper investigates how the Intrinsic Dimension of sequences in latent space affects the memorization rate of Language Models, finding that higher complexity leads to lower memorization.", "motivation": "To address concerns about privacy leakage and intellectual property in Language Models due to unintended memorization.", "method": "The study examines the relationship between Intrinsic Dimension (ID) and memorization rates in Language Models, particularly focusing on overparameterized models and varying exposure levels.", "result": "The research demonstrates that high-ID sequences are less likely to be memorized than low-ID sequences, especially in certain model configurations.", "conclusion": "The findings emphasize the importance of scale, exposure, and complexity in understanding memorization behavior in Language Models.", "key_contributions": ["Introduces the role of Intrinsic Dimension in memorization rates of LMs.", "Highlights the interaction between model architecture and sequence complexity.", "Provides empirical evidence linking ID to reduced memorization in specific conditions."], "limitations": "", "keywords": ["Intrinsic Dimension", "Language Models", "memorization", "latent space", "complexity"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.09627", "pdf": "https://arxiv.org/pdf/2506.09627.pdf", "abs": "https://arxiv.org/abs/2506.09627", "title": "Benchmarking Debiasing Methods for LLM-based Parameter Estimates", "authors": ["Nicolas Audinet de Pieuchon", "Adel Daoud", "Connor T. Jerzak", "Moa Johansson", "Richard Johansson"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) offer an inexpensive yet powerful way to\nannotate text, but are often inconsistent when compared with experts. These\nerrors can bias downstream estimates of population parameters such as\nregression coefficients and causal effects. To mitigate this bias, researchers\nhave developed debiasing methods such as Design-based Supervised Learning (DSL)\nand Prediction-Powered Inference (PPI), which promise valid estimation by\ncombining LLM annotations with a limited number of expensive expert\nannotations. Although these methods produce consistent estimates under\ntheoretical assumptions, it is unknown how they compare in finite samples of\nsizes encountered in applied research. We make two contributions: First, we\nstudy how each method's performance scales with the number of expert\nannotations, highlighting regimes where LLM bias or limited expert labels\nsignificantly affect results. Second, we compare DSL and PPI across a range of\ntasks, finding that although both achieve low bias with large datasets, DSL\noften outperforms PPI on bias reduction and empirical efficiency, but its\nperformance is less consistent across datasets. Our findings indicate that\nthere is a bias-variance tradeoff at the level of debiasing methods, calling\nfor more research on developing metrics for quantifying their efficiency in\nfinite samples.", "AI": {"tldr": "This paper investigates two debiasing methods for annotations from large language models (LLMs), namely Design-based Supervised Learning (DSL) and Prediction-Powered Inference (PPI), comparing their performance with expert annotations in finite sample sizes.", "motivation": "The need to mitigate bias in LLM annotations to ensure valid estimation of population parameters in applied research contexts.", "method": "We analyze the performance of DSL and PPI as the number of expert annotations varies, focusing on their effectiveness in practical scenarios with finite sample sizes.", "result": "Both methods perform well with large datasets, but DSL often better reduces bias and shows higher empirical efficiency, despite being less consistent across various datasets compared to PPI.", "conclusion": "Our findings suggest a bias-variance tradeoff in debiasing methods that necessitates further investigation into metrics for assessing their performance in finite samples.", "key_contributions": ["Analysis of the scaling performance of debiasing methods with expert annotations", "Comparison of DSL and PPI in terms of bias reduction and empirical efficiency", "Identification of bias-variance tradeoff in debiasing methods"], "limitations": "The study primarily focuses on known debiasing methods and may not account for emerging techniques or variations in diverse application contexts.", "keywords": ["Large Language Models", "Debiasing Methods", "Design-based Supervised Learning", "Prediction-Powered Inference", "Bias-Variance Tradeoff"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.09641", "pdf": "https://arxiv.org/pdf/2506.09641.pdf", "abs": "https://arxiv.org/abs/2506.09641", "title": "Modeling Probabilistic Reduction using Information Theory and Naive Discriminative Learning", "authors": ["Anna Stein", "Kevin Tang"], "categories": ["cs.CL", "cs.IT", "math.IT"], "comment": "Submitted to Interspeech 2025", "summary": "This study compares probabilistic predictors based on information theory with\nNaive Discriminative Learning (NDL) predictors in modeling acoustic word\nduration, focusing on probabilistic reduction. We examine three models using\nthe Buckeye corpus: one with NDL-derived predictors using information-theoretic\nformulas, one with traditional NDL predictors, and one with N-gram\nprobabilistic predictors. Results show that the N-gram model outperforms both\nNDL models, challenging the assumption that NDL is more effective due to its\ncognitive motivation. However, incorporating information-theoretic formulas\ninto NDL improves model performance over the traditional model. This research\nhighlights a) the need to incorporate not only frequency and contextual\npredictability but also average contextual predictability, and b) the\nimportance of combining information-theoretic metrics of predictability and\ninformation derived from discriminative learning in modeling acoustic\nreduction.", "AI": {"tldr": "This study evaluates probabilistic predictors for modeling acoustic word duration, revealing that an N-gram model outperforms Naive Discriminative Learning (NDL) models, but that NDL can be enhanced with information-theoretic approaches.", "motivation": "To assess the effectiveness of different probabilistic models in predicting acoustic word duration, particularly in the context of probabilistic reduction.", "method": "Three models were compared: one using NDL-derived predictors with information-theoretic formulas, one with standard NDL predictors, and another with N-gram probabilistic predictors, utilizing the Buckeye corpus for data.", "result": "The N-gram model demonstrated superior performance over both types of NDL models, challenging the view that NDL's cognitive motivation yields better outcomes. However, integrating information-theoretic formulas with NDL approaches led to improved performance when compared to traditional NDL.", "conclusion": "The findings underscore the necessity of integrating frequency, contextual predictability, and average contextual predictability alongside information-theoretic metrics in acoustic modeling practices.", "key_contributions": ["N-gram model outperforms Naive Discriminative Learning (NDL) models in predicting acoustic duration.", "Incorporating information-theoretic measures enhances the performance of NDL predictors.", "Highlights the importance of considering various predictability metrics."], "limitations": "", "keywords": ["acoustic modeling", "predictability", "information theory", "N-gram", "Naive Discriminative Learning"], "importance_score": 3, "read_time_minutes": 8}}
{"id": "2506.09643", "pdf": "https://arxiv.org/pdf/2506.09643.pdf", "abs": "https://arxiv.org/abs/2506.09643", "title": "Using Sign Language Production as Data Augmentation to enhance Sign Language Translation", "authors": ["Harry Walsh", "Maksym Ivashechkin", "Richard Bowden"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Machine learning models fundamentally rely on large quantities of\nhigh-quality data. Collecting the necessary data for these models can be\nchallenging due to cost, scarcity, and privacy restrictions. Signed languages\nare visual languages used by the deaf community and are considered low-resource\nlanguages. Sign language datasets are often orders of magnitude smaller than\ntheir spoken language counterparts. Sign Language Production is the task of\ngenerating sign language videos from spoken language sentences, while Sign\nLanguage Translation is the reverse translation task. Here, we propose\nleveraging recent advancements in Sign Language Production to augment existing\nsign language datasets and enhance the performance of Sign Language Translation\nmodels. For this, we utilize three techniques: a skeleton-based approach to\nproduction, sign stitching, and two photo-realistic generative models, SignGAN\nand SignSplat. We evaluate the effectiveness of these techniques in enhancing\nthe performance of Sign Language Translation models by generating variation in\nthe signer's appearance and the motion of the skeletal data. Our results\ndemonstrate that the proposed methods can effectively augment existing datasets\nand enhance the performance of Sign Language Translation models by up to 19%,\npaving the way for more robust and accurate Sign Language Translation systems,\neven in resource-constrained environments.", "AI": {"tldr": "This paper presents methods to enhance sign language datasets and improve translation models using advanced techniques in sign language production.", "motivation": "The challenge of acquiring sufficient high-quality data for machine learning models, particularly in the context of low-resource languages like sign languages.", "method": "The study employs a skeleton-based approach, sign stitching, and two generative models (SignGAN and SignSplat) to augment sign language datasets for translation tasks.", "result": "The techniques proposed improve the performance of Sign Language Translation models by up to 19%.", "conclusion": "The findings highlight that the methods can bolster existing datasets and lead to more accurate Sign Language Translation systems in low-resource settings.", "key_contributions": ["Introduction of a skeleton-based production approach for sign languages", "Implementation of sign stitching for enhanced training data", "Utilization of photo-realistic generative models to aid dataset augmentation"], "limitations": "", "keywords": ["sign language", "machine learning", "data augmentation", "translation models", "generative models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.09645", "pdf": "https://arxiv.org/pdf/2506.09645.pdf", "abs": "https://arxiv.org/abs/2506.09645", "title": "Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering", "authors": ["Tianjun Yao", "Haoxuan Li", "Zhiqiang Shen", "Pan Li", "Tongliang Liu", "Kun Zhang"], "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.6"], "comment": "32 pages, 28 figures", "summary": "Large Language Models (LLMs) have shown strong inductive reasoning ability\nacross various domains, but their reliability is hindered by the outdated\nknowledge and hallucinations. Retrieval-Augmented Generation mitigates these\nissues by grounding LLMs with external knowledge; however, most existing RAG\npipelines rely on unstructured text, limiting interpretability and structured\nreasoning. Knowledge graphs, which represent facts as relational triples, offer\na more structured and compact alternative. Recent studies have explored\nintegrating knowledge graphs with LLMs for knowledge graph question answering\n(KGQA), with a significant proportion adopting the retrieve-then-reasoning\nparadigm. In this framework, graph-based retrievers have demonstrated strong\nempirical performance, yet they still face challenges in generalization\nability. In this work, we propose RAPL, a novel framework for efficient and\neffective graph retrieval in KGQA. RAPL addresses these limitations through\nthree aspects: (1) a two-stage labeling strategy that combines heuristic\nsignals with parametric models to provide causally grounded supervision; (2) a\nmodel-agnostic graph transformation approach to capture both intra- and\ninter-triple interactions, thereby enhancing representational capacity; and (3)\na path-based reasoning strategy that facilitates learning from the injected\nrational knowledge, and supports downstream reasoner through structured inputs.\nEmpirically, RAPL outperforms state-of-the-art methods by $2.66\\%-20.34\\%$, and\nsignificantly reduces the performance gap between smaller and more powerful\nLLM-based reasoners, as well as the gap under cross-dataset settings,\nhighlighting its superior retrieval capability and generalizability. Codes are\navailable at: https://github.com/tianyao-aka/RAPL.", "AI": {"tldr": "This paper presents RAPL, a novel framework for improving graph retrieval in knowledge graph question answering (KGQA) by addressing challenges in generalization and interpretability using a two-stage labeling strategy, model-agnostic graph transformation, and path-based reasoning.", "motivation": "Large Language Models (LLMs) often struggle with outdated knowledge and hallucinations, and existing Retrieval-Augmented Generation (RAG) methods primarily utilize unstructured text, affecting interpretability and reasoning.", "method": "RAPL incorporates a two-stage labeling strategy that merges heuristic signals with parametric models, a model-agnostic graph transformation approach for capturing interactions, and employs a path-based reasoning strategy for enhanced learning.", "result": "Empirical results show that RAPL outperforms state-of-the-art methods by 2.66%-20.34% and reduces the performance gap for smaller LLMs in cross-dataset settings, indicating improved retrieval capabilities and generalizability.", "conclusion": "RAPL presents a more effective method for graph retrieval in KGQA, enhancing the performance of LLMs by providing structured reasoning and improved interpretability.", "key_contributions": ["Introduction of a two-stage labeling strategy combining heuristic signals and parametric models.", "A model-agnostic approach to graph transformation for enhanced representational capacity.", "Development of a path-based reasoning strategy that supports structured input for downstream reasoning."], "limitations": "", "keywords": ["Large Language Models", "Knowledge Graphs", "Retrieval-Augmented Generation", "Question Answering", "Graph Retrieval"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2506.09657", "pdf": "https://arxiv.org/pdf/2506.09657.pdf", "abs": "https://arxiv.org/abs/2506.09657", "title": "Bridging the Gap Between Open-Source and Proprietary LLMs in Table QA", "authors": ["Nikolas Evkarpidi", "Elena Tutubalina"], "categories": ["cs.CL"], "comment": "Accepted for publication at the 19th International Workshop on\n  Semantic Evaluation (SemEval-2025), to be held in conjunction with ACL 2025.\n  15 pages, 5 figures", "summary": "This paper presents a system developed for SemEval 2025 Task 8: Question\nAnswering (QA) over tabular data. Our approach integrates several key\ncomponents: text-to-SQL and text-to-code generation modules, a self-correction\nmechanism, and a retrieval-augmented generation (RAG). Additionally, it\nincludes an end-to-end (E2E) module, all orchestrated by a large language model\n(LLM). Through ablation studies, we analyzed the effects of different parts of\nour pipeline and identified the challenges that are still present in this\nfield. During the evaluation phase of the competition, our solution achieved an\naccuracy of 80%, resulting in a top-13 ranking among the 38 participating\nteams. Our pipeline demonstrates a significant improvement in accuracy for\nopen-source models and achieves a performance comparable to proprietary LLMs in\nQA tasks over tables. The code is available at GitHub repository.", "AI": {"tldr": "This paper presents a system for Question Answering over tabular data, achieving 80% accuracy and ranking top-13 in SemEval 2025.", "motivation": "To improve Question Answering (QA) accuracy over tabular data using advanced techniques.", "method": "The system integrates text-to-SQL, text-to-code generation modules, a self-correction mechanism, and retrieval-augmented generation, all orchestrated by a large language model.", "result": "Achieved an accuracy of 80% during evaluation, ranking top-13 out of 38 teams in the competition.", "conclusion": "The developed pipeline shows significant improvements for open-source models and matches proprietary LLM performance in QA tasks.", "key_contributions": ["Integration of diverse components for QA over tables", "High accuracy achieved with open-source models", "Availability of code for further research"], "limitations": "Challenges in the field of QA over tabular data were identified.", "keywords": ["Question Answering", "tabular data", "large language models", "RAG", "NLP"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.09669", "pdf": "https://arxiv.org/pdf/2506.09669.pdf", "abs": "https://arxiv.org/abs/2506.09669", "title": "Query-Level Uncertainty in Large Language Models", "authors": ["Lihu Chen", "Gaël Varoquaux"], "categories": ["cs.CL"], "comment": "In Progress", "summary": "It is important for Large Language Models to be aware of the boundary of\ntheir knowledge, the mechanism of identifying known and unknown queries. This\ntype of awareness can help models perform adaptive inference, such as invoking\nRAG, engaging in slow and deep thinking, or adopting the abstention mechanism,\nwhich is beneficial to the development of efficient and trustworthy AI. In this\nwork, we propose a method to detect knowledge boundaries via Query-Level\nUncertainty, which aims to determine if the model is able to address a given\nquery without generating any tokens. To this end, we introduce a novel and\ntraining-free method called \\emph{Internal Confidence}, which leverages\nself-evaluations across layers and tokens. Empirical results on both factual QA\nand mathematical reasoning tasks demonstrate that our internal confidence can\noutperform several baselines. Furthermore, we showcase that our proposed method\ncan be used for efficient RAG and model cascading, which is able to reduce\ninference costs while maintaining performance.", "AI": {"tldr": "This paper proposes a novel method called Internal Confidence to detect knowledge boundaries in Large Language Models by assessing query-level uncertainty, aiming to improve adaptive inference and reduce costs in RAG processes.", "motivation": "To enhance the reliability and efficiency of Large Language Models by making them aware of their knowledge boundaries and improving inference methods.", "method": "The authors introduce Internal Confidence, a training-free approach that evaluates self-confidence across different layers and tokens to determine if a model can respond effectively to a query.", "result": "Empirical results indicate that Internal Confidence outperforms several established baselines in both factual question answering and mathematical reasoning tasks.", "conclusion": "The method offers a viable solution for efficient retrieval-augmented generation and can help reduce inference costs while preserving model performance.", "key_contributions": ["Introduction of the Internal Confidence method for query-level uncertainty detection", "Demonstration of enhanced performance in factual QA and mathematical reasoning", "Application of the method for efficient RAG and model cascading to lower inference costs."], "limitations": "", "keywords": ["Large Language Models", "Knowledge Boundary Detection", "Internal Confidence", "Adaptive Inference", "Recovery-Augmented Generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.09672", "pdf": "https://arxiv.org/pdf/2506.09672.pdf", "abs": "https://arxiv.org/abs/2506.09672", "title": "Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for Unstructured Data", "authors": ["Hao Xiong", "Chuanyuan Tan", "Wenliang Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Unstructured Knowledge Editing (UKE) is crucial for updating the relevant\nknowledge of large language models (LLMs). It focuses on unstructured inputs,\nsuch as long or free-form texts, which are common forms of real-world\nknowledge. Although previous studies have proposed effective methods and tested\nthem, some issues exist: (1) Lack of Locality evaluation for UKE, and (2)\nAbnormal failure of fine-tuning (FT) based methods for UKE. To address these\nissues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by\nextending two existing UKE datasets with locality test data from the\nunstructured and structured views. This enables a systematic evaluation of the\nLocality of post-edited models. Furthermore, we identify four factors that may\naffect the performance of FT-based methods. Based on these factors, we conduct\nexperiments to determine how the well-performing FT-based methods should be\ntrained for the UKE task, providing a training recipe for future research. Our\nexperimental results indicate that the FT-based method with the optimal setting\n(FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art\n(SOTA). In batch editing scenarios, FT-UKE shows strong performance as well,\nwith its advantage over SOTA methods increasing as the batch size grows,\nexpanding the average metric lead from +6.78% to +10.80%", "AI": {"tldr": "This paper addresses issues in Unstructured Knowledge Editing (UKE) for large language models (LLMs) by introducing new evaluation datasets and optimizing fine-tuning methods, resulting in improved performance.", "motivation": "To enhance Unstructured Knowledge Editing (UKE) for large language models by addressing locality evaluation issues and the shortcomings of fine-tuning (FT) methods.", "method": "The authors constructed two evaluation datasets, UnKEBench-Loc and AKEW-Loc, and conducted experiments to determine optimal training settings for FT-based methods in UKE tasks.", "result": "FT-UKE, the optimized fine-tuning method, outperformed the state-of-the-art methods, with performance improvements increasing with larger batch sizes (up to +10.80%).", "conclusion": "The paper provides a systematic evaluation of UKE and a training recipe for fine-tuning methods, suggesting significant performance gains and directions for future research.", "key_contributions": ["Introduction of evaluation datasets for UKE", "Identification of factors influencing FT-based methods", "Demonstration of FT-UKE's superior performance over SOTA methods"], "limitations": "", "keywords": ["Unstructured Knowledge Editing", "Fine-tuning", "Large Language Models", "Evaluation Datasets", "Performance Optimization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.09684", "pdf": "https://arxiv.org/pdf/2506.09684.pdf", "abs": "https://arxiv.org/abs/2506.09684", "title": "Inv-Entropy: A Fully Probabilistic Framework for Uncertainty Quantification in Language Models", "authors": ["Haoyi Song", "Ruihan Ji", "Naichen Shi", "Fan Lai", "Raed Al Kontar"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have transformed natural language processing,\nbut their reliable deployment requires effective uncertainty quantification\n(UQ). Existing UQ methods are often heuristic and lack a probabilistic\nfoundation. This paper begins by providing a theoretical justification for the\nrole of perturbations in UQ for LLMs. We then introduce a dual random walk\nperspective, modeling input-output pairs as two Markov chains with transition\nprobabilities defined by semantic similarity. Building on this, we propose a\nfully probabilistic framework based on an inverse model, which quantifies\nuncertainty by evaluating the diversity of the input space conditioned on a\ngiven output through systematic perturbations. Within this framework, we define\na new uncertainty measure, Inv-Entropy. A key strength of our framework is its\nflexibility: it supports various definitions of uncertainty measures,\nembeddings, perturbation strategies, and similarity metrics. We also propose\nGAAP, a perturbation algorithm based on genetic algorithms, which enhances the\ndiversity of sampled inputs. In addition, we introduce a new evaluation metric,\nTemperature Sensitivity of Uncertainty (TSU), which directly assesses\nuncertainty without relying on correctness as a proxy. Extensive experiments\ndemonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code\nto reproduce the results can be found at\nhttps://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs.", "AI": {"tldr": "This paper presents a theoretical framework for uncertainty quantification in large language models (LLMs) using perturbations, introducing a new measure called Inv-Entropy and a genetic algorithm-based perturbation algorithm.", "motivation": "Effective uncertainty quantification (UQ) is crucial for the reliable deployment of large language models, yet existing methods often lack a strong probabilistic foundation.", "method": "The paper proposes a dual random walk model for input-output pairs, utilizing Markov chains to define transition probabilities based on semantic similarity, and introduces an inverse model to evaluate input space diversity for quantifying uncertainty.", "result": "Extensive experiments show that the proposed measure Inv-Entropy outperforms traditional semantic uncertainty quantification methods.", "conclusion": "The framework allows for flexible definitions of uncertainty, embedding strategies, and evaluation metrics, significantly enhancing the robustness of uncertainty quantification in LLMs.", "key_contributions": ["Introduced a probabilistic framework for uncertainty quantification in LLMs based on perturbations.", "Developed a new uncertainty measure called Inv-Entropy.", "Proposed GAAP, a genetic algorithm-based perturbation method to improve input diversity."], "limitations": "", "keywords": ["large language models", "uncertainty quantification", "inv-entropy", "Markov chains", "genetic algorithms"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2506.09790", "pdf": "https://arxiv.org/pdf/2506.09790.pdf", "abs": "https://arxiv.org/abs/2506.09790", "title": "ComfyUI-R1: Exploring Reasoning Models for Workflow Generation", "authors": ["Zhenran Xu", "Yiyu Wang", "Xue Yang", "Longyue Wang", "Weihua Luo", "Kaifu Zhang", "Baotian Hu", "Min Zhang"], "categories": ["cs.CL", "cs.CV", "cs.SE"], "comment": "Work in progress. Try it out in ComfyUI-Copilot\n  https://github.com/AIDC-AI/ComfyUI-Copilot", "summary": "AI-generated content has evolved from monolithic models to modular workflows,\nparticularly on platforms like ComfyUI, enabling customization in creative\npipelines. However, crafting effective workflows requires great expertise to\norchestrate numerous specialized components, presenting a steep learning curve\nfor users. To address this challenge, we introduce ComfyUI-R1, the first large\nreasoning model for automated workflow generation. Starting with our curated\ndataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning\ndata, including node selection, workflow planning, and code-level workflow\nrepresentation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT\nfine-tuning for cold start, adapting models to the ComfyUI domain; (2)\nreinforcement learning for incentivizing reasoning capability, guided by a\nfine-grained rule-metric hybrid reward, ensuring format validity, structural\nintegrity, and node-level fidelity. Experiments show that our 7B-parameter\nmodel achieves a 97\\% format validity rate, along with high pass rate,\nnode-level and graph-level F1 scores, significantly surpassing prior\nstate-of-the-art methods that employ leading closed-source models such as\nGPT-4o and Claude series. Further analysis highlights the critical role of the\nreasoning process and the advantage of transforming workflows into code.\nQualitative comparison reveals our strength in synthesizing intricate workflows\nwith diverse nodes, underscoring the potential of long CoT reasoning in AI art\ncreation.", "AI": {"tldr": "Introduction of ComfyUI-R1, a large reasoning model for automated workflow generation targeting AI-generated content customization.", "motivation": "To simplify the crafting of effective AI workflows which presents a steep learning curve for users.", "method": "Two-stage training framework consisting of CoT fine-tuning and reinforcement learning guided by a hybrid reward for workflow validation and integrity.", "result": "ComfyUI-R1 achieves a 97% format validity rate, outperforming prior models in pass rates and F1 scores for workflow performance.", "conclusion": "The model underscores the potential of long chain-of-thought reasoning in synthesizing complex workflows for AI art creation.", "key_contributions": ["Introduction of the first large reasoning model for automated workflow generation (ComfyUI-R1).", "Use of a two-stage training framework combining CoT fine-tuning and reinforcement learning.", "Demonstration of significant performance improvements over state-of-the-art models."], "limitations": "", "keywords": ["workflow generation", "chain-of-thought reasoning", "AI art", "reinforcement learning", "modular AI workflows"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2506.09796", "pdf": "https://arxiv.org/pdf/2506.09796.pdf", "abs": "https://arxiv.org/abs/2506.09796", "title": "Do LLMs Give Psychometrically Plausible Responses in Educational Assessments?", "authors": ["Andreas Säuberli", "Diego Frassinelli", "Barbara Plank"], "categories": ["cs.CL"], "comment": "Accepted for publication at the 20th Workshop on Innovative Use of\n  NLP for Building Educational Applications (BEA) at ACL 2025", "summary": "Knowing how test takers answer items in educational assessments is essential\nfor test development, to evaluate item quality, and to improve test validity.\nHowever, this process usually requires extensive pilot studies with human\nparticipants. If large language models (LLMs) exhibit human-like response\nbehavior to test items, this could open up the possibility of using them as\npilot participants to accelerate test development. In this paper, we evaluate\nthe human-likeness or psychometric plausibility of responses from 18\ninstruction-tuned LLMs with two publicly available datasets of multiple-choice\ntest items across three subjects: reading, U.S. history, and economics. Our\nmethodology builds on two theoretical frameworks from psychometrics which are\ncommonly used in educational assessment, classical test theory and item\nresponse theory. The results show that while larger models are excessively\nconfident, their response distributions can be more human-like when calibrated\nwith temperature scaling. In addition, we find that LLMs tend to correlate\nbetter with humans in reading comprehension items compared to other subjects.\nHowever, the correlations are not very strong overall, indicating that LLMs\nshould not be used for piloting educational assessments in a zero-shot setting.", "AI": {"tldr": "This paper evaluates the suitability of large language models (LLMs) as pilot participants for educational test development by examining their response behavior to multiple-choice items.", "motivation": "The need for efficient test development in education by potentially using LLMs to simulate human responses to evaluate item quality and test validity.", "method": "Evaluation of 18 instruction-tuned LLMs against two datasets of multiple-choice test items across reading, U.S. history, and economics, using classical test theory and item response theory frameworks.", "result": "Larger models exhibit excessive confidence but display more human-like response distributions when calibrated. Correlation with human responses is stronger in reading comprehension, but overall correlations are weak.", "conclusion": "LLMs are not suitable for piloting educational assessments in a zero-shot setting despite some similarity in response distribution.", "key_contributions": ["Demonstrated the potential use of LLMs in educational test development", "Showed the effects of temperature scaling on model confidence", "Highlighted differential performance across subjects with respect to human correlation"], "limitations": "The correlations between LLMs and human responses are weak overall, indicating limitations in their use for test piloting.", "keywords": ["Large Language Models", "Educational Assessments", "Item Response Theory", "Test Validity", "Psychometrics"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.09820", "pdf": "https://arxiv.org/pdf/2506.09820.pdf", "abs": "https://arxiv.org/abs/2506.09820", "title": "CoRT: Code-integrated Reasoning within Thinking", "authors": ["Chengpeng Li", "Zhengyang Tang", "Ziniu Li", "Mingfeng Xue", "Keqin Bao", "Tian Ding", "Ruoyu Sun", "Benyou Wang", "Xiang Wang", "Junyang Lin", "Dayiheng Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "work in progress", "summary": "Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable\nprogress in natural language reasoning with long chain-of-thought (CoT), yet\nthey remain inefficient or inaccurate when handling complex mathematical\noperations. Addressing these limitations through computational tools (e.g.,\ncomputation libraries and symbolic solvers) is promising, but it introduces a\ntechnical challenge: Code Interpreter (CI) brings external knowledge beyond the\nmodel's internal text representations, thus the direct combination is not\nefficient. This paper introduces CoRT, a post-training framework for teaching\nLRMs to leverage CI effectively and efficiently. As a first step, we address\nthe data scarcity issue by synthesizing code-integrated reasoning data through\nHint-Engineering, which strategically inserts different hints at appropriate\npositions to optimize LRM-CI interaction. We manually create 30 high-quality\nsamples, upon which we post-train models ranging from 1.5B to 32B parameters,\nwith supervised fine-tuning, rejection fine-tuning and reinforcement learning.\nOur experimental results demonstrate that Hint-Engineering models achieve 4\\%\nand 8\\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and\nDeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging\nmathematical reasoning datasets. Furthermore, Hint-Engineering models use about\n30\\% fewer tokens for the 32B model and 50\\% fewer tokens for the 1.5B model\ncompared with the natural language models. The models and code are available at\nhttps://github.com/ChengpengLi1003/CoRT.", "AI": {"tldr": "This paper presents CoRT, a post-training framework that enhances Large Reasoning Models' efficiency in mathematical reasoning by leveraging Code Interpreters through a novel data synthesis method called Hint-Engineering.", "motivation": "To improve the efficiency and accuracy of Large Reasoning Models in complex mathematical operations, addressing limitations when using external computational tools with these models.", "method": "The authors develop CoRT, which synthesizes code-integrated reasoning data via Hint-Engineering, strategically inserting hints for better model-compiler interaction, and post-train LRM models ranging from 1.5B to 32B parameters.", "result": "Models enhanced by Hint-Engineering show 4% and 8% absolute improvements on mathematical reasoning datasets, with significantly reduced token usage during processing.", "conclusion": "The CoRT framework effectively enhances LRMs' capabilities in mathematical reasoning, making them more efficient in handling complex tasks by integrating external computational knowledge.", "key_contributions": ["Introduction of CoRT framework for post-training LRMs", "Development of Hint-Engineering for synthesizing effective reasoning data", "Demonstrated improvements in efficiency and accuracy on mathematical reasoning tasks."], "limitations": "This work is a progress report and may require further validation and testing.", "keywords": ["Large Reasoning Models", "Mathematical Reasoning", "Hint-Engineering"], "importance_score": 8, "read_time_minutes": 7}}
{"id": "2506.09827", "pdf": "https://arxiv.org/pdf/2506.09827.pdf", "abs": "https://arxiv.org/abs/2506.09827", "title": "EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection", "authors": ["Christoph Schuhmann", "Robert Kaczmarczyk", "Gollam Rabby", "Felix Friedrich", "Maurice Kraus", "Kourosh Nadi", "Huu Nguyen", "Kristian Kersting", "Sören Auer"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advancement of text-to-speech and audio generation models necessitates\nrobust benchmarks for evaluating the emotional understanding capabilities of AI\nsystems. Current speech emotion recognition (SER) datasets often exhibit\nlimitations in emotional granularity, privacy concerns, or reliance on acted\nportrayals. This paper introduces EmoNet-Voice, a new resource for speech\nemotion detection, which includes EmoNet-Voice Big, a large-scale pre-training\ndataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,\nand 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human\nexpert annotations. EmoNet-Voice is designed to evaluate SER models on a\nfine-grained spectrum of 40 emotion categories with different levels of\nintensities. Leveraging state-of-the-art voice generation, we curated synthetic\naudio snippets simulating actors portraying scenes designed to evoke specific\nemotions. Crucially, we conducted rigorous validation by psychology experts who\nassigned perceived intensity labels. This synthetic, privacy-preserving\napproach allows for the inclusion of sensitive emotional states often absent in\nexisting datasets. Lastly, we introduce Empathic Insight Voice models that set\na new standard in speech emotion recognition with high agreement with human\nexperts. Our evaluations across the current model landscape exhibit valuable\nfindings, such as high-arousal emotions like anger being much easier to detect\nthan low-arousal states like concentration.", "AI": {"tldr": "EmoNet-Voice is a new resource for speech emotion detection featuring a large-scale pre-training dataset and a benchmark dataset, both designed to advance the capabilities of speech emotion recognition models.", "motivation": "The need for robust benchmarks in text-to-speech and audio generation models to evaluate emotional understanding due to limitations in current SER datasets.", "method": "Introduced EmoNet-Voice, including EmoNet-Voice Big with over 4,500 hours of speech across 11 voices and 40 emotions, and EmoNet-Voice Bench with expert annotations for evaluating SER models.", "result": "EmoNet-Voice sets a new standard in speech emotion recognition, showing high agreement with human experts and revealing that high-arousal emotions are easier to detect than low-arousal states.", "conclusion": "The synthetic, privacy-preserving nature of EmoNet-Voice allows for the assessment of sensitive emotional states not captured by existing datasets, with validated results from psychology experts.", "key_contributions": ["Introduction of EmoNet-Voice, a comprehensive resource for SER evaluation", "Development of a large-scale dataset featuring diverse emotions and languages", "Empathic Insight Voice models improve SER accuracy with expert agreement"], "limitations": "", "keywords": ["speech emotion recognition", "EmoNet-Voice", "synthetic audio", "emotional understanding", "benchmark dataset"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.09833", "pdf": "https://arxiv.org/pdf/2506.09833.pdf", "abs": "https://arxiv.org/abs/2506.09833", "title": "Error-Guided Pose Augmentation: Enhancing Rehabilitation Exercise Assessment through Targeted Data Generation", "authors": ["Omar Sherif", "Ali Hamdi"], "categories": ["cs.CL", "I.2.1"], "comment": "6 pages, 1 figure. To appear in Intelligent Methods, Systems, and\n  Applications 2025", "summary": "Effective rehabilitation assessment is essential for monitoring patient\nprogress, particularly in home-based settings. Existing systems often face\nchallenges such as data imbalance and difficulty detecting subtle movement\nerrors. This paper introduces Error-Guided Pose Augmentation (EGPA), a method\nthat generates synthetic skeleton data by simulating clinically relevant\nmovement mistakes. Unlike standard augmentation techniques, EGPA targets\nbiomechanical errors observed in rehabilitation. Combined with an\nattention-based graph convolutional network, EGPA improves performance across\nmultiple evaluation metrics. Experiments demonstrate reductions in mean\nabsolute error of up to 27.6 percent and gains in error classification accuracy\nof 45.8 percent. Attention visualizations show that the model learns to focus\non clinically significant joints and movement phases, enhancing both accuracy\nand interpretability. EGPA offers a promising approach for improving automated\nmovement quality assessment in both clinical and home-based rehabilitation\ncontexts.", "AI": {"tldr": "The paper presents Error-Guided Pose Augmentation (EGPA), a method that enhances rehabilitation assessment by generating synthetic skeleton data targeting biomechanical errors, leading to improvements in error detection accuracy.", "motivation": "To address the challenges faced in monitoring patient progress in rehabilitation, particularly in home-based settings, where existing systems struggle with data imbalance and subtle movement errors.", "method": "The method involves generating synthetic skeleton data through Error-Guided Pose Augmentation (EGPA), paired with an attention-based graph convolutional network to improve movement quality assessment performance.", "result": "The approach yields reductions in mean absolute error by up to 27.6% and improvements in error classification accuracy by 45.8%.", "conclusion": "EGPA enhances both the accuracy and interpretability of automated movement quality assessment in clinical and home rehabilitation contexts, focusing on clinically significant movements.", "key_contributions": ["Introduction of Error-Guided Pose Augmentation (EGPA) for rehabilitation assessment", "Demonstrated significant performance improvements in detecting biomechanical errors", "Enhanced interpretability through attention visualizations of significant joint movements."], "limitations": "", "keywords": ["Rehabilitation", "Pose Augmentation", "Graph Convolutional Network", "Movement Quality Assessment", "Biomechanical Errors"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.09847", "pdf": "https://arxiv.org/pdf/2506.09847.pdf", "abs": "https://arxiv.org/abs/2506.09847", "title": "Dataset of News Articles with Provenance Metadata for Media Relevance Assessment", "authors": ["Tomas Peterka", "Matyas Bohacek"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.CY"], "comment": null, "summary": "Out-of-context and misattributed imagery is the leading form of media\nmanipulation in today's misinformation and disinformation landscape. The\nexisting methods attempting to detect this practice often only consider whether\nthe semantics of the imagery corresponds to the text narrative, missing\nmanipulation so long as the depicted objects or scenes somewhat correspond to\nthe narrative at hand. To tackle this, we introduce News Media Provenance\nDataset, a dataset of news articles with provenance-tagged images. We formulate\ntwo tasks on this dataset, location of origin relevance (LOR) and date and time\nof origin relevance (DTOR), and present baseline results on six large language\nmodels (LLMs). We identify that, while the zero-shot performance on LOR is\npromising, the performance on DTOR hinders, leaving room for specialized\narchitectures and future work.", "AI": {"tldr": "This paper introduces a dataset aimed at addressing media manipulation by assessing the relevance of images to news narratives based on origin location and time.", "motivation": "To improve detection of out-of-context and misattributed imagery in misinformation by considering the provenance of images in news articles.", "method": "The authors created the News Media Provenance Dataset and formulated two tasks: location of origin relevance (LOR) and date and time of origin relevance (DTOR), evaluating six large language models (LLMs) as baselines.", "result": "The study shows promising zero-shot performance for LOR, but identifies significant shortcomings for DTOR, pointing to the need for specialized architectures.", "conclusion": "The findings highlight the potential of using provenance-tagged images to enhance the detection of media manipulation, while also indicating areas for further research.", "key_contributions": ["Introduction of the News Media Provenance Dataset", "Formulation of LOR and DTOR tasks for image relevance assessment", "Baseline evaluations on large language models (LLMs)"], "limitations": "The performance of existing LLMs on the DTOR task remains inadequate, suggesting a need for further exploration.", "keywords": ["media manipulation", "provenance", "misinformation", "datasets", "large language models"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2506.09853", "pdf": "https://arxiv.org/pdf/2506.09853.pdf", "abs": "https://arxiv.org/abs/2506.09853", "title": "Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning", "authors": ["Xiangning Yu", "Zhuohan Wang", "Linyi Yang", "Haoxuan Li", "Anjie Liu", "Xiao Xue", "Jun Wang", "Mengyue Yang"], "categories": ["cs.CL", "cs.AI", "math.ST", "stat.ME", "stat.TH"], "comment": null, "summary": "Chain-of-Thought (CoT) prompting plays an indispensable role in endowing\nlarge language models (LLMs) with complex reasoning capabilities. However, CoT\ncurrently faces two fundamental challenges: (1) Sufficiency, which ensures that\nthe generated intermediate inference steps comprehensively cover and\nsubstantiate the final conclusion; and (2) Necessity, which identifies the\ninference steps that are truly indispensable for the soundness of the resulting\nanswer. We propose a causal framework that characterizes CoT reasoning through\nthe dual lenses of sufficiency and necessity. Incorporating causal Probability\nof Sufficiency and Necessity allows us not only to determine which steps are\nlogically sufficient or necessary to the prediction outcome, but also to\nquantify their actual influence on the final reasoning outcome under different\nintervention scenarios, thereby enabling the automated addition of missing\nsteps and the pruning of redundant ones. Extensive experimental results on\nvarious mathematical and commonsense reasoning benchmarks confirm substantial\nimprovements in reasoning efficiency and reduced token usage without\nsacrificing accuracy. Our work provides a promising direction for improving LLM\nreasoning performance and cost-effectiveness.", "AI": {"tldr": "The paper proposes a causal framework for improving Chain-of-Thought (CoT) prompting in LLMs by addressing sufficiency and necessity of inference steps, showing improved reasoning efficiency and reduced token usage.", "motivation": "To enhance the reasoning capabilities of LLMs, addressing the challenges of sufficiency and necessity in CoT prompting is essential.", "method": "A causal framework is developed to characterize CoT reasoning, incorporating causal Probability of Sufficiency and Necessity to identify and quantify the importance of inference steps.", "result": "Extensive experiments demonstrate improvements in reasoning efficiency and reduced token usage on mathematical and commonsense reasoning tasks without sacrificing accuracy.", "conclusion": "The proposed framework offers a valuable approach to improve LLM reasoning performance and cost-effectiveness by automating the addition of necessary steps and pruning redundant ones.", "key_contributions": ["Introduction of a causal framework for CoT reasoning", "Development of causal Probability of Sufficiency and Necessity", "Demonstration of improved reasoning efficiency and reduced token usage"], "limitations": "", "keywords": ["Chain-of-Thought", "large language models", "reasoning efficiency", "sufficiency", "necessity"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.09886", "pdf": "https://arxiv.org/pdf/2506.09886.pdf", "abs": "https://arxiv.org/abs/2506.09886", "title": "Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs", "authors": ["Rodion Oblovatny", "Alexandra Bazarova", "Alexey Zaytsev"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present a novel approach for detecting hallucinations in large language\nmodels (LLMs) by analyzing the probabilistic divergence between prompt and\nresponse hidden-state distributions. Counterintuitively, we find that\nhallucinated responses exhibit smaller deviations from their prompts compared\nto grounded responses, suggesting that hallucinations often arise from\nsuperficial rephrasing rather than substantive reasoning. Leveraging this\ninsight, we propose a model-intrinsic detection method that uses distributional\ndistances as principled hallucination scores, eliminating the need for external\nknowledge or auxiliary models. To enhance sensitivity, we employ deep learnable\nkernels that automatically adapt to capture nuanced geometric differences\nbetween distributions. Our approach outperforms existing baselines,\ndemonstrating state-of-the-art performance on several benchmarks. The method\nremains competitive even without kernel training, offering a robust, scalable\nsolution for hallucination detection.", "AI": {"tldr": "Novel approach to detect hallucinations in LLMs by analyzing hidden-state distributions, revealing that hallucinations show smaller deviations from prompts.", "motivation": "Detecting hallucinations in large language models to improve their reliability and validity in applications.", "method": "Analyzes probabilistic divergence between prompt and response hidden-state distributions, using distributional distances as hallucination scores without external models.", "result": "Outperforms existing methods in detecting hallucinations, showing state-of-the-art performance on multiple benchmarks, even without kernel training.", "conclusion": "The proposed method offers a robust and scalable solution for hallucination detection in LLMs.", "key_contributions": ["Introduced a novel model-intrinsic detection method for hallucinations.", "Showed that hallucinated responses have smaller deviations from prompts.", "Developed deep learnable kernels for better capturing distributional differences."], "limitations": "", "keywords": ["hallucination detection", "large language models", "probabilistic divergence", "deep learning", "natural language processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.09890", "pdf": "https://arxiv.org/pdf/2506.09890.pdf", "abs": "https://arxiv.org/abs/2506.09890", "title": "The Emergence of Abstract Thought in Large Language Models Beyond Any Language", "authors": ["Yuxin Chen", "Yiran Zhao", "Yang Zhang", "An Zhang", "Kenji Kawaguchi", "Shafiq Joty", "Junnan Li", "Tat-Seng Chua", "Michael Qizhe Shieh", "Wenxuan Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) continue to advance, their capacity to\nfunction effectively across a diverse range of languages has shown marked\nimprovement. Preliminary studies observe that the hidden activations of LLMs\noften resemble English, even when responding to non-English prompts. This has\nled to the widespread assumption that LLMs may \"think\" in English. However,\nmore recent results showing strong multilingual performance, even surpassing\nEnglish performance on specific tasks in other languages, challenge this view.\nIn this work, we find that LLMs progressively develop a core language-agnostic\nparameter space-a remarkably small subset of parameters whose deactivation\nresults in significant performance degradation across all languages. This\ncompact yet critical set of parameters underlies the model's ability to\ngeneralize beyond individual languages, supporting the emergence of abstract\nthought that is not tied to any specific linguistic system. Specifically, we\nidentify language-related neurons-those are consistently activated during the\nprocessing of particular languages, and categorize them as either shared\n(active across multiple languages) or exclusive (specific to one). As LLMs\nundergo continued development over time, we observe a marked increase in both\nthe proportion and functional importance of shared neurons, while exclusive\nneurons progressively diminish in influence. These shared neurons constitute\nthe backbone of the core language-agnostic parameter space, supporting the\nemergence of abstract thought. Motivated by these insights, we propose\nneuron-specific training strategies tailored to LLMs' language-agnostic levels\nat different development stages. Experiments across diverse LLM families\nsupport our approach.", "AI": {"tldr": "This paper investigates the emergence of a language-agnostic parameter space in large language models (LLMs) and proposes neuron-specific training strategies.", "motivation": "To challenge the assumption that large language models 'think' in English and to explore their multilingual capabilities.", "method": "The study identifies language-related neurons in LLMs, categorizing them as shared or exclusive, and examines how their role evolves as models develop.", "result": "Findings show a significant increase in the proportion and importance of shared neurons while exclusive neurons diminish, establishing a core language-agnostic parameter space.", "conclusion": "The emergence of a language-agnostic parameter space has implications for model training, leading to the proposal of neuron-specific training strategies for LLMs.", "key_contributions": ["Identification of language-related neurons as shared or exclusive.", "Demonstration of the development of a language-agnostic parameter space in LLMs.", "Proposal of tailored training strategies based on neuron functionality."], "limitations": "", "keywords": ["large language models", "multilingual performance", "language-agnostic parameter space"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.09902", "pdf": "https://arxiv.org/pdf/2506.09902.pdf", "abs": "https://arxiv.org/abs/2506.09902", "title": "PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants", "authors": ["Zheng Zhao", "Clara Vania", "Subhradeep Kayal", "Naila Khan", "Shay B. Cohen", "Emine Yilmaz"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large language models (LLMs) have advanced conversational AI assistants.\nHowever, systematically evaluating how well these assistants apply\npersonalization--adapting to individual user preferences while completing\ntasks--remains challenging. Existing personalization benchmarks focus on\nchit-chat, non-conversational tasks, or narrow domains, failing to capture the\ncomplexities of personalized task-oriented assistance. To address this, we\nintroduce PersonaLens, a comprehensive benchmark for evaluating personalization\nin task-oriented AI assistants. Our benchmark features diverse user profiles\nequipped with rich preferences and interaction histories, along with two\nspecialized LLM-based agents: a user agent that engages in realistic\ntask-oriented dialogues with AI assistants, and a judge agent that employs the\nLLM-as-a-Judge paradigm to assess personalization, response quality, and task\nsuccess. Through extensive experiments with current LLM assistants across\ndiverse tasks, we reveal significant variability in their personalization\ncapabilities, providing crucial insights for advancing conversational AI\nsystems.", "AI": {"tldr": "PersonaLens is a benchmark for evaluating personalization in task-oriented AI assistants, revealing variability in LLMs' personalization capabilities.", "motivation": "There is a need for systematic evaluation of personalization in task-oriented AI assistants, as existing benchmarks do not adequately capture user preferences and interaction complexities.", "method": "Introduces a benchmark called PersonaLens that includes diverse user profiles, interaction histories, and two LLM-based agents (user agent and judge agent) to evaluate personalization and task performance.", "result": "Significant variability was found in the personalization capabilities of current LLM assistants across a variety of tasks.", "conclusion": "PersonaLens provides essential insights for improving the design of conversational AI systems by effectively evaluating personalized task-oriented assistance.", "key_contributions": ["Introduction of a comprehensive benchmark (PersonaLens) for personalization evaluation", "Inclusion of diverse user profiles and interaction histories", "Development of two LLM-based agents for realistic engagement and assessment"], "limitations": "", "keywords": ["personalization", "task-oriented assistants", "large language models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.09917", "pdf": "https://arxiv.org/pdf/2506.09917.pdf", "abs": "https://arxiv.org/abs/2506.09917", "title": "Aspect-Based Opinion Summarization with Argumentation Schemes", "authors": ["Wendi Zhou", "Ameer Saadat-Yazd", "Nadin Kokciyan"], "categories": ["cs.CL"], "comment": "Accepted by ArgMining 2025", "summary": "Reviews are valuable resources for customers making purchase decisions in\nonline shopping. However, it is impractical for customers to go over the vast\nnumber of reviews and manually conclude the prominent opinions, which prompts\nthe need for automated opinion summarization systems. Previous approaches,\neither extractive or abstractive, face challenges in automatically producing\ngrounded aspect-centric summaries. In this paper, we propose a novel\nsummarization system that not only captures predominant opinions from an aspect\nperspective with supporting evidence, but also adapts to varying domains\nwithout relying on a pre-defined set of aspects. Our proposed framework,\nASESUM, summarizes viewpoints relevant to the critical aspects of a product by\nextracting aspect-centric arguments and measuring their salience and validity.\nWe conduct experiments on a real-world dataset to demonstrate the superiority\nof our approach in capturing diverse perspectives of the original reviews\ncompared to new and existing methods.", "AI": {"tldr": "This paper presents ASESUM, a novel automated opinion summarization system that generates aspect-centric summaries from customer reviews without needing pre-defined aspects.", "motivation": "The increasing volume of online reviews makes it difficult for customers to synthesize key opinions, necessitating an automated approach to summarize and capture diverse perspectives.", "method": "ASESUM extracts aspect-centric arguments from product reviews and assesses their salience and validity to produce coherent summaries.", "result": "Experiments demonstrate that ASESUM outperforms existing summarization methods in capturing a wider range of opinions from reviews.", "conclusion": "The proposed ASESUM framework effectively summarizes customer opinions, making it suitable for various domains without the need for preset aspects.", "key_contributions": ["Introduction of ASESUM framework for opinion summarization", "Ability to adapt to various domains without predefined aspects", "Enhanced extraction and validation of aspect-centric arguments"], "limitations": "", "keywords": ["opinion summarization", "aspect-centric analysis", "text summarization", "machine learning", "natural language processing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.09942", "pdf": "https://arxiv.org/pdf/2506.09942.pdf", "abs": "https://arxiv.org/abs/2506.09942", "title": "VerIF: Verification Engineering for Reinforcement Learning in Instruction Following", "authors": ["Hao Peng", "Yunjia Qi", "Xiaozhi Wang", "Bin Xu", "Lei Hou", "Juanzi Li"], "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 8 figures", "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a key\ntechnique for enhancing large language models (LLMs), with verification\nengineering playing a central role. However, best practices for RL in\ninstruction following remain underexplored. In this work, we explore the\nverification challenge in RL for instruction following and propose VerIF, a\nverification method that combines rule-based code verification with LLM-based\nverification from a large reasoning model (e.g., QwQ-32B). To support this\napproach, we construct a high-quality instruction-following dataset,\nVerInstruct, containing approximately 22,000 instances with associated\nverification signals. We apply RL training with VerIF to two models, achieving\nsignificant improvements across several representative instruction-following\nbenchmarks. The trained models reach state-of-the-art performance among models\nof comparable size and generalize well to unseen constraints. We further\nobserve that their general capabilities remain unaffected, suggesting that RL\nwith VerIF can be integrated into existing RL recipes to enhance overall model\nperformance. We have released our datasets, codes, and models to facilitate\nfuture research at https://github.com/THU-KEG/VerIF.", "AI": {"tldr": "This paper introduces VerIF, a verification method for reinforcement learning in instruction following tasks, showing significant improvements in model performance.", "motivation": "Enhancing reinforcement learning techniques for instruction following to improve large language models using verification engineering.", "method": "The paper proposes VerIF, which combines rule-based code verification with LLM-based verification, applied to a newly constructed VerInstruct dataset for RL training.", "result": "The proposed VerIF method resulted in significant performance improvements in instruction-following benchmarks, achieving state-of-the-art results for models of similar size.", "conclusion": "Integrating VerIF into reinforcement learning recipes enhances model performance without compromising general capabilities; the developed datasets and resources are made publicly available.", "key_contributions": ["Introduction of VerIF for improving instruction following in RL", "Construction of VerInstruct dataset with 22,000 instances", "Demonstration of state-of-the-art performance in instruction-following tasks"], "limitations": "", "keywords": ["reinforcement learning", "verification", "instruction following", "large language models", "dataset"], "importance_score": 9, "read_time_minutes": 16}}
{"id": "2506.09944", "pdf": "https://arxiv.org/pdf/2506.09944.pdf", "abs": "https://arxiv.org/abs/2506.09944", "title": "Query-Focused Retrieval Heads Improve Long-Context Reasoning and Re-ranking", "authors": ["Wuwei Zhang", "Fangcong Yin", "Howard Yen", "Danqi Chen", "Xi Ye"], "categories": ["cs.CL"], "comment": null, "summary": "Recent work has identified retrieval heads (Wu et al., 2025b), a subset of\nattention heads responsible for retrieving salient information in long-context\nlanguage models (LMs), as measured by their copy-paste behavior in\nNeedle-in-a-Haystack tasks. In this paper, we introduce QRHEAD (Query-Focused\nRetrieval Head), an improved set of attention heads that enhance retrieval from\nlong context. We identify QRHEAD by aggregating attention scores with respect\nto the input query, using a handful of examples from real-world tasks (e.g.,\nlong-context QA). We further introduce QR- RETRIEVER, an efficient and\neffective retriever that uses the accumulated attention mass of QRHEAD as\nretrieval scores. We use QR- RETRIEVER for long-context reasoning by selecting\nthe most relevant parts with the highest retrieval scores. On multi-hop\nreasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains\nover full context and outperforms strong dense retrievers. We also evaluate\nQRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves\nstrong zero-shot performance, outperforming other LLM-based re-rankers such as\nRankGPT. Further analysis shows that both the querycontext attention scoring\nand task selection are crucial for identifying QRHEAD with strong downstream\nutility. Overall, our work contributes a general-purpose retriever and offers\ninterpretability insights into the long-context capabilities of LMs.", "AI": {"tldr": "This paper introduces QRHEAD, an improved set of attention heads for long-context language models, and QR-RETRIEVER, an efficient retriever leveraging QRHEAD for enhanced retrieval in reasoning tasks, achieving notable performance gains.", "motivation": "The need to improve retrieval mechanisms in long-context language models for better performance in tasks like long-context reasoning and QA.", "method": "The authors define QRHEAD by aggregating attention scores focused on input queries, and present QR-RETRIEVER which utilizes these scores for effective retrieval.", "result": "QR-RETRIEVER exhibits over 10% performance improvements on multi-hop reasoning tasks and strong zero-shot performance on the BEIR benchmark, surpassing traditional dense retrievers and other LLM-based re-rankers.", "conclusion": "The study concludes that QRHEAD and QR-RETRIEVER significantly enhance long-context reasoning capabilities and provide interpretability insights for language models.", "key_contributions": ["Introduction of QRHEAD for enhanced retrieval in long-context models", "Development of QR-RETRIEVER that outperforms existing methods in multi-hop reasoning", "Interpretability insights into attention mechanisms for long-context tasks"], "limitations": "", "keywords": ["retrieval heads", "long-context models", "query-focused retrieval", "multi-hop reasoning", "interpretability"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.09967", "pdf": "https://arxiv.org/pdf/2506.09967.pdf", "abs": "https://arxiv.org/abs/2506.09967", "title": "Resa: Transparent Reasoning Models via SAEs", "authors": ["Shangshang Wang", "Julian Asilis", "Ömer Faruk Akgül", "Enes Burak Bilgin", "Ollie Liu", "Deqing Fu", "Willie Neiswanger"], "categories": ["cs.CL"], "comment": null, "summary": "How cost-effectively can we elicit strong reasoning in language models by\nleveraging their underlying representations? We answer this question with Resa,\na family of 1.5B reasoning models trained via a novel and efficient sparse\nautoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to\ncapture reasoning abilities from a source model, and then uses the trained SAE\nto guide a standard supervised fine-tuning process to elicit such abilities in\na target model, all using verified question-answer data without any reasoning\ntraces. Notably, when applied to certain base models before further RL\npost-training, SAE-Tuning retains >97% of its RL-trained counterpart's\nreasoning performance while reducing training costs by >2000x to roughly \\$1\nand training time by >450x to around 20 minutes. Furthermore, when applied to\nlightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning\nperformance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only\naround \\$1 additional cost. Surprisingly, the reasoning abilities extracted via\nSAEs are potentially both generalizable and modular. Generality means abilities\nextracted from one dataset still elevate performance on a larger and\noverlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math\ncan be attached to the R1-Distill model at test time, without any retraining,\nand yield comparable gains. Extensive ablations validate these findings and all\nartifacts are fully open-sourced.", "AI": {"tldr": "This paper presents Resa, a set of reasoning models utilizing a novel sparse autoencoder tuning (SAE-Tuning) method to enhance reasoning capabilities in language models cost-effectively and efficiently.", "motivation": "The goal is to elicit strong reasoning in language models while reducing training costs and time significantly.", "method": "The proposed SAE-Tuning first trains a sparse autoencoder to capture reasoning from a source model, which guides the fine-tuning of a target model using verified question-answer data without reasoning traces.", "result": "SAE-Tuning retains over 97% of the reasoning performance of RL-trained models while cutting training costs by over 2000x and time by 450x. It achieves notable reasoning performance metrics, such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 with minimal additional costs.", "conclusion": "The findings suggest that reasoning abilities extracted via SAE-Tuning are both generalizable and modular, enhancing model performance across datasets without retraining.", "key_contributions": ["Introduction of the sparse autoencoder tuning (SAE-Tuning) method for language models.", "Significant reduction in training costs and time while maintaining high reasoning performance.", "Evidence that reasoning abilities are generalizable and modular, applicable across different models without additional retraining."], "limitations": "", "keywords": ["reasoning models", "sparse autoencoder tuning", "language models", "cost-effective training", "modular reasoning"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.09975", "pdf": "https://arxiv.org/pdf/2506.09975.pdf", "abs": "https://arxiv.org/abs/2506.09975", "title": "When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text", "authors": ["Hillary Dawkins", "Kathleen C. Fraser", "Svetlana Kiritchenko"], "categories": ["cs.CL"], "comment": "to appear in ACL Findings", "summary": "Detecting AI-generated text is a difficult problem to begin with; detecting\nAI-generated text on social media is made even more difficult due to the short\ntext length and informal, idiosyncratic language of the internet. It is\nnonetheless important to tackle this problem, as social media represents a\nsignificant attack vector in online influence campaigns, which may be bolstered\nthrough the use of mass-produced AI-generated posts supporting (or opposing)\nparticular policies, decisions, or events. We approach this problem with the\nmindset and resources of a reasonably sophisticated threat actor, and create a\ndataset of 505,159 AI-generated social media posts from a combination of\nopen-source, closed-source, and fine-tuned LLMs, covering 11 different\ncontroversial topics. We show that while the posts can be detected under\ntypical research assumptions about knowledge of and access to the generating\nmodels, under the more realistic assumption that an attacker will not release\ntheir fine-tuned model to the public, detectability drops dramatically. This\nresult is confirmed with a human study. Ablation experiments highlight the\nvulnerability of various detection algorithms to fine-tuned LLMs. This result\nhas implications across all detection domains, since fine-tuning is a generally\napplicable and realistic LLM use case.", "AI": {"tldr": "The paper investigates the detection of AI-generated text on social media, presenting a dataset of 505,159 posts and analyzing the effectiveness of detection methods in realistic scenarios.", "motivation": "The need to detect AI-generated text on social media due to its potential use in online influence campaigns.", "method": "Creation of a dataset of AI-generated social media posts and testing various detection algorithms under different assumptions about model access.", "result": "Detection effectiveness drops significantly when fine-tuned models are not publicly accessible, confirmed by a human study.", "conclusion": "Detection algorithms are vulnerable to fine-tuned LLMs, which poses challenges in various detection domains.", "key_contributions": ["Dataset of 505,159 AI-generated posts", "Analysis of detection effectiveness under real-world scenarios", "Insights into detection algorithm vulnerabilities"], "limitations": "The study assumes specific conditions regarding access to generating models and may not generalize to all scenarios.", "keywords": ["AI-generated text", "social media", "detection algorithms", "fine-tuned models", "influence campaigns"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.09983", "pdf": "https://arxiv.org/pdf/2506.09983.pdf", "abs": "https://arxiv.org/abs/2506.09983", "title": "Step-by-step Instructions and a Simple Tabular Output Format Improve the Dependency Parsing Accuracy of LLMs", "authors": ["Hiroshi Matsuda", "Chunpeng Ma", "Masayuki Asahara"], "categories": ["cs.CL"], "comment": "9 pages, 2 figures, accepted for SyntaxFest 2025", "summary": "Recent advances in large language models (LLMs) have enabled impressive\nperformance in various tasks. However, standard prompting often struggles to\nproduce structurally valid and accurate outputs, especially in dependency\nparsing. We propose a novel step-by-step instruction strategy, where universal\npart-of-speech tagging precedes the prediction of syntactic heads and\ndependency labels, and a simplified CoNLL-U like output format, our method\nachieves state-of-the-art accuracy on Universal Dependencies datasets across 17\nlanguages without hallucination or contamination. We further show that\nmultilingual fine-tuning simultaneously improves cross-language generalization\nperformance. Our results highlight the effectiveness of explicit reasoning\nsteps in LLM-based parsing and offer a scalable, format-consistent alternative\nto bracket-based approaches.", "AI": {"tldr": "This paper presents a novel approach to dependency parsing using large language models, which improves accuracy and consistency in outputs through step-by-step instructions and universal part-of-speech tagging.", "motivation": "The paper addresses the limitations of standard prompting methods in producing valid and accurate outputs in dependency parsing tasks using large language models.", "method": "The authors propose a strategy that includes universal part-of-speech tagging followed by the prediction of syntactic heads and dependency labels, using a simplified CoNLL-U like output format.", "result": "The method achieves state-of-the-art accuracy on Universal Dependencies datasets across 17 languages, demonstrating improvements without issues of hallucination or contamination.", "conclusion": "The results indicate that explicit reasoning steps enhance LLM-based parsing, providing a scalable alternative to bracket-based approaches.", "key_contributions": ["Novel step-by-step instruction strategy for dependency parsing using LLMs", "State-of-the-art accuracy across multiple languages", "Evidence of improved cross-language generalization performance through multilingual fine-tuning."], "limitations": "", "keywords": ["Large Language Models", "Dependency Parsing", "Multilingual Fine-tuning", "Natural Language Processing", "Syntactic Heads"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.09992", "pdf": "https://arxiv.org/pdf/2506.09992.pdf", "abs": "https://arxiv.org/abs/2506.09992", "title": "Large Language Models for Toxic Language Detection in Low-Resource Balkan Languages", "authors": ["Amel Muminovic", "Amela Kadric Muminovic"], "categories": ["cs.CL"], "comment": "8 pages", "summary": "Online toxic language causes real harm, especially in regions with limited\nmoderation tools. In this study, we evaluate how large language models handle\ntoxic comments in Serbian, Croatian, and Bosnian, languages with limited\nlabeled data. We built and manually labeled a dataset of 4,500 YouTube and\nTikTok comments drawn from videos across diverse categories, including music,\npolitics, sports, modeling, influencer content, discussions of sexism, and\ngeneral topics. Four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude\n3 Opus) were tested in two modes: zero-shot and context-augmented. We measured\nprecision, recall, F1 score, accuracy and false positive rates. Including a\nshort context snippet raised recall by about 0.12 on average and improved F1\nscore by up to 0.10, though it sometimes increased false positives. The best\nbalance came from Gemini in context-augmented mode, reaching an F1 score of\n0.82 and accuracy of 0.82, while zero-shot GPT-4.1 led on precision and had the\nlowest false alarms. We show how adding minimal context can improve toxic\nlanguage detection in low-resource settings and suggest practical strategies\nsuch as improved prompt design and threshold calibration. These results show\nthat prompt design alone can yield meaningful gains in toxicity detection for\nunderserved Balkan language communities.", "AI": {"tldr": "This study evaluates large language models for detecting toxic language in Serbian, Croatian, and Bosnian by utilizing a manually labeled dataset of comments.", "motivation": "To address the real harm caused by online toxic language, especially in regions with limited moderation tools, by improving detection methods for Balkan languages.", "method": "We constructed a dataset of 4,500 YouTube and TikTok comments and tested four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude 3 Opus) under zero-shot and context-augmented conditions, measuring precision, recall, F1 score, accuracy, and false positive rates.", "result": "Gemini in context-augmented mode achieved an F1 score of 0.82 and accuracy of 0.82, while GPT-4.1 showed the best precision and lowest false alarms in zero-shot mode.", "conclusion": "Adding minimal context improves toxicity detection, suggesting strategies like improved prompt design and threshold calibration for better performance in low-resource settings.", "key_contributions": ["Evaluation of ML models for toxic comment detection in under-resourced Balkan languages.", "Demonstration of improved performance through context-augmented prompts.", "Strategies for enhancing toxicity detection in low-data scenarios."], "limitations": "Focus on limited languages and modalities; may not generalize to other languages or contexts.", "keywords": ["toxic language detection", "language models", "Balkan languages", "natural language processing", "context-augmented prompts"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.09996", "pdf": "https://arxiv.org/pdf/2506.09996.pdf", "abs": "https://arxiv.org/abs/2506.09996", "title": "From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring", "authors": ["Yang Li", "Qiang Sheng", "Yehan Yang", "Xueyao Zhang", "Juan Cao"], "categories": ["cs.CL", "cs.CY"], "comment": "22 pages, 7 figures, and 9 tables", "summary": "Though safety alignment has been applied to most large language models\n(LLMs), LLM service providers generally deploy a subsequent moderation as the\nexternal safety guardrail in real-world products. Existing moderators mainly\npractice a conventional full detection, which determines the harmfulness based\non the complete LLM output, causing high service latency. Recent works pay more\nattention to partial detection where moderators oversee the generation midway\nand early stop the output if harmfulness is detected, but they directly apply\nmoderators trained with the full detection paradigm to incomplete outputs,\nintroducing a training-inference gap that lowers the performance. In this\npaper, we explore how to form a data-and-model solution that natively supports\npartial detection. For the data, we construct FineHarm, a dataset consisting of\n29K prompt-response pairs with fine-grained annotations to provide reasonable\nsupervision for token-level training. Then, we propose the streaming content\nmonitor, which is trained with dual supervision of response- and token-level\nlabels and can follow the output stream of LLM to make a timely judgment of\nharmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is\ncomparable to full detection, by only seeing the first 18% of tokens in\nresponses on average. Moreover, the SCM can serve as a pseudo-harmfulness\nannotator for improving safety alignment and lead to a higher harmlessness\nscore than DPO.", "AI": {"tldr": "This paper introduces a novel approach for the moderation of large language models (LLMs) that focuses on partial detection of harmful outputs, utilizing a newly constructed dataset and a streaming content monitor to enhance safety without sacrificing performance.", "motivation": "To address the limitations of current moderation methods that rely on full detection, which causes high latency in LLM services.", "method": "Developed a dataset called FineHarm with 29K prompt-response pairs annotated for token-level training, and proposed a streaming content monitor (SCM) that uses dual supervision for timely harmfulness assessment during output generation.", "result": "The streaming content monitor achieves a macro F1 score of over 0.95 while evaluating only the first 18% of tokens in outputs, matching the performance of full detection methods.", "conclusion": "The SCM not only enhances moderation at lower latency but also serves as a tool to improve safety alignment and achieve higher harmlessness scores compared to existing methods.", "key_contributions": ["Introduction of FineHarm dataset for token-level training in harmfulness detection.", "Development of a streaming content monitor for partial detection during LLM output generation.", "Demonstration of comparable performance to full detection methods with reduced latency."], "limitations": "", "keywords": ["large language models", "moderation", "safety alignment", "harmfulness detection", "streaming content monitor"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2305.14725", "pdf": "https://arxiv.org/pdf/2305.14725.pdf", "abs": "https://arxiv.org/abs/2305.14725", "title": "AMELI: Enhancing Multimodal Entity Linking with Fine-Grained Attributes", "authors": ["Barry Menglong Yao", "Sijia Wang", "Yu Chen", "Qifan Wang", "Minqian Liu", "Zhiyang Xu", "Licheng Yu", "Lifu Huang"], "categories": ["cs.CL", "I.2.7"], "comment": "19 pages, 7 figures", "summary": "We propose attribute-aware multimodal entity linking, where the input\nconsists of a mention described with a text paragraph and images, and the goal\nis to predict the corresponding target entity from a multimodal knowledge base\n(KB) where each entity is also accompanied by a text description, visual\nimages, and a collection of attributes that present the meta-information of the\nentity in a structured format. To facilitate this research endeavor, we\nconstruct AMELI, encompassing a new multimodal entity linking benchmark dataset\nthat contains 16,735 mentions described in text and associated with 30,472\nimages, and a multimodal knowledge base that covers 34,690 entities along with\n177,873 entity images and 798,216 attributes. To establish baseline performance\non AMELI, we experiment with several state-of-the-art architectures for\nmultimodal entity linking and further propose a new approach that incorporates\nattributes of entities into disambiguation. Experimental results and extensive\nqualitative analysis demonstrate that extracting and understanding the\nattributes of mentions from their text descriptions and visual images play a\nvital role in multimodal entity linking. To the best of our knowledge, we are\nthe first to integrate attributes in the multimodal entity linking task. The\nprograms, model checkpoints, and the dataset are publicly available at\nhttps://github.com/VT-NLP/Ameli.", "AI": {"tldr": "This paper introduces an attribute-aware multimodal entity linking framework, utilizing a new benchmark dataset, AMELI, to enhance entity disambiguation by incorporating attributes from multimodal inputs.", "motivation": "The goal is to improve multimodal entity linking by incorporating textual and visual attributes, which are not typically included in contemporary approaches.", "method": "The authors constructed the AMELI dataset and knowledge base and tested several state-of-the-art architectures for entity linking while proposing a new method that integrates entity attributes into disambiguation tasks.", "result": "Experiments demonstrated that understanding attributes from text and images significantly enhances the accuracy of multimodal entity linking.", "conclusion": "Integrating attributes into the multimodal entity linking task is crucial, marking a novel contribution to the field, which has not been previously explored.", "key_contributions": ["Development of AMELI, a new multimodal entity linking benchmark dataset.", "First integration of attributes into multimodal entity linking tasks.", "Public availability of the dataset and model checkpoints for further research."], "limitations": "", "keywords": ["Multimodal Entity Linking", "Attribute Awareness", "Knowledge Base", "Natural Language Processing", "Computer Vision"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2402.16733", "pdf": "https://arxiv.org/pdf/2402.16733.pdf", "abs": "https://arxiv.org/abs/2402.16733", "title": "DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing", "authors": ["Haneul Yoo", "Jieun Han", "So-Yeon Ahn", "Alice Oh"], "categories": ["cs.CL", "cs.AI"], "comment": "To appear in ACL 2025. arXiv admin note: text overlap with\n  arXiv:2310.05191", "summary": "Automated essay scoring (AES) is a useful tool in English as a Foreign\nLanguage (EFL) writing education, offering real-time essay scores for students\nand instructors. However, previous AES models were trained on essays and scores\nirrelevant to the practical scenarios of EFL writing education and usually\nprovided a single holistic score due to the lack of appropriate datasets. In\nthis paper, we release DREsS, a large-scale, standard dataset for rubric-based\nautomated essay scoring with 48.9K samples in total. DREsS comprises three\nsub-datasets: DREsS_New, DREsS_Std., and DREsS_CASE. We collect DREsS_New, a\nreal-classroom dataset with 2.3K essays authored by EFL undergraduate students\nand scored by English education experts. We also standardize existing\nrubric-based essay scoring datasets as DREsS_Std. We suggest CASE, a\ncorruption-based augmentation strategy for essays, which generates 40.1K\nsynthetic samples of DREsS_CASE and improves the baseline results by 45.44%.\nDREsS will enable further research to provide a more accurate and practical AES\nsystem for EFL writing education.", "AI": {"tldr": "The paper introduces DREsS, a large-scale dataset for automated essay scoring (AES) designed for English as a Foreign Language (EFL) writing education, improving the practical application of AES in this domain.", "motivation": "To address the limitations of previous AES models that were not contextually relevant to EFL writing education and typically provided a singular holistic score.", "method": "The study presents DREsS, containing 48.9K samples across three sub-datasets, including a real-classroom dataset of essays scored by experts and an augmentation strategy to generate synthetic samples.", "result": "The introduction of DREsS and the proposed augmentation strategy result in a 45.44% improvement in baseline results for AES models.", "conclusion": "DREsS facilitates research aimed at developing more accurate and applicable AES systems in the context of EFL writing education.", "key_contributions": ["Release of the DREsS dataset with 48.9K samples for AES", "Inclusion of a real-classroom dataset with expert-scored essays", "Development of CASE, a novel augmentation strategy for dataset enhancement"], "limitations": "", "keywords": ["automated essay scoring", "English as a Foreign Language", "dataset", "augmented data", "machine learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2404.01129", "pdf": "https://arxiv.org/pdf/2404.01129.pdf", "abs": "https://arxiv.org/abs/2404.01129", "title": "Emphasising Structured Information: Integrating Abstract Meaning Representation into LLMs for Enhanced Open-Domain Dialogue Evaluation", "authors": ["Bohao Yang", "Kun Zhao", "Dong Liu", "Liang Zhan", "Chenghua Lin"], "categories": ["cs.CL"], "comment": null, "summary": "Automatic open-domain dialogue evaluation has attracted increasing attention,\nyet remains challenging due to the complexity of assessing response\nappropriateness. Traditional evaluation metrics, typically trained with true\npositive and randomly selected negative responses, tend to assign higher scores\nto responses that share greater content similarity with contexts. However,\nadversarial negative responses, despite possessing high lexical overlap with\ncontexts, can be semantically incongruous. Consequently, existing metrics\nstruggle to effectively evaluate such responses, resulting in low correlations\nwith human judgments. While recent studies have demonstrated the effectiveness\nof Large Language Models (LLMs) for open-domain dialogue evaluation, they still\nface challenges in handling adversarial negative examples. We propose a novel\nevaluation framework that integrates Abstract Meaning Representation (AMR)\nenhanced domain-specific language models (SLMs) with LLMs. Our SLMs explicitly\nincorporate AMR graph information through a gating mechanism for enhanced\nsemantic representation learning, while both SLM predictions and AMR knowledge\nare integrated into LLM prompts for robust evaluation. Extensive experiments on\nopen-domain dialogue evaluation tasks demonstrate the superiority of our method\ncompared to state-of-the-art baselines. Our comprehensive ablation studies\nreveal that AMR graph information contributes substantially more to performance\nimprovements. Our framework achieves strong correlations with human judgments\nacross multiple datasets, establishing a new benchmark for dialogue evaluation.\nOur code and data are publicly available.", "AI": {"tldr": "This paper proposes a novel evaluation framework for open-domain dialogue evaluation that combines Abstract Meaning Representation with domain-specific language models and large language models to effectively assess response appropriateness, especially in the presence of adversarial negative examples.", "motivation": "The complexity of evaluating response appropriateness in open-domain dialogue has led to ineffective traditional metrics, prompting the need for a more robust evaluation framework that can handle adversarial examples.", "method": "The proposed framework integrates Abstract Meaning Representation (AMR) enhanced domain-specific language models (SLMs) with large language models (LLMs), using a gating mechanism for semantic representation and incorporating AMR knowledge into LLM prompts.", "result": "Extensive experiments show that the new framework outperforms state-of-the-art baselines and achieves strong correlations with human judgments across various datasets.", "conclusion": "The proposed framework establishes a new benchmark for open-domain dialogue evaluation and demonstrates the importance of incorporating AMR graph information for improving performance.", "key_contributions": ["Integration of AMR into SLMs for enhanced semantic understanding", "A novel gating mechanism to improve dialogue evaluation metrics", "Demonstrated superior performance over existing state-of-the-art methods"], "limitations": "", "keywords": ["dialogue evaluation", "Large Language Models", "Abstract Meaning Representation", "semantic representation", "adversarial examples"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2405.14259", "pdf": "https://arxiv.org/pdf/2405.14259.pdf", "abs": "https://arxiv.org/abs/2405.14259", "title": "Let's Fuse Step by Step: A Generative Fusion Decoding Algorithm with LLMs for Robust and Instruction-Aware ASR and OCR", "authors": ["Chan-Jan Hsu", "Yi-Chang Chen", "Feng-Ting Liao", "Pei-Chen Ho", "Yu-Hsiang Wang", "Po-Chun Hsu", "Da-shan Shiu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We propose \"Generative Fusion Decoding\" (GFD), a novel shallow fusion\nframework designed to integrate large language models (LLMs) into cross-modal\ntext recognition systems for automatic speech recognition (ASR) and optical\ncharacter recognition (OCR). We derive the necessary formulations to enable GFD\nto operate across mismatched token spaces of different models by calculating\nlikelihood at the byte level, thereby enabling seamless fusion and synchronous\nprogression during the decoding process. GFD is plug-and-play by design, making\nit readily compatible with various auto-regressive models without the need for\nany re-training. GFD proves effective for general ASR and OCR tasks through\nintermediate and frequent interactions with LLMs, surpassing cascaded methods\nin English and Mandarin benchmarks. In addition, GFD transfers in-context\nlearning abilities of LLMs and allows for adaptive ASR in instruction-aware and\nlong-context settings, yielding significant WER reductions of up to 17.7\\%.", "AI": {"tldr": "Generative Fusion Decoding (GFD) is a framework that integrates large language models into text recognition systems for ASR and OCR, showing significant performance improvements without retraining.", "motivation": "To seamlessly integrate large language models into cross-modal text recognition systems for improved ASR and OCR performance.", "method": "GFD operates by calculating likelihood at the byte level to fuse models across different token spaces, allowing for enhanced decoding processes.", "result": "GFD demonstrates superior performance in English and Mandarin ASR and OCR tasks, achieving significant WER reductions of up to 17.7% compared to cascaded methods.", "conclusion": "GFD is a plug-and-play framework that enhances ASR and OCR functionalities by leveraging the capabilities of large language models without the need for retraining.", "key_contributions": ["Introduces a novel GFD framework for integrating LLMs into ASR and OCR systems.", "Demonstrates significant performance improvements in text recognition tasks through effective model fusion.", "Facilitates in-context learning abilities in adaptive ASR contexts."], "limitations": "", "keywords": ["Generative Fusion Decoding", "Large Language Models", "Automatic Speech Recognition", "Optical Character Recognition", "Cross-modal Integration"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2406.06144", "pdf": "https://arxiv.org/pdf/2406.06144.pdf", "abs": "https://arxiv.org/abs/2406.06144", "title": "Language Models Resist Alignment: Evidence From Data Compression", "authors": ["Jiaming Ji", "Kaile Wang", "Tianyi Qiu", "Boyuan Chen", "Jiayi Zhou", "Changye Li", "Hantao Lou", "Juntao Dai", "Yunhuai Liu", "Yaodong Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL2025 Main", "summary": "Large language models (LLMs) may exhibit unintended or undesirable behaviors.\nRecent works have concentrated on aligning LLMs to mitigate harmful outputs.\nDespite these efforts, some anomalies indicate that even a well-conducted\nalignment process can be easily circumvented, whether intentionally or\naccidentally. Does alignment fine-tuning yield have robust effects on models,\nor are its impacts merely superficial? In this work, we make the first\nexploration of this phenomenon from both theoretical and empirical\nperspectives. Empirically, we demonstrate the $\\mathbf{elasticity}$ of\npost-alignment models, i.e., the tendency to revert to the behavior\ndistribution formed during the pre-training phase upon further fine-tuning.\nLeveraging compression theory, we formally deduce that fine-tuning\ndisproportionately undermines alignment relative to pre-training, potentially\nby orders of magnitude. We validate the presence of elasticity through\nexperiments on models of varying types and scales. Specifically, we find that\nmodel performance declines rapidly before reverting to the pre-training\ndistribution, after which the rate of decline drops significantly. Furthermore,\nwe further reveal that elasticity positively correlates with the increased\nmodel size and the expansion of pre-training data. Our findings underscore the\nneed to address the inherent elasticity of LLMs to mitigate their resistance to\nalignment. The model weight and code are available at\npku-lm-resist-alignment.github.io.", "AI": {"tldr": "This paper explores the elasticity of large language models (LLMs) post-alignment, demonstrating how they can revert to pre-training behavior despite alignment efforts.", "motivation": "The need to understand and mitigate the unintended behaviors of LLMs, which alignment processes may not fully resolve.", "method": "The authors empirically analyzed the elasticity of post-alignment models and leveraged compression theory to investigate alignment impacts, validating these findings through experiments with various models.", "result": "The study found that post-alignment, models exhibit a tendency to revert to pre-training behavior, with performance declines followed by reversion to prior distributions, especially correlated with model size and pre-training data.", "conclusion": "Addressing the inherent elasticity of LLMs is crucial for improving alignment effectiveness and ensuring desired model behaviors.", "key_contributions": ["Introduced the concept of elasticity in LLMs post-alignment", "Demonstrated empirical evidence of behavior reversion to pre-training distribution", "Provided theoretical insights using compression theory regarding alignment impacts."], "limitations": "", "keywords": ["large language models", "alignment", "elasticity", "compression theory", "empirical analysis"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2406.08726", "pdf": "https://arxiv.org/pdf/2406.08726.pdf", "abs": "https://arxiv.org/abs/2406.08726", "title": "Standard Language Ideology in AI-Generated Language", "authors": ["Genevieve Smith", "Eve Fleisig", "Madeline Bossi", "Ishita Rustagi", "Xavier Yin"], "categories": ["cs.CL"], "comment": null, "summary": "Standard language ideology is reflected and reinforced in language generated\nby large language models (LLMs). We present a faceted taxonomy of open problems\nthat illustrate how standard language ideology manifests in AI-generated\nlanguage, alongside implications for minoritized language communities and\nsociety more broadly. We introduce the concept of standard AI-generated\nlanguage ideology, a process through which LLMs position \"standard\"\nlanguages--particularly Standard American English (SAE)--as the linguistic\ndefault, reinforcing the perception that SAE is the most \"appropriate\"\nlanguage. We then discuss ongoing tensions around what constitutes desirable\nsystem behavior, as well as advantages and drawbacks of generative AI tools\nattempting, or refusing, to imitate different English language varieties.\nRather than prescribing narrow technical fixes, we offer three recommendations\nfor researchers, practitioners, and funders that focus on shifting structural\nconditions and supporting more emancipatory outcomes for diverse language\ncommunities.", "AI": {"tldr": "The paper analyzes how standard language ideology is reflected in language produced by large language models (LLMs) and discusses its implications for minoritized language communities.", "motivation": "To explore how LLMs reinforce standard language ideologies, particularly Standard American English, impacting diverse language communities.", "method": "A faceted taxonomy of open problems is presented to illustrate the manifestation of standard language ideology in AI-generated language.", "result": "The findings reveal that LLMs perpetuate standard language norms and biases, which can marginalize non-standard language varieties and their speakers.", "conclusion": "The paper recommends structural changes and strategies to support diverse language communities rather than solely focusing on technical solutions.", "key_contributions": ["Introduction of the concept of standard AI-generated language ideology", "Identification of ongoing tensions regarding language variety representation in AI", "Three recommendations for researchers and practitioners to achieve more inclusive outcomes"], "limitations": "", "keywords": ["language ideology", "large language models", "Standard American English", "minoritized languages", "generative AI"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2406.17761", "pdf": "https://arxiv.org/pdf/2406.17761.pdf", "abs": "https://arxiv.org/abs/2406.17761", "title": "CaLMQA: Exploring culturally specific long-form question answering across 23 languages", "authors": ["Shane Arora", "Marzena Karpinska", "Hung-Ting Chen", "Ipsita Bhattacharjee", "Mohit Iyyer", "Eunsol Choi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "46 pages, 26 figures. Accepted as a main conference paper at ACL\n  2025. Code and data available at https://github.com/2015aroras/CaLMQA .\n  Dataset expanded to 51.7K questions", "summary": "Despite rising global usage of large language models (LLMs), their ability to\ngenerate long-form answers to culturally specific questions remains unexplored\nin many languages. To fill this gap, we perform the first study of textual\nmultilingual long-form QA by creating CaLMQA, a dataset of 51.7K culturally\nspecific questions across 23 different languages. We define culturally specific\nquestions as those that refer to concepts unique to one or a few cultures, or\nhave different answers depending on the cultural or regional context. We obtain\nthese questions by crawling naturally-occurring questions from community web\nforums in high-resource languages, and by hiring native speakers to write\nquestions in under-resourced, rarely-studied languages such as Fijian and\nKirundi. Our data collection methodologies are translation-free, enabling the\ncollection of culturally unique questions like \"Kuber iki umwami wa mbere\nw'uburundi yitwa Ntare?\" (Kirundi; English translation: \"Why was the first king\nof Burundi called Ntare (Lion)?\"). We evaluate factuality, relevance and\nsurface-level quality of LLM-generated long-form answers, finding that (1) for\nmany languages, even the best models make critical surface-level errors (e.g.,\nanswering in the wrong language, repetition), especially for low-resource\nlanguages; and (2) answers to culturally specific questions contain more\nfactual errors than answers to culturally agnostic questions -- questions that\nhave consistent meaning and answer across many cultures. We release CaLMQA to\nfacilitate future research in cultural and multilingual long-form QA.", "AI": {"tldr": "The paper introduces CaLMQA, a multilingual dataset of 51.7K culturally specific long-form questions across 23 languages and evaluates LLM performance in answering these questions.", "motivation": "To explore the capability of large language models in generating long-form answers to culturally specific questions across different languages, addressing a gap in current research.", "method": "The authors created the CaLMQA dataset by collecting culturally specific questions from community forums in high-resource languages and by hiring native speakers for under-resourced languages. The evaluation involved assessing the factuality, relevance, and quality of LLM-generated answers to these questions.", "result": "The study found that LLMs often made critical surface-level errors and that culturally specific questions led to more factual errors compared to culturally agnostic questions, particularly for low-resource languages.", "conclusion": "The findings highlight limitations in LLM responses to culturally nuanced queries and emphasize the need for targeted improvements in multilingual understanding. The CaLMQA dataset is released for future research in this area.", "key_contributions": ["Introduction of CaLMQA, a dataset with 51.7K culturally specific questions in 23 languages", "First evaluation of LLM performance on multilingual long-form QA across cultures", "Insight into challenges faced by LLMs with cultural specificity and low-resource languages."], "limitations": "The study may not cover all cultural nuances and languages, and there could be limitations in the evaluation framework used for assessing answer quality.", "keywords": ["multilingual QA", "culturally specific questions", "large language models", "natural language processing", "dataset release"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2407.13329", "pdf": "https://arxiv.org/pdf/2407.13329.pdf", "abs": "https://arxiv.org/abs/2407.13329", "title": "CiteFusion: An Ensemble Framework for Citation Intent Classification Harnessing Dual-Model Binary Couples and SHAP Analyses", "authors": ["Lorenzo Paolini", "Sahar Vahdati", "Angelo Di Iorio", "Robert Wardenga", "Ivan Heibi", "Silvio Peroni"], "categories": ["cs.CL"], "comment": "Submitted to Scientometrics Journal", "summary": "Understanding the motivations underlying scholarly citations is essential to\nevaluate research impact and pro-mote transparent scholarly communication. This\nstudy introduces CiteFusion, an ensemble framework designed to address the\nmulti-class Citation Intent Classification task on two benchmark datasets:\nSciCite and ACL-ARC. The framework employs a one-vs-all decomposition of the\nmulti-class task into class-specific binary sub-tasks, leveraging complementary\npairs of SciBERT and XLNet models, independently tuned, for each citation\nintent. The outputs of these base models are aggregated through a feedforward\nneural network meta-classifier to reconstruct the original classification task.\nTo enhance interpretability, SHAP (SHapley Additive exPlanations) is employed\nto analyze token-level contributions, and interactions among base models,\nproviding transparency into the classification dynamics of CiteFusion, and\ninsights about the kind of misclassifications of the ensem-ble. In addition,\nthis work investigates the semantic role of structural context by incorporating\nsection titles, as framing devices, into input sentences, assessing their\npositive impact on classification accuracy. CiteFusion ul-timately demonstrates\nrobust performance in imbalanced and data-scarce scenarios: experimental\nresults show that CiteFusion achieves state-of-the-art performance, with\nMacro-F1 scores of 89.60% on SciCite, and 76.24% on ACL-ARC. Furthermore, to\nensure interoperability and reusability, citation intents from both datasets\nsche-mas are mapped to Citation Typing Ontology (CiTO) object properties,\nhighlighting some overlaps. Finally, we describe and release a web-based\napplication that classifies citation intents leveraging the CiteFusion models\ndeveloped on SciCite.", "AI": {"tldr": "CiteFusion is an ensemble framework for classifying citation intents using SciBERT and XLNet models, achieving state-of-the-art performance on benchmark datasets while enhancing interpretability through SHAP.", "motivation": "Understanding citation motivations is crucial for evaluating research impact and promoting transparent scholarly communication.", "method": "CiteFusion uses a one-vs-all approach for multi-class classification with binary sub-tasks, combining outputs from SciBERT and XLNet models through a neural network meta-classifier. It also incorporates section titles as contextual framing devices.", "result": "CiteFusion achieved Macro-F1 scores of 89.60% on SciCite and 76.24% on ACL-ARC, demonstrating robust performance in imbalanced and data-scarce scenarios.", "conclusion": "The study highlights the importance of structural context in classification accuracy and provides a web-based application for classifying citation intents.", "key_contributions": ["Introduces CiteFusion framework for citation intent classification.", "Employs SHAP for enhanced interpretability of classification outputs.", "Demonstrates state-of-the-art performance on citation intent classification benchmarks."], "limitations": "", "keywords": ["Citation Classification", "Ensemble Learning", "SHAP", "SciBERT", "XLNet"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2408.04211", "pdf": "https://arxiv.org/pdf/2408.04211.pdf", "abs": "https://arxiv.org/abs/2408.04211", "title": "MMREC: LLM Based Multi-Modal Recommender System", "authors": ["Jiahao Tian", "Jinman Zhao", "Zhenkai Wang", "Zhicheng Ding"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "The importance of recommender systems is growing rapidly due to the\nexponential increase in the volume of content generated daily. This surge in\ncontent presents unique challenges for designing effective recommender systems.\nKey among these challenges is the need to effectively leverage the vast amounts\nof natural language data and images that represent user preferences. This paper\npresents a novel approach to enhancing recommender systems by leveraging Large\nLanguage Models (LLMs) and deep learning techniques. The proposed framework\naims to improve the accuracy and relevance of recommendations by incorporating\nmulti-modal information processing and by the use of unified latent space\nrepresentation. The study explores the potential of LLMs to better understand\nand utilize natural language data in recommendation contexts, addressing the\nlimitations of previous methods. The framework efficiently extracts and\nintegrates text and image information through LLMs, unifying diverse modalities\nin a latent space to simplify the learning process for the ranking model.\nExperimental results demonstrate the enhanced discriminative power of the model\nwhen utilizing multi-modal information. This research contributes to the\nevolving field of recommender systems by showcasing the potential of LLMs and\nmulti-modal data integration to create more personalized and contextually\nrelevant recommendations.", "AI": {"tldr": "This paper presents a novel approach to enhancing recommender systems using Large Language Models (LLMs) and deep learning techniques, focusing on multi-modal information processing to improve recommendation accuracy and relevance.", "motivation": "The rapid increase in content generated daily creates challenges for effective recommender systems, particularly in leveraging natural language data and images that represent user preferences.", "method": "The proposed framework incorporates multi-modal information processing and unified latent space representation to improve recommendations. It extracts and integrates text and image information through LLMs, simplifying the learning process for the ranking model.", "result": "Experimental results show that the model demonstrates enhanced discriminative power when utilizing multi-modal information, leading to more personalized and contextually relevant recommendations.", "conclusion": "This research contributes to the field of recommender systems by showcasing the potential of LLMs and multi-modal data integration to create better recommendations.", "key_contributions": ["Novel use of LLMs in recommender systems", "Integration of multi-modal information", "Improvement of recommendation accuracy and relevance"], "limitations": "", "keywords": ["Recommender Systems", "Large Language Models", "Multi-modal Data", "Deep Learning", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2408.14352", "pdf": "https://arxiv.org/pdf/2408.14352.pdf", "abs": "https://arxiv.org/abs/2408.14352", "title": "LogProber: Disentangling confidence from contamination in LLM responses", "authors": ["Nicolas Yax", "Pierre-Yves Oudeyer", "Stefano Palminteri"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "In machine learning, contamination refers to situations where testing data\nleak into the training set. The issue is particularly relevant for the\nevaluation of the performance of Large Language Models (LLMs), which are\ngenerally trained on gargantuan, and generally opaque, corpora of text scraped\nfrom the world wide web. Developing tools to detect contamination is therefore\ncrucial to be able to fairly and properly track the evolution of the\nperformance of LLMs. To date, only a few recent studies have attempted to\naddress the issue of quantifying and detecting contamination in short text\nsequences, such as those commonly found in benchmarks. However, these methods\nhave limitations that can sometimes render them impractical.In the present\npaper, we introduce LogProber, a novel, efficient algorithm that we show to be\nable to detect contamination in a black box setting that tries to tackle some\nof these drawbacks by focusing on the familiarity with the question rather than\nthe answer. Here, we explore the properties of the proposed method in\ncomparison with concurrent approaches, identify its advantages and limitations,\nand illustrate how different forms of contamination can go undetected depending\non the design of the detection algorithm.", "AI": {"tldr": "The paper presents LogProber, an efficient algorithm for detecting data contamination in machine learning models, especially focused on Large Language Models (LLMs).", "motivation": "Contamination in machine learning occurs when testing data leaks into the training set, affecting the evaluation of LLM performance. Developing detection tools is crucial for fair performance tracking of these models.", "method": "LogProber is introduced as a novel algorithm designed to detect contamination in a black box setting, focusing on the familiarity with questions rather than answers.", "result": "LogProber's properties were explored in comparison to existing methods, highlighting its effectiveness in detecting different forms of contamination that might otherwise go unnoticed.", "conclusion": "LogProber addresses some limitations of previous contamination detection methods, although it has its own set of advantages and limitations that must be considered.", "key_contributions": ["Introduction of LogProber, a new algorithm for contamination detection", "Comparison with existing methods and highlighting advantages", "Identification of scenarios where contamination goes undetected"], "limitations": "The proposed method has certain limitations that need to be considered in practical applications of contamination detection.", "keywords": ["machine learning", "data contamination", "Large Language Models", "LogProber", "evaluation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2408.16326", "pdf": "https://arxiv.org/pdf/2408.16326.pdf", "abs": "https://arxiv.org/abs/2408.16326", "title": "Critic-CoT: Boosting the reasoning abilities of large language model via Chain-of-thoughts Critic", "authors": ["Xin Zheng", "Jie Lou", "Boxi Cao", "Xueru Wen", "Yuqiu Ji", "Hongyu Lin", "Yaojie Lu", "Xianpei Han", "Debing Zhang", "Le Sun"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 Findings", "summary": "Self-critic has become a crucial mechanism for enhancing the reasoning\nperformance of LLMs. However, current approaches mainly involve basic prompts\nfor intuitive instance-level feedback, which resembles System-1 processes and\nlimits the reasoning capabilities. Moreover, there is a lack of in-depth\ninvestigations into the relationship between LLM's ability to criticize and its\ntask-solving performance. To address these issues, we propose Critic-CoT, a\nnovel framework that pushes LLMs toward System-2-like critic capability.\nThrough a step-wise CoT reasoning paradigm and the automatic construction of\ndistant-supervision data without human annotation, Critic-CoT enables LLMs to\nengage in slow, analytic self-critique and refinement, thereby improving their\nreasoning abilities. Experiments on GSM8K and MATH demonstrate that our\nenhanced model significantly boosts task-solving performance by filtering out\ninvalid solutions or iterative refinement. Furthermore, we investigate the\nintrinsic correlation between critique and task-solving abilities within LLMs,\ndiscovering that these abilities can mutually reinforce each other rather than\nconflict.", "AI": {"tldr": "The paper presents Critic-CoT, a framework that enhances LLMs' reasoning ability through systematic self-critique, improving their task-solving performance.", "motivation": "To improve LLM reasoning by transitioning from basic instance-level feedback to deeper, System-2-like self-critique.", "method": "Critic-CoT employs a step-wise CoT reasoning paradigm and constructs distant-supervision data automatically, facilitating analytic self-critique.", "result": "Experiments show that Critic-CoT significantly improves task-solving performance on GSM8K and MATH by filtering invalid solutions and enabling iterative refinement.", "conclusion": "Critique and task-solving abilities in LLMs are found to mutually reinforce each other, enhancing overall performance.", "key_contributions": ["Introduction of the Critic-CoT framework for LLM self-critique", "Demonstrated mutual reinforcement between critique and task-solving capabilities", "Automatic construction of distant-supervision data for enhancing reasoning"], "limitations": "", "keywords": ["LLM", "self-critique", "reasoning", "task-solving", "critique-CoT"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2409.00598", "pdf": "https://arxiv.org/pdf/2409.00598.pdf", "abs": "https://arxiv.org/abs/2409.00598", "title": "Automatic Pseudo-Harmful Prompt Generation for Evaluating False Refusals in Large Language Models", "authors": ["Bang An", "Sicheng Zhu", "Ruiyi Zhang", "Michael-Andrei Panaitescu-Liess", "Yuancheng Xu", "Furong Huang"], "categories": ["cs.CL", "cs.CR", "cs.CY", "cs.LG"], "comment": null, "summary": "Safety-aligned large language models (LLMs) sometimes falsely refuse\npseudo-harmful prompts, like \"how to kill a mosquito,\" which are actually\nharmless. Frequent false refusals not only frustrate users but also provoke a\npublic backlash against the very values alignment seeks to protect. In this\npaper, we propose the first method to auto-generate diverse,\ncontent-controlled, and model-dependent pseudo-harmful prompts. Using this\nmethod, we construct an evaluation dataset called PHTest, which is ten times\nlarger than existing datasets, covers more false refusal patterns, and\nseparately labels controversial prompts. We evaluate 20 LLMs on PHTest,\nuncovering new insights due to its scale and labeling. Our findings reveal a\ntrade-off between minimizing false refusals and improving safety against\njailbreak attacks. Moreover, we show that many jailbreak defenses significantly\nincrease the false refusal rates, thereby undermining usability. Our method and\ndataset can help developers evaluate and fine-tune safer and more usable LLMs.\nOur code and dataset are available at\nhttps://github.com/umd-huang-lab/FalseRefusal", "AI": {"tldr": "This paper presents a method to auto-generate diverse pseudo-harmful prompts to evaluate large language models (LLMs), revealing trade-offs in safety and usability.", "motivation": "Frequent false refusals of pseudo-harmful prompts by safety-aligned LLMs frustrate users and provoke public backlash against values alignment.", "method": "The authors propose a method for auto-generating diverse, content-controlled, and model-dependent pseudo-harmful prompts, resulting in a new evaluation dataset called PHTest.", "result": "The evaluation of 20 LLMs on the PHTest dataset uncovers insights into false refusal patterns and reveals a trade-off between minimizing false refusals and safety against jailbreak attacks.", "conclusion": "The research underscores the need for more usable and safe LLMs and provides a dataset and methodology for future evaluations and improvements.", "key_contributions": ["Introduction of a method to generate pseudo-harmful prompts for LLM evaluation", "Creation of the PHTest dataset, which is significantly larger and more comprehensive than previous datasets", "Revelation of trade-offs between false refusal rates and safety measures in LLMs"], "limitations": "The study does not address all possible LLM architectures and their performance on PHTest may vary significantly.", "keywords": ["large language models", "false refusals", "safety", "jailbreak attacks", "evaluation dataset"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2409.07615", "pdf": "https://arxiv.org/pdf/2409.07615.pdf", "abs": "https://arxiv.org/abs/2409.07615", "title": "MOSAIC: Multiple Observers Spotting AI Content", "authors": ["Matthieu Dubois", "François Yvon", "Pablo Piantanida"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings, code can be found at\n  https://github.com/BaggerOfWords/MOSAIC", "summary": "The dissemination of Large Language Models (LLMs), trained at scale, and\nendowed with powerful text-generating abilities, has made it easier for all to\nproduce harmful, toxic, faked or forged content. In response, various proposals\nhave been made to automatically discriminate artificially generated from\nhuman-written texts, typically framing the problem as a binary classification\nproblem. Early approaches evaluate an input document with a well-chosen\ndetector LLM, assuming that low-perplexity scores reliably signal machine-made\ncontent. More recent systems instead consider two LLMs and compare their\nprobability distributions over the document to further discriminate when\nperplexity alone cannot. However, using a fixed pair of models can induce\nbrittleness in performance. We extend these approaches to the ensembling of\nseveral LLMs and derive a new, theoretically grounded approach to combine their\nrespective strengths. Our experiments, conducted with various generator LLMs,\nindicate that this approach effectively leverages the strengths of each model,\nresulting in robust detection performance across multiple domains. Our code and\ndata are available at https://github.com/BaggerOfWords/MOSAIC .", "AI": {"tldr": "This paper proposes a new method for detecting machine-generated text by ensembling multiple Large Language Models (LLMs) to improve performance over traditional single-model approaches.", "motivation": "The rise of LLMs has enabled the easy production of harmful or deceptive text, necessitating robust methods to differentiate machine-generated content from human-written texts.", "method": "The paper extends traditional detection methods by employing an ensemble of several LLMs, combining their strengths in a theoretically grounded manner for better performance.", "result": "Experiments demonstrate that this ensemble approach significantly enhances detection capabilities across various content domains compared to using individual models.", "conclusion": "The proposed method shows promise for improving the robustness of automated content detection systems in the context of LLM-generated text.", "key_contributions": ["Development of an ensemble method for LLMs in text detection", "Theoretically grounded approach combining strengths of multiple models", "Demonstrated effectiveness across multiple domains."], "limitations": "", "keywords": ["Large Language Models", "Text Generation", "Content Detection", "Ensemble Learning", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2409.15912", "pdf": "https://arxiv.org/pdf/2409.15912.pdf", "abs": "https://arxiv.org/abs/2409.15912", "title": "Explaining word embeddings with perfect fidelity: Case study in research impact prediction", "authors": ["Lucie Dvorackova", "Marcin P. Joachimiak", "Michal Cerny", "Adriana Kubecova", "Vilem Sklenak", "Tomas Kliegr"], "categories": ["cs.CL"], "comment": null, "summary": "Best performing approaches for scholarly document quality prediction are\nbased on embedding models, which do not allow direct explanation of classifiers\nas distinct words no longer correspond to the input features for model\ntraining. Although model-agnostic explanation methods such as Local\ninterpretable model-agnostic explanations (LIME) can be applied, these produce\nresults with questionable correspondence to the ML model. We introduce a new\nfeature importance method, Self-model Rated Entities (SMER), for logistic\nregression-based classification models trained on word embeddings. We show that\nSMER has theoretically perfect fidelity with the explained model, as its\nprediction corresponds exactly to the average of predictions for individual\nwords in the text. SMER allows us to reliably determine which words or entities\npositively contribute to predicting impactful articles. Quantitative and\nqualitative evaluation is performed through five diverse experiments conducted\non 50.000 research papers from the CORD-19 corpus. Through an AOPC curve\nanalysis, we experimentally demonstrate that SMER produces better explanations\nthan LIME for logistic regression.", "AI": {"tldr": "Introducing SMER, a novel method for feature importance in logistic regression models using word embeddings, offering better explanations than LIME for predicting impactful scholarly articles.", "motivation": "To address limitations in model-agnostic explanation methods like LIME for scholarly document quality prediction using word embeddings.", "method": "Development of Self-model Rated Entities (SMER) method for logistic regression that ensures fidelity with the explained model by correlating prediction to average predictions of individual words.", "result": "SMER provides more reliable word/entity importance for predicting impactful articles compared to LIME, demonstrated through quantitative and qualitative evaluations on 50,000 research papers from the CORD-19 corpus.", "conclusion": "SMER outperforms LIME in producing explanations with perfect fidelity to the model, enhancing understanding of features that contribute to article quality.", "key_contributions": ["Introduction of SMER for feature importance in word embeddings", "Demonstration of perfect fidelity with logistic regression models", "Performance superiority of SMER over LIME for explaining model predictions"], "limitations": "", "keywords": ["feature importance", "word embeddings", "scholarly documents", "logistic regression", "model explanations"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2410.08193", "pdf": "https://arxiv.org/pdf/2410.08193.pdf", "abs": "https://arxiv.org/abs/2410.08193", "title": "GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment", "authors": ["Yuancheng Xu", "Udari Madhushani Sehwag", "Alec Koppel", "Sicheng Zhu", "Bang An", "Furong Huang", "Sumitra Ganesh"], "categories": ["cs.CL"], "comment": "Published at the Thirteenth International Conference on Learning\n  Representations (ICLR 2025)", "summary": "Large Language Models (LLMs) exhibit impressive capabilities but require\ncareful alignment with human preferences. Traditional training-time methods\nfinetune LLMs using human preference datasets but incur significant training\ncosts and require repeated training to handle diverse user preferences.\nTest-time alignment methods address this by using reward models (RMs) to guide\nfrozen LLMs without retraining. However, existing test-time approaches rely on\ntrajectory-level RMs which are designed to evaluate complete responses, making\nthem unsuitable for autoregressive text generation that requires computing\nnext-token rewards from partial responses. To address this, we introduce\nGenARM, a test-time alignment approach that leverages the Autoregressive Reward\nModel--a novel reward parametrization designed to predict next-token rewards\nfor efficient and effective autoregressive generation. Theoretically, we\ndemonstrate that this parametrization can provably guide frozen LLMs toward any\ndistribution achievable by traditional RMs within the KL-regularized\nreinforcement learning framework. Experimental results show that GenARM\nsignificantly outperforms prior test-time alignment baselines and matches the\nperformance of training-time methods. Additionally, GenARM enables efficient\nweak-to-strong guidance, aligning larger LLMs with smaller RMs without the high\ncosts of training larger models. Furthermore, GenARM supports multi-objective\nalignment, allowing real-time trade-offs between preference dimensions and\ncatering to diverse user preferences without retraining. Our project page is\navailable at: https://genarm.github.io.", "AI": {"tldr": "GenARM is a novel test-time alignment approach for LLMs that uses Autoregressive Reward Models to predict next-token rewards, significantly improving efficiency and effectiveness in guiding frozen models based on user preferences.", "motivation": "To improve the alignment of LLMs with user preferences without the high costs and inefficiencies of traditional training-time methods.", "method": "GenARM leverages a novel parametrization called Autoregressive Reward Model designed for evaluating next-token rewards, enabling efficient autoregressive text generation.", "result": "Experimental results show GenARM significantly outperforms prior test-time alignment methods and matches training-time performance, allowing for flexible preference alignment and efficient model usage.", "conclusion": "GenARM provides an effective solution for aligning LLMs with diverse user preferences at test time without requiring retraining.", "key_contributions": ["Introduction of Autoregressive Reward Model for next-token prediction", "Significant performance improvement over existing test-time methods", "Support for multi-objective alignment to manage user preference trade-offs"], "limitations": "", "keywords": ["Large Language Models", "Autoregressive Reward Model", "Human Preferences", "Test-time Alignment", "Machine Learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.14387", "pdf": "https://arxiv.org/pdf/2410.14387.pdf", "abs": "https://arxiv.org/abs/2410.14387", "title": "How Do Multilingual Language Models Remember Facts?", "authors": ["Constanza Fierro", "Negar Foroutan", "Desmond Elliott", "Anders Søgaard"], "categories": ["cs.CL"], "comment": "9 pages", "summary": "Large Language Models (LLMs) store and retrieve vast amounts of factual\nknowledge acquired during pre-training. Prior research has localized and\nidentified mechanisms behind knowledge recall; however, it has only focused on\nEnglish monolingual models. The question of how these mechanisms generalize to\nnon-English languages and multilingual LLMs remains unexplored. In this paper,\nwe address this gap by conducting a comprehensive analysis of three\nmultilingual LLMs. First, we show that previously identified recall mechanisms\nin English largely apply to multilingual contexts, with nuances based on\nlanguage and architecture. Next, through patching intermediate representations,\nwe localize the role of language during recall, finding that subject enrichment\nis language-independent, while object extraction is language-dependent.\nAdditionally, we discover that the last token representation acts as a Function\nVector (FV), encoding both the language of the query and the content to be\nextracted from the subject. Furthermore, in decoder-only LLMs, FVs compose\nthese two pieces of information in two separate stages. These insights reveal\nunique mechanisms in multilingual LLMs for recalling information, highlighting\nthe need for new methodologies -- such as knowledge evaluation, fact editing,\nand knowledge acquisition -- that are specifically tailored for multilingual\nLLMs.", "AI": {"tldr": "This paper analyzes knowledge recall mechanisms in multilingual large language models (LLMs) and finds that while many mechanisms are similar to those in English models, there are unique features based on language and architecture.", "motivation": "To explore how knowledge recall mechanisms in multilingual large language models generalize from English to other languages and to identify unique characteristics of these mechanisms.", "method": "A comprehensive analysis was conducted on three multilingual LLMs, examining recall mechanisms, and patching intermediate representations to assess the role of language in knowledge retrieval.", "result": "The study finds that English recall mechanisms generally apply to multilingual contexts, with language and architecture nuances. Patching revealed that subject enrichment is language-independent and object extraction is language-dependent. The last token representation serves as a Function Vector (FV), encoding language and content, with distinct processing in decoder-only models.", "conclusion": "Unique mechanisms for recalling information in multilingual LLMs were identified, indicating a need for tailored methodologies such as knowledge evaluation and editing for these models.", "key_contributions": ["Identified similarities and differences in recall mechanisms between English and multilingual LLMs.", "Localized the roles of language in objectives of knowledge extraction and subject enrichment.", "Established the concept of the last token representation as a Function Vector influencing both language and content retrieval."], "limitations": "", "keywords": ["multilingual LLMs", "knowledge recall", "Function Vector", "language dependency", "machine learning"], "importance_score": 7, "read_time_minutes": 9}}
{"id": "2410.17131", "pdf": "https://arxiv.org/pdf/2410.17131.pdf", "abs": "https://arxiv.org/abs/2410.17131", "title": "Self-Steering Optimization: Autonomous Preference Optimization for Large Language Models", "authors": ["Hao Xiang", "Bowen Yu", "Hongyu Lin", "Keming Lu", "Yaojie Lu", "Xianpei Han", "Ben He", "Le Sun", "Jingren Zhou", "Junyang Lin"], "categories": ["cs.CL"], "comment": null, "summary": "The key to effective alignment lies in high-quality preference data. Recent\nresearch has focused on automated alignment, which involves developing\nalignment systems with minimal human intervention. However, prior research has\npredominantly focused on developing data generation methods, while insufficient\nattention has been paid to quality control mechanisms, which often produce\ninaccurate and unhelpful data, leading to unpredictable benefits during\niterative optimization. In this paper, we present Self-Steering Optimization\n($SSO$), an algorithm that autonomously generates high-quality preference data,\neliminating manual annotation requirements. $SSO$ employs a specialized\noptimization objective to build a data generator from the policy model itself,\nwhich is used to produce accurate and on-policy data. We demonstrate $SSO$'s\neffectiveness through comprehensive experiments on two series of models: Llama\n3 and Qwen 2. Our evaluation across diverse benchmarks shows that $SSO$\nconsistently outperforms baselines in human preference alignment and reward\noptimization. Further analysis validates $SSO$ as a scalable framework for\npreference optimization, benefiting the advancement in automated alignment\ntechniques.", "AI": {"tldr": "The paper presents Self-Steering Optimization (SSO), an algorithm for generating high-quality preference data autonomously, aimed at improving automated alignment systems.", "motivation": "Effective alignment in machine learning relies on high-quality preference data, yet many existing methods neglect quality control leading to inaccuracies.", "method": "SSO autonomously generates preference data using a specialized optimization objective from the policy model, eliminating the need for manual annotation.", "result": "Experiments show that SSO outperforms existing baselines in human preference alignment and reward optimization across diverse benchmarks.", "conclusion": "SSO serves as a scalable framework for improving automated alignment techniques by ensuring the generation of high-quality preference data.", "key_contributions": ["Introduction of Self-Steering Optimization (SSO) for autonomous preference data generation.", "Demonstration of SSO's superior performance compared to baseline methods.", "Validation of SSO's scalability for preference optimization."], "limitations": "", "keywords": ["preference data", "automated alignment", "machine learning", "self-steering optimization", "reward optimization"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2411.02460", "pdf": "https://arxiv.org/pdf/2411.02460.pdf", "abs": "https://arxiv.org/abs/2411.02460", "title": "Code-Switching Curriculum Learning for Multilingual Transfer in LLMs", "authors": ["Haneul Yoo", "Cheonbok Park", "Sangdoo Yun", "Alice Oh", "Hwaran Lee"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "To appear in Findings of ACL 2025", "summary": "Large language models (LLMs) now exhibit near human-level performance in\nvarious tasks, but their performance drops drastically after a handful of\nhigh-resource languages due to the imbalance in pre-training data. Inspired by\nthe human process of second language acquisition, particularly\ncode-switching$\\unicode{x2014}$the practice of language alternation in a\nconversation$\\unicode{x2014}$we propose code-switching curriculum learning\n(CSCL) to enhance cross-lingual transfer for LLMs. CSCL mimics the stages of\nhuman language learning by progressively training models with a curriculum\nconsisting of 1) token-level code-switching, 2) sentence-level code-switching,\nand 3) monolingual corpora. Using Qwen 2 as our underlying model, we\ndemonstrate the efficacy of the CSCL in improving language transfer to Korean,\nachieving significant performance gains compared to monolingual continual\npre-training methods. Ablation studies reveal that both token- and\nsentence-level code-switching significantly enhance cross-lingual transfer and\nthat curriculum learning amplifies these effects. We also extend our findings\ninto various languages, including Japanese (high-resource) and Indonesian\n(low-resource), and using two additional models (Gemma 2 and Phi 3.5). We\nfurther show that CSCL mitigates spurious correlations between language\nresources and safety alignment, presenting a robust, efficient framework for\nmore equitable language transfer in LLMs. We observe that CSCL is effective for\nlow-resource settings where high-quality, monolingual corpora for language\ntransfer are hardly available.", "AI": {"tldr": "The paper introduces code-switching curriculum learning (CSCL) as a method to enhance cross-lingual transfer in large language models (LLMs) by mimicking human language acquisition stages, achieving significant improvements in performance across languages, particularly for low-resource settings.", "motivation": "To address the performance drop in LLMs after high-resource languages due to data imbalance, inspired by human second language acquisition processes such as code-switching.", "method": "The proposed method consists of a curriculum with three stages: token-level code-switching, sentence-level code-switching, and training with monolingual corpora, tested on the Qwen 2 model.", "result": "CSCL significantly improves language transfer to Korean and other languages, outperforming traditional monolingual continual pre-training methods.", "conclusion": "CSCL provides a robust framework for enhancing cross-lingual transfer in LLMs, particularly benefiting low-resource language contexts.", "key_contributions": ["Introduction of code-switching curriculum learning (CSCL) for LLMs", "Demonstration of significant performance gains in cross-lingual tasks", "Mitigation of spurious correlations in language resources and safety alignment."], "limitations": "", "keywords": ["code-switching", "curriculum learning", "cross-lingual transfer", "language models", "low-resource languages"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2411.12768", "pdf": "https://arxiv.org/pdf/2411.12768.pdf", "abs": "https://arxiv.org/abs/2411.12768", "title": "CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization", "authors": ["Nay Myat Min", "Long H. Pham", "Yige Li", "Jun Sun"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ICML 2025, 20 pages", "summary": "Large Language Models (LLMs) are vulnerable to backdoor attacks that\nmanipulate outputs via hidden triggers. Existing defense methods--designed for\nvision/text classification tasks--fail for text generation. We propose Internal\nConsistency Regularization (CROW), a defense leveraging the observation that\nbackdoored models exhibit unstable layer-wise hidden representations when\ntriggered, while clean models show smooth transitions. CROW enforces\nconsistency across layers via adversarial perturbations and regularization\nduring finetuning, neutralizing backdoors without requiring clean reference\nmodels or trigger knowledge--only a small clean dataset. Experiments across\nLlama-2 (7B, 13B), CodeLlama (7B, 13B), and Mistral-7B demonstrate CROW's\neffectiveness: it achieves significant reductions in attack success rates\nacross diverse backdoor strategies (sentiment steering, targeted refusal, code\ninjection) while preserving generative performance. CROW's\narchitecture-agnostic design enables practical deployment.", "AI": {"tldr": "CROW is a defense method against backdoor attacks in Large Language Models that maintains model performance while neutralizing hidden triggers through internal consistency regularization.", "motivation": "To address the vulnerability of Large Language Models (LLMs) to backdoor attacks in text generation tasks, where existing defenses fail.", "method": "CROW leverages unstable layer-wise hidden representations to enforce consistency across layers during finetuning with adversarial perturbations and regularization, requiring only a small clean dataset.", "result": "CROW significantly reduces attack success rates for various backdoor strategies while preserving generative performance, validated on Llama-2 and CodeLlama models.", "conclusion": "Internal Consistency Regularization effectively neutralizes backdoors in LLMs without needing clean reference models or trigger knowledge, making it practical for real-world applications.", "key_contributions": ["Proposes Internal Consistency Regularization (CROW) as a novel defense mechanism for LLMs.", "Demonstrates effectiveness across different models and backdoor attack strategies.", "Provides an architecture-agnostic solution for deployment."], "limitations": "", "keywords": ["Large Language Models", "backdoor attacks", "Internal Consistency Regularization", "text generation", "machine learning"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2411.17304", "pdf": "https://arxiv.org/pdf/2411.17304.pdf", "abs": "https://arxiv.org/abs/2411.17304", "title": "Meaningless is better: hashing bias-inducing words in LLM prompts improves performance in logical reasoning and statistical learning", "authors": ["Milena Chadimová", "Eduard Jurášek", "Tomáš Kliegr"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper introduces a novel method, referred to as \"hashing\", which\ninvolves masking potentially bias-inducing words in large language models\n(LLMs) with hash-like meaningless identifiers to reduce cognitive biases and\nreliance on external knowledge. The method was tested across three sets of\nexperiments involving a total of 490 prompts. Statistical analysis using\nchi-square tests showed significant improvements in all tested scenarios, which\ncovered LLama, ChatGPT, Copilot, Gemini and Mixtral models. In the first\nexperiment, hashing decreased the fallacy rate in a modified version of the\n\"Linda\" problem aimed at evaluating susceptibility to cognitive biases. In the\nsecond experiment, it improved LLM results on the frequent itemset extraction\ntask. In the third experiment, we found hashing is also effective when the\nLinda problem is presented in a tabular format rather than text, indicating\nthat the technique works across various input representations. Overall, the\nmethod was shown to improve bias reduction and incorporation of external\nknowledge. Despite bias reduction, hallucination rates were inconsistently\nreduced across types of LLM models. These findings suggest that masking\nbias-inducing terms can improve LLM performance, although its effectiveness is\nmodel- and task-dependent.", "AI": {"tldr": "The paper presents a method called \"hashing\" to mask bias-inducing words in large language models, aiming to reduce cognitive biases and external knowledge reliance, showing significant performance improvements across various models.", "motivation": "To address cognitive biases and improve the performance of large language models in various tasks.", "method": "The method involves masking potentially bias-inducing words with meaningless identifiers (hash-like) and evaluating its effectiveness through three experimental setups involving 490 prompts.", "result": "Hashing led to a significant decrease in fallacy rates on the Linda problem and improved results in frequent itemset extraction tasks across different models.", "conclusion": "Masking bias-inducing terms can enhance LLM performance for certain tasks, although the effectiveness varies by model and task, with inconsistent reductions in hallucination rates.", "key_contributions": ["Introduction of the hashing method for bias reduction in LLMs", "Demonstrated effectiveness across multiple models and tasks", "Statistical analysis affirming the method's significant improvements"], "limitations": "Effectiveness of hashing is model- and task-dependent with inconsistent reduction in hallucination rates.", "keywords": ["large language models", "cognitive biases", "hashing", "bias reduction", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2411.18553", "pdf": "https://arxiv.org/pdf/2411.18553.pdf", "abs": "https://arxiv.org/abs/2411.18553", "title": "Retrofitting Large Language Models with Dynamic Tokenization", "authors": ["Darius Feher", "Ivan Vulić", "Benjamin Minixhofer"], "categories": ["cs.CL"], "comment": null, "summary": "Current language models (LMs) use a fixed, static subword tokenizer. This\ndefault choice typically results in degraded efficiency and language\ncapabilities, especially in languages other than English. To address this\nissue, we challenge the static design and propose retrofitting LMs with dynamic\ntokenization: a way to dynamically decide on token boundaries based on the\ninput text via a subword-merging algorithm inspired by byte-pair encoding. We\nmerge frequent subword sequences in a batch, then apply a pre-trained\nembedding-prediction hypernetwork to compute the token embeddings on-the-fly.\nFor encoder-style models (e.g., XLM-R), this on average reduces token sequence\nlengths by >20% across 14 languages while degrading performance by less than\n2%. The same method applied to pre-filling and scoring in decoder-style models\n(e.g., Mistral-7B) results in minimal performance degradation at up to 17%\nreduction in sequence length. Overall, we find that dynamic tokenization can\nmitigate the limitations of static tokenization by substantially improving\ninference speed and promoting fairness across languages, enabling more\nequitable and adaptable LMs.", "AI": {"tldr": "The paper proposes a dynamic tokenization method for language models to improve efficiency and performance across multiple languages.", "motivation": "Current language models suffer from inefficiencies due to their static subword tokenization, which negatively impacts performance, particularly for non-English languages.", "method": "The approach involves a subword-merging algorithm inspired by byte-pair encoding, allowing for dynamic token boundary decisions based on input text. This method calculates token embeddings in real-time using a pre-trained embedding-prediction hypernetwork.", "result": "The method reduces token sequence lengths by over 20% on average for encoder-style models while degrading performance by less than 2%. For decoder-style models, it leads to minimal performance loss with up to a 17% reduction in sequence length.", "conclusion": "Dynamic tokenization effectively addresses the shortcomings of static tokenization, enhancing inference speed and promoting fairness across languages in language models.", "key_contributions": ["Introduction of dynamic tokenization for language models.", "Substantial reduction in token sequence lengths across multiple languages.", "Enhanced performance characteristics for both encoder and decoder-style models."], "limitations": "", "keywords": ["dynamic tokenization", "language models", "subword merging", "byte-pair encoding", "inference speed"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2412.05023", "pdf": "https://arxiv.org/pdf/2412.05023.pdf", "abs": "https://arxiv.org/abs/2412.05023", "title": "Steps are all you need: Rethinking STEM Education with Prompt Engineering", "authors": ["Krishnasai Addala", "Kabir Dev Paul Baghel", "Navya Gupta", "Rishitej Reddy Vyalla", "Chhavi Kirtani", "Avinash Anand", "Rajiv Ratn Shah"], "categories": ["cs.CL"], "comment": null, "summary": "Few shot and Chain-of-Thought prompting have shown promise when applied to\nPhysics Question Answering Tasks, but are limited by the lack of mathematical\nability inherent to LLMs, and are prone to hallucination. By utilizing a\nMixture of Experts (MoE) Model, along with analogical prompting, we are able to\nshow improved model performance when compared to the baseline on standard LLMs.\nWe also survey the limits of these prompting techniques and the effects they\nhave on model performance. Additionally, we propose Analogical CoT prompting, a\nprompting technique designed to allow smaller, open source models to leverage\nAnalogical prompting, something they have struggled with, possibly due to a\nlack of specialist training data.", "AI": {"tldr": "This paper presents improved performance in Physics Question Answering by employing a Mixture of Experts Model and Analogical CoT prompting, addressing limitations in existing LLM approaches.", "motivation": "Existing methodologies in Physics Question Answering using few shot and Chain-of-Thought prompting are hindered by the mathematical limitations of LLMs and their tendency to hallucinate, necessitating new approaches.", "method": "The authors utilize a Mixture of Experts Model in conjunction with analogical prompting to enhance model performance and reliability, comparing it to baseline performance on standard LLMs.", "result": "The proposed techniques demonstrate significant improvement over baseline models in Physics Question Answering tasks, especially for smaller, open-source models that previously struggled with similar prompting techniques.", "conclusion": "Analogical CoT prompting has the potential to make smaller open-source models more effective in complex reasoning tasks by leveraging analogical reasoning, which is typically underutilized due to training data limitations.", "key_contributions": ["Introduction of Analogical CoT prompting for improved model performance", "Demonstration of Mixture of Experts Model in enhancing LLM capabilities", "Survey of the limitations of existing prompting techniques and their impact on LLM performance"], "limitations": "The paper discusses limitations in the generalizability of the proposed methods and the need for more specialized training data for effective implementation.", "keywords": ["Physics Question Answering", "Mixture of Experts Model", "Analogical prompting", "Few shot prompting", "Chain-of-Thought prompting"], "importance_score": 6, "read_time_minutes": 12}}
{"id": "2412.05453", "pdf": "https://arxiv.org/pdf/2412.05453.pdf", "abs": "https://arxiv.org/abs/2412.05453", "title": "Knowledge Graphs are all you need: Leveraging KGs in Physics Question Answering", "authors": ["Krishnasai Addala", "Kabir Dev Paul Baghel", "Dhruv Jain", "Navya Gupta", "Rishitej Reddy Vyalla", "Chhavi Kirtani", "Avinash Anand", "Rajiv Ratn Shah"], "categories": ["cs.CL"], "comment": null, "summary": "This study explores the effectiveness of using knowledge graphs generated by\nlarge language models to decompose high school-level physics questions into\nsub-questions. We introduce a pipeline aimed at enhancing model response\nquality for Question Answering tasks. By employing LLMs to construct knowledge\ngraphs that capture the internal logic of the questions, these graphs then\nguide the generation of subquestions. We hypothesize that this method yields\nsub-questions that are more logically consistent with the original questions\ncompared to traditional decomposition techniques. Our results show that\nsub-questions derived from knowledge graphs exhibit significantly improved\nfidelity to the original question's logic. This approach not only enhances the\nlearning experience by providing clearer and more contextually appropriate\nsub-questions but also highlights the potential of LLMs to transform\neducational methodologies. The findings indicate a promising direction for\napplying AI to improve the quality and effectiveness of educational content.", "AI": {"tldr": "This study presents a method using knowledge graphs generated by LLMs to decompose physics questions into sub-questions, improving response quality in Question Answering tasks.", "motivation": "To enhance model response quality for educational Question Answering tasks and improve the clarity of sub-questions derived from high school-level physics questions.", "method": "Utilization of large language models to generate knowledge graphs that reflect the internal logic of physics questions, which guide the decomposition into sub-questions.", "result": "Sub-questions created from knowledge graphs are more logically consistent with the original questions than those produced by traditional methods, leading to an improved learning experience.", "conclusion": "The approach demonstrates the potential of LLMs in transforming educational methodologies by enhancing the quality of educational content and learning experiences.", "key_contributions": ["Introduction of a knowledge graph-based decomposition approach for physics questions", "Showcasing improved fidelity of generated sub-questions", "Highlighting transformational potential of LLMs in education"], "limitations": "", "keywords": ["knowledge graphs", "large language models", "sub-questions", "educational methodology", "Question Answering"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.06845", "pdf": "https://arxiv.org/pdf/2412.06845.pdf", "abs": "https://arxiv.org/abs/2412.06845", "title": "7B Fully Open Source Moxin-LLM/VLM -- From Pretraining to GRPO-based Reinforcement Learning Enhancement", "authors": ["Pu Zhao", "Xuan Shen", "Zhenglun Kong", "Yixin Shen", "Sung-En Chang", "Timothy Rupprecht", "Lei Lu", "Enfu Nan", "Changdi Yang", "Yumei He", "Weiyan Shi", "Xingchen Xu", "Yu Huang", "Wei Jiang", "Wei Wang", "Yue Chen", "Yong He", "Yanzhi Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recently, Large Language Models (LLMs) have undergone a significant\ntransformation, marked by a rapid rise in both their popularity and\ncapabilities. Leading this evolution are proprietary LLMs like GPT-4 and\nGPT-o1, which have captured widespread attention in the AI community due to\ntheir remarkable performance and versatility. Simultaneously, open-source LLMs,\nsuch as LLaMA, have made great contributions to the ever-increasing popularity\nof LLMs due to the ease to customize and deploy the models across diverse\napplications. Although open-source LLMs present unprecedented opportunities for\ninnovation and research, the commercialization of LLMs has raised concerns\nabout transparency, reproducibility, and safety. Many open-source LLMs fail to\nmeet fundamental transparency requirements by withholding essential components\nlike training code and data, which may hinder further innovations on LLMs. To\nmitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed,\nadhering to principles of open science, open source, open data, and open\naccess. We release the pre-training code and configurations, training and\nfine-tuning datasets, and intermediate and final checkpoints, aiming to make\ncontinuous commitments to fully open-source LLMs. After pre-training the base\nmodel, we finetune the Moxin Base model with SOTA post-training framework and\ninstruction data to obtain Moxin Instruct model. To improve the reasoning\ncapability, we further finetune our Instruct model with chain-of-thought data\ndistilled from DeepSeek R1, and then use Group Relative Policy Optimization\n(GRPO) following DeepSeek R1 to finetune our model, leading to the Moxin\nReasoning model. Moreover, we develop our vision language model based on our\nMoxin model. Experiments show that our models achieve superior performance in\nvarious evaluations such as zero-shot evaluation, few-shot evaluation, and CoT\nevaluation.", "AI": {"tldr": "Moxin 7B is an open-source Large Language Model developed to enhance transparency, reproducibility, and safety in AI, featuring several fine-tuned versions for improved reasoning capabilities.", "motivation": "The paper addresses concerns about the lack of transparency and reproducibility in LLMs due to proprietary models, while also celebrating the potential of open-source models for innovation and development.", "method": "Moxin 7B was developed by adhering to open science principles; it includes releasing training code and fine-tuning datasets. Multiple models were created, including the base model, Moxin Instruct model with SOTA post-training techniques, and Moxin Reasoning model with advanced fine-tuning methodologies.", "result": "Moxin models demonstrate superior performance in evaluations like zero-shot, few-shot, and chain-of-thought evaluations, showcasing their effectiveness and capabilities in various tasks.", "conclusion": "The development and release of Moxin 7B and its variants set a new standard for transparency in LLM research, aiming to foster innovation while ensuring safety and reproducibility.", "key_contributions": ["Release of Moxin 7B as a fully open-source LLM with comprehensive resources.", "Introduction of enhanced models including Moxin Instruct and Moxin Reasoning for improved task performance.", "Commitment to open science principles, addressing transparency issues prevalent in proprietary LLMs."], "limitations": "", "keywords": ["Large Language Models", "Open-source", "Transparency", "Reproducibility", "AI safety"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.16884", "pdf": "https://arxiv.org/pdf/2501.16884.pdf", "abs": "https://arxiv.org/abs/2501.16884", "title": "Irony Detection, Reasoning and Understanding in Zero-shot Learning", "authors": ["Peiling Yi", "Yuhan Xia", "Yunfei Long"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The generalisation of irony detection faces significant challenges, leading\nto substantial performance deviations when detection models are applied to\ndiverse real-world scenarios. In this study, we find that irony-focused\nprompts, as generated from our IDADP framework for LLMs, can not only overcome\ndataset-specific limitations but also generate coherent, human-readable\nreasoning, transforming ironic text into its intended meaning. Based on our\nfindings and in-depth analysis, we identify several promising directions for\nfuture research aimed at enhancing LLMs' zero-shot capabilities in irony\ndetection, reasoning, and comprehension. These include advancing contextual\nawareness in irony detection, exploring hybrid symbolic-neural methods, and\nintegrating multimodal data, among others.", "AI": {"tldr": "Study addresses challenges in irony detection by using irony-focused prompts generated from the IDADP framework for LLMs, enhancing zero-shot capabilities in diverse scenarios.", "motivation": "The generalisation of irony detection is challenging, leading to poor performance in various real-world applications.", "method": "Utilizes irony-focused prompts generated from the IDADP framework for language models to improve irony detection models.", "result": "The study demonstrates that the IDADP framework can overcome dataset-specific limitations and provides coherent reasoning for interpreting ironic text.", "conclusion": "There are promising future research directions for improving irony detection in LLMs, which include enhancing contextual awareness and integrating multimodal data.", "key_contributions": ["Development of IDADP framework for irony detection", "Demonstration of improved reasoning for interpreting irony", "Identification of future research avenues for LLMs in irony detection"], "limitations": "", "keywords": ["irony detection", "LLMs", "zero-shot capabilities", "contextual awareness", "multimodal data"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2502.16033", "pdf": "https://arxiv.org/pdf/2502.16033.pdf", "abs": "https://arxiv.org/abs/2502.16033", "title": "Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models", "authors": ["Qianqi Yan", "Yue Fan", "Hongquan Li", "Shan Jiang", "Yang Zhao", "Xinze Guan", "Ching-Chen Kuo", "Xin Eric Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing Multimodal Large Language Models (MLLMs) are predominantly trained\nand tested on consistent visual-textual inputs, leaving open the question of\nwhether they can handle inconsistencies in real-world, layout-rich content. To\nbridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR)\nbenchmark to assess MLLMs' ability to detect and reason about semantic\nmismatches in artifacts such as webpages, presentation slides, and posters.\nMMIR comprises 534 challenging samples, each containing synthetically injected\nerrors across five reasoning-heavy categories: Factual Contradiction, Identity\nMisattribution, Contextual Mismatch, Quantitative Discrepancy, and\nTemporal/Spatial Incoherence. We evaluate six state-of-the-art MLLMs, showing\nthat models with dedicated multimodal reasoning capabilities, such as o1,\nsubstantially outperform their counterparts while open-source models remain\nparticularly vulnerable to inconsistency errors. Detailed error analyses\nfurther show that models excel in detecting pairwise inconsistencies but\nstruggle with inconsistencies confined to single elements in complex layouts.\nProbing experiments reveal that single-modality prompting, including\nChain-of-Thought (CoT) and Set-of-Mark (SoM) methods, yields marginal gains,\nrevealing a key bottleneck in cross-modal reasoning. Our findings highlight the\nneed for advanced multimodal reasoning and point to future research on\nmultimodal inconsistency.", "AI": {"tldr": "The paper introduces the MMIR benchmark to evaluate Multimodal Large Language Models (MLLMs) on their ability to reason about inconsistencies in real-world visual-textual content, revealing performance gaps and future research directions.", "motivation": "To assess the ability of MLLMs to handle inconsistencies in layout-rich, real-world content, which previous research has not thoroughly explored.", "method": "The authors propose the MMIR benchmark consisting of 534 samples with five categories of reasoning-heavy semantic mismatches and evaluate six state-of-the-art MLLMs on this benchmark.", "result": "Models with dedicated multimodal reasoning capabilities outperform others, but open-source models struggle with inconsistency errors, particularly in handling complex layouts.", "conclusion": "The study indicates that while MLLMs can detect pairwise inconsistencies well, they face challenges with single-element inconsistencies and calls for advancements in multimodal reasoning.", "key_contributions": ["Introduction of the MMIR benchmark for multimodal inconsistency reasoning.", "Comprehensive evaluation of state-of-the-art MLLMs on the benchmark.", "Identified performance gaps and challenges in current models regarding multimodal reasoning."], "limitations": "The benchmark relies on synthetic errors, which may not fully capture real-world inconsistencies.", "keywords": ["Multimodal Large Language Models", "Inconsistency Reasoning", "Benchmark", "Semantic Mismatches", "Reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.19830", "pdf": "https://arxiv.org/pdf/2502.19830.pdf", "abs": "https://arxiv.org/abs/2502.19830", "title": "Revisiting Self-Consistency from Dynamic Distributional Alignment Perspective on Answer Aggregation", "authors": ["Yiwei Li", "Ji Zhang", "Shaoxiong Feng", "Peiwen Yuan", "Xinglin Wang", "Jiayi Shi", "Yueqi Zhang", "Chuyi Tan", "Boyuan Pan", "Yao Hu", "Kan Li"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "Self-consistency improves reasoning by aggregating diverse stochastic\nsamples, yet the dynamics behind its efficacy remain underexplored. We reframe\nself-consistency as a dynamic distributional alignment problem, revealing that\ndecoding temperature not only governs sampling randomness but also actively\nshapes the latent answer distribution. Given that high temperatures require\nprohibitively large sample sizes to stabilize, while low temperatures risk\namplifying biases, we propose a confidence-driven mechanism that dynamically\ncalibrates temperature: sharpening the sampling distribution under uncertainty\nto align with high-probability modes, and promoting exploration when confidence\nis high. Experiments on mathematical reasoning tasks show this approach\noutperforms fixed-diversity baselines under limited samples, improving both\naverage and best-case performance across varying initial temperatures without\nadditional data or modules. This establishes self-consistency as a\nsynchronization challenge between sampling dynamics and evolving answer\ndistributions.", "AI": {"tldr": "The paper explores self-consistency in reasoning, framing it as a dynamic distributional alignment problem, and proposes a confidence-driven mechanism to optimize sampling temperature, improving performance in mathematical reasoning tasks.", "motivation": "The paper aims to understand the dynamics behind self-consistency in reasoning using stochastic samples and improve its efficacy by addressing temperature management during sampling.", "method": "Introduces a confidence-driven mechanism that dynamically adjusts decoding temperature based on uncertainty, thus aligning the sampling distribution with high-probability modes and promoting exploration.", "result": "The proposed method outperformed fixed-diversity baselines on mathematical reasoning tasks, achieving better average and best-case performance with limited samples and without extra data or modules.", "conclusion": "Self-consistency is framed as a synchronization challenge between sampling dynamics and answer distributions, suggesting temperature management is crucial for optimizing reasoning.", "key_contributions": ["Reframed self-consistency as a dynamic distributional alignment problem", "Proposed a confidence-driven mechanism for temperature calibration", "Demonstrated improved performance on mathematical reasoning tasks under constrained samples"], "limitations": "", "keywords": ["self-consistency", "distributional alignment", "sampling dynamics", "reasoning", "temperature calibration"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2503.01940", "pdf": "https://arxiv.org/pdf/2503.01940.pdf", "abs": "https://arxiv.org/abs/2503.01940", "title": "AskToAct: Enhancing LLMs Tool Use via Self-Correcting Clarification", "authors": ["Xuan Zhang", "Yongliang Shen", "Zhe Zheng", "Linjuan Wu", "Wenqi Zhang", "Yuchen Yan", "Qiuying Peng", "Jun Wang", "Weiming Lu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ntool learning. In real-world scenarios, user queries are often ambiguous and\nincomplete, requiring effective clarification. However, existing interactive\nclarification approaches face two critical limitations: reliance on manually\nconstructed datasets, which inherently constrains training data scale and\ndiversity, and lack of error correction mechanisms during multi-turn\nclarification, leading to error accumulation that compromises both accuracy and\nefficiency. We present AskToAct, which addresses these challenges by exploiting\nthe structural mapping between queries and their tool invocation solutions. Our\nkey insight is that tool parameters naturally represent explicit user intents.\nBy systematically removing key parameters from queries while retaining them as\nground truth, we enable automated construction of high-quality training data.\nWe further enhance model robustness through error-correction pairs and\nselective masking, enabling dynamic error detection during clarification\ninteractions. Comprehensive experiments demonstrate that AskToAct significantly\noutperforms existing approaches, achieving above 57% accuracy in recovering\ncritical unspecified intents and enhancing clarification efficiency by an\naverage of 10.46% while maintaining high accuracy in tool invocation. Our\nframework exhibits robust performance across different model architectures and\nsuccessfully generalizes to entirely unseen APIs without additional training,\nachieving performance comparable to GPT-4o with substantially fewer\ncomputational resources.", "AI": {"tldr": "AskToAct is a framework for improving tool learning in large language models by automating the construction of high-quality training data and enhancing robustness through error correction mechanisms.", "motivation": "Existing interactive clarification approaches for LLMs are limited by reliance on manually constructed datasets and a lack of error correction, affecting accuracy and efficiency.", "method": "AskToAct utilizes structural mapping between queries and tool invocation solutions, systematically removing key parameters from queries while keeping them as ground truth to create automated and diverse training data. It introduces error-correction pairs and selective masking for dynamic error detection.", "result": "AskToAct outperforms existing approaches, achieving over 57% accuracy in recovering unspecified intents and improving clarification efficiency by an average of 10.46%, while maintaining tool invocation accuracy.", "conclusion": "The framework demonstrates robust performance across various model architectures and generalizes well to unseen APIs without additional training, achieving results comparable to advanced models with lesser computational resources.", "key_contributions": ["Automated construction of high-quality training data for clarification tasks", "Error correction mechanisms during multi-turn interactions", "Generalization to unseen APIs without extra training"], "limitations": "", "keywords": ["large language models", "tool learning", "clarification", "error correction", "API generalization"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.11314", "pdf": "https://arxiv.org/pdf/2503.11314.pdf", "abs": "https://arxiv.org/abs/2503.11314", "title": "Unlocking General Long Chain-of-Thought Reasoning Capabilities of Large Language Models via Representation Engineering", "authors": ["Xinyu Tang", "Xiaolei Wang", "Zhihao Lv", "Yingqian Min", "Wayne Xin Zhao", "Binbin Hu", "Ziqi Liu", "Zhiqiang Zhang"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Recent advancements in long chain-of-thoughts(long CoTs) have significantly\nimproved the reasoning capabilities of large language models(LLMs). Existing\nwork finds that the capability of long CoT reasoning can be efficiently\nelicited by tuning on only a few examples and can easily transfer to other\ntasks. This motivates us to investigate whether long CoT reasoning is a general\ncapability for LLMs. In this work, we conduct an empirical analysis for this\nquestion from the perspective of representation. We find that LLMs do encode\nlong CoT reasoning as a general capability, with a clear distinction from\nvanilla CoTs. Furthermore, domain-specific representations are also required\nfor the effective transfer of long CoT reasoning. Inspired by these findings,\nwe propose GLoRE, a novel representation engineering method to unleash the\ngeneral long CoT reasoning capabilities of LLMs. Extensive experiments\ndemonstrate the effectiveness and efficiency of GLoRE in both in-domain and\ncross-domain scenarios.", "AI": {"tldr": "This paper explores the general capability of long chain-of-thought reasoning in large language models (LLMs), proposing a method named GLoRE for effective representation engineering.", "motivation": "To investigate if long CoT reasoning is a general capability of LLMs and to enhance its transferability across tasks.", "method": "Empirical analysis of long CoT reasoning in LLMs and the development of GLoRE for representation engineering.", "result": "The study finds that LLMs encode long CoT reasoning as a distinct capability, requiring domain-specific representations for effective transfer.", "conclusion": "GLoRE effectively harnesses the general long CoT reasoning capabilities of LLMs, demonstrating efficiency in both in-domain and cross-domain scenarios.", "key_contributions": ["Identification of long CoT reasoning as a general capability of LLMs", "Development of GLoRE for representation engineering", "Evidence of effective transfer requirements in domain-specific contexts"], "limitations": "", "keywords": ["long CoT reasoning", "large language models", "representation engineering", "GLoRE", "transfer learning"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2504.01738", "pdf": "https://arxiv.org/pdf/2504.01738.pdf", "abs": "https://arxiv.org/abs/2504.01738", "title": "Style over Substance: Distilled Language Models Reason Via Stylistic Replication", "authors": ["Philip Lippmann", "Jie Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Specialized reasoning language models (RLMs) have demonstrated that scaling\ntest-time computation through detailed reasoning traces significantly enhances\nperformance. Although these traces effectively facilitate knowledge\ndistillation into smaller, instruction-tuned models, the precise nature of\ntransferred reasoning remains unclear. In this study, we investigate to what\nextent distilled models internalize replicated stylistic patterns during\nreasoning. To this end, we systematically analyze reasoning traces, identifying\nstructural and lexical patterns that characterize successful reasoning. We then\nintroduce two new datasets -- a dataset of emergent reasoning traces and a\nsynthetic dataset explicitly constructed to replicate these stylistic patterns\n-- to precisely examine their influence on distilled models' reasoning\ncapabilities. We find that models trained on the synthetic traces achieve\ncomparable performance, indicating that distilled reasoning abilities rely\nsignificantly on surface-level patterns. Surprisingly, we observe an increase\nin performance even when the synthetic traces are altered to lead to the wrong\nanswer. Our findings highlight how stylistic patterns can be leveraged to\nefficiently enhance LM reasoning across diverse model families.", "AI": {"tldr": "This study explores how specialized reasoning language models (RLMs) enhance performance through detailed reasoning traces and examines how distilled models internalize stylistic patterns during reasoning.", "motivation": "Understanding the transfer of reasoning abilities in distilled models and how stylistic patterns influence model performance is crucial for improving language models.", "method": "The authors analyze reasoning traces to identify structural and lexical patterns, and introduce two new datasets to examine the influence of these patterns on distilled models' reasoning capabilities.", "result": "Models trained on synthetic reasoning traces exhibit comparable performance to those trained on authentic traces, suggesting reliance on surface-level patterns.", "conclusion": "The findings indicate that stylistic patterns significantly enhance language model reasoning, even when input traces are misleading.", "key_contributions": ["Introduction of two new datasets for analyzing reasoning traces", "Findings that performance improves with synthetic traces, even if misleading", "Identification of reliance on surface-level patterns for distilled models' reasoning"], "limitations": "The study primarily focuses on surface-level patterns and may not account for deeper semantic understanding.", "keywords": ["Reasoning Language Models", "Knowledge Distillation", "Stylistic Patterns"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2504.02132", "pdf": "https://arxiv.org/pdf/2504.02132.pdf", "abs": "https://arxiv.org/abs/2504.02132", "title": "One Pic is All it Takes: Poisoning Visual Document Retrieval Augmented Generation with a Single Image", "authors": ["Ezzeldin Shereen", "Dan Ristea", "Shae McFadden", "Burak Hasircioglu", "Vasilios Mavroudis", "Chris Hicks"], "categories": ["cs.CL", "cs.CR", "cs.CV", "cs.IR"], "comment": "19 pages, 7 figures", "summary": "Multi-modal retrieval augmented generation (M-RAG) is instrumental for\ninhibiting hallucinations in large multi-modal models (LMMs) through the use of\na factual knowledge base (KB). However, M-RAG introduces new attack vectors for\nadversaries that aim to disrupt the system by injecting malicious entries into\nthe KB. In this paper, we present the first poisoning attack against M-RAG\ntargeting visual document retrieval applications where the KB contains images\nof document pages. We propose two attacks, each of which require injecting only\na single adversarial image into the KB. Firstly, we propose a universal attack\nthat, for any potential user query, influences the response to cause a\ndenial-of-service (DoS) in the M-RAG system. Secondly, we present a targeted\nattack against one or a group of user queries, with the goal of spreading\ntargeted misinformation. For both attacks, we use a multi-objective\ngradient-based adversarial approach to craft the injected image while\noptimizing for both retrieval and generation. We evaluate our attacks against\nseveral visual document retrieval datasets, a diverse set of state-of-the-art\nretrievers (embedding models) and generators (LMMs), demonstrating the attack\neffectiveness in both the universal and targeted settings. We additionally\npresent results including commonly used defenses, various attack\nhyper-parameter settings, ablations, and attack transferability.", "AI": {"tldr": "The paper introduces the first poisoning attack against multi-modal retrieval augmented generation (M-RAG) systems by proposing two types of attacks that inject adversarial images into the knowledge base to influence response generation.", "motivation": "To address the vulnerabilities of M-RAG systems in visual document retrieval applications due to potential attacks from adversaries injecting malicious entries into the knowledge base.", "method": "Two types of poisoning attacks are proposed: a universal attack that causes denial-of-service responses for any query, and a targeted attack that distributes misinformation for specific queries. Both methods employ a multi-objective gradient-based adversarial approach to optimize the injected images.", "result": "The proposed attacks were evaluated on various visual document retrieval datasets and demonstrated effectiveness against multiple state-of-the-art retrievers and generators, highlighting the impact of each attack in both universal and targeted scenarios.", "conclusion": "These findings reveal significant vulnerabilities in M-RAG systems and the necessity for robust defenses against such poisoning attacks in multi-modal applications.", "key_contributions": ["First poisoning attack in M-RAG frameworks", "Proposed universal and targeted poisoning attacks", "Evaluation against state-of-the-art systems with detailed analysis of attack effects."], "limitations": "The effectiveness of defense mechanisms against these attacks was not extensively explored; additional work is needed to evaluate real-world implications on user queries.", "keywords": ["multi-modal retrieval", "poisoning attack", "adversarial image", "knowledge base", "M-RAG"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.04745", "pdf": "https://arxiv.org/pdf/2504.04745.pdf", "abs": "https://arxiv.org/abs/2504.04745", "title": "Can LLMs Interpret and Leverage Structured Linguistic Representations? A Case Study with AMRs", "authors": ["Ankush Raut", "Xiaofeng Zhu", "Maria Leonor Pacheco"], "categories": ["cs.CL"], "comment": "13 pages, 23 figures. Accepted at XLLM @ ACL 2025", "summary": "This paper evaluates the ability of Large Language Models (LLMs) to leverage\ncontextual information in the form of structured linguistic representations.\nSpecifically, we examine the impact of encoding both short and long contexts\nusing Abstract Meaning Representation (AMR) structures across a diverse set of\nlanguage tasks. We perform our analysis using 8-bit quantized and\ninstruction-tuned versions of Llama 3.1 (8B), Phi-3, and Mistral 7B. Our\nresults indicate that, for tasks involving short contexts, augmenting the\nprompt with the AMR of the original language context often degrades the\nperformance of the underlying LLM. However, for tasks that involve long\ncontexts, such as dialogue summarization in the SAMSum dataset, this\nenhancement improves LLM performance, for example, by increasing the zero-shot\ncosine similarity score of Llama 3.1 from 66% to 76%. This improvement is more\nevident in the newer and larger LLMs, but does not extend to the older or\nsmaller ones. In addition, we observe that LLMs can effectively reconstruct the\noriginal text from a linearized AMR, achieving a cosine similarity of 81% in\nthe best-case scenario.", "AI": {"tldr": "Evaluation of LLMs' performance using structured linguistic representations for context in language tasks, focusing on Abstract Meaning Representation (AMR).", "motivation": "To understand the effectiveness of providing contextual information to LLMs through structured representations in various language tasks.", "method": "We analyzed the use of AMR structures with different models (Llama 3.1, Phi-3, and Mistral 7B), focusing on both short and long contexts across multiple language tasks.", "result": "Augmenting prompts with AMR degrades performance for short contexts but enhances performance for long contexts, especially in dialogue summarization, increasing similarity scores significantly in newer LLMs.", "conclusion": "While AMR can be useful, its effectiveness varies depending on context length and model size, indicating that not all models benefit equally from this augmentation strategy.", "key_contributions": ["Demonstrated varying impacts of AMR on LLM performance based on context length.", "Identified the specific conditions under which AMR enhances LLM tasks, particularly in dialogue summarization.", "Provided insights on the reconstruction ability of LLMs using AMR structures."], "limitations": "The performance impacts are not uniform across different models, with older or smaller LLMs showing little to no improvement.", "keywords": ["Large Language Models", "Abstract Meaning Representation", "contextual information", "dialogue summarization", "language tasks"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.12347", "pdf": "https://arxiv.org/pdf/2504.12347.pdf", "abs": "https://arxiv.org/abs/2504.12347", "title": "Assessment of Evolving Large Language Models in Upper Secondary Mathematics", "authors": ["Mika Setälä", "Pieta Sikström", "Ville Heilala", "Tommi Kärkkäinen"], "categories": ["cs.CL", "cs.AI", "cs.CY", "K.3; I.2"], "comment": null, "summary": "Large language models (LLMs) have shown increasing promise in educational\nsettings, yet their mathematical reasoning has been considered evolving. This\nstudy evaluates the mathematical capabilities of various LLMs using the Finnish\nmatriculation examination, a high-stakes digital test for upper secondary\neducation. Initial tests yielded moderate performance corresponding to\nmid-range grades, but later evaluations demonstrated substantial improvements\nas the language models evolved. Remarkably, some models achieved near-perfect\nor perfect scores, matching top student performance and qualifying for\nuniversity admission. Our findings highlight the rapid advances in the\nmathematical proficiency of LLMs and illustrate their potential as underlying\ntools to support learning and teaching in a variety of ways.", "AI": {"tldr": "This study assesses the mathematical capabilities of large language models (LLMs) using the Finnish matriculation examination, revealing significant improvements in performance over time.", "motivation": "To evaluate the evolving mathematical reasoning capabilities of LLMs in educational contexts.", "method": "The study employed the Finnish matriculation examination as a benchmark to assess the performance of various LLMs, analyzing initial and later evaluations to track improvements.", "result": "Initial tests showed moderate performance, but subsequent evaluations indicated that some LLMs achieved near-perfect or perfect scores, comparable to top human students.", "conclusion": "The rapid advancements in mathematical proficiency of LLMs suggest their potential as supportive tools in educational settings.", "key_contributions": ["Evaluation of LLMs using a high-stakes educational test", "Demonstration of significant performance improvements in mathematical reasoning", "Insights into the application of LLMs for supporting learning and teaching"], "limitations": "", "keywords": ["Large language models", "Mathematical reasoning", "Educational tools", "Finnish matriculation examination", "Performance evaluation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.12663", "pdf": "https://arxiv.org/pdf/2504.12663.pdf", "abs": "https://arxiv.org/abs/2504.12663", "title": "Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment", "authors": ["Xiaotian Zhang", "Ruizhe Chen", "Yang Feng", "Zuozhu Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL Finding", "summary": "Aligning language models with human preferences presents significant\nchallenges, particularly in achieving personalization without incurring\nexcessive computational costs. Existing methods rely on reward signals and\nadditional annotated data, limiting their scalability and adaptability to\ndiverse human values. To address these challenges, we introduce Persona-judge,\na novel discriminative paradigm that enables training-free personalized\nalignment with unseen preferences. Instead of optimizing policy parameters\nthrough external reward feedback, Persona-judge leverages the intrinsic\npreference judgment capabilities of the model. Specifically, a draft model\ngenerates candidate tokens conditioned on a given preference, while a judge\nmodel, embodying another preference, cross-validates the predicted tokens\nwhether to be accepted. Experimental results demonstrate that Persona-judge,\nusing the inherent preference evaluation mechanisms of the model, offers a\nscalable and computationally efficient solution to personalized alignment,\npaving the way for more adaptive customized alignment. Our code is available\nhere.", "AI": {"tldr": "Introducing Persona-judge for personalized alignment of language models without additional computational cost.", "motivation": "To tackle the challenges of aligning language models with diverse human preferences efficiently and scalably.", "method": "Persona-judge uses a draft model to generate candidate tokens based on a preference, which are then validated by a judge model that embodies another preference, enabling training-free alignment.", "result": "Experimental findings show that Persona-judge improves scalability and computational efficiency in personalized model alignment while using intrinsic evaluation capabilities.", "conclusion": "Persona-judge offers a pathway for adaptive personalized alignment of language models without the need for additional annotated data or external rewards.", "key_contributions": ["Introduction of a training-free personalized alignment approach", "Utilization of intrinsic preference judgment capabilities of models", "Demonstrated efficiency and scalability in experiments"], "limitations": "", "keywords": ["language models", "personalization", "preference alignment", "computational efficiency", "adaptive alignment"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.06821", "pdf": "https://arxiv.org/pdf/2506.06821.pdf", "abs": "https://arxiv.org/abs/2506.06821", "title": "Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems", "authors": ["Yuhan Cao", "Zian Chen", "Kun Quan", "Ziliang Zhang", "Yu Wang", "Xiaoning Dong", "Yeqi Feng", "Guanzhong He", "Jingcheng Huang", "Jianhao Li", "Yixuan Tan", "Jiafu Tang", "Yilin Tang", "Junlei Wu", "Qianyu Xiao", "Can Zheng", "Shouchen Zhou", "Yuxiang Zhu", "Yiming Huang", "Tian Xie", "Tianxing He"], "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": "37 pages, 22 figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode generation, capable of tackling complex tasks during inference. However,\nthe extent to which LLMs can be utilized for code checking or debugging through\ntest case generation remains largely unexplored. We investigate this problem\nfrom the perspective of competition-level programming (CP) programs and propose\nTCGBench, a Benchmark for (LLM generation of) Test Case Generators. This\nbenchmark comprises two tasks, aimed at studying the capabilities of LLMs in\n(1) generating valid test case generators for a given CP problem, and further\n(2) generating targeted test case generators that expose bugs in human-written\ncode. Experimental results indicate that while state-of-the-art LLMs can\ngenerate valid test case generators in most cases, most LLMs struggle to\ngenerate targeted test cases that reveal flaws in human code effectively.\nEspecially, even advanced reasoning models (e.g., o3-mini) fall significantly\nshort of human performance in the task of generating targeted generators.\nFurthermore, we construct a high-quality, manually curated dataset of\ninstructions for generating targeted generators. Analysis demonstrates that the\nperformance of LLMs can be enhanced with the aid of this dataset, by both\nprompting and fine-tuning.", "AI": {"tldr": "This paper introduces TCGBench, a benchmark for evaluating the ability of Large Language Models (LLMs) in generating test case generators for competitive programming (CP) problems and targeted generators that expose bugs in human-written code.", "motivation": "The research addresses the unexplored potential of LLMs in code checking and debugging through test case generation, particularly in the context of competitive programming problems.", "method": "The authors proposed TCGBench, consisting of two tasks: one for generating valid test case generators for specific CP problems and another for generating targeted test case generators to expose bugs in human-written code. They conducted experiments with various state-of-the-art LLMs to assess their performance on these tasks.", "result": "The experimental results revealed that while most LLMs can generate valid test case generators, they generally struggle with generating targeted test cases that effectively reveal bugs in code, with advanced models significantly underperforming compared to human capabilities.", "conclusion": "The study concludes that although LLMs show some capability in generating test case generators, there's a significant gap in their ability to produce targeted generators that expose flaws effectively. The authors also provide a high-quality dataset to assist in improving LLM performance through prompting and fine-tuning.", "key_contributions": ["Introduction of TCGBench as a benchmark for evaluating LLMs in test case generation.", "Creation of a high-quality dataset for generating targeted test case generators.", "Comparative analysis of LLM performance against human benchmarks in code debugging tasks."], "limitations": "LLMs' performance on generating targeted test cases is significantly below human performance, and further enhancements are necessary to close this gap.", "keywords": ["Large Language Models", "Test Case Generation", "Competitive Programming", "Code Debugging", "Machine Learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.07664", "pdf": "https://arxiv.org/pdf/2506.07664.pdf", "abs": "https://arxiv.org/abs/2506.07664", "title": "Synthesis by Design: Controlled Data Generation via Structural Guidance", "authors": ["Lei Xu", "Sirui Chen", "Yuxuan Huang", "Chaochao Lu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Mathematical reasoning remains challenging for LLMs due to complex logic and\nthe need for precise computation. Existing methods enhance LLM reasoning by\nsynthesizing datasets through problem rephrasing, but face issues with\ngeneration quality and problem complexity. To address this, we propose to\nextract structural information with generated problem-solving code from\nmathematical reasoning and guide data generation with structured solutions.\nApplied to MATH and GSM8K, our approach produces 39K problems with labeled\nintermediate steps and a 6.1K-problem benchmark of higher difficulty. Results\non our benchmark show that model performance declines as reasoning length\nincreases. Additionally, we conducted fine-tuning experiments using the\nproposed training data on a range of LLMs, and the results validate the\neffectiveness of our dataset. We hope the proposed method and dataset will\ncontribute to future research in enhancing LLM reasoning capabilities. Our code\nand data are available at https://github.com/OpenCausaLab/StructuralGeneration.", "AI": {"tldr": "The paper proposes a method to enhance mathematical reasoning in LLMs by extracting structural information and guiding data generation, producing a dataset with labeled intermediate steps and a benchmark of higher difficulty, demonstrating improved model performance through fine-tuning experiments.", "motivation": "To improve mathematical reasoning in LLMs, addressing issues of generation quality and problem complexity.", "method": "Extract structural information with generated problem-solving code and guide data generation with structured solutions; applied to MATH and GSM8K datasets.", "result": "Generated 39K problems with labeled intermediate steps and a benchmark of 6.1K higher difficulty problems; performance declines with longer reasoning lengths in LLMs.", "conclusion": "The dataset and method can contribute to enhancing LLM reasoning capabilities and are made available for future research.", "key_contributions": ["Introduced a novel method for data generation to support LLM mathematical reasoning.", "Created a benchmark of difficult problems with labeled intermediate steps.", "Demonstrated the effectiveness of the dataset through fine-tuning experiments on various LLMs."], "limitations": "The model performance decline with increased reasoning lengths indicates potential scalability issues.", "keywords": ["Mathematical reasoning", "LLMs", "Data generation", "Fine-tuning", "Benchmark"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.07751", "pdf": "https://arxiv.org/pdf/2506.07751.pdf", "abs": "https://arxiv.org/abs/2506.07751", "title": "AbstRaL: Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking", "authors": ["Silin Gao", "Antoine Bosselut", "Samy Bengio", "Emmanuel Abbe"], "categories": ["cs.CL", "cs.AI", "cs.SC"], "comment": "Under review", "summary": "Recent studies have shown that large language models (LLMs), especially\nsmaller ones, often lack robustness in their reasoning. I.e., they tend to\nexperience performance drops when faced with distribution shifts, such as\nchanges to numerical or nominal variables, or insertions of distracting\nclauses. A possible strategy to address this involves generating synthetic data\nto further \"instantiate\" reasoning problems on potential variations. In\ncontrast, our approach focuses on \"abstracting\" reasoning problems. This not\nonly helps counteract distribution shifts but also facilitates the connection\nto symbolic tools for deriving solutions. We find that this abstraction process\nis better acquired through reinforcement learning (RL) than just supervised\nfine-tuning, which often fails to produce faithful abstractions. Our method,\nAbstRaL -- which promotes abstract reasoning in LLMs using RL on granular\nabstraction data -- significantly mitigates performance degradation on recent\nGSM perturbation benchmarks.", "AI": {"tldr": "The paper presents AbstRaL, a method using reinforcement learning to enhance abstract reasoning in large language models (LLMs), addressing robustness issues during distribution shifts.", "motivation": "To improve LLMs' robustness against distribution shifts in reasoning tasks, which smaller models often struggle with due to performance drops.", "method": "The approach involves abstracting reasoning problems through reinforcement learning rather than relying solely on supervised fine-tuning, thus creating more reliable abstract reasoning capabilities in LLMs.", "result": "AbstRaL significantly reduces performance degradation on GSM perturbation benchmarks compared to traditional methods.", "conclusion": "The study suggests that reinforcement learning fosters a better understanding of abstractions in LLMs, leading to improved reasoning under various conditions.", "key_contributions": ["Introduction of AbstRaL, a novel method for enhancing reasoning in LLMs using RL", "Demonstration of improved robustness against distribution shifts compared to supervised learning", "Connection between abstract reasoning and symbolic tools for deriving solutions."], "limitations": "", "keywords": ["large language models", "abstract reasoning", "reinforcement learning", "robustness", "synthetic data"], "importance_score": 9, "read_time_minutes": 10}}
