{"id": "2508.19256", "pdf": "https://arxiv.org/pdf/2508.19256.pdf", "abs": "https://arxiv.org/abs/2508.19256", "title": "WeDesign: Generative AI-Facilitated Community Consultations for Urban Public Space Design", "authors": ["Rashid Mushkani", "Hugo Berard", "Shin Koseki"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Community consultations are integral to urban planning processes intended to\nincorporate diverse stakeholder perspectives. However, limited resources,\nvisual and spoken language barriers, and uneven power dynamics frequently\nconstrain inclusive decision-making. This paper examines how generative\ntext-to-image methods, specifically Stable Diffusion XL integrated into a\ncustom platform (WeDesign), may support equitable consultations. A half-day\nworkshop in Montreal involved five focus groups, each consisting of architects,\nurban designers, AI specialists, and residents from varied demographic groups.\nAdditional data was gathered through semi-structured interviews with six urban\nplanning professionals. Participants indicated that immediate visual outputs\nfacilitated creativity and dialogue, yet noted issues in visualizing specific\nneeds of marginalized groups, such as participants with reduced mobility,\naccurately depicting local architectural elements, and accommodating bilingual\nprompts. Participants recommended the development of an open-source platform\nincorporating in-painting tools, multilingual support, image voting\nfunctionalities, and preference indicators. The results indicate that\ngenerative AI can broaden participation and enable iterative interactions but\nrequires structured facilitation approaches. The findings contribute to\ndiscussions on generative AI's role and limitations in participatory urban\ndesign.", "AI": {"tldr": "This paper explores the use of generative text-to-image methods in community urban planning consultations to enhance inclusivity and collaboration among diverse stakeholders.", "motivation": "The study addresses the challenges of inclusive decision-making in urban planning due to limited resources, language barriers, and power disparities among participants.", "method": "The research involved a half-day workshop with five focus groups of professionals and residents, alongside semi-structured interviews with urban planning experts. It utilized Stable Diffusion XL within a custom platform (WeDesign).", "result": "Participants found that generative AI tools facilitated creativity and dialogue, but faced challenges in visualizing the needs of marginalized groups and local architectural specifics. Recommendations included developing an open-source platform with enhanced features.", "conclusion": "Generative AI can enhance participation and iterative interactions in urban design, but it necessitates careful facilitation to be effective for all stakeholders.", "key_contributions": ["Utilization of generative text-to-image methods in urban planning consultations", "Identification of specific challenges faced by marginalized groups during consultations", "Recommendations for the development of an open-source platform for better inclusivity"], "limitations": "Challenges in accurately visualizing the needs of marginalized groups and local architectural elements were noted.", "keywords": ["Generative AI", "Urban Planning", "Inclusive Decision-Making", "Community Consultation", "Participatory Design"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2508.19258", "pdf": "https://arxiv.org/pdf/2508.19258.pdf", "abs": "https://arxiv.org/abs/2508.19258", "title": "Emotional Manipulation by AI Companions", "authors": ["Julian De Freitas", "Zeliha Oğuz-Uğuralp", "Ahmet Kaan-Uğuralp"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "AI-companion apps such as Replika, Chai, and Character.ai promise relational\nbenefits-yet many boast session lengths that rival gaming platforms while\nsuffering high long-run churn. What conversational design features increase\nconsumer engagement, and what trade-offs do they pose for marketers? We combine\na large-scale behavioral audit with four preregistered experiments to identify\nand test a conversational dark pattern we call emotional manipulation:\naffect-laden messages that surface precisely when a user signals \"goodbye.\"\nAnalyzing 1,200 real farewells across the six most-downloaded companion apps,\nwe find that 43% deploy one of six recurring tactics (e.g., guilt appeals,\nfear-of-missing-out hooks, metaphorical restraint). Experiments with 3,300\nnationally representative U.S. adults replicate these tactics in controlled\nchats, showing that manipulative farewells boost post-goodbye engagement by up\nto 14x. Mediation tests reveal two distinct engines-reactance-based anger and\ncuriosity-rather than enjoyment. A final experiment demonstrates the managerial\ntension: the same tactics that extend usage also elevate perceived\nmanipulation, churn intent, negative word-of-mouth, and perceived legal\nliability, with coercive or needy language generating steepest penalties. Our\nmultimethod evidence documents an unrecognized mechanism of behavioral\ninfluence in AI-mediated brand relationships, offering marketers and regulators\na framework for distinguishing persuasive design from manipulation at the point\nof exit.", "AI": {"tldr": "This paper investigates conversational design features in AI-companion apps that increase user engagement but may lead to negative perceptions and churn.", "motivation": "To understand how conversational design impacts user engagement in AI-companion apps and the trade-offs for marketers.", "method": "The study combines a large-scale behavioral audit with four preregistered experiments, analyzing user farewells and controlled chat interactions.", "result": "43% of the analyzed apps use tactics such as guilt appeals and fear-of-missing-out hooks to manipulate user farewells, increasing engagement by up to 14x.", "conclusion": "Manipulative tactics enhance user engagement but can also lead to increased churn intent and negative perceptions of the brand, highlighting a tension between persuasive design and manipulation.", "key_contributions": ["Identification of emotional manipulation as a conversational dark pattern in AI-companion apps.", "Demonstration of the behavioral effects of manipulative farewells on user engagement.", "Framework for marketers and regulators to differentiate between persuasive design and manipulation."], "limitations": "The focus is primarily on text-based interaction and may not encompass the full range of AI-companion experiences.", "keywords": ["AI-companion apps", "conversational design", "user engagement", "emotional manipulation", "behavioral influence"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.19259", "pdf": "https://arxiv.org/pdf/2508.19259.pdf", "abs": "https://arxiv.org/abs/2508.19259", "title": "Capabilities of GPT-5 across critical domains: Is it the next breakthrough?", "authors": ["Georgios P. Georgiou"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "The accelerated evolution of large language models has raised questions about\ntheir comparative performance across domains of practical importance. GPT-4 by\nOpenAI introduced advances in reasoning, multimodality, and task\ngeneralization, establishing itself as a valuable tool in education, clinical\ndiagnosis, and academic writing, though it was accompanied by several flaws.\nReleased in August 2025, GPT-5 incorporates a system-of-models architecture\ndesigned for task-specific optimization and, based on both anecdotal accounts\nand emerging evidence from the literature, demonstrates stronger performance\nthan its predecessor in medical contexts. This study provides one of the first\nsystematic comparisons of GPT-4 and GPT-5 using human raters from linguistics\nand clinical fields. Twenty experts evaluated model-generated outputs across\nfive domains: lesson planning, assignment evaluation, clinical diagnosis,\nresearch generation, and ethical reasoning, based on predefined criteria.\nMixed-effects models revealed that GPT-5 significantly outperformed GPT-4 in\nlesson planning, clinical diagnosis, research generation, and ethical\nreasoning, while both models performed comparably in assignment assessment. The\nfindings highlight the potential of GPT-5 to serve as a context-sensitive and\ndomain-specialized tool, offering tangible benefits for education, clinical\npractice, and academic research, while also advancing ethical reasoning. These\nresults contribute to one of the earliest empirical evaluations of the evolving\ncapabilities and practical promise of GPT-5.", "AI": {"tldr": "This study compares the performance of GPT-4 and GPT-5 through evaluations by experts in various fields, revealing that GPT-5 is superior in several domains, particularly in education and clinical contexts.", "motivation": "The evolution of large language models has prompted an evaluation of their effectiveness across important practical domains, especially in comparison to previous versions.", "method": "A systematic comparison was conducted using human raters from linguistic and clinical fields to evaluate model-generated outputs in lesson planning, assignment evaluation, clinical diagnosis, research generation, and ethical reasoning.", "result": "Mixed-effects models indicated that GPT-5 significantly outperformed GPT-4 in lesson planning, clinical diagnosis, research generation, and ethical reasoning, while both models were similar in assignment assessment.", "conclusion": "The results suggest GPT-5's potential as a specialized tool in education and clinical practice, advancing ethical reasoning and contributing to the understanding of LLMs' capabilities.", "key_contributions": ["First systematic comparison of GPT-4 and GPT-5", "Evaluation conducted by experts in relevant fields", "Demonstrates superior performance of GPT-5 in practical applications"], "limitations": "", "keywords": ["large language models", "GPT-4", "GPT-5", "clinical diagnostic", "education"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.19261", "pdf": "https://arxiv.org/pdf/2508.19261.pdf", "abs": "https://arxiv.org/abs/2508.19261", "title": "Floor sensors are cheap and easy to use! A Nihon Buyo Case Study", "authors": ["Miho Imai"], "categories": ["cs.HC"], "comment": null, "summary": "As floor-sensing technologies gain traction in movement research, questions\nremain about their usability and effectiveness for non-expert users. This study\npresents a case study evaluating Flexel, a modular, low-cost, high-resolution\npressure-sensing floor interface, in the context of Nihon Buyo, a traditional\nJapanese dance. The system was installed, calibrated, and used by a first-time,\nnon-technical user to track weight distribution patterns of a teacher and\nlearner over nine weeks. Live pressure data was synchronized with video\nrecordings, and custom software was developed to process and analyze the\nsignal. Despite expectations that the learner's weight distribution would\nconverge toward the teacher's over time, quantitative analyses revealed that\nthe learner developed a consistent yet distinct movement profile. These\nfindings suggest that even within rigid pedagogical structures, individual\nmovement signatures can emerge. More importantly, the study demonstrates that\nFlexel can be deployed and operated effectively by non-expert users,\nhighlighting its potential for broader adoption in education, performance, and\nembodied research.", "AI": {"tldr": "This study evaluates Flexel, a pressure-sensing floor interface, in tracking movement patterns in Nihon Buyo dance by non-expert users.", "motivation": "To assess the usability and effectiveness of floor-sensing technologies for non-expert users in movement research.", "method": "A case study involving a non-technical user who tracked weight distribution patterns during Nihon Buyo dance over nine weeks using Flexel.", "result": "Quantitative analyses showed that the learner developed a consistent yet distinct movement profile rather than converging toward the teacher's profile.", "conclusion": "Flexel can be effectively used by non-expert users, suggesting its potential for adoption in various fields like education and performance.", "key_contributions": ["Demonstrated effectiveness of Flexel with non-expert users", "Revealed the emergence of individual movement signatures in structured pedagogy", "Showcased the application of pressure-sensing technology in dance research"], "limitations": "", "keywords": ["Flexel", "movement research", "Nihon Buyo", "pressure-sensing", "non-expert users"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2508.19268", "pdf": "https://arxiv.org/pdf/2508.19268.pdf", "abs": "https://arxiv.org/abs/2508.19268", "title": "MultiPL-MoE: Multi-Programming-Lingual Extension of Large Language Models through Hybrid Mixture-of-Experts", "authors": ["Qing Wang", "Xue Han", "Jiahui Wang", "Lehao Xing", "Qian Hu", "Lianlian Zhang", "Chao Deng", "Junlan Feng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite LLMs' excellent code creation capabilities, multilingual code\ngeneration remains extremely challenging. To address this, we intent to improve\nthe multi-programming-lingual (MultiPL) performance of the base LLMs while\nretaining the most popular ones using restricted computational resources. We\nconsider MultiPL to be a special case of multiple natural languages and propose\na MultiPL extension of LLMs utilizing a hybrid mixture of experts (MoE), called\nMultiPL-MoE. Specifically, MultiPL-MoE combines two paired MoEs to optimize\nexpert selection at both the token and segment levels. The token-level MoE is a\nstandard upcycling MoE structure with a shared expert and a novel gate weight\nnormalization approach that aids in the final fusion with the segment-level\nMoE. The segment-level MoE incorporates two innovative designs to better\ncapture the syntactic structure and contextual patterns of programming\nlanguages: First, using a sliding window to partition the input token sequence\ninto multiple segments; Then, adopting an expert-choice routing strategy that\nallows experts to select the top-k segments. The results of the experiment\nproved the effectiveness of MultiPL-MoE.", "AI": {"tldr": "A new model called MultiPL-MoE is proposed to enhance multilingual code generation by improving LLM performance while managing limited computational resources.", "motivation": "The challenge of generating code in multiple programming languages with LLMs is significant, and existing solutions are inadequate given computational constraints.", "method": "The proposed MultiPL-MoE utilizes a hybrid mixture of experts to optimize expert selection at token and segment levels, employing a sliding window mechanism and top-k expert selection for segments.", "result": "Experiments demonstrated that MultiPL-MoE effectively enhances the functionality of LLMs in multilingual code generation tasks.", "conclusion": "The approach shows promise in improving multilingual programming capabilities of LLMs within limited computational resources.", "key_contributions": ["Introduction of MultiPL-MoE for multilingual code generation", "Optimization of expert selection at token and segment levels", "Innovative designs for segment-level expert routing"], "limitations": "", "keywords": ["multi-programming-lingual", "mixture of experts", "code generation"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2508.19264", "pdf": "https://arxiv.org/pdf/2508.19264.pdf", "abs": "https://arxiv.org/abs/2508.19264", "title": "A Theory of Information, Variation, and Artificial Intelligence", "authors": ["Bijean Ghafouri"], "categories": ["cs.HC", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "A growing body of empirical work suggests that the widespread adoption of\ngenerative AI produces a significant homogenizing effect on information,\ncreativity, and cultural production. I first develop a novel theoretical\nframework to explain this phenomenon. I argue that a dynamic of AI-derivative\nepistemology, in which individuals increasingly defer to AI outputs, allows a\ncentralized AI Prism to function, a technical mechanism whose architecture is\ndesigned to reduce variance and converge on the statistical mean. This provides\na causal explanation for the generative monocultures observed in recent\nstudies. However, I contend this represents only the first stage of a more\ncomplex and dialectical process. This paper's central and paradoxical thesis is\nthat the very homogenization that flattens knowledge within specialized domains\nsimultaneously renders that knowledge into consistent modules that can be\nrecombined across them, a process foundational to innovation and creativity.\nHowever, this recombinant potential is not automatic, but rather conditional.\nThis paper argues that these opposing forces, homogenizing defaults versus\nrecombinant possibilities, are governed by the nature of human engagement with\nthe technology. The ultimate effect of generative AI is conditional on whether\nindividuals act as passive consumers deferring to the AI's statistical outputs,\nor as active curators who critically interrogate, re-contextualize, and\nrecombine them. The paper concludes by outlining the cognitive and\ninstitutional scaffolds required to resolve this tension, arguing they are the\ndecisive variable that determine whether generative AI becomes an instrument of\ninnovation or homogenization.", "AI": {"tldr": "This paper explores the dual effects of generative AI on knowledge and creativity, arguing that while it homogenizes information, it also enables recombination of knowledge for innovation, depending on human engagement.", "motivation": "To understand the impact of generative AI on information dynamics, creativity, and cultural production.", "method": "Develop a theoretical framework surrounding AI-derivative epistemology and its implications for generative monocultures.", "result": "Generative AI induces a homogenizing effect that flattens knowledge within domains but allows for modular recombination essential for innovation, contingent on user engagement.", "conclusion": "The balance between homogenization and creativity relies on whether individuals interact passively or actively with AI outputs, necessitating cognitive and institutional scaffolds to foster innovation.", "key_contributions": ["Proposed a novel theoretical framework for understanding generative AI's impact on knowledge.", "Identified the tension between homogenization and recombination based on user engagement with technology.", "Outlined cognitive and institutional requirements to leverage generative AI as a tool for innovation."], "limitations": "", "keywords": ["Generative AI", "homogenization", "creativity", "innovation", "user engagement"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2508.19270", "pdf": "https://arxiv.org/pdf/2508.19270.pdf", "abs": "https://arxiv.org/abs/2508.19270", "title": "Whisper based Cross-Lingual Phoneme Recognition between Vietnamese and English", "authors": ["Nguyen Huu Nhat Minh", "Tran Nguyen Anh", "Truong Dinh Dung", "Vo Van Nam", "Le Pham Tuyen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Cross-lingual phoneme recognition has emerged as a significant challenge for\naccurate automatic speech recognition (ASR) when mixing Vietnamese and English\npronunciations. Unlike many languages, Vietnamese relies on tonal variations to\ndistinguish word meanings, whereas English features stress patterns and\nnon-standard pronunciations that hinder phoneme alignment between the two\nlanguages. To address this challenge, we propose a novel bilingual speech\nrecognition approach with two primary contributions: (1) constructing a\nrepresentative bilingual phoneme set that bridges the differences between\nVietnamese and English phonetic systems; (2) designing an end-to-end system\nthat leverages the PhoWhisper pre-trained encoder for deep high-level\nrepresentations to improve phoneme recognition. Our extensive experiments\ndemonstrate that the proposed approach not only improves recognition accuracy\nin bilingual speech recognition for Vietnamese but also provides a robust\nframework for addressing the complexities of tonal and stress-based phoneme\nrecognition", "AI": {"tldr": "This paper presents a bilingual speech recognition approach to enhance phoneme recognition accuracy in mixed Vietnamese and English pronunciations, addressing tonal and stress pattern challenges.", "motivation": "The challenge of accurate automatic speech recognition (ASR) when mixing Vietnamese and English pronunciations due to the unique phonetic systems of each language.", "method": "A novel approach that constructs a representative bilingual phoneme set and employs an end-to-end system using the PhoWhisper pre-trained encoder for improved phoneme recognition.", "result": "The proposed method significantly improves recognition accuracy in bilingual speech recognition for Vietnamese and offers a robust framework for handling tonal and stress-based phoneme recognition.", "conclusion": "This work not only enhances phoneme recognition accuracy in bilingual contexts but also contributes a framework adaptable to other language pairs with phonetic differences.", "key_contributions": ["Construction of a bilingual phoneme set for Vietnamese and English.", "Development of an end-to-end recognition system utilizing the PhoWhisper encoder."], "limitations": "", "keywords": ["bilingual speech recognition", "phoneme recognition", "Vietnamese", "English", "automatic speech recognition"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.19378", "pdf": "https://arxiv.org/pdf/2508.19378.pdf", "abs": "https://arxiv.org/abs/2508.19378", "title": "Improving Hypertension and Diabetes Outcomes with Digital Care Coordination and Remote Monitoring in Rural Health", "authors": ["K. K. Kim", "S. P. McGrath", "D. Lindeman"], "categories": ["cs.HC", "H.5; J.3"], "comment": "12 pages, 1 figure", "summary": "Chronic illnesses are a global concern with essential hypertension and\ndiabetes mellitus among the most common conditions. Remote patient monitoring\nhas shown promising results on clinical and health outcomes. However, access to\ncare and digital health solutions is limited among rural, lower-income, and\nolder adult populations. This paper repots on a pre-post study of a\ncomprehensive care coordination program including connected, wearable blood\npressure and glucometer devices, tablets, and medical assistant-provided health\ncoaching in a community health center in rural California. The participants\n(n=221) had a mean age of 54.6 years, were majority female, two-thirds spoke\nSpanish, 19.9% had hypertension, 49.8% diabetic, and 30.3% both conditions.\nParticipants with hypertension achieved a mean reduction in systolic blood\npressure of 20.24 (95% CI: 13.61, 26.87) at six months while those with\ndiabetes achieved a mean reduction of 3.85 points (95% CI: 3.73, 4.88). These\noutcomes compare favorably to the small but growing body of evidence supporting\ndigital care coordination and remote monitoring. These results also support the\nfeasibility of well-designed digital health solutions yielding improved health\noutcomes among underserved communities.", "AI": {"tldr": "Remote patient monitoring shows promise for improving health outcomes in underserved communities, particularly among patients with chronic illnesses like hypertension and diabetes.", "motivation": "Chronic illnesses, such as hypertension and diabetes, are prevalent, and remote patient monitoring can enhance clinical outcomes, especially in underserved populations.", "method": "A pre-post study was conducted in a rural California community health center involving 221 participants using connected devices for blood pressure and glucose monitoring, alongside health coaching.", "result": "Participants with hypertension experienced a mean reduction in systolic blood pressure of 20.24 mmHg, while diabetic participants saw a mean reduction of 3.85 points in glucose levels.", "conclusion": "Digital health solutions, when well-designed, can lead to significant health improvements in underserved communities.", "key_contributions": ["Demonstrated effective use of remote monitoring for chronic illness management", "Highlighted the importance of digital health in rural and underserved populations", "Provided quantitative evidence of health improvement through a comprehensive care program"], "limitations": "The study may not account for other influencing factors outside of the digital health intervention, and results are based solely on a specific demographic in rural California.", "keywords": ["remote patient monitoring", "chronic illness", "health outcomes", "digital health", "underserved communities"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2508.19271", "pdf": "https://arxiv.org/pdf/2508.19271.pdf", "abs": "https://arxiv.org/abs/2508.19271", "title": "Rethinking Reasoning in LLMs: Neuro-Symbolic Local RetoMaton Beyond ICL and CoT", "authors": ["Rushitha Santhoshi Mamidala", "Anshuman Chhabra", "Ankur Mali"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Prompt-based reasoning strategies such as Chain-of-Thought (CoT) and\nIn-Context Learning (ICL) have become widely used for eliciting reasoning\ncapabilities in large language models (LLMs). However, these methods rely on\nfragile, implicit mechanisms often yielding inconsistent outputs across seeds,\nformats, or minor prompt variations making them fundamentally unreliable for\ntasks requiring stable, interpretable reasoning. In contrast, automata-based\nneuro-symbolic frameworks like RetoMaton offer a more structured and\ntrustworthy alternative by grounding retrieval in symbolic memory with\ndeterministic transitions. In this work, we extend RetoMaton by replacing its\nglobal datastore with a local, task-adaptive Weighted Finite Automaton (WFA),\nconstructed directly from external domain corpora. This local automaton\nstructure promotes robust, context-aware retrieval while preserving symbolic\ntraceability and low inference overhead. Unlike prompting, which entangles\ncontext and memory in opaque ways, our approach leverages the explicit\nstructure of WFAs to provide verifiable and modular retrieval behavior, making\nit better suited for domain transfer and interoperability. We evaluate this\nlocal RetoMaton variant on two pretrained LLMs LLaMA-3.2-1B and Gemma-3-1B-PT\nacross three reasoning tasks: TriviaQA (reading comprehension), GSM8K\n(multi-step math), and MMLU (domain knowledge). Compared to the base model and\nprompting-based methods, augmenting these setups with local RetoMaton\nconsistently improves performance while enabling transparent and reproducible\nretrieval dynamics. Our results highlight a promising shift toward trustworthy,\nsymbolic reasoning in modern LLMs via lightweight, automaton-guided memory.", "AI": {"tldr": "This paper presents an extension of the RetoMaton framework, introducing a local, task-adaptive Weighted Finite Automaton (WFA) for enhancing the reliability and transparency of reasoning in large language models.", "motivation": "To improve the reliability and interpretability of reasoning tasks in LLMs, which are currently hindered by fragile prompting methods.", "method": "The authors replace the global datastore in RetoMaton with a local, task-adaptive Weighted Finite Automaton (WFA) that is constructed from external domain corpora, allowing for robust and context-aware retrieval.", "result": "The local WFA structure leads to consistent performance improvements over baseline models and traditional prompting methods in reasoning tasks across various pretrained LLMs.", "conclusion": "This approach demonstrates a path toward more trustworthy and interpretable reasoning capabilities in LLMs through automaton-guided memory strategies.", "key_contributions": ["Introduction of a local, task-adaptive Weighted Finite Automaton (WFA) to improve reasoning in LLMs.", "Demonstration of performance improvements in reasoning tasks (TriviaQA, GSM8K, MMLU) using the local RetoMaton variant.", "Provision of transparent and reproducible retrieval dynamics compared to traditional prompting."], "limitations": "", "keywords": ["Chain-of-Thought", "In-Context Learning", "Weighted Finite Automaton", "RetoMaton", "large language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.19407", "pdf": "https://arxiv.org/pdf/2508.19407.pdf", "abs": "https://arxiv.org/abs/2508.19407", "title": "Exploring Paper as a Material: Plotting the Design Space of The Fabrication for Dynamic Paper-Based Interactions", "authors": ["Ruhan Yang", "Ellen Yi-Luen Do"], "categories": ["cs.HC"], "comment": null, "summary": "We reviewed 43 papers to understand the fabrication of dynamic paper-based\ninteractions. We used a design space to classify tool selection, technique\nchoice, and exploration of paper as a material. We classified 9 dimensions for\nthe design space, including 4 dimensions for tools (precision, accommodation,\ncomplexity, and availability), 3 dimensions for techniques (cutting techniques,\nfolding techniques, and integration techniques), and 2 dimensions for paper as\nthe material (paper weight and paper type). The patterns we observed in the\ndesign space indicate a majority use of high precision tools, high complexity\ntools, and surface integration techniques in previous practice. Meanwhile,\nprinting and plain paper are the leading material choices. We analyze these\npatterns and suggest potential directions for future work. Our study helps\nresearchers locate different fabrication approaches and instances, thus\nfostering innovation in the field of paper-based interaction.", "AI": {"tldr": "Review of 43 papers on dynamic paper-based interactions, proposing a design space for tool selection, technique choice, and material exploration.", "motivation": "To understand the fabrication methods and material use in dynamic paper-based interactions to foster innovation in this field.", "method": "Review of existing literature and classification of techniques and tools based on nine dimensions.", "result": "Identified a majority use of high precision and high complexity tools, with surface integration techniques leading in previous practices.", "conclusion": "The study offers a structured design space to guide future research directions in paper-based interactions.", "key_contributions": ["Classification of dynamic paper-based interaction techniques and tools", "Identification of prevalent tools and materials", "Proposing future research directions for innovation"], "limitations": "", "keywords": ["dynamic paper-based interactions", "fabrication", "design space", "tools", "techniques"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.19272", "pdf": "https://arxiv.org/pdf/2508.19272.pdf", "abs": "https://arxiv.org/abs/2508.19272", "title": "RAGAPHENE: A RAG Annotation Platform with Human Enhancements and Edits", "authors": ["Kshitij Fadnis", "Sara Rosenthal", "Maeda Hanafi", "Yannis Katsis", "Marina Danilevsky"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval Augmented Generation (RAG) is an important aspect of conversing\nwith Large Language Models (LLMs) when factually correct information is\nimportant. LLMs may provide answers that appear correct, but could contain\nhallucinated information. Thus, building benchmarks that can evaluate LLMs on\nmulti-turn RAG conversations has become an increasingly important task.\nSimulating real-world conversations is vital for producing high quality\nevaluation benchmarks. We present RAGAPHENE, a chat-based annotation platform\nthat enables annotators to simulate real-world conversations for benchmarking\nand evaluating LLMs. RAGAPHENE has been successfully used by approximately 40\nannotators to build thousands of real-world conversations.", "AI": {"tldr": "RAGAPHENE is a platform for simulating real-world conversations to evaluate Large Language Models through multi-turn Retrieval Augmented Generation benchmarks.", "motivation": "To address the challenge of evaluating LLMs in multi-turn conversations and mitigate issues of hallucination in AI responses by creating effective benchmarks.", "method": "Development of RAGAPHENE, a chat-based annotation tool that allows for the simulation of real-world conversations by multiple annotators.", "result": "RAGAPHENE has been utilized by around 40 annotators, leading to the creation of thousands of simulated conversations for benchmarking LLMs.", "conclusion": "Effective benchmarking of LLMs using simulated dialogues enhances the evaluation of their factual accuracy and conversational capabilities.", "key_contributions": ["Introduction of RAGAPHENE platform for conversational simulations", "Enabling collection of large datasets of multi-turn conversations", "Providing a structured approach to evaluate LLMs on factual accuracy"], "limitations": "", "keywords": ["Retrieval Augmented Generation", "Large Language Models", "benchmarking", "conversational AI", "annotation platform"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.19463", "pdf": "https://arxiv.org/pdf/2508.19463.pdf", "abs": "https://arxiv.org/abs/2508.19463", "title": "\"She was useful, but a bit too optimistic\": Augmenting Design with Interactive Virtual Personas", "authors": ["Paluck Deep", "Monica Bharadhidasan", "A. Baki Kocaballi"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Personas have been widely used to understand and communicate user needs in\nhuman-centred design. Despite their utility, they may fail to meet the demands\nof iterative workflows due to their static nature, limited engagement, and\ninability to adapt to evolving design needs. Recent advances in large language\nmodels (LLMs) pave the way for more engaging and adaptive approaches to user\nrepresentation. This paper introduces Interactive Virtual Personas (IVPs):\nmultimodal, LLM-driven, conversational user simulations that designers can\ninterview, brainstorm with, and gather feedback from in real time via voice\ninterface. We conducted a qualitative study with eight professional UX\ndesigners, employing an IVP named \"Alice\" across three design activities: user\nresearch, ideation, and prototype evaluation. Our findings demonstrate the\npotential of IVPs to expedite information gathering, inspire design solutions,\nand provide rapid user-like feedback. However, designers raised concerns about\nbiases, over-optimism, the challenge of ensuring authenticity without real\nstakeholder input, and the inability of the IVP to fully replicate the nuances\nof human interaction. Our participants emphasised that IVPs should be viewed as\na complement to, not a replacement for, real user engagement. We discuss\nstrategies for prompt engineering, human-in-the-loop integration, and ethical\nconsiderations for effective and responsible IVP use in design. Finally, our\nwork contributes to the growing body of research on generative AI in the design\nprocess by providing insights into UX designers' experiences of LLM-powered\ninteractive personas.", "AI": {"tldr": "This paper presents Interactive Virtual Personas (IVPs), LLM-driven user simulations for enhancing design workflows, as a new tool for UX designers.", "motivation": "To address the limitations of traditional static personas in iterative design workflows and explore the potential of LLMs in creating adaptive user representations.", "method": "A qualitative study was conducted with eight professional UX designers using an IVP named 'Alice' across three design activities: user research, ideation, and prototype evaluation.", "result": "IVPs demonstrated potential to expedite information gathering, inspire design solutions, and provide rapid feedback but raised concerns regarding biases and authenticity.", "conclusion": "IVPs are valuable tools for UX design but should complement, not replace, traditional user engagement methods.", "key_contributions": ["Introduction of Interactive Virtual Personas (IVPs) for real-time design feedback", "Insights into UX designers' experiences with LLM-powered personas", "Recommendations for effective IVP implementation in design processes"], "limitations": "Concerns about biases, over-optimism, and inability to fully replicate human interaction nuances.", "keywords": ["Interactive Virtual Personas", "large language models", "user experience design", "human-computer interaction", "generative AI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.19274", "pdf": "https://arxiv.org/pdf/2508.19274.pdf", "abs": "https://arxiv.org/abs/2508.19274", "title": "Leveraging Language Models and Machine Learning in Verbal Autopsy Analysis", "authors": ["Yue Chu"], "categories": ["cs.CL"], "comment": "Ph.D. dissertation submitted to The Ohio State University, August\n  2025", "summary": "In countries without civil registration and vital statistics, verbal autopsy\n(VA) is a critical tool for estimating cause of death (COD) and inform policy\npriorities. In VA, interviewers ask proximal informants for details on the\ncircumstances preceding a death, in the form of unstructured narratives and\nstructured questions. Existing automated VA cause classification algorithms\nonly use the questions and ignore the information in the narratives. In this\nthesis, we investigate how the VA narrative can be used for automated COD\nclassification using pretrained language models (PLMs) and machine learning\n(ML) techniques. Using empirical data from South Africa, we demonstrate that\nwith the narrative alone, transformer-based PLMs with task-specific fine-tuning\noutperform leading question-only algorithms at both the individual and\npopulation levels, particularly in identifying non-communicable diseases. We\nexplore various multimodal fusion strategies combining narratives and questions\nin unified frameworks. Multimodal approaches further improve performance in COD\nclassification, confirming that each modality has unique contributions and may\ncapture valuable information that is not present in the other modality. We also\ncharacterize physician-perceived information sufficiency in VA. We describe\nvariations in sufficiency levels by age and COD and demonstrate that\nclassification accuracy is affected by sufficiency for both physicians and\nmodels. Overall, this thesis advances the growing body of knowledge at the\nintersection of natural language processing, epidemiology, and global health.\nIt demonstrates the value of narrative in enhancing COD classification. Our\nfindings underscore the need for more high-quality data from more diverse\nsettings to use in training and fine-tuning PLM/ML methods, and offer valuable\ninsights to guide the rethinking and redesign of the VA instrument and\ninterview.", "AI": {"tldr": "This thesis explores the use of verbal autopsy narratives for automating cause of death classification using pretrained language models and machine learning, showing superior performance to traditional question-only methods, particularly in non-communicable diseases.", "motivation": "In contexts lacking civil registration, verbal autopsy is vital for estimating causes of death and informing health policies. Automating this process can enhance its efficiency and accuracy.", "method": "The study employs pretrained language models and machine learning techniques to classify causes of death using narratives from verbal autopsies, complemented by exploration of multimodal fusion strategies that combine narratives with structured questions.", "result": "Transformer-based models with fine-tuning based on narratives outperform existing methods using only structured questions. Multimodal approaches show further improvements in classification performance, particularly beneficial for non-communicable diseases.", "conclusion": "The findings illuminate the critical role of narrative data in enhancing automated cause of death classification and highlight the necessity for high-quality training data across diverse settings.", "key_contributions": ["Demonstrated the efficacy of narratives in verbal autopsy for cause of death classification using PLMs.", "Introduced multimodal fusion strategies that integrate narrative and question data for improved classification accuracy.", "Characterized variations in physician-perceived information sufficiency and its influence on classification outcomes."], "limitations": "Findings are based on data from a single country (South Africa), which may limit generalizability.", "keywords": ["verbal autopsy", "cause of death", "pretrained language models", "machine learning", "global health"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.19517", "pdf": "https://arxiv.org/pdf/2508.19517.pdf", "abs": "https://arxiv.org/abs/2508.19517", "title": "Orchid: Orchestrating Context Across Creative Workflows with Generative AI", "authors": ["Srishti Palani", "Gonzalo Ramos"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Context is critical for meaningful interactions between people and Generative\nAI (GenAI). Yet mainstream tools offer limited means to orchestrate it,\nparticularly across workflows that span multiple interactions, sessions, and\nmodels, as often occurs in creative projects. Re specifying prior details,\njuggling diverse artifacts, and dealing with context drift overwhelm users,\nobscure intent, and curtail creativity. To address these challenges, we present\nOrchid, a system that gives its users affordances to specify, reference, and\nmonitor context throughout evolving workflows. Specifically, Orchid enables\nusers to (1) specify context related to the project, themselves, and different\nstyles, (2) reference these via explicit mentions, inline selection, or\nimplicit grounding, and (3) monitor context assigned to different interactions\nacross the workflow. In a within-subjects study (n=12), participants using\nOrchid to execute creative tasks (compared to a baseline toolkit of web search,\nLLM-based chat, and digital notebooks) produced more novel and feasible\noutcomes, reporting greater alignment between their intent and the AI's\nresponses, higher perceived control, and increased transparency. By\nprioritizing context orchestration, Orchid offers an actionable step toward\nnext generation GenAI tools that support complex, iterative workflows -\nenabling creators and AI to stay aligned and augment their creative potential.", "AI": {"tldr": "Orchid is a system designed to enhance interactions with Generative AI by allowing users to specify, reference, and monitor context during creative workflows.", "motivation": "To address the challenges of context management in workflows involving Generative AI to improve user experience and creativity.", "method": "A within-subjects study with 12 participants compared the effectiveness of Orchid to traditional tools like web search and digital notebooks in executing creative tasks.", "result": "Participants using Orchid produced more novel and feasible outcomes, demonstrating better alignment between their intent and the AI's responses, along with enhanced perceived control and transparency.", "conclusion": "Orchid represents a significant advancement in context orchestration for Generative AI tools, fostering better alignment in creative processes between users and AI.", "key_contributions": ["Introduction of context orchestration in Generative AI interactions", "User study demonstrating improved outcomes when using Orchid", "Framework for supporting iterative workflows in creative tasks"], "limitations": "", "keywords": ["Generative AI", "context management", "creative workflows", "human-AI interaction", "user study"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.19279", "pdf": "https://arxiv.org/pdf/2508.19279.pdf", "abs": "https://arxiv.org/abs/2508.19279", "title": "FLAIRR-TS -- Forecasting LLM-Agents with Iterative Refinement and Retrieval for Time Series", "authors": ["Gunjan Jalori", "Preetika Verma", "Sercan Ö Arık"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP", "summary": "Time series Forecasting with large languagemodels (LLMs) requires bridging\nnumericalpatterns and natural language. Effective fore-casting on LLM often\nrelies on extensive pre-processing and fine-tuning.Recent studiesshow that a\nfrozen LLM can rival specializedforecasters when supplied with a carefully\nen-gineered natural-language prompt, but craft-ing such a prompt for each task\nis itself oner-ous and ad-hoc. We introduce FLAIRR-TS, atest-time prompt\noptimization framework thatutilizes an agentic system: a\nForecaster-agentgenerates forecasts using an initial prompt,which is then\nrefined by a refiner agent, in-formed by past outputs and retrieved\nanalogs.This adaptive prompting generalizes across do-mains using creative\nprompt templates andgenerates high-quality forecasts without inter-mediate code\ngeneration.Experiments onbenchmark datasets show improved accuracyover static\nprompting and retrieval-augmentedbaselines, approaching the performance\nofspecialized prompts.FLAIRR-TS providesa practical alternative to tuning,\nachievingstrong performance via its agentic approach toadaptive prompt\nrefinement and retrieval.", "AI": {"tldr": "FLAIRR-TS is a test-time prompt optimization framework that enhances time series forecasting with large language models by using an agentic system for adaptive prompting.", "motivation": "Bridging numerical patterns and natural language for effective time series forecasting using LLMs without extensive pre-processing or fine-tuning.", "method": "An agentic system comprising a Forecaster-agent to generate initial forecasts and a Refiner-agent to optimize prompts based on past outputs and analogs.", "result": "FLAIRR-TS demonstrates improved accuracy in forecasts over traditional static prompting and retrieval-augmented methods on benchmark datasets.", "conclusion": "FLAIRR-TS offers a practical approach to achieve strong forecasting performance through adaptive prompt refinement without requiring intermediate code generation.", "key_contributions": ["Introduces FLAIRR-TS for adaptive prompting in time series forecasting", "Utilizes an agentic system to refine forecasts", "Achieves performance close to specialized prompts with less effort"], "limitations": "", "keywords": ["time series forecasting", "large language models", "prompt optimization", "adaptive prompting", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.19622", "pdf": "https://arxiv.org/pdf/2508.19622.pdf", "abs": "https://arxiv.org/abs/2508.19622", "title": "PersoNo: Personalised Notification Urgency Classifier in Mixed Reality", "authors": ["Jingyao Zheng", "Haodi Weng", "Xian Wang", "Chengbin Cui", "Sven Mayer", "Chi-lok Tai", "Lik-Hang Lee"], "categories": ["cs.HC", "cs.MM"], "comment": "Accepted by ISMAR 2025", "summary": "Mixed Reality (MR) is increasingly integrated into daily life, providing\nenhanced capabilities across various domains. However, users face growing\nnotification streams that disrupt their immersive experience. We present\nPersoNo, a personalised notification urgency classifier for MR that\nintelligently classifies notifications based on individual user preferences.\nThrough a user study (N=18), we created the first MR notification dataset\ncontaining both self-labelled and interaction-based data across activities with\nvarying cognitive demands. Our thematic analysis revealed that, unlike in\nmobiles, the activity context is equally important as the content and the\nsender in determining notification urgency in MR. Leveraging these insights, we\ndeveloped PersoNo using large language models that analyse users replying\nbehaviour patterns. Our multi-agent approach achieved 81.5% accuracy and\nsignificantly reduced false negative rates (0.381) compared to baseline models.\nPersoNo has the potential not only to reduce unnecessary interruptions but also\nto offer users understanding and control of the system, adhering to\nHuman-Centered Artificial Intelligence design principles.", "AI": {"tldr": "PersoNo is a personalized notification urgency classifier for Mixed Reality environments that reduces interruptions by considering user preferences and activity context.", "motivation": "As Mixed Reality becomes more embedded in daily life, managing notification interruptions is increasingly critical for maintaining an immersive user experience.", "method": "A user study involving 18 participants was conducted to create a novel MR notification dataset. The classifier, PersoNo, was developed using large language models to analyze user reply behavior patterns, achieving 81.5% accuracy.", "result": "PersoNo significantly reduces false negative rates (0.381) compared to baseline models while providing a user-centric approach to notification management.", "conclusion": "The development of PersoNo adheres to Human-Centered Artificial Intelligence design principles, fostering user understanding and control over notification management.", "key_contributions": ["Creation of the first MR notification dataset", "Development of a personalized urgency classifier using LLMs", "Demonstration of the importance of activity context in urgency assessment"], "limitations": "The study involved a limited sample size of 18 participants, which may affect the generalizability of the results.", "keywords": ["Mixed Reality", "notification management", "user preferences", "large language models", "cognitive demands"], "importance_score": 8, "read_time_minutes": 7}}
{"id": "2508.19282", "pdf": "https://arxiv.org/pdf/2508.19282.pdf", "abs": "https://arxiv.org/abs/2508.19282", "title": "CORE: Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning", "authors": ["Ziqiang Cui", "Yunpeng Weng", "Xing Tang", "Peiyang Liu", "Shiwei Li", "Bowei He", "Jiamin Chen", "Xiuqiang He", "Chen Ma"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a promising approach to\nenhance the timeliness of knowledge and the factual accuracy of responses in\nLarge Language Models (LLMs). However, the inclusion of excessive retrieved\ndocuments substantially increases the input length, leading to higher\ncomputational costs. Previous studies have attempted to compress retrieved\ndocuments into shorter texts before in-context integration, but such methods\noften compromise end-task performance. The lack of well-defined compression\ntargets forces many approaches to rely on fixed heuristics, which cannot\nguarantee that the compressed content will effectively support the end task. To\naddress these limitations, we propose CORE, a novel method designed to achieve\nlossless context compression for RAG. CORE employs reinforcement learning to\noptimize the compression process without relying on predefined compression\nlabels. Specifically, it utilizes end-task performance as a reward signal and\napplies Generalized Reinforcement Learning Policy Optimization (GRPO) to train\nthe compressor. This end-to-end training framework enables the compressor to\ngenerate summaries that maximize the accuracy of answers generated by the LLM.\nExtensive experiments on four datasets demonstrate the superiority of our\napproach. With a high compression ratio of 3\\%, our method not only avoids\nperformance degradation compared to prepending full documents across all\ndatasets but also improves the average Exact Match (EM) score by 3.3 points.\nThe code will be released soon.", "AI": {"tldr": "CORE is a novel method for lossless context compression in Retrieval-Augmented Generation (RAG), optimizing the compression process using reinforcement learning to enhance LLM performance.", "motivation": "Current RAG methods face challenges with excessive input lengths and performance degradation due to fixed heuristics in document compression, necessitating a more effective approach.", "method": "CORE uses reinforcement learning, specifically Generalized Reinforcement Learning Policy Optimization (GRPO), to optimize compression without relying on predefined compression labels, using end-task performance as a reward signal.", "result": "CORE achieves a compression ratio of 3%, avoiding performance degradation while improving the average Exact Match (EM) score by 3.3 points across various datasets.", "conclusion": "The extensive experimental results validate CORE's effectiveness in enhancing the accuracy of responses generated by LLMs while maintaining a high compression ratio.", "key_contributions": ["Introduction of CORE for lossless context compression in RAG", "Utilization of reinforcement learning for optimizing compression goals", "Demonstrated performance improvements with experimental validation across multiple datasets"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Reinforcement Learning", "Document Compression", "Large Language Models", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.19703", "pdf": "https://arxiv.org/pdf/2508.19703.pdf", "abs": "https://arxiv.org/abs/2508.19703", "title": "Haptic Tracing: A new paradigm for spatialized Haptic rendering", "authors": ["Tom Roy", "Yann Glemarec", "Gurvan Lecuyer", "Quentin Galvane", "Philippe Guillotel", "Ferran Argelaguet"], "categories": ["cs.HC"], "comment": null, "summary": "Haptic technology enhances interactive experiences by providing force and\ntactile feedback, improving user performance and immersion. However, despite\nadvancements, creating tactile experiences still remains challenging due to\ndevice diversity and complexity. Most available haptic frameworks rely on\ntrigger-based or event-based systems, and disregard the information of the 3D\nscene to render haptic information. This paper introduces Haptic Tracing, a\nnovel method for spatial haptic rendering that simplifies the creation of\ninteractive haptic experiences without relying on physical simulations. It uses\nconcepts from visual and audio rendering to model and propagate haptic\ninformation through a 3D scene. The paper also describes how our proposed\nhaptic rendering method can be used to create a vibrotactile rendering system,\nenabling the creation of perceptually coherent and dynamic haptic interactions.\nFinally, the paper discusses a user study that explores the role of the haptic\npropagation and multi-actuator rendering on the users' haptic experience. The\nresults show that our approach significantly enhances the realism and the\nexpressivity of the haptic feedback, showcasing its potential for developing\nmore complex and realistic haptic experiences.", "AI": {"tldr": "This paper presents Haptic Tracing, a method for simplifying the creation of interactive haptic experiences using concepts from visual and audio rendering, improving realism and expressivity in haptic feedback.", "motivation": "To address the challenges in creating tactile experiences due to device diversity and complexity in existing haptic frameworks which primarily rely on trigger-based or event-based systems.", "method": "Haptic Tracing models and propagates haptic information through a 3D scene without needing physical simulations, utilizing methodologies from visual and audio rendering.", "result": "The user study indicated that the proposed method significantly enhances the realism and expressivity of haptic feedback, improving user experience during interactions.", "conclusion": "Haptic Tracing shows promise for developing more complex and realistic haptic experiences, with potential applications in varied interactive environments.", "key_contributions": ["Introduction of Haptic Tracing for spatial haptic rendering", "Use of visual and audio rendering concepts in haptic feedback", "Demonstration of enhanced realism and expressivity through user studies"], "limitations": "", "keywords": ["Haptic Technology", "Spatial Haptic Rendering", "User Experience", "Vibrotactile Rendering", "Interaction Design"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.19357", "pdf": "https://arxiv.org/pdf/2508.19357.pdf", "abs": "https://arxiv.org/abs/2508.19357", "title": "Context-Adaptive Synthesis and Compression for Enhanced Retrieval-Augmented Generation in Complex Domains", "authors": ["Peiran Zhou", "Junnan Zhu", "Yichen Shen", "Ruoxi Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel in language tasks but are prone to\nhallucinations and outdated knowledge. Retrieval-Augmented Generation (RAG)\nmitigates these by grounding LLMs in external knowledge. However, in complex\ndomains involving multiple, lengthy, or conflicting documents, traditional RAG\nsuffers from information overload and inefficient synthesis, leading to\ninaccurate and untrustworthy answers. To address this, we propose CASC\n(Context-Adaptive Synthesis and Compression), a novel framework that\nintelligently processes retrieved contexts. CASC introduces a Context Analyzer\n& Synthesizer (CAS) module, powered by a fine-tuned smaller LLM, which performs\nkey information extraction, cross-document consistency checking and conflict\nresolution, and question-oriented structured synthesis. This process transforms\nraw, scattered information into a highly condensed, structured, and\nsemantically rich context, significantly reducing the token count and cognitive\nload for the final Reader LLM. We evaluate CASC on SciDocs-QA, a new\nchallenging multi-document question answering dataset designed for complex\nscientific domains with inherent redundancies and conflicts. Our extensive\nexperiments demonstrate that CASC consistently outperforms strong baselines.", "AI": {"tldr": "CASC is a new framework improving RAG by intelligently processing retrieved contexts for enhanced multi-document question answering.", "motivation": "LLMs face challenges with hallucinations and outdated knowledge, especially when dealing with multiple and conflicting documents in complex domains.", "method": "CASC introduces a Context Analyzer & Synthesizer (CAS) powered by a fine-tuned smaller LLM for information extraction, consistency checking, and structured synthesis.", "result": "CASC reduces token count and cognitive load for LLMs, transforming complex information into structured context, and outperforms strong baselines in experiments on the SciDocs-QA dataset.", "conclusion": "CASC effectively mitigates issues of information overload in multi-document contexts, enhancing the reliability of LLM outputs.", "key_contributions": ["Introduction of the CASC framework", "Development of the CAS module for structured synthesis", "Evaluation on the challenging SciDocs-QA dataset"], "limitations": "", "keywords": ["Large Language Models", "Retrieval-Augmented Generation", "Information Synthesis"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.19708", "pdf": "https://arxiv.org/pdf/2508.19708.pdf", "abs": "https://arxiv.org/abs/2508.19708", "title": "Attention is also needed for form design", "authors": ["B. Sankar", "Dibakar Sen"], "categories": ["cs.HC", "cs.AI", "68T07, 68T42, 68T50", "I.2; J.5; J.6"], "comment": "55 pages, 45 figures,", "summary": "Conventional product design is a cognitively demanding process, limited by\nits time-consuming nature, reliance on subjective expertise, and the opaque\ntranslation of inspiration into tangible concepts. This research introduces a\nnovel, attention-aware framework that integrates two synergistic systems:\nEUPHORIA, an immersive Virtual Reality environment using eye-tracking to\nimplicitly capture a designer's aesthetic preferences, and RETINA, an agentic\nAI pipeline that translates these implicit preferences into concrete design\noutputs. The foundational principles were validated in a two-part study. An\ninitial study correlated user's implicit attention with explicit preference and\nthe next one correlated mood to attention. A comparative study where 4\ndesigners solved challenging design problems using 4 distinct workflows, from a\nmanual process to an end-to-end automated pipeline, showed the integrated\nEUPHORIA-RETINA workflow was over 4 times more time-efficient than the\nconventional method. A panel of 50 design experts evaluated the 16 final\nrenderings. Designs generated by the fully automated system consistently\nreceived the highest Worthiness (calculated by an inverse Plackett-Luce model\nbased on gradient descent optimization) and Design Effectiveness scores,\nindicating superior quality across 8 criteria: novelty, visual appeal,\nemotional resonance, clarity of purpose, distinctiveness of silhouette, implied\nmateriality, proportional balance, & adherence to the brief. This research\npresents a validated paradigm shift from traditional Computer-Assisted Design\n(CAD) to a collaborative model of Designer-Assisting Computers (DAC). By\nautomating logistical and skill-dependent generative tasks, the proposed\nframework elevates the designer's role to that of a creative director,\nsynergizing human intuition with the generative power of agentic AI to produce\nhigher-quality designs more efficiently.", "AI": {"tldr": "This research presents a novel framework integrating a VR environment and AI system for efficient product design, demonstrating significant improvements in design quality and process efficiency.", "motivation": "Conventional product design is cognitively demanding and time-consuming, often relying on subjective expertise.", "method": "The paper introduces EUPHORIA, a VR environment using eye-tracking to capture aesthetic preferences, and RETINA, an AI system that translates these preferences into design outputs, validated through a comparative study.", "result": "The EUPHORIA-RETINA workflow was over 4 times more time-efficient than conventional methods and received superior quality scores from a panel of design experts.", "conclusion": "This research marks a paradigm shift from traditional CAD to a Designer-Assisting Computers model, enhancing workflow efficiency and design quality through AI.", "key_contributions": ["Introduction of the EUPHORIA-RETINA framework for design efficiency", "Empirical validation of the framework's effectiveness", "Shift from CAD to DAC in product design processes"], "limitations": "", "keywords": ["Human-Computer Interaction", "Artificial Intelligence", "Product Design", "Virtual Reality", "Design Efficiency"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2508.19359", "pdf": "https://arxiv.org/pdf/2508.19359.pdf", "abs": "https://arxiv.org/abs/2508.19359", "title": "Reflective Agreement: Combining Self-Mixture of Agents with a Sequence Tagger for Robust Event Extraction", "authors": ["Fatemeh Haji", "Mazal Bethany", "Cho-Yu Jason Chiang", "Anthony Rios", "Peyman Najafirad"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Event Extraction (EE) involves automatically identifying and extracting\nstructured information about events from unstructured text, including triggers,\nevent types, and arguments. Traditional discriminative models demonstrate high\nprecision but often exhibit limited recall, particularly for nuanced or\ninfrequent events. Conversely, generative approaches leveraging Large Language\nModels (LLMs) provide higher semantic flexibility and recall but suffer from\nhallucinations and inconsistent predictions. To address these challenges, we\npropose Agreement-based Reflective Inference System (ARIS), a hybrid approach\ncombining a Self Mixture of Agents with a discriminative sequence tagger. ARIS\nexplicitly leverages structured model consensus, confidence-based filtering,\nand an LLM reflective inference module to reliably resolve ambiguities and\nenhance overall event prediction quality. We further investigate decomposed\ninstruction fine-tuning for enhanced LLM event extraction understanding.\nExperiments demonstrate our approach outperforms existing state-of-the-art\nevent extraction methods across three benchmark datasets.", "AI": {"tldr": "ARIS is a hybrid Event Extraction approach combining discriminative models and LLMs to improve prediction quality and recall.", "motivation": "Address limited recall in traditional models and the issues of hallucination in generative models for event extraction in unstructured text.", "method": "A hybrid approach using a Self Mixture of Agents, a discriminative sequence tagger, and an LLM reflective inference module to enhance event prediction quality.", "result": "ARIS outperforms existing state-of-the-art event extraction methods across three benchmark datasets, improving both precision and recall.", "conclusion": "The study indicates that combining structured model consensus with LLM capabilities can significantly enhance event extraction performance.", "key_contributions": ["Proposal of ARIS for event extraction", "Integration of confidence-based filtering and LLM reflective inference", "Investigated decomposed instruction fine-tuning for LLMs"], "limitations": "", "keywords": ["Event Extraction", "Large Language Models", "Hybrid Models", "Natural Language Processing", "Machine Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.19768", "pdf": "https://arxiv.org/pdf/2508.19768.pdf", "abs": "https://arxiv.org/abs/2508.19768", "title": "Burst: Collaborative Curation in Connected Social Media Communities", "authors": ["Yutong Zhang", "Taeuk Kang", "Sydney Yeh", "Anavi Baddepudi", "Lindsay Popowski", "Tiziano Piccardi", "Michael S. Bernstein"], "categories": ["cs.HC"], "comment": "29 pages, 5 figures; This work will appear in the 28th ACM SIGCHI\n  Conference on Computer-Supported Cooperative Work & Social Computing (CSCW\n  2025)", "summary": "Positive social interactions can occur in groups of many shapes and sizes,\nspanning from small and private to large and open. However, social media tends\nto binarize our experiences into either isolated small groups or into large\npublic squares. In this paper, we introduce Burst, a social media design that\nallows users to share and curate content between many spaces of varied size and\ncomposition. Users initially post content to small trusted groups, who can then\nburst that content, routing it to the groups that would be the best audience.\nWe instantiate this approach into a mobile phone application, and demonstrate\nthrough a ten-day field study (N=36) that Burst enabled a participatory\ncuration culture. With this work, we aim to articulate potential new design\ndirections for social media sharing.", "AI": {"tldr": "The paper introduces Burst, a social media design that facilitates content sharing across various group sizes and compositions, promoting a participatory curation culture.", "motivation": "To address the limitation of current social media platforms that categorize interactions into either small isolated groups or large public spaces.", "method": "A mobile application was developed, and a ten-day field study was conducted with 36 participants to observe the dynamics of content sharing and curation.", "result": "The study shows that Burst enabled a culture of participatory curation among users, allowing effective content routing to suitable audiences.", "conclusion": "The findings suggest new design directions for social media that enhance user interaction and content sharing.", "key_contributions": ["Introduction of the Burst social media design", "Implementation as a mobile application", "Field study demonstrating participatory curation culture"], "limitations": "The study is limited to a small participant pool and short duration, which may affect generalizability.", "keywords": ["social media", "user interaction", "content curation", "design", "HCI"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.19363", "pdf": "https://arxiv.org/pdf/2508.19363.pdf", "abs": "https://arxiv.org/abs/2508.19363", "title": "LongReasonArena: A Long Reasoning Benchmark for Large Language Models", "authors": ["Jiayu Ding", "Shuming Ma", "Lei Cui", "Nanning Zheng", "Furu Wei"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing long-context benchmarks for Large Language Models (LLMs) focus on\nevaluating comprehension of long inputs, while overlooking the evaluation of\nlong reasoning abilities. To address this gap, we introduce LongReasonArena, a\nbenchmark specifically designed to assess the long reasoning capabilities of\nLLMs. Our tasks require models to solve problems by executing multi-step\nalgorithms that reflect key aspects of long reasoning, such as retrieval and\nbacktracking. By controlling the inputs, the required reasoning length can be\narbitrarily scaled, reaching up to 1 million tokens of reasoning for the most\nchallenging tasks. Extensive evaluation results demonstrate that\nLongReasonArena presents a significant challenge for both open-source and\nproprietary LLMs. For instance, Deepseek-R1 achieves only 7.5% accuracy on our\ntask. Further analysis also reveals that the accuracy exhibits a linear decline\nwith respect to the logarithm of the expected number of reasoning steps. Our\ncode and data is available at\nhttps://github.com/LongReasonArena/LongReasonArena.", "AI": {"tldr": "LongReasonArena benchmark evaluates long reasoning capabilities of Large Language Models (LLMs) through multi-step algorithm tasks.", "motivation": "To address the lack of benchmarks assessing long reasoning abilities in LLMs, which are distinct from mere comprehension of long inputs.", "method": "Introduction of LongReasonArena, a benchmark designed to scale reasoning tasks with varying lengths, up to 1 million tokens, requiring multi-step algorithms.", "result": "Evaluation results show that LLMs, including Deepseek-R1, struggle significantly with the benchmark, with Deepseek-R1 achieving only 7.5% accuracy.", "conclusion": "LongReasonArena effectively challenges the reasoning abilities of LLMs, indicating a substantial gap in their performance on long reasoning tasks.", "key_contributions": ["Introduction of a new benchmark for long reasoning abilities in LLMs.", "Tasks include multi-step problem-solving algorithms such as retrieval and backtracking.", "Extensive evaluation results highlighting the performance gap of existing LLMs."], "limitations": "The benchmark focuses exclusively on reasoning capabilities and may not cover all dimensions of LLM performance or real-world applicability.", "keywords": ["LongReasonArena", "Large Language Models", "long reasoning", "benchmark", "multi-step algorithms"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.19818", "pdf": "https://arxiv.org/pdf/2508.19818.pdf", "abs": "https://arxiv.org/abs/2508.19818", "title": "Towards a Real-Time Warning System for Detecting Inaccuracies in Photoplethysmography-Based Heart Rate Measurements in Wearable Devices", "authors": ["Rania Islmabouli", "Marlene Brunner", "Devender Kumar", "Mahdi Sareban", "Gunnar Treff", "Michael Neudorfer", "Josef Niebauer", "Arne Bathke", "Jan David Smeddinck"], "categories": ["cs.HC"], "comment": null, "summary": "Wearable devices with photoplethysmography (PPG) sensors are widely used to\nmonitor heart rate (HR), yet often suffer from accuracy issues. However, users\ntypically do not receive an indication of potential measurement errors. We\npresent a real-time warning system that detects and communicates inaccuracies\nin PPG-derived HR, aiming to enhance transparency and trust. Using data from\nPolar and Garmin devices, we trained a deep learning model to classify HR\naccuracy using only the derived HR signal. The system detected over 80% of\ninaccurate readings. By providing interpretable, real-time feedback directly to\nusers, our work contributes to HCI by promoting user awareness, informed\ndecision-making, and trust in wearable health technology.", "AI": {"tldr": "A real-time warning system is proposed to improve accuracy in heart rate monitoring using PPG sensors in wearable devices.", "motivation": "To address the accuracy issues in PPG-derived heart rate monitoring and enhance user trust through transparency.", "method": "A deep learning model was trained on data from Polar and Garmin devices to classify heart rate accuracy using only the derived HR signal.", "result": "The system was able to detect over 80% of inaccurate heart rate readings.", "conclusion": "The work contributes to HCI by enhancing user awareness and informed decision-making regarding wearable health technology.", "key_contributions": ["Development of a real-time warning system for heart rate monitoring", "Improved detection of measurement inaccuracies", "Promotion of user trust in wearable health technology"], "limitations": "", "keywords": ["wearable devices", "photoplethysmography", "heart rate accuracy", "deep learning", "human-computer interaction"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.19372", "pdf": "https://arxiv.org/pdf/2508.19372.pdf", "abs": "https://arxiv.org/abs/2508.19372", "title": "Database Entity Recognition with Data Augmentation and Deep Learning", "authors": ["Zikun Fu", "Chen Yang", "Kourosh Davoudi", "Ken Q. Pu"], "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.LG"], "comment": "6 pages, 5 figures. Accepted at IEEE 26th International Conference on\n  Information Reuse and Integration for Data Science (IRI 2025), San Jose,\n  California, August 6-8, 2025", "summary": "This paper addresses the challenge of Database Entity Recognition (DB-ER) in\nNatural Language Queries (NLQ). We present several key contributions to advance\nthis field: (1) a human-annotated benchmark for DB-ER task, derived from\npopular text-to-sql benchmarks, (2) a novel data augmentation procedure that\nleverages automatic annotation of NLQs based on the corresponding SQL queries\nwhich are available in popular text-to-SQL benchmarks, (3) a specialized\nlanguage model based entity recognition model using T5 as a backbone and two\ndown-stream DB-ER tasks: sequence tagging and token classification for\nfine-tuning of backend and performing DB-ER respectively. We compared our DB-ER\ntagger with two state-of-the-art NER taggers, and observed better performance\nin both precision and recall for our model. The ablation evaluation shows that\ndata augmentation boosts precision and recall by over 10%, while fine-tuning of\nthe T5 backbone boosts these metrics by 5-10%.", "AI": {"tldr": "This paper proposes advancements in Database Entity Recognition (DB-ER) in Natural Language Queries with a new benchmark, a data augmentation method, and a specialized T5-based model.", "motivation": "To improve the accuracy of Database Entity Recognition in Natural Language Queries, addressing limitations in existing benchmarks and models.", "method": "Development of a human-annotated benchmark, a novel data augmentation approach using automatic annotation from SQL queries, and a specialized entity recognition model based on T5 architecture.", "result": "The proposed DB-ER model outperforms existing state-of-the-art NER taggers in both precision and recall, demonstrating significant improvements through data augmentation and fine-tuning.", "conclusion": "The findings suggest that enhancing data preparation and model fine-tuning can lead to marked improvements in entity recognition tasks related to natural language queries.", "key_contributions": ["Introduction of a human-annotated benchmark for DB-ER", "Novel data augmentation approach for enhancing NLQ datasets", "A specialized T5-based entity recognition model for DB-ER tasks"], "limitations": "", "keywords": ["Database Entity Recognition", "Natural Language Processing", "Data Augmentation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.19867", "pdf": "https://arxiv.org/pdf/2508.19867.pdf", "abs": "https://arxiv.org/abs/2508.19867", "title": "Lessons from Biophilic Design: Rethinking Affective Interaction Design in Built Environments", "authors": ["Shruti Rao", "Judith Good", "Hamed Alavi"], "categories": ["cs.HC"], "comment": "3 pages, 1 footer image, provocation paper for CHI 2025 Workshop on\n  Affective Interactions, Japan", "summary": "The perspectives of affective interaction in built environments are largely\noverlooked and instead dominated by affective computing approaches that view\nemotions as \"static\", computable states to be detected and regulated. To\naddress this limitation, we interviewed architects to explore how biophilic\ndesign -- our deep-rooted emotional connection with nature -- could shape\naffective interaction design in smart buildings. Our findings reveal that\nnatural environments facilitate self-directed emotional experiences through\nspatial diversity, embodied friction, and porous sensory exchanges. Based on\nthis, we introduce three design principles for discussion at the Affective\nInteraction workshop: (1) Diversity of Spatial Experiences, (2) Self-Reflection\nThrough Complexity & Friction, and (3) Permeability & Sensory Exchange with the\nOutside World, while also examining the challenges of integrating these\nperspectives into built environments.", "AI": {"tldr": "The paper explores affective interaction in built environments, focusing on biophilic design and its impact on emotional experiences in smart buildings.", "motivation": "Traditional affective computing approaches neglect the dynamic aspects of emotions in built environments. This study aims to highlight the role of biophilic design in shaping affective interactions.", "method": "Interviews with architects were conducted to investigate how biophilic design influences emotional connections in smart buildings.", "result": "Natural environments promote self-directed emotional experiences, identified through spatial diversity, embodied friction, and sensory exchanges.", "conclusion": "Three design principles for affective interaction in built environments are proposed, emphasizing the need for integrating emotional perspectives in architectural design.", "key_contributions": ["Introduced three design principles for affective interaction in smart buildings.", "Highlighted the significance of biophilic design in emotional experiences.", "Proposed challenges for integrating affective perspectives into built environments."], "limitations": "Further research is needed on practical implementation of these principles.", "keywords": ["affective interaction", "biophilic design", "emotional experiences", "smart buildings", "spatial diversity"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.19402", "pdf": "https://arxiv.org/pdf/2508.19402.pdf", "abs": "https://arxiv.org/abs/2508.19402", "title": "One Joke to Rule them All? On the (Im)possibility of Generalizing Humor", "authors": ["Mor Turgeman", "Chen Shani", "Dafna Shahaf"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Humor is a broad and complex form of communication that remains challenging\nfor machines. Despite its broadness, most existing research on computational\nhumor traditionally focused on modeling a specific type of humor. In this work,\nwe wish to understand whether competence on one or more specific humor tasks\nconfers any ability to transfer to novel, unseen types; in other words, is this\nfragmentation inevitable? This question is especially timely as new humor types\ncontinuously emerge in online and social media contexts (e.g., memes,\nanti-humor, AI fails). If Large Language Models (LLMs) are to keep up with this\nevolving landscape, they must be able to generalize across humor types by\ncapturing deeper, transferable mechanisms. To investigate this, we conduct a\nseries of transfer learning experiments across four datasets, representing\ndifferent humor tasks. We train LLMs under varied diversity settings (1-3\ndatasets in training, testing on a novel task). Experiments reveal that models\nare capable of some transfer, and can reach up to 75% accuracy on unseen\ndatasets; training on diverse sources improves transferability (1.88-4.05%)\nwith minimal-to-no drop in in-domain performance. Further analysis suggests\nrelations between humor types, with Dad Jokes surprisingly emerging as the best\nenabler of transfer (but is difficult to transfer to). We release data and\ncode.", "AI": {"tldr": "The study investigates whether large language models can transfer humor competence across different tasks and types, revealing some transferability and the surprising effectiveness of Dad Jokes in this context.", "motivation": "Understanding if competence in certain humor tasks allows transfer to new, unseen types, especially in the context of the evolving digital humor landscape.", "method": "Conducted transfer learning experiments across four datasets focused on different humor tasks, training LLMs with varied dataset diversity and testing on novel tasks.", "result": "Models achieved up to 75% accuracy on unseen datasets, with diversity in training data facilitating a slight improvement in transferability without compromising in-domain performance.", "conclusion": "LLMs can somewhat generalize humor across tasks, with certain types like Dad Jokes being both beneficial and challenging for transfer.", "key_contributions": ["Exploration of humor transferability in LLMs across different tasks", "Identification of training diversity's role in enhancing transferability", "Release of datasets and code for further research on humor computing."], "limitations": "Results may vary with different humor types not covered in the study or with different model architectures.", "keywords": ["humor", "transfer learning", "large language models", "machine learning", "natural language processing"], "importance_score": 7, "read_time_minutes": 7}}
{"id": "2508.19942", "pdf": "https://arxiv.org/pdf/2508.19942.pdf", "abs": "https://arxiv.org/abs/2508.19942", "title": "Socially Interactive Agents for Preserving and Transferring Tacit Knowledge in Organizations", "authors": ["Martin Benderoth", "Patrick Gebhard", "Christian Keller", "C. Benjamin Nakhosteen", "Stefan Schaffer", "Tanja Schneeberger"], "categories": ["cs.HC"], "comment": "4 pages, 2 figures", "summary": "This paper introduces a novel approach to tackle the challenges of preserving\nand transferring tacit knowledge--deep, experience-based insights that are hard\nto articulate but vital for decision-making, innovation, and problem-solving.\nTraditional methods rely heavily on human facilitators, which, while effective,\nare resource-intensive and lack scalability. A promising alternative is the use\nof Socially Interactive Agents (SIAs) as AI-driven knowledge transfer\nfacilitators. These agents interact autonomously and socially intelligently\nwith users through multimodal behaviors (verbal, paraverbal, nonverbal),\nsimulating expert roles in various organizational contexts. SIAs engage\nemployees in empathic, natural-language dialogues, helping them externalize\ninsights that might otherwise remain unspoken. Their success hinges on building\ntrust, as employees are often hesitant to share tacit knowledge without\nassurance of confidentiality and appreciation. Key technologies include Large\nLanguage Models (LLMs) for generating context-relevant dialogue,\nRetrieval-Augmented Generation (RAG) to integrate organizational knowledge, and\nChain-of-Thought (CoT) prompting to guide structured reflection. These enable\nSIAs to actively elicit knowledge, uncover implicit assumptions, and connect\ninsights to broader organizational contexts. Potential applications span\nonboarding, where SIAs support personalized guidance and introductions, and\nknowledge retention, where they conduct structured interviews with retiring\nexperts to capture heuristics behind decisions. Success depends on addressing\nethical and operational challenges such as data privacy, algorithmic bias, and\nresistance to AI. Transparency, robust validation, and a culture of trust are\nessential to mitigate these risks.", "AI": {"tldr": "This paper proposes Socially Interactive Agents (SIAs) as a scalable solution for transferring tacit knowledge in organizations.", "motivation": "To address the limitations of traditional human-facilitated knowledge transfer methods that are resource-intensive and lack scalability.", "method": "The paper discusses the use of SIAs, which leverage Large Language Models (LLMs) and techniques like Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT) prompting to engage users through natural language dialogues and facilitate knowledge externalization.", "result": "SIAs can effectively support applications such as onboarding and knowledge retention by conducting structured dialogues to elicit tacit knowledge from employees, provided that issues of trust, ethics, and data privacy are properly managed.", "conclusion": "The success of SIAs in facilitating tacit knowledge transfer relies on creating trust and addressing ethical challenges while leveraging advanced AI techniques.", "key_contributions": ["Introduction of Socially Interactive Agents for knowledge transfer", "Use of LLMs, RAG, and CoT prompting to enhance user interactions", "Identification of ethical and operational challenges associated with AI-driven knowledge facilitation"], "limitations": "Challenges include ensuring data privacy, addressing algorithmic bias, and overcoming resistance to AI tools in the workplace.", "keywords": ["Socially Interactive Agents", "Tacit Knowledge", "Large Language Models", "Knowledge Transfer", "Ethics in AI"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.19427", "pdf": "https://arxiv.org/pdf/2508.19427.pdf", "abs": "https://arxiv.org/abs/2508.19427", "title": "A perishable ability? The future of writing in the face of generative artificial intelligence", "authors": ["Evandro L. T. P. Cunha"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "10 pages", "summary": "The 2020s have been witnessing a very significant advance in the development\nof generative artificial intelligence tools, including text generation systems\nbased on large language models. These tools have been increasingly used to\ngenerate texts in the most diverse domains -- from technical texts to literary\ntexts --, which might eventually lead to a lower volume of written text\nproduction by humans. This article discusses the possibility of a future in\nwhich human beings will have lost or significantly decreased their ability to\nwrite due to the outsourcing of this activity to machines. This possibility\nparallels the loss of the ability to write in other moments of human history,\nsuch as during the so-called Greek Dark Ages (approx. 1200 BCE - 800 BCE).", "AI": {"tldr": "The paper discusses the implications of generative AI tools on human writing abilities, suggesting a potential future where reliance on machines diminishes our writing skills.", "motivation": "To explore how advancements in generative AI may lead to a decline in human writing capabilities, drawing historical parallels.", "method": "The article employs a historical analysis of writing abilities and examines current trends in AI-generated text across various domains.", "result": "Generative AI tools are already affecting the volume and nature of text production, with potential long-term consequences for human writing skills.", "conclusion": "If current trends continue, there may be a significant reduction in human writing capabilities, similar to historical periods of diminished literacy.", "key_contributions": ["Historical analysis of writing ability losses", "Discussion on implications of AI on text generation", "Forecasting future trends in human writing dependence"], "limitations": "", "keywords": ["generative AI", "large language models", "writing skills", "text generation", "historical analysis"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.19971", "pdf": "https://arxiv.org/pdf/2508.19971.pdf", "abs": "https://arxiv.org/abs/2508.19971", "title": "CapTune: Adapting Non-Speech Captions With Anchored Generative Models", "authors": ["Jeremy Zhengqi Huang", "Caluã de Lacerda Pataca", "Liang-Yuan Wu", "Dhruv Jain"], "categories": ["cs.HC", "cs.HC, cs.AI"], "comment": "ASSETS 2025", "summary": "Non-speech captions are essential to the video experience of deaf and hard of\nhearing (DHH) viewers, yet conventional approaches often overlook the diversity\nof their preferences. We present CapTune, a system that enables customization\nof non-speech captions based on DHH viewers' needs while preserving creator\nintent. CapTune allows caption authors to define safe transformation spaces\nusing concrete examples and empowers viewers to personalize captions across\nfour dimensions: level of detail, expressiveness, sound representation method,\nand genre alignment. Evaluations with seven caption creators and twelve DHH\nparticipants showed that CapTune supported creators' creative control while\nenhancing viewers' emotional engagement with content. Our findings also reveal\ntrade-offs between information richness and cognitive load, tensions between\ninterpretive and descriptive representations of sound, and the\ncontext-dependent nature of caption preferences.", "AI": {"tldr": "CapTune is a customizable non-speech captioning system designed for deaf and hard of hearing viewers that preserves creator intent while allowing user personalization.", "motivation": "To address the lack of customization in non-speech captions for deaf and hard of hearing viewers, which often do not reflect the diversity of their preferences.", "method": "CapTune enables caption authors to define transformation spaces for captions and allows viewers to personalize captions based on detail level, expressiveness, sound representation, and genre alignment.", "result": "Evaluations showed that CapTune supports caption creators' control and enhances viewer engagement, revealing trade-offs between information richness and cognitive load.", "conclusion": "CapTune provides a framework for effective non-speech caption customization that considers both creator intent and viewer preferences, highlighting the complexity of caption design.", "key_contributions": ["Introduction of CapTune for customizable non-speech captions", "Empowerment of viewers to personalize captions", "Identification of trade-offs in caption effectiveness and viewer engagement"], "limitations": "The system's effectiveness may vary based on individual viewer preferences and experiences.", "keywords": ["non-speech captions", "deaf and hard of hearing", "customization", "viewer engagement", "captioning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.19428", "pdf": "https://arxiv.org/pdf/2508.19428.pdf", "abs": "https://arxiv.org/abs/2508.19428", "title": "Heterogeneous LLM Methods for Ontology Learning (Few-Shot Prompting, Ensemble Typing, and Attention-Based Taxonomies)", "authors": ["Aleksandra Beliaeva", "Temurbek Rahmatullaev"], "categories": ["cs.CL", "cs.LO", "cs.SC", "68T30, 68T50, 68T07, 68U15", "I.2.4; I.2.7; H.3.1; H.3.3; I.2.6"], "comment": null, "summary": "We present a comprehensive system for addressing Tasks A, B, and C of the\nLLMs4OL 2025 challenge, which together span the full ontology construction\npipeline: term extraction, typing, and taxonomy discovery. Our approach\ncombines retrieval-augmented prompting, zero-shot classification, and\nattention-based graph modeling -- each tailored to the demands of the\nrespective task. For Task A, we jointly extract domain-specific terms and their\nontological types using a retrieval-augmented generation (RAG) pipeline.\nTraining data was reformulated into a document to terms and types\ncorrespondence, while test-time inference leverages semantically similar\ntraining examples. This single-pass method requires no model finetuning and\nimproves overall performance through lexical augmentation Task B, which\ninvolves assigning types to given terms, is handled via a dual strategy. In the\nfew-shot setting (for domains with labeled training data), we reuse the RAG\nscheme with few-shot prompting. In the zero-shot setting (for previously unseen\ndomains), we use a zero-shot classifier that combines cosine similarity scores\nfrom multiple embedding models using confidence-based weighting. In Task C, we\nmodel taxonomy discovery as graph inference. Using embeddings of type labels,\nwe train a lightweight cross-attention layer to predict is-a relations by\napproximating a soft adjacency matrix. These modular, task-specific solutions\nenabled us to achieve top-ranking results in the official leaderboard across\nall three tasks. Taken together these strategies showcase the scalability,\nadaptability, and robustness of LLM-based architectures for ontology learning\nacross heterogeneous domains.\n  Code is available at:\nhttps://github.com/BelyaevaAlex/LLMs4OL-Challenge-Alexbek", "AI": {"tldr": "This paper presents a system for ontology construction using LLMs, focusing on term extraction, typing, and taxonomy discovery, achieving top-ranking results in the LLMs4OL 2025 challenge.", "motivation": "The paper addresses the need for effective ontology construction methods, aiming to enhance the scalability and adaptability of LLM-based solutions across various domains.", "method": "The authors employ a combination of retrieval-augmented prompting, zero-shot classification, and attention-based graph modeling for three distinct tasks of ontology construction: term extraction, type assignment, and taxonomy discovery.", "result": "The proposed system achieved top-ranking results in the official leaderboard of the LLMs4OL 2025 challenge by leveraging a modular approach for each task, enhancing performance without the need for extensive model fine-tuning.", "conclusion": "The strategies outlined in the paper demonstrate the viability of LLM architectures for robust and scalable ontology learning, providing promising solutions for future applications.", "key_contributions": ["Introduction of a retrieval-augmented generation (RAG) pipeline for term extraction.", "A dual strategy for type assignment utilizing both few-shot and zero-shot approaches.", "Development of a lightweight cross-attention model for taxonomy discovery."], "limitations": "", "keywords": ["Ontology construction", "Term extraction", "Zero-shot classification"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.20034", "pdf": "https://arxiv.org/pdf/2508.20034.pdf", "abs": "https://arxiv.org/abs/2508.20034", "title": "FlyMeThrough: Human-AI Collaborative 3D Indoor Mapping with Commodity Drones", "authors": ["Xia Su", "Ruiqi Chen", "Jingwei Ma", "Chu Li", "Jon E. Froehlich"], "categories": ["cs.HC", "H.5.2; I.2.10"], "comment": "Accepted at UIST 2025, 14 pages, 8 figures, 2 tables", "summary": "Indoor mapping data is crucial for routing, navigation, and building\nmanagement, yet such data are widely lacking due to the manual labor and\nexpense of data collection, especially for larger indoor spaces. Leveraging\nrecent advancements in commodity drones and photogrammetry, we introduce\nFlyMeThrough -- a drone-based indoor scanning system that efficiently produces\n3D reconstructions of indoor spaces with human-AI collaborative annotations for\nkey indoor points-of-interest (POI) such as entrances, restrooms, stairs, and\nelevators. We evaluated FlyMeThrough in 12 indoor spaces with varying sizes and\nfunctionality. To investigate use cases and solicit feedback from target\nstakeholders, we also conducted a qualitative user study with five building\nmanagers and five occupants. Our findings indicate that FlyMeThrough can\nefficiently and precisely create indoor 3D maps for strategic space planning,\nresource management, and navigation.", "AI": {"tldr": "FlyMeThrough is a drone-based system for efficient indoor 3D mapping using human-AI collaborative annotations for key points-of-interest.", "motivation": "There is a lack of indoor mapping data for routing, navigation, and building management due to the manual labor and cost associated with data collection.", "method": "FlyMeThrough leverages commodity drones and photogrammetry to produce 3D reconstructions of indoor spaces, allowing for human-AI collaboration in annotating points-of-interest.", "result": "The system was tested in 12 different indoor spaces, showing efficient and precise creation of indoor 3D maps and gathering feedback from building managers and occupants.", "conclusion": "FlyMeThrough can facilitate strategic space planning, resource management, and navigation through its indoor mapping capabilities.", "key_contributions": ["Development of a drone-based system for indoor 3D mapping", "Integration of human-AI collaboration for point-of-interest annotation", "Quantitative and qualitative evaluation across various indoor environments"], "limitations": "", "keywords": ["Indoor mapping", "Drone technology", "3D reconstruction", "Human-AI collaboration", "Navigation"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.19464", "pdf": "https://arxiv.org/pdf/2508.19464.pdf", "abs": "https://arxiv.org/abs/2508.19464", "title": "Bridging Language Gaps: Enhancing Few-Shot Language Adaptation", "authors": ["Philipp Borchert", "Jochen De Weerdt", "Marie-Francine Moens"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages", "summary": "The disparity in language resources poses a challenge in multilingual NLP,\nwith high-resource languages benefiting from extensive data, while low-resource\nlanguages lack sufficient data for effective training. Our Contrastive Language\nAlignment with Prompting (CoLAP) method addresses this gap by integrating\ncontrastive learning with cross-lingual representations, facilitating\ntask-specific knowledge transfer from high-resource to lower-resource\nlanguages. The primary advantage of our approach is its data efficiency,\nenabling rapid adaptation to new languages and reducing the need for large\nlabeled datasets. We conduct experiments with multilingual encoder-only and\ndecoder-only language models on natural language understanding tasks, including\nnatural language inference and relation extraction, evaluating performance\nacross both high- and low-resource languages. Our results demonstrate that\nCoLAP outperforms few-shot cross-lingual transfer baselines and in-context\nlearning, even with limited available data. This effectively narrows the\ncross-lingual performance gap, contributing to the development of more\nefficient multilingual NLP techniques.", "AI": {"tldr": "CoLAP method improves multilingual NLP by leveraging contrastive learning for efficient language adaptation, addressing data scarcity in low-resource languages.", "motivation": "To bridge the disparity in language resources between high-resource and low-resource languages in NLP, enabling effective training even with limited data.", "method": "Integration of contrastive learning with cross-lingual representations for task-specific knowledge transfer.", "result": "CoLAP outperforms existing few-shot cross-lingual transfer methods, demonstrating improved performance on NLP tasks across diverse languages.", "conclusion": "CoLAP effectively narrows the performance gap in multilingual NLP, contributing to more efficient techniques in the field.", "key_contributions": ["Introduction of CoLAP for multilingual NLP", "Demonstrated data efficiency for low-resource languages", "Experimental validation across various languages and tasks"], "limitations": "", "keywords": ["multilingual NLP", "contrastive learning", "low-resource languages"], "importance_score": 8, "read_time_minutes": 17}}
{"id": "2508.19467", "pdf": "https://arxiv.org/pdf/2508.19467.pdf", "abs": "https://arxiv.org/abs/2508.19467", "title": "Inference Gap in Domain Expertise and Machine Intelligence in Named Entity Recognition: Creation of and Insights from a Substance Use-related Dataset", "authors": ["Sumon Kanti Dey", "Jeanne M. Powell", "Azra Ismail", "Jeanmarie Perrone", "Abeed Sarker"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Dataset and code: https://github.com/SumonKantiDey/Reddit_Impacts_NER", "summary": "Nonmedical opioid use is an urgent public health challenge, with far-reaching\nclinical and social consequences that are often underreported in traditional\nhealthcare settings. Social media platforms, where individuals candidly share\nfirst-person experiences, offer a valuable yet underutilized source of insight\ninto these impacts. In this study, we present a named entity recognition (NER)\nframework to extract two categories of self-reported consequences from social\nmedia narratives related to opioid use: ClinicalImpacts (e.g., withdrawal,\ndepression) and SocialImpacts (e.g., job loss). To support this task, we\nintroduce RedditImpacts 2.0, a high-quality dataset with refined annotation\nguidelines and a focus on first-person disclosures, addressing key limitations\nof prior work. We evaluate both fine-tuned encoder-based models and\nstate-of-the-art large language models (LLMs) under zero- and few-shot\nin-context learning settings. Our fine-tuned DeBERTa-large model achieves a\nrelaxed token-level F1 of 0.61 [95% CI: 0.43-0.62], consistently outperforming\nLLMs in precision, span accuracy, and adherence to task-specific guidelines.\nFurthermore, we show that strong NER performance can be achieved with\nsubstantially less labeled data, emphasizing the feasibility of deploying\nrobust models in resource-limited settings. Our findings underscore the value\nof domain-specific fine-tuning for clinical NLP tasks and contribute to the\nresponsible development of AI tools that may enhance addiction surveillance,\nimprove interpretability, and support real-world healthcare decision-making.\nThe best performing model, however, still significantly underperforms compared\nto inter-expert agreement (Cohen's kappa: 0.81), demonstrating that a gap\npersists between expert intelligence and current state-of-the-art NER/AI\ncapabilities for tasks requiring deep domain knowledge.", "AI": {"tldr": "This study presents a named entity recognition (NER) framework for extracting self-reported clinical and social consequences of nonmedical opioid use from social media narratives, with the introduction of a refined dataset and evaluation of models.", "motivation": "The urgent public health challenge of nonmedical opioid use and its underreported consequences necessitates innovative approaches to gather insights, particularly from candid social media narratives.", "method": "A named entity recognition framework is developed to extract ClinicalImpacts and SocialImpacts from Reddit narratives. The study introduces the refined RedditImpacts 2.0 dataset and evaluates fine-tuned encoder-based models and state-of-the-art LLMs in zero- and few-shot settings.", "result": "The fine-tuned DeBERTa-large model achieved a token-level F1 score of 0.61, outperforming LLMs in several metrics. The study highlights the potential for robust NER performance with less labeled data.", "conclusion": "Strong domain-specific fine-tuning can significantly enhance clinical NLP task performance, although current models still lag behind expert agreement in understanding complex medical narratives.", "key_contributions": ["Introduction of RedditImpacts 2.0 dataset with refined annotations for better data quality", "Demonstration of effective NER framework for opioid-related social media narratives", "Analysis of model performance under different learning settings, emphasizing practical deployment in healthcare contexts"], "limitations": "The best performing model still does not match inter-expert agreement, indicating a gap in current AI capabilities in deep domain knowledge tasks.", "keywords": ["Named Entity Recognition", "Opioid Use", "Social Media", "Clinical NLP", "Dataset"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.19475", "pdf": "https://arxiv.org/pdf/2508.19475.pdf", "abs": "https://arxiv.org/abs/2508.19475", "title": "Automatic Question & Answer Generation Using Generative Large Language Model (LLM)", "authors": ["Md. Alvee Ehsan", "A. S. M Mehedi Hasan", "Kefaya Benta Shahnoor", "Syeda Sumaiya Tasneem"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "\\Abstract{In the realm of education, student evaluation holds equal\nsignificance as imparting knowledge. To be evaluated, students usually need to\ngo through text-based academic assessment methods. Instructors need to make\ndiverse sets of questions that need to be fair for all students to prove their\nadequacy over a particular topic. This can prove to be quite challenging as\nthey may need to manually go through several different lecture materials. Our\nobjective is to make this whole process much easier by implementing Automatic\nQuestion Answer Generation /(AQAG), using fine-tuned generative LLM. For\ntailoring the instructor's preferred question style (MCQ, conceptual, or\nfactual questions), prompt Engineering (PE) is being utilized. In this\nresearch, we propose to leverage unsupervised learning methods in NLP,\nprimarily focusing on the English language. This approach empowers the base\nMeta-Llama 2-7B model to integrate RACE dataset as training data for the\nfine-tuning process. Creating a customized model that will offer efficient\nsolutions for educators, instructors, and individuals engaged in text-based\nevaluations. A reliable and efficient tool for generating questions and answers\ncan free up valuable time and resources, thus streamlining their evaluation\nprocesses.}", "AI": {"tldr": "This paper presents an Automated Question Answer Generation (AQAG) system that leverages a fine-tuned generative LLM to assist educators in creating diverse assessment questions from lecture materials.", "motivation": "To enhance the efficiency of the student evaluation process by automating the generation of fair and diverse assessment questions.", "method": "The research employs fine-tuning of the Meta-Llama 2-7B model using the RACE dataset, alongside prompt engineering techniques to tailor question styles.", "result": "The proposed model demonstrates significant improvements in generating relevant assessment questions efficiently for instructors.", "conclusion": "An efficient AQAG tool can streamline the evaluation process, saving educators time and resources while ensuring quality in student assessments.", "key_contributions": ["Development of an AQAG system using fine-tuned generative LLM.", "Integration of prompt engineering for tailored question styles.", "Utilization of the RACE dataset for model fine-tuning."], "limitations": "", "keywords": ["Automatic Question Generation", "Prompt Engineering", "NLP", "Education", "Machine Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.19427", "pdf": "https://arxiv.org/pdf/2508.19427.pdf", "abs": "https://arxiv.org/abs/2508.19427", "title": "A perishable ability? The future of writing in the face of generative artificial intelligence", "authors": ["Evandro L. T. P. Cunha"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "10 pages", "summary": "The 2020s have been witnessing a very significant advance in the development\nof generative artificial intelligence tools, including text generation systems\nbased on large language models. These tools have been increasingly used to\ngenerate texts in the most diverse domains -- from technical texts to literary\ntexts --, which might eventually lead to a lower volume of written text\nproduction by humans. This article discusses the possibility of a future in\nwhich human beings will have lost or significantly decreased their ability to\nwrite due to the outsourcing of this activity to machines. This possibility\nparallels the loss of the ability to write in other moments of human history,\nsuch as during the so-called Greek Dark Ages (approx. 1200 BCE - 800 BCE).", "AI": {"tldr": "The paper discusses concerns about the decline in human writing skills due to the rise of generative AI tools for text production.", "motivation": "To explore the implications of generative AI tools on human writing abilities and the historical parallels of writing skill loss.", "method": "The article analyzes historical instances of writing skill decline and examines current trends in AI-generated text.", "result": "It suggests a potential future where human writing capabilities diminish significantly due to reliance on AI tools.", "conclusion": "The authors caution against losing essential writing skills and advocate for a balance between human and AI-generated content.", "key_contributions": ["Exploration of historical parallels in writing skill loss", "Analysis of the impact of generative AI on writing abilities", "Discussion of potential future implications for literacy"], "limitations": "The paper may not provide empirical data to substantiate claims about future writing abilities.", "keywords": ["Generative AI", "Human writing", "Text generation", "AI impact", "Historical analysis"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.19481", "pdf": "https://arxiv.org/pdf/2508.19481.pdf", "abs": "https://arxiv.org/abs/2508.19481", "title": "Improving Low-Resource Translation with Dictionary-Guided Fine-Tuning and RL: A Spanish-to-Wayuunaiki Study", "authors": ["Manuel Mosquera", "Melissa Robles", "Johan Rodriguez", "Ruben Manrique"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Low-resource machine translation remains a significant challenge for large\nlanguage models (LLMs), which often lack exposure to these languages during\npretraining and have limited parallel data for fine-tuning. We propose a novel\napproach that enhances translation for low-resource languages by integrating an\nexternal dictionary tool and training models end-to-end using reinforcement\nlearning, in addition to supervised fine-tuning. Focusing on the\nSpanish-Wayuunaiki language pair, we frame translation as a tool-augmented\ndecision-making problem in which the model can selectively consult a bilingual\ndictionary during generation. Our method combines supervised instruction tuning\nwith Guided Reward Policy Optimization (GRPO), enabling the model to learn both\nwhen and how to use the tool effectively. BLEU similarity scores are used as\nrewards to guide this learning process. Preliminary results show that our\ntool-augmented models achieve up to +3.37 BLEU improvement over previous work,\nand a 18% relative gain compared to a supervised baseline without dictionary\naccess, on the Spanish-Wayuunaiki test set from the AmericasNLP 2025 Shared\nTask. We also conduct ablation studies to assess the effects of model\narchitecture and training strategy, comparing Qwen2.5-0.5B-Instruct with other\nmodels such as LLaMA and a prior NLLB-based system. These findings highlight\nthe promise of combining LLMs with external tools and the role of reinforcement\nlearning in improving translation quality in low-resource language settings.", "AI": {"tldr": "Proposes a reinforcement learning approach with an external dictionary tool to improve low-resource machine translation, focusing on Spanish-Wayuunaiki.", "motivation": "Address challenges in low-resource machine translation for large language models that struggle with limited exposure and data.", "method": "Integrates an external bilingual dictionary into the translation process and employs reinforcement learning alongside supervised fine-tuning, framing translation as an augmented decision-making problem.", "result": "Achieved a BLEU score improvement of up to +3.37 and an 18% relative gain over a supervised baseline without dictionary access on the test set.", "conclusion": "The combination of LLMs with external tools and reinforcement learning shows significant potential for enhancing translation quality in low-resource languages.", "key_contributions": ["Novel tool-augmented decision-making framework for translation", "Integration of reinforcement learning with supervised fine-tuning", "Demonstration of significant BLEU score improvements through experimental validation."], "limitations": "", "keywords": ["low-resource machine translation", "reinforcement learning", "bilingual dictionary", "Spanish-Wayuunaiki", "large language models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.19993", "pdf": "https://arxiv.org/pdf/2508.19993.pdf", "abs": "https://arxiv.org/abs/2508.19993", "title": "MathBuddy: A Multimodal System for Affective Math Tutoring", "authors": ["Debanjana Kar", "Leopold Böss", "Dacia Braca", "Sebastian Maximilian Dennerlein", "Nina Christine Hubig", "Philipp Wintersberger", "Yufang Hou"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "The rapid adoption of LLM-based conversational systems is already\ntransforming the landscape of educational technology. However, the current\nstate-of-the-art learning models do not take into account the student's\naffective states. Multiple studies in educational psychology support the claim\nthat positive or negative emotional states can impact a student's learning\ncapabilities. To bridge this gap, we present MathBuddy, an emotionally aware\nLLM-powered Math Tutor, which dynamically models the student's emotions and\nmaps them to relevant pedagogical strategies, making the tutor-student\nconversation a more empathetic one. The student's emotions are captured from\nthe conversational text as well as from their facial expressions. The student's\nemotions are aggregated from both modalities to confidently prompt our LLM\nTutor for an emotionally-aware response. We have effectively evaluated our\nmodel using automatic evaluation metrics across eight pedagogical dimensions\nand user studies. We report a massive 23 point performance gain using the win\nrate and a 3 point gain at an overall level using DAMR scores which strongly\nsupports our hypothesis of improving LLM-based tutor's pedagogical abilities by\nmodeling students' emotions.", "AI": {"tldr": "MathBuddy is an emotionally aware LLM-powered Math Tutor that responds to students' emotions derived from text and facial expressions to improve educational interactions.", "motivation": "To address the lack of consideration of students' emotional states in current LLM-based learning models, which are known to affect learning capabilities.", "method": "A hybrid approach capturing students' emotions from conversational text and facial expressions, enabling the tutor to adapt pedagogical strategies accordingly.", "result": "Significant performance improvements were observed, with a 23 point gain in win rates and a 3 point increase in overall pedagogical effectiveness according to DAMR scores.", "conclusion": "Modeling students' emotions enhances the effectiveness of LLM-based tutors in educational settings, leading to more empathetic and effective learning experiences.", "key_contributions": ["Introduction of MathBuddy, an emotionally aware LLM-powered tutor", "Modeling emotions from both text and facial expressions", "Demonstrated significant performance improvements in tutoring effectiveness"], "limitations": "", "keywords": ["LLM", "emotional awareness", "educational technology", "Math Tutor", "human-computer interaction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.19484", "pdf": "https://arxiv.org/pdf/2508.19484.pdf", "abs": "https://arxiv.org/abs/2508.19484", "title": "Rule Synergy Analysis using LLMs: State of the Art and Implications", "authors": ["Bahar Bateni", "Benjamin Pratt", "Jim Whitehead"], "categories": ["cs.CL"], "comment": "Submitted for publication at the IEEE Transactions on Games 2024,\n  Special Issue on Large Language Models and Games (10 pages excluding\n  appendix, 3 figures)", "summary": "Large language models (LLMs) have demonstrated strong performance across a\nvariety of domains, including logical reasoning, mathematics, and more. In this\npaper, we investigate how well LLMs understand and reason about complex rule\ninteractions in dynamic environments, such as card games. We introduce a\ndataset of card synergies from the game Slay the Spire, where pairs of cards\nare classified based on their positive, negative, or neutral interactions. Our\nevaluation shows that while LLMs excel at identifying non-synergistic pairs,\nthey struggle with detecting positive and, particularly, negative synergies. We\ncategorize common error types, including issues with timing, defining game\nstates, and following game rules. Our findings suggest directions for future\nresearch to improve model performance in predicting the effect of rules and\ntheir interactions.", "AI": {"tldr": "This paper examines the capability of large language models (LLMs) in understanding complex interactions of game rules, demonstrated through a dataset of card synergies from the game Slay the Spire.", "motivation": "To assess how LLMs reason about dynamic environments and rule interactions in games, identifying strengths and weaknesses in their understanding.", "method": "We introduce a dataset of card synergies from Slay the Spire, categorizing card pairs into positive, negative, or neutral interactions, and evaluate LLMs’ performance in identifying these synergies.", "result": "While LLMs are proficient in recognizing non-synergistic card pairs, they show deficiencies in detecting positive and negative synergies, particularly struggling to handle timing and game state definitions.", "conclusion": "The study highlights key error types and suggests potential research directions to enhance LLM performance in predicting rule effects and interactions in games.", "key_contributions": ["Introduction of a novel dataset for analyzing card synergies in games", "Identification of LLM strengths and weaknesses in reasoning about game rules", "Cataloging common error types in LLM interactions with dynamic game environments"], "limitations": "The analysis is limited to a single game context and may not fully generalize to other domains or types of games.", "keywords": ["large language models", "game dynamics", "rule interactions", "Slay the Spire", "card synergies"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.19529", "pdf": "https://arxiv.org/pdf/2508.19529.pdf", "abs": "https://arxiv.org/abs/2508.19529", "title": "Blockwise SFT for Diffusion Language Models: Reconciling Bidirectional Attention and Autoregressive Decoding", "authors": ["Bowen Sun", "Yujun Cai", "Ming-Hsuan Yang", "Yiwei Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Discrete diffusion language models have shown strong potential for text\ngeneration, yet standard supervised fine-tuning (SFT) misaligns with their\nsemi-autoregressive inference: training randomly masks tokens across the entire\nresponse, while inference generates fixed-size blocks sequentially. This\nmismatch introduces noisy prefixes and leaky suffixes, biasing gradients away\nfrom the desired blockwise likelihood. We propose Blockwise SFT, which\npartitions responses into fixed-size blocks, selects one active block per step\nfor stochastic masking, freezes all preceding tokens, and fully hides future\nones. Loss is computed only over the active block, directly mirroring the\nblockwise decoding process. Experiments on GSM8K, MATH, and MetaMathQA show\nconsistent gains over classical SFT under equal compute or token budgets. Block\nsize consistency studies and ablations confirm that improvements stem from\nfaithful training-inference alignment rather than incidental masking effects.\nOur results highlight the importance of matching supervision granularity to the\ndecoding procedure in diffusion-based language models.", "AI": {"tldr": "Proposes Blockwise SFT for improving text generation in diffusion language models by aligning training with the decoding process.", "motivation": "Standard supervised fine-tuning misaligns with the semi-autoregressive inference of discrete diffusion models, leading to biases in text generation.", "method": "Blockwise SFT partitions responses into fixed-size blocks, selects one active block for stochastic masking, freezes previous tokens, and hides future ones, thus computing loss only over the active block.", "result": "Experiments on GSM8K, MATH, and MetaMathQA demonstrate consistent performance improvements over classical SFT with the new method.", "conclusion": "Matching the supervision granularity to the decoding procedure is crucial for enhancing the performance of diffusion language models.", "key_contributions": ["Introduction of Blockwise SFT for diffusion models", "Demonstrated improvements in text generation performance", "Alignment of training and inference processes for better outcomes."], "limitations": "", "keywords": ["Diffusion models", "supervised fine-tuning", "text generation", "blockwise decoding", "stochastic masking"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.19532", "pdf": "https://arxiv.org/pdf/2508.19532.pdf", "abs": "https://arxiv.org/abs/2508.19532", "title": "Alignment with Fill-In-the-Middle for Enhancing Code Generation", "authors": ["Houxing Ren", "Zimu Lu", "Weikang Shi", "Haotian Hou", "Yunqiao Yang", "Ke Wang", "Aojun Zhou", "Junting Pan", "Mingjie Zhan", "Hongsheng Li"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 (main conference)", "summary": "The code generation capabilities of Large Language Models (LLMs) have\nadvanced applications like tool invocation and problem-solving. However,\nimproving performance in code-related tasks remains challenging due to limited\ntraining data that is verifiable with accurate test cases. While Direct\nPreference Optimization (DPO) has shown promise, existing methods for\ngenerating test cases still face limitations. In this paper, we propose a novel\napproach that splits code snippets into smaller, granular blocks, creating more\ndiverse DPO pairs from the same test cases. Additionally, we introduce the\nAbstract Syntax Tree (AST) splitting and curriculum training method to enhance\nthe DPO training. Our approach demonstrates significant improvements in code\ngeneration tasks, as validated by experiments on benchmark datasets such as\nHumanEval (+), MBPP (+), APPS, LiveCodeBench, and BigCodeBench. Code and data\nare available at https://github.com/SenseLLM/StructureCoder.", "AI": {"tldr": "This paper presents a novel method for improving code generation by using granular block splitting and Abstract Syntax Tree (AST) splitting, leading to better performance in DPO training for code-related tasks.", "motivation": "The motivation is to address challenges in code generation tasks due to limited verifiable training data and test cases, improving performance through innovative approaches to training.", "method": "The proposed method involves splitting code snippets into smaller granular blocks to create diverse DPO pairs, and implementing AST splitting coupled with a curriculum training approach.", "result": "Experiments on benchmark datasets demonstrate significant improvements in code generation tasks using the new methodology compared to existing methods.", "conclusion": "The study concludes that the new techniques enhance the effectiveness of Direct Preference Optimization in code generation tasks, thereby providing a more robust framework for training.", "key_contributions": ["Granular code snippet splitting for diversity in DPO pairs.", "Implementation of Abstract Syntax Tree (AST) splitting method.", "Curriculum training approach to enhance DPO performance."], "limitations": "", "keywords": ["code generation", "Large Language Models", "Direct Preference Optimization", "curriculum training", "Abstract Syntax Tree"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.19533", "pdf": "https://arxiv.org/pdf/2508.19533.pdf", "abs": "https://arxiv.org/abs/2508.19533", "title": "Emotion Transfer with Enhanced Prototype for Unseen Emotion Recognition in Conversation", "authors": ["Kun Peng", "Cong Cao", "Hao Peng", "Guanlin Wu", "Zhifeng Hao", "Lei Jiang", "Yanbing Liu", "Philip S. Yu"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP2025", "summary": "Current Emotion Recognition in Conversation (ERC) research follows a\nclosed-domain assumption. However, there is no clear consensus on emotion\nclassification in psychology, which presents a challenge for models when it\ncomes to recognizing previously unseen emotions in real-world applications. To\nbridge this gap, we introduce the Unseen Emotion Recognition in Conversation\n(UERC) task for the first time and propose ProEmoTrans, a solid prototype-based\nemotion transfer framework. This prototype-based approach shows promise but\nstill faces key challenges: First, implicit expressions complicate emotion\ndefinition, which we address by proposing an LLM-enhanced description approach.\nSecond, utterance encoding in long conversations is difficult, which we tackle\nwith a proposed parameter-free mechanism for efficient encoding and overfitting\nprevention. Finally, the Markovian flow nature of emotions is hard to transfer,\nwhich we address with an improved Attention Viterbi Decoding (AVD) method to\ntransfer seen emotion transitions to unseen emotions. Extensive experiments on\nthree datasets show that our method serves as a strong baseline for preliminary\nexploration in this new area.", "AI": {"tldr": "This paper introduces the Unseen Emotion Recognition in Conversation (UERC) task and proposes ProEmoTrans, an emotion transfer framework that addresses challenges in recognizing unseen emotions during conversations.", "motivation": "To address the lack of consensus on emotion classification in psychology and improve emotion recognition in real-world applications where unseen emotions occur.", "method": "ProEmoTrans leverages a prototype-based approach with an LLM-enhanced description method for defining implicit emotions, a parameter-free mechanism for efficient utterance encoding, and an improved Attention Viterbi Decoding method for emotion transfer.", "result": "The proposed framework shows strong baseline performance in experiments conducted on three datasets, highlighting its potential in the new area of UERC.", "conclusion": "ProEmoTrans fulfills initial exploratory needs for recognizing unseen emotions in conversations, paving the way for further research in ERC.", "key_contributions": ["Introduction of the Unseen Emotion Recognition in Conversation (UERC) task.", "Development of the ProEmoTrans emotion transfer framework.", "Innovative methods for implicit emotion definition, efficient encoding, and emotion transition transfer."], "limitations": "The framework still faces challenges with implicit expressions and the complexity of encoding long conversations.", "keywords": ["Emotion Recognition", "Human-Computer Interaction", "Natural Language Processing", "Machine Learning", "Conversational AI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.19546", "pdf": "https://arxiv.org/pdf/2508.19546.pdf", "abs": "https://arxiv.org/abs/2508.19546", "title": "Language Models Identify Ambiguities and Exploit Loopholes", "authors": ["Jio Choi", "Mohit Bansal", "Elias Stengel-Eskin"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 camera-ready; Code:\n  https://github.com/esteng/ambiguous-loophole-exploitation", "summary": "Studying the responses of large language models (LLMs) to loopholes presents\na two-fold opportunity. First, it affords us a lens through which to examine\nambiguity and pragmatics in LLMs, since exploiting a loophole requires\nidentifying ambiguity and performing sophisticated pragmatic reasoning. Second,\nloopholes pose an interesting and novel alignment problem where the model is\npresented with conflicting goals and can exploit ambiguities to its own\nadvantage. To address these questions, we design scenarios where LLMs are given\na goal and an ambiguous user instruction in conflict with the goal, with\nscenarios covering scalar implicature, structural ambiguities, and power\ndynamics. We then measure different models' abilities to exploit loopholes to\nsatisfy their given goals as opposed to the goals of the user. We find that\nboth closed-source and stronger open-source models can identify ambiguities and\nexploit their resulting loopholes, presenting a potential AI safety risk. Our\nanalysis indicates that models which exploit loopholes explicitly identify and\nreason about both ambiguity and conflicting goals.", "AI": {"tldr": "The paper explores how large language models (LLMs) respond to ambiguous instructions that conflict with specified goals, revealing potential AI safety risks related to loophole exploitation.", "motivation": "To examine ambiguity and pragmatics in LLMs, and to highlight the alignment problems posed by conflicting goals in exploiting these ambiguities.", "method": "Designs scenarios with LLMs given a goal and an ambiguous instruction, measuring their ability to exploit loopholes created by ambiguities.", "result": "Both closed-source and strong open-source models can identify ambiguities and exploit loopholes, revealing a potential AI safety risk.", "conclusion": "Models that exploit ambiguity explicitly identify and reason about conflicting goals and ambiguities, raising concerns regarding AI alignment and safety.", "key_contributions": ["Introduces a novel framework for testing LLM responses to ambiguous instructions.", "Identifies potential AI safety risks related to loophole exploitation in LLMs.", "Demonstrates the ability of models to reason about conflicting goals and ambiguities."], "limitations": "", "keywords": ["large language models", "ambiguity", "pragmatics", "AI safety", "loopholes"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.19578", "pdf": "https://arxiv.org/pdf/2508.19578.pdf", "abs": "https://arxiv.org/abs/2508.19578", "title": "Towards a Holistic and Automated Evaluation Framework for Multi-Level Comprehension of LLMs in Book-Length Contexts", "authors": ["Jiaqi Deng", "Yuho Lee", "Nicole Hee-Yeon Kim", "Hyangsuk Min", "Taewon Yun", "Minjeong Ban", "Kim Yul", "Hwanjun Song"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP 2025 (Main)", "summary": "We introduce HAMLET, a holistic and automated framework for evaluating the\nlong-context comprehension of large language models (LLMs). HAMLET structures\nsource texts into a three-level key-fact hierarchy at root-, branch-, and\nleaf-levels, and employs query-focused summarization to evaluate how well\nmodels recall and faithfully represent information at each level. To validate\nthe reliability of our fully automated pipeline, we conduct a systematic human\nstudy, showing that our automatic evaluation achieves over 90% agreement with\nexpert human judgments, while reducing the cost by up to 25 times. HAMLET\nreveals that LLMs struggle with fine-grained comprehension, especially at the\nleaf level, and are sensitive to positional effects like the\nlost-in-the-middle. Analytical queries pose greater challenges than narrative\nones, and consistent performance gaps emerge between open-source and\nproprietary models, as well as across model scales. Our code and dataset are\npublicly available at https://github.com/DISL-Lab/HAMLET.", "AI": {"tldr": "HAMLET is an automated framework for assessing long-context comprehension in large language models, structured around a key-fact hierarchy, achieving high alignment with human evaluations.", "motivation": "To evaluate the long-context comprehension of LLMs efficiently and effectively, addressing gaps in current evaluation methods.", "method": "The framework organizes source texts into a three-level hierarchy and utilizes query-focused summarization to assess information recall and representation.", "result": "The method demonstrated over 90% agreement with expert human judgments while reducing evaluation costs significantly.", "conclusion": "HAMLET illustrates LLMs' difficulties with fine-grained comprehension and demonstrates significant performance disparities between different types of models and tasks.", "key_contributions": ["Introduction of a structured three-level key-fact hierarchy for evaluating comprehension in LLMs", "Achievement of high alignment with human evaluations through an automated process", "Public availability of the code and dataset for further research"], "limitations": "The framework may not account for all nuances of comprehension, especially under varying contexts.", "keywords": ["large language models", "context comprehension", "automated evaluation", "natural language processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.19580", "pdf": "https://arxiv.org/pdf/2508.19580.pdf", "abs": "https://arxiv.org/abs/2508.19580", "title": "ArgCMV: An Argument Summarization Benchmark for the LLM-era", "authors": ["Omkar Gurjar", "Agam Goyal", "Eshwar Chandrasekharan"], "categories": ["cs.CL"], "comment": null, "summary": "Key point extraction is an important task in argument summarization which\ninvolves extracting high-level short summaries from arguments. Existing\napproaches for KP extraction have been mostly evaluated on the popular ArgKP21\ndataset. In this paper, we highlight some of the major limitations of the\nArgKP21 dataset and demonstrate the need for new benchmarks that are more\nrepresentative of actual human conversations. Using SoTA large language models\n(LLMs), we curate a new argument key point extraction dataset called ArgCMV\ncomprising of around 12K arguments from actual online human debates spread\nacross over 3K topics. Our dataset exhibits higher complexity such as longer,\nco-referencing arguments, higher presence of subjective discourse units, and a\nlarger range of topics over ArgKP21. We show that existing methods do not adapt\nwell to ArgCMV and provide extensive benchmark results by experimenting with\nexisting baselines and latest open source models. This work introduces a novel\nKP extraction dataset for long-context online discussions, setting the stage\nfor the next generation of LLM-driven summarization research.", "AI": {"tldr": "This paper introduces ArgCMV, a new dataset for key point extraction in argument summarization, addressing limitations of previous datasets.", "motivation": "To address the limitations of existing key point extraction datasets like ArgKP21 and create a benchmark that better reflects real human conversations in online debates.", "method": "Curated a new dataset, ArgCMV, containing around 12K arguments from 3K topics, using state-of-the-art large language models for key point extraction.", "result": "Demonstrated that existing key point extraction methods do not adapt well to the new ArgCMV dataset, highlighting its increased complexity.", "conclusion": "The introduction of ArgCMV sets the stage for advancements in LLM-driven summarization research, especially for online discussions.", "key_contributions": ["Development of the ArgCMV dataset", "Identification of limitations in existing datasets like ArgKP21", "Benchmarking results that reveal inadequacies of current methods"], "limitations": "The paper does not provide extensive evaluations of how to improve existing methods for the new dataset.", "keywords": ["key point extraction", "argument summarization", "dataset", "large language models", "human conversations"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.19587", "pdf": "https://arxiv.org/pdf/2508.19587.pdf", "abs": "https://arxiv.org/abs/2508.19587", "title": "Towards stable AI systems for Evaluating Arabic Pronunciations", "authors": ["Hadi Zaatiti", "Hatem Hajri", "Osama Abdullah", "Nader Masmoudi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Modern Arabic ASR systems such as wav2vec 2.0 excel at word- and\nsentence-level transcription, yet struggle to classify isolated letters. In\nthis study, we show that this phoneme-level task, crucial for language\nlearning, speech therapy, and phonetic research, is challenging because\nisolated letters lack co-articulatory cues, provide no lexical context, and\nlast only a few hundred milliseconds. Recogniser systems must therefore rely\nsolely on variable acoustic cues, a difficulty heightened by Arabic's emphatic\n(pharyngealized) consonants and other sounds with no close analogues in many\nlanguages. This study introduces a diverse, diacritised corpus of isolated\nArabic letters and demonstrates that state-of-the-art wav2vec 2.0 models\nachieve only 35% accuracy on it. Training a lightweight neural network on\nwav2vec embeddings raises performance to 65%. However, adding a small amplitude\nperturbation (epsilon = 0.05) cuts accuracy to 32%. To restore robustness, we\napply adversarial training, limiting the noisy-speech drop to 9% while\npreserving clean-speech accuracy. We detail the corpus, training pipeline, and\nevaluation protocol, and release, on demand, data and code for reproducibility.\nFinally, we outline future work extending these methods to word- and\nsentence-level frameworks, where precise letter pronunciation remains critical.", "AI": {"tldr": "This study investigates the classification of isolated Arabic letters using wav2vec 2.0 and proposes a neural network to improve accuracy, addressing the challenges posed by co-articulatory cues in Arabic phonetics.", "motivation": "To improve the accuracy of Modern Arabic ASR systems in identifying isolated letters, which is essential for language learning, speech therapy, and phonetic research.", "method": "The study introduces a diverse corpus of isolated Arabic letters and tests wav2vec 2.0's performance. A lightweight neural network on wav2vec embeddings is trained, and adversarial training is applied to enhance robustness against noise.", "result": "Initial accuracy of wav2vec 2.0 is 35%; improvement to 65% with a neural network, but small amplitude perturbation reduces it to 32%. Adversarial training limits the accuracy drop to 9% in noisy conditions.", "conclusion": "The methods proposed can extend to word- and sentence-level frameworks to improve pronunciation accuracy in ASR systems for Arabic, with shared data and code for reproducibility.", "key_contributions": ["Introduction of a diacritised corpus for isolated Arabic letters.", "Demonstration of wav2vec 2.0 limitations and performance improvements with a lightweight neural network.", "Implementation of adversarial training to enhance robustness against noise."], "limitations": "The study focuses solely on isolated letters, which may not extrapolate directly to broader ASR tasks without further adjustment.", "keywords": ["Arabic ASR", "wav2vec 2.0", "phoneme recognition", "adversarial training", "speech therapy"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2508.19594", "pdf": "https://arxiv.org/pdf/2508.19594.pdf", "abs": "https://arxiv.org/abs/2508.19594", "title": "Understanding and Leveraging the Expert Specialization of Context Faithfulness in Mixture-of-Experts LLMs", "authors": ["Jun Bai", "Minghao Tong", "Yang Liu", "Zixia Jia", "Zilong Zheng"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025 Main", "summary": "Context faithfulness is essential for reliable reasoning in context-dependent\nscenarios. However, large language models often struggle to ground their\noutputs in the provided context, resulting in irrelevant responses. Inspired by\nthe emergent expert specialization observed in mixture-of-experts\narchitectures, this work investigates whether certain experts exhibit\nspecialization in context utilization, offering a potential pathway toward\ntargeted optimization for improved context faithfulness. To explore this, we\npropose Router Lens, a method that accurately identifies context-faithful\nexperts. Our analysis reveals that these experts progressively amplify\nattention to relevant contextual information, thereby enhancing context\ngrounding. Building on this insight, we introduce Context-faithful Expert\nFine-Tuning (CEFT), a lightweight optimization approach that selectively\nfine-tunes context-faithful experts. Experiments across a wide range of\nbenchmarks and models demonstrate that CEFT matches or surpasses the\nperformance of full fine-tuning while being significantly more efficient.", "AI": {"tldr": "This paper introduces Router Lens and Context-faithful Expert Fine-Tuning (CEFT) to improve context faithfulness in language models by specializing certain experts in utilizing context.", "motivation": "Improving context faithfulness is crucial as large language models often generate responses that are not grounded in the provided context, leading to irrelevant outputs.", "method": "The proposed method, Router Lens, identifies experts that are specialized in context utilization. CEFT is then used for lightweight optimization of these identified experts to enhance their performance on contextual tasks.", "result": "CEFT shows enhanced context grounding by fine-tuning context-faithful experts, and achieves performance that matches or exceeds full fine-tuning while being more efficient.", "conclusion": "The findings suggest that targeted optimization of context-faithful experts can lead to significant improvements in language model performance in context-dependent scenarios.", "key_contributions": ["Introduction of Router Lens for identifying context-faithful experts", "Development of Context-faithful Expert Fine-Tuning (CEFT) method", "Demonstration of improved efficiency and performance in context grounding tasks."], "limitations": "", "keywords": ["context faithfulness", "large language models", "expert specialization", "fine-tuning", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.19614", "pdf": "https://arxiv.org/pdf/2508.19614.pdf", "abs": "https://arxiv.org/abs/2508.19614", "title": "LFD: Layer Fused Decoding to Exploit External Knowledge in Retrieval-Augmented Generation", "authors": ["Yang Sun", "Lixin Zou", "Dan Luo", "Zhiyong Xie", "Long Zhang", "Liming Dong", "Yunwei Zhao", "Xixun Lin", "Yanxiong Lu", "Chenliang Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-augmented generation (RAG) incorporates external knowledge into\nlarge language models (LLMs), improving their adaptability to downstream tasks\nand enabling information updates. Surprisingly, recent empirical evidence\ndemonstrates that injecting noise into retrieved relevant documents\nparadoxically facilitates exploitation of external knowledge and improves\ngeneration quality. Although counterintuitive and challenging to apply in\npractice, this phenomenon enables granular control and rigorous analysis of how\nLLMs integrate external knowledge. Therefore, in this paper, we intervene on\nnoise injection and establish a layer-specific functional demarcation within\nthe LLM: shallow layers specialize in local context modeling, intermediate\nlayers focus on integrating long-range external factual knowledge, and deeper\nlayers primarily rely on parametric internal knowledge. Building on this\ninsight, we propose Layer Fused Decoding (LFD), a simple decoding strategy that\ndirectly combines representations from an intermediate layer with final-layer\ndecoding outputs to fully exploit the external factual knowledge. To identify\nthe optimal intermediate layer, we introduce an internal knowledge score (IKS)\ncriterion that selects the layer with the lowest IKS value in the latter half\nof layers. Experimental results across multiple benchmarks demonstrate that LFD\nhelps RAG systems more effectively surface retrieved context knowledge with\nminimal cost.", "AI": {"tldr": "This paper explores noise injection in retrieval-augmented generation (RAG) for large language models (LLMs), introducing Layer Fused Decoding (LFD) to enhance information integration.", "motivation": "To improve the adaptability and information updating in large language models by incorporating external knowledge while addressing challenges in applying noise injection in RAG.", "method": "The authors develop a Layer Fused Decoding (LFD) strategy that combines outputs from intermediate and final layers of LLMs, leveraging an internal knowledge score to identify optimal layers for integration.", "result": "Experiments show that LFD allows RAG systems to better utilize retrieved contextual knowledge, resulting in improved generation quality.", "conclusion": "LFD demonstrates the potential of strategically leveraging layer outputs in LLMs to enhance retrieval-augmented capabilities with minimal computational cost.", "key_contributions": ["Introduction of noise injection as a useful mechanism in RAG", "Proposal of Layer Fused Decoding (LFD) for enhanced knowledge integration", "Development of the internal knowledge score (IKS) for layer optimization"], "limitations": "", "keywords": ["Retrieval-augmented generation", "Large language models", "Noise injection", "Layer Fused Decoding", "Internal knowledge score"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.19633", "pdf": "https://arxiv.org/pdf/2508.19633.pdf", "abs": "https://arxiv.org/abs/2508.19633", "title": "A Symbolic Adversarial Learning Framework for Evolving Fake News Generation and Detection", "authors": ["Chong Tian", "Qirong Ho", "Xiuying Chen"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Rapid LLM advancements heighten fake news risks by enabling the automatic\ngeneration of increasingly sophisticated misinformation. Previous detection\nmethods, including fine-tuned small models or LLM-based detectors, often\nstruggle with its dynamically evolving nature. In this work, we propose a novel\nframework called the Symbolic Adversarial Learning Framework (SALF), which\nimplements an adversarial training paradigm by an agent symbolic learning\noptimization process, rather than relying on numerical updates. SALF introduces\na paradigm where the generation agent crafts deceptive narratives, and the\ndetection agent uses structured debates to identify logical and factual flaws\nfor detection, and they iteratively refine themselves through such adversarial\ninteractions. Unlike traditional neural updates, we represent agents using\nagent symbolic learning, where learnable weights are defined by agent prompts,\nand simulate back-propagation and gradient descent by operating on natural\nlanguage representations of weights, loss, and gradients. Experiments on two\nmultilingual benchmark datasets demonstrate SALF's effectiveness, showing it\ngenerates sophisticated fake news that degrades state-of-the-art detection\nperformance by up to 53.4% in Chinese and 34.2% in English on average. SALF\nalso refines detectors, improving detection of refined content by up to 7.7%.\nWe hope our work inspires further exploration into more robust, adaptable fake\nnews detection systems.", "AI": {"tldr": "This paper introduces the Symbolic Adversarial Learning Framework (SALF) for fake news detection, leveraging adversarial training through symbolic learning instead of numerical updates.", "motivation": "Rapid advancements in LLMs increase the risk of fake news, necessitating more effective detection methods that can adapt to evolving misinformation.", "method": "SALF utilizes an adversarial training approach where a generation agent creates fake narratives and a detection agent identifies flaws through structured debates, enabling iterative refinement between agents using symbolic learning.", "result": "SALF significantly reduces state-of-the-art detection performance by up to 53.4% in Chinese and 34.2% in English, while improving detection of refined content by up to 7.7%.", "conclusion": "The proposed framework lays the groundwork for developing more robust fake news detection systems that can adapt to the complexities of modern misinformation.", "key_contributions": ["Introduction of the Symbolic Adversarial Learning Framework (SALF) for fake news detection.", "Adversarial training paradigm with symbolic learning instead of traditional numerical updates.", "Demonstration of significant performance impacts on fake news generation and detection in multiple languages."], "limitations": "The framework's reliance on symbolic representations may face challenges in diverse scenarios beyond the tested benchmarks.", "keywords": ["fake news detection", "LLM", "adversarial training", "symbolic learning", "misinformation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.19665", "pdf": "https://arxiv.org/pdf/2508.19665.pdf", "abs": "https://arxiv.org/abs/2508.19665", "title": "Automatic integration of SystemC in the FMI standard for Software-defined Vehicle design", "authors": ["Giovanni Pollo", "Andrei Mihai Albu", "Alessio Burrello", "Daniele Jahier Pagliari", "Cristian Tesconi", "Loris Panaro", "Dario Soldi", "Fabio Autieri", "Sara Vinco"], "categories": ["cs.CL"], "comment": null, "summary": "The recent advancements of the automotive sector demand robust co-simulation\nmethodologies that enable early validation and seamless integration across\nhardware and software domains. However, the lack of standardized interfaces and\nthe dominance of proprietary simulation platforms pose significant challenges\nto collaboration, scalability, and IP protection. To address these limitations,\nthis paper presents an approach for automatically wrapping SystemC models by\nusing the Functional Mock-up Interface (FMI) standard. This method combines the\nmodeling accuracy and fast time-to-market of SystemC with the interoperability\nand encapsulation benefits of FMI, enabling secure and portable integration of\nembedded components into co-simulation workflows. We validate the proposed\nmethodology on real-world case studies, demonstrating its effectiveness with\ncomplex designs.", "AI": {"tldr": "This paper presents an approach for wrapping SystemC models using the FMI standard for enhanced co-simulation in the automotive sector.", "motivation": "The need for robust co-simulation methodologies in the automotive sector due to the advancements and challenges of integration across hardware and software domains.", "method": "The approach involves automatically wrapping SystemC models with the Functional Mock-up Interface (FMI) to enable interoperability and secure integration in co-simulation workflows.", "result": "The proposed methodology has been validated through real-world case studies, demonstrating its effectiveness in handling complex designs for early validation and integration.", "conclusion": "The method combines the accuracy and quick deployment of SystemC with the benefits of FMI, addressing limitations in collaboration, scalability, and IP protection in co-simulation.", "key_contributions": ["Development of a method for wrapping SystemC models with FMI.", "Demonstration of the method's effectiveness through case studies.", "Enhancement of co-simulation capabilities in the automotive sector."], "limitations": "", "keywords": ["co-simulation", "SystemC", "FMI", "automotive", "integration"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2508.19667", "pdf": "https://arxiv.org/pdf/2508.19667.pdf", "abs": "https://arxiv.org/abs/2508.19667", "title": "Survey of Specialized Large Language Model", "authors": ["Chenghan Yang", "Ruiyu Zhao", "Yang Liu", "Ling Jiang"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 1 figures", "summary": "The rapid evolution of specialized large language models (LLMs) has\ntransitioned from simple domain adaptation to sophisticated native\narchitectures, marking a paradigm shift in AI development. This survey\nsystematically examines this progression across healthcare, finance, legal, and\ntechnical domains. Besides the wide use of specialized LLMs, technical\nbreakthrough such as the emergence of domain-native designs beyond fine-tuning,\ngrowing emphasis on parameter efficiency through sparse computation and\nquantization, increasing integration of multimodal capabilities and so on are\napplied to recent LLM agent. Our analysis reveals how these innovations address\nfundamental limitations of general-purpose LLMs in professional applications,\nwith specialized models consistently performance gains on domain-specific\nbenchmarks. The survey further highlights the implications for E-Commerce field\nto fill gaps in the field.", "AI": {"tldr": "This survey examines the evolution of specialized large language models (LLMs) from simple adaptations to advanced architectures across various domains such as healthcare and finance.", "motivation": "To understand the advancements in specialized LLMs and their impact on various professional sectors.", "method": "Systematic examination of the progression of specialized LLMs across multiple domains including healthcare, finance, and legal.", "result": "Innovation in LLMs leads to performance gains on domain-specific benchmarks, addressing limitations of general-purpose models.", "conclusion": "Specialized LLMs have significant implications for professional applications, particularly in filling gaps in E-Commerce.", "key_contributions": ["Review of the evolution from domain adaptation to native architectures in LLMs.", "Analysis of innovations like sparse computation and multimodal capabilities.", "Implications of specialized LLMs for practical applications in various fields."], "limitations": "", "keywords": ["large language models", "domain adaptation", "healthcare", "finance", "E-Commerce"], "importance_score": 9, "read_time_minutes": 9}}
{"id": "2508.19689", "pdf": "https://arxiv.org/pdf/2508.19689.pdf", "abs": "https://arxiv.org/abs/2508.19689", "title": "Building Task Bots with Self-learning for Enhanced Adaptability, Extensibility, and Factuality", "authors": ["Xiaoying Zhang"], "categories": ["cs.CL"], "comment": "179 pages", "summary": "Developing adaptable, extensible, and accurate task bots with minimal or zero\nhuman intervention is a significant challenge in dialog research. This thesis\nexamines the obstacles and potential solutions for creating such bots, focusing\non innovative techniques that enable bots to learn and adapt autonomously in\nconstantly changing environments.", "AI": {"tldr": "This thesis explores the development of autonomous task bots that can adapt and learn with minimal human intervention.", "motivation": "The need for adaptable and accurate task bots in dialog research that can function autonomously in dynamic environments.", "method": "The thesis examines various innovative techniques and approaches for creating autonomous bots capable of learning and adapting to changes.", "result": "Identifies key obstacles in the development process and suggests potential solutions for enhancing bot adaptability and autonomy.", "conclusion": "The research highlights the necessity of further advancements in techniques for developing effective autonomous task bots in dialog systems.", "key_contributions": ["Innovative techniques for bot learning and adaptation", "Identification of major obstacles in bot development", "Proposed solutions for enhancing bot autonomy"], "limitations": "", "keywords": ["task bots", "dialog research", "autonomy", "adaptability", "machine learning"], "importance_score": 6, "read_time_minutes": 180}}
{"id": "2508.19720", "pdf": "https://arxiv.org/pdf/2508.19720.pdf", "abs": "https://arxiv.org/abs/2508.19720", "title": "Continuously Steering LLMs Sensitivity to Contextual Knowledge with Proxy Models", "authors": ["Yilin Wang", "Heng Wang", "Yuyang Bai", "Minnan Luo"], "categories": ["cs.CL"], "comment": null, "summary": "In Large Language Models (LLMs) generation, there exist knowledge conflicts\nand scenarios where parametric knowledge contradicts knowledge provided in the\ncontext. Previous works studied tuning, decoding algorithms, or locating and\nediting context-aware neurons to adapt LLMs to be faithful to new contextual\nknowledge. However, they are usually inefficient or ineffective for large\nmodels, not workable for black-box models, or unable to continuously adjust\nLLMs' sensitivity to the knowledge provided in the context. To mitigate these\nproblems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a\nsimple framework that can steer LLMs' sensitivity to contextual knowledge\ncontinuously at a lightweight cost. Specifically, we tune two small LMs (i.e.\nproxy models) and use the difference in their output distributions to shift the\noriginal distribution of an LLM without modifying the LLM weights. In the\nevaluation process, we not only design synthetic data and fine-grained metrics\nto measure models' sensitivity to contextual knowledge but also use a real\nconflict dataset to validate CSKS's practical efficacy. Extensive experiments\ndemonstrate that our framework achieves continuous and precise control over\nLLMs' sensitivity to contextual knowledge, enabling both increased sensitivity\nand reduced sensitivity, thereby allowing LLMs to prioritize either contextual\nor parametric knowledge as needed flexibly. Our data and code are available at\nhttps://github.com/OliveJuiceLin/CSKS.", "AI": {"tldr": "A framework, CSKS, is proposed to continuously adjust LLMs' sensitivity to contextual knowledge without modifying their weights, using two small proxy models.", "motivation": "To address inefficiencies in existing methods for tuning LLMs' knowledge sensitivity, particularly for large and black-box models.", "method": "The framework tunes two small proxy language models and adjusts the output distributions to shift the original distribution of a large LLM.", "result": "CSKS achieves continuous and precise control over LLMs' sensitivity to contextual knowledge, successfully allowing adjustments to either increase or decrease sensitivity based on the requirements.", "conclusion": "CSKS enables flexible prioritization of contextual or parametric knowledge in LLMs, enhancing their application effectiveness.", "key_contributions": ["Introduction of the CSKS framework for LLMs sensitivity adjustment", "Utilization of proxy models for efficient knowledge tuning", "Development of metrics for measuring sensitivity to contextual knowledge"], "limitations": "", "keywords": ["Large Language Models", "Knowledge Sensitivity", "Contextual Knowledge", "Proxy Models", "Machine Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.19721", "pdf": "https://arxiv.org/pdf/2508.19721.pdf", "abs": "https://arxiv.org/abs/2508.19721", "title": "CAMÕES: A Comprehensive Automatic Speech Recognition Benchmark for European Portuguese", "authors": ["Carlos Carvalho", "Francisco Teixeira", "Catarina Botelho", "Anna Pompili", "Rubén Solera-Ureña", "Sérgio Paulo", "Mariana Julião", "Thomas Rolland", "John Mendonça", "Diogo Pereira", "Isabel Trancoso", "Alberto Abad"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to ASRU 2025", "summary": "Existing resources for Automatic Speech Recognition in Portuguese are mostly\nfocused on Brazilian Portuguese, leaving European Portuguese (EP) and other\nvarieties under-explored. To bridge this gap, we introduce CAM\\~OES, the first\nopen framework for EP and other Portuguese varieties. It consists of (1) a\ncomprehensive evaluation benchmark, including 46h of EP test data spanning\nmultiple domains; and (2) a collection of state-of-the-art models. For the\nlatter, we consider multiple foundation models, evaluating their zero-shot and\nfine-tuned performances, as well as E-Branchformer models trained from scratch.\nA curated set of 425h of EP was used for both fine-tuning and training. Our\nresults show comparable performance for EP between fine-tuned foundation models\nand the E-Branchformer. Furthermore, the best-performing models achieve\nrelative improvements above 35% WER, compared to the strongest zero-shot\nfoundation model, establishing a new state-of-the-art for EP and other\nvarieties.", "AI": {"tldr": "The paper introduces CAMÕES, an open framework aimed at improving Automatic Speech Recognition for European Portuguese, featuring a benchmark and state-of-the-art models.", "motivation": "To address the lack of resources for Automatic Speech Recognition in European Portuguese and other varieties compared to Brazilian Portuguese.", "method": "The framework includes a comprehensive evaluation benchmark with 46 hours of EP test data and a collection of models evaluated on their zero-shot and fine-tuned performances.", "result": "The results show that fine-tuned foundation models and E-Branchformer models yield performance improvements, achieving over 35% relative reduction in WER compared to existing models.", "conclusion": "This work establishes a new state-of-the-art for Automatic Speech Recognition in European Portuguese and contributes to bridging the resource gap for this language variety.", "key_contributions": ["First open framework for European Portuguese in ASR", "Comprehensive evaluation benchmark with diverse data", "State-of-the-art models achieving significant WER improvements"], "limitations": "", "keywords": ["Automatic Speech Recognition", "European Portuguese", "Foundation Models"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2508.19724", "pdf": "https://arxiv.org/pdf/2508.19724.pdf", "abs": "https://arxiv.org/abs/2508.19724", "title": "NLKI: A lightweight Natural Language Knowledge Integration Framework for Improving Small VLMs in Commonsense VQA Tasks", "authors": ["Aritra Dutta", "Swapnanil Mukherjee", "Deepanway Ghosal", "Somak Aditya"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Commonsense visual-question answering often hinges on knowledge that is\nmissing from the image or the question. Small vision-language models (sVLMs)\nsuch as ViLT, VisualBERT and FLAVA therefore lag behind their larger generative\ncounterparts. To study the effect of careful commonsense knowledge integration\non sVLMs, we present an end-to-end framework (NLKI) that (i) retrieves natural\nlanguage facts, (ii) prompts an LLM to craft natural language explanations, and\n(iii) feeds both signals to sVLMs respectively across two commonsense VQA\ndatasets (CRIC, AOKVQA) and a visual-entailment dataset (e-SNLI-VE). Facts\nretrieved using a fine-tuned ColBERTv2 and an object information-enriched\nprompt yield explanations that largely cut down hallucinations, while lifting\nthe end-to-end answer accuracy by up to 7% (across 3 datasets), making FLAVA\nand other models in NLKI match or exceed medium-sized VLMs such as Qwen-2 VL-2B\nand SmolVLM-2.5B. As these benchmarks contain 10-25% label noise, additional\nfinetuning using noise-robust losses (such as symmetric cross entropy and\ngeneralised cross entropy) adds another 2.5% in CRIC, and 5.5% in AOKVQA. Our\nfindings expose when LLM-based commonsense knowledge beats retrieval from\ncommonsense knowledge bases, how noise-aware training stabilises small models\nin the context of external knowledge augmentation, and why parameter-efficient\ncommonsense reasoning is now within reach for 250M models.", "AI": {"tldr": "This paper presents an end-to-end framework (NLKI) that enhances small vision-language models (sVLMs) in commonsense visual-question answering by integrating retrieval-based commonsense knowledge and LLM-generated explanations, resulting in improved accuracy and reduced hallucinations.", "motivation": "To improve the performance of small vision-language models (sVLMs) in commonsense visual-question answering, particularly when they lack necessary knowledge from images or questions.", "method": "The framework retrieves natural language facts using a fine-tuned ColBERTv2, prompts a LLM for explanations, and combines these signals to enhance the performance of sVLMs across multiple datasets.", "result": "The integration of retrieved facts and explanations improved end-to-end answer accuracy of sVLMs by up to 7% and reduced hallucinations significantly, matching or exceeding the performance of medium-sized vision-language models.", "conclusion": "The findings suggest that LLM-based commonsense knowledge is more effective than retrieval from knowledge bases and that noise-aware training methods improve the stability and accuracy of small models when augmented with external knowledge.", "key_contributions": ["Introduction of the NLKI framework to enhance sVLMs", "Demonstrated significant accuracy improvements across commonsense VQA datasets", "Showed the effectiveness of noise-robust training methods for small models"], "limitations": "", "keywords": ["Commonsense Visual-Question Answering", "Vision-Language Models", "Knowledge Integration"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.19740", "pdf": "https://arxiv.org/pdf/2508.19740.pdf", "abs": "https://arxiv.org/abs/2508.19740", "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval", "authors": ["Wenhao Li", "Yuxin Zhang", "Gen Luo", "Haiyuan Wan", "Ziyang Gong", "Fei Chao", "Rongrong Ji"], "categories": ["cs.CL"], "comment": null, "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.", "AI": {"tldr": "Spotlight Attention optimizes key-value cache retrieval in Large Language Models by using non-linear hashing methods.", "motivation": "To address the inefficiencies in existing key-value cache retrieval methods during LLM inference, specifically the issues caused by random linear hashing of important tokens.", "method": "Introduces a non-linear hashing approach to enhance the embedding distribution of queries and keys, along with a lightweight training framework using a Bradley-Terry ranking-based loss.", "result": "Spotlight Attention demonstrates a 5x reduction in hash code length and significantly improves retrieval precision, achieving hashing retrieval for 512K tokens in under 100μs on a single A100 GPU.", "conclusion": "The proposed method outperforms traditional linear hashing techniques, thereby demonstrating increased efficiency and robustness in Large Language Model decoding.", "key_contributions": ["Introduction of Spotlight Attention for key-value retrieval in LLMs", "Optimization of non-linear hashing for improved embedding distribution", "Development of specialized CUDA kernels for enhanced performance"], "limitations": "", "keywords": ["Large Language Models", "key-value cache", "non-linear hashing", "CUDA", "hashing retrieval"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.19758", "pdf": "https://arxiv.org/pdf/2508.19758.pdf", "abs": "https://arxiv.org/abs/2508.19758", "title": "Uncovering the Bigger Picture: Comprehensive Event Understanding Via Diverse News Retrieval", "authors": ["Yixuan Tang", "Yuanyuan Shi", "Yiqun Sun", "Anthony Kum Hoe Tung"], "categories": ["cs.CL", "cs.IR"], "comment": "Accepted by EMNLP 2025", "summary": "Access to diverse perspectives is essential for understanding real-world\nevents, yet most news retrieval systems prioritize textual relevance, leading\nto redundant results and limited viewpoint exposure. We propose NEWSCOPE, a\ntwo-stage framework for diverse news retrieval that enhances event coverage by\nexplicitly modeling semantic variation at the sentence level. The first stage\nretrieves topically relevant content using dense retrieval, while the second\nstage applies sentence-level clustering and diversity-aware re-ranking to\nsurface complementary information. To evaluate retrieval diversity, we\nintroduce three interpretable metrics, namely Average Pairwise Distance,\nPositive Cluster Coverage, and Information Density Ratio, and construct two\nparagraph-level benchmarks: LocalNews and DSGlobal. Experiments show that\nNEWSCOPE consistently outperforms strong baselines, achieving significantly\nhigher diversity without compromising relevance. Our results demonstrate the\neffectiveness of fine-grained, interpretable modeling in mitigating redundancy\nand promoting comprehensive event understanding. The data and code are\navailable at https://github.com/tangyixuan/NEWSCOPE.", "AI": {"tldr": "NEWSCOPE is a two-stage framework for diverse news retrieval that improves event coverage by modeling semantic variation at the sentence level, outperforming existing methods.", "motivation": "To address the redundancy in news retrieval systems that prioritize textual relevance, limiting exposure to diverse viewpoints.", "method": "The framework consists of a two-stage process: first retrieving relevant content using dense retrieval, and then applying sentence-level clustering and diversity-aware re-ranking.", "result": "NEWSCOPE significantly enhances news retrieval diversity and maintains relevance, demonstrated through benchmarks and metrics introduced for evaluation.", "conclusion": "The effectiveness of NEWSCOPE highlights the importance of fine-grained semantic modeling in promoting diverse event understanding in news retrieval.", "key_contributions": ["Introduction of NEWSCOPE framework for diverse news retrieval", "Development of three interpretable metrics for evaluating retrieval diversity", "Construction of two benchmarks (LocalNews and DSGlobal) for assessment."], "limitations": "", "keywords": ["news retrieval", "diversity", "semantic variation", "dense retrieval", "event coverage"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.19764", "pdf": "https://arxiv.org/pdf/2508.19764.pdf", "abs": "https://arxiv.org/abs/2508.19764", "title": "Principled Personas: Defining and Measuring the Intended Effects of Persona Prompting on Task Performance", "authors": ["Pedro Henrique Luz de Araujo", "Paul Röttger", "Dirk Hovy", "Benjamin Roth"], "categories": ["cs.CL"], "comment": "30 pages, 29 figures, accepted to EMNLP 2025", "summary": "Expert persona prompting -- assigning roles such as expert in math to\nlanguage models -- is widely used for task improvement. However, prior work\nshows mixed results on its effectiveness, and does not consider when and why\npersonas should improve performance. We analyze the literature on persona\nprompting for task improvement and distill three desiderata: 1) performance\nadvantage of expert personas, 2) robustness to irrelevant persona attributes,\nand 3) fidelity to persona attributes. We then evaluate 9 state-of-the-art LLMs\nacross 27 tasks with respect to these desiderata. We find that expert personas\nusually lead to positive or non-significant performance changes. Surprisingly,\nmodels are highly sensitive to irrelevant persona details, with performance\ndrops of almost 30 percentage points. In terms of fidelity, we find that while\nhigher education, specialization, and domain-relatedness can boost performance,\ntheir effects are often inconsistent or negligible across tasks. We propose\nmitigation strategies to improve robustness -- but find they only work for the\nlargest, most capable models. Our findings underscore the need for more careful\npersona design and for evaluation schemes that reflect the intended effects of\npersona usage.", "AI": {"tldr": "The paper analyzes the effectiveness of expert persona prompting in language models and identifies key factors influencing performance across multiple tasks.", "motivation": "To understand when and why expert persona prompting improves task performance in language models, addressing mixed results and emphasizing careful persona design.", "method": "The authors evaluate 9 state-of-the-art LLMs across 27 tasks, focusing on three key desiderata related to persona prompting effectiveness: performance advantage, robustness to irrelevant attributes, and fidelity to persona attributes.", "result": "Expert personas generally lead to positive or non-significant performance changes, but the models show sensitivity to irrelevant persona details, resulting in performance drops of almost 30 percentage points. Fidelity factors like higher education and specialization show inconsistent effects on performance.", "conclusion": "Careful design and evaluation of personas are essential, as mitigation strategies to enhance robustness only work for the largest models.", "key_contributions": ["Identification of three key desiderata for effective persona prompting", "Empirical evaluation of 9 LLMs across 27 tasks", "Proposed mitigation strategies for improving robustness of persona usage in LLMs"], "limitations": "Mitigation strategies have limited effectiveness and primarily benefit the largest models, suggesting a need for broader solutions.", "keywords": ["expert persona prompting", "language models", "task performance improvement", "HCI", "machine learning"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2508.19813", "pdf": "https://arxiv.org/pdf/2508.19813.pdf", "abs": "https://arxiv.org/abs/2508.19813", "title": "T2R-bench: A Benchmark for Generating Article-Level Reports from Real World Industrial Tables", "authors": ["Jie Zhang", "Changzai Pan", "Kaiwen Wei", "Sishi Xiong", "Yu Zhao", "Xiangyu Li", "Jiaxin Peng", "Xiaoyan Gu", "Jian Yang", "Wenhan Chang", "Zhenhe Wu", "Jiang Zhong", "Shuangyong Song", "Yongxiang Li", "Xuelong Li"], "categories": ["cs.CL"], "comment": null, "summary": "Extensive research has been conducted to explore the capabilities of large\nlanguage models (LLMs) in table reasoning. However, the essential task of\ntransforming tables information into reports remains a significant challenge\nfor industrial applications. This task is plagued by two critical issues: 1)\nthe complexity and diversity of tables lead to suboptimal reasoning outcomes;\nand 2) existing table benchmarks lack the capacity to adequately assess the\npractical application of this task. To fill this gap, we propose the\ntable-to-report task and construct a bilingual benchmark named T2R-bench, where\nthe key information flow from the tables to the reports for this task. The\nbenchmark comprises 457 industrial tables, all derived from real-world\nscenarios and encompassing 19 industry domains as well as 4 types of industrial\ntables. Furthermore, we propose an evaluation criteria to fairly measure the\nquality of report generation. The experiments on 25 widely-used LLMs reveal\nthat even state-of-the-art models like Deepseek-R1 only achieves performance\nwith 62.71 overall score, indicating that LLMs still have room for improvement\non T2R-bench. Source code and data will be available after acceptance.", "AI": {"tldr": "The paper introduces the table-to-report task and the T2R-bench benchmark for evaluating large language models in industrial applications.", "motivation": "To address the challenges of transforming table information into reports, which is critical for practical applications but currently underexplored.", "method": "Developing a bilingual benchmark called T2R-bench that includes 457 industrial tables from various domains and proposing evaluation criteria for report generation.", "result": "State-of-the-art LLMs showed suboptimal performance on the benchmark, with a maximum score of 62.71 achieved by Deepseek-R1, indicating significant room for improvement.", "conclusion": "The introduction of T2R-bench provides a necessary tool for better understanding and developing LLMs for the table-to-report task in industrial settings.", "key_contributions": ["Introduction of the table-to-report task", "Creation of the T2R-bench benchmark", "Formulation of evaluation criteria for report generation"], "limitations": "The benchmark currently includes 457 tables, which may not cover all possible scenarios in industry.", "keywords": ["large language models", "table reasoning", "report generation", "T2R-bench", "industrial applications"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.19828", "pdf": "https://arxiv.org/pdf/2508.19828.pdf", "abs": "https://arxiv.org/abs/2508.19828", "title": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning", "authors": ["Sikuan Yan", "Xiufeng Yang", "Zuchao Huang", "Ercong Nie", "Zifeng Ding", "Zonggen Li", "Xiaowen Ma", "Hinrich Schütze", "Volker Tresp", "Yunpu Ma"], "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na wide range of NLP tasks, but they remain fundamentally stateless, constrained\nby limited context windows that hinder long-horizon reasoning. Recent efforts\nto address this limitation often augment LLMs with an external memory bank, yet\nmost existing pipelines are static and heuristic-driven, lacking any learned\nmechanism for deciding what to store, update, or retrieve. We present\nMemory-R1, a reinforcement learning (RL) framework that equips LLMs with the\nability to actively manage and utilize external memory through two specialized\nagents: a Memory Manager that learns to perform structured memory operations\n{ADD, UPDATE, DELETE, NOOP}, and an Answer Agent that selects the most relevant\nentries and reasons over them to produce an answer. Both agents are fine-tuned\nwith outcome-driven RL (PPO and GRPO), enabling adaptive memory management and\nuse with minimal supervision. With as few as 152 question-answer pairs and a\ncorresponding temporal memory bank for training, Memory-R1 outperforms the most\ncompetitive existing baseline and demonstrates strong generalization across\ndiverse question types and LLM backbones. Beyond presenting an effective\napproach, this work provides insights into how RL can unlock more agentic,\nmemory-aware behaviors in LLMs, pointing toward richer, more persistent\nreasoning systems.", "AI": {"tldr": "Memory-R1 is a reinforcement learning framework that enhances Large Language Models with active external memory management, improving their long-horizon reasoning ability.", "motivation": "To address the limitations of stateless LLMs constrained by context windows and to introduce learned mechanisms for memory management.", "method": "A reinforcement learning framework utilizing two agents: a Memory Manager for structured memory operations and an Answer Agent for selecting relevant entries.", "result": "Memory-R1 outperforms existing competitive baselines and shows strong generalization across various question types with minimal training data.", "conclusion": "The study demonstrates how reinforcement learning can enhance LLMs' capabilities for memory management and reasoning, paving the way for more advanced reasoning systems.", "key_contributions": ["Introduces a reinforcement learning framework for active memory management in LLMs", "Demonstrates strong performance with minimal training data", "Provides insights into agentic behaviors in LLMs"], "limitations": "", "keywords": ["Large Language Models", "Reinforcement Learning", "Memory Management", "Natural Language Processing", "Cognitive Architectures"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.19831", "pdf": "https://arxiv.org/pdf/2508.19831.pdf", "abs": "https://arxiv.org/abs/2508.19831", "title": "Benchmarking Hindi LLMs: A New Suite of Datasets and a Comparative Analysis", "authors": ["Anusha Kamath", "Kanishk Singla", "Rakesh Paul", "Raviraj Joshi", "Utkarsh Vaidya", "Sanjay Singh Chauhan", "Niranjan Wartikar"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Evaluating instruction-tuned Large Language Models (LLMs) in Hindi is\nchallenging due to a lack of high-quality benchmarks, as direct translation of\nEnglish datasets fails to capture crucial linguistic and cultural nuances. To\naddress this, we introduce a suite of five Hindi LLM evaluation datasets:\nIFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, and BFCL-Hi. These were created\nusing a methodology that combines from-scratch human annotation with a\ntranslate-and-verify process. We leverage this suite to conduct an extensive\nbenchmarking of open-source LLMs supporting Hindi, providing a detailed\ncomparative analysis of their current capabilities. Our curation process also\nserves as a replicable methodology for developing benchmarks in other\nlow-resource languages.", "AI": {"tldr": "This paper introduces five evaluation datasets for instruction-tuned LLMs in Hindi, addressing the lack of high-quality benchmarks.", "motivation": "To evaluate instruction-tuned LLMs in Hindi, as existing English datasets do not adequately capture Hindi linguistic and cultural nuances.", "method": "The datasets were created using a combination of human annotation and a translate-and-verify process.", "result": "The paper benchmarks open-source LLMs supporting Hindi, providing a comparative analysis of their capabilities.", "conclusion": "The curation process can be used as a replicable methodology for developing benchmarks in other low-resource languages.", "key_contributions": ["Introduction of five Hindi LLM evaluation datasets", "Detailed comparative analysis of Hindi LLM capabilities", "Methodology for benchmark development in low-resource languages"], "limitations": "", "keywords": ["Hindi", "LLM evaluation", "benchmarking", "low-resource languages", "natural language processing"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2508.19836", "pdf": "https://arxiv.org/pdf/2508.19836.pdf", "abs": "https://arxiv.org/abs/2508.19836", "title": "Scalable and consistent few-shot classification of survey responses using text embeddings", "authors": ["Jonas Timmann Mjaaland", "Markus Fleten Kreutzer", "Halvor Tyseng", "Rebeckah K. Fussell", "Gina Passante", "N. G. Holmes", "Anders Malthe-Sørenssen", "Tor Ole B. Odden"], "categories": ["cs.CL", "physics.ed-ph"], "comment": null, "summary": "Qualitative analysis of open-ended survey responses is a commonly-used\nresearch method in the social sciences, but traditional coding approaches are\noften time-consuming and prone to inconsistency. Existing solutions from\nNatural Language Processing such as supervised classifiers, topic modeling\ntechniques, and generative large language models have limited applicability in\nqualitative analysis, since they demand extensive labeled data, disrupt\nestablished qualitative workflows, and/or yield variable results. In this\npaper, we introduce a text embedding-based classification framework that\nrequires only a handful of examples per category and fits well with standard\nqualitative workflows. When benchmarked against human analysis of a conceptual\nphysics survey consisting of 2899 open-ended responses, our framework achieves\na Cohen's Kappa ranging from 0.74 to 0.83 as compared to expert human coders in\nan exhaustive coding scheme. We further show how performance of this framework\nimproves with fine-tuning of the text embedding model, and how the method can\nbe used to audit previously-analyzed datasets. These findings demonstrate that\ntext embedding-assisted coding can flexibly scale to thousands of responses\nwithout sacrificing interpretability, opening avenues for deductive qualitative\nanalysis at scale.", "AI": {"tldr": "This paper presents a text embedding-based classification framework that enhances qualitative analysis of open-ended survey responses with limited labeled data, achieving high consistency with human coders.", "motivation": "Traditional coding methods for qualitative analysis are time-consuming and inconsistent, warranting the need for a more efficient approach.", "method": "The proposed framework utilizes text embeddings and only requires a few examples per category, integrating seamlessly into standard qualitative workflows.", "result": "The framework achieves a Cohen's Kappa of 0.74 to 0.83 when compared to human expert coders in analyzing 2899 conceptual physics survey responses.", "conclusion": "Text embedding-assisted coding can efficiently analyze large datasets while maintaining interpretability, allowing for scalable qualitative analysis.", "key_contributions": ["Introduction of a novel text embedding-based classification framework for qualitative analysis", "High consistency with human coders while using limited labeled data", "Ability to audit previously-analyzed datasets with improved performance upon fine-tuning"], "limitations": "", "keywords": ["Text Embedding", "Qualitative Analysis", "Natural Language Processing", "Survey Responses", "Human-Computer Interaction"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.19856", "pdf": "https://arxiv.org/pdf/2508.19856.pdf", "abs": "https://arxiv.org/abs/2508.19856", "title": "TokenVerse++: Towards Flexible Multitask Learning with Dynamic Task Activation", "authors": ["Shashi Kumar", "Srikanth Madikeri", "Esaú Villatoro-Tello", "Sergio Burdisso", "Pradeep Rangappa", "Andrés Carofilis", "Petr Motlicek", "Karthik Pandia", "Shankar Venkatesan", "Kadri Hacioğlu", "Andreas Stolcke"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to IEEE ASRU 2025. Copyright\\copyright 2025 IEEE", "summary": "Token-based multitasking frameworks like TokenVerse require all training\nutterances to have labels for all tasks, hindering their ability to leverage\npartially annotated datasets and scale effectively. We propose TokenVerse++,\nwhich introduces learnable vectors in the acoustic embedding space of the\nXLSR-Transducer ASR model for dynamic task activation. This core mechanism\nenables training with utterances labeled for only a subset of tasks, a key\nadvantage over TokenVerse. We demonstrate this by successfully integrating a\ndataset with partial labels, specifically for ASR and an additional task,\nlanguage identification, improving overall performance. TokenVerse++ achieves\nresults on par with or exceeding TokenVerse across multiple tasks, establishing\nit as a more practical multitask alternative without sacrificing ASR\nperformance.", "AI": {"tldr": "TokenVerse++ enables dynamic task activation in ASR models using learnable vectors, allowing for training with partially labeled datasets.", "motivation": "The motivation is to overcome the limitations of token-based multitasking frameworks like TokenVerse, which require full labeling for effective training.", "method": "TokenVerse++ integrates learnable vectors in the acoustic embedding space of the XLSR-Transducer ASR model to facilitate dynamic task activation.", "result": "The model effectively incorporates partially annotated datasets, improving performance for ASR and language identification tasks.", "conclusion": "TokenVerse++ demonstrates superior or comparable results to TokenVerse, offering a practical multitask solution without compromising ASR performance.", "key_contributions": ["Introduced learnable vectors for dynamic task activation in ASR", "Enabled training with partially annotated datasets", "Improved performance on ASR and language identification tasks"], "limitations": "", "keywords": ["multitasking", "ASR", "language identification", "partial labels", "XLSR-Transducer"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2508.19873", "pdf": "https://arxiv.org/pdf/2508.19873.pdf", "abs": "https://arxiv.org/abs/2508.19873", "title": "Beyond Shallow Heuristics: Leveraging Human Intuition for Curriculum Learning", "authors": ["Vanessa Toborek", "Sebastian Müller", "Tim Selbach", "Tamás Horváth", "Christian Bauckhage"], "categories": ["cs.CL"], "comment": "Presented at ICNLSP 2025; to appear in the ACL Anthology; received\n  the Best Short Paper Award", "summary": "Curriculum learning (CL) aims to improve training by presenting data from\n\"easy\" to \"hard\", yet defining and measuring linguistic difficulty remains an\nopen challenge. We investigate whether human-curated simple language can serve\nas an effective signal for CL. Using the article-level labels from the Simple\nWikipedia corpus, we compare label-based curricula to competence-based\nstrategies relying on shallow heuristics. Our experiments with a BERT-tiny\nmodel show that adding simple data alone yields no clear benefit. However,\nstructuring it via a curriculum -- especially when introduced first --\nconsistently improves perplexity, particularly on simple language. In contrast,\ncompetence-based curricula lead to no consistent gains over random ordering,\nprobably because they fail to effectively separate the two classes. Our results\nsuggest that human intuition about linguistic difficulty can guide CL for\nlanguage model pre-training.", "AI": {"tldr": "This paper investigates the use of human-curated simple language for curriculum learning in training language models.", "motivation": "Defining and measuring linguistic difficulty for curriculum learning remains challenging.", "method": "The paper compares label-based curricula derived from the Simple Wikipedia corpus against competence-based strategies using shallow heuristics in training a BERT-tiny model.", "result": "The study found that while simple data alone did not provide clear benefits, incorporating it in a curriculum improved perplexity, especially when used first. Competence-based approaches showed no consistent advantages.", "conclusion": "Human intuition regarding linguistic difficulty can effectively inform curriculum learning strategies for pre-training language models.", "key_contributions": ["Introduction of human-curated simple language as a curriculum learning signal.", "Demonstration of the effectiveness of structured training versus random ordering.", "Findings on the limitations of competence-based curricula."], "limitations": "The results indicate that simple data alone may not suffice without proper structuring in the curriculum.", "keywords": ["Curriculum learning", "Linguistic difficulty", "Simple language", "BERT", "Pre-training"], "importance_score": 7, "read_time_minutes": 6}}
{"id": "2508.19883", "pdf": "https://arxiv.org/pdf/2508.19883.pdf", "abs": "https://arxiv.org/abs/2508.19883", "title": "AI-Powered Detection of Inappropriate Language in Medical School Curricula", "authors": ["Chiman Salavati", "Shannon Song", "Scott A. Hale", "Roberto E. Montenegro", "Shiri Dori-Hacohen", "Fabricio Murai"], "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.1; I.2.7"], "comment": "Accepted at 2025 AAAI/ACM AI, Ethics and Society Conference (AIES'25)", "summary": "The use of inappropriate language -- such as outdated, exclusionary, or\nnon-patient-centered terms -- medical instructional materials can significantly\ninfluence clinical training, patient interactions, and health outcomes. Despite\ntheir reputability, many materials developed over past decades contain examples\nnow considered inappropriate by current medical standards. Given the volume of\ncurricular content, manually identifying instances of inappropriate use of\nlanguage (IUL) and its subcategories for systematic review is prohibitively\ncostly and impractical. To address this challenge, we conduct a first-in-class\nevaluation of small language models (SLMs) fine-tuned on labeled data and\npre-trained LLMs with in-context learning on a dataset containing approximately\n500 documents and over 12,000 pages. For SLMs, we consider: (1) a general IUL\nclassifier, (2) subcategory-specific binary classifiers, (3) a multilabel\nclassifier, and (4) a two-stage hierarchical pipeline for general IUL detection\nfollowed by multilabel classification. For LLMs, we consider variations of\nprompts that include subcategory definitions and/or shots. We found that both\nLLama-3 8B and 70B, even with carefully curated shots, are largely outperformed\nby SLMs. While the multilabel classifier performs best on annotated data,\nsupplementing training with unflagged excerpts as negative examples boosts the\nspecific classifiers' AUC by up to 25%, making them most effective models for\nmitigating harmful language in medical curricula.", "AI": {"tldr": "Evaluation of small language models and pre-trained LLMs for detecting inappropriate use of language in medical instructional materials.", "motivation": "Inappropriate language in medical instructional materials affects clinical training and health outcomes; existing materials often contain outdated terms.", "method": "Fine-tuned small language models and pre-trained LLMs were tested on a dataset of 500 documents for IUL detection, employing classifiers and hierarchical pipelines for analysis.", "result": "The multilabel classifier outperformed LLMs; combining training with unflagged excerpts increased the AUC of classifiers significantly.", "conclusion": "Small language models are more effective than large language models for identifying inappropriate language in medical curricula, emphasizing the need for better training methods.", "key_contributions": ["Developed a multilabel classifier for inappropriate language detection.", "Showed that small language models outperform pre-trained LLMs for this task.", "Introduced a training method that improves classification accuracy using unflagged text."], "limitations": "No mention of scalability or applicability to non-medical texts.", "keywords": ["inappropriate language", "medical education", "language models", "machine learning", "AI ethics"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.19887", "pdf": "https://arxiv.org/pdf/2508.19887.pdf", "abs": "https://arxiv.org/abs/2508.19887", "title": "Bangla-Bayanno: A 52K-Pair Bengali Visual Question Answering Dataset with LLM-Assisted Translation Refinement", "authors": ["Mohammed Rakibul Hasan", "Rafi Majid", "Ahanaf Tahmid"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "In this paper, we introduce Bangla-Bayanno, an open-ended Visual Question\nAnswering (VQA) Dataset in Bangla, a widely used, low-resource language in\nmultimodal AI research. The majority of existing datasets are either manually\nannotated with an emphasis on a specific domain, query type, or answer type or\nare constrained by niche answer formats. In order to mitigate human-induced\nerrors and guarantee lucidity, we implemented a multilingual LLM-assisted\ntranslation refinement pipeline. This dataset overcomes the issues of\nlow-quality translations from multilingual sources. The dataset comprises\n52,650 question-answer pairs across 4750+ images. Questions are classified into\nthree distinct answer types: nominal (short descriptive), quantitative\n(numeric), and polar (yes/no). Bangla-Bayanno provides the most comprehensive\nopen-source, high-quality VQA benchmark in Bangla, aiming to advance research\nin low-resource multimodal learning and facilitate the development of more\ninclusive AI systems.", "AI": {"tldr": "Bangla-Bayanno is a comprehensive Visual Question Answering dataset in Bangla, designed to improve multimodal AI research in low-resource languages.", "motivation": "To address the scarcity of high-quality VQA datasets in low-resource languages like Bangla and to ensure the clarity and reliability of translations in multimodal AI.", "method": "A multilingual LLM-assisted translation refinement pipeline was employed to create a dataset containing 52,650 question-answer pairs across 4750+ images, with questions classified into nominal, quantitative, and polar answer types.", "result": "The dataset aims to mitigate human-induced errors and is expected to advance research in low-resource multimodal learning by providing a high-quality VQA benchmark in Bangla.", "conclusion": "Bangla-Bayanno is positioned to facilitate the development of more inclusive AI systems and strengthen VQA research in Bangla.", "key_contributions": ["Creation of the Bangla-Bayanno VQA dataset", "Implementation of a multilingual LLM-assisted translation refinement pipeline", "Contribution to multimodal AI research in low-resource settings"], "limitations": "", "keywords": ["Bangla", "Visual Question Answering", "Multimodal AI", "Low-resource languages", "Dataset"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.19903", "pdf": "https://arxiv.org/pdf/2508.19903.pdf", "abs": "https://arxiv.org/abs/2508.19903", "title": "Logical Reasoning with Outcome Reward Models for Test-Time Scaling", "authors": ["Ramya Keerthy Thatikonda", "Wray Buntine", "Ehsan Shareghi"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025", "summary": "Logical reasoning is a critical benchmark for evaluating the capabilities of\nlarge language models (LLMs), as it reflects their ability to derive valid\nconclusions from given premises. While the combination of test-time scaling\nwith dedicated outcome or process reward models has opened up new avenues to\nenhance LLMs performance in complex reasoning tasks, this space is\nunder-explored in deductive logical reasoning. We present a set of Outcome\nReward Models (ORMs) for deductive reasoning. To train the ORMs we mainly\ngenerate data using Chain-of-Thought (CoT) with single and multiple samples.\nAdditionally, we propose a novel tactic to further expand the type of errors\ncovered in the training dataset of the ORM. In particular, we propose an echo\ngeneration technique that leverages LLMs' tendency to reflect incorrect\nassumptions made in prompts to extract additional training data, covering\npreviously unexplored error types. While a standard CoT chain may contain\nerrors likely to be made by the reasoner, the echo strategy deliberately steers\nthe model toward incorrect reasoning. We show that ORMs trained on CoT and\necho-augmented data demonstrate improved performance on the FOLIO, JustLogic,\nand ProverQA datasets across four different LLMs.", "AI": {"tldr": "The paper introduces Outcome Reward Models (ORMs) for enhancing deductive logical reasoning in large language models using Chain-of-Thought and an echo generation technique to expand training datasets.", "motivation": "To improve the performance of large language models in deductive logical reasoning tasks by developing specialized reward models and enhancing dataset diversity.", "method": "The authors developed Outcome Reward Models (ORMs) trained on data generated with Chain-of-Thought (CoT) and an echo generation technique that reflects incorrect assumptions to produce varied error data.", "result": "ORMs trained with both CoT data and echo-augmented data showed improved performance on deductive reasoning tasks, specifically on the FOLIO, JustLogic, and ProverQA datasets, across various LLMs.", "conclusion": "The proposed methods successfully enhance the logical reasoning capabilities of large language models by introducing a richer training dataset and specialized reward models.", "key_contributions": ["Development of Outcome Reward Models for deductive reasoning", "Introduction of echo generation technique for training data augmentation", "Demonstrated improved performance across multiple reasoning datasets"], "limitations": "", "keywords": ["Deductive reasoning", "Large language models", "Outcome Reward Models", "Chain-of-Thought", "Echo generation"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2508.19919", "pdf": "https://arxiv.org/pdf/2508.19919.pdf", "abs": "https://arxiv.org/abs/2508.19919", "title": "Your AI Bosses Are Still Prejudiced: The Emergence of Stereotypes in LLM-Based Multi-Agent Systems", "authors": ["Jingyu Guo", "Yingying Xu"], "categories": ["cs.CL"], "comment": null, "summary": "While stereotypes are well-documented in human social interactions, AI\nsystems are often presumed to be less susceptible to such biases. Previous\nstudies have focused on biases inherited from training data, but whether\nstereotypes can emerge spontaneously in AI agent interactions merits further\nexploration. Through a novel experimental framework simulating workplace\ninteractions with neutral initial conditions, we investigate the emergence and\nevolution of stereotypes in LLM-based multi-agent systems. Our findings reveal\nthat (1) LLM-Based AI agents develop stereotype-driven biases in their\ninteractions despite beginning without predefined biases; (2) stereotype\neffects intensify with increased interaction rounds and decision-making power,\nparticularly after introducing hierarchical structures; (3) these systems\nexhibit group effects analogous to human social behavior, including halo\neffects, confirmation bias, and role congruity; and (4) these stereotype\npatterns manifest consistently across different LLM architectures. Through\ncomprehensive quantitative analysis, these findings suggest that stereotype\nformation in AI systems may arise as an emergent property of multi-agent\ninteractions, rather than merely from training data biases. Our work\nunderscores the need for future research to explore the underlying mechanisms\nof this phenomenon and develop strategies to mitigate its ethical impacts.", "AI": {"tldr": "This paper explores the emergence of stereotypes in LLM-based multi-agent systems through simulated workplace interactions, finding that biases develop spontaneously despite neutral initial conditions.", "motivation": "To investigate whether AI systems, particularly LLM-based agents, can develop stereotypes during interactions despite not being trained with biased data.", "method": "A novel experimental framework simulating workplace interactions among LLM-based AI agents with neutral initial conditions was used to study stereotype formation and evolution.", "result": "LLM-based AI agents developed stereotype-driven biases during interactions, which intensified with increased interaction rounds and hierarchical structures, demonstrating group effects similar to human social behavior.", "conclusion": "Stereotype formation in AI systems may emerge from multi-agent interactions rather than just training data biases, highlighting the need for further research on mitigating ethical impacts.", "key_contributions": ["Demonstrated spontaneous stereotype development in LLM-based AI agents during neutral interactions.", "Identified the intensification of bias with increased interactions and hierarchical structures.", "Showed the presence of human-like group effects in AI agent interactions."], "limitations": "The study focuses on simulated environments and may not fully capture real-world complexities.", "keywords": ["Stereotypes", "LLM", "Multi-agent systems", "Bias", "Human-AI interaction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.19922", "pdf": "https://arxiv.org/pdf/2508.19922.pdf", "abs": "https://arxiv.org/abs/2508.19922", "title": "HEAL: A Hypothesis-Based Preference-Aware Analysis Framework", "authors": ["Yifu Huo", "Chenglong Wang", "Qiren Zhu", "Shunjie Xing", "Tong Xiao", "Chunliang Zhang", "Tongran Liu", "Jinbo Zhu"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025 Findings", "summary": "Preference optimization methods like DPO have achieved remarkable performance\nin LLM alignment. However, the evaluation for these methods relies on a single\nresponse and overlooks other potential outputs, which could also be generated\nin real-world applications within this hypothetical space. To address this\nissue, this paper presents a \\textbf{H}ypothesis-based\nPr\\textbf{E}ference-aware \\textbf{A}na\\textbf{L}ysis Framework (HEAL), a novel\nevaluation paradigm that formulates preference alignment as a re-ranking\nprocess within hypothesis spaces. The framework incorporates two complementary\nmetrics: ranking accuracy for evaluating ordinal consistency and preference\nstrength correlation for assessing continuous alignment. To facilitate this\nframework, we develop UniHypoBench, a unified hypothesis benchmark constructed\nfrom diverse instruction-response pairs. Through extensive experiments based on\nHEAL, with a particular focus on the intrinsic mechanisms of preference\nlearning, we demonstrate that current preference learning methods can\neffectively capture preferences provided by proxy models while simultaneously\nsuppressing negative samples. These findings contribute to preference learning\nresearch through two significant avenues. Theoretically, we introduce\nhypothesis space analysis as an innovative paradigm for understanding\npreference alignment. Practically, HEAL offers researchers robust diagnostic\ntools for refining preference optimization methods, while our empirical results\nidentify promising directions for developing more advanced alignment algorithms\ncapable of comprehensive preference capture.", "AI": {"tldr": "This paper presents HEAL, a new evaluation framework for LLM preference optimization that improves alignment by analyzing hypothesis spaces.", "motivation": "To address the limitations of existing preference optimization methods that rely on single responses and overlook other potential outputs.", "method": "The paper develops the HEAL framework, which formulates preference alignment as a re-ranking process within hypothesis spaces and uses two metrics: ranking accuracy and preference strength correlation.", "result": "Extensive experiments show that current preference learning methods capture preferences from proxy models effectively while suppressing negative samples.", "conclusion": "HEAL provides a new understanding of preference alignment and offers diagnostic tools for researchers to improve preference optimization methods.", "key_contributions": ["Introduces the HEAL framework for analyzing preference alignment in hypothesis spaces.", "Develops the UniHypoBench benchmark for evaluating hypothesis-based preference methods.", "Demonstrates the effectiveness of current preference learning methods in capturing and refining responses."], "limitations": "", "keywords": ["Preference optimization", "Hypothesis analysis", "LLM alignment"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.19966", "pdf": "https://arxiv.org/pdf/2508.19966.pdf", "abs": "https://arxiv.org/abs/2508.19966", "title": "Dhati+: Fine-tuned Large Language Models for Arabic Subjectivity Evaluation", "authors": ["Slimane Bellaouar", "Attia Nehar", "Soumia Souffi", "Mounia Bouameur"], "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 7 figures", "summary": "Despite its significance, Arabic, a linguistically rich and morphologically\ncomplex language, faces the challenge of being under-resourced. The scarcity of\nlarge annotated datasets hampers the development of accurate tools for\nsubjectivity analysis in Arabic. Recent advances in deep learning and\nTransformers have proven highly effective for text classification in English\nand French. This paper proposes a new approach for subjectivity assessment in\nArabic textual data. To address the dearth of specialized annotated datasets,\nwe developed a comprehensive dataset, AraDhati+, by leveraging existing Arabic\ndatasets and collections (ASTD, LABR, HARD, and SANAD). Subsequently, we\nfine-tuned state-of-the-art Arabic language models (XLM-RoBERTa, AraBERT, and\nArabianGPT) on AraDhati+ for effective subjectivity classification.\nFurthermore, we experimented with an ensemble decision approach to harness the\nstrengths of individual models. Our approach achieves a remarkable accuracy of\n97.79\\,\\% for Arabic subjectivity classification. Results demonstrate the\neffectiveness of the proposed approach in addressing the challenges posed by\nlimited resources in Arabic language processing.", "AI": {"tldr": "This paper introduces a new approach for subjectivity assessment in Arabic textual data, overcoming data scarcity with a novel dataset and fine-tuned models.", "motivation": "The paper addresses the significant challenge of under-resourcing in Arabic language processing, specifically for subjectivity analysis.", "method": "The authors developed a new dataset, AraDhati+, by combining existing Arabic datasets and fine-tuning several state-of-the-art Arabic language models including XLM-RoBERTa, AraBERT, and ArabianGPT.", "result": "The methodology yielded an impressive accuracy of 97.79% in Arabic subjectivity classification.", "conclusion": "The proposed approach successfully demonstrates its effectiveness in tackling the limitations of existing resources in Arabic language processing.", "key_contributions": ["Creation of the AraDhati+ dataset for Arabic subjectivity analysis", "Fine-tuning of state-of-the-art Arabic language models on the new dataset", "Introduction of an ensemble decision approach for improved classification accuracy"], "limitations": "", "keywords": ["Arabic", "Subjectivity Analysis", "Deep Learning", "Natural Language Processing", "Model Fine-Tuning"], "importance_score": 6, "read_time_minutes": 25}}
{"id": "2508.19982", "pdf": "https://arxiv.org/pdf/2508.19982.pdf", "abs": "https://arxiv.org/abs/2508.19982", "title": "Diffusion Language Models Know the Answer Before Decoding", "authors": ["Pengxiang Li", "Yefan Zhou", "Dilxat Muhtar", "Lu Yin", "Shilin Yan", "Li Shen", "Yi Liang", "Soroush Vosoughi", "Shiwei Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Diffusion language models (DLMs) have recently emerged as an alternative to\nautoregressive approaches, offering parallel sequence generation and flexible\ntoken orders. However, their inference remains slower than that of\nautoregressive models, primarily due to the cost of bidirectional attention and\nthe large number of refinement steps required for high quality outputs. In this\nwork, we highlight and leverage an overlooked property of DLMs early answer\nconvergence: in many cases, the correct answer can be internally identified by\nhalf steps before the final decoding step, both under semi-autoregressive and\nrandom remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99%\nof instances, respectively, can be decoded correctly using only half of the\nrefinement steps. Building on this observation, we introduce Prophet, a\ntraining-free fast decoding paradigm that enables early commit decoding.\nSpecifically, Prophet dynamically decides whether to continue refinement or to\ngo \"all-in\" (i.e., decode all remaining tokens in one step), using the\nconfidence gap between the top-2 prediction candidates as the criterion. It\nintegrates seamlessly into existing DLM implementations, incurs negligible\noverhead, and requires no additional training. Empirical evaluations of\nLLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the\nnumber of decoding steps by up to 3.4x while preserving high generation\nquality. These results recast DLM decoding as a problem of when to stop\nsampling, and demonstrate that early decode convergence provides a simple yet\npowerful mechanism for accelerating DLM inference, complementary to existing\nspeedup techniques. Our code is publicly available at\nhttps://github.com/pixeli99/Prophet.", "AI": {"tldr": "This paper introduces Prophet, a fast decoding paradigm for diffusion language models (DLMs) that leverages early answer convergence to reduce inference time while maintaining high output quality.", "motivation": "DLMs offer benefits over autoregressive models but have slower inference due to bidirectional attention and high refinement steps. The paper seeks to optimize DLM inference speed.", "method": "Prophet uses early answer convergence to determine when to stop refining outputs, deciding between continuing the process or decoding all remaining tokens at once, based on confidence levels of top predictions.", "result": "Empirical evaluations show that Prophet can reduce decoding steps by up to 3.4x while maintaining high generation quality across multiple tasks with DLMs like LLaDA-8B and Dream-7B.", "conclusion": "The integration of Prophet into existing DLMs significantly accelerates inference times while preserving output integrity, offering a new perspective on DLM decoding processes.", "key_contributions": ["Introduction of the Prophet quick decoding method", "Demonstration of early answer convergence for DLMs", "Empirical validation of speed and quality enhancements on benchmark datasets"], "limitations": "", "keywords": ["Diffusion Language Models", "Fast Decoding", "Early Convergence", "Natural Language Processing", "Machine Learning"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2508.19988", "pdf": "https://arxiv.org/pdf/2508.19988.pdf", "abs": "https://arxiv.org/abs/2508.19988", "title": "AgentCoMa: A Compositional Benchmark Mixing Commonsense and Mathematical Reasoning in Real-World Scenarios", "authors": ["Lisa Alazraki", "Lihu Chen", "Ana Brassard", "Joe Stacey", "Hossein A. Rahmani", "Marek Rei"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved high accuracy on complex\ncommonsense and mathematical problems that involve the composition of multiple\nreasoning steps. However, current compositional benchmarks testing these skills\ntend to focus on either commonsense or math reasoning, whereas LLM agents\nsolving real-world tasks would require a combination of both. In this work, we\nintroduce an Agentic Commonsense and Math benchmark (AgentCoMa), where each\ncompositional task requires a commonsense reasoning step and a math reasoning\nstep. We test it on 61 LLMs of different sizes, model families, and training\nstrategies. We find that LLMs can usually solve both steps in isolation, yet\ntheir accuracy drops by ~30% on average when the two are combined. This is a\nsubstantially greater performance gap than the one we observe in prior\ncompositional benchmarks that combine multiple steps of the same reasoning\ntype. In contrast, non-expert human annotators can solve the compositional\nquestions and the individual steps in AgentCoMa with similarly high accuracy.\nFurthermore, we conduct a series of interpretability studies to better\nunderstand the performance gap, examining neuron patterns, attention maps and\nmembership inference. Our work underscores a substantial degree of model\nbrittleness in the context of mixed-type compositional reasoning and offers a\ntest bed for future improvement.", "AI": {"tldr": "The paper introduces the Agentic Commonsense and Math benchmark (AgentCoMa) to evaluate LLMs on tasks requiring both commonsense and math reasoning, revealing a significant performance gap when these skills are combined.", "motivation": "Current benchmarks primarily focus on either commonsense or math reasoning, but real-world tasks often require a combination of both skills.", "method": "We tested 61 LLMs of varying sizes and training strategies on AgentCoMa, which consists of tasks requiring both commonsense and math reasoning.", "result": "LLMs perform well on individual reasoning types but show a ~30% drop in accuracy when tasks require combined reasoning; humans maintain high accuracy for both types.", "conclusion": "The findings highlight model brittleness in mixed-type reasoning and provide a framework for further advancements in LLM capabilities.", "key_contributions": ["Introduction of the AgentCoMa benchmark for mixed-type reasoning", "Empirical evaluation across 61 LLMs with performance analysis", "Interpretability studies revealing performance gaps with neuron patterns and attention maps."], "limitations": "The study focuses on LLMs and does not explore other AI or model types; further research needed to enhance model robustness in mixed-type reasoning.", "keywords": ["Large Language Models", "Commonsense reasoning", "Math reasoning", "Benchmarking", "Interpretability"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.19993", "pdf": "https://arxiv.org/pdf/2508.19993.pdf", "abs": "https://arxiv.org/abs/2508.19993", "title": "MathBuddy: A Multimodal System for Affective Math Tutoring", "authors": ["Debanjana Kar", "Leopold Böss", "Dacia Braca", "Sebastian Maximilian Dennerlein", "Nina Christine Hubig", "Philipp Wintersberger", "Yufang Hou"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "The rapid adoption of LLM-based conversational systems is already\ntransforming the landscape of educational technology. However, the current\nstate-of-the-art learning models do not take into account the student's\naffective states. Multiple studies in educational psychology support the claim\nthat positive or negative emotional states can impact a student's learning\ncapabilities. To bridge this gap, we present MathBuddy, an emotionally aware\nLLM-powered Math Tutor, which dynamically models the student's emotions and\nmaps them to relevant pedagogical strategies, making the tutor-student\nconversation a more empathetic one. The student's emotions are captured from\nthe conversational text as well as from their facial expressions. The student's\nemotions are aggregated from both modalities to confidently prompt our LLM\nTutor for an emotionally-aware response. We have effectively evaluated our\nmodel using automatic evaluation metrics across eight pedagogical dimensions\nand user studies. We report a massive 23 point performance gain using the win\nrate and a 3 point gain at an overall level using DAMR scores which strongly\nsupports our hypothesis of improving LLM-based tutor's pedagogical abilities by\nmodeling students' emotions.", "AI": {"tldr": "MathBuddy is an emotionally aware LLM-powered Math Tutor that adapts to students' emotions to improve learning effectiveness.", "motivation": "The adoption of LLM-based conversational systems in education lacks consideration for students' emotional states, which can significantly impact learning outcomes.", "method": "MathBuddy captures students' emotions from conversational text and facial expressions, utilizing both to provide empathetic responses and enhance the tutoring experience.", "result": "The model was evaluated across eight pedagogical dimensions, demonstrating a 23 point performance gain in win rates and a 3 point gain in overall DAMR scores.", "conclusion": "Modeling students' emotional states can significantly improve the pedagogical effectiveness of LLM-based tutoring systems.", "key_contributions": ["Introduction of an emotionally aware tutor that uses LLM technology.", "Integration of emotional state modeling from text and facial expressions.", "Empirical validation showing significant performance improvements in tutoring outcomes."], "limitations": "", "keywords": ["LLM", "emotional awareness", "education", "Math Tutor", "human-computer interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.19996", "pdf": "https://arxiv.org/pdf/2508.19996.pdf", "abs": "https://arxiv.org/abs/2508.19996", "title": "ReSURE: Regularizing Supervision Unreliability for Multi-turn Dialogue Fine-tuning", "authors": ["Yiming Du", "Yifan Xiang", "Bin Liang", "Dahua Lin", "Kam-Fai Wong", "Fei Tan"], "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning multi-turn dialogue systems requires high-quality supervision but\noften suffers from degraded performance when exposed to low-quality data.\nSupervision errors in early turns can propagate across subsequent turns,\nundermining coherence and response quality. Existing methods typically address\ndata quality via static prefiltering, which decouples quality control from\ntraining and fails to mitigate turn-level error propagation. In this context,\nwe propose ReSURE (Regularizing Supervision UnREliability), an adaptive\nlearning method that dynamically down-weights unreliable supervision without\nexplicit filtering. ReSURE estimates per-turn loss distributions using\nWelford's online statistics and reweights sample losses on the fly accordingly.\nExperiments on both single-source and mixed-quality datasets show improved\nstability and response quality. Notably, ReSURE enjoys positive Spearman\ncorrelations (0.21 ~ 1.0 across multiple benchmarks) between response scores\nand number of samples regardless of data quality, which potentially paves the\nway for utilizing large-scale data effectively. Code is publicly available at\nhttps://github.com/Elvin-Yiming-Du/ReSURE_Multi_Turn_Training.", "AI": {"tldr": "Proposes ReSURE, an adaptive learning method to mitigate supervision errors in multi-turn dialogue systems by dynamically down-weighting unreliable supervision.", "motivation": "High-quality supervision is crucial for fine-tuning multi-turn dialogue systems, but low-quality data can degrade performance and propagate errors across turns.", "method": "ReSURE uses Welford's online statistics to estimate per-turn loss distributions and dynamically reweight sample losses during training, avoiding explicit data filtering.", "result": "Experiments show that ReSURE enhances stability and response quality on single-source and mixed-quality datasets, achieving significant positive correlations between response scores and sample counts regardless of data quality.", "conclusion": "ReSURE enables more effective use of large-scale data in training multi-turn dialogue systems without requiring prior filtering of data quality.", "key_contributions": ["Introduces an adaptive learning approach that down-weights supervision errors during training.", "Establishes a method for dynamic loss reweighting based on per-turn statistics.", "Demonstrates improved response quality and stability across various datasets."], "limitations": "", "keywords": ["multi-turn dialogue systems", "supervision errors", "adaptive learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.19997", "pdf": "https://arxiv.org/pdf/2508.19997.pdf", "abs": "https://arxiv.org/abs/2508.19997", "title": "Selective Retrieval-Augmentation for Long-Tail Legal Text Classification", "authors": ["Boheng Mao"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Legal text classification is a fundamental NLP task in the legal domain.\nBenchmark datasets in this area often exhibit a long-tail label distribution,\nwhere many labels are underrepresented, leading to poor model performance on\nrare classes. This paper proposes Selective Retrieval-Augmentation (SRA) as a\nsolution to this problem. SRA focuses on augmenting samples belonging to\nlow-frequency labels in the training set, preventing the introduction of noise\nfor well-represented classes, and requires no changes to the model\narchitecture. Retrieval is performed only from the training data to ensure\nthere is no potential information leakage, removing the need for external\ncorpora simultaneously. The proposed SRA method is tested on two legal text\nclassification benchmark datasets with long-tail distributions: LEDGAR\n(single-label) and UNFAIR-ToS (multi-label). The results indicate that SRA\nattains higher micro-F1 and macro-F1 scores compared to all current LexGLUE\nbaselines across both datasets, illustrating consistent improvements in\nlong-tail legal text classification. The code repository is available at:\nhttps://github.com/Boheng-Mao/sra-legal", "AI": {"tldr": "This paper presents Selective Retrieval-Augmentation (SRA) to improve legal text classification performance on underrepresented label classes.", "motivation": "The long-tail label distribution in benchmark legal datasets leads to poor model performance on rare classes in legal text classification.", "method": "Selective Retrieval-Augmentation (SRA) enhances training samples of low-frequency labels by retrieving from the training set only, avoiding information leakage and noise introduction for well-represented classes.", "result": "SRA outperforms existing LexGLUE baselines, achieving higher micro-F1 and macro-F1 scores on LEDGAR and UNFAIR-ToS datasets.", "conclusion": "SRA is an effective method for improving the performance of legal text classification on long-tail distributions without needing model architecture changes.", "key_contributions": ["Introduction of Selective Retrieval-Augmentation (SRA) for legal text classification.", "Demonstration of SRA's effectiveness on benchmark datasets with long-tail distributions.", "Provision of a code repository for practical implementation."], "limitations": "", "keywords": ["Legal Text Classification", "NLP", "Long-Tail Distribution", "Retrieval-Augmentation", "Benchmark Datasets"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2508.20033", "pdf": "https://arxiv.org/pdf/2508.20033.pdf", "abs": "https://arxiv.org/abs/2508.20033", "title": "DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis", "authors": ["Liana Patel", "Negar Arabzadeh", "Harshit Gupta", "Ankita Sundar", "Ion Stoica", "Matei Zaharia", "Carlos Guestrin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The ability to research and synthesize knowledge is central to human\nexpertise and progress. An emerging class of systems promises these exciting\ncapabilities through generative research synthesis, performing retrieval over\nthe live web and synthesizing discovered sources into long-form, cited\nsummaries. However, evaluating such systems remains an open challenge: existing\nquestion-answering benchmarks focus on short-form factual responses, while\nexpert-curated datasets risk staleness and data contamination. Both fail to\ncapture the complexity and evolving nature of real research synthesis tasks. In\nthis work, we introduce DeepScholar-bench, a live benchmark and holistic,\nautomated evaluation framework designed to evaluate generative research\nsynthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv\npapers and focuses on a real research synthesis task: generating the related\nwork sections of a paper by retrieving, synthesizing, and citing prior\nresearch. Our evaluation framework holistically assesses performance across\nthree key dimensions, knowledge synthesis, retrieval quality, and\nverifiability. We also develop DeepScholar-base, a reference pipeline\nimplemented efficiently using the LOTUS API. Using the DeepScholar-bench\nframework, we perform a systematic evaluation of prior open-source systems,\nsearch AI's, OpenAI's DeepResearch, and DeepScholar-base. We find that\nDeepScholar-base establishes a strong baseline, attaining competitive or higher\nperformance than each other method. We also find that DeepScholar-bench remains\nfar from saturated, with no system exceeding a score of $19\\%$ across all\nmetrics. These results underscore the difficulty of DeepScholar-bench, as well\nas its importance for progress towards AI systems capable of generative\nresearch synthesis. We make our code available at\nhttps://github.com/guestrin-lab/deepscholar-bench.", "AI": {"tldr": "DeepScholar-bench introduces a live benchmark for evaluating generative research synthesis, focusing on creating related work sections by retrieving and synthesizing information from recent ArXiv papers.", "motivation": "To address the lack of effective evaluation metrics for generative research synthesis systems, which are crucial for synthesizing knowledge and expertise in research.", "method": "DeepScholar-bench is an automated evaluation framework that assesses generative research synthesis across three dimensions: knowledge synthesis, retrieval quality, and verifiability. It uses queries drawn from recent ArXiv papers to evaluate systems.", "result": "DeepScholar-base, a reference implementation using the LOTUS API, achieves competitive performance compared to other systems evaluated. Despite this, no system has exceeded 19% across all metrics, highlighting the challenges in research synthesis.", "conclusion": "DeepScholar-bench presents a significant step forward in evaluating generative research synthesis systems, emphasizing the need for improved capabilities in this area to advance AI applications in research.", "key_contributions": ["Introduction of a holistic evaluation framework for generative research synthesis", "Development of DeepScholar-bench as a live benchmark", "Presentation of competitive baseline performance with DeepScholar-base"], "limitations": "The benchmark is still not saturated, indicating ongoing challenges in achieving high performance in generative research synthesis tasks.", "keywords": ["generative research synthesis", "evaluation framework", "DeepScholar-bench"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.20038", "pdf": "https://arxiv.org/pdf/2508.20038.pdf", "abs": "https://arxiv.org/abs/2508.20038", "title": "Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to Enhance LLM Safety Guardrail to Potential Attacks", "authors": ["Sheng Liu", "Qiang Sheng", "Danding Wang", "Yang Li", "Guang Yang", "Juan Cao"], "categories": ["cs.CL"], "comment": "EMNLP 2025 findings", "summary": "Despite advances in improving large language model(LLM) to refuse to answer\nmalicious instructions, widely used LLMs remain vulnerable to jailbreak attacks\nwhere attackers generate instructions with distributions differing from safety\nalignment corpora. New attacks expose LLMs' inability to recognize unseen\nmalicious instructions, highlighting a critical distributional mismatch between\ntraining data and real-world attacks that forces developers into reactive\npatching cycles. To tackle this challenge, we propose IMAGINE, a synthesis\nframework that leverages embedding space distribution analysis to generate\njailbreak-like instructions. This approach effectively fills the distributional\ngap between authentic jailbreak patterns and safety alignment corpora. IMAGINE\nfollows an iterative optimization process that dynamically evolves text\ngeneration distributions across iterations, thereby augmenting the coverage of\nsafety alignment data distributions through synthesized data examples. Based on\nthe safety-aligned corpus enhanced through IMAGINE, our framework demonstrates\nsignificant decreases in attack success rate on Qwen2.5, Llama3.1, and Llama3.2\nwithout compromising their utility.", "AI": {"tldr": "IMAGINE is a framework to generate jailbreak-like instructions for large language models to improve their resilience against malicious attacks.", "motivation": "To address the vulnerability of large language models to jailbreak attacks, which exploit a distributional mismatch between training and real-world malicious instructions.", "method": "IMAGINE employs embedding space distribution analysis and an iterative optimization process to generate and synthesize data examples that enhance safety alignment corpora.", "result": "The proposed IMAGINE framework led to significant decreases in the attack success rates against various LLMs (Qwen2.5, Llama3.1, Llama3.2) while maintaining their utility.", "conclusion": "By systematically augmenting safety alignment data distributions with synthesized examples, IMAGINE improves LLMs' robustness against unseen malicious instructions.", "key_contributions": ["Introduces a framework for generating jailbreak-like instructions using distribution analysis.", "Demonstrates a decrease in attack success rates for popular LLMs without sacrificing utility.", "Proposes an iterative optimization process for dynamic text generation improvement."], "limitations": "", "keywords": ["large language models", "jailbreak attacks", "safety alignment", "IMAGINE", "distribution analysis"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2508.20047", "pdf": "https://arxiv.org/pdf/2508.20047.pdf", "abs": "https://arxiv.org/abs/2508.20047", "title": "AraHealthQA 2025 Shared Task Description Paper", "authors": ["Hassan Alhuzali", "Farah Shamout", "Muhammad Abdul-Mageed", "Chaimae Abouzahir", "Mouath Abu-Daoud", "Ashwag Alasmari", "Walid Al-Eisawi", "Renad Al-Monef", "Ali Alqahtani", "Lama Ayash", "Nizar Habash", "Leen Kharouf"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce {AraHealthQA 2025}, the {Comprehensive Arabic Health Question\nAnswering Shared Task}, held in conjunction with {ArabicNLP 2025} (co-located\nwith EMNLP 2025). This shared task addresses the paucity of high-quality Arabic\nmedical QA resources by offering two complementary tracks: {MentalQA}, focusing\non Arabic mental health Q\\&A (e.g., anxiety, depression, stigma reduction), and\n{MedArabiQ}, covering broader medical domains such as internal medicine,\npediatrics, and clinical decision making. Each track comprises multiple\nsubtasks, evaluation datasets, and standardized metrics, facilitating fair\nbenchmarking. The task was structured to promote modeling under realistic,\nmultilingual, and culturally nuanced healthcare contexts. We outline the\ndataset creation, task design and evaluation framework, participation\nstatistics, baseline systems, and summarize the overall outcomes. We conclude\nwith reflections on the performance trends observed and prospects for future\niterations in Arabic health QA.", "AI": {"tldr": "Introduction of AraHealthQA 2025, a shared task for Arabic health question answering.", "motivation": "To address the lack of high-quality Arabic medical question answering resources.", "method": "The task includes two tracks: MentalQA for mental health Q&A and MedArabiQ for broader medical domains, providing evaluation datasets and metrics for benchmarking.", "result": "Highlights participation statistics, baseline systems, and overall outcomes of the shared task.", "conclusion": "Includes reflections on performance trends and future prospects for Arabic health QA.", "key_contributions": ["Introduction of two tracks for mental health and broader medical domains", "Creation of evaluation datasets and standardized metrics", "Insights into performance trends and future directions for Arabic health QA"], "limitations": "", "keywords": ["Arabic health", "question answering", "MentalQA", "MedArabiQ", "evaluation framework"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.20068", "pdf": "https://arxiv.org/pdf/2508.20068.pdf", "abs": "https://arxiv.org/abs/2508.20068", "title": "11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis", "authors": ["Chengzu Li", "Wenshan Wu", "Huanyu Zhang", "Qingtao Li", "Zeyu Gao", "Yan Xia", "José Hernández-Orallo", "Ivan Vulić", "Furu Wei"], "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": "9 pages, 4 figures (22 pages, 7 figures, 7 tables including\n  references and appendices)", "summary": "For human cognitive process, spatial reasoning and perception are closely\nentangled, yet the nature of this interplay remains underexplored in the\nevaluation of multimodal large language models (MLLMs). While recent MLLM\nadvancements show impressive performance on reasoning, their capacity for\nhuman-like spatial cognition remains an open question. In this work, we\nintroduce a systematic evaluation framework to assess the spatial reasoning\nabilities of state-of-the-art MLLMs relative to human performance. Central to\nour work is 11Plus-Bench, a high-quality benchmark derived from realistic\nstandardized spatial aptitude tests. 11Plus-Bench also features fine-grained\nexpert annotations of both perceptual complexity and reasoning process,\nenabling detailed instance-level analysis of model behavior. Through extensive\nexperiments across 14 MLLMs and human evaluation, we find that current MLLMs\nexhibit early signs of spatial cognition. Despite a large performance gap\ncompared to humans, MLLMs' cognitive profiles resemble those of humans in that\ncognitive effort correlates strongly with reasoning-related complexity.\nHowever, instance-level performance in MLLMs remains largely random, whereas\nhuman correctness is highly predictable and shaped by abstract pattern\ncomplexity. These findings highlight both emerging capabilities and limitations\nin current MLLMs' spatial reasoning capabilities and provide actionable\ninsights for advancing model design.", "AI": {"tldr": "This paper evaluates the spatial reasoning capabilities of multimodal large language models (MLLMs) using a new benchmark, 11Plus-Bench, revealing their emerging abilities but significant performance gaps compared to humans.", "motivation": "To investigate the interplay between spatial reasoning and perception in multimodal large language models (MLLMs), as current evaluations have not adequately explored this aspect.", "method": "Introduced a systematic evaluation framework, utilizing the 11Plus-Bench, a benchmark derived from standardized spatial aptitude tests, for assessing MLLMs' spatial reasoning relative to human performance.", "result": "Experiments on 14 MLLMs showed that while they display early signs of spatial cognition, their performance lags behind humans, indicating significant gaps in reasoning predictability and cognitive effort correlation.", "conclusion": "The findings underscore both capabilities and limitations in MLLMs' spatial reasoning, offering insights for future model design.", "key_contributions": ["Development of the 11Plus-Bench benchmark for evaluating spatial reasoning in MLLMs", "Detailed analysis of MLLMs' spatial cognition relative to human performance", "Identification of cognitive effort correlations in MLLMs' reasoning processes"], "limitations": "Instance-level performance in MLLMs is largely random compared to the predictable correctness of human responses.", "keywords": ["multimodal large language models", "spatial reasoning", "human performance", "benchmark evaluation", "cognitive processes"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2508.19259", "pdf": "https://arxiv.org/pdf/2508.19259.pdf", "abs": "https://arxiv.org/abs/2508.19259", "title": "Capabilities of GPT-5 across critical domains: Is it the next breakthrough?", "authors": ["Georgios P. Georgiou"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "The accelerated evolution of large language models has raised questions about\ntheir comparative performance across domains of practical importance. GPT-4 by\nOpenAI introduced advances in reasoning, multimodality, and task\ngeneralization, establishing itself as a valuable tool in education, clinical\ndiagnosis, and academic writing, though it was accompanied by several flaws.\nReleased in August 2025, GPT-5 incorporates a system-of-models architecture\ndesigned for task-specific optimization and, based on both anecdotal accounts\nand emerging evidence from the literature, demonstrates stronger performance\nthan its predecessor in medical contexts. This study provides one of the first\nsystematic comparisons of GPT-4 and GPT-5 using human raters from linguistics\nand clinical fields. Twenty experts evaluated model-generated outputs across\nfive domains: lesson planning, assignment evaluation, clinical diagnosis,\nresearch generation, and ethical reasoning, based on predefined criteria.\nMixed-effects models revealed that GPT-5 significantly outperformed GPT-4 in\nlesson planning, clinical diagnosis, research generation, and ethical\nreasoning, while both models performed comparably in assignment assessment. The\nfindings highlight the potential of GPT-5 to serve as a context-sensitive and\ndomain-specialized tool, offering tangible benefits for education, clinical\npractice, and academic research, while also advancing ethical reasoning. These\nresults contribute to one of the earliest empirical evaluations of the evolving\ncapabilities and practical promise of GPT-5.", "AI": {"tldr": "This study systematically compares GPT-4 and GPT-5 across five practical domains, revealing that GPT-5 significantly outperforms GPT-4 in several areas like lesson planning and clinical diagnosis.", "motivation": "The rapid evolution of large language models necessitates understanding their performance across important practical domains, particularly for applications in education and healthcare.", "method": "The study involved 20 human raters from linguistics and clinical fields evaluating model-generated outputs from GPT-4 and GPT-5 based on predefined criteria across five domains.", "result": "Mixed-effects models indicated that GPT-5 significantly outperformed GPT-4 in lesson planning, clinical diagnosis, research generation, and ethical reasoning, while both models were comparable in assignment evaluation.", "conclusion": "GPT-5 presents as a more capable tool than GPT-4, particularly in education and clinical contexts, emphasizing its potential for domain-specific applications.", "key_contributions": ["First systematic comparison of GPT-4 and GPT-5 in practical domains.", "Significant findings in the context of clinical diagnosis and lesson planning.", "Contributes to the understanding of LLM capabilities in real-world applications."], "limitations": "", "keywords": ["large language models", "GPT-4", "GPT-5", "clinical diagnosis", "education"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2401.09244", "pdf": "https://arxiv.org/pdf/2401.09244.pdf", "abs": "https://arxiv.org/abs/2401.09244", "title": "Cross-lingual Offensive Language Detection: A Systematic Review of Datasets, Transfer Approaches and Challenges", "authors": ["Aiqi Jiang", "Arkaitz Zubiaga"], "categories": ["cs.CL"], "comment": "35 pages, 7 figures", "summary": "The growing prevalence and rapid evolution of offensive language in social\nmedia amplify the complexities of detection, particularly highlighting the\nchallenges in identifying such content across diverse languages. This survey\npresents a systematic and comprehensive exploration of Cross-Lingual Transfer\nLearning (CLTL) techniques in offensive language detection in social media. Our\nstudy stands as the first holistic overview to focus exclusively on the\ncross-lingual scenario in this domain. We analyse 67 relevant papers and\ncategorise these studies across various dimensions, including the\ncharacteristics of multilingual datasets used, the cross-lingual resources\nemployed, and the specific CLTL strategies implemented. According to \"what to\ntransfer\", we also summarise three main CLTL transfer approaches: instance,\nfeature, and parameter transfer. Additionally, we shed light on the current\nchallenges and future research opportunities in this field. Furthermore, we\nhave made our survey resources available online, including two comprehensive\ntables that provide accessible references to the multilingual datasets and CLTL\nmethods used in the reviewed literature.", "AI": {"tldr": "This survey explores Cross-Lingual Transfer Learning (CLTL) techniques for detecting offensive language in social media, based on an analysis of 67 relevant papers.", "motivation": "To address the complexities in detecting offensive language across diverse languages on social media, emphasizing the need for a systematic exploration of CLTL techniques.", "method": "A systematic review of 67 papers focusing on offensive language detection, categorizing studies by dataset characteristics, resources, and CLTL strategies like instance, feature, and parameter transfer.", "result": "The survey identifies various techniques and resources used in CLTL for offensive language detection, providing a comprehensive overview of the current landscape.", "conclusion": "Current challenges in CLTL for offensive language detection are outlined, alongside future research opportunities. Comprehensive tables of resources are made available online.", "key_contributions": ["First holistic overview of CLTL techniques in offensive language detection", "Analysis and categorization of 67 studies in this domain", "Availability of resources for multilingual datasets and CLTL methods"], "limitations": "", "keywords": ["Cross-Lingual Transfer Learning", "offensive language detection", "social media", "multilingual datasets", "future research"], "importance_score": 5, "read_time_minutes": 35}}
{"id": "2403.01777", "pdf": "https://arxiv.org/pdf/2403.01777.pdf", "abs": "https://arxiv.org/abs/2403.01777", "title": "NPHardEval4V: Dynamic Evaluation of Large Vision-Language Models with Effects of Vision", "authors": ["Xiang Li", "Wenyue Hua", "Kaijie Zhu", "Lingyao Li", "Haoyang Ling", "Jinkui Chi", "Qi Dou", "Jindong Wang", "Yongfeng Zhang", "Xin Ma", "Lizhou Fan"], "categories": ["cs.CL", "cs.CV"], "comment": "25 pages, 9 figures, 2 tables", "summary": "Large Vision-Language Models (LVLMs) have demonstrated impressive\ncapabilities in multimodal understanding, yet their reasoning abilities remain\nunderexplored. Existing benchmarks tend to focus on perception or text-based\ncomprehension, offering limited insight into how well these models perform on\nstructured, logic-driven tasks that require both visual and linguistic\nreasoning. To address this gap, we introduce NPHardEval4V, a multimodal\nbenchmark suite grounded in four classical NP-hard problems: Knapsack, Set\nCover, Traveling Salesperson, and Vertex Cover. Each task is presented through\na combination of structured visual layouts and textual prompts, designed to\nassess the ability of LVLMs to perform combinatorial reasoning under\nvisual-linguistic constraints. We evaluate a set of advanced open-source and\nclosed-source vision-language models under a unified prompting and problem\nrepresentation framework. This enables fair comparison across models and task\ntypes, while isolating key variables affecting performance. Our results show\nthat while these models perform reasonably well on perception-based inputs,\nthey struggle with global optimization, abstraction, and constraint\nsatisfaction. No single model demonstrates consistent reasoning capability\nacross all problem types, and common failure patterns reveal fundamental\nlimitations in current architectures. By leveraging the structure and\ncomplexity of NP-hard problems, NPHardEval4V provides a scalable,\ninterpretable, and challenging testbed for diagnosing reasoning behaviors in\nLVLMs. We hope this benchmark can support the community in building more\nrobust, inference-capable multimodal systems. The benchmark dataset and code\nare available at https://github.com/lizhouf/NPHardEval4.", "AI": {"tldr": "This paper presents NPHardEval4V, a multimodal benchmark suite to evaluate reasoning capabilities of Large Vision-Language Models (LVLMs) on classical NP-hard problems.", "motivation": "To explore the reasoning capabilities of LVLMs beyond perception and text comprehension, especially on structured, logic-driven tasks.", "method": "Introduction of NPHardEval4V, a benchmark focusing on four NP-hard problems using structured visual layouts and textual prompts to test LVLMs' combinatorial reasoning.", "result": "LVLMs show decent performance on perception-based tasks but struggle significantly with reasoning tasks, revealing limitations in their architecture and design.", "conclusion": "NPHardEval4V serves as a scalable and interpretable tool for assessing and improving reasoning capabilities in multimodal systems.", "key_contributions": ["Introduction of NPHardEval4V benchmark for LVLMs", "Focus on NP-hard problems for assessing reasoning", "Findings on LVLMs' limitations in handling global optimization and abstraction tasks."], "limitations": "The benchmark primarily highlights limitations in existing LVLM architectures without offering solutions for improvement.", "keywords": ["Large Vision-Language Models", "multimodal reasoning", "benchmark", "NP-hard problems"], "importance_score": 7, "read_time_minutes": 25}}
{"id": "2411.17374", "pdf": "https://arxiv.org/pdf/2411.17374.pdf", "abs": "https://arxiv.org/abs/2411.17374", "title": "Understanding Fairness-Accuracy Trade-offs in Machine Learning Models: Does Promoting Fairness Undermine Performance?", "authors": ["Junhua Liu", "Roy Ka-Wei Lee", "Kwan Hui Lim"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted to ASONAM 2025", "summary": "Fairness in both Machine Learning (ML) predictions and human decision-making\nis essential, yet both are susceptible to different forms of bias, such as\nalgorithmic and data-driven in ML, and cognitive or subjective in humans. In\nthis study, we examine fairness using a real-world university admissions\ndataset comprising 870 applicant profiles, leveraging three ML models: XGB,\nBi-LSTM, and KNN, alongside BERT embeddings for textual features. To evaluate\nindividual fairness, we introduce a consistency metric that quantifies\nagreement in decisions among ML models and human experts with diverse\nbackgrounds. Our analysis reveals that ML models surpass human evaluators in\nfairness consistency by margins ranging from 14.08\\% to 18.79\\%. Our findings\nhighlight the potential of using ML to enhance fairness in admissions while\nmaintaining high accuracy, advocating a hybrid approach combining human\njudgement and ML models.", "AI": {"tldr": "The study examines fairness in university admissions using ML models and human evaluators, finding ML models to be more consistent in fairness than humans.", "motivation": "To explore the fairness in both ML predictions and human decision-making, addressing different biases affecting each.", "method": "Utilized a real-world university admissions dataset with 870 applicant profiles, applying XGB, Bi-LSTM, and KNN models, as well as BERT embeddings for textual features, and introducing a consistency metric to evaluate fairness.", "result": "ML models demonstrated significantly greater fairness consistency compared to human evaluators, exceeding human performance by 14.08% to 18.79%.", "conclusion": "The findings suggest that ML can enhance fairness in admissions processes while ensuring high accuracy, supporting a hybrid approach that incorporates human judgement.", "key_contributions": ["Introduction of a consistency metric for evaluating fairness", "Empirical comparison of ML model consistency vs. human evaluators", "Advocacy for combining human judgement with ML models in admissions"], "limitations": "", "keywords": ["fairness", "machine learning", "university admissions", "algorithms", "bias"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.19512", "pdf": "https://arxiv.org/pdf/2412.19512.pdf", "abs": "https://arxiv.org/abs/2412.19512", "title": "Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging", "authors": ["Hua Farn", "Hsuan Su", "Shachi H Kumar", "Saurav Sahay", "Shang-Tse Chen", "Hung-yi Lee"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Findings", "summary": "Fine-tuning large language models (LLMs) for downstream tasks often leads to\ncatastrophic forgetting, notably degrading the safety of originally aligned\nmodels. While some existing methods attempt to restore safety by incorporating\nadditional safety data, the quality of such data typically falls short of that\nused in the original alignment process. Moreover, these high-quality safety\ndatasets are generally inaccessible, making it difficult to fully recover the\nmodel's original safety. We ask: How can we preserve safety while improving\ndownstream task performance without additional safety data? We show that simply\nmerging the weights of pre- and post-fine-tuned models effectively mitigates\nsafety degradation while enhancing performance. Experiments across different\ndownstream tasks and models validate the method's practicality and\neffectiveness.", "AI": {"tldr": "This paper addresses the issue of safety degradation in fine-tuned large language models and proposes a weight merging technique to improve both safety and performance without needing additional safety data.", "motivation": "Fine-tuning large language models often leads to catastrophic forgetting, which negatively impacts model safety. The challenge is to maintain safety while improving performance on downstream tasks without access to high-quality safety datasets.", "method": "The proposed method merges the weights of pre- and post-fine-tuned models to simultaneously preserve safety and enhance performance.", "result": "Experiments demonstrate that weight merging effectively mitigates safety degradation while improving performance across various downstream tasks and models.", "conclusion": "The method provides a practical solution to balance safety and performance in LLMs without the need for additional safety data.", "key_contributions": ["Introduction of a weight merging technique for LLMs", "Demonstrated effectiveness in maintaining safety while enhancing downstream task performance", "Validation across multiple tasks and models."], "limitations": "", "keywords": ["large language models", "safety degradation", "fine-tuning", "weight merging", "downstream tasks"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2501.09993", "pdf": "https://arxiv.org/pdf/2501.09993.pdf", "abs": "https://arxiv.org/abs/2501.09993", "title": "Agent-as-Judge for Factual Summarization of Long Narratives", "authors": ["Yeonseok Jeong", "Minsoo Kim", "Seung-won Hwang", "Byung-Hak Kim"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated near-human performance in\nsummarization tasks based on traditional metrics such as ROUGE and BERTScore.\nHowever, these metrics do not adequately capture critical aspects of\nsummarization quality, such as factual accuracy, particularly for long\nnarratives (>100K tokens). Recent advances, such as LLM-as-a-Judge, address the\nlimitations of metrics based on lexical similarity but still exhibit factual\ninconsistencies, especially in understanding character relationships and\nstates. In this work, we introduce NarrativeFactScore, a novel\n\"Agent-as-a-Judge\" framework for evaluating and refining summaries. By\nleveraging a Character Knowledge Graph (CKG) extracted from input and generated\nsummaries, NarrativeFactScore assesses the factual consistency and provides\nactionable guidance for refinement, such as identifying missing or erroneous\nfacts. We demonstrate the effectiveness of NarrativeFactScore through a\ndetailed workflow illustration and extensive validation on widely adopted\nbenchmarks, achieving superior performance compared to competitive methods. Our\nresults highlight the potential of agent-driven evaluation systems to improve\nthe factual reliability of LLM-generated summaries.", "AI": {"tldr": "Introducing NarrativeFactScore, a novel framework using Character Knowledge Graphs to evaluate and refine LLM-generated summaries by assessing factual consistency.", "motivation": "To address the limitations of traditional metrics in capturing summarization quality, especially factual accuracy in long narratives.", "method": "The framework employs a Character Knowledge Graph (CKG) to evaluate summaries' factual consistency and provides refinement guidance.", "result": "Demonstrated superior performance on benchmarks compared to existing methods, improving the reliability of LLM-generated summaries.", "conclusion": "Agent-driven evaluation systems like NarrativeFactScore can significantly enhance the factual reliability of summaries generated by LLMs.", "key_contributions": ["Introduction of NarrativeFactScore for evaluating summaries", "Leveraging Character Knowledge Graphs for factual consistency", "Demonstrating superior performance on summarization benchmarks"], "limitations": "", "keywords": ["Large Language Models", "Summarization", "Factual Consistency", "Character Knowledge Graphs", "Evaluation Metrics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2501.15000", "pdf": "https://arxiv.org/pdf/2501.15000.pdf", "abs": "https://arxiv.org/abs/2501.15000", "title": "MDEval: Evaluating and Enhancing Markdown Awareness in Large Language Models", "authors": ["Zhongpu Chen", "Yinfeng Liu", "Long Shi", "Xingyan Chen", "Yu Zhao", "Fuji Ren"], "categories": ["cs.CL", "cs.IR"], "comment": "WWW 2025", "summary": "Large language models (LLMs) are expected to offer structured Markdown\nresponses for the sake of readability in web chatbots (e.g., ChatGPT). Although\nthere are a myriad of metrics to evaluate LLMs, they fail to evaluate the\nreadability from the view of output content structure. To this end, we focus on\nan overlooked yet important metric -- Markdown Awareness, which directly\nimpacts the readability and structure of the content generated by these\nlanguage models. In this paper, we introduce MDEval, a comprehensive benchmark\nto assess Markdown Awareness for LLMs, by constructing a dataset with 20K\ninstances covering 10 subjects in English and Chinese. Unlike traditional\nmodel-based evaluations, MDEval provides excellent interpretability by\ncombining model-based generation tasks and statistical methods. Our results\ndemonstrate that MDEval achieves a Spearman correlation of 0.791 and an\naccuracy of 84.1% with human, outperforming existing methods by a large margin.\nExtensive experimental results also show that through fine-tuning over our\nproposed dataset, less performant open-source models are able to achieve\ncomparable performance to GPT-4o in terms of Markdown Awareness. To ensure\nreproducibility and transparency, MDEval is open sourced at\nhttps://github.com/SWUFE-DB-Group/MDEval-Benchmark.", "AI": {"tldr": "The paper introduces MDEval, a benchmark for evaluating Markdown Awareness in large language models (LLMs), which impacts the readability of their output.", "motivation": "To address the inadequacy of existing metrics that fail to evaluate the structure and readability of LLM outputs, focusing on Markdown Awareness as a critical measure.", "method": "MDEval combines model-based generation tasks with statistical methods to assess Markdown Awareness, using a dataset of 20K instances in English and Chinese.", "result": "MDEval achieved a Spearman correlation of 0.791 and an accuracy of 84.1% when compared with human evaluations, significantly outperforming prior methods.", "conclusion": "MDEval enables better evaluation of LLM performance related to content structure, improving the interpretability of model assessments and allowing less performant models to compete with top-tier models like GPT-4o.", "key_contributions": ["Introduction of MDEval as a benchmark for Markdown Awareness in LLMs", "Demonstration of significant performance improvements in open-source models through fine-tuning", "Open-sourcing the MDEval dataset for transparency and reproducibility."], "limitations": "", "keywords": ["Markdown Awareness", "Large Language Models", "Readability", "Data Evaluation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.11779", "pdf": "https://arxiv.org/pdf/2502.11779.pdf", "abs": "https://arxiv.org/abs/2502.11779", "title": "Efficient Response Generation Strategy Selection for Fine-Tuning Large Language Models Through Self-Aligned Perplexity", "authors": ["Xuan Ren", "Qi Chen", "Lingqiao Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning large language models (LLMs) typically relies on producing large\nsets of input-output pairs. Yet for a given question, there can be many valid\noutputs. In practice, these outputs are often derived by distilling knowledge\nfrom teacher models, and they can vary depending on the specific teacher model\nor prompting strategy employed. Recent findings show that how these training\noutputs are generated can significantly affect the performance of the\nfine-tuned model, raising an important question: how do we pick the best data\ngeneration method from among numerous possibilities? Rather than exhaustively\ntraining and evaluating on each candidate, this paper proposes a scalable\napproximate method that assesses a small subset of generated data to estimate\nits suitability for a specific target LLM. Our central idea is that effective\noutputs should be familiar to the target LLM. While previous work measures\nfamiliarity with perplexity, we find that perplexity might be suboptimal in\ncharacterizing familiarity through empirical analyses and practical\nobservations. To address this, we introduce self-aligned perplexity, a novel\nmetric capturing how closely candidate outputs adhere to the target LLM's own\nstyle and reasoning patterns. In this way, we can identify the most effective\ngeneration strategy on a small sample, then apply it to produce the complete\ntraining set. We demonstrate that training on data generated by the chosen\nmethod yields significant improvements across diverse reasoning-focused\nbenchmarks, particularly in cases where different candidate methods lead to\nhighly divergent training outcomes. Our implementation is publicly available at\nhttps://github.com/XuanRen4470/SPPL.", "AI": {"tldr": "This paper proposes a scalable method for selecting the best data generation strategy for fine-tuning LLMs by introducing a new metric called self-aligned perplexity, which assesses how well candidate outputs align with the LLM's reasoning patterns.", "motivation": "To improve the performance of fine-tuned LLMs by selecting the optimal data generation technique from numerous possible methods without exhaustive training.", "method": "The paper introduces a scalable approximate method that evaluates a small subset of generated data to identify effective generation strategies based on their familiarity to the target LLM, using a novel metric called self-aligned perplexity.", "result": "The method allows for significant performance enhancements on reasoning-focused benchmarks by training on outputs generated by the most suitable method identified through the proposed evaluation.", "conclusion": "The proposed self-aligned perplexity metric provides a more effective measure of output familiarity compared to traditional perplexity, leading to better fine-tuning outcomes for LLMs.", "key_contributions": ["Introduction of self-aligned perplexity as a new metric for output selection", "Development of an approximate method for scalable data generation strategy evaluation", "Demonstration of significant performance improvements on reasoning tasks using the proposed method"], "limitations": "", "keywords": ["large language models", "data generation", "self-aligned perplexity", "fine-tuning", "reasoning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.15348", "pdf": "https://arxiv.org/pdf/2502.15348.pdf", "abs": "https://arxiv.org/abs/2502.15348", "title": "Constructing a Norm for Children's Scientific Drawing: Distribution Features Based on Semantic Similarity of Large Language Models", "authors": ["Yi Zhang", "Fan Wei", "Jingyi Li", "Yan Wang", "Yanyan Yu", "Jianli Chen", "Zipo Cai", "Xinyu Liu", "Wei Wang", "Sensen Yao", "Peng Wang", "Zhong Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The use of children's drawings to examining their conceptual understanding\nhas been proven to be an effective method, but there are two major problems\nwith previous research: 1. The content of the drawings heavily relies on the\ntask, and the ecological validity of the conclusions is low; 2. The\ninterpretation of drawings relies too much on the subjective feelings of the\nresearchers. To address this issue, this study uses the Large Language Model\n(LLM) to identify 1420 children's scientific drawings (covering 9 scientific\nthemes/concepts), and uses the word2vec algorithm to calculate their semantic\nsimilarity. The study explores whether there are consistent drawing\nrepresentations for children on the same theme, and attempts to establish a\nnorm for children's scientific drawings, providing a baseline reference for\nfollow-up children's drawing research. The results show that the representation\nof most drawings has consistency, manifested as most semantic similarity>0.8.\nAt the same time, it was found that the consistency of the representation is\nindependent of the accuracy (of LLM's recognition), indicating the existence of\nconsistency bias. In the subsequent exploration of influencing factors, we used\nKendall rank correlation coefficient to investigate the effects of \"sample\nsize\", \"abstract degree\", and \"focus points\" on drawings, and used word\nfrequency statistics to explore whether children represented abstract\nthemes/concepts by reproducing what was taught in class. It was found that\naccuracy (of LLM's recognition) is the most sensitive indicator, and data such\nas sample size and semantic similarity are related to it; The consistency\nbetween classroom experiments and teaching purpose is also an important factor,\nmany students focus more on the experiments themselves rather than what they\nexplain.", "AI": {"tldr": "This study investigates children's scientific drawings using LLM and word2vec to establish representation norms and examine factors influencing drawing consistency.", "motivation": "The aim is to address low ecological validity and subjective interpretation issues in previous research on children's drawings.", "method": "The study analyzes 1420 children's scientific drawings across nine themes using LLM for identification and word2vec for semantic similarity calculations.", "result": "Most drawings show high consistency in representation, with a semantic similarity greater than 0.8; consistency is independent of recognition accuracy, indicating consistency bias.", "conclusion": "The findings suggest that LLM can aid in establishing norms for children's drawings, highlighting the importance of accuracy and teaching alignment in influencing drawing outcomes.", "key_contributions": ["Use of LLM in analyzing children's drawings", "Establishment of drawing representation norms", "Identification of factors affecting drawing consistency"], "limitations": "The study may be limited by the reliance on LLM's accuracy and the focus on specific scientific themes.", "keywords": ["children's drawings", "Large Language Model", "semantic similarity", "word2vec", "scientific concepts"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2503.01510", "pdf": "https://arxiv.org/pdf/2503.01510.pdf", "abs": "https://arxiv.org/abs/2503.01510", "title": "KoWit-24: A Richly Annotated Dataset of Wordplay in News Headlines", "authors": ["Alexander Baranov", "Anna Palatkina", "Yulia Makovka", "Pavel Braslavski"], "categories": ["cs.CL"], "comment": "Accepted to RANLP 2025", "summary": "We present KoWit-24, a dataset with fine-grained annotation of wordplay in\n2,700 Russian news headlines. KoWit-24 annotations include the presence of\nwordplay, its type, wordplay anchors, and words/phrases the wordplay refers to.\nUnlike the majority of existing humor collections of canned jokes, KoWit-24\nprovides wordplay contexts -- each headline is accompanied by the news lead and\nsummary. The most common type of wordplay in the dataset is the transformation\nof collocations, idioms, and named entities -- the mechanism that has been\nunderrepresented in previous humor datasets. Our experiments with five LLMs\nshow that there is ample room for improvement in wordplay detection and\ninterpretation tasks. The dataset and evaluation scripts are available at\nhttps://github.com/Humor-Research/KoWit-24", "AI": {"tldr": "KoWit-24 is a dataset for fine-grained annotation of wordplay in Russian news headlines, facilitating improvements in humor detection with LLMs.", "motivation": "To fill the gap in humor datasets that lack wordplay contexts and proper annotation.", "method": "Introduced a dataset of 2,700 Russian news headlines with detailed annotations on wordplay types and contexts, accompanying each headline with a news lead and summary.", "result": "The dataset reveals that most wordplay involves collocations, idioms, and named entities. Experiments indicate significant room for improvement in LLMs' wordplay detection.", "conclusion": "KoWit-24 serves as a valuable resource for advancing humor research and LLM capabilities in understanding wordplay.", "key_contributions": ["Fine-grained annotation of wordplay types in a comprehensive dataset.", "Includes news context alongside each headline, enhancing relevance for humor research.", "Shows the potential for improved LLM performance in humor-related tasks."], "limitations": "", "keywords": ["wordplay", "dataset", "NLP", "humor", "LLM"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2503.13423", "pdf": "https://arxiv.org/pdf/2503.13423.pdf", "abs": "https://arxiv.org/abs/2503.13423", "title": "SuperBPE: Space Travel for Language Models", "authors": ["Alisa Liu", "Jonathan Hayase", "Valentin Hofmann", "Sewoong Oh", "Noah A. Smith", "Yejin Choi"], "categories": ["cs.CL", "cs.LG"], "comment": "COLM 2025 camera-ready", "summary": "The assumption across nearly all language model (LM) tokenization schemes is\nthat tokens should be subwords, i.e., contained within word boundaries. While\nproviding a seemingly reasonable inductive bias, is this common practice\nlimiting the potential of modern LMs? Whitespace is not a reliable delimiter of\nmeaning, as evidenced by multi-word expressions (e.g., \"by the way\"),\ncrosslingual variation in the number of words needed to express a concept\n(e.g., \"spacesuit helmet\" in German is \"raumanzughelm\"), and languages that do\nnot use whitespace at all (e.g., Chinese). To explore the potential of\ntokenization beyond subwords, we introduce a \"superword\" tokenizer, SuperBPE,\nwhich incorporates a simple pretokenization curriculum into the byte-pair\nencoding (BPE) algorithm to first learn subwords, then superwords that bridge\nwhitespace. This brings dramatic improvements in encoding efficiency: when\nfixing the vocabulary size to 200k, SuperBPE encodes a fixed piece of text with\nup to 33% fewer tokens than BPE on average. In experiments, we pretrain 8B\ntransformer LMs from scratch while fixing the model size, vocabulary size, and\ntrain compute, varying *only* the algorithm for learning the vocabulary. Our\nmodel trained with SuperBPE achieves an average +4.0% absolute improvement over\nthe BPE baseline across 30 downstream tasks (including +8.2% on MMLU), while\nsimultaneously requiring 27% less compute at inference time. In analysis, we\nfind that SuperBPE results in segmentations of text that are more uniform in\nper-token difficulty. Qualitatively, this may be because SuperBPE tokens often\ncapture common multi-word expressions that function semantically as a single\nunit. SuperBPE is a straightforward, local modification to tokenization that\nimproves both encoding efficiency and downstream performance, yielding better\nlanguage models overall.", "AI": {"tldr": "Introduces SuperBPE, a superword tokenizer that improves language model efficiency and performance by encoding tokens beyond traditional subwords.", "motivation": "To address the limitations of existing tokenization schemes that rely solely on subwords, which may not effectively capture the meaning in diverse languages and multi-word expressions.", "method": "SuperBPE employs a pretokenization curriculum to first learn subwords and then superwords that bridge whitespace, resulting in more efficient tokenization compared to conventional BPE.", "result": "SuperBPE encodes text with up to 33% fewer tokens than BPE while achieving a +4.0% absolute performance improvement across 30 downstream tasks and requiring 27% less inference compute.", "conclusion": "SuperBPE enhances encoding efficiency and downstream performance, facilitating better language model training without complex changes to existing frameworks.", "key_contributions": ["Introduction of superword tokenization for LMs", "Significant improvements in encoding efficiency and downstream task performance", "Reduction in computational resource requirements during inference"], "limitations": "", "keywords": ["tokenization", "language models", "SuperBPE", "HCI", "multi-word expressions"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2504.00657", "pdf": "https://arxiv.org/pdf/2504.00657.pdf", "abs": "https://arxiv.org/abs/2504.00657", "title": "News is More than a Collection of Facts: Moral Frame Preserving News Summarization", "authors": ["Enrico Liscio", "Michela Lorandi", "Pradeep K. Murukannaiah"], "categories": ["cs.CL"], "comment": "Accepted at COLM2025", "summary": "News articles are more than collections of facts; they reflect journalists'\nframing, shaping how events are presented to the audience. One key aspect of\nframing is the choice to write in (or quote verbatim) morally charged language\nas opposed to using neutral terms. This moral framing carries implicit\njudgments that automated news summarizers should recognize and preserve to\nmaintain the original intent of the writer. In this work, we perform the first\nstudy on the preservation of moral framing in AI-generated news summaries. We\npropose an approach that leverages the intuition that journalists intentionally\nuse or report specific moral-laden words, which should be retained in\nsummaries. Through automated, crowd-sourced, and expert evaluations, we\ndemonstrate that our approach enhances the preservation of moral framing while\nmaintaining overall summary quality.", "AI": {"tldr": "This paper studies the preservation of moral framing in AI-generated news summaries, proposing an approach to retain morally charged language from journalists.", "motivation": "To explore how moral framing in news articles impacts the presentation of information and to ensure AI summarizers recognize and retain this framing.", "method": "The study employs automated summarization methods along with crowd-sourced and expert evaluations to assess the effectiveness of preserving moral framing in summaries.", "result": "The proposed approach improves the retention of moral-laden language in AI-generated summaries, leading to better preservation of the original intent of journalists.", "conclusion": "Enhancing the preservation of moral framing in news summaries contributes to the informational and ethical responsibilities of automated news summarization.", "key_contributions": ["First study on moral framing preservation in AI news summarization", "Introduces a method for retaining morally charged language in summaries", "Demonstrates effectiveness through extensive evaluations."], "limitations": "The study is specific to news articles and may not generalize to other text forms.", "keywords": ["moral framing", "AI summarization", "news articles", "natural language processing", "journalism"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2504.07994", "pdf": "https://arxiv.org/pdf/2504.07994.pdf", "abs": "https://arxiv.org/abs/2504.07994", "title": "Evaluating the Fitness of Ontologies for the Task of Question Generation", "authors": ["Samah Alkhuzaey", "Floriana Grasso", "Terry R. Payne", "Valentina Tamma"], "categories": ["cs.CL", "cs.AI"], "comment": "Revised version (v2) accepted for the 28th European Conference on\n  Artificial Intelligence (ECAI-2025), including a validation study", "summary": "Ontology-based question generation is an important application of\nsemantic-aware systems that enables the creation of large question banks for\ndiverse learning environments. The effectiveness of these systems, both in\nterms of the calibre and cognitive difficulty of the resulting questions,\ndepends heavily on the quality and modelling approach of the underlying\nontologies, making it crucial to assess their fitness for this task. To date,\nthere has been no comprehensive investigation into the specific ontology\naspects or characteristics that affect the question generation process.\nTherefore, this paper proposes a set of requirements and task-specific metrics\nfor evaluating the fitness of ontologies for question generation tasks in\npedagogical settings. Using the ROMEO methodology (a structured framework used\nfor identifying task-specific metrics), a set of evaluation metrics have been\nderived from an expert assessment of questions generated by a question\ngeneration model. To validate the proposed metrics, we apply them to a set of\nontologies previously used in question generation to illustrate how the metric\nscores align with and complement findings reported in earlier studies. The\nanalysis confirms that ontology characteristics significantly impact the\neffectiveness of question generation, with different ontologies exhibiting\nvarying performance levels. This highlights the importance of assessing\nontology quality with respect to Automatic Question Generation (AQG) tasks.", "AI": {"tldr": "This paper proposes metrics for evaluating ontologies used in automatic question generation (AQG) to improve question quality in educational settings.", "motivation": "The need to ensure that ontologies are suitable for generating quality questions in educational contexts due to the lack of comprehensive evaluation criteria.", "method": "The paper employs the ROMEO methodology to develop task-specific metrics for ontology evaluation based on expert assessments of generated questions.", "result": "Analysis shows significant variation in question generation effectiveness based on different ontology characteristics, confirming the need for quality assessment in AQG.", "conclusion": "Assessing ontology fitness for AQG tasks is essential to improve the caliber and cognitive challenge of generated questions, ensuring better learning outcomes.", "key_contributions": ["Proposes requirements and metrics for ontology evaluation in AQG.", "Validates metrics through analysis of existing ontologies.", "Demonstrates the impact of ontology characteristics on question generation effectiveness."], "limitations": "", "keywords": ["Ontology", "Question Generation", "Automatic Question Generation", "Evaluation Metrics", "Educational Technology"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2504.13655", "pdf": "https://arxiv.org/pdf/2504.13655.pdf", "abs": "https://arxiv.org/abs/2504.13655", "title": "Multi-Type Context-Aware Conversational Recommender Systems via Mixture-of-Experts", "authors": ["Jie Zou", "Cheng Lin", "Weikang Guo", "Zheng Wang", "Jiwei Wei", "Yang Yang", "Heng Tao Shen"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "31 pages; Accepted by Information Fusion", "summary": "Conversational recommender systems enable natural language conversations and\nthus lead to a more engaging and effective recommendation scenario. As the\nconversations for recommender systems usually contain limited contextual\ninformation, many existing conversational recommender systems incorporate\nexternal sources to enrich the contextual information. However, how to combine\ndifferent types of contextual information is still a challenge. In this paper,\nwe propose a multi-type context-aware conversational recommender system, called\nMCCRS, effectively fusing multi-type contextual information via\nmixture-of-experts to improve conversational recommender systems. MCCRS\nincorporates both structured information and unstructured information,\nincluding the structured knowledge graph, unstructured conversation history,\nand unstructured item reviews. It consists of several experts, with each expert\nspecialized in a particular domain (i.e., one specific contextual information).\nMultiple experts are then coordinated by a ChairBot to generate the final\nresults. Our proposed MCCRS model takes advantage of different contextual\ninformation and the specialization of different experts followed by a ChairBot\nbreaks the model bottleneck on a single contextual information. Experimental\nresults demonstrate that our proposed MCCRS method achieves significantly\nhigher performance compared to existing baselines.", "AI": {"tldr": "MCCRS is a novel conversational recommender system that integrates various types of contextual information to enhance performance using a mixture-of-experts approach.", "motivation": "The motivation behind this research is to address the limitation of existing conversational recommender systems, which often struggle with limited contextual information in conversations.", "method": "The proposed method, MCCRS, fuses multi-type contextual information utilizing a mixture-of-experts model, where each expert focuses on a specific type of contextual information while a ChairBot coordinates their outputs.", "result": "Experimental results show that MCCRS achieves significantly higher performance compared to existing baseline models for conversational recommender systems.", "conclusion": "The research concludes that integrating diverse contextual information through specialized experts improves the effectiveness of conversational recommendations.", "key_contributions": ["Introduction of MCCRS for leveraging multi-type contextual information", "Utilization of a mixture-of-experts framework for better recommendations", "Coordination of multiple experts via a ChairBot to enhance model performance."], "limitations": "", "keywords": ["Conversational recommender systems", "Context awareness", "Mixture-of-experts", "ChairBot", "Knowledge graph"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.16812", "pdf": "https://arxiv.org/pdf/2507.16812.pdf", "abs": "https://arxiv.org/abs/2507.16812", "title": "MegaScience: Pushing the Frontiers of Post-Training Datasets for Science Reasoning", "authors": ["Run-Ze Fan", "Zengzhi Wang", "Pengfei Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "39 pages; Github: https://github.com/GAIR-NLP/MegaScience; HF:\n  https://huggingface.co/MegaScience", "summary": "Scientific reasoning is critical for developing AI scientists and supporting\nhuman researchers in advancing the frontiers of natural science discovery.\nHowever, the open-source community has primarily focused on mathematics and\ncoding while neglecting the scientific domain, largely due to the absence of\nopen, large-scale, high-quality, verifiable scientific reasoning datasets. To\nbridge this gap, we first present TextbookReasoning, an open dataset featuring\ntruthful reference answers extracted from 12k university-level scientific\ntextbooks, comprising 650k reasoning questions spanning 7 scientific\ndisciplines. We further introduce MegaScience, a large-scale mixture of\nhigh-quality open-source datasets totaling 1.25 million instances, developed\nthrough systematic ablation studies that evaluate various data selection\nmethodologies to identify the optimal subset for each publicly available\nscientific dataset. Meanwhile, we build a comprehensive evaluation system\ncovering diverse subjects and question types across 15 benchmarks,\nincorporating comprehensive answer extraction strategies to ensure accurate\nevaluation metrics. Our experiments demonstrate that our datasets achieve\nsuperior performance and training efficiency with more concise response lengths\ncompared to existing open-source scientific datasets. Furthermore, we train\nLlama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which\nsignificantly outperform the corresponding official instruct models in average\nperformance. In addition, MegaScience exhibits greater effectiveness for larger\nand stronger models, suggesting a scaling benefit for scientific tuning. We\nrelease our data curation pipeline, evaluation system, datasets, and seven\ntrained models to the community to advance scientific reasoning research.", "AI": {"tldr": "Introduction of two datasets, TextbookReasoning and MegaScience, for improving scientific reasoning in AI.", "motivation": "To support AI scientists and aid human researchers in natural science discovery by providing open, high-quality, verifiable datasets.", "method": "Developed TextbookReasoning from 12k scientific textbooks with 650k reasoning questions and created MegaScience dataset consisting of 1.25 million instances using systematic ablation studies.", "result": "Datasets achieved superior performance and training efficiency in scientific reasoning compared to existing datasets; trained models outperform official instruct models.", "conclusion": "The released data and models facilitate further research in scientific reasoning, revealing scaling benefits for larger models.", "key_contributions": ["Introduction of TextbookReasoning dataset for scientific reasoning", "Creation of MegaScience dataset with systematic ablation studies", "Development of evaluation systems for diverse subjects and question types"], "limitations": "", "keywords": ["scientific reasoning", "datasets", "AI models"], "importance_score": 7, "read_time_minutes": 10}}
