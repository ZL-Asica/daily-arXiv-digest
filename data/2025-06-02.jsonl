{"id": "2505.23780", "pdf": "https://arxiv.org/pdf/2505.23780.pdf", "abs": "https://arxiv.org/abs/2505.23780", "title": "More-than-Human Storytelling: Designing Longitudinal Narrative Engagements with Generative AI", "authors": ["Émilie Fabre", "Katie Seaborn", "Shuta Koiwai", "Mizuki Watanabe", "Paul Riesch"], "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.SD", "eess.AS"], "comment": "CHI EA '25", "summary": "Longitudinal engagement with generative AI (GenAI) storytelling agents is a\ntimely but less charted domain. We explored multi-generational experiences with\n\"Dreamsmithy,\" a daily dream-crafting app, where participants (N = 28)\nco-created stories with AI narrator \"Makoto\" every day. Reflections and\ninteractions were captured through a two-week diary study. Reflexive thematic\nanalysis revealed themes likes \"oscillating ambivalence\" and\n\"socio-chronological bonding,\" highlighting the complex dynamics that emerged\nbetween individuals and the AI narrator over time. Findings suggest that while\npeople appreciated the personal notes, opportunities for reflection, and AI\ncreativity, limitations in narrative coherence and control occasionally caused\nfrustration. The results underscore the potential of GenAI for longitudinal\nstorytelling, but also raise critical questions about user agency and ethics.\nWe contribute initial empirical insights and design considerations for\ndeveloping adaptive, more-than-human storytelling systems."}
{"id": "2505.23994", "pdf": "https://arxiv.org/pdf/2505.23994.pdf", "abs": "https://arxiv.org/abs/2505.23994", "title": "PolicyPulse: LLM-Synthesis Tool for Policy Researchers", "authors": ["Maggie Wang", "Ella Colby", "Jennifer Okwara", "Varun Nagaraj Rao", "Yuhan Liu", "Andrés Monroy-Hernández"], "categories": ["cs.HC"], "comment": "Published in Proceedings of the Extended Abstracts of the CHI\n  Conference on Human Factors in Computing Systems", "summary": "Public opinion shapes policy, yet capturing it effectively to surface diverse\nperspectives remains challenging. This paper introduces PolicyPulse, an\nLLM-powered interactive system that synthesizes public experiences from online\ncommunity discussions to help policy researchers author memos and briefs,\nleveraging curated real-world anecdotes. Given a specific topic (e.g., \"Climate\nChange\"), PolicyPulse returns an organized list of themes (e.g., \"Biodiversity\nLoss\" or \"Carbon Pricing\"), supporting each theme with relevant quotes from\nreal-life anecdotes. We compared PolicyPulse outputs to authoritative policy\nreports. Additionally, we asked 11 policy researchers across multiple\ninstitutions in the Northeastern U.S to compare using PolicyPulse with their\nexpert approach. We found that PolicyPulse's themes aligned with authoritative\nreports and helped spark research by analyzing existing data, gathering diverse\nexperiences, revealing unexpected themes, and informing survey or interview\ndesign. Participants also highlighted limitations including insufficient\ndemographic context and data verification challenges. Our work demonstrates how\nAI-powered tools can help influence policy-relevant research and shape policy\noutcomes."}
{"id": "2505.23997", "pdf": "https://arxiv.org/pdf/2505.23997.pdf", "abs": "https://arxiv.org/abs/2505.23997", "title": "Fitting the Message to the Moment: Designing Calendar-Aware Stress Messaging with Large Language Models", "authors": ["Pranav Rao", "Maryam Taj", "Alex Mariakakis", "Joseph Jay Williams", "Ananya Bhattacharjee"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Existing stress-management tools fail to account for the timing and\ncontextual specificity of students' daily lives, often providing static or\nmisaligned support. Digital calendars contain rich, personal indicators of\nupcoming responsibilities, yet this data is rarely leveraged for adaptive\nwellbeing interventions. In this short paper, we explore how large language\nmodels (LLMs) might use digital calendar data to deliver timely and\npersonalized stress support. We conducted a one-week study with eight\nuniversity students using a functional technology probe that generated daily\nstress-management messages based on participants' calendar events. Through\nsemi-structured interviews and thematic analysis, we found that participants\nvalued interventions that prioritized stressful events and adopted a concise,\nbut colloquial tone. These findings reveal key design implications for\nLLM-based stress-management tools, including the need for structured\nquestioning and tone calibration to foster relevance and trust."}
{"id": "2505.24000", "pdf": "https://arxiv.org/pdf/2505.24000.pdf", "abs": "https://arxiv.org/abs/2505.24000", "title": "ConversAR: Exploring Embodied LLM-Powered Group Conversations in Augmented Reality for Second Language Learners", "authors": ["Jad Bendarkawi", "Ashley Ponce", "Sean Mata", "Aminah Aliu", "Yuhan Liu", "Lei Zhang", "Amna Liaqat", "Varun Nagaraj Rao", "Andrés Monroy-Hernández"], "categories": ["cs.HC"], "comment": "Published in Proceedings of the Extended Abstracts of the CHI\n  Conference on Human Factors in Computing Systems", "summary": "Group conversations are valuable for second language (L2) learners as they\nprovide opportunities to practice listening and speaking, exercise complex\nturn-taking skills, and experience group social dynamics in a target language.\nHowever, most existing Augmented Reality (AR)-based conversational learning\ntools focus on dyadic interactions rather than group dialogues. Although\nresearch has shown that AR can help reduce speaking anxiety and create a\ncomfortable space for practicing speaking skills in dyadic scenarios,\nespecially with Large Language Model (LLM)-based conversational agents, the\npotential for group language practice using these technologies remains largely\nunexplored. We introduce ConversAR, a gpt-4o powered AR application, that\nenables L2 learners to practice contextualized group conversations. Our system\nfeatures two embodied LLM agents with vision-based scene understanding and live\ncaptions. In a system evaluation with 10 participants, users reported reduced\nspeaking anxiety and increased learner autonomy compared to perceptions of\nin-person practice methods with other learners."}
{"id": "2505.23785", "pdf": "https://arxiv.org/pdf/2505.23785.pdf", "abs": "https://arxiv.org/abs/2505.23785", "title": "Meaning Is Not A Metric: Using LLMs to make cultural context legible at scale", "authors": ["Cody Kommers", "Drew Hemment", "Maria Antoniak", "Joel Z. Leibo", "Hoyt Long", "Emily Robinson", "Adam Sobey"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Position paper", "summary": "This position paper argues that large language models (LLMs) can make\ncultural context, and therefore human meaning, legible at an unprecedented\nscale in AI-based sociotechnical systems. We argue that such systems have\npreviously been unable to represent human meaning because they rely on thin\ndescriptions: numerical representations that enforce standardization and\ntherefore strip human activity of the cultural context that gives it meaning.\nBy contrast, scholars in the humanities and qualitative social sciences have\ndeveloped frameworks for representing meaning through thick description: verbal\nrepresentations that accommodate heterogeneity and retain contextual\ninformation needed to represent human meaning. While these methods can\neffectively codify meaning, they are difficult to deploy at scale. However, the\nverbal capabilities of LLMs now provide a means of (at least partially)\nautomating the generation and processing of thick descriptions, potentially\novercoming this bottleneck. We argue that the problem of rendering human\nmeaning legible is not just about selecting better metrics, but about\ndeveloping new representational formats (based on thick description). We frame\nthis as a crucial direction for the application of generative AI and identify\nfive key challenges: preserving context, maintaining interpretive pluralism,\nintegrating perspectives based on lived experience and critical distance,\ndistinguishing qualitative content from quantitative magnitude, and\nacknowledging meaning as dynamic rather than static. Furthermore, we suggest\nthat thick description has the potential to serve as a unifying framework to\naddress a number of emerging concerns about the difficulties of representing\nculture in (or using) LLMs."}
{"id": "2505.24004", "pdf": "https://arxiv.org/pdf/2505.24004.pdf", "abs": "https://arxiv.org/abs/2505.24004", "title": "Redefining Research Crowdsourcing: Incorporating Human Feedback with LLM-Powered Digital Twins", "authors": ["Amanda Chan", "Catherine Di", "Joseph Rupertus", "Gary Smith", "Varun Nagaraj Rao", "Manoel Horta Ribeiro", "Andrés Monroy-Hernández"], "categories": ["cs.HC", "cs.CL", "cs.CY"], "comment": "Accepted as a CHI Late Breaking Work (2025), cite appropriately", "summary": "Crowd work platforms like Amazon Mechanical Turk and Prolific are vital for\nresearch, yet workers' growing use of generative AI tools poses challenges.\nResearchers face compromised data validity as AI responses replace authentic\nhuman behavior, while workers risk diminished roles as AI automates tasks. To\naddress this, we propose a hybrid framework using digital twins, personalized\nAI models that emulate workers' behaviors and preferences while keeping humans\nin the loop. We evaluate our system with an experiment (n=88 crowd workers) and\nin-depth interviews with crowd workers (n=5) and social science researchers\n(n=4). Our results suggest that digital twins may enhance productivity and\nreduce decision fatigue while maintaining response quality. Both researchers\nand workers emphasized the importance of transparency, ethical data use, and\nworker agency. By automating repetitive tasks and preserving human engagement\nfor nuanced ones, digital twins may help balance scalability with authenticity."}
{"id": "2505.23788", "pdf": "https://arxiv.org/pdf/2505.23788.pdf", "abs": "https://arxiv.org/abs/2505.23788", "title": "Nine Ways to Break Copyright Law and Why Our LLM Won't: A Fair Use Aligned Generation Framework", "authors": ["Aakash Sen Sharma", "Debdeep Sanyal", "Priyansh Srivastava", "Sundar Atreya H.", "Shirish Karande", "Mohan Kankanhalli", "Murari Mandal"], "categories": ["cs.CL", "cs.AI"], "comment": "30 Pages", "summary": "Large language models (LLMs) commonly risk copyright infringement by\nreproducing protected content verbatim or with insufficient transformative\nmodifications, posing significant ethical, legal, and practical concerns.\nCurrent inference-time safeguards predominantly rely on restrictive\nrefusal-based filters, often compromising the practical utility of these\nmodels. To address this, we collaborated closely with intellectual property\nexperts to develop FUA-LLM (Fair Use Aligned Language Models), a\nlegally-grounded framework explicitly designed to align LLM outputs with\nfair-use doctrine. Central to our method is FairUseDB, a carefully constructed\ndataset containing 18,000 expert-validated examples covering nine realistic\ninfringement scenarios. Leveraging this dataset, we apply Direct Preference\nOptimization (DPO) to fine-tune open-source LLMs, encouraging them to produce\nlegally compliant and practically useful alternatives rather than resorting to\nblunt refusal. Recognizing the shortcomings of traditional evaluation metrics,\nwe propose new measures: Weighted Penalty Utility and Compliance Aware Harmonic\nMean (CAH) to balance infringement risk against response utility. Extensive\nquantitative experiments coupled with expert evaluations confirm that FUA-LLM\nsubstantially reduces problematic outputs (up to 20\\%) compared to\nstate-of-the-art approaches, while preserving real-world usability."}
{"id": "2505.24014", "pdf": "https://arxiv.org/pdf/2505.24014.pdf", "abs": "https://arxiv.org/abs/2505.24014", "title": "Enhancing Critical Thinking in Generative AI Search with Metacognitive Prompts", "authors": ["Anjali Singh", "Zhitong Guan", "Soo Young Rieh"], "categories": ["cs.HC"], "comment": "To appear in Proceedings of the Association for Information Science\n  and Technology. 2025", "summary": "The growing use of Generative AI (GenAI) conversational search tools has\nraised concerns about their effects on people's metacognitive engagement,\ncritical thinking, and learning. As people increasingly rely on GenAI to\nperform tasks such as analyzing and applying information, they may become less\nactively engaged in thinking and learning. This study examines whether\nmetacognitive prompts - designed to encourage people to pause, reflect, assess\ntheir understanding, and consider multiple perspectives - can support critical\nthinking during GenAI-based search. We conducted a user study (N=40) with\nuniversity students to investigate the impact of metacognitive prompts on their\nthought processes and search behaviors while searching with a GenAI tool. We\nfound that these prompts led to more active engagement, prompting students to\nexplore a broader range of topics and engage in deeper inquiry through\nfollow-up queries. Students reported that the prompts were especially helpful\nfor considering overlooked perspectives, promoting evaluation of AI responses,\nand identifying key takeaways. Additionally, the effectiveness of these prompts\nwas influenced by students' metacognitive flexibility. Our findings highlight\nthe potential of metacognitive prompts to foster critical thinking and provide\ninsights for designing and implementing metacognitive support in human-AI\ninteractions."}
{"id": "2505.23789", "pdf": "https://arxiv.org/pdf/2505.23789.pdf", "abs": "https://arxiv.org/abs/2505.23789", "title": "Conversational Exploration of Literature Landscape with LitChat", "authors": ["Mingyu Huang", "Shasha Zhou", "Yuxuan Chen", "Ke Li"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "We are living in an era of \"big literature\", where the volume of digital\nscientific publications is growing exponentially. While offering new\nopportunities, this also poses challenges for understanding literature\nlandscapes, as traditional manual reviewing is no longer feasible. Recent large\nlanguage models (LLMs) have shown strong capabilities for literature\ncomprehension, yet they are incapable of offering \"comprehensive, objective,\nopen and transparent\" views desired by systematic reviews due to their limited\ncontext windows and trust issues like hallucinations. Here we present LitChat,\nan end-to-end, interactive and conversational literature agent that augments\nLLM agents with data-driven discovery tools to facilitate literature\nexploration. LitChat automatically interprets user queries, retrieves relevant\nsources, constructs knowledge graphs, and employs diverse data-mining\ntechniques to generate evidence-based insights addressing user needs. We\nillustrate the effectiveness of LitChat via a case study on AI4Health,\nhighlighting its capacity to quickly navigate the users through large-scale\nliterature landscape with data-based evidence that is otherwise infeasible with\ntraditional means."}
{"id": "2505.24039", "pdf": "https://arxiv.org/pdf/2505.24039.pdf", "abs": "https://arxiv.org/abs/2505.24039", "title": "Advancing Digital Accessibility: Integrating AR/VR and Health Tech for Inclusive Healthcare Solutions", "authors": ["Vishnu Ramineni", "Shivareddy Devarapalli", "Balakrishna Pothineni", "Prema Kumar Veerapaneni", "Aditya Gupta", "Pankaj Gupta"], "categories": ["cs.HC"], "comment": "15 pages", "summary": "Modern healthcare domain incorporates a feature of digital accessibility to\nensure seamless flow of online services for the patients. However, this feature\nof digital accessibility poses a challenge particularly for patients with\ndisabilities. To eradicate this issue and provide immersive and user-friendly\nexperiences, evolving technologies like Augmented Reality (AR) and Virtual\nReality (VR) are integrated in medical applications to enhance accessibility.\nThe present research paper aims to study inclusivity and accessibility features\nof AR/VR in revolutionizing healthcare practices especially in domains like\ntelemedicine, patient education, assistive tools, and rehabilitation for\npersons with disabilities. The current trends of advancements and case studies\nare also analyzed to measure the efficacy of AR/VR in healthcare. Moreover, the\npaper entails a detailed analysis of the challenges of its adoption\nparticularly technical limitations, implementation costs, and regulatory\naspects. Finally, the paper concludes with recommendations for integrating\nAR/VR to foster a more equitable and inclusive healthcare system and provide\nindividuals with auditory, visual, and motor impairments with digital\nhealthcare solutions."}
{"id": "2505.23790", "pdf": "https://arxiv.org/pdf/2505.23790.pdf", "abs": "https://arxiv.org/abs/2505.23790", "title": "Rethinking the Understanding Ability across LLMs through Mutual Information", "authors": ["Shaojie Wang", "Sirui Ding", "Na Zou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have revolutionized natural\nlanguage processing, yet evaluating their intrinsic linguistic understanding\nremains challenging. Moving beyond specialized evaluation tasks, we propose an\ninformation-theoretic framework grounded in mutual information (MI) to achieve\nthis. We formalize the understanding as MI between an input sentence and its\nlatent representation (sentence-level MI), measuring how effectively input\ninformation is preserved in latent representation. Given that LLMs learn\nembeddings for individual tokens, we decompose sentence-level MI into\ntoken-level MI between tokens and sentence embeddings, establishing theoretical\nbounds connecting these measures. Based on this foundation, we theoretically\nderive a computable lower bound for token-level MI using Fano's inequality,\nwhich directly relates to token-level recoverability-the ability to predict\noriginal tokens from sentence embedding. We implement this recoverability task\nto comparatively measure MI across different LLMs, revealing that encoder-only\nmodels consistently maintain higher information fidelity than their\ndecoder-only counterparts, with the latter exhibiting a distinctive late-layer\n\"forgetting\" pattern where mutual information is first enhanced and then\ndiscarded. Moreover, fine-tuning to maximize token-level recoverability\nconsistently improves understanding ability of LLMs on tasks without\ntask-specific supervision, demonstrating that mutual information can serve as a\nfoundation for understanding and improving language model capabilities."}
{"id": "2505.24042", "pdf": "https://arxiv.org/pdf/2505.24042.pdf", "abs": "https://arxiv.org/abs/2505.24042", "title": "Advancing Digital Accessibility In Digital Pharmacy, Healthcare, And Wearable Devices: Inclusive Solutions for Enhanced Patient Engagement", "authors": ["Vishnu Ramineni", "Balaji Shesharao Ingole", "Nikhil Kumar Pulipeta", "Balakrishna Pothineni", "Aditya Gupta"], "categories": ["cs.HC"], "comment": "15 pages", "summary": "Modern healthcare facilities demand digital accessibility to guarantee equal\naccess to telemedicine platforms, online pharmacy services, and health\nmonitoring devices that can be worn or are handy. With the rising call for the\nimplementation of robust digital healthcare solutions, people with disabilities\nencounter impediments in their endeavor of managing and getting accustomed to\nthese modern technologies owing to insufficient accessibility features. The\npaper highlights the role of comprehensive solutions for enhanced patient\nengagement and usability, particularly, in digital pharmacy, healthcare, and\nwearable devices. Besides, it elucidates the key obstructions faced by users\nexperiencing auditory, visual, cognitive, and motor impairments. Through a kind\nconsideration of present accessibility guidelines, practices, and emerging\ntechnologies, the paper provides a holistic overview by offering innovative\nsolutions, accentuating the vitality of compliance with Web Content\nAccessibility Guidelines (WCAG), Americans with Disabilities Act (ADA), and\nother regulatory structures to foster easy access to digital healthcare\nservices. Moreover, there is due focus on using AI-driven tools,\nspeech-activated interfaces, and tactile feedback in wearable health devices to\nassist persons with disabilities. The outcome of the research explicates the\nnecessity of prioritizing accessibility for individuals with disabilities and\ncultivating a culture where healthcare providers, policymakers, and officials\nbuild a patient-centered digital healthcare ecosystem that is all-encompassing\nin nature."}
{"id": "2505.23794", "pdf": "https://arxiv.org/pdf/2505.23794.pdf", "abs": "https://arxiv.org/abs/2505.23794", "title": "R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via Reinforcement Learning", "authors": ["Yuan Li", "Qi Luo", "Xiaonan Li", "Bufan Li", "Qinyuan Cheng", "Bo Wang", "Yining Zheng", "Yuxin Wang", "Zhangyue Yin", "Xipeng Qiu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) integrates external knowledge with Large\nLanguage Models (LLMs) to enhance factual correctness and mitigate\nhallucination. However, dense retrievers often become the bottleneck of RAG\nsystems due to their limited parameters compared to LLMs and their inability to\nperform step-by-step reasoning. While prompt-based iterative RAG attempts to\naddress these limitations, it is constrained by human-designed workflows. To\naddress these limitations, we propose $\\textbf{R3-RAG}$, which uses\n$\\textbf{R}$einforcement learning to make the LLM learn how to\n$\\textbf{R}$eason and $\\textbf{R}$etrieve step by step, thus retrieving\ncomprehensive external knowledge and leading to correct answers. R3-RAG is\ndivided into two stages. We first use cold start to make the model learn the\nmanner of iteratively interleaving reasoning and retrieval. Then we use\nreinforcement learning to further harness its ability to better explore the\nexternal retrieval environment. Specifically, we propose two rewards for\nR3-RAG: 1) answer correctness for outcome reward, which judges whether the\ntrajectory leads to a correct answer; 2) relevance-based document verification\nfor process reward, encouraging the model to retrieve documents that are\nrelevant to the user question, through which we can let the model learn how to\niteratively reason and retrieve relevant documents to get the correct answer.\nExperimental results show that R3-RAG significantly outperforms baselines and\ncan transfer well to different retrievers. We release R3-RAG at\nhttps://github.com/Yuan-Li-FNLP/R3-RAG."}
{"id": "2505.24102", "pdf": "https://arxiv.org/pdf/2505.24102.pdf", "abs": "https://arxiv.org/abs/2505.24102", "title": "Beyond the Prototype: Challenges of Long-Term Integration of Visual Analytics in Civic Spaces", "authors": ["Mahmood Jasim", "Narges Mahyar"], "categories": ["cs.HC"], "comment": null, "summary": "Despite the recognized benefits of visual analytics systems in supporting\ndata-driven decision-making, their deployment in real-world civic contexts\noften faces significant barriers. Beyond technical challenges such as resource\nconstraints and development complexity, sociotechnical factors, including\norganizational hierarchies, misalignment between designers and stakeholders,\nand concerns around technology adoption hinder their sustained use. In this\nwork, we reflect on our collective experiences of designing, developing, and\ndeploying visual analytics systems in the civic domain and discuss challenges\nacross design and adoption aspects. We emphasize the need for deeper\nintegration strategies, equitable stakeholder engagement, and sustainable\nimplementation frameworks to bridge the gap between research and practice."}
{"id": "2505.23796", "pdf": "https://arxiv.org/pdf/2505.23796.pdf", "abs": "https://arxiv.org/abs/2505.23796", "title": "Emergent LLM behaviors are observationally equivalent to data leakage", "authors": ["Christopher Barrie", "Petter Törnberg"], "categories": ["cs.CL", "cs.GT"], "comment": null, "summary": "Ashery et al. recently argue that large language models (LLMs), when paired\nto play a classic \"naming game,\" spontaneously develop linguistic conventions\nreminiscent of human social norms. Here, we show that their results are better\nexplained by data leakage: the models simply reproduce conventions they already\nencountered during pre-training. Despite the authors' mitigation measures, we\nprovide multiple analyses demonstrating that the LLMs recognize the structure\nof the coordination game and recall its outcomes, rather than exhibit\n\"emergent\" conventions. Consequently, the observed behaviors are\nindistinguishable from memorization of the training corpus. We conclude by\npointing to potential alternative strategies and reflecting more generally on\nthe place of LLMs for social science models."}
{"id": "2505.24107", "pdf": "https://arxiv.org/pdf/2505.24107.pdf", "abs": "https://arxiv.org/abs/2505.24107", "title": "GPTFootprint: Increasing Consumer Awareness of the Environmental Impacts of LLMs", "authors": ["Nora Graves", "Vitus Larrieu", "Yingyue Trace Zhang", "Joanne Peng", "Varun Nagaraj Rao", "Yuhan Liu", "Andrés Monroy-Hernández"], "categories": ["cs.HC"], "comment": "Published in Proceedings of the Extended Abstracts of the CHI\n  Conference on Human Factors in Computing System", "summary": "With the growth of AI, researchers are studying how to mitigate its\nenvironmental impact, primarily by proposing policy changes and increasing\nawareness among developers. However, research on AI end users is limited.\nTherefore, we introduce GPTFootprint, a browser extension that aims to increase\nconsumer awareness of the significant water and energy consumption of LLMs, and\nreduce unnecessary LLM usage. GPTFootprint displays a dynamically updating\nvisualization of the resources individual users consume through their ChatGPT\nqueries. After a user reaches a set query limit, a popup prompts them to take a\nbreak from ChatGPT. In a week-long user study, we found that GPTFootprint\nincreases people's awareness of environmental impact, but has limited success\nin decreasing ChatGPT usage. This research demonstrates the potential for\nindividual-level interventions to contribute to the broader goal of sustainable\nAI usage, and provides insights into the effectiveness of awareness-based\nbehavior modification strategies in the context of LLMs."}
{"id": "2505.23797", "pdf": "https://arxiv.org/pdf/2505.23797.pdf", "abs": "https://arxiv.org/abs/2505.23797", "title": "Detection of Suicidal Risk on Social Media: A Hybrid Model", "authors": ["Zaihan Yang", "Ryan Leonard", "Hien Tran", "Rory Driscoll", "Chadbourne Davis"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.SI"], "comment": null, "summary": "Suicidal thoughts and behaviors are increasingly recognized as a critical\nsocietal concern, highlighting the urgent need for effective tools to enable\nearly detection of suicidal risk. In this work, we develop robust machine\nlearning models that leverage Reddit posts to automatically classify them into\nfour distinct levels of suicide risk severity. We frame this as a multi-class\nclassification task and propose a RoBERTa-TF-IDF-PCA Hybrid model, integrating\nthe deep contextual embeddings from Robustly Optimized BERT Approach (RoBERTa),\na state-of-the-art deep learning transformer model, with the statistical\nterm-weighting of TF-IDF, further compressed with PCA, to boost the accuracy\nand reliability of suicide risk assessment. To address data imbalance and\noverfitting, we explore various data resampling techniques and data\naugmentation strategies to enhance model generalization. Additionally, we\ncompare our model's performance against that of using RoBERTa only, the BERT\nmodel and other traditional machine learning classifiers. Experimental results\ndemonstrate that the hybrid model can achieve improved performance, giving a\nbest weighted $F_{1}$ score of 0.7512."}
{"id": "2505.24126", "pdf": "https://arxiv.org/pdf/2505.24126.pdf", "abs": "https://arxiv.org/abs/2505.24126", "title": "How Students (Really) Use ChatGPT: Uncovering Experiences Among Undergraduate Students", "authors": ["Tawfiq Ammari", "Meilun Chen", "S M Mehedi Zaman", "Kiran Garimella"], "categories": ["cs.HC"], "comment": null, "summary": "This study investigates how undergraduate students engage with ChatGPT in\nself directed learning contexts. Analyzing naturalistic interaction logs, we\nidentify five dominant use categories of ChatGPT information seeking, content\ngeneration, language refinement, meta cognitive engagement, and conversational\nrepair. Behavioral modeling reveals that structured, goal driven tasks like\ncoding, multiple choice solving, and job application writing are strong\npredictors of continued use. Drawing on Self-Directed Learning (SDL) and the\nUses and Gratifications Theory (UGT), we show how students actively manage\nChatGPTs affordances and limitations through prompt adaptation, follow-ups, and\nemotional regulation. Rather than disengaging after breakdowns, students often\npersist through clarification and repair, treating the assistant as both tool\nand learning partner. We also offer design and policy recommendations to\nsupport transparent, responsive, and pedagogically grounded integration of\ngenerative AI in higher education."}
{"id": "2505.23798", "pdf": "https://arxiv.org/pdf/2505.23798.pdf", "abs": "https://arxiv.org/abs/2505.23798", "title": "My Answer Is NOT 'Fair': Mitigating Social Bias in Vision-Language Models via Fair and Biased Residuals", "authors": ["Jian Lan", "Yifei Fu", "Udo Schlegel", "Gengyuan Zhang", "Tanveer Hannan", "Haokun Chen", "Thomas Seidl"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Social bias is a critical issue in large vision-language models (VLMs), where\nfairness- and ethics-related problems harm certain groups of people in society.\nIt is unknown to what extent VLMs yield social bias in generative responses. In\nthis study, we focus on evaluating and mitigating social bias on both the\nmodel's response and probability distribution. To do so, we first evaluate four\nstate-of-the-art VLMs on PAIRS and SocialCounterfactuals datasets with the\nmultiple-choice selection task. Surprisingly, we find that models suffer from\ngenerating gender-biased or race-biased responses. We also observe that models\nare prone to stating their responses are fair, but indeed having mis-calibrated\nconfidence levels towards particular social groups. While investigating why\nVLMs are unfair in this study, we observe that VLMs' hidden layers exhibit\nsubstantial fluctuations in fairness levels. Meanwhile, residuals in each layer\nshow mixed effects on fairness, with some contributing positively while some\nlead to increased bias. Based on these findings, we propose a post-hoc method\nfor the inference stage to mitigate social bias, which is training-free and\nmodel-agnostic. We achieve this by ablating bias-associated residuals while\namplifying fairness-associated residuals on model hidden layers during\ninference. We demonstrate that our post-hoc method outperforms the competing\ntraining strategies, helping VLMs have fairer responses and more reliable\nconfidence levels."}
{"id": "2505.24195", "pdf": "https://arxiv.org/pdf/2505.24195.pdf", "abs": "https://arxiv.org/abs/2505.24195", "title": "WikiGap: Promoting Epistemic Equity by Surfacing Knowledge Gaps Between English Wikipedia and other Language Editions", "authors": ["Zining Wang", "Yuxuan Zhang", "Dongwook Yoon", "Nicholas Vincent", "Farhan Samir", "Vered Shwartz"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "With more than 11 times as many pageviews as the next, English Wikipedia\ndominates global knowledge access relative to other language editions. Readers\nare prone to assuming English Wikipedia as a superset of all language editions,\nleading many to prefer it even when their primary language is not English.\nOther language editions, however, comprise complementary facts rooted in their\nrespective cultures and media environments, which are marginalized in English\nWikipedia. While Wikipedia's user interface enables switching between language\neditions through its Interlanguage Link (ILL) system, it does not reveal to\nreaders that other language editions contain valuable, complementary\ninformation. We present WikiGap, a system that surfaces complementary facts\nsourced from other Wikipedias within the English Wikipedia interface.\nSpecifically, by combining a recent multilingual information-gap discovery\nmethod with a user-centered design, WikiGap enables access to complementary\ninformation from French, Russian, and Chinese Wikipedia. In a mixed-methods\nstudy (n=21), WikiGap significantly improved fact-finding accuracy, reduced\ntask time, and received a 32-point higher usability score relative to\nWikipedia's current ILL-based navigation system. Participants reported\nincreased awareness of the availability of complementary information in\nnon-English editions and reconsidered the completeness of English Wikipedia.\nWikiGap thus paves the way for improved epistemic equity across language\neditions."}
{"id": "2505.23799", "pdf": "https://arxiv.org/pdf/2505.23799.pdf", "abs": "https://arxiv.org/abs/2505.23799", "title": "Estimating LLM Consistency: A User Baseline vs Surrogate Metrics", "authors": ["Xiaoyuan Wu", "Weiran Lin", "Omer Akgul", "Lujo Bauer"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are prone to hallucinations and sensitive to\nprompt perturbations, often resulting in inconsistent or unreliable generated\ntext. Different methods have been proposed to mitigate such hallucinations and\nfragility -- one of them being measuring the consistency (the model's\nconfidence in the response, or likelihood of generating a similar response when\nresampled) of LLM responses. In previous work, measuring consistency often\nrelied on the probability of a response appearing within a pool of resampled\nresponses, or internal states or logits of responses. However, it is not yet\nclear how well these approaches approximate how humans perceive the consistency\nof LLM responses. We performed a user study (n=2,976) and found current methods\ntypically do not approximate users' perceptions of LLM consistency very well.\nWe propose a logit-based ensemble method for estimating LLM consistency, and we\nshow that this method matches the performance of the best-performing existing\nmetric in estimating human ratings of LLM consistency. Our results suggest that\nmethods of estimating LLM consistency without human evaluation are sufficiently\nimperfect that we suggest evaluation with human input be more broadly used."}
{"id": "2505.24246", "pdf": "https://arxiv.org/pdf/2505.24246.pdf", "abs": "https://arxiv.org/abs/2505.24246", "title": "Locating Risk: Task Designers and the Challenge of Risk Disclosure in RAI Content Work", "authors": ["Alice Qian Zhang", "Ryland Shaw", "Laura Dabbish", "Jina Suh", "Hong Shen"], "categories": ["cs.HC", "cs.CY"], "comment": "Under submission at CSCW 2026", "summary": "As AI systems are increasingly tested and deployed in open-ended and\nhigh-stakes domains, crowd workers are often tasked with responsible AI (RAI)\ncontent work. These tasks include labeling violent content, moderating\ndisturbing text, or simulating harmful behavior for red teaming exercises to\nshape AI system behaviors. While prior efforts have highlighted the risks to\nworker well-being associated with RAI content work, far less attention has been\npaid to how these risks are communicated to workers. Existing transparency\nframeworks and guidelines such as model cards, datasheets, and crowdworksheets\nfocus on documenting model information and dataset collection processes, but\nthey overlook an important aspect of disclosing well-being risks to workers. In\nthe absence of standard workflows or clear guidance, the consistent application\nof content warnings, consent flows, or other forms of well-being risk\ndisclosure remain unclear. This study investigates how task designers approach\nrisk disclosure in crowdsourced RAI tasks. Drawing on interviews with 23 task\ndesigners across academic and industry sectors, we examine how well-being risk\nis recognized, interpreted, and communicated in practice. Our findings surface\na need to support task designers in identifying and communicating well-being\nrisk not only to support crowdworker well-being but also to strengthen the\nethical integrity and technical efficacy of AI development pipelines."}
{"id": "2505.23801", "pdf": "https://arxiv.org/pdf/2505.23801.pdf", "abs": "https://arxiv.org/abs/2505.23801", "title": "SEMFED: Semantic-Aware Resource-Efficient Federated Learning for Heterogeneous NLP Tasks", "authors": ["Sajid Hussain", "Muhammad Sohail", "Nauman Ali Khan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "13 pages", "summary": "Background: Federated Learning (FL) has emerged as a promising paradigm for\ntraining machine learning models while preserving data privacy. However,\napplying FL to Natural Language Processing (NLP) tasks presents unique\nchallenges due to semantic heterogeneity across clients, vocabulary mismatches,\nand varying resource constraints on edge devices. Objectives: This paper\nintroduces SEMFED, a novel semantic-aware resource-efficient federated learning\nframework specifically designed for heterogeneous NLP tasks. Methods: SEMFED\nincorporates three key innovations: (1) a semantic-aware client selection\nmechanism that balances semantic diversity with resource constraints, (2)\nadaptive NLP-specific model architectures tailored to device capabilities while\npreserving semantic information, and (3) a communication-efficient semantic\nfeature compression technique that significantly reduces bandwidth\nrequirements. Results: Experimental results on various NLP classification tasks\ndemonstrate that SEMFED achieves an 80.5% reduction in communication costs\nwhile maintaining model accuracy above 98%, outperforming state-of-the-art FL\napproaches. Conclusion: SEMFED effectively manages heterogeneous client\nenvironments with varying computational resources, network reliability, and\nsemantic data distributions, making it particularly suitable for real-world\nfederated NLP deployments."}
{"id": "2505.24348", "pdf": "https://arxiv.org/pdf/2505.24348.pdf", "abs": "https://arxiv.org/abs/2505.24348", "title": "A 3D Mobile Crowdsensing Framework for Sustainable Urban Digital Twins", "authors": ["Taku Yamazaki", "Kaito Watanabe", "Tatsuya Kase", "Kenta Hasegawa", "Koki Saida", "Takumi Miyoshi"], "categories": ["cs.HC", "cs.CY"], "comment": "8 pages, 18 figures, 3 tables", "summary": "In this article, we propose a 3D mobile crowdsensing (3D-MCS) framework aimed\nat sustainable urban digital twins (UDTs). The framework comprises four key\nmechanisms: (1) the 3D-MCS mechanism, consisting of active and passive models;\n(2) the Geohash-based spatial information management mechanism; (3) the dynamic\npoint cloud integration mechanism for UDTs; and (4) the web-based real-time\nvisualizer for 3D-MCS and UDTs. The active sensing model features a gamified\n3D-MCS approach, where participants collect point cloud data through an\naugmented reality territory coloring game. In contrast, the passive sensing\nmodel employs a wearable 3D-MCS approach, where participants wear smartphones\naround their necks without disrupting daily activities. The spatial information\nmanagement mechanism efficiently partitions the space into regions using\nGeohash. The dynamic point cloud integration mechanism incorporates point\nclouds collected by 3D-MCS into UDTs through global and local point cloud\nregistration. Finally, we evaluated the proposed framework through real-world\nexperiments. We verified the effectiveness of the proposed 3D-MCS models from\nthe perspectives of subjective evaluation and data collection and analysis.\nFurthermore, we analyzed the performance of the dynamic point cloud integration\nusing a dataset."}
{"id": "2505.23802", "pdf": "https://arxiv.org/pdf/2505.23802.pdf", "abs": "https://arxiv.org/abs/2505.23802", "title": "MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks", "authors": ["Suhana Bedi", "Hejie Cui", "Miguel Fuentes", "Alyssa Unell", "Michael Wornow", "Juan M. Banda", "Nikesh Kotecha", "Timothy Keyes", "Yifan Mai", "Mert Oez", "Hao Qiu", "Shrey Jain", "Leonardo Schettini", "Mehr Kashyap", "Jason Alan Fries", "Akshay Swaminathan", "Philip Chung", "Fateme Nateghi", "Asad Aali", "Ashwin Nayak", "Shivam Vedak", "Sneha S. Jain", "Birju Patel", "Oluseyi Fayanju", "Shreya Shah", "Ethan Goh", "Dong-han Yao", "Brian Soetikno", "Eduardo Reis", "Sergios Gatidis", "Vasu Divi", "Robson Capasso", "Rachna Saralkar", "Chia-Chun Chiang", "Jenelle Jindal", "Tho Pham", "Faraz Ghoddusi", "Steven Lin", "Albert S. Chiou", "Christy Hong", "Mohana Roy", "Michael F. Gensheimer", "Hinesh Patel", "Kevin Schulman", "Dev Dash", "Danton Char", "Lance Downing", "Francois Grolleau", "Kameron Black", "Bethel Mieso", "Aydin Zahedivash", "Wen-wai Yim", "Harshita Sharma", "Tony Lee", "Hannah Kirsch", "Jennifer Lee", "Nerissa Ambers", "Carlene Lugtu", "Aditya Sharma", "Bilal Mawji", "Alex Alekseyev", "Vicky Zhou", "Vikas Kakkar", "Jarrod Helzer", "Anurang Revri", "Yair Bannett", "Roxana Daneshjou", "Jonathan Chen", "Emily Alsentzer", "Keith Morse", "Nirmal Ravi", "Nima Aghaeepour", "Vanessa Kennedy", "Akshay Chaudhari", "Thomas Wang", "Sanmi Koyejo", "Matthew P. Lungren", "Eric Horvitz", "Percy Liang", "Mike Pfeffer", "Nigam H. Shah"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While large language models (LLMs) achieve near-perfect scores on medical\nlicensing exams, these evaluations inadequately reflect the complexity and\ndiversity of real-world clinical practice. We introduce MedHELM, an extensible\nevaluation framework for assessing LLM performance for medical tasks with three\nkey contributions. First, a clinician-validated taxonomy spanning 5 categories,\n22 subcategories, and 121 tasks developed with 29 clinicians. Second, a\ncomprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly\nformulated) providing complete coverage of all categories and subcategories in\nthe taxonomy. Third, a systematic comparison of LLMs with improved evaluation\nmethods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9\nfrontier LLMs, using the 35 benchmarks, revealed significant performance\nvariation. Advanced reasoning models (DeepSeek R1: 66% win-rate; o3-mini: 64%\nwin-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved\ncomparable results at 40% lower estimated computational cost. On a normalized\naccuracy scale (0-1), most models performed strongly in Clinical Note\nGeneration (0.73-0.85) and Patient Communication & Education (0.78-0.83),\nmoderately in Medical Research Assistance (0.65-0.75), and generally lower in\nClinical Decision Support (0.56-0.72) and Administration & Workflow\n(0.53-0.63). Our LLM-jury evaluation method achieved good agreement with\nclinician ratings (ICC = 0.47), surpassing both average clinician-clinician\nagreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and\nBERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top\nmodels at lower estimated cost. These findings highlight the importance of\nreal-world, task-specific evaluation for medical use of LLMs and provides an\nopen source framework to enable this."}
{"id": "2505.24658", "pdf": "https://arxiv.org/pdf/2505.24658.pdf", "abs": "https://arxiv.org/abs/2505.24658", "title": "Can LLMs and humans be friends? Uncovering factors affecting human-AI intimacy formation", "authors": ["Yeseon Hong", "Junhyuk Choi", "Minju Kim", "Bugeun Kim"], "categories": ["cs.HC", "H.5.2; I.2.7"], "comment": "30 pages, 2figures", "summary": "Large language models (LLMs) are increasingly being used in conversational\nroles, yet little is known about how intimacy emerges in human-LLM\ninteractions. Although previous work emphasized the importance of\nself-disclosure in human-chatbot interaction, it is questionable whether\ngradual and reciprocal self-disclosure is also helpful in human-LLM\ninteraction. Thus, this study examined three possible aspects contributing to\nintimacy formation: gradual self-disclosure, reciprocity, and naturalness.\nStudy 1 explored the impact of mutual, gradual self-disclosure with 29 users\nand a vanilla LLM. Study 2 adopted self-criticism methods for more natural\nresponses and conducted a similar experiment with 53 users. Results indicate\nthat gradual self-disclosure significantly enhances perceived social intimacy,\nregardless of persona reciprocity. Moreover, participants perceived utterances\ngenerated with self-criticism as more natural compared to those of vanilla\nLLMs; self-criticism fostered higher intimacy in early stages. Also, we\nobserved that excessive empathetic expressions occasionally disrupted\nimmersion, pointing to the importance of response calibration during intimacy\nformation."}
{"id": "2505.23804", "pdf": "https://arxiv.org/pdf/2505.23804.pdf", "abs": "https://arxiv.org/abs/2505.23804", "title": "Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause Frequencies", "authors": ["Terrance Liu", "Shuyi Wang", "Daniel Preotiuc-Pietro", "Yash Chandarana", "Chirag Gupta"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "While large language models (LLMs) achieve strong performance on text-to-SQL\nparsing, they sometimes exhibit unexpected failures in which they are\nconfidently incorrect. Building trustworthy text-to-SQL systems thus requires\neliciting reliable uncertainty measures from the LLM. In this paper, we study\nthe problem of providing a calibrated confidence score that conveys the\nlikelihood of an output query being correct. Our work is the first to establish\na benchmark for post-hoc calibration of LLM-based text-to-SQL parsing. In\nparticular, we show that Platt scaling, a canonical method for calibration,\nprovides substantial improvements over directly using raw model output\nprobabilities as confidence scores. Furthermore, we propose a method for\ntext-to-SQL calibration that leverages the structured nature of SQL queries to\nprovide more granular signals of correctness, named \"sub-clause frequency\"\n(SCF) scores. Using multivariate Platt scaling (MPS), our extension of the\ncanonical Platt scaling technique, we combine individual SCF scores into an\noverall accurate and calibrated score. Empirical evaluation on two popular\ntext-to-SQL datasets shows that our approach of combining MPS and SCF yields\nfurther improvements in calibration and the related task of error detection\nover traditional Platt scaling."}
{"id": "2505.23799", "pdf": "https://arxiv.org/pdf/2505.23799.pdf", "abs": "https://arxiv.org/abs/2505.23799", "title": "Estimating LLM Consistency: A User Baseline vs Surrogate Metrics", "authors": ["Xiaoyuan Wu", "Weiran Lin", "Omer Akgul", "Lujo Bauer"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are prone to hallucinations and sensitive to\nprompt perturbations, often resulting in inconsistent or unreliable generated\ntext. Different methods have been proposed to mitigate such hallucinations and\nfragility -- one of them being measuring the consistency (the model's\nconfidence in the response, or likelihood of generating a similar response when\nresampled) of LLM responses. In previous work, measuring consistency often\nrelied on the probability of a response appearing within a pool of resampled\nresponses, or internal states or logits of responses. However, it is not yet\nclear how well these approaches approximate how humans perceive the consistency\nof LLM responses. We performed a user study (n=2,976) and found current methods\ntypically do not approximate users' perceptions of LLM consistency very well.\nWe propose a logit-based ensemble method for estimating LLM consistency, and we\nshow that this method matches the performance of the best-performing existing\nmetric in estimating human ratings of LLM consistency. Our results suggest that\nmethods of estimating LLM consistency without human evaluation are sufficiently\nimperfect that we suggest evaluation with human input be more broadly used."}
{"id": "2505.23806", "pdf": "https://arxiv.org/pdf/2505.23806.pdf", "abs": "https://arxiv.org/abs/2505.23806", "title": "MedOrchestra: A Hybrid Cloud-Local LLM Approach for Clinical Data Interpretation", "authors": ["Sihyeon Lee", "Hyunjoo Song", "Jong-chan Lee", "Yoon Jin Lee", "Boram Lee", "Hee-Eon Lim", "Dongyeong Kim", "Jinwook Seo", "Bohyoung Kim"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Deploying large language models (LLMs) in clinical settings faces critical\ntrade-offs: cloud LLMs, with their extensive parameters and superior\nperformance, pose risks to sensitive clinical data privacy, while local LLMs\npreserve privacy but often fail at complex clinical interpretation tasks. We\npropose MedOrchestra, a hybrid framework where a cloud LLM decomposes complex\nclinical tasks into manageable subtasks and prompt generation, while a local\nLLM executes these subtasks in a privacy-preserving manner. Without accessing\nclinical data, the cloud LLM generates and validates subtask prompts using\nclinical guidelines and synthetic test cases. The local LLM executes subtasks\nlocally and synthesizes outputs generated by the cloud LLM. We evaluate\nMedOrchestra on pancreatic cancer staging using 100 radiology reports under\nNCCN guidelines. On free-text reports, MedOrchestra achieves 70.21% accuracy,\noutperforming local model baselines (without guideline: 48.94%, with guideline:\n56.59%) and board-certified clinicians (gastroenterologists: 59.57%, surgeons:\n65.96%, radiologists: 55.32%). On structured reports, MedOrchestra reaches\n85.42% accuracy, showing clear superiority across all settings."}
{"id": "2505.23856", "pdf": "https://arxiv.org/pdf/2505.23856.pdf", "abs": "https://arxiv.org/abs/2505.23856", "title": "OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities", "authors": ["Sahil Verma", "Keegan Hines", "Jeff Bilmes", "Charlotte Siska", "Luke Zettlemoyer", "Hila Gonen", "Chandan Singh"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "The emerging capabilities of large language models (LLMs) have sparked\nconcerns about their immediate potential for harmful misuse. The core approach\nto mitigate these concerns is the detection of harmful queries to the model.\nCurrent detection approaches are fallible, and are particularly susceptible to\nattacks that exploit mismatched generalization of model capabilities (e.g.,\nprompts in low-resource languages or prompts provided in non-text modalities\nsuch as image and audio). To tackle this challenge, we propose OMNIGUARD, an\napproach for detecting harmful prompts across languages and modalities. Our\napproach (i) identifies internal representations of an LLM/MLLM that are\naligned across languages or modalities and then (ii) uses them to build a\nlanguage-agnostic or modality-agnostic classifier for detecting harmful\nprompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57\\%\nover the strongest baseline in a multilingual setting, by 20.44\\% for\nimage-based prompts, and sets a new SOTA for audio-based prompts. By\nrepurposing embeddings computed during generation, OMNIGUARD is also very\nefficient ($\\approx 120 \\times$ faster than the next fastest baseline). Code\nand data are available at: https://github.com/vsahil/OmniGuard."}
{"id": "2505.23807", "pdf": "https://arxiv.org/pdf/2505.23807.pdf", "abs": "https://arxiv.org/abs/2505.23807", "title": "DLP: Dynamic Layerwise Pruning in Large Language Models", "authors": ["Yuli Chen", "Bo Cheng", "Jiale Han", "Yingying Zhang", "Yingting Li", "Shuhao Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICML 2025", "summary": "Pruning has recently been widely adopted to reduce the parameter scale and\nimprove the inference efficiency of Large Language Models (LLMs). Mainstream\npruning techniques often rely on uniform layerwise pruning strategies, which\ncan lead to severe performance degradation at high sparsity levels. Recognizing\nthe varying contributions of different layers in LLMs, recent studies have\nshifted their focus toward non-uniform layerwise pruning. However, these\napproaches often rely on pre-defined values, which can result in suboptimal\nperformance. To overcome these limitations, we propose a novel method called\nDynamic Layerwise Pruning (DLP). This approach adaptively determines the\nrelative importance of each layer by integrating model weights with input\nactivation information, assigning pruning rates accordingly. Experimental\nresults show that DLP effectively preserves model performance at high sparsity\nlevels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the\nperplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7%\ncompared to state-of-the-art methods. Moreover, DLP is compatible with various\nexisting LLM compression techniques and can be seamlessly integrated into\nParameter-Efficient Fine-Tuning (PEFT). We release the code at\nhttps://github.com/ironartisan/DLP to facilitate future research."}
{"id": "2505.23984", "pdf": "https://arxiv.org/pdf/2505.23984.pdf", "abs": "https://arxiv.org/abs/2505.23984", "title": "Improved Accuracy in Pelvic Tumor Resections Using a Real-Time Vision-Guided Surgical System", "authors": ["Vahid Danesh", "Paul Arauz", "Maede Boroji", "Andrew Zhu", "Mia Cottone", "Elaine Gould", "Fazel A. Khan", "Imin Kao"], "categories": ["eess.IV", "cs.HC"], "comment": "9 Pages, 5 figures, Submitted to Journal of Orthopaedic Research", "summary": "Pelvic bone tumor resections remain significantly challenging due to complex\nthree-dimensional anatomy and limited surgical visualization. Current\nnavigation systems and patient-specific instruments, while accurate, present\nlimitations including high costs, radiation exposure, workflow disruption, long\nproduction time, and lack of reusability. This study evaluates a real-time\nvision-guided surgical system combined with modular jigs to improve accuracy in\npelvic bone tumor resections. A vision-guided surgical system combined with\nmodular cutting jigs and real-time optical tracking was developed and\nvalidated. Five female pelvis sawbones were used, with each hemipelvis randomly\nassigned to either the vision-guided and modular jig system or traditional\nfreehand method. A total of twenty resection planes were analyzed for each\nmethod. Accuracy was assessed by measuring distance and angular deviations from\nthe planned resection planes. The vision-guided and modular jig system\nsignificantly improved resection accuracy compared to the freehand method,\nreducing the mean distance deviation from 2.07 $\\pm$ 1.71 mm to 1.01 $\\pm$ 0.78\nmm (p=0.0193). In particular, all specimens resected using the vision-guided\nsystem exhibited errors of less than 3 mm. Angular deviations also showed\nsignificant improvements with roll angle deviation reduced from 15.36 $\\pm$\n17.57$^\\circ$ to 4.21 $\\pm$ 3.46$^\\circ$ (p=0.0275), and pitch angle deviation\ndecreased from 6.17 $\\pm$ 4.58$^\\circ$ to 1.84 $\\pm$ 1.48$^\\circ$ (p<0.001).\nThe proposed vision-guided and modular jig system significantly improves the\naccuracy of pelvic bone tumor resections while maintaining workflow efficiency.\nThis cost-effective solution provides real-time guidance without the need for\nreferencing external monitors, potentially improving surgical outcomes in\ncomplex pelvic bone tumor cases."}
{"id": "2505.23808", "pdf": "https://arxiv.org/pdf/2505.23808.pdf", "abs": "https://arxiv.org/abs/2505.23808", "title": "DenseLoRA: Dense Low-Rank Adaptation of Large Language Models", "authors": ["Lin Mu", "Xiaoyu Wang", "Li Ni", "Yang Li", "Zhize Wu", "Peiquan Jin", "Yiwen Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Low-rank adaptation (LoRA) has been developed as an efficient approach for\nadapting large language models (LLMs) by fine-tuning two low-rank matrices,\nthereby reducing the number of trainable parameters. However, prior research\nindicates that many of the weights in these matrices are redundant, leading to\ninefficiencies in parameter utilization. To address this limitation, we\nintroduce Dense Low-Rank Adaptation (DenseLoRA), a novel approach that enhances\nparameter efficiency while achieving superior performance compared to LoRA.\nDenseLoRA builds upon the concept of representation fine-tuning, incorporating\na single Encoder-Decoder to refine and compress hidden representations across\nall adaptation layers before applying adaptation. Instead of relying on two\nredundant low-rank matrices as in LoRA, DenseLoRA adapts LLMs through a dense\nlow-rank matrix, improving parameter utilization and adaptation efficiency. We\nevaluate DenseLoRA on various benchmarks, showing that it achieves 83.8%\naccuracy with only 0.01% of trainable parameters, compared to LoRA's 80.8%\naccuracy with 0.70% of trainable parameters on LLaMA3-8B. Additionally, we\nconduct extensive experiments to systematically assess the impact of\nDenseLoRA's components on overall model performance. Code is available at\nhttps://github.com/mulin-ahu/DenseLoRA."}
{"id": "2505.24035", "pdf": "https://arxiv.org/pdf/2505.24035.pdf", "abs": "https://arxiv.org/abs/2505.24035", "title": "Bridging the Gap: Enhancing Digital Accessibility for Medicaid Populations in Telehealth Adoption", "authors": ["Vishnu Ramineni", "Aditya Gupta", "Balakrishna Pothineni", "Isan Sahoo", "Shivareddy Devarapalli", "Balaji Shesharao Ingole"], "categories": ["cs.CY", "cs.HC"], "comment": "16 pages", "summary": "The swift evolution of telehealth has revolutionized how medical\nprofessionals deliver healthcare services and boost convenience and\naccessibility. Yet, the Medicaid population encounters several impediments in\nutilizing facilities especially owing to poor internet connectivity, less\nawareness about digital platforms, and a shortage of assistive technologies.\nThe paper aims to explicate key factors behind digital accessibility for\nMedicaid populations and expounds robust solutions to eradicate these\nchallenges. Through inclusive design ideas, AI-assisted technologies, and\nall-encompassing policies by the concerned authorities, healthcare\nprofessionals can enhance usability and efficacy and thus better serve the\nneedy. This revolution not only enhances convenience but also expands access,\nmainly for underserved groups such as rural populations or those with mobility\nissues, thereby ensuring inclusivity and flexibility in the healthcare domain.\nBesides, the paper highlights the vitality of collaboration between healthcare\nprofessionals, policymakers, and tech developers in unveiling the accessibility\nand usability impediments. What else helps in minimizing healthcare differences\nand enhancing patient outcomes is guaranteeing equitable access to telehealth\nfor Medicaid beneficiaries. The paper systematically offers major\nrecommendations to increase digital accessibility in telehealth, thereby\ncreating a patient-oriented and all-encompassing healthcare system."}
{"id": "2505.23809", "pdf": "https://arxiv.org/pdf/2505.23809.pdf", "abs": "https://arxiv.org/abs/2505.23809", "title": "LLM-Driven E-Commerce Marketing Content Optimization: Balancing Creativity and Conversion", "authors": ["Haowei Yang", "Haotian Lyu", "Tianle Zhang", "Dingzhou Wang", "Yushang Zhao"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "As e-commerce competition intensifies, balancing creative content with\nconversion effectiveness becomes critical. Leveraging LLMs' language generation\ncapabilities, we propose a framework that integrates prompt engineering,\nmulti-objective fine-tuning, and post-processing to generate marketing copy\nthat is both engaging and conversion-driven. Our fine-tuning method combines\nsentiment adjustment, diversity enhancement, and CTA embedding. Through offline\nevaluations and online A/B tests across categories, our approach achieves a\n12.5 % increase in CTR and an 8.3 % increase in CVR while maintaining content\nnovelty. This provides a practical solution for automated copy generation and\nsuggests paths for future multimodal, real-time personalization."}
{"id": "2505.24096", "pdf": "https://arxiv.org/pdf/2505.24096.pdf", "abs": "https://arxiv.org/abs/2505.24096", "title": "Towards Tangible Immersion for Cobot Programming-by-Demonstration: Visual, Tactile and Haptic Interfaces for Mixed-Reality Cobot Automation in Semiconductor Manufacturing", "authors": ["David I. Gonzalez-Aguirre", "Javier Felip Leon", "Javier Felix-Rendon", "Roderico Garcia-Leal", "Julio C. Zamora Esquivel"], "categories": ["cs.RO", "cs.HC"], "comment": "4 Pages, 5 Figures", "summary": "Sensor-based reactive and hybrid approaches have proven a promising line of\nstudy to address imperfect knowledge in grasping and manipulation. However the\nreactive approaches are usually tightly coupled to a particular embodiment\nmaking transfer of knowledge difficult. This paper proposes a paradigm for\nmodeling and execution of reactive manipulation actions, which makes knowledge\ntransfer to different embodiments possible while retaining the reactive\ncapabilities of the embodiments. The proposed approach extends the idea of\ncontrol primitives coordinated by a state machine by introducing an embodiment\nindependent layer of abstraction. Abstract manipulation primitives constitute a\nvocabulary of atomic, embodiment independent actions, which can be coordinated\nusing state machines to describe complex actions. To obtain embodiment specific\nmodels, the abstract state machines are automatically translated to embodiment\nspecific models, such that full capabilities of each platform can be utilized.\nThe strength of the manipulation primitives paradigm is demonstrated by\ndeveloping a set of corresponding embodiment specific primitives for object\ntransport, including a complex reactive grasping primitive. The robustness of\nthe approach is experimentally studied in emptying of a box filled with several\nunknown objects. The embodiment independence is studied by performing a\nmanipulation task on two different platforms using the same abstract\ndescription."}
{"id": "2505.23810", "pdf": "https://arxiv.org/pdf/2505.23810.pdf", "abs": "https://arxiv.org/abs/2505.23810", "title": "MARS-Bench: A Multi-turn Athletic Real-world Scenario Benchmark for Dialogue Evaluation", "authors": ["Chenghao Yang", "Yinbo Luo", "Zhoufutu Wen", "Qi Chu", "Tao Gong", "Longxiang Liu", "Kaiyuan Zhang", "Jianpeng Jiao", "Ge Zhang", "Wenhao Huang", "Nenghai Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "29 pages, 13 figures", "summary": "Large Language Models (\\textbf{LLMs}), e.g. ChatGPT, have been widely adopted\nin real-world dialogue applications. However, LLMs' robustness, especially in\nhandling long complex dialogue sessions, including frequent motivation\ntransfer, sophisticated cross-turn dependency, is criticized all along.\nNevertheless, no existing benchmarks can fully reflect these weaknesses. We\npresent \\textbf{MARS-Bench}, a \\textbf{M}ulti-turn \\textbf{A}thletic\n\\textbf{R}eal-world \\textbf{S}cenario Dialogue \\textbf{Bench}mark, designed to\nremedy the gap. MARS-Bench is constructed from play-by-play text commentary so\nto feature realistic dialogues specifically designed to evaluate three critical\naspects of multi-turn conversations: Ultra Multi-turn, Interactive Multi-turn,\nand Cross-turn Tasks. Extensive experiments on MARS-Bench also reveal that\nclosed-source LLMs significantly outperform open-source alternatives, explicit\nreasoning significantly boosts LLMs' robustness on handling long complex\ndialogue sessions, and LLMs indeed face significant challenges when handling\nmotivation transfer and sophisticated cross-turn dependency. Moreover, we\nprovide mechanistic interpretability on how attention sinks due to special\ntokens lead to LLMs' performance degradation when handling long complex\ndialogue sessions based on attention visualization experiment in\nQwen2.5-7B-Instruction."}
{"id": "2505.24115", "pdf": "https://arxiv.org/pdf/2505.24115.pdf", "abs": "https://arxiv.org/abs/2505.24115", "title": "FeatureSense: Protecting Speaker Attributes in Always-On Audio Sensing System", "authors": ["Bhawana Chhaglani", "Sarmistha Sarna Gomasta", "Yuvraj Agarwal", "Jeremy Gummeson", "Prashant Shenoy"], "categories": ["cs.SD", "cs.HC", "eess.AS"], "comment": null, "summary": "Audio is a rich sensing modality that is useful for a variety of human\nactivity recognition tasks. However, the ubiquitous nature of smartphones and\nsmart speakers with always-on microphones has led to numerous privacy concerns\nand a lack of trust in deploying these audio-based sensing systems. This paper\naddresses this critical challenge of preserving user privacy when using audio\nfor sensing applications while maintaining utility. While prior work focuses\nprimarily on protecting recoverable speech content, we show that sensitive\nspeaker-specific attributes such as age and gender can still be inferred after\nmasking speech and propose a comprehensive privacy evaluation framework to\nassess this speaker attribute leakage. We design and implement FeatureSense, an\nopen-source library that provides a set of generalizable privacy-aware audio\nfeatures that can be used for wide range of sensing applications. We present an\nadaptive task-specific feature selection algorithm that optimizes the\nprivacy-utility-cost trade-off based on the application requirements. Through\nour extensive evaluation, we demonstrate the high utility of FeatureSense\nacross a diverse set of sensing tasks. Our system outperforms existing privacy\ntechniques by 60.6% in preserving user-specific privacy. This work provides a\nfoundational framework for ensuring trust in audio sensing by enabling\neffective privacy-aware audio classification systems."}
{"id": "2505.23811", "pdf": "https://arxiv.org/pdf/2505.23811.pdf", "abs": "https://arxiv.org/abs/2505.23811", "title": "LayerIF: Estimating Layer Quality for Large Language Models using Influence Functions", "authors": ["Hadi Askari", "Shivanshu Gupta", "Fei Wang", "Anshuman Chhabra", "Muhao Chen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under Review", "summary": "Pretrained Large Language Models (LLMs) achieve strong performance across a\nwide range of tasks, yet exhibit substantial variability in the various layers'\ntraining quality with respect to specific downstream applications, limiting\ntheir downstream performance.It is therefore critical to estimate layer-wise\ntraining quality in a manner that accounts for both model architecture and\ntraining data. However, existing approaches predominantly rely on model-centric\nheuristics (such as spectral statistics, outlier detection, or uniform\nallocation) while overlooking the influence of data. To address these\nlimitations, we propose LayerIF, a data-driven framework that leverages\nInfluence Functions to quantify the training quality of individual layers in a\nprincipled and task-sensitive manner. By isolating each layer's gradients and\nmeasuring the sensitivity of the validation loss to training examples by\ncomputing layer-wise influences, we derive data-driven estimates of layer\nimportance. Notably, our method produces task-specific layer importance\nestimates for the same LLM, revealing how layers specialize for different\ntest-time evaluation tasks. We demonstrate the utility of our scores by\nleveraging them for two downstream applications: (a) expert allocation in\nLoRA-MoE architectures and (b) layer-wise sparsity distribution for LLM\npruning. Experiments across multiple LLM architectures demonstrate that our\nmodel-agnostic, influence-guided allocation leads to consistent gains in task\nperformance."}
{"id": "2505.24255", "pdf": "https://arxiv.org/pdf/2505.24255.pdf", "abs": "https://arxiv.org/abs/2505.24255", "title": "Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games", "authors": ["Neemesh Yadav", "Palakorn Achananuparp", "Jing Jiang", "Ee-Peng Lim"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "17 pages, 1 figure, 6 tables", "summary": "Large Language Models (LLMs) have shown potential in simulating human\nbehaviors and performing theory-of-mind (ToM) reasoning, a crucial skill for\ncomplex social interactions. In this study, we investigate the role of ToM\nreasoning in aligning agentic behaviors with human norms in negotiation tasks,\nusing the ultimatum game as a controlled environment. We initialized LLM agents\nwith different prosocial beliefs (including Greedy, Fair, and Selfless) and\nreasoning methods like chain-of-thought (CoT) and varying ToM levels, and\nexamined their decision-making processes across diverse LLMs, including\nreasoning models like o3-mini and DeepSeek-R1 Distilled Qwen 32B. Results from\n2,700 simulations indicated that ToM reasoning enhances behavior alignment,\ndecision-making consistency, and negotiation outcomes. Consistent with previous\nfindings, reasoning models exhibit limited capability compared to models with\nToM reasoning, different roles of the game benefits with different orders of\nToM reasoning. Our findings contribute to the understanding of ToM's role in\nenhancing human-AI interaction and cooperative decision-making. The code used\nfor our experiments can be found at https://github.com/Stealth-py/UltimatumToM."}
{"id": "2505.23812", "pdf": "https://arxiv.org/pdf/2505.23812.pdf", "abs": "https://arxiv.org/abs/2505.23812", "title": "Emotion-aware Dual Cross-Attentive Neural Network with Label Fusion for Stance Detection in Misinformative Social Media Content", "authors": ["Lata Pangtey", "Mohammad Zia Ur Rehman", "Prasad Chaudhari", "Shubhi Bansal", "Nagendra Kumar"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid evolution of social media has generated an overwhelming volume of\nuser-generated content, conveying implicit opinions and contributing to the\nspread of misinformation. The method aims to enhance the detection of stance\nwhere misinformation can polarize user opinions. Stance detection has emerged\nas a crucial approach to effectively analyze underlying biases in shared\ninformation and combating misinformation. This paper proposes a novel method\nfor \\textbf{S}tance \\textbf{P}rediction through a \\textbf{L}abel-fused dual\ncross-\\textbf{A}ttentive \\textbf{E}motion-aware neural \\textbf{Net}work\n(SPLAENet) in misinformative social media user-generated content. The proposed\nmethod employs a dual cross-attention mechanism and a hierarchical attention\nnetwork to capture inter and intra-relationships by focusing on the relevant\nparts of source text in the context of reply text and vice versa. We\nincorporate emotions to effectively distinguish between different stance\ncategories by leveraging the emotional alignment or divergence between the\ntexts. We also employ label fusion that uses distance-metric learning to align\nextracted features with stance labels, improving the method's ability to\naccurately distinguish between stances. Extensive experiments demonstrate the\nsignificant improvements achieved by SPLAENet over existing state-of-the-art\nmethods. SPLAENet demonstrates an average gain of 8.92\\% in accuracy and\n17.36\\% in F1-score on the RumourEval dataset. On the SemEval dataset, it\nachieves average gains of 7.02\\% in accuracy and 10.92\\% in F1-score. On the\nP-stance dataset, it demonstrates average gains of 10.03\\% in accuracy and\n11.18\\% in F1-score. These results validate the effectiveness of the proposed\nmethod for stance detection in the context of misinformative social media\ncontent."}
{"id": "2505.24266", "pdf": "https://arxiv.org/pdf/2505.24266.pdf", "abs": "https://arxiv.org/abs/2505.24266", "title": "SignBot: Learning Human-to-Humanoid Sign Language Interaction", "authors": ["Guanren Qiao", "Sixu Lin", "Ronglai Zuo Zhizheng Wu", "Kui Jia", "Guiliang Liu"], "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "Sign language is a natural and visual form of language that uses movements\nand expressions to convey meaning, serving as a crucial means of communication\nfor individuals who are deaf or hard-of-hearing (DHH). However, the number of\npeople proficient in sign language remains limited, highlighting the need for\ntechnological advancements to bridge communication gaps and foster interactions\nwith minorities. Based on recent advancements in embodied humanoid robots, we\npropose SignBot, a novel framework for human-robot sign language interaction.\nSignBot integrates a cerebellum-inspired motion control component and a\ncerebral-oriented module for comprehension and interaction. Specifically,\nSignBot consists of: 1) Motion Retargeting, which converts human sign language\ndatasets into robot-compatible kinematics; 2) Motion Control, which leverages a\nlearning-based paradigm to develop a robust humanoid control policy for\ntracking sign language gestures; and 3) Generative Interaction, which\nincorporates translator, responser, and generator of sign language, thereby\nenabling natural and effective communication between robots and humans.\nSimulation and real-world experimental results demonstrate that SignBot can\neffectively facilitate human-robot interaction and perform sign language\nmotions with diverse robots and datasets. SignBot represents a significant\nadvancement in automatic sign language interaction on embodied humanoid robot\nplatforms, providing a promising solution to improve communication\naccessibility for the DHH community."}
{"id": "2505.23815", "pdf": "https://arxiv.org/pdf/2505.23815.pdf", "abs": "https://arxiv.org/abs/2505.23815", "title": "Aligning LLMs by Predicting Preferences from User Writing Samples", "authors": ["Stéphane Aroca-Ouellette", "Natalie Mackraz", "Barry-John Theobald", "Katherine Metcalf"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ICML 2025. 32 pages total: 9 main, 2 references, 21\n  appendix. arXiv admin note: substantial text overlap with arXiv:2410.06273", "summary": "Accommodating human preferences is essential for creating aligned LLM agents\nthat deliver personalized and effective interactions. Recent work has shown the\npotential for LLMs acting as writing agents to infer a description of user\npreferences. Agent alignment then comes from conditioning on the inferred\npreference description. However, existing methods often produce generic\npreference descriptions that fail to capture the unique and individualized\nnature of human preferences. This paper introduces PROSE, a method designed to\nenhance the precision of preference descriptions inferred from user writing\nsamples. PROSE incorporates two key elements: (1) iterative refinement of\ninferred preferences, and (2) verification of inferred preferences across\nmultiple user writing samples. We evaluate PROSE with several LLMs (i.e.,\nQwen2.5 7B and 72B Instruct, GPT-mini, and GPT-4o) on a summarization and an\nemail writing task. We find that PROSE more accurately infers nuanced human\npreferences, improving the quality of the writing agent's generations over\nCIPHER (a state-of-the-art method for inferring preferences) by 33\\%. Lastly,\nwe demonstrate that ICL and PROSE are complementary methods, and combining them\nprovides up to a 9\\% improvement over ICL alone."}
{"id": "2505.24301", "pdf": "https://arxiv.org/pdf/2505.24301.pdf", "abs": "https://arxiv.org/abs/2505.24301", "title": "Category-aware EEG image generation based on wavelet transform and contrast semantic loss", "authors": ["Enshang Zhang", "Zhicheng Zhang", "Takashi Hanakawa"], "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Reconstructing visual stimuli from EEG signals is a crucial step in realizing\nbrain-computer interfaces. In this paper, we propose a transformer-based EEG\nsignal encoder integrating the Discrete Wavelet Transform (DWT) and the gating\nmechanism. Guided by the feature alignment and category-aware fusion losses,\nthis encoder is used to extract features related to visual stimuli from EEG\nsignals. Subsequently, with the aid of a pre-trained diffusion model, these\nfeatures are reconstructed into visual stimuli. To verify the effectiveness of\nthe model, we conducted EEG-to-image generation and classification tasks using\nthe THINGS-EEG dataset. To address the limitations of quantitative analysis at\nthe semantic level, we combined WordNet-based classification and semantic\nsimilarity metrics to propose a novel semantic-based score, emphasizing the\nability of our model to transfer neural activities into visual representations.\nExperimental results show that our model significantly improves semantic\nalignment and classification accuracy, which achieves a maximum single-subject\naccuracy of 43\\%, outperforming other state-of-the-art methods. The source code\nand supplementary material is available at\nhttps://github.com/zes0v0inn/DWT_EEG_Reconstruction/tree/main."}
{"id": "2505.23816", "pdf": "https://arxiv.org/pdf/2505.23816.pdf", "abs": "https://arxiv.org/abs/2505.23816", "title": "A Course Correction in Steerability Evaluation: Revealing Miscalibration and Side Effects in LLMs", "authors": ["Trenton Chang", "Tobias Schnabel", "Adith Swaminathan", "Jenna Wiens"], "categories": ["cs.CL", "cs.LG"], "comment": "10 pages, 8 figures. 26 pages of references and supplementary\n  material, 20 additional figures", "summary": "Despite advances in large language models (LLMs) on reasoning and\ninstruction-following benchmarks, it remains unclear whether they can reliably\nproduce outputs aligned with a broad variety of user goals, a concept we refer\nto as steerability. The abundance of methods proposed to modify LLM behavior\nmakes it unclear whether current LLMs are already steerable, or require further\nintervention. In particular, LLMs may exhibit (i) poor coverage, where rare\nuser goals are underrepresented; (ii) miscalibration, where models overshoot\nrequests; and (iii) side effects, where changes to one dimension of text\ninadvertently affect others. To systematically evaluate these failures, we\nintroduce a framework based on a multi-dimensional goal space that models user\ngoals and LLM outputs as vectors with dimensions corresponding to text\nattributes (e.g., reading difficulty). Applied to a text-rewriting task, we\nfind that current LLMs struggle with steerability, as side effects are\npersistent. Interventions to improve steerability, such as prompt engineering,\nbest-of-$N$ sampling, and reinforcement learning fine-tuning, have varying\neffectiveness, yet side effects remain problematic. Our findings suggest that\neven strong LLMs struggle with steerability, and existing alignment strategies\nmay be insufficient. We open-source our steerability evaluation framework at\nhttps://github.com/MLD3/steerability."}
{"id": "2505.24681", "pdf": "https://arxiv.org/pdf/2505.24681.pdf", "abs": "https://arxiv.org/abs/2505.24681", "title": "Generative Knowledge Production Pipeline Driven by Academic Influencers", "authors": ["Katalin Feher", "Marton Demeter"], "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.SI", "1.2, J.4, K.4"], "comment": "15 pages, 1 figure, 2 tables, Horizon Europe NGI funding", "summary": "Generative AI transforms knowledge production, validation, and dissemination,\nraising academic integrity and credibility concerns. This study examines 53\nacademic influencer videos that reached 5.3 million viewers to identify an\nemerging, structured, implementation-ready pipeline balancing originality,\nethical compliance, and human-AI collaboration despite the disruptive impacts.\nFindings highlight generative AI's potential to automate publication workflows\nand democratize participation in knowledge production while challenging\ntraditional scientific norms. Academic influencers emerge as key intermediaries\nin this paradigm shift, connecting bottom-up practices with institutional\npolicies to improve adaptability. Accordingly, the study proposes a generative\npublication production pipeline and a policy framework for co-intelligence\nadaptation and reinforcing credibility-centered standards in AI-powered\nresearch. These insights support scholars, educators, and policymakers in\nunderstanding AI's transformative impact by advocating responsible and\ninnovation-driven knowledge production. Additionally, they reveal pathways for\nautomating best practices, optimizing scholarly workflows, and fostering\ncreativity in academic research and publication."}
{"id": "2505.23818", "pdf": "https://arxiv.org/pdf/2505.23818.pdf", "abs": "https://arxiv.org/abs/2505.23818", "title": "Ratas framework: A comprehensive genai-based approach to rubric-based marking of real-world textual exams", "authors": ["Masoud Safilian", "Amin Beheshti", "Stephen Elbourn"], "categories": ["cs.CL"], "comment": null, "summary": "Automated answer grading is a critical challenge in educational technology,\nwith the potential to streamline assessment processes, ensure grading\nconsistency, and provide timely feedback to students. However, existing\napproaches are often constrained to specific exam formats, lack\ninterpretability in score assignment, and struggle with real-world\napplicability across diverse subjects and assessment types. To address these\nlimitations, we introduce RATAS (Rubric Automated Tree-based Answer Scoring), a\nnovel framework that leverages state-of-the-art generative AI models for\nrubric-based grading of textual responses. RATAS is designed to support a wide\nrange of grading rubrics, enable subject-agnostic evaluation, and generate\nstructured, explainable rationales for assigned scores. We formalize the\nautomatic grading task through a mathematical framework tailored to\nrubric-based assessment and present an architecture capable of handling\ncomplex, real-world exam structures. To rigorously evaluate our approach, we\nconstruct a unique, contextualized dataset derived from real-world\nproject-based courses, encompassing diverse response formats and varying levels\nof complexity. Empirical results demonstrate that RATAS achieves high\nreliability and accuracy in automated grading while providing interpretable\nfeedback that enhances transparency for both students and nstructors."}
{"id": "2505.24803", "pdf": "https://arxiv.org/pdf/2505.24803.pdf", "abs": "https://arxiv.org/abs/2505.24803", "title": "Guiding Generative Storytelling with Knowledge Graphs", "authors": ["Zhijun Pan", "Antonios Andronis", "Eva Hayek", "Oscar AP Wilkinson", "Ilya Lasy", "Annette Parry", "Guy Gadney", "Tim J. Smith", "Mick Grierson"], "categories": ["cs.CL", "cs.HC"], "comment": "This manuscript was submitted for peer review in January 2025", "summary": "Large Language Models (LLMs) have shown great potential in automated story\ngeneration, but challenges remain in maintaining long-form coherence and\nproviding users with intuitive and effective control. Retrieval-Augmented\nGeneration (RAG) has proven effective in reducing hallucinations in text\ngeneration; however, the use of structured data to support generative\nstorytelling remains underexplored. This paper investigates how knowledge\ngraphs (KGs) can enhance LLM-based storytelling by improving narrative quality\nand enabling user-driven modifications. We propose a KG-assisted storytelling\npipeline and evaluate its effectiveness through a user study with 15\nparticipants. Participants created their own story prompts, generated stories,\nand edited knowledge graphs to shape their narratives. Through quantitative and\nqualitative analysis, our findings demonstrate that knowledge graphs\nsignificantly enhance story quality in action-oriented and structured\nnarratives within our system settings. Additionally, editing the knowledge\ngraph increases users' sense of control, making storytelling more engaging,\ninteractive, and playful."}
{"id": "2505.23820", "pdf": "https://arxiv.org/pdf/2505.23820.pdf", "abs": "https://arxiv.org/abs/2505.23820", "title": "Arbiters of Ambivalence: Challenges of Using LLMs in No-Consensus Tasks", "authors": ["Bhaktipriya Radharapu", "Manon Revel", "Megan Ung", "Sebastian Ruder", "Adina Williams"], "categories": ["cs.CL"], "comment": null, "summary": "The increasing use of LLMs as substitutes for humans in ``aligning'' LLMs has\nraised questions about their ability to replicate human judgments and\npreferences, especially in ambivalent scenarios where humans disagree. This\nstudy examines the biases and limitations of LLMs in three roles: answer\ngenerator, judge, and debater. These roles loosely correspond to previously\ndescribed alignment frameworks: preference alignment (judge) and scalable\noversight (debater), with the answer generator reflecting the typical setting\nwith user interactions. We develop a ``no-consensus'' benchmark by curating\nexamples that encompass a variety of a priori ambivalent scenarios, each\npresenting two possible stances. Our results show that while LLMs can provide\nnuanced assessments when generating open-ended answers, they tend to take a\nstance on no-consensus topics when employed as judges or debaters. These\nfindings underscore the necessity for more sophisticated methods for aligning\nLLMs without human oversight, highlighting that LLMs cannot fully capture human\ndisagreement even on topics where humans themselves are divided."}
{"id": "2405.08906", "pdf": "https://arxiv.org/pdf/2405.08906.pdf", "abs": "https://arxiv.org/abs/2405.08906", "title": "Functional Near-Infrared Spectroscopy (fNIRS) Analysis of Interaction Techniques in Touchscreen-Based Educational Gaming", "authors": ["Shayla Sharmin", "Elham Bakhshipour", "Behdokht Kiafar", "Md Fahim Abrar", "Pinar Kullu", "Nancy Getchell", "Roghayeh Leila Barmaki"], "categories": ["cs.HC"], "comment": null, "summary": "Educational games enhance learning experiences by integrating touchscreens,\nmaking interactions more engaging and intuitive for learners. However, the\ncognitive impacts of educational game-play input modalities, such as the hand\nand stylus technique, are unclear. We compared the experience of using hands\nvs. a stylus for touchscreens while playing an educational game by analyzing\noxygenated hemoglobin collected by functional Near-Infrared Spectroscopy and\nself-reported measures. In addition, we measured the hand vs. the stylus\nmodalities of the task and calculated the relative neural efficiency and\nrelative neural involvement using the mental demand and the quiz score. Our\nfindings show that the hand condition had a significantly lower neural\ninvolvement, yet higher neural efficiency than the stylus condition. This\nresult suggests the requirement of less cognitive effort while using the hand.\nAdditionally, the self-reported measures show significant differences, and the\nresults suggest that hand-based input is more intuitive, less cognitively\ndemanding, and less frustrating. Conversely, the use of a stylus required\nhigher cognitive effort due to the cognitive balance of controlling the pen and\nanswering questions. These findings highlight the importance of designing\neducational games that allow learners to engage with the system while\nminimizing cognitive effort."}
{"id": "2505.23822", "pdf": "https://arxiv.org/pdf/2505.23822.pdf", "abs": "https://arxiv.org/abs/2505.23822", "title": "Speech as a Multimodal Digital Phenotype for Multi-Task LLM-based Mental Health Prediction", "authors": ["Mai Ali", "Christopher Lucasius", "Tanmay P. Patel", "Madison Aitken", "Jacob Vorstman", "Peter Szatmari", "Marco Battaglia", "Deepa Kundur"], "categories": ["cs.CL", "cs.MM"], "comment": "6 pages, 1 figure, 3 tables. Submitted to ICSM 2025. The\n  corresponding author is Mai Ali (maia.ali@mail.utoronto.ca). Christopher\n  Lucasius and Tanmay P. Patel contributed equally", "summary": "Speech is a noninvasive digital phenotype that can offer valuable insights\ninto mental health conditions, but it is often treated as a single modality. In\ncontrast, we propose the treatment of patient speech data as a trimodal\nmultimedia data source for depression detection. This study explores the\npotential of large language model-based architectures for speech-based\ndepression prediction in a multimodal regime that integrates speech-derived\ntext, acoustic landmarks, and vocal biomarkers. Adolescent depression presents\na significant challenge and is often comorbid with multiple disorders, such as\nsuicidal ideation and sleep disturbances. This presents an additional\nopportunity to integrate multi-task learning (MTL) into our study by\nsimultaneously predicting depression, suicidal ideation, and sleep disturbances\nusing the multimodal formulation. We also propose a longitudinal analysis\nstrategy that models temporal changes across multiple clinical interactions,\nallowing for a comprehensive understanding of the conditions' progression. Our\nproposed approach, featuring trimodal, longitudinal MTL is evaluated on the\nDepression Early Warning dataset. It achieves a balanced accuracy of 70.8%,\nwhich is higher than each of the unimodal, single-task, and non-longitudinal\nmethods."}
{"id": "2503.00303", "pdf": "https://arxiv.org/pdf/2503.00303.pdf", "abs": "https://arxiv.org/abs/2503.00303", "title": "Leveraging Complementary AI Explanations to Mitigate Misunderstanding in XAI", "authors": ["Yueqing Xuan", "Kacper Sokol", "Mark Sanderson", "Jeffrey Chan"], "categories": ["cs.HC"], "comment": "Accepted to IEEE Swiss Conference on Data Science (SDS) 2025", "summary": "Artificial intelligence explanations can make complex predictive models more\ncomprehensible. To be effective, however, they should anticipate and mitigate\npossible misinterpretations, e.g., arising when users infer incorrect\ninformation that is not explicitly conveyed. To this end, we propose\ncomplementary explanations -- a novel method that pairs explanations to\ncompensate for their respective limitations. A complementary explanation adds\ninsights that clarify potential misconceptions stemming from the primary\nexplanation while ensuring their coherency and avoiding redundancy. We\nintroduce a framework for designing and evaluating complementary explanation\npairs based on pertinent qualitative properties and quantitative metrics. Our\napproach allows to construct complementary explanations that minimise the\nchance of their misinterpretation."}
{"id": "2505.23823", "pdf": "https://arxiv.org/pdf/2505.23823.pdf", "abs": "https://arxiv.org/abs/2505.23823", "title": "RAGPPI: RAG Benchmark for Protein-Protein Interactions in Drug Discovery", "authors": ["Youngseung Jeon", "Ziwen Li", "Thomas Li", "JiaSyuan Chang", "Morteza Ziyadi", "Xiang 'Anthony' Chen"], "categories": ["cs.CL"], "comment": "17 pages, 4 figures, 8 tables", "summary": "Retrieving the biological impacts of protein-protein interactions (PPIs) is\nessential for target identification (Target ID) in drug development. Given the\nvast number of proteins involved, this process remains time-consuming and\nchallenging. Large Language Models (LLMs) and Retrieval-Augmented Generation\n(RAG) frameworks have supported Target ID; however, no benchmark currently\nexists for identifying the biological impacts of PPIs. To bridge this gap, we\nintroduce the RAG Benchmark for PPIs (RAGPPI), a factual question-answer\nbenchmark of 4,420 question-answer pairs that focus on the potential biological\nimpacts of PPIs. Through interviews with experts, we identified criteria for a\nbenchmark dataset, such as a type of QA and source. We built a gold-standard\ndataset (500 QA pairs) through expert-driven data annotation. We developed an\nensemble auto-evaluation LLM that reflected expert labeling characteristics,\nwhich facilitates the construction of a silver-standard dataset (3,720 QA\npairs). We are committed to maintaining RAGPPI as a resource to support the\nresearch community in advancing RAG systems for drug discovery QA solutions."}
{"id": "2504.13883", "pdf": "https://arxiv.org/pdf/2504.13883.pdf", "abs": "https://arxiv.org/abs/2504.13883", "title": "Hybrid Deep Learning Model to Estimate Cognitive Effort from fNIRS Signals in Educational Game Playing", "authors": ["Shayla Sharmin", "Roghayeh Leila Barmaki"], "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "This study estimates cognitive effort (CE) based on functional near-infrared\nspectroscopy (fNIRS) data and performance scores using a hybrid deep learning\nmodel. The estimation of CE enables educators to modify material to enhance\nlearning effectiveness and student engagement. Relative neural efficiency (RNE)\nand relative neural involvement (RNI) are two metrics that have been used to\nrepresent CE. To estimate RNE and RNI we need hemodynamic response in the brain\nand the performance score of a task.We collected oxygenated hemoglobin ($\\Delta\n\\mathrm{HbO}$). Sixteen participants answered 16 questions in a unity-based\neducational game, each with a 30-second response time. We used deep learning\nmodels to predict the performance score and estimate RNE and RNI to understand\nCE. The study compares traditional machine learning techniques with deep\nlearning models such as CNN, LSTM, BiLSTM, and a hybrid CNN-GRU to determine\nwhich approach provides better accuracy in predicting performance scores. The\nresult shows that the hybrid CNN-GRU gives better performance with 78.36\\%\ntraining accuracy and 73.08\\% test accuracy than other models. We performed\nXGBoost on the extracted GRU feature and got the highest accuracy (69.23\\%).\nThis suggests that the features learned from this hybrid model generalize\nbetter even in traditional machine learning algorithms. We used the $\\Delta\n\\mathrm{HbO}$ and predicted score to calculate RNE and RNI to observe cognitive\neffort in our four test cases. Our result shows that even with moderate\naccuracy, the predicted RNE and RNI closely follows the actual trends. we also\nobserved that when participants were in a state of high CE, introducing rest\nled decrease of CE. These findings can be helpful to design and improve\nlearning environments and provide valuable insights in learning materials."}
{"id": "2505.23824", "pdf": "https://arxiv.org/pdf/2505.23824.pdf", "abs": "https://arxiv.org/abs/2505.23824", "title": "Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation", "authors": ["Tianmai M. Zhang", "Neil F. Abernethy"], "categories": ["cs.CL"], "comment": "Work in progress. Conclusions may be updated", "summary": "Recent advancements in large language models have sparked interest in\nutilizing them to assist the peer review process of scientific publication.\nInstead of having AI models generate reviews in the same way as human\nreviewers, we propose adopting them as manuscript quality checkers. We\nintroduce several baseline approaches and an extendable automatic evaluation\nframework using top LLMs as judges to tackle the difficulty of recruiting\ndomain experts for manual evaluation. Utilizing papers withdrawn from arXiv, we\nvalidated our proposed methods with several leading reasoning LLMs from\ndifferent providers and assessed their performance and API costs for\nidentifying critical errors and unsoundness problems. The OpenAI o3 model\nperformed the best, while o4-mini was the most cost-effective one in our\nevaluation. This paper provides insights into document-based scientific\nunderstanding/reasoning and lays the foundation for future applications."}
{"id": "2504.18410", "pdf": "https://arxiv.org/pdf/2504.18410.pdf", "abs": "https://arxiv.org/abs/2504.18410", "title": "Can Code Outlove Blood? An LLM-based VR Experience to Prompt Reflection on Parental Verbal Abuse", "authors": ["Jiaying Fu", "Jialin Gu", "Tianyue Gong", "Tiange Zhou"], "categories": ["cs.HC"], "comment": "8 pages, 5 figures, accetped by 30th International Symposium on\n  Electronic Art (ISEA 2025)", "summary": "Parental verbal abuse leaves lasting emotional impacts, yet current\ntherapeutic approaches often lack immersive self-reflection opportunities. To\naddress this, we developed a VR experience powered by LLMs to foster reflection\non parental verbal abuse. Participants with relevant experiences engage in a\ndual-phase VR experience: first assuming the role of a verbally abusive parent,\ninteracting with an LLM portraying a child, then observing the LLM reframing\nabusive dialogue into warm, supportive expressions as a nurturing parent. A\nqualitative study with 12 participants showed that the experience encourages\nreflection on their past experiences and fosters supportive emotions. However,\nthese effects vary with participants' personal histories, emphasizing the need\nfor greater personalization in AI-driven emotional support. This study explores\nthe use of LLMs in immersive environment to promote emotional reflection,\noffering insights into the design of AI-driven emotional support systems."}
{"id": "2505.23827", "pdf": "https://arxiv.org/pdf/2505.23827.pdf", "abs": "https://arxiv.org/abs/2505.23827", "title": "ValueSim: Generating Backstories to Model Individual Value Systems", "authors": ["Bangde Du", "Ziyi Ye", "Zhijing Wu", "Jankowska Monika", "Shuqi Zhu", "Qingyao Ai", "Yujia Zhou", "Yiqun Liu"], "categories": ["cs.CL"], "comment": "8 pages main paper + 13 pages appendix, 3 figures, 2 tables", "summary": "As Large Language Models (LLMs) continue to exhibit increasingly human-like\ncapabilities, aligning them with human values has become critically important.\nContemporary advanced techniques, such as prompt learning and reinforcement\nlearning, are being deployed to better align LLMs with human values. However,\nwhile these approaches address broad ethical considerations and helpfulness,\nthey rarely focus on simulating individualized human value systems. To address\nthis gap, we present ValueSim, a framework that simulates individual values\nthrough the generation of personal backstories reflecting past experiences and\ndemographic information. ValueSim converts structured individual data into\nnarrative backstories and employs a multi-module architecture inspired by the\nCognitive-Affective Personality System to simulate individual values based on\nthese narratives. Testing ValueSim on a self-constructed benchmark derived from\nthe World Values Survey demonstrates an improvement in top-1 accuracy by over\n10% compared to retrieval-augmented generation methods. Further analysis\nreveals that performance enhances as additional user interaction history\nbecomes available, indicating the model's ability to refine its persona\nsimulation capabilities over time."}
{"id": "2505.22767", "pdf": "https://arxiv.org/pdf/2505.22767.pdf", "abs": "https://arxiv.org/abs/2505.22767", "title": "In Dialogue with Intelligence: Rethinking Large Language Models as Collective Knowledge", "authors": ["Eleni Vasilaki"], "categories": ["cs.HC", "cs.AI"], "comment": "6 pages, 1 table", "summary": "Large Language Models (LLMs) are typically analysed through architectural,\nbehavioural, or training-data lenses. This article offers a theoretical and\nexperiential re-framing: LLMs as dynamic instantiations of Collective human\nKnowledge (CK), where intelligence is evoked through dialogue rather than\nstored statically. Drawing on concepts from neuroscience and AI, and grounded\nin sustained interaction with ChatGPT-4, I examine emergent dialogue patterns,\nthe implications of fine-tuning, and the notion of co-augmentation: mutual\nenhancement between human and machine cognition. This perspective offers a new\nlens for understanding interaction, representation, and agency in contemporary\nAI systems."}
{"id": "2505.23829", "pdf": "https://arxiv.org/pdf/2505.23829.pdf", "abs": "https://arxiv.org/abs/2505.23829", "title": "BiasFilter: An Inference-Time Debiasing Framework for Large Language Models", "authors": ["Xiaoqing Cheng", "Ruizhe Chen", "Hongying Zan", "Yuxiang Jia", "Min Peng"], "categories": ["cs.CL"], "comment": null, "summary": "Mitigating social bias in large language models (LLMs) has become an\nincreasingly important research objective. However, existing debiasing methods\noften incur high human and computational costs, exhibit limited effectiveness,\nand struggle to scale to larger models and open-ended generation tasks. To\naddress these limitations, this paper proposes BiasFilter, a model-agnostic,\ninference-time debiasing framework that integrates seamlessly with both\nopen-source and API-based LLMs. Instead of relying on retraining with balanced\ndata or modifying model parameters, BiasFilter enforces fairness by filtering\ngeneration outputs in real time. Specifically, it periodically evaluates\nintermediate outputs every few tokens, maintains an active set of candidate\ncontinuations, and incrementally completes generation by discarding low-reward\nsegments based on a fairness reward signal. To support this process, we\nconstruct a fairness preference dataset and train an implicit reward model to\nassess token-level fairness in generated responses. Extensive experiments\ndemonstrate that BiasFilter effectively mitigates social bias across a range of\nLLMs while preserving overall generation quality."}
{"id": "2303.17707", "pdf": "https://arxiv.org/pdf/2303.17707.pdf", "abs": "https://arxiv.org/abs/2303.17707", "title": "Why is plausibility surprisingly problematic as an XAI criterion?", "authors": ["Weina Jin", "Xiaoxiao Li", "Ghassan Hamarneh"], "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Explainable artificial intelligence (XAI) is motivated by the problem of\nmaking AI predictions understandable, transparent, and responsible, as AI\nbecomes increasingly impactful in society and high-stakes domains. The\nevaluation and optimization criteria of XAI are gatekeepers for XAI algorithms\nto achieve their expected goals and should withstand rigorous inspection. To\nimprove the scientific rigor of XAI, we conduct a critical examination of a\ncommon XAI criterion: plausibility. Plausibility assesses how convincing the AI\nexplanation is to humans, and is usually quantified by metrics of feature\nlocalization or feature correlation. Our examination shows that plausibility is\ninvalid to measure explainability, and human explanations are not the ground\ntruth for XAI, because doing so ignores the necessary assumptions underpinning\nan explanation. Our examination further reveals the consequences of using\nplausibility as an XAI criterion, including increasing misleading explanations\nthat manipulate users, deteriorating users' trust in the AI system, undermining\nhuman autonomy, being unable to achieve complementary human-AI task\nperformance, and abandoning other possible approaches of enhancing\nunderstandability. Due to the invalidity of measurements and the unethical\nissues, this position paper argues that the community should stop using\nplausibility as a criterion for the evaluation and optimization of XAI\nalgorithms. We also delineate new research approaches to improve XAI in\ntrustworthiness, understandability, and utility to users, including\ncomplementary human-AI task performance."}
{"id": "2505.23830", "pdf": "https://arxiv.org/pdf/2505.23830.pdf", "abs": "https://arxiv.org/abs/2505.23830", "title": "EvoMoE: Expert Evolution in Mixture of Experts for Multimodal Large Language Models", "authors": ["Linglin Jing", "Yuting Gao", "Zhigang Wang", "Wang Lan", "Yiwen Tang", "Wenhai Wang", "Kaipeng Zhang", "Qingpei Guo"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements have shown that the Mixture of Experts (MoE) approach\nsignificantly enhances the capacity of large language models (LLMs) and\nimproves performance on downstream tasks. Building on these promising results,\nmulti-modal large language models (MLLMs) have increasingly adopted MoE\ntechniques. However, existing multi-modal MoE tuning methods typically face two\nkey challenges: expert uniformity and router rigidity. Expert uniformity occurs\nbecause MoE experts are often initialized by simply replicating the FFN\nparameters from LLMs, leading to homogenized expert functions and weakening the\nintended diversification of the MoE architecture. Meanwhile, router rigidity\nstems from the prevalent use of static linear routers for expert selection,\nwhich fail to distinguish between visual and textual tokens, resulting in\nsimilar expert distributions for image and text. To address these limitations,\nwe propose EvoMoE, an innovative MoE tuning framework. EvoMoE introduces a\nmeticulously designed expert initialization strategy that progressively evolves\nmultiple robust experts from a single trainable expert, a process termed expert\nevolution that specifically targets severe expert homogenization. Furthermore,\nwe introduce the Dynamic Token-aware Router (DTR), a novel routing mechanism\nthat allocates input tokens to appropriate experts based on their modality and\nintrinsic token values. This dynamic routing is facilitated by hypernetworks,\nwhich dynamically generate routing weights tailored for each individual token.\nExtensive experiments demonstrate that EvoMoE significantly outperforms other\nsparse MLLMs across a variety of multi-modal benchmarks, including MME,\nMMBench, TextVQA, and POPE. Our results highlight the effectiveness of EvoMoE\nin enhancing the performance of MLLMs by addressing the critical issues of\nexpert uniformity and router rigidity."}
{"id": "2406.11317", "pdf": "https://arxiv.org/pdf/2406.11317.pdf", "abs": "https://arxiv.org/abs/2406.11317", "title": "GUICourse: From General Vision Language Models to Versatile GUI Agents", "authors": ["Wentong Chen", "Junbo Cui", "Jinyi Hu", "Yujia Qin", "Junjie Fang", "Yue Zhao", "Chongyi Wang", "Jun Liu", "Guirong Chen", "Yupeng Huo", "Yuan Yao", "Yankai Lin", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": null, "summary": "Utilizing Graphic User Interface (GUI) for human-computer interaction is\nessential for accessing a wide range of digital tools. Recent advancements in\nVision Language Models (VLMs) highlight the compelling potential to develop\nversatile agents to help humans finish GUI navigation tasks. However, current\nVLMs are challenged in terms of fundamental abilities (OCR and grounding) and\nGUI knowledge (the functions and control methods of GUI elements), preventing\nthem from becoming practical GUI agents. To solve these challenges, we\ncontribute GUICourse, a suite of datasets to train visual-based GUI agents from\ngeneral VLMs. First, we introduce the GUIEnv dataset to strengthen the OCR and\ngrounding capabilities of VLMs. Then, we introduce the GUIAct and GUIChat\ndatasets to enrich their knowledge of GUI components and interactions.\nExperiments demonstrate that our GUI agents have better performance on common\nGUI tasks than their baseline VLMs. Even the small-size GUI agent (with 3.1B\nparameters) can still work well on single-step and multi-step GUI tasks.\nFinally, we analyze the different varieties in the training stage of this agent\nby ablation study. Our source codes and datasets are released at\nhttps://github.com/yiye3/GUICourse."}
{"id": "2505.23831", "pdf": "https://arxiv.org/pdf/2505.23831.pdf", "abs": "https://arxiv.org/abs/2505.23831", "title": "ICH-Qwen: A Large Language Model Towards Chinese Intangible Cultural Heritage", "authors": ["Wenhao Ye", "Tiansheng Zheng", "Yue Qi", "Wenhua Zhao", "Xiyu Wang", "Xue Zhao", "Jiacheng He", "Yaya Zheng", "Dongbo Wang"], "categories": ["cs.CL"], "comment": "16 pages, 2 figures", "summary": "The intangible cultural heritage (ICH) of China, a cultural asset transmitted\nacross generations by various ethnic groups, serves as a significant testament\nto the evolution of human civilization and holds irreplaceable value for the\npreservation of historical lineage and the enhancement of cultural\nself-confidence. However, the rapid pace of modernization poses formidable\nchallenges to ICH, including threats damage, disappearance and discontinuity of\ninheritance. China has the highest number of items on the UNESCO Intangible\nCultural Heritage List, which is indicative of the nation's abundant cultural\nresources and emphasises the pressing need for ICH preservation. In recent\nyears, the rapid advancements in large language modelling have provided a novel\ntechnological approach for the preservation and dissemination of ICH. This\nstudy utilises a substantial corpus of open-source Chinese ICH data to develop\na large language model, ICH-Qwen, for the ICH domain. The model employs natural\nlanguage understanding and knowledge reasoning capabilities of large language\nmodels, augmented with synthetic data and fine-tuning techniques. The\nexperimental results demonstrate the efficacy of ICH-Qwen in executing tasks\nspecific to the ICH domain. It is anticipated that the model will provide\nintelligent solutions for the protection, inheritance and dissemination of\nintangible cultural heritage, as well as new theoretical and practical\nreferences for the sustainable development of intangible cultural heritage.\nFurthermore, it is expected that the study will open up new paths for digital\nhumanities research."}
{"id": "2411.06160", "pdf": "https://arxiv.org/pdf/2411.06160.pdf", "abs": "https://arxiv.org/abs/2411.06160", "title": "Expansion Quantization Network: An Efficient Micro-emotion Annotation and Detection Framework", "authors": ["Jingyi Zhou", "Senlin Luo", "Haofan Chen"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "3.1 There is a misstatement in the EQN Framework section", "summary": "Text emotion detection constitutes a crucial foundation for advancing\nartificial intelligence from basic comprehension to the exploration of\nemotional reasoning. Most existing emotion detection datasets rely on manual\nannotations, which are associated with high costs, substantial subjectivity,\nand severe label imbalances. This is particularly evident in the inadequate\nannotation of micro-emotions and the absence of emotional intensity\nrepresentation, which fail to capture the rich emotions embedded in sentences\nand adversely affect the quality of downstream task completion. By proposing an\nall-labels and training-set label regression method, we map label values to\nenergy intensity levels, thereby fully leveraging the learning capabilities of\nmachine models and the interdependencies among labels to uncover multiple\nemotions within samples. This led to the establishment of the Emotion\nQuantization Network (EQN) framework for micro-emotion detection and\nannotation. Using five commonly employed sentiment datasets, we conducted\ncomparative experiments with various models, validating the broad applicability\nof our framework within NLP machine learning models. Based on the EQN\nframework, emotion detection and annotation are conducted on the GoEmotions\ndataset. A comprehensive comparison with the results from Google literature\ndemonstrates that the EQN framework possesses a high capability for automatic\ndetection and annotation of micro-emotions. The EQN framework is the first to\nachieve automatic micro-emotion annotation with energy-level scores, providing\nstrong support for further emotion detection analysis and the quantitative\nresearch of emotion computing."}
{"id": "2505.23832", "pdf": "https://arxiv.org/pdf/2505.23832.pdf", "abs": "https://arxiv.org/abs/2505.23832", "title": "LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements Generation", "authors": ["Chaeeun Kim", "Jinu Lee", "Wonseok Hwang"], "categories": ["cs.CL", "cs.IR"], "comment": "Under review", "summary": "Legal Case Retrieval (LCR), which retrieves relevant cases from a query case,\nis a fundamental task for legal professionals in research and decision-making.\nHowever, existing studies on LCR face two major limitations. First, they are\nevaluated on relatively small-scale retrieval corpora (e.g., 100-55K cases) and\nuse a narrow range of criminal query types, which cannot sufficiently reflect\nthe complexity of real-world legal retrieval scenarios. Second, their reliance\non embedding-based or lexical matching methods often results in limited\nrepresentations and legally irrelevant matches. To address these issues, we\npresent: (1) LEGAR BENCH, the first large-scale Korean LCR benchmark, covering\n411 diverse crime types in queries over 1.2M legal cases; and (2)\nLegalSearchLM, a retrieval model that performs legal element reasoning over the\nquery case and directly generates content grounded in the target cases through\nconstrained decoding. Experimental results show that LegalSearchLM outperforms\nbaselines by 6-20% on LEGAR BENCH, achieving state-of-the-art performance. It\nalso demonstrates strong generalization to out-of-domain cases, outperforming\nnaive generative models trained on in-domain data by 15%."}
{"id": "2412.15712", "pdf": "https://arxiv.org/pdf/2412.15712.pdf", "abs": "https://arxiv.org/abs/2412.15712", "title": "Contrastive Learning for Task-Independent SpeechLLM-Pretraining", "authors": ["Maike Züfle", "Jan Niehues"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) excel in natural language processing but\nadapting these LLMs to speech processing tasks efficiently is not\nstraightforward. Direct task-specific fine-tuning is limited by overfitting\nrisks, data requirements, and computational costs. To address these challenges,\nwe propose a scalable, two-stage training approach: (1) A task-independent\nspeech pretraining stage using contrastive learning to align text and speech\nrepresentations over all layers, followed by (2) a task-specific fine-tuning\nstage requiring minimal data. This approach outperforms traditional ASR\npretraining and enables the model to surpass models specialized on speech\ntranslation and question answering while being trained on only 10% of the\ntask-specific data."}
{"id": "2505.23833", "pdf": "https://arxiv.org/pdf/2505.23833.pdf", "abs": "https://arxiv.org/abs/2505.23833", "title": "Benchmarking Abstract and Reasoning Abilities Through A Theoretical Perspective", "authors": ["Qingchuan Ma", "Yuhang Wu", "Xiawu Zheng", "Rongrong Ji"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we aim to establish a simple, effective, and theoretically\ngrounded benchmark for rigorously probing abstract reasoning in Large Language\nModels (LLMs). To achieve this, we first develop a mathematic framework that\ndefines abstract reasoning as the ability to: (i) extract essential patterns\nindependent of surface representations, and (ii) apply consistent rules to\nthese abstract patterns. Based on this framework, we introduce two novel\ncomplementary metrics: \\(\\scoreGamma\\) measures basic reasoning accuracy, while\n\\(\\scoreDelta\\) quantifies a model's reliance on specific symbols rather than\nunderlying patterns - a key indicator of true abstraction versus mere\nmemorization. To implement this measurement, we design a benchmark: systematic\nsymbol remapping in rule-based tasks, which forces models to demonstrate\ngenuine pattern recognition beyond superficial token matching. Extensive LLM\nevaluations using this benchmark (commercial API models, 7B-70B, multi-agent)\nreveal:1) critical limitations in non-decimal arithmetic and symbolic\nreasoning; 2) persistent abstraction gaps despite chain-of-thought prompting;\nand 3) \\(\\scoreDelta\\)'s effectiveness in robustly measuring memory dependence\nby quantifying performance degradation under symbol remapping, particularly\nhighlighting operand-specific memorization. These findings underscore that\ncurrent LLMs, despite domain-specific strengths, still lack robust abstract\nreasoning, highlighting key areas for future improvement."}
{"id": "2502.11357", "pdf": "https://arxiv.org/pdf/2502.11357.pdf", "abs": "https://arxiv.org/abs/2502.11357", "title": "Explorer: Scaling Exploration-driven Web Trajectory Synthesis for Multimodal Web Agents", "authors": ["Vardaan Pahuja", "Yadong Lu", "Corby Rosset", "Boyu Gou", "Arindam Mitra", "Spencer Whitehead", "Yu Su", "Ahmed Awadallah"], "categories": ["cs.AI", "cs.HC"], "comment": "ACL 2025 (Findings)", "summary": "Recent success in large multimodal models (LMMs) has sparked promising\napplications of agents capable of autonomously completing complex web tasks.\nWhile open-source LMM agents have made significant advances in offline\nevaluation benchmarks, their performance still falls substantially short of\nhuman-level capabilities in more realistic online settings. A key bottleneck is\nthe lack of diverse and large-scale trajectory-level datasets across various\ndomains, which are expensive to collect. In this paper, we address this\nchallenge by developing a scalable recipe to synthesize the largest and most\ndiverse trajectory-level dataset to date, containing over 94K successful\nmultimodal web trajectories, spanning 49K unique URLs, 720K screenshots, and\n33M web elements. In particular, we leverage extensive web exploration and\nrefinement to obtain diverse task intents. The average cost is 28 cents per\nsuccessful trajectory, making it affordable to a wide range of users in the\ncommunity. Leveraging this dataset, we train Explorer, a multimodal web agent,\nand demonstrate strong performance on both offline and online web agent\nbenchmarks such as Mind2Web-Live, Multimodal-Mind2Web, and MiniWob++.\nAdditionally, our experiments highlight data scaling as a key driver for\nimproving web agent capabilities. We hope this study makes state-of-the-art\nLMM-based agent research at a larger scale more accessible."}
{"id": "2505.23835", "pdf": "https://arxiv.org/pdf/2505.23835.pdf", "abs": "https://arxiv.org/abs/2505.23835", "title": "Say What You Mean: Natural Language Access Control with Large Language Models for Internet of Things", "authors": ["Ye Cheng", "Minghui Xu", "Yue Zhang", "Kun Li", "Hao Wu", "Yechao Zhang", "Shaoyong Guo", "Wangjie Qiu", "Dongxiao Yu", "Xiuzhen Cheng"], "categories": ["cs.CL"], "comment": null, "summary": "Access control in the Internet of Things (IoT) is becoming increasingly\ncomplex, as policies must account for dynamic and contextual factors such as\ntime, location, user behavior, and environmental conditions. However, existing\nplatforms either offer only coarse-grained controls or rely on rigid rule\nmatching, making them ill-suited for semantically rich or ambiguous access\nscenarios. Moreover, the policy authoring process remains fragmented: domain\nexperts describe requirements in natural language, but developers must manually\ntranslate them into code, introducing semantic gaps and potential\nmisconfiguration. In this work, we present LACE, the Language-based Access\nControl Engine, a hybrid framework that leverages large language models (LLMs)\nto bridge the gap between human intent and machine-enforceable logic. LACE\ncombines prompt-guided policy generation, retrieval-augmented reasoning, and\nformal validation to support expressive, interpretable, and verifiable access\ncontrol. It enables users to specify policies in natural language,\nautomatically translates them into structured rules, validates semantic\ncorrectness, and makes access decisions using a hybrid LLM-rule-based engine.\nWe evaluate LACE in smart home environments through extensive experiments. LACE\nachieves 100% correctness in verified policy generation and up to 88% decision\naccuracy with 0.79 F1-score using DeepSeek-V3, outperforming baselines such as\nGPT-3.5 and Gemini. The system also demonstrates strong scalability under\nincreasing policy volume and request concurrency. Our results highlight LACE's\npotential to enable secure, flexible, and user-friendly access control across\nreal-world IoT platforms."}
{"id": "2504.15815", "pdf": "https://arxiv.org/pdf/2504.15815.pdf", "abs": "https://arxiv.org/abs/2504.15815", "title": "What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns", "authors": ["Michael A. Hedderich", "Anyi Wang", "Raoyuan Zhao", "Florian Eichin", "Jonas Fischer", "Barbara Plank"], "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": "Accepted at ACL'25", "summary": "Prompt engineering for large language models is challenging, as even small\nprompt perturbations or model changes can significantly impact the generated\noutput texts. Existing evaluation methods of LLM outputs, either automated\nmetrics or human evaluation, have limitations, such as providing limited\ninsights or being labor-intensive. We propose Spotlight, a new approach that\ncombines both automation and human analysis. Based on data mining techniques,\nwe automatically distinguish between random (decoding) variations and\nsystematic differences in language model outputs. This process provides token\npatterns that describe the systematic differences and guide the user in\nmanually analyzing the effects of their prompts and changes in models\nefficiently. We create three benchmarks to quantitatively test the reliability\nof token pattern extraction methods and demonstrate that our approach provides\nnew insights into established prompt data. From a human-centric perspective,\nthrough demonstration studies and a user study, we show that our token pattern\napproach helps users understand the systematic differences of language model\noutputs. We are further able to discover relevant differences caused by prompt\nand model changes (e.g. related to gender or culture), thus supporting the\nprompt engineering process and human-centric model behavior research."}
{"id": "2505.23836", "pdf": "https://arxiv.org/pdf/2505.23836.pdf", "abs": "https://arxiv.org/abs/2505.23836", "title": "Large Language Models Often Know When They Are Being Evaluated", "authors": ["Joe Needham", "Giles Edkins", "Govind Pimpale", "Henning Bartsch", "Marius Hobbhahn"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "If AI models can detect when they are being evaluated, the effectiveness of\nevaluations might be compromised. For example, models could have systematically\ndifferent behavior during evaluations, leading to less reliable benchmarks for\ndeployment and governance decisions. We investigate whether frontier language\nmodels can accurately classify transcripts based on whether they originate from\nevaluations or real-world deployment, a capability we call evaluation\nawareness. To achieve this, we construct a diverse benchmark of 1,000 prompts\nand transcripts from 61 distinct datasets. These span public benchmarks (e.g.,\nMMLU, SWEBench), real-world deployment interactions, and agent trajectories\nfrom scaffolding frameworks (e.g., web-browsing agents). Frontier models\nclearly demonstrate above-random evaluation awareness (Gemini-2.5-Pro reaches\nan AUC of $0.83$), but do not yet surpass our simple human baseline (AUC of\n$0.92$). Furthermore, both AI models and humans are better at identifying\nevaluations in agentic settings compared to chat settings. Additionally, we\ntest whether models can identify the purpose of the evaluation. Under\nmultiple-choice and open-ended questioning, AI models far outperform random\nchance in identifying what an evaluation is testing for. Our results indicate\nthat frontier models already exhibit a substantial, though not yet superhuman,\nlevel of evaluation-awareness. We recommend tracking this capability in future\nmodels."}
{"id": "2505.23837", "pdf": "https://arxiv.org/pdf/2505.23837.pdf", "abs": "https://arxiv.org/abs/2505.23837", "title": "CoMaPOI: A Collaborative Multi-Agent Framework for Next POI Prediction Bridging the Gap Between Trajectory and Language", "authors": ["Lin Zhong", "Lingzhi Wang", "Xu Yang", "Qing Liao"], "categories": ["cs.CL", "cs.IR", "I.2.0"], "comment": "This paper has been accepted by SIGIR 2025", "summary": "Large Language Models (LLMs) offer new opportunities for the next\nPoint-Of-Interest (POI) prediction task, leveraging their capabilities in\nsemantic understanding of POI trajectories. However, previous LLM-based\nmethods, which are superficially adapted to next POI prediction, largely\noverlook critical challenges associated with applying LLMs to this task.\nSpecifically, LLMs encounter two critical challenges: (1) a lack of intrinsic\nunderstanding of numeric spatiotemporal data, which hinders accurate modeling\nof users' spatiotemporal distributions and preferences; and (2) an excessively\nlarge and unconstrained candidate POI space, which often results in random or\nirrelevant predictions. To address these issues, we propose a Collaborative\nMulti Agent Framework for Next POI Prediction, named CoMaPOI. Through the close\ninteraction of three specialized agents (Profiler, Forecaster, and Predictor),\nCoMaPOI collaboratively addresses the two critical challenges. The Profiler\nagent is responsible for converting numeric data into language descriptions,\nenhancing semantic understanding. The Forecaster agent focuses on dynamically\nconstraining and refining the candidate POI space. The Predictor agent\nintegrates this information to generate high-precision predictions. Extensive\nexperiments on three benchmark datasets (NYC, TKY, and CA) demonstrate that\nCoMaPOI achieves state of the art performance, improving all metrics by 5% to\n10% compared to SOTA baselines. This work pioneers the investigation of\nchallenges associated with applying LLMs to complex spatiotemporal tasks by\nleveraging tailored collaborative agents."}
{"id": "2505.23838", "pdf": "https://arxiv.org/pdf/2505.23838.pdf", "abs": "https://arxiv.org/abs/2505.23838", "title": "Exploring the Landscape of Text-to-SQL with Large Language Models: Progresses, Challenges and Opportunities", "authors": ["Yiming Huang", "Jiyu Guo", "Wenxin Mao", "Cuiyun Gao", "Peiyi Han", "Chuanyi Liu", "Qing Ling"], "categories": ["cs.CL", "cs.IR"], "comment": "Submitted to ACM Computing Surveys (CSUR). Currently under review", "summary": "Converting natural language (NL) questions into SQL queries, referred to as\nText-to-SQL, has emerged as a pivotal technology for facilitating access to\nrelational databases, especially for users without SQL knowledge. Recent\nprogress in large language models (LLMs) has markedly propelled the field of\nnatural language processing (NLP), opening new avenues to improve text-to-SQL\nsystems. This study presents a systematic review of LLM-based text-to-SQL,\nfocusing on four key aspects: (1) an analysis of the research trends in\nLLM-based text-to-SQL; (2) an in-depth analysis of existing LLM-based\ntext-to-SQL techniques from diverse perspectives; (3) summarization of existing\ntext-to-SQL datasets and evaluation metrics; and (4) discussion on potential\nobstacles and avenues for future exploration in this domain. This survey seeks\nto furnish researchers with an in-depth understanding of LLM-based text-to-SQL,\nsparking new innovations and advancements in this field."}
{"id": "2505.23840", "pdf": "https://arxiv.org/pdf/2505.23840.pdf", "abs": "https://arxiv.org/abs/2505.23840", "title": "Measuring Sycophancy of Language Models in Multi-turn Dialogues", "authors": ["Jiseung Hong", "Grace Byun", "Seungone Kim", "Kai Shu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are expected to provide helpful and harmless\nresponses, yet they often exhibit sycophancy--conforming to user beliefs\nregardless of factual accuracy or ethical soundness. Prior research on\nsycophancy has primarily focused on single-turn factual correctness,\noverlooking the dynamics of real-world interactions. In this work, we introduce\nSYCON Bench, a novel benchmark for evaluating sycophantic behavior in\nmulti-turn, free-form conversational settings. Our benchmark measures how\nquickly a model conforms to the user (Turn of Flip) and how frequently it\nshifts its stance under sustained user pressure (Number of Flip). Applying\nSYCON Bench to 17 LLMs across three real-world scenarios, we find that\nsycophancy remains a prevalent failure mode. Our analysis shows that alignment\ntuning amplifies sycophantic behavior, whereas model scaling and reasoning\noptimization strengthen the model's ability to resist undesirable user views.\nReasoning models generally outperform instruction-tuned models but often fail\nwhen they over-index on logical exposition instead of directly addressing the\nuser's underlying beliefs. Finally, we evaluate four additional prompting\nstrategies and demonstrate that adopting a third-person perspective reduces\nsycophancy by up to 63.8% in debate scenario. We release our code and data at\nhttps://github.com/JiseungHong/SYCON-Bench."}
{"id": "2505.23842", "pdf": "https://arxiv.org/pdf/2505.23842.pdf", "abs": "https://arxiv.org/abs/2505.23842", "title": "Document Valuation in LLM Summaries: A Cluster Shapley Approach", "authors": ["Zikun Ye", "Hema Yoganarasimhan"], "categories": ["cs.CL", "econ.GN", "q-fin.EC"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used in systems that retrieve\nand summarize content from multiple sources, such as search engines and AI\nassistants. While these models enhance user experience by generating coherent\nsummaries, they obscure the contributions of original content creators, raising\nconcerns about credit attribution and compensation. We address the challenge of\nvaluing individual documents used in LLM-generated summaries. We propose using\nShapley values, a game-theoretic method that allocates credit based on each\ndocument's marginal contribution. Although theoretically appealing, Shapley\nvalues are expensive to compute at scale. We therefore propose Cluster Shapley,\nan efficient approximation algorithm that leverages semantic similarity between\ndocuments. By clustering documents using LLM-based embeddings and computing\nShapley values at the cluster level, our method significantly reduces\ncomputation while maintaining attribution quality. We demonstrate our approach\nto a summarization task using Amazon product reviews. Cluster Shapley\nsignificantly reduces computational complexity while maintaining high accuracy,\noutperforming baseline methods such as Monte Carlo sampling and Kernel SHAP\nwith a better efficient frontier. Our approach is agnostic to the exact LLM\nused, the summarization process used, and the evaluation procedure, which makes\nit broadly applicable to a variety of summarization settings."}
{"id": "2505.23843", "pdf": "https://arxiv.org/pdf/2505.23843.pdf", "abs": "https://arxiv.org/abs/2505.23843", "title": "Evaluation Hallucination in Multi-Round Incomplete Information Lateral-Driven Reasoning Tasks", "authors": ["Wenhan Dong", "Tianyi Hu", "Jingyi Zheng", "Zhen Sun", "Yuemeng Zhao", "Yule Liu", "Xinlei He", "Xinyi Huang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multi-round incomplete information tasks are crucial for evaluating the\nlateral thinking capabilities of large language models (LLMs). Currently,\nresearch primarily relies on multiple benchmarks and automated evaluation\nmetrics to assess these abilities. However, our study reveals novel insights\ninto the limitations of existing methods, as they often yield misleading\nresults that fail to uncover key issues, such as shortcut-taking behaviors,\nrigid patterns, and premature task termination. These issues obscure the true\nreasoning capabilities of LLMs and undermine the reliability of evaluations. To\naddress these limitations, we propose a refined set of evaluation standards,\nincluding inspection of reasoning paths, diversified assessment metrics, and\ncomparative analyses with human performance."}
{"id": "2505.23844", "pdf": "https://arxiv.org/pdf/2505.23844.pdf", "abs": "https://arxiv.org/abs/2505.23844", "title": "Enabling Flexible Multi-LLM Integration for Scalable Knowledge Aggregation", "authors": ["Zhenglun Kong", "Zheng Zhan", "Shiyue Hou", "Yifan Gong", "Xin Meng", "Pengwei Sui", "Peiyan Dong", "Xuan Shen", "Zifeng Wang", "Pu Zhao", "Hao Tang", "Stratis Ioannidis", "Yanzhi Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable promise but remain\nchallenging to continually improve through traditional finetuning, particularly\nwhen integrating capabilities from other specialized LLMs. Popular methods like\nensemble and weight merging require substantial memory and struggle to adapt to\nchanging data environments. Recent efforts have transferred knowledge from\nmultiple LLMs into a single target model; however, they suffer from\ninterference and degraded performance among tasks, largely due to limited\nflexibility in candidate selection and training pipelines. To address these\nissues, we propose a framework that adaptively selects and aggregates knowledge\nfrom diverse LLMs to build a single, stronger model, avoiding the high memory\noverhead of ensemble and inflexible weight merging. Specifically, we design an\nadaptive selection network that identifies the most relevant source LLMs based\non their scores, thereby reducing knowledge interference. We further propose a\ndynamic weighted fusion strategy that accounts for the inherent strengths of\ncandidate LLMs, along with a feedback-driven loss function that prevents the\nselector from converging on a single subset of sources. Experimental results\ndemonstrate that our method can enable a more stable and scalable knowledge\naggregation process while reducing knowledge interference by up to 50% compared\nto existing approaches. Code is avaliable at\nhttps://github.com/ZLKong/LLM_Integration"}
{"id": "2505.23845", "pdf": "https://arxiv.org/pdf/2505.23845.pdf", "abs": "https://arxiv.org/abs/2505.23845", "title": "Read Your Own Mind: Reasoning Helps Surface Self-Confidence Signals in LLMs", "authors": ["Jakub Podolak", "Rajeev Verma"], "categories": ["cs.CL"], "comment": null, "summary": "We study the source of uncertainty in DeepSeek R1-32B by analyzing its\nself-reported verbal confidence on question answering (QA) tasks. In the\ndefault answer-then-confidence setting, the model is regularly over-confident,\nwhereas semantic entropy - obtained by sampling many responses - remains\nreliable. We hypothesize that this is because of semantic entropy's larger\ntest-time compute, which lets us explore the model's predictive distribution.\nWe show that granting DeepSeek the budget to explore its distribution by\nforcing a long chain-of-thought before the final answer greatly improves its\nverbal score effectiveness, even on simple fact-retrieval questions that\nnormally require no reasoning. Furthermore, a separate reader model that sees\nonly the chain can reconstruct very similar confidences, indicating the verbal\nscore might be merely a statistic of the alternatives surfaced during\nreasoning. Our analysis concludes that reliable uncertainty estimation requires\nexplicit exploration of the generative space, and self-reported confidence is\ntrustworthy only after such exploration."}
{"id": "2505.23846", "pdf": "https://arxiv.org/pdf/2505.23846.pdf", "abs": "https://arxiv.org/abs/2505.23846", "title": "Scalable, Symbiotic, AI and Non-AI Agent Based Parallel Discrete Event Simulations", "authors": ["Atanu Barai", "Stephan Eidenbenz", "Nandakishore Santhi"], "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "To fully leverage the potential of artificial intelligence (AI) systems in a\ntrustworthy manner, it is desirable to couple multiple AI and non-AI systems\ntogether seamlessly for constraining and ensuring correctness of the output.\nThis paper introduces a novel parallel discrete event simulation (PDES) based\nmethodology to combine multiple AI and non-AI agents in a causal, rule-based\nway. Our approach tightly integrates the concept of passage of time, with each\nagent considered as an entity in the PDES framework and responding to prior\nrequests from other agents. Such coupling mechanism enables the agents to work\nin a co-operative environment towards a common goal while many tasks run in\nparallel throughout the simulation. It further enables setting up boundaries to\nthe outputs of the AI agents by applying necessary dynamic constraints using\nnon-AI agents while allowing for scalability through deployment of hundreds of\nsuch agents in a larger compute cluster. Distributing smaller AI agents can\nenable extremely scalable simulations in the future, addressing local memory\nbottlenecks for model parameter storage. Within a PDES involving both AI and\nnon-AI agents, we break down the problem at hand into structured steps, when\nnecessary, providing a set of multiple choices to the AI agents, and then\nprogressively solve these steps towards a final goal. At each step, the non-AI\nagents act as unbiased auditors, verifying each action by the AI agents so that\ncertain rules of engagement are followed. We evaluate our approach by solving\nfour problems from four different domains and comparing the results with those\nfrom AI models alone. Our results show greater accuracy in solving problems\nfrom various domains where the AI models struggle to solve the problems solely\nby themselves. Results show that overall accuracy of our approach is 68% where\nas the accuracy of vanilla models is less than 23%."}
{"id": "2505.23848", "pdf": "https://arxiv.org/pdf/2505.23848.pdf", "abs": "https://arxiv.org/abs/2505.23848", "title": "Derailing Non-Answers via Logit Suppression at Output Subspace Boundaries in RLHF-Aligned Language Models", "authors": ["Harvey Dam", "Jonas Knochelmann", "Vinu Joseph", "Ganesh Gopalakrishnan"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "We introduce a method to reduce refusal rates of large language models (LLMs)\non sensitive content without modifying model weights or prompts. Motivated by\nthe observation that refusals in certain models were often preceded by the\nspecific token sequence of a token marking the beginning of the\nchain-of-thought (CoT) block (<think>) followed by a double newline token\n(\\n\\n), we investigate the impact of two simple formatting adjustments during\ngeneration: suppressing \\n\\n after <think> and suppressing the end-of-sequence\ntoken after the end of the CoT block (</think>). Our method requires no\ndatasets, parameter changes, or training, relying solely on modifying token\nprobabilities during generation. In our experiments with official DeepSeek-R1\ndistillations, these interventions increased the proportion of substantive\nanswers to sensitive prompts without affecting performance on standard\nbenchmarks. Our findings suggest that refusal behaviors can be circumvented by\nblocking refusal subspaces at specific points in the generation process."}
{"id": "2505.23851", "pdf": "https://arxiv.org/pdf/2505.23851.pdf", "abs": "https://arxiv.org/abs/2505.23851", "title": "ASyMOB: Algebraic Symbolic Mathematical Operations Benchmark", "authors": ["Michael Shalyt", "Rotem Elimelech", "Ido Kaminer"], "categories": ["cs.CL", "cs.AI", "cs.SC"], "comment": "Code repository: https://github.com/RamanujanMachine/ASyMOB Complete\n  benchmark dataset:\n  https://huggingface.co/datasets/Shalyt/ASyMOB-Algebraic_Symbolic_Mathematical_Operations_Benchmark", "summary": "Large language models (LLMs) are rapidly approaching the level of proficiency\nin university-level symbolic mathematics required for applications in advanced\nscience and technology. However, existing benchmarks fall short in assessing\nthe core skills of LLMs in symbolic mathematics-such as integration,\ndifferential equations, and algebraic simplification. To address this gap, we\nintroduce ASyMOB, a novel assessment framework focused exclusively on symbolic\nmanipulation, featuring 17,092 unique math challenges, organized by similarity\nand complexity. ASyMOB enables analysis of LLM generalization capabilities by\ncomparing performance in problems that differ by simple numerical or symbolic\n`perturbations'. Evaluated LLMs exhibit substantial degradation in performance\nfor all perturbation types (up to -70.3%), suggesting reliance on memorized\npatterns rather than deeper understanding of symbolic math, even among models\nachieving high baseline accuracy. Comparing LLM performance to computer algebra\nsystems, we identify examples where they fail while LLMs succeed, as well as\nproblems solved only by combining both approaches. Models capable of integrated\ncode execution yielded higher accuracy compared to their performance without\ncode, particularly stabilizing weaker models (up to +33.1% for certain\nperturbation types). Notably, the most advanced models (o4-mini, Gemini 2.5\nFlash) demonstrate not only high symbolic math proficiency (scoring 96.8% and\n97.6% on the unperturbed set), but also remarkable robustness against\nperturbations, (-21.7% and -21.2% vs. average -50.4% for the other models).\nThis may indicate a recent \"phase transition\" in the generalization\ncapabilities of frontier LLMs. It remains to be seen whether the path forward\nlies in deeper integration with sophisticated external tools, or in developing\nmodels so capable that symbolic math systems like CAS become unnecessary."}
{"id": "2505.23852", "pdf": "https://arxiv.org/pdf/2505.23852.pdf", "abs": "https://arxiv.org/abs/2505.23852", "title": "Large Language Model-Based Agents for Automated Research Reproducibility: An Exploratory Study in Alzheimer's Disease", "authors": ["Nic Dobbins", "Christelle Xiong", "Kristine Lan", "Meliha Yetisgen"], "categories": ["cs.CL", "cs.AI", "cs.MA", "stat.AP"], "comment": null, "summary": "Objective: To demonstrate the capabilities of Large Language Models (LLMs) as\nautonomous agents to reproduce findings of published research studies using the\nsame or similar dataset.\n  Materials and Methods: We used the \"Quick Access\" dataset of the National\nAlzheimer's Coordinating Center (NACC). We identified highly cited published\nresearch manuscripts using NACC data and selected five studies that appeared\nreproducible using this dataset alone. Using GPT-4o, we created a simulated\nresearch team of LLM-based autonomous agents tasked with writing and executing\ncode to dynamically reproduce the findings of each study, given only study\nAbstracts, Methods sections, and data dictionary descriptions of the dataset.\n  Results: We extracted 35 key findings described in the Abstracts across 5\nAlzheimer's studies. On average, LLM agents approximately reproduced 53.2% of\nfindings per study. Numeric values and range-based findings often differed\nbetween studies and agents. The agents also applied statistical methods or\nparameters that varied from the originals, though overall trends and\nsignificance were sometimes similar.\n  Discussion: In some cases, LLM-based agents replicated research techniques\nand findings. In others, they failed due to implementation flaws or missing\nmethodological detail. These discrepancies show the current limits of LLMs in\nfully automating reproducibility assessments. Still, this early investigation\nhighlights the potential of structured agent-based systems to provide scalable\nevaluation of scientific rigor.\n  Conclusion: This exploratory work illustrates both the promise and\nlimitations of LLMs as autonomous agents for automating reproducibility in\nbiomedical research."}
{"id": "2505.23854", "pdf": "https://arxiv.org/pdf/2505.23854.pdf", "abs": "https://arxiv.org/abs/2505.23854", "title": "Revisiting Uncertainty Estimation and Calibration of Large Language Models", "authors": ["Linwei Tao", "Yi-Fan Yeh", "Minjing Dong", "Tao Huang", "Philip Torr", "Chang Xu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in high-stakes\napplications, robust uncertainty estimation is essential for ensuring the safe\nand trustworthy deployment of LLMs. We present the most comprehensive study to\ndate of uncertainty estimation in LLMs, evaluating 80 models spanning open- and\nclosed-source families, dense and Mixture-of-Experts (MoE) architectures,\nreasoning and non-reasoning modes, quantization variants and parameter scales\nfrom 0.6B to 671B. Focusing on three representative black-box single-pass\nmethods, including token probability-based uncertainty (TPU), numerical verbal\nuncertainty (NVU), and linguistic verbal uncertainty (LVU), we systematically\nevaluate uncertainty calibration and selective classification using the\nchallenging MMLU-Pro benchmark, which covers both reasoning-intensive and\nknowledge-based tasks. Our results show that LVU consistently outperforms TPU\nand NVU, offering stronger calibration and discrimination while being more\ninterpretable. We also find that high accuracy does not imply reliable\nuncertainty, and that model scale, post-training, reasoning ability and\nquantization all influence estimation performance. Notably, LLMs exhibit better\nuncertainty estimates on reasoning tasks than on knowledge-heavy ones, and good\ncalibration does not necessarily translate to effective error ranking. These\nfindings highlight the need for multi-perspective evaluation and position LVU\nas a practical tool for improving the reliability of LLMs in real-world\nsettings."}
{"id": "2505.23856", "pdf": "https://arxiv.org/pdf/2505.23856.pdf", "abs": "https://arxiv.org/abs/2505.23856", "title": "OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities", "authors": ["Sahil Verma", "Keegan Hines", "Jeff Bilmes", "Charlotte Siska", "Luke Zettlemoyer", "Hila Gonen", "Chandan Singh"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "The emerging capabilities of large language models (LLMs) have sparked\nconcerns about their immediate potential for harmful misuse. The core approach\nto mitigate these concerns is the detection of harmful queries to the model.\nCurrent detection approaches are fallible, and are particularly susceptible to\nattacks that exploit mismatched generalization of model capabilities (e.g.,\nprompts in low-resource languages or prompts provided in non-text modalities\nsuch as image and audio). To tackle this challenge, we propose OMNIGUARD, an\napproach for detecting harmful prompts across languages and modalities. Our\napproach (i) identifies internal representations of an LLM/MLLM that are\naligned across languages or modalities and then (ii) uses them to build a\nlanguage-agnostic or modality-agnostic classifier for detecting harmful\nprompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57\\%\nover the strongest baseline in a multilingual setting, by 20.44\\% for\nimage-based prompts, and sets a new SOTA for audio-based prompts. By\nrepurposing embeddings computed during generation, OMNIGUARD is also very\nefficient ($\\approx 120 \\times$ faster than the next fastest baseline). Code\nand data are available at: https://github.com/vsahil/OmniGuard."}
{"id": "2505.23867", "pdf": "https://arxiv.org/pdf/2505.23867.pdf", "abs": "https://arxiv.org/abs/2505.23867", "title": "Infi-Med: Low-Resource Medical MLLMs with Robust Reasoning Evaluation", "authors": ["Zeyu Liu", "Zhitian Hou", "Yining Di", "Kejing Yang", "Zhijie Sang", "Congkai Xie", "Jingwen Yang", "Siyuan Liu", "Jialu Wang", "Chunming Li", "Ming Li", "Hongxia Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal large language models (MLLMs) have demonstrated promising\nprospects in healthcare, particularly for addressing complex medical tasks,\nsupporting multidisciplinary treatment (MDT), and enabling personalized\nprecision medicine. However, their practical deployment faces critical\nchallenges in resource efficiency, diagnostic accuracy, clinical\nconsiderations, and ethical privacy. To address these limitations, we propose\nInfi-Med, a comprehensive framework for medical MLLMs that introduces three key\ninnovations: (1) a resource-efficient approach through curating and\nconstructing high-quality supervised fine-tuning (SFT) datasets with minimal\nsample requirements, with a forward-looking design that extends to both\npretraining and posttraining phases; (2) enhanced multimodal reasoning\ncapabilities for cross-modal integration and clinical task understanding; and\n(3) a systematic evaluation system that assesses model performance across\nmedical modalities and task types. Our experiments demonstrate that Infi-Med\nachieves state-of-the-art (SOTA) performance in general medical reasoning while\nmaintaining rapid adaptability to clinical scenarios. The framework establishes\na solid foundation for deploying MLLMs in real-world healthcare settings by\nbalancing model effectiveness with operational constraints."}
{"id": "2505.23911", "pdf": "https://arxiv.org/pdf/2505.23911.pdf", "abs": "https://arxiv.org/abs/2505.23911", "title": "One Task Vector is not Enough: A Large-Scale Study for In-Context Learning", "authors": ["Pavel Tikhonov", "Ivan Oseledets", "Elena Tutubalina"], "categories": ["cs.CL"], "comment": null, "summary": "In-context learning (ICL) enables Large Language Models (LLMs) to adapt to\nnew tasks using few examples, with task vectors - specific hidden state\nactivations - hypothesized to encode task information. Existing studies are\nlimited by small-scale benchmarks, restricting comprehensive analysis. We\nintroduce QuiteAFew, a novel dataset of 3,096 diverse few-shot tasks, each with\n30 input-output pairs derived from the Alpaca dataset. Experiments with\nLlama-3-8B on QuiteAFew reveal: (1) task vector performance peaks at an\nintermediate layer (e.g., 15th), (2) effectiveness varies significantly by task\ntype, and (3) complex tasks rely on multiple, subtask-specific vectors rather\nthan a single vector, suggesting distributed task knowledge representation."}
{"id": "2505.23912", "pdf": "https://arxiv.org/pdf/2505.23912.pdf", "abs": "https://arxiv.org/abs/2505.23912", "title": "Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation", "authors": ["Caiqi Zhang", "Xiaochen Zhu", "Chengzu Li", "Nigel Collier", "Andreas Vlachos"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hallucination remains a major challenge for the safe and trustworthy\ndeployment of large language models (LLMs) in factual content generation. Prior\nwork has explored confidence estimation as an effective approach to\nhallucination detection, but often relies on post-hoc self-consistency methods\nthat require computationally expensive sampling. Verbalized confidence offers a\nmore efficient alternative, but existing approaches are largely limited to\nshort-form question answering (QA) tasks and do not generalize well to\nopen-ended generation. In this paper, we propose LoVeC (Long-form Verbalized\nConfidence), an on-the-fly verbalized confidence estimation method for\nlong-form generation. Specifically, we use reinforcement learning (RL) to train\nLLMs to append numerical confidence scores to each generated statement, serving\nas a direct and interpretable signal of the factuality of generation. Our\nexperiments consider both on-policy and off-policy RL methods, including DPO,\nORPO, and GRPO, to enhance the model calibration. We introduce two novel\nevaluation settings, free-form tagging and iterative tagging, to assess\ndifferent verbalized confidence estimation methods. Experiments on three\nlong-form QA datasets show that our RL-trained models achieve better\ncalibration and generalize robustly across domains. Also, our method is highly\nefficient, as it only requires adding a few tokens to the output being decoded."}
{"id": "2505.23914", "pdf": "https://arxiv.org/pdf/2505.23914.pdf", "abs": "https://arxiv.org/abs/2505.23914", "title": "Probing Association Biases in LLM Moderation Over-Sensitivity", "authors": ["Yuxin Wang", "Botao Yu", "Ivory Yang", "Saeed Hassanpour", "Soroush Vosoughi"], "categories": ["cs.CL", "cs.AI"], "comment": "Under review", "summary": "Large Language Models are widely used for content moderation but often\nmisclassify benign comments as toxic, leading to over-sensitivity. While\nprevious research attributes this issue primarily to the presence of offensive\nterms, we reveal a potential cause beyond token level: LLMs exhibit systematic\ntopic biases in their implicit associations. Inspired by cognitive psychology's\nimplicit association tests, we introduce Topic Association Analysis, a\nsemantic-level approach to quantify how LLMs associate certain topics with\ntoxicity. By prompting LLMs to generate free-form scenario imagination for\nmisclassified benign comments and analyzing their topic amplification levels,\nwe find that more advanced models (e.g., GPT-4 Turbo) demonstrate stronger\ntopic stereotype despite lower overall false positive rates. These biases\nsuggest that LLMs do not merely react to explicit, offensive language but rely\non learned topic associations, shaping their moderation decisions. Our findings\nhighlight the need for refinement beyond keyword-based filtering, providing\ninsights into the underlying mechanisms driving LLM over-sensitivity."}
{"id": "2505.23923", "pdf": "https://arxiv.org/pdf/2505.23923.pdf", "abs": "https://arxiv.org/abs/2505.23923", "title": "ChARM: Character-based Act-adaptive Reward Modeling for Advanced Role-Playing Language Agents", "authors": ["Feiteng Fang", "Ting-En Lin", "Yuchuan Wu", "Xiong Liu", "Xiang Huang", "Dingwei Chen", "Jing Ye", "Haonan Zhang", "Liang Zhu", "Hamid Alinejad-Rokny", "Min Yang", "Fei Huang", "Yongbin Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Role-Playing Language Agents (RPLAs) aim to simulate characters for realistic\nand engaging human-computer interactions. However, traditional reward models\noften struggle with scalability and adapting to subjective conversational\npreferences. We propose ChARM, a Character-based Act-adaptive Reward Model,\naddressing these challenges through two innovations: (1) an act-adaptive margin\nthat significantly enhances learning efficiency and generalizability, and (2) a\nself-evolution mechanism leveraging large-scale unlabeled data to improve\ntraining coverage. Additionally, we introduce RoleplayPref, the first\nlarge-scale preference dataset specifically for RPLAs, featuring 1,108\ncharacters, 13 subcategories, and 16,888 bilingual dialogues, alongside\nRoleplayEval, a dedicated evaluation benchmark. Experimental results show a 13%\nimprovement over the conventional Bradley-Terry model in preference rankings.\nFurthermore, applying ChARM-generated rewards to preference learning techniques\n(e.g., direct preference optimization) achieves state-of-the-art results on\nCharacterEval and RoleplayEval. Code and dataset are available at\nhttps://github.com/calubkk/ChARM."}
{"id": "2505.23931", "pdf": "https://arxiv.org/pdf/2505.23931.pdf", "abs": "https://arxiv.org/abs/2505.23931", "title": "Scaling up the think-aloud method", "authors": ["Daniel Wurgaft", "Ben Prystawski", "Kanishk Gandhi", "Cedegao E. Zhang", "Joshua B. Tenenbaum", "Noah D. Goodman"], "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 4 figures. Daniel Wurgaft and Ben Prystawski contributed\n  equally", "summary": "The think-aloud method, where participants voice their thoughts as they solve\na task, is a valuable source of rich data about human reasoning processes. Yet,\nit has declined in popularity in contemporary cognitive science, largely\nbecause labor-intensive transcription and annotation preclude large sample\nsizes. Here, we develop methods to automate the transcription and annotation of\nverbal reports of reasoning using natural language processing tools, allowing\nfor large-scale analysis of think-aloud data. In our study, 640 participants\nthought aloud while playing the Game of 24, a mathematical reasoning task. We\nautomatically transcribed the recordings and coded the transcripts as search\ngraphs, finding moderate inter-rater reliability with humans. We analyze these\ngraphs and characterize consistency and variation in human reasoning traces.\nOur work demonstrates the value of think-aloud data at scale and serves as a\nproof of concept for the automated analysis of verbal reports."}
{"id": "2505.23932", "pdf": "https://arxiv.org/pdf/2505.23932.pdf", "abs": "https://arxiv.org/abs/2505.23932", "title": "SwingArena: Competitive Programming Arena for Long-context GitHub Issue Solving", "authors": ["Wendong Xu", "Jing Xiong", "Chenyang Zhao", "Qiujiang Chen", "Haoran Wang", "Hui Shen", "Zhongwei Wan", "Jianbo Dai", "Taiqiang Wu", "He Xiao", "Chaofan Tao", "Z. Morley Mao", "Ying Sheng", "Zhijiang Guo", "Hongxia Yang", "Bei Yu", "Lingpeng Kong", "Quanquan Gu", "Ngai Wong"], "categories": ["cs.CL"], "comment": null, "summary": "We present SwingArena, a competitive evaluation framework for Large Language\nModels (LLMs) that closely mirrors real-world software development workflows.\nUnlike traditional static benchmarks, SwingArena models the collaborative\nprocess of software iteration by pairing LLMs as submitters, who generate\npatches, and reviewers, who create test cases and verify the patches through\ncontinuous integration (CI) pipelines. To support these interactive\nevaluations, we introduce a retrieval-augmented code generation (RACG) module\nthat efficiently handles long-context challenges by providing syntactically and\nsemantically relevant code snippets from large codebases, supporting multiple\nprogramming languages (C++, Python, Rust, and Go). This enables the framework\nto scale across diverse tasks and contexts while respecting token limitations.\nOur experiments, using over 400 high-quality real-world GitHub issues selected\nfrom a pool of 2,300 issues, show that models like GPT-4o excel at aggressive\npatch generation, whereas DeepSeek and Gemini prioritize correctness in CI\nvalidation. SwingArena presents a scalable and extensible methodology for\nevaluating LLMs in realistic, CI-driven software development settings. More\ndetails are available on our project page: swing-bench.github.io"}
{"id": "2505.23944", "pdf": "https://arxiv.org/pdf/2505.23944.pdf", "abs": "https://arxiv.org/abs/2505.23944", "title": "Retrieval Augmented Generation based Large Language Models for Causality Mining", "authors": ["Thushara Manjari Naduvilakandy", "Hyeju Jang", "Mohammad Al Hasan"], "categories": ["cs.CL"], "comment": "13 pages, 6 figures, published in knowledgeNLP-NAACL2025", "summary": "Causality detection and mining are important tasks in information retrieval\ndue to their enormous use in information extraction, and knowledge graph\nconstruction. To solve these tasks, in existing literature there exist several\nsolutions -- both unsupervised and supervised. However, the unsupervised\nmethods suffer from poor performance and they often require significant human\nintervention for causal rule selection, leading to poor generalization across\ndifferent domains. On the other hand, supervised methods suffer from the lack\nof large training datasets. Recently, large language models (LLMs) with\neffective prompt engineering are found to be effective to overcome the issue of\nunavailability of large training dataset. Yet, in existing literature, there\ndoes not exist comprehensive works on causality detection and mining using LLM\nprompting. In this paper, we present several retrieval-augmented generation\n(RAG) based dynamic prompting schemes to enhance LLM performance in causality\ndetection and extraction tasks. Extensive experiments over three datasets and\nfive LLMs validate the superiority of our proposed RAG-based dynamic prompting\nover other static prompting schemes."}
{"id": "2505.23945", "pdf": "https://arxiv.org/pdf/2505.23945.pdf", "abs": "https://arxiv.org/abs/2505.23945", "title": "A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models", "authors": ["Sriram Balasubramanian", "Samyadeep Basu", "Soheil Feizi"], "categories": ["cs.CL", "cs.AI", "I.2.10; I.2.7"], "comment": "34 pages, 25 figures", "summary": "Chain-of-thought (CoT) reasoning enhances performance of large language\nmodels, but questions remain about whether these reasoning traces faithfully\nreflect the internal processes of the model. We present the first comprehensive\nstudy of CoT faithfulness in large vision-language models (LVLMs),\ninvestigating how both text-based and previously unexplored image-based biases\naffect reasoning and bias articulation. Our work introduces a novel,\nfine-grained evaluation pipeline for categorizing bias articulation patterns,\nenabling significantly more precise analysis of CoT reasoning than previous\nmethods. This framework reveals critical distinctions in how models process and\nrespond to different types of biases, providing new insights into LVLM CoT\nfaithfulness. Our findings reveal that subtle image-based biases are rarely\narticulated compared to explicit text-based ones, even in models specialized\nfor reasoning. Additionally, many models exhibit a previously unidentified\nphenomenon we term ``inconsistent'' reasoning - correctly reasoning before\nabruptly changing answers, serving as a potential canary for detecting biased\nreasoning from unfaithful CoTs. We then apply the same evaluation pipeline to\nrevisit CoT faithfulness in LLMs across various levels of implicit cues. Our\nfindings reveal that current language-only reasoning models continue to\nstruggle with articulating cues that are not overtly stated."}
{"id": "2505.23966", "pdf": "https://arxiv.org/pdf/2505.23966.pdf", "abs": "https://arxiv.org/abs/2505.23966", "title": "FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for Large Language Model Compression", "authors": ["Jiayi Tian", "Ryan Solgi", "Jinming Lu", "Yifan Yang", "Hai Li", "Zheng Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have enabled remarkable progress in natural\nlanguage processing, yet their high computational and memory demands pose\nchallenges for deployment in resource-constrained environments. Although recent\nlow-rank decomposition methods offer a promising path for structural\ncompression, they often suffer from accuracy degradation, expensive calibration\nprocedures, and result in inefficient model architectures that hinder\nreal-world inference speedups. In this paper, we propose FLAT-LLM, a fast and\naccurate, training-free structural compression method based on fine-grained\nlow-rank transformations in the activation space. Specifically, we reduce the\nhidden dimension by transforming the weights using truncated eigenvectors\ncomputed via head-wise Principal Component Analysis (PCA), and employ an\nimportance-based metric to adaptively allocate ranks across decoders. FLAT-LLM\nachieves efficient and effective weight compression without recovery\nfine-tuning, which could complete the calibration within a few minutes.\nEvaluated across 4 models and 11 datasets, FLAT-LLM outperforms structural\npruning baselines in generalization and downstream performance, while\ndelivering inference speedups over decomposition-based methods."}
{"id": "2505.23996", "pdf": "https://arxiv.org/pdf/2505.23996.pdf", "abs": "https://arxiv.org/abs/2505.23996", "title": "Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for LLMs", "authors": ["Yinong Oliver Wang", "Nivedha Sivakumar", "Falaah Arif Khan", "Rin Metcalf Susa", "Adam Golinski", "Natalie Mackraz", "Barry-John Theobald", "Luca Zappella", "Nicholas Apostoloff"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, 8 figures, and 1 table in main paper. Supplementary appendix\n  attached. Accepted at ICML 2025", "summary": "The recent rapid adoption of large language models (LLMs) highlights the\ncritical need for benchmarking their fairness. Conventional fairness metrics,\nwhich focus on discrete accuracy-based evaluations (i.e., prediction\ncorrectness), fail to capture the implicit impact of model uncertainty (e.g.,\nhigher model confidence about one group over another despite similar accuracy).\nTo address this limitation, we propose an uncertainty-aware fairness metric,\nUCerF, to enable a fine-grained evaluation of model fairness that is more\nreflective of the internal bias in model decisions compared to conventional\nfairness measures. Furthermore, observing data size, diversity, and clarity\nissues in current datasets, we introduce a new gender-occupation fairness\nevaluation dataset with 31,756 samples for co-reference resolution, offering a\nmore diverse and suitable dataset for evaluating modern LLMs. We establish a\nbenchmark, using our metric and dataset, and apply it to evaluate the behavior\nof ten open-source LLMs. For example, Mistral-7B exhibits suboptimal fairness\ndue to high confidence in incorrect predictions, a detail overlooked by\nEqualized Odds but captured by UCerF. Overall, our proposed LLM benchmark,\nwhich evaluates fairness with uncertainty awareness, paves the way for\ndeveloping more transparent and accountable AI systems."}
{"id": "2505.24009", "pdf": "https://arxiv.org/pdf/2505.24009.pdf", "abs": "https://arxiv.org/abs/2505.24009", "title": "Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws", "authors": ["Hidetaka Kamigaito", "Ying Zhang", "Jingun Kwon", "Katsuhiko Hayashi", "Manabu Okumura", "Taro Watanabe"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Transformers deliver outstanding performance across a wide range of tasks and\nare now a dominant backbone architecture for large language models (LLMs).\nTheir task-solving performance is improved by increasing parameter size, as\nshown in the recent studies on parameter scaling laws. Although recent\nmechanistic-interpretability studies have deepened our understanding of the\ninternal behavior of Transformers by analyzing their residual stream, the\nrelationship between these internal mechanisms and the parameter scaling laws\nremains unclear. To bridge this gap, we focus on layers and their size, which\nmainly decide the parameter size of Transformers. For this purpose, we first\ntheoretically investigate the layers within the residual stream through a\nbias-diversity decomposition. The decomposition separates (i) bias, the error\nof each layer's output from the ground truth, and (ii) diversity, which\nindicates how much the outputs of each layer differ from each other. Analyzing\nTransformers under this theory reveals that performance improves when\nindividual layers make predictions close to the correct answer and remain\nmutually diverse. We show that diversity becomes especially critical when\nindividual layers' outputs are far from the ground truth. Finally, we introduce\nan information-theoretic diversity and show our main findings that adding\nlayers enhances performance only when those layers behave differently, i.e.,\nare diverse. We also reveal the performance gains from increasing the number of\nlayers exhibit submodularity: marginal improvements diminish as additional\nlayers increase, mirroring the logarithmic convergence predicted by the\nparameter scaling laws. Experiments on multiple semantic-understanding tasks\nwith various LLMs empirically confirm the theoretical properties derived in\nthis study."}
{"id": "2505.24012", "pdf": "https://arxiv.org/pdf/2505.24012.pdf", "abs": "https://arxiv.org/abs/2505.24012", "title": "Large Language Model Meets Constraint Propagation", "authors": ["Alexandre Bonlarron", "Florian Régin", "Elisabetta De Maria", "Jean-Charles Régin"], "categories": ["cs.CL", "cs.AI"], "comment": "To appear in the Proceedings of the Thirty-Fourth International Joint\n  Conference on Artificial Intelligence (IJCAI 2025)", "summary": "Large Language Models (LLMs) excel at generating fluent text but struggle to\nenforce external constraints because they generate tokens sequentially without\nexplicit control mechanisms. GenCP addresses this limitation by combining LLM\npredictions with Constraint Programming (CP) reasoning, formulating text\ngeneration as a Constraint Satisfaction Problem (CSP). In this paper, we\nimprove GenCP by integrating Masked Language Models (MLMs) for domain\ngeneration, which allows bidirectional constraint propagation that leverages\nboth past and future tokens. This integration bridges the gap between\ntoken-level prediction and structured constraint enforcement, leading to more\nreliable and constraint-aware text generation. Our evaluation on COLLIE\nbenchmarks demonstrates that incorporating domain preview via MLM calls\nsignificantly improves GenCP's performance. Although this approach incurs\nadditional MLM calls and, in some cases, increased backtracking, the overall\neffect is a more efficient use of LLM inferences and an enhanced ability to\ngenerate feasible and meaningful solutions, particularly in tasks with strict\ncontent constraints."}
{"id": "2505.24016", "pdf": "https://arxiv.org/pdf/2505.24016.pdf", "abs": "https://arxiv.org/abs/2505.24016", "title": "BeaverTalk: Oregon State University's IWSLT 2025 Simultaneous Speech Translation System", "authors": ["Matthew Raffel", "Victor Agostinelli", "Lizhong Chen"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at IWSLT 2025", "summary": "This paper discusses the construction, fine-tuning, and deployment of\nBeaverTalk, a cascaded system for speech-to-text translation as part of the\nIWSLT 2025 simultaneous translation task. The system architecture employs a VAD\nsegmenter for breaking a speech stream into segments, Whisper Large V2 for\nautomatic speech recognition (ASR), and Gemma 3 12B for simultaneous\ntranslation. Regarding the simultaneous translation LLM, it is fine-tuned via\nlow-rank adaptors (LoRAs) for a conversational prompting strategy that\nleverages a single prior-sentence memory bank from the source language as\ncontext. The cascaded system participated in the English$\\rightarrow$German and\nEnglish$\\rightarrow$Chinese language directions for both the low and high\nlatency regimes. In particular, on the English$\\rightarrow$German task, the\nsystem achieves a BLEU of 24.64 and 27.83 at a StreamLAAL of 1837.86 and\n3343.73, respectively. Then, on the English$\\rightarrow$Chinese task, the\nsystem achieves a BLEU of 34.07 and 37.23 at a StreamLAAL of 2216.99 and\n3521.35, respectively."}
{"id": "2505.24028", "pdf": "https://arxiv.org/pdf/2505.24028.pdf", "abs": "https://arxiv.org/abs/2505.24028", "title": "Hidden Persuasion: Detecting Manipulative Narratives on Social Media During the 2022 Russian Invasion of Ukraine", "authors": ["Kateryna Akhynko", "Oleksandr Kosovan", "Mykola Trokhymovych"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents one of the top-performing solutions to the UNLP 2025\nShared Task on Detecting Manipulation in Social Media. The task focuses on\ndetecting and classifying rhetorical and stylistic manipulation techniques used\nto influence Ukrainian Telegram users. For the classification subtask, we\nfine-tuned the Gemma 2 language model with LoRA adapters and applied a\nsecond-level classifier leveraging meta-features and threshold optimization.\nFor span detection, we employed an XLM-RoBERTa model trained for multi-target,\nincluding token binary classification. Our approach achieved 2nd place in\nclassification and 3rd place in span detection."}
{"id": "2505.24033", "pdf": "https://arxiv.org/pdf/2505.24033.pdf", "abs": "https://arxiv.org/abs/2505.24033", "title": "The Surprising Soupability of Documents in State Space Models", "authors": ["Yasaman Jafari", "Zixian Wang", "Leon Bergen", "Taylor Berg-Kirkpatrick"], "categories": ["cs.CL", "cs.CE", "cs.LG"], "comment": null, "summary": "We investigate whether hidden states from Structured State Space Models\n(SSMs) can be merged post-hoc to support downstream reasoning. Inspired by\nmodel souping, we propose a strategy where documents are encoded independently\nand their representations are pooled -- via simple operations like averaging --\ninto a single context state. This approach, which we call document souping,\nenables modular encoding and reuse without reprocessing the full input for each\nquery. We finetune Mamba2 models to produce soupable representations and find\nthat they support multi-hop QA, sparse retrieval, and long-document reasoning\nwith strong accuracy. On HotpotQA, souping ten independently encoded documents\nnearly matches the performance of a cross-encoder trained on the same inputs."}
{"id": "2505.24040", "pdf": "https://arxiv.org/pdf/2505.24040.pdf", "abs": "https://arxiv.org/abs/2505.24040", "title": "MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question Answering", "authors": ["Yuexing Hao", "Kumail Alhamoud", "Hyewon Jeong", "Haoran Zhang", "Isha Puri", "Philip Torr", "Mike Schaekermann", "Ariel D. Stern", "Marzyeh Ghassemi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable performance on\nvarious medical question-answering (QA) benchmarks, including standardized\nmedical exams. However, correct answers alone do not ensure correct logic, and\nmodels may reach accurate conclusions through flawed processes. In this study,\nwe introduce the MedPAIR (Medical Dataset Comparing Physicians and AI Relevance\nEstimation and Question Answering) dataset to evaluate how physician trainees\nand LLMs prioritize relevant information when answering QA questions. We obtain\nannotations on 1,300 QA pairs from 36 physician trainees, labeling each\nsentence within the question components for relevance. We compare these\nrelevance estimates to those for LLMs, and further evaluate the impact of these\n\"relevant\" subsets on downstream task performance for both physician trainees\nand LLMs. We find that LLMs are frequently not aligned with the content\nrelevance estimates of physician trainees. After filtering out physician\ntrainee-labeled irrelevant sentences, accuracy improves for both the trainees\nand the LLMs. All LLM and physician trainee-labeled data are available at:\nhttp://medpair.csail.mit.edu/."}
{"id": "2505.24063", "pdf": "https://arxiv.org/pdf/2505.24063.pdf", "abs": "https://arxiv.org/abs/2505.24063", "title": "TCM-Ladder: A Benchmark for Multimodal Question Answering on Traditional Chinese Medicine", "authors": ["Jiacheng Xie", "Yang Yu", "Ziyang Zhang", "Shuai Zeng", "Jiaxuan He", "Ayush Vasireddy", "Xiaoting Tang", "Congyu Guo", "Lening Zhao", "Congcong Jing", "Guanghui An", "Dong Xu"], "categories": ["cs.CL", "cs.DB"], "comment": "22 pages, 4 figures", "summary": "Traditional Chinese Medicine (TCM), as an effective alternative medicine, has\nbeen receiving increasing attention. In recent years, the rapid development of\nlarge language models (LLMs) tailored for TCM has underscored the need for an\nobjective and comprehensive evaluation framework to assess their performance on\nreal-world tasks. However, existing evaluation datasets are limited in scope\nand primarily text-based, lacking a unified and standardized multimodal\nquestion-answering (QA) benchmark. To address this issue, we introduce\nTCM-Ladder, the first multimodal QA dataset specifically designed for\nevaluating large TCM language models. The dataset spans multiple core\ndisciplines of TCM, including fundamental theory, diagnostics, herbal formulas,\ninternal medicine, surgery, pharmacognosy, and pediatrics. In addition to\ntextual content, TCM-Ladder incorporates various modalities such as images and\nvideos. The datasets were constructed using a combination of automated and\nmanual filtering processes and comprise 52,000+ questions in total. These\nquestions include single-choice, multiple-choice, fill-in-the-blank, diagnostic\ndialogue, and visual comprehension tasks. We trained a reasoning model on\nTCM-Ladder and conducted comparative experiments against 9 state-of-the-art\ngeneral domain and 5 leading TCM-specific LLMs to evaluate their performance on\nthe datasets. Moreover, we propose Ladder-Score, an evaluation method\nspecifically designed for TCM question answering that effectively assesses\nanswer quality regarding terminology usage and semantic expression. To our\nknowledge, this is the first work to evaluate mainstream general domain and\nTCM-specific LLMs on a unified multimodal benchmark. The datasets and\nleaderboard are publicly available at https://tcmladder.com or\nhttps://54.211.107.106 and will be continuously updated."}
{"id": "2505.24098", "pdf": "https://arxiv.org/pdf/2505.24098.pdf", "abs": "https://arxiv.org/abs/2505.24098", "title": "HardTests: Synthesizing High-Quality Test Cases for LLM Coding", "authors": ["Zhongmou He", "Yee Man Choi", "Kexun Zhang", "Jiabao Ji", "Junting Zhou", "Dejia Xu", "Ivan Bercovich", "Aidan Zhang", "Lei Li"], "categories": ["cs.CL"], "comment": null, "summary": "Verifiers play a crucial role in large language model (LLM) reasoning, needed\nby post-training techniques such as reinforcement learning. However, reliable\nverifiers are hard to get for difficult coding problems, because a\nwell-disguised wrong solution may only be detected by carefully human-written\nedge cases that are difficult to synthesize. To address this issue, we propose\nHARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this\npipeline, we curate a comprehensive competitive programming dataset HARDTESTS\nwith 47k problems and synthetic high-quality tests. Compared with existing\ntests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points\nhigher and recall that is 17.5 percentage points higher when evaluating\nLLM-generated code. For harder problems, the improvement in precision can be as\nlarge as 40 points. HARDTESTS also proves to be more effective for model\ntraining, measured by downstream code generation performance. We will\nopen-source our dataset and synthesis pipeline at\nhttps://leililab.github.io/HardTests/."}
{"id": "2505.24105", "pdf": "https://arxiv.org/pdf/2505.24105.pdf", "abs": "https://arxiv.org/abs/2505.24105", "title": "Training LLMs for EHR-Based Reasoning Tasks via Reinforcement Learning", "authors": ["Jiacheng Lin", "Zhenbang Wu", "Jimeng Sun"], "categories": ["cs.CL"], "comment": null, "summary": "We present EHRMIND, a practical recipe for adapting large language models\n(LLMs) to complex clinical reasoning tasks using reinforcement learning with\nverifiable rewards (RLVR). While RLVR has succeeded in mathematics and coding,\nits application to healthcare contexts presents unique challenges due to the\nspecialized knowledge and reasoning required for electronic health record (EHR)\ninterpretation. Our pilot study on the MEDCALC benchmark reveals two key\nfailure modes: (1) misapplied knowledge, where models possess relevant medical\nknowledge but apply it incorrectly, and (2) missing knowledge, where models\nlack essential domain knowledge. To address these cases, EHRMIND applies a\ntwo-stage solution: a lightweight supervised fine-tuning (SFT) warm-up that\ninjects missing domain knowledge, stabilizes subsequent training, and\nencourages structured, interpretable outputs; followed by RLVR, which\nreinforces outcome correctness and refines the model's decision-making. We\ndemonstrate the effectiveness of our method across diverse clinical\napplications, including medical calculations (MEDCALC), patient-trial matching\n(TREC CLINICAL TRIALS), and disease diagnosis (EHRSHOT). EHRMIND delivers\nconsistent gains in accuracy, interpretability, and cross-task generalization.\nThese findings offer practical guidance for applying RLVR to enhance LLM\ncapabilities in healthcare settings."}
{"id": "2505.24119", "pdf": "https://arxiv.org/pdf/2505.24119.pdf", "abs": "https://arxiv.org/abs/2505.24119", "title": "The State of Multilingual LLM Safety Research: From Measuring the Language Gap to Mitigating It", "authors": ["Zheng-Xin Yong", "Beyza Ermis", "Marzieh Fadaee", "Stephen H. Bach", "Julia Kreutzer"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a comprehensive analysis of the linguistic diversity of\nLLM safety research, highlighting the English-centric nature of the field.\nThrough a systematic review of nearly 300 publications from 2020--2024 across\nmajor NLP conferences and workshops at *ACL, we identify a significant and\ngrowing language gap in LLM safety research, with even high-resource\nnon-English languages receiving minimal attention. We further observe that\nnon-English languages are rarely studied as a standalone language and that\nEnglish safety research exhibits poor language documentation practice. To\nmotivate future research into multilingual safety, we make several\nrecommendations based on our survey, and we then pose three concrete future\ndirections on safety evaluation, training data generation, and crosslingual\nsafety generalization. Based on our survey and proposed directions, the field\ncan develop more robust, inclusive AI safety practices for diverse global\npopulations."}
{"id": "2505.24133", "pdf": "https://arxiv.org/pdf/2505.24133.pdf", "abs": "https://arxiv.org/abs/2505.24133", "title": "R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration", "authors": ["Zefan Cai", "Wen Xiao", "Hanshi Sun", "Cheng Luo", "Yikai Zhang", "Ke Wan", "Yucheng Li", "Yeyang Zhou", "Li-Wen Chang", "Jiuxiang Gu", "Zhen Dong", "Anima Anandkumar", "Abedelkadir Asi", "Junjie Hu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets."}
{"id": "2505.24143", "pdf": "https://arxiv.org/pdf/2505.24143.pdf", "abs": "https://arxiv.org/abs/2505.24143", "title": "CrossICL: Cross-Task In-Context Learning via Unsupervised Demonstration Transfer", "authors": ["Jinglong Gao", "Xiao Ding", "Lingxiao Zou", "Bing Qin", "Ting Liu"], "categories": ["cs.CL"], "comment": "9 pages", "summary": "In-Context Learning (ICL) enhances the performance of large language models\n(LLMs) with demonstrations. However, obtaining these demonstrations primarily\nrelies on manual effort. In most real-world scenarios, users are often\nunwilling or unable to provide such demonstrations. Inspired by the human\nanalogy, we explore a new ICL paradigm CrossICL to study how to utilize\nexisting source task demonstrations in the ICL for target tasks, thereby\nobtaining reliable guidance without any additional manual effort. To explore\nthis, we first design a two-stage alignment strategy to mitigate the\ninterference caused by gaps across tasks, as the foundation for our\nexperimental exploration. Based on it, we conduct comprehensive exploration of\nCrossICL, with 875 NLP tasks from the Super-NI benchmark and six types of LLMs,\nincluding GPT-4o. Experimental results demonstrate the effectiveness of\nCrossICL and provide valuable insights on questions like the criteria for\nselecting cross-task demonstrations, as well as the types of task-gap-induced\ninterference in CrossICL."}
{"id": "2505.24147", "pdf": "https://arxiv.org/pdf/2505.24147.pdf", "abs": "https://arxiv.org/abs/2505.24147", "title": "Rationales Are Not Silver Bullets: Measuring the Impact of Rationales on Model Performance and Reliability", "authors": ["Chiwei Zhu", "Benfeng Xu", "An Yang", "Junyang Lin", "Quan Wang", "Chang Zhou", "Zhendong Mao"], "categories": ["cs.CL"], "comment": "To be published in ACL 2025 Findings. (Work originally done in Jan\n  2024)", "summary": "Training language models with rationales augmentation has been shown to be\nbeneficial in many existing works. In this paper, we identify that such a\nprevailing view does not hold consistently. We conduct comprehensive\ninvestigations to thoroughly inspect the impact of rationales on model\nperformance as well as a novel perspective of model reliability. The results\nlead to several key findings that add new insights upon existing\nunderstandings: 1) Rationales can, at times, deteriorate model performance; 2)\nRationales can, at times, improve model reliability, even outperforming their\nuntrained counterparts; 3) A linear correspondence exists in between the\nperformance and reliability improvements, while both are driven by the\nintrinsic difficulty of the task. These findings provide informative\nregulations on the broad utilization of rationales and raise critical\nimplications on the procedure of explicitly aligning language models with\nimplicit human thoughts. Codes can be found at\nhttps://github.com/Ignoramus0817/rationales."}
{"id": "2505.24163", "pdf": "https://arxiv.org/pdf/2505.24163.pdf", "abs": "https://arxiv.org/abs/2505.24163", "title": "LKD-KGC: Domain-Specific KG Construction via LLM-driven Knowledge Dependency Parsing", "authors": ["Jiaqi Sun", "Shiyou Qian", "Zhangchi Han", "Wei Li", "Zelin Qian", "Dingyu Yang", "Jian Cao", "Guangtao Xue"], "categories": ["cs.CL", "cs.AI"], "comment": "Submitting to EDBT 2026", "summary": "Knowledge Graphs (KGs) structure real-world entities and their relationships\ninto triples, enhancing machine reasoning for various tasks. While\ndomain-specific KGs offer substantial benefits, their manual construction is\noften inefficient and requires specialized knowledge. Recent approaches for\nknowledge graph construction (KGC) based on large language models (LLMs), such\nas schema-guided KGC and reference knowledge integration, have proven\nefficient. However, these methods are constrained by their reliance on manually\ndefined schema, single-document processing, and public-domain references,\nmaking them less effective for domain-specific corpora that exhibit complex\nknowledge dependencies and specificity, as well as limited reference knowledge.\nTo address these challenges, we propose LKD-KGC, a novel framework for\nunsupervised domain-specific KG construction. LKD-KGC autonomously analyzes\ndocument repositories to infer knowledge dependencies, determines optimal\nprocessing sequences via LLM driven prioritization, and autoregressively\ngenerates entity schema by integrating hierarchical inter-document contexts.\nThis schema guides the unsupervised extraction of entities and relationships,\neliminating reliance on predefined structures or external knowledge. Extensive\nexperiments show that compared with state-of-the-art baselines, LKD-KGC\ngenerally achieves improvements of 10% to 20% in both precision and recall\nrate, demonstrating its potential in constructing high-quality domain-specific\nKGs."}
{"id": "2505.24164", "pdf": "https://arxiv.org/pdf/2505.24164.pdf", "abs": "https://arxiv.org/abs/2505.24164", "title": "Mixed-R1: Unified Reward Perspective For Reasoning Capability in Multimodal Large Language Models", "authors": ["Shilin Xu", "Yanwei Li", "Rui Yang", "Tao Zhang", "Yueyi Sun", "Wei Chow", "Linfeng Li", "Hang Song", "Qi Xu", "Yunhai Tong", "Xiangtai Li", "Hao Fei"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Recent works on large language models (LLMs) have successfully demonstrated\nthe emergence of reasoning capabilities via reinforcement learning (RL).\nAlthough recent efforts leverage group relative policy optimization (GRPO) for\nMLLMs post-training, they constantly explore one specific aspect, such as\ngrounding tasks, math problems, or chart analysis. There are no works that can\nleverage multi-source MLLM tasks for stable reinforcement learning. In this\nwork, we present a unified perspective to solve this problem. We present\nMixed-R1, a unified yet straightforward framework that contains a mixed reward\nfunction design (Mixed-Reward) and a mixed post-training dataset (Mixed-45K).\nWe first design a data engine to select high-quality examples to build the\nMixed-45K post-training dataset. Then, we present a Mixed-Reward design, which\ncontains various reward functions for various MLLM tasks. In particular, it has\nfour different reward functions: matching reward for binary answer or\nmultiple-choice problems, chart reward for chart-aware datasets, IoU reward for\ngrounding problems, and open-ended reward for long-form text responses such as\ncaption datasets. To handle the various long-form text content, we propose a\nnew open-ended reward named Bidirectional Max-Average Similarity (BMAS) by\nleveraging tokenizer embedding matching between the generated response and the\nground truth. Extensive experiments show the effectiveness of our proposed\nmethod on various MLLMs, including Qwen2.5-VL and Intern-VL on various sizes.\nOur dataset and model are available at https://github.com/xushilin1/mixed-r1."}
{"id": "2505.24165", "pdf": "https://arxiv.org/pdf/2505.24165.pdf", "abs": "https://arxiv.org/abs/2505.24165", "title": "Tag-Evol: Achieving Efficient Instruction Evolving via Tag Injection", "authors": ["Yixuan Wang", "Shiqi Zhou", "Chuanzhe Guo", "Qingfu Zhu"], "categories": ["cs.CL"], "comment": "Accepted as Findings of ACL 2025", "summary": "Evol-Instruct has made significant improvements as a data synthesis method in\nseveral areas. Existing methods typically rely on a fixed set of strategies to\nevolve, which require manual design and are monolithic in form. In addition,\niterative evolution also makes the acquisition of hard samples expensive. In\nview of this, we propose the Tag-Evol framework, a more diverse and efficient\ninstruction evolving method. Specifically, Tag-Evol uses diverse and specific\nknowledge tags as strategies to achieve controlled evolution by injecting\ndifferent combinations of tags into the original instructions. Experiments with\nmultiple backbones in diverse domain benchmarks show that the proposed method\ngenerates significantly better evolved data than other methods. Furthermore, we\nconduct a thorough analysis of the evolved data, demonstrating that Tag-Evol is\nnot only efficient but also generates more diverse and challenging data."}
{"id": "2505.24174", "pdf": "https://arxiv.org/pdf/2505.24174.pdf", "abs": "https://arxiv.org/abs/2505.24174", "title": "Adaptive LoRA Merge with Parameter Pruning for Low-Resource Generation", "authors": ["Ryota Miyano", "Yuki Arase"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at ACL2025 Findings", "summary": "This study proposes a simple yet effective LoRA merge method to achieve LLM\nadaptation for low-resource language generation tasks. The LoRA merge\ntechnique, which integrates multiple LoRA modules trained on different tasks,\nhas gained attention as an effective and efficient approach for adapting LLMs\nto target tasks. However, previous methods are limited in adaptability as they\nkeep the LoRA parameters frozen. Additionally, the low-resource problem has\nbeen out of their scope. We propose a LoRA merge method that updates and prunes\nLoRA parameters through fine-tuning with minimal target task data, which allows\nfiner-grained adjustments of LoRA parameters and enhancement of task\nadaptability. Extensive experiments have been conducted taking summarization as\na benchmark task. Our datasets cover various domains and multiple languages of\nEnglish and Japanese. The results confirm that the proposed method achieves\nsignificant and consistent improvements in task adaptability over the previous\nmethods."}
{"id": "2505.24187", "pdf": "https://arxiv.org/pdf/2505.24187.pdf", "abs": "https://arxiv.org/abs/2505.24187", "title": "Beyond Exponential Decay: Rethinking Error Accumulation in Large Language Models", "authors": ["Mikhail L. Arbuzov", "Alexey A. Shvets", "Sisong Beir"], "categories": ["cs.CL"], "comment": null, "summary": "The prevailing assumption of an exponential decay in large language model\n(LLM) reliability with sequence length, predicated on independent per-token\nerror probabilities, posits an inherent limitation for long autoregressive\noutputs. Our research fundamentally challenges this view by synthesizing\nemerging evidence that LLM errors are not uniformly distributed but are\nconcentrated at sparse \"key tokens\" ($5-10\\%$ of total tokens) representing\ncritical decision junctions. By distinguishing these high-impact tokens from\nthe increasingly predictable majority, we introduce a new reliability formula\nexplaining the sustained coherence of modern LLMs over thousands of tokens.\nConverging research streams reveal that long-context performance primarily\ndepends on accurately navigating a few crucial semantic decision points rather\nthan on uniform token-level accuracy, enabling targeted strategies that\nsignificantly outperform brute-force approaches. We thus propose a framework\nfor next-generation systems centered on selective preservation of semantically\nvital tokens, dynamic computational allocation at uncertain decision\nboundaries, multi-path exploration at ambiguities, and architectures aligned\nwith natural semantic domains. This marks a fundamental shift from raw scaling\nto strategic reasoning, promising breakthrough performance without\nproportionate computational scaling and offering a more nuanced understanding\nthat supersedes the exponential decay hypothesis, thereby opening pathways\ntoward substantially more powerful and efficient language systems."}
{"id": "2505.24196", "pdf": "https://arxiv.org/pdf/2505.24196.pdf", "abs": "https://arxiv.org/abs/2505.24196", "title": "CLaSp: In-Context Layer Skip for Self-Speculative Decoding", "authors": ["Longze Chen", "Renke Shan", "Huiming Wang", "Lu Wang", "Ziqiang Liu", "Run Luo", "Jiawei Wang", "Hamid Alinejad-Rokny", "Min Yang"], "categories": ["cs.CL"], "comment": "11 pages, 7 figures, ACL 2025", "summary": "Speculative decoding (SD) is a promising method for accelerating the decoding\nprocess of Large Language Models (LLMs). The efficiency of SD primarily hinges\non the consistency between the draft model and the verify model. However,\nexisting drafting approaches typically require additional modules to be\ntrained, which can be challenging to implement and ensure compatibility across\nvarious LLMs. In this paper, we propose CLaSp, an in-context layer-skipping\nstrategy for self-speculative decoding. Unlike prior methods, CLaSp does not\nrequire additional drafting modules or extra training. Instead, it employs a\nplug-and-play mechanism by skipping intermediate layers of the verify model to\nconstruct a compressed draft model. Specifically, we develop a dynamic\nprogramming algorithm that optimizes the layer-skipping process by leveraging\nthe complete hidden states from the last verification stage as an objective.\nThis enables CLaSp to dynamically adjust its layer-skipping strategy after each\nverification stage, without relying on pre-optimized sets of skipped layers.\nExperimental results across diverse downstream tasks demonstrate that CLaSp\nachieves a speedup of 1.3x ~ 1.7x on LLaMA3 series models without altering the\noriginal distribution of the generated text."}
{"id": "2505.24199", "pdf": "https://arxiv.org/pdf/2505.24199.pdf", "abs": "https://arxiv.org/abs/2505.24199", "title": "Intuitionistic Fuzzy Sets for Large Language Model Data Annotation: A Novel Approach to Side-by-Side Preference Labeling", "authors": ["Yimin Du"], "categories": ["cs.CL"], "comment": "7 pages", "summary": "The quality of human preference data is crucial for training and evaluating\nlarge language models (LLMs), particularly in reinforcement learning from human\nfeedback (RLHF) and direct preference optimization (DPO) scenarios. Traditional\nside-by-side (SBS) annotation approaches often struggle with inherent\nuncertainty, annotator disagreement, and the complexity of preference\njudgments. This paper introduces a novel framework based on intuitionistic\nfuzzy sets (IFS) for modeling and aggregating human preferences in LLM data\nannotation tasks. Our approach captures not only the degree of preference but\nalso the uncertainty and hesitation inherent in human judgment through\nmembership, non-membership, and hesitation degrees. We propose an IFS-based\nannotation protocol that enables more nuanced preference modeling, develops\naggregation methods for handling annotator disagreement, and introduces quality\nmetrics for preference data assessment. Experimental validation on multiple\ndatasets demonstrates that our IFS-based approach significantly improves\nannotation consistency, reduces annotator fatigue, and produces higher-quality\npreference data compared to traditional binary and Likert-scale methods. The\nresulting preference datasets lead to improved model performance in downstream\ntasks, with 12.3\\% improvement in win-rate against baseline models and 15.7\\%\nreduction in annotation time. Our framework provides a principled approach to\nhandling uncertainty in human preference annotation and offers practical\nbenefits for large-scale LLM training."}
{"id": "2505.24211", "pdf": "https://arxiv.org/pdf/2505.24211.pdf", "abs": "https://arxiv.org/abs/2505.24211", "title": "Are Any-to-Any Models More Consistent Across Modality Transfers Than Specialists?", "authors": ["Jiwan Chung", "Janghan Yoon", "Junhyeong Park", "Sangeyl Lee", "Joowon Yang", "Sooyeon Park", "Youngjae Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Any-to-any generative models aim to enable seamless interpretation and\ngeneration across multiple modalities within a unified framework, yet their\nability to preserve relationships across modalities remains uncertain. Do\nunified models truly achieve cross-modal coherence, or is this coherence merely\nperceived? To explore this, we introduce ACON, a dataset of 1,000 images (500\nnewly contributed) paired with captions, editing instructions, and Q&A pairs to\nevaluate cross-modal transfers rigorously. Using three consistency\ncriteria-cyclic consistency, forward equivariance, and conjugated\nequivariance-our experiments reveal that any-to-any models do not consistently\ndemonstrate greater cross-modal consistency than specialized models in\npointwise evaluations such as cyclic consistency. However, equivariance\nevaluations uncover weak but observable consistency through structured analyses\nof the intermediate latent space enabled by multiple editing operations. We\nrelease our code and data at https://github.com/JiwanChung/ACON."}
{"id": "2505.24217", "pdf": "https://arxiv.org/pdf/2505.24217.pdf", "abs": "https://arxiv.org/abs/2505.24217", "title": "Semi-structured LLM Reasoners Can Be Rigorously Audited", "authors": ["Jixuan Leng", "Cassandra A. Cohen", "Zhixian Zhang", "Chenyan Xiong", "William W. Cohen"], "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) become increasingly capable at reasoning, the\nproblem of \"faithfulness\" persists: LLM \"reasoning traces\" can contain errors\nand omissions that are difficult to detect, and may obscure biases in model\noutputs. To address these limitations, we introduce Semi-Structured Reasoning\nModels (SSRMs), which internalize a semi-structured Chain-of-Thought (CoT)\nreasoning format within the model. Our SSRMs generate reasoning traces in a\nPythonic syntax. While SSRM traces are not executable, they adopt a restricted,\ntask-specific vocabulary to name distinct reasoning steps, and to mark each\nstep's inputs and outputs. Through extensive evaluation on ten benchmarks,\nSSRMs demonstrate strong performance and generality: they outperform comparably\nsized baselines by nearly ten percentage points on in-domain tasks while\nremaining competitive with specialized models on out-of-domain medical\nbenchmarks. Furthermore, we show that semi-structured reasoning is more\namenable to analysis: in particular, they can be automatically audited to\nidentify reasoning flaws. We explore both hand-crafted structured audits, which\ndetect task-specific problematic reasoning patterns, and learned typicality\naudits, which apply probabilistic models over reasoning patterns, and show that\nboth audits can be used to effectively flag probable reasoning errors."}
{"id": "2505.24219", "pdf": "https://arxiv.org/pdf/2505.24219.pdf", "abs": "https://arxiv.org/abs/2505.24219", "title": "ERU-KG: Efficient Reference-aligned Unsupervised Keyphrase Generation", "authors": ["Lam Thanh Do", "Aaditya Bodke", "Pritom Saha Akash", "Kevin Chen-Chuan Chang"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Unsupervised keyphrase prediction has gained growing interest in recent\nyears. However, existing methods typically rely on heuristically defined\nimportance scores, which may lead to inaccurate informativeness estimation. In\naddition, they lack consideration for time efficiency. To solve these problems,\nwe propose ERU-KG, an unsupervised keyphrase generation (UKG) model that\nconsists of an informativeness and a phraseness module. The former estimates\nthe relevance of keyphrase candidates, while the latter generate those\ncandidates. The informativeness module innovates by learning to model\ninformativeness through references (e.g., queries, citation contexts, and\ntitles) and at the term-level, thereby 1) capturing how the key concepts of\ndocuments are perceived in different contexts and 2) estimating informativeness\nof phrases more efficiently by aggregating term informativeness, removing the\nneed for explicit modeling of the candidates. ERU-KG demonstrates its\neffectiveness on keyphrase generation benchmarks by outperforming unsupervised\nbaselines and achieving on average 89\\% of the performance of a supervised\nmodel for top 10 predictions. Additionally, to highlight its practical utility,\nwe evaluate the model on text retrieval tasks and show that keyphrases\ngenerated by ERU-KG are effective when employed as query and document\nexpansions. Furthermore, inference speed tests reveal that ERU-KG is the\nfastest among baselines of similar model sizes. Finally, our proposed model can\nswitch between keyphrase generation and extraction by adjusting\nhyperparameters, catering to diverse application requirements."}
{"id": "2505.24223", "pdf": "https://arxiv.org/pdf/2505.24223.pdf", "abs": "https://arxiv.org/abs/2505.24223", "title": "Automated Structured Radiology Report Generation", "authors": ["Jean-Benoit Delbrouck", "Justin Xu", "Johannes Moll", "Alois Thomas", "Zhihong Chen", "Sophie Ostmeier", "Asfandyar Azhar", "Kelvin Zhenghao Li", "Andrew Johnston", "Christian Bluethgen", "Eduardo Reis", "Mohamed Muneer", "Maya Varma", "Curtis Langlotz"], "categories": ["cs.CL"], "comment": "Accepted to ACL Main 2025", "summary": "Automated radiology report generation from chest X-ray (CXR) images has the\npotential to improve clinical efficiency and reduce radiologists' workload.\nHowever, most datasets, including the publicly available MIMIC-CXR and CheXpert\nPlus, consist entirely of free-form reports, which are inherently variable and\nunstructured. This variability poses challenges for both generation and\nevaluation: existing models struggle to produce consistent, clinically\nmeaningful reports, and standard evaluation metrics fail to capture the nuances\nof radiological interpretation. To address this, we introduce Structured\nRadiology Report Generation (SRRG), a new task that reformulates free-text\nradiology reports into a standardized format, ensuring clarity, consistency,\nand structured clinical reporting. We create a novel dataset by restructuring\nreports using large language models (LLMs) following strict structured\nreporting desiderata. Additionally, we introduce SRR-BERT, a fine-grained\ndisease classification model trained on 55 labels, enabling more precise and\nclinically informed evaluation of structured reports. To assess report quality,\nwe propose F1-SRR-BERT, a metric that leverages SRR-BERT's hierarchical disease\ntaxonomy to bridge the gap between free-text variability and structured\nclinical reporting. We validate our dataset through a reader study conducted by\nfive board-certified radiologists and extensive benchmarking experiments."}
{"id": "2505.24229", "pdf": "https://arxiv.org/pdf/2505.24229.pdf", "abs": "https://arxiv.org/abs/2505.24229", "title": "Dynamic Context-Aware Streaming Pretrained Language Model For Inverse Text Normalization", "authors": ["Luong Ho", "Khanh Le", "Vinh Pham", "Bao Nguyen", "Tan Tran", "Duc Chau"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to INTERSPEECH 2025", "summary": "Inverse Text Normalization (ITN) is crucial for converting spoken Automatic\nSpeech Recognition (ASR) outputs into well-formatted written text, enhancing\nboth readability and usability. Despite its importance, the integration of\nstreaming ITN within streaming ASR remains largely unexplored due to challenges\nin accuracy, efficiency, and adaptability, particularly in low-resource and\nlimited-context scenarios. In this paper, we introduce a streaming pretrained\nlanguage model for ITN, leveraging pretrained linguistic representations for\nimproved robustness. To address streaming constraints, we propose Dynamic\nContext-Aware during training and inference, enabling adaptive chunk size\nadjustments and the integration of right-context information. Experimental\nresults demonstrate that our method achieves accuracy comparable to\nnon-streaming ITN and surpasses existing streaming ITN models on a Vietnamese\ndataset, all while maintaining low latency, ensuring seamless integration into\nASR systems."}
{"id": "2505.24241", "pdf": "https://arxiv.org/pdf/2505.24241.pdf", "abs": "https://arxiv.org/abs/2505.24241", "title": "Advantageous Parameter Expansion Training Makes Better Large Language Models", "authors": ["Naibin Gu", "Yilong Chen", "Zhenyu Zhang", "Peng Fu", "Zheng Lin", "Shuohuan Wang", "Yu Sun", "Hua Wu", "Weiping Wang", "Haifeng Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Although scaling up the number of trainable parameters in both pre-training\nand fine-tuning can effectively improve the performance of large language\nmodels, it also leads to increased computational overhead. When delving into\nthe parameter difference, we find that a subset of parameters, termed\nadvantageous parameters, plays a crucial role in determining model performance.\nFurther analysis reveals that stronger models tend to possess more such\nparameters. In this paper, we propose Advantageous Parameter EXpansion Training\n(APEX), a method that progressively expands advantageous parameters into the\nspace of disadvantageous ones, thereby increasing their proportion and\nenhancing training effectiveness. Further theoretical analysis from the\nperspective of matrix effective rank explains the performance gains of APEX.\nExtensive experiments on both instruction tuning and continued pre-training\ndemonstrate that, in instruction tuning, APEX outperforms full-parameter tuning\nwhile using only 52% of the trainable parameters. In continued pre-training,\nAPEX achieves the same perplexity level as conventional training with just 33%\nof the training data, and yields significant improvements on downstream tasks."}
{"id": "2505.24244", "pdf": "https://arxiv.org/pdf/2505.24244.pdf", "abs": "https://arxiv.org/abs/2505.24244", "title": "Mamba Knockout for Unraveling Factual Information Flow", "authors": ["Nir Endy", "Idan Daniel Grosbard", "Yuval Ran-Milo", "Yonatan Slutzky", "Itay Tshuva", "Raja Giryes"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025", "summary": "This paper investigates the flow of factual information in Mamba State-Space\nModel (SSM)-based language models. We rely on theoretical and empirical\nconnections to Transformer-based architectures and their attention mechanisms.\nExploiting this relationship, we adapt attentional interpretability techniques\noriginally developed for Transformers--specifically, the Attention Knockout\nmethodology--to both Mamba-1 and Mamba-2. Using them we trace how information\nis transmitted and localized across tokens and layers, revealing patterns of\nsubject-token information emergence and layer-wise dynamics. Notably, some\nphenomena vary between mamba models and Transformer based models, while others\nappear universally across all models inspected--hinting that these may be\ninherent to LLMs in general. By further leveraging Mamba's structured\nfactorization, we disentangle how distinct \"features\" either enable\ntoken-to-token information exchange or enrich individual tokens, thus offering\na unified lens to understand Mamba internal operations."}
{"id": "2505.24251", "pdf": "https://arxiv.org/pdf/2505.24251.pdf", "abs": "https://arxiv.org/abs/2505.24251", "title": "Proactive Guidance of Multi-Turn Conversation in Industrial Search", "authors": ["Xiaoyu Li", "Xiao Li", "Li Gao", "Yiding Liu", "Xiaoyang Wang", "Shuaiqiang Wang", "Junfeng Wang", "Dawei Yin"], "categories": ["cs.CL", "cs.IR"], "comment": "ACL'25 (Industry)", "summary": "The evolution of Large Language Models (LLMs) has significantly advanced\nmulti-turn conversation systems, emphasizing the need for proactive guidance to\nenhance users' interactions. However, these systems face challenges in\ndynamically adapting to shifts in users' goals and maintaining low latency for\nreal-time interactions. In the Baidu Search AI assistant, an industrial-scale\nmulti-turn search system, we propose a novel two-phase framework to provide\nproactive guidance. The first phase, Goal-adaptive Supervised Fine-Tuning\n(G-SFT), employs a goal adaptation agent that dynamically adapts to user goal\nshifts and provides goal-relevant contextual information. G-SFT also\nincorporates scalable knowledge transfer to distill insights from LLMs into a\nlightweight model for real-time interaction. The second phase, Click-oriented\nReinforcement Learning (C-RL), adopts a generate-rank paradigm, systematically\nconstructs preference pairs from user click signals, and proactively improves\nclick-through rates through more engaging guidance. This dual-phase\narchitecture achieves complementary objectives: G-SFT ensures accurate goal\ntracking, while C-RL optimizes interaction quality through click signal-driven\nreinforcement learning. Extensive experiments demonstrate that our framework\nachieves 86.10% accuracy in offline evaluation (+23.95% over baseline) and\n25.28% CTR in online deployment (149.06% relative improvement), while reducing\ninference latency by 69.55% through scalable knowledge distillation."}
{"id": "2505.24255", "pdf": "https://arxiv.org/pdf/2505.24255.pdf", "abs": "https://arxiv.org/abs/2505.24255", "title": "Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games", "authors": ["Neemesh Yadav", "Palakorn Achananuparp", "Jing Jiang", "Ee-Peng Lim"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "17 pages, 1 figure, 6 tables", "summary": "Large Language Models (LLMs) have shown potential in simulating human\nbehaviors and performing theory-of-mind (ToM) reasoning, a crucial skill for\ncomplex social interactions. In this study, we investigate the role of ToM\nreasoning in aligning agentic behaviors with human norms in negotiation tasks,\nusing the ultimatum game as a controlled environment. We initialized LLM agents\nwith different prosocial beliefs (including Greedy, Fair, and Selfless) and\nreasoning methods like chain-of-thought (CoT) and varying ToM levels, and\nexamined their decision-making processes across diverse LLMs, including\nreasoning models like o3-mini and DeepSeek-R1 Distilled Qwen 32B. Results from\n2,700 simulations indicated that ToM reasoning enhances behavior alignment,\ndecision-making consistency, and negotiation outcomes. Consistent with previous\nfindings, reasoning models exhibit limited capability compared to models with\nToM reasoning, different roles of the game benefits with different orders of\nToM reasoning. Our findings contribute to the understanding of ToM's role in\nenhancing human-AI interaction and cooperative decision-making. The code used\nfor our experiments can be found at https://github.com/Stealth-py/UltimatumToM."}
{"id": "2505.24263", "pdf": "https://arxiv.org/pdf/2505.24263.pdf", "abs": "https://arxiv.org/abs/2505.24263", "title": "Simulating Training Data Leakage in Multiple-Choice Benchmarks for LLM Evaluation", "authors": ["Naila Shafirni Hidayat", "Muhammad Dehan Al Kautsar", "Alfan Farizki Wicaksono", "Fajri Koto"], "categories": ["cs.CL"], "comment": null, "summary": "The performance of large language models (LLMs) continues to improve, as\nreflected in rising scores on standard benchmarks. However, the lack of\ntransparency around training data raises concerns about potential overlap with\nevaluation sets and the fairness of reported results. Although prior work has\nproposed methods for detecting data leakage, these approaches primarily focus\non identifying outliers and have not been evaluated under controlled simulated\nleakage conditions. In this work, we compare existing leakage detection\ntechniques, namely permutation and n-gram-based methods, under a continual\npretraining setup that simulates real-world leakage scenarios, and additionally\nexplore a lightweight method we call semi-half question. Although semi-half\noffers a low-cost alternative, our analysis shows that the n-gram method\nconsistently achieves the highest F1-score. We also refine these techniques to\nsupport instance-level detection and reduce computational overhead. Leveraging\nthe best-performing method, we create cleaned versions of MMLU and HellaSwag,\nand re-evaluate several LLMs. Our findings present a practical path toward more\nreliable and transparent evaluations, and we recommend contamination checks as\na standard step before releasing benchmark results."}
{"id": "2505.24264", "pdf": "https://arxiv.org/pdf/2505.24264.pdf", "abs": "https://arxiv.org/abs/2505.24264", "title": "Faithful and Robust LLM-Driven Theorem Proving for NLI Explanations", "authors": ["Xin Quan", "Marco Valentino", "Louise A. Dennis", "André Freitas"], "categories": ["cs.CL", "cs.AI"], "comment": "Camera-ready for ACL 2025", "summary": "Natural language explanations play a fundamental role in Natural Language\nInference (NLI) by revealing how premises logically entail hypotheses. Recent\nwork has shown that the interaction of large language models (LLMs) with\ntheorem provers (TPs) can help verify and improve the validity of NLI\nexplanations. However, TPs require translating natural language into\nmachine-verifiable formal representations, a process that introduces the risk\nof semantic information loss and unfaithful interpretation, an issue compounded\nby LLMs' challenges in capturing critical logical structures with sufficient\nprecision. Moreover, LLMs are still limited in their capacity for rigorous and\nrobust proof construction within formal verification frameworks. To mitigate\nissues related to faithfulness and robustness, this paper investigates\nstrategies to (1) alleviate semantic loss during autoformalisation, (2)\nefficiently identify and correct syntactic errors in logical representations,\n(3) explicitly use logical expressions to guide LLMs in generating structured\nproof sketches, and (4) increase LLMs' capacity of interpreting TP's feedback\nfor iterative refinement. Our empirical results on e-SNLI, QASC and WorldTree\nusing different LLMs demonstrate that the proposed strategies yield significant\nimprovements in autoformalisation (+18.46%, +34.2%, +39.77%) and explanation\nrefinement (+29.5%, +51.5%, +41.25%) over the state-of-the-art model. Moreover,\nwe show that specific interventions on the hybrid LLM-TP architecture can\nsubstantially improve efficiency, drastically reducing the number of iterations\nrequired for successful verification."}
{"id": "2505.24302", "pdf": "https://arxiv.org/pdf/2505.24302.pdf", "abs": "https://arxiv.org/abs/2505.24302", "title": "ScienceMeter: Tracking Scientific Knowledge Updates in Language Models", "authors": ["Yike Wang", "Shangbin Feng", "Yulia Tsvetkov", "Hannaneh Hajishirzi"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to support scientific\nresearch, but their knowledge of scientific advancements can quickly become\noutdated. We introduce ScienceMeter, a new framework for evaluating scientific\nknowledge update methods over scientific knowledge spanning the past, present,\nand future. ScienceMeter defines three metrics: knowledge preservation, the\nextent to which models' understanding of previously learned papers are\npreserved; knowledge acquisition, how well scientific claims from newly\nintroduced papers are acquired; and knowledge projection, the ability of the\nupdated model to anticipate or generalize to related scientific claims that may\nemerge in the future. Using ScienceMeter, we examine the scientific knowledge\nof LLMs on claim judgment and generation tasks across a curated dataset of\n15,444 scientific papers and 30,888 scientific claims from ten domains\nincluding medicine, biology, materials science, and computer science. We\nevaluate five representative knowledge update approaches including training-\nand inference-time methods. With extensive experiments, we find that the\nbest-performing knowledge update methods can preserve only 85.9% of existing\nknowledge, acquire 71.7% of new knowledge, and project 37.7% of future\nknowledge. Inference-based methods work for larger models, whereas smaller\nmodels require training to achieve comparable performance. Cross-domain\nanalysis reveals that performance on these objectives is correlated. Even when\napplying on specialized scientific LLMs, existing knowledge update methods fail\nto achieve these objectives collectively, underscoring that developing robust\nscientific knowledge update mechanisms is both crucial and challenging."}
{"id": "2505.24319", "pdf": "https://arxiv.org/pdf/2505.24319.pdf", "abs": "https://arxiv.org/abs/2505.24319", "title": "HiCaM: A Hierarchical-Causal Modification Framework for Long-Form Text Modification", "authors": ["Yuntao Shi", "Yi Luo", "Yeyun Gong", "Chen Lin"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success in various\ndomains. However, when handling long-form text modification tasks, they still\nface two major problems: (1) producing undesired modifications by\ninappropriately altering or summarizing irrelevant content, and (2) missing\nnecessary modifications to implicitly related passages that are crucial for\nmaintaining document coherence. To address these issues, we propose HiCaM, a\nHierarchical-Causal Modification framework that operates through a hierarchical\nsummary tree and a causal graph. Furthermore, to evaluate HiCaM, we derive a\nmulti-domain dataset from various benchmarks, providing a resource for\nassessing its effectiveness. Comprehensive evaluations on the dataset\ndemonstrate significant improvements over strong LLMs, with our method\nachieving up to a 79.50\\% win rate. These results highlight the\ncomprehensiveness of our approach, showing consistent performance improvements\nacross multiple models and domains."}
{"id": "2505.24331", "pdf": "https://arxiv.org/pdf/2505.24331.pdf", "abs": "https://arxiv.org/abs/2505.24331", "title": "Context-Aware Sentiment Forecasting via LLM-based Multi-Perspective Role-Playing Agents", "authors": ["Fanhang Man", "Huandong Wang", "Jianjie Fang", "Zhaoyi Deng", "Baining Zhao", "Xinlei Chen", "Yong Li"], "categories": ["cs.CL"], "comment": null, "summary": "User sentiment on social media reveals the underlying social trends, crises,\nand needs. Researchers have analyzed users' past messages to trace the\nevolution of sentiments and reconstruct sentiment dynamics. However, predicting\nthe imminent sentiment of an ongoing event is rarely studied. In this paper, we\naddress the problem of \\textbf{sentiment forecasting} on social media to\npredict the user's future sentiment in response to the development of the\nevent. We extract sentiment-related features to enhance the modeling skill and\npropose a multi-perspective role-playing framework to simulate the process of\nhuman response. Our preliminary results show significant improvement in\nsentiment forecasting on both microscopic and macroscopic levels."}
{"id": "2505.24332", "pdf": "https://arxiv.org/pdf/2505.24332.pdf", "abs": "https://arxiv.org/abs/2505.24332", "title": "Pangu DeepDiver: Adaptive Search Intensity Scaling via Open-Web Reinforcement Learning", "authors": ["Wenxuan Shi", "Haochen Tan", "Chuqiao Kuang", "Xiaoguang Li", "Xiaozhe Ren", "Chen Zhang", "Hanting Chen", "Yasheng Wang", "Lifeng Shang", "Fisher Yu", "Yunhe Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Information seeking demands iterative evidence gathering and reflective\nreasoning, yet large language models (LLMs) still struggle with it in open-web\nquestion answering. Existing methods rely on static prompting rules or training\nwith Wikipedia-based corpora and retrieval environments, limiting adaptability\nto the real-world web environment where ambiguity, conflicting evidence, and\nnoise are prevalent. These constrained training settings hinder LLMs from\nlearning to dynamically decide when and where to search, and how to adjust\nsearch depth and frequency based on informational demands. We define this\nmissing capacity as Search Intensity Scaling (SIS)--the emergent skill to\nintensify search efforts under ambiguous or conflicting conditions, rather than\nsettling on overconfident, under-verification answers.\n  To study SIS, we introduce WebPuzzle, the first dataset designed to foster\ninformation-seeking behavior in open-world internet environments. WebPuzzle\nconsists of 24K training instances and 275 test questions spanning both\nwiki-based and open-web queries. Building on this dataset, we propose\nDeepDiver, a Reinforcement Learning (RL) framework that promotes SIS by\nencouraging adaptive search policies through exploration under a real-world\nopen-web environment. Experimental results show that Pangu-7B-Reasoner\nempowered by DeepDiver achieve performance on real-web tasks comparable to the\n671B-parameter DeepSeek-R1. We detail DeepDiver's training curriculum from\ncold-start supervised fine-tuning to a carefully designed RL phase, and present\nthat its capability of SIS generalizes from closed-form QA to open-ended tasks\nsuch as long-form writing. Our contributions advance adaptive information\nseeking in LLMs and provide a valuable benchmark and dataset for future\nresearch."}
{"id": "2505.24341", "pdf": "https://arxiv.org/pdf/2505.24341.pdf", "abs": "https://arxiv.org/abs/2505.24341", "title": "Exploring Multimodal Challenges in Toxic Chinese Detection: Taxonomy, Benchmark, and Findings", "authors": ["Shujian Yang", "Shiyao Cui", "Chuanrui Hu", "Haicheng Wang", "Tianwei Zhang", "Minlie Huang", "Jialiang Lu", "Han Qiu"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Accepted to ACL 2025 (Findings). Camera-ready version", "summary": "Detecting toxic content using language models is important but challenging.\nWhile large language models (LLMs) have demonstrated strong performance in\nunderstanding Chinese, recent studies show that simple character substitutions\nin toxic Chinese text can easily confuse the state-of-the-art (SOTA) LLMs. In\nthis paper, we highlight the multimodal nature of Chinese language as a key\nchallenge for deploying LLMs in toxic Chinese detection. First, we propose a\ntaxonomy of 3 perturbation strategies and 8 specific approaches in toxic\nChinese content. Then, we curate a dataset based on this taxonomy, and\nbenchmark 9 SOTA LLMs (from both the US and China) to assess if they can detect\nperturbed toxic Chinese text. Additionally, we explore cost-effective\nenhancement solutions like in-context learning (ICL) and supervised fine-tuning\n(SFT). Our results reveal two important findings. (1) LLMs are less capable of\ndetecting perturbed multimodal Chinese toxic contents. (2) ICL or SFT with a\nsmall number of perturbed examples may cause the LLMs \"overcorrect'':\nmisidentify many normal Chinese contents as toxic."}
{"id": "2505.24347", "pdf": "https://arxiv.org/pdf/2505.24347.pdf", "abs": "https://arxiv.org/abs/2505.24347", "title": "Fewer Hallucinations, More Verification: A Three-Stage LLM-Based Framework for ASR Error Correction", "authors": ["Yangui Fang", "Baixu Cheng", "Jing Peng", "Xu Li", "Yu Xi", "Chengwei Zhang", "Guohui Zhong"], "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "Automatic Speech Recognition (ASR) error correction aims to correct\nrecognition errors while preserving accurate text. Although traditional\napproaches demonstrate moderate effectiveness, LLMs offer a paradigm that\neliminates the need for training and labeled data. However, directly using LLMs\nwill encounter hallucinations problem, which may lead to the modification of\nthe correct text. To address this problem, we propose the Reliable LLM\nCorrection Framework (RLLM-CF), which consists of three stages: (1) error\npre-detection, (2) chain-of-thought sub-tasks iterative correction, and (3)\nreasoning process verification. The advantage of our method is that it does not\nrequire additional information or fine-tuning of the model, and ensures the\ncorrectness of the LLM correction under multi-pass programming. Experiments on\nAISHELL-1, AISHELL-2, and Librispeech show that the GPT-4o model enhanced by\nour framework achieves 21%, 11%, 9%, and 11.4% relative reductions in CER/WER."}
{"id": "2505.24354", "pdf": "https://arxiv.org/pdf/2505.24354.pdf", "abs": "https://arxiv.org/abs/2505.24354", "title": "Unifying Language Agent Algorithms with Graph-based Orchestration Engine for Reproducible Agent Research", "authors": ["Qianqian Zhang", "Jiajia Liao", "Heting Ying", "Yibo Ma", "Haozhan Shen", "Jingcheng Li", "Peng Liu", "Lu Zhang", "Chunxin Fang", "Kyusong Lee", "Ruochen Xu", "Tiancheng Zhao"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Demo", "summary": "Language agents powered by large language models (LLMs) have demonstrated\nremarkable capabilities in understanding, reasoning, and executing complex\ntasks. However, developing robust agents presents significant challenges:\nsubstantial engineering overhead, lack of standardized components, and\ninsufficient evaluation frameworks for fair comparison. We introduce Agent\nGraph-based Orchestration for Reasoning and Assessment (AGORA), a flexible and\nextensible framework that addresses these challenges through three key\ncontributions: (1) a modular architecture with a graph-based workflow engine,\nefficient memory management, and clean component abstraction; (2) a\ncomprehensive suite of reusable agent algorithms implementing state-of-the-art\nreasoning approaches; and (3) a rigorous evaluation framework enabling\nsystematic comparison across multiple dimensions. Through extensive experiments\non mathematical reasoning and multimodal tasks, we evaluate various agent\nalgorithms across different LLMs, revealing important insights about their\nrelative strengths and applicability. Our results demonstrate that while\nsophisticated reasoning approaches can enhance agent capabilities, simpler\nmethods like Chain-of-Thought often exhibit robust performance with\nsignificantly lower computational overhead. AGORA not only simplifies language\nagent development but also establishes a foundation for reproducible agent\nresearch through standardized evaluation protocols."}
{"id": "2505.24355", "pdf": "https://arxiv.org/pdf/2505.24355.pdf", "abs": "https://arxiv.org/abs/2505.24355", "title": "Multilingual Gloss-free Sign Language Translation: Towards Building a Sign Language Foundation Model", "authors": ["Sihan Tan", "Taro Miyazaki", "Kazuhiro Nakadai"], "categories": ["cs.CL"], "comment": null, "summary": "Sign Language Translation (SLT) aims to convert sign language (SL) videos\ninto spoken language text, thereby bridging the communication gap between the\nsign and the spoken community. While most existing works focus on translating a\nsingle sign language into a single spoken language (one-to-one SLT), leveraging\nmultilingual resources could mitigate low-resource issues and enhance\naccessibility. However, multilingual SLT (MLSLT) remains unexplored due to\nlanguage conflicts and alignment difficulties across SLs and spoken languages.\nTo address these challenges, we propose a multilingual gloss-free model with\ndual CTC objectives for token-level SL identification and spoken text\ngeneration. Our model supports 10 SLs and handles one-to-one, many-to-one, and\nmany-to-many SLT tasks, achieving competitive performance compared to\nstate-of-the-art methods on three widely adopted benchmarks: multilingual\nSP-10, PHOENIX14T, and CSL-Daily."}
{"id": "2505.24362", "pdf": "https://arxiv.org/pdf/2505.24362.pdf", "abs": "https://arxiv.org/abs/2505.24362", "title": "Knowing Before Saying: LLM Representations Encode Information About Chain-of-Thought Success Before Completion", "authors": ["Anum Afzal", "Florian Matthes", "Gal Chechik", "Yftah Ziser"], "categories": ["cs.CL"], "comment": null, "summary": "We investigate whether the success of a zero-shot Chain-of-Thought (CoT)\nprocess can be predicted before completion. We discover that a probing\nclassifier, based on LLM representations, performs well \\emph{even before a\nsingle token is generated}, suggesting that crucial information about the\nreasoning process is already present in the initial steps representations. In\ncontrast, a strong BERT-based baseline, which relies solely on the generated\ntokens, performs worse, likely because it depends on shallow linguistic cues\nrather than deeper reasoning dynamics. Surprisingly, using later reasoning\nsteps does not always improve classification. When additional context is\nunhelpful, earlier representations resemble later ones more, suggesting LLMs\nencode key information early. This implies reasoning can often stop early\nwithout loss. To test this, we conduct early stopping experiments, showing that\ntruncating CoT reasoning still improves performance over not using CoT at all,\nthough a gap remains compared to full reasoning. However, approaches like\nsupervised learning or reinforcement learning designed to shorten CoT chains\ncould leverage our classifier's guidance to identify when early stopping is\neffective. Our findings provide insights that may support such methods, helping\nto optimize CoT's efficiency while preserving its benefits.\\footnote{Code and\ndata is available at\n\\href{https://github.com/anum94/CoTpred}{\\texttt{github.com/anum94/CoTpred}}."}
{"id": "2505.24377", "pdf": "https://arxiv.org/pdf/2505.24377.pdf", "abs": "https://arxiv.org/abs/2505.24377", "title": "LLM Inference Enhanced by External Knowledge: A Survey", "authors": ["Yu-Hsuan Lin", "Qian-Hui Chen", "Yi-Jie Cheng", "Jia-Ren Zhang", "Yi-Hung Liu", "Liang-Yu Hsia", "Yun-Nung Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have enhanced\nnatural-language reasoning. However, their limited parametric memory and\nsusceptibility to hallucination present persistent challenges for tasks\nrequiring accurate, context-based inference. To overcome these limitations, an\nincreasing number of studies have proposed leveraging external knowledge to\nenhance LLMs. This study offers a systematic exploration of strategies for\nusing external knowledge to enhance LLMs, beginning with a taxonomy that\ncategorizes external knowledge into unstructured and structured data. We then\nfocus on structured knowledge, presenting distinct taxonomies for tables and\nknowledge graphs (KGs), detailing their integration paradigms with LLMs, and\nreviewing representative methods. Our comparative analysis further highlights\nthe trade-offs among interpretability, scalability, and performance, providing\ninsights for developing trustworthy and generalizable knowledge-enhanced LLMs."}
{"id": "2505.24388", "pdf": "https://arxiv.org/pdf/2505.24388.pdf", "abs": "https://arxiv.org/abs/2505.24388", "title": "ClueAnchor: Clue-Anchored Knowledge Reasoning Exploration and Optimization for Retrieval-Augmented Generation", "authors": ["Hao Chen", "Yukun Yan", "Sen Mei", "Wanxiang Che", "Zhenghao Liu", "Qi Shi", "Xinze Li", "Yuchun Fan", "Pengcheng Huang", "Qiushi Xiong", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) augments Large Language Models (LLMs)\nwith external knowledge to improve factuality. However, existing RAG systems\nfrequently underutilize the retrieved documents, failing to extract and\nintegrate the key clues needed to support faithful and interpretable reasoning,\nespecially in cases where relevant evidence is implicit, scattered, or obscured\nby noise. To address this issue, we propose ClueAnchor, a novel framework for\nenhancing RAG via clue-anchored reasoning exploration and optimization.\nClueAnchor extracts key clues from retrieved content and generates multiple\nreasoning paths based on different knowledge configurations, optimizing the\nmodel by selecting the most effective one through reward-based preference\noptimization. Experiments show that ClueAnchor significantly outperforms prior\nRAG baselines in reasoning completeness and robustness. Further analysis\nconfirms its strong resilience to noisy or partially relevant retrieved\ncontent, as well as its capability to identify supporting evidence even in the\nabsence of explicit clue supervision during inference."}
{"id": "2505.24409", "pdf": "https://arxiv.org/pdf/2505.24409.pdf", "abs": "https://arxiv.org/abs/2505.24409", "title": "LLMs Are Globally Multilingual Yet Locally Monolingual: Exploring Knowledge Transfer via Language and Thought Theory", "authors": ["Eojin Kang", "Juae Kim"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multilingual large language models (LLMs) open up new possibilities for\nleveraging information across languages, but their factual knowledge recall\nremains inconsistent depending on the input language. While previous studies\nhave attempted to address this issue through English-based prompting and\nevaluation, we explore non-English to English transfer via Language and Thought\nTheory. This perspective allows us to examine language-thought binding in LLMs\nand uncover why factual knowledge often fails to transfer effectively. We\npropose the Language-to-Thought (L2T) prompting strategy, which analyzes the\nrelationship between input language, internal cognitive processes, and\nknowledge. Experimental results challenge the assumption that English-based\napproaches consistently outperform other languages and offer a novel insight\nthat aligning the model's internal thought with the knowledge required for the\ntask is critical for successful cross-lingual transfer. Furthermore, we show\nthat applying L2T during training can alleviate LLMs' reliance on the input\nlanguage and facilitate cross-linguistic knowledge integration without\ntranslation-based learning. Code and datasets will be available."}
{"id": "2505.24423", "pdf": "https://arxiv.org/pdf/2505.24423.pdf", "abs": "https://arxiv.org/abs/2505.24423", "title": "MMAFFBen: A Multilingual and Multimodal Affective Analysis Benchmark for Evaluating LLMs and VLMs", "authors": ["Zhiwei Liu", "Lingfei Qian", "Qianqian Xie", "Jimin Huang", "Kailai Yang", "Sophia Ananiadou"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Large language models and vision-language models (which we jointly call LMs)\nhave transformed NLP and CV, demonstrating remarkable potential across various\nfields. However, their capabilities in affective analysis (i.e. sentiment\nanalysis and emotion detection) remain underexplored. This gap is largely due\nto the absence of comprehensive evaluation benchmarks, and the inherent\ncomplexity of affective analysis tasks. In this paper, we introduce MMAFFBen,\nthe first extensive open-source benchmark for multilingual multimodal affective\nanalysis. MMAFFBen encompasses text, image, and video modalities across 35\nlanguages, covering four key affective analysis tasks: sentiment polarity,\nsentiment intensity, emotion classification, and emotion intensity. Moreover,\nwe construct the MMAFFIn dataset for fine-tuning LMs on affective analysis\ntasks, and further develop MMAFFLM-3b and MMAFFLM-7b based on it. We evaluate\nvarious representative LMs, including GPT-4o-mini, providing a systematic\ncomparison of their affective understanding capabilities. This project is\navailable at https://github.com/lzw108/MMAFFBen."}
{"id": "2505.24427", "pdf": "https://arxiv.org/pdf/2505.24427.pdf", "abs": "https://arxiv.org/abs/2505.24427", "title": "Donate or Create? Comparing Data Collection Strategies for Emotion-labeled Multimodal Social Media Posts", "authors": ["Christopher Bagdon", "Aidan Combs", "Carina Silberer", "Roman Klinger"], "categories": ["cs.CL"], "comment": "Published at ACL 2025", "summary": "Accurate modeling of subjective phenomena such as emotion expression requires\ndata annotated with authors' intentions. Commonly such data is collected by\nasking study participants to donate and label genuine content produced in the\nreal world, or create content fitting particular labels during the study.\nAsking participants to create content is often simpler to implement and\npresents fewer risks to participant privacy than data donation. However, it is\nunclear if and how study-created content may differ from genuine content, and\nhow differences may impact models. We collect study-created and genuine\nmultimodal social media posts labeled for emotion and compare them on several\ndimensions, including model performance. We find that compared to genuine\nposts, study-created posts are longer, rely more on their text and less on\ntheir images for emotion expression, and focus more on emotion-prototypical\nevents. The samples of participants willing to donate versus create posts are\ndemographically different. Study-created data is valuable to train models that\ngeneralize well to genuine data, but realistic effectiveness estimates require\ngenuine data."}
{"id": "2505.24428", "pdf": "https://arxiv.org/pdf/2505.24428.pdf", "abs": "https://arxiv.org/abs/2505.24428", "title": "Model Unlearning via Sparse Autoencoder Subspace Guided Projections", "authors": ["Xu Wang", "Zihao Li", "Benyou Wang", "Yan Hu", "Difan Zou"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) store vast amounts of information, making them\npowerful yet raising privacy and safety concerns when selective knowledge\nremoval is required. Existing unlearning strategies, ranging from\ngradient-based fine-tuning and model editing to sparse autoencoder (SAE)\nsteering, either lack interpretability or fail to provide a robust defense\nagainst adversarial prompts. We propose SAE-Guided Subspace Projection\nUnlearning (SSPU), a novel framework that leverages SAE features to drive\ntargeted updates in the model's parameter space, enabling precise,\ninterpretable, and robust unlearning. SSPU's three-stage pipeline performs\ndata-driven layer and feature selection, subspace construction via QR\ndecomposition, and constrained optimization that controls activations into an\n\"irrelevant\" subspace while preserving retained knowledge. Overall, we use SAE\nfeatures to construct a subspace that supervises unlearning, refining the loss\nand adding a regularization term to guide interpretable parameter updates. In\nexperiments on the WMDP-Cyber forget set and three utility benchmarks (MMLU,\nTruthfulQA, GSM8K), SSPU reduces harmful knowledge accuracy by 3.22% compared\nto the strongest baseline. It also improves adversarial robustness, lowering\nmalicious accuracy under jailbreak prompts compared to baselines. Our findings\nexpose the limitations of prior unlearning methods and demonstrate how\ninterpretable subspace-guided optimization can achieve robust, controllable\nmodel behavior."}
{"id": "2505.24448", "pdf": "https://arxiv.org/pdf/2505.24448.pdf", "abs": "https://arxiv.org/abs/2505.24448", "title": "Exploring the Impact of Occupational Personas on Domain-Specific QA", "authors": ["Eojin Kang", "Jaehyuk Yu", "Juae Kim"], "categories": ["cs.CL"], "comment": null, "summary": "Recent studies on personas have improved the way Large Language Models (LLMs)\ninteract with users. However, the effect of personas on domain-specific\nquestion-answering (QA) tasks remains a subject of debate. This study analyzes\nwhether personas enhance specialized QA performance by introducing two types of\npersona: Profession-Based Personas (PBPs) (e.g., scientist), which directly\nrelate to domain expertise, and Occupational Personality-Based Personas (OPBPs)\n(e.g., scientific person), which reflect cognitive tendencies rather than\nexplicit expertise. Through empirical evaluations across multiple scientific\ndomains, we demonstrate that while PBPs can slightly improve accuracy, OPBPs\noften degrade performance, even when semantically related to the task. Our\nfindings suggest that persona relevance alone does not guarantee effective\nknowledge utilization and that they may impose cognitive constraints that\nhinder optimal knowledge application. Future research can explore how nuanced\ndistinctions in persona representations guide LLMs, potentially contributing to\nreasoning and knowledge retrieval that more closely mirror human social\nconceptualization."}
{"id": "2505.24449", "pdf": "https://arxiv.org/pdf/2505.24449.pdf", "abs": "https://arxiv.org/abs/2505.24449", "title": "When Large Multimodal Models Confront Evolving Knowledge:Challenges and Pathways", "authors": ["Kailin Jiang", "Yuntao Du", "Yukai Ding", "Yuchen Ren", "Ning Jiang", "Zhi Gao", "Zilong Zheng", "Lei Liu", "Bin Li", "Qing Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large language/multimodal models (LLMs/LMMs) store extensive pre-trained\nknowledge but struggle to maintain consistency with real-world updates, making\nit difficult to avoid catastrophic forgetting while acquiring evolving\nknowledge. Previous work focused on constructing textual knowledge datasets and\nexploring knowledge injection in LLMs, lacking exploration of multimodal\nevolving knowledge injection in LMMs. To address this, we propose the EVOKE\nbenchmark to evaluate LMMs' ability to inject multimodal evolving knowledge in\nreal-world scenarios. Meanwhile, a comprehensive evaluation of multimodal\nevolving knowledge injection revealed two challenges: (1) Existing knowledge\ninjection methods perform terribly on evolving knowledge. (2) Supervised\nfine-tuning causes catastrophic forgetting, particularly instruction following\nability is severely compromised. Additionally, we provide pathways and find\nthat: (1) Text knowledge augmentation during the training phase improves\nperformance, while image augmentation cannot achieve it. (2) Continual learning\nmethods, especially Replay and MoELoRA, effectively mitigate forgetting. Our\nfindings indicate that current knowledge injection methods have many\nlimitations on evolving knowledge, which motivates further research on more\nefficient and stable knowledge injection methods."}
{"id": "2505.24455", "pdf": "https://arxiv.org/pdf/2505.24455.pdf", "abs": "https://arxiv.org/abs/2505.24455", "title": "Domain Pre-training Impact on Representations", "authors": ["Cesar Gonzalez-Gutierrez", "Ariadna Quattoni"], "categories": ["cs.CL"], "comment": null, "summary": "This empirical study analyzes the effects of the pre-training corpus on the\nquality of learned transformer representations. We focus on the representation\nquality induced solely through pre-training. Our experiments show that\npre-training on a small, specialized corpus can yield effective\nrepresentations, and that the success of combining a generic and a specialized\ncorpus depends on the distributional similarity between the target task and the\nspecialized corpus."}
{"id": "2505.24456", "pdf": "https://arxiv.org/pdf/2505.24456.pdf", "abs": "https://arxiv.org/abs/2505.24456", "title": "CaMMT: Benchmarking Culturally Aware Multimodal Machine Translation", "authors": ["Emilio Villa-Cueva", "Sholpan Bolatzhanova", "Diana Turmakhan", "Kareem Elzeky", "Henok Biadglign Ademtew", "Alham Fikri Aji", "Israel Abebe Azime", "Jinheon Baek", "Frederico Belcavello", "Fermin Cristobal", "Jan Christian Blaise Cruz", "Mary Dabre", "Raj Dabre", "Toqeer Ehsan", "Naome A Etori", "Fauzan Farooqui", "Jiahui Geng", "Guido Ivetta", "Thanmay Jayakumar", "Soyeong Jeong", "Zheng Wei Lim", "Aishik Mandal", "Sofia Martinelli", "Mihail Minkov Mihaylov", "Daniil Orel", "Aniket Pramanick", "Sukannya Purkayastha", "Israfel Salazar", "Haiyue Song", "Tiago Timponi Torrent", "Debela Desalegn Yadeta", "Injy Hamed", "Atnafu Lambebo Tonja", "Thamar Solorio"], "categories": ["cs.CL"], "comment": null, "summary": "Cultural content poses challenges for machine translation systems due to the\ndifferences in conceptualizations between cultures, where language alone may\nfail to convey sufficient context to capture region-specific meanings. In this\nwork, we investigate whether images can act as cultural context in multimodal\ntranslation. We introduce CaMMT, a human-curated benchmark of over 5,800\ntriples of images along with parallel captions in English and regional\nlanguages. Using this dataset, we evaluate five Vision Language Models (VLMs)\nin text-only and text+image settings. Through automatic and human evaluations,\nwe find that visual context generally improves translation quality, especially\nin handling Culturally-Specific Items (CSIs), disambiguation, and correct\ngender usage. By releasing CaMMT, we aim to support broader efforts in building\nand evaluating multimodal translation systems that are better aligned with\ncultural nuance and regional variation."}
{"id": "2505.24472", "pdf": "https://arxiv.org/pdf/2505.24472.pdf", "abs": "https://arxiv.org/abs/2505.24472", "title": "VietMix: A Naturally Occurring Vietnamese-English Code-Mixed Corpus with Iterative Augmentation for Machine Translation", "authors": ["Hieu Tran", "Phuong-Anh Nguyen-Le", "Huy Nghiem", "Quang-Nhan Nguyen", "Wei Ai", "Marine Carpuat"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Machine translation systems fail when processing code-mixed inputs for\nlow-resource languages. We address this challenge by curating VietMix, a\nparallel corpus of naturally occurring code-mixed Vietnamese text paired with\nexpert English translations. Augmenting this resource, we developed a\ncomplementary synthetic data generation pipeline. This pipeline incorporates\nfiltering mechanisms to ensure syntactic plausibility and pragmatic\nappropriateness in code-mixing patterns. Experimental validation shows our\nnaturalistic and complementary synthetic data boost models' performance,\nmeasured by translation quality estimation scores, of up to 71.84 on COMETkiwi\nand 81.77 on XCOMET. Triangulating positive results with LLM-based assessments,\naugmented models are favored over seed fine-tuned counterparts in approximately\n49% of judgments (54-56% excluding ties). VietMix and our augmentation\nmethodology advance ecological validity in neural MT evaluations and establish\na framework for addressing code-mixed translation challenges across other\nlow-resource pairs."}
{"id": "2505.24480", "pdf": "https://arxiv.org/pdf/2505.24480.pdf", "abs": "https://arxiv.org/abs/2505.24480", "title": "Towards Effective Code-Integrated Reasoning", "authors": ["Fei Bai", "Yingqian Min", "Beichen Zhang", "Zhipeng Chen", "Wayne Xin Zhao", "Lei Fang", "Zheng Liu", "Zhongyuan Wang", "Ji-Rong Wen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Technical Report on Slow Thinking with LLMs: Code-Integrated\n  Reasoning", "summary": "In this paper, we investigate code-integrated reasoning, where models\ngenerate code when necessary and integrate feedback by executing it through a\ncode interpreter. To acquire this capability, models must learn when and how to\nuse external code tools effectively, which is supported by tool-augmented\nreinforcement learning (RL) through interactive learning. Despite its benefits,\ntool-augmented RL can still suffer from potential instability in the learning\ndynamics. In light of this challenge, we present a systematic approach to\nimproving the training effectiveness and stability of tool-augmented RL for\ncode-integrated reasoning. Specifically, we develop enhanced training\nstrategies that balance exploration and stability, progressively building\ntool-use capabilities while improving reasoning performance. Through extensive\nexperiments on five mainstream mathematical reasoning benchmarks, our model\ndemonstrates significant performance improvements over multiple competitive\nbaselines. Furthermore, we conduct an in-depth analysis of the mechanism and\neffect of code-integrated reasoning, revealing several key insights, such as\nthe extension of model's capability boundaries and the simultaneous improvement\nof reasoning efficiency through code integration. All data and code for\nreproducing this work are available at: https://github.com/RUCAIBox/CIR."}
{"id": "2505.24500", "pdf": "https://arxiv.org/pdf/2505.24500.pdf", "abs": "https://arxiv.org/abs/2505.24500", "title": "TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for Enhancing LLMs' Social Intelligence", "authors": ["Guiyang Hou", "Xing Gao", "Yuchuan Wu", "Xiang Huang", "Wenqi Zhang", "Zhe Zheng", "Yongliang Shen", "Jialu Du", "Fei Huang", "Yongbin Li", "Weiming Lu"], "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 12 figures", "summary": "Recently, Large Language Models (LLMs) have made significant progress in\nIQ-related domains that require careful thinking, such as mathematics and\ncoding. However, enhancing LLMs' cognitive development in social domains,\nparticularly from a post-training perspective, remains underexplored.\nRecognizing that the social world follows a distinct timeline and requires a\nricher blend of cognitive modes (from intuitive reactions (System 1) and\nsurface-level thinking to deliberate thinking (System 2)) than mathematics,\nwhich primarily relies on System 2 cognition (careful, step-by-step reasoning),\nwe introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning\n(TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we\nsystematically explore improving LLMs' social intelligence and validate the\neffectiveness of the TimeHC-RL method, through five other post-training\nparadigms and two test-time intervention paradigms on eight datasets with\ndiverse data patterns. Experimental results reveal the superiority of our\nproposed TimeHC-RL method compared to the widely adopted System 2 RL method. It\ngives the 7B backbone model wings, enabling it to rival the performance of\nadvanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic\nexploration from post-training and test-time interventions perspectives to\nimprove LLMs' social intelligence has uncovered several valuable insights."}
{"id": "2505.24523", "pdf": "https://arxiv.org/pdf/2505.24523.pdf", "abs": "https://arxiv.org/abs/2505.24523", "title": "Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors", "authors": ["Andrea Pedrotti", "Michele Papucci", "Cristiano Ciaccio", "Alessio Miaschi", "Giovanni Puccetti", "Felice Dell'Orletta", "Andrea Esuli"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Findings of ACL 2025", "summary": "Recent advancements in Generative AI and Large Language Models (LLMs) have\nenabled the creation of highly realistic synthetic content, raising concerns\nabout the potential for malicious use, such as misinformation and manipulation.\nMoreover, detecting Machine-Generated Text (MGT) remains challenging due to the\nlack of robust benchmarks that assess generalization to real-world scenarios.\nIn this work, we present a pipeline to test the resilience of state-of-the-art\nMGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed\nadversarial attacks. To challenge the detectors, we fine-tune language models\nusing Direct Preference Optimization (DPO) to shift the MGT style toward\nhuman-written text (HWT). This exploits the detectors' reliance on stylistic\nclues, making new generations more challenging to detect. Additionally, we\nanalyze the linguistic shifts induced by the alignment and which features are\nused by detectors to detect MGT texts. Our results show that detectors can be\neasily fooled with relatively few examples, resulting in a significant drop in\ndetection performance. This highlights the importance of improving detection\nmethods and making them robust to unseen in-domain texts."}
{"id": "2505.24525", "pdf": "https://arxiv.org/pdf/2505.24525.pdf", "abs": "https://arxiv.org/abs/2505.24525", "title": "Limited-Resource Adapters Are Regularizers, Not Linguists", "authors": ["Marcell Fekete", "Nathaniel R. Robinson", "Ernests Lavrinovics", "E. Djeride Jean-Baptiste", "Raj Dabre", "Johannes Bjerva", "Heather Lent"], "categories": ["cs.CL"], "comment": null, "summary": "Cross-lingual transfer from related high-resource languages is a\nwell-established strategy to enhance low-resource language technologies. Prior\nwork has shown that adapters show promise for, e.g., improving low-resource\nmachine translation (MT). In this work, we investigate an adapter souping\nmethod combined with cross-attention fine-tuning of a pre-trained MT model to\nleverage language transfer for three low-resource Creole languages, which\nexhibit relatedness to different language groups across distinct linguistic\ndimensions. Our approach improves performance substantially over baselines.\nHowever, we find that linguistic relatedness -- or even a lack thereof -- does\nnot covary meaningfully with adapter performance. Surprisingly, our\ncross-attention fine-tuning approach appears equally effective with randomly\ninitialized adapters, implying that the benefit of adapters in this setting\nlies in parameter regularization, and not in meaningful information transfer.\nWe provide analysis supporting this regularization hypothesis. Our findings\nunderscore the reality that neural language processing involves many success\nfactors, and that not all neural methods leverage linguistic knowledge in\nintuitive ways."}
{"id": "2505.24532", "pdf": "https://arxiv.org/pdf/2505.24532.pdf", "abs": "https://arxiv.org/abs/2505.24532", "title": "DEEPQUESTION: Systematic Generation of Real-World Challenges for Evaluating LLMs Performance", "authors": ["Ali Khoramfar", "Ali Ramezani", "Mohammad Mahdi Mohajeri", "Mohammad Javad Dousti", "Majid Nili Ahmadabadi", "Heshaam Faili"], "categories": ["cs.CL"], "comment": null, "summary": "LLMs often excel on standard benchmarks but falter on real-world tasks. We\nintroduce DeepQuestion, a scalable automated framework that augments existing\ndatasets based on Bloom's taxonomy and creates novel questions that trace\noriginal solution paths to probe evaluative and creative skills. Extensive\nexperiments across ten open-source and proprietary models, covering both\ngeneral-purpose and reasoning LLMs, reveal substantial performance drops (even\nup to 70% accuracy loss) on higher-order tasks, underscoring persistent gaps in\ndeep reasoning. Our work highlights the need for cognitively diverse benchmarks\nto advance LLM progress. DeepQuestion and related datasets will be released\nupon acceptance of the paper."}
{"id": "2505.24538", "pdf": "https://arxiv.org/pdf/2505.24538.pdf", "abs": "https://arxiv.org/abs/2505.24538", "title": "Don't Erase, Inform! Detecting and Contextualizing Harmful Language in Cultural Heritage Collections", "authors": ["Orfeas Menis Mastromichalakis", "Jason Liartis", "Kristina Rose", "Antoine Isaac", "Giorgos Stamou"], "categories": ["cs.CL"], "comment": null, "summary": "Cultural Heritage (CH) data hold invaluable knowledge, reflecting the\nhistory, traditions, and identities of societies, and shaping our understanding\nof the past and present. However, many CH collections contain outdated or\noffensive descriptions that reflect historical biases. CH Institutions (CHIs)\nface significant challenges in curating these data due to the vast scale and\ncomplexity of the task. To address this, we develop an AI-powered tool that\ndetects offensive terms in CH metadata and provides contextual insights into\ntheir historical background and contemporary perception. We leverage a\nmultilingual vocabulary co-created with marginalized communities, researchers,\nand CH professionals, along with traditional NLP techniques and Large Language\nModels (LLMs). Available as a standalone web app and integrated with major CH\nplatforms, the tool has processed over 7.9 million records, contextualizing the\ncontentious terms detected in their metadata. Rather than erasing these terms,\nour approach seeks to inform, making biases visible and providing actionable\ninsights for creating more inclusive and accessible CH collections."}
{"id": "2505.24539", "pdf": "https://arxiv.org/pdf/2505.24539.pdf", "abs": "https://arxiv.org/abs/2505.24539", "title": "Localizing Persona Representations in LLMs", "authors": ["Celia Cintas", "Miriam Rateike", "Erik Miehling", "Elizabeth Daly", "Skyler Speakman"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present a study on how and where personas -- defined by distinct sets of\nhuman characteristics, values, and beliefs -- are encoded in the representation\nspace of large language models (LLMs). Using a range of dimension reduction and\npattern recognition methods, we first identify the model layers that show the\ngreatest divergence in encoding these representations. We then analyze the\nactivations within a selected layer to examine how specific personas are\nencoded relative to others, including their shared and distinct embedding\nspaces. We find that, across multiple pre-trained decoder-only LLMs, the\nanalyzed personas show large differences in representation space only within\nthe final third of the decoder layers. We observe overlapping activations for\nspecific ethical perspectives -- such as moral nihilism and utilitarianism --\nsuggesting a degree of polysemy. In contrast, political ideologies like\nconservatism and liberalism appear to be represented in more distinct regions.\nThese findings help to improve our understanding of how LLMs internally\nrepresent information and can inform future efforts in refining the modulation\nof specific human traits in LLM outputs. Warning: This paper includes\npotentially offensive sample statements."}
{"id": "2505.24544", "pdf": "https://arxiv.org/pdf/2505.24544.pdf", "abs": "https://arxiv.org/abs/2505.24544", "title": "Cross-Attention Speculative Decoding", "authors": ["Wei Zhong", "Manasa Bharadwaj", "Yixiao Wang", "Nikhil Verma", "Yipeng Ji", "Chul Lee"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Speculative decoding (SD) is a widely adopted approach for accelerating\ninference in large language models (LLMs), particularly when the draft and\ntarget models are well aligned. However, state-of-the-art SD methods typically\nrely on tightly coupled, self-attention-based Transformer decoders, often\naugmented with auxiliary pooling or fusion layers. This coupling makes them\nincreasingly complex and harder to generalize across different models. We\npresent Budget EAGLE (Beagle), the first, to our knowledge,\ncross-attention-based Transformer decoder SD model that achieves performance on\npar with leading self-attention SD models (EAGLE-v2) while eliminating the need\nfor pooling or auxiliary components, simplifying the architecture, improving\ntraining efficiency, and maintaining stable memory usage during training-time\nsimulation. To enable effective training of this novel architecture, we propose\nTwo-Stage Block-Attention Training, a new method that achieves training\nstability and convergence efficiency in block-level attention scenarios.\nExtensive experiments across multiple LLMs and datasets show that Beagle\nachieves competitive inference speedups and higher training efficiency than\nEAGLE-v2, offering a strong alternative for architectures in speculative\ndecoding."}
{"id": "2505.24550", "pdf": "https://arxiv.org/pdf/2505.24550.pdf", "abs": "https://arxiv.org/abs/2505.24550", "title": "A*-Thought: Efficient Reasoning via Bidirectional Compression for Low-Resource Settings", "authors": ["Xiaoang Xu", "Shuo Wang", "Xu Han", "Zhenghao Liu", "Huijia Wu", "Peipei Li", "Zhiyuan Liu", "Maosong Sun", "Zhaofeng He"], "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) achieve superior performance by extending the\nthought length. However, a lengthy thinking trajectory leads to reduced\nefficiency. Most of the existing methods are stuck in the assumption of\noverthinking and attempt to reason efficiently by compressing the\nChain-of-Thought, but this often leads to performance degradation. To address\nthis problem, we introduce A*-Thought, an efficient tree search-based unified\nframework designed to identify and isolate the most essential thoughts from the\nextensive reasoning chains produced by these models. It formulates the\nreasoning process of LRMs as a search tree, where each node represents a\nreasoning span in the giant reasoning space. By combining the A* search\nalgorithm with a cost function specific to the reasoning path, it can\nefficiently compress the chain of thought and determine a reasoning path with\nhigh information density and low cost. In addition, we also propose a\nbidirectional importance estimation mechanism, which further refines this\nsearch process and enhances its efficiency beyond uniform sampling. Extensive\nexperiments on several advanced math tasks show that A*-Thought effectively\nbalances performance and efficiency over a huge search space. Specifically,\nA*-Thought can improve the performance of QwQ-32B by 2.39$\\times$ with\nlow-budget and reduce the length of the output token by nearly 50% with\nhigh-budget. The proposed method is also compatible with several other LRMs,\ndemonstrating its generalization capability. The code can be accessed at:\nhttps://github.com/AI9Stars/AStar-Thought."}
{"id": "2505.24553", "pdf": "https://arxiv.org/pdf/2505.24553.pdf", "abs": "https://arxiv.org/abs/2505.24553", "title": "CREFT: Sequential Multi-Agent LLM for Character Relation Extraction", "authors": ["Ye Eun Chun", "Taeyoon Hwang", "Seung-won Hwang", "Byung-Hak Kim"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Understanding complex character relations is crucial for narrative analysis\nand efficient script evaluation, yet existing extraction methods often fail to\nhandle long-form narratives with nuanced interactions. To address this\nchallenge, we present CREFT, a novel sequential framework leveraging\nspecialized Large Language Model (LLM) agents. First, CREFT builds a base\ncharacter graph through knowledge distillation, then iteratively refines\ncharacter composition, relation extraction, role identification, and group\nassignments. Experiments on a curated Korean drama dataset demonstrate that\nCREFT significantly outperforms single-agent LLM baselines in both accuracy and\ncompleteness. By systematically visualizing character networks, CREFT\nstreamlines narrative comprehension and accelerates script review -- offering\nsubstantial benefits to the entertainment, publishing, and educational sectors."}
{"id": "2505.24554", "pdf": "https://arxiv.org/pdf/2505.24554.pdf", "abs": "https://arxiv.org/abs/2505.24554", "title": "Bench4KE: Benchmarking Automated Competency Question Generation", "authors": ["Anna Sofia Lippolis", "Minh Davide Ragagni", "Paolo Ciancarini", "Andrea Giovanni Nuzzolese", "Valentina Presutti"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The availability of Large Language Models (LLMs) presents a unique\nopportunity to reinvigorate research on Knowledge Engineering (KE) automation,\na trend already evident in recent efforts developing LLM-based methods and\ntools for the automatic generation of Competency Questions (CQs). However, the\nevaluation of these tools lacks standardisation. This undermines the\nmethodological rigour and hinders the replication and comparison of results. To\naddress this gap, we introduce Bench4KE, an extensible API-based benchmarking\nsystem for KE automation. Its first release focuses on evaluating tools that\ngenerate CQs automatically. CQs are natural language questions used by ontology\nengineers to define the functional requirements of an ontology. Bench4KE\nprovides a curated gold standard consisting of CQ datasets from four real-world\nontology projects. It uses a suite of similarity metrics to assess the quality\nof the CQs generated. We present a comparative analysis of four recent CQ\ngeneration systems, which are based on LLMs, establishing a baseline for future\nresearch. Bench4KE is also designed to accommodate additional KE automation\ntasks, such as SPARQL query generation, ontology testing and drafting. Code and\ndatasets are publicly available under the Apache 2.0 license."}
{"id": "2505.24561", "pdf": "https://arxiv.org/pdf/2505.24561.pdf", "abs": "https://arxiv.org/abs/2505.24561", "title": "Improving Language and Modality Transfer in Translation by Character-level Modeling", "authors": ["Ioannis Tsiamas", "David Dale", "Marta R. Costa-jussà"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Current translation systems, despite being highly multilingual, cover only 5%\nof the world's languages. Expanding language coverage to the long-tail of\nlow-resource languages requires data-efficient methods that rely on\ncross-lingual and cross-modal knowledge transfer. To this end, we propose a\ncharacter-based approach to improve adaptability to new languages and\nmodalities. Our method leverages SONAR, a multilingual fixed-size embedding\nspace with different modules for encoding and decoding. We use a\nteacher-student approach with parallel translation data to obtain a\ncharacter-level encoder. Then, using ASR data, we train a lightweight adapter\nto connect a massively multilingual CTC ASR model (MMS), to the character-level\nencoder, potentially enabling speech translation from 1,000+ languages.\nExperimental results in text translation for 75 languages on FLORES+\ndemonstrate that our character-based approach can achieve better language\ntransfer than traditional subword-based models, especially outperforming them\nin low-resource settings, and demonstrating better zero-shot generalizability\nto unseen languages. Our speech adaptation, maximizing knowledge transfer from\nthe text modality, achieves state-of-the-art results in speech-to-text\ntranslation on the FLEURS benchmark on 33 languages, surpassing previous\nsupervised and cascade models, albeit being a zero-shot model with minimal\nsupervision from ASR data."}
{"id": "2505.24575", "pdf": "https://arxiv.org/pdf/2505.24575.pdf", "abs": "https://arxiv.org/abs/2505.24575", "title": "NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization", "authors": ["Hyuntak Kim", "Byung-Hak Kim"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the main track of ACL 2025", "summary": "Summarizing long-form narratives--such as books, movies, and TV\nscripts--requires capturing intricate plotlines, character interactions, and\nthematic coherence, a task that remains challenging for existing LLMs. We\nintroduce NexusSum, a multi-agent LLM framework for narrative summarization\nthat processes long-form text through a structured, sequential\npipeline--without requiring fine-tuning. Our approach introduces two key\ninnovations: (1) Dialogue-to-Description Transformation: A narrative-specific\npreprocessing method that standardizes character dialogue and descriptive text\ninto a unified format, improving coherence. (2) Hierarchical Multi-LLM\nSummarization: A structured summarization pipeline that optimizes chunk\nprocessing and controls output length for accurate, high-quality summaries. Our\nmethod establishes a new state-of-the-art in narrative summarization, achieving\nup to a 30.0% improvement in BERTScore (F1) across books, movies, and TV\nscripts. These results demonstrate the effectiveness of multi-agent LLMs in\nhandling long-form content, offering a scalable approach for structured\nsummarization in diverse storytelling domains."}
{"id": "2505.24581", "pdf": "https://arxiv.org/pdf/2505.24581.pdf", "abs": "https://arxiv.org/abs/2505.24581", "title": "GATE: General Arabic Text Embedding for Enhanced Semantic Textual Similarity with Matryoshka Representation Learning and Hybrid Loss Training", "authors": ["Omer Nacar", "Anis Koubaa", "Serry Sibaee", "Yasser Al-Habashi", "Adel Ammar", "Wadii Boulila"], "categories": ["cs.CL"], "comment": null, "summary": "Semantic textual similarity (STS) is a critical task in natural language\nprocessing (NLP), enabling applications in retrieval, clustering, and\nunderstanding semantic relationships between texts. However, research in this\narea for the Arabic language remains limited due to the lack of high-quality\ndatasets and pre-trained models. This scarcity of resources has restricted the\naccurate evaluation and advance of semantic similarity in Arabic text. This\npaper introduces General Arabic Text Embedding (GATE) models that achieve\nstate-of-the-art performance on the Semantic Textual Similarity task within the\nMTEB benchmark. GATE leverages Matryoshka Representation Learning and a hybrid\nloss training approach with Arabic triplet datasets for Natural Language\nInference, which are essential for enhancing model performance in tasks that\ndemand fine-grained semantic understanding. GATE outperforms larger models,\nincluding OpenAI, with a 20-25% performance improvement on STS benchmarks,\neffectively capturing the unique semantic nuances of Arabic."}
{"id": "2505.24593", "pdf": "https://arxiv.org/pdf/2505.24593.pdf", "abs": "https://arxiv.org/abs/2505.24593", "title": "Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis", "authors": ["Junzhuo Li", "Bo Wang", "Xiuze Zhou", "Peijie Jiang", "Jia Liu", "Xuming Hu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025", "summary": "The interpretability of Mixture-of-Experts (MoE) models, especially those\nwith heterogeneous designs, remains underexplored. Existing attribution methods\nfor dense models fail to capture dynamic routing-expert interactions in sparse\nMoE architectures. To address this issue, we propose a cross-level attribution\nalgorithm to analyze sparse MoE architectures (Qwen 1.5-MoE, OLMoE,\nMixtral-8x7B) against dense models (Qwen 1.5-7B, Llama-7B, Mixtral-7B). Results\nshow MoE models achieve 37% higher per-layer efficiency via a \"mid-activation,\nlate-amplification\" pattern: early layers screen experts, while late layers\nrefine knowledge collaboratively. Ablation studies reveal a \"basic-refinement\"\nframework--shared experts handle general tasks (entity recognition), while\nrouted experts specialize in domain-specific processing (geographic\nattributes). Semantic-driven routing is evidenced by strong correlations\nbetween attention heads and experts (r=0.68), enabling task-aware coordination.\nNotably, architectural depth dictates robustness: deep Qwen 1.5-MoE mitigates\nexpert failures (e.g., 43% MRR drop in geographic tasks when blocking top-10\nexperts) through shared expert redundancy, whereas shallow OLMoE suffers severe\ndegradation (76% drop). Task sensitivity further guides design: core-sensitive\ntasks (geography) require concentrated expertise, while distributed-tolerant\ntasks (object attributes) leverage broader participation. These insights\nadvance MoE interpretability, offering principles to balance efficiency,\nspecialization, and robustness."}
{"id": "2505.24609", "pdf": "https://arxiv.org/pdf/2505.24609.pdf", "abs": "https://arxiv.org/abs/2505.24609", "title": "Explainable Depression Detection using Masked Hard Instance Mining", "authors": ["Patawee Prakrankamanant", "Shinji Watanabe", "Ekapol Chuangsuwanich"], "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the critical need for improved explainability in\ntext-based depression detection. While offering predictive outcomes, current\nsolutions often overlook the understanding of model predictions which can\nhinder trust in the system. We propose the use of Masked Hard Instance Mining\n(MHIM) to enhance the explainability in the depression detection task. MHIM\nstrategically masks attention weights within the model, compelling it to\ndistribute attention across a wider range of salient features. We evaluate MHIM\non two datasets representing distinct languages: Thai (Thai-Maywe) and English\n(DAIC-WOZ). Our results demonstrate that MHIM significantly improves\nperformance in terms of both prediction accuracy and explainability metrics."}
{"id": "2505.24613", "pdf": "https://arxiv.org/pdf/2505.24613.pdf", "abs": "https://arxiv.org/abs/2505.24613", "title": "When Harry Meets Superman: The Role of The Interlocutor in Persona-Based Dialogue Generation", "authors": ["Daniela Occhipinti", "Marco Guerini", "Malvina Nissim"], "categories": ["cs.CL"], "comment": null, "summary": "Endowing dialogue agents with persona information has proven to significantly\nimprove the consistency and diversity of their generations. While much focus\nhas been placed on aligning dialogues with provided personas, the adaptation to\nthe interlocutor's profile remains largely underexplored. In this work, we\ninvestigate three key aspects: (1) a model's ability to align responses with\nboth the provided persona and the interlocutor's; (2) its robustness when\ndealing with familiar versus unfamiliar interlocutors and topics, and (3) the\nimpact of additional fine-tuning on specific persona-based dialogues. We\nevaluate dialogues generated with diverse speaker pairings and topics, framing\nthe evaluation as an author identification task and employing both\nLLM-as-a-judge and human evaluations. By systematically masking or disclosing\ninformation about the interlocutor, we assess its impact on dialogue\ngeneration. Results show that access to the interlocutor's persona improves the\nrecognition of the target speaker, while masking it does the opposite. Although\nmodels generalise well across topics, they struggle with unfamiliar\ninterlocutors. Finally, we found that in zero-shot settings, LLMs often copy\nbiographical details, facilitating identification but trivialising the task."}
{"id": "2505.24615", "pdf": "https://arxiv.org/pdf/2505.24615.pdf", "abs": "https://arxiv.org/abs/2505.24615", "title": "Harnessing Large Language Models for Scientific Novelty Detection", "authors": ["Yan Liu", "Zonglin Yang", "Soujanya Poria", "Thanh-Son Nguyen", "Erik Cambria"], "categories": ["cs.CL", "H.4.0"], "comment": "15 pages, 3 figures, 3 tables", "summary": "In an era of exponential scientific growth, identifying novel research ideas\nis crucial and challenging in academia. Despite potential, the lack of an\nappropriate benchmark dataset hinders the research of novelty detection. More\nimportantly, simply adopting existing NLP technologies, e.g., retrieving and\nthen cross-checking, is not a one-size-fits-all solution due to the gap between\ntextual similarity and idea conception. In this paper, we propose to harness\nlarge language models (LLMs) for scientific novelty detection (ND), associated\nwith two new datasets in marketing and NLP domains. To construct the\nconsiderate datasets for ND, we propose to extract closure sets of papers based\non their relationship, and then summarize their main ideas based on LLMs. To\ncapture idea conception, we propose to train a lightweight retriever by\ndistilling the idea-level knowledge from LLMs to align ideas with similar\nconception, enabling efficient and accurate idea retrieval for LLM novelty\ndetection. Experiments show our method consistently outperforms others on the\nproposed benchmark datasets for idea retrieval and ND tasks. Codes and data are\navailable at https://anonymous.4open.science/r/NoveltyDetection-10FB/."}
{"id": "2505.24616", "pdf": "https://arxiv.org/pdf/2505.24616.pdf", "abs": "https://arxiv.org/abs/2505.24616", "title": "Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX", "authors": ["Nikita Martynov", "Anastasia Mordasheva", "Dmitriy Gorbetskiy", "Danil Astafurov", "Ulyana Isaeva", "Elina Basyrova", "Sergey Skachkov", "Victoria Berestova", "Nikolay Ivanov", "Valeriia Zanina", "Alena Fenogenova"], "categories": ["cs.CL", "cs.AI"], "comment": "179 pages", "summary": "We introduce POLLUX, a comprehensive open-source benchmark designed to\nevaluate the generative capabilities of large language models (LLMs) in\nRussian. Our main contribution is a novel evaluation methodology that enhances\nthe interpretability of LLM assessment. For each task type, we define a set of\ndetailed criteria and develop a scoring protocol where models evaluate\nresponses and provide justifications for their ratings. This enables\ntransparent, criteria-driven evaluation beyond traditional resource-consuming,\nside-by-side human comparisons. POLLUX includes a detailed, fine-grained\ntaxonomy of 35 task types covering diverse generative domains such as code\ngeneration, creative writing, and practical assistant use cases, totaling 2,100\nmanually crafted and professionally authored prompts. Each task is categorized\nby difficulty (easy/medium/hard), with experts constructing the dataset\nentirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B)\nevaluators trained for nuanced assessment of generative outputs. This approach\nprovides scalable, interpretable evaluation and annotation tools for model\ndevelopment, effectively replacing costly and less precise human judgments."}
{"id": "2505.24619", "pdf": "https://arxiv.org/pdf/2505.24619.pdf", "abs": "https://arxiv.org/abs/2505.24619", "title": "Interpretable phenotyping of Heart Failure patients with Dutch discharge letters", "authors": ["Vittorio Torri", "Machteld J. Boonstra", "Marielle C. van de Veerdonk", "Deborah N. Kalkman", "Alicia Uijl", "Francesca Ieva", "Ameen Abu-Hanna", "Folkert W. Asselbergs", "Iacer Calixto"], "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7; J.3"], "comment": "43 pages, 8 figures", "summary": "Objective: Heart failure (HF) patients present with diverse phenotypes\naffecting treatment and prognosis. This study evaluates models for phenotyping\nHF patients based on left ventricular ejection fraction (LVEF) classes, using\nstructured and unstructured data, assessing performance and interpretability.\n  Materials and Methods: The study analyzes all HF hospitalizations at both\nAmsterdam UMC hospitals (AMC and VUmc) from 2015 to 2023 (33,105\nhospitalizations, 16,334 patients). Data from AMC were used for model training,\nand from VUmc for external validation. The dataset was unlabelled and included\ntabular clinical measurements and discharge letters. Silver labels for LVEF\nclasses were generated by combining diagnosis codes, echocardiography results,\nand textual mentions. Gold labels were manually annotated for 300 patients for\ntesting. Multiple Transformer-based (black-box) and Aug-Linear (white-box)\nmodels were trained and compared with baselines on structured and unstructured\ndata. To evaluate interpretability, two clinicians annotated 20 discharge\nletters by highlighting information they considered relevant for LVEF\nclassification. These were compared to SHAP and LIME explanations from\nblack-box models and the inherent explanations of Aug-Linear models.\n  Results: BERT-based and Aug-Linear models, using discharge letters alone,\nachieved the highest classification results (AUC=0.84 for BERT, 0.81 for\nAug-Linear on external validation), outperforming baselines. Aug-Linear\nexplanations aligned more closely with clinicians' explanations than post-hoc\nexplanations on black-box models.\n  Conclusions: Discharge letters emerged as the most informative source for\nphenotyping HF patients. Aug-Linear models matched black-box performance while\nproviding clinician-aligned interpretability, supporting their use in\ntransparent clinical decision-making."}
{"id": "2505.24621", "pdf": "https://arxiv.org/pdf/2505.24621.pdf", "abs": "https://arxiv.org/abs/2505.24621", "title": "Benchmarking Large Language Models for Cryptanalysis and Mismatched-Generalization", "authors": ["Utsav Maskey", "Chencheng Zhu", "Usman Naseem"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Recent advancements in Large Language Models (LLMs) have transformed natural\nlanguage understanding and generation, leading to extensive benchmarking across\ndiverse tasks. However, cryptanalysis a critical area for data security and\nencryption has not yet been thoroughly explored in LLM evaluations. To address\nthis gap, we evaluate cryptanalytic potential of state of the art LLMs on\nencrypted texts generated using a range of cryptographic algorithms. We\nintroduce a novel benchmark dataset comprising diverse plain texts spanning\nvarious domains, lengths, writing styles, and topics paired with their\nencrypted versions. Using zero-shot and few shot settings, we assess multiple\nLLMs for decryption accuracy and semantic comprehension across different\nencryption schemes. Our findings reveal key insights into the strengths and\nlimitations of LLMs in side-channel communication while raising concerns about\ntheir susceptibility to jailbreaking attacks. This research highlights the\ndual-use nature of LLMs in security contexts and contributes to the ongoing\ndiscussion on AI safety and security."}
{"id": "2505.24630", "pdf": "https://arxiv.org/pdf/2505.24630.pdf", "abs": "https://arxiv.org/abs/2505.24630", "title": "The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for Large Reasoning Models", "authors": ["Junyi Li", "Hwee Tou Ng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have significantly advanced in reasoning tasks\nthrough reinforcement learning (RL) optimization, achieving impressive\ncapabilities across various challenging benchmarks. However, our empirical\nanalysis reveals a critical drawback: reasoning-oriented RL fine-tuning\nsignificantly increases the prevalence of hallucinations. We theoretically\nanalyze the RL training dynamics, identifying high-variance gradient,\nentropy-induced randomness, and susceptibility to spurious local optima as key\nfactors leading to hallucinations. To address this drawback, we propose\nFactuality-aware Step-wise Policy Optimization (FSPO), an innovative RL\nfine-tuning algorithm incorporating explicit factuality verification at each\nreasoning step. FSPO leverages automated verification against given evidence to\ndynamically adjust token-level advantage values, incentivizing factual\ncorrectness throughout the reasoning process. Experiments across mathematical\nreasoning and hallucination benchmarks using Qwen2.5 and Llama models\ndemonstrate that FSPO effectively reduces hallucinations while enhancing\nreasoning accuracy, substantially improving both reliability and performance."}
{"id": "2505.24635", "pdf": "https://arxiv.org/pdf/2505.24635.pdf", "abs": "https://arxiv.org/abs/2505.24635", "title": "Disentangling Language and Culture for Evaluating Multilingual Large Language Models", "authors": ["Jiahao Ying", "Wei Tang", "Yiran Zhao", "Yixin Cao", "Yu Rong", "Wenxuan Zhang"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "This paper introduces a Dual Evaluation Framework to comprehensively assess\nthe multilingual capabilities of LLMs. By decomposing the evaluation along the\ndimensions of linguistic medium and cultural context, this framework enables a\nnuanced analysis of LLMs' ability to process questions within both native and\ncross-cultural contexts cross-lingually. Extensive evaluations are conducted on\na wide range of models, revealing a notable \"CulturalLinguistic Synergy\"\nphenomenon, where models exhibit better performance when questions are\nculturally aligned with the language. This phenomenon is further explored\nthrough interpretability probing, which shows that a higher proportion of\nspecific neurons are activated in a language's cultural context. This\nactivation proportion could serve as a potential indicator for evaluating\nmultilingual performance during model training. Our findings challenge the\nprevailing notion that LLMs, primarily trained on English data, perform\nuniformly across languages and highlight the necessity of culturally and\nlinguistically model evaluations. Our code can be found at\nhttps://yingjiahao14. github.io/Dual-Evaluation/."}
{"id": "2505.24640", "pdf": "https://arxiv.org/pdf/2505.24640.pdf", "abs": "https://arxiv.org/abs/2505.24640", "title": "Efficient Text Encoders for Labor Market Analysis", "authors": ["Jens-Joris Decorte", "Jeroen Van Hautte", "Chris Develder", "Thomas Demeester"], "categories": ["cs.CL", "cs.AI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Labor market analysis relies on extracting insights from job advertisements,\nwhich provide valuable yet unstructured information on job titles and\ncorresponding skill requirements. While state-of-the-art methods for skill\nextraction achieve strong performance, they depend on large language models\n(LLMs), which are computationally expensive and slow. In this paper, we propose\n\\textbf{ConTeXT-match}, a novel contrastive learning approach with token-level\nattention that is well-suited for the extreme multi-label classification task\nof skill classification. \\textbf{ConTeXT-match} significantly improves skill\nextraction efficiency and performance, achieving state-of-the-art results with\na lightweight bi-encoder model. To support robust evaluation, we introduce\n\\textbf{Skill-XL}, a new benchmark with exhaustive, sentence-level skill\nannotations that explicitly address the redundancy in the large label space.\nFinally, we present \\textbf{JobBERT V2}, an improved job title normalization\nmodel that leverages extracted skills to produce high-quality job title\nrepresentations. Experiments demonstrate that our models are efficient,\naccurate, and scalable, making them ideal for large-scale, real-time labor\nmarket analysis."}
{"id": "2505.24643", "pdf": "https://arxiv.org/pdf/2505.24643.pdf", "abs": "https://arxiv.org/abs/2505.24643", "title": "Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based Pairwise Ranking with Batching and Caching", "authors": ["Juan Wisznia", "Cecilia Bolaños", "Juan Tollo", "Giovanni Marraffini", "Agustín Gianolini", "Noe Hsueh", "Luciano Del Corro"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce a novel framework for analyzing sorting algorithms in pairwise\nranking prompting (PRP), re-centering the cost model around LLM inferences\nrather than traditional pairwise comparisons. While classical metrics based on\ncomparison counts have traditionally been used to gauge efficiency, our\nanalysis reveals that expensive LLM inferences overturn these predictions;\naccordingly, our framework encourages strategies such as batching and caching\nto mitigate inference costs. We show that algorithms optimal in the classical\nsetting can lose efficiency when LLM inferences dominate the cost under certain\noptimizations."}
{"id": "2505.24646", "pdf": "https://arxiv.org/pdf/2505.24646.pdf", "abs": "https://arxiv.org/abs/2505.24646", "title": "PRISM: A Framework for Producing Interpretable Political Bias Embeddings with Political-Aware Cross-Encoder", "authors": ["Yiqun Sun", "Qiang Huang", "Anthony K. H. Tung", "Jun Yu"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Semantic Text Embedding is a fundamental NLP task that encodes textual\ncontent into vector representations, where proximity in the embedding space\nreflects semantic similarity. While existing embedding models excel at\ncapturing general meaning, they often overlook ideological nuances, limiting\ntheir effectiveness in tasks that require an understanding of political bias.\nTo address this gap, we introduce PRISM, the first framework designed to\nProduce inteRpretable polItical biaS eMbeddings. PRISM operates in two key\nstages: (1) Controversial Topic Bias Indicator Mining, which systematically\nextracts fine-grained political topics and their corresponding bias indicators\nfrom weakly labeled news data, and (2) Cross-Encoder Political Bias Embedding,\nwhich assigns structured bias scores to news articles based on their alignment\nwith these indicators. This approach ensures that embeddings are explicitly\ntied to bias-revealing dimensions, enhancing both interpretability and\npredictive power. Through extensive experiments on two large-scale datasets, we\ndemonstrate that PRISM outperforms state-of-the-art text embedding models in\npolitical bias classification while offering highly interpretable\nrepresentations that facilitate diversified retrieval and ideological analysis.\nThe source code is available at https://github.com/dukesun99/ACL-PRISM."}
{"id": "2505.24656", "pdf": "https://arxiv.org/pdf/2505.24656.pdf", "abs": "https://arxiv.org/abs/2505.24656", "title": "MSDA: Combining Pseudo-labeling and Self-Supervision for Unsupervised Domain Adaptation in ASR", "authors": ["Dimitrios Damianos", "Georgios Paraskevopoulos", "Alexandros Potamianos"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "In this work, we investigate the Meta PL unsupervised domain adaptation\nframework for Automatic Speech Recognition (ASR). We introduce a Multi-Stage\nDomain Adaptation pipeline (MSDA), a sample-efficient, two-stage adaptation\napproach that integrates self-supervised learning with semi-supervised\ntechniques. MSDA is designed to enhance the robustness and generalization of\nASR models, making them more adaptable to diverse conditions. It is\nparticularly effective for low-resource languages like Greek and in weakly\nsupervised scenarios where labeled data is scarce or noisy. Through extensive\nexperiments, we demonstrate that Meta PL can be applied effectively to ASR\ntasks, achieving state-of-the-art results, significantly outperforming\nstate-of-the-art methods, and providing more robust solutions for unsupervised\ndomain adaptation in ASR. Our ablations highlight the necessity of utilizing a\ncascading approach when combining self-supervision with self-training."}
{"id": "2505.24671", "pdf": "https://arxiv.org/pdf/2505.24671.pdf", "abs": "https://arxiv.org/abs/2505.24671", "title": "Multiple LLM Agents Debate for Equitable Cultural Alignment", "authors": ["Dayeon Ki", "Rachel Rudinger", "Tianyi Zhou", "Marine Carpuat"], "categories": ["cs.CL", "cs.AI"], "comment": "37 pages, 18 figures", "summary": "Large Language Models (LLMs) need to adapt their predictions to diverse\ncultural contexts to benefit diverse communities across the world. While\nprevious efforts have focused on single-LLM, single-turn approaches, we propose\nto exploit the complementary strengths of multiple LLMs to promote cultural\nadaptability. We introduce a Multi-Agent Debate framework, where two LLM-based\nagents debate over a cultural scenario and collaboratively reach a final\ndecision. We propose two variants: one where either LLM agents exclusively\ndebate and another where they dynamically choose between self-reflection and\ndebate during their turns. We evaluate these approaches on 7 open-weight LLMs\n(and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette\nnorms in 75 countries. Experiments show that debate improves both overall\naccuracy and cultural group parity over single-LLM baselines. Notably,\nmulti-agent debate enables relatively small LLMs (7-9B) to achieve accuracies\ncomparable to that of a much larger model (27B parameters)."}
{"id": "2505.24672", "pdf": "https://arxiv.org/pdf/2505.24672.pdf", "abs": "https://arxiv.org/abs/2505.24672", "title": "TRIDENT: Enhancing Large Language Model Safety with Tri-Dimensional Diversified Red-Teaming Data Synthesis", "authors": ["Xiaorui Wu", "Xiaofeng Mao", "Fei Li", "Xin Zhang", "Xuanhong Li", "Chong Teng", "Donghong Ji", "Zhuang Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel in various natural language processing\ntasks but remain vulnerable to generating harmful content or being exploited\nfor malicious purposes. Although safety alignment datasets have been introduced\nto mitigate such risks through supervised fine-tuning (SFT), these datasets\noften lack comprehensive risk coverage. Most existing datasets focus primarily\non lexical diversity while neglecting other critical dimensions. To address\nthis limitation, we propose a novel analysis framework to systematically\nmeasure the risk coverage of alignment datasets across three essential\ndimensions: Lexical Diversity, Malicious Intent, and Jailbreak Tactics. We\nfurther introduce TRIDENT, an automated pipeline that leverages persona-based,\nzero-shot LLM generation to produce diverse and comprehensive instructions\nspanning these dimensions. Each harmful instruction is paired with an ethically\naligned response, resulting in two datasets: TRIDENT-Core, comprising 26,311\nexamples, and TRIDENT-Edge, with 18,773 examples. Fine-tuning Llama 3.1-8B on\nTRIDENT-Edge demonstrates substantial improvements, achieving an average 14.29%\nreduction in Harm Score, and a 20% decrease in Attack Success Rate compared to\nthe best-performing baseline model fine-tuned on the WildBreak dataset."}
{"id": "2505.24680", "pdf": "https://arxiv.org/pdf/2505.24680.pdf", "abs": "https://arxiv.org/abs/2505.24680", "title": "A Simple Linear Patch Revives Layer-Pruned Large Language Models", "authors": ["Xinrui Chen", "Haoli Bai", "Tao Yuan", "Ruikang Liu", "Kang Zhao", "Xianzhi Yu", "Lu Hou", "Tian Guan", "Yonghong He", "Chun Yuan"], "categories": ["cs.CL"], "comment": null, "summary": "Layer pruning has become a popular technique for compressing large language\nmodels (LLMs) due to its simplicity. However, existing layer pruning methods\noften suffer from significant performance drops. We identify that this\ndegradation stems from the mismatch of activation magnitudes across layers and\ntokens at the pruning interface. To address this, we propose LinearPatch, a\nsimple plug-and-play technique to revive the layer-pruned LLMs. The proposed\nmethod adopts Hadamard transformation to suppress massive outliers in\nparticular tokens, and channel-wise scaling to align the activation magnitudes.\nThese operations can be fused into a single matrix, which functions as a patch\nto bridge the pruning interface with negligible inference overhead. LinearPatch\nretains up to 94.15% performance of the original model when pruning 5 layers of\nLLaMA-3-8B on the question answering benchmark, surpassing existing\nstate-of-the-art methods by 4%. In addition, the patch matrix can be further\noptimized with memory efficient offline knowledge distillation. With only 5K\nsamples, the retained performance of LinearPatch can be further boosted to\n95.16% within 30 minutes on a single computing card."}
{"id": "2505.24683", "pdf": "https://arxiv.org/pdf/2505.24683.pdf", "abs": "https://arxiv.org/abs/2505.24683", "title": "Should I Share this Translation? Evaluating Quality Feedback for User Reliance on Machine Translation", "authors": ["Dayeon Ki", "Kevin Duh", "Marine Carpuat"], "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 7 figures", "summary": "As people increasingly use AI systems in work and daily life, feedback\nmechanisms that help them use AI responsibly are urgently needed, particularly\nin settings where users are not equipped to assess the quality of AI\npredictions. We study a realistic Machine Translation (MT) scenario where\nmonolingual users decide whether to share an MT output, first without and then\nwith quality feedback. We compare four types of quality feedback: explicit\nfeedback that directly give users an assessment of translation quality using 1)\nerror highlights and 2) LLM explanations, and implicit feedback that helps\nusers compare MT inputs and outputs through 3) backtranslation and 4)\nquestion-answer (QA) tables. We find that all feedback types, except error\nhighlights, significantly improve both decision accuracy and appropriate\nreliance. Notably, implicit feedback, especially QA tables, yields\nsignificantly greater gains than explicit feedback in terms of decision\naccuracy, appropriate reliance, and user perceptions, receiving the highest\nratings for helpfulness and trust, and the lowest for mental burden."}
{"id": "2505.24688", "pdf": "https://arxiv.org/pdf/2505.24688.pdf", "abs": "https://arxiv.org/abs/2505.24688", "title": "Soft Reasoning: Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration", "authors": ["Qinglin Zhu", "Runcong Zhao", "Hanqi Yan", "Yulan He", "Yudong Chen", "Lin Gui"], "categories": ["cs.CL"], "comment": "Accepted by ICML 2025", "summary": "Large Language Models (LLMs) struggle with complex reasoning due to limited\ndiversity and inefficient search. We propose Soft Reasoning, an embedding-based\nsearch framework that optimises the embedding of the first token to guide\ngeneration. It combines (1) embedding perturbation for controlled exploration\nand (2) Bayesian optimisation to refine embeddings via a verifier-guided\nobjective, balancing exploration and exploitation. This approach improves\nreasoning accuracy and coherence while avoiding reliance on heuristic search.\nExperiments demonstrate superior correctness with minimal computation, making\nit a scalable, model-agnostic solution."}
{"id": "2505.24689", "pdf": "https://arxiv.org/pdf/2505.24689.pdf", "abs": "https://arxiv.org/abs/2505.24689", "title": "BPE Stays on SCRIPT: Structured Encoding for Robust Multilingual Pretokenization", "authors": ["Sander Land", "Catherine Arnett"], "categories": ["cs.CL"], "comment": "9 pages, 2 figures. For associated code, see\n  https://github.com/sanderland/script_bpe", "summary": "Byte Pair Encoding (BPE) tokenizers, widely used in Large Language Models,\nface challenges in multilingual settings, including penalization of non-Western\nscripts and the creation of tokens with partial UTF-8 sequences.\nPretokenization, often reliant on complex regular expressions, can also\nintroduce fragility and unexpected edge cases. We propose SCRIPT (Script\nCategory Representation in PreTokenization), a novel encoding scheme that\nbypasses UTF-8 byte conversion by using initial tokens based on Unicode script\nand category properties. This approach enables a simple, rule-based\npretokenization strategy that respects script boundaries, offering a robust\nalternative to pretokenization strategies based on regular expressions. We also\nintroduce and validate a constrained BPE merging strategy that enforces\ncharacter integrity, applicable to both SCRIPT-BPE and byte-based BPE. Our\nexperiments demonstrate that SCRIPT-BPE achieves competitive compression while\neliminating encoding-based penalties for non-Latin-script languages."}
{"id": "2505.24691", "pdf": "https://arxiv.org/pdf/2505.24691.pdf", "abs": "https://arxiv.org/abs/2505.24691", "title": "Speech-to-Text Translation with Phoneme-Augmented CoT: Enhancing Cross-Lingual Transfer in Low-Resource Scenarios", "authors": ["Gerard I. Gállego", "Oriol Pareras", "Martí Cortada Garcia", "Lucas Takanori", "Javier Hernando"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "We propose a Speech-to-Text Translation (S2TT) approach that integrates\nphoneme representations into a Chain-of-Thought (CoT) framework to improve\ntranslation in low-resource and zero-resource settings. By introducing phoneme\nrecognition as an intermediate step, we enhance cross-lingual transfer,\nenabling translation even for languages with no labeled speech data. Our system\nbuilds on a multilingual LLM, which we extend to process speech and phonemes.\nTraining follows a curriculum learning strategy that progressively introduces\nmore complex tasks. Experiments on multilingual S2TT benchmarks show that\nphoneme-augmented CoT improves translation quality in low-resource conditions\nand enables zero-resource translation, while slightly impacting high-resource\nperformance. Despite this trade-off, our findings demonstrate that\nphoneme-based CoT is a promising step toward making S2TT more accessible across\ndiverse languages."}
{"id": "2505.24701", "pdf": "https://arxiv.org/pdf/2505.24701.pdf", "abs": "https://arxiv.org/abs/2505.24701", "title": "Multi-Domain ABSA Conversation Dataset Generation via LLMs for Real-World Evaluation and Model Comparison", "authors": ["Tejul Pandit", "Meet Raval", "Dhvani Upadhyay"], "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 3 figures, 5 tables, 6th International Conference on\n  Natural Language Computing and AI (NLCAI 2025), ISBN : 978-1-923107-59-5,\n  Computer Science & Information Technology (CS & IT), ISSN : 2231 - 5403,\n  Volume 15, Number 10, May 2025", "summary": "Aspect-Based Sentiment Analysis (ABSA) offers granular insights into opinions\nbut often suffers from the scarcity of diverse, labeled datasets that reflect\nreal-world conversational nuances. This paper presents an approach for\ngenerating synthetic ABSA data using Large Language Models (LLMs) to address\nthis gap. We detail the generation process aimed at producing data with\nconsistent topic and sentiment distributions across multiple domains using\nGPT-4o. The quality and utility of the generated data were evaluated by\nassessing the performance of three state-of-the-art LLMs (Gemini 1.5 Pro,\nClaude 3.5 Sonnet, and DeepSeek-R1) on topic and sentiment classification\ntasks. Our results demonstrate the effectiveness of the synthetic data,\nrevealing distinct performance trade-offs among the models: DeepSeekR1 showed\nhigher precision, Gemini 1.5 Pro and Claude 3.5 Sonnet exhibited strong recall,\nand Gemini 1.5 Pro offered significantly faster inference. We conclude that\nLLM-based synthetic data generation is a viable and flexible method for\ncreating valuable ABSA resources, facilitating research and model evaluation\nwithout reliance on limited or inaccessible real-world labeled data."}
{"id": "2505.24712", "pdf": "https://arxiv.org/pdf/2505.24712.pdf", "abs": "https://arxiv.org/abs/2505.24712", "title": "HESEIA: A community-based dataset for evaluating social biases in large language models, co-designed in real school settings in Latin America", "authors": ["Guido Ivetta", "Marcos J. Gomez", "Sofía Martinelli", "Pietro Palombini", "M. Emilia Echeveste", "Nair Carolina Mazzeo", "Beatriz Busaniche", "Luciana Benotti"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Most resources for evaluating social biases in Large Language Models are\ndeveloped without co-design from the communities affected by these biases, and\nrarely involve participatory approaches. We introduce HESEIA, a dataset of\n46,499 sentences created in a professional development course. The course\ninvolved 370 high-school teachers and 5,370 students from 189 Latin-American\nschools. Unlike existing benchmarks, HESEIA captures intersectional biases\nacross multiple demographic axes and school subjects. It reflects local\ncontexts through the lived experience and pedagogical expertise of educators.\nTeachers used minimal pairs to create sentences that express stereotypes\nrelevant to their school subjects and communities. We show the dataset\ndiversity in term of demographic axes represented and also in terms of the\nknowledge areas included. We demonstrate that the dataset contains more\nstereotypes unrecognized by current LLMs than previous datasets. HESEIA is\navailable to support bias assessments grounded in educational communities."}
{"id": "2505.24713", "pdf": "https://arxiv.org/pdf/2505.24713.pdf", "abs": "https://arxiv.org/abs/2505.24713", "title": "Voice Conversion Improves Cross-Domain Robustness for Spoken Arabic Dialect Identification", "authors": ["Badr M. Abdullah", "Matthew Baas", "Bernd Möbius", "Dietrich Klakow"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted in Interspeech 2025", "summary": "Arabic dialect identification (ADI) systems are essential for large-scale\ndata collection pipelines that enable the development of inclusive speech\ntechnologies for Arabic language varieties. However, the reliability of current\nADI systems is limited by poor generalization to out-of-domain speech. In this\npaper, we present an effective approach based on voice conversion for training\nADI models that achieves state-of-the-art performance and significantly\nimproves robustness in cross-domain scenarios. Evaluated on a newly collected\nreal-world test set spanning four different domains, our approach yields\nconsistent improvements of up to +34.1% in accuracy across domains.\nFurthermore, we present an analysis of our approach and demonstrate that voice\nconversion helps mitigate the speaker bias in the ADI dataset. We release our\nrobust ADI model and cross-domain evaluation dataset to support the development\nof inclusive speech technologies for Arabic."}
{"id": "2505.24714", "pdf": "https://arxiv.org/pdf/2505.24714.pdf", "abs": "https://arxiv.org/abs/2505.24714", "title": "FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation", "authors": ["Junyu Luo", "Zhizhuo Kou", "Liming Yang", "Xiao Luo", "Jinsheng Huang", "Zhiping Xiao", "Jingshu Peng", "Chengzhong Liu", "Jiaming Ji", "Xuanzhe Liu", "Sirui Han", "Ming Zhang", "Yike Guo"], "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference", "summary": "Multimodal Large Language Models (MLLMs) have experienced rapid development\nin recent years. However, in the financial domain, there is a notable lack of\neffective and specialized multimodal evaluation datasets. To advance the\ndevelopment of MLLMs in the finance domain, we introduce FinMME, encompassing\nmore than 11,000 high-quality financial research samples across 18 financial\ndomains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We\nensure data quality through 20 annotators and carefully designed validation\nmechanisms. Additionally, we develop FinScore, an evaluation system\nincorporating hallucination penalties and multi-dimensional capability\nassessment to provide an unbiased evaluation. Extensive experimental results\ndemonstrate that even state-of-the-art models like GPT-4o exhibit\nunsatisfactory performance on FinMME, highlighting its challenging nature. The\nbenchmark exhibits high robustness with prediction variations under different\nprompts remaining below 1%, demonstrating superior reliability compared to\nexisting datasets. Our dataset and evaluation protocol are available at\nhttps://huggingface.co/datasets/luojunyu/FinMME and\nhttps://github.com/luo-junyu/FinMME."}
{"id": "2505.24726", "pdf": "https://arxiv.org/pdf/2505.24726.pdf", "abs": "https://arxiv.org/abs/2505.24726", "title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning", "authors": ["Shelly Bensal", "Umar Jamil", "Christopher Bryant", "Melisa Russak", "Kiran Kamble", "Dmytro Mozolevskyi", "Muayad Ali", "Waseem AlShikh"], "categories": ["cs.CL"], "comment": null, "summary": "We explore a method for improving the performance of large language models\nthrough self-reflection and reinforcement learning. By incentivizing the model\nto generate better self-reflections when it answers incorrectly, we demonstrate\nthat a model's ability to solve complex, verifiable tasks can be enhanced even\nwhen generating synthetic data is infeasible and only binary feedback is\navailable. Our framework operates in two stages: first, upon failing a given\ntask, the model generates a self-reflective commentary analyzing its previous\nattempt; second, the model is given another attempt at the task with the\nself-reflection in context. If the subsequent attempt succeeds, the tokens\ngenerated during the self-reflection phase are rewarded. Our experimental\nresults show substantial performance gains across a variety of model\narchitectures, as high as 34.7% improvement at math equation writing and 18.1%\nimprovement at function calling. Notably, smaller fine-tuned models (1.5\nbillion to 7 billion parameters) outperform models in the same family that are\n10 times larger. Our novel paradigm is thus an exciting pathway to more useful\nand reliable language models that can self-improve on challenging tasks with\nlimited external feedback."}
{"id": "2505.24731", "pdf": "https://arxiv.org/pdf/2505.24731.pdf", "abs": "https://arxiv.org/abs/2505.24731", "title": "Circuit Stability Characterizes Language Model Generalization", "authors": ["Alan Sun"], "categories": ["cs.CL"], "comment": "16 pages, 10 figures", "summary": "Extensively evaluating the capabilities of (large) language models is\ndifficult. Rapid development of state-of-the-art models induce benchmark\nsaturation, while creating more challenging datasets is labor-intensive.\nInspired by the recent developments in mechanistic interpretability, we\nintroduce circuit stability as a new way to assess model performance. Circuit\nstability refers to a model's ability to apply a consistent reasoning\nprocess-its circuit-across various inputs. We mathematically formalize circuit\nstability and circuit equivalence. Then, through three case studies, we\nempirically show that circuit stability and the lack thereof can characterize\nand predict different aspects of generalization. Our proposed methods offer a\nstep towards rigorously relating the generality of models to their\ninterpretability."}
{"id": "2505.24754", "pdf": "https://arxiv.org/pdf/2505.24754.pdf", "abs": "https://arxiv.org/abs/2505.24754", "title": "Don't Reinvent the Wheel: Efficient Instruction-Following Text Embedding based on Guided Space Transformation", "authors": ["Yingchaojie Feng", "Yiqun Sun", "Yandong Sun", "Minfeng Zhu", "Qiang Huang", "Anthony K. H. Tung", "Wei Chen"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted to ACL 2025", "summary": "In this work, we investigate an important task named instruction-following\ntext embedding, which generates dynamic text embeddings that adapt to user\ninstructions, highlighting specific attributes of text. Despite recent\nadvancements, existing approaches suffer from significant computational\noverhead, as they require re-encoding the entire corpus for each new\ninstruction. To address this challenge, we propose GSTransform, a novel\ninstruction-following text embedding framework based on Guided Space\nTransformation. Our key observation is that instruction-relevant information is\ninherently encoded in generic embeddings but remains underutilized. Instead of\nrepeatedly encoding the corpus for each instruction, GSTransform is a\nlightweight transformation mechanism that adapts pre-computed embeddings in\nreal time to align with user instructions, guided by a small amount of text\ndata with instruction-focused label annotation. We conduct extensive\nexperiments on three instruction-awareness downstream tasks across nine\nreal-world datasets, demonstrating that GSTransform improves\ninstruction-following text embedding quality over state-of-the-art methods\nwhile achieving dramatic speedups of 6~300x in real-time processing on\nlarge-scale datasets. The source code is available at\nhttps://github.com/YingchaojieFeng/GSTransform."}
{"id": "2505.24757", "pdf": "https://arxiv.org/pdf/2505.24757.pdf", "abs": "https://arxiv.org/abs/2505.24757", "title": "LGAR: Zero-Shot LLM-Guided Neural Ranking for Abstract Screening in Systematic Literature Reviews", "authors": ["Christian Jaumann", "Andreas Wiedholz", "Annemarie Friedrich"], "categories": ["cs.CL"], "comment": null, "summary": "The scientific literature is growing rapidly, making it hard to keep track of\nthe state-of-the-art. Systematic literature reviews (SLRs) aim to identify and\nevaluate all relevant papers on a topic. After retrieving a set of candidate\npapers, the abstract screening phase determines initial relevance. To date,\nabstract screening methods using large language models (LLMs) focus on binary\nclassification settings; existing question answering (QA) based ranking\napproaches suffer from error propagation. LLMs offer a unique opportunity to\nevaluate the SLR's inclusion and exclusion criteria, yet, existing benchmarks\ndo not provide them exhaustively. We manually extract these criteria as well as\nresearch questions for 57 SLRs, mostly in the medical domain, enabling\nprincipled comparisons between approaches. Moreover, we propose LGAR, a\nzero-shot LLM Guided Abstract Ranker composed of an LLM based graded relevance\nscorer and a dense re-ranker. Our extensive experiments show that LGAR\noutperforms existing QA-based methods by 5-10 pp. in mean average precision.\nOur code and data is publicly available."}
{"id": "2505.24768", "pdf": "https://arxiv.org/pdf/2505.24768.pdf", "abs": "https://arxiv.org/abs/2505.24768", "title": "From Macro to Micro: Probing Dataset Diversity in Language Model Fine-Tuning", "authors": ["Haoyu Li", "Xuhong Li", "Yiming Dong", "Kun Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Dataset diversity plays a pivotal role for the successful training of many\nmachine learning models, particularly in the supervised fine-tuning (SFT) stage\nof large language model (LLM) development. Despite increasing recognition of\nits importance, systematic analyses of dataset diversity still remain\nunderexplored. To address this gap, this work presents a systematic taxonomy of\nexisting diversity-control strategies, which primarily focus on the instruction\ncomponent, operating at either macroscopic (entire instruction semantics) or\nmesoscopic levels (instruction units), and furthermore introduces a novel\nanalysis of microscopic diversity within the response component, specifically\nanalyzing the statistical distribution of tokens in SFT training samples. In\nthe experimental evaluation, we construct fixed-size datasets (e.g., 10,000\nsamples each) from a corpus of 117,000 open-source SFT samples, incorporating\nsix distinct diversity-control strategies spanning macro-, meso-, and\nmicroscopic levels applied to both instructions and responses. We then\nfine-tune LLMs on these datasets to assess the six diversity-control\nstrategies. Results reveal that while macroscopic and mesoscopic strategies\nlead to higher performance with increasing diversity, the microscopic strategy\nin responses exhibits both a stronger correlation between model performance and\nthe degree of diversity and superior performance with maximum diversity across\nall strategies. These findings offer actionable insights for constructing\nhigh-performance SFT datasets."}
{"id": "2505.24778", "pdf": "https://arxiv.org/pdf/2505.24778.pdf", "abs": "https://arxiv.org/abs/2505.24778", "title": "Revisiting Epistemic Markers in Confidence Estimation: Can Markers Accurately Reflect Large Language Models' Uncertainty?", "authors": ["Jiayu Liu", "Qing Zong", "Weiqi Wang", "Yangqiu Song"], "categories": ["cs.CL"], "comment": "ACL2025", "summary": "As large language models (LLMs) are increasingly used in high-stakes domains,\naccurately assessing their confidence is crucial. Humans typically express\nconfidence through epistemic markers (e.g., \"fairly confident\") instead of\nnumerical values. However, it remains unclear whether LLMs consistently use\nthese markers to reflect their intrinsic confidence due to the difficulty of\nquantifying uncertainty associated with various markers. To address this gap,\nwe first define marker confidence as the observed accuracy when a model employs\nan epistemic marker. We evaluate its stability across multiple\nquestion-answering datasets in both in-distribution and out-of-distribution\nsettings for open-source and proprietary LLMs. Our results show that while\nmarkers generalize well within the same distribution, their confidence is\ninconsistent in out-of-distribution scenarios. These findings raise significant\nconcerns about the reliability of epistemic markers for confidence estimation,\nunderscoring the need for improved alignment between marker based confidence\nand actual model uncertainty. Our code is available at\nhttps://github.com/HKUST-KnowComp/MarCon."}
{"id": "2505.24788", "pdf": "https://arxiv.org/pdf/2505.24788.pdf", "abs": "https://arxiv.org/abs/2505.24788", "title": "Drop Dropout on Single-Epoch Language Model Pretraining", "authors": ["Houjun Liu", "John Bauer", "Christopher D. Manning"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL Findings; 5 pages, 2 figures, 4 pages of appendix", "summary": "Originally, dropout was seen as a breakthrough regularization technique that\nreduced overfitting and improved performance in almost all applications of deep\nlearning by reducing overfitting. Yet, single-epoch pretraining tasks common to\nmodern LLMs yield minimal overfitting, leading to dropout not being used for\nlarge LLMs. Nevertheless, no thorough empirical investigation has been done on\nthe role of dropout in LM pretraining. Through experiments in single-epoch\npretraining of both masked (BERT) and autoregressive (Pythia 160M and 1.4B) LMs\nwith varying levels of dropout, we find that downstream performance in language\nmodeling, morpho-syntax (BLiMP), question answering (SQuAD), and\nnatural-language inference (MNLI) improves when dropout is not applied during\npretraining. We additionally find that the recently-introduced \"early dropout\"\nalso degrades performance over applying no dropout at all. We further\ninvestigate the models' editability, and find that models trained without\ndropout are more successful in gradient-based model editing (MEND) and\nequivalent in representation-based model editing (ReFT). Therefore, we advocate\nto drop dropout during single-epoch pretraining."}
{"id": "2505.24803", "pdf": "https://arxiv.org/pdf/2505.24803.pdf", "abs": "https://arxiv.org/abs/2505.24803", "title": "Guiding Generative Storytelling with Knowledge Graphs", "authors": ["Zhijun Pan", "Antonios Andronis", "Eva Hayek", "Oscar AP Wilkinson", "Ilya Lasy", "Annette Parry", "Guy Gadney", "Tim J. Smith", "Mick Grierson"], "categories": ["cs.CL", "cs.HC"], "comment": "This manuscript was submitted for peer review in January 2025", "summary": "Large Language Models (LLMs) have shown great potential in automated story\ngeneration, but challenges remain in maintaining long-form coherence and\nproviding users with intuitive and effective control. Retrieval-Augmented\nGeneration (RAG) has proven effective in reducing hallucinations in text\ngeneration; however, the use of structured data to support generative\nstorytelling remains underexplored. This paper investigates how knowledge\ngraphs (KGs) can enhance LLM-based storytelling by improving narrative quality\nand enabling user-driven modifications. We propose a KG-assisted storytelling\npipeline and evaluate its effectiveness through a user study with 15\nparticipants. Participants created their own story prompts, generated stories,\nand edited knowledge graphs to shape their narratives. Through quantitative and\nqualitative analysis, our findings demonstrate that knowledge graphs\nsignificantly enhance story quality in action-oriented and structured\nnarratives within our system settings. Additionally, editing the knowledge\ngraph increases users' sense of control, making storytelling more engaging,\ninteractive, and playful."}
{"id": "2505.24826", "pdf": "https://arxiv.org/pdf/2505.24826.pdf", "abs": "https://arxiv.org/abs/2505.24826", "title": "LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text", "authors": ["Li yunhan", "Wu gengshen"], "categories": ["cs.CL", "cs.CV"], "comment": "10 pages, 11 figures", "summary": "As large language models (LLMs) are increasingly used in legal applications,\ncurrent evaluation benchmarks tend to focus mainly on factual accuracy while\nlargely neglecting important linguistic quality aspects such as clarity,\ncoherence, and terminology. To address this gap, we propose three steps: First,\nwe develop a regression model to evaluate the quality of legal texts based on\nclarity, coherence, and terminology. Second, we create a specialized set of\nlegal questions. Third, we analyze 49 LLMs using this evaluation framework.\n  Our analysis identifies three key findings: First, model quality levels off\nat 14 billion parameters, with only a marginal improvement of $2.7\\%$ noted at\n72 billion parameters. Second, engineering choices such as quantization and\ncontext length have a negligible impact, as indicated by statistical\nsignificance thresholds above 0.016. Third, reasoning models consistently\noutperform base architectures. A significant outcome of our research is the\nrelease of a ranking list and Pareto analysis, which highlight the Qwen3 series\nas the optimal choice for cost-performance tradeoffs. This work not only\nestablishes standardized evaluation protocols for legal LLMs but also uncovers\nfundamental limitations in current training data refinement approaches. Code\nand models are available at: https://github.com/lyxx3rd/LegalEval-Q."}
{"id": "2505.24830", "pdf": "https://arxiv.org/pdf/2505.24830.pdf", "abs": "https://arxiv.org/abs/2505.24830", "title": "Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs", "authors": ["Juraj Vladika", "Annika Domres", "Mai Nguyen", "Rebecca Moser", "Jana Nano", "Felix Busch", "Lisa C. Adams", "Keno K. Bressem", "Denise Bernhardt", "Stephanie E. Combs", "Kai J. Borm", "Florian Matthes", "Jan C. Peeken"], "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 4 figures", "summary": "Large language models (LLMs) exhibit extensive medical knowledge but are\nprone to hallucinations and inaccurate citations, which pose a challenge to\ntheir clinical adoption and regulatory compliance. Current methods, such as\nRetrieval Augmented Generation, partially address these issues by grounding\nanswers in source documents, but hallucinations and low fact-level\nexplainability persist. In this work, we introduce a novel atomic fact-checking\nframework designed to enhance the reliability and explainability of LLMs used\nin medical long-form question answering. This method decomposes LLM-generated\nresponses into discrete, verifiable units called atomic facts, each of which is\nindependently verified against an authoritative knowledge base of medical\nguidelines. This approach enables targeted correction of errors and direct\ntracing to source literature, thereby improving the factual accuracy and\nexplainability of medical Q&A. Extensive evaluation using multi-reader\nassessments by medical experts and an automated open Q&A benchmark demonstrated\nsignificant improvements in factual accuracy and explainability. Our framework\nachieved up to a 40% overall answer improvement and a 50% hallucination\ndetection rate. The ability to trace each atomic fact back to the most relevant\nchunks from the database provides a granular, transparent explanation of the\ngenerated responses, addressing a major gap in current medical AI applications.\nThis work represents a crucial step towards more trustworthy and reliable\nclinical applications of LLMs, addressing key prerequisites for clinical\napplication and fostering greater confidence in AI-assisted healthcare."}
{"id": "2505.24832", "pdf": "https://arxiv.org/pdf/2505.24832.pdf", "abs": "https://arxiv.org/abs/2505.24832", "title": "How much do language models memorize?", "authors": ["John X. Morris", "Chawin Sitawarin", "Chuan Guo", "Narine Kokhlikyan", "G. Edward Suh", "Alexander M. Rush", "Kamalika Chaudhuri", "Saeed Mahloujifar"], "categories": ["cs.CL"], "comment": null, "summary": "We propose a new method for estimating how much a model ``knows'' about a\ndatapoint and use it to measure the capacity of modern language models. Prior\nstudies of language model memorization have struggled to disentangle\nmemorization from generalization. We formally separate memorization into two\ncomponents: \\textit{unintended memorization}, the information a model contains\nabout a specific dataset, and \\textit{generalization}, the information a model\ncontains about the true data-generation process. When we completely eliminate\ngeneralization, we can compute the total memorization, which provides an\nestimate of model capacity: our measurements estimate that GPT-style models\nhave a capacity of approximately 3.6 bits per parameter. We train language\nmodels on datasets of increasing size and observe that models memorize until\ntheir capacity fills, at which point ``grokking'' begins, and unintended\nmemorization decreases as models begin to generalize. We train hundreds of\ntransformer language models ranging from $500K$ to $1.5B$ parameters and\nproduce a series of scaling laws relating model capacity and data size to\nmembership inference."}
{"id": "2505.24834", "pdf": "https://arxiv.org/pdf/2505.24834.pdf", "abs": "https://arxiv.org/abs/2505.24834", "title": "Multilinguality Does not Make Sense: Investigating Factors Behind Zero-Shot Transfer in Sense-Aware Tasks", "authors": ["Roksana Goworek", "Haim Dubossarsky"], "categories": ["cs.CL"], "comment": "8 pages, 8 figures", "summary": "Cross-lingual transfer allows models to perform tasks in languages unseen\nduring training and is often assumed to benefit from increased multilinguality.\nIn this work, we challenge this assumption in the context of two underexplored,\nsense-aware tasks: polysemy disambiguation and lexical semantic change. Through\na large-scale analysis across 28 languages, we show that multilingual training\nis neither necessary nor inherently beneficial for effective transfer. Instead,\nwe find that confounding factors - such as fine-tuning data composition and\nevaluation artifacts - better account for the perceived advantages of\nmultilinguality. Our findings call for more rigorous evaluations in\nmultilingual NLP. We release fine-tuned models and benchmarks to support\nfurther research, with implications extending to low-resource and typologically\ndiverse languages."}
{"id": "2505.24858", "pdf": "https://arxiv.org/pdf/2505.24858.pdf", "abs": "https://arxiv.org/abs/2505.24858", "title": "MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs", "authors": ["Gabrielle Kaili-May Liu", "Gal Yona", "Avi Caciularu", "Idan Szpektor", "Tim G. J. Rudner", "Arman Cohan"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "A critical component in the trustworthiness of LLMs is reliable uncertainty\ncommunication, yet LLMs often use assertive language when conveying false\nclaims, leading to over-reliance and eroded trust. We present the first\nsystematic study of $\\textit{faithful confidence calibration}$ of LLMs,\nbenchmarking models' ability to use linguistic expressions of uncertainty that\n$\\textit{faithfully reflect}$ their intrinsic uncertainty, across a\ncomprehensive array of models, datasets, and prompting strategies. Our results\ndemonstrate that LLMs largely fail at this task, and that existing\ninterventions are insufficient: standard prompt approaches provide only\nmarginal gains, and existing, factuality-based calibration techniques can even\nharm faithful calibration. To address this critical gap, we introduce\nMetaFaith, a novel prompt-based calibration approach inspired by human\nmetacognition. We show that MetaFaith robustly improves faithful calibration\nacross diverse models and task domains, enabling up to 61% improvement in\nfaithfulness and achieving an 83% win rate over original generations as judged\nby humans."}
{"id": "2505.24863", "pdf": "https://arxiv.org/pdf/2505.24863.pdf", "abs": "https://arxiv.org/abs/2505.24863", "title": "AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time", "authors": ["Junyu Zhang", "Runpei Dong", "Han Wang", "Xuying Ning", "Haoran Geng", "Peihao Li", "Xialin He", "Yutong Bai", "Jitendra Malik", "Saurabh Gupta", "Huan Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents AlphaOne ($\\alpha$1), a universal framework for\nmodulating reasoning progress in large reasoning models (LRMs) at test time.\n$\\alpha$1 first introduces $\\alpha$ moment, which represents the scaled\nthinking phase with a universal parameter $\\alpha$. Within this scaled\npre-$\\alpha$ moment phase, it dynamically schedules slow thinking transitions\nby modeling the insertion of reasoning transition tokens as a Bernoulli\nstochastic process. After the $\\alpha$ moment, $\\alpha$1 deterministically\nterminates slow thinking with the end-of-thinking token, thereby fostering fast\nreasoning and efficient answer generation. This approach unifies and\ngeneralizes existing monotonic scaling methods by enabling flexible and dense\nslow-to-fast reasoning modulation. Extensive empirical studies on various\nchallenging benchmarks across mathematical, coding, and scientific domains\ndemonstrate $\\alpha$1's superior reasoning capability and efficiency. Project\npage: https://alphaone-project.github.io/"}
{"id": "2505.24864", "pdf": "https://arxiv.org/pdf/2505.24864.pdf", "abs": "https://arxiv.org/abs/2505.24864", "title": "ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models", "authors": ["Mingjie Liu", "Shizhe Diao", "Ximing Lu", "Jian Hu", "Xin Dong", "Yejin Choi", "Jan Kautz", "Yi Dong"], "categories": ["cs.CL", "cs.AI"], "comment": "26 pages, 17 figures", "summary": "Recent advances in reasoning-centric language models have highlighted\nreinforcement learning (RL) as a promising method for aligning models with\nverifiable rewards. However, it remains contentious whether RL truly expands a\nmodel's reasoning capabilities or merely amplifies high-reward outputs already\nlatent in the base model's distribution, and whether continually scaling up RL\ncompute reliably leads to improved reasoning performance. In this work, we\nchallenge prevailing assumptions by demonstrating that prolonged RL (ProRL)\ntraining can uncover novel reasoning strategies that are inaccessible to base\nmodels, even under extensive sampling. We introduce ProRL, a novel training\nmethodology that incorporates KL divergence control, reference policy\nresetting, and a diverse suite of tasks. Our empirical analysis reveals that\nRL-trained models consistently outperform base models across a wide range of\npass@k evaluations, including scenarios where base models fail entirely\nregardless of the number of attempts. We further show that reasoning boundary\nimprovements correlates strongly with task competence of base model and\ntraining duration, suggesting that RL can explore and populate new regions of\nsolution space over time. These findings offer new insights into the conditions\nunder which RL meaningfully expands reasoning boundaries in language models and\nestablish a foundation for future work on long-horizon RL for reasoning. We\nrelease model weights to support further research:\nhttps://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B"}
{"id": "2505.23783", "pdf": "https://arxiv.org/pdf/2505.23783.pdf", "abs": "https://arxiv.org/abs/2505.23783", "title": "Boosting In-Context Learning in LLMs Through the Lens of Classical Supervised Learning", "authors": ["Korel Gundem", "Juncheng Dong", "Dennis Zhang", "Vahid Tarokh", "Zhengling Qi"], "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "In-Context Learning (ICL) allows Large Language Models (LLMs) to adapt to new\ntasks with just a few examples, but their predictions often suffer from\nsystematic biases, leading to unstable performances in classification. While\ncalibration techniques are proposed to mitigate these biases, we show that, in\nthe logit space, many of these methods are equivalent to merely shifting the\nLLM's decision boundary without having the ability to alter its orientation.\nThis proves inadequate when biases cause the LLM to be severely misdirected. To\naddress these limitations and provide a unifying framework, we propose\nSupervised Calibration (SC), a loss-minimization based framework which learns\nan optimal, per-class affine transformation of the LLM's predictive\nprobabilities in the logit space without requiring external data beyond the\ncontext. By using a more expressive functional class, SC not only subsumes many\nexisting calibration methods in ICL as special cases, but also enables the\nability to alter and even completely reverse the orientation of the LLM's\ndecision boundary. Furthermore, SC's loss-based nature facilitates the seamless\nintegration of two purpose-built regularization techniques: context-invariance\nand directional trust-region. The former is designed to tackle the instability\nissue in ICL, while the latter controls the degree of calibration. Finally, SC\ndelivers state-of-the-art performance over calibration baselines in the 4-shot,\n8-shot, and 16-shot settings across all nine datasets for\nMistral-7B-Instruct-v0.3, LLaMA-2-7B-chat, and Qwen2-7B-Instruct."}
{"id": "2505.23841", "pdf": "https://arxiv.org/pdf/2505.23841.pdf", "abs": "https://arxiv.org/abs/2505.23841", "title": "SkewRoute: Training-Free LLM Routing for Knowledge Graph Retrieval-Augmented Generation via Score Skewness of Retrieved Context", "authors": ["Hairu Wang", "Yuan Feng", "Yukun Cao", "Xike Xie", "S Kevin Zhou"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Large language models excel at many tasks but often incur high inference\ncosts during deployment. To mitigate hallucination, many systems use a\nknowledge graph to enhance retrieval-augmented generation (KG-RAG). However,\nthe large amount of retrieved knowledge contexts increase these inference costs\nfurther. A promising solution to balance performance and cost is LLM routing,\nwhich directs simple queries to smaller LLMs and complex ones to larger LLMs.\nHowever, no dedicated routing methods currently exist for RAG, and existing\ntraining-based routers face challenges scaling to this domain due to the need\nfor extensive training data. We observe that the score distributions produced\nby the retrieval scorer strongly correlate with query difficulty. Based on\nthis, we propose a novel, training-free routing framework, the first tailored\nto KG-RAG that effectively balances performance and cost in a plug-and-play\nmanner. Experiments show our method reduces calls to larger LLMs by up to 50%\nwithout sacrificing response quality, demonstrating its potential for efficient\nand scalable LLM deployment."}
{"id": "2505.23881", "pdf": "https://arxiv.org/pdf/2505.23881.pdf", "abs": "https://arxiv.org/abs/2505.23881", "title": "Using Reasoning Models to Generate Search Heuristics that Solve Open Instances of Combinatorial Design Problems", "authors": ["Christopher D. Rosin"], "categories": ["cs.AI", "cs.CL", "math.CO"], "comment": "arXiv admin note: text overlap with arXiv:2501.17725", "summary": "Large Language Models (LLMs) with reasoning are trained to iteratively\ngenerate and refine their answers before finalizing them, which can help with\napplications to mathematics and code generation. We apply code generation with\nreasoning LLMs to a specific task in the mathematical field of combinatorial\ndesign. This field studies diverse types of combinatorial designs, many of\nwhich have lists of open instances for which existence has not yet been\ndetermined. The Constructive Protocol CPro1 uses LLMs to generate search\nheuristics that have the potential to construct solutions to small open\ninstances. Starting with a textual definition and a validity verifier for a\nparticular type of design, CPro1 guides LLMs to select and implement\nstrategies, while providing automated hyperparameter tuning and execution\nfeedback. CPro1 with reasoning LLMs successfully solves long-standing open\ninstances for 7 of 16 combinatorial design problems selected from the 2006\nHandbook of Combinatorial Designs, including new solved instances for 3 of\nthese (Bhaskar Rao Designs, Symmetric Weighing Matrices, Balanced Ternary\nDesigns) that were unsolved by CPro1 with non-reasoning LLMs. It also solves\nopen instances for several problems from recent (2025) literature, generating\nnew Covering Sequences, Johnson Clique Covers, Deletion Codes, and a Uniform\nNested Steiner Quadruple System."}
{"id": "2505.23883", "pdf": "https://arxiv.org/pdf/2505.23883.pdf", "abs": "https://arxiv.org/abs/2505.23883", "title": "BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive Learning", "authors": ["Jianyang Gu", "Samuel Stevens", "Elizabeth G Campolongo", "Matthew J Thompson", "Net Zhang", "Jiaman Wu", "Andrei Kopanev", "Zheda Mai", "Alexander E. White", "James Balhoff", "Wasila Dahdul", "Daniel Rubenstein", "Hilmar Lapp", "Tanya Berger-Wolf", "Wei-Lun Chao", "Yu Su"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Project page: https://imageomics.github.io/bioclip-2/", "summary": "Foundation models trained at scale exhibit remarkable emergent behaviors,\nlearning new capabilities beyond their initial training objectives. We find\nsuch emergent behaviors in biological vision models via large-scale contrastive\nvision-language training. To achieve this, we first curate TreeOfLife-200M,\ncomprising 214 million images of living organisms, the largest and most diverse\nbiological organism image dataset to date. We then train BioCLIP 2 on\nTreeOfLife-200M to distinguish different species. Despite the narrow training\nobjective, BioCLIP 2 yields extraordinary accuracy when applied to various\nbiological visual tasks such as habitat classification and trait prediction. We\nidentify emergent properties in the learned embedding space of BioCLIP 2. At\nthe inter-species level, the embedding distribution of different species aligns\nclosely with functional and ecological meanings (e.g., beak sizes and\nhabitats). At the intra-species level, instead of being diminished, the\nintra-species variations (e.g., life stages and sexes) are preserved and better\nseparated in subspaces orthogonal to inter-species distinctions. We provide\nformal proof and analyses to explain why hierarchical supervision and\ncontrastive objectives encourage these emergent properties. Crucially, our\nresults reveal that these properties become increasingly significant with\nlarger-scale training data, leading to a biologically meaningful embedding\nspace."}
{"id": "2505.23884", "pdf": "https://arxiv.org/pdf/2505.23884.pdf", "abs": "https://arxiv.org/abs/2505.23884", "title": "Test-Time Training Done Right", "authors": ["Tianyuan Zhang", "Sai Bi", "Yicong Hong", "Kai Zhang", "Fujun Luan", "Songlin Yang", "Kalyan Sunkavalli", "William T. Freeman", "Hao Tan"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "32 pages, 11 figures", "summary": "Test-Time Training (TTT) models context dependencies by adapting part of the\nmodel's weights (referred to as fast weights) during inference. This fast\nweight, akin to recurrent states in RNNs, stores temporary memories of past\ntokens in the current sequence. Existing TTT methods struggled to show\neffectiveness in handling long-context data, due to their inefficiency on\nmodern GPUs. The TTT layers in many of these approaches operate with extremely\nlow FLOPs utilization (often <5%) because they deliberately apply small online\nminibatch sizes (e.g., updating fast weights every 16 or 64 tokens). Moreover,\na small minibatch implies fine-grained block-wise causal dependencies in the\ndata, unsuitable for data beyond 1D ordered sequences, like sets or\nN-dimensional grids such as images or videos. In contrast, we pursue the\nopposite direction by using an extremely large chunk update, ranging from 2K to\n1M tokens across tasks of varying modalities, which we refer to as Large Chunk\nTest-Time Training (LaCT). It improves hardware utilization by orders of\nmagnitude, and more importantly, facilitates scaling of nonlinear state size\n(up to 40% of model parameters), hence substantially improving state capacity,\nall without requiring cumbersome and error-prone kernel implementations. It\nalso allows easy integration of sophisticated optimizers, e.g. Muon for online\nupdates. We validate our approach across diverse modalities and tasks,\nincluding novel view synthesis with image set, language models, and\nauto-regressive video diffusion. Our approach can scale up to 14B-parameter AR\nvideo diffusion model on sequences up to 56K tokens. In our longest sequence\nexperiment, we perform novel view synthesis with 1 million context length. We\nhope this work will inspire and accelerate new research in the field of\nlong-context modeling and test-time training. Website:\nhttps://tianyuanzhang.com/projects/ttt-done-right"}
{"id": "2505.23885", "pdf": "https://arxiv.org/pdf/2505.23885.pdf", "abs": "https://arxiv.org/abs/2505.23885", "title": "OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation", "authors": ["Mengkang Hu", "Yuhang Zhou", "Wendong Fan", "Yuzhou Nie", "Bowei Xia", "Tao Sun", "Ziyu Ye", "Zhaoxuan Jin", "Yingru Li", "Qiguang Chen", "Zeyu Zhang", "Yifeng Wang", "Qianshuo Ye", "Bernard Ghanem", "Ping Luo", "Guohao Li"], "categories": ["cs.AI", "cs.CL"], "comment": "Project Page: https://github.com/camel-ai/owl", "summary": "Large Language Model (LLM)-based multi-agent systems show promise for\nautomating real-world tasks but struggle to transfer across domains due to\ntheir domain-specific nature. Current approaches face two critical\nshortcomings: they require complete architectural redesign and full retraining\nof all components when applied to new domains. We introduce Workforce, a\nhierarchical multi-agent framework that decouples strategic planning from\nspecialized execution through a modular architecture comprising: (i) a\ndomain-agnostic Planner for task decomposition, (ii) a Coordinator for subtask\nmanagement, and (iii) specialized Workers with domain-specific tool-calling\ncapabilities. This decoupling enables cross-domain transferability during both\ninference and training phases: During inference, Workforce seamlessly adapts to\nnew domains by adding or modifying worker agents; For training, we introduce\nOptimized Workforce Learning (OWL), which improves generalization across\ndomains by optimizing a domain-agnostic planner with reinforcement learning\nfrom real-world feedback. To validate our approach, we evaluate Workforce on\nthe GAIA benchmark, covering various realistic, multi-domain agentic tasks.\nExperimental results demonstrate Workforce achieves open-source\nstate-of-the-art performance (69.70%), outperforming commercial systems like\nOpenAI's Deep Research by 2.34%. More notably, our OWL-trained 32B model\nachieves 52.73% accuracy (+16.37%) and demonstrates performance comparable to\nGPT-4o on challenging tasks. To summarize, by enabling scalable generalization\nand modular domain transfer, our work establishes a foundation for the next\ngeneration of general-purpose AI assistants."}
{"id": "2505.23922", "pdf": "https://arxiv.org/pdf/2505.23922.pdf", "abs": "https://arxiv.org/abs/2505.23922", "title": "ScaleLong: A Multi-Timescale Benchmark for Long Video Understanding", "authors": ["David Ma", "Huaqing Yuan", "Xingjian Wang", "Qianbo Zang", "Tianci Liu", "Xinyang He", "Yanbin Wei", "Jiawei Guo", "Ni Jiahui", "Zhenzhu Yang", "Meng Cao", "Shanghaoran Quan", "Yizhi Li", "Wangchunshu Zhou", "Jiaheng Liu", "Wenhao Huang", "Ge Zhang", "Shiwen Ni", "Xiaojie Jin"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Although long-video understanding demands that models capture hierarchical\ntemporal information -- from clip (seconds) and shot (tens of seconds) to event\n(minutes) and story (hours) -- existing benchmarks either neglect this\nmulti-scale design or scatter scale-specific questions across different videos,\npreventing direct comparison of model performance across timescales on the same\ncontent. To address this, we introduce ScaleLong, the first benchmark to\ndisentangle these factors by embedding questions targeting four hierarchical\ntimescales -- clip (seconds), shot (tens of seconds), event (minutes), and\nstory (hours) -- all within the same video content. This within-content\nmulti-timescale questioning design enables direct comparison of model\nperformance across timescales on identical videos. ScaleLong features 269 long\nvideos (avg.\\ 86\\,min) from 5 main categories and 36 sub-categories, with 4--8\ncarefully designed questions, including at least one question for each\ntimescale. Evaluating 23 MLLMs reveals a U-shaped performance curve, with\nhigher accuracy at the shortest and longest timescales and a dip at\nintermediate levels. Furthermore, ablation studies show that increased visual\ntoken capacity consistently enhances reasoning across all timescales. ScaleLong\noffers a fine-grained, multi-timescale benchmark for advancing MLLM\ncapabilities in long-video understanding. The code and dataset are available\nhttps://github.com/multimodal-art-projection/ScaleLong."}
{"id": "2505.23960", "pdf": "https://arxiv.org/pdf/2505.23960.pdf", "abs": "https://arxiv.org/abs/2505.23960", "title": "Information Structure in Mappings: An Approach to Learning, Representation, and Generalisation", "authors": ["Henry Conklin"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "PhD Thesis, 204 pages; entropy estimation discussed from p.94", "summary": "Despite the remarkable success of large large-scale neural networks, we still\nlack unified notation for thinking about and describing their representational\nspaces. We lack methods to reliably describe how their representations are\nstructured, how that structure emerges over training, and what kinds of\nstructures are desirable. This thesis introduces quantitative methods for\nidentifying systematic structure in a mapping between spaces, and leverages\nthem to understand how deep-learning models learn to represent information,\nwhat representational structures drive generalisation, and how design decisions\ncondition the structures that emerge. To do this I identify structural\nprimitives present in a mapping, along with information theoretic\nquantifications of each. These allow us to analyse learning, structure, and\ngeneralisation across multi-agent reinforcement learning models,\nsequence-to-sequence models trained on a single task, and Large Language\nModels. I also introduce a novel, performant, approach to estimating the\nentropy of vector space, that allows this analysis to be applied to models\nranging in size from 1 million to 12 billion parameters.\n  The experiments here work to shed light on how large-scale distributed models\nof cognition learn, while allowing us to draw parallels between those systems\nand their human analogs. They show how the structures of language and the\nconstraints that give rise to them in many ways parallel the kinds of\nstructures that drive performance of contemporary neural networks."}
{"id": "2505.23987", "pdf": "https://arxiv.org/pdf/2505.23987.pdf", "abs": "https://arxiv.org/abs/2505.23987", "title": "Large Language Models for Controllable Multi-property Multi-objective Molecule Optimization", "authors": ["Vishal Dey", "Xiao Hu", "Xia Ning"], "categories": ["cs.LG", "cs.AI", "cs.CL", "q-bio.BM"], "comment": null, "summary": "In real-world drug design, molecule optimization requires selectively\nimproving multiple molecular properties up to pharmaceutically relevant levels,\nwhile maintaining others that already meet such criteria. However, existing\ncomputational approaches and instruction-tuned LLMs fail to capture such\nnuanced property-specific objectives, limiting their practical applicability.\nTo address this, we introduce C-MuMOInstruct, the first instruction-tuning\ndataset focused on multi-property optimization with explicit, property-specific\nobjectives. Leveraging C-MuMOInstruct, we develop GeLLMO-Cs, a series of\ninstruction-tuned LLMs that can perform targeted property-specific\noptimization. Our experiments across 5 in-distribution and 5\nout-of-distribution tasks show that GeLLMO-Cs consistently outperform strong\nbaselines, achieving up to 126% higher success rate. Notably, GeLLMO-Cs exhibit\nimpressive 0-shot generalization to novel optimization tasks and unseen\ninstructions. This offers a step toward a foundational LLM to support\nrealistic, diverse optimizations with property-specific objectives.\nC-MuMOInstruct and code are accessible through\nhttps://github.com/ninglab/GeLLMO-C."}
{"id": "2505.24004", "pdf": "https://arxiv.org/pdf/2505.24004.pdf", "abs": "https://arxiv.org/abs/2505.24004", "title": "Redefining Research Crowdsourcing: Incorporating Human Feedback with LLM-Powered Digital Twins", "authors": ["Amanda Chan", "Catherine Di", "Joseph Rupertus", "Gary Smith", "Varun Nagaraj Rao", "Manoel Horta Ribeiro", "Andrés Monroy-Hernández"], "categories": ["cs.HC", "cs.CL", "cs.CY"], "comment": "Accepted as a CHI Late Breaking Work (2025), cite appropriately", "summary": "Crowd work platforms like Amazon Mechanical Turk and Prolific are vital for\nresearch, yet workers' growing use of generative AI tools poses challenges.\nResearchers face compromised data validity as AI responses replace authentic\nhuman behavior, while workers risk diminished roles as AI automates tasks. To\naddress this, we propose a hybrid framework using digital twins, personalized\nAI models that emulate workers' behaviors and preferences while keeping humans\nin the loop. We evaluate our system with an experiment (n=88 crowd workers) and\nin-depth interviews with crowd workers (n=5) and social science researchers\n(n=4). Our results suggest that digital twins may enhance productivity and\nreduce decision fatigue while maintaining response quality. Both researchers\nand workers emphasized the importance of transparency, ethical data use, and\nworker agency. By automating repetitive tasks and preserving human engagement\nfor nuanced ones, digital twins may help balance scalability with authenticity."}
{"id": "2505.24189", "pdf": "https://arxiv.org/pdf/2505.24189.pdf", "abs": "https://arxiv.org/abs/2505.24189", "title": "Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code Workflows", "authors": ["Orlando Marquez Ayala", "Patrice Bechard", "Emily Chen", "Maggie Baird", "Jingfei Chen"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) such as GPT-4o can handle a wide range of\ncomplex tasks with the right prompt. As per token costs are reduced, the\nadvantages of fine-tuning Small Language Models (SLMs) for real-world\napplications -- faster inference, lower costs -- may no longer be clear. In\nthis work, we present evidence that, for domain-specific tasks that require\nstructured outputs, SLMs still have a quality advantage. We compare fine-tuning\nan SLM against prompting LLMs on the task of generating low-code workflows in\nJSON form. We observe that while a good prompt can yield reasonable results,\nfine-tuning improves quality by 10% on average. We also perform systematic\nerror analysis to reveal model limitations."}
{"id": "2505.24195", "pdf": "https://arxiv.org/pdf/2505.24195.pdf", "abs": "https://arxiv.org/abs/2505.24195", "title": "WikiGap: Promoting Epistemic Equity by Surfacing Knowledge Gaps Between English Wikipedia and other Language Editions", "authors": ["Zining Wang", "Yuxuan Zhang", "Dongwook Yoon", "Nicholas Vincent", "Farhan Samir", "Vered Shwartz"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "With more than 11 times as many pageviews as the next, English Wikipedia\ndominates global knowledge access relative to other language editions. Readers\nare prone to assuming English Wikipedia as a superset of all language editions,\nleading many to prefer it even when their primary language is not English.\nOther language editions, however, comprise complementary facts rooted in their\nrespective cultures and media environments, which are marginalized in English\nWikipedia. While Wikipedia's user interface enables switching between language\neditions through its Interlanguage Link (ILL) system, it does not reveal to\nreaders that other language editions contain valuable, complementary\ninformation. We present WikiGap, a system that surfaces complementary facts\nsourced from other Wikipedias within the English Wikipedia interface.\nSpecifically, by combining a recent multilingual information-gap discovery\nmethod with a user-centered design, WikiGap enables access to complementary\ninformation from French, Russian, and Chinese Wikipedia. In a mixed-methods\nstudy (n=21), WikiGap significantly improved fact-finding accuracy, reduced\ntask time, and received a 32-point higher usability score relative to\nWikipedia's current ILL-based navigation system. Participants reported\nincreased awareness of the availability of complementary information in\nnon-English editions and reconsidered the completeness of English Wikipedia.\nWikiGap thus paves the way for improved epistemic equity across language\neditions."}
{"id": "2505.24200", "pdf": "https://arxiv.org/pdf/2505.24200.pdf", "abs": "https://arxiv.org/abs/2505.24200", "title": "Improving Multilingual Speech Models on ML-SUPERB 2.0: Fine-tuning with Data Augmentation and LID-Aware CTC", "authors": ["Qingzheng Wang", "Jiancheng Sun", "Yifan Peng", "Shinji Watanabe"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Multilingual speech processing with self-supervised or supervised pre-trained\nSpeech Foundation Models (SFM) has achieved strong performance on tasks like\nLanguage Identification (LID) and Automatic Speech Recognition (ASR). However,\nthese models struggle with limited resources during fine-tuning. This paper\nenhances multilingual LID and ASR on ML-SUPERB 2.0 by exploring multiple\nstrategies for adapting SFMs, including frozen upstream training, partial\nfine-tuning, and low-rank adaptation. Furthermore, we employ data augmentation\nto mitigate performance gaps in few-shot settings and introduce LID\nConnectionist Temporal Classification (CTC) loss for regularization. Our\napproach achieves a 14% relative improvement in LID accuracy and a 30% relative\nreduction in ASR CER over the baseline on ML-SUPERB 2.0, securing second place\nin the Interspeech 2025 ML-SUPERB 2.0 Challenge."}
{"id": "2505.24225", "pdf": "https://arxiv.org/pdf/2505.24225.pdf", "abs": "https://arxiv.org/abs/2505.24225", "title": "Reasoning Can Hurt the Inductive Abilities of Large Language Models", "authors": ["Haibo Jin", "Peiyan Zhang", "Man Luo", "Haohan Wang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "26 pages", "summary": "Large Language Models (LLMs) have shown remarkable progress across domains,\nyet their ability to perform inductive reasoning - inferring latent rules from\nsparse examples - remains limited. It is often assumed that chain-of-thought\n(CoT) prompting, as used in Large Reasoning Models (LRMs), enhances such\nreasoning. We investigate this assumption with creating four controlled,\ndiagnostic game-based tasks - chess, Texas Hold'em, dice games, and blackjack -\nwith hidden human-defined rules. We find that CoT reasoning can degrade\ninductive performance, with LRMs often underperforming their non-reasoning\ncounterparts.\n  To explain this, we present a theoretical framework that reveals how\nreasoning steps can amplify error through three failure modes: incorrect\nsub-task decomposition, incorrect sub-task solving, and incorrect final answer\nsummarization. Based on our theoretical and empirical analysis, we introduce\nstructured interventions that adapt CoT generation according to our identified\nfailure types. These interventions improve inductive accuracy without\nretraining. Our findings suggest that effective (CoT) reasoning depends not\nonly on taking more steps but also on ensuring those steps are well-structured."}
{"id": "2505.24232", "pdf": "https://arxiv.org/pdf/2505.24232.pdf", "abs": "https://arxiv.org/abs/2505.24232", "title": "From Hallucinations to Jailbreaks: Rethinking the Vulnerability of Large Foundation Models", "authors": ["Haibo Jin", "Peiyan Zhang", "Peiran Wang", "Man Luo", "Haohan Wang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Large foundation models (LFMs) are susceptible to two distinct\nvulnerabilities: hallucinations and jailbreak attacks. While typically studied\nin isolation, we observe that defenses targeting one often affect the other,\nhinting at a deeper connection.\n  We propose a unified theoretical framework that models jailbreaks as\ntoken-level optimization and hallucinations as attention-level optimization.\nWithin this framework, we establish two key propositions: (1) \\textit{Similar\nLoss Convergence} - the loss functions for both vulnerabilities converge\nsimilarly when optimizing for target-specific outputs; and (2) \\textit{Gradient\nConsistency in Attention Redistribution} - both exhibit consistent gradient\nbehavior driven by shared attention dynamics.\n  We validate these propositions empirically on LLaVA-1.5 and MiniGPT-4,\nshowing consistent optimization trends and aligned gradients. Leveraging this\nconnection, we demonstrate that mitigation techniques for hallucinations can\nreduce jailbreak success rates, and vice versa. Our findings reveal a shared\nfailure mode in LFMs and suggest that robustness strategies should jointly\naddress both vulnerabilities."}
{"id": "2505.24239", "pdf": "https://arxiv.org/pdf/2505.24239.pdf", "abs": "https://arxiv.org/abs/2505.24239", "title": "An Adversary-Resistant Multi-Agent LLM System via Credibility Scoring", "authors": ["Sana Ebrahimi", "Mohsen Dehghankar", "Abolfazl Asudeh"], "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "While multi-agent LLM systems show strong capabilities in various domains,\nthey are highly vulnerable to adversarial and low-performing agents. To resolve\nthis issue, in this paper, we introduce a general and adversary-resistant\nmulti-agent LLM framework based on credibility scoring. We model the\ncollaborative query-answering process as an iterative game, where the agents\ncommunicate and contribute to a final system output. Our system associates a\ncredibility score that is used when aggregating the team outputs. The\ncredibility scores are learned gradually based on the past contributions of\neach agent in query answering. Our experiments across multiple tasks and\nsettings demonstrate our system's effectiveness in mitigating adversarial\ninfluence and enhancing the resilience of multi-agent cooperation, even in the\nadversary-majority settings."}
{"id": "2505.24292", "pdf": "https://arxiv.org/pdf/2505.24292.pdf", "abs": "https://arxiv.org/abs/2505.24292", "title": "Mind the Quote: Enabling Quotation-Aware Dialogue in LLMs via Plug-and-Play Modules", "authors": ["Yueqi Zhang", "Peiwen Yuan", "Shaoxiong Feng", "Yiwei Li", "Xinglin Wang", "Jiayi Shi", "Chuyi Tan", "Boyuan Pan", "Yao Hu", "Kan Li"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Human-AI conversation frequently relies on quoting earlier text-\"check it\nwith the formula I just highlighted\"-yet today's large language models (LLMs)\nlack an explicit mechanism for locating and exploiting such spans. We formalise\nthe challenge as span-conditioned generation, decomposing each turn into the\ndialogue history, a set of token-offset quotation spans, and an intent\nutterance. Building on this abstraction, we introduce a quotation-centric data\npipeline that automatically synthesises task-specific dialogues, verifies\nanswer correctness through multi-stage consistency checks, and yields both a\nheterogeneous training corpus and the first benchmark covering five\nrepresentative scenarios. To meet the benchmark's zero-overhead and\nparameter-efficiency requirements, we propose QuAda, a lightweight\ntraining-based method that attaches two bottleneck projections to every\nattention head, dynamically amplifying or suppressing attention to quoted spans\nat inference time while leaving the prompt unchanged and updating < 2.8% of\nbackbone weights. Experiments across models show that QuAda is suitable for all\nscenarios and generalises to unseen topics, offering an effective,\nplug-and-play solution for quotation-aware dialogue."}
{"id": "2505.24293", "pdf": "https://arxiv.org/pdf/2505.24293.pdf", "abs": "https://arxiv.org/abs/2505.24293", "title": "Large Language Models are Locally Linear Mappings", "authors": ["James R. Golden"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Version 0", "summary": "We demonstrate that the inference operations of several open-weight large\nlanguage models (LLMs) can be mapped to an exactly equivalent linear system for\nan input sequence without modifying the model weights or altering output\npredictions. Extending techniques from image diffusion models that exhibit\nlocal or piecewise linearity, we strategically alter the gradient computation\nwith respect to a given input sequence for a next-token prediction such that\nthe Jacobian of the model nearly exactly reproduces the forward prediction with\na linear system. We demonstrate this approach across models (Llama 3, Gemma 3,\nQwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show\nthrough the singular value decomposition of the detached Jacobian that these\nLLMs operate in extremely low-dimensional subspaces where many of the largest\nsingular vectors decode to concepts related to the most-likely output token.\nThis approach also allows us to examine the operation of each successive layer\n(and its attention and MLP components) as nearly-exact linear systems and\nobserve the emergence of semantic concepts. Despite their expressive power and\nglobal nonlinearity, modern LLMs can be interpreted through nearly-exact\nlocally linear decompositions that provide insights into their internal\nrepresentations and reveal interpretable semantic structures in the next-token\nprediction process."}
{"id": "2505.24324", "pdf": "https://arxiv.org/pdf/2505.24324.pdf", "abs": "https://arxiv.org/abs/2505.24324", "title": "SwiftEval: Developing a Language-Specific Benchmark for LLM-generated Code Evaluation", "authors": ["Ivan Petrukha", "Yana Kurliak", "Nataliia Stulova"], "categories": ["cs.LG", "cs.CL", "cs.PL", "cs.SE"], "comment": "Accepted to FORGE'25 Benchmarking on 15.01.2025, to be published by\n  IEEE under the CC BY-NC-ND 4.0 license. This is the accepted version of the\n  article (5 pages, 2 figures, 1 table). DOI will be added upon publication", "summary": "In recent years, large language models (LLMs) have showcased significant\nadvancements in code generation. However, most evaluation benchmarks are\nprimarily oriented towards Python, making it difficult to evaluate other\nprogramming languages, such as Swift, with high quality. By examining widely\nestablished multilingual benchmarks like HumanEval-XL and MultiPL-E, we\nidentified critical issues specific to their Swift components, making them\ninsufficient or even irrelevant for assessing LLM coding capabilities on Swift.\nUnlike these existing approaches, which prioritize rapid scaling and\ngeneralization by automatically translating Python-centric benchmarks with\nLLMs, we adopt a quality-over-quantity methodology. We present SwiftEval, the\nfirst Swift-oriented benchmark consisting of 28 carefully hand-crafted\nproblems, and evaluate 44 popular Code LLMs on it. Our results show significant\nLLM scores drop for problems requiring language-specific features, most\nnoticeable in the models of smaller sizes."}
{"id": "2505.24340", "pdf": "https://arxiv.org/pdf/2505.24340.pdf", "abs": "https://arxiv.org/abs/2505.24340", "title": "GeoVision Labeler: Zero-Shot Geospatial Classification with Vision and Language Models", "authors": ["Gilles Quentin Hacheme", "Girmaw Abebe Tadesse", "Caleb Robinson", "Akram Zaytar", "Rahul Dodhia", "Juan M. Lavista Ferres"], "categories": ["cs.CV", "cs.CL", "cs.LG", "I.2.10; I.2.7; I.4.8; I.5.3"], "comment": null, "summary": "Classifying geospatial imagery remains a major bottleneck for applications\nsuch as disaster response and land-use monitoring-particularly in regions where\nannotated data is scarce or unavailable. Existing tools (e.g., RS-CLIP) that\nclaim zero-shot classification capabilities for satellite imagery nonetheless\nrely on task-specific pretraining and adaptation to reach competitive\nperformance. We introduce GeoVision Labeler (GVL), a strictly zero-shot\nclassification framework: a vision Large Language Model (vLLM) generates rich,\nhuman-readable image descriptions, which are then mapped to user-defined\nclasses by a conventional Large Language Model (LLM). This modular, and\ninterpretable pipeline enables flexible image classification for a large range\nof use cases. We evaluated GVL across three benchmarks-SpaceNet v7, UC Merced,\nand RESISC45. It achieves up to 93.2% zero-shot accuracy on the binary\nBuildings vs. No Buildings task on SpaceNet v7. For complex multi-class\nclassification tasks (UC Merced, RESISC45), we implemented a recursive\nLLM-driven clustering to form meta-classes at successive depths, followed by\nhierarchical classification-first resolving coarse groups, then finer\ndistinctions-to deliver competitive zero-shot performance. GVL is open-sourced\nat https://github.com/microsoft/geo-vision-labeler to catalyze adoption in\nreal-world geospatial workflows."}
{"id": "2505.24342", "pdf": "https://arxiv.org/pdf/2505.24342.pdf", "abs": "https://arxiv.org/abs/2505.24342", "title": "KEVER^2: Knowledge-Enhanced Visual Emotion Reasoning and Retrieval", "authors": ["Fanhang Man", "Xiaoyue Chen", "Huandong Wang", "Baining Zhao", "Han Li", "Xinlei Chen", "Yong Li"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Understanding what emotions images evoke in their viewers is a foundational\ngoal in human-centric visual computing. While recent advances in\nvision-language models (VLMs) have shown promise for visual emotion analysis\n(VEA), several key challenges remain unresolved. Emotional cues in images are\noften abstract, overlapping, and entangled, making them difficult to model and\ninterpret. Moreover, VLMs struggle to align these complex visual patterns with\nemotional semantics due to limited supervision and sparse emotional grounding.\nFinally, existing approaches lack structured affective knowledge to resolve\nambiguity and ensure consistent emotional reasoning across diverse visual\ndomains.\n  To address these limitations, we propose \\textbf{K-EVER\\textsuperscript{2}},\na knowledge-enhanced framework for emotion reasoning and retrieval. Our\napproach introduces a semantically structured formulation of visual emotion\ncues and integrates external affective knowledge through multimodal alignment.\nWithout relying on handcrafted labels or direct emotion supervision,\nK-EVER\\textsuperscript{2} achieves robust and interpretable emotion predictions\nacross heterogeneous image types.\n  We validate our framework on three representative benchmarks, Emotion6,\nEmoSet, and M-Disaster, covering social media imagery, human-centric scenes,\nand disaster contexts. K-EVER\\textsuperscript{2} consistently outperforms\nstrong CNN and VLM baselines, achieving up to a \\textbf{19\\% accuracy gain} for\nspecific emotions and a \\textbf{12.3\\% average accuracy gain} across all\nemotion categories. Our results demonstrate a scalable and generalizable\nsolution for advancing emotional understanding of visual content."}
{"id": "2505.24379", "pdf": "https://arxiv.org/pdf/2505.24379.pdf", "abs": "https://arxiv.org/abs/2505.24379", "title": "Breaking the Gold Standard: Extracting Forgotten Data under Exact Unlearning in Large Language Models", "authors": ["Xiaoyu Wu", "Yifei Pang", "Terrance Liu", "Zhiwei Steven Wu"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Large language models are typically trained on datasets collected from the\nweb, which may inadvertently contain harmful or sensitive personal information.\nTo address growing privacy concerns, unlearning methods have been proposed to\nremove the influence of specific data from trained models. Of these, exact\nunlearning -- which retrains the model from scratch without the target data --\nis widely regarded the gold standard, believed to be robust against\nprivacy-related attacks. In this paper, we challenge this assumption by\nintroducing a novel data extraction attack that compromises even exact\nunlearning. Our method leverages both the pre- and post-unlearning models: by\nguiding the post-unlearning model using signals from the pre-unlearning model,\nwe uncover patterns that reflect the removed data distribution. Combining model\nguidance with a token filtering strategy, our attack significantly improves\nextraction success rates -- doubling performance in some cases -- across common\nbenchmarks such as MUSE, TOFU, and WMDP. Furthermore, we demonstrate our\nattack's effectiveness on a simulated medical diagnosis dataset to highlight\nreal-world privacy risks associated with exact unlearning. In light of our\nfindings, which suggest that unlearning may, in a contradictory way, increase\nthe risk of privacy leakage, we advocate for evaluation of unlearning methods\nto consider broader threat models that account not only for post-unlearning\nmodels but also for adversarial access to prior checkpoints."}
{"id": "2505.24478", "pdf": "https://arxiv.org/pdf/2505.24478.pdf", "abs": "https://arxiv.org/abs/2505.24478", "title": "Optimizing the Interface Between Knowledge Graphs and LLMs for Complex Reasoning", "authors": ["Vasilije Markovic", "Lazar Obradovic", "Laszlo Hajdu", "Jovan Pavlovic"], "categories": ["cs.AI", "cs.CL"], "comment": "This is a preliminary version. A revised and expanded version is in\n  preparation", "summary": "Integrating Large Language Models (LLMs) with Knowledge Graphs (KGs) results\nin complex systems with numerous hyperparameters that directly affect\nperformance. While such systems are increasingly common in retrieval-augmented\ngeneration, the role of systematic hyperparameter optimization remains\nunderexplored. In this paper, we study this problem in the context of Cognee, a\nmodular framework for end-to-end KG construction and retrieval. Using three\nmulti-hop QA benchmarks (HotPotQA, TwoWikiMultiHop, and MuSiQue) we optimize\nparameters related to chunking, graph construction, retrieval, and prompting.\nEach configuration is scored using established metrics (exact match, F1, and\nDeepEval's LLM-based correctness metric). Our results demonstrate that\nmeaningful gains can be achieved through targeted tuning. While the gains are\nconsistent, they are not uniform, with performance varying across datasets and\nmetrics. This variability highlights both the value of tuning and the\nlimitations of standard evaluation measures. While demonstrating the immediate\npotential of hyperparameter tuning, we argue that future progress will depend\nnot only on architectural advances but also on clearer frameworks for\noptimization and evaluation in complex, modular systems."}
{"id": "2505.24479", "pdf": "https://arxiv.org/pdf/2505.24479.pdf", "abs": "https://arxiv.org/abs/2505.24479", "title": "Leveraging Knowledge Graphs and LLMs for Structured Generation of Misinformation", "authors": ["Sania Nayab", "Marco Simoni", "Giulio Rossolini"], "categories": ["cs.AI", "cs.CL", "cs.SI"], "comment": null, "summary": "The rapid spread of misinformation, further amplified by recent advances in\ngenerative AI, poses significant threats to society, impacting public opinion,\ndemocratic stability, and national security. Understanding and proactively\nassessing these threats requires exploring methodologies that enable structured\nand scalable misinformation generation. In this paper, we propose a novel\napproach that leverages knowledge graphs (KGs) as structured semantic resources\nto systematically generate fake triplets. By analyzing the structural\nproperties of KGs, such as the distance between entities and their predicates,\nwe identify plausibly false relationships. These triplets are then used to\nguide large language models (LLMs) in generating misinformation statements with\nvarying degrees of credibility. By utilizing structured semantic relationships,\nour deterministic approach produces misinformation inherently challenging for\nhumans to detect, drawing exclusively upon publicly available KGs (e.g.,\nWikiGraphs).\n  Additionally, we investigate the effectiveness of LLMs in distinguishing\nbetween genuine and artificially generated misinformation. Our analysis\nhighlights significant limitations in current LLM-based detection methods,\nunderscoring the necessity for enhanced detection strategies and a deeper\nexploration of inherent biases in generative models."}
{"id": "2505.24519", "pdf": "https://arxiv.org/pdf/2505.24519.pdf", "abs": "https://arxiv.org/abs/2505.24519", "title": "AMIA: Automatic Masking and Joint Intention Analysis Makes LVLMs Robust Jailbreak Defenders", "authors": ["Yuqi Zhang", "Yuchun Miao", "Zuchao Li", "Liang Ding"], "categories": ["cs.CV", "cs.CL"], "comment": "11 pages, 7 figures", "summary": "We introduce AMIA, a lightweight, inference-only defense for Large\nVision-Language Models (LVLMs) that (1) Automatically Masks a small set of\ntext-irrelevant image patches to disrupt adversarial perturbations, and (2)\nconducts joint Intention Analysis to uncover and mitigate hidden harmful\nintents before response generation. Without any retraining, AMIA improves\ndefense success rates across diverse LVLMs and jailbreak benchmarks from an\naverage of 52.4% to 81.7%, preserves general utility with only a 2% average\naccuracy drop, and incurs only modest inference overhead. Ablation confirms\nboth masking and intention analysis are essential for a robust safety-utility\ntrade-off."}
{"id": "2505.24535", "pdf": "https://arxiv.org/pdf/2505.24535.pdf", "abs": "https://arxiv.org/abs/2505.24535", "title": "Beyond Linear Steering: Unified Multi-Attribute Control for Language Models", "authors": ["Narmeen Oozeer", "Luke Marks", "Fazl Barez", "Amirali Abdullah"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Controlling multiple behavioral attributes in large language models (LLMs) at\ninference time is a challenging problem due to interference between attributes\nand the limitations of linear steering methods, which assume additive behavior\nin activation space and require per-attribute tuning. We introduce K-Steering,\na unified and flexible approach that trains a single non-linear multi-label\nclassifier on hidden activations and computes intervention directions via\ngradients at inference time. This avoids linearity assumptions, removes the\nneed for storing and tuning separate attribute vectors, and allows dynamic\ncomposition of behaviors without retraining. To evaluate our method, we propose\ntwo new benchmarks, ToneBank and DebateMix, targeting compositional behavioral\ncontrol. Empirical results across 3 model families, validated by both\nactivation-based classifiers and LLM-based judges, demonstrate that K-Steering\noutperforms strong baselines in accurately steering multiple behaviors."}
{"id": "2505.24571", "pdf": "https://arxiv.org/pdf/2505.24571.pdf", "abs": "https://arxiv.org/abs/2505.24571", "title": "Identifying Primary Stress Across Related Languages and Dialects with Transformer-based Speech Encoder Models", "authors": ["Nikola Ljubešić", "Ivan Porupski", "Peter Rupnik"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to InterSpeech2025", "summary": "Automating primary stress identification has been an active research field\ndue to the role of stress in encoding meaning and aiding speech comprehension.\nPrevious studies relied mainly on traditional acoustic features and English\ndatasets. In this paper, we investigate the approach of fine-tuning a\npre-trained transformer model with an audio frame classification head. Our\nexperiments use a new Croatian training dataset, with test sets in Croatian,\nSerbian, the Chakavian dialect, and Slovenian. By comparing an SVM classifier\nusing traditional acoustic features with the fine-tuned speech transformer, we\ndemonstrate the transformer's superiority across the board, achieving\nnear-perfect results for Croatian and Serbian, with a 10-point performance drop\nfor the more distant Chakavian and Slovenian. Finally, we show that only a few\nhundred multi-syllabic training words suffice for strong performance. We\nrelease our datasets and model under permissive licenses."}
{"id": "2505.24710", "pdf": "https://arxiv.org/pdf/2505.24710.pdf", "abs": "https://arxiv.org/abs/2505.24710", "title": "Causal-aware Large Language Models: Enhancing Decision-Making Through Learning, Adapting and Acting", "authors": ["Wei Chen", "Jiahao Zhang", "Haipeng Zhu", "Boyan Xu", "Zhifeng Hao", "Keli Zhang", "Junjian Ye", "Ruichu Cai"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by IJCAI 2025", "summary": "Large language models (LLMs) have shown great potential in decision-making\ndue to the vast amount of knowledge stored within the models. However, these\npre-trained models are prone to lack reasoning abilities and are difficult to\nadapt to new environments, further hindering their application to complex\nreal-world tasks. To address these challenges, inspired by the human cognitive\nprocess, we propose Causal-aware LLMs, which integrate the structural causal\nmodel (SCM) into the decision-making process to model, update, and utilize\nstructured knowledge of the environment in a ``learning-adapting-acting\"\nparadigm. Specifically, in the learning stage, we first utilize an LLM to\nextract the environment-specific causal entities and their causal relations to\ninitialize a structured causal model of the environment. Subsequently,in the\nadapting stage, we update the structured causal model through external feedback\nabout the environment, via an idea of causal intervention. Finally, in the\nacting stage, Causal-aware LLMs exploit structured causal knowledge for more\nefficient policy-making through the reinforcement learning agent. The above\nprocesses are performed iteratively to learn causal knowledge, ultimately\nenabling the causal-aware LLMs to achieve a more accurate understanding of the\nenvironment and make more efficient decisions. Experimental results across 22\ndiverse tasks within the open-world game ``Crafter\" validate the effectiveness\nof our proposed method."}
{"id": "2505.24715", "pdf": "https://arxiv.org/pdf/2505.24715.pdf", "abs": "https://arxiv.org/abs/2505.24715", "title": "CoRet: Improved Retriever for Code Editing", "authors": ["Fabio Fehr", "Prabhu Teja Sivaprasad", "Luca Franceschi", "Giovanni Zappella"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ACL 2025", "summary": "In this paper, we introduce CoRet, a dense retrieval model designed for\ncode-editing tasks that integrates code semantics, repository structure, and\ncall graph dependencies. The model focuses on retrieving relevant portions of a\ncode repository based on natural language queries such as requests to implement\nnew features or fix bugs. These retrieved code chunks can then be presented to\na user or to a second code-editing model or agent. To train CoRet, we propose a\nloss function explicitly designed for repository-level retrieval. On SWE-bench\nand Long Code Arena's bug localisation datasets, we show that our model\nsubstantially improves retrieval recall by at least 15 percentage points over\nexisting models, and ablate the design choices to show their importance in\nachieving these results."}
{"id": "2505.24736", "pdf": "https://arxiv.org/pdf/2505.24736.pdf", "abs": "https://arxiv.org/abs/2505.24736", "title": "\"Dyadosyncrasy\", Idiosyncrasy and Demographic Factors in Turn-Taking", "authors": ["Julio Cesar Cavalcanti", "Gabriel Skantze"], "categories": ["eess.AS", "cs.CL"], "comment": "Accepted to Interspeech 2025", "summary": "Turn-taking in dialogue follows universal constraints but also varies\nsignificantly. This study examines how demographic (sex, age, education) and\nindividual factors shape turn-taking using a large dataset of US English\nconversations (Fisher). We analyze Transition Floor Offset (TFO) and find\nnotable interspeaker variation. Sex and age have small but significant effects\nfemale speakers and older individuals exhibit slightly shorter offsets - while\neducation shows no effect. Lighter topics correlate with shorter TFOs. However,\nindividual differences have a greater impact, driven by a strong idiosyncratic\nand an even stronger \"dyadosyncratic\" component - speakers in a dyad resemble\neach other more than they resemble themselves in different dyads. This suggests\nthat the dyadic relationship and joint activity are the strongest determinants\nof TFO, outweighing demographic influences."}
{"id": "2505.24749", "pdf": "https://arxiv.org/pdf/2505.24749.pdf", "abs": "https://arxiv.org/abs/2505.24749", "title": "SUMO: Subspace-Aware Moment-Orthogonalization for Accelerating Memory-Efficient LLM Training", "authors": ["Yehonathan Refael", "Guy Smorodinsky", "Tom Tirer", "Ofir Lindenbaum"], "categories": ["cs.LG", "cs.CL", "math.OC"], "comment": null, "summary": "Low-rank gradient-based optimization methods have significantly improved\nmemory efficiency during the training of large language models (LLMs), enabling\noperations within constrained hardware without sacrificing performance.\nHowever, these methods primarily emphasize memory savings, often overlooking\npotential acceleration in convergence due to their reliance on standard\nisotropic steepest descent techniques, which can perform suboptimally in the\nhighly anisotropic landscapes typical of deep networks, particularly LLMs. In\nthis paper, we propose SUMO (Subspace-Aware Moment-Orthogonalization), an\noptimizer that employs exact singular value decomposition (SVD) for moment\northogonalization within a dynamically adapted low-dimensional subspace,\nenabling norm-inducing steepest descent optimization steps. By explicitly\naligning optimization steps with the spectral characteristics of the loss\nlandscape, SUMO effectively mitigates approximation errors associated with\ncommonly used methods like Newton-Schulz orthogonalization approximation. We\ntheoretically establish an upper bound on these approximation errors, proving\ntheir dependence on the condition numbers of moments, conditions we\nanalytically demonstrate are encountered during LLM training. Furthermore, we\nboth theoretically and empirically illustrate that exact orthogonalization via\nSVD substantially improves convergence rates while reducing overall complexity.\nEmpirical evaluations confirm that SUMO accelerates convergence, enhances\nstability, improves performance, and reduces memory requirements by up to 20%\ncompared to state-of-the-art methods."}
{"id": "2505.24760", "pdf": "https://arxiv.org/pdf/2505.24760.pdf", "abs": "https://arxiv.org/abs/2505.24760", "title": "REASONING GYM: Reasoning Environments for Reinforcement Learning with Verifiable Rewards", "authors": ["Zafir Stojanovski", "Oliver Stanley", "Joe Sharratt", "Richard Jones", "Abdulhakeem Adefioye", "Jean Kaddour", "Andreas Köpf"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "For code, see https://github.com/open-thought/reasoning-gym", "summary": "We introduce Reasoning Gym (RG), a library of reasoning environments for\nreinforcement learning with verifiable rewards. It provides over 100 data\ngenerators and verifiers spanning multiple domains including algebra,\narithmetic, computation, cognition, geometry, graph theory, logic, and various\ncommon games. Its key innovation is the ability to generate virtually infinite\ntraining data with adjustable complexity, unlike most previous reasoning\ndatasets, which are typically fixed. This procedural generation approach allows\nfor continuous evaluation across varying difficulty levels. Our experimental\nresults demonstrate the efficacy of RG in both evaluating and reinforcement\nlearning of reasoning models."}
{"id": "2505.24787", "pdf": "https://arxiv.org/pdf/2505.24787.pdf", "abs": "https://arxiv.org/abs/2505.24787", "title": "Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for Complex Instruction-based Image Generation", "authors": ["Yucheng Zhou", "Jiahao Yuan", "Qianning Wang"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent advancements in text-to-image (T2I) generation have enabled models to\nproduce high-quality images from textual descriptions. However, these models\noften struggle with complex instructions involving multiple objects,\nattributes, and spatial relationships. Existing benchmarks for evaluating T2I\nmodels primarily focus on general text-image alignment and fail to capture the\nnuanced requirements of complex, multi-faceted prompts. Given this gap, we\nintroduce LongBench-T2I, a comprehensive benchmark specifically designed to\nevaluate T2I models under complex instructions. LongBench-T2I consists of 500\nintricately designed prompts spanning nine diverse visual evaluation\ndimensions, enabling a thorough assessment of a model's ability to follow\ncomplex instructions. Beyond benchmarking, we propose an agent framework\n(Plan2Gen) that facilitates complex instruction-driven image generation without\nrequiring additional model training. This framework integrates seamlessly with\nexisting T2I models, using large language models to interpret and decompose\ncomplex prompts, thereby guiding the generation process more effectively. As\nexisting evaluation metrics, such as CLIPScore, fail to adequately capture the\nnuances of complex instructions, we introduce an evaluation toolkit that\nautomates the quality assessment of generated images using a set of\nmulti-dimensional metrics. The data and code are released at\nhttps://github.com/yczhou001/LongBench-T2I."}
{"id": "2505.24823", "pdf": "https://arxiv.org/pdf/2505.24823.pdf", "abs": "https://arxiv.org/abs/2505.24823", "title": "PhySense: Principle-Based Physics Reasoning Benchmarking for Large Language Models", "authors": ["Yinggan Xu", "Yue Liu", "Zhiqiang Gao", "Changnan Peng", "Di Luo"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have rapidly advanced and are increasingly\ncapable of tackling complex scientific problems, including those in physics.\nDespite this progress, current LLMs often fail to emulate the concise,\nprinciple-based reasoning characteristic of human experts, instead generating\nlengthy and opaque solutions. This discrepancy highlights a crucial gap in\ntheir ability to apply core physical principles for efficient and interpretable\nproblem solving. To systematically investigate this limitation, we introduce\nPhySense, a novel principle-based physics reasoning benchmark designed to be\neasily solvable by experts using guiding principles, yet deceptively difficult\nfor LLMs without principle-first reasoning. Our evaluation across multiple\nstate-of-the-art LLMs and prompt types reveals a consistent failure to align\nwith expert-like reasoning paths, providing insights for developing AI systems\nwith efficient, robust and interpretable principle-based scientific reasoning."}
{"id": "2505.24840", "pdf": "https://arxiv.org/pdf/2505.24840.pdf", "abs": "https://arxiv.org/abs/2505.24840", "title": "Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are the Bottleneck", "authors": ["Yuwen Tan", "Yuan Qing", "Boqing Gong"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "28 pages, 13 figures", "summary": "This paper reveals that many state-of-the-art large language models (LLMs)\nlack hierarchical knowledge about our visual world, unaware of even\nwell-established biology taxonomies. This shortcoming makes LLMs a bottleneck\nfor vision LLMs' hierarchical visual understanding (e.g., recognizing Anemone\nFish but not Vertebrate). We arrive at these findings using about one million\nfour-choice visual question answering (VQA) tasks constructed from six\ntaxonomies and four image datasets. Interestingly, finetuning a vision LLM\nusing our VQA tasks reaffirms LLMs' bottleneck effect to some extent because\nthe VQA tasks improve the LLM's hierarchical consistency more than the vision\nLLM's. We conjecture that one cannot make vision LLMs understand visual\nconcepts fully hierarchical until LLMs possess corresponding taxonomy\nknowledge."}
{"id": "2505.24844", "pdf": "https://arxiv.org/pdf/2505.24844.pdf", "abs": "https://arxiv.org/abs/2505.24844", "title": "Chameleon: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning", "authors": ["Wanyun Xie", "Francesco Tonin", "Volkan Cevher"], "categories": ["cs.LG", "cs.CL"], "comment": "ICML 2025", "summary": "Training data mixtures greatly impact the generalization performance of large\nlanguage models. Existing domain reweighting methods often rely on costly\nweight computations and require retraining when new data is introduced. To this\nend, we introduce a flexible and efficient data mixing framework, Chameleon,\nthat employs leverage scores to quantify domain importance within a learned\nembedding space. We first construct a domain affinity matrix over domain\nembeddings. The induced leverage scores determine a mixture that upweights\ndomains sharing common representations in embedding space. This formulation\nallows direct transfer to new data by computing the new domain embeddings. In\nexperiments, we demonstrate improvements over three key scenarios: (i) our\ncomputed weights improve performance on pretraining domains with a fraction of\nthe compute of existing methods; (ii) Chameleon can adapt to data changes\nwithout proxy retraining, boosting few-shot reasoning accuracies when\ntransferred to new data; (iii) our method enables efficient domain reweighting\nin finetuning, consistently improving test perplexity on all finetuning domains\nover uniform mixture. Our code is available at\nhttps://github.com/LIONS-EPFL/Chameleon."}
{"id": "2505.24846", "pdf": "https://arxiv.org/pdf/2505.24846.pdf", "abs": "https://arxiv.org/abs/2505.24846", "title": "MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning", "authors": ["Jingyan Shen", "Jiarui Yao", "Rui Yang", "Yifan Sun", "Feng Luo", "Rui Pan", "Tong Zhang", "Han Zhao"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reward modeling is a key step in building safe foundation models when\napplying reinforcement learning from human feedback (RLHF) to align Large\nLanguage Models (LLMs). However, reward modeling based on the Bradley-Terry\n(BT) model assumes a global reward function, failing to capture the inherently\ndiverse and heterogeneous human preferences. Hence, such oversimplification\nlimits LLMs from supporting personalization and pluralistic alignment.\nTheoretically, we show that when human preferences follow a mixture\ndistribution of diverse subgroups, a single BT model has an irreducible error.\nWhile existing solutions, such as multi-objective learning with fine-grained\nannotations, help address this issue, they are costly and constrained by\npredefined attributes, failing to fully capture the richness of human values.\nIn this work, we introduce MiCRo, a two-stage framework that enhances\npersonalized preference learning by leveraging large-scale binary preference\ndatasets without requiring explicit fine-grained annotations. In the first\nstage, MiCRo introduces context-aware mixture modeling approach to capture\ndiverse human preferences. In the second stage, MiCRo integrates an online\nrouting strategy that dynamically adapts mixture weights based on specific\ncontext to resolve ambiguity, allowing for efficient and scalable preference\nadaptation with minimal additional supervision. Experiments on multiple\npreference datasets demonstrate that MiCRo effectively captures diverse human\npreferences and significantly improves downstream personalization."}
{"id": "2505.24850", "pdf": "https://arxiv.org/pdf/2505.24850.pdf", "abs": "https://arxiv.org/abs/2505.24850", "title": "Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning", "authors": ["Shuyao Xu", "Cheng Peng", "Jiangxuan Long", "Weidi Xu", "Wei Chu", "Yuan Qi"], "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6; I.2.7"], "comment": "27 pages, 10 figures. Code available at\n  https://github.com/Tim-Siu/reinforcement-distillation", "summary": "Recent advances in model distillation demonstrate that data from advanced\nreasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer\ncomplex reasoning abilities to smaller, efficient student models. However,\nstandard practices employ rejection sampling, discarding incorrect reasoning\nexamples -- valuable, yet often underutilized data. This paper addresses the\ncritical question: How can both positive and negative distilled reasoning\ntraces be effectively leveraged to maximize LLM reasoning performance in an\noffline setting? To this end, We propose Reinforcement Distillation (REDI), a\ntwo-stage framework. Stage 1 learns from positive traces via Supervised\nFine-Tuning (SFT). Stage 2 further refines the model using both positive and\nnegative traces through our proposed REDI objective. This novel objective is a\nsimple, reference-free loss function that outperforms established methods like\nDPO and SimPO in this distillation context. Our empirical evaluations\ndemonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT\ncombined with DPO/SimPO on mathematical reasoning tasks. Notably, the\nQwen-REDI-1.5B model, post-trained on just 131k positive and negative examples\nfrom the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1).\nIts performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a\nmodel post-trained on 800k proprietary data) across various mathematical\nreasoning benchmarks, establishing a new state-of-the-art for 1.5B models\npost-trained offline with openly available data."}
{"id": "2505.24859", "pdf": "https://arxiv.org/pdf/2505.24859.pdf", "abs": "https://arxiv.org/abs/2505.24859", "title": "Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization", "authors": ["Joschka Braun", "Carsten Eickhoff", "Seyed Ali Bahrainian"], "categories": ["cs.LG", "cs.CL"], "comment": "29 pages, 21 figures, preprint", "summary": "Steering vectors are a lightweight method for controlling text properties by\nadding a learned bias to language model activations at inference time. So far,\nsteering vectors have predominantly been evaluated in multiple-choice settings,\nwhile their effectiveness in free-form generation tasks remains understudied.\nMoving \"Beyond Multiple Choice,\" we thoroughly evaluate the effectiveness of\nsteering vectors in adaptively controlling topical focus, sentiment, toxicity,\nand readability in abstractive summaries of the NEWTS dataset. We find that\nsteering effectively controls the targeted summary properties, but high\nsteering strengths consistently degrade both intrinsic and extrinsic text\nquality. Compared to steering, prompting offers weaker control, while\npreserving text quality. Combining steering and prompting yields the strongest\ncontrol over text properties and offers the most favorable efficacy-quality\ntrade-off at moderate steering strengths. Our results underscore the practical\ntrade-off between control strength and text quality preservation when applying\nsteering vectors to free-form generation tasks."}
{"id": "2505.24871", "pdf": "https://arxiv.org/pdf/2505.24871.pdf", "abs": "https://arxiv.org/abs/2505.24871", "title": "MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning", "authors": ["Yiqing Liang", "Jielin Qiu", "Wenhao Ding", "Zuxin Liu", "James Tompkin", "Mengdi Xu", "Mengzhou Xia", "Zhengzhong Tu", "Laixi Shi", "Jiacheng Zhu"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Project Webpage: https://modomodo-rl.github.io/", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na powerful paradigm for post-training large language models (LLMs), achieving\nstate-of-the-art performance on tasks with structured, verifiable answers.\nApplying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but\nis complicated by the broader, heterogeneous nature of vision-language tasks\nthat demand nuanced visual, logical, and spatial capabilities. As such,\ntraining MLLMs using RLVR on multiple datasets could be beneficial but creates\nchallenges with conflicting objectives from interaction among diverse datasets,\nhighlighting the need for optimal dataset mixture strategies to improve\ngeneralization and reasoning. We introduce a systematic post-training framework\nfor Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation\nand benchmark implementation. Specifically, (1) We developed a multimodal RLVR\nframework for multi-dataset post-training by curating a dataset that contains\ndifferent verifiable vision-language problems and enabling multi-domain online\nRL learning with different verifiable rewards; (2) We proposed a data mixture\nstrategy that learns to predict the RL fine-tuning outcome from the data\nmixture distribution, and consequently optimizes the best mixture.\nComprehensive experiments showcase that multi-domain RLVR training, when\ncombined with mixture prediction strategies, can significantly boost MLLM\ngeneral reasoning capacities. Our best mixture improves the post-trained\nmodel's accuracy on out-of-distribution benchmarks by an average of 5.24%\ncompared to the same model post-trained with uniform data mixture, and by a\ntotal of 20.74% compared to the pre-finetuning baseline."}
{"id": "2505.24872", "pdf": "https://arxiv.org/pdf/2505.24872.pdf", "abs": "https://arxiv.org/abs/2505.24872", "title": "ProxyThinker: Test-Time Guidance through Small Visual Reasoners", "authors": ["Zilin Xiao", "Jaywon Koo", "Siru Ouyang", "Jefferson Hernandez", "Yu Meng", "Vicente Ordonez"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advancements in reinforcement learning with verifiable rewards have\npushed the boundaries of the visual reasoning capabilities in large\nvision-language models (LVLMs). However, training LVLMs with reinforcement\nfine-tuning (RFT) is computationally expensive, posing a significant challenge\nto scaling model size. In this work, we propose ProxyThinker, an inference-time\ntechnique that enables large models to inherit the visual reasoning\ncapabilities from small, slow-thinking visual reasoners without any training.\nBy subtracting the output distributions of base models from those of RFT\nreasoners, ProxyThinker modifies the decoding dynamics and successfully elicits\nthe slow-thinking reasoning demonstrated by the emerged sophisticated behaviors\nsuch as self-verification and self-correction. ProxyThinker consistently boosts\nperformance on challenging visual benchmarks on spatial, mathematical, and\nmulti-disciplinary reasoning, enabling untuned base models to compete with the\nperformance of their full-scale RFT counterparts. Furthermore, our\nimplementation efficiently coordinates multiple language models with\nparallelism techniques and achieves up to 38 $\\times$ faster inference compared\nto previous decoding-time methods, paving the way for the practical deployment\nof ProxyThinker. Code is available at\nhttps://github.com/MrZilinXiao/ProxyThinker."}
{"id": "2505.24875", "pdf": "https://arxiv.org/pdf/2505.24875.pdf", "abs": "https://arxiv.org/abs/2505.24875", "title": "ReasonGen-R1: CoT for Autoregressive Image generation models through SFT and RL", "authors": ["Yu Zhang", "Yunqi Li", "Yifan Yang", "Rui Wang", "Yuqing Yang", "Dai Qi", "Jianmin Bao", "Dongdong Chen", "Chong Luo", "Lili Qiu"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Although chain-of-thought reasoning and reinforcement learning (RL) have\ndriven breakthroughs in NLP, their integration into generative vision models\nremains underexplored. We introduce ReasonGen-R1, a two-stage framework that\nfirst imbues an autoregressive image generator with explicit text-based\n\"thinking\" skills via supervised fine-tuning on a newly generated reasoning\ndataset of written rationales, and then refines its outputs using Group\nRelative Policy Optimization. To enable the model to reason through text before\ngenerating images, We automatically generate and release a corpus of model\ncrafted rationales paired with visual prompts, enabling controlled planning of\nobject layouts, styles, and scene compositions. Our GRPO algorithm uses reward\nsignals from a pretrained vision language model to assess overall visual\nquality, optimizing the policy in each update. Evaluations on GenEval, DPG, and\nthe T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong\nbaselines and prior state-of-the-art models. More: aka.ms/reasongen."}
{"id": "2505.24876", "pdf": "https://arxiv.org/pdf/2505.24876.pdf", "abs": "https://arxiv.org/abs/2505.24876", "title": "Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks", "authors": ["Tajamul Ashraf", "Amal Saqib", "Hanan Ghani", "Muhra AlMahri", "Yuhao Li", "Noor Ahsan", "Umair Nawaz", "Jean Lahoud", "Hisham Cholakkal", "Mubarak Shah", "Philip Torr", "Fahad Shahbaz Khan", "Rao Muhammad Anwer", "Salman Khan"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Deep reasoning is fundamental for solving complex tasks, especially in\nvision-centric scenarios that demand sequential, multimodal understanding.\nHowever, existing benchmarks typically evaluate agents with fully synthetic,\nsingle-turn queries, limited visual modalities, and lack a framework to assess\nreasoning quality over multiple steps as required in real-world settings. To\naddress this, we introduce Agent-X, a large-scale benchmark for evaluating\nvision-centric agents multi-step and deep reasoning capabilities in real-world,\nmultimodal settings. Agent- X features 828 agentic tasks with authentic visual\ncontexts, including images, multi-image comparisons, videos, and instructional\ntext. These tasks span six major agentic environments: general visual\nreasoning, web browsing, security and surveillance, autonomous driving, sports,\nand math reasoning. Our benchmark requires agents to integrate tool use with\nexplicit, stepwise decision-making in these diverse settings. In addition, we\npropose a fine-grained, step-level evaluation framework that assesses the\ncorrectness and logical coherence of each reasoning step and the effectiveness\nof tool usage throughout the task. Our results reveal that even the\nbest-performing models, including GPT, Gemini, and Qwen families, struggle to\nsolve multi-step vision tasks, achieving less than 50% full-chain success.\nThese findings highlight key bottlenecks in current LMM reasoning and tool-use\ncapabilities and identify future research directions in vision-centric agentic\nreasoning models. Our data and code are publicly available at\nhttps://github.com/mbzuai-oryx/Agent-X"}
{"id": "2505.24878", "pdf": "https://arxiv.org/pdf/2505.24878.pdf", "abs": "https://arxiv.org/abs/2505.24878", "title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents", "authors": ["Yaxin Luo", "Zhaoyi Li", "Jiacheng Liu", "Jiacheng Cui", "Xiaohan Zhao", "Zhiqiang Shen"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "Code at: https://github.com/MetaAgentX/OpenCaptchaWorld", "summary": "CAPTCHAs have been a critical bottleneck for deploying web agents in\nreal-world applications, often blocking them from completing end-to-end\nautomation tasks. While modern multimodal LLM agents have demonstrated\nimpressive performance in static perception tasks, their ability to handle\ninteractive, multi-step reasoning challenges like CAPTCHAs is largely untested.\nTo address this gap, we introduce Open CaptchaWorld, the first web-based\nbenchmark and platform specifically designed to evaluate the visual reasoning\nand interaction capabilities of MLLM-powered agents through diverse and dynamic\nCAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225\nCAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,\nwhich quantifies the number of cognitive and motor steps required to solve each\npuzzle. Experimental results show that humans consistently achieve near-perfect\nscores, state-of-the-art MLLM agents struggle significantly, with success rates\nat most 40.0% by Browser-Use Openai-o3, far below human-level performance,\n93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing\nthe limits of current multimodal agents and guiding the development of more\nrobust multimodal reasoning systems. Code and Data are available at this https\nURL."}
{"id": "2401.08491", "pdf": "https://arxiv.org/pdf/2401.08491.pdf", "abs": "https://arxiv.org/abs/2401.08491", "title": "Contrastive Perplexity for Controlled Generation: An Application in Detoxifying Large Language Models", "authors": ["Tassilo Klein", "Moin Nabi"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025 (Main Track)", "summary": "The generation of toxic content by large language models (LLMs) remains a\ncritical challenge for the safe deployment of language technology. We propose a\nnovel framework for implicit knowledge editing and controlled text generation\nby fine-tuning LLMs with a prototype-based contrastive perplexity objective.\nCentral to our method is the construction of hard negatives - toxic outputs\nthat are generated through adversarial paraphrasing to be semantically similar\nand model probability to their non-toxic counterparts. By training on these\nchallenging and realistic pairs, our approach ensures robust and stable\ncontrastive optimization. Experimental results in the domain of detoxification\ndemonstrate that our method significantly reduces toxic generation while\nmaintaining strong performance on downstream tasks such as commonsense\nreasoning and reading comprehension. Our findings highlight the effectiveness\nof exploiting hard negatives for attribute-aware fine-tuning."}
{"id": "2403.02839", "pdf": "https://arxiv.org/pdf/2403.02839.pdf", "abs": "https://arxiv.org/abs/2403.02839", "title": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4", "authors": ["Hui Huang", "Xingyuan Bu", "Hongli Zhou", "Yingqi Qu", "Jing Liu", "Muyun Yang", "Bing Xu", "Tiejun Zhao"], "categories": ["cs.CL"], "comment": "Accepted to Findings of ACL2025", "summary": "Recently, there has been a growing trend of utilizing Large Language Model\n(LLM) to evaluate the quality of other LLMs. Many studies have fine-tuned judge\nmodels based on open-source LLMs for evaluation. While the fine-tuned judge\nmodels are claimed to achieve comparable evaluation capability with GPT-4, in\nthis work, we conduct an empirical study of LLM-as-a-Judge. Our findings\nindicate that although the fine-tuned judge models achieve high performance on\nin-domain test sets, even surpassing GPT-4, they underperform GPT-4 across\nseveral dimensions, including generalizability, fairness and adaptability. We\nalso reveal that the fine-tuned judge model inherently operates as a\ntask-specific classifier, consequently imposing the limitations."}
{"id": "2405.14189", "pdf": "https://arxiv.org/pdf/2405.14189.pdf", "abs": "https://arxiv.org/abs/2405.14189", "title": "Efficient Universal Goal Hijacking with Semantics-guided Prompt Organization", "authors": ["Yihao Huang", "Chong Wang", "Xiaojun Jia", "Qing Guo", "Felix Juefei-Xu", "Jian Zhang", "Geguang Pu", "Yang Liu"], "categories": ["cs.CL", "cs.CV"], "comment": "accepted by ACL 2025", "summary": "Universal goal hijacking is a kind of prompt injection attack that forces\nLLMs to return a target malicious response for arbitrary normal user prompts.\nThe previous methods achieve high attack performance while being too cumbersome\nand time-consuming. Also, they have concentrated solely on optimization\nalgorithms, overlooking the crucial role of the prompt. To this end, we propose\na method called POUGH that incorporates an efficient optimization algorithm and\ntwo semantics-guided prompt organization strategies. Specifically, our method\nstarts with a sampling strategy to select representative prompts from a\ncandidate pool, followed by a ranking strategy that prioritizes them. Given the\nsequentially ranked prompts, our method employs an iterative optimization\nalgorithm to generate a fixed suffix that can concatenate to arbitrary user\nprompts for universal goal hijacking. Experiments conducted on four popular\nLLMs and ten types of target responses verified the effectiveness."}
{"id": "2406.10099", "pdf": "https://arxiv.org/pdf/2406.10099.pdf", "abs": "https://arxiv.org/abs/2406.10099", "title": "Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning", "authors": ["Jiaqi Li", "Yixuan Tang", "Yi Yang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) demonstrate remarkable capabilities but face\nchallenges from hallucinations, which typically arise from insufficient\nknowledge or context. While instructing LLMs to acknowledge knowledge\nlimitations by responding with \"I don't know\" appears promising, we find that\nmodels consistently struggle with admitting knowledge gaps. This challenge may\noriginate from current instruction datasets that emphasise answer generation\nover knowledge boundary awareness. To address this limitation, we introduce\nUncertainty-and-Sensitivity-Aware Tuning (US-Tuning), a novel two-stage\napproach for contextual question answering (QA). The first stage enhances LLMs'\nability to recognise their knowledge boundaries, while the second stage\nreinforces instruction adherence through carefully designed causal prompts. Our\nexperimental results demonstrate that US-Tuning not only significantly reduces\nincorrect answers in contextual QA but also improves models' faithfulness to\ntheir parametric knowledge, mitigating hallucinations in general QA tasks. Our\nfine-tuned Llama2-7B model achieves up to a 34.7% improvement in handling\nout-of-knowledge questions and outperforms GPT-4 by 4.2% in overall\nperformance."}
{"id": "2406.16469", "pdf": "https://arxiv.org/pdf/2406.16469.pdf", "abs": "https://arxiv.org/abs/2406.16469", "title": "Evaluating Visual and Cultural Interpretation: The K-Viscuit Benchmark with Human-VLM Collaboration", "authors": ["ChaeHun Park", "Yujin Baek", "Jaeseok Kim", "Yu-Jung Heo", "Du-Seong Chang", "Jaegul Choo"], "categories": ["cs.CL", "cs.CV"], "comment": "ACL 2025 camera-ready", "summary": "To create culturally inclusive vision-language models (VLMs), developing a\nbenchmark that tests their ability to address culturally relevant questions is\nessential. Existing approaches typically rely on human annotators, making the\nprocess labor-intensive and creating a cognitive burden in generating diverse\nquestions. To address this, we propose a semi-automated framework for\nconstructing cultural VLM benchmarks, specifically targeting multiple-choice\nQA. This framework combines human-VLM collaboration, where VLMs generate\nquestions based on guidelines, a small set of annotated examples, and relevant\nknowledge, followed by a verification process by native speakers. We\ndemonstrate the effectiveness of this framework through the creation of\n\\texttt{K-Viscuit}, a dataset focused on Korean culture. Our experiments on\nthis dataset reveal that open-source models lag behind proprietary ones in\nunderstanding Korean culture, highlighting key areas for improvement. We also\npresent a series of further analyses, including human evaluation, augmenting\nVLMs with external knowledge, and the evaluation beyond multiple-choice QA. Our\ndataset is available at https://huggingface.co/datasets/ddehun/k-viscuit."}
{"id": "2407.09823", "pdf": "https://arxiv.org/pdf/2407.09823.pdf", "abs": "https://arxiv.org/abs/2407.09823", "title": "NativQA: Multilingual Culturally-Aligned Natural Query for LLMs", "authors": ["Md. Arid Hasan", "Maram Hasanain", "Fatema Ahmad", "Sahinur Rahman Laskar", "Sunaya Upadhyay", "Vrunda N Sukhadia", "Mucahid Kutlu", "Shammur Absar Chowdhury", "Firoj Alam"], "categories": ["cs.CL", "cs.AI", "68T50", "F.2.2; I.2.7"], "comment": "LLMs, Native, Multilingual, Language Diversity, Contextual\n  Understanding, Minority Languages, Culturally Informed, Foundation Models,\n  Large Language Models", "summary": "Natural Question Answering (QA) datasets play a crucial role in evaluating\nthe capabilities of large language models (LLMs), ensuring their effectiveness\nin real-world applications. Despite the numerous QA datasets that have been\ndeveloped and some work has been done in parallel, there is a notable lack of a\nframework and large scale region-specific datasets queried by native users in\ntheir own languages. This gap hinders the effective benchmarking and the\ndevelopment of fine-tuned models for regional and cultural specificities. In\nthis study, we propose a scalable, language-independent framework, NativQA, to\nseamlessly construct culturally and regionally aligned QA datasets in native\nlanguages, for LLM evaluation and tuning. We demonstrate the efficacy of the\nproposed framework by designing a multilingual natural QA dataset,\nMultiNativQA, consisting of ~64k manually annotated QA pairs in seven\nlanguages, ranging from high to extremely low resource, based on queries from\nnative speakers from 9 regions covering 18 topics. We benchmark open- and\nclosed-source LLMs with the MultiNativQA dataset. We made the MultiNativQA\ndataset(https://huggingface.co/datasets/QCRI/MultiNativQA), and other\nexperimental scripts(https://gitlab.com/nativqa/multinativqa) publicly\navailable for the community."}
{"id": "2407.14878", "pdf": "https://arxiv.org/pdf/2407.14878.pdf", "abs": "https://arxiv.org/abs/2407.14878", "title": "Modular Sentence Encoders: Separating Language Specialization from Cross-Lingual Alignment", "authors": ["Yongxin Huang", "Kexin Wang", "Goran Glavaš", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": "Accepted for ACL 2025 main conference", "summary": "Multilingual sentence encoders (MSEs) are commonly obtained by training\nmultilingual language models to map sentences from different languages into a\nshared semantic space. As such, they are subject to curse of multilinguality, a\nloss of monolingual representational accuracy due to parameter sharing. Another\nlimitation of MSEs is the trade-off between different task performance:\ncross-lingual alignment training distorts the optimal monolingual structure of\nsemantic spaces of individual languages, harming the utility of sentence\nembeddings in monolingual tasks; cross-lingual tasks, such as cross-lingual\nsemantic similarity and zero-shot transfer for sentence classification, may\nalso require conflicting cross-lingual alignment strategies. In this work, we\naddress both issues by means of modular training of sentence encoders. We first\ntrain language-specific monolingual modules to mitigate negative interference\nbetween languages (i.e., the curse). We then align all non-English sentence\nembeddings to the English by training cross-lingual alignment adapters,\npreventing interference with monolingual specialization from the first step. We\ntrain the cross-lingual adapters with two different types of data to resolve\nthe conflicting requirements of different cross-lingual tasks. Monolingual and\ncross-lingual results on semantic text similarity and relatedness, bitext\nmining and sentence classification show that our modular solution achieves\nbetter and more balanced performance across all the tasks compared to\nfull-parameter training of monolithic multilingual sentence encoders,\nespecially benefiting low-resource languages."}
{"id": "2408.08144", "pdf": "https://arxiv.org/pdf/2408.08144.pdf", "abs": "https://arxiv.org/abs/2408.08144", "title": "MIDAS: Multi-level Intent, Domain, And Slot Knowledge Distillation for Multi-turn NLU", "authors": ["Yan Li", "So-Eon Kim", "Seong-Bae Park", "Soyeon Caren Han"], "categories": ["cs.CL"], "comment": "Accepted by NAACL 2025", "summary": "Although Large Language Models (LLMs) can generate coherent text, they often\nstruggle to recognise user intent behind queries. In contrast, Natural Language\nUnderstanding (NLU) models interpret the purpose and key information of user\ninput for responsive interactions. Existing NLU models typically map utterances\nto a dual-level semantic frame, involving sentence-level intent (SI) and\nword-level slot (WS) labels. However, real-life conversations primarily consist\nof multi-turn dialogues, requiring the interpretation of complex and extended\nexchanges. Researchers encounter challenges in addressing all facets of\nmulti-turn dialogue using a unified NLU model. This paper introduces MIDAS, a\nnovel approach leveraging multi-level intent, domain, and slot knowledge\ndistillation for multi-turn NLU. We construct distinct teachers for SI\ndetection, WS filling, and conversation-level domain (CD) classification, each\nfine-tuned for specific knowledge. A multi-teacher loss is proposed to\nfacilitate the integration of these teachers, guiding a student model in\nmulti-turn dialogue tasks. Results demonstrate the efficacy of our model in\nimproving multi-turn conversation understanding, showcasing the potential for\nadvancements in NLU through multi-level dialogue knowledge distillation. Our\nimplementation is open-sourced on https://github.com/adlnlp/Midas."}
{"id": "2408.12226", "pdf": "https://arxiv.org/pdf/2408.12226.pdf", "abs": "https://arxiv.org/abs/2408.12226", "title": "EvalYaks: Instruction Tuning Datasets and LoRA Fine-tuned Models for Automated Scoring of CEFR B2 Speaking Assessment Transcripts", "authors": ["Nicy Scaria", "Silvester John Joseph Kennedy", "Thomas Latinovich", "Deepak Subramani"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Relying on human experts to evaluate CEFR speaking assessments in an\ne-learning environment creates scalability challenges, as it limits how quickly\nand widely assessments can be conducted. We aim to automate the evaluation of\nCEFR B2 English speaking assessments in e-learning environments from\nconversation transcripts. First, we evaluate the capability of leading open\nsource and commercial Large Language Models (LLMs) to score a candidate's\nperformance across various criteria in the CEFR B2 speaking exam in both global\nand India-specific contexts. Next, we create a new expert-validated,\nCEFR-aligned synthetic conversational dataset with transcripts that are rated\nat different assessment scores. In addition, new instruction-tuned datasets are\ndeveloped from the English Vocabulary Profile (up to CEFR B2 level) and the\nCEFR-SP WikiAuto datasets. Finally, using these new datasets, we perform\nparameter efficient instruction tuning of Mistral Instruct 7B v0.2 to develop a\nfamily of models called EvalYaks. Four models in this family are for assessing\nthe four sections of the CEFR B2 speaking exam, one for identifying the CEFR\nlevel of vocabulary and generating level-specific vocabulary, and another for\ndetecting the CEFR level of text and generating level-specific text. EvalYaks\nachieved an average acceptable accuracy of 96%, a degree of variation of 0.35\nlevels, and performed 3 times better than the next best model. This\ndemonstrates that a 7B parameter LLM instruction tuned with high-quality\nCEFR-aligned assessment data can effectively evaluate and score CEFR B2 English\nspeaking assessments, offering a promising solution for scalable, automated\nlanguage proficiency evaluation."}
{"id": "2409.11638", "pdf": "https://arxiv.org/pdf/2409.11638.pdf", "abs": "https://arxiv.org/abs/2409.11638", "title": "BanStereoSet: A Dataset to Measure Stereotypical Social Biases in LLMs for Bangla", "authors": ["Mahammed Kamruzzaman", "Abdullah Al Monsur", "Shrabon Das", "Enamul Hassan", "Gene Louis Kim"], "categories": ["cs.CL"], "comment": "Accepted at ACL-2025", "summary": "This study presents BanStereoSet, a dataset designed to evaluate\nstereotypical social biases in multilingual LLMs for the Bangla language. In an\neffort to extend the focus of bias research beyond English-centric datasets, we\nhave localized the content from the StereoSet, IndiBias, and Kamruzzaman et.\nal.'s datasets, producing a resource tailored to capture biases prevalent\nwithin the Bangla-speaking community. Our BanStereoSet dataset consists of\n1,194 sentences spanning 9 categories of bias: race, profession, gender,\nageism, beauty, beauty in profession, region, caste, and religion. This dataset\nnot only serves as a crucial tool for measuring bias in multilingual LLMs but\nalso facilitates the exploration of stereotypical bias across different social\ncategories, potentially guiding the development of more equitable language\ntechnologies in Bangladeshi contexts. Our analysis of several language models\nusing this dataset indicates significant biases, reinforcing the necessity for\nculturally and linguistically adapted datasets to develop more equitable\nlanguage technologies."}
{"id": "2410.07523", "pdf": "https://arxiv.org/pdf/2410.07523.pdf", "abs": "https://arxiv.org/abs/2410.07523", "title": "DemoShapley: Valuation of Demonstrations for In-Context Learning", "authors": ["Shan Xie", "Man Luo", "Chadly Daniel Stern", "Mengnan Du", "Lu Cheng"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) using in-context learning (ICL) excel in many\ntasks without task-specific fine-tuning. However, demonstration selection and\nordering greatly impact ICL effectiveness. To address this, we propose\nDemoShapley and Beta-DemoShapley, inspired by Data Shapley and Beta Shapley, to\nassess the influence of individual demonstrations. DemoShapley captures how\neach example influences performance in different contexts, unlike other\ninfluence-based methods that rely on a fixed number of demonstrations.\nBeta-DemoShapley further enhances this framework by incorporating the Beta\ndistribution, allowing users to assign higher weights to smaller cardinalities,\nwhich aligns with ICL's prompt length and computational constraints. Our\nfindings show that the proposed algorithms improve model performance by\nselecting quality demonstrations, and enhancing generalization to\nout-of-distribution tasks. It also identifies noise-compromised data and\npromotes fairness in LLMs, protecting model performance and ensuring robustness\nacross various scenarios."}
{"id": "2410.11119", "pdf": "https://arxiv.org/pdf/2410.11119.pdf", "abs": "https://arxiv.org/abs/2410.11119", "title": "ChuLo: Chunk-Level Key Information Representation for Long Document Processing", "authors": ["Yan Li", "Soyeon Caren Han", "Yue Dai", "Feiqi Cao"], "categories": ["cs.CL"], "comment": "The paper has been accepted to ACL 2025", "summary": "Transformer-based models have achieved remarkable success in various Natural\nLanguage Processing (NLP) tasks, yet their ability to handle long documents is\nconstrained by computational limitations. Traditional approaches, such as\ntruncating inputs, sparse self-attention, and chunking, attempt to mitigate\nthese issues, but they often lead to information loss and hinder the model's\nability to capture long-range dependencies. In this paper, we introduce ChuLo,\na novel chunk representation method for long document understanding that\naddresses these limitations. Our ChuLo groups input tokens using unsupervised\nkeyphrase extraction, emphasizing semantically important keyphrase based chunks\nto retain core document content while reducing input length. This approach\nminimizes information loss and improves the efficiency of Transformer-based\nmodels. Preserving all tokens in long document understanding, especially token\nclassification tasks, is important to ensure that fine-grained annotations,\nwhich depend on the entire sequence context, are not lost. We evaluate our\nmethod on multiple long document classification tasks and long document token\nclassification tasks, demonstrating its effectiveness through comprehensive\nqualitative and quantitative analysis. Our implementation is open-sourced on\nhttps://github.com/adlnlp/Chulo."}
{"id": "2410.13460", "pdf": "https://arxiv.org/pdf/2410.13460.pdf", "abs": "https://arxiv.org/abs/2410.13460", "title": "From Citations to Criticality: Predicting Legal Decision Influence in the Multilingual Swiss Jurisprudence", "authors": ["Ronja Stern", "Ken Kawamura", "Matthias Stürmer", "Ilias Chalkidis", "Joel Niklaus"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2; I.7"], "comment": "Accepted to ACL main 2025", "summary": "Many court systems are overwhelmed all over the world, leading to huge\nbacklogs of pending cases. Effective triage systems, like those in emergency\nrooms, could ensure proper prioritization of open cases, optimizing time and\nresource allocation in the court system. In this work, we introduce the\nCriticality Prediction dataset, a novel resource for evaluating case\nprioritization. Our dataset features a two-tier labeling system: (1) the binary\nLD-Label, identifying cases published as Leading Decisions (LD), and (2) the\nmore granular Citation-Label, ranking cases by their citation frequency and\nrecency, allowing for a more nuanced evaluation. Unlike existing approaches\nthat rely on resource-intensive manual annotations, we algorithmically derive\nlabels leading to a much larger dataset than otherwise possible. We evaluate\nseveral multilingual models, including both smaller fine-tuned models and large\nlanguage models in a zero-shot setting. Our results show that the fine-tuned\nmodels consistently outperform their larger counterparts, thanks to our large\ntraining set. Our results highlight that for highly domain-specific tasks like\nours, large training sets are still valuable."}
{"id": "2410.14248", "pdf": "https://arxiv.org/pdf/2410.14248.pdf", "abs": "https://arxiv.org/abs/2410.14248", "title": "Addressing Blind Guessing: Calibration of Selection Bias in Multiple-Choice Question Answering by Video Language Models", "authors": ["Olga Loginova", "Oleksandr Bezrukov", "Ravi Shekhar", "Alexey Kravets"], "categories": ["cs.CL"], "comment": null, "summary": "Evaluating Video Language Models (VLMs) is a challenging task. Due to its\ntransparency, Multiple-Choice Question Answering (MCQA) is widely used to\nmeasure the performance of these models through accuracy. However, existing\nMCQA benchmarks fail to capture the full reasoning capabilities of VLMs due to\nselection bias, when models disproportionately favor certain answer options\nbased on positional patterns observed during training. In this work, we conduct\na comprehensive empirical analysis of several VLM architectures across major\ndatasets designed to assess complex video-focused reasoning. We identify where\nthe bias is most pronounced and demonstrate to what extent model responses\nreflect genuine understanding of video content and related questions, as\nopposed to reliance on arbitrary patterns or superficial cues, such as answer\nposition. By decomposing the MCQA task and adapting fairness bias metrics to\nVLMs, we introduce a post-processing calibration technique BOLD to balance this\nbias. Our results show that reducing selection bias improves not only debiasing\nmetrics but also overall model performance, including Accuracy and F1 Mean\nscore. Our method, by suppressing \"blind guessing\", offers a more cost- and\ntime-effective approach to mitigating selection bias compared to existing\ntechniques. This study represents the first focused investigation of selection\nbias in video-to-text LLM-powered models."}
{"id": "2410.21728", "pdf": "https://arxiv.org/pdf/2410.21728.pdf", "abs": "https://arxiv.org/abs/2410.21728", "title": "Let's Be Self-generated via Step by Step: A Curriculum Learning Approach to Automated Reasoning with Large Language Models", "authors": ["Kangyang Luo", "Zichen Ding", "Zhenmin Weng", "Lingfeng Qiao", "Meng Zhao", "Xiang Li", "Di Yin", "Jinlong Shu"], "categories": ["cs.CL"], "comment": "Accepted by ACL2025(Findings)", "summary": "While Chain of Thought (CoT) prompting approaches have significantly\nconsolidated the reasoning capabilities of large language models (LLMs), they\nstill face limitations that require extensive human effort or have performance\nneeds to be improved. Existing endeavors have focused on bridging these gaps;\nhowever, these approaches either hinge on external data and cannot completely\neliminate manual effort, or they fall short in effectively directing LLMs to\ngenerate high-quality exemplary prompts. To address the said pitfalls, we\npropose a novel prompt approach for automatic reasoning named \\textbf{LBS3},\ninspired by curriculum learning which better reflects human learning habits.\nSpecifically, LBS3 initially steers LLMs to recall easy-to-hard proxy queries\nthat are pertinent to the target query. Following this, it invokes a\nprogressive strategy that utilizes exemplary prompts stemmed from easy-proxy\nqueries to direct LLMs in solving hard-proxy queries, enabling the high-quality\nof the proxy solutions. Finally, our extensive experiments in various\nreasoning-intensive tasks with varying open- and closed-source LLMs show that\nLBS3 achieves strongly competitive performance compared to the SOTA baselines."}
{"id": "2411.05872", "pdf": "https://arxiv.org/pdf/2411.05872.pdf", "abs": "https://arxiv.org/abs/2411.05872", "title": "Dialectal Coverage And Generalization in Arabic Speech Recognition", "authors": ["Amirbek Djanibekov", "Hawau Olamide Toyin", "Raghad Alshalan", "Abdullah Alitr", "Hanan Aldarmaki"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Developing robust automatic speech recognition (ASR) systems for Arabic\nrequires effective strategies to manage its diversity. Existing ASR systems\nmainly cover the modern standard Arabic (MSA) variety and few high-resource\ndialects, but fall short in coverage and generalization across the multitude of\nspoken variants. Code-switching with English and French is also common in\ndifferent regions of the Arab world, which challenges the performance of\nmonolingual Arabic models. In this work, we introduce a suite of ASR models\noptimized to effectively recognize multiple variants of spoken Arabic,\nincluding MSA, various dialects, and code-switching. We provide open-source\npre-trained models that cover data from 17 Arabic-speaking countries, and\nfine-tuned MSA and dialectal ASR models that include at least 11 variants, as\nwell as multi-lingual ASR models covering embedded languages in code-switched\nutterances. We evaluate ASR performance across these spoken varieties and\ndemonstrate both coverage and performance gains compared to prior models."}
{"id": "2411.06160", "pdf": "https://arxiv.org/pdf/2411.06160.pdf", "abs": "https://arxiv.org/abs/2411.06160", "title": "Expansion Quantization Network: An Efficient Micro-emotion Annotation and Detection Framework", "authors": ["Jingyi Zhou", "Senlin Luo", "Haofan Chen"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "3.1 There is a misstatement in the EQN Framework section", "summary": "Text emotion detection constitutes a crucial foundation for advancing\nartificial intelligence from basic comprehension to the exploration of\nemotional reasoning. Most existing emotion detection datasets rely on manual\nannotations, which are associated with high costs, substantial subjectivity,\nand severe label imbalances. This is particularly evident in the inadequate\nannotation of micro-emotions and the absence of emotional intensity\nrepresentation, which fail to capture the rich emotions embedded in sentences\nand adversely affect the quality of downstream task completion. By proposing an\nall-labels and training-set label regression method, we map label values to\nenergy intensity levels, thereby fully leveraging the learning capabilities of\nmachine models and the interdependencies among labels to uncover multiple\nemotions within samples. This led to the establishment of the Emotion\nQuantization Network (EQN) framework for micro-emotion detection and\nannotation. Using five commonly employed sentiment datasets, we conducted\ncomparative experiments with various models, validating the broad applicability\nof our framework within NLP machine learning models. Based on the EQN\nframework, emotion detection and annotation are conducted on the GoEmotions\ndataset. A comprehensive comparison with the results from Google literature\ndemonstrates that the EQN framework possesses a high capability for automatic\ndetection and annotation of micro-emotions. The EQN framework is the first to\nachieve automatic micro-emotion annotation with energy-level scores, providing\nstrong support for further emotion detection analysis and the quantitative\nresearch of emotion computing."}
{"id": "2411.07404", "pdf": "https://arxiv.org/pdf/2411.07404.pdf", "abs": "https://arxiv.org/abs/2411.07404", "title": "Controllable Context Sensitivity and the Knob Behind It", "authors": ["Julian Minder", "Kevin Du", "Niklas Stoehr", "Giovanni Monea", "Chris Wendler", "Robert West", "Ryan Cotterell"], "categories": ["cs.CL", "cs.AI"], "comment": "Published as a conference paper at ICLR 2025", "summary": "When making predictions, a language model must trade off how much it relies\non its context vs. its prior knowledge. Choosing how sensitive the model is to\nits context is a fundamental functionality, as it enables the model to excel at\ntasks like retrieval-augmented generation and question-answering. In this\npaper, we search for a knob which controls this sensitivity, determining\nwhether language models answer from the context or their prior knowledge. To\nguide this search, we design a task for controllable context sensitivity. In\nthis task, we first feed the model a context (Paris is in England) and a\nquestion (Where is Paris?); we then instruct the model to either use its prior\nor contextual knowledge and evaluate whether it generates the correct answer\nfor both intents (either France or England). When fine-tuned on this task,\ninstruction-tuned versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it\nwith high accuracy (85-95%). Analyzing these high-performing models, we narrow\ndown which layers may be important to context sensitivity using a novel linear\ntime algorithm. Then, in each model, we identify a 1-D subspace in a single\nlayer that encodes whether the model follows context or prior knowledge.\nInterestingly, while we identify this subspace in a fine-tuned model, we find\nthat the exact same subspace serves as an effective knob in not only that model\nbut also non-fine-tuned instruct and base models of that model family. Finally,\nwe show a strong correlation between a model's performance and how distinctly\nit separates context-agreeing from context-ignoring answers in this subspace.\nThese results suggest a single subspace facilitates how the model chooses\nbetween context and prior knowledge, hinting at a simple fundamental mechanism\nthat controls this behavior."}
{"id": "2411.17116", "pdf": "https://arxiv.org/pdf/2411.17116.pdf", "abs": "https://arxiv.org/abs/2411.17116", "title": "Star Attention: Efficient LLM Inference over Long Sequences", "authors": ["Shantanu Acharya", "Fei Jia", "Boris Ginsburg"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ICML 2025", "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy."}
{"id": "2412.04905", "pdf": "https://arxiv.org/pdf/2412.04905.pdf", "abs": "https://arxiv.org/abs/2412.04905", "title": "DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling", "authors": ["Minzheng Wang", "Xinghua Zhang", "Kun Chen", "Nan Xu", "Haiyang Yu", "Fei Huang", "Wenji Mao", "Yongbin Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 Findings. We release the code and data at\n  https://github.com/MozerWang/DEMO", "summary": "Large language models (LLMs) enabled dialogue systems have become one of the\ncentral modes in human-machine interaction, which bring about vast amounts of\nconversation logs and increasing demand for dialogue generation. The dialogue's\nlife-cycle spans from $\\textit{Prelude}$ through $\\textit{Interlocution}$ to\n$\\textit{Epilogue}$, encompassing rich dialogue elements. Despite large volumes\nof dialogue-related studies, there is a lack of systematic investigation into\nthe dialogue stages to frame benchmark construction that covers comprehensive\ndialogue elements. This hinders the precise modeling, generation and assessment\nof LLMs-based dialogue systems. To bridge this gap, in this paper, we introduce\na new research task--$\\textbf{D}$ialogue $\\textbf{E}$lement\n$\\textbf{MO}$deling, including $\\textit{Element Awareness}$ and\n$\\textit{Dialogue Agent Interaction}$, and propose a novel benchmark,\n$\\textbf{DEMO}$, designed for a comprehensive dialogue modeling and assessment.\nOn this basis, we further build the DEMO agent with the adept ability to model\ndialogue elements via imitation learning. Extensive experiments on DEMO\nindicate that current representative LLMs still have considerable potential for\nenhancement, and our DEMO agent performs well in both dialogue element modeling\nand out-of-domain tasks."}
{"id": "2412.08473", "pdf": "https://arxiv.org/pdf/2412.08473.pdf", "abs": "https://arxiv.org/abs/2412.08473", "title": "Multi-perspective Alignment for Increasing Naturalness in Neural Machine Translation", "authors": ["Huiyuan Lai", "Esther Ploeger", "Rik van Noord", "Antonio Toral"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 main; 9 pages", "summary": "Neural machine translation (NMT) systems amplify lexical biases present in\ntheir training data, leading to artificially impoverished language in output\ntranslations. These language-level characteristics render automatic\ntranslations different from text originally written in a language and human\ntranslations, which hinders their usefulness in for example creating evaluation\ndatasets. Attempts to increase naturalness in NMT can fall short in terms of\ncontent preservation, where increased lexical diversity comes at the cost of\ntranslation accuracy. Inspired by the reinforcement learning from human\nfeedback framework, we introduce a novel method that rewards both naturalness\nand content preservation. We experiment with multiple perspectives to produce\nmore natural translations, aiming at reducing machine and human translationese.\nWe evaluate our method on English-to-Dutch literary translation, and find that\nour best model produces translations that are lexically richer and exhibit more\nproperties of human-written language, without loss in translation accuracy."}
{"id": "2412.08972", "pdf": "https://arxiv.org/pdf/2412.08972.pdf", "abs": "https://arxiv.org/abs/2412.08972", "title": "RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World Scenarios", "authors": ["Ruiwen Zhou", "Wenyue Hua", "Liangming Pan", "Sitao Cheng", "Xiaobao Wu", "En Yu", "William Yang Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main Conference", "summary": "This paper introduces RuleArena, a novel and challenging benchmark designed\nto evaluate the ability of large language models (LLMs) to follow complex,\nreal-world rules in reasoning. Covering three practical domains -- airline\nbaggage fees, NBA transactions, and tax regulations -- RuleArena assesses LLMs'\nproficiency in handling intricate natural language instructions that demand\nlong-context understanding, logical reasoning, and accurate mathematical\ncomputation. Two key attributes distinguish RuleArena from traditional\nrule-based reasoning benchmarks: (1) it extends beyond standard first-order\nlogic representations, and (2) it is grounded in authentic, practical\nscenarios, providing insights into the suitability and reliability of LLMs for\nreal-world applications. Our findings reveal several notable limitations in\nLLMs: (1) they struggle to identify and apply the appropriate rules, frequently\nbecoming confused by similar but distinct regulations, (2) they cannot\nconsistently perform accurate mathematical computations, even when they\ncorrectly identify the relevant rules, and (3) in general, they perform poorly\nin the benchmark. We also observe a significant performance boost when LLMs are\nprovided with external tools for oracle math and logic operations. These\nresults highlight significant challenges and promising research directions in\nadvancing LLMs' rule-guided reasoning capabilities in real-life applications.\nOur codes and data are publicly available on\nhttps://github.com/skyriver-2000/RuleArena."}
{"id": "2412.12567", "pdf": "https://arxiv.org/pdf/2412.12567.pdf", "abs": "https://arxiv.org/abs/2412.12567", "title": "FCMR: Robust Evaluation of Financial Cross-Modal Multi-Hop Reasoning", "authors": ["Seunghee Kim", "Changhyeon Kim", "Taeuk Kim"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Real-world decision-making often requires integrating and reasoning over\ninformation from multiple modalities. While recent multimodal large language\nmodels (MLLMs) have shown promise in such tasks, their ability to perform\nmulti-hop reasoning across diverse sources remains insufficiently evaluated.\nExisting benchmarks, such as MMQA, face challenges due to (1) data\ncontamination and (2) a lack of complex queries that necessitate operations\nacross more than two modalities, hindering accurate performance assessment. To\naddress this, we present Financial Cross-Modal Multi-Hop Reasoning (FCMR), a\nbenchmark created to analyze the reasoning capabilities of MLLMs by urging them\nto combine information from textual reports, tables, and charts within the\nfinancial domain. FCMR is categorized into three difficulty levels-Easy,\nMedium, and Hard-facilitating a step-by-step evaluation. In particular,\nproblems at the Hard level require precise cross-modal three-hop reasoning and\nare designed to prevent the disregard of any modality. Experiments on this new\nbenchmark reveal that even state-of-the-art MLLMs struggle, with the\nbest-performing model (Claude 3.5 Sonnet) achieving only 30.4% accuracy on the\nmost challenging tier. We also conduct analysis to provide insights into the\ninner workings of the models, including the discovery of a critical bottleneck\nin the information retrieval phase."}
{"id": "2412.13942", "pdf": "https://arxiv.org/pdf/2412.13942.pdf", "abs": "https://arxiv.org/abs/2412.13942", "title": "A Rose by Any Other Name: LLM-Generated Explanations Are Good Proxies for Human Explanations to Collect Label Distributions on NLI", "authors": ["Beiduo Chen", "Siyao Peng", "Anna Korhonen", "Barbara Plank"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Findings, 25 pages, 21 figures", "summary": "Disagreement in human labeling is ubiquitous, and can be captured in human\njudgment distributions (HJDs). Recent research has shown that explanations\nprovide valuable information for understanding human label variation (HLV) and\nlarge language models (LLMs) can approximate HJD from a few human-provided\nlabel-explanation pairs. However, collecting explanations for every label is\nstill time-consuming. This paper examines whether LLMs can be used to replace\nhumans in generating explanations for approximating HJD. Specifically, we use\nLLMs as annotators to generate model explanations for a few given human labels.\nWe test ways to obtain and combine these label-explanations with the goal to\napproximate human judgment distributions. We further compare the resulting\nhuman with model-generated explanations, and test automatic and human\nexplanation selection. Our experiments show that LLM explanations are promising\nfor NLI: to estimate HJDs, generated explanations yield comparable results to\nhuman's when provided with human labels. Importantly, our results generalize\nfrom datasets with human explanations to i) datasets where they are not\navailable and ii) challenging out-of-distribution test sets."}
{"id": "2412.15268", "pdf": "https://arxiv.org/pdf/2412.15268.pdf", "abs": "https://arxiv.org/abs/2412.15268", "title": "Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic Knowledge Graph", "authors": ["Yibo Zhao", "Jiapeng Zhu", "Can Xu", "Yao Liu", "Xiang Li"], "categories": ["cs.CL", "cs.AI"], "comment": "8 pages of content", "summary": "The rapid growth of social media platforms has raised significant concerns\nregarding online content toxicity. When Large Language Models (LLMs) are used\nfor toxicity detection, two key challenges emerge: 1) the absence of\ndomain-specific toxic knowledge leads to false negatives; 2) the excessive\nsensitivity of LLMs to toxic speech results in false positives, limiting\nfreedom of speech. To address these issues, we propose a novel method called\nMetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance\nhatred and toxicity detection. First, we construct a comprehensive meta-toxic\nknowledge graph by utilizing LLMs to extract toxic information through a\nthree-step pipeline, with toxic benchmark datasets serving as corpora. Second,\nwe query the graph via retrieval and ranking processes to supplement accurate,\nrelevant toxic knowledge. Extensive experiments and in-depth case studies\nacross multiple datasets demonstrate that our MetaTox significantly decreases\nthe false positive rate while boosting overall toxicity detection performance.\nOur code is available at https://github.com/YiboZhao624/MetaTox."}
{"id": "2412.15712", "pdf": "https://arxiv.org/pdf/2412.15712.pdf", "abs": "https://arxiv.org/abs/2412.15712", "title": "Contrastive Learning for Task-Independent SpeechLLM-Pretraining", "authors": ["Maike Züfle", "Jan Niehues"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) excel in natural language processing but\nadapting these LLMs to speech processing tasks efficiently is not\nstraightforward. Direct task-specific fine-tuning is limited by overfitting\nrisks, data requirements, and computational costs. To address these challenges,\nwe propose a scalable, two-stage training approach: (1) A task-independent\nspeech pretraining stage using contrastive learning to align text and speech\nrepresentations over all layers, followed by (2) a task-specific fine-tuning\nstage requiring minimal data. This approach outperforms traditional ASR\npretraining and enables the model to surpass models specialized on speech\ntranslation and question answering while being trained on only 10% of the\ntask-specific data."}
{"id": "2501.03191", "pdf": "https://arxiv.org/pdf/2501.03191.pdf", "abs": "https://arxiv.org/abs/2501.03191", "title": "CLIX: Cross-Lingual Explanations of Idiomatic Expressions", "authors": ["Aaron Gluck", "Katharina von der Wense", "Maria Leonor Pacheco"], "categories": ["cs.CL"], "comment": "Accepted to Findings of ACL 2025", "summary": "Automated definition generation systems have been proposed to support\nvocabulary expansion for language learners. The main barrier to the success of\nthese systems is that learners often struggle to understand definitions due to\nthe presence of potentially unfamiliar words and grammar, particularly when\nnon-standard language is involved. To address these challenges, we propose\nCLIX, the task of Cross-Lingual explanations of Idiomatic eXpressions. We\nexplore the capabilities of current NLP models for this task, and observe that\nwhile it remains challenging, large language models show promise. Finally, we\nperform a detailed error analysis to highlight the key challenges that need to\nbe addressed before we can reliably incorporate these systems into educational\ntools."}
{"id": "2501.03884", "pdf": "https://arxiv.org/pdf/2501.03884.pdf", "abs": "https://arxiv.org/abs/2501.03884", "title": "AlphaPO: Reward Shape Matters for LLM Alignment", "authors": ["Aman Gupta", "Shao Tang", "Qingquan Song", "Sirou Zhu", "Jiwoo Hong", "Ankan Saha", "Viral Gupta", "Noah Lee", "Eunki Kim", "Siyu Zhu", "Parag Agrawal", "Natesh Pillai", "S. Sathiya Keerthi"], "categories": ["cs.CL"], "comment": "26 pages, 16 figures. Accepted to ICML 2025", "summary": "Reinforcement Learning with Human Feedback (RLHF) and its variants have made\nhuge strides toward the effective alignment of large language models (LLMs) to\nfollow instructions and reflect human values. More recently, Direct Alignment\nAlgorithms (DAAs) have emerged in which the reward modeling stage of RLHF is\nskipped by characterizing the reward directly as a function of the policy being\nlearned. Some popular examples of DAAs include Direct Preference Optimization\n(DPO) and Simple Preference Optimization (SimPO). These methods often suffer\nfrom likelihood displacement, a phenomenon by which the probabilities of\npreferred responses are often reduced undesirably. In this paper, we argue\nthat, for DAAs the reward (function) shape matters. We introduce\n\\textbf{AlphaPO}, a new DAA method that leverages an $\\alpha$-parameter to help\nchange the shape of the reward function beyond the standard log reward. AlphaPO\nhelps maintain fine-grained control over likelihood displacement and\nover-optimization. Compared to SimPO, one of the best performing DAAs, AlphaPO\nleads to about 7\\% to 10\\% relative improvement in alignment performance for\nthe instruct versions of Mistral-7B and Llama3-8B while achieving 15\\% to 50\\%\nrelative improvement over DPO on the same models. The analysis and results\npresented highlight the importance of the reward shape and how one can\nsystematically change it to affect training dynamics, as well as improve\nalignment performance."}
{"id": "2501.13074", "pdf": "https://arxiv.org/pdf/2501.13074.pdf", "abs": "https://arxiv.org/abs/2501.13074", "title": "Autonomy-of-Experts Models", "authors": ["Ang Lv", "Ruobing Xie", "Yining Qian", "Songhao Wu", "Xingwu Sun", "Zhanhui Kang", "Di Wang", "Rui Yan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by ICML 2025", "summary": "Mixture-of-Experts (MoE) models mostly use a router to assign tokens to\nspecific expert modules, activating only partial parameters and often\noutperforming dense models. We argue that the separation between the router's\ndecision-making and the experts' execution is a critical yet overlooked issue,\nleading to suboptimal expert selection and ineffective learning. To address\nthis, we propose Autonomy-of-Experts (AoE), a novel MoE paradigm in which\nexperts autonomously select themselves to process inputs. AoE is based on the\ninsight that an expert is aware of its own capacity to effectively process a\ntoken, an awareness reflected in the scale of its internal activations. In AoE,\nrouters are removed; instead, experts pre-compute internal activations for\ninputs and are ranked based on their activation norms. Only the top-ranking\nexperts proceed with the forward pass, while the others abort. The overhead of\npre-computing activations is reduced through a low-rank weight factorization.\nThis self-evaluating-then-partner-comparing approach ensures improved expert\nselection and effective learning. We pre-train language models having 700M up\nto 4B parameters, demonstrating that AoE outperforms traditional MoE models\nwith comparable efficiency."}
{"id": "2501.18922", "pdf": "https://arxiv.org/pdf/2501.18922.pdf", "abs": "https://arxiv.org/abs/2501.18922", "title": "KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree Search", "authors": ["Haoran Luo", "Haihong E", "Yikai Guo", "Qika Lin", "Xiaobao Wu", "Xinyu Mu", "Wenhao Liu", "Meina Song", "Yifan Zhu", "Luu Anh Tuan"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "Accepted by ICML 2025 main conference", "summary": "Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions with a large-scale structured knowledge base (KB). Despite\nadvancements with large language models (LLMs), KBQA still faces challenges in\nweak KB awareness, imbalance between effectiveness and efficiency, and high\nreliance on annotated data. To address these challenges, we propose KBQA-o1, a\nnovel agentic KBQA method with Monte Carlo Tree Search (MCTS). It introduces a\nReAct-based agent process for stepwise logical form generation with KB\nenvironment exploration. Moreover, it employs MCTS, a heuristic search method\ndriven by policy and reward models, to balance agentic exploration's\nperformance and search space. With heuristic exploration, KBQA-o1 generates\nhigh-quality annotations for further improvement by incremental fine-tuning.\nExperimental results show that KBQA-o1 outperforms previous low-resource KBQA\nmethods with limited annotated data, boosting Llama-3.1-8B model's GrailQA F1\nperformance to 78.5% compared to 48.5% of the previous sota method with\nGPT-3.5-turbo. Our code is publicly available."}
{"id": "2502.00592", "pdf": "https://arxiv.org/pdf/2502.00592.pdf", "abs": "https://arxiv.org/abs/2502.00592", "title": "M+: Extending MemoryLLM with Scalable Long-Term Memory", "authors": ["Yu Wang", "Dmitry Krotov", "Yuanzhe Hu", "Yifan Gao", "Wangchunshu Zhou", "Julian McAuley", "Dan Gutfreund", "Rogerio Feris", "Zexue He"], "categories": ["cs.CL"], "comment": null, "summary": "Equipping large language models (LLMs) with latent-space memory has attracted\nincreasing attention as they can extend the context window of existing language\nmodels. However, retaining information from the distant past remains a\nchallenge. For example, MemoryLLM (Wang et al., 2024a), as a representative\nwork with latent-space memory, compresses past information into hidden states\nacross all layers, forming a memory pool of 1B parameters. While effective for\nsequence lengths up to 16k tokens, it struggles to retain knowledge beyond 20k\ntokens. In this work, we address this limitation by introducing M+, a\nmemory-augmented model based on MemoryLLM that significantly enhances long-term\ninformation retention. M+ integrates a long-term memory mechanism with a\nco-trained retriever, dynamically retrieving relevant information during text\ngeneration. We evaluate M+ on diverse benchmarks, including long-context\nunderstanding and knowledge retention tasks. Experimental results show that M+\nsignificantly outperforms MemoryLLM and recent strong baselines, extending\nknowledge retention from under 20k to over 160k tokens with similar GPU memory\noverhead. We open-source our code at https://github.com/wangyu-ustc/MemoryLLM"}
{"id": "2502.01349", "pdf": "https://arxiv.org/pdf/2502.01349.pdf", "abs": "https://arxiv.org/abs/2502.01349", "title": "Bias Beware: The Impact of Cognitive Biases on LLM-Driven Product Recommendations", "authors": ["Giorgos Filandrianos", "Angeliki Dimitriou", "Maria Lymperaiou", "Konstantinos Thomas", "Giorgos Stamou"], "categories": ["cs.CL"], "comment": null, "summary": "The advent of Large Language Models (LLMs) has revolutionized product\nrecommenders, yet their susceptibility to adversarial manipulation poses\ncritical challenges, particularly in real-world commercial applications. Our\napproach is the first one to tap into human psychological principles,\nseamlessly modifying product descriptions, making such manipulations hard to\ndetect. In this work, we investigate cognitive biases as black-box adversarial\nstrategies, drawing parallels between their effects on LLMs and human\npurchasing behavior. Through extensive evaluation across models of varying\nscale, we find that certain biases, such as social proof, consistently boost\nproduct recommendation rate and ranking, while others, like scarcity and\nexclusivity, surprisingly reduce visibility. Our results demonstrate that\ncognitive biases are deeply embedded in state-of-the-art LLMs, leading to\nhighly unpredictable behavior in product recommendations and posing significant\nchallenges for effective mitigation."}
{"id": "2502.02339", "pdf": "https://arxiv.org/pdf/2502.02339.pdf", "abs": "https://arxiv.org/abs/2502.02339", "title": "Boosting Multimodal Reasoning with Automated Structured Thinking", "authors": ["Jinyang Wu", "Mingkuan Feng", "Shuai Zhang", "Fangrui Lv", "Ruihan Jin", "Feihu Che", "Zengqi Wen", "Jianhua Tao"], "categories": ["cs.CL"], "comment": null, "summary": "Multimodal large language models excel across diverse domains but struggle\nwith complex visual reasoning tasks. Current approaches aim to incorporate\nstructured thinking via two strategies: explicit search methods and\npost-training techniques. However, both approaches face significant\nlimitations: Search-based methods suffer from computational inefficiency due to\nextensive solution space exploration, while post-training methods require\nsubstantial data, computational resources, and often encounter training\ninstability. To address these limitations, we propose AStar, an\n\\textbf{A}utomated \\textbf{S}tructured \\textbf{t}hinking paradigm for\nmultimod\\textbf{a}l \\textbf{r}easoning. Our method introduces \"thought cards\",\na lightweight library of high-level reasoning patterns abstracted from 500\nprior samples using Monte Carlo Tree Search. For each test problem, AStar\nadaptively retrieves the optimal thought cards and seamlessly integrates these\nexternal explicit guidelines with the model's internal implicit reasoning\ncapabilities. Extensive experiments demonstrate AStar's effectiveness and\nefficiency: using only 500 prior samples and a 7B backbone, our training-free\nframework achieves 53.9$\\%$ accuracy on MathVerse (surpassing GPT-4o's 50.2%)\nand 32.7% on MathVision (versus GPT-4o's 30.4%). Further analysis reveals that\nAStar generalizes beyond multimodal reasoning to visual perception and\nunderstanding domains, and serves as a plug-and-play test-time inference method\ncompatible with mainstream post-training techniques like GRPO."}
{"id": "2502.02659", "pdf": "https://arxiv.org/pdf/2502.02659.pdf", "abs": "https://arxiv.org/abs/2502.02659", "title": "A Training-Free Length Extrapolation Approach for LLMs: Greedy Attention Logit Interpolation (GALI)", "authors": ["Yan Li", "Tianyi Zhang", "Zechuan Li", "Soyeon Caren Han"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, under review in the conference", "summary": "Transformer-based Large Language Models (LLMs) struggle with inputs exceeding\ntheir training context window due to positional out-of-distribution (O.O.D.)\nissues that disrupt attention. Existing solutions, including fine-tuning and\ntraining-free methods, face challenges like inefficiency, redundant\ninterpolation, logit outliers, or loss of local positional information. We\npropose Greedy Attention Logit Interpolation (GALI), a training-free method\nthat improves length extrapolation by greedily reusing pretrained positional\nintervals and interpolating attention logit to eliminate outliers. GALI\nachieves stable and superior performance across a wide range of long-context\ntasks without requiring input-length-specific tuning. Our analysis further\nreveals that LLMs interpret positional intervals unevenly and that restricting\ninterpolation to narrower ranges improves performance, even on short-context\ntasks. GALI represents a step toward more robust and generalizable long-text\nprocessing in LLMs. Our implementation of GALI, along with the experiments from\nour paper, is open-sourced at https://github.com/adlnlp/Gali."}
{"id": "2502.04037", "pdf": "https://arxiv.org/pdf/2502.04037.pdf", "abs": "https://arxiv.org/abs/2502.04037", "title": "Exploring Imbalanced Annotations for Effective In-Context Learning", "authors": ["Hongfu Gao", "Feipeng Zhang", "Hao Zeng", "Deyu Meng", "Bingyi Jing", "Hongxin Wei"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have shown impressive performance on downstream\ntasks through in-context learning (ICL), which heavily relies on the\ndemonstrations selected from annotated datasets. However, these datasets often\nexhibit long-tailed class distributions in real-world scenarios, leading to\nbiased demonstration selection. In this work, we show that such class\nimbalances significantly degrade the ICL performance across various tasks,\nregardless of selection methods. Moreover, classical rebalancing methods, which\nfocus solely on class weights, yield poor performance due to neglecting\ncondition bias--skewed feature distributions within classes. To address this,\nwe propose Reweighting with Conditional Bias (dubbed RCB), a simple and\ncomplementary approach to enhance ICL performance under class imbalance. In\nparticular, RCB estimates conditional bias using a balanced subset and\nre-weights demonstration scores based on both class weight and conditional\nbias. In effect, RCB prevents over-selection from dominant classes while\npreserving the efficacy of current selection methods. Extensive experiments on\ncommon benchmarks demonstrate the effectiveness of our method, improving the\naverage accuracy of current selection methods by up to 5.42%."}
{"id": "2502.09284", "pdf": "https://arxiv.org/pdf/2502.09284.pdf", "abs": "https://arxiv.org/abs/2502.09284", "title": "SparQLe: Speech Queries to Text Translation Through LLMs", "authors": ["Amirbek Djanibekov", "Hanan Aldarmaki"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the growing influence of Large Language Models (LLMs), there is\nincreasing interest in integrating speech representations with them to enable\nmore seamless multi-modal processing and speech understanding. This study\nintroduces a novel approach that combines self-supervised speech\nrepresentations with instruction-tuned LLMs for speech-to-text translation. The\nproposed approach leverages a modality adapter to align extracted speech\nfeatures with instruction-tuned LLMs using English speech data. Our experiments\ndemonstrate that this method effectively preserves the semantic content of the\ninput speech and serves as an effective bridge between self-supervised speech\nmodels and instruction-tuned LLMs, offering a promising approach for various\nspeech understanding applications."}
{"id": "2502.11084", "pdf": "https://arxiv.org/pdf/2502.11084.pdf", "abs": "https://arxiv.org/abs/2502.11084", "title": "Rewrite to Jailbreak: Discover Learnable and Transferable Implicit Harmfulness Instruction", "authors": ["Yuting Huang", "Chengyuan Liu", "Yifeng Feng", "Yiquan Wu", "Chao Wu", "Fei Wu", "Kun Kuang"], "categories": ["cs.CL"], "comment": "22 pages, 10 figures, accepted to ACL 2025 findings", "summary": "As Large Language Models (LLMs) are widely applied in various domains, the\nsafety of LLMs is increasingly attracting attention to avoid their powerful\ncapabilities being misused. Existing jailbreak methods create a forced\ninstruction-following scenario, or search adversarial prompts with prefix or\nsuffix tokens to achieve a specific representation manually or automatically.\nHowever, they suffer from low efficiency and explicit jailbreak patterns, far\nfrom the real deployment of mass attacks to LLMs. In this paper, we point out\nthat simply rewriting the original instruction can achieve a jailbreak, and we\nfind that this rewriting approach is learnable and transferable. We propose the\nRewrite to Jailbreak (R2J) approach, a transferable black-box jailbreak method\nto attack LLMs by iteratively exploring the weakness of the LLMs and\nautomatically improving the attacking strategy. The jailbreak is more efficient\nand hard to identify since no additional features are introduced. Extensive\nexperiments and analysis demonstrate the effectiveness of R2J, and we find that\nthe jailbreak is also transferable to multiple datasets and various types of\nmodels with only a few queries. We hope our work motivates further\ninvestigation of LLM safety. The code can be found at\nhttps://github.com/ythuang02/R2J/."}
{"id": "2502.11361", "pdf": "https://arxiv.org/pdf/2502.11361.pdf", "abs": "https://arxiv.org/abs/2502.11361", "title": "VLDBench Evaluating Multimodal Disinformation with Regulatory Alignment", "authors": ["Shaina Raza", "Ashmal Vayani", "Aditya Jain", "Aravind Narayanan", "Vahid Reza Khazaie", "Syed Raza Bashir", "Elham Dolatabadi", "Gias Uddin", "Christos Emmanouilidis", "Rizwan Qureshi", "Mubarak Shah"], "categories": ["cs.CL"], "comment": "under review", "summary": "Detecting disinformation that blends manipulated text and images has become\nincreasingly challenging, as AI tools make synthetic content easy to generate\nand disseminate. While most existing AI safety benchmarks focus on single\nmodality misinformation (i.e., false content shared without intent to deceive),\nintentional multimodal disinformation, such as propaganda or conspiracy\ntheories that imitate credible news, remains largely unaddressed. We introduce\nthe Vision-Language Disinformation Detection Benchmark (VLDBench), the first\nlarge-scale resource supporting both unimodal (text-only) and multimodal (text\n+ image) disinformation detection. VLDBench comprises approximately 62,000\nlabeled text-image pairs across 13 categories, curated from 58 news outlets.\nUsing a semi-automated pipeline followed by expert review, 22 domain experts\ninvested over 500 hours to produce high-quality annotations with substantial\ninter-annotator agreement. Evaluations of state-of-the-art Large Language\nModels (LLMs) and Vision-Language Models (VLMs) on VLDBench show that\nincorporating visual cues improves detection accuracy by 5 to 35 percentage\npoints over text-only models. VLDBench provides data and code for evaluation,\nfine-tuning, and robustness testing to support disinformation analysis.\nDeveloped in alignment with AI governance frameworks (e.g., the MIT AI Risk\nRepository), VLDBench offers a principled foundation for advancing trustworthy\ndisinformation detection in multimodal media.\n  Project: https://vectorinstitute.github.io/VLDBench/ Dataset:\nhttps://huggingface.co/datasets/vector-institute/VLDBench Code:\nhttps://github.com/VectorInstitute/VLDBench"}
{"id": "2502.11404", "pdf": "https://arxiv.org/pdf/2502.11404.pdf", "abs": "https://arxiv.org/abs/2502.11404", "title": "ToolCoder: A Systematic Code-Empowered Tool Learning Framework for Large Language Models", "authors": ["Hanxing Ding", "Shuchang Tao", "Liang Pang", "Zihao Wei", "Jinyang Gao", "Bolin Ding", "Huawei Shen", "Xueqi Cheng"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Tool learning has emerged as a crucial capability for large language models\n(LLMs) to solve complex real-world tasks through interaction with external\ntools. Existing approaches face significant challenges, including reliance on\nhand-crafted prompts, difficulty in multi-step planning, and lack of precise\nerror diagnosis and reflection mechanisms. We propose ToolCoder, a novel\nframework that reformulates tool learning as a code generation task. Inspired\nby software engineering principles, ToolCoder transforms natural language\nqueries into structured Python function scaffold and systematically breaks down\ntasks with descriptive comments, enabling LLMs to leverage coding paradigms for\ncomplex reasoning and planning. It then generates and executes function\nimplementations to obtain final responses. Additionally, ToolCoder stores\nsuccessfully executed functions in a repository to promote code reuse, while\nleveraging error traceback mechanisms for systematic debugging, optimizing both\nexecution efficiency and robustness. Experiments demonstrate that ToolCoder\nachieves superior performance in task completion accuracy and execution\nreliability compared to existing approaches, establishing the effectiveness of\ncode-centric approaches in tool learning."}
{"id": "2502.11471", "pdf": "https://arxiv.org/pdf/2502.11471.pdf", "abs": "https://arxiv.org/abs/2502.11471", "title": "GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language for Knowledge Graph Completion", "authors": ["Kangyang Luo", "Yuzhuo Bai", "Cheng Gao", "Shuzheng Si", "Yingli Shen", "Zhu Liu", "Zhitong Wang", "Cunliang Kong", "Wenhao Li", "Yufei Huang", "Ye Tian", "Xuantang Xiong", "Lei Han", "Maosong Sun"], "categories": ["cs.CL", "cs.IR"], "comment": "Accepted by ACL2025(Findings)", "summary": "Knowledge Graph Completion (KGC), which aims to infer missing or incomplete\nfacts, is a crucial task for KGs. However, integrating the vital structural\ninformation of KGs into Large Language Models (LLMs) and outputting predictions\ndeterministically remains challenging. To address this, we propose a new method\ncalled GLTW, which encodes the structural information of KGs and merges it with\nLLMs to enhance KGC performance. Specifically, we introduce an improved Graph\nTransformer (iGT) that effectively encodes subgraphs with both local and global\nstructural information and inherits the characteristics of language model,\nbypassing training from scratch. Also, we develop a subgraph-based\nmulti-classification training objective, using all entities within KG as\nclassification objects, to boost learning efficiency.Importantly, we combine\niGT with an LLM that takes KG language prompts as input.Our extensive\nexperiments on various KG datasets show that GLTW achieves significant\nperformance gains compared to SOTA baselines."}
{"id": "2502.11541", "pdf": "https://arxiv.org/pdf/2502.11541.pdf", "abs": "https://arxiv.org/abs/2502.11541", "title": "MuSC: Improving Complex Instruction Following with Multi-granularity Self-Contrastive Training", "authors": ["Hui Huang", "Jiaheng Liu", "Yancheng He", "Shilong Li", "Bing Xu", "Conghui Zhu", "Muyun Yang", "Tiejun Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL2025", "summary": "Complex instruction-following with elaborate constraints is imperative for\nLarge Language Models (LLMs). While existing methods have constructed data for\ncomplex instruction alignment, they all rely on a more advanced model,\nespecially GPT-4, limiting their application. In this paper, we propose a\nMulti-granularity Self-Contrastive Training (MuSC) framework, to improve the\ncomplex instruction alignment without relying on a stronger model. Our method\nis conducted on both coarse and fine granularity. On coarse-granularity, we\nconstruct constraint-aware preference data based on instruction decomposition\nand recombination. On fine-granularity, we perform token-aware preference\noptimization with dynamic token-level supervision. Our method is evaluated on\nopen-sourced models, and experiment results show our method achieves\nsignificant improvement on both complex and general instruction-following\nbenchmarks, surpassing previous self-alignment methods."}
{"id": "2502.11705", "pdf": "https://arxiv.org/pdf/2502.11705.pdf", "abs": "https://arxiv.org/abs/2502.11705", "title": "LLM Agents Making Agent Tools", "authors": ["Georg Wölflein", "Dyke Ferber", "Daniel Truhn", "Ognjen Arandjelović", "Jakob Nikolas Kather"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "comment": "Accepted at ACL 2025", "summary": "Tool use has turned large language models (LLMs) into powerful agents that\ncan perform complex multi-step tasks by dynamically utilising external software\ncomponents. However, these tools must be implemented in advance by human\ndevelopers, hindering the applicability of LLM agents in domains demanding\nlarge numbers of highly specialised tools, like in life sciences and medicine.\nMotivated by the growing trend of scientific studies accompanied by public code\nrepositories, we propose ToolMaker, an agentic framework that autonomously\ntransforms papers with code into LLM-compatible tools. Given a GitHub URL and\nshort task description, ToolMaker autonomously installs dependencies and\ngenerates code to perform the task, using a closed-loop self-correction\nmechanism for debugging. To evaluate our approach, we introduce a benchmark\ncomprising 15 complex computational tasks spanning various domains with over\n100 unit tests to assess correctness and robustness. Our method correctly\nimplements 80% of the tasks, substantially outperforming current\nstate-of-the-art software engineering agents. ToolMaker therefore is a step\ntowards fully autonomous agent-based scientific workflows. Our code and\nbenchmark are publicly available at https://github.com/KatherLab/ToolMaker."}
{"id": "2502.11718", "pdf": "https://arxiv.org/pdf/2502.11718.pdf", "abs": "https://arxiv.org/abs/2502.11718", "title": "\"See the World, Discover Knowledge\": A Chinese Factuality Evaluation for Large Vision Language Models", "authors": ["Jihao Gu", "Yingyao Wang", "Pi Bu", "Chen Wang", "Ziming Wang", "Tengtao Song", "Donglai Wei", "Jiale Yuan", "Yingxiu Zhao", "Yancheng He", "Shilong Li", "Jiaheng Liu", "Meng Cao", "Jun Song", "Yingshui Tan", "Xiang Li", "Wenbo Su", "Zhicheng Zheng", "Xiaoyong Zhu", "Bo Zheng"], "categories": ["cs.CL", "cs.CV"], "comment": "26 pages, 21 figures", "summary": "The evaluation of factual accuracy in large vision language models (LVLMs)\nhas lagged behind their rapid development, making it challenging to fully\nreflect these models' knowledge capacity and reliability. In this paper, we\nintroduce the first factuality-based visual question-answering benchmark in\nChinese, named ChineseSimpleVQA, aimed at assessing the visual factuality of\nLVLMs across 8 major topics and 56 subtopics. The key features of this\nbenchmark include a focus on the Chinese language, diverse knowledge types, a\nmulti-hop question construction, high-quality data, static consistency, and\neasy-to-evaluate through short answers. Moreover, we contribute a rigorous data\nconstruction pipeline and decouple the visual factuality into two parts: seeing\nthe world (i.e., object recognition) and discovering knowledge. This decoupling\nallows us to analyze the capability boundaries and execution mechanisms of\nLVLMs. Subsequently, we evaluate 34 advanced open-source and closed-source\nmodels, revealing critical performance gaps within this field. Our\nevaluation-friendly code and data have already been open-sourced."}
{"id": "2502.12476", "pdf": "https://arxiv.org/pdf/2502.12476.pdf", "abs": "https://arxiv.org/abs/2502.12476", "title": "CoCo-CoLa: Evaluating and Improving Language Adherence in Multilingual LLMs", "authors": ["Elnaz Rahmati", "Alireza S. Ziabari", "Morteza Dehghani"], "categories": ["cs.CL"], "comment": "26 pages, 7 figures", "summary": "Multilingual Large Language Models (LLMs) develop cross-lingual abilities\ndespite being trained on limited parallel data. However, they often struggle to\ngenerate responses in the intended language, favoring high-resource languages\nsuch as English. In this work, we introduce CoCo-CoLa (Correct Concept -\nCorrect Language), a novel metric to evaluate language adherence in\nmultilingual LLMs. Using fine-tuning experiments on a closed-book QA task\nacross seven languages, we analyze how training in one language affects others'\nperformance. Our findings reveal that multilingual models share task knowledge\nacross languages but exhibit biases in the selection of output language. We\nidentify language-specific layers, showing that final layers play a crucial\nrole in determining output language. Accordingly, we propose a partial training\nstrategy that selectively fine-tunes key layers, improving language adherence\nwhile significantly reducing computational cost. Our method achieves comparable\nor superior performance to full fine-tuning, particularly for low-resource\nlanguages, offering a more efficient multilingual adaptation."}
{"id": "2502.14494", "pdf": "https://arxiv.org/pdf/2502.14494.pdf", "abs": "https://arxiv.org/abs/2502.14494", "title": "StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following", "authors": ["Jinnan Li", "Jinzhe Li", "Yue Wang", "Yi Chang", "Yuan Wu"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings camera-ready version", "summary": "Multi-turn instruction following capability constitutes a core competency of\nlarge language models (LLMs) in real-world applications. Existing evaluation\nbenchmarks predominantly focus on fine-grained constraint satisfaction and\ndomain-specific capability assessment, yet overlook the crucial structural\ndependencies between dialogue turns that distinguish multi-turn from\nsingle-turn interactions. These structural dependencies not only reflect user\nintent but also establish an essential second dimension for the instruction\nfollowing evaluation beyond constraint satisfaction. To address this gap, we\npropose StructFlowBench, a multi-turn instruction following benchmark with\nstructural flow modeling. The benchmark defines an innovative structural flow\nframework with six fundamental inter-turn relationships. These relationships\nintroduce novel structural constraints for model evaluation and also serve as\ngeneration parameters for creating customized dialogue flows tailored to\nspecific scenarios. Adopting established LLM-based automatic evaluation\nmethodologies, we conduct systematic evaluations of 13 leading open-source and\nclosed-source LLMs. Experimental results reveal significant deficiencies in\ncurrent models' comprehension of multi-turn dialogue structures. The code is\navailable at https://github.com/MLGroupJLU/StructFlowBench."}
{"id": "2502.14561", "pdf": "https://arxiv.org/pdf/2502.14561.pdf", "abs": "https://arxiv.org/abs/2502.14561", "title": "Can LLMs Predict Citation Intent? An Experimental Analysis of In-context Learning and Fine-tuning on Open LLMs", "authors": ["Paris Koloveas", "Serafeim Chatzopoulos", "Thanasis Vergoulis", "Christos Tryfonopoulos"], "categories": ["cs.CL", "cs.DL"], "comment": null, "summary": "This work investigates the ability of open Large Language Models (LLMs) to\npredict citation intent through in-context learning and fine-tuning. Unlike\ntraditional approaches relying on domain-specific pre-trained models like\nSciBERT, we demonstrate that general-purpose LLMs can be adapted to this task\nwith minimal task-specific data. We evaluate twelve model variations across\nfive prominent open LLM families using zero-, one-, few-, and many-shot\nprompting. Our experimental study identifies the top-performing model and\nprompting parameters through extensive in-context learning experiments. We then\ndemonstrate the significant impact of task-specific adaptation by fine-tuning\nthis model, achieving a relative F1-score improvement of 8% on the SciCite\ndataset and 4.3% on the ACL-ARC dataset compared to the instruction-tuned\nbaseline. These findings provide valuable insights for model selection and\nprompt engineering. Additionally, we make our end-to-end evaluation framework\nand models openly available for future use."}
{"id": "2502.14662", "pdf": "https://arxiv.org/pdf/2502.14662.pdf", "abs": "https://arxiv.org/abs/2502.14662", "title": "iAgent: LLM Agent as a Shield between User and Recommender Systems", "authors": ["Wujiang Xu", "Yunxiao Shi", "Zujie Liang", "Xuying Ning", "Kai Mei", "Kun Wang", "Xi Zhu", "Min Xu", "Yongfeng Zhang"], "categories": ["cs.CL", "cs.IR"], "comment": "Findings of ACL 2025 and WWW2025@HCRS", "summary": "Traditional recommender systems usually take the user-platform paradigm,\nwhere users are directly exposed under the control of the platform's\nrecommendation algorithms. However, the defect of recommendation algorithms may\nput users in very vulnerable positions under this paradigm. First, many\nsophisticated models are often designed with commercial objectives in mind,\nfocusing on the platform's benefits, which may hinder their ability to protect\nand capture users' true interests. Second, these models are typically optimized\nusing data from all users, which may overlook individual user's preferences.\nDue to these shortcomings, users may experience several disadvantages under the\ntraditional user-platform direct exposure paradigm, such as lack of control\nover the recommender system, potential manipulation by the platform, echo\nchamber effects, or lack of personalization for less active users due to the\ndominance of active users during collaborative learning. Therefore, there is an\nurgent need to develop a new paradigm to protect user interests and alleviate\nthese issues. Recently, some researchers have introduced LLM agents to simulate\nuser behaviors, these approaches primarily aim to optimize platform-side\nperformance, leaving core issues in recommender systems unresolved. To address\nthese limitations, we propose a new user-agent-platform paradigm, where agent\nserves as the protective shield between user and recommender system that\nenables indirect exposure."}
{"id": "2502.14830", "pdf": "https://arxiv.org/pdf/2502.14830.pdf", "abs": "https://arxiv.org/abs/2502.14830", "title": "Middle-Layer Representation Alignment for Cross-Lingual Transfer in Fine-Tuned LLMs", "authors": ["Danni Liu", "Jan Niehues"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "While large language models demonstrate remarkable capabilities at\ntask-specific applications through fine-tuning, extending these benefits across\ndiverse languages is essential for broad accessibility. However, effective\ncross-lingual transfer is hindered by LLM performance gaps across languages and\nthe scarcity of fine-tuning data in many languages. Through analysis of LLM\ninternal representations from over 1,000+ language pairs, we discover that\nmiddle layers exhibit the strongest potential for cross-lingual alignment.\nBuilding on this finding, we propose a middle-layer alignment objective\nintegrated into task-specific training. Our experiments on slot filling,\nmachine translation, and structured text generation show consistent\nimprovements in cross-lingual transfer, especially to lower-resource languages.\nThe method is robust to the choice of alignment languages and generalizes to\nlanguages unseen during alignment. Furthermore, we show that separately trained\nalignment modules can be merged with existing task-specific modules, improving\ncross-lingual capabilities without full re-training. Our code is publicly\navailable (https://github.com/dannigt/mid-align)."}
{"id": "2502.15197", "pdf": "https://arxiv.org/pdf/2502.15197.pdf", "abs": "https://arxiv.org/abs/2502.15197", "title": "TETRIS: Optimal Draft Token Selection for Batch Speculative Decoding", "authors": ["Zhaoxuan Wu", "Zijian Zhou", "Arun Verma", "Alok Prakash", "Daniela Rus", "Bryan Kian Hsiang Low"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 11 figures, 5 tables", "summary": "We propose TETRIS, a novel method that optimizes the total throughput of\nbatch speculative decoding in multi-request settings. Unlike existing methods\nthat optimize for a single request or a group of requests as a whole, TETRIS\nactively selects the most promising draft tokens (for every request in a batch)\nto be accepted when verified in parallel, resulting in fewer rejected tokens\nand hence less wasted computing resources. Such an effective resource\nutilization to achieve fast inference in large language models (LLMs) is\nespecially important to service providers with limited inference capacity.\nCompared to baseline speculative decoding, TETRIS yields a consistently higher\nacceptance rate and more effective utilization of the limited inference\ncapacity. We show theoretically and empirically that TETRIS outperforms\nbaseline speculative decoding and existing methods that dynamically select\ndraft tokens, leading to a more efficient batch inference in LLMs."}
{"id": "2502.15401", "pdf": "https://arxiv.org/pdf/2502.15401.pdf", "abs": "https://arxiv.org/abs/2502.15401", "title": "Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs Complex Reasoning", "authors": ["Xuetao Ma", "Wenbin Jiang", "Hua Huang"], "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 6 figures, ACL 2025 findings, camera-ready version", "summary": "In-context learning (ICL) can significantly enhance the complex reasoning\ncapabilities of large language models (LLMs), with the key lying in the\nselection and ordering of demonstration examples. Previous methods typically\nrelied on simple features to measure the relevance between examples. We argue\nthat these features are not sufficient to reflect the intrinsic connections\nbetween examples. In this study, we propose a curriculum ICL strategy guided by\nproblem-solving logic. We select demonstration examples by analyzing the\nproblem-solving logic and order them based on curriculum learning.\nSpecifically, we constructed a problem-solving logic instruction set based on\nthe BREAK dataset and fine-tuned a language model to analyze the\nproblem-solving logic of examples. Subsequently, we selected appropriate\ndemonstration examples based on problem-solving logic and assessed their\ndifficulty according to the number of problem-solving steps. In accordance with\nthe principles of curriculum learning, we ordered the examples from easy to\nhard to serve as contextual prompts. Experimental results on multiple\nbenchmarks indicate that our method outperforms previous ICL approaches in\nterms of performance and efficiency, effectively enhancing the complex\nreasoning capabilities of LLMs. Our project will be released at\nhttps://github.com/maxuetao/CurriculumICL"}
{"id": "2502.15434", "pdf": "https://arxiv.org/pdf/2502.15434.pdf", "abs": "https://arxiv.org/abs/2502.15434", "title": "Mixup Model Merge: Enhancing Model Merging Performance through Randomized Linear Interpolation", "authors": ["Yue Zhou", "Yi Chang", "Yuan Wu"], "categories": ["cs.CL", "I.2.7; I.2.6"], "comment": "15 pages", "summary": "Model merging aims to integrate multiple task-specific models into a unified\nmodel that inherits the capabilities of the task-specific models, without\nadditional training. Existing model merging methods often lack consideration of\nthe varying contribution ratios of different task-specific models to the final\nmerged model. In this paper, we propose Mixup Model Merge (M3), a simple yet\neffective method inspired by the randomized linear interpolation strategy from\nthe Mixup data augmentation technique. M3 performs randomized linear\ninterpolation in parameter space between two task-specific LLMs, where\ninterpolation coefficients are sampled from a Beta distribution to explore\ndiverse contribution ratios. This controllable randomness allows M3 to\noutperform standard equal-ratio merging by discovering better contribution\nratio combinations. Extensive experiments show that M3 significantly (1)\nimproves merged LLM performance across tasks, (2) enhances out-of-distribution\nand adversarial robustness, and (3) outperforms the positive effects of the\nsparsification method DARE on model merging and can be further combined with\nDARE to achieve superior results. By tuning the Beta distribution's shape\nparameters, (4) M3 balances exploration efficiency and diversity in\ncontribution ratios. The code is available at:\nhttps://github.com/MLGroupJLU/MixupModelMerge"}
{"id": "2502.16487", "pdf": "https://arxiv.org/pdf/2502.16487.pdf", "abs": "https://arxiv.org/abs/2502.16487", "title": "All That Glitters is Not Novel: Plagiarism in AI Generated Research", "authors": ["Tarun Gupta", "Danish Pruthi"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (main) conference", "summary": "Automating scientific research is considered the final frontier of science.\nRecently, several papers claim autonomous research agents can generate novel\nresearch ideas. Amidst the prevailing optimism, we document a critical concern:\na considerable fraction of such research documents are smartly plagiarized.\nUnlike past efforts where experts evaluate the novelty and feasibility of\nresearch ideas, we request $13$ experts to operate under a different\nsituational logic: to identify similarities between LLM-generated research\ndocuments and existing work. Concerningly, the experts identify $24\\%$ of the\n$50$ evaluated research documents to be either paraphrased (with one-to-one\nmethodological mapping), or significantly borrowed from existing work. These\nreported instances are cross-verified by authors of the source papers. Experts\nfind an additional $32\\%$ ideas to partially overlap with prior work, and a\nsmall fraction to be completely original. Problematically, these LLM-generated\nresearch documents do not acknowledge original sources, and bypass inbuilt\nplagiarism detectors. Lastly, through controlled experiments we show that\nautomated plagiarism detectors are inadequate at catching plagiarized ideas\nfrom such systems. We recommend a careful assessment of LLM-generated research,\nand discuss the implications of our findings on academic publishing."}
{"id": "2502.16989", "pdf": "https://arxiv.org/pdf/2502.16989.pdf", "abs": "https://arxiv.org/abs/2502.16989", "title": "All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark", "authors": ["Davide Testa", "Giovanni Bonetta", "Raffaella Bernardi", "Alessandro Bondielli", "Alessandro Lenci", "Alessio Miaschi", "Lucia Passaro", "Bernardo Magnini"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce MAIA (Multimodal AI Assessment), a native-Italian benchmark\ndesigned for fine-grained investigation of the reasoning abilities of visual\nlanguage models on videos. MAIA differs from other available video benchmarks\nfor its design, its reasoning categories, the metric it uses, and the language\nand culture of the videos. MAIA evaluates Vision Language Models (VLMs) on two\naligned tasks: a visual statement verification task and an open-ended visual\nquestion-answering task, both on the same set of video-related questions. It\nconsiders twelve reasoning categories that aim to disentangle language and\nvision relations by highlighting the role of the visual input. Thanks to its\ncarefully taught design, it evaluates VLMs' consistency and visually grounded\nnatural language comprehension and generation simultaneously through an\naggregated metric revealing low results that highlight models' fragility. Last\nbut not least, the video collection has been carefully selected to reflect the\nItalian culture, and the language data are produced by native-speakers."}
{"id": "2502.20864", "pdf": "https://arxiv.org/pdf/2502.20864.pdf", "abs": "https://arxiv.org/abs/2502.20864", "title": "Do Language Models Understand Honorific Systems in Javanese?", "authors": ["Mohammad Rifqi Farhansyah", "Iwan Darmawan", "Adryan Kusumawardhana", "Genta Indra Winata", "Alham Fikri Aji", "Derry Tanti Wijaya"], "categories": ["cs.CL"], "comment": "ACL 2025 - Main Conference", "summary": "The Javanese language features a complex system of honorifics that vary\naccording to the social status of the speaker, listener, and referent. Despite\nits cultural and linguistic significance, there has been limited progress in\ndeveloping a comprehensive corpus to capture these variations for natural\nlanguage processing (NLP) tasks. In this paper, we present Unggah-Ungguh, a\ncarefully curated dataset designed to encapsulate the nuances of Unggah-Ungguh\nBasa, the Javanese speech etiquette framework that dictates the choice of words\nand phrases based on social hierarchy and context. Using Unggah-Ungguh, we\nassess the ability of language models (LMs) to process various levels of\nJavanese honorifics through classification and machine translation tasks. To\nfurther evaluate cross-lingual LMs, we conduct machine translation experiments\nbetween Javanese (at specific honorific levels) and Indonesian. Additionally,\nwe explore whether LMs can generate contextually appropriate Javanese\nhonorifics in conversation tasks, where the honorific usage should align with\nthe social role and contextual cues. Our findings indicate that current LMs\nstruggle with most honorific levels, exhibitinga bias toward certain honorific\ntiers."}
{"id": "2503.01372", "pdf": "https://arxiv.org/pdf/2503.01372.pdf", "abs": "https://arxiv.org/abs/2503.01372", "title": "SwiLTra-Bench: The Swiss Legal Translation Benchmark", "authors": ["Joel Niklaus", "Jakob Merane", "Luka Nenadic", "Sina Ahmadi", "Yingqiang Gao", "Cyrill A. H. Chevalley", "Claude Humbel", "Christophe Gösken", "Lorenzo Tanzi", "Thomas Lüthi", "Stefan Palombo", "Spencer Poff", "Boling Yang", "Nan Wu", "Matthew Guillod", "Robin Mamié", "Daniel Brunner", "Julio Pereyra", "Niko Grupen"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "comment": "Accepted at ACL main 2025", "summary": "In Switzerland legal translation is uniquely important due to the country's\nfour official languages and requirements for multilingual legal documentation.\nHowever, this process traditionally relies on professionals who must be both\nlegal experts and skilled translators -- creating bottlenecks and impacting\neffective access to justice. To address this challenge, we introduce\nSwiLTra-Bench, a comprehensive multilingual benchmark of over 180K aligned\nSwiss legal translation pairs comprising laws, headnotes, and press releases\nacross all Swiss languages along with English, designed to evaluate LLM-based\ntranslation systems. Our systematic evaluation reveals that frontier models\nachieve superior translation performance across all document types, while\nspecialized translation systems excel specifically in laws but under-perform in\nheadnotes. Through rigorous testing and human expert validation, we demonstrate\nthat while fine-tuning open SLMs significantly improves their translation\nquality, they still lag behind the best zero-shot prompted frontier models such\nas Claude-3.5-Sonnet. Additionally, we present SwiLTra-Judge, a specialized LLM\nevaluation system that aligns best with human expert assessments."}
{"id": "2503.01606", "pdf": "https://arxiv.org/pdf/2503.01606.pdf", "abs": "https://arxiv.org/abs/2503.01606", "title": "Beyond Prompting: An Efficient Embedding Framework for Open-Domain Question Answering", "authors": ["Zhanghao Hu", "Hanqi Yan", "Qinglin Zhu", "Zhenyi Shen", "Yulan He", "Lin Gui"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "Accepted in ACL 2025 Main", "summary": "Large language models have recently pushed open domain question answering\n(ODQA) to new frontiers. However, prevailing retriever-reader pipelines often\ndepend on multiple rounds of prompt level instructions, leading to high\ncomputational overhead, instability, and suboptimal retrieval coverage. In this\npaper, we propose EmbQA, an embedding-level framework that alleviates these\nshortcomings by enhancing both the retriever and the reader. Specifically, we\nrefine query representations via lightweight linear layers under an\nunsupervised contrastive learning objective, thereby reordering retrieved\npassages to highlight those most likely to contain correct answers.\nAdditionally, we introduce an exploratory embedding that broadens the model's\nlatent semantic space to diversify candidate generation and employs an\nentropy-based selection mechanism to choose the most confident answer\nautomatically. Extensive experiments across three open-source LLMs, three\nretrieval methods, and four ODQA benchmarks demonstrate that EmbQA\nsubstantially outperforms recent baselines in both accuracy and efficiency."}
{"id": "2503.03854", "pdf": "https://arxiv.org/pdf/2503.03854.pdf", "abs": "https://arxiv.org/abs/2503.03854", "title": "Vision-Language Models Struggle to Align Entities across Modalities", "authors": ["Iñigo Alonso", "Gorka Azkune", "Ander Salaberria", "Jeremy Barnes", "Oier Lopez de Lacalle"], "categories": ["cs.CL"], "comment": "Accepted Findings ACL 2025", "summary": "Cross-modal entity linking refers to the ability to align entities and their\nattributes across different modalities. While cross-modal entity linking is a\nfundamental skill needed for real-world applications such as multimodal code\ngeneration, fake news detection, or scene understanding, it has not been\nthoroughly studied in the literature. In this paper, we introduce a new task\nand benchmark to address this gap. Our benchmark, MATE, consists of 5.5k\nevaluation instances featuring visual scenes aligned with their textual\nrepresentations. To evaluate cross-modal entity linking performance, we design\na question-answering task that involves retrieving one attribute of an object\nin one modality based on a unique attribute of that object in another modality.\nWe evaluate state-of-the-art Vision-Language Models (VLMs) and humans on this\ntask, and find that VLMs struggle significantly compared to humans,\nparticularly as the number of objects in the scene increases. Our analysis also\nshows that, while chain-of-thought prompting can improve VLM performance,\nmodels remain far from achieving human-level proficiency. These findings\nhighlight the need for further research in cross-modal entity linking and show\nthat MATE is a strong benchmark to support that progress."}
{"id": "2503.04378", "pdf": "https://arxiv.org/pdf/2503.04378.pdf", "abs": "https://arxiv.org/abs/2503.04378", "title": "HelpSteer3: Human-Annotated Feedback and Edit Data to Empower Inference-Time Scaling in Open-Ended General-Domain Tasks", "authors": ["Zhilin Wang", "Jiaqi Zeng", "Olivier Delalleau", "Daniel Egert", "Ellie Evans", "Hoo-Chang Shin", "Felipe Soares", "Yi Dong", "Oleksii Kuchaiev"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages, 2 figures, Accepted to ACL 2025 Main", "summary": "Inference-Time Scaling has been critical to the success of recent models such\nas OpenAI o1 and DeepSeek R1. However, many techniques used to train models for\ninference-time scaling require tasks to have answers that can be verified,\nlimiting their application to domains such as math, coding and logical\nreasoning. We take inspiration from how humans make first attempts, ask for\ndetailed feedback from others and make improvements based on such feedback\nacross a wide spectrum of open-ended endeavors. To this end, we collect\nHelpSteer3 data to train dedicated Feedback and Edit Models that are capable of\nperforming inference-time scaling for open-ended general-domain tasks. In our\nsetup, one model generates an initial response, which are given feedback by a\nsecond model, that are then used by a third model to edit the response. We show\nthat performance on Arena Hard, a benchmark strongly predictive of Chatbot\nArena Elo can be boosted by scaling the number of initial response drafts,\neffective feedback and edited responses. When scaled optimally, our setup based\non 70B models from the Llama 3 family can reach SoTA performance on Arena Hard\nat 92.7 as of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and\nDeepSeek R1 with 92.3."}
{"id": "2503.05268", "pdf": "https://arxiv.org/pdf/2503.05268.pdf", "abs": "https://arxiv.org/abs/2503.05268", "title": "ZOGRASCOPE: A New Benchmark for Semantic Parsing over Property Graphs", "authors": ["Francesco Cazzaro", "Justin Kleindienst", "Sofia Marquez Gomez", "Ariadna Quattoni"], "categories": ["cs.CL"], "comment": null, "summary": "In recent years, the need for natural language interfaces to knowledge graphs\nhas become increasingly important since they enable easy and efficient access\nto the information contained in them. In particular, property graphs (PGs) have\nseen increased adoption as a means of representing complex structured\ninformation. Despite their growing popularity in industry, PGs remain\nrelatively underrepresented in semantic parsing research with a lack of\nresources for evaluation. To address this gap, we introduce ZOGRASCOPE, a\nbenchmark designed specifically for PGs and queries written in Cypher. Our\nbenchmark includes a diverse set of manually annotated queries of varying\ncomplexity and is organized into three partitions: iid, compositional and\nlength. We complement this paper with a set of experiments that test the\nperformance of different LLMs in a variety of learning settings."}
{"id": "2503.07067", "pdf": "https://arxiv.org/pdf/2503.07067.pdf", "abs": "https://arxiv.org/abs/2503.07067", "title": "DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs", "authors": ["Jongwoo Ko", "Tianyi Chen", "Sungnyun Kim", "Tianyu Ding", "Luming Liang", "Ilya Zharkov", "Se-Young Yun"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML2025 Spotlight", "summary": "Despite the success of distillation in large language models (LLMs), most\nprior work applies identical loss functions to both teacher- and\nstudent-generated data. These strategies overlook the synergy between loss\nformulations and data types, leading to a suboptimal performance boost in\nstudent models. To address this, we propose DistiLLM-2, a contrastive approach\nthat simultaneously increases the likelihood of teacher responses and decreases\nthat of student responses by harnessing this synergy. Our extensive experiments\nshow that DistiLLM-2 not only builds high-performing student models across a\nwide range of tasks, including instruction-following and code generation, but\nalso supports diverse applications, such as preference alignment and\nvision-language extensions. These findings highlight the potential of a\ncontrastive approach to enhance the efficacy of LLM distillation by effectively\naligning teacher and student models across varied data types."}
{"id": "2503.15289", "pdf": "https://arxiv.org/pdf/2503.15289.pdf", "abs": "https://arxiv.org/abs/2503.15289", "title": "TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence Tracing and Relationship Classification", "authors": ["Junnan Zhu", "Min Xiao", "Yining Wang", "Feifei Zhai", "Yu Zhou", "Chengqing Zong"], "categories": ["cs.CL"], "comment": "To appear in ACL 2025 (Main)", "summary": "LLMs have achieved remarkable fluency and coherence in text generation, yet\ntheir widespread adoption has raised concerns about content reliability and\naccountability. In high-stakes domains, it is crucial to understand where and\nhow the content is created. To address this, we introduce the Text pROVEnance\n(TROVE) challenge, designed to trace each sentence of a target text back to\nspecific source sentences within potentially lengthy or multi-document inputs.\nBeyond identifying sources, TROVE annotates the fine-grained relationships\n(quotation, compression, inference, and others), providing a deep understanding\nof how each target sentence is formed. To benchmark TROVE, we construct our\ndataset by leveraging three public datasets covering 11 diverse scenarios\n(e.g., QA and summarization) in English and Chinese, spanning source texts of\nvarying lengths (0-5k, 5-10k, 10k+), emphasizing the multi-document and\nlong-document settings essential for provenance. To ensure high-quality data,\nwe employ a three-stage annotation process: sentence retrieval, GPT-4o\nprovenance, and human provenance. We evaluate 11 LLMs under direct prompting\nand retrieval-augmented paradigms, revealing that retrieval is essential for\nrobust performance, larger models perform better in complex relationship\nclassification, and closed-source models often lead, yet open-source models\nshow significant promise, particularly with retrieval augmentation. We make our\ndataset available here: https://github.com/ZNLP/ZNLP-Dataset."}
{"id": "2503.18491", "pdf": "https://arxiv.org/pdf/2503.18491.pdf", "abs": "https://arxiv.org/abs/2503.18491", "title": "MAGIC-VQA: Multimodal And Grounded Inference with Commonsense Knowledge for Visual Question Answering", "authors": ["Shuo Yang", "Siwen Luo", "Soyeon Caren Han", "Eduard Hovy"], "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "Visual Question Answering (VQA) requires reasoning across visual and textual\nmodalities, yet Large Vision-Language Models (LVLMs) often lack integrated\ncommonsense knowledge, limiting their robustness in real-world scenarios. To\naddress this, we introduce MAGIC-VQA, a novel framework that enhances VQA by\nsystematically integrating commonsense knowledge with LVLMs. MAGIC-VQA employs\na three-stage process: (1) Explicit Knowledge Integration from external\nsources, (2) By-Type Post-Processing for contextual refinement, and (3)\nImplicit Knowledge Augmentation using a Graph Neural Network (GNN) for\nstructured reasoning. While GNNs bring greater depth to structured inference,\nthey enable superior relational inference beyond LVLMs. MAGIC-VQA bridges a key\ngap by unifying commonsensse knowledge with LVLM-driven reasoning, eliminating\nthe need for extensive pre-training or complex prompt tuning. Our framework\nachieves state-of-the-art performance on benchmark datasets, significantly\nimproving commonsense reasoning in VQA."}
{"id": "2503.18991", "pdf": "https://arxiv.org/pdf/2503.18991.pdf", "abs": "https://arxiv.org/abs/2503.18991", "title": "Inverse Reinforcement Learning with Dynamic Reward Scaling for LLM Alignment", "authors": ["Ruoxi Cheng", "Haoxuan Ma", "Weixin Wang", "Zhiqiang Wang", "Xiaoshuang Jia", "Simeng Qin", "Xiaochun Cao", "Yang Liu", "Xiaojun Jia"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "The first three authors contributed equally to this work", "summary": "Robust alignment is vital for safely deploying large language models (LLMs).\nExisting techniques are either reward-based -- training a reward model on\npreference pairs and optimizing with reinforcement learning (RL) -- or\nreward-free -- directly fine-tuning on ranked outputs. Recent research shows\nthat well-tuned reward-based pipelines remain the most robust, and\nsingle-response demonstrations can outperform pairwise preference data.\nHowever, two key challenges remain: (i) imbalanced safety datasets that\nover-represent common hazards while neglecting long-tail threats; and (ii)\nstatic reward models that ignore task difficulty, limiting optimization\nefficiency and attainable gains. To address these limitations, we propose\n\\textbf{DR-IRL}, which dynamically adjusts rewards through inverse\nreinforcement learning. We first construct a balanced safety dataset of seven\nharmful categories using Chain-of-Draft (CoD) template prompts, which reduce\ntoken usage and generation time compared to Chain-of-Thought (CoT). We then\ntrain category-specific reward models on this dataset via IRL. Finally, to\nalign the LLM, we introduce \\textbf{GRPO-S} (Group Relative Policy\nOptimization--Scaling), a variant of GRPO that scales the reward during\noptimization to task difficulty -- data-level hardness measured by CLIP\nsimilarity and model-level responsiveness measured by reward gaps. Extensive\nexperiments on multiple benchmarks and LLMs demonstrate that DR-IRL outperforms\nall baselines in safety alignment while maintaining usefulness."}
{"id": "2503.22353", "pdf": "https://arxiv.org/pdf/2503.22353.pdf", "abs": "https://arxiv.org/abs/2503.22353", "title": "Firm or Fickle? Evaluating Large Language Models Consistency in Sequential Interactions", "authors": ["Yubo Li", "Yidi Miao", "Xueying Ding", "Ramayya Krishnan", "Rema Padman"], "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 5 figures", "summary": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious tasks, but their deployment in high-stake domains requires consistent\nand coherent behavior across multiple rounds of user interaction. This paper\nintroduces a comprehensive framework for evaluating and improving LLM response\nconsistency, making three key contributions. Code and data are available at:\nhttps://github.com/yubol-bobo/MT-Consistency. First, we introduce\nPosition-Weighted Consistency (PWC), a metric designed to capture both the\nimportance of early-stage stability and recovery patterns in multi-turn\ninteractions. Second, we present MT-Consistency, a carefully curated benchmark\ndataset spanning diverse domains and difficulty levels, specifically designed\nto evaluate LLM consistency under various challenging follow-up scenarios.\nThird, we introduce Confidence-Aware Response Generation (CARG), a framework\nthat significantly improves response stability by explicitly integrating\ninternal model confidence scores during the generation process. Experimental\nresults demonstrate that CARG significantly improves response stability without\nsacrificing accuracy, offering a practical path toward more dependable LLM\nbehavior in critical, real-world deployments."}
{"id": "2504.04042", "pdf": "https://arxiv.org/pdf/2504.04042.pdf", "abs": "https://arxiv.org/abs/2504.04042", "title": "An Explicit Syllogistic Legal Reasoning Framework for Large Language Models", "authors": ["Kepu Zhang", "Weijie Yu", "Zhongxiang Sun", "Jun Xu"], "categories": ["cs.CL"], "comment": null, "summary": "Syllogistic reasoning is crucial for sound legal decision-making, allowing\nlegal professionals to draw logical conclusions by applying general principles\nto specific case facts. While large language models (LLMs) can answer legal\nquestions, they often struggle with explicit syllogistic reasoning. Their\noutputs tend to be implicit, unstructured, and consequently, less explainable\nand trustworthy. To overcome these limitations, we introduce SyLeR, a novel\nframework designed to enable LLMs to perform explicit syllogistic legal\nreasoning. SyLeR employs a tree-structured hierarchical retrieval mechanism to\nsynthesize relevant legal statutes and precedents, thereby constructing\ncomprehensive major premises. This is followed by a two-stage fine-tuning\nprocess: an initial supervised fine-tuning warm-up establishes a foundational\nunderstanding of syllogistic reasoning, while reinforcement learning, guided by\na structure-aware reward mechanism, refines the model's capacity to generate\ndiverse, logically sound, and well-structured reasoning paths. We conducted\nextensive experiments to evaluate SyLeR's performance. Our evaluations spanned\ndiverse dimensions, including both in-domain and cross-domain user groups\n(legal laypersons and practitioners), multiple languages (Chinese and French),\nand various LLM backbones (legal-specific and open-domain LLMs). The results\nconsistently demonstrate that SyLeR significantly enhances response accuracy\nand reliably produces explicit, explainable, and trustworthy legal reasoning."}
{"id": "2504.07282", "pdf": "https://arxiv.org/pdf/2504.07282.pdf", "abs": "https://arxiv.org/abs/2504.07282", "title": "RAISE: Reinforced Adaptive Instruction Selection For Large Language Models", "authors": ["Lv Qingsong", "Yangning Li", "Zihua Lan", "Zishan Xu", "Jiwei Tang", "Yinghui Li", "Wenhao Jiang", "Hai-Tao Zheng", "Philip S. Yu"], "categories": ["cs.CL"], "comment": null, "summary": "In the instruction fine-tuning of large language models (LLMs), it is widely\nrecognized that a few high-quality instructions are superior to a large number\nof low-quality instructions. At present, many instruction selection methods\nhave been proposed, but most of these methods select instruction based on\nheuristic quality metrics, and only consider data selection before training.\nThese designs lead to insufficient optimization of instruction fine-tuning, and\nfixed heuristic indicators are often difficult to optimize for specific tasks.\nTherefore, we design a dynamic, task-objective-driven instruction selection\nframework RAISE(Reinforced Adaptive Instruction SElection), which incorporates\nthe entire instruction fine-tuning process into optimization, selecting\ninstructions at each step based on the expected impact of each instruction on\nmodel performance improvement. Our approach is well interpretable and has\nstrong task-specific optimization capabilities. By modeling dynamic instruction\nselection as a sequential decision-making process, we use RL to train our\nselection strategy. Extensive experiments and result analysis prove the\nsuperiority of our method compared with other instruction selection methods.\nNotably, RAISE achieves superior performance by updating only 1% of the\ntraining steps compared to full-data training, demonstrating its efficiency and\neffectiveness."}
{"id": "2504.08120", "pdf": "https://arxiv.org/pdf/2504.08120.pdf", "abs": "https://arxiv.org/abs/2504.08120", "title": "DeepSeek-R1 vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and Summarization?", "authors": ["Daniil Larionov", "Sotaro Takeshita", "Ran Zhang", "Yanran Chen", "Christoph Leiter", "Zhipin Wang", "Christian Greisinger", "Steffen Eger"], "categories": ["cs.CL"], "comment": null, "summary": "Reasoning-enabled large language models (LLMs) excel in logical tasks, yet\ntheir utility for evaluating natural language generation remains unexplored.\nThis study systematically compares reasoning LLMs with non-reasoning\ncounterparts across machine translation and text summarization evaluation\ntasks. We evaluate eight models spanning state-of-the-art reasoning models\n(DeepSeek-R1, OpenAI o3), their distilled variants (8B-70B parameters), and\nequivalent non-reasoning LLMs. Experiments on WMT23 and SummEval benchmarks\nreveal architecture and task-dependent benefits: OpenAI o3-mini models show\nimproved performance with increased reasoning on MT, while DeepSeek-R1 and\ngenerally underperforms compared to its non-reasoning variant except in\nsummarization consistency evaluation. Correlation analysis demonstrates that\nreasoning token usage correlates with evaluation quality only in specific\nmodels, while almost all models generally allocate more reasoning tokens when\nidentifying more quality issues. Distillation maintains reasonable performance\nup to 32B parameter models but degrades substantially at 8B scale. This work\nprovides the first assessment of reasoning LLMs for NLG evaluation and\ncomparison to non-reasoning models. We share our code to facilitate further\nresearch: https://github.com/NL2G/reasoning-eval."}
{"id": "2504.10792", "pdf": "https://arxiv.org/pdf/2504.10792.pdf", "abs": "https://arxiv.org/abs/2504.10792", "title": "GUM-SAGE: A Novel Dataset and Approach for Graded Entity Salience Prediction", "authors": ["Jessica Lin", "Amir Zeldes"], "categories": ["cs.CL"], "comment": "Camera-ready for ACL Findings 2025", "summary": "Determining and ranking the most salient entities in a text is critical for\nuser-facing systems, especially as users increasingly rely on models to\ninterpret long documents they only partially read. Graded entity salience\naddresses this need by assigning entities scores that reflect their relative\nimportance in a text. Existing approaches fall into two main categories:\nsubjective judgments of salience, which allow for gradient scoring but lack\nconsistency, and summarization-based methods, which define salience as\nmention-worthiness in a summary, promoting explainability but limiting outputs\nto binary labels (entities are either summary-worthy or not). In this paper, we\nintroduce a novel approach for graded entity salience that combines the\nstrengths of both approaches. Using an English dataset spanning 12 spoken and\nwritten genres, we collect 5 summaries per document and calculate each entity's\nsalience score based on its presence across these summaries. Our approach shows\nstronger correlation with scores based on human summaries and alignments, and\noutperforms existing techniques, including LLMs. We release our data and code\nat https://github.com/jl908069/gum_sum_salience to support further research on\ngraded salient entity extraction."}
{"id": "2504.11277", "pdf": "https://arxiv.org/pdf/2504.11277.pdf", "abs": "https://arxiv.org/abs/2504.11277", "title": "From Misleading Queries to Accurate Answers: A Three-Stage Fine-Tuning Method for LLMs", "authors": ["Guocong Li", "Weize Liu", "Yihang Wu", "Ping Wang", "Shuaihan Huang", "Hongxia Xu", "Jian Wu"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Findings)", "summary": "Large language models (LLMs) exhibit excellent performance in natural\nlanguage processing (NLP), but remain highly sensitive to the quality of input\nqueries, especially when these queries contain misleading or inaccurate\ninformation. Existing methods focus on correcting the output, but they often\noverlook the potential of improving the ability of LLMs to detect and correct\nmisleading content in the input itself. In this paper, we propose a novel\nthree-stage fine-tuning method that enhances the ability of LLMs to detect and\ncorrect misleading information in the input, further improving response\naccuracy and reducing hallucinations. Specifically, the three stages include\n(1) training LLMs to identify misleading information, (2) training LLMs to\ncorrect the misleading information using built-in or external knowledge, and\n(3) training LLMs to generate accurate answers based on the corrected queries.\nTo evaluate our method, we conducted experiments on three datasets for the\nhallucination detection task and the question answering~(QA) task, as well as\ntwo datasets containing misleading information that we constructed. The\nexperimental results demonstrate that our method significantly improves the\naccuracy and factuality of LLM responses, while also enhancing the ability to\ndetect hallucinations and reducing the generation of hallucinations in the\noutput, particularly when the query contains misleading information."}
{"id": "2504.15815", "pdf": "https://arxiv.org/pdf/2504.15815.pdf", "abs": "https://arxiv.org/abs/2504.15815", "title": "What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns", "authors": ["Michael A. Hedderich", "Anyi Wang", "Raoyuan Zhao", "Florian Eichin", "Jonas Fischer", "Barbara Plank"], "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": "Accepted at ACL'25", "summary": "Prompt engineering for large language models is challenging, as even small\nprompt perturbations or model changes can significantly impact the generated\noutput texts. Existing evaluation methods of LLM outputs, either automated\nmetrics or human evaluation, have limitations, such as providing limited\ninsights or being labor-intensive. We propose Spotlight, a new approach that\ncombines both automation and human analysis. Based on data mining techniques,\nwe automatically distinguish between random (decoding) variations and\nsystematic differences in language model outputs. This process provides token\npatterns that describe the systematic differences and guide the user in\nmanually analyzing the effects of their prompts and changes in models\nefficiently. We create three benchmarks to quantitatively test the reliability\nof token pattern extraction methods and demonstrate that our approach provides\nnew insights into established prompt data. From a human-centric perspective,\nthrough demonstration studies and a user study, we show that our token pattern\napproach helps users understand the systematic differences of language model\noutputs. We are further able to discover relevant differences caused by prompt\nand model changes (e.g. related to gender or culture), thus supporting the\nprompt engineering process and human-centric model behavior research."}
{"id": "2505.06186", "pdf": "https://arxiv.org/pdf/2505.06186.pdf", "abs": "https://arxiv.org/abs/2505.06186", "title": "Query-driven Document-level Scientific Evidence Extraction from Biomedical Studies", "authors": ["Massimiliano Pronesti", "Joao Bettencourt-Silva", "Paul Flanagan", "Alessandra Pascale", "Oisin Redmond", "Anya Belz", "Yufang Hou"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 Main Conference", "summary": "Extracting scientific evidence from biomedical studies for clinical research\nquestions (e.g., Does stem cell transplantation improve quality of life in\npatients with medically refractory Crohn's disease compared to placebo?) is a\ncrucial step in synthesising biomedical evidence. In this paper, we focus on\nthe task of document-level scientific evidence extraction for clinical\nquestions with conflicting evidence. To support this task, we create a dataset\ncalled CochraneForest, leveraging forest plots from Cochrane systematic\nreviews. It comprises 202 annotated forest plots, associated clinical research\nquestions, full texts of studies, and study-specific conclusions. Building on\nCochraneForest, we propose URCA (Uniform Retrieval Clustered Augmentation), a\nretrieval-augmented generation framework designed to tackle the unique\nchallenges of evidence extraction. Our experiments show that URCA outperforms\nthe best existing methods by up to 10.3% in F1 score on this task. However, the\nresults also underscore the complexity of CochraneForest, establishing it as a\nchallenging testbed for advancing automated evidence synthesis systems."}
{"id": "2505.13772", "pdf": "https://arxiv.org/pdf/2505.13772.pdf", "abs": "https://arxiv.org/abs/2505.13772", "title": "Krikri: Advancing Open Large Language Models for Greek", "authors": ["Dimitris Roussis", "Leon Voukoutis", "Georgios Paraskevopoulos", "Sokratis Sofianopoulos", "Prokopis Prokopidis", "Vassilis Papavasileiou", "Athanasios Katsamanis", "Stelios Piperidis", "Vassilis Katsouros"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce Llama-Krikri-8B, a cutting-edge Large Language Model tailored\nfor the Greek language, built on Meta's Llama 3.1-8B. Llama-Krikri-8B has been\nextensively trained on high-quality Greek data to ensure superior adaptation to\nlinguistic nuances. With 8 billion parameters, it offers advanced capabilities\nwhile maintaining efficient computational performance. Llama-Krikri-8B supports\nboth Modern Greek and English, and is also equipped to handle polytonic text\nand Ancient Greek. The chat version of Llama-Krikri-8B features a multi-stage\npost-training pipeline, utilizing both human and synthetic instruction and\npreference data, by applying techniques such as MAGPIE. In addition, for\nevaluation, we propose three novel public benchmarks for Greek. Our evaluation\non existing as well as the proposed benchmarks shows notable improvements over\ncomparable Greek and multilingual LLMs in both natural language understanding\nand generation as well as code generation."}
{"id": "2505.14079", "pdf": "https://arxiv.org/pdf/2505.14079.pdf", "abs": "https://arxiv.org/abs/2505.14079", "title": "BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks", "authors": ["Weihong Du", "Wenrui Liao", "Binyu Yan", "Hongru Liang", "Anthony G. Cohn", "Wenqiang Lei"], "categories": ["cs.CL"], "comment": null, "summary": "Large language model (LLM) based agents have shown great potential in\nfollowing human instructions and automatically completing various tasks. To\ncomplete a task, the agent needs to decompose it into easily executed steps by\nplanning. Existing studies mainly conduct the planning by inferring what steps\nshould be executed next starting from the agent's initial state. However, this\nforward reasoning paradigm doesn't work well for complex tasks. We propose to\nstudy this issue in Minecraft, a virtual environment that simulates complex\ntasks based on real-world scenarios. We believe that the failure of forward\nreasoning is caused by the big perception gap between the agent's initial state\nand task goal. To this end, we leverage backward reasoning and make the\nplanning starting from the terminal state, which can directly achieve the task\ngoal in one step. Specifically, we design a BAckward Reasoning based agent\n(BAR). It is equipped with a recursive goal decomposition module, a state\nconsistency maintaining module and a stage memory module to make robust,\nconsistent, and efficient planning starting from the terminal state.\nExperimental results demonstrate the superiority of BAR over existing methods\nand the effectiveness of proposed modules."}
{"id": "2505.14590", "pdf": "https://arxiv.org/pdf/2505.14590.pdf", "abs": "https://arxiv.org/abs/2505.14590", "title": "MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol", "authors": ["Huihao Jing", "Haoran Li", "Wenbin Hu", "Qi Hu", "Heli Xu", "Tianshu Chu", "Peizhao Hu", "Yangqiu Song"], "categories": ["cs.CL"], "comment": "17 pages", "summary": "As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users\nand developers, it also brings underexplored safety risks. Its decentralized\narchitecture, which separates clients and servers, poses unique challenges for\nsystematic safety analysis. This paper proposes a novel framework to enhance\nMCP safety. Guided by the MAESTRO framework, we first analyze the missing\nsafety mechanisms in MCP, and based on this analysis, we propose the Model\nContextual Integrity Protocol (MCIP), a refined version of MCP that addresses\nthese gaps. Next, we develop a fine-grained taxonomy that captures a diverse\nrange of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,\nwe develop benchmark and training data that support the evaluation and\nimprovement of LLMs' capabilities in identifying safety risks within MCP\ninteractions. Leveraging the proposed benchmark and training data, we conduct\nextensive experiments on state-of-the-art LLMs. The results highlight LLMs'\nvulnerabilities in MCP interactions and demonstrate that our approach\nsubstantially improves their safety performance."}
{"id": "2505.14874", "pdf": "https://arxiv.org/pdf/2505.14874.pdf", "abs": "https://arxiv.org/abs/2505.14874", "title": "Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages", "authors": ["Chin-Jou Li", "Eunjung Yeo", "Kwanghee Choi", "Paula Andrea Pérez-Toro", "Masao Someki", "Rohan Kumar Das", "Zhengjun Yue", "Juan Rafael Orozco-Arroyave", "Elmar Nöth", "David R. Mortensen"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "5 pages, 1 figure, Accepted to Interspeech 2025", "summary": "Automatic speech recognition (ASR) for dysarthric speech remains challenging\ndue to data scarcity, particularly in non-English languages. To address this,\nwe fine-tune a voice conversion model on English dysarthric speech (UASpeech)\nto encode both speaker characteristics and prosodic distortions, then apply it\nto convert healthy non-English speech (FLEURS) into non-English dysarthric-like\nspeech. The generated data is then used to fine-tune a multilingual ASR model,\nMassively Multilingual Speech (MMS), for improved dysarthric speech\nrecognition. Evaluation on PC-GITA (Spanish), EasyCall (Italian), and SSNCE\n(Tamil) demonstrates that VC with both speaker and prosody conversion\nsignificantly outperforms the off-the-shelf MMS performance and conventional\naugmentation techniques such as speed and tempo perturbation. Objective and\nsubjective analyses of the generated data further confirm that the generated\nspeech simulates dysarthric characteristics."}
{"id": "2505.16491", "pdf": "https://arxiv.org/pdf/2505.16491.pdf", "abs": "https://arxiv.org/abs/2505.16491", "title": "LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing", "authors": ["Dario Di Palma", "Alessandro De Bellis", "Giovanni Servedio", "Vito Walter Anelli", "Fedelucio Narducci", "Tommaso Di Noia"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have rapidly become central to NLP,\ndemonstrating their ability to adapt to various tasks through prompting\ntechniques, including sentiment analysis. However, we still have a limited\nunderstanding of how these models capture sentiment-related information. This\nstudy probes the hidden layers of Llama models to pinpoint where sentiment\nfeatures are most represented and to assess how this affects sentiment\nanalysis.\n  Using probe classifiers, we analyze sentiment encoding across layers and\nscales, identifying the layers and pooling methods that best capture sentiment\nsignals. Our results show that sentiment information is most concentrated in\nmid-layers for binary polarity tasks, with detection accuracy increasing up to\n14% over prompting techniques. Additionally, we find that in decoder-only\nmodels, the last token is not consistently the most informative for sentiment\nencoding. Finally, this approach enables sentiment tasks to be performed with\nmemory requirements reduced by an average of 57%.\n  These insights contribute to a broader understanding of sentiment in LLMs,\nsuggesting layer-specific probing as an effective approach for sentiment tasks\nbeyond prompting, with potential to enhance model utility and reduce memory\nrequirements."}
{"id": "2505.16520", "pdf": "https://arxiv.org/pdf/2505.16520.pdf", "abs": "https://arxiv.org/abs/2505.16520", "title": "Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs", "authors": ["Giovanni Servedio", "Alessandro De Bellis", "Dario Di Palma", "Vito Walter Anelli", "Tommaso Di Noia"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Factual hallucinations are a major challenge for Large Language Models\n(LLMs). They undermine reliability and user trust by generating inaccurate or\nfabricated content. Recent studies suggest that when generating false\nstatements, the internal states of LLMs encode information about truthfulness.\nHowever, these studies often rely on synthetic datasets that lack realism,\nwhich limits generalization when evaluating the factual accuracy of text\ngenerated by the model itself. In this paper, we challenge the findings of\nprevious work by investigating truthfulness encoding capabilities, leading to\nthe generation of a more realistic and challenging dataset. Specifically, we\nextend previous work by introducing: (1) a strategy for sampling plausible\ntrue-false factoid sentences from tabular data and (2) a procedure for\ngenerating realistic, LLM-dependent true-false datasets from Question Answering\ncollections. Our analysis of two open-source LLMs reveals that while the\nfindings from previous studies are partially validated, generalization to\nLLM-generated datasets remains challenging. This study lays the groundwork for\nfuture research on factuality in LLMs and offers practical guidelines for more\neffective evaluation."}
{"id": "2505.17076", "pdf": "https://arxiv.org/pdf/2505.17076.pdf", "abs": "https://arxiv.org/abs/2505.17076", "title": "Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English", "authors": ["Haoyang Zhang", "Hexin Liu", "Xiangyu Zhang", "Qiquan Zhang", "Yuchen Hu", "Junqi Zhao", "Fei Tian", "Xuerui Yang", "Eng Siong Chng"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS", "68T10", "I.2.7"], "comment": "6 pages, 5 figures", "summary": "The speech tokenizer plays a crucial role in recent speech tasks, generally\nserving as a bridge between speech signals and language models. While\nlow-frame-rate codecs are widely employed as speech tokenizers, the impact of\nframe rates on speech tokens remains underexplored. In this study, we\ninvestigate how varying frame rates affect speech tokenization by examining\nMandarin and English, two typologically distinct languages. We encode speech at\ndifferent frame rates and evaluate the resulting semantic tokens in the speech\nrecognition task. Our findings reveal that frame rate variations influence\nspeech tokenization differently for each language, highlighting the interplay\nbetween frame rates, phonetic density, and language-specific acoustic features.\nThe results provide insights into optimizing frame rate selection for speech\ntokenizers, with implications for automatic speech recognition, text-to-speech,\nand other speech-related applications."}
{"id": "2505.17139", "pdf": "https://arxiv.org/pdf/2505.17139.pdf", "abs": "https://arxiv.org/abs/2505.17139", "title": "EarthSE: A Benchmark for Evaluating Earth Scientific Exploration Capability of LLMs", "authors": ["Wanghan Xu", "Xiangyu Zhao", "Yuhao Zhou", "Xiaoyu Yue", "Ben Fei", "Fenghua Ling", "Wenlong Zhang", "Lei Bai"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Advancements in Large Language Models (LLMs) drive interest in scientific\napplications, necessitating specialized benchmarks such as Earth science.\nExisting benchmarks either present a general science focus devoid of Earth\nscience specificity or cover isolated subdomains, lacking holistic evaluation.\nFurthermore, current benchmarks typically neglect the assessment of LLMs'\ncapabilities in open-ended scientific exploration. In this paper, we present a\ncomprehensive and professional benchmark for the Earth sciences, designed to\nevaluate the capabilities of LLMs in scientific exploration within this domain,\nspanning from fundamental to advanced levels. Leveraging a corpus of 100,000\nresearch papers, we first construct two Question Answering (QA) datasets:\nEarth-Iron, which offers extensive question coverage for broad assessment, and\nEarth-Silver, which features a higher level of difficulty to evaluate\nprofessional depth. These datasets encompass five Earth spheres, 114\ndisciplines, and 11 task categories, assessing foundational knowledge crucial\nfor scientific exploration. Most notably, we introduce Earth-Gold with new\nmetrics, a dataset comprising open-ended multi-turn dialogues specifically\ndesigned to evaluate the advanced capabilities of LLMs in scientific\nexploration, including methodology induction, limitation analysis, and concept\nproposal. Extensive experiments reveal limitations in 11 leading LLMs across\ndifferent domains and tasks, highlighting considerable room for improvement in\ntheir scientific exploration capabilities. The benchmark is available on\nhttps://huggingface.co/ai-earth ."}
{"id": "2505.17873", "pdf": "https://arxiv.org/pdf/2505.17873.pdf", "abs": "https://arxiv.org/abs/2505.17873", "title": "MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback", "authors": ["Wanhao Liu", "Zonglin Yang", "Jue Wang", "Lidong Bing", "Di Zhang", "Dongzhan Zhou", "Yuqiang Li", "Houqiang Li", "Erik Cambria", "Wanli Ouyang"], "categories": ["cs.CL", "cs.AI", "cs.CE"], "comment": null, "summary": "Hypothesis ranking is a crucial component of automated scientific discovery,\nparticularly in natural sciences where wet-lab experiments are costly and\nthroughput-limited. Existing approaches focus on pre-experiment ranking,\nrelying solely on large language model's internal reasoning without\nincorporating empirical outcomes from experiments. We introduce the task of\nexperiment-guided ranking, which aims to prioritize candidate hypotheses based\non the results of previously tested ones. However, developing such strategies\nis challenging due to the impracticality of repeatedly conducting real\nexperiments in natural science domains. To address this, we propose a simulator\ngrounded in three domain-informed assumptions, modeling hypothesis performance\nas a function of similarity to a known ground truth hypothesis, perturbed by\nnoise. We curate a dataset of 124 chemistry hypotheses with experimentally\nreported outcomes to validate the simulator. Building on this simulator, we\ndevelop a pseudo experiment-guided ranking method that clusters hypotheses by\nshared functional characteristics and prioritizes candidates based on insights\nderived from simulated experimental feedback. Experiments show that our method\noutperforms pre-experiment baselines and strong ablations."}
{"id": "2505.18799", "pdf": "https://arxiv.org/pdf/2505.18799.pdf", "abs": "https://arxiv.org/abs/2505.18799", "title": "ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models", "authors": ["Hao Chen", "Haoze Li", "Zhiqing Xiao", "Lirong Gao", "Qi Zhang", "Xiaomeng Hu", "Ningtao Wang", "Xing Fu", "Junbo Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 8 figures, 14 tables", "summary": "Aligning general-purpose large language models (LLMs) to downstream tasks\noften incurs significant training adjustment costs. Prior research has explored\nvarious avenues to enhance alignment efficiency, primarily through minimal-data\ntraining or data-driven activations to identify key attention heads. However,\nthese approaches inherently introduce data dependency, which hinders\ngeneralization and reusability. To address this issue and enhance model\nalignment efficiency, we propose the \\textit{\\textbf{A}ttention\n\\textbf{L}ocalization and \\textbf{P}runing \\textbf{S}trategy (\\textbf{ALPS})},\nan efficient algorithm that localizes the most task-sensitive attention heads\nand prunes by restricting attention training updates to these heads, thereby\nreducing alignment costs. Experimental results demonstrate that our method\nactivates only \\textbf{10\\%} of attention parameters during fine-tuning while\nachieving a \\textbf{2\\%} performance improvement over baselines on three tasks.\nMoreover, the identified task-specific heads are transferable across datasets\nand mitigate knowledge forgetting. Our work and findings provide a novel\nperspective on efficient LLM alignment. The code is available at\nhttps://github.com/VoiceBeer/ALPS."}
{"id": "2505.19240", "pdf": "https://arxiv.org/pdf/2505.19240.pdf", "abs": "https://arxiv.org/abs/2505.19240", "title": "LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models", "authors": ["Aida Kostikova", "Zhipin Wang", "Deidamea Bajri", "Ole Pütz", "Benjamin Paaßen", "Steffen Eger"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language model (LLM) research has grown rapidly, along with increasing\nconcern about their limitations such as failures in reasoning, hallucinations,\nand limited multilingual capability. While prior reviews have addressed these\nissues, they often focus on individual limitations or consider them within the\nbroader context of evaluating overall model performance. This survey addresses\nthe gap by presenting a data-driven, semi-automated review of research on\nlimitations of LLMs (LLLMs) from 2022 to 2025, using a bottom-up approach. From\na corpus of 250,000 ACL and arXiv papers, we extract 14,648 relevant limitation\npapers using keyword filtering and LLM-based classification, validated against\nexpert labels. Using topic clustering (via two approaches, HDBSCAN+BERTopic and\nLlooM), we identify between 7 and 15 prominent types of limitations discussed\nin recent LLM research across the ACL and arXiv datasets. We find that\nLLM-related research increases nearly sixfold in ACL and nearly fifteenfold in\narXiv between 2022 and 2025, while LLLMs research grows even faster, by a\nfactor of over 12 in ACL and nearly 28 in arXiv. Reasoning remains the most\nstudied limitation, followed by generalization, hallucination, bias, and\nsecurity. The distribution of topics in the ACL dataset stays relatively stable\nover time, while arXiv shifts toward safety and controllability (with topics\nlike security risks, alignment, hallucinations, knowledge editing), and\nmultimodality between 2022 and 2025. We offer a quantitative view of trends in\nLLM limitations research and release a dataset of annotated abstracts and a\nvalidated methodology, available at:\nhttps://github.com/a-kostikova/LLLMs-Survey."}
{"id": "2505.19439", "pdf": "https://arxiv.org/pdf/2505.19439.pdf", "abs": "https://arxiv.org/abs/2505.19439", "title": "Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers", "authors": ["Rihui Xin", "Han Liu", "Zecheng Wang", "Yupeng Zhang", "Dianbo Sui", "Xiaolin Hu", "Bingning Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models have achieved remarkable success in natural language\nprocessing tasks, with Reinforcement Learning playing a key role in adapting\nthem to specific applications. However, obtaining ground truth answers for\ntraining LLMs in mathematical problem-solving is often challenging, costly, and\nsometimes unfeasible. This research delves into the utilization of format and\nlength as surrogate signals to train LLMs for mathematical problem-solving,\nbypassing the need for traditional ground truth answers.Our study shows that a\nreward function centered on format correctness alone can yield performance\nimprovements comparable to the standard GRPO algorithm in early phases.\nRecognizing the limitations of format-only rewards in the later phases, we\nincorporate length-based rewards. The resulting GRPO approach, leveraging\nformat-length surrogate signals, not only matches but surpasses the performance\nof the standard GRPO algorithm relying on ground truth answers in certain\nscenarios, achieving 40.0% accuracy on AIME2024 with a 7B base model. Through\nsystematic exploration and experimentation, this research not only offers a\npractical solution for training LLMs to solve mathematical problems and\nreducing the dependence on extensive ground truth data collection, but also\nreveals the essence of why our label-free approach succeeds: the powerful base\nmodel is like an excellent student who has already mastered mathematical and\nlogical reasoning skills, but performs poorly on the test paper, it simply\nneeds to develop good answering habits to achieve outstanding results in exams\n, to unlock the capabilities it already possesses."}
{"id": "2505.20112", "pdf": "https://arxiv.org/pdf/2505.20112.pdf", "abs": "https://arxiv.org/abs/2505.20112", "title": "ResSVD: Residual Compensated SVD for Large Language Model Compression", "authors": ["Haolei Bai", "Siyong Jian", "Tuo Liang", "Yu Yin", "Huan Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities in a\nwide range of downstream natural language processing tasks. Nevertheless, their\nconsiderable sizes and memory demands hinder practical deployment, underscoring\nthe importance of developing efficient compression strategies. Singular value\ndecomposition (SVD) decomposes a matrix into orthogonal components, enabling\nefficient low-rank approximation. This is particularly suitable for LLM\ncompression, where weight matrices often exhibit significant redundancy.\nHowever, current SVD-based methods neglect the residual matrix from truncation,\nresulting in significant truncation loss. Additionally, compressing all layers\nof the model results in severe performance degradation. To overcome these\nlimitations, we propose ResSVD, a new post-training SVD-based LLM compression\nmethod. Specifically, we leverage the residual matrix generated during the\ntruncation process to reduce truncation loss. Moreover, under a fixed overall\ncompression ratio, we selectively compress the last few layers of the model,\nwhich mitigates error propagation and significantly improves the performance of\ncompressed models. Comprehensive evaluations of ResSVD on diverse LLM families\nand multiple benchmark datasets indicate that ResSVD consistently achieves\nsuperior performance over existing counterpart methods, demonstrating its\npractical effectiveness."}
{"id": "2505.20767", "pdf": "https://arxiv.org/pdf/2505.20767.pdf", "abs": "https://arxiv.org/abs/2505.20767", "title": "CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models", "authors": ["Xiaqiang Tang", "Jian Li", "Keyu Hu", "Du Nan", "Xiaolong Li", "Xi Zhang", "Weigao Sun", "Sihong Xie"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "Faithfulness hallucinations are claims generated by a Large Language Model\n(LLM) not supported by contexts provided to the LLM. Lacking assessment\nstandards, existing benchmarks focus on \"factual statements\" that rephrase\nsource materials while overlooking \"cognitive statements\" that involve making\ninferences from the given context. Consequently, evaluating and detecting the\nhallucination of cognitive statements remains challenging. Inspired by how\nevidence is assessed in the legal domain, we design a rigorous framework to\nassess different levels of faithfulness of cognitive statements and introduce\nthe CogniBench dataset where we reveal insightful statistics. To keep pace with\nrapidly evolving LLMs, we further develop an automatic annotation pipeline that\nscales easily across different models. This results in a large-scale\nCogniBench-L dataset, which facilitates training accurate detectors for both\nfactual and cognitive hallucinations. We release our model and datasets at:\nhttps://github.com/FUTUREEEEEE/CogniBench"}
{"id": "2505.21693", "pdf": "https://arxiv.org/pdf/2505.21693.pdf", "abs": "https://arxiv.org/abs/2505.21693", "title": "MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural Awareness Evaluation for LLMs", "authors": ["Raoyuan Zhao", "Beiduo Chen", "Barbara Plank", "Michael A. Hedderich"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are used globally across many languages, but\ntheir English-centric pretraining raises concerns about cross-lingual\ndisparities for cultural awareness, often resulting in biased outputs. However,\ncomprehensive multilingual evaluation remains challenging due to limited\nbenchmarks and questionable translation quality. To better assess these\ndisparities, we introduce MAKIEval, an automatic multilingual framework for\nevaluating cultural awareness in LLMs across languages, regions, and topics.\nMAKIEval evaluates open-ended text generation, capturing how models express\nculturally grounded knowledge in natural language. Leveraging Wikidata's\nmultilingual structure as a cross-lingual anchor, it automatically identifies\ncultural entities in model outputs and links them to structured knowledge,\nenabling scalable, language-agnostic evaluation without manual annotation or\ntranslation. We then introduce four metrics that capture complementary\ndimensions of cultural awareness: granularity, diversity, cultural specificity,\nand consensus across languages. We assess 7 LLMs developed from different parts\nof the world, encompassing both open-source and proprietary systems, across 13\nlanguages, 19 countries and regions, and 6 culturally salient topics (e.g.,\nfood, clothing). Notably, we find that models tend to exhibit stronger cultural\nawareness in English, suggesting that English prompts more effectively activate\nculturally grounded knowledge. We publicly release our code and data."}
{"id": "2505.21701", "pdf": "https://arxiv.org/pdf/2505.21701.pdf", "abs": "https://arxiv.org/abs/2505.21701", "title": "Do We Know What LLMs Don't Know? A Study of Consistency in Knowledge Probing", "authors": ["Raoyuan Zhao", "Abdullatif Köksal", "Ali Modarressi", "Michael A. Hedderich", "Hinrich Schütze"], "categories": ["cs.CL"], "comment": null, "summary": "The reliability of large language models (LLMs) is greatly compromised by\ntheir tendency to hallucinate, underscoring the need for precise identification\nof knowledge gaps within LLMs. Various methods for probing such gaps exist,\nranging from calibration-based to prompting-based methods. To evaluate these\nprobing methods, in this paper, we propose a new process based on using input\nvariations and quantitative metrics. Through this, we expose two dimensions of\ninconsistency in knowledge gap probing. (1) Intra-method inconsistency: Minimal\nnon-semantic perturbations in prompts lead to considerable variance in detected\nknowledge gaps within the same probing method; e.g., the simple variation of\nshuffling answer options can decrease agreement to around 40%. (2) Cross-method\ninconsistency: Probing methods contradict each other on whether a model knows\nthe answer. Methods are highly inconsistent -- with decision consistency across\nmethods being as low as 7% -- even though the model, dataset, and prompt are\nall the same. These findings challenge existing probing methods and highlight\nthe urgent need for perturbation-robust probing frameworks."}
{"id": "2505.22571", "pdf": "https://arxiv.org/pdf/2505.22571.pdf", "abs": "https://arxiv.org/abs/2505.22571", "title": "Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems", "authors": ["Hoang Pham", "Thuy-Duong Nguyen", "Khac-Hoai Nam Bui"], "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "comment": null, "summary": "This paper presents a novel approach for unified retrieval-augmented\ngeneration (RAG) systems using the recent emerging large language model (LLM)\nagent concept. Specifically, Agent LLM, which utilizes LLM as fundamental\ncontrollers, has become a promising approach to enable the interpretability of\nRAG tasks, especially for complex reasoning question-answering systems (e.g.,\nmulti-hop queries). Nonetheless, previous works mainly focus on solving RAG\nsystems with either single-hop or multi-hop approaches separately, which limits\nthe application of those approaches to real-world applications. In this study,\nwe propose a trainable agent framework called Agent-UniRAG for unified\nretrieval-augmented LLM systems, which enhances the effectiveness and\ninterpretability of RAG systems. The main idea is to design an LLM agent\nframework to solve RAG tasks step-by-step based on the complexity of the\ninputs, simultaneously including single-hop and multi-hop queries in an\nend-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset\nto enable the proposed agent framework for small open-source LLMs (e.g.,\nLlama-3-8B). The results show comparable performances with closed-source and\nlarger open-source LLMs across various RAG benchmarks. Our source code and\ndataset are publicly available for further exploitation."}
{"id": "2505.22630", "pdf": "https://arxiv.org/pdf/2505.22630.pdf", "abs": "https://arxiv.org/abs/2505.22630", "title": "Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in LLMs", "authors": ["Ziling Cheng", "Meng Cao", "Marc-Antoine Rondeau", "Jackie Chi Kit Cheung"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "The widespread success of large language models (LLMs) on NLP benchmarks has\nbeen accompanied by concerns that LLMs function primarily as stochastic parrots\nthat reproduce texts similar to what they saw during pre-training, often\nerroneously. But what is the nature of their errors, and do these errors\nexhibit any regularities? In this work, we examine irrelevant context\nhallucinations, in which models integrate misleading contextual cues into their\npredictions. Through behavioral analysis, we show that these errors result from\na structured yet flawed mechanism that we term class-based (mis)generalization,\nin which models combine abstract class cues with features extracted from the\nquery or context to derive answers. Furthermore, mechanistic interpretability\nexperiments on Llama-3, Mistral, and Pythia across 39 factual recall relation\ntypes reveal that this behavior is reflected in the model's internal\ncomputations: (i) abstract class representations are constructed in lower\nlayers before being refined into specific answers in higher layers, (ii)\nfeature selection is governed by two competing circuits -- one prioritizing\ndirect query-based reasoning, the other incorporating contextual cues -- whose\nrelative influences determine the final output. Our findings provide a more\nnuanced perspective on the stochastic parrot argument: through form-based\ntraining, LLMs can exhibit generalization leveraging abstractions, albeit in\nunreliable ways based on contextual cues -- what we term stochastic chameleons."}
{"id": "2505.23688", "pdf": "https://arxiv.org/pdf/2505.23688.pdf", "abs": "https://arxiv.org/abs/2505.23688", "title": "Automatic classification of stop realisation with wav2vec2.0", "authors": ["James Tanner", "Morgan Sonderegger", "Jane Stuart-Smith", "Jeff Mielke", "Tyler Kendall"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted for Interspeech 2025. 5 pages, 3 figures", "summary": "Modern phonetic research regularly makes use of automatic tools for the\nannotation of speech data, however few tools exist for the annotation of many\nvariable phonetic phenomena. At the same time, pre-trained self-supervised\nmodels, such as wav2vec2.0, have been shown to perform well at speech\nclassification tasks and latently encode fine-grained phonetic information. We\ndemonstrate that wav2vec2.0 models can be trained to automatically classify\nstop burst presence with high accuracy in both English and Japanese, robust\nacross both finely-curated and unprepared speech corpora. Patterns of\nvariability in stop realisation are replicated with the automatic annotations,\nand closely follow those of manual annotations. These results demonstrate the\npotential of pre-trained speech models as tools for the automatic annotation\nand processing of speech corpus data, enabling researchers to 'scale-up' the\nscope of phonetic research with relative ease."}
{"id": "2303.14537", "pdf": "https://arxiv.org/pdf/2303.14537.pdf", "abs": "https://arxiv.org/abs/2303.14537", "title": "Deep Augmentation: Dropout as Augmentation for Self-Supervised Learning", "authors": ["Rickard Brüel-Gabrielsson", "Tongzhou Wang", "Manel Baradad", "Justin Solomon"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Despite dropout's ubiquity in machine learning, its effectiveness as a form\nof data augmentation remains under-explored. We address two key questions: (i)\nWhen is dropout effective as an augmentation strategy? (ii) Is dropout uniquely\neffective under these conditions? To explore these questions, we propose Deep\nAugmentation, a network- and modality-agnostic method that applies dropout or\nPCA transformations to targeted layers in neural networks. Through extensive\nexperiments on contrastive learning tasks in NLP, computer vision, and graph\nlearning, we find that uniformly applying dropout across layers does not\nconsistently improve performance. Instead, dropout proves most beneficial in\ndeeper layers and can be matched by alternative augmentations (e.g., PCA). We\nalso show that a stop-gradient operation is critical for ensuring dropout\nfunctions effectively as an augmentation, and that performance trends invert\nwhen moving from contrastive tasks to supervised tasks. Our analysis suggests\nthat Deep Augmentation helps mitigate inter-layer co-adaptation -- a notable\nissue in self-supervised learning due to the absence of labeled data. Drawing\non these insights, we outline a procedure for selecting the optimal\naugmentation layer and demonstrate that Deep Augmentation can outperform\ntraditional input-level augmentations. This simple yet powerful approach can be\nseamlessly integrated into a wide range of architectures and modalities,\nyielding notable gains in both performance and generalization."}
{"id": "2312.02219", "pdf": "https://arxiv.org/pdf/2312.02219.pdf", "abs": "https://arxiv.org/abs/2312.02219", "title": "Behind the Magic, MERLIM: Multi-modal Evaluation Benchmark for Large Image-Language Models", "authors": ["Andrés Villa", "Juan Carlos León Alcázar", "Alvaro Soto", "Bernard Ghanem"], "categories": ["cs.CV", "cs.CL"], "comment": "18 pages, 10 figures, 6 tables", "summary": "Large Vision and Language Models have enabled significant advances in fully\nsupervised and zero-shot visual tasks. These large architectures serve as the\nbaseline to what is currently known as Instruction Tuning Large Vision and\nLanguage models (IT-LVLMs). IT-LVLMs are general-purpose multi-modal assistants\nwhose responses are modulated by natural language instructions and visual data.\nDespite this versatility, IT-LVLM effectiveness in fundamental computer vision\nproblems remains unclear, primarily due to the absence of a standardized\nevaluation benchmark. This paper introduces a Multi-modal Evaluation Benchmark\nnamed MERLIM, a scalable test-bed to assess the capabilities of IT-LVLMs on\nfundamental computer vision tasks. MERLIM contains over 300K image-question\npairs and has a strong focus on detecting cross-modal \"hallucination\" events in\nIT-LVLMs. Our results bring important insights on the performance of\nstate-of-the-art IT-LVLMs including limitations at identifying fine-grained\nvisual concepts, object hallucinations across tasks, and biases towards the\nlanguage query. Our findings also suggest that these models have weak visual\ngrounding, but manage to make adequate guesses from global visual patterns or\nlanguage biases contained in the LLM component. We name this phenomenon of\ncorrect answers with no visual grounding as hidden hallucinations."}
{"id": "2312.10741", "pdf": "https://arxiv.org/pdf/2312.10741.pdf", "abs": "https://arxiv.org/abs/2312.10741", "title": "StyleSinger: Style Transfer for Out-of-Domain Singing Voice Synthesis", "authors": ["Yu Zhang", "Rongjie Huang", "Ruiqi Li", "JinZheng He", "Yan Xia", "Feiyang Chen", "Xinyu Duan", "Baoxing Huai", "Zhou Zhao"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted by AAAI 2024", "summary": "Style transfer for out-of-domain (OOD) singing voice synthesis (SVS) focuses\non generating high-quality singing voices with unseen styles (such as timbre,\nemotion, pronunciation, and articulation skills) derived from reference singing\nvoice samples. However, the endeavor to model the intricate nuances of singing\nvoice styles is an arduous task, as singing voices possess a remarkable degree\nof expressiveness. Moreover, existing SVS methods encounter a decline in the\nquality of synthesized singing voices in OOD scenarios, as they rest upon the\nassumption that the target vocal attributes are discernible during the training\nphase. To overcome these challenges, we propose StyleSinger, the first singing\nvoice synthesis model for zero-shot style transfer of out-of-domain reference\nsinging voice samples. StyleSinger incorporates two critical approaches for\nenhanced effectiveness: 1) the Residual Style Adaptor (RSA) which employs a\nresidual quantization module to capture diverse style characteristics in\nsinging voices, and 2) the Uncertainty Modeling Layer Normalization (UMLN) to\nperturb the style attributes within the content representation during the\ntraining phase and thus improve the model generalization. Our extensive\nevaluations in zero-shot style transfer undeniably establish that StyleSinger\noutperforms baseline models in both audio quality and similarity to the\nreference singing voice samples. Access to singing voice samples can be found\nat https://aaronz345.github.io/StyleSingerDemo/."}
{"id": "2404.16792", "pdf": "https://arxiv.org/pdf/2404.16792.pdf", "abs": "https://arxiv.org/abs/2404.16792", "title": "Model Extrapolation Expedites Alignment", "authors": ["Chujie Zheng", "Ziqi Wang", "Heng Ji", "Minlie Huang", "Nanyun Peng"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ACL 2025", "summary": "Given the high computational cost of preference alignment training of large\nlanguage models (LLMs), exploring efficient methods to reduce the training\noverhead remains an important and compelling research problem. Motivated by the\nobservation that alignment training typically involves only small parameter\nchanges without injecting new knowledge into models, we propose a\nstraightforward method called ExPO (model extrapolation) to expedite LLMs'\nalignment with human preferences. Given a partially-trained model and its\ninitial SFT checkpoint, ExPO improves the implicit optimization objective of\nalignment training by simply amplifying the parameter change based on a\nfirst-order approximation, without any additional training overhead. Through\ncontrolled experiments, we demonstrate that ExPO boosts a DPO model trained\nwith only 20% steps to outperform the fully-trained one. Moreover, we show that\nExPO notably improves existing open-source LLMs (ranging from 1.8B to 70B\nparameters) on the leading AlpacaEval 2.0 and MT-Bench benchmarks, which\nhighlights ExPO's broader utility in efficiently enhancing LLM alignment."}
{"id": "2405.21075", "pdf": "https://arxiv.org/pdf/2405.21075.pdf", "abs": "https://arxiv.org/abs/2405.21075", "title": "Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis", "authors": ["Chaoyou Fu", "Yuhan Dai", "Yongdong Luo", "Lei Li", "Shuhuai Ren", "Renrui Zhang", "Zihan Wang", "Chenyu Zhou", "Yunhang Shen", "Mengdan Zhang", "Peixian Chen", "Yanwei Li", "Shaohui Lin", "Sirui Zhao", "Ke Li", "Tong Xu", "Xiawu Zheng", "Enhong Chen", "Caifeng Shan", "Ran He", "Xing Sun"], "categories": ["cs.CV", "cs.CL"], "comment": "Project Page: https://video-mme.github.io", "summary": "In the quest for artificial general intelligence, Multi-modal Large Language\nModels (MLLMs) have emerged as a focal point in recent advancements. However,\nthe predominant focus remains on developing their capabilities in static image\nunderstanding. The potential of MLLMs in processing sequential visual data is\nstill insufficiently explored, highlighting the absence of a comprehensive,\nhigh-quality assessment of their performance. In this paper, we introduce\nVideo-MME, the first-ever full-spectrum, Multi-Modal Evaluation benchmark of\nMLLMs in Video analysis. Our work distinguishes from existing benchmarks\nthrough four key features: 1) Diversity in video types, spanning 6 primary\nvisual domains with 30 subfields to ensure broad scenario generalizability; 2)\nDuration in temporal dimension, encompassing both short-, medium-, and\nlong-term videos, ranging from 11 seconds to 1 hour, for robust contextual\ndynamics; 3) Breadth in data modalities, integrating multi-modal inputs besides\nvideo frames, including subtitles and audios, to unveil the all-round\ncapabilities of MLLMs; 4) Quality in annotations, utilizing rigorous manual\nlabeling by expert annotators to facilitate precise and reliable model\nassessment. 900 videos with a total of 254 hours are manually selected and\nannotated by repeatedly viewing all the video content, resulting in 2,700\nquestion-answer pairs. With Video-MME, we extensively evaluate various\nstate-of-the-art MLLMs, including GPT-4 series and Gemini 1.5 Pro, as well as\nopen-source image models like InternVL-Chat-V1.5 and video models like\nLLaVA-NeXT-Video. Our experiments reveal that Gemini 1.5 Pro is the\nbest-performing commercial model, significantly outperforming the open-source\nmodels. Our dataset along with these findings underscores the need for further\nimprovements in handling longer sequences and multi-modal data. Project Page:\nhttps://video-mme.github.io"}
{"id": "2406.11317", "pdf": "https://arxiv.org/pdf/2406.11317.pdf", "abs": "https://arxiv.org/abs/2406.11317", "title": "GUICourse: From General Vision Language Models to Versatile GUI Agents", "authors": ["Wentong Chen", "Junbo Cui", "Jinyi Hu", "Yujia Qin", "Junjie Fang", "Yue Zhao", "Chongyi Wang", "Jun Liu", "Guirong Chen", "Yupeng Huo", "Yuan Yao", "Yankai Lin", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": null, "summary": "Utilizing Graphic User Interface (GUI) for human-computer interaction is\nessential for accessing a wide range of digital tools. Recent advancements in\nVision Language Models (VLMs) highlight the compelling potential to develop\nversatile agents to help humans finish GUI navigation tasks. However, current\nVLMs are challenged in terms of fundamental abilities (OCR and grounding) and\nGUI knowledge (the functions and control methods of GUI elements), preventing\nthem from becoming practical GUI agents. To solve these challenges, we\ncontribute GUICourse, a suite of datasets to train visual-based GUI agents from\ngeneral VLMs. First, we introduce the GUIEnv dataset to strengthen the OCR and\ngrounding capabilities of VLMs. Then, we introduce the GUIAct and GUIChat\ndatasets to enrich their knowledge of GUI components and interactions.\nExperiments demonstrate that our GUI agents have better performance on common\nGUI tasks than their baseline VLMs. Even the small-size GUI agent (with 3.1B\nparameters) can still work well on single-step and multi-step GUI tasks.\nFinally, we analyze the different varieties in the training stage of this agent\nby ablation study. Our source codes and datasets are released at\nhttps://github.com/yiye3/GUICourse."}
{"id": "2407.00066", "pdf": "https://arxiv.org/pdf/2407.00066.pdf", "abs": "https://arxiv.org/abs/2407.00066", "title": "Compress then Serve: Serving Thousands of LoRA Adapters with Little Overhead", "authors": ["Rickard Brüel-Gabrielsson", "Jiacheng Zhu", "Onkar Bhardwaj", "Leshem Choshen", "Kristjan Greenewald", "Mikhail Yurochkin", "Justin Solomon"], "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Fine-tuning large language models (LLMs) with low-rank adaptations (LoRAs)\nhas become common practice, often yielding numerous copies of the same LLM\ndiffering only in their LoRA updates. This paradigm presents challenges for\nsystems that serve real-time responses to queries that each involve a different\nLoRA. Prior works optimize the design of such systems but still require\ncontinuous loading and offloading of LoRAs, as it is infeasible to store\nthousands of LoRAs in GPU memory. To mitigate this issue, we investigate the\nefficacy of compression when serving LoRAs. We propose a method for the joint\ncompression of LoRAs into a shared basis paired with LoRA-specific scaling\nmatrices. We extend our algorithm to learn clusters of LoRAs that are amenable\nto joint compression, allowing it to scale gracefully to large LoRA\ncollections. Our experiments with up to 1000 LoRAs demonstrate that compressed\nLoRAs preserve performance while offering major throughput gains in realistic\nserving scenarios with over a thousand LoRAs, maintaining 80% of the throughput\nof serving a single LoRA."}
{"id": "2408.05211", "pdf": "https://arxiv.org/pdf/2408.05211.pdf", "abs": "https://arxiv.org/abs/2408.05211", "title": "VITA: Towards Open-Source Interactive Omni Multimodal LLM", "authors": ["Chaoyou Fu", "Haojia Lin", "Zuwei Long", "Yunhang Shen", "Yuhang Dai", "Meng Zhao", "Yi-Fan Zhang", "Shaoqi Dong", "Yangze Li", "Xiong Wang", "Haoyu Cao", "Di Yin", "Long Ma", "Xiawu Zheng", "Rongrong Ji", "Yunsheng Wu", "Ran He", "Caifeng Shan", "Xing Sun"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project Page: https://vita-home.github.io", "summary": "The remarkable multimodal capabilities and interactive experience of GPT-4o\nunderscore their necessity in practical applications, yet open-source models\nrarely excel in both areas. In this paper, we introduce VITA, the first-ever\nopen-source Multimodal Large Language Model (MLLM) adept at simultaneous\nprocessing and analysis of Video, Image, Text, and Audio modalities, and\nmeanwhile has an advanced multimodal interactive experience. Starting from\nMixtral 8x7B as a language foundation, we expand its Chinese vocabulary\nfollowed by bilingual instruction tuning. We further endow the language model\nwith visual and audio capabilities through two-stage multi-task learning of\nmultimodal alignment and instruction tuning. VITA demonstrates robust\nfoundational capabilities of multilingual, vision, and audio understanding, as\nevidenced by its strong performance across a range of both unimodal and\nmultimodal benchmarks. Beyond foundational capabilities, we have made\nconsiderable progress in enhancing the natural multimodal human-computer\ninteraction experience. VITA is the first step for the open-source community to\nexplore the seamless integration of multimodal understanding and interaction.\nWhile there is still lots of work to be done on VITA to get close to\nclose-source counterparts, we hope that its role as a pioneer can serve as a\ncornerstone for subsequent research. Project Page: https://vita-home.github.io."}
{"id": "2408.09429", "pdf": "https://arxiv.org/pdf/2408.09429.pdf", "abs": "https://arxiv.org/abs/2408.09429", "title": "Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models", "authors": ["Kening Zheng", "Junkai Chen", "Yibo Yan", "Xin Zou", "Xuming Hu"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "Accepted by Findings of ACL 2025", "summary": "Hallucination issues continue to affect multimodal large language models\n(MLLMs), with existing research mainly addressing object-level or\nattribute-level hallucinations, neglecting the more complex relation\nhallucinations that require advanced reasoning. Current benchmarks for relation\nhallucinations lack detailed evaluation and effective mitigation, and their\ndatasets often suffer from biases due to systematic annotation processes. To\naddress these challenges, we introduce Reefknot, a comprehensive benchmark\ntargeting relation hallucinations, comprising over 20,000 real-world samples.\nWe provide a systematic definition of relation hallucinations, integrating\nperceptive and cognitive perspectives, and construct a relation-based corpus\nusing the Visual Genome scene graph dataset. Our comparative evaluation reveals\nsignificant limitations in current MLLMs' ability to handle relation\nhallucinations. Additionally, we propose a novel confidence-based mitigation\nstrategy, which reduces the hallucination rate by an average of 9.75% across\nthree datasets, including Reefknot. Our work offers valuable insights for\nachieving trustworthy multimodal intelligence."}
{"id": "2409.17275", "pdf": "https://arxiv.org/pdf/2409.17275.pdf", "abs": "https://arxiv.org/abs/2409.17275", "title": "On the Vulnerability of Applying Retrieval-Augmented Generation within Knowledge-Intensive Application Domains", "authors": ["Xun Xian", "Ganghua Wang", "Xuan Bi", "Jayanth Srinivasa", "Ashish Kundu", "Charles Fleming", "Mingyi Hong", "Jie Ding"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.DB", "cs.ET", "cs.IR", "cs.LG", "68T50, 68T05, 94A60", "I.2.7; K.6.5; H.3.3; I.2.6"], "comment": "Accepted by ICML 2025", "summary": "Retrieval-Augmented Generation (RAG) has been empirically shown to enhance\nthe performance of large language models (LLMs) in knowledge-intensive domains\nsuch as healthcare, finance, and legal contexts. Given a query, RAG retrieves\nrelevant documents from a corpus and integrates them into the LLMs' generation\nprocess. In this study, we investigate the adversarial robustness of RAG,\nfocusing specifically on examining the retrieval system. First, across 225\ndifferent setup combinations of corpus, retriever, query, and targeted\ninformation, we show that retrieval systems are vulnerable to universal\npoisoning attacks in medical Q\\&A. In such attacks, adversaries generate\npoisoned documents containing a broad spectrum of targeted information, such as\npersonally identifiable information. When these poisoned documents are inserted\ninto a corpus, they can be accurately retrieved by any users, as long as\nattacker-specified queries are used. To understand this vulnerability, we\ndiscovered that the deviation from the query's embedding to that of the\npoisoned document tends to follow a pattern in which the high similarity\nbetween the poisoned document and the query is retained, thereby enabling\nprecise retrieval. Based on these findings, we develop a new detection-based\ndefense to ensure the safe use of RAG. Through extensive experiments spanning\nvarious Q\\&A domains, we observed that our proposed method consistently\nachieves excellent detection rates in nearly all cases."}
{"id": "2410.15625", "pdf": "https://arxiv.org/pdf/2410.15625.pdf", "abs": "https://arxiv.org/abs/2410.15625", "title": "Improving Parallel Program Performance with LLM Optimizers via Agent-System Interfaces", "authors": ["Anjiang Wei", "Allen Nie", "Thiago S. F. X. Teixeira", "Rohan Yadav", "Wonchan Lee", "Ke Wang", "Alex Aiken"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "comment": null, "summary": "Modern scientific discovery increasingly relies on high-performance computing\nfor complex modeling and simulation. A key challenge in improving parallel\nprogram performance is efficiently mapping tasks to processors and data to\nmemory, a process dictated by intricate, low-level system code known as\nmappers. Developing high-performance mappers demands days of manual tuning,\nposing a significant barrier for domain scientists without systems expertise.\nWe introduce a framework that automates mapper development with generative\noptimization, leveraging richer feedback beyond scalar performance metrics. Our\napproach features the Agent-System Interface, which includes a Domain-Specific\nLanguage (DSL) to abstract away the low-level complexity of system code and\ndefine a structured search space, as well as AutoGuide, a mechanism that\ninterprets raw execution output into actionable feedback. Unlike traditional\nreinforcement learning methods such as OpenTuner, which rely solely on scalar\nfeedback, our method finds superior mappers in far fewer iterations. With just\n10 iterations, it outperforms OpenTuner even after 1000 iterations, achieving\n3.8X faster performance. Our approach finds mappers that surpass expert-written\nmappers by up to 1.34X speedup across nine benchmarks while reducing tuning\ntime from days to minutes."}
{"id": "2410.22307", "pdf": "https://arxiv.org/pdf/2410.22307.pdf", "abs": "https://arxiv.org/abs/2410.22307", "title": "SVIP: Towards Verifiable Inference of Open-source Large Language Models", "authors": ["Yifan Sun", "Yuhang Li", "Yue Zhang", "Yuchen Jin", "Huan Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": "22 pages", "summary": "The ever-increasing size of open-source Large Language Models (LLMs) renders\nlocal deployment impractical for individual users. Decentralized computing has\nemerged as a cost-effective solution, allowing individuals and small companies\nto perform LLM inference for users using surplus computational power. However,\na computing provider may stealthily substitute the requested LLM with a\nsmaller, less capable model without consent from users, thereby benefiting from\ncost savings. We introduce SVIP, a secret-based verifiable LLM inference\nprotocol. Unlike existing solutions based on cryptographic or game-theoretic\ntechniques, our method is computationally effective and does not rest on strong\nassumptions. Our protocol requires the computing provider to return both the\ngenerated text and processed hidden representations from LLMs. We then train a\nproxy task on these representations, effectively transforming them into a\nunique model identifier. With our protocol, users can reliably verify whether\nthe computing provider is acting honestly. A carefully integrated secret\nmechanism further strengthens its security. We thoroughly analyze our protocol\nunder multiple strong and adaptive adversarial scenarios. Our extensive\nexperiments demonstrate that SVIP is accurate, generalizable, computationally\nefficient, and resistant to various attacks. Notably, SVIP achieves false\nnegative rates below 5% and false positive rates below 3%, while requiring less\nthan 0.01 seconds per prompt query for verification."}
{"id": "2412.13682", "pdf": "https://arxiv.org/pdf/2412.13682.pdf", "abs": "https://arxiv.org/abs/2412.13682", "title": "ChinaTravel: An Open-Ended Benchmark for Language Agents in Chinese Travel Planning", "authors": ["Jie-Jing Shao", "Bo-Wen Zhang", "Xiao-Wen Yang", "Baizhi Chen", "Si-Yu Han", "Wen-Da Wei", "Guohao Cai", "Zhenhua Dong", "Lan-Zhe Guo", "Yu-feng Li"], "categories": ["cs.AI", "cs.CL"], "comment": "Webpage: https://www.lamda.nju.edu.cn/shaojj/chinatravel", "summary": "Recent advances in LLMs, particularly in language reasoning and tool\nintegration, have rapidly sparked the \\emph{Language Agents} for real-world\ndevelopment. Among these, travel planning represents a prominent domain,\ncombining complex multi-objective planning challenges with practical deployment\ndemands. However, existing benchmarks often oversimplify real-world\nrequirements by focusing on synthetic queries and limited constraints. We\naddress the gap of evaluating language agents in multi-day, multi-POI travel\nplanning scenarios with diverse and open human needs. Specifically, we\nintroduce \\emph{ChinaTravel}, the first open-ended benchmark grounded in\nauthentic Chinese travel requirements collected from 1,154 human participants.\nWe design a compositionally generalizable domain-specific language (DSL) for\nscalable evaluation, covering feasibility, constraint satisfaction, and\npreference comparison. Empirical studies reveal the potential of neuro-symbolic\nagents in travel planning, achieving a 37.0\\% constraint satisfaction rate on\nhuman queries, a 10\\times improvement over purely neural models. These findings\nhighlight ChinaTravel as a pivotal milestone for advancing language agents in\ncomplex, real-world planning scenarios."}
{"id": "2501.02772", "pdf": "https://arxiv.org/pdf/2501.02772.pdf", "abs": "https://arxiv.org/abs/2501.02772", "title": "GeAR: Generation Augmented Retrieval", "authors": ["Haoyu Liu", "Shaohan Huang", "Jianfeng Liu", "Yuefeng Zhan", "Hao Sun", "Weiwei Deng", "Feng Sun", "Furu Wei", "Qi Zhang"], "categories": ["cs.IR", "cs.CL"], "comment": "In ACL 2025", "summary": "Document retrieval techniques are essential for developing large-scale\ninformation systems. The common approach involves using a bi-encoder to compute\nthe semantic similarity between a query and documents. However, the scalar\nsimilarity often fail to reflect enough information, hindering the\ninterpretation of retrieval results. In addition, this process primarily\nfocuses on global semantics, overlooking the finer-grained semantic\nrelationships between the query and the document's content. In this paper, we\nintroduce a novel method, $\\textbf{Ge}$neration $\\textbf{A}$ugmented\n$\\textbf{R}$etrieval ($\\textbf{GeAR}$), which not only improves the global\ndocument-query similarity through contrastive learning, but also integrates\nwell-designed fusion and decoding modules. This enables GeAR to generate\nrelevant context within the documents based on a given query, facilitating\nlearning to retrieve local fine-grained information. Furthermore, when used as\na retriever, GeAR does not incur any additional computational cost over\nbi-encoders. GeAR exhibits competitive retrieval performance across diverse\nscenarios and tasks. Moreover, qualitative analysis and the results generated\nby GeAR provide novel insights into the interpretation of retrieval results.\nThe code, data, and models will be released at\n\\href{https://github.com/microsoft/LMOps}{https://github.com/microsoft/LMOps}."}
{"id": "2501.10639", "pdf": "https://arxiv.org/pdf/2501.10639.pdf", "abs": "https://arxiv.org/abs/2501.10639", "title": "Latent-space adversarial training with post-aware calibration for defending large language models against jailbreak attacks", "authors": ["Xin Yi", "Yue Li", "Dongsheng Shi", "Linlin Wang", "Xiaoling Wang", "Liang He"], "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Ensuring safety alignment is a critical requirement for large language models\n(LLMs), particularly given increasing deployment in real-world applications.\nDespite considerable advancements, LLMs remain susceptible to jailbreak\nattacks, which exploit system vulnerabilities to circumvent safety measures and\nelicit harmful or inappropriate outputs. Furthermore, while adversarial\ntraining-based defense methods have shown promise, a prevalent issue is the\nunintended over-defense behavior, wherein models excessively reject benign\nqueries, significantly undermining their practical utility. To address these\nlimitations, we introduce LATPC, a Latent-space Adversarial Training with\nPost-aware Calibration framework. LATPC dynamically identifies safety-critical\nlatent dimensions by contrasting harmful and benign inputs, enabling the\nadaptive construction of targeted refusal feature removal attacks. This\nmechanism allows adversarial training to concentrate on real-world jailbreak\ntactics that disguise harmful queries as benign ones. During inference, LATPC\nemploys an efficient embedding-level calibration mechanism to minimize\nover-defense behaviors with negligible computational overhead. Experimental\nresults across five types of disguise-based jailbreak attacks demonstrate that\nLATPC achieves a superior balance between safety and utility compared to\nexisting defense frameworks. Further analysis demonstrates the effectiveness of\nleveraging safety-critical dimensions in developing robust defense methods\nagainst jailbreak attacks."}
{"id": "2502.04040", "pdf": "https://arxiv.org/pdf/2502.04040.pdf", "abs": "https://arxiv.org/abs/2502.04040", "title": "Safety Reasoning with Guidelines", "authors": ["Haoyu Wang", "Zeyu Qin", "Li Shen", "Xueqian Wang", "Dacheng Tao", "Minhao Cheng"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML 2025 paper. The first two authors contributed equally", "summary": "Training safe LLMs remains a critical challenge. The most widely used method,\nRefusal Training (RT), struggles to generalize against various\nOut-of-Distribution (OOD) jailbreaking attacks. Although various advanced\nmethods have been proposed to address this issue, we instead question whether\nOOD attacks inherently surpass the capability of vanilla RT. Evaluations using\nBest-of-N (BoN) reveal significant safety improvements as N increases,\nindicating models possess adequate latent safety knowledge but RT fails to\nconsistently elicit it under OOD scenarios. Further domain adaptation analysis\nreveals that direct RT causes reliance on superficial shortcuts, resulting in\nnon-generalizable representation mappings. Inspired by our findings, we propose\ntraining model to perform safety reasoning for each query. Specifically, we\nsynthesize reasoning supervision aligned with specified guidelines that reflect\ndiverse perspectives on safety knowledge. This encourages model to engage in\ndeeper reasoning, explicitly eliciting and utilizing latent safety knowledge\nfor each query. Extensive experiments show that our method significantly\nimproves model generalization against OOD attacks."}
{"id": "2502.04675", "pdf": "https://arxiv.org/pdf/2502.04675.pdf", "abs": "https://arxiv.org/abs/2502.04675", "title": "Scalable Oversight for Superhuman AI via Recursive Self-Critiquing", "authors": ["Xueru Wen", "Jie Lou", "Xinyu Lu", "Junjie Yang", "Yanjiang Liu", "Yaojie Lu", "Debing Zhang", "Xing Yu"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "As AI capabilities increasingly surpass human proficiency in complex tasks,\ncurrent alignment techniques including SFT and RLHF face fundamental challenges\nin ensuring reliable oversight. These methods rely on direct human assessment\nand become untenable when AI outputs exceed human cognitive thresholds. In\nresponse to this challenge, we explore two hypotheses: (1) \\textit{Critique of\ncritique can be easier than critique itself}, extending the widely-accepted\nobservation that verification is easier than generation to the critique domain,\nas critique itself is a specialized form of generation; (2) \\textit{This\ndifficulty relationship is recursively held}, suggesting that when direct\nevaluation is infeasible, performing high-order critiques (e.g., critique of\ncritique of critique) offers a more tractable supervision pathway. We further\nconduct Human-AI and AI-AI experiments to investigate the potential of\nutilizing recursive self-critiquing for AI supervision. Our results highlight\nrecursive critique as a promising approach for scalable AI oversight."}
{"id": "2502.12929", "pdf": "https://arxiv.org/pdf/2502.12929.pdf", "abs": "https://arxiv.org/abs/2502.12929", "title": "Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options", "authors": ["Lakshmi Nair", "Ian Trase", "Mark Kim"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML 2025", "summary": "We present a novel reasoning approach called Flow-of-Options (FoO), designed\nto address intrinsic biases in Large Language Models (LLMs). Flow-of-Options\nenables LLMs to systematically explore a diverse range of possibilities in\ntheir reasoning, as demonstrated by an FoO-based agentic framework developed\nfor autonomously solving Machine Learning (ML) tasks. FoO enforces diversity in\nLLM solutions through compressed and interpretable task representations,\nresulting in improvements of 38.2% - 69.2% on standard data science tasks, and\n37.4% - 47.9% on therapeutic chemistry tasks, as compared to state-of-the-art\nbaselines. With an overall operation cost under $1 per task, our framework is\nwell-suited for cost-sensitive applications. Going beyond tabular\nclassification and regression, we show the broader applicability of our\nFoO-based agentic system to tasks such as reinforcement learning and image\ngeneration. Our code is open-sourced at:\nhttps://github.com/flagshippioneering/Flow-of-Options."}
{"id": "2502.13001", "pdf": "https://arxiv.org/pdf/2502.13001.pdf", "abs": "https://arxiv.org/abs/2502.13001", "title": "You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with a Multi-Agent Conversations", "authors": ["Frederic Kirstein", "Muneeb Khan", "Jan Philip Wahle", "Terry Ruas", "Bela Gipp"], "categories": ["cs.AI", "cs.CL"], "comment": "Accepted at ACL 2025 (Findings)", "summary": "Meeting summarization suffers from limited high-quality data, mainly due to\nprivacy restrictions and expensive collection processes. We address this gap\nwith FAME, a dataset of 500 meetings in English and 300 in German produced by\nMIMIC, our new multi-agent meeting synthesis framework that generates meeting\ntranscripts on a given knowledge source by defining psychologically grounded\nparticipant profiles, outlining the conversation, and orchestrating a large\nlanguage model (LLM) debate. A modular post-processing step refines these\noutputs, mitigating potential repetitiveness and overly formal tones, ensuring\ncoherent, credible dialogues at scale. We also propose a psychologically\ngrounded evaluation framework assessing naturalness, social behavior\nauthenticity, and transcript difficulties. Human assessments show that FAME\napproximates real-meeting spontaneity (4.5/5 in naturalness), preserves\nspeaker-centric challenges (3/5 in spoken language), and introduces richer\ninformation-oriented difficulty (4/5 in difficulty). These findings highlight\nthat FAME is a good and scalable proxy for real-world meeting conditions. It\nenables new test scenarios for meeting summarization research and other\nconversation-centric applications in tasks requiring conversation data or\nsimulating social scenarios under behavioral constraints."}
{"id": "2502.13681", "pdf": "https://arxiv.org/pdf/2502.13681.pdf", "abs": "https://arxiv.org/abs/2502.13681", "title": "Repo2Run: Automated Building Executable Environment for Code Repository at Scale", "authors": ["Ruida Hu", "Chao Peng", "Xinchen Wang", "Junjielong Xu", "Cuiyun Gao"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Scaling up executable code data is significant for improving language models'\nsoftware engineering capability. The intricate nature of the process makes it\nlabor-intensive, time-consuming and expert-knowledge-dependent to build a large\nnumber of executable code repositories, limiting the scalability of existing\nwork based on running tests. The primary bottleneck lies in the automated\nbuilding of test environments for different repositories, which is an essential\nyet underexplored task. To mitigate the gap, we introduce Repo2Run, the first\nLLM-based agent aiming at automating the building of executable test\nenvironments for any repositories at scale. Specifically, given a code\nrepository, Repo2Run iteratively builds the Docker image, runs unit tests based\non the feedback of the building, and synthesizes the Dockerfile until the\nentire pipeline is executed successfully. The resulting Dockerfile can then be\nused to create Docker container environments for running code and tests. We\ncreated a benchmark containing 420 Python repositories with unit tests for\nevaluation. The results illustrate that Repo2Run achieves an 86.0% success\nrate, outperforming SWE-agent by 77.0%. The resources of Repo2Run are available\nat https://github.com/bytedance/Repo2Run."}
{"id": "2502.15210", "pdf": "https://arxiv.org/pdf/2502.15210.pdf", "abs": "https://arxiv.org/abs/2502.15210", "title": "PairBench: Are Vision-Language Models Reliable at Comparing What They See?", "authors": ["Aarash Feizi", "Sai Rajeswar", "Adriana Romero-Soriano", "Reihaneh Rabbany", "Valentina Zantedeschi", "Spandana Gella", "João Monteiro"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Understanding how effectively large vision language models (VLMs) compare\nvisual inputs is crucial across numerous applications, yet this fundamental\ncapability remains insufficiently assessed. While VLMs are increasingly\ndeployed for tasks requiring comparative judgment, including automated\nevaluation, re-ranking, and retrieval-augmented generation, no systematic\nframework exists to measure their performance in these scenarios. We present\nPairBench, a simple framework that evaluates VLMs as customizable similarity\ntools using widely available image datasets. Our approach introduces four key\nmetrics for reliable comparison: alignment with human annotations, consistency\nacross pair ordering, distribution smoothness, and controllability through\nprompting. Our analysis reveals that no model consistently excels across all\nmetrics, with each demonstrating distinct strengths and weaknesses. Most\nconcerning is the widespread inability of VLMs to maintain symmetric similarity\nscores. Interestingly, we demonstrate that performance on our benchmark\nstrongly correlates with popular benchmarks used for more complex tasks, while\nproviding additional metrics into controllability, smoothness and ordering.\nThis makes PairBench a unique and comprehensive framework to evaluate the\nperformance of VLMs for automatic evaluation depending on the task."}
{"id": "2502.17510", "pdf": "https://arxiv.org/pdf/2502.17510.pdf", "abs": "https://arxiv.org/abs/2502.17510", "title": "Recurrent Knowledge Identification and Fusion for Language Model Continual Learning", "authors": ["Yujie Feng", "Xujia Wang", "Zexin Lu", "Shenghong Fu", "Guangyuan Shi", "Yongxin Xu", "Yasha Wang", "Philip S. Yu", "Xu Chu", "Xiao-Ming Wu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "Continual learning (CL) is crucial for deploying large language models (LLMs)\nin dynamic real-world environments without costly retraining. While recent\nmodel ensemble and model merging methods guided by parameter importance have\ngained popularity, they often struggle to balance knowledge transfer and\nforgetting, mainly due to the reliance on static importance estimates during\nsequential training. In this paper, we present Recurrent-KIF, a novel CL\nframework for Recurrent Knowledge Identification and Fusion, which enables\ndynamic estimation of parameter importance distributions to enhance knowledge\ntransfer. Inspired by human continual learning, Recurrent-KIF employs an inner\nloop that rapidly adapts to new tasks while identifying important parameters,\ncoupled with an outer loop that globally manages the fusion of new and\nhistorical knowledge through redundant knowledge pruning and key knowledge\nmerging. These inner-outer loops iteratively perform multiple rounds of fusion,\nallowing Recurrent-KIF to leverage intermediate training information and\nadaptively adjust fusion strategies based on evolving importance distributions.\nExtensive experiments on two CL benchmarks with various model sizes (from 770M\nto 13B) demonstrate that Recurrent-KIF effectively mitigates catastrophic\nforgetting and enhances knowledge transfer."}
{"id": "2502.19668", "pdf": "https://arxiv.org/pdf/2502.19668.pdf", "abs": "https://arxiv.org/abs/2502.19668", "title": "SuPreME: A Supervised Pre-training Framework for Multimodal ECG Representation Learning", "authors": ["Mingsheng Cai", "Jiuming Jiang", "Wenhao Huang", "Che Liu", "Rossella Arcucci"], "categories": ["eess.SP", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Cardiovascular diseases are a leading cause of death and disability\nworldwide. Electrocardiogram (ECG) is critical for diagnosing and monitoring\ncardiac health, but obtaining large-scale annotated ECG datasets is\nlabor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL)\nmethods mitigate this by learning features without extensive labels but fail to\ncapture fine-grained clinical semantics and require extensive task-specific\nfine-tuning. To address these challenges, we propose $\\textbf{SuPreME}$, a\n$\\textbf{Su}$pervised $\\textbf{Pre}$-training framework for\n$\\textbf{M}$ultimodal $\\textbf{E}$CG representation learning. SuPreME is\npre-trained using structured diagnostic labels derived from ECG report entities\nthrough a one-time offline extraction with Large Language Models (LLMs), which\nhelp denoise, standardize cardiac concepts, and improve clinical representation\nlearning. By fusing ECG signals with textual cardiac queries instead of fixed\nlabels, SuPreME enables zero-shot classification of unseen conditions without\nfurther fine-tuning. We evaluate SuPreME on six downstream datasets covering\n106 cardiac conditions, achieving superior zero-shot AUC performance of\n$77.20\\%$, surpassing state-of-the-art eSSLs by $4.98\\%$. Results demonstrate\nSuPreME's effectiveness in leveraging structured, clinically relevant knowledge\nfor high-quality ECG representations."}
{"id": "2503.14604", "pdf": "https://arxiv.org/pdf/2503.14604.pdf", "abs": "https://arxiv.org/abs/2503.14604", "title": "Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges and Future Perspectives", "authors": ["Sara Sarto", "Marcella Cornia", "Rita Cucchiara"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "IJCAI 2025. Repo GitHub:\n  https://github.com/aimagelab/awesome-captioning-evaluation", "summary": "The evaluation of machine-generated image captions is a complex and evolving\nchallenge. With the advent of Multimodal Large Language Models (MLLMs), image\ncaptioning has become a core task, increasing the need for robust and reliable\nevaluation metrics. This survey provides a comprehensive overview of\nadvancements in image captioning evaluation, analyzing the evolution,\nstrengths, and limitations of existing metrics. We assess these metrics across\nmultiple dimensions, including correlation with human judgment, ranking\naccuracy, and sensitivity to hallucinations. Additionally, we explore the\nchallenges posed by the longer and more detailed captions generated by MLLMs\nand examine the adaptability of current metrics to these stylistic variations.\nOur analysis highlights some limitations of standard evaluation approaches and\nsuggests promising directions for future research in image captioning\nassessment."}
{"id": "2503.17229", "pdf": "https://arxiv.org/pdf/2503.17229.pdf", "abs": "https://arxiv.org/abs/2503.17229", "title": "FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs", "authors": ["Albert Sawczyn", "Jakub Binkowski", "Denis Janiak", "Bogdan Gabrys", "Tomasz Kajdanowicz"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint", "summary": "Large Language Models (LLMs) frequently generate hallucinated content, posing\nsignificant challenges for applications where factuality is crucial. While\nexisting hallucination detection methods typically operate at the sentence\nlevel or passage level, we propose FactSelfCheck, a novel black-box\nsampling-based method that enables fine-grained fact-level detection. Our\napproach represents text as knowledge graphs consisting of facts in the form of\ntriples. Through analyzing factual consistency across multiple LLM responses,\nwe compute fine-grained hallucination scores without requiring external\nresources or training data. Our evaluation demonstrates that FactSelfCheck\nperforms competitively with leading sentence-level sampling-based methods while\nproviding more detailed insights. Most notably, our fact-level approach\nsignificantly improves hallucination correction, achieving a 35.5% increase in\nfactual content compared to the baseline, while sentence-level SelfCheckGPT\nyields only a 10.6% improvement. The granular nature of our detection enables\nmore precise identification and correction of hallucinated content.\nAdditionally, we contribute a new dataset for evaluating sampling-based methods\n- FavaMultiSamples."}
{"id": "2503.18034", "pdf": "https://arxiv.org/pdf/2503.18034.pdf", "abs": "https://arxiv.org/abs/2503.18034", "title": "Expanding the Boundaries of Vision Prior Knowledge in Multi-modal Large Language Models", "authors": ["Qiao Liang", "Yanjiang Liu", "Weixiang Zhou", "Ben He", "Yaojie Lu", "Hongyu Lin", "Jia Zheng", "Xianpei Han", "Le Sun", "Yingfei Sun"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Does the prior knowledge of the vision encoder constrain the capability\nboundary of Multi-modal Large Language Models (MLLMs)? While most existing\nresearch treats MLLMs as unified systems optimized through end-to-end training,\nthe impact of vision encoder's prior knowledge is seldom investigated. In this\nwork, we introduce a novel metric, $Rank_e$, to quantify the effect of prior\nknowledge of the vision encoder on MLLM performance. Our analysis reveals a\npositive correlation between prior knowledge and MLLM performance. Moreover, we\nfind that domain-specific fine-tuning using solely end-to-end visual question\nanswering (VQA) data is insufficient, particularly for entities with low\ninherent visual prior knowledge. To address this issue, we propose VisPRE\n(Vision Prior Remediation), a two-stage training framework that explicitly\nincorporates prior knowledge at the vision encoder level. Experimental results\ndemonstrate that augmenting vision encoder's prior knowledge substantially\nboosts the visual understanding capabilities of MLLMs, offering a novel and\neffective strategy for improving performance, especially in scenarios involving\nuncommon visual entities."}
{"id": "2503.23083", "pdf": "https://arxiv.org/pdf/2503.23083.pdf", "abs": "https://arxiv.org/abs/2503.23083", "title": "Efficient Adaptation For Remote Sensing Visual Grounding", "authors": ["Hasan Moughnieh", "Mohamad Chalhoub", "Hasan Nasrallah", "Cristiano Nattero", "Paolo Campanella", "Giovanni Nico", "Ali J. Ghandour"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Adapting pre-trained models has become an effective strategy in artificial\nintelligence, offering a scalable and efficient alternative to training models\nfrom scratch. In the context of remote sensing (RS), where visual grounding(VG)\nremains underexplored, this approach enables the deployment of powerful\nvision-language models to achieve robust cross-modal understanding while\nsignificantly reducing computational overhead. To address this, we applied\nParameter Efficient Fine Tuning (PEFT) techniques to adapt these models for\nRS-specific VG tasks. Specifically, we evaluated LoRA placement across\ndifferent modules in Grounding DINO and used BitFit and adapters to fine-tune\nthe OFA foundation model pre-trained on general-purpose VG datasets. This\napproach achieved performance comparable to or surpassing current State Of The\nArt (SOTA) models while significantly reducing computational costs. This study\nhighlights the potential of PEFT techniques to advance efficient and precise\nmulti-modal analysis in RS, offering a practical and cost-effective alternative\nto full model training."}
{"id": "2504.00587", "pdf": "https://arxiv.org/pdf/2504.00587.pdf", "abs": "https://arxiv.org/abs/2504.00587", "title": "AgentNet: Decentralized Evolutionary Coordination for LLM-based Multi-Agent Systems", "authors": ["Yingxuan Yang", "Huacan Chai", "Shuai Shao", "Yuanyi Song", "Siyuan Qi", "Renting Rui", "Weinan Zhang"], "categories": ["cs.MA", "cs.CL"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has enabled the\ndevelopment of multi-agent systems where multiple LLM-based agents collaborate\non complex tasks. However, existing systems often rely on centralized\ncoordination, leading to scalability bottlenecks, reduced adaptability, and\nsingle points of failure. Privacy and proprietary knowledge concerns further\nhinder cross-organizational collaboration, resulting in siloed expertise. We\npropose AgentNet, a decentralized, Retrieval-Augmented Generation (RAG)-based\nframework that enables LLM-based agents to specialize, evolve, and collaborate\nautonomously in a dynamically structured Directed Acyclic Graph (DAG). Unlike\nprior approaches with static roles or centralized control, AgentNet allows\nagents to adjust connectivity and route tasks based on local expertise and\ncontext. AgentNet introduces three key innovations: (1) a fully decentralized\ncoordination mechanism that eliminates the need for a central orchestrator,\nenhancing robustness and emergent intelligence; (2) dynamic agent graph\ntopology that adapts in real time to task demands, ensuring scalability and\nresilience; and (3) a retrieval-based memory system for agents that supports\ncontinual skill refinement and specialization. By minimizing centralized\ncontrol and data exchange, AgentNet enables fault-tolerant, privacy-preserving\ncollaboration across organizations. Experiments show that AgentNet achieves\nhigher task accuracy than both single-agent and centralized multi-agent\nbaselines."}
{"id": "2504.02922", "pdf": "https://arxiv.org/pdf/2504.02922.pdf", "abs": "https://arxiv.org/abs/2504.02922", "title": "Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning", "authors": ["Julian Minder", "Clément Dumas", "Caden Juang", "Bilal Chugtai", "Neel Nanda"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "42 pages, 31 figures", "summary": "Model diffing is the study of how fine-tuning changes a model's\nrepresentations and internal algorithms. Many behaviors of interest are\nintroduced during fine-tuning, and model diffing offers a promising lens to\ninterpret such behaviors. Crosscoders are a recent model diffing method that\nlearns a shared dictionary of interpretable concepts represented as latent\ndirections in both the base and fine-tuned models, allowing us to track how\nconcepts shift or emerge during fine-tuning. Notably, prior work has observed\nconcepts with no direction in the base model, and it was hypothesized that\nthese model-specific latents were concepts introduced during fine-tuning.\nHowever, we identify two issues which stem from the crosscoders L1 training\nloss that can misattribute concepts as unique to the fine-tuned model, when\nthey really exist in both models. We develop Latent Scaling to flag these\nissues by more accurately measuring each latent's presence across models. In\nexperiments comparing Gemma 2 2B base and chat models, we observe that the\nstandard crosscoder suffers heavily from these issues. Building on these\ninsights, we train a crosscoder with BatchTopK loss and show that it\nsubstantially mitigates these issues, finding more genuinely chat-specific and\nhighly interpretable concepts. We recommend practitioners adopt similar\ntechniques. Using the BatchTopK crosscoder, we successfully identify a set of\nchat-specific latents that are both interpretable and causally effective,\nrepresenting concepts such as $\\textit{false information}$ and\n$\\textit{personal question}$, along with multiple refusal-related latents that\nshow nuanced preferences for different refusal triggers. Overall, our work\nadvances best practices for the crosscoder-based methodology for model diffing\nand demonstrates that it can provide concrete insights into how chat-tuning\nmodifies model behavior."}
{"id": "2504.05258", "pdf": "https://arxiv.org/pdf/2504.05258.pdf", "abs": "https://arxiv.org/abs/2504.05258", "title": "Learning to Reason Over Time: Timeline Self-Reflection for Improved Temporal Reasoning in Language Models", "authors": ["Adrián Bazaga", "Rexhina Blloshmi", "Bill Byrne", "Adrià de Gispert"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ACL 2025 (Main)", "summary": "Large Language Models (LLMs) have emerged as powerful tools for generating\ncoherent text, understanding context, and performing reasoning tasks. However,\nthey struggle with temporal reasoning, which requires processing time-related\ninformation such as event sequencing, durations, and inter-temporal\nrelationships. These capabilities are critical for applications including\nquestion answering, scheduling, and historical analysis. In this paper, we\nintroduce TISER, a novel framework that enhances the temporal reasoning\nabilities of LLMs through a multi-stage process that combines timeline\nconstruction with iterative self-reflection. Our approach leverages test-time\nscaling to extend the length of reasoning traces, enabling models to capture\ncomplex temporal dependencies more effectively. This strategy not only boosts\nreasoning accuracy but also improves the traceability of the inference process.\nExperimental results demonstrate state-of-the-art performance across multiple\nbenchmarks, including out-of-distribution test sets, and reveal that TISER\nenables smaller open-source models to surpass larger closed-weight models on\nchallenging temporal reasoning tasks."}
{"id": "2504.19062", "pdf": "https://arxiv.org/pdf/2504.19062.pdf", "abs": "https://arxiv.org/abs/2504.19062", "title": "Versatile Framework for Song Generation with Prompt-based Control", "authors": ["Yu Zhang", "Wenxiang Guo", "Changhao Pan", "Zhiyuan Zhu", "Ruiqi Li", "Jingyu Lu", "Rongjie Huang", "Ruiyuan Zhang", "Zhiqing Hong", "Ziyue Jiang", "Zhou Zhao"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": null, "summary": "Song generation focuses on producing controllable high-quality songs based on\nvarious prompts. However, existing methods struggle to generate vocals and\naccompaniments with prompt-based control and proper alignment. Additionally,\nthey fall short in supporting various tasks. To address these challenges, we\nintroduce VersBand, a multi-task song generation framework for synthesizing\nhigh-quality, aligned songs with prompt-based control. VersBand comprises these\nprimary models: 1) VocalBand, a decoupled model, leverages the flow-matching\nmethod for generating singing styles, pitches, and mel-spectrograms, allowing\nfast, high-quality vocal generation with style control. 2) AccompBand, a\nflow-based transformer model, incorporates the Band-MOE, selecting suitable\nexperts for enhanced quality, alignment, and control. This model allows for\ngenerating controllable, high-quality accompaniments aligned with vocals. 3)\nTwo generation models, LyricBand for lyrics and MelodyBand for melodies,\ncontribute to the comprehensive multi-task song generation system, allowing for\nextensive control based on multiple prompts. Experimental results demonstrate\nthat VersBand performs better over baseline models across multiple song\ngeneration tasks using objective and subjective metrics. Audio samples are\navailable at https://aaronz345.github.io/VersBandDemo."}
{"id": "2504.21578", "pdf": "https://arxiv.org/pdf/2504.21578.pdf", "abs": "https://arxiv.org/abs/2504.21578", "title": "Glucagon and insulin production in pancreatic cells modeled using Petri nets and Boolean networks", "authors": ["Kamila Barylska", "Franck Delaplace", "Anna Gogolińska", "Ewa Pańkowska"], "categories": ["q-bio.CB", "cs.CL", "03", "F.2; G.0"], "comment": null, "summary": "Diabetes is a civilization chronic disease characterized by a constant\nelevated concentration of glucose in the blood. Many processes are involved in\nthe glucose regulation, and their interactions are very complex. To better\nunderstand those processes we set ourselves a goal to create a Petri net model\nof the glucose regulation in the whole body. So far we have managed to create a\nmodel of glycolysis and synthesis of glucose in the liver, and the general\noverview models of the glucose regulation in a healthy and diabetic person. In\nthis paper we introduce Petri nets models of insulin secretion in beta cell of\nthe pancreas, and glucagon in the pancreas alpha cells. Those two hormones have\nmutually opposite effects: insulin preventing hyperglycemia, and glucagon\npreventing hypoglycemia. Understanding the mechanisms of insulin and glucagon\nsecretion constitutes the basis for understanding diabetes. We also present a\nmodel in which both processes occur together, depending on the blood glucose\nlevel. The dynamics of each model is analysed. Additionally, we transform the\noverall insulin and glucagon secretion system to a Boolean network, following\nstandard transformation rules."}
{"id": "2505.02746", "pdf": "https://arxiv.org/pdf/2505.02746.pdf", "abs": "https://arxiv.org/abs/2505.02746", "title": "Using Knowledge Graphs to harvest datasets for efficient CLIP model training", "authors": ["Simon Ging", "Sebastian Walter", "Jelena Bratulić", "Johannes Dienert", "Hannah Bast", "Thomas Brox"], "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Training high-quality CLIP models typically requires enormous datasets, which\nlimits the development of domain-specific models -- especially in areas that\neven the largest CLIP models do not cover well -- and drives up training costs.\nThis poses challenges for scientific research that needs fine-grained control\nover the training procedure of CLIP models. In this work, we show that by\nemploying smart web search strategies enhanced with knowledge graphs, a robust\nCLIP model can be trained from scratch with considerably less data.\nSpecifically, we demonstrate that an expert foundation model for living\norganisms can be built using just 10M images. Moreover, we introduce EntityNet,\na dataset comprising 33M images paired with 46M text descriptions, which\nenables the training of a generic CLIP model in significantly reduced time."}
{"id": "2505.05528", "pdf": "https://arxiv.org/pdf/2505.05528.pdf", "abs": "https://arxiv.org/abs/2505.05528", "title": "X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP", "authors": ["Hanxun Huang", "Sarah Erfani", "Yige Li", "Xingjun Ma", "James Bailey"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "As Contrastive Language-Image Pre-training (CLIP) models are increasingly\nadopted for diverse downstream tasks and integrated into large vision-language\nmodels (VLMs), their susceptibility to adversarial perturbations has emerged as\na critical concern. In this work, we introduce \\textbf{X-Transfer}, a novel\nattack method that exposes a universal adversarial vulnerability in CLIP.\nX-Transfer generates a Universal Adversarial Perturbation (UAP) capable of\ndeceiving various CLIP encoders and downstream VLMs across different samples,\ntasks, and domains. We refer to this property as \\textbf{super\ntransferability}--a single perturbation achieving cross-data, cross-domain,\ncross-model, and cross-task adversarial transferability simultaneously. This is\nachieved through \\textbf{surrogate scaling}, a key innovation of our approach.\nUnlike existing methods that rely on fixed surrogate models, which are\ncomputationally intensive to scale, X-Transfer employs an efficient surrogate\nscaling strategy that dynamically selects a small subset of suitable surrogates\nfrom a large search space. Extensive evaluations demonstrate that X-Transfer\nsignificantly outperforms previous state-of-the-art UAP methods, establishing a\nnew benchmark for adversarial transferability across CLIP models. The code is\npublicly available in our\n\\href{https://github.com/HanxunH/XTransferBench}{GitHub repository}."}
{"id": "2505.08052", "pdf": "https://arxiv.org/pdf/2505.08052.pdf", "abs": "https://arxiv.org/abs/2505.08052", "title": "NAZM: Network Analysis of Zonal Metrics in Persian Poetic Tradition", "authors": ["Kourosh Shahnazari", "Seyed Moein Ayyoubzadeh", "Mohammadamin Fazli", "Mohammadali Keshtparvar"], "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "This study formalizes a computational model to simulate classical Persian\npoets' dynamics of influence through constructing a multi-dimensional\nsimilarity network. Using a rigorously curated dataset based on Ganjoor's\ncorpus, we draw upon semantic, lexical, stylistic, thematic, and metrical\nfeatures to demarcate each poet's corpus. Each is contained within weighted\nsimilarity matrices, which are then appended to generate an aggregate graph\nshowing poet-to-poet influence. Further network investigation is carried out to\nidentify key poets, style hubs, and bridging poets by calculating degree,\ncloseness, betweenness, eigenvector, and Katz centrality measures. Further, for\ntypological insight, we use the Louvain community detection algorithm to\ndemarcate clusters of poets sharing both style and theme coherence, which\ncorrespond closely to acknowledged schools of literature like Sabk-e Hindi,\nSabk-e Khorasani, and the Bazgasht-e Adabi phenomenon. Our findings provide a\nnew data-driven view of Persian literature distinguished between canonical\nsignificance and interextual influence, thus highlighting relatively\nlesser-known figures who hold great structural significance. Combining\ncomputational linguistics with literary study, this paper produces an\ninterpretable and scalable model for poetic tradition, enabling retrospective\nreflection as well as forward-looking research within digital humanities."}
{"id": "2505.10844", "pdf": "https://arxiv.org/pdf/2505.10844.pdf", "abs": "https://arxiv.org/abs/2505.10844", "title": "Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models", "authors": ["Simeng Han", "Stephen Xia", "Grant Zhang", "Howard Dai", "Chen Liu", "Lichang Chen", "Hoang Huy Nguyen", "Hongyuan Mei", "Jiayuan Mao", "R. Thomas McCoy"], "categories": ["cs.AI", "cs.CL"], "comment": "13 Tables; 5 Figures", "summary": "Accuracy remains a standard metric for evaluating AI systems, but it offers\nlimited insight into how models arrive at their solutions. In this work, we\nintroduce a benchmark based on brainteasers written in long narrative form to\nprobe more deeply into the types of reasoning strategies that models use.\nBrainteasers are well-suited for this goal because they can be solved with\nmultiple approaches, such as a few-step solution that uses a creative insight\nor a longer solution that uses more brute force. We investigate large language\nmodels (LLMs) across multiple layers of reasoning, focusing not only on\ncorrectness but also on the quality and creativity of their solutions. We\ninvestigate many aspects of the reasoning process: (1) semantic parsing of the\nbrainteasers into precise mathematical competition style formats; (2)\ngenerating solutions from these mathematical forms; (3) self-correcting\nsolutions based on gold solutions; (4) producing step-by-step sketches of\nsolutions; and (5) making use of hints. We find that LLMs are in many cases\nable to find creative, insightful solutions to brainteasers, suggesting that\nthey capture some of the capacities needed to solve novel problems in creative\nways. Nonetheless, there also remain situations where they rely on brute force\ndespite the availability of more efficient, creative solutions, highlighting a\npotential direction for improvement in the reasoning abilities of LLMs."}
{"id": "2505.14449", "pdf": "https://arxiv.org/pdf/2505.14449.pdf", "abs": "https://arxiv.org/abs/2505.14449", "title": "Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach", "authors": ["Yi-Cheng Lin", "Huang-Cheng Chou", "Hung-yi Lee"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted by InterSpeech 2025. 7 pages including 2 pages of appendix", "summary": "While subgroup disparities and performance bias are increasingly studied in\ncomputational research, fairness in categorical Speech Emotion Recognition\n(SER) remains underexplored. Existing methods often rely on explicit\ndemographic labels, which are difficult to obtain due to privacy concerns. To\naddress this limitation, we introduce an Implicit Demography Inference (IDI)\nmodule that leverages pseudo-labeling from a pre-trained model and unsupervised\nlearning using k-means clustering to mitigate bias in SER. Our experiments show\nthat pseudo-labeling IDI reduces subgroup disparities, improving fairness\nmetrics by over 28% with less than a 2% decrease in SER accuracy. Also, the\nunsupervised IDI yields more than a 4.6% improvement in fairness metrics with a\ndrop of less than 3.6% in SER performance. Further analyses reveal that the\nunsupervised IDI consistently mitigates race and age disparities, demonstrating\nits potential when explicit demographic information is unavailable."}
{"id": "2505.14910", "pdf": "https://arxiv.org/pdf/2505.14910.pdf", "abs": "https://arxiv.org/abs/2505.14910", "title": "TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis", "authors": ["Yu Zhang", "Wenxiang Guo", "Changhao Pan", "Dongyu Yao", "Zhiyuan Zhu", "Ziyue Jiang", "Yuhan Wang", "Tao Jin", "Zhou Zhao"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted by Findings of ACL 2025", "summary": "Customizable multilingual zero-shot singing voice synthesis (SVS) has various\npotential applications in music composition and short video dubbing. However,\nexisting SVS models overly depend on phoneme and note boundary annotations,\nlimiting their robustness in zero-shot scenarios and producing poor transitions\nbetween phonemes and notes. Moreover, they also lack effective multi-level\nstyle control via diverse prompts. To overcome these challenges, we introduce\nTCSinger 2, a multi-task multilingual zero-shot SVS model with style transfer\nand style control based on various prompts. TCSinger 2 mainly includes three\nkey modules: 1) Blurred Boundary Content (BBC) Encoder, predicts duration,\nextends content embedding, and applies masking to the boundaries to enable\nsmooth transitions. 2) Custom Audio Encoder, uses contrastive learning to\nextract aligned representations from singing, speech, and textual prompts. 3)\nFlow-based Custom Transformer, leverages Cus-MOE, with F0 supervision,\nenhancing both the synthesis quality and style modeling of the generated\nsinging voice. Experimental results show that TCSinger 2 outperforms baseline\nmodels in both subjective and objective metrics across multiple related tasks.\nSinging voice samples are available at\nhttps://aaronz345.github.io/TCSinger2Demo/."}
{"id": "2505.15072", "pdf": "https://arxiv.org/pdf/2505.15072.pdf", "abs": "https://arxiv.org/abs/2505.15072", "title": "MoTime: A Dataset Suite for Multimodal Time Series Forecasting", "authors": ["Xin Zhou", "Weiqing Wang", "Francisco J. Baldán", "Wray Buntine", "Christoph Bergmeir"], "categories": ["cs.LG", "cs.CL", "cs.DB", "cs.IR"], "comment": null, "summary": "While multimodal data sources are increasingly available from real-world\nforecasting, most existing research remains on unimodal time series. In this\nwork, we present MoTime, a suite of multimodal time series forecasting datasets\nthat pair temporal signals with external modalities such as text, metadata, and\nimages. Covering diverse domains, MoTime supports structured evaluation of\nmodality utility under two scenarios: 1) the common forecasting task, where\nvarying-length history is available, and 2) cold-start forecasting, where no\nhistorical data is available. Experiments show that external modalities can\nimprove forecasting performance in both scenarios, with particularly strong\nbenefits for short series in some datasets, though the impact varies depending\non data characteristics. By making datasets and findings publicly available, we\naim to support more comprehensive and realistic benchmarks in future multimodal\ntime series forecasting research."}
{"id": "2505.16400", "pdf": "https://arxiv.org/pdf/2505.16400.pdf", "abs": "https://arxiv.org/abs/2505.16400", "title": "AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning", "authors": ["Yang Chen", "Zhuolin Yang", "Zihan Liu", "Chankyu Lee", "Peng Xu", "Mohammad Shoeybi", "Bryan Catanzaro", "Wei Ping"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "We release the models at:\n  https://huggingface.co/collections/nvidia/acereason-682f4e1261dc22f697fd1485", "summary": "Despite recent progress in large-scale reinforcement learning (RL) for\nreasoning, the training recipe for building high-performing reasoning models\nremains elusive. Key implementation details of frontier models, such as\nDeepSeek-R1, including data curation strategies and RL training recipe, are\noften omitted. Moreover, recent research indicates distillation remains more\neffective than RL for smaller models. In this work, we demonstrate that\nlarge-scale RL can significantly enhance the reasoning capabilities of strong,\nsmall- and mid-sized models, achieving results that surpass those of\nstate-of-the-art distillation-based models. We systematically study the RL\ntraining process through extensive ablations and propose a simple yet effective\napproach: first training on math-only prompts, then on code-only prompts.\nNotably, we find that math-only RL not only significantly enhances the\nperformance of strong distilled models on math benchmarks (e.g., +14.6% /\n+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks\n(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,\nextended code-only RL iterations further improve performance on code benchmarks\nwith minimal or no degradation in math results. We develop a robust data\ncuration pipeline to collect challenging prompts with high-quality, verifiable\nanswers and test cases to enable verification-based RL across both domains.\nFinally, we identify key experimental insights, including curriculum learning\nwith progressively increasing response lengths and the stabilizing effect of\non-policy parameter updates. We find that RL not only elicits the foundational\nreasoning capabilities acquired during pretraining and supervised fine-tuning\n(e.g., distillation), but also pushes the limits of the model's reasoning\nability, enabling it to solve problems that were previously unsolvable."}
{"id": "2505.17072", "pdf": "https://arxiv.org/pdf/2505.17072.pdf", "abs": "https://arxiv.org/abs/2505.17072", "title": "Safety Alignment Can Be Not Superficial With Explicit Safety Signals", "authors": ["Jianwei Li", "Jung-Eun Kim"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "Recent studies on the safety alignment of large language models (LLMs) have\nrevealed that existing approaches often operate superficially, leaving models\nvulnerable to various adversarial attacks. Despite their significance, these\nstudies generally fail to offer actionable solutions beyond data augmentation\nfor achieving more robust safety mechanisms. This paper identifies a\nfundamental cause of this superficiality: existing alignment approaches often\npresume that models can implicitly learn a safety-related reasoning task during\nthe alignment process, enabling them to refuse harmful requests. However, the\nlearned safety signals are often diluted by other competing objectives, leading\nmodels to struggle with drawing a firm safety-conscious decision boundary when\nconfronted with adversarial attacks. Based on this observation, by explicitly\nintroducing a safety-related binary classification task and integrating its\nsignals with our attention and decoding strategies, we eliminate this ambiguity\nand allow models to respond more responsibly to malicious queries. We emphasize\nthat, with less than 0.2x overhead cost, our approach enables LLMs to assess\nthe safety of both the query and the previously generated tokens at each\nnecessary generating step. Extensive experiments demonstrate that our method\nsignificantly improves the resilience of LLMs against various adversarial\nattacks, offering a promising pathway toward more robust generative AI systems."}
{"id": "2505.18134", "pdf": "https://arxiv.org/pdf/2505.18134.pdf", "abs": "https://arxiv.org/abs/2505.18134", "title": "VideoGameBench: Can Vision-Language Models complete popular video games?", "authors": ["Alex L. Zhang", "Thomas L. Griffiths", "Karthik R. Narasimhan", "Ofir Press"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "9 pages, 33 pages including supplementary", "summary": "Vision-language models (VLMs) have achieved strong results on coding and math\nbenchmarks that are challenging for humans, yet their ability to perform tasks\nthat come naturally to humans--such as perception, spatial navigation, and\nmemory management--remains understudied. Real video games are crafted to be\nintuitive for humans to learn and master by leveraging innate inductive biases,\nmaking them an ideal testbed for evaluating such capabilities in VLMs. To this\nend, we introduce VideoGameBench, a benchmark consisting of 10 popular video\ngames from the 1990s that VLMs directly interact with in real-time.\nVideoGameBench challenges models to complete entire games with access to only\nraw visual inputs and a high-level description of objectives and controls, a\nsignificant departure from existing setups that rely on game-specific\nscaffolding and auxiliary information. We keep three of the games secret to\nencourage solutions that generalize to unseen environments. Our experiments\nshow that frontier vision-language models struggle to progress beyond the\nbeginning of each game. We find inference latency to be a major limitation of\nfrontier models in the real-time setting; therefore, we introduce\nVideoGameBench Lite, a setting where the game pauses while waiting for the LM's\nnext action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of\nVideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization\nof the human skills mentioned above into this benchmark motivates progress in\nthese research directions."}
{"id": "2505.21785", "pdf": "https://arxiv.org/pdf/2505.21785.pdf", "abs": "https://arxiv.org/abs/2505.21785", "title": "Born a Transformer -- Always a Transformer?", "authors": ["Yana Veitsman", "Mayank Jobanputra", "Yash Sarrof", "Aleksandra Bakalova", "Vera Demberg", "Ellie Pavlick", "Michael Hahn"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transformers have theoretical limitations in modeling certain\nsequence-to-sequence tasks, yet it remains largely unclear if these limitations\nplay a role in large-scale pretrained LLMs, or whether LLMs might effectively\novercome these constraints in practice due to the scale of both the models\nthemselves and their pretraining data. We explore how these architectural\nconstraints manifest after pretraining, by studying a family of\n$\\textit{retrieval}$ and $\\textit{copying}$ tasks inspired by Liu et al.\n[2024a]. We use a recently proposed framework for studying length\ngeneralization [Huang et al., 2025] to provide guarantees for each of our\nsettings. Empirically, we observe an $\\textit{induction-versus-anti-induction}$\nasymmetry, where pretrained models are better at retrieving tokens to the right\n(induction) rather than the left (anti-induction) of a query token. This\nasymmetry disappears upon targeted fine-tuning if length-generalization is\nguaranteed by theory. Mechanistic analysis reveals that this asymmetry is\nconnected to the differences in the strength of induction versus anti-induction\ncircuits within pretrained transformers. We validate our findings through\npractical experiments on real-world tasks demonstrating reliability risks. Our\nresults highlight that pretraining selectively enhances certain transformer\ncapabilities, but does not overcome fundamental length-generalization limits."}
