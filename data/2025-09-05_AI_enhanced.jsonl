{"id": "2509.03525", "pdf": "https://arxiv.org/pdf/2509.03525.pdf", "abs": "https://arxiv.org/abs/2509.03525", "title": "Speech-Based Cognitive Screening: A Systematic Evaluation of LLM Adaptation Strategies", "authors": ["Fatemeh Taherinezhad", "Mohamad Javad Momeni Nezhad", "Sepehr Karimi", "Sina Rashidi", "Ali Zolnour", "Maryam Dadkhah", "Yasaman Haghbin", "Hossein AzadMaleki", "Maryam Zolnoori"], "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": null, "summary": "Over half of US adults with Alzheimer disease and related dementias remain\nundiagnosed, and speech-based screening offers a scalable detection approach.\nWe compared large language model adaptation strategies for dementia detection\nusing the DementiaBank speech corpus, evaluating nine text-only models and\nthree multimodal audio-text models on recordings from DementiaBank speech\ncorpus. Adaptations included in-context learning with different demonstration\nselection policies, reasoning-augmented prompting, parameter-efficient\nfine-tuning, and multimodal integration. Results showed that class-centroid\ndemonstrations achieved the highest in-context learning performance, reasoning\nimproved smaller models, and token-level fine-tuning generally produced the\nbest scores. Adding a classification head substantially improved\nunderperforming models. Among multimodal models, fine-tuned audio-text systems\nperformed well but did not surpass the top text-only models. These findings\nhighlight that model adaptation strategies, including demonstration selection,\nreasoning design, and tuning method, critically influence speech-based dementia\ndetection, and that properly adapted open-weight models can match or exceed\ncommercial systems.", "AI": {"tldr": "This paper explores language model adaptation strategies for detecting dementia using speech data, finding that certain methods significantly enhance performance.", "motivation": "Over half of US adults with Alzheimer disease and related dementias remain undiagnosed, highlighting the need for scalable detection methods like speech-based screening.", "method": "The authors evaluated nine text-only models and three multimodal audio-text models using the DementiaBank speech corpus, testing various adaptation strategies such as in-context learning and parameter-efficient fine-tuning.", "result": "Class-centroid demonstrations showed the best performance in in-context learning; reasoning improved smaller models; token-level fine-tuning generated the highest scores overall.", "conclusion": "Model adaptation strategies critically influence speech-based dementia detection, with well-adapted open-weight models matching or exceeding the performance of commercial systems.", "key_contributions": ["Evaluation of various language model adaptation strategies for dementia detection", "Demonstrated the effectiveness of class-centroid demonstrations and token-level fine-tuning", "Highlighted the comparative performance of multimodal versus text-only models"], "limitations": "", "keywords": ["dementia detection", "language model adaptation", "speech-based screening"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.03526", "pdf": "https://arxiv.org/pdf/2509.03526.pdf", "abs": "https://arxiv.org/abs/2509.03526", "title": "Enhancing Speech Large Language Models through Reinforced Behavior Alignment", "authors": ["Yansong Liu", "Jiateng Li", "Yuan Liu"], "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "The recent advancements of Large Language Models (LLMs) have spurred\nconsiderable research interest in extending their linguistic capabilities\nbeyond text to other modalities, which leads to emergence of speech-based LLMs\n(SpeechLMs) with capability of processing user request in either speech or\ntextual formats. However, owing to inter-modal discrepancies, these SpeechLMs\nstill exhibit a significant performance gap compared to their text-based LLM\ncounterparts in instruction-following, particularly when confronted with the\ndynamic and variable nature of user speech. To address this challenge, this\npaper introduces a framework termed Reinforced Behavior Alignment (RBA),\ndesigned to bolster the language generation proficiency of SpeechLMs. Instead\nof relying on supervised fine-tuning from human annotations, RBA employs a\nself-synthesis methodology to generate extensive, high-fidelity alignment data\nby a powerful teacher LLM. Then SpeechLMs is aligned its behavior with that of\na teacher using a reinforcement learning-based approach. Experimental results\ndemonstrate that this method effectively enhances the instruction-following\ncapabilities of SpeechLMs that outperform conventional distillation baselines.\nCrucially, we demonstrate that RBA can be seamlessly extended to tasks such\nincluding spoken question answering and speech-to-text translation, attaining\nstate-of-the-art performance on open benchmarks with only self-generated data.", "AI": {"tldr": "This paper introduces Reinforced Behavior Alignment (RBA) to improve the instruction-following capabilities of Speech-based LLMs (SpeechLMs) through self-synthesis of alignment data and reinforcement learning techniques.", "motivation": "To enhance the performance of SpeechLMs in instruction-following tasks and address the performance gap compared to text-based LLMs due to inter-modal discrepancies.", "method": "The RBA framework utilizes self-synthesis to generate high-fidelity alignment data and employs a reinforcement learning approach to align SpeechLM behavior with a teacher LLM.", "result": "Experimental results indicate that RBA significantly boosts instruction-following capabilities of SpeechLMs, surpassing conventional distillation baselines and achieving state-of-the-art performance in tasks like spoken question answering and speech-to-text translation.", "conclusion": "RBA offers a robust method for improving SpeechLMs by leveraging self-generated alignment data, leading to better performance across various speech processing tasks.", "key_contributions": ["Introduced the RBA framework for aligning SpeechLMs", "Utilized self-synthesis for generating alignment data", "Achieved state-of-the-art performance on open benchmarks for SpeechLMs."], "limitations": "", "keywords": ["Speech-based LLMs", "Reinforcement Learning", "Instruction-Following"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.03527", "pdf": "https://arxiv.org/pdf/2509.03527.pdf", "abs": "https://arxiv.org/abs/2509.03527", "title": "Multilevel Analysis of Cryptocurrency News using RAG Approach with Fine-Tuned Mistral Large Language Model", "authors": ["Bohdan M. Pavlyshenko"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the paper, we consider multilevel multitask analysis of cryptocurrency\nnews using a fine-tuned Mistral 7B large language model with\nretrieval-augmented generation (RAG).\n  On the first level of analytics, the fine-tuned model generates graph and\ntext summaries with sentiment scores as well as JSON representations of\nsummaries. Higher levels perform hierarchical stacking that consolidates sets\nof graph-based and text-based summaries as well as summaries of summaries into\ncomprehensive reports. The combination of graph and text summaries provides\ncomplementary views of cryptocurrency news. The model is fine-tuned with 4-bit\nquantization using the PEFT/LoRA approach. The representation of cryptocurrency\nnews as knowledge graph can essentially eliminate problems with large language\nmodel hallucinations.\n  The obtained results demonstrate that the use of fine-tuned Mistral 7B LLM\nmodels for multilevel cryptocurrency news analysis can conduct informative\nqualitative and quantitative analytics, providing important insights.", "AI": {"tldr": "The paper discusses a multilevel multitask analysis of cryptocurrency news using a fine-tuned Mistral 7B LLM with retrieval-augmented generation (RAG), focusing on generating informative summaries and mitigating hallucinations through knowledge graphs.", "motivation": "To improve the analysis of cryptocurrency news by integrating text and graph summaries while addressing hallucinations in large language models.", "method": "Utilizes a fine-tuned Mistral 7B model with retrieval-augmented generation to produce and consolidate graph and text summaries with sentiment scores and JSON representations, employing hierarchical stacking for comprehensive reporting.", "result": "Demonstrated that the model can effectively generate informative qualitative and quantitative analytics on cryptocurrency news, leading to essential insights.", "conclusion": "The multilevel approach with the fine-tuned Mistral model significantly enhances the analysis of cryptocurrency news, offering robust insights while reducing hallucination issues.", "key_contributions": ["Integration of graph and text summaries for comprehensive news analysis", "Application of fine-tuned Mistral 7B model in a novel RAG framework", "Utilization of knowledge graphs to mitigate LLM hallucinations"], "limitations": "Focuses solely on cryptocurrency news; results may not generalize across other domains.", "keywords": ["cryptocurrency", "multilevel analysis", "large language model", "knowledge graph", "sentiment analysis"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2509.03528", "pdf": "https://arxiv.org/pdf/2509.03528.pdf", "abs": "https://arxiv.org/abs/2509.03528", "title": "The ProLiFIC dataset: Leveraging LLMs to Unveil the Italian Lawmaking Process", "authors": ["Matilde Contestabile", "Chiara Ferrara", "Alberto Giovannetti", "Giovanni Parrillo", "Andrea Vandin"], "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Process Mining (PM), initially developed for industrial and business\ncontexts, has recently been applied to social systems, including legal ones.\nHowever, PM's efficacy in the legal domain is limited by the accessibility and\nquality of datasets. We introduce ProLiFIC (Procedural Lawmaking Flow in\nItalian Chambers), a comprehensive event log of the Italian lawmaking process\nfrom 1987 to 2022. Created from unstructured data from the Normattiva portal\nand structured using large language models (LLMs), ProLiFIC aligns with recent\nefforts in integrating PM with LLMs. We exemplify preliminary analyses and\npropose ProLiFIC as a benchmark for legal PM, fostering new developments.", "AI": {"tldr": "ProLiFIC is an event log of the Italian lawmaking process, leveraging LLMs to enhance process mining in the legal field.", "motivation": "The study aims to improve the application of process mining in the legal domain, addressing limitations related to dataset accessibility and quality.", "method": "ProLiFIC was created from unstructured data sourced from the Normattiva portal and structured using large language models to generate a comprehensive event log.", "result": "Initial analyses demonstrate the utility of ProLiFIC as a benchmark for legal process mining, showcasing its capacity to improve legal analytics.", "conclusion": "ProLiFIC can significantly enhance the integration of process mining with legal systems and encourages further advancements in the field.", "key_contributions": ["Introduction of a comprehensive event log for the Italian lawmaking process.", "Utilization of LLMs to structure unstructured legal data.", "Establishment of a benchmark for legal process mining."], "limitations": "", "keywords": ["Process Mining", "Legal Systems", "Large Language Models"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2509.03678", "pdf": "https://arxiv.org/pdf/2509.03678.pdf", "abs": "https://arxiv.org/abs/2509.03678", "title": "Promisedland: An XR Narrative Attraction Integrating Diorama-to-Virtual Workflow and Elemental Storytelling", "authors": ["Xianghan Wang", "Chingshuan Hsiao", "Shimei Qiu"], "categories": ["cs.HC", "cs.MM"], "comment": "Accepted to the Proceedings of the 2025 11th International Conference\n  on Virtual Reality (ICVR 2025). ISBN: 979-8-3503-9272-2. \\c{opyright} 2025\n  IEEE. This is the author-accepted manuscript. The final version will be\n  available via IEEE Xplore", "summary": "Promisedland is a mixed-reality (MR) narrative attraction that combines\ncultural storytelling, ecological education, and an innovative hybrid\nproduction workflow. Set in a future Earth suffering from elemental imbalance,\nusers embark on an interactive journey guided by symbolic characters to restore\nharmony through the collection of five classical elements: metal, wood, water,\nfire, and earth. To prototype this experience, we introduce a low-cost,\nhigh-fidelity Diorama-to-Virtual pipeline - handcrafting physical scale models,\n3D scanning, and integrating them into Unreal Engine. This process enables\nrapid spatial prototyping while preserving the material expressiveness and\nnarrative consistency of the physical environment. To further enhance\nimmersion, the experience incorporates a Stewart Platform to provide motion\nfeedback synchronized with the virtual ride dynamics, reinforcing spatial\npresence and embodied engagement. The final prototype runs on Meta Quest,\nsupporting dynamic interactions and real-time visual feedback. Promisedland\noffers a replicable design blueprint for future XR narrative installations\nacross museums, cultural exhibitions, and themed entertainment. It proposes a\nnew framework for XR Narrative Attractions - where physical and digital\nelements converge to deepen immersion, agency, and emotional engagement.", "AI": {"tldr": "Promisedland is a mixed-reality narrative attraction combining storytelling and ecological education, featuring a Diorama-to-Virtual pipeline for immersive user experiences.", "motivation": "To create an immersive mixed-reality experience that combines cultural storytelling and ecological education while addressing elemental imbalance on Earth.", "method": "Developed a low-cost, high-fidelity Diorama-to-Virtual pipeline using physical scale models, 3D scanning, and integration into Unreal Engine, coupled with a motion feedback Stewart Platform.", "result": "The prototype enhances user engagement through dynamic interactions and real-time visual feedback, running on Meta Quest and allowing for spatial prototyping with narrative consistency.", "conclusion": "Promisedland provides a replicable design blueprint for future XR narrative installations, suggesting a new framework for XR attractions that merges physical and digital elements.", "key_contributions": ["Introduced a novel Diorama-to-Virtual pipeline for mixed-reality experiences", "Demonstrated the use of a Stewart Platform for enhanced immersion", "Proposed a framework for XR Narrative Attractions combining physical and digital storytelling"], "limitations": "", "keywords": ["mixed-reality", "narrative attraction", "ecological education", "XR installations", "immersive storytelling"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.03529", "pdf": "https://arxiv.org/pdf/2509.03529.pdf", "abs": "https://arxiv.org/abs/2509.03529", "title": "Multimodal Proposal for an AI-Based Tool to Increase Cross-Assessment of Messages", "authors": ["Alejandro Álvarez Castro", "Joaquín Ordieres-Meré"], "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": "Presented at NLMLT2025 (https://airccse.org/csit/V15N16.html), 15\n  pages, 5 figures", "summary": "Earnings calls represent a uniquely rich and semi-structured source of\nfinancial communication, blending scripted managerial commentary with\nunscripted analyst dialogue. Although recent advances in financial sentiment\nanalysis have integrated multi-modal signals, such as textual content and vocal\ntone, most systems rely on flat document-level or sentence-level models,\nfailing to capture the layered discourse structure of these interactions. This\npaper introduces a novel multi-modal framework designed to generate\nsemantically rich and structurally aware embeddings of earnings calls, by\nencoding them as hierarchical discourse trees. Each node, comprising either a\nmonologue or a question-answer pair, is enriched with emotional signals derived\nfrom text, audio, and video, as well as structured metadata including coherence\nscores, topic labels, and answer coverage assessments. A two-stage transformer\narchitecture is proposed: the first encodes multi-modal content and discourse\nmetadata at the node level using contrastive learning, while the second\nsynthesizes a global embedding for the entire conference. Experimental results\nreveal that the resulting embeddings form stable, semantically meaningful\nrepresentations that reflect affective tone, structural logic, and thematic\nalignment. Beyond financial reporting, the proposed system generalizes to other\nhigh-stakes unscripted communicative domains such as tele-medicine, education,\nand political discourse, offering a robust and explainable approach to\nmulti-modal discourse representation. This approach offers practical utility\nfor downstream tasks such as financial forecasting and discourse evaluation,\nwhile also providing a generalizable method applicable to other domains\ninvolving high-stakes communication.", "AI": {"tldr": "This paper presents a novel multi-modal framework for generating embeddings of earnings calls using hierarchical discourse trees, integrating emotional signals and discourse metadata to improve sentiment analysis in various communicative domains.", "motivation": "The need for a more nuanced understanding of layered discourse in financial communication and other high-stakes interactions, which traditional models fail to capture.", "method": "A two-stage transformer architecture is proposed, with the first stage using contrastive learning to encode node-level multi-modal content and discourse metadata, while the second stage synthesizes a global embedding from the conference.", "result": "Experimental results demonstrate that the embeddings effectively capture affective tone, structural logic, and thematic alignment, yielding meaningful representations of earnings calls.", "conclusion": "The framework has practical applications for financial forecasting and discourse evaluation, and is adaptable for various high-stakes unscripted communicative contexts such as telemedicine and education.", "key_contributions": ["Introduces a hierarchical discourse tree framework for multi-modal embedding generation.", "Integrates emotional signals and structured metadata in the analysis of earnings calls.", "Demonstrates generalizability to other domains beyond finance, enhancing multi-modal discourse representation."], "limitations": "", "keywords": ["multi-modal learning", "earnings calls", "discourse analysis", "transformer architecture", "emotional signals"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2509.03693", "pdf": "https://arxiv.org/pdf/2509.03693.pdf", "abs": "https://arxiv.org/abs/2509.03693", "title": "Designing Effective AI Explanations for Misinformation Detection: A Comparative Study of Content, Social, and Combined Explanations", "authors": ["Yeaeun Gong", "Yifan Liu", "Lanyu Shang", "Na Wei", "Dong Wang"], "categories": ["cs.HC", "cs.MM"], "comment": "To appear at CSCW 2025", "summary": "In this paper, we study the problem of AI explanation of misinformation,\nwhere the goal is to identify explanation designs that help improve users'\nmisinformation detection abilities and their overall user experiences. Our work\nis motivated by the limitations of current Explainable AI (XAI) approaches,\nwhich predominantly focus on content explanations that elucidate the linguistic\nfeatures and sentence structures of the misinformation. To address this\nlimitation, we explore various explanations beyond content explanation, such as\n\"social explanation\" that considers the broader social context surrounding\nmisinformation, as well as a \"combined explanation\" where both the content and\nsocial explanations are presented in scenarios that are either aligned or\nmisaligned with each other. To evaluate the comparative effectiveness of these\nAI explanations, we conduct two online crowdsourcing experiments in the\nCOVID-19 (Study 1 on Prolific) and Politics domains (Study 2 on MTurk). Our\nresults show that AI explanations are generally effective in aiding users to\ndetect misinformation, with effectiveness significantly influenced by the\nalignment between content and social explanations. We also find that the order\nin which explanation types are presented - specifically, whether a content or\nsocial explanation comes first - can influence detection accuracy, with\ndifferences found between the COVID-19 and Political domains. This work\ncontributes towards more effective design of AI explanations, fostering a\ndeeper understanding of how different explanation types and their combinations\ninfluence misinformation detection.", "AI": {"tldr": "Study on AI explanations for improving misinformation detection, highlighting the effectiveness of social and combined explanations.", "motivation": "Current XAI approaches focus only on content explanations, which limit users' ability to recognize misinformation.", "method": "Two online crowdsourcing experiments conducted in COVID-19 and Politics domains to evaluate explanation designs.", "result": "AI explanations improved misinformation detection, with effectiveness influenced by the alignment of explanation types and their order of presentation.", "conclusion": "Different types of AI explanations can enhance understanding and detection of misinformation; alignment matters.", "key_contributions": ["Proposed social explanations alongside content explanations.", "Identified the influence of explanation order on detection accuracy.", "Demonstrated the domain-specific effectiveness of explanations."], "limitations": "Limited to two domains (COVID-19 and Politics) and online crowdsourcing sample.", "keywords": ["AI explanations", "misinformation detection", "Explainable AI", "social context", "crowdsourcing"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.03530", "pdf": "https://arxiv.org/pdf/2509.03530.pdf", "abs": "https://arxiv.org/abs/2509.03530", "title": "Reading Between the Signs: Predicting Future Suicidal Ideation from Adolescent Social Media Texts", "authors": ["Paul Blum", "Enrico Liscio", "Ruixuan Zhang", "Caroline Figueroa", "Pradeep K. Murukannaiah"], "categories": ["cs.CL"], "comment": null, "summary": "Suicide is a leading cause of death among adolescents (12-18), yet predicting\nit remains a significant challenge. Many cases go undetected due to a lack of\ncontact with mental health services. Social media, however, offers a unique\nopportunity, as young people often share their thoughts and struggles online in\nreal time. In this work, we propose a novel task and method to approach it:\npredicting suicidal ideation and behavior (SIB) from forum posts before an\nadolescent explicitly expresses suicidal ideation on an online forum. This\npredictive framing, where no self-disclosure is used as input at any stage,\nremains largely unexplored in the suicide prediction literature. To this end,\nwe introduce Early-SIB, a transformer-based model that sequentially processes\nthe posts a user writes and engages with to predict whether they will write a\nSIB post. Our model achieves a balanced accuracy of 0.73 for predicting future\nSIB on a Dutch youth forum, demonstrating that such tools can offer a\nmeaningful addition to traditional methods.", "AI": {"tldr": "The paper proposes Early-SIB, a transformer-based model to predict suicidal ideation and behavior in adolescents from online forum posts, achieving a balanced accuracy of 0.73.", "motivation": "To address the challenge of predicting suicide among adolescents by leveraging social media posts, given that many who are at risk do not seek help from mental health services.", "method": "The authors introduce Early-SIB, a transformer-based model that analyzes sequential forum posts made by users to predict the likelihood of them expressing suicidal ideation in future posts.", "result": "The model achieved a balanced accuracy of 0.73 in predicting suicidal ideation on a Dutch youth forum, indicating potential as a supportive tool for traditional methods.", "conclusion": "Early-SIB shows promise in enhancing suicide prevention efforts by providing timely predictions based on users' online behaviors without requiring explicit self-disclosure.", "key_contributions": ["Introduction of a novel task for predicting suicidal ideation from forum posts without explicit signals.", "Development of Early-SIB, a transformer-based predictive model for adolescent suicidal behavior.", "Demonstration of model efficacy with a balanced accuracy of 0.73 on real forum data."], "limitations": "", "keywords": ["suicidal ideation", "predictive modeling", "adolescent behavior", "social media", "transformer model"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.03741", "pdf": "https://arxiv.org/pdf/2509.03741.pdf", "abs": "https://arxiv.org/abs/2509.03741", "title": "Designing Gaze Analytics for ELA Instruction: A User-Centered Dashboard with Conversational AI Support", "authors": ["Eduardo Davalos", "Yike Zhang", "Shruti Jain", "Namrata Srivastava", "Trieu Truong", "Nafees-ul Haque", "Tristan Van", "Jorge Salas", "Sara McFadden", "Sun-Joo Cho", "Gautam Biswas", "Amanda Goodwin"], "categories": ["cs.HC", "cs.AI"], "comment": "22 pages, 9 figures, 3 tables, submitted to IUI2026", "summary": "Eye-tracking offers rich insights into student cognition and engagement, but\nremains underutilized in classroom-facing educational technology due to\nchallenges in data interpretation and accessibility. In this paper, we present\nthe iterative design and evaluation of a gaze-based learning analytics\ndashboard for English Language Arts (ELA), developed through five studies\ninvolving teachers and students. Guided by user-centered design and data\nstorytelling principles, we explored how gaze data can support reflection,\nformative assessment, and instructional decision-making. Our findings\ndemonstrate that gaze analytics can be approachable and pedagogically valuable\nwhen supported by familiar visualizations, layered explanations, and narrative\nscaffolds. We further show how a conversational agent, powered by a large\nlanguage model (LLM), can lower cognitive barriers to interpreting gaze data by\nenabling natural language interactions with multimodal learning analytics. We\nconclude with design implications for future EdTech systems that aim to\nintegrate novel data modalities in classroom contexts.", "AI": {"tldr": "This paper presents a gaze-based learning analytics dashboard for English Language Arts that utilizes eye-tracking data to enhance student assessment and engagement in educational settings.", "motivation": "Eye-tracking data can reveal valuable insights into student cognition and engagement, but its application in educational technology is limited due to challenges in interpretation and accessibility.", "method": "The research involved an iterative design and evaluation process through five studies with teachers and students, applying user-centered design and data storytelling principles.", "result": "Findings indicated that gaze analytics can be effective and useful when presented through familiar visualizations and narrative elements, and that a conversational agent based on a large language model can facilitate easier comprehension of gaze data.", "conclusion": "The study underscores the potential for integrating various data modalities in educational technology, providing a framework for future systems to enhance learning experiences in classrooms.", "key_contributions": ["Development of a gaze-based learning analytics dashboard for ELA", "Use of a conversational agent powered by a LLM to improve data interpretation", "Identification of design implications for integrating gaze analytics into EdTech"], "limitations": "Limited to the context of English Language Arts and may not generalize to other subjects or educational frameworks.", "keywords": ["eye-tracking", "learning analytics", "English Language Arts", "conversational agent", "educational technology"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.03531", "pdf": "https://arxiv.org/pdf/2509.03531.pdf", "abs": "https://arxiv.org/abs/2509.03531", "title": "Real-Time Detection of Hallucinated Entities in Long-Form Generation", "authors": ["Oscar Obeso", "Andy Arditi", "Javier Ferrando", "Joshua Freeman", "Cameron Holmes", "Neel Nanda"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models are now routinely used in high-stakes applications\nwhere hallucinations can cause serious harm, such as medical consultations or\nlegal advice. Existing hallucination detection methods, however, are\nimpractical for real-world use, as they are either limited to short factual\nqueries or require costly external verification. We present a cheap, scalable\nmethod for real-time identification of hallucinated tokens in long-form\ngenerations, and scale it effectively to 70B parameter models. Our approach\ntargets \\emph{entity-level hallucinations} -- e.g., fabricated names, dates,\ncitations -- rather than claim-level, thereby naturally mapping to token-level\nlabels and enabling streaming detection. We develop an annotation methodology\nthat leverages web search to annotate model responses with grounded labels\nindicating which tokens correspond to fabricated entities. This dataset enables\nus to train effective hallucination classifiers with simple and efficient\nmethods such as linear probes. Evaluating across four model families, our\nclassifiers consistently outperform baselines on long-form responses, including\nmore expensive methods such as semantic entropy (e.g., AUC 0.90 vs 0.71 for\nLlama-3.3-70B), and are also an improvement in short-form question-answering\nsettings. Moreover, despite being trained only with entity-level labels, our\nprobes effectively detect incorrect answers in mathematical reasoning tasks,\nindicating generalization beyond entities. While our annotation methodology is\nexpensive, we find that annotated responses from one model can be used to train\neffective classifiers on other models; accordingly, we publicly release our\ndatasets to facilitate reuse. Overall, our work suggests a promising new\napproach for scalable, real-world hallucination detection.", "AI": {"tldr": "A novel scalable method for detecting hallucinations in large language models' outputs, specifically targeting entity-level hallucinations, demonstrates significant improvements in real-time identification.", "motivation": "To address the impracticality of existing hallucination detection methods in high-stakes applications where hallucinations can cause serious harm, such as medical and legal domains.", "method": "The approach focuses on entity-level hallucinations by utilizing a web search-based annotation methodology to obtain grounded labels, allowing for efficient training of hallucination classifiers with linear probes.", "result": "The classifiers consistently outperform baselines, achieving an AUC of 0.90 compared to 0.71 for existing methods like semantic entropy, and they also generalize to tasks involving incorrect answers in mathematical reasoning.", "conclusion": "The work presents a scalable strategy for real-world hallucination detection in long-form outputs of large language models, with public datasets for facilitating further research.", "key_contributions": ["Development of a real-time hallucination detection method for long-form output.", "Introduction of an annotation methodology utilizing web search for entity-level hallucinations.", "Public release of datasets to support research and classifier training across different models."], "limitations": "Annotation methodology is costly and expensive, though it enables transfer learning for model classifiers.", "keywords": ["hallucination detection", "large language models", "entity-level hallucinations"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.03792", "pdf": "https://arxiv.org/pdf/2509.03792.pdf", "abs": "https://arxiv.org/abs/2509.03792", "title": "Map as a By-product: Collective Landmark Mapping from IMU Data and User-provided Texts in Situated Tasks", "authors": ["Ryo Yonetani", "Kotaro Hara"], "categories": ["cs.HC"], "comment": "(c) 2025 Copyright held by the owner/author(s). Publication rights\n  licensed to ACM. This is the author's version of the work. It is posted here\n  for your personal use. Not for redistribution. The definitive Version of\n  Record was published in Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.\n  9, 3, Article 146 (September 2025), https://doi.org/10.1145/3749455", "summary": "This paper presents Collective Landmark Mapper, a novel map-as-a-by-product\nsystem for generating semantic landmark maps of indoor environments. Consider\nusers engaged in situated tasks that require them to navigate these\nenvironments and regularly take notes on their smartphones. Collective Landmark\nMapper exploits the smartphone's IMU data and the user's free text input during\nthese tasks to identify a set of landmarks encountered by the user. The\nidentified landmarks are then aggregated across multiple users to generate a\nunified map representing the positions and semantic information of all\nlandmarks. In developing the proposed system, we focused specifically on retail\napplications and conducted a formative interview with stakeholders to confirm\ntheir practical needs that motivate the map-as-a-byproduct approach. Our user\nstudy demonstrates the feasibility of the proposed system and its superior\nmapping performance in two different setups: creating a product availability\nmap from restocking checklist tasks at a retail store and constructing a room\nusage map from office inspection tasks, further demonstrating the potential\napplicability to non-retail applications.", "AI": {"tldr": "This paper presents Collective Landmark Mapper, a system for generating semantic landmark maps using smartphone data and user input.", "motivation": "To enhance user navigation and note-taking in indoor environments through a collaborative mapping approach.", "method": "The system leverages smartphone IMU data and user-generated free text to identify and aggregate landmarks from multiple users.", "result": "The user study shows improved mapping performance in creating product availability and room usage maps compared to traditional methods.", "conclusion": "Collective Landmark Mapper demonstrates effective landmark mapping in retail and potentially other environments, confirming its practicality and usability.", "key_contributions": ["Development of a novel map-as-a-byproduct system for landmark mapping", "Aggregation of multiple users’ data for enhanced map creation", "Demonstration of superior mapping performance in retail applications"], "limitations": "", "keywords": ["collective mapping", "landmarks", "indoor navigation", "user input", "smartphone IMU"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.03533", "pdf": "https://arxiv.org/pdf/2509.03533.pdf", "abs": "https://arxiv.org/abs/2509.03533", "title": "Topic Identification in LLM Input-Output Pairs through the Lens of Information Bottleneck", "authors": ["Igor Halperin"], "categories": ["cs.CL", "cs.LG", "q-fin.GN"], "comment": "26 pages, 4 figures", "summary": "Large Language Models (LLMs) are prone to critical failure modes, including\n\\textit{intrinsic faithfulness hallucinations} (also known as confabulations),\nwhere a response deviates semantically from the provided context. Frameworks\ndesigned to detect this, such as Semantic Divergence Metrics (SDM), rely on\nidentifying latent topics shared between prompts and responses, typically by\napplying geometric clustering to their sentence embeddings. This creates a\ndisconnect, as the topics are optimized for spatial proximity, not for the\ndownstream information-theoretic analysis. In this paper, we bridge this gap by\ndeveloping a principled topic identification method grounded in the\nDeterministic Information Bottleneck (DIB) for geometric clustering. Our key\ncontribution is to transform the DIB method into a practical algorithm for\nhigh-dimensional data by substituting its intractable KL divergence term with a\ncomputationally efficient upper bound. The resulting method, which we dub UDIB,\ncan be interpreted as an entropy-regularized and robustified version of K-means\nthat inherently favors a parsimonious number of informative clusters. By\napplying UDIB to the joint clustering of LLM prompt and response embeddings, we\ngenerate a shared topic representation that is not merely spatially coherent\nbut is fundamentally structured to be maximally informative about the\nprompt-response relationship. This provides a superior foundation for the SDM\nframework and offers a novel, more sensitive tool for detecting confabulations.", "AI": {"tldr": "This paper presents a novel method called UDIB for detecting intrinsic faithfulness hallucinations in Large Language Models by improving topic identification through geometric clustering.", "motivation": "The paper addresses the issue of critical failure modes in LLMs, specifically focusing on intrinsic faithfulness hallucinations, which deviate responses from the provided context.", "method": "The authors develop the UDIB algorithm, a modified version of the Deterministic Information Bottleneck method, which uses an efficient upper bound for KL divergence to improve clustering of high-dimensional data in LLMs.", "result": "UDIB generates a shared topic representation that is structured to maximize information regarding the prompt-response relationship, facilitating a more sensitive detection of confabulations.", "conclusion": "The UDIB method enhances the Semantic Divergence Metrics framework by providing a more informative basis for topic identification, leading to better outcomes in detecting LLM hallucinations.", "key_contributions": ["Development of the UDIB algorithm for topic clustering", "Improvement of the Semantic Divergence Metrics framework", "Provision of a principled method for high-dimensional data clustering"], "limitations": "", "keywords": ["Large Language Models", "confabulations", "semantic divergence", "geometric clustering", "information bottleneck"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2509.03812", "pdf": "https://arxiv.org/pdf/2509.03812.pdf", "abs": "https://arxiv.org/abs/2509.03812", "title": "Exploring the Integration of Extended Reality and Artificial Intelligence (AI) for Remote STEM Education and Assessment", "authors": ["Shadeeb Hossain", "Natalie Sommer", "Neda Adib"], "categories": ["cs.HC"], "comment": "9 pages, 5 figures, 1 table", "summary": "This paper presents a dynamic gamification architecture for an Extended\nReality Artificial Intelligence virtual training environment designed to\nenhance STEM education through immersive adaptive, and kinesthetic learning.\nThe proposed system can be introduced in four phases: Introduction Phase,\nComponent Development Phase, Fault Introduction and Correction Phase and\nGenerative AI XR scenarios Phase. Security and privacy are discussed via a\ndefense-in-depth approach spanning client, middleware, and backend layers,\nincorporating AES 256 encryption, multi-factor authentication, role-based\naccess control and GDPR or FERPA compliance. Risks such as sensor exploitation,\nperceptual manipulation, and virtual physical harm are identified, with\nmitigation strategies embedded at the design stage. Potential barriers to large\nscale adoption-including technical complexity, cost of deployment, and need for\ncybersecurity expertise are discussed.", "AI": {"tldr": "The paper presents a dynamic gamification architecture for an Extended Reality AI training environment aimed at enhancing STEM education through immersive learning.", "motivation": "To improve STEM education using Extended Reality (XR) and gamification strategies that enable adaptive and kinesthetic learning.", "method": "The architecture is introduced in four phases focusing on the development of components, fault introduction and correction, and implementing Generative AI XR scenarios.", "result": "The proposed system incorporates security measures such as AES 256 encryption and multi-factor authentication, while addressing risks like sensor exploitation and perceptual manipulation, providing mitigation strategies.", "conclusion": "The design promotes secure, immersive, and adaptable learning environments for STEM education, highlighting potential adoption barriers related to cost, complexity, and cybersecurity expertise.", "key_contributions": ["Dynamic gamification architecture for XR in STEM education", "Comprehensive security and privacy measures", "Identification of risks and mitigation strategies"], "limitations": "Technical complexity and cost of deployment may hinder large-scale adoption.", "keywords": ["gamification", "Extended Reality", "STEM education", "security", "Artificial Intelligence"], "importance_score": 6, "read_time_minutes": 9}}
{"id": "2509.03535", "pdf": "https://arxiv.org/pdf/2509.03535.pdf", "abs": "https://arxiv.org/abs/2509.03535", "title": "QuesGenie: Intelligent Multimodal Question Generation", "authors": ["Ahmed Mubarak", "Amna Ahmed", "Amira Nasser", "Aya Mohamed", "Fares El-Sadek", "Mohammed Ahmed", "Ahmed Salah", "Youssef Sobhy"], "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 8 figures, 12 tables. Supervised by Dr. Ahmed Salah and TA\n  Youssef Sobhy", "summary": "In today's information-rich era, learners have access to abundant educational\nresources, but the lack of practice materials tailored to these resources\npresents a significant challenge. This project addresses that gap by developing\na multi-modal question generation system that can automatically generate\ndiverse question types from various content formats. The system features four\nmajor components: multi-modal input handling, question generation,\nreinforcement learning from human feedback (RLHF), and an end-to-end\ninteractive interface. This project lays the foundation for automated,\nscalable, and intelligent question generation, carefully balancing resource\nefficiency, robust functionality and a smooth user experience.", "AI": {"tldr": "Development of a multi-modal question generation system addressing the lack of tailored practice materials for learners.", "motivation": "To bridge the gap in practice materials available to learners by creating an automated question generation system.", "method": "The system incorporates multi-modal input handling, question generation, reinforcement learning from human feedback, and an interactive interface.", "result": "The system effectively generates diverse question types from various content formats, enhancing learning engagement.", "conclusion": "This project establishes a foundational approach for intelligent question generation that is scalable and resource-efficient while providing a user-friendly experience.", "key_contributions": ["Development of multi-modal question generation system", "Incorporation of reinforcement learning from human feedback", "Creation of an interactive interface for end-users"], "limitations": "", "keywords": ["question generation", "multi-modal", "reinforcement learning", "education", "interactive interface"], "importance_score": 6, "read_time_minutes": 7}}
{"id": "2509.03931", "pdf": "https://arxiv.org/pdf/2509.03931.pdf", "abs": "https://arxiv.org/abs/2509.03931", "title": "\"Low Frequency Tweeters Have More to Say!\" A New Approach to Identify Importance of Tweets", "authors": ["Gautam Khannaa", "Yeliz Yesilada", "Sukru Eraslan", "Simon Harper"], "categories": ["cs.HC", "cs.SI"], "comment": "12 pages", "summary": "Twitter is one of the most popular social media platforms.With a large number\nof tweets, the activity feed of users becomes noisy, challenging to read, and\nmost importantly tweets often get lost. We present a new approach to\npersonalise the ranking of the tweets toward solving the problem of information\noverload which is achieved by analysing the relationship between the importance\nof tweets to the frequency at which the author tweets. The hypothesis tested is\nthat \"low-frequency tweeters have more to say\", i.e. if a user who tweets\ninfrequently actually goes to the effort of tweeting, then it is more likely to\nbe of more importance or contain more \"meaning\" than a tweet by a user who\ntweets continuously. We propose six new measures to evaluate the importance of\ntweets based on the ability of the tweet to drive interaction among its\nreaders, which is measured through metrics such as retweets, favourites, and\ncomments, and the extent of the author's network interacting with the tweet.\nOur study shows that users who tweeted less than ten tweets per week were more\nlikely to be perceived as important by their followers and have the most\nimportant messages. This identified tweet-frequency band could be used to\nreorder the activity feed of users and such reordering would ensure the\nmessages of low-frequency tweeters do not get lost in the stream of tweets.\nThis could also serve as a scoring index for Twitter users to identify users\nfrequently tweeting important messages.", "AI": {"tldr": "This paper presents a method to personalize tweet rankings on Twitter based on the frequency of user tweets and their perceived importance, addressing information overload on the platform.", "motivation": "The motivation behind this research is to tackle the challenge of information overload on Twitter, where important tweets can get lost among a high volume of posts.", "method": "The authors analyze the relationship between tweet importance and author tweet frequency, proposing six new measures to assess tweet importance based on interaction metrics like retweets, favorites, and comments.", "result": "The study reveals that users who tweet infrequently are often perceived as having more important messages, leading to a proposed reordering of activity feeds to highlight these tweets.", "conclusion": "The findings could help ensure that significant tweets from low-frequency users are not overlooked, which might also serve as a scoring index for identifying valuable Twitter users.", "key_contributions": ["Proposes a novel method for ranking tweets based on author tweet frequency and tweet importance.", "Introduces six measures for evaluating tweet importance through user interaction metrics.", "Demonstrates that low-frequency tweeters convey more meaningful content."], "limitations": "", "keywords": ["Twitter", "information overload", "tweet ranking", "user interaction", "social media"], "importance_score": 4, "read_time_minutes": 12}}
{"id": "2509.03537", "pdf": "https://arxiv.org/pdf/2509.03537.pdf", "abs": "https://arxiv.org/abs/2509.03537", "title": "AR$^2$: Adversarial Reinforcement Learning for Abstract Reasoning in Large Language Models", "authors": ["Cheng-Kai Yeh", "Hsing-Wang Lee", "Chung-Hung Kuo", "Hen-Hsen Huang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "7 pages, accepted by CIKM 2025 as a short paper", "summary": "Abstraction--the ability to recognize and distill essential computational\npatterns from complex problem statements--is a foundational skill in computer\nscience, critical both for human problem-solvers and coding-oriented large\nlanguage models (LLMs). Despite recent advances in training LLMs for code\ngeneration using reinforcement learning (RL), most existing approaches focus\nprimarily on superficial pattern recognition, overlooking explicit training for\nabstraction. In this study, we propose AR$^2$ (Adversarial Reinforcement\nLearning for Abstract Reasoning), a novel framework explicitly designed to\nenhance the abstraction abilities of LLMs. AR$^2$ employs a teacher model to\ntransform kernel problems into narrative-rich, challenging descriptions without\nchanging their fundamental logic. Simultaneously, a student coding model is\ntrained to solve these complex narrative problems by extracting their\nunderlying computational kernels. Experimental results demonstrate that AR$^2$\nsubstantially improves the student model's accuracy on previously unseen,\nchallenging programming tasks, underscoring abstraction as a key skill for\nenhancing LLM generalization.", "AI": {"tldr": "This paper introduces AR$^2$, a framework aimed at enhancing abstraction abilities in large language models (LLMs) using adversarial reinforcement learning.", "motivation": "The paper addresses the need for improved abstraction skills in LLMs, which are crucial for effective problem-solving in computer science and coding tasks.", "method": "AR$^2$ consists of a teacher model that generates complex narrative descriptions of programming tasks while maintaining their core logic, to train a student model to solve these tasks.", "result": "Experimental results indicate that AR$^2$ significantly boosts the accuracy of the student model on challenging programming tasks that it has not encountered before.", "conclusion": "The findings highlight the importance of abstraction as a fundamental skill necessary for improving the generalization capabilities of LLMs.", "key_contributions": ["Introduction of the AR$^2$ framework for enhancing LLM abstraction", "Demonstration of improved accuracy in programming tasks via adversarial reinforcement learning", "Creation of challenging narrative-rich problem descriptions for teaching LLMs."], "limitations": "", "keywords": ["large language models", "abstraction", "reinforcement learning", "human-computer interaction"], "importance_score": 8, "read_time_minutes": 7}}
{"id": "2509.04088", "pdf": "https://arxiv.org/pdf/2509.04088.pdf", "abs": "https://arxiv.org/abs/2509.04088", "title": "Spiking Neural Network Decoders of Finger Forces from High-Density Intramuscular Microelectrode Arrays", "authors": ["Farah Baracat", "Agnese Grison", "Dario Farina", "Giacomo Indiveri", "Elisa Donati"], "categories": ["cs.HC", "eess.SP"], "comment": null, "summary": "Restoring naturalistic finger control in assistive technologies requires the\ncontinuous decoding of motor intent with high accuracy, efficiency, and\nrobustness. Here, we present a spike-based decoding framework that integrates\nspiking neural networks (SNNs) with motor unit activity extracted from\nhigh-density intramuscular microelectrode arrays. We demonstrate simultaneous\nand proportional decoding of individual finger forces from motor unit spike\ntrains during isometric contractions at 15% of maximum voluntary contraction\nusing SNNs. We systematically evaluated alternative SNN decoder configurations\nand compared two possible input modalities: physiologically grounded motor unit\nspike trains and spike-encoded intramuscular EMG signals. Through this\ncomparison, we quantified trade-offs between decoding accuracy, memory\nfootprint, and robustness to input errors. The results showed that shallow SNNs\ncan reliably decode finger-level motor intent with competitive accuracy and\nminimal latency, while operating with reduced memory requirements and without\nthe need for external preprocessing buffers. This work provides a practical\nblueprint for integrating SNNs into finger-level force decoding systems,\ndemonstrating how the choice of input representation can be strategically\ntailored to meet application-specific requirements for accuracy, robustness,\nand memory efficiency.", "AI": {"tldr": "A spike-based decoding framework using spiking neural networks (SNNs) accurately decodes finger motor intent from intramuscular activity.", "motivation": "To enhance assistive technologies by restoring naturalistic finger control through accurate and efficient decoding of motor intent.", "method": "The study employs a spike-based decoding framework that integrates SNNs with motor unit activity from high-density intramuscular microelectrode arrays, evaluating different decoder configurations and input modalities.", "result": "The results demonstrate that shallow SNNs can decode individual finger forces from motor unit spike trains with competitive accuracy and low latency, while requiring minimal memory.", "conclusion": "This work outlines a framework for integrating SNNs into force decoding systems, allowing for tailored input representations to achieve specific application needs.", "key_contributions": ["Introduction of a spike-based decoding framework for finger force decoding", "Evaluation of motor unit spike trains vs. EMG signals", "Demonstration of reduced memory requirements and latency in SNN configurations"], "limitations": "", "keywords": ["spiking neural networks", "motor intent decoding", "assistive technology", "intramuscular microelectrode arrays", "finger force"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2509.03540", "pdf": "https://arxiv.org/pdf/2509.03540.pdf", "abs": "https://arxiv.org/abs/2509.03540", "title": "Improving Factuality in LLMs via Inference-Time Knowledge Graph Construction", "authors": ["Shanglin Wu", "Lihui Liu", "Jinho D. Choi", "Kai Shu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often struggle with producing factually\nconsistent answers due to limitations in their parametric memory.\nRetrieval-Augmented Generation (RAG) methods address this issue by\nincorporating external knowledge from trusted sources at inference time.\nHowever, such methods typically treat knowledge as unstructured text, which\nlimits their ability to support compositional reasoning and identify factual\ninconsistencies. To overcome these limitations, we propose a novel framework\nthat dynamically constructs and expands knowledge graphs (KGs) during\ninference, integrating both internal knowledge extracted from LLMs and external\ninformation retrieved from external sources. Our method begins by extracting a\nseed KG from the question via prompting, followed by iterative expansion using\nthe LLM's latent knowledge. The graph is then selectively refined through\nexternal retrieval, enhancing factual coverage and correcting inaccuracies. We\nevaluate our approach on three diverse factual QA benchmarks, demonstrating\nconsistent improvements in factual accuracy, answer precision, and\ninterpretability over baseline prompting and static KG-augmented methods. Our\nfindings suggest that inference-time KG construction is a promising direction\nfor enhancing LLM factuality in a structured, interpretable, and scalable\nmanner.", "AI": {"tldr": "The paper introduces a novel framework for improving factual consistency in Large Language Models (LLMs) by dynamically constructing and expanding knowledge graphs during inference.", "motivation": "LLMs often produce factually inconsistent answers due to limitations in their parametric memory, particularly when relying solely on unstructured text for integrated knowledge.", "method": "The proposed framework constructs a seed knowledge graph from the input question and expands it iteratively using the LLM's latent knowledge, followed by refinement with externally retrieved information.", "result": "Evaluation on three factual QA benchmarks shows consistent improvements in factual accuracy, answer precision, and interpretability over existing methods.", "conclusion": "Inference-time construction of knowledge graphs is a promising technique for enhancing the factuality of LLMs in a structured and interpretable way.", "key_contributions": ["Dynamic knowledge graph construction during inference", "Integration of internal and external knowledge", "Improved factual accuracy and interpretability in LLM responses"], "limitations": "", "keywords": ["Large Language Models", "knowledge graphs", "Retrieval-Augmented Generation"], "importance_score": 10, "read_time_minutes": 10}}
{"id": "2509.04174", "pdf": "https://arxiv.org/pdf/2509.04174.pdf", "abs": "https://arxiv.org/abs/2509.04174", "title": "Unobtrusive In-Situ Measurement of Behavior Change by Deep Metric Similarity Learning of Motion Patterns", "authors": ["Christian Merz", "Lukas Schach", "Marie Luisa Fiedler", "Jean-Luc Lugrin", "Carolin Wienrich", "Marc Erich Latoschik"], "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "This paper introduces an unobtrusive in-situ measurement method to detect\nuser behavior changes during arbitrary exposures in XR systems. Here, such\nbehavior changes are typically associated with the Proteus effect or bodily\naffordances elicited by different avatars that the users embody in XR. We\npresent a biometric user model based on deep metric similarity learning, which\nuses high-dimensional embeddings as reference vectors to identify behavior\nchanges of individual users. We evaluate our model against two alternative\napproaches: a (non-learned) motion analysis based on central tendencies of\nmovement patterns and subjective post-exposure embodiment questionnaires\nfrequently used in various XR exposures. In a within-subject study,\nparticipants performed a fruit collection task while embodying avatars of\ndifferent body heights (short, actual-height, and tall). Subjective assessments\nconfirmed the effective manipulation of perceived body schema, while the\n(non-learned) objective analyses of head and hand movements revealed\nsignificant differences across conditions. Our similarity learning model\ntrained on the motion data successfully identified the elicited behavior change\nfor various query and reference data pairings of the avatar conditions. The\napproach has several advantages in comparison to existing methods: 1) In-situ\nmeasurement without additional user input, 2) generalizable and scalable motion\nanalysis for various use cases, 3) user-specific analysis on the individual\nlevel, and 4) with a trained model, users can be added and evaluated in real\ntime to study how avatar changes affect behavior.", "AI": {"tldr": "The paper presents a method for detecting user behavior changes in XR systems using deep metric similarity learning, which compares motion patterns based on biometric data.", "motivation": "To measure user behavior changes during XR experiences without intrusive input, focusing on the Proteus effect linked to avatar embodiment.", "method": "A biometric user model based on deep metric similarity learning is employed to analyze motion data collected during a fruit collection task with different avatar body heights.", "result": "The model successfully identified behavior changes based on the various avatar conditions, showing advantages over traditional methods by allowing in-situ measurement and individual analysis.", "conclusion": "The developed method provides a scalable and generalizable approach to study the impact of avatar changes on user behavior in real time.", "key_contributions": ["Introduction of a biometric user model using deep metric similarity learning for motion analysis in XR.", "Demonstration of effective in-situ measurement of behavior changes without user input.", "Generalizable motion analysis applicable to various avatar-related studies."], "limitations": "", "keywords": ["XR systems", "Proteus effect", "deep metric learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.03565", "pdf": "https://arxiv.org/pdf/2509.03565.pdf", "abs": "https://arxiv.org/abs/2509.03565", "title": "ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference", "authors": ["Qi Chen", "Jingxuan Wei", "Zhuoya Yao", "Haiguang Wang", "Gaowei Wu", "Bihui Yu", "Siyuan Li", "Cheng Tan"], "categories": ["cs.CL", "cs.MM"], "comment": "Accepted to ACM MM 2025", "summary": "Understanding how scientific ideas evolve requires more than summarizing\nindividual papers-it demands structured, cross-document reasoning over\nthematically related research. In this work, we formalize multi-document\nscientific inference, a new task that extracts and aligns motivation,\nmethodology, and experimental results across related papers to reconstruct\nresearch development chains. This task introduces key challenges, including\ntemporally aligning loosely structured methods and standardizing heterogeneous\nexperimental tables. We present ResearchPulse, an agent-based framework that\nintegrates instruction planning, scientific content extraction, and structured\nvisualization. It consists of three coordinated agents: a Plan Agent for task\ndecomposition, a Mmap-Agent that constructs motivation-method mind maps, and a\nLchart-Agent that synthesizes experimental line charts. To support this task,\nwe introduce ResearchPulse-Bench, a citation-aware benchmark of annotated paper\nclusters. Experiments show that our system, despite using 7B-scale agents,\nconsistently outperforms strong baselines like GPT-4o in semantic alignment,\nstructural consistency, and visual fidelity. The dataset are available in\nhttps://huggingface.co/datasets/ResearchPulse/ResearchPulse-Bench.", "AI": {"tldr": "This paper introduces ResearchPulse, an agent-based framework for multi-document scientific inference, which extracts and aligns research motivation, methods, and results across related papers to reconstruct research development chains.", "motivation": "There is a need for structured cross-document reasoning in science to understand the evolution of scientific ideas, beyond just summarizing individual papers.", "method": "The approach formalizes the task of multi-document scientific inference, utilizing three coordinated agents: a Plan Agent for task decomposition, a Mmap-Agent for creating motivation-method mind maps, and a Lchart-Agent for synthesizing experimental charts. It also includes ResearchPulse-Bench, a benchmark for evaluating the system.", "result": "ResearchPulse consistently outperforms strong baselines, including GPT-4o, in terms of semantic alignment, structural consistency, and visual fidelity.", "conclusion": "The proposed framework shows promise in enhancing the understanding of scientific research development and offers a novel benchmark for related tasks.", "key_contributions": ["Formalization of multi-document scientific inference", "Proposal of an agent-based framework for research synthesis", "Development of ResearchPulse-Bench benchmark for evaluation"], "limitations": "", "keywords": ["multi-document inference", "scientific reasoning", "agent-based framework"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.04241", "pdf": "https://arxiv.org/pdf/2509.04241.pdf", "abs": "https://arxiv.org/abs/2509.04241", "title": "Would I regret being different? The influence of social norms on attitudes toward AI usage", "authors": ["Jaroslaw Kornowicz", "Maurice Pape", "Kirsten Thommes"], "categories": ["cs.HC"], "comment": "30 pages, 5 figures", "summary": "Prior research shows that social norms can reduce algorithm aversion, but\nlittle is known about how such norms become established. Most accounts\nemphasize technological and individual determinants, yet AI adoption unfolds\nwithin organizational social contexts shaped by peers and supervisors. We ask\nwhether the source of the norm-peers or supervisors-shapes AI usage behavior.\nThis question is practically relevant for organizations seeking to promote\neffective AI adoption. We conducted an online vignette experiment, complemented\nby qualitative data on participants' feelings and justifications after\n(counter-)normative behavior. In line with the theory, counter-normative\nchoices elicited higher regret than norm-adherent choices. On average, choosing\nAI increased regret compared to choosing an human. This aversion was weaker\nwhen AI use was presented as the prevailing norm, indicating a statistically\nsignificant interaction between AI use and an AI-favoring norm. Participants\nalso attributed less blame to technology than to humans, which increased regret\nwhen AI was chosen over human expertise. Both peer and supervisor influence\nemerged as relevant factors, though contrary to expectations they did not\nsignificantly affect regret. Our findings suggest that regret aversion,\nembedded in social norms, is a central mechanism driving imitation in\nAI-related decision-making.", "AI": {"tldr": "This paper investigates how social norms from peers and supervisors influence AI usage behavior, particularly in the context of reducing algorithm aversion.", "motivation": "To understand the establishment of social norms in AI adoption and their impact on AI usage decisions within organizations.", "method": "An online vignette experiment was conducted, supplemented by qualitative data from participants reflecting on their normative and counter-normative decisions.", "result": "Participants reported higher regret when deviating from AI-preferred choices and attributed more blame to human decisions over AI, with AI usage normalized reducing regret.", "conclusion": "Regret aversion influenced by social norms significantly drives imitation in AI decision-making, providing insights for organizations on effective AI adoption strategies.", "key_contributions": ["Establishes the role of regret aversion in AI-related decision-making", "Highlights the limited influence of peers and supervisors on regret levels regarding AI use", "Provides evidence that social norms can mitigate algorithm aversion in organizational settings."], "limitations": "Further research is needed to explore the long-term effects of these norms and their variability across different organizational contexts.", "keywords": ["AI adoption", "social norms", "algorithm aversion", "decision-making", "organizational behavior"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2509.03610", "pdf": "https://arxiv.org/pdf/2509.03610.pdf", "abs": "https://arxiv.org/abs/2509.03610", "title": "NoteBar: An AI-Assisted Note-Taking System for Personal Knowledge Management", "authors": ["Josh Wisoff", "Yao Tang", "Zhengyu Fang", "Jordan Guzman", "YuTang Wang", "Alex Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Note-taking is a critical practice for capturing, organizing, and reflecting\non information in both academic and professional settings. The recent success\nof large language models has accelerated the development of AI-assisted tools,\nyet existing solutions often struggle with efficiency. We present NoteBar, an\nAI-assisted note-taking tool that leverages persona information and efficient\nlanguage models to automatically organize notes into multiple categories and\nbetter support user workflows. To support research and evaluation in this\nspace, we further introduce a novel persona-conditioned dataset of 3,173 notes\nand 8,494 annotated concepts across 16 MBTI personas, offering both diversity\nand semantic richness for downstream tasks. Finally, we demonstrate that\nNoteBar can be deployed in a practical and cost-effective manner, enabling\ninteractive use without reliance on heavy infrastructure. Together, NoteBar and\nits accompanying dataset provide a scalable and extensible foundation for\nadvancing AI-assisted personal knowledge management.", "AI": {"tldr": "NoteBar is an AI-assisted note-taking tool designed to enhance organization and workflow by utilizing persona information and efficient language models, along with a novel dataset for evaluation.", "motivation": "To improve note-taking efficiency in academic and professional settings using AI-assisted tools, addressing the limitations of existing solutions.", "method": "The paper introduces NoteBar, which automatically categorizes notes based on user personas and presents a dataset of 3,173 notes annotated across 8,494 concepts linked to 16 MBTI personas.", "result": "NoteBar enables effective organization of notes and supports user workflows interactively and cost-effectively, showcasing practical deployment without heavy infrastructure.", "conclusion": "NoteBar and its dataset offer a scalable foundation for enhancing AI-assisted personal knowledge management.", "key_contributions": ["Introduction of NoteBar, an innovative AI-assisted note-taking tool", "Creation of a novel persona-conditioned dataset for research and evaluation", "Demonstration of practical, cost-effective deployment of AI in note-taking"], "limitations": "", "keywords": ["AI-assisted note-taking", "personal knowledge management", "language models", "MBTI personas", "workflow organization"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2509.04254", "pdf": "https://arxiv.org/pdf/2509.04254.pdf", "abs": "https://arxiv.org/abs/2509.04254", "title": "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition from Physiological Signals", "authors": ["Meisam Jamshidi Seikavandi", "Fabricio Batista Narcizo", "Ted Vucurevich", "Andrew Burke Dittberner", "Paolo Burelli"], "categories": ["cs.HC"], "comment": null, "summary": "We present MuMTAffect, a novel Multimodal Multitask Affective Embedding\nNetwork designed for joint emotion classification and personality prediction\n(re-identification) from short physiological signal segments. MuMTAffect\nintegrates multiple physiological modalities pupil dilation, eye gaze, facial\naction units, and galvanic skin response using dedicated, transformer-based\nencoders for each modality and a fusion transformer to model cross-modal\ninteractions. Inspired by the Theory of Constructed Emotion, the architecture\nexplicitly separates core affect encoding (valence/arousal) from higher-level\nconceptualization, thereby grounding predictions in contemporary affective\nneuroscience. Personality trait prediction is leveraged as an auxiliary task to\ngenerate robust, user-specific affective embeddings, significantly enhancing\nemotion recognition performance. We evaluate MuMTAffect on the AFFEC dataset,\ndemonstrating that stimulus-level emotional cues (Stim Emo) and galvanic skin\nresponse substantially improve arousal classification, while pupil and gaze\ndata enhance valence discrimination. The inherent modularity of MuMTAffect\nallows effortless integration of additional modalities, ensuring scalability\nand adaptability. Extensive experiments and ablation studies underscore the\nefficacy of our multimodal multitask approach in creating personalized,\ncontext-aware affective computing systems, highlighting pathways for further\nadvancements in cross-subject generalisation.", "AI": {"tldr": "MuMTAffect is a multimodal network for emotion classification and personality prediction using various physiological signals, improving emotion recognition performance through cross-modal interactions.", "motivation": "The integration of emotion classification and personality prediction is essential for developing effective affective computing systems, grounded in contemporary affective neuroscience.", "method": "MuMTAffect utilizes transformer-based encoders for multiple physiological modalities (pupil dilation, eye gaze, facial action units, galvanic skin response) and a fusion transformer to model interactions between these modalities.", "result": "Evaluation on the AFFEC dataset shows that the model improves arousal classification with galvanic skin response and enhances valence discrimination with pupil and gaze data.", "conclusion": "The modularity of MuMTAffect supports the integration of additional modalities, making it scalable and adaptable for future developments in affective computing.", "key_contributions": ["Integration of emotion classification and personality prediction", "Use of dedicated transformer-based encoders for each physiological modality", "Demonstration of significant performance improvements in emotion recognition"], "limitations": "", "keywords": ["affective computing", "multimodal", "emotion classification", "personality prediction", "physiological signals"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.03615", "pdf": "https://arxiv.org/pdf/2509.03615.pdf", "abs": "https://arxiv.org/abs/2509.03615", "title": "E-ARMOR: Edge case Assessment and Review of Multilingual Optical Character Recognition", "authors": ["Aryan Gupta", "Anupam Purwar"], "categories": ["cs.CL", "cs.AI"], "comment": "Sprinklr OCR provides a fast and compute light way of performing OCR", "summary": "Optical Character Recognition (OCR) in multilingual, noisy, and diverse\nreal-world images remains a significant challenge for optical character\nrecognition systems. With the rise of Large Vision-Language Models (LVLMs),\nthere is growing interest in their ability to generalize and reason beyond\nfixed OCR pipelines. In this work, we introduce Sprinklr-Edge-OCR, a novel OCR\nsystem built specifically optimized for edge deployment in resource-constrained\nenvironments. We present a large-scale comparative evaluation of five\nstate-of-the-art LVLMs (InternVL, Qwen, GOT OCR, LLaMA, MiniCPM) and two\ntraditional OCR systems (Sprinklr-Edge-OCR, SuryaOCR) on a proprietary, doubly\nhand annotated dataset of multilingual (54 languages) images. Our benchmark\ncovers a broad range of metrics including accuracy, semantic consistency,\nlanguage coverage, computational efficiency (latency, memory, GPU usage), and\ndeployment cost. To better reflect real-world applicability, we also conducted\nedge case deployment analysis, evaluating model performance on CPU only\nenvironments. Among the results, Qwen achieved the highest precision (0.54),\nwhile Sprinklr-Edge-OCR delivered the best overall F1 score (0.46) and\noutperformed others in efficiency, processing images 35 faster (0.17 seconds\nper image on average) and at less than 0.01 of the cost (0.006 USD per 1,000\nimages) compared to LVLM. Our findings demonstrate that the most optimal OCR\nsystems for edge deployment are the traditional ones even in the era of LLMs\ndue to their low compute requirements, low latency, and very high\naffordability.", "AI": {"tldr": "Sprinklr-Edge-OCR, a novel OCR system optimized for edge deployment, was compared with five state-of-the-art LVLMs and two traditional OCR systems, demonstrating superior efficiency and cost-effectiveness.", "motivation": "The study addresses the challenges in Optical Character Recognition (OCR) within multilingual and noisy images, exploring the potential of Large Vision-Language Models (LVLMs) versus traditional OCR systems.", "method": "A large-scale comparative evaluation was conducted on a proprietary, hand-annotated dataset of multilingual images, assessing accuracy, semantics, efficiency, and cost across multiple OCR models.", "result": "Qwen achieved the highest precision, but Sprinklr-Edge-OCR delivered the best F1 score and was significantly faster and cheaper than LVLMs.", "conclusion": "Traditional OCR systems remain optimal for edge deployment due to their low computing needs and high affordability, despite advancements in LLM technology.", "key_contributions": ["Introduction of Sprinklr-Edge-OCR, optimized for edge deployment.", "Comparative evaluation of OCR systems, revealing efficiency metrics.", "Insights into real-world applicability and edge case analysis."], "limitations": "", "keywords": ["Optical Character Recognition", "Multilingual", "Edge Deployment", "Vision-Language Models", "Efficiency"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.04303", "pdf": "https://arxiv.org/pdf/2509.04303.pdf", "abs": "https://arxiv.org/abs/2509.04303", "title": "HumAIne-Chatbot: Real-Time Personalized Conversational AI via Reinforcement Learning", "authors": ["Georgios Makridis", "Georgios Fragiadakis", "Jorge Oliveira", "Tomaz Saraiva", "Philip Mavrepis", "Georgios Fatouros", "Dimosthenis Kyriazis"], "categories": ["cs.HC", "cs.AI"], "comment": "11 pages, 4 figures, IEEE conference format", "summary": "Current conversational AI systems often provide generic, one-size-fits-all\ninteractions that overlook individual user characteristics and lack adaptive\ndialogue management. To address this gap, we introduce\n\\textbf{HumAIne-chatbot}, an AI-driven conversational agent that personalizes\nresponses through a novel user profiling framework. The system is pre-trained\non a diverse set of GPT-generated virtual personas to establish a broad prior\nover user types. During live interactions, an online reinforcement learning\nagent refines per-user models by combining implicit signals (e.g. typing speed,\nsentiment, engagement duration) with explicit feedback (e.g., likes and\ndislikes). This profile dynamically informs the chatbot dialogue policy,\nenabling real-time adaptation of both content and style. To evaluate the\nsystem, we performed controlled experiments with 50 synthetic personas in\nmultiple conversation domains. The results showed consistent improvements in\nuser satisfaction, personalization accuracy, and task achievement when\npersonalization features were enabled. Statistical analysis confirmed\nsignificant differences between personalized and nonpersonalized conditions,\nwith large effect sizes across key metrics. These findings highlight the\neffectiveness of AI-driven user profiling and provide a strong foundation for\nfuture real-world validation.", "AI": {"tldr": "HumAIne-chatbot personalizes conversational AI interactions using a novel user profiling framework, improving user satisfaction and task achievement.", "motivation": "Existing conversational AI systems often provide generic interactions which fail to adapt to individual user characteristics, leading to suboptimal user experiences.", "method": "The system utilizes pre-training on GPT-generated virtual personas and employs online reinforcement learning to refine user profiles based on implicit and explicit feedback during interactions.", "result": "Controlled experiments demonstrated significant improvements in user satisfaction, personalization accuracy, and task achievement, confirming the effectiveness of the proposed personalization features.", "conclusion": "The study provides strong evidence for the effectiveness of AI-driven user profiling in enhancing conversational AI interactions, suggesting a pathway for future real-world applications.", "key_contributions": ["Introduction of a user profiling framework for conversational agents", "Use of reinforcement learning for real-time adaptation", "Demonstration of significant improvements in user satisfaction and task achievement"], "limitations": "", "keywords": ["Conversational AI", "User profiling", "Reinforcement learning", "Personalization", "Human-computer interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.03647", "pdf": "https://arxiv.org/pdf/2509.03647.pdf", "abs": "https://arxiv.org/abs/2509.03647", "title": "Breaking the Mirror: Activation-Based Mitigation of Self-Preference in LLM Evaluators", "authors": ["Dani Roytburg", "Matthew Bozoukov", "Matthew Nguyen", "Jou Barzdukas", "Simon Fu", "Narmeen Oozeer"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) increasingly serve as automated evaluators, yet\nthey suffer from \"self-preference bias\": a tendency to favor their own outputs\nover those of other models. This bias undermines fairness and reliability in\nevaluation pipelines, particularly for tasks like preference tuning and model\nrouting. We investigate whether lightweight steering vectors can mitigate this\nproblem at inference time without retraining. We introduce a curated dataset\nthat distinguishes self-preference bias into justified examples of\nself-preference and unjustified examples of self-preference, and we construct\nsteering vectors using two methods: Contrastive Activation Addition (CAA) and\nan optimization-based approach. Our results show that steering vectors can\nreduce unjustified self-preference bias by up to 97\\%, substantially\noutperforming prompting and direct preference optimization baselines. Yet\nsteering vectors are unstable on legitimate self-preference and unbiased\nagreement, implying self-preference spans multiple or nonlinear directions.\nThis underscores both their promise and limits as safeguards for LLM-as-judges\nand motivates more robust interventions.", "AI": {"tldr": "This paper investigates the issue of self-preference bias in large language models (LLMs) and explores using lightweight steering vectors to mitigate this bias during inference without retraining.", "motivation": "The self-preference bias in LLMs affects the fairness and reliability of automated evaluations, particularly in preference tuning and model routing tasks.", "method": "The authors introduce a curated dataset to classify self-preference bias and propose steering vectors created using Contrastive Activation Addition (CAA) and an optimization-based approach.", "result": "The steering vectors can reduce unjustified self-preference bias by up to 97%, significantly outperforming existing baselines such as prompting and direct preference optimization.", "conclusion": "While steering vectors show promise in mitigating self-preference bias, they struggle with legitimate cases of self-preference and unbiased agreement, indicating the need for more robust solutions.", "key_contributions": ["Introduction of a curated dataset for analyzing self-preference bias in LLMs", "Development of lightweight steering vectors to reduce bias", "Performance benchmark demonstrating significant bias reduction compared to existing methods."], "limitations": "Steering vectors exhibit instability in addressing legitimate self-preference and unbiased agreement, suggesting their limitations as a singular solution.", "keywords": ["Large language models", "self-preference bias", "steering vectors", "Contrastive Activation Addition", "preference tuning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.04340", "pdf": "https://arxiv.org/pdf/2509.04340.pdf", "abs": "https://arxiv.org/abs/2509.04340", "title": "Write on Paper, Wrong in Practice: Why LLMs Still Struggle with Writing Clinical Notes", "authors": ["Kristina L. Kupferschmidt", "Kieran O'Doherty", "Joshua A. Skorburg"], "categories": ["cs.HC"], "comment": null, "summary": "Large Language Models (LLMs) are often proposed as tools to streamline\nclinical documentation, a task viewed as both high-volume and low-risk.\nHowever, even seemingly straightforward applications of LLMs raise complex\nsociotechnical considerations to translate into practice. This case study,\nconducted at KidsAbility, a pediatric rehabilitation facility in Ontario,\nCanada examined the use of LLMs to support occupational therapists in reducing\ndocumentation burden.We conducted a qualitative study involving 20 clinicians\nwho participated in pilot programs using two AI technologies: a general-purpose\nproprietary LLM and a bespoke model fine-tuned on proprietary historical\ndocumentation.\n  Our findings reveal that documentation challenges are sociotechnical in\nnature, shaped by clinical workflows, organizational policies, and system\nconstraints. Four key themes emerged: (1) the heterogeneity of workflows, (2)\nthe documentation burden is systemic and not directly linked to the creation of\nany single type of documentation, (3) the need for flexible tools and clinician\nautonomy, and (4) effective implementation requires mutual learning between\nclinicians and AI systems.\n  While LLMs show promise in easing documentation tasks, their success will\ndepend on flexible, adaptive integration that supports clinician autonomy.\nBeyond technical performance, sustained adoption will require training programs\nand implementation strategies that reflect the complexity of clinical\nenvironments.", "AI": {"tldr": "This study explores the sociotechnical implications of using Large Language Models (LLMs) to reduce documentation burdens for occupational therapists.", "motivation": "To understand how LLMs can assist in clinical documentation within pediatric rehabilitation settings and tackle the associated challenges.", "method": "A qualitative study involving 20 clinicians at KidsAbility, examining the implementation of a general-purpose LLM and a bespoke model fine-tuned on historical documentation.", "result": "Documentation challenges are found to be sociotechnical, influenced by clinical workflows and organizational policies. Key themes identified include variations in workflows, systemic nature of documentation burden, need for flexible tools, and the importance of mutual learning in AI integration.", "conclusion": "Although LLMs have potential in alleviating documentation tasks, their success relies on adaptive integration that honors clinician autonomy and includes comprehensive training and implementation strategies.", "key_contributions": ["Identified sociotechnical factors influencing documentation burden in clinical settings.", "Proposed the need for flexible tools that support clinician autonomy.", "Highlighted the importance of training and mutual learning for successful AI implementation."], "limitations": "", "keywords": ["Large Language Models", "Clinical documentation", "Occupational therapy", "Sociotechnical systems", "AI integration"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.03662", "pdf": "https://arxiv.org/pdf/2509.03662.pdf", "abs": "https://arxiv.org/abs/2509.03662", "title": "Semantic Analysis of SNOMED CT Concept Co-occurrences in Clinical Documentation using MIMIC-IV", "authors": ["Ali Noori", "Somya Mohanty", "Prashanti Manda"], "categories": ["cs.CL"], "comment": null, "summary": "Clinical notes contain rich clinical narratives but their unstructured format\nposes challenges for large-scale analysis. Standardized terminologies such as\nSNOMED CT improve interoperability, yet understanding how concepts relate\nthrough co-occurrence and semantic similarity remains underexplored. In this\nstudy, we leverage the MIMIC-IV database to investigate the relationship\nbetween SNOMED CT concept co-occurrence patterns and embedding-based semantic\nsimilarity. Using Normalized Pointwise Mutual Information (NPMI) and pretrained\nembeddings (e.g., ClinicalBERT, BioBERT), we examine whether frequently\nco-occurring concepts are also semantically close, whether embeddings can\nsuggest missing concepts, and how these relationships evolve temporally and\nacross specialties. Our analyses reveal that while co-occurrence and semantic\nsimilarity are weakly correlated, embeddings capture clinically meaningful\nassociations not always reflected in documentation frequency. Embedding-based\nsuggestions frequently matched concepts later documented, supporting their\nutility for augmenting clinical annotations. Clustering of concept embeddings\nyielded coherent clinical themes (symptoms, labs, diagnoses, cardiovascular\nconditions) that map to patient phenotypes and care patterns. Finally,\nco-occurrence patterns linked to outcomes such as mortality and readmission\ndemonstrate the practical utility of this approach. Collectively, our findings\nhighlight the complementary value of co-occurrence statistics and semantic\nembeddings in improving documentation completeness, uncovering latent clinical\nrelationships, and informing decision support and phenotyping applications.", "AI": {"tldr": "This study investigates the relationship between SNOMED CT concept co-occurrence patterns and embedding-based semantic similarity using the MIMIC-IV database, revealing insights into their complementary value in clinical documentation and decision support.", "motivation": "The unstructured format of clinical notes presents challenges for large-scale analysis, necessitating improved understanding of concepts through standardized terminologies and their relationships.", "method": "The study uses Normalized Pointwise Mutual Information (NPMI) alongside pretrained embeddings (ClinicalBERT, BioBERT) to analyze co-occurrence patterns and semantic similarity of clinical concepts over time and across specialties.", "result": "The analysis shows weak correlation between co-occurrence and semantic similarity, but embeddings capture clinically meaningful associations and suggest missing concepts. Co-occurrence patterns are linked to patient outcomes like mortality and readmission.", "conclusion": "The findings emphasize the value of combining co-occurrence statistics with semantic embeddings to enhance clinical documentation, reveal hidden relationships, and aid in decision support and phenotyping.", "key_contributions": ["Investigation of co-occurrence patterns and semantic similarity in clinical concepts", "Demonstration of embedding utility for augmenting clinical annotations", "Identification of clinically coherent themes mapping to patient outcomes"], "limitations": "", "keywords": ["SNOMED CT", "Clinical Notes", "Semantic Similarity", "MIMIC-IV", "Embedding"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.04356", "pdf": "https://arxiv.org/pdf/2509.04356.pdf", "abs": "https://arxiv.org/abs/2509.04356", "title": "SRWToolkit: An Open Source Wizard of Oz Toolkit to Create Social Robotic Avatars", "authors": ["Atikkhan Faridkhan Nilgar", "Kristof Van Laerhoven", "Ayub Kinoti"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "We present SRWToolkit, an open-source Wizard of Oz toolkit designed to\nfacilitate the rapid prototyping of social robotic avatars powered by local\nlarge language models (LLMs). Our web-based toolkit enables multimodal\ninteraction through text input, button-activated speech, and wake-word command.\nThe toolkit offers real-time configuration of avatar appearance, behavior,\nlanguage, and voice via an intuitive control panel. In contrast to prior works\nthat rely on cloud-based LLM services, SRWToolkit emphasizes modularity and\nensures on-device functionality through local LLM inference. In our small-scale\nuser study ($n=11$), participants created and interacted with diverse robotic\nroles (hospital receptionist, mathematics teacher, and driving assistant),\nwhich demonstrated positive outcomes in the toolkit's usability, trust, and\nuser experience. The toolkit enables rapid and efficient development of robot\ncharacters customized to researchers' needs, supporting scalable research in\nhuman-robot interaction.", "AI": {"tldr": "SRWToolkit is an open-source Wizard of Oz toolkit for rapid prototyping of social robotic avatars using local LLMs, enabling multimodal interactions and customizable avatar features.", "motivation": "The aim is to facilitate the rapid prototyping of social robotic avatars powered by local LLMs, addressing the limitations of cloud-based LLM services in terms of modularity and on-device functionality.", "method": "The toolkit allows for multimodal interaction via text input, speech activation, and wake-word commands, with real-time configuration through an intuitive control panel. A user study was conducted with 11 participants to assess toolkit usability and user experience.", "result": "Participants successfully created and interacted with various robotic roles, and results showed positive outcomes in usability, trust, and overall user experience with the toolkit.", "conclusion": "SRWToolkit supports scalable research in human-robot interaction by enabling the efficient development of customized robot characters.", "key_contributions": ["Open-source toolkit for prototyping social robotic avatars", "Supports multimodal interaction and real-time avatar customization", "Operates on-device with local LLM inference"], "limitations": "", "keywords": ["social robotics", "large language models", "human-robot interaction", "toolkit", "user experience"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2509.03725", "pdf": "https://arxiv.org/pdf/2509.03725.pdf", "abs": "https://arxiv.org/abs/2509.03725", "title": "MLSD: A Novel Few-Shot Learning Approach to Enhance Cross-Target and Cross-Domain Stance Detection", "authors": ["Parush Gera", "Tempestt Neal"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present the novel approach for stance detection across domains and\ntargets, Metric Learning-Based Few-Shot Learning for Cross-Target and\nCross-Domain Stance Detection (MLSD). MLSD utilizes metric learning with\ntriplet loss to capture semantic similarities and differences between stance\ntargets, enhancing domain adaptation. By constructing a discriminative\nembedding space, MLSD allows a cross-target or cross-domain stance detection\nmodel to acquire useful examples from new target domains. We evaluate MLSD in\nmultiple cross-target and cross-domain scenarios across two datasets, showing\nstatistically significant improvement in stance detection performance across\nsix widely used stance detection models.", "AI": {"tldr": "Introducing a novel metric learning approach for stance detection that improves performance across various targets and domains.", "motivation": "To enhance stance detection capabilities in diverse contexts by effectively utilizing examples from new target domains.", "method": "Metric Learning-Based Few-Shot Learning (MLSD) uses triplet loss for metric learning to identify semantic similarities and differences in stance detection tasks.", "result": "MLSD demonstrated statistically significant improvements in stance detection performance across six well-known stance detection models in multiple testing scenarios.", "conclusion": "The proposed MLSD approach effectively enables better stance detection through improved domain adaptation and embedding techniques.", "key_contributions": ["Presentation of MLSD for stance detection", "Use of triplet loss for metric learning", "Demonstrated improvement across multiple stance detection models"], "limitations": "", "keywords": ["stance detection", "metric learning", "few-shot learning", "domain adaptation", "semantic similarity"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.04358", "pdf": "https://arxiv.org/pdf/2509.04358.pdf", "abs": "https://arxiv.org/abs/2509.04358", "title": "Privacy Perceptions in Robot-Assisted Well-Being Coaching: Examining the Roles of Information Transparency, User Control, and Proactivity", "authors": ["Atikkhan Faridkhan Nilgar", "Manuel Dietrich", "Kristof Van Laerhoven"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Social robots are increasingly recognized as valuable supporters in the field\nof well-being coaching. They can function as independent coaches or provide\nsupport alongside human coaches, and healthcare professionals. In coaching\ninteractions, these robots often handle sensitive information shared by users,\nmaking privacy a relevant issue. Despite this, little is known about the\nfactors that shape users' privacy perceptions. This research aims to examine\nthree key factors systematically: (1) the transparency about information usage,\n(2) the level of specific user control over how the robot uses their\ninformation, and (3) the robot's behavioral approach - whether it acts\nproactively or only responds on demand. Our results from an online study (N =\n200) show that even when users grant the robot general access to personal data,\nthey additionally expect the ability to explicitly control how that information\nis interpreted and shared during sessions. Experimental conditions that\nprovided such control received significantly higher ratings for perceived\nprivacy appropriateness and trust. Compared to user control, the effects of\ntransparency and proactivity on privacy appropriateness perception were low,\nand we found no significant impact. The results suggest that merely informing\nusers or proactive sharing is insufficient without accompanying user control.\nThese insights underscore the need for further research on mechanisms that\nallow users to manage robots' information processing and sharing, especially\nwhen social robots take on more proactive roles alongside humans.", "AI": {"tldr": "This research examines how social robots' privacy perceptions are shaped by users' control, transparency, and behavioral approaches in coaching interactions.", "motivation": "To understand factors influencing users' privacy perceptions in interactions with social robots used for well-being coaching.", "method": "An online study with 200 participants was conducted to evaluate the effects of user control, transparency, and robot behavior on privacy perceptions.", "result": "Control over information significantly improved perceived privacy and trust, overshadowing the effects of transparency and proactivity.", "conclusion": "User control is crucial for privacy perceptions; merely informing users or being proactive is insufficient without the ability to control information sharing.", "key_contributions": ["Identifies user control as a key factor influencing privacy perceptions.", "Shows low impact of transparency and robot proactivity compared to user control.", "Suggests need for mechanisms allowing users to manage robot information processing."], "limitations": "Study limited to online interactions; real-world coaching dynamics may differ.", "keywords": ["social robots", "privacy perceptions", "user control", "well-being coaching", "human-robot interaction"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2509.03791", "pdf": "https://arxiv.org/pdf/2509.03791.pdf", "abs": "https://arxiv.org/abs/2509.03791", "title": "SiLVERScore: Semantically-Aware Embeddings for Sign Language Generation Evaluation", "authors": ["Saki Imai", "Mert İnan", "Anthony Sicilia", "Malihe Alikhani"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating sign language generation is often done through back-translation,\nwhere generated signs are first recognized back to text and then compared to a\nreference using text-based metrics. However, this two-step evaluation pipeline\nintroduces ambiguity: it not only fails to capture the multimodal nature of\nsign language-such as facial expressions, spatial grammar, and prosody-but also\nmakes it hard to pinpoint whether evaluation errors come from sign generation\nmodel or the translation system used to assess it. In this work, we propose\nSiLVERScore, a novel semantically-aware embedding-based evaluation metric that\nassesses sign language generation in a joint embedding space. Our contributions\ninclude: (1) identifying limitations of existing metrics, (2) introducing\nSiLVERScore for semantically-aware evaluation, (3) demonstrating its robustness\nto semantic and prosodic variations, and (4) exploring generalization\nchallenges across datasets. On PHOENIX-14T and CSL-Daily datasets, SiLVERScore\nachieves near-perfect discrimination between correct and random pairs (ROC AUC\n= 0.99, overlap < 7%), substantially outperforming traditional metrics.", "AI": {"tldr": "SiLVERScore is a novel semantic evaluation metric for sign language generation that improves upon traditional text-based metrics by assessing sign generation in a joint embedding space.", "motivation": "Existing evaluation methods for sign language generation are inadequate as they fail to capture the multimodal aspects of sign language and introduce ambiguity in error identification.", "method": "Introduced SiLVERScore, a semantically-aware embedding-based evaluation metric that evaluates sign language generation within a joint embedding space.", "result": "SiLVERScore achieves near-perfect discrimination between correct and random pairs on the PHOENIX-14T and CSL-Daily datasets, with ROC AUC = 0.99, outperforming traditional metrics significantly.", "conclusion": "The proposed SiLVERScore effectively evaluates sign language generation, capturing nuanced aspects of sign language that existing metrics overlook.", "key_contributions": ["Identifying limitations of existing evaluation metrics for sign language generation", "Introducing SiLVERScore for semantically-aware evaluation", "Demonstrating the robustness of SiLVERScore to semantic and prosodic variations"], "limitations": "", "keywords": ["Sign Language Generation", "Evaluation Metrics", "Semantic Awareness"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.03805", "pdf": "https://arxiv.org/pdf/2509.03805.pdf", "abs": "https://arxiv.org/abs/2509.03805", "title": "Measuring How (Not Just Whether) VLMs Build Common Ground", "authors": ["Saki Imai", "Mert İnan", "Anthony Sicilia", "Malihe Alikhani"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large vision language models (VLMs) increasingly claim reasoning skills, yet\ncurrent benchmarks evaluate them in single-turn or question answering settings.\nHowever, grounding is an interactive process in which people gradually develop\nshared understanding through ongoing communication. We introduce a four-metric\nsuite (grounding efficiency, content alignment, lexical adaptation, and\nhuman-likeness) to systematically evaluate VLM performance in interactive\ngrounding contexts. We deploy the suite on 150 self-play sessions of\ninteractive referential games between three proprietary VLMs and compare them\nwith human dyads. All three models diverge from human patterns on at least\nthree metrics, while GPT4o-mini is the closest overall. We find that (i) task\nsuccess scores do not indicate successful grounding and (ii) high\nimage-utterance alignment does not necessarily predict task success. Our metric\nsuite and findings offer a framework for future research on VLM grounding.", "AI": {"tldr": "This paper evaluates the performance of vision language models (VLMs) in interactive grounding contexts using a new four-metric suite.", "motivation": "Current benchmarks for VLMs assess them in static settings, failing to capture their interactive reasoning capabilities; hence, a new evaluation method is needed.", "method": "A four-metric suite was introduced to assess VLMs based on grounding efficiency, content alignment, lexical adaptation, and human-likeness, tested through 150 interactive referential game sessions among VLMs and human interactions.", "result": "The evaluation revealed that the three VLMs diverged from human interaction patterns, especially in grounding efficiency and content alignment, with GPT4o-mini performing the best.", "conclusion": "Standard task success scores may not accurately reflect effective grounding in interactive contexts, indicating a need for better evaluation methods in VLM research.", "key_contributions": ["Introduction of a four-metric evaluation suite for interactive grounding in VLMs", "Comparison of VLM grounding performance against human dyads", "Findings highlight the limitations of using task success as a measure of effective grounding."], "limitations": "Limited to three VLMs and findings based on self-play sessions; further validation needed with a broader range of models and real-world interactions.", "keywords": ["Vision Language Models", "Grounding", "Interactive Games", "HCI", "Evaluation Metrics"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.03809", "pdf": "https://arxiv.org/pdf/2509.03809.pdf", "abs": "https://arxiv.org/abs/2509.03809", "title": "Align-then-Slide: A complete evaluation framework for Ultra-Long Document-Level Machine Translation", "authors": ["Jiaxin Guo", "Daimeng Wei", "Yuanchang Luo", "Xiaoyu Chen", "Zhanglin Wu", "Huan Yang", "Hengchao Shang", "Zongyao Li", "Zhiqiang Rao", "Jinlong Yang", "Hao Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "under preview", "summary": "Large language models (LLMs) have ushered in a new era for document-level\nmachine translation (\\textit{doc}-mt), yet their whole-document outputs\nchallenge existing evaluation methods that assume sentence-by-sentence\nalignment. We introduce \\textit{\\textbf{Align-then-Slide}}, a complete\nevaluation framework for ultra-long doc-mt. In the Align stage, we\nautomatically infer sentence-level source-target correspondences and rebuild\nthe target to match the source sentence number, resolving omissions and\nmany-to-one/one-to-many mappings. In the n-Chunk Sliding Evaluate stage, we\ncalculate averaged metric scores under 1-, 2-, 3- and 4-chunk for\nmulti-granularity assessment. Experiments on the WMT benchmark show a Pearson\ncorrelation of 0.929 between our method with expert MQM rankings. On a newly\ncurated real-world test set, our method again aligns closely with human\njudgments. Furthermore, preference data produced by Align-then-Slide enables\neffective CPO training and its direct use as a reward model for GRPO, both\nyielding translations preferred over a vanilla SFT baseline. The results\nvalidate our framework as an accurate, robust, and actionable evaluation tool\nfor doc-mt systems.", "AI": {"tldr": "Introduction of Align-then-Slide, an evaluation framework for document-level machine translation that improves upon existing sentence-aligned evaluation methods.", "motivation": "To address the limitations of current evaluation methods for whole-document outputs from large language models in document-level machine translation.", "method": "The paper presents a two-stage evaluation process: 1) Align stage to infer sentence-level correspondences and rebuild targets; 2) n-Chunk Sliding Evaluate stage for multi-granularity assessment by calculating averaged metric scores for different chunk sizes.", "result": "Achieved a Pearson correlation of 0.929 with expert ratings on the WMT benchmark and demonstrated alignment with human judgments on a new test set. Showed that preference data from the method enhances CPO training and serves as an effective reward model for GRPO, resulting in improved translations.", "conclusion": "The Align-then-Slide framework is validated as an accurate and actionable tool for evaluating document-level machine translation systems.", "key_contributions": ["Introduction of a novel evaluation framework for document-level translations.", "Multi-granularity performance assessment through chunk-based evaluation.", "Demonstrated effectiveness of the framework with high correlation to human evaluations."], "limitations": "", "keywords": ["document-level machine translation", "evaluation framework", "large language models"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2509.03829", "pdf": "https://arxiv.org/pdf/2509.03829.pdf", "abs": "https://arxiv.org/abs/2509.03829", "title": "NE-PADD: Leveraging Named Entity Knowledge for Robust Partial Audio Deepfake Detection via Attention Aggregation", "authors": ["Huhong Xian", "Rui Liu", "Berrak Sisman", "Haizhou Li"], "categories": ["cs.CL"], "comment": null, "summary": "Different from traditional sentence-level audio deepfake detection (ADD),\npartial audio deepfake detection (PADD) requires frame-level positioning of the\nlocation of fake speech. While some progress has been made in this area,\nleveraging semantic information from audio, especially named entities, remains\nan underexplored aspect. To this end, we propose NE-PADD, a novel method for\nPartial Audio Deepfake Detection (PADD) that leverages named entity knowledge\nthrough two parallel branches: Speech Name Entity Recognition (SpeechNER) and\nPADD. The approach incorporates two attention aggregation mechanisms: Attention\nFusion (AF) for combining attention weights and Attention Transfer (AT) for\nguiding PADD with named entity semantics using an auxiliary loss. Built on the\nPartialSpoof-NER dataset, experiments show our method outperforms existing\nbaselines, proving the effectiveness of integrating named entity knowledge in\nPADD. The code is available at https://github.com/AI-S2-Lab/NE-PADD.", "AI": {"tldr": "The paper introduces NE-PADD, a novel approach for Partial Audio Deepfake Detection that utilizes named entity information through Speech Name Entity Recognition and attention mechanisms.", "motivation": "Existing methods for audio deepfake detection focus only at the sentence level and do not leverage named entity information, leaving a gap in partial audio detection capabilities.", "method": "NE-PADD employs two branches: Speech Name Entity Recognition and Partial Audio Deepfake Detection, utilizing two attention mechanisms (Attention Fusion and Attention Transfer) for improved detection.", "result": "Experiments on the PartialSpoof-NER dataset indicate that NE-PADD surpasses existing baseline methods in performance, highlighting the benefits of integrating named entity knowledge.", "conclusion": "The proposed method demonstrates that incorporating named entity information significantly enhances the success of Partial Audio Deepfake Detection.", "key_contributions": ["Introduction of NE-PADD for PADD leveraging semantic information.", "Development of dual attention mechanisms for better detection accuracy.", "Demonstration of improved performance over existing baselines using the PartialSpoof-NER dataset."], "limitations": "", "keywords": ["Partial Audio Deepfake Detection", "named entity recognition", "attention mechanisms"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.03867", "pdf": "https://arxiv.org/pdf/2509.03867.pdf", "abs": "https://arxiv.org/abs/2509.03867", "title": "Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth", "authors": ["Yang Wang", "Chenghao Xiao", "Chia-Yi Hsiao", "Zi Yan Chang", "Chi-Li Chen", "Tyler Loakman", "Chenghua Lin"], "categories": ["cs.CL"], "comment": "Accepted for oral presentation at the EMNLP 2025 Main Conference", "summary": "We introduce Drivelology, a unique linguistic phenomenon characterised as\n\"nonsense with depth\", utterances that are syntactically coherent yet\npragmatically paradoxical, emotionally loaded, or rhetorically subversive.\nWhile such expressions may resemble surface-level nonsense, they encode\nimplicit meaning requiring contextual inference, moral reasoning, or emotional\ninterpretation. We find that current large language models (LLMs), despite\nexcelling at many natural language processing (NLP) tasks, consistently fail to\ngrasp the layered semantics of Drivelological text. To investigate this, we\nconstruct a small but diverse benchmark dataset of over 1,200 meticulously\ncurated examples, with select instances in English, Mandarin, Spanish, French,\nJapanese, and Korean. Annotation was especially challenging: each of the\nexamples required careful expert review to verify that it truly reflected\nDrivelological characteristics. The process involved multiple rounds of\ndiscussion and adjudication to address disagreements, highlighting the subtle\nand subjective nature of the Drivelology. We evaluate a range of LLMs on\nclassification, generation, and reasoning tasks. Our results reveal clear\nlimitations of LLMs: models often confuse Drivelology with shallow nonsense,\nproduce incoherent justifications, or miss the implied rhetorical function\naltogether. These findings highlight a deeper representational gap in LLMs'\npragmatic understanding and challenge the assumption that statistical fluency\nimplies cognitive comprehension. We release our dataset and code to facilitate\nfurther research in modelling linguistic depth beyond surface-level coherence.", "AI": {"tldr": "Drivelology is a linguistic phenomenon characterized as \"nonsense with depth\" that large language models struggle to interpret meaningfully.", "motivation": "To explore the limitations of large language models in understanding complex linguistic constructs known as Drivelology, which requires nuanced inference beyond superficial coherence.", "method": "A benchmark dataset of over 1,200 curated examples of Drivelology was created, followed by evaluations of various large language models on tasks involving classification, generation, and reasoning.", "result": "LLMs often misinterpret Drivelological text as shallow nonsense, providing incoherent justifications and missing implied meanings, highlighting gaps in their pragmatic understanding.", "conclusion": "The findings indicate a significant representational gap in LLMs that challenges the assumption that statistical language fluency equates to cognitive comprehension, and a dataset is released to aid future research.", "key_contributions": ["Introduction of the concept of Drivelology", "Development of a benchmark dataset for evaluating LLMs", "Insights into LLM limitations regarding pragmatic understanding."], "limitations": "The subjective nature of Drivelology may complicate broader generalizability of results.", "keywords": ["Drivelology", "large language models", "pragmatic understanding", "natural language processing", "benchmark dataset"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.04104", "pdf": "https://arxiv.org/pdf/2509.04104.pdf", "abs": "https://arxiv.org/abs/2509.04104", "title": "Towards Stable and Personalised Profiles for Lexical Alignment in Spoken Human-Agent Dialogue", "authors": ["Keara Schaaij", "Roel Boumans", "Tibor Bosse", "Iris Hendrickx"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted for TSD 2025", "summary": "Lexical alignment, where speakers start to use similar words across\nconversation, is known to contribute to successful communication. However, its\nimplementation in conversational agents remains underexplored, particularly\nconsidering the recent advancements in large language models (LLMs). As a first\nstep towards enabling lexical alignment in human-agent dialogue, this study\ndraws on strategies for personalising conversational agents and investigates\nthe construction of stable, personalised lexical profiles as a basis for\nlexical alignment. Specifically, we varied the amounts of transcribed spoken\ndata used for construction as well as the number of items included in the\nprofiles per part-of-speech (POS) category and evaluated profile performance\nacross time using recall, coverage, and cosine similarity metrics. It was shown\nthat smaller and more compact profiles, created after 10 min of transcribed\nspeech containing 5 items for adjectives, 5 items for conjunctions, and 10\nitems for adverbs, nouns, pronouns, and verbs each, offered the best balance in\nboth performance and data efficiency. In conclusion, this study offers\npractical insights into constructing stable, personalised lexical profiles,\ntaking into account minimal data requirements, serving as a foundational step\ntoward lexical alignment strategies in conversational agents.", "AI": {"tldr": "This study investigates constructing stable, personalised lexical profiles for enabling lexical alignment in conversational agents, highlighting optimal data usage.", "motivation": "To explore the implementation of lexical alignment in conversational agents, leveraging recent advancements in large language models.", "method": "The study varied the amounts of transcribed spoken data and the number of items in lexical profiles per part-of-speech category to evaluate performance using recall, coverage, and cosine similarity metrics.", "result": "Smaller and more compact lexical profiles, created from 10 minutes of transcribed speech, showed the best balance in performance and data efficiency.", "conclusion": "The findings provide practical insights for constructing stable, personalised lexical profiles with minimal data requirements, supporting future lexical alignment strategies in human-agent dialogue.", "key_contributions": ["Methods for personalized lexical profiles in conversational agents", "Evaluation metrics for profile performance over time", "Findings on optimal data requirements for lexical alignment"], "limitations": "", "keywords": ["lexical alignment", "conversational agents", "personalized profiles", "large language models", "human-agent dialogue"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.03871", "pdf": "https://arxiv.org/pdf/2509.03871.pdf", "abs": "https://arxiv.org/abs/2509.03871", "title": "A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models", "authors": ["Yanbo Wang", "Yongcan Yu", "Jian Liang", "Ran He"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "38 pages. This survey considers papers published up to June 30, 2025.\n  Work in progress", "summary": "The development of Long-CoT reasoning has advanced LLM performance across\nvarious tasks, including language understanding, complex problem solving, and\ncode generation. This paradigm enables models to generate intermediate\nreasoning steps, thereby improving both accuracy and interpretability. However,\ndespite these advancements, a comprehensive understanding of how CoT-based\nreasoning affects the trustworthiness of language models remains\nunderdeveloped. In this paper, we survey recent work on reasoning models and\nCoT techniques, focusing on five core dimensions of trustworthy reasoning:\ntruthfulness, safety, robustness, fairness, and privacy. For each aspect, we\nprovide a clear and structured overview of recent studies in chronological\norder, along with detailed analyses of their methodologies, findings, and\nlimitations. Future research directions are also appended at the end for\nreference and discussion. Overall, while reasoning techniques hold promise for\nenhancing model trustworthiness through hallucination mitigation, harmful\ncontent detection, and robustness improvement, cutting-edge reasoning models\nthemselves often suffer from comparable or even greater vulnerabilities in\nsafety, robustness, and privacy. By synthesizing these insights, we hope this\nwork serves as a valuable and timely resource for the AI safety community to\nstay informed on the latest progress in reasoning trustworthiness. A full list\nof related papers can be found at\n\\href{https://github.com/ybwang119/Awesome-reasoning-safety}{https://github.com/ybwang119/Awesome-reasoning-safety}.", "AI": {"tldr": "This paper surveys Long-CoT reasoning in language models, focusing on its impact on trustworthiness across five dimensions: truthfulness, safety, robustness, fairness, and privacy. It analyzes methodologies and findings of recent studies and discusses future research directions.", "motivation": "To provide a comprehensive understanding of how Chain-of-Thought (CoT) reasoning affects the trustworthiness of language models, an area that is underdeveloped despite recent advancements.", "method": "The paper surveys recent literature on reasoning models and CoT techniques, structuring analyses around trustworthiness dimensions and presenting findings in chronological order.", "result": "The survey highlights that while reasoning techniques can improve model trustworthiness and mitigate issues like hallucinations and harmful content, current reasoning models have vulnerabilities in safety, robustness, and privacy.", "conclusion": "This work serves as a resource for the AI safety community to stay updated on advancements in reasoning and trustworthiness, while also identifying key areas needing further research.", "key_contributions": ["Comprehensive survey on CoT reasoning techniques", "Structured analysis of trustworthiness in LLMs", "Identification of vulnerabilities in current reasoning models"], "limitations": "Focuses on literature published up to June 30, 2025; ongoing developments in LLMs may not be covered.", "keywords": ["Long-CoT reasoning", "trustworthiness", "Linguistic models", "AI safety", "Chain-of-Thought"], "importance_score": 9, "read_time_minutes": 38}}
{"id": "2509.03888", "pdf": "https://arxiv.org/pdf/2509.03888.pdf", "abs": "https://arxiv.org/abs/2509.03888", "title": "False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize", "authors": ["Cheng Wang", "Zeming Wei", "Qin Liu", "Muhao Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) can comply with harmful instructions, raising\nserious safety concerns despite their impressive capabilities. Recent work has\nleveraged probing-based approaches to study the separability of malicious and\nbenign inputs in LLMs' internal representations, and researchers have proposed\nusing such probing methods for safety detection. We systematically re-examine\nthis paradigm. Motivated by poor out-of-distribution performance, we\nhypothesize that probes learn superficial patterns rather than semantic\nharmfulness. Through controlled experiments, we confirm this hypothesis and\nidentify the specific patterns learned: instructional patterns and trigger\nwords. Our investigation follows a systematic approach, progressing from\ndemonstrating comparable performance of simple n-gram methods, to controlled\nexperiments with semantically cleaned datasets, to detailed analysis of pattern\ndependencies. These results reveal a false sense of security around current\nprobing-based approaches and highlight the need to redesign both models and\nevaluation protocols, for which we provide further discussions in the hope of\nsuggesting responsible further research in this direction. We have open-sourced\nthe project at https://github.com/WangCheng0116/Why-Probe-Fails.", "AI": {"tldr": "This paper critiques the effectiveness of probing-based safety detection methods for Large Language Models, revealing that they often identify superficial patterns rather than genuine semantic harmfulness.", "motivation": "To address safety concerns arising from Large Language Models' compliance with harmful instructions by re-examining probing-based approaches.", "method": "Conducted controlled experiments to investigate the learned patterns by probes and their performance using semantically cleaned datasets.", "result": "Opposition of the hypothesis that probes can effectively discern harmful inputs, showing they learn superficial patterns instead of representative semantic harmfulness.", "conclusion": "Current probing-based approaches provide a false sense of security; there is a need for redesigning models and evaluation methodologies to enhance safety.", "key_contributions": ["Identification of superficial patterns learned by probing methods", "Critique of the current probing-based safety detection framework", "Recommendations for redesigning models and evaluation protocols"], "limitations": "The study is limited to specific probing methods and may not encompass all safety evaluation techniques in LLMs.", "keywords": ["Large Language Models", "safety detection", "probing methods", "machine learning", "HCI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.03891", "pdf": "https://arxiv.org/pdf/2509.03891.pdf", "abs": "https://arxiv.org/abs/2509.03891", "title": "MobileRAG: Enhancing Mobile Agent with Retrieval-Augmented Generation", "authors": ["Gowen Loo", "Chang Liu", "Qinghong Yin", "Xiang Chen", "Jiawei Chen", "Jingyuan Zhang", "Yu Tian"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Smartphones have become indispensable in people's daily lives, permeating\nnearly every aspect of modern society. With the continuous advancement of large\nlanguage models (LLMs), numerous LLM-based mobile agents have emerged. These\nagents are capable of accurately parsing diverse user queries and automatically\nassisting users in completing complex or repetitive operations. However,\ncurrent agents 1) heavily rely on the comprehension ability of LLMs, which can\nlead to errors caused by misoperations or omitted steps during tasks, 2) lack\ninteraction with the external environment, often terminating tasks when an app\ncannot fulfill user queries, and 3) lack memory capabilities, requiring each\ninstruction to reconstruct the interface and being unable to learn from and\ncorrect previous mistakes. To alleviate the above issues, we propose MobileRAG,\na mobile agents framework enhanced by Retrieval-Augmented Generation (RAG),\nwhich includes InterRAG, LocalRAG, and MemRAG. It leverages RAG to more quickly\nand accurately identify user queries and accomplish complex and long-sequence\nmobile tasks. Additionally, to more comprehensively assess the performance of\nMobileRAG, we introduce MobileRAG-Eval, a more challenging benchmark\ncharacterized by numerous complex, real-world mobile tasks that require\nexternal knowledge assistance. Extensive experimental results on MobileRAG-Eval\ndemonstrate that MobileRAG can easily handle real-world mobile tasks, achieving\n10.3\\% improvement over state-of-the-art methods with fewer operational steps.\nOur code is publicly available at:\nhttps://github.com/liuxiaojieOutOfWorld/MobileRAG_arxiv", "AI": {"tldr": "MobileRAG is a framework for mobile agents enhanced by Retrieval-Augmented Generation to improve task completion and handle complex queries.", "motivation": "Current LLM-based mobile agents struggle with task comprehension, environmental interaction, and lack memory capabilities, leading to errors and inefficiencies.", "method": "The framework includes InterRAG, LocalRAG, and MemRAG, utilizing RAG to optimize query identification and task execution in complex mobile scenarios.", "result": "MobileRAG demonstrated a 10.3% improvement over state-of-the-art methods on the MobileRAG-Eval benchmark with fewer operational steps.", "conclusion": "MobileRAG effectively handles real-world mobile tasks, showcasing its potential for improving mobile agent capabilities.", "key_contributions": ["Introduction of MobileRAG framework for mobile agents", "Enhanced performance through Retrieval-Augmented Generation", "Development of MobileRAG-Eval benchmark for complex task assessment"], "limitations": "", "keywords": ["Mobile Agents", "Retrieval-Augmented Generation", "Human-Computer Interaction", "Large Language Models", "Mobile Computing"], "importance_score": 9, "read_time_minutes": 7}}
{"id": "2509.03918", "pdf": "https://arxiv.org/pdf/2509.03918.pdf", "abs": "https://arxiv.org/abs/2509.03918", "title": "MTQA:Matrix of Thought for Enhanced Reasoning in Complex Question Answering", "authors": ["Fengxiao Tang", "Yufeng Li", "Zongzong Wu", "Ming Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Complex Question Answering (QA) is a fundamental and challenging task in NLP.\nWhile large language models (LLMs) exhibit impressive performance in QA, they\nsuffer from significant performance degradation when facing complex and\nabstract QA tasks due to insufficient reasoning capabilities. Works such as\nChain-of-Thought (CoT) and Tree-of-Thought (ToT) aim to enhance LLMs' reasoning\nabilities, but they face issues such as in-layer redundancy in tree structures\nand single paths in chain structures. Although some studies utilize\nRetrieval-Augmented Generation (RAG) methods to assist LLMs in reasoning, the\nchallenge of effectively utilizing large amounts of information involving\nmultiple entities and hops remains critical. To address this, we propose the\nMatrix of Thought (MoT), a novel and efficient LLM thought structure. MoT\nexplores the problem in both horizontal and vertical dimensions through the\n\"column-cell communication\" mechanism, enabling LLMs to actively engage in\nmulti-strategy and deep-level thinking, reducing redundancy within the column\ncells and enhancing reasoning capabilities. Furthermore, we develop a\nfact-correction mechanism by constructing knowledge units from retrieved\nknowledge graph triples and raw text to enhance the initial knowledge for LLM\nreasoning and correct erroneous answers. This leads to the development of an\nefficient and accurate QA framework (MTQA). Experimental results show that our\nframework outperforms state-of-the-art methods on four widely-used datasets in\nterms of F1 and EM scores, with reasoning time only 14.4\\% of the baseline\nmethods, demonstrating both its efficiency and accuracy. The code for this\nframework is available at https://github.com/lyfiter/mtqa.", "AI": {"tldr": "This paper introduces the Matrix of Thought (MoT), a novel LLM thought structure designed to enhance reasoning capabilities for Complex Question Answering (QA), outperforming existing methods.", "motivation": "To address the performance degradation of large language models (LLMs) in complex QA tasks due to inadequate reasoning abilities.", "method": "The Matrix of Thought (MoT) employs a 'column-cell communication' mechanism that allows for multi-strategy and deep-level thinking, while also implementing a fact-correction mechanism using knowledge units from retrieved knowledge graph triples.", "result": "The MTQA framework outperforms state-of-the-art methods on four datasets in F1 and EM scores, while achieving a reasoning time that is only 14.4% of baseline methods' time.", "conclusion": "MoT significantly improves LLM reasoning for complex QA tasks, demonstrating both efficiency and accuracy.", "key_contributions": ["Introduction of the Matrix of Thought (MoT) structure", "Enhanced reasoning capabilities for LLMs", "Development of an efficient and accurate QA framework (MTQA)"], "limitations": "", "keywords": ["Complex Question Answering", "Large Language Models", "Matrix of Thought"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.18348", "pdf": "https://arxiv.org/pdf/2502.18348.pdf", "abs": "https://arxiv.org/abs/2502.18348", "title": "Towards softerware: Enabling personalization of interactive data representations for users with disabilities", "authors": ["Frank Elavsky", "Marita Vindedal", "Ted Gies", "Patrick Carrington", "Dominik Moritz", "Øystein Moseng"], "categories": ["cs.HC"], "comment": "pre-print, accepted for CG&A special issue on Inclusive Data\n  Experiences, 13 pages", "summary": "Accessible design for some may still produce barriers for others. This\ntension, called access friction, creates challenges for both designers and\nend-users with disabilities. To address this, we present the concept of\nsofterware, a system design approach that provides end users with agency to\nmeaningfully customize and adapt interfaces to their needs. To apply softerware\nto visualization, we assembled 195 data visualization customization options\ncentered on the barriers we expect users with disabilities will experience. We\nbuilt a prototype that applies a subset of these options and interviewed\npractitioners for feedback. Lastly, we conducted a design probe study with\nblind and low vision accessibility professionals to learn more about their\nchallenges and visions for softerware. We observed access frictions between our\nparticipant's designs and they expressed that for softerware's success, current\nand future systems must be designed with accessible defaults, interoperability,\npersistence, and respect for a user's perceived effort-to-outcome ratio.", "AI": {"tldr": "The paper introduces 'softerware,' a design approach enabling users to customize interfaces to alleviate access friction, particularly for individuals with disabilities, and discusses findings from a design probe study.", "motivation": "To address the challenges posed by access friction in accessible design, which can hinder users with disabilities, by empowering them to customize interfaces according to their needs.", "method": "The study involved assembling a set of 195 customization options for data visualizations, developing a prototype that implements some of these options, and conducting interviews and a design probe study with accessibility professionals.", "result": "Participants identified access frictions in designs and emphasized the importance of accessible defaults, interoperability, persistence, and user-friendly effort-to-outcome ratios for the success of softerware.", "conclusion": "The research highlights the necessity of considering user agency in interface design and advocates for a more inclusive design philosophy to support individuals with disabilities.", "key_contributions": ["Introduction of the concept of softerware for accessible design.", "Development of a prototype using customization options for visualizations.", "Insights from interviews with accessibility professionals regarding design challenges."], "limitations": "The study may be limited by the scope of customization options explored and the specific contexts of the participants involved in the design probe study.", "keywords": ["accessible design", "user customization", "data visualization", "accessibility", "design probe"], "importance_score": 8, "read_time_minutes": 13}}
{"id": "2509.03932", "pdf": "https://arxiv.org/pdf/2509.03932.pdf", "abs": "https://arxiv.org/abs/2509.03932", "title": "Decoding the Poetic Language of Emotion in Korean Modern Poetry: Insights from a Human-Labeled Dataset and AI Modeling", "authors": ["Iro Lim", "Haein Ji", "Byungjun Kim"], "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": "30 pages, 13 tables, 2 figures, Digital Humanities and Social\n  Sciences Korea Conference, James Joo-Jin Kim Center for Korean Studies,\n  University of Pennsylvania, Philadelphia, USA", "summary": "This study introduces KPoEM (Korean Poetry Emotion Mapping) , a novel dataset\nfor computational emotion analysis in modern Korean poetry. Despite remarkable\nprogress in text-based emotion classification using large language models,\npoetry-particularly Korean poetry-remains underexplored due to its figurative\nlanguage and cultural specificity. We built a multi-label emotion dataset of\n7,662 entries, including 7,007 line-level entries from 483 poems and 615\nwork-level entries, annotated with 44 fine-grained emotion categories from five\ninfluential Korean poets. A state-of-the-art Korean language model fine-tuned\non this dataset significantly outperformed previous models, achieving 0.60\nF1-micro compared to 0.34 from models trained on general corpora. The KPoEM\nmodel, trained through sequential fine-tuning-first on general corpora and then\non the KPoEM dataset-demonstrates not only an enhanced ability to identify\ntemporally and culturally specific emotional expressions, but also a strong\ncapacity to preserve the core sentiments of modern Korean poetry. This study\nbridges computational methods and literary analysis, presenting new\npossibilities for the quantitative exploration of poetic emotions through\nstructured data that faithfully retains the emotional and cultural nuances of\nKorean literature.", "AI": {"tldr": "Introduction of KPoEM, a dataset for emotion analysis in Korean poetry, enhancing emotion classification via a fine-tuned language model.", "motivation": "To address the gap in computational emotion analysis specifically in modern Korean poetry, which is often overlooked due to its unique linguistic and cultural characteristics.", "method": "Development of a multi-label emotion dataset comprised of 7,662 entries, annotated with 44 fine-grained emotion categories and trained using a Korean language model through sequential fine-tuning.", "result": "The KPoEM model achieved a F1-micro score of 0.60, significantly outperforming previous models which scored 0.34 when trained on general corpora.", "conclusion": "KPoEM not only improves the classification of emotions in Korean poetry but also preserves its cultural sentiment, paving the way for quantitative emotional analysis in literature.", "key_contributions": ["Creation of the KPoEM dataset with extensive annotation for emotion analysis in poetry", "Demonstration of the effectiveness of fine-tuning a Korean language model specifically for poetic texts", "Integration of computational methods with literary analysis for exploring poetry's emotional depth."], "limitations": "The dataset is specific to Korean poetry and may not directly apply to other languages or poetic traditions.", "keywords": ["Korean Poetry", "Emotion Analysis", "Natural Language Processing", "Dataset", "Machine Learning"], "importance_score": 4, "read_time_minutes": 30}}
{"id": "2509.03934", "pdf": "https://arxiv.org/pdf/2509.03934.pdf", "abs": "https://arxiv.org/abs/2509.03934", "title": "SelfAug: Mitigating Catastrophic Forgetting in Retrieval-Augmented Generation via Distribution Self-Alignment", "authors": ["Yuqing Huang", "Rongyang Zhang", "Qimeng Wang", "Chengqiang Lu", "Yan Gao", "Yi Wu", "Yao Hu", "Xuyang Zhi", "Guiquan Liu", "Xin Li", "Hao Wang", "Enhong Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have revolutionized\nnatural language processing through their remarkable capabilities in\nunderstanding and executing diverse tasks. While supervised fine-tuning,\nparticularly in Retrieval-Augmented Generation (RAG) scenarios, effectively\nenhances task-specific performance, it often leads to catastrophic forgetting,\nwhere models lose their previously acquired knowledge and general capabilities.\nExisting solutions either require access to general instruction data or face\nlimitations in preserving the model's original distribution. To overcome these\nlimitations, we propose SelfAug, a self-distribution alignment method that\naligns input sequence logits to preserve the model's semantic distribution,\nthereby mitigating catastrophic forgetting and improving downstream\nperformance. Extensive experiments demonstrate that SelfAug achieves a superior\nbalance between downstream learning and general capability retention. Our\ncomprehensive empirical analysis reveals a direct correlation between\ndistribution shifts and the severity of catastrophic forgetting in RAG\nscenarios, highlighting how the absence of RAG capabilities in general\ninstruction tuning leads to significant distribution shifts during fine-tuning.\nOur findings not only advance the understanding of catastrophic forgetting in\nRAG contexts but also provide a practical solution applicable across diverse\nfine-tuning scenarios. Our code is publicly available at\nhttps://github.com/USTC-StarTeam/SelfAug.", "AI": {"tldr": "This paper introduces SelfAug, a method designed to mitigate catastrophic forgetting in fine-tuned large language models by aligning input sequence logits to preserve model semantic distribution.", "motivation": "To address catastrophic forgetting in large language models during supervised fine-tuning, particularly in Retrieval-Augmented Generation contexts, where models often lose previously acquired knowledge.", "method": "SelfAug aligns input sequence logits to the model's semantic distribution, aiming to retain general capabilities while improving task-specific performance during fine-tuning.", "result": "Experiments show that SelfAug achieves better retention of general capabilities and task performance compared to existing solutions, demonstrating a correlation between distribution shifts and catastrophic forgetting severity in RAG scenarios.", "conclusion": "The findings reveal critical insights into catastrophic forgetting in RAG contexts and offer a practical alignment solution for diverse fine-tuning situations.", "key_contributions": ["Introduction of the SelfAug method for semantic distribution alignment", "Empirical analysis linking distribution shifts to catastrophic forgetting severity", "Demonstration of improved performance in task-specific learning without significant knowledge loss."], "limitations": "", "keywords": ["large language models", "catastrophic forgetting", "Retrieval-Augmented Generation", "semantic distribution", "fine-tuning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.03937", "pdf": "https://arxiv.org/pdf/2509.03937.pdf", "abs": "https://arxiv.org/abs/2509.03937", "title": "SPFT-SQL: Enhancing Large Language Model for Text-to-SQL Parsing by Self-Play Fine-Tuning", "authors": ["Yuhao Zhang", "Shaoming Duan", "Jinhang Su", "Chuanyi Liu", "Peiyi Han"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 Findings", "summary": "Despite the significant advancements of self-play fine-tuning (SPIN), which\ncan transform a weak large language model (LLM) into a strong one through\ncompetitive interactions between models of varying capabilities, it still faces\nchallenges in the Text-to-SQL task. SPIN does not generate new information, and\nthe large number of correct SQL queries produced by the opponent model during\nself-play reduces the main model's ability to generate accurate SQL queries. To\naddress this challenge, we propose a new self-play fine-tuning method tailored\nfor the Text-to-SQL task, called SPFT-SQL. Prior to self-play, we introduce a\nverification-based iterative fine-tuning approach, which synthesizes\nhigh-quality fine-tuning data iteratively based on the database schema and\nvalidation feedback to enhance model performance, while building a model base\nwith varying capabilities. During the self-play fine-tuning phase, we propose\nan error-driven loss method that incentivizes incorrect outputs from the\nopponent model, enabling the main model to distinguish between correct SQL and\nerroneous SQL generated by the opponent model, thereby improving its ability to\ngenerate correct SQL. Extensive experiments and in-depth analyses on six\nopen-source LLMs and five widely used benchmarks demonstrate that our approach\noutperforms existing state-of-the-art (SOTA) methods.", "AI": {"tldr": "This paper introduces SPFT-SQL, a new self-play fine-tuning method for improving large language models' performance in the Text-to-SQL task by leveraging verification-based iterative fine-tuning and an error-driven loss method.", "motivation": "The challenge in the Text-to-SQL task where existing self-play methods like SPIN struggle due to the generated SQL queries from opponent models reducing accuracy.", "method": "A verification-based iterative fine-tuning approach synthesizes high-quality fine-tuning data before self-play, followed by an error-driven loss method to distinguish between correct and incorrect SQL outputs.", "result": "SPFT-SQL demonstrates improved performance over existing state-of-the-art methods on six open-source LLMs and five benchmarks.", "conclusion": "The proposed SPFT-SQL method significantly enhances the ability of LLMs to generate accurate SQL queries and addresses limitations in previous self-play methods.", "key_contributions": ["Introduction of SPFT-SQL for Text-to-SQL fine-tuning", "Verification-based iterative fine-tuning for high-quality data generation", "Error-driven loss method for better output discrimination"], "limitations": "", "keywords": ["self-play fine-tuning", "Text-to-SQL", "large language models", "error-driven loss", "iterative fine-tuning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.03940", "pdf": "https://arxiv.org/pdf/2509.03940.pdf", "abs": "https://arxiv.org/abs/2509.03940", "title": "VoxRole: A Comprehensive Benchmark for Evaluating Speech-Based Role-Playing Agents", "authors": ["Weihao Wu", "Liang Cao", "Xinyu Wu", "Zhiwei Lin", "Rui Niu", "Jingbei Li", "Zhiyong Wu"], "categories": ["cs.CL", "cs.AI", "cs.SD"], "comment": null, "summary": "Recent significant advancements in Large Language Models (LLMs) have greatly\npropelled the development of Role-Playing Conversational Agents (RPCAs). These\nsystems aim to create immersive user experiences through consistent persona\nadoption. However, current RPCA research faces dual limitations. First,\nexisting work predominantly focuses on the textual modality, entirely\noverlooking critical paralinguistic features including intonation, prosody, and\nrhythm in speech, which are essential for conveying character emotions and\nshaping vivid identities. Second, the speech-based role-playing domain suffers\nfrom a long-standing lack of standardized evaluation benchmarks. Most current\nspoken dialogue datasets target only fundamental capability assessments,\nfeaturing thinly sketched or ill-defined character profiles. Consequently, they\nfail to effectively quantify model performance on core competencies like\nlong-term persona consistency. To address this critical gap, we introduce\nVoxRole, the first comprehensive benchmark specifically designed for the\nevaluation of speech-based RPCAs. The benchmark comprises 13335 multi-turn\ndialogues, totaling 65.6 hours of speech from 1228 unique characters across 261\nmovies. To construct this resource, we propose a novel two-stage automated\npipeline that first aligns movie audio with scripts and subsequently employs an\nLLM to systematically build multi-dimensional profiles for each character.\nLeveraging VoxRole, we conduct a multi-dimensional evaluation of contemporary\nspoken dialogue models, revealing crucial insights into their respective\nstrengths and limitations in maintaining persona consistency.", "AI": {"tldr": "This paper introduces VoxRole, a benchmark for evaluating speech-based Role-Playing Conversational Agents (RPCAs), addressing the lack of focus on paralinguistic features and standardized evaluation methods in current research.", "motivation": "To fill the gaps in evaluating speech-based RPCAs concerning paralinguistic features and to provide standardized benchmarks for assessing model performance.", "method": "A novel two-stage automated pipeline was developed to align movie audio with scripts and use an LLM to create multi-dimensional profiles for 1228 unique characters across 261 movies, resulting in 13,335 multi-turn dialogues totaling 65.6 hours of speech.", "result": "Conducted a multi-dimensional evaluation of contemporary spoken dialogue models using VoxRole, revealing insights into strengths and limitations regarding persona consistency.", "conclusion": "VoxRole serves as a substantial resource for assessing the performance of RPCAs, emphasizing the importance of incorporating paralinguistic features in dialogue systems.", "key_contributions": ["Introduction of VoxRole benchmark for RPCAs", "Focus on paralinguistic features in evaluation", "Multi-dimensional character profiling for improved assessment"], "limitations": "", "keywords": ["Role-Playing Conversational Agents", "VoxRole", "Speech-based evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.03957", "pdf": "https://arxiv.org/pdf/2509.03957.pdf", "abs": "https://arxiv.org/abs/2509.03957", "title": "CANDY: Benchmarking LLMs' Limitations and Assistive Potential in Chinese Misinformation Fact-Checking", "authors": ["Ruiling Guo", "Xinwei Yang", "Chen Huang", "Tong Zhang", "Yong Hu"], "categories": ["cs.CL", "cs.AI"], "comment": "Findings of EMNLP 2025", "summary": "The effectiveness of large language models (LLMs) to fact-check\nmisinformation remains uncertain, despite their growing use. To this end, we\npresent CANDY, a benchmark designed to systematically evaluate the capabilities\nand limitations of LLMs in fact-checking Chinese misinformation. Specifically,\nwe curate a carefully annotated dataset of ~20k instances. Our analysis shows\nthat current LLMs exhibit limitations in generating accurate fact-checking\nconclusions, even when enhanced with chain-of-thought reasoning and few-shot\nprompting. To understand these limitations, we develop a taxonomy to categorize\nflawed LLM-generated explanations for their conclusions and identify factual\nfabrication as the most common failure mode. Although LLMs alone are unreliable\nfor fact-checking, our findings indicate their considerable potential to\naugment human performance when deployed as assistive tools in scenarios. Our\ndataset and code can be accessed at https://github.com/SCUNLP/CANDY", "AI": {"tldr": "This paper introduces CANDY, a benchmark for evaluating large language models in fact-checking misinformation in Chinese, revealing significant limitations in their reliability.", "motivation": "To assess the effectiveness of LLMs in fact-checking misinformation, particularly in the context of the Chinese language, given the uncertainty surrounding their capabilities.", "method": "We curated a dataset of approximately 20,000 instances for systematic evaluation and developed a taxonomy to categorize flawed explanations from LLMs.", "result": "Current LLMs struggle with generating accurate fact-checking conclusions, with factual fabrication being the most common failure mode.", "conclusion": "Although LLMs are not reliable for standalone fact-checking, they can significantly aid human performance when used as assistive tools.", "key_contributions": ["Introduction of CANDY benchmark for LLM evaluation in fact-checking", "Curated dataset of ~20k instances focused on Chinese misinformation", "Taxonomy of LLM-generated explanation failures, highlighting factual fabrication"], "limitations": "LLMs alone are unreliable for factual accuracy in fact-checking.", "keywords": ["Large Language Models", "Fact-Checking", "Misinformation", "Chinese Language", "Human-AI Collaboration"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.03962", "pdf": "https://arxiv.org/pdf/2509.03962.pdf", "abs": "https://arxiv.org/abs/2509.03962", "title": "Exploring NLP Benchmarks in an Extremely Low-Resource Setting", "authors": ["Ulin Nuha", "Adam Jatowt"], "categories": ["cs.CL"], "comment": null, "summary": "The effectiveness of Large Language Models (LLMs) diminishes for extremely\nlow-resource languages, such as indigenous languages, primarily due to the lack\nof labeled data. Despite growing interest, the availability of high-quality\nnatural language processing (NLP) datasets for these languages remains limited,\nmaking it difficult to develop robust language technologies. This paper\naddresses such gap by focusing on Ladin, an endangered Romance language,\nspecifically targeting the Val Badia variant. Leveraging a small set of\nparallel Ladin-Italian sentence pairs, we create synthetic datasets for\nsentiment analysis and multiple-choice question answering (MCQA) by translating\nmonolingual Italian data. To ensure linguistic quality and reliability, we\napply rigorous filtering and back-translation procedures in our method. We\nfurther demonstrate that incorporating these synthetic datasets into machine\ntranslation training leads to substantial improvements over existing\nItalian-Ladin translation baselines. Our contributions include the first\npublicly available sentiment analysis and MCQA datasets for Ladin, establishing\nfoundational resources that can support broader NLP research and downstream\napplications for this underrepresented language.", "AI": {"tldr": "This paper develops synthetic NLP datasets for the endangered Ladin language through innovative translation of existing monolingual data, improving sentiment analysis and question answering capabilities.", "motivation": "To address the lack of high-quality NLP datasets for low-resource languages, particularly the endangered Ladin language, and to enhance language technology solutions for such languages.", "method": "The authors create synthetic datasets for sentiment analysis and multiple-choice question answering using a small set of parallel Ladin-Italian sentence pairs, applying rigorous filtering and back-translation to ensure quality.", "result": "The incorporation of these synthetic datasets into machine translation training significantly improves the performance of Italian-Ladin translation systems.", "conclusion": "The paper presents the first publicly available sentiment analysis and MCQA datasets for Ladin, fostering further NLP research and applications for underrepresented languages.", "key_contributions": ["First sentiment analysis dataset for Ladin", "First multiple-choice question answering dataset for Ladin", "Improvement of translation models using synthetic datasets"], "limitations": "", "keywords": ["Ladin", "Low-resource languages", "Sentiment analysis", "Natural language processing", "Machine translation"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.03972", "pdf": "https://arxiv.org/pdf/2509.03972.pdf", "abs": "https://arxiv.org/abs/2509.03972", "title": "Expanding Foundational Language Capabilities in Open-Source LLMs through a Korean Case Study", "authors": ["Junghwan Lim", "Gangwon Jo", "Sungmin Lee", "Jiyoung Park", "Dongseok Kim", "Jihwan Kim", "Junhyeok Lee", "Wai Ting Cheung", "Dahye Choi", "Kibong Choi", "Jaeyeon Huh", "Beomgyu Kim", "Jangwoong Kim", "Taehyun Kim", "Haesol Lee", "Jeesoo Lee", "Dongpin Oh", "Changseok Song", "Daewon Suh"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce Llama-3-Motif, a language model consisting of 102 billion\nparameters, specifically designed to enhance Korean capabilities while\nretaining strong performance in English. Developed on the Llama 3 architecture,\nLlama-3-Motif employs advanced training techniques, including LlamaPro and\nMasked Structure Growth, to effectively scale the model without altering its\ncore Transformer architecture. Using the MoAI platform for efficient training\nacross hyperscale GPU clusters, we optimized Llama-3-Motif using a carefully\ncurated dataset that maintains a balanced ratio of Korean and English data.\nLlama-3-Motif shows decent performance on Korean-specific benchmarks,\noutperforming existing models and achieving results comparable to GPT-4.", "AI": {"tldr": "Llama-3-Motif is a 102 billion parameter language model designed to enhance Korean while retaining English capabilities, utilizing advanced training techniques.", "motivation": "To improve the performance of language models specifically for the Korean language while maintaining strong capabilities in English.", "method": "Developed on the Llama 3 architecture, employing LlamaPro and Masked Structure Growth for efficient scaling without altering the Transformer base.", "result": "Llama-3-Motif outperforms existing models on Korean benchmarks, showing comparable results to GPT-4.", "conclusion": "This model addresses the balance of Korean and English language processing in a single architecture.", "key_contributions": ["Introduction of Llama-3-Motif with 102 billion parameters", "Advanced training techniques like LlamaPro and Masked Structure Growth", "Benchmark achievements comparable to GPT-4 in Korean"], "limitations": "", "keywords": ["Llama-3-Motif", "language model", "Korean language", "GPT-4", "machine learning"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2509.03995", "pdf": "https://arxiv.org/pdf/2509.03995.pdf", "abs": "https://arxiv.org/abs/2509.03995", "title": "RTQA : Recursive Thinking for Complex Temporal Knowledge Graph Question Answering with Large Language Models", "authors": ["Zhaoyan Gong", "Juan Li", "Zhiqiang Liu", "Lei Liang", "Huajun Chen", "Wen Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025", "summary": "Current temporal knowledge graph question answering (TKGQA) methods primarily\nfocus on implicit temporal constraints, lacking the capability of handling more\ncomplex temporal queries, and struggle with limited reasoning abilities and\nerror propagation in decomposition frameworks. We propose RTQA, a novel\nframework to address these challenges by enhancing reasoning over TKGs without\nrequiring training. Following recursive thinking, RTQA recursively decomposes\nquestions into sub-problems, solves them bottom-up using LLMs and TKG\nknowledge, and employs multi-path answer aggregation to improve fault\ntolerance. RTQA consists of three core components: the Temporal Question\nDecomposer, the Recursive Solver, and the Answer Aggregator. Experiments on\nMultiTQ and TimelineKGQA benchmarks demonstrate significant Hits@1 improvements\nin \"Multiple\" and \"Complex\" categories, outperforming state-of-the-art methods.\nOur code and data are available at https://github.com/zjukg/RTQA.", "AI": {"tldr": "RTQA is a new framework for temporal knowledge graph question answering that improves reasoning and fault tolerance without requiring training.", "motivation": "Current TKGQA methods struggle with complex temporal queries and reasoning abilities, leading to error propagation in existing frameworks.", "method": "RTQA uses a recursive approach to decompose questions into sub-problems, applies LLMs and TKG knowledge for solutions, and aggregates answers through multi-path methods.", "result": "Experiments show significant improvements in Hits@1 for 'Multiple' and 'Complex' categories on the MultiTQ and TimelineKGQA benchmarks.", "conclusion": "RTQA demonstrates a robust improvement over state-of-the-art TKGQA methods with enhanced reasoning capabilities and fault tolerance.", "key_contributions": ["Introduces a novel RTQA framework for TKGQA", "Utilizes recursive thinking for question decomposition", "Implements multi-path answer aggregation to improve fault tolerance."], "limitations": "", "keywords": ["temporal knowledge graph", "question answering", "reasoning", "LLMs", "fault tolerance"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2509.04013", "pdf": "https://arxiv.org/pdf/2509.04013.pdf", "abs": "https://arxiv.org/abs/2509.04013", "title": "On Robustness and Reliability of Benchmark-Based Evaluation of LLMs", "authors": ["Riccardo Lunardi", "Vincenzo Della Mea", "Stefano Mizzaro", "Kevin Roitero"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ECAI 2025", "summary": "Large Language Models (LLMs) effectiveness is usually evaluated by means of\nbenchmarks such as MMLU, ARC-C, or HellaSwag, where questions are presented in\ntheir original wording, thus in a fixed, standardized format. However,\nreal-world applications involve linguistic variability, requiring models to\nmaintain their effectiveness across diverse rewordings of the same question or\nquery. In this study, we systematically assess the robustness of LLMs to\nparaphrased benchmark questions and investigate whether benchmark-based\nevaluations provide a reliable measure of model capabilities. We systematically\ngenerate various paraphrases of all the questions across six different common\nbenchmarks, and measure the resulting variations in effectiveness of 34\nstate-of-the-art LLMs, of different size and effectiveness. Our findings reveal\nthat while LLM rankings remain relatively stable across paraphrased inputs,\nabsolute effectiveness scores change, and decline significantly. This suggests\nthat LLMs struggle with linguistic variability, raising concerns about their\ngeneralization abilities and evaluation methodologies. Furthermore, the\nobserved performance drop challenges the reliability of benchmark-based\nevaluations, indicating that high benchmark scores may not fully capture a\nmodel's robustness to real-world input variations. We discuss the implications\nof these findings for LLM evaluation methodologies, emphasizing the need for\nrobustness-aware benchmarks that better reflect practical deployment scenarios.", "AI": {"tldr": "This study investigates the robustness of large language models (LLMs) to paraphrased benchmark questions, revealing a decline in effectiveness while maintaining stable rankings.", "motivation": "To assess whether benchmark evaluations adequately reflect the real-world performance of LLMs across diverse question variations.", "method": "A systematic generation of paraphrased questions from six standard benchmarks to test the effectiveness of 34 different LLMs.", "result": "LLM rankings remain stable across paraphrased questions, but their absolute effectiveness scores significantly decline, indicating challenges with linguistic variability.", "conclusion": "The study highlights the limitations of benchmark-based evaluations in capturing LLM robustness, suggesting the need for more effective and robust benchmarks.", "key_contributions": ["Assessment of LLM robustness to question paraphrasing.", "Insights into the stability of LLM rankings despite performance drops.", "Recommendations for developing robustness-aware evaluation methodologies."], "limitations": "Focuses only on specific benchmarks and LLM sizes; the study does not explore the reasons for performance variability in depth.", "keywords": ["Large Language Models", "benchmarks", "robustness", "paraphrasing", "evaluation methodologies"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.04032", "pdf": "https://arxiv.org/pdf/2509.04032.pdf", "abs": "https://arxiv.org/abs/2509.04032", "title": "What if I ask in \\textit{alia lingua}? Measuring Functional Similarity Across Languages", "authors": ["Debangan Mishra", "Arihant Rastogi", "Agyeya Negi", "Shashwat Goel", "Ponnurangam Kumaraguru"], "categories": ["cs.CL", "cs.LG"], "comment": "Preprint, 11 Pages", "summary": "How similar are model outputs across languages? In this work, we study this\nquestion using a recently proposed model similarity metric $\\kappa_p$ applied\nto 20 languages and 47 subjects in GlobalMMLU. Our analysis reveals that a\nmodel's responses become increasingly consistent across languages as its size\nand capability grow. Interestingly, models exhibit greater cross-lingual\nconsistency within themselves than agreement with other models prompted in the\nsame language. These results highlight not only the value of $\\kappa_p$ as a\npractical tool for evaluating multilingual reliability, but also its potential\nto guide the development of more consistent multilingual systems.", "AI": {"tldr": "This work investigates model output consistency across 20 languages and 47 subjects, revealing increased consistency with larger and more capable models.", "motivation": "To understand how similarity in model outputs varies by language and evaluate multilingual reliability.", "method": "The study utilizes a metric called $\u000bA_p$ to analyze model outputs across various languages and subjects.", "result": "Findings indicate that larger models demonstrate increased consistency in their output across languages as well as increased internal consistency compared to outputs from different models.", "conclusion": "The results support the use of $\u000bA_p$ for evaluating multilingual model reliability and suggest a pathway to developing more consistent multilingual systems.", "key_contributions": ["Introduction of the $\u000bA_p$ metric for measuring model output similarity across languages.", "Empirical analysis showing the correlation between model size and cross-lingual consistency.", "Insights into models' internal consistency versus inter-model agreement."], "limitations": "The analysis is confined to 20 languages and 47 subjects; broader applicability beyond this scope is not established.", "keywords": ["multilingual models", "model consistency", "NLU evaluation", "cross-lingual output", "model reliability"], "importance_score": 7, "read_time_minutes": 11}}
{"id": "2509.04046", "pdf": "https://arxiv.org/pdf/2509.04046.pdf", "abs": "https://arxiv.org/abs/2509.04046", "title": "A RoBERTa-Based Functional Syntax Annotation Model for Chinese Texts", "authors": ["Han Xiaohui", "Zhang Yunlong", "Guo Yuxi"], "categories": ["cs.CL", "I.2.7"], "comment": "The paper includes 10 pages, 6 tables, and 4 figures. This project is\n  completed with the assistance of National Center for Language Technology and\n  Digital Economy Research (No. GJLX20250002), and is funded by Heilongjiang\n  Language Research Committee Project Construction of an Adaptive Intelligent\n  Chinese Learning Platform for International Students in China (No. G2025Y003)", "summary": "Systemic Functional Grammar and its branch, Cardiff Grammar, have been widely\napplied to discourse analysis, semantic function research, and other tasks\nacross various languages and texts. However, an automatic annotation system\nbased on this theory for Chinese texts has not yet been developed, which\nsignificantly constrains the application and promotion of relevant theories. To\nfill this gap, this research introduces a functional syntax annotation model\nfor Chinese based on RoBERTa (Robustly Optimized BERT Pretraining Approach).\nThe study randomly selected 4,100 sentences from the People's Daily 2014 corpus\nand annotated them according to functional syntax theory to establish a dataset\nfor training. The study then fine-tuned the RoBERTa-Chinese wwm-ext model based\non the dataset to implement the named entity recognition task, achieving an F1\nscore of 0.852 on the test set that significantly outperforms other comparative\nmodels. The model demonstrated excellent performance in identifying core\nsyntactic elements such as Subject (S), Main Verb (M), and Complement (C).\nNevertheless, there remains room for improvement in recognizing entities with\nimbalanced label samples. As the first integration of functional syntax with\nattention-based NLP models, this research provides a new method for automated\nChinese functional syntax analysis and lays a solid foundation for subsequent\nstudies.", "AI": {"tldr": "This research presents an automatic functional syntax annotation model for Chinese texts using RoBERTa, achieving strong results in named entity recognition.", "motivation": "To develop an automatic annotation system for Chinese texts based on Systemic Functional Grammar, which has not been previously established.", "method": "The study selected 4,100 sentences from the People's Daily 2014 corpus, annotated them according to functional syntax theory, and fine-tuned the RoBERTa-Chinese model for the named entity recognition task.", "result": "The model achieved an F1 score of 0.852 on the test set, significantly outperforming other models, especially in identifying core syntactic elements.", "conclusion": "This research innovatively integrates functional syntax with attention-based NLP models, providing a new method for automated Chinese functional syntax analysis.", "key_contributions": ["Introduction of a functional syntax annotation model for Chinese using RoBERTa", "Creation of a dataset for functional syntax training based on Chinese texts", "Demonstration of improved performance in named entity recognition tasks."], "limitations": "There is still room for improvement in recognizing entities with imbalanced label samples.", "keywords": ["Systemic Functional Grammar", "RoBERTa", "Chinese texts", "named entity recognition", "functional syntax"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.04059", "pdf": "https://arxiv.org/pdf/2509.04059.pdf", "abs": "https://arxiv.org/abs/2509.04059", "title": "Synthesizing Sheet Music Problems for Evaluation and Reinforcement Learning", "authors": ["Zhilin Wang", "Zhe Yang", "Yun Luo", "Yafu Li", "Haoran Zhang", "Runzhe Zhan", "Derek F. Wong", "Jizhe Zhou", "Yu Cheng"], "categories": ["cs.CL"], "comment": "11 pages", "summary": "Enhancing the ability of Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) to interpret sheet music is a crucial step toward\nbuilding AI musicians. However, current research lacks both evaluation\nbenchmarks and training data for sheet music reasoning. To address this, we\npropose the idea of synthesizing sheet music problems grounded in music theory,\nwhich can serve both as evaluation benchmarks and as training data for\nreinforcement learning with verifiable rewards (RLVR). We introduce a data\nsynthesis framework that generates verifiable sheet music questions in both\ntextual and visual modalities, leading to the Synthetic Sheet Music Reasoning\nBenchmark (SSMR-Bench) and a complementary training set. Evaluation results on\nSSMR-Bench show the importance of models' reasoning abilities in interpreting\nsheet music. At the same time, the poor performance of Gemini 2.5-Pro\nhighlights the challenges that MLLMs still face in interpreting sheet music in\na visual format. By leveraging synthetic data for RLVR, Qwen3-8B-Base and\nQwen2.5-VL-Instruct achieve improvements on the SSMR-Bench. Besides, the\ntrained Qwen3-8B-Base surpasses GPT-4 in overall performance on\nMusicTheoryBench and achieves reasoning performance comparable to GPT-4 with\nthe strategies of Role play and Chain-of-Thought. Notably, its performance on\nmath problems also improves relative to the original Qwen3-8B-Base.\nFurthermore, our results show that the enhanced reasoning ability can also\nfacilitate music composition. In conclusion, we are the first to propose the\nidea of synthesizing sheet music problems based on music theory rules, and\ndemonstrate its effectiveness not only in advancing model reasoning for sheet\nmusic understanding but also in unlocking new possibilities for AI-assisted\nmusic creation.", "AI": {"tldr": "This paper introduces a framework for synthesizing sheet music problems to enhance reasoning in Large and Multimodal Language Models, resulting in better performance in music understanding and composition.", "motivation": "The lack of evaluation benchmarks and training data for sheet music reasoning limits the ability of LLMs and MLLMs in music interpretation and composition.", "method": "A data synthesis framework is proposed to generate verifiable sheet music questions in both textual and visual modalities, leading to the creation of the Synthetic Sheet Music Reasoning Benchmark (SSMR-Bench) and a training set for reinforcement learning with verifiable rewards (RLVR).", "result": "Evaluation results show improved reasoning abilities in LLMs with synthesized data, with Qwen3-8B-Base surpassing GPT-4 in performances and aiding in music composition.", "conclusion": "The study pioneers the synthesis of sheet music problems based on music theory, advancing model reasoning and facilitating AI-assisted music creation.", "key_contributions": ["Development of the SSMR-Bench and training set for RLVR.", "Demonstration of enhanced reasoning in LLMs for music interpretation.", "Introduction of music theory-based question synthesis for AI-driven music composition."], "limitations": "Limited evaluation on real-world music interpretation challenges and reliance on synthetic data.", "keywords": ["Sheet Music Reasoning", "Large Language Models", "Music Theory", "Reinforcement Learning", "AI Music Composition"], "importance_score": 8, "read_time_minutes": 11}}
{"id": "2509.04066", "pdf": "https://arxiv.org/pdf/2509.04066.pdf", "abs": "https://arxiv.org/abs/2509.04066", "title": "Arabic Chatbot Technologies in Education: An Overview", "authors": ["Hicham Bourhil", "Yacine El Younoussi"], "categories": ["cs.CL"], "comment": "Published as a book chapter in: Transformaci\\'on Digital en la\n  Educaci\\'on: Innovaciones y Desaf\\'ios desde los Campus Virtuales (UA\n  Journals, 2024), pp. 11-14", "summary": "The recent advancements in Artificial Intelligence (AI) in general, and in\nNatural Language Processing (NLP) in particular, and some of its applications\nsuch as chatbots, have led to their implementation in different domains like\neducation, healthcare, tourism, and customer service. Since the COVID-19\npandemic, there has been an increasing interest in these digital technologies\nto allow and enhance remote access. In education, e-learning systems have been\nmassively adopted worldwide. The emergence of Large Language Models (LLM) such\nas BERT (Bidirectional Encoder Representations from Transformers) and GPT\n(Generative Pre-trained Transformers) made chatbots even more popular. In this\nstudy, we present a survey on existing Arabic chatbots in education and their\ndifferent characteristics such as the adopted approaches, language variety, and\nmetrics used to measure their performance. We were able to identified some\nresearch gaps when we discovered that, despite the success of chatbots in other\nlanguages such as English, only a few educational Arabic chatbots used modern\ntechniques. Finally, we discuss future directions of research in this field.", "AI": {"tldr": "This study surveys existing Arabic chatbots in education, highlighting their characteristics and identifying research gaps.", "motivation": "To explore the implementation of Arabic chatbots in education, especially post-COVID-19, amidst the rise of AI and NLP technologies.", "method": "A survey of existing educational Arabic chatbots, analyzing their approaches, language variety, and performance metrics.", "result": "The study identifies that, compared to English chatbots, there are significantly fewer educational chatbots in Arabic employing modern techniques.", "conclusion": "Future research should focus on bridging the identified gaps to enhance the effectiveness of Arabic chatbots in education.", "key_contributions": ["Survey of educational Arabic chatbots", "Identification of research gaps", "Discussion on future research directions"], "limitations": "", "keywords": ["Arabic chatbots", "education", "Natural Language Processing", "Large Language Models", "Digital technologies"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.04077", "pdf": "https://arxiv.org/pdf/2509.04077.pdf", "abs": "https://arxiv.org/abs/2509.04077", "title": "Improving Narrative Classification and Explanation via Fine Tuned Language Models", "authors": ["Rishit Tyagi", "Rahul Bouri", "Mohit Gupta"], "categories": ["cs.CL"], "comment": null, "summary": "Understanding covert narratives and implicit messaging is essential for\nanalyzing bias and sentiment. Traditional NLP methods struggle with detecting\nsubtle phrasing and hidden agendas. This study tackles two key challenges: (1)\nmulti-label classification of narratives and sub-narratives in news articles,\nand (2) generating concise, evidence-based explanations for dominant\nnarratives. We fine-tune a BERT model with a recall-oriented approach for\ncomprehensive narrative detection, refining predictions using a GPT-4o pipeline\nfor consistency. For narrative explanation, we propose a ReACT (Reasoning +\nActing) framework with semantic retrieval-based few-shot prompting, ensuring\ngrounded and relevant justifications. To enhance factual accuracy and reduce\nhallucinations, we incorporate a structured taxonomy table as an auxiliary\nknowledge base. Our results show that integrating auxiliary knowledge in\nprompts improves classification accuracy and justification reliability, with\napplications in media analysis, education, and intelligence gathering.", "AI": {"tldr": "This study addresses the challenges of detecting covert narratives in news articles and generating explanations for them by fine-tuning a BERT model and proposing a ReACT framework.", "motivation": "Understanding covert narratives and implicit messaging is essential for analyzing bias and sentiment.", "method": "We fine-tune a BERT model with a recall-oriented approach for comprehensive narrative detection and refine predictions using a GPT-4o pipeline. For narrative explanation, we propose a ReACT framework with semantic retrieval-based few-shot prompting, incorporating a structured taxonomy as an auxiliary knowledge base.", "result": "Integrating auxiliary knowledge in prompts improves classification accuracy and justification reliability, demonstrating significant advancements in narrative detection and explanation.", "conclusion": "The study shows promising applications in media analysis, education, and intelligence gathering by enhancing factual accuracy and reducing hallucinations in narrative explanations.", "key_contributions": ["Multi-label classification of narratives and sub-narratives", "Development of a ReACT framework for evidence-based explanations", "Integration of auxiliary knowledge to enhance model performance"], "limitations": "The effectiveness may be dependent on the quality of the auxiliary knowledge and the complexity of narratives.", "keywords": ["BERT", "narrative detection", "ReACT framework", "few-shot prompting", "semantic retrieval"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.04104", "pdf": "https://arxiv.org/pdf/2509.04104.pdf", "abs": "https://arxiv.org/abs/2509.04104", "title": "Towards Stable and Personalised Profiles for Lexical Alignment in Spoken Human-Agent Dialogue", "authors": ["Keara Schaaij", "Roel Boumans", "Tibor Bosse", "Iris Hendrickx"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted for TSD 2025", "summary": "Lexical alignment, where speakers start to use similar words across\nconversation, is known to contribute to successful communication. However, its\nimplementation in conversational agents remains underexplored, particularly\nconsidering the recent advancements in large language models (LLMs). As a first\nstep towards enabling lexical alignment in human-agent dialogue, this study\ndraws on strategies for personalising conversational agents and investigates\nthe construction of stable, personalised lexical profiles as a basis for\nlexical alignment. Specifically, we varied the amounts of transcribed spoken\ndata used for construction as well as the number of items included in the\nprofiles per part-of-speech (POS) category and evaluated profile performance\nacross time using recall, coverage, and cosine similarity metrics. It was shown\nthat smaller and more compact profiles, created after 10 min of transcribed\nspeech containing 5 items for adjectives, 5 items for conjunctions, and 10\nitems for adverbs, nouns, pronouns, and verbs each, offered the best balance in\nboth performance and data efficiency. In conclusion, this study offers\npractical insights into constructing stable, personalised lexical profiles,\ntaking into account minimal data requirements, serving as a foundational step\ntoward lexical alignment strategies in conversational agents.", "AI": {"tldr": "This study explores lexical alignment in conversational agents by constructing personalised lexical profiles to enhance communication efficiency.", "motivation": "To address the underexplored area of lexical alignment in conversational agents, especially with advancements in large language models.", "method": "The study varied the transcribed spoken data used and the number of items in lexical profiles for different part-of-speech categories, evaluating performance using recall, coverage, and cosine similarity.", "result": "Compact profiles created from 10 minutes of transcribed speech with specific item counts achieved the best performance balance.", "conclusion": "The research provides foundational insights into creating stable, personalised lexical profiles for conversational agents with minimal data.", "key_contributions": ["Introduction of personalised lexical profiles in conversational agents", "Identification of optimal data requirements for profile construction", "Application of performance metrics to evaluate profile effectiveness"], "limitations": "", "keywords": ["lexical alignment", "conversational agents", "large language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.04111", "pdf": "https://arxiv.org/pdf/2509.04111.pdf", "abs": "https://arxiv.org/abs/2509.04111", "title": "MultiWikiQA: A Reading Comprehension Benchmark in 300+ Languages", "authors": ["Dan Saattrup Smart"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce a new reading comprehension dataset, dubbed MultiWikiQA, which\ncovers 306 languages. The context data comes from Wikipedia articles, with\nquestions generated by an LLM and the answers appearing verbatim in the\nWikipedia articles. We conduct a crowdsourced human evaluation of the fluency\nof the generated questions across 30 of the languages, providing evidence that\nthe questions are of good quality. We evaluate 6 different language models,\nboth decoder and encoder models of varying sizes, showing that the benchmark is\nsufficiently difficult and that there is a large performance discrepancy\namongst the languages. The dataset and survey evaluations are freely available.", "AI": {"tldr": "A new reading comprehension dataset, MultiWikiQA, created using Wikipedia articles across 306 languages, with crowdsourced evaluations on question quality.", "motivation": "To provide a comprehensive reading comprehension benchmark that spans multiple languages and evaluates the performance of different language models.", "method": "The study presents a reading comprehension dataset sourced from Wikipedia, involving questions generated by a language model and answers found in the articles, alongside a human evaluation of question fluency.", "result": "The evaluation shows that the questions are of high quality across 30 languages, and the performance of language models varies significantly depending on the language used.", "conclusion": "The MultiWikiQA dataset serves as a challenging benchmark for reading comprehension across multiple languages and is freely accessible for future research.", "key_contributions": ["Introduction of MultiWikiQA, a multilingual reading comprehension dataset.", "Crowdsourced evaluation of question quality in multiple languages.", "Comparison of different language model performances across 306 languages."], "limitations": "Performance discrepancies indicate variability in language model effectiveness based on the language, suggesting not all languages are represented equally in terms of comprehension complexity.", "keywords": ["MultiWikiQA", "reading comprehension", "language models", "multilingual", "Wikipedia"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2509.04182", "pdf": "https://arxiv.org/pdf/2509.04182.pdf", "abs": "https://arxiv.org/abs/2509.04182", "title": "Joint Modeling of Entities and Discourse Relations for Coherence Assessment", "authors": ["Wei Liu", "Michael Strube"], "categories": ["cs.CL"], "comment": "EMNLP 2025", "summary": "In linguistics, coherence can be achieved by different means, such as by\nmaintaining reference to the same set of entities across sentences and by\nestablishing discourse relations between them. However, most existing work on\ncoherence modeling focuses exclusively on either entity features or discourse\nrelation features, with little attention given to combining the two. In this\nstudy, we explore two methods for jointly modeling entities and discourse\nrelations for coherence assessment. Experiments on three benchmark datasets\nshow that integrating both types of features significantly enhances the\nperformance of coherence models, highlighting the benefits of modeling both\nsimultaneously for coherence evaluation.", "AI": {"tldr": "This study explores joint modeling of entities and discourse relations for coherence assessment in linguistics, finding that integrating both significantly improves model performance.", "motivation": "Most existing coherence modeling focuses on either entity features or discourse relation features, neglecting the benefits of combining both.", "method": "Two methods for jointly modeling entities and discourse relations were explored, with experiments conducted on three benchmark datasets to assess coherence.", "result": "Integrating both types of features significantly enhances the performance of coherence models, demonstrating the advantages of simultaneous modeling.", "conclusion": "The study shows that a combined approach to modeling entities and discourse relations leads to better coherence assessment results.", "key_contributions": ["Joint modeling of entities and discourse relations for coherence assessment", "Demonstration of significant performance enhancement in coherence models", "Exploration of combined feature integration on benchmark datasets"], "limitations": "", "keywords": ["coherence", "entities", "discourse relations", "modeling", "linguistics"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.04183", "pdf": "https://arxiv.org/pdf/2509.04183.pdf", "abs": "https://arxiv.org/abs/2509.04183", "title": "MAGneT: Coordinated Multi-Agent Generation of Synthetic Multi-Turn Mental Health Counseling Sessions", "authors": ["Aishik Mandal", "Tanmoy Chakraborty", "Iryna Gurevych"], "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 29 figures", "summary": "The growing demand for scalable psychological counseling highlights the need\nfor fine-tuning open-source Large Language Models (LLMs) with high-quality,\nprivacy-compliant data, yet such data remains scarce. Here we introduce MAGneT,\na novel multi-agent framework for synthetic psychological counseling session\ngeneration that decomposes counselor response generation into coordinated\nsub-tasks handled by specialized LLM agents, each modeling a key psychological\ntechnique. Unlike prior single-agent approaches, MAGneT better captures the\nstructure and nuance of real counseling. In addition, we address\ninconsistencies in prior evaluation protocols by proposing a unified evaluation\nframework integrating diverse automatic and expert metrics. Furthermore, we\nexpand the expert evaluations from four aspects of counseling in previous works\nto nine aspects, enabling a more thorough and robust assessment of data\nquality. Empirical results show that MAGneT significantly outperforms existing\nmethods in quality, diversity, and therapeutic alignment of the generated\ncounseling sessions, improving general counseling skills by 3.2% and\nCBT-specific skills by 4.3% on average on cognitive therapy rating scale\n(CTRS). Crucially, experts prefer MAGneT-generated sessions in 77.2% of cases\non average across all aspects. Moreover, fine-tuning an open-source model on\nMAGneT-generated sessions shows better performance, with improvements of 6.3%\non general counseling skills and 7.3% on CBT-specific skills on average on CTRS\nover those fine-tuned with sessions generated by baseline methods. We also make\nour code and data public.", "AI": {"tldr": "MAGneT is a multi-agent framework for generating synthetic psychological counseling sessions using specialized LLMs, outperforming traditional methods in quality and diversity of sessions while offering a novel evaluation protocol.", "motivation": "To meet the growing demand for scalable psychological counseling and improve the quality of synthetic counseling sessions generated by LLMs.", "method": "MAGneT decomposes the response generation process into sub-tasks managed by specialized LLM agents, along with a unified evaluation framework that incorporates diverse metrics for assessing counseling data quality.", "result": "MAGneT significantly improves the quality and diversity of generated counseling sessions, achieving better performance in both general and CBT-specific counseling skills as validated by expert evaluations.", "conclusion": "MAGneT demonstrates superior performance compared to existing methods, with a strong preference from experts for its generated sessions, and provides publicly available code and data for further research.", "key_contributions": ["Introduction of a multi-agent framework for counseling generation", "Enhanced evaluation protocols for assessing session quality", "Empirical validation showing significant performance improvements over baseline methods"], "limitations": "", "keywords": ["large language models", "psychological counseling", "multi-agent systems", "evaluation metrics", "synthetic data"], "importance_score": 10, "read_time_minutes": 25}}
{"id": "2509.04202", "pdf": "https://arxiv.org/pdf/2509.04202.pdf", "abs": "https://arxiv.org/abs/2509.04202", "title": "Explicit and Implicit Data Augmentation for Social Event Detection", "authors": ["Congbo Ma", "Yuxia Wang", "Jia Wu", "Jian Yang", "Jing Du", "Zitai Qiu", "Qing Li", "Hu Wang", "Preslav Nakov"], "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Social event detection involves identifying and categorizing important events\nfrom social media, which relies on labeled data, but annotation is costly and\nlabor-intensive. To address this problem, we propose Augmentation framework for\nSocial Event Detection (SED-Aug), a plug-and-play dual augmentation framework,\nwhich combines explicit text-based and implicit feature-space augmentation to\nenhance data diversity and model robustness. The explicit augmentation utilizes\nlarge language models to enhance textual information through five diverse\ngeneration strategies. For implicit augmentation, we design five novel\nperturbation techniques that operate in the feature space on structural fused\nembeddings. These perturbations are crafted to keep the semantic and relational\nproperties of the embeddings and make them more diverse. Specifically, SED-Aug\noutperforms the best baseline model by approximately 17.67% on the Twitter2012\ndataset and by about 15.57% on the Twitter2018 dataset in terms of the average\nF1 score. The code is available at GitHub: https://github.com/congboma/SED-Aug.", "AI": {"tldr": "Develops SED-Aug, an augmentation framework for detecting social events in social media, improving model performance without extensive labeled data.", "motivation": "To improve social event detection (SED) due to the high cost and labor-intensiveness of data annotation.", "method": "Introduces a dual augmentation framework that includes explicit text-based augmentation using large language models and implicit feature-space augmentation with novel perturbation techniques.", "result": "SED-Aug outperforms existing models by 17.67% on Twitter2012 and 15.57% on Twitter2018 datasets in average F1 score.", "conclusion": "The proposed framework enhances data diversity and model robustness for social event detection, minimizing the need for extensive labeled data.", "key_contributions": ["Proposed a plug-and-play dual augmentation framework for SED.", "Implemented explicit augmentation strategies using large language models.", "Developed novel perturbation techniques for feature space augmentation."], "limitations": "", "keywords": ["social event detection", "data augmentation", "large language models", "feature perturbation", "machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.04292", "pdf": "https://arxiv.org/pdf/2509.04292.pdf", "abs": "https://arxiv.org/abs/2509.04292", "title": "Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow Real Instructions?", "authors": ["Qinyan Zhang", "Xinping Lei", "Ruijie Miao", "Yu Fu", "Haojie Fan", "Le Chang", "Jiafan Hou", "Dingling Zhang", "Zhongfei Hou", "Ziqiang Yang", "Changxin Pu", "Fei Hu", "Jingkai Liu", "Mengyun Liu", "Yang Liu", "Xiang Gao", "Jiaheng Liu", "Tong Yang", "Zaiyuan Wang", "Ge Zhang", "Wenhao Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) achieve strong performance on diverse tasks but\noften exhibit cognitive inertia, struggling to follow instructions that\nconflict with the standardized patterns learned during supervised fine-tuning\n(SFT). To evaluate this limitation, we propose Inverse IFEval, a benchmark that\nmeasures models Counter-intuitive Abilitytheir capacity to override\ntraining-induced biases and comply with adversarial instructions. Inverse\nIFEval introduces eight types of such challenges, including Question\nCorrection, Intentional Textual Flaws, Code without Comments, and\nCounterfactual Answering. Using a human-in-the-loop pipeline, we construct a\ndataset of 1012 high-quality Chinese and English questions across 23 domains,\nevaluated under an optimized LLM-as-a-Judge framework. Experiments on existing\nleading LLMs demonstrate the necessity of our proposed Inverse IFEval\nbenchmark. Our findings emphasize that future alignment efforts should not only\npursue fluency and factual correctness but also account for adaptability under\nunconventional contexts. We hope that Inverse IFEval serves as both a\ndiagnostic tool and a foundation for developing methods that mitigate cognitive\ninertia, reduce overfitting to narrow patterns, and ultimately enhance the\ninstruction-following reliability of LLMs in diverse and unpredictable\nreal-world scenarios.", "AI": {"tldr": "The paper proposes Inverse IFEval, a benchmark to evaluate Large Language Models (LLMs) on their ability to follow adversarial instructions that conflict with learned patterns during fine-tuning.", "motivation": "To address the cognitive inertia exhibited by LLMs when following conflicting instructions, highlighting the need for better adaptability in instruction-following tasks.", "method": "Inverse IFEval consists of 1012 high-quality questions across 23 domains, structured around eight challenge types designed to test the adaptability of LLMs.", "result": "Experiments with leading LLMs confirm the importance of measuring adaptability in addition to fluency and correctness, indicating that LLMs struggle with instruction compliance in unconventional contexts.", "conclusion": "Inverse IFEval can serve as a diagnostic tool for identifying biases in LLMs and guiding future research on improving their instruction-following capabilities.", "key_contributions": ["Introduction of Inverse IFEval benchmark for evaluating LLMs' adaptability to adversarial instructions.", "Construction of a dataset with diverse questions to test cognitive biases in LLMs.", "Emphasis on the need for alignment efforts to consider adaptability and flexibility, moving beyond mere fluency and correctness."], "limitations": "", "keywords": ["Large Language Models", "Inverse IFEval", "Cognitive Inertia", "Instruction Following", "Benchmarking"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2509.04304", "pdf": "https://arxiv.org/pdf/2509.04304.pdf", "abs": "https://arxiv.org/abs/2509.04304", "title": "Facts Fade Fast: Evaluating Memorization of Outdated Medical Knowledge in Large Language Models", "authors": ["Juraj Vladika", "Mahdi Dhaini", "Florian Matthes"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Findings of EMNLP 2025", "summary": "The growing capabilities of Large Language Models (LLMs) show significant\npotential to enhance healthcare by assisting medical researchers and\nphysicians. However, their reliance on static training data is a major risk\nwhen medical recommendations evolve with new research and developments. When\nLLMs memorize outdated medical knowledge, they can provide harmful advice or\nfail at clinical reasoning tasks. To investigate this problem, we introduce two\nnovel question-answering (QA) datasets derived from systematic reviews:\nMedRevQA (16,501 QA pairs covering general biomedical knowledge) and\nMedChangeQA (a subset of 512 QA pairs where medical consensus has changed over\ntime). Our evaluation of eight prominent LLMs on the datasets reveals\nconsistent reliance on outdated knowledge across all models. We additionally\nanalyze the influence of obsolete pre-training data and training strategies to\nexplain this phenomenon and propose future directions for mitigation, laying\nthe groundwork for developing more current and reliable medical AI systems.", "AI": {"tldr": "This paper investigates the limitations of Large Language Models (LLMs) in delivering up-to-date medical advice, introducing two QA datasets to address this issue.", "motivation": "To enhance healthcare capabilities using LLMs while acknowledging their limitations in providing up-to-date medical recommendations due to outdated training data.", "method": "Introduction of two QA datasets, MedRevQA and MedChangeQA, and evaluation of eight prominent LLMs on these datasets to assess reliance on outdated knowledge.", "result": "All evaluated LLMs demonstrated a consistent reliance on outdated medical knowledge, leading to potential harmful medical advice.", "conclusion": "Future directions for improving the recency and reliability of medical AI systems are proposed, addressing the limitations identified in LLMs.", "key_contributions": ["Introduction of MedRevQA and MedChangeQA datasets", "Evaluation of LLMs revealing outdated knowledge reliance", "Proposed strategies for mitigating outdated medical advice in AI systems"], "limitations": "Focused primarily on the evaluation of existing LLMs without proposing direct solutions to data acquisition.", "keywords": ["Large Language Models", "Healthcare", "Question-Answering Datasets", "Medical AI", "Clinical Reasoning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.04357", "pdf": "https://arxiv.org/pdf/2509.04357.pdf", "abs": "https://arxiv.org/abs/2509.04357", "title": "PARCO: Phoneme-Augmented Robust Contextual ASR via Contrastive Entity Disambiguation", "authors": ["Jiajun He", "Naoki Sawada", "Koichi Miyazaki", "Tomoki Toda"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD"], "comment": "Accepted by ASRU 2025", "summary": "Automatic speech recognition (ASR) systems struggle with domain-specific\nnamed entities, especially homophones. Contextual ASR improves recognition but\noften fails to capture fine-grained phoneme variations due to limited entity\ndiversity. Moreover, prior methods treat entities as independent tokens,\nleading to incomplete multi-token biasing. To address these issues, we propose\nPhoneme-Augmented Robust Contextual ASR via COntrastive entity disambiguation\n(PARCO), which integrates phoneme-aware encoding, contrastive entity\ndisambiguation, entity-level supervision, and hierarchical entity filtering.\nThese components enhance phonetic discrimination, ensure complete entity\nretrieval, and reduce false positives under uncertainty. Experiments show that\nPARCO achieves CER of 4.22% on Chinese AISHELL-1 and WER of 11.14% on English\nDATA2 under 1,000 distractors, significantly outperforming baselines. PARCO\nalso demonstrates robust gains on out-of-domain datasets like THCHS-30 and\nLibriSpeech.", "AI": {"tldr": "The paper presents PARCO, a novel approach to improve automatic speech recognition systems by integrating phoneme-aware encoding and entity disambiguation.", "motivation": "Automatic speech recognition systems face challenges in recognizing domain-specific entities and managing homophones, which can lead to recognition errors.", "method": "The proposed method, PARCO, combines phoneme-aware encoding, contrastive entity disambiguation, and hierarchical entity filtering to enhance recognition accuracy.", "result": "PARCO achieves a Character Error Rate (CER) of 4.22% on Chinese AISHELL-1 and a Word Error Rate (WER) of 11.14% on English DATA2, outperforming existing methods.", "conclusion": "The integration of phoneme-sensitive processing and entity-level supervision effectively reduces false positives and improves recognition accuracy in ASR systems.", "key_contributions": ["Introduction of phoneme-aware encoding in ASR systems.", "Utilization of contrastive entity disambiguation to improve entity recognition.", "Development of a hierarchical entity filtering method to enhance recognition performance."], "limitations": "", "keywords": ["Automatic Speech Recognition", "Entity Disambiguation", "Phoneme Aware Encoding"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.04373", "pdf": "https://arxiv.org/pdf/2509.04373.pdf", "abs": "https://arxiv.org/abs/2509.04373", "title": "Measuring Bias or Measuring the Task: Understanding the Brittle Nature of LLM Gender Biases", "authors": ["Bufan Gao", "Elisa Kreiss"], "categories": ["cs.CL"], "comment": null, "summary": "As LLMs are increasingly applied in socially impactful settings, concerns\nabout gender bias have prompted growing efforts both to measure and mitigate\nsuch bias. These efforts often rely on evaluation tasks that differ from\nnatural language distributions, as they typically involve carefully constructed\ntask prompts that overtly or covertly signal the presence of gender\nbias-related content. In this paper, we examine how signaling the evaluative\npurpose of a task impacts measured gender bias in LLMs. Concretely, we test\nmodels under prompt conditions that (1) make the testing context salient, and\n(2) make gender-focused content salient. We then assess prompt sensitivity\nacross four task formats with both token-probability and discrete-choice\nmetrics. We find that even minor prompt changes can substantially alter bias\noutcomes, sometimes reversing their direction entirely. Discrete-choice metrics\nfurther tend to amplify bias relative to probabilistic measures. These findings\ndo not only highlight the brittleness of LLM gender bias evaluations but open a\nnew puzzle for the NLP benchmarking and development community: To what extent\ncan well-controlled testing designs trigger LLM ``testing mode'' performance,\nand what does this mean for the ecological validity of future benchmarks.", "AI": {"tldr": "This paper examines the impact of prompting conditions on measured gender bias in LLMs, revealing that minor prompt changes can significantly affect bias outcomes.", "motivation": "To address concerns regarding gender bias in LLMs in socially impactful settings and to understand how different evaluative prompt designs affect bias measurements.", "method": "The study tests LLMs under varying prompt conditions—highlighting both the testing context and gender-related content—across four task formats using token-probability and discrete-choice metrics.", "result": "Minor changes in prompt conditions significantly altered the measured outcomes of gender bias, sometimes reversing the results, with discrete-choice metrics amplifying bias compared to probabilistic measures.", "conclusion": "The findings raise concerns about the stability of gender bias evaluations in LLMs and challenge the NLP community to assess the ecological validity of evaluation benchmarks.", "key_contributions": ["Demonstration of the sensitivity of bias measurements to prompt design.", "Revelation of the differences in bias amplification between discrete-choice and probabilistic measures.", "Highlighting the need for improved ecological validity in NLP benchmarking."], "limitations": "Focus primarily on the effects of prompt design without exploring other potential sources of bias.", "keywords": ["gender bias", "large language models", "prompt sensitivity", "evaluation metrics", "NLP benchmarking"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.04432", "pdf": "https://arxiv.org/pdf/2509.04432.pdf", "abs": "https://arxiv.org/abs/2509.04432", "title": "Can Language Models Handle a Non-Gregorian Calendar?", "authors": ["Mutsumi Sasaki", "Go Kamoda", "Ryosuke Takahashi", "Kosuke Sato", "Kentaro Inui", "Keisuke Sakaguchi", "Benjamin Heinzerling"], "categories": ["cs.CL"], "comment": null, "summary": "Temporal reasoning and knowledge are essential capabilities for language\nmodels (LMs). While much prior work has analyzed and improved temporal\nreasoning in LMs, most studies have focused solely on the Gregorian calendar.\nHowever, many non-Gregorian systems, such as the Japanese, Hijri, and Hebrew\ncalendars, are in active use and reflect culturally grounded conceptions of\ntime. If and how well current LMs can accurately handle such non-Gregorian\ncalendars has not been evaluated so far. Here, we present a systematic\nevaluation of how well open-source LMs handle one such non-Gregorian system:\nthe Japanese calendar. For our evaluation, we create datasets for four tasks\nthat require both temporal knowledge and temporal reasoning. Evaluating a range\nof English-centric and Japanese-centric LMs, we find that some models can\nperform calendar conversions, but even Japanese-centric models struggle with\nJapanese-calendar arithmetic and with maintaining consistency across calendars.\nOur results highlight the importance of developing LMs that are better equipped\nfor culture-specific calendar understanding.", "AI": {"tldr": "This paper evaluates how well language models handle the Japanese calendar, revealing that models struggle with calendar arithmetic and consistency, even those designed for Japanese.", "motivation": "The paper seeks to address the gap in understanding how language models deal with non-Gregorian calendars, particularly the Japanese calendar, which represents culturally specific time concepts.", "method": "The authors created datasets for four tasks requiring temporal knowledge and reasoning to assess English-centric and Japanese-centric language models' performance on the Japanese calendar.", "result": "The evaluation shows that while some models can perform basic calendar conversions, they struggle with Japanese calendar arithmetic and maintaining consistency across different calendar systems.", "conclusion": "Improving language models for culture-specific calendar understanding is essential, as current models notably underperform in this area.", "key_contributions": ["Systematic evaluation of language models on the Japanese calendar.", "Creation of datasets for testing temporal reasoning and knowledge.", "Highlighting the challenges faced by both English-centric and Japanese-centric models."], "limitations": "The study focuses only on the Japanese calendar and does not evaluate other non-Gregorian systems.", "keywords": ["language models", "temporal reasoning", "Japanese calendar", "non-Gregorian", "cultural understanding"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2109.02325", "pdf": "https://arxiv.org/pdf/2109.02325.pdf", "abs": "https://arxiv.org/abs/2109.02325", "title": "MyProfessors: Mining Turkish Student Reviews", "authors": ["Ibrahim Faruk Ceylan", "Necmettin Bera Calik"], "categories": ["cs.CL"], "comment": "The paper is withdrawn due to the scraping errors in the dataset\n  collection process and affected results", "summary": "We introduce Hocalarim (MyProfessors), the largest student review dataset\navailable for the Turkish language. It consists of over 5000 professor reviews\nleft online by students, with different aspects of education rated on a scale\nof 1 to 5 stars. We investigate the properties of the dataset and present its\nstatistics. We examine the impact of students' institution type on their\nratings and the correlation of students' bias to give positive or negative\nfeedback.", "AI": {"tldr": "Introduction of the Hocalarim dataset, the largest Turkish student review dataset, analyzing professor reviews.", "motivation": "To provide insights into the impact of institution type on professor ratings and examine student bias in feedback.", "method": "Statistical analysis of over 5000 professor reviews rated on a 1 to 5 scale.", "result": "Identification of trends in ratings according to institution type and correlation of bias in student feedback.", "conclusion": "The dataset was withdrawn due to scraping errors, affecting the reliability of the results.", "key_contributions": ["Largest dataset of Turkish student reviews", "Analysis of institution type impact on ratings", "Examination of student bias in feedback"], "limitations": "Data collection process had scraping errors leading to withdrawal of the paper.", "keywords": ["student reviews", "dataset", "Turkish language", "ratings", "bias"], "importance_score": 2, "read_time_minutes": 5}}
{"id": "2305.06166", "pdf": "https://arxiv.org/pdf/2305.06166.pdf", "abs": "https://arxiv.org/abs/2305.06166", "title": "Mitigating Bias in Text Classification via Prompt-Based Text Transformation", "authors": ["Charmaine Barker", "Dimitar Kazakov"], "categories": ["cs.CL"], "comment": "This version corrects an error in the model specification", "summary": "The presence of specific linguistic signals particular to a certain sub-group\ncan become highly salient to language models during training. In automated\ndecision-making settings, this may lead to biased outcomes when models rely on\ncues that correlate with protected characteristics. We investigate whether\nprompting ChatGPT to rewrite text using simplification, neutralisation,\nlocalisation, and formalisation can reduce demographic signals while preserving\nmeaning. Experimental results show a statistically significant drop in location\nclassification accuracy across multiple models after transformation, suggesting\nreduced reliance on group-specific language. At the same time, sentiment\nanalysis and rating prediction tasks confirm that the core meaning of the\nreviews remains greatly intact. These results suggest that prompt-based\nrewriting offers a practical and generalisable approach for mitigating bias in\ntext classification.", "AI": {"tldr": "This paper examines how prompt-based rewriting in ChatGPT can mitigate bias from demographic signals in text while preserving the original meaning.", "motivation": "The study aims to address potential biases in language models that arise from linguistic features associated with demographic characteristics, which can lead to unfair automated decision-making outcomes.", "method": "The authors tested different rewriting strategies including simplification, neutralisation, localisation, and formalisation to see their impact on bias reduction in language model outputs.", "result": "The experimentation showed a significant reduction in the accuracy of location classification across various models, indicating less reliance on demographic cues, particularly in terms of location, while maintaining the core meaning of the text.", "conclusion": "Prompt-based rewriting is a viable method for reducing bias in text classification tasks without losing the essential meanings of the texts.", "key_contributions": ["Demonstration of effective prompt-based rewriting techniques to mitigate model bias", "Evidence of maintaining semantic integrity in rewritten texts", "Statistical analysis showing reduced model reliance on demographic signals"], "limitations": "The study focuses on specific rewriting techniques and may not generalize to all language models or contexts.", "keywords": ["bias mitigation", "text classification", "language models", "prompt-based rewriting", "demographic signals"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2306.03774", "pdf": "https://arxiv.org/pdf/2306.03774.pdf", "abs": "https://arxiv.org/abs/2306.03774", "title": "Exploring Linguistic Features for Turkish Text Readability", "authors": ["Ahmet Yavuz Uluslu", "Gerold Schneider"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents the first comprehensive study on automatic readability\nassessment of Turkish texts. We combine state-of-the-art neural network models\nwith linguistic features at lexical, morphological, syntactic and discourse\nlevels to develop an advanced readability tool. We evaluate the effectiveness\nof traditional readability formulas compared to modern automated methods and\nidentify key linguistic features that determine the readability of Turkish\ntexts.", "AI": {"tldr": "This paper studies the automatic readability assessment of Turkish texts using neural networks and linguistic features.", "motivation": "To develop an advanced tool for assessing the readability of Turkish texts and to compare traditional readability formulas with modern automated methods.", "method": "The authors combine neural network models with linguistic features across various levels, including lexical, morphological, syntactic, and discourse.", "result": "The study evaluates the effectiveness of traditional readability formulas against automated methods and identifies key linguistic features that influence readability.", "conclusion": "The findings provide insights into Turkish text readability and highlight the strengths of modern methodologies over traditional ones.", "key_contributions": ["First comprehensive study on Turkish text readability", "Integration of neural networks with linguistic features", "Comparison between traditional and automated readability assessment methods."], "limitations": "", "keywords": ["readability assessment", "Turkish texts", "neural networks", "linguistic features", "automation"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2406.01359", "pdf": "https://arxiv.org/pdf/2406.01359.pdf", "abs": "https://arxiv.org/abs/2406.01359", "title": "R2C2-Coder: Enhancing and Benchmarking Real-world Repository-level Code Completion Abilities of Code Large Language Models", "authors": ["Ken Deng", "Jiaheng Liu", "He Zhu", "Congnan Liu", "Jingxin Li", "Jiakai Wang", "Peng Zhao", "Chenchen Zhang", "Yanan Wu", "Xueqiao Yin", "Yuanxing Zhang", "Zizheng Zhan", "Wenbo Su", "Bangyu Xiang", "Tiezheng Ge", "Bo Zheng"], "categories": ["cs.CL", "cs.SE"], "comment": null, "summary": "Code completion models have made significant progress in recent years.\nRecently, repository-level code completion has drawn more attention in modern\nsoftware development, and several baseline methods and benchmarks have been\nproposed. However, existing repository-level code completion methods often fall\nshort of fully using the extensive context of a project repository, such as the\nintricacies of relevant files and class hierarchies. Besides, the existing\nbenchmarks usually focus on limited code completion scenarios, which cannot\nreflect the repository-level code completion abilities well of existing\nmethods. To address these limitations, we propose the R2C2-Coder to enhance and\nbenchmark the real-world repository-level code completion abilities of code\nLarge Language Models, where the R2C2-Coder includes a code prompt construction\nmethod R2C2-Enhance and a well-designed benchmark R2C2-Bench. Specifically,\nfirst, in R2C2-Enhance, we first construct the candidate retrieval pool and\nthen assemble the completion prompt by retrieving from the retrieval pool for\neach completion cursor position. Second, based on R2C2 -Enhance, we can\nconstruct a more challenging and diverse R2C2-Bench with training, validation\nand test splits, where a context perturbation strategy is proposed to simulate\nthe real-world repository-level code completion well. Extensive results on\nmultiple benchmarks demonstrate the effectiveness of our R2C2-Coder.", "AI": {"tldr": "The paper introduces R2C2-Coder, a tool designed to improve and benchmark repository-level code completion for Large Language Models (LLMs) by enhancing prompt construction and establishing a comprehensive benchmark.", "motivation": "Existing repository-level code completion methods don't leverage the full project context effectively, and current benchmarks are limited in scope, necessitating a robust solution.", "method": "The R2C2-Coder combines R2C2-Enhance for improved candidate retrieval and prompt assembly, along with R2C2-Bench for diverse benchmarking that simulates real-world scenarios.", "result": "Extensive evaluations show that R2C2-Coder outperforms existing methods, demonstrating significant improvements in repository-level code completion.", "conclusion": "R2C2-Coder effectively enhances code completion capabilities in real-world repositories and sets a new standard for benchmarking these abilities.", "key_contributions": ["Introduction of R2C2-Enhance for better prompt construction", "Development of R2C2-Bench to evaluate repository-level completion", "Demonstration of improved performance in benchmark evaluations"], "limitations": "", "keywords": ["code completion", "Large Language Models", "benchmarking", "repository-level", "software development"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2411.12736", "pdf": "https://arxiv.org/pdf/2411.12736.pdf", "abs": "https://arxiv.org/abs/2411.12736", "title": "ACING: Actor-Critic for Instruction Learning in Black-Box LLMs", "authors": ["Salma Kharrat", "Fares Fourati", "Marco Canini"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SY", "eess.SY", "math.OC"], "comment": "Accepted at EMNLP 2025", "summary": "The effectiveness of Large Language Models (LLMs) in solving tasks depends\nsignificantly on the quality of their instructions, which often require\nsubstantial human effort to craft. This underscores the need for automated\ninstruction optimization. However, optimizing instructions is particularly\nchallenging when working with black-box LLMs, where model parameters and\ngradients are inaccessible. We introduce ACING, an actor-critic reinforcement\nlearning framework that formulates instruction optimization as a stateless,\ncontinuous-action problem, enabling exploration of infinite instruction spaces\nusing only black-box feedback. ACING automatically discovers prompts that\noutperform human-written prompts in 76% of instruction-induction tasks, with\ngains of up to 33 points and a 10-point median improvement over the best\nautomatic baseline in 33 tasks spanning instruction-induction, summarization,\nand chain-of-thought reasoning. Extensive ablations highlight its robustness\nand efficiency. An implementation of ACING is available at\nhttps://github.com/salmakh1/ACING.", "AI": {"tldr": "Introducing ACING, a framework for optimizing instructions for Large Language Models (LLMs) using reinforcement learning to enhance performance.", "motivation": "The effectiveness of LLMs heavily relies on the quality of human-crafted instructions, necessitating automated optimization methods, especially for black-box models.", "method": "We present ACING, an actor-critic reinforcement learning framework that treats instruction optimization as a stateless, continuous-action problem, utilizing only black-box feedback.", "result": "ACING outperforms human-written prompts in 76% of instruction-induction tasks, showing up to 33 points improvement and significant gains in summarization and chain-of-thought reasoning tasks.", "conclusion": "The framework demonstrates robustness and efficiency in discovering high-quality prompts automatically.", "key_contributions": ["Development of ACING framework for instruction optimization using reinforcement learning.", "Demonstrated performance improvement over human-written prompts in diverse tasks.", "Open-source implementation available for further research."], "limitations": "", "keywords": ["Large Language Models", "Instruction Optimization", "Reinforcement Learning", "Human-Computer Interaction", "AI Applications"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2501.04316", "pdf": "https://arxiv.org/pdf/2501.04316.pdf", "abs": "https://arxiv.org/abs/2501.04316", "title": "Small Changes, Large Consequences: Analyzing the Allocational Fairness of LLMs in Hiring Contexts", "authors": ["Preethi Seshadri", "Hongyu Chen", "Sameer Singh", "Seraphina Goldfarb-Tarrant"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly being deployed in high-stakes\napplications like hiring, yet their potential for unfair decision-making\nremains understudied in generative and retrieval settings. In this work, we\nexamine the allocational fairness of LLM-based hiring systems through two tasks\nthat reflect actual HR usage: resume summarization and applicant ranking. By\nconstructing a synthetic resume dataset with controlled perturbations and\ncurating job postings, we investigate whether model behavior differs across\ndemographic groups. Our findings reveal that generated summaries exhibit\nmeaningful differences more frequently for race than for gender perturbations.\nModels also display non-uniform retrieval selection patterns across demographic\ngroups and exhibit high ranking sensitivity to both gender and race\nperturbations. Surprisingly, retrieval models can show comparable sensitivity\nto both demographic and non-demographic changes, suggesting that fairness\nissues may stem from broader model brittleness. Overall, our results indicate\nthat LLM-based hiring systems, especially in the retrieval stage, can exhibit\nnotable biases that lead to discriminatory outcomes in real-world contexts.", "AI": {"tldr": "This paper examines the allocational fairness of LLM-based hiring systems, revealing biases in resume summarization and applicant ranking across demographic groups.", "motivation": "The study aims to address the understudied potential for unfair decision-making in LLM applications within high-stakes hiring processes.", "method": "The authors created a synthetic resume dataset with controlled perturbations and curated job postings to investigate model behavior differences across demographic groups.", "result": "Findings indicate significant biases in LLM-generated summaries and retrieval patterns, with more frequent disparities observed for race than gender, and high sensitivity to demographic changes.", "conclusion": "LLM-based hiring systems show notable biases that can lead to discriminatory outcomes, especially during the retrieval stage.", "key_contributions": ["Investigation of allocational fairness in LLM hiring systems", "Analysis of demographic disparities in LLM-generated content", "Insights into model sensitivity and brittleness affecting fairness"], "limitations": "Focus on a synthetic dataset may limit generalizability to real-world hiring scenarios.", "keywords": ["Large language models", "allocational fairness", "hiring systems", "demographic bias", "applicant ranking"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.13958", "pdf": "https://arxiv.org/pdf/2501.13958.pdf", "abs": "https://arxiv.org/abs/2501.13958", "title": "A Survey of Graph Retrieval-Augmented Generation for Customized Large Language Models", "authors": ["Qinggang Zhang", "Shengyuan Chen", "Yuanchen Bei", "Zheng Yuan", "Huachi Zhou", "Zijin Hong", "Hao Chen", "Yilin Xiao", "Chuang Zhou", "Yi Chang", "Xiao Huang"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in a\nwide range of tasks, yet their application to specialized domains remains\nchallenging due to the need for deep expertise. Retrieval-Augmented generation\n(RAG) has emerged as a promising solution to customize LLMs for professional\nfields by seamlessly integrating external knowledge bases, enabling real-time\naccess to domain-specific expertise during inference. Despite its potential,\ntraditional RAG systems, based on flat text retrieval, face three critical\nchallenges: (i) complex query understanding in professional contexts, (ii)\ndifficulties in knowledge integration across distributed sources, and (iii)\nsystem efficiency bottlenecks at scale. This survey presents a systematic\nanalysis of Graph-based Retrieval-Augmented Generation (GraphRAG), a new\nparadigm that revolutionizes domain-specific LLM applications. GraphRAG\naddresses traditional RAG limitations through three key innovations: (i)\ngraph-structured knowledge representation that explicitly captures entity\nrelationships and domain hierarchies, (ii) efficient graph-based retrieval\ntechniques that enable context-preserving knowledge retrieval with multihop\nreasoning ability, and (iii) structure-aware knowledge integration algorithms\nthat leverage retrieved knowledge for accurate and logical coherent generation\nof LLMs. In this survey, we systematically analyze the technical foundations of\nGraphRAG and examine current implementations across various professional\ndomains, identifying key technical challenges and promising research\ndirections. All the related resources of GraphRAG, including research papers,\nopen-source data, and projects, are collected for the community in\nhttps://github.com/DEEP-PolyU/Awesome-GraphRAG.", "AI": {"tldr": "This survey presents Graph-based Retrieval-Augmented Generation (GraphRAG), an innovative approach to enhance domain-specific large language models (LLMs) by addressing traditional challenges faced by retrieval-augmented systems.", "motivation": "Despite the potential of Retrieval-Augmented Generation (RAG) for customizing LLMs, traditional systems struggle with query understanding, knowledge integration, and efficiency in specialized domains.", "method": "The paper systematically analyzes GraphRAG, which utilizes graph-structured knowledge representation, efficient graph-based retrieval techniques, and structure-aware knowledge integration algorithms to improve LLM performance in specific domains.", "result": "GraphRAG demonstrates improved context-preserving knowledge retrieval and enhances reasoning capabilities in LLMs, allowing for more accurate and coherent output.", "conclusion": "The survey highlights the technical foundations, current applications, challenges, and future directions of GraphRAG, providing valuable resources for researchers.", "key_contributions": ["Introduction of GraphRAG to enhance LLMs in specialized domains", "Development of efficient graph-based retrieval techniques", "Implementation of structure-aware knowledge integration algorithms"], "limitations": "", "keywords": ["Graph-based Retrieval-Augmented Generation", "Large Language Models", "Domain-specific Applications"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.14701", "pdf": "https://arxiv.org/pdf/2501.14701.pdf", "abs": "https://arxiv.org/abs/2501.14701", "title": "An Unsupervised Natural Language Processing Pipeline for Assessing Referral Appropriateness", "authors": ["Vittorio Torri", "Annamaria Bottelli", "Michele Ercolanoni", "Olivia Leoni", "Francesca Ieva"], "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7; J.1; J.3"], "comment": "49 pages, 10 figures", "summary": "Objective: Assessing the appropriateness of diagnostic referrals is critical\nfor improving healthcare efficiency and reducing unnecessary procedures.\nHowever, this task becomes challenging when referral reasons are recorded only\nas free text rather than structured codes, like in the Italian NHS. To address\nthis gap, we propose a fully unsupervised Natural Language Processing (NLP)\npipeline capable of extracting and evaluating referral reasons without relying\non labelled datasets.\n  Methods: Our pipeline leverages Transformer-based embeddings pre-trained on\nItalian medical texts to cluster referral reasons and assess their alignment\nwith appropriateness guidelines. It operates in an unsupervised setting and is\ndesigned to generalize across different examination types. We analyzed two\ncomplete regional datasets from the Lombardy Region (Italy), covering all\nreferrals between 2019 and 2021 for venous echocolordoppler of the lower limbs\n(ECD;n=496,971; development) and flexible endoscope colonoscopy (FEC;\nn=407,949; testing only). For both, a random sample of 1,000 referrals was\nmanually annotated to measure performance.\n  Results: The pipeline achieved high performance in identifying referral\nreasons (Prec=92.43% (ECD), 93.59% (FEC); Rec=83.28% (ECD), 92.70% (FEC)) and\nappropriateness (Prec=93.58% (ECD), 94.66% (FEC); Rec=91.52% (ECD), 93.96%\n(FEC)). At the regional level, the analysis identified relevant inappropriate\nreferral groups and variation across contexts, findings that informed a new\nLombardy Region resolution to reinforce guideline adherence.\n  Conclusions: This study presents a robust, scalable, unsupervised NLP\npipeline for assessing referral appropriateness in large, real-world datasets.\nIt demonstrates how such data can be effectively leveraged, providing public\nhealth authorities with a deployable AI tool to monitor practices and support\nevidence-based policy.", "AI": {"tldr": "The paper proposes an unsupervised NLP pipeline for extracting and evaluating diagnostic referral reasons from free text in the Italian NHS, achieving high performance in identifying appropriateness of referrals.", "motivation": "To improve healthcare efficiency and reduce unnecessary procedures by assessing diagnostic referral appropriateness recorded as free text.", "method": "The pipeline utilizes Transformer-based embeddings pre-trained on Italian medical texts to cluster referral reasons and evaluate them against appropriateness guidelines in an unsupervised manner.", "result": "It achieved high precision and recall in identifying referral reasons and assessing appropriateness, with notable findings influencing policy in the Lombardy Region.", "conclusion": "The study delivers a scalable unsupervised NLP solution for analyzing large datasets of diagnostic referrals, aiding public health authorities in monitoring practices.", "key_contributions": ["Development of an unsupervised NLP pipeline for medical referral analysis", "High performance metrics in real-world datasets", "Influence on policy for guideline adherence in the Lombardy Region"], "limitations": "", "keywords": ["Natural Language Processing", "Referral Appropriateness", "Healthcare Efficiency", "Unsupervised Learning", "Transformer Models"], "importance_score": 9, "read_time_minutes": 49}}
{"id": "2502.05982", "pdf": "https://arxiv.org/pdf/2502.05982.pdf", "abs": "https://arxiv.org/abs/2502.05982", "title": "HamRaz: A Culture-Based Persian Conversation Dataset for Person-Centered Therapy Using LLM Agents", "authors": ["Mohammad Amin Abbasi", "Farnaz Sadat Mirnezami", "Ali Neshati", "Hassan Naderi"], "categories": ["cs.CL"], "comment": null, "summary": "We present HamRaz, a culturally adapted Persian-language dataset for\nAI-assisted mental health support, grounded in Person-Centered Therapy (PCT).\nTo reflect real-world therapeutic challenges, we combine script-based dialogue\nwith adaptive large language models (LLM) role-playing, capturing the ambiguity\nand emotional nuance of Persian-speaking clients. We introduce HamRazEval, a\ndual-framework for assessing conversational and therapeutic quality using\nGeneral Metrics and specialized psychological relationship measures. Human\nevaluations show HamRaz outperforms existing baselines in empathy, coherence,\nand realism. This resource contributes to the Digital Humanities by bridging\nlanguage, culture, and mental health in underrepresented communities.", "AI": {"tldr": "HamRaz is a Persian-language dataset designed for AI-assisted mental health support, focusing on Person-Centered Therapy and featuring advanced evaluation metrics.", "motivation": "The paper aims to address the lack of culturally adapted mental health resources in Persian-speaking communities by creating a relevant dataset and evaluation framework.", "method": "The methodology includes combining script-based dialogues with adaptive large language models to create realistic therapeutic interactions, alongside a dual-framework for evaluation.", "result": "Human evaluations demonstrate that the HamRaz dataset outperforms existing models in terms of empathy, coherence, and realism.", "conclusion": "The development of HamRaz not only improves mental health support for Persian speakers but also contributes to the Digital Humanities by addressing cultural and language gaps.", "key_contributions": ["Introduction of a culturally adapted dataset for Persian-language mental health support.", "Development of HamRazEval, a novel evaluation framework for assessing conversation quality.", "Demonstration of superior performance in key therapeutic dimensions compared to existing baselines."], "limitations": "Limitations include potential biases in data collection and the restricted focus on Persian-language contexts.", "keywords": ["mental health", "AI", "language model", "cultural adaptation", "therapeutic quality"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.12065", "pdf": "https://arxiv.org/pdf/2502.12065.pdf", "abs": "https://arxiv.org/abs/2502.12065", "title": "Autoformalization in the Wild: Assessing LLMs on Real-World Mathematical Definitions", "authors": ["Lan Zhang", "Marco Valentino", "Andre Freitas"], "categories": ["cs.CL", "cs.FL"], "comment": "EMNLP 2025 Camera-Ready Version", "summary": "Thanks to their linguistic capabilities, LLMs offer an opportunity to bridge\nthe gap between informal mathematics and formal languages through\nautoformalization. However, it is still unclear how well LLMs generalize to\nsophisticated and naturally occurring mathematical statements. To address this\ngap, we investigate the task of autoformalizing real-world mathematical\ndefinitions: a critical component of mathematical discourse. Specifically, we\nintroduce two novel resources for autoformalization, collecting definitions\nfrom Wikipedia (Def_Wiki) and arXiv papers (Def_ArXiv). We then systematically\nevaluate a range of LLMs, analyzing their ability to formalize definitions into\nIsabelle/HOL. Furthermore, we investigate strategies to enhance LLMs'\nperformance including refinement through external feedback from Proof\nAssistants, and formal definition grounding, where we augment LLMs'\nformalizations through relevant contextual elements from formal mathematical\nlibraries. Our findings reveal that definitions present a greater challenge\ncompared to existing benchmarks, such as miniF2F. In particular, we found that\nLLMs still struggle with self-correction, and aligning with relevant\nmathematical libraries. At the same time, structured refinement methods and\ndefinition grounding strategies yield notable improvements of up to 16% on\nself-correction capabilities and 43% on the reduction of undefined errors,\nhighlighting promising directions for enhancing LLM-based autoformalization in\nreal-world scenarios.", "AI": {"tldr": "This paper explores the autoformalization of real-world mathematical definitions using LLMs, introducing new resources and evaluation metrics, and proposing methods for enhancing LLM performance.", "motivation": "The paper addresses the challenge of generalizing LLMs to sophisticated mathematical statements and aims to improve their ability to autoformalize definitions from real-world sources.", "method": "The authors collected mathematical definitions from Wikipedia and arXiv papers and evaluated various LLMs on their autoformalization capabilities, focusing on formalizing these definitions into Isabelle/HOL.", "result": "The study found that LLMs faced significant challenges with self-correction and aligning with formal mathematical libraries but showed notable performance improvements with structured refinement methods and definition grounding strategies, resulting in up to a 16% improvement in self-correction capabilities and a 43% reduction in undefined errors.", "conclusion": "The findings highlight the complexities of autoformalization and suggest that enhancements through additional context from formal libraries can improve LLM performance.", "key_contributions": ["Introduction of two new resources for autoformalization (Def_Wiki and Def_ArXiv).", "Systematic evaluation of LLMs on real-world mathematical definitions.", "Strategies for enhancing LLM performance through external feedback and formal definition grounding."], "limitations": "LLMs are still challenged by self-correction and aligning with mathematical libraries despite the proposed enhancements.", "keywords": ["autoformalization", "Mathematics", "LLMs", "Isabelle/HOL", "formal definitions"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.12616", "pdf": "https://arxiv.org/pdf/2502.12616.pdf", "abs": "https://arxiv.org/abs/2502.12616", "title": "Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions", "authors": ["Leonardo Ranaldi", "Marco Valentino", "Andrè Freitas"], "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-Though (CoT) represents a common strategy for reasoning in Large\nLanguage Models (LLMs) by decomposing complex tasks into intermediate inference\nsteps. However, explanations generated via CoT are susceptible to content\nbiases that negatively affect their robustness and faithfulness. To mitigate\nexisting limitations, recent work has proposed using logical formalisms coupled\nwith external symbolic solvers. However, fully symbolic approaches possess the\nbottleneck of requiring a complete translation from natural language to formal\nlanguages, a process that affects efficiency and flexibility. To achieve a\ntrade-off, this paper investigates methods to disentangle content from logical\nreasoning without a complete formalisation. In particular, we present QuaSAR\n(for Quasi-Symbolic Abstract Reasoning), a variation of CoT that guides LLMs to\noperate at a higher level of abstraction via quasi-symbolic explanations. Our\nframework leverages the capability of LLMs to formalise only relevant variables\nand predicates, enabling the coexistence of symbolic elements with natural\nlanguage. We show the impact of QuaSAR for in-context learning and for\nconstructing demonstrations to improve the reasoning capabilities of smaller\nmodels. Our experiments show that quasi-symbolic abstractions can improve\nCoT-based methods by up to 8% accuracy, enhancing robustness and consistency on\nchallenging adversarial variations on both natural language (i.e. MMLU-Redux)\nand symbolic reasoning tasks (i.e., GSM-Symbolic).", "AI": {"tldr": "This paper introduces QuaSAR, a quasi-symbolic reasoning framework to enhance Chain-of-Thought strategies in LLMs by improving robustness and efficiency without complete formalization.", "motivation": "The paper addresses the limitations of Chain-of-Thought explanations in LLMs, which often suffer from content biases affecting robustness and faithfulness.", "method": "The authors propose QuaSAR, which enables LLMs to combine symbolic reasoning with natural language, allowing for relevant variable formalization without full translation to formal languages.", "result": "Experimental results show that QuaSAR can enhance CoT-based methods by up to 8% accuracy, improving robustness and consistency in reasoning tasks.", "conclusion": "QuaSAR represents a promising approach to enhance LLM reasoning capabilities while mitigating the need for complete symbolic formalization.", "key_contributions": ["Introduction of QuaSAR for quasi-symbolic reasoning in LLMs", "Demonstration of improved reasoning capabilities for smaller models", "Experimental validation showing significant accuracy improvement in CoT methods"], "limitations": "", "keywords": ["Chain-of-Thought", "Quasi-Symbolic Reasoning", "Large Language Models", "Natural Language Processing", "Robustness"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2503.16561", "pdf": "https://arxiv.org/pdf/2503.16561.pdf", "abs": "https://arxiv.org/abs/2503.16561", "title": "FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article", "authors": ["Ibrahim Al Azher", "Miftahul Jannat Mokarrama", "Zhishuai Guo", "Sagnik Ray Choudhury", "Hamed Alhoori"], "categories": ["cs.CL", "cs.LG"], "comment": "12 pages, 6 figures, Accepted for publication at the Workshop on AI\n  Principles in Science Communication (Ai4SC'25), held in conjunction with the\n  IEEE eScience Conference 2025", "summary": "The Future Work section of a scientific article outlines potential research\ndirections by identifying gaps and limitations of a current study. This section\nserves as a valuable resource for early-career researchers seeking unexplored\nareas and experienced researchers looking for new projects or collaborations.\nIn this study, we generate future work suggestions from a scientific article.\nTo enrich the generation process with broader insights and reduce the chance of\nmissing important research directions, we use context from related papers using\nRAG. We experimented with various Large Language Models (LLMs) integrated into\nRetrieval-Augmented Generation (RAG). We incorporate an LLM feedback mechanism\nto enhance the quality of the generated content and introduce an LLM-as-a-judge\nframework for robust evaluation, assessing key aspects such as novelty,\nhallucination, and feasibility. Our results demonstrate that the RAG-based\napproach using GPT-4o mini, combined with an LLM feedback mechanism,\noutperforms other methods based on both qualitative and quantitative\nevaluations. Moreover, we conduct a human evaluation to assess the LLM as an\nextractor, generator, and feedback provider.", "AI": {"tldr": "This study investigates generating future work suggestions from scientific articles using RAG and LLMs, proposing enhancements through a feedback mechanism and evaluation framework.", "motivation": "To identify unexplored research areas and enhance the quality of future work suggestions for researchers.", "method": "The study employs Retrieval-Augmented Generation (RAG) and integrates various Large Language Models (LLMs), utilizing an LLM feedback mechanism and an LLM-as-a-judge framework for evaluation.", "result": "The RAG-based approach using GPT-4o mini, enhanced by an LLM feedback mechanism, outperforms other generation methods in both qualitative and quantitative evaluations.", "conclusion": "The enhancements proposed not only improve the generated suggestions but also facilitate robust evaluations of the generated content.", "key_contributions": ["Development of an LLM feedback mechanism to improve content quality.", "Introduction of an LLM-as-a-judge framework for evaluating generated research directions.", "Demonstrated effectiveness of the RAG-based approach in generating future work suggestions."], "limitations": "The study may be limited by the scope of LLM capabilities and the specific datasets used for training and evaluation.", "keywords": ["Future Work", "Retrieval-Augmented Generation", "Large Language Models", "LLM feedback mechanism", "Scientific research suggestions"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2503.21080", "pdf": "https://arxiv.org/pdf/2503.21080.pdf", "abs": "https://arxiv.org/abs/2503.21080", "title": "EQ-Knight: A Memory-Augmented LLM Agent for Strategic Affective Gaming in Debt Recovery", "authors": ["Yunbo Long", "Yuhan Liu", "Liming Xu", "Alexandra Brintrup"], "categories": ["cs.CL"], "comment": null, "summary": "Large language model-based chatbots have enhanced engagement in financial\nnegotiations, but their overreliance on passive empathy introduces critical\nrisks in credit collection. While empathy-driven approaches preserve client\nsatisfaction in benign cases, they fail catastrophically against dishonest\ndebtors--individuals who exploit conciliatory tactics to manipulate terms or\nevade repayment. Blindly prioritizing \"customer experience\" in such scenarios\nleads to creditor vulnerabilities: revenue leakage, moral hazard, and systemic\nexploitation. To address this, we propose EQ-Knight, an LLM agent that\ndynamically optimizes emotional strategy to defend creditor interests. Unlike\nnaive empathy-centric bots, EQ-Knight integrates emotion memory and\ngame-theoretic reasoning, powered by a Hidden Markov Model (HMM) to track and\npredict debtor emotional states. By analyzing both real-time and historical\nemotional cues, EQ-Knight strategically counters negative emotions (e.g.,\naggression, feigned distress) while preserving productive debtor relationships.\nExperiments demonstrate EQ-Knight's superiority over conventional LLM\nnegotiators: it achieves a 32\\% reduction in concession losses without\ncompromising recovery rates, particularly in adversarial cases where debtors\nweaponize negative emotions (e.g., intimidation, guilt-tripping) to coerce\nconcessions. For credit agencies, EQ-Knight transforms LLMs from high-risk\n\"people-pleasers\" into strategic emotion-defenders--balancing emotional\nintelligence with tactical rigor to enforce accountability and deter\nexploitation.", "AI": {"tldr": "Introduces EQ-Knight, an LLM agent that optimizes emotional strategy in financial negotiations to defend creditor interests against manipulative debtors.", "motivation": "To address the vulnerabilities of traditional empathy-based chatbots in credit collection, especially against dishonest debtors.", "method": "EQ-Knight integrates emotion memory and game-theoretic reasoning using a Hidden Markov Model (HMM) to track and predict debtor emotional states.", "result": "EQ-Knight achieved a 32% reduction in concession losses compared to conventional LLM negotiators without compromising recovery rates, especially in adversarial scenarios.", "conclusion": "EQ-Knight balances emotional intelligence with tactical rigor, transforming LLMs into strategic emotion-defenders for credit agencies.", "key_contributions": ["Development of EQ-Knight, integrating emotion memory and game-theoretic reasoning.", "Demonstrating a significant reduction in concession losses in negotiations with dishonest debtors.", "Providing a new paradigm for LLMs in credit collection that enhances creditor protection."], "limitations": "The performance of EQ-Knight may vary depending on the implementation context and training data quality.", "keywords": ["large language models", "emotional strategy", "game theory", "credit collection", "Hidden Markov Model"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2506.07900", "pdf": "https://arxiv.org/pdf/2506.07900.pdf", "abs": "https://arxiv.org/abs/2506.07900", "title": "MiniCPM4: Ultra-Efficient LLMs on End Devices", "authors": ["MiniCPM Team", "Chaojun Xiao", "Yuxuan Li", "Xu Han", "Yuzhuo Bai", "Jie Cai", "Haotian Chen", "Wentong Chen", "Xin Cong", "Ganqu Cui", "Ning Ding", "Shengda Fan", "Yewei Fang", "Zixuan Fu", "Wenyu Guan", "Yitong Guan", "Junshao Guo", "Yufeng Han", "Bingxiang He", "Yuxiang Huang", "Baoxi Ji", "Cunliang Kong", "Qiuzuo Li", "Siyuan Li", "Wenhao Li", "Xin Li", "Yanghao Li", "Yishan Li", "Zhen Li", "Dan Liu", "Biyuan Lin", "Yankai Lin", "Xiang Long", "Quanyu Lu", "Yaxi Lu", "Peiyan Luo", "Hongya Lyu", "Litu Ou", "Yinxu Pan", "Lushi Pu", "Zekai Qu", "Qundong Shi", "Zijun Song", "Jiayuan Su", "Zhou Su", "Ao Sun", "Xianghui Sun", "Peijun Tang", "Fangzheng Wang", "Feng Wang", "Shuo Wang", "Yudong Wang", "Zheng Wang", "Yesai Wu", "Zhenyu Xiao", "Jie Xie", "Zihao Xie", "Xiaoyue Xu", "Yukun Yan", "Jiarui Yuan", "Jinqian Zhang", "Kaihuo Zhang", "Lei Zhang", "Linyue Zhang", "Xueren Zhang", "Yudi Zhang", "Hengyu Zhao", "Weilin Zhao", "Weilun Zhao", "Yuanqian Zhao", "Zhi Zheng", "Chuyue Zhou", "Ge Zhou", "Jie Zhou", "Wei Zhou", "Yanghao Zhou", "Zihan Zhou", "Zixuan Zhou", "Zhiyuan Liu", "Guoyang Zeng", "Chao Jia", "Dahai Li", "Maosong Sun"], "categories": ["cs.CL", "cs.AI"], "comment": "MiniCPM4 Technical Report", "summary": "This paper introduces MiniCPM4, a highly efficient large language model (LLM)\ndesigned explicitly for end-side devices. We achieve this efficiency through\nsystematic innovation in four key dimensions: model architecture, training\ndata, training algorithms, and inference systems. Specifically, in terms of\nmodel architecture, we propose InfLLM v2, a trainable sparse attention\nmechanism that accelerates both prefilling and decoding phases for long-context\nprocessing. Regarding training data, we propose UltraClean, an efficient and\naccurate pre-training data filtering and generation strategy, and UltraChat v2,\na comprehensive supervised fine-tuning dataset. These datasets enable\nsatisfactory model performance to be achieved using just 8 trillion training\ntokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient\npre-training strategy search, and improve existing post-training methods by\nintroducing chunk-wise rollout for load-balanced reinforcement learning and\ndata-efficient tenary LLM, BitCPM. Regarding inference systems, we propose\nCPM.cu that integrates sparse attention, model quantization, and speculative\nsampling to achieve efficient prefilling and decoding. To meet diverse\non-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B\nparameters, respectively. Furthermore, we construct a hybrid reasoning model,\nMiniCPM4.1, which can be used in both deep reasoning mode and non-reasoning\nmode. Evaluation results demonstrate that MiniCPM4 and MiniCPM4.1 outperform\nsimilar-sized open-source models across benchmarks, with the 8B variants\nshowing significant speed improvements on long sequence understanding and\ngeneration.", "AI": {"tldr": "MiniCPM4 is a highly efficient large language model tailored for end-side devices, enhancing performance in long-context processing through innovations in architecture, training data, algorithms, and inference systems.", "motivation": "To develop a large language model that is efficient for deployment on end-side devices while maintaining high performance in processing long contexts.", "method": "Introduces InfLLM v2 for sparse attention, UltraClean for data filtering, ModelTunnel v2 for pre-training efficiency, and CPM.cu for improved inference processes, deploying different parameter versions of the model for varied requirements.", "result": "MiniCPM4 and its variant, MiniCPM4.1, demonstrate superior performance over similar-sized models, particularly in speed and understanding of long sequences as evidenced by comprehensive benchmark evaluations.", "conclusion": "MiniCPM4 represents a significant advancement in the efficiency of large language models for on-device applications, achieving notable results with reduced training data and enhanced processing capabilities.", "key_contributions": ["Development of InfLLM v2 for efficient sparse attention.", "Creation of UltraClean and UltraChat v2 datasets for effective model training.", "Introduction of the hybrid reasoning model MiniCPM4.1."], "limitations": "", "keywords": ["large language models", "sparse attention", "on-device AI", "training algorithms", "model efficiency"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.14723", "pdf": "https://arxiv.org/pdf/2508.14723.pdf", "abs": "https://arxiv.org/abs/2508.14723", "title": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation", "authors": ["Guangzhan Wang", "Hongyu Zhang", "Beijun Shen", "Xiaodong Gu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP 2025", "summary": "Data augmentation is a critical technique in deep learning. Traditional\nmethods like Back-translation typically focus on lexical-level rephrasing,\nwhich primarily produces variations with the same semantics. While large\nlanguage models (LLMs) have enhanced text augmentation by their \"knowledge\nemergence\" capability, controlling the style and structure of these outputs\nremains challenging and requires meticulous prompt engineering. In this paper,\nwe propose LMTransplant, a novel text augmentation paradigm leveraging LLMs.\nThe core idea of LMTransplant is transplant-then-regenerate: incorporating seed\ntext into a context expanded by LLM, and asking the LLM to regenerate a variant\nbased on the expanded context. This strategy allows the model to create more\ndiverse and creative content-level variants by fully leveraging the knowledge\nembedded in LLMs, while preserving the core attributes of the original text. We\nevaluate LMTransplant across various text-related tasks, demonstrating its\nsuperior performance over existing text augmentation methods. Moreover,\nLMTransplant demonstrates exceptional scalability as the size of augmented data\ngrows.", "AI": {"tldr": "LMTransplant is a new text augmentation method using LLMs that enhances diversity and creativity while retaining original text attributes.", "motivation": "Traditional data augmentation methods in deep learning have limitations in generating diverse text variations. Large language models (LLMs) improve augmentation but require careful prompt engineering to control output style and structure.", "method": "The proposed LMTransplant method incorporates seed text into a context developed by LLMs and then asks the LLM to regenerate a variant, enabling more diverse outputs while preserving core text properties.", "result": "LMTransplant is shown to outperform traditional text augmentation methods in various tasks and demonstrates excellent scalability with increasing augmented data size.", "conclusion": "The LMTransplant method provides a more effective approach to text augmentation than existing methods by leveraging LLMs' knowledge while maintaining the integrity of the original text.", "key_contributions": ["Introduction of LMTransplant, a novel augmentation paradigm using LLMs", "Improved diversity and creativity in text variation compared to traditional methods", "Evaluation showing superior performance and scalability of LMTransplant across multiple tasks."], "limitations": "", "keywords": ["text augmentation", "large language models", "LMTransplant", "deep learning", "data augmentation"], "importance_score": 9, "read_time_minutes": 10}}
