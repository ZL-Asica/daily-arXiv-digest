{"id": "2508.06512", "pdf": "https://arxiv.org/pdf/2508.06512.pdf", "abs": "https://arxiv.org/abs/2508.06512", "title": "Accessibility Literacy: Increasing accessibility awareness among young content creators", "authors": ["Alina Karakanta"], "categories": ["cs.HC", "cs.CY"], "comment": "Master thesis: MASTER OF ARTS IN ACCESSIBILITY TO MEDIA, ARTS AND\n  CULTURE", "summary": "The proliferation of audiovisual and web content has created an increasing\nneed for media accessibility education in various fields. However,\naccessibility remains a low priority in university curricula. This project\nexplores the feasibility of an alternative learning experience aimed at\nincreasing the accessibility literacy of young content creators, taking web\naccessibility as a case study. We propose a mini module that uses simple,\neasy-to-use training materials, such as infographics and short quizzes, and can\nbe easily incorporated in educational programmes along existing courses. A\nsurvey was conducted to investigate the participants' accessibility literacy\nbefore and after training. The findings show that young content creators\ngenerally have limited accessibility literacy but even brief exposure to\naccessibility materials contributed to a shift in perceptions. After training,\nparticipants expressed more willingness to implement accessibility tools in\ntheir content, with ways varying depending on content type and purpose. This\nsuggests that small, yet targeted interventions could be an alternative for\nintegrating accessibility training into formal education across various\ndisciplines. While some responses reflected traces of the medical model of\ndisability and a particularlist view of accessibility, accessibility was\nrecognised as important for increasing inclusion, improving content, and\nshaping a fairer society.", "AI": {"tldr": "This project proposes a mini module to enhance accessibility literacy among young content creators, using web accessibility as a case study. A survey indicated that brief training improved participants' perceptions and willingness to adopt accessibility tools.", "motivation": "There's a growing necessity for media accessibility education, yet it remains underrepresented in university curricula.", "method": "A mini module utilizing infographics and quizzes was created to educate young content creators on web accessibility, followed by a survey to assess literacy before and after training.", "result": "Participants showed low initial accessibility literacy but demonstrated improved perceptions and willingness to implement accessibility tools post-training.", "conclusion": "Targeted, brief training sessions can effectively integrate accessibility education into formal academic settings, promoting inclusivity and improved content creation.", "key_contributions": ["Development of an easy-to-use module for teaching accessibility", "Demonstration of improved perceptions and willingness post-training", "Highlights the importance of accessibility in content creation for inclusivity"], "limitations": "Responses revealed some concerning views on disability and accessibility, highlighting the need for a broader understanding of inclusion.", "keywords": ["accessibility literacy", "content creators", "web accessibility"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.06732", "pdf": "https://arxiv.org/pdf/2508.06732.pdf", "abs": "https://arxiv.org/abs/2508.06732", "title": "ClimateSOM: A Visual Analysis Workflow for Climate Ensemble Datasets", "authors": ["Yuya Kawakami", "Daniel Cayan", "Dongyu Liu", "Kwan-Liu Ma"], "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "Ensemble datasets are ever more prevalent in various scientific domains. In\nclimate science, ensemble datasets are used to capture variability in\nprojections under plausible future conditions including greenhouse and aerosol\nemissions. Each ensemble model run produces projections that are fundamentally\nsimilar yet meaningfully distinct. Understanding this variability among\nensemble model runs and analyzing its magnitude and patterns is a vital task\nfor climate scientists. In this paper, we present ClimateSOM, a visual analysis\nworkflow that leverages a self-organizing map (SOM) and Large Language Models\n(LLMs) to support interactive exploration and interpretation of climate\nensemble datasets. The workflow abstracts climate ensemble model runs -\nspatiotemporal time series - into a distribution over a 2D space that captures\nthe variability among the ensemble model runs using a SOM. LLMs are integrated\nto assist in sensemaking of this SOM-defined 2D space, the basis for the visual\nanalysis tasks. In all, ClimateSOM enables users to explore the variability\namong ensemble model runs, identify patterns, compare and cluster the ensemble\nmodel runs. To demonstrate the utility of ClimateSOM, we apply the workflow to\nan ensemble dataset of precipitation projections over California and the\nNorthwestern United States. Furthermore, we conduct a short evaluation of our\nLLM integration, and conduct an expert review of the visual workflow and the\ninsights from the case studies with six domain experts to evaluate our approach\nand its utility.", "AI": {"tldr": "ClimateSOM is a visual analysis workflow that utilizes self-organizing maps and Large Language Models to explore and interpret climate ensemble datasets.", "motivation": "Understanding variability among ensemble model runs in climate science is crucial for capturing projections under future conditions.", "method": "The paper introduces ClimateSOM, which abstracts spatiotemporal time series of climate ensemble model runs into a 2D distribution using a self-organizing map, and integrates LLMs for analysis tasks.", "result": "ClimateSOM allows users to explore variability, identify patterns, and cluster ensemble model runs, demonstrating its utility with ensemble precipitation projections for California and the Northwestern United States.", "conclusion": "The workflow was evaluated through expert reviews, confirming its utility and insights generated from case studies.", "key_contributions": ["Introduction of ClimateSOM for climate ensemble analysis", "Integration of self-organizing maps and LLMs for interactive exploration", "Application of the framework to real-world precipitation data"], "limitations": "", "keywords": ["climate ensemble datasets", "self-organizing maps", "large language models"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.06751", "pdf": "https://arxiv.org/pdf/2508.06751.pdf", "abs": "https://arxiv.org/abs/2508.06751", "title": "Toward a Logic of Generalization about Visualization as a Decision Aid", "authors": ["Alex Kale"], "categories": ["cs.HC"], "comment": null, "summary": "Visualization as a discipline often grapples with generalization by reasoning\nabout how study results on the efficacy of a tool in one context might apply to\nanother context. This work offers an account of the logic of generalization in\nvisualization research and argues that it struggles in particular with\napplications of visualization as a decision aid. We use decision theory to\ndefine the dimensions on which decision problems can vary, and we present an\nanalysis of heterogeneity in scenarios where visualization supports\ndecision-making. Our findings identify utility as a focal and under-examined\nconcept in visualization research on decision-making, demonstrating how the\nvisualization community's logic of generalization might benefit from using\ndecision theory as a lens for understanding context variation.", "AI": {"tldr": "This paper explores the challenges of generalizing visualization research findings across different contexts, particularly in decision-making. It employs decision theory to analyze the variations in decision problems and emphasizes the concept of utility in visualizations as a decision aid.", "motivation": "To address the challenges of generalization in visualization research and improve its applications as a decision-making tool.", "method": "The authors utilize decision theory to define dimensions of decision problems and analyze the heterogeneity in scenarios where visualization supports decision-making.", "result": "The analysis reveals that utility is an under-explored concept in visualization research, suggesting that employing decision theory can enhance the understanding of context variation in visualizations.", "conclusion": "Adopting decision theory may improve the logic of generalization in visualization research and yield better insights into its application in decision-making contexts.", "key_contributions": ["Introduces decision theory to visualization research", "Identifies utility as a key concept in decision-making visualizations", "Analyzes the heterogeneity in decision-making scenarios"], "limitations": "", "keywords": ["visualization", "decision-making", "decision theory", "generalization", "utility"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.06772", "pdf": "https://arxiv.org/pdf/2508.06772.pdf", "abs": "https://arxiv.org/abs/2508.06772", "title": "Story Ribbons: Reimagining Storyline Visualizations with Large Language Models", "authors": ["Catherine Yeh", "Tara Menon", "Robin Singh Arya", "Helen He", "Moira Weigel", "Fernanda Vi√©gas", "Martin Wattenberg"], "categories": ["cs.HC", "cs.CL", "cs.LG"], "comment": "Accepted to IEEE VIS 2025 (11 pages, 9 figures)", "summary": "Analyzing literature involves tracking interactions between characters,\nlocations, and themes. Visualization has the potential to facilitate the\nmapping and analysis of these complex relationships, but capturing structured\ninformation from unstructured story data remains a challenge. As large language\nmodels (LLMs) continue to advance, we see an opportunity to use their text\nprocessing and analysis capabilities to augment and reimagine existing\nstoryline visualization techniques. Toward this goal, we introduce an\nLLM-driven data parsing pipeline that automatically extracts relevant narrative\ninformation from novels and scripts. We then apply this pipeline to create\nStory Ribbons, an interactive visualization system that helps novice and expert\nliterary analysts explore detailed character and theme trajectories at multiple\nnarrative levels. Through pipeline evaluations and user studies with Story\nRibbons on 36 literary works, we demonstrate the potential of LLMs to\nstreamline narrative visualization creation and reveal new insights about\nfamiliar stories. We also describe current limitations of AI-based systems, and\ninteraction motifs designed to address these issues.", "AI": {"tldr": "This paper introduces an LLM-driven pipeline for visualizing narratives, implementing an interactive system called Story Ribbons to enhance literary analysis.", "motivation": "The motivation is to improve the analysis of literature by utilizing LLMs to efficiently parse and visualize complex narrative structures from unstructured data.", "method": "An LLM-driven data parsing pipeline extracts relevant narrative information from novels and scripts, facilitating the creation of an interactive visualization system.", "result": "User studies and evaluations show Story Ribbons effectively aids both novice and expert analysts in exploring character and theme trajectories across narratives.", "conclusion": "LLMs have the potential to enhance narrative visualization, unveiling new insights from literature while addressing current AI limitations.", "key_contributions": ["Introduction of an LLM-driven data parsing pipeline for narrative analysis", "Development of Story Ribbons, an interactive visualization tool for literary analysis", "Demonstration of the effectiveness of LLMs in revealing insights in narrative structures"], "limitations": "Current limitations of AI-based systems and interaction motifs are discussed.", "keywords": ["Large Language Models", "Narrative Visualization", "Literary Analysis", "Interactive System", "Story Ribbons"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.06495", "pdf": "https://arxiv.org/pdf/2508.06495.pdf", "abs": "https://arxiv.org/abs/2508.06495", "title": "Semi-automated Fact-checking in Portuguese: Corpora Enrichment using Retrieval with Claim extraction", "authors": ["Juliana Resplande Sant'anna Gomes", "Arlindo Rodrigues Galv√£o Filho"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Master Thesis in Computer Science at Federal University on Goias\n  (UFG). Written in Portuguese", "summary": "The accelerated dissemination of disinformation often outpaces the capacity\nfor manual fact-checking, highlighting the urgent need for Semi-Automated\nFact-Checking (SAFC) systems. Within the Portuguese language context, there is\na noted scarcity of publicly available datasets that integrate external\nevidence, an essential component for developing robust AFC systems, as many\nexisting resources focus solely on classification based on intrinsic text\nfeatures. This dissertation addresses this gap by developing, applying, and\nanalyzing a methodology to enrich Portuguese news corpora (Fake.Br, COVID19.BR,\nMuMiN-PT) with external evidence. The approach simulates a user's verification\nprocess, employing Large Language Models (LLMs, specifically Gemini 1.5 Flash)\nto extract the main claim from texts and search engine APIs (Google Search API,\nGoogle FactCheck Claims Search API) to retrieve relevant external documents\n(evidence). Additionally, a data validation and preprocessing framework,\nincluding near-duplicate detection, is introduced to enhance the quality of the\nbase corpora.", "AI": {"tldr": "This dissertation develops a methodology for Semi-Automated Fact-Checking (SAFC) for Portuguese news by enriching news corpora with external evidence and leveraging LLMs for claim extraction.", "motivation": "The increasing pace of disinformation dissemination necessitates improved fact-checking systems, particularly for Portuguese-language content where data is scarce.", "method": "The methodology involves utilizing Large Language Models to extract main claims from news articles and employing various search engine APIs to gather relevant external evidence. A validation framework is also introduced to enhance the quality of the datasets.", "result": "The developed methodology successfully enriches Portuguese news corpora with external evidence, which is crucial for the effectiveness of SAFC systems.", "conclusion": "Improving the accessibility and quality of fact-checking resources in Portuguese can significantly enhance the fight against disinformation.", "key_contributions": ["Development of a methodology for enriching news corpora with external evidence", "Use of LLMs for claim extraction in fact-checking", "Introduction of a validation framework for quality enhancement"], "limitations": "Focused solely on the Portuguese language and may not generalize to other languages.", "keywords": ["Semi-Automated Fact-Checking", "disinformation", "Large Language Models", "Portuguese", "news corpora"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.06773", "pdf": "https://arxiv.org/pdf/2508.06773.pdf", "abs": "https://arxiv.org/abs/2508.06773", "title": "Methodology for Business Intelligence Solutions in Internet Banking Companies", "authors": ["Alex Escalante Viteri", "Javier Gamboa Cruzado", "Leonidas Asto Huaman"], "categories": ["cs.HC"], "comment": "9 pages 3 figures", "summary": "Business intelligence in the banking industry has been studied extensively in\nthe last decade; however, business executives still do not perceive efficiency\nin the decision-making process since the management and treatment of\ninformation are very timeconsuming for the deliverer, generating costs in the\nprocess. On the other hand, there is no formal methodology for developing\nbusiness intelligence solutions in this sector. This work aims to optimize\ndecision-making in a business unit that works with internet banking companies,\nreducing the time, the number of people, and the costs involved in\ndecision-making. To meet the objective, basic and applied research was\nconducted. The basic research allowed the construction of a new methodology\nfrom a study of critical success factors and approaches from the business\nintelligence literature. The applied research involved the implementation of a\nbusiness intelligence solution applying the new methodology in a\npre-experimental study. Thirty decision-making processes were analyzed using\npre-test and post-test data. Tools such as a stopwatch and observation were\nused to collect and record data on time spent, the number of people, and the\ndecision-making costs. This information was processed in the specialized\nMinitab18 statistical software, which allowed the observation and confirmation\nof relevant results regarding time reduction, the number of people, and the\ncosts generated. Therefore, it was concluded that the business intelligence\nsolution, applying the new methodology, optimized decision making in the\nbusiness unit that works with internet banking for companies.", "AI": {"tldr": "This paper presents a new methodology for optimizing decision-making in internet banking companies through a business intelligence solution, demonstrating significant time, personnel, and cost reductions.", "motivation": "To address the inefficiencies in the decision-making process within the banking sector, particularly concerning information management which is often time-consuming and costly.", "method": "The research involved basic and applied methodologies; basic research identified critical success factors and established a new methodology, while applied research tested this methodology via a pre-experimental study analyzing thirty decision-making processes before and after implementation.", "result": "The study found significant reductions in the time taken, the number of people involved, and the costs associated with decision-making after the implementation of the business intelligence solution.", "conclusion": "The new business intelligence methodology effectively optimized the decision-making processes in the business unit dealing with internet banking, leading to improved efficiency.", "key_contributions": ["Development of a formal methodology for business intelligence in banking", "Application of the new methodology in a practical setting", "Demonstration of effectiveness through statistical analysis of decision-making processes"], "limitations": "", "keywords": ["business intelligence", "decision-making", "banking", "methodology", "efficiency"], "importance_score": 3, "read_time_minutes": 9}}
{"id": "2508.06504", "pdf": "https://arxiv.org/pdf/2508.06504.pdf", "abs": "https://arxiv.org/abs/2508.06504", "title": "Retrieval augmented generation based dynamic prompting for few-shot biomedical named entity recognition using large language models", "authors": ["Yao Ge", "Sudeshna Das", "Yuting Guo", "Abeed Sarker"], "categories": ["cs.CL", "cs.AI"], "comment": "31 pages, 4 figures, 15 tables", "summary": "Biomedical named entity recognition (NER) is a high-utility natural language\nprocessing (NLP) task, and large language models (LLMs) show promise\nparticularly in few-shot settings (i.e., limited training data). In this\narticle, we address the performance challenges of LLMs for few-shot biomedical\nNER by investigating a dynamic prompting strategy involving retrieval-augmented\ngeneration (RAG). In our approach, the annotated in-context learning examples\nare selected based on their similarities with the input texts, and the prompt\nis dynamically updated for each instance during inference. We implemented and\noptimized static and dynamic prompt engineering techniques and evaluated them\non five biomedical NER datasets. Static prompting with structured components\nincreased average F1-scores by 12% for GPT-4, and 11% for GPT-3.5 and LLaMA\n3-70B, relative to basic static prompting. Dynamic prompting further improved\nperformance, with TF-IDF and SBERT retrieval methods yielding the best results,\nimproving average F1-scores by 7.3% and 5.6% in 5-shot and 10-shot settings,\nrespectively. These findings highlight the utility of contextually adaptive\nprompts via RAG for biomedical NER.", "AI": {"tldr": "This paper explores a dynamic prompting strategy using retrieval-augmented generation to enhance the performance of large language models in few-shot biomedical named entity recognition (NER).", "motivation": "To address the performance challenges faced by large language models in few-shot biomedical named entity recognition, particularly when training data is limited.", "method": "The authors investigated dynamic prompting strategies wherein annotated in-context learning examples are selected based on their similarities with the input texts, resulting in contextually adaptive prompts during inference.", "result": "Static prompting techniques improved average F1-scores significantly by 12% for GPT-4, and 11% for GPT-3.5 and LLaMA 3-70B compared to basic static prompting. Dynamic prompting yielded further enhancements, with TF-IDF and SBERT retrieval methods achieving the best results, improving average scores by 7.3% and 5.6% in 5-shot and 10-shot settings, respectively.", "conclusion": "Dynamic prompting through retrieval-augmented generation can markedly enhance the performance of large language models in biomedical named entity recognition tasks, particularly in few-shot scenarios.", "key_contributions": ["Dynamic prompting strategy that improves NER performance", "Implementation and optimization of static and dynamic prompt engineering techniques", "Evaluation across five biomedical NER datasets"], "limitations": "", "keywords": ["biomedical NER", "large language models", "few-shot learning", "dynamic prompting", "retrieval-augmented generation"], "importance_score": 9, "read_time_minutes": 31}}
{"id": "2508.06775", "pdf": "https://arxiv.org/pdf/2508.06775.pdf", "abs": "https://arxiv.org/abs/2508.06775", "title": "Visualization Vibes: The Socio-Indexical Function of Visualization Design", "authors": ["Michelle Morgenstern", "Amy Rae Fox", "Graham M. Jones", "Arvind Satyanarayan"], "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "In contemporary information ecologies saturated with misinformation,\ndisinformation, and a distrust of science itself, public data communication\nfaces significant hurdles. Although visualization research has broadened\ncriteria for effective design, governing paradigms privilege the accurate and\nefficient transmission of data. Drawing on theory from linguistic anthropology,\nwe argue that such approaches-focused on encoding and decoding propositional\ncontent-cannot fully account for how people engage with visualizations and why\nparticular visualizations might invite adversarial or receptive responses. In\nthis paper, we present evidence that data visualizations communicate not only\nsemantic, propositional meaning$\\unicode{x2013}$meaning about\ndata$\\unicode{x2013}$but also social, indexical meaning$\\unicode{x2013}$meaning\nbeyond data. From a series of ethnographically-informed interviews, we document\nhow readers make rich and varied assessments of a visualization's\n\"vibes\"$\\unicode{x2013}$inferences about the social provenance of a\nvisualization based on its design features. Furthermore, these social\nattributions have the power to influence reception, as readers' decisions about\nhow to engage with a visualization concern not only content, or even aesthetic\nappeal, but also their sense of alignment or disalignment with the entities\nthey imagine to be involved in its production and circulation. We argue these\ninferences hinge on a function of human sign systems that has thus far been\nlittle studied in data visualization: socio-indexicality, whereby the formal\nfeatures (rather than the content) of communication evoke social contexts,\nidentities, and characteristics. Demonstrating the presence and significance of\nthis socio-indexical function in visualization, this paper offers both a\nconceptual foundation and practical intervention for troubleshooting breakdowns\nin public data communication.", "AI": {"tldr": "This paper explores how data visualizations transmit not only propositional meanings related to data but also social, indexical meanings that influence audience perception and engagement.", "motivation": "The study addresses the challenges of public data communication in a context of misinformation and distrust in science, emphasizing the need for a broader understanding of how visualizations are received.", "method": "Ethnographically-informed interviews were conducted to analyze readers' assessments of visualizations, focusing on social attributions and the concept of socio-indexicality.", "result": "The findings reveal that readers draw inferences about a visualization's 'vibes' based on its design features, which significantly impacts their engagement and reception.", "conclusion": "The paper highlights the importance of social contexts and identities in the interpretation of data visualizations, offering insights for improving public data communication strategies.", "key_contributions": ["Introduces the concept of socio-indexicality in data visualization interpretation.", "Demonstrates how design features influence reader engagement beyond content.", "Provides empirical evidence from ethnographic interviews on audience responses to visualizations."], "limitations": "", "keywords": ["Data Visualization", "Public Data Communication", "Socio-Indexicality", "User Engagement", "Information Ecology"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.06524", "pdf": "https://arxiv.org/pdf/2508.06524.pdf", "abs": "https://arxiv.org/abs/2508.06524", "title": "CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models", "authors": ["Lei Jiang", "Fan Chen"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.DC", "cs.LG"], "comment": "8 pages", "summary": "Neural scaling laws have driven the development of increasingly large\nlanguage models (LLMs) by linking accuracy improvements to growth in parameter\ncount, dataset size, and compute. However, these laws overlook the carbon\nemissions that scale exponentially with LLM size. This paper presents\n\\textit{CarbonScaling}, an analytical framework that extends neural scaling\nlaws to incorporate both operational and embodied carbon in LLM training. By\nintegrating models for neural scaling, GPU hardware evolution, parallelism\noptimization, and carbon estimation, \\textit{CarbonScaling} quantitatively\nconnects model accuracy to carbon footprint. Results show that while a\npower-law relationship between accuracy and carbon holds, real-world\ninefficiencies significantly increase the scaling factor. Hardware technology\nscaling reduces carbon emissions for small to mid-sized models, but offers\ndiminishing returns for extremely large LLMs due to communication overhead and\nunderutilized GPUs. Training optimizations-especially aggressive critical batch\nsize scaling-help alleviate this inefficiency. \\textit{CarbonScaling} offers\nkey insights for training more sustainable and carbon-efficient LLMs.", "AI": {"tldr": "This paper introduces \textit{CarbonScaling}, a framework linking LLM training accuracy to carbon footprint, highlighting the need for sustainable AI practices.", "motivation": "The paper addresses the growing concern of carbon emissions associated with the training of large language models, which have been overlooked in traditional neural scaling laws.", "method": "The authors present \textit{CarbonScaling}, which combines models of neural scaling, GPU hardware evolution, parallelism optimization, and carbon estimation to analyze the relationship between model accuracy and carbon emissions.", "result": "Results indicate a power-law relationship between model accuracy and carbon emissions, with significant real-world inefficiencies that increase the scaling factor. While hardware scaling can reduce emissions for small to mid-sized models, it yields diminishing returns for very large LLMs due to operational inefficiencies.", "conclusion": "The framework provides insights for developing carbon-efficient LLMs and emphasizes the importance of training optimizations to reduce carbon footprints in large-scale AI models.", "key_contributions": ["Introduction of the \textit{CarbonScaling} framework for LLM training", "Quantification of the relationship between accuracy and carbon footprint", "Recommendations for training optimizations to enhance sustainability."], "limitations": "The analysis may not account for all environmental impacts associated with LLM training, and results may vary based on hardware specifics and usage scenarios.", "keywords": ["language models", "carbon emissions", "scaling laws", "sustainability", "HCI"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2508.06778", "pdf": "https://arxiv.org/pdf/2508.06778.pdf", "abs": "https://arxiv.org/abs/2508.06778", "title": "Gender and Careers in Platform-Mediated Work: A Longitudinal Study of Online Freelancers", "authors": ["Pyeonghwa Kim", "Steve Sawyer", "Michael Dunn"], "categories": ["cs.HC"], "comment": "Accepted to CSCW 2025", "summary": "We advance gender-inclusive research within the CSCW field by investigating\nthe long-term gendered experiences of online freelancers on digital labor\nplatforms. The prevalence of gender-based inequalities has attracted\nsignificant attention within the CSCW community. Yet, insights remain limited\non how these inequalities shape workers' long-term experiences on digital labor\nplatforms. Through a five-year longitudinal study of 105 freelancers on Upwork,\nwe reveal persistent gender disparities that influence workers' long-term work\nand career trajectories, raising concerns about the sustainability of\nplatform-mediated work. We advance the ongoing dialogue on gender inclusivity\nin the community by introducing the concepts of career disempowerment and\nplatform-mediated motherhood penalty and by offering research and design\nimplications for CSCW to foster more sustainable, equitable platform work\nenvironments for all genders.", "AI": {"tldr": "This paper explores long-term gender inequalities experienced by online freelancers, particularly on digital labor platforms like Upwork, revealing significant disparities affecting career trajectories and proposing implications for gender inclusivity.", "motivation": "To investigate the long-term gendered experiences of online freelancers and address persistent gender-based inequalities in the CSCW community.", "method": "A five-year longitudinal study involving 105 freelancers on Upwork, analyzing their experiences and career trajectories.", "result": "The study reveals persistent gender disparities impacting long-term work experiences, raising concerns about the sustainability of platform-mediated work.", "conclusion": "The findings introduce concepts like career disempowerment and platform-mediated motherhood penalty, suggesting needed design implications for equitable work environments.", "key_contributions": ["Introduced career disempowerment and platform-mediated motherhood penalty concepts.", "Provided implications for research and design in CSCW to enhance gender inclusivity.", "Highlighted long-term effects of gender disparities in digital labor."], "limitations": "", "keywords": ["Gender inclusivity", "Digital labor platforms", "CSCW", "Longitudinal study", "Freelancers"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2508.06533", "pdf": "https://arxiv.org/pdf/2508.06533.pdf", "abs": "https://arxiv.org/abs/2508.06533", "title": "The Art of Breaking Words: Rethinking Multilingual Tokenizer Design", "authors": ["Aamod Thakur", "Ajay Nagpal", "Atharva Savarkar", "Kundeshwar Pundalik", "Siddhesh Dosi", "Piyush Sawarkar", "Viraj Thakur", "Rohit Saluja", "Maunendra Sankar Desarkar", "Ganesh Ramakrishnan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While model architecture and training objectives are well-studied,\ntokenization, particularly in multilingual contexts, remains a relatively\nneglected aspect of Large Language Model (LLM) development. Existing tokenizers\noften exhibit high token-to-word ratios, inefficient use of context length, and\nslower inference. We present a systematic study that links vocabulary size,\npre-tokenization rules, and training-corpus composition to both token-to-word\nefficiency and model quality. To ground our analysis in a linguistically\ndiverse context, we conduct extensive experiments on Indic scripts, which\npresent unique challenges due to their high script diversity and orthographic\ncomplexity. Drawing on the insights from these analyses, we propose a novel\nalgorithm for data composition that balances multilingual data for tokenizer\ntraining. Our observations on pretokenization strategies significantly improve\nmodel performance, and our data composition algorithm reduces the average\ntoken-to-word ratio by approximately 6% with respect to the conventional data\nrandomization approach. Our tokenizer achieves more than 40% improvement on\naverage token-to-word ratio against stateof-the-art multilingual Indic models.\nThis improvement yields measurable gains in both model performance and\ninference speed. This highlights tokenization alongside architecture and\ntraining objectives as a critical lever for building efficient, scalable\nmultilingual LLMs", "AI": {"tldr": "This paper presents a systematic study on the impact of tokenization in multilingual Large Language Models (LLMs), focusing on vocabulary size and pre-tokenization rules, culminating in a novel data composition algorithm that significantly improves token-to-word efficiency and model performance.", "motivation": "The motivation is to address the neglected aspect of tokenization in LLM development, particularly in multilingual contexts, which affects model quality and efficiency.", "method": "The study involves a systematic analysis linking vocabulary size, pre-tokenization rules, and training corpus composition to token-to-word efficiency. Experiments are conducted using Indic scripts to examine the challenges posed by their diversity.", "result": "The proposed algorithm for data composition reduces the average token-to-word ratio by approximately 6%, while achieving over 40% improvement on average token-to-word ratio compared to existing multilingual Indic models, leading to enhanced model performance and inference speed.", "conclusion": "The findings demonstrate that tokenization should be considered alongside model architecture and training objectives as a vital component in developing efficient and scalable multilingual LLMs.", "key_contributions": ["Introduction of a novel data composition algorithm for tokenizer training", "Empirical evidence linking tokenization strategies to model performance", "Demonstration of improved token-to-word efficiency in multilingual settings"], "limitations": "", "keywords": ["tokenization", "multilingual models", "LLM efficiency"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.06786", "pdf": "https://arxiv.org/pdf/2508.06786.pdf", "abs": "https://arxiv.org/abs/2508.06786", "title": "Quantifying Visualization Vibes: Measuring Socio-Indexicality at Scale", "authors": ["Amy Rae Fox", "Michelle Morgenstern", "Graham M. Jones", "Arvind Satyanarayan"], "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "What impressions might readers form with visualizations that go beyond the\ndata they encode? In this paper, we build on recent work that demonstrates the\nsocio-indexical function of visualization, showing that visualizations\ncommunicate more than the data they explicitly encode. Bridging this with prior\nwork examining public discourse about visualizations, we contribute an analytic\nframework for describing inferences about an artifact's social provenance. Via\na series of attribution-elicitation surveys, we offer descriptive evidence that\nthese social inferences: (1) can be studied asynchronously, (2) are not unique\nto a particular sociocultural group or a function of limited data literacy, and\n(3) may influence assessments of trust. Further, we demonstrate (4) how design\nfeatures act in concert with the topic and underlying messages of an artifact's\ndata to give rise to such 'beyond-data' readings. We conclude by discussing the\ndesign and research implications of inferences about social provenance, and why\nwe believe broadening the scope of research on human factors in visualization\nto include sociocultural phenomena can yield actionable design recommendations\nto address urgent challenges in public data communication.", "AI": {"tldr": "This paper explores how visualizations convey social inferences beyond the data they encode, proposing an analytic framework to understand these impressions and their implications for trust and design.", "motivation": "To investigate the socio-indexical function of visualizations and how they communicate beyond explicit data, impacting trust and design choices in public data communication.", "method": "The study employs attribution-elicitation surveys to examine how readers form impressions about the social provenance of visualizations.", "result": "Findings reveal that social inferences from visualizations are not restricted to specific cultural groups, influence trust assessments, and interact with design features and the data's underlying messages.", "conclusion": "Broadening the understanding of visualization to include sociocultural impacts can inform actionable design recommendations for effective public data communication.", "key_contributions": ["Analytic framework for analyzing social inferences in visualizations", "Evidence that social inferences affect trust and are culturally universal", "Insights into the interplay between design features and social messages of visualizations."], "limitations": "", "keywords": ["visualization", "social provenance", "trust", "data communication", "sociocultural phenomena"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.06548", "pdf": "https://arxiv.org/pdf/2508.06548.pdf", "abs": "https://arxiv.org/abs/2508.06548", "title": "Factor Augmented Supervised Learning with Text Embeddings", "authors": ["Zhanye Luo", "Yuefeng Han", "Xiufan Yu"], "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "Large language models (LLMs) generate text embeddings from text data,\nproducing vector representations that capture the semantic meaning and\ncontextual relationships of words. However, the high dimensionality of these\nembeddings often impedes efficiency and drives up computational cost in\ndownstream tasks. To address this, we propose AutoEncoder-Augmented Learning\nwith Text (AEALT), a supervised, factor-augmented framework that incorporates\ndimension reduction directly into pre-trained LLM workflows. First, we extract\nembeddings from text documents; next, we pass them through a supervised\naugmented autoencoder to learn low-dimensional, task-relevant latent factors.\nBy modeling the nonlinear structure of complex embeddings, AEALT outperforms\nconventional deep-learning approaches that rely on raw embeddings. We validate\nits broad applicability with extensive experiments on classification, anomaly\ndetection, and prediction tasks using multiple real-world public datasets.\nNumerical results demonstrate that AEALT yields substantial gains over both\nvanilla embeddings and several standard dimension reduction methods.", "AI": {"tldr": "AEALT is a framework for incorporating dimension reduction into LLM workflows to enhance efficiency and performance in various tasks.", "motivation": "The high dimensionality of LLM-generated embeddings increases computational cost and hinders performance in downstream tasks.", "method": "The framework uses a supervised, augmented autoencoder to learn low-dimensional, task-relevant latent factors from text embeddings.", "result": "AEALT outperforms conventional methods, demonstrating significant improvements across classification, anomaly detection, and prediction tasks.", "conclusion": "AEALT effectively reduces dimensionality while maintaining the semantic integrity of embeddings, leading to better performance in machine learning tasks.", "key_contributions": ["Introduction of the AEALT framework which leverages autoencoders for dimension reduction in LLM workflows.", "Demonstrated improvements in efficiency and performance across multiple real-world tasks using AEALT.", "Validation of AEALT's applicability on diverse datasets through extensive experiments."], "limitations": "", "keywords": ["large language models", "dimension reduction", "autoencoders", "machine learning", "text embeddings"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.06791", "pdf": "https://arxiv.org/pdf/2508.06791.pdf", "abs": "https://arxiv.org/abs/2508.06791", "title": "Entendimento de Campanhas no Contexto da Aten√ß√£o Prim√°ria √† Sa√∫de: Um Processo de Design Socialmente Consciente", "authors": ["De√≥genes P. da Silva Junior", "Jonas Lopes Guerra", "Krissia Menezes", "Marisa Sel Franco", "Roberto Pereira"], "categories": ["cs.HC"], "comment": "62 pages, in Portuguese language, 8 figures", "summary": "This report presents the results of an exploratory analysis of the work\ncontext of Community Health Agents and Endemic Disease Control Agents in\nPrimary Health Care (PHC), with a particular focus on Health Campaigns. To\nunderstand this context, the study adopted the Socially Aware Design framework,\nwhich employs artifacts and techniques to examine problem domains in a\ncomprehensive and sociotechnical manner. Methods such as the Stakeholder\nIdentification Diagram, Evaluation Frame, and Semiotic Framework were applied\nto identify stakeholders, anticipate challenges, and elicit social and\ntechnical requirements for the solution. Personas and Scenarios were also used\nto illustrate the potential impacts of a solution on various stakeholders and\ntheir life contexts within health campaigns. This report presents the analysis\nmethod, its application, and results, discussing the study's findings to inform\nthe development of medium-fidelity prototypes for a PHC health campaign\nmanagement solution.", "AI": {"tldr": "Exploratory analysis of Community Health Agents' work context in Primary Health Care focusing on health campaigns using Socially Aware Design.", "motivation": "To understand the work context of Community Health Agents and Endemic Disease Control Agents in Primary Health Care, particularly regarding health campaigns.", "method": "Utilized the Socially Aware Design framework along with techniques like Stakeholder Identification Diagram, Evaluation Frame, and Semiotic Framework to analyze social and technical requirements.", "result": "Identified stakeholders and potential impacts on their life contexts through personas and scenarios, informing the development of medium-fidelity prototypes.", "conclusion": "The findings provide insights for developing solutions for health campaign management in Primary Health Care.", "key_contributions": ["Application of Socially Aware Design framework in health campaigns", "Identification of social and technical requirements for health agents", "Development of medium-fidelity prototypes for health campaign management"], "limitations": "", "keywords": ["Community Health Agents", "Primary Health Care", "Health Campaigns"], "importance_score": 4, "read_time_minutes": 62}}
{"id": "2508.06583", "pdf": "https://arxiv.org/pdf/2508.06583.pdf", "abs": "https://arxiv.org/abs/2508.06583", "title": "Discerning minds or generic tutors? Evaluating instructional guidance capabilities in Socratic LLMs", "authors": ["Ying Liu", "Can Li", "Ting Zhang", "Mei Wang", "Qiannan Zhu", "Jian Li", "Hua Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The conversational capabilities of large language models hold significant\npromise for enabling scalable and interactive tutoring. While prior research\nhas primarily examined their capacity for Socratic questioning, it often\noverlooks a critical dimension: adaptively guiding learners based on their\ncognitive states. This study shifts focus from mere question generation to the\nbroader instructional guidance capability. We ask: Can LLMs emulate expert\ntutors who dynamically adjust strategies in response to learners'\nunderstanding? To investigate this, we propose GuideEval, a benchmark grounded\nin authentic educational dialogues that evaluates pedagogical guidance through\na three-phase behavioral framework: (1) Perception, inferring learner states;\n(2) Orchestration, adapting instructional strategies; and (3) Elicitation,\nstimulating proper reflections. Empirical findings reveal that existing LLMs\nfrequently fail to provide effective adaptive scaffolding when learners exhibit\nconfusion or require redirection. Furthermore, we introduce a behavior-guided\nfinetuning strategy that leverages behavior-prompted instructional dialogues,\nsignificantly enhancing guidance performance. By shifting the focus from\nisolated content evaluation to learner-centered interaction, our work advocates\na more dialogic paradigm for evaluating Socratic LLMs.", "AI": {"tldr": "This study proposes GuideEval, a benchmark evaluating large language models (LLMs) in adaptive tutoring by focusing on instructional guidance responsive to learners' cognitive states.", "motivation": "To explore how LLMs can emulate expert tutors by adaptively guiding learners based on their cognitive states rather than just generating questions.", "method": "The study introduces GuideEval, a benchmark that uses a three-phase framework: Perception (inferring learner states), Orchestration (adapting strategies), and Elicitation (stimulating reflections).", "result": "Existing LLMs often fail to provide effective adaptive scaffolding. However, a behavior-guided finetuning strategy significantly improves their guidance performance.", "conclusion": "There is a need to shift from content evaluation to a learner-centered interaction paradigm for effective Socratic LLMs.", "key_contributions": ["Introduction of GuideEval as a benchmark for evaluating adaptive pedagogical guidance", "Establishment of a three-phase behavioral framework for instructional strategies", "Development of a behavior-guided finetuning strategy that improves LLM performance in tutoring contexts."], "limitations": "", "keywords": ["Large Language Models", "Adaptive Tutoring", "Pedagogical Guidance", "Cognitive States", "Behavior-Guided Finetuning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.06801", "pdf": "https://arxiv.org/pdf/2508.06801.pdf", "abs": "https://arxiv.org/abs/2508.06801", "title": "Understanding Pedestrian Gesture Misrecognition: Insights from Vision-Language Model Reasoning", "authors": ["Tram Thi Minh Tran", "Xinyan Yu", "Callum Parker", "Julie Stephany Berrio Perez", "Stewart Worrall", "Martin Tomitsch"], "categories": ["cs.HC"], "comment": null, "summary": "Pedestrian gestures play an important role in traffic communication,\nparticularly in interactions with autonomous vehicles (AVs), yet their subtle,\nambiguous, and context-dependent nature poses persistent challenges for machine\ninterpretation. This study investigates these challenges by using GPT-4V, a\nvision-language model, not as a performance benchmark but as a diagnostic tool\nto reveal patterns and causes of gesture misrecognition. We analysed a public\ndataset of pedestrian-vehicle interactions, combining manual video review with\nthematic analysis of the model's qualitative reasoning. This dual approach\nsurfaced recurring factors influencing misrecognition, including gesture\nvisibility, pedestrian behaviour, interaction context, and environmental\nconditions. The findings suggest practical considerations for gesture design,\nincluding the value of salience and contextual redundancy, and highlight\nopportunities to improve AV recognition systems through richer context\nmodelling and uncertainty-aware interpretations. While centred on AV-pedestrian\ninteraction, the method and insights are applicable to other domains where\nmachines interpret human gestures, such as wearable AR and assistive\ntechnologies.", "AI": {"tldr": "This study explores the challenges of machine interpretation of pedestrian gestures in traffic, focusing on interactions with autonomous vehicles (AVs) using GPT-4V as a diagnostic tool.", "motivation": "To address the persistent challenges of machine interpretation of ambiguous and context-dependent pedestrian gestures and improve AV recognition systems.", "method": "Analysis of a public dataset of pedestrian-vehicle interactions through manual video review and thematic analysis of GPT-4V's qualitative reasoning.", "result": "Identified recurring factors influencing gesture misrecognition, such as gesture visibility, pedestrian behavior, interaction context, and environmental conditions.", "conclusion": "Practical considerations for gesture design and opportunities for enhancing AV recognition through better context modeling and uncertainty awareness were highlighted.", "key_contributions": ["Revealed patterns and causes of gesture misrecognition using GPT-4V", "Suggested enhancements for gesture design in traffic contexts", "Proposed insights applicable to other domains involving human gesture interpretation"], "limitations": "", "keywords": ["pedestrian gestures", "autonomous vehicles", "gesture misrecognition", "context modeling", "human-computer interaction"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.06595", "pdf": "https://arxiv.org/pdf/2508.06595.pdf", "abs": "https://arxiv.org/abs/2508.06595", "title": "LLM Unlearning Without an Expert Curated Dataset", "authors": ["Xiaoyuan Zhu", "Muru Zhang", "Ollie Liu", "Robin Jia", "Willie Neiswanger"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Modern large language models often encode sensitive, harmful, or copyrighted\nknowledge, raising the need for post-hoc unlearning-the ability to remove\nspecific domains of knowledge from a model without full retraining. A major\nbottleneck in current unlearning pipelines is constructing effective forget\nsets-datasets that approximate the target domain and guide the model to forget\nit. In this work, we introduce a scalable, automated approach to generate\nhigh-quality forget sets using language models themselves. Our method\nsynthesizes textbook-style data through a structured prompting pipeline,\nrequiring only a domain name as input. Through experiments on unlearning\nbiosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic\ndatasets consistently outperform the baseline synthetic alternatives and are\ncomparable to the expert-curated ones. Additionally, ablation studies reveal\nthat the multi-step generation pipeline significantly boosts data diversity,\nwhich in turn improves unlearning utility. Overall, our findings suggest that\nsynthetic datasets offer a promising path toward practical, scalable unlearning\nfor a wide range of emerging domains without the need for manual intervention.\nWe release our code and dataset at\nhttps://github.com/xyzhu123/Synthetic_Textbook.", "AI": {"tldr": "This paper presents an automated approach for generating high-quality forget sets to facilitate the unlearning of specific knowledge from large language models.", "motivation": "There is a need to remove harmful or sensitive knowledge from language models without full retraining, necessitating effective forget sets.", "method": "The authors propose a method that uses a structured prompting pipeline to synthesize textbook-style data based only on a domain name input.", "result": "Experiments show that the synthetic datasets outperform baseline alternatives and match expert-curated sets in effectiveness for unlearning tasks in various domains.", "conclusion": "Synthetic datasets can enable practical and scalable unlearning without manual intervention, making them a promising solution for emerging domains.", "key_contributions": ["Introduction of an automated method for generating forget sets using language models.", "Demonstration of the superiority of synthetic datasets over baseline methods.", "Release of code and dataset to encourage further research."], "limitations": "", "keywords": ["large language models", "unlearning", "synthetic datasets", "biosecurity", "cybersecurity"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.06826", "pdf": "https://arxiv.org/pdf/2508.06826.pdf", "abs": "https://arxiv.org/abs/2508.06826", "title": "AdjustAR: AI-Driven In-Situ Adjustment of Site-Specific Augmented Reality Content", "authors": ["Nels Numan", "Jessica Van Brummelen", "Ziwen Lu", "Anthony Steed"], "categories": ["cs.HC"], "comment": "4 pages, 1 figure, ACM UIST 2025 Poster", "summary": "Site-specific outdoor AR experiences are typically authored using static 3D\nmodels, but are deployed in physical environments that change over time. As a\nresult, virtual content may become misaligned with its intended real-world\nreferents, degrading user experience and compromising contextual\ninterpretation. We present AdjustAR, a system that supports in-situ correction\nof AR content in dynamic environments using multimodal large language models\n(MLLMs). Given a composite image comprising the originally authored view and\nthe current live user view from the same perspective, an MLLM detects\ncontextual misalignments and proposes revised 2D placements for affected AR\nelements. These corrections are backprojected into 3D space to update the scene\nat runtime. By leveraging MLLMs for visual-semantic reasoning, this approach\nenables automated runtime corrections to maintain alignment with the authored\nintent as real-world target environments evolve.", "AI": {"tldr": "AdjustAR supports in-situ correction of AR content in dynamic environments using multimodal large language models to maintain alignment with authored intent.", "motivation": "Virtual AR content often becomes misaligned with real-world environments, degrading user experience.", "method": "The system uses a composite image of the original and current user view to detect misalignments via MLLMs, and proposes revised placements for AR elements based in 3D space.", "result": "Automated corrections maintain scene alignment in real-time as environments evolve, improving user interaction.", "conclusion": "AdjustAR enhances AR applications by ensuring content remains contextually relevant despite changes in physical environments.", "key_contributions": ["Introduces a system for in-situ AR content correction using MLLMs.", "Employs visual-semantic reasoning for real-time AR updates.", "Backprojects corrections into 3D space for improved user experience."], "limitations": "", "keywords": ["Augmented Reality", "Multimodal Large Language Models", "User Experience", "Real-time corrections", "Contextual alignment"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.06600", "pdf": "https://arxiv.org/pdf/2508.06600.pdf", "abs": "https://arxiv.org/abs/2508.06600", "title": "BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent", "authors": ["Zijian Chen", "Xueguang Ma", "Shengyao Zhuang", "Ping Nie", "Kai Zou", "Andrew Liu", "Joshua Green", "Kshama Patel", "Ruoxi Meng", "Mingyi Su", "Sahel Sharifymoghaddam", "Yanxi Li", "Haoran Hong", "Xinyu Shi", "Xuye Liu", "Nandan Thakur", "Crystina Zhang", "Luyu Gao", "Wenhu Chen", "Jimmy Lin"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Deep-Research agents, which integrate large language models (LLMs) with\nsearch tools, have shown success in improving the effectiveness of handling\ncomplex queries that require iterative search planning and reasoning over\nsearch results. Evaluations on current benchmarks like BrowseComp relies on\nblack-box live web search APIs, have notable limitations in (1) fairness:\ndynamic and opaque web APIs hinder fair comparisons and reproducibility of deep\nresearch methods; (2) transparency: lack of control over the document corpus\nmakes it difficult to isolate retriever contributions. In other words, the\ncurrent evaluations may compare a complete deep research system at a given\ntime, but they do not foster well-controlled experiments to provide insights\ninto the capability of underlying deep research LLMs. To address these\nchallenges, we introduce BrowseComp-Plus, a benchmark derived from BrowseComp,\nemploying a fixed, carefully curated corpus. Each query in BrowseComp-Plus\nincludes human-verified supporting documents and mined challenging negatives,\nenabling controlled experimentation. The benchmark is shown to be effective in\ndistinguishing the performance of deep research systems. For instance, the\nopen-source model Search-R1, when paired with the BM25 retriever, achieves\n3.86% accuracy, whereas the GPT-5 achieves 55.9%. Integrating the GPT-5 with\nthe Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with\nfewer search calls. This benchmark allows comprehensive evaluation and\ndisentangled analysis of deep research agents and retrieval methods, fostering\ninsights into retrieval effectiveness, citation accuracy, and context\nengineering in Deep-Research system.", "AI": {"tldr": "Introduction of BrowseComp-Plus, a benchmark for evaluating Deep-Research agents integrating LLMs with search tools, addressing limitations of current benchmarks.", "motivation": "Current benchmarks like BrowseComp face issues of fairness and transparency due to reliance on dynamic web APIs, hindering effective evaluation of deep research LLMs.", "method": "BrowseComp-Plus introduces a fixed, curated corpus for experiments, including verified supporting documents and challenging negatives for each query.", "result": "BrowseComp-Plus effectively distinguishes performance, with models like Search-R1 achieving 3.86% accuracy and GPT-5 reaching up to 70.1% when integrated with effective retrievers.", "conclusion": "The benchmark allows for a detailed evaluation of deep research agents, offering insights into retrieval effectiveness and performance metrics.", "key_contributions": ["Introduction of BrowseComp-Plus benchmark", "Enhanced evaluation process for deep research LLMs", "Better isolation of retriever contributions in performance analysis"], "limitations": "", "keywords": ["Deep Research", "Large Language Models", "Benchmarking", "Information Retrieval", "HCI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.06846", "pdf": "https://arxiv.org/pdf/2508.06846.pdf", "abs": "https://arxiv.org/abs/2508.06846", "title": "Highlight All the Phrases: Enhancing LLM Transparency through Visual Factuality Indicators", "authors": ["Hyo Jin Do", "Rachel Ostrand", "Werner Geyer", "Keerthiram Murugesan", "Dennis Wei", "Justin Weisz"], "categories": ["cs.HC", "cs.AI"], "comment": "16 pages, 8 figures, To be published in Proceedings of the 8th\n  AAAI/ACM Conference on AI, Ethics, and Society (AIES 2025)", "summary": "Large language models (LLMs) are susceptible to generating inaccurate or\nfalse information, often referred to as \"hallucinations\" or \"confabulations.\"\nWhile several technical advancements have been made to detect hallucinated\ncontent by assessing the factuality of the model's responses, there is still\nlimited research on how to effectively communicate this information to users.\nTo address this gap, we conducted two scenario-based experiments with a total\nof 208 participants to systematically compare the effects of various design\nstrategies for communicating factuality scores by assessing participants'\nratings of trust, ease in validating response accuracy, and preference. Our\nfindings reveal that participants preferred and trusted a design in which all\nphrases within a response were color-coded based on factuality scores.\nParticipants also found it easier to validate accuracy of the response in this\nstyle compared to a baseline with no style applied. Our study offers practical\ndesign guidelines for LLM application developers and designers, aimed at\ncalibrating user trust, aligning with user preferences, and enhancing users'\nability to scrutinize LLM outputs.", "AI": {"tldr": "This paper investigates effective design strategies for communicating factuality scores of LLM responses, revealing that color-coded responses are preferred for enhancing user trust and validation.", "motivation": "Addressing the gap in research on how to effectively communicate hallucinations in LLM outputs to users.", "method": "Two scenario-based experiments with 208 participants comparing design strategies for displaying factuality scores.", "result": "Participants preferred color-coded phrases to indicate factuality, leading to higher trust and easier validation of response accuracy.", "conclusion": "Design guidelines are proposed for LLM developers to improve user trust and the ability to scrutinize model outputs.", "key_contributions": ["Comparative analysis of design strategies for communicating factuality in LLM outputs", "Evidence supporting color-coded responses enhances trust and validation", "Practical design guidelines for LLM application developers"], "limitations": "", "keywords": ["Large Language Models", "Factuality", "Design Strategies", "User Trust", "Hallucinations"], "importance_score": 9, "read_time_minutes": 16}}
{"id": "2508.06621", "pdf": "https://arxiv.org/pdf/2508.06621.pdf", "abs": "https://arxiv.org/abs/2508.06621", "title": "Train It and Forget It: Merge Lists are Unnecessary for BPE Inference in Language Models", "authors": ["Tomohiro Sawada", "Kartik Goyal"], "categories": ["cs.CL"], "comment": "Submitted to EMNLP", "summary": "Standard Byte-Pair Encoding (BPE) tokenization compresses text by pairing a\nlearned token vocabulary with a detailed merge list. Recent work has shown that\nthis merge list exposes a potential attack surface for extracting information\nabout language model's training data. In this paper, we explore the downstream\nimpact of BPE inference algorithms that do not rely on this merge list at all,\nand hence differ from the encoding process during BPE training. To address this\nquestion, we investigate two broad classes of BPE inference schemes that differ\nfrom BPE application during training: a) targeted deviation from merge-lists\nincluding random merge orders, and various corruptions of merge list involving\ndeletion/truncation, and b) non-targeted BPE inference algorithms that do not\ndepend on the merge list but focus on compressing the text either greedily or\nexactly. Extensive experiments across diverse language modeling tasks like\naccuracy-based QA benchmarks, machine translation, and open-ended generation\nreveal that while targeted deviation from the merge lists exhibits significant\ndegradation in language model performance, the non-targeted merge-list-free\ninference algorithms result in minimal impact on downstream performance that is\noften much smaller than expected. These findings pave way for simpler and\npotentially more privacy-preserving tokenization schemes that do not\ncatastrophically compromise model performance.", "AI": {"tldr": "This paper investigates the effects of alternative BPE inference algorithms that do not rely on the traditional merge list and evaluates their impact on language model performance.", "motivation": "To explore and mitigate the privacy risks associated with the standard Byte-Pair Encoding (BPE) approach by analyzing alternative inference methods.", "method": "The authors analyze two classes of BPE inference schemes: targeted deviations from merge lists (including random merge orders and corruptions) and non-targeted BPE methods that do not use merge lists (focusing on greedy or exact compression).", "result": "Experiments show that targeted deviations from merge lists significantly degrade model performance, whereas non-targeted merge-list-free algorithms have minimal impact on downstream performance.", "conclusion": "The study suggests that simpler, privacy-preserving tokenization methods can be developed without severely compromising language model efficacy.", "key_contributions": ["Introduction of alternative BPE inference algorithms without relying on the merge list.", "Empirical evidence demonstrating the performance impact of these alternatives across various language tasks.", "Highlighting potential for more privacy-preserving tokenization approaches."], "limitations": "The study focuses on specific language tasks, which may not generalize to all applications of BPE tokenization.", "keywords": ["Byte-Pair Encoding", "language models", "tokenization", "privacy preservation", "machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.06872", "pdf": "https://arxiv.org/pdf/2508.06872.pdf", "abs": "https://arxiv.org/abs/2508.06872", "title": "Perceiving Slope and Acceleration: Evidence for Variable Tempo Sampling in Pitch-Based Sonification of Functions", "authors": ["Danyang Fan", "Walker Smith", "Takako Fujioka", "Chris Chage", "Sile O'Modhrain", "Diana Deutsch", "Sean Follmer"], "categories": ["cs.HC"], "comment": null, "summary": "Sonification offers a non-visual way to understand data, with pitch-based\nencodings being the most common. Yet, how well people perceive slope and\nacceleration-key features of data trends-remains poorly understood. Drawing on\npeople's natural abilities to perceive tempo, we introduce a novel sampling\nmethod for pitch-based sonification to enhance the perception of slope and\nacceleration in univariate functions. While traditional sonification methods\noften sample data at uniform x-spacing, yielding notes played at a fixed tempo\nwith variable pitch intervals (Variable Pitch Interval), our approach samples\nat uniform y-spacing, producing notes with consistent pitch intervals but\nvariable tempo (Variable Tempo). We conducted psychoacoustic experiments to\nunderstand slope and acceleration perception across three sampling methods:\nVariable Pitch Interval, Variable Tempo, and a Continuous (no sampling)\nbaseline. In slope comparison tasks, Variable Tempo was more accurate than the\nother methods when modulated by the magnitude ratio between slopes. For\nacceleration perception, just-noticeable differences under Variable Tempo were\nover 13 times finer than with other methods. Participants also commonly\nreported higher confidence, lower mental effort, and a stronger preference for\nVariable Tempo compared to other methods. This work contributes models of slope\nand acceleration perception across pitch-based sonification techniques,\nintroduces Variable Tempo as a novel and preferred sampling method, and\nprovides promising initial evidence that leveraging timing can lead to more\nsensitive, accurate, and precise interpretation of derivative-based data\nfeatures.", "AI": {"tldr": "This paper introduces a new sampling method for pitch-based sonification called Variable Tempo, which enhances the perception of slope and acceleration in data trends compared to traditional methods.", "motivation": "To improve the perception of key data trends such as slope and acceleration through an innovative approach to pitch-based sonification.", "method": "The study compares three sampling methods: Variable Pitch Interval, Variable Tempo, and Continuous baseline, through psychoacoustic experiments that measure slope and acceleration perception.", "result": "Variable Tempo outperformed other methods in accuracy for slope tasks and provided much finer just-noticeable differences for acceleration, alongside greater participant confidence and preference.", "conclusion": "Variable Tempo is a novel and effective sampling method for pitch-based sonification, improving the interpretation of derivative-based data features.", "key_contributions": ["Introduced Variable Tempo as a novel sampling method for sonification.", "Demonstrated improved accuracy in slope perception through psychoacoustic experiments.", "Provided initial evidence supporting the benefits of tempo in sonification for enhanced data interpretation."], "limitations": "", "keywords": ["sonification", "pitch-based encoding", "slope perception", "acceleration perception", "psychoacoustic experiments"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.06649", "pdf": "https://arxiv.org/pdf/2508.06649.pdf", "abs": "https://arxiv.org/abs/2508.06649", "title": "Measuring Stereotype and Deviation Biases in Large Language Models", "authors": ["Daniel Wang", "Eli Brignac", "Minjia Mao", "Xiao Fang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are widely applied across diverse domains,\nraising concerns about their limitations and potential risks. In this study, we\ninvestigate two types of bias that LLMs may display: stereotype bias and\ndeviation bias. Stereotype bias refers to when LLMs consistently associate\nspecific traits with a particular demographic group. Deviation bias reflects\nthe disparity between the demographic distributions extracted from\nLLM-generated content and real-world demographic distributions. By asking four\nadvanced LLMs to generate profiles of individuals, we examine the associations\nbetween each demographic group and attributes such as political affiliation,\nreligion, and sexual orientation. Our experimental results show that all\nexamined LLMs exhibit both significant stereotype bias and deviation bias\ntowards multiple groups. Our findings uncover the biases that occur when LLMs\ninfer user attributes and shed light on the potential harms of LLM-generated\noutputs.", "AI": {"tldr": "This study examines stereotype and deviation biases in large language models (LLMs) by analyzing their generated profiles, revealing significant biases towards various demographic groups.", "motivation": "To explore the limitations and risks associated with the biases exhibited by large language models in demographic inferences.", "method": "Four advanced LLMs were tasked with generating profiles of individuals, focusing on associations between demographic groups and attributes like political affiliation, religion, and sexual orientation.", "result": "All examined LLMs showed significant stereotype bias and deviation bias, indicating a substantial gap between LLM-generated demographic distributions and those in reality.", "conclusion": "The study highlights the biases in LLM outputs that can lead to misrepresentations of user attributes, raising concerns about their implications.", "key_contributions": ["Identification of stereotype and deviation biases in LLMs", "Empirical analysis of biases across different demographic groups", "Discussion of potential harms from LLM-generated outputs"], "limitations": "The study may not encompass all demographic attributes and contexts; further research is needed to explore additional biases.", "keywords": ["bias", "large language models", "demographic groups", "stereotype bias", "deviation bias"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.06889", "pdf": "https://arxiv.org/pdf/2508.06889.pdf", "abs": "https://arxiv.org/abs/2508.06889", "title": "Viewpoint-Tolerant Depth Perception for Shared Extended Space Experience on Wall-Sized Display", "authors": ["Dooyoung Kim", "Jinseok Hong", "Heejeong Ko", "Woontack Woo"], "categories": ["cs.HC"], "comment": "11 pages, 5 figures, 3 tables, Accepted in TVCG Special Issue on the\n  2025 IEEE Symposium on Mixed and Augmented Reality (IEEE ISMAR)", "summary": "We proposed viewpoint-tolerant shared depth perception without individual\ntracking by leveraging human cognitive compensation in universally 3D rendered\nimages on a wall-sized display. While traditional 3D perception-enabled display\nsystems have primarily focused on single-user scenarios-adapting rendering\nbased on head and eye tracking the use of wall-sized displays to extend spatial\nexperiences and support perceptually coherent multi-user interactions remains\nunderexplored. We investigated the effects of virtual depths (dv) and absolute\nviewing distance (da) on human cognitive compensation factors (perceived\ndistance difference, viewing angle threshold, and perceived presence) to\nconstruct the wall display-based eXtended Reality (XR) space. Results show that\nparticipants experienced a compelling depth perception even from off-center\nangles of 23 to 37 degrees, and largely increasing virtual depth worsens depth\nperception and presence factors, highlighting the importance of balancing\nextended depth of virtual space and viewing distance from the wall-sized\ndisplay. Drawing on these findings, wall-sized displays in venues such as\nmuseums, galleries, and classrooms can evolve beyond 2D information sharing to\noffer immersive, spatially extended group experiences without individualized\ntracking or wearables.", "AI": {"tldr": "The paper presents a framework for viewpoint-tolerant shared depth perception on wall-sized displays, enhancing multi-user experience without individual tracking.", "motivation": "Exploring immersive multi-user interactions without personalized tracking in 3D displays, particularly in educational spaces.", "method": "Investigating the impact of virtual depths and viewing distances on human cognitive factors related to depth perception.", "result": "Participants perceived compelling depth even from significant off-center angles; however, too much virtual depth negatively impacted presence and perception.", "conclusion": "Well-designed wall-sized displays can enhance group experiences in various public settings without the need for individual tracking technologies.", "key_contributions": ["Introduces viewpoint-tolerant depth perception for multi-users on wall-sized displays", "Analyzes cognitive factors affecting depth perception and presence", "Proposes applications in immersive environments like museums and classrooms"], "limitations": "", "keywords": ["3D perception", "cognitive compensation", "shared depth", "eXtended Reality", "wall-sized displays"], "importance_score": 8, "read_time_minutes": 11}}
{"id": "2508.06665", "pdf": "https://arxiv.org/pdf/2508.06665.pdf", "abs": "https://arxiv.org/abs/2508.06665", "title": "Testing the Limits of Machine Translation from One Book", "authors": ["Jonathan Shaw", "Dillon Mee", "Timothy Khouw", "Zackary Leech", "Daniel Wilson"], "categories": ["cs.CL"], "comment": null, "summary": "Current state-of-the-art models demonstrate capacity to leverage in-context\nlearning to translate into previously unseen language contexts. Tanzer et al.\n[2024] utilize language materials (e.g. a grammar) to improve translation\nquality for Kalamang using large language models (LLMs). We focus on Kanuri, a\nlanguage that, despite having substantial speaker population, has minimal\ndigital resources. We design two datasets for evaluation: one focused on health\nand humanitarian terms, and another containing generalized terminology,\ninvestigating how domain-specific tasks impact LLM translation quality.\n  By providing different combinations of language resources (grammar,\ndictionary, and parallel sentences), we measure LLM translation effectiveness,\ncomparing results to native speaker translations and human linguist\nperformance. We evaluate using both automatic metrics and native speaker\nassessments of fluency and accuracy.\n  Results demonstrate that parallel sentences remain the most effective data\nsource, outperforming other methods in human evaluations and automatic metrics.\nWhile incorporating grammar improves over zero-shot translation, it fails as an\neffective standalone data source. Human evaluations reveal that LLMs achieve\naccuracy (meaning) more effectively than fluency (grammaticality).\n  These findings suggest LLM translation evaluation benefits from\nmultidimensional assessment beyond simple accuracy metrics, and that grammar\nalone, without parallel sentences, does not provide sufficient context for\neffective domain-specific translation.", "AI": {"tldr": "This paper evaluates LLM translation quality for the Kanuri language using various resource combinations and datasets, highlighting the importance of parallel sentences over grammar alone.", "motivation": "The paper addresses the challenge of improving LLM translation quality for Kanuri, a language with limited digital resources, by focusing on health and humanitarian vocabulary.", "method": "Two datasets were designed - one for health and humanitarian terms and another for generalized terminology. Different combinations of language resources were tested, and LLM performance was evaluated against native speaker translations and human linguist performance.", "result": "Results showed that parallel sentences were the most effective data source for improving translation quality, achieving better scores in both human evaluations and automatic metrics compared to grammar and dictionary resources alone.", "conclusion": "LLM translation evaluation should include multidimensional assessments beyond simple accuracy metrics, revealing that grammar alone is insufficient for effective domain-specific translation.", "key_contributions": ["Demonstration of the effectiveness of parallel sentences in LLM translation for Kanuri.", "Highlighting the limitations of grammar as a standalone data source in translation tasks.", "Provision of datasets focused on health and humanitarian terms for future research."], "limitations": "The study primarily focuses on two data sources and may not generalize across other languages or larger datasets.", "keywords": ["machine translation", "large language models", "Kanuri", "health informatics", "language resources"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2508.06955", "pdf": "https://arxiv.org/pdf/2508.06955.pdf", "abs": "https://arxiv.org/abs/2508.06955", "title": "Your Thoughtful Opponent: Embracing Cognitive Conflict with Peer Agent", "authors": ["Kyuwon Kim", "Jaeryeong Hwang", "Younseo Lee", "Jeanhee Lee", "Sung-Eun Kimm", "Hyo-Jeong So"], "categories": ["cs.HC"], "comment": null, "summary": "As complex societal issues continue to emerge, fostering democratic skills\nlike valuing diverse perspectives and collaborative decision-making is\nincreasingly vital in education. In this paper, we propose a Peer Agent (PA)\nsystem designed to simulate a deliberative conversational partner that induces\nsocio-cognitive conflict within dilemma-based game play. Drawing on by the\nInner Thoughts framework and grounded in value-sensitive discourse analysis,\nthe PA actively participates in voice-based multi-party deliberation with human\nplayers. The system architecture consists of five core modules: Context\nInterpreter, Agent State Manager, Thought Generator, Thought Evaluator, and\nThought Articulator.", "AI": {"tldr": "The paper presents a Peer Agent system aimed at enhancing democratic skills through deliberative games, integrating socio-cognitive conflict to promote diverse perspectives in education.", "motivation": "There is a growing need to cultivate democratic skills such as valuing diverse perspectives and collaborative decision-making in education due to the emergence of complex societal issues.", "method": "The Peer Agent system simulates a deliberative conversational partner during dilemma-based gameplay, utilizing the Inner Thoughts framework and value-sensitive discourse analysis, featuring modules for handling context, agent state, thought generation, evaluation, and articulation.", "result": "The system actively engages in voice-based multi-party deliberation, facilitating socio-cognitive conflict among players.", "conclusion": "The Peer Agent system can potentially improve educational practices by fostering skills necessary for democratic engagement through interactive gameplay.", "key_contributions": ["Development of the Peer Agent system for educational environments", "Integration of value-sensitive discourse analysis", "Active engagement in deliberative conversations to enhance democratic skills"], "limitations": "The effectiveness and scalability of the system in diverse educational contexts are not fully addressed.", "keywords": ["Peer Agent", "democratic skills", "deliberative gameplay", "socio-cognitive conflict", "education"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2508.06671", "pdf": "https://arxiv.org/pdf/2508.06671.pdf", "abs": "https://arxiv.org/abs/2508.06671", "title": "Do Biased Models Have Biased Thoughts?", "authors": ["Swati Rajwal", "Shivank Garg", "Reem Abdel-Salam", "Abdelrahman Zayed"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "Accepted at main track of the Second Conference on Language Modeling\n  (COLM 2025)", "summary": "The impressive performance of language models is undeniable. However, the\npresence of biases based on gender, race, socio-economic status, physical\nappearance, and sexual orientation makes the deployment of language models\nchallenging. This paper studies the effect of chain-of-thought prompting, a\nrecent approach that studies the steps followed by the model before it\nresponds, on fairness. More specifically, we ask the following question:\n\\textit{Do biased models have biased thoughts}? To answer our question, we\nconduct experiments on $5$ popular large language models using fairness metrics\nto quantify $11$ different biases in the model's thoughts and output. Our\nresults show that the bias in the thinking steps is not highly correlated with\nthe output bias (less than $0.6$ correlation with a $p$-value smaller than\n$0.001$ in most cases). In other words, unlike human beings, the tested models\nwith biased decisions do not always possess biased thoughts.", "AI": {"tldr": "This paper investigates the relationship between biased outputs and biased thought processes in language models through chain-of-thought prompting.", "motivation": "To address the challenges posed by biases in language models related to gender, race, and other factors.", "method": "Experiments were conducted on 5 popular language models using fairness metrics to quantify 11 different biases in their thoughts and outputs.", "result": "The study found that the correlation between bias in thought processes and bias in outputs is low (<0.6), indicating that models with biased outputs do not necessarily have biased thoughts.", "conclusion": "The results suggest that biased decisions in language models do not stem from biased thinking processes, differing from human reasoning.", "key_contributions": ["Investigation of biases in language models' thought processes", "Empirical analysis using 11 distinct biases", "Insights into the relationship between model outputs and internal reasoning"], "limitations": "Focus on only 5 popular language models; results may not generalize to others.", "keywords": ["Language models", "Bias", "Chain-of-thought prompting", "Fairness metrics", "Machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.07057", "pdf": "https://arxiv.org/pdf/2508.07057.pdf", "abs": "https://arxiv.org/abs/2508.07057", "title": "Rethinking Privacy Indicators in Extended Reality: Multimodal Design for Situationally Impaired Bystanders", "authors": ["Syed Ibrahim Mustafa Shah Bukhari", "Maha Sajid", "Bo Ji", "Brendan David-John"], "categories": ["cs.HC", "cs.CY", "cs.ET"], "comment": null, "summary": "As Extended Reality (XR) devices become increasingly prevalent in everyday\nsettings, they raise significant privacy concerns for bystanders: individuals\nin the vicinity of an XR device during its use, whom the device sensors may\naccidentally capture. Current privacy indicators, such as small LEDs, often\npresume that bystanders are attentive enough to interpret the privacy signals.\nHowever, these cues can be easily overlooked when bystanders are distracted or\nhave limited vision. We define such individuals as situationally impaired\nbystanders. This study explores XR privacy indicator designs that are effective\nfor situationally impaired bystanders. A focus group with eight participants\nwas conducted to design five novel privacy indicators. We evaluated these\ndesigns through a user study with seven additional participants. Our results\nshow that visual-only indicators, typical in commercial XR devices, received\nlow ratings for perceived usefulness in impairment scenarios. In contrast,\nmultimodal indicators were preferred in privacy-sensitive scenarios with\nsituationally impaired bystanders. Ultimately, our results highlight the need\nto move toward adaptable, multimodal, and situationally aware designs that\neffectively support bystander privacy in everyday XR environments.", "AI": {"tldr": "The study investigates privacy indicators for XR devices designed for situationally impaired bystanders, showing that multimodal indicators are preferred over traditional visual-only cues.", "motivation": "To address privacy concerns for situationally impaired bystanders around XR devices, who may not notice current privacy indicators.", "method": "Focus group discussions led to the design of five novel privacy indicators, evaluated through a user study.", "result": "Visual-only indicators were rated low for usefulness, while multimodal indicators were favored in privacy-sensitive situations.", "conclusion": "There is a need for adaptable and multimodal privacy designs to better support bystander privacy in XR environments.", "key_contributions": ["Identification of situationally impaired bystanders in XR contexts.", "Design and evaluation of five novel privacy indicators.", "Demonstration that multimodal indicators enhance privacy perceived usefulness."], "limitations": "Limited sample sizes for focus group and user study.", "keywords": ["Extended Reality", "privacy indicators", "situationally impaired bystanders", "multimodal design", "user study"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2508.06709", "pdf": "https://arxiv.org/pdf/2508.06709.pdf", "abs": "https://arxiv.org/abs/2508.06709", "title": "Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge", "authors": ["Evangelia Spiliopoulou", "Riccardo Fogliato", "Hanna Burnsky", "Tamer Soliman", "Jie Ma", "Graham Horwood", "Miguel Ballesteros"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) can serve as judges that offer rapid and\nreliable assessments of other LLM outputs. However, models may systematically\nassign overly favorable ratings to their own outputs, a phenomenon known as\nself-bias, which can distort evaluations of true model performance. Previous\nstudies often conflate genuine differences in model quality with bias or\nincorrectly assume that evaluations from LLMs and humans follow the same rating\ndistributions. In this work, we present a statistical framework that explicitly\nformalizes assumptions under which self-bias can be identified and estimated.\nOur method models the difference in the scoring distribution that\nLLM-as-a-judge assigns to its own completions compared to other models, while\naccounting for the underlying quality of the completions provided by an\nindependent, third-party judge (e.g., humans). Our method reliably isolates and\nquantifies self-bias, even when models vary in ability, ensuring that genuine\nperformance differences are not mistaken for self-bias. We conduct an empirical\nanalysis of self-bias on a large dataset (>5000 prompt-completion pairs)\nconsisting of expert human annotations and judgments from nine different LLM\njudges. We find that some models, such as GPT-4o and Claude 3.5 Sonnet,\nsystematically assign higher scores to their own outputs. These models also\ndisplay family-bias; systematically assigning higher ratings to outputs\nproduced by other models of the same family. Our findings highlight potential\npitfalls of using LLM judges and offer practical guidance to mitigate biases\nwhen interpreting automated evaluations.", "AI": {"tldr": "This paper presents a framework to identify and estimate self-bias in LLM evaluations, highlighting the risks of inaccurate assessments due to models favoring their own outputs.", "motivation": "The growing reliance on LLMs for evaluating other LLM outputs necessitates a systematic approach to discern genuine performance from inflated ratings due to self-bias.", "method": "A statistical framework is introduced that formalizes the conditions for assessing self-bias by comparing LLM evaluations of their own outputs against those of a third-party judge.", "result": "Empirical analysis reveals that certain models, notably GPT-4o and Claude 3.5 Sonnet, consistently give higher scores to their own outputs, along with a tendency to exhibit family-bias.", "conclusion": "The findings underscore the importance of recognizing self-bias in LLM assessments and provide strategies for mitigating these biases in automated evaluations.", "key_contributions": ["Development of a statistical framework for identifying self-bias in LLMs", "Empirical evidence of self-bias in models like GPT-4o and Claude 3.5 Sonnet", "Guidance on interpreting LLM evaluation results to avoid bias pitfalls."], "limitations": "The framework may require further validation across more diverse models and evaluation tasks.", "keywords": ["self-bias", "large language models", "automated evaluation", "statistical framework", "model performance evaluation"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2508.07058", "pdf": "https://arxiv.org/pdf/2508.07058.pdf", "abs": "https://arxiv.org/abs/2508.07058", "title": "Beyond Problem Solving: Framing and Problem-Solution Co-Evolution in Data Visualization Design", "authors": ["Paul C. Parsons", "Prakash Chandra Shukla"], "categories": ["cs.HC"], "comment": "Author version; article accepted to IEEE VIS 2025; will be published\n  in IEEE TVCG in 2026", "summary": "Visualization design is often described as the process of solving a\nwell-defined problem by navigating a design space. While existing visualization\ndesign models have provided valuable structure and guidance, they tend to\nforeground technical problem-solving and underemphasize the interpretive,\njudgment-based aspects of design. In contrast, research in other design\ndisciplines has emphasized the importance of framing--how designers define and\nredefine what the problem is--and the co-evolution of problem and solution\nspaces through reflective practice. These dimensions remain underexplored in\nvisualization research, particularly from the perspective of expert\npractitioners. This paper investigates how visualization designers frame\nproblems and navigate the dynamic interplay between problem understanding and\nsolution development. We conducted a mixed-methods study with 11 expert\npractitioners using design challenges, diary entries, and semi-structured\ninterviews. Through reflexive thematic analysis, we identified key strategies\nthat participants used to frame problems, reframe them in response to evolving\nconstraints or insights, and build bridges between problem and solution spaces.\nThese included using metaphors, heuristics, sketching, primary generators, and\nreflective evaluation of failed or incomplete ideas. Our findings contribute an\nempirically grounded account of visualization design as a reflective,\nco-evolutionary practice, where framing is not a preliminary step but a\ncontinuous activity embedded in design. Participants often reshaped their\nunderstanding of the problem based on solution attempts, tool feedback, and\nethical or narrative concerns. These insights extend current visualization\ndesign models and highlight the need for frameworks that better account for\nframing and interpretive judgment. (See paper for full abstract.)", "AI": {"tldr": "This paper explores how expert visualization designers frame and navigate the interplay between problem understanding and solution development through a mixed-methods study.", "motivation": "Existing visualization design models focus heavily on technical problem-solving and overlook the interpretive and judgment-based aspects of the design process.", "method": "A mixed-methods study involving 11 expert practitioners, using design challenges, diary entries, and semi-structured interviews, followed by reflexive thematic analysis.", "result": "Identified strategies that practitioners use to frame and reframe problems, including metaphors, heuristics, sketching, and reflective evaluation.", "conclusion": "Visualization design is a continuous co-evolutionary practice where framing is integral, requiring frameworks that emphasize interpretive judgment.", "key_contributions": ["Empirical insights into visualization design as a reflective practice", "Identification of key strategies used by expert designers to frame problems", "Highlighting the importance of framing in design processes"], "limitations": "", "keywords": ["visualization design", "framing", "reflective practice", "design thinking", "human-computer interaction"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2508.06729", "pdf": "https://arxiv.org/pdf/2508.06729.pdf", "abs": "https://arxiv.org/abs/2508.06729", "title": "Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis", "authors": ["Komala Subramanyam Cherukuri", "Pranav Abishai Moses", "Aisa Sakata", "Jiangping Chen", "Haihua Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Oral histories are vital records of lived experience, particularly within\ncommunities affected by systemic injustice and historical erasure. Effective\nand efficient analysis of their oral history archives can promote access and\nunderstanding of the oral histories. However, Large-scale analysis of these\narchives remains limited due to their unstructured format, emotional\ncomplexity, and high annotation costs. This paper presents a scalable framework\nto automate semantic and sentiment annotation for Japanese American\nIncarceration Oral History. Using LLMs, we construct a high-quality dataset,\nevaluate multiple models, and test prompt engineering strategies in\nhistorically sensitive contexts. Our multiphase approach combines expert\nannotation, prompt design, and LLM evaluation with ChatGPT, Llama, and Qwen. We\nlabeled 558 sentences from 15 narrators for sentiment and semantic\nclassification, then evaluated zero-shot, few-shot, and RAG strategies. For\nsemantic classification, ChatGPT achieved the highest F1 score (88.71%),\nfollowed by Llama (84.99%) and Qwen (83.72%). For sentiment analysis, Llama\nslightly outperformed Qwen (82.66%) and ChatGPT (82.29%), with all models\nshowing comparable results. The best prompt configurations were used to\nannotate 92,191 sentences from 1,002 interviews in the JAIOH collection. Our\nfindings show that LLMs can effectively perform semantic and sentiment\nannotation across large oral history collections when guided by well-designed\nprompts. This study provides a reusable annotation pipeline and practical\nguidance for applying LLMs in culturally sensitive archival analysis. By\nbridging archival ethics with scalable NLP techniques, this work lays the\ngroundwork for responsible use of artificial intelligence in digital humanities\nand preservation of collective memory. GitHub:\nhttps://github.com/kc6699c/LLM4OralHistoryAnalysis.", "AI": {"tldr": "This paper presents a scalable framework using LLMs for automating semantic and sentiment annotation of Japanese American incarceration oral histories.", "motivation": "To promote access and understanding of oral histories affected by systemic injustice through efficient analysis.", "method": "The framework combines expert annotation, prompt design, and LLM evaluation using ChatGPT, Llama, and Qwen, with a focus on historically sensitive contexts.", "result": "ChatGPT achieved the highest F1 score for semantic classification (88.71%), while Llama excelled slightly in sentiment analysis. The framework labeled 92,191 sentences from a collection of interviews.", "conclusion": "The study demonstrates that LLMs can effectively annotate large oral history collections with appropriate prompts and provides a reusable annotation pipeline for digital humanities.", "key_contributions": ["Development of an LLM-based annotation pipeline for oral histories", "Evaluation of multiple LLMs in a culturally sensitive context", "Demonstration of the effectiveness of LLMs in semantic and sentiment analysis"], "limitations": "", "keywords": ["oral history", "sentiment analysis", "semantic classification", "LLM", "Digital Humanities"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.07095", "pdf": "https://arxiv.org/pdf/2508.07095.pdf", "abs": "https://arxiv.org/abs/2508.07095", "title": "Hide or Highlight: Understanding the Impact of Factuality Expression on User Trust", "authors": ["Hyo Jin Do", "Werner Geyer"], "categories": ["cs.HC", "cs.AI"], "comment": "17 pages, 3 figures, To be published in Proceedings of the 8th\n  AAAI/ACM Conference on AI, Ethics, and Society (AIES 2025)", "summary": "Large language models are known to produce outputs that are plausible but\nfactually incorrect. To prevent people from making erroneous decisions by\nblindly trusting AI, researchers have explored various ways of communicating\nfactuality estimates in AI-generated outputs to end-users. However, little is\nknown about whether revealing content estimated to be factually incorrect\ninfluences users' trust when compared to hiding it altogether. We tested four\ndifferent ways of disclosing an AI-generated output with factuality\nassessments: transparent (highlights less factual content), attention\n(highlights factual content), opaque (removes less factual content), ambiguity\n(makes less factual content vague), and compared them with a baseline response\nwithout factuality information. We conducted a human subjects research (N =\n148) using the strategies in question-answering scenarios. We found that the\nopaque and ambiguity strategies led to higher trust while maintaining perceived\nanswer quality, compared to the other strategies. We discuss the efficacy of\nhiding presumably less factual content to build end-user trust.", "AI": {"tldr": "The study investigates how disclosing factuality assessments of AI-generated outputs affects user trust, revealing that opaque and ambiguity strategies enhance trust compared to other methods.", "motivation": "To understand how disclosing factuality assessments influences user trust in AI-generated outputs, especially in preventing erroneous decision-making.", "method": "The study tested four disclosure strategies (transparent, attention, opaque, ambiguity) in question-answering scenarios with 148 participants to assess the impact on trust and perceived answer quality.", "result": "Opaque and ambiguity disclosure strategies increased user trust while maintaining perceived answer quality compared to other strategies and a baseline without factuality information.", "conclusion": "Hiding less factual content can effectively build end-user trust in AI systems, suggesting implications for design and communication methods in AI applications.", "key_contributions": ["Identified effective strategies for disclosing AI-generated output factuality assessments that enhance user trust.", "Provided empirical evidence through human subjects research on the impact of different disclosure methods on trust and perceived quality.", "Insight into user behavior regarding AI trust and the efficacy of content disclosure in AI systems."], "limitations": "The study is limited by the sample size and context-specific scenarios, which may not generalize to all AI applications.", "keywords": ["AI-generated outputs", "factuality assessments", "user trust", "human subjects research", "trust strategies"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.06755", "pdf": "https://arxiv.org/pdf/2508.06755.pdf", "abs": "https://arxiv.org/abs/2508.06755", "title": "Many-Turn Jailbreaking", "authors": ["Xianjun Yang", "Liqiang Xiao", "Shiyang Li", "Faisal Ladhak", "Hyokun Yun", "Linda Ruth Petzold", "Yi Xu", "William Yang Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current jailbreaking work on large language models (LLMs) aims to elicit\nunsafe outputs from given prompts. However, it only focuses on single-turn\njailbreaking targeting one specific query. On the contrary, the advanced LLMs\nare designed to handle extremely long contexts and can thus conduct multi-turn\nconversations. So, we propose exploring multi-turn jailbreaking, in which the\njailbroken LLMs are continuously tested on more than the first-turn\nconversation or a single target query. This is an even more serious threat\nbecause 1) it is common for users to continue asking relevant follow-up\nquestions to clarify certain jailbroken details, and 2) it is also possible\nthat the initial round of jailbreaking causes the LLMs to respond to additional\nirrelevant questions consistently. As the first step (First draft done at June\n2024) in exploring multi-turn jailbreaking, we construct a Multi-Turn Jailbreak\nBenchmark (MTJ-Bench) for benchmarking this setting on a series of open- and\nclosed-source models and provide novel insights into this new safety threat. By\nrevealing this new vulnerability, we aim to call for community efforts to build\nsafer LLMs and pave the way for a more in-depth understanding of jailbreaking\nLLMs.", "AI": {"tldr": "This paper introduces the concept of multi-turn jailbreaking in large language models (LLMs), highlighting its risks and establishing the Multi-Turn Jailbreak Benchmark (MTJ-Bench) for evaluation.", "motivation": "To address the insufficient focus on multi-turn jailbreaking in existing research on the safety of LLMs, as current studies mainly concentrate on single-turn queries which are less representative of real user interactions.", "method": "Development of the Multi-Turn Jailbreak Benchmark (MTJ-Bench) to assess and benchmark multi-turn interactions with open- and closed-source LLMs.", "result": "Identification of vulnerabilities in LLMs when subjected to follow-up questions after initial jailbreaking, confirming that initial attacks can lead to successive unsafe outputs.", "conclusion": "The study underlines the need for enhanced safety measures in LLMs and encourages ongoing research efforts to mitigate the threats posed by multi-turn jailbreaking.", "key_contributions": ["Introduction of multi-turn jailbreaking as a significant threat to LLM safety", "Establishment of the Multi-Turn Jailbreak Benchmark (MTJ-Bench) for evaluating multi-turn interactions", "Insights into the implications of successive questioning on LLM outputs"], "limitations": "The study serves as an initial exploration and does not provide comprehensive solutions to the vulnerabilities identified.", "keywords": ["multi-turn jailbreaking", "large language models", "safety threats", "benchmark", "user interactions"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.07129", "pdf": "https://arxiv.org/pdf/2508.07129.pdf", "abs": "https://arxiv.org/abs/2508.07129", "title": "Toward AI Matching Policies in Homeless Services: A Qualitative Study with Policymakers", "authors": ["Caroline M. Johnston", "Olga Koumoundouros", "Angel Hsing-Chi Hwang", "Laura Onasch-Vera", "Eric Rice", "Phebe Vayanos"], "categories": ["cs.HC", "cs.AI"], "comment": "21 pages, 1 figure, 2 tables", "summary": "Artificial intelligence researchers have proposed various data-driven\nalgorithms to improve the processes that match individuals experiencing\nhomelessness to scarce housing resources. It remains unclear whether and how\nthese algorithms are received or adopted by practitioners and what their\ncorresponding consequences are. Through semi-structured interviews with 13\npolicymakers in homeless services in Los Angeles, we investigate whether such\nchange-makers are open to the idea of integrating AI into the housing resource\nmatching process, identifying where they see potential gains and drawbacks from\nsuch a system in issues of efficiency, fairness, and transparency. Our\nqualitative analysis indicates that, even when aware of various complicating\nfactors, policymakers welcome the idea of an AI matching tool if thoughtfully\ndesigned and used in tandem with human decision-makers. Though there is no\nconsensus as to the exact design of such an AI system, insights from\npolicymakers raise open questions and design considerations that can be\nenlightening for future researchers and practitioners who aim to build\nresponsible algorithmic systems to support decision-making in low-resource\nscenarios.", "AI": {"tldr": "The paper explores the reception and adoption of AI algorithms for matching homeless individuals to housing resources through interviews with policymakers in Los Angeles.", "motivation": "To understand how AI algorithms can be integrated into the housing resource matching process and their potential impacts on efficiency, fairness, and transparency.", "method": "Semi-structured interviews with 13 policymakers in homeless services in Los Angeles.", "result": "Policymakers are generally open to integrating AI if it is thoughtfully designed and used alongside human decision-makers, citing potential gains and drawbacks.", "conclusion": "There is no consensus on the design of AI systems, but insights gained can guide future responsible development in low-resource scenarios.", "key_contributions": ["Investigates AI adoption by policymakers in homeless services.", "Identifies design considerations for AI systems in low-resource contexts.", "Highlights views on efficiency, fairness, and transparency in AI applications."], "limitations": "The study is based on a small sample of 13 policymakers, limiting generalizability.", "keywords": ["Artificial Intelligence", "Homelessness", "Policy", "Interviews", "Algorithm Design"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2508.06803", "pdf": "https://arxiv.org/pdf/2508.06803.pdf", "abs": "https://arxiv.org/abs/2508.06803", "title": "SEVADE: Self-Evolving Multi-Agent Analysis with Decoupled Evaluation for Hallucination-Resistant Irony Detection", "authors": ["Ziqi Liu", "Yangbin Chen", "Ziyang Zhou", "Yilin Li", "Mingxuan Hu", "Yushan Pan", "Zhijie Xu"], "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "Sarcasm detection is a crucial yet challenging Natural Language Processing\ntask. Existing Large Language Model methods are often limited by\nsingle-perspective analysis, static reasoning pathways, and a susceptibility to\nhallucination when processing complex ironic rhetoric, which impacts their\naccuracy and reliability. To address these challenges, we propose **SEVADE**, a\nnovel **S**elf-**Ev**olving multi-agent **A**nalysis framework with\n**D**ecoupled **E**valuation for hallucination-resistant sarcasm detection. The\ncore of our framework is a Dynamic Agentive Reasoning Engine (DARE), which\nutilizes a team of specialized agents grounded in linguistic theory to perform\na multifaceted deconstruction of the text and generate a structured reasoning\nchain. Subsequently, a separate lightweight rationale adjudicator (RA) performs\nthe final classification based solely on this reasoning chain. This decoupled\narchitecture is designed to mitigate the risk of hallucination by separating\ncomplex reasoning from the final judgment. Extensive experiments on four\nbenchmark datasets demonstrate that our framework achieves state-of-the-art\nperformance, with average improvements of **6.75%** in Accuracy and **6.29%**\nin Macro-F1 score.", "AI": {"tldr": "SEVADE is a framework for sarcasm detection that improves accuracy and reduces hallucination through a multi-agent reasoning approach.", "motivation": "Sarcasm detection is critical in NLP, and current LLM methods struggle with complexities of ironic rhetoric, leading to inaccuracies.", "method": "SEVADE employs a Dynamic Agentive Reasoning Engine with specialized agents for text analysis and a separate rationale adjudicator for classification.", "result": "The framework showed state-of-the-art performance with an average increase of 6.75% in Accuracy and 6.29% in Macro-F1 score across four datasets.", "conclusion": "SEVADE effectively reduces hallucination and enhances sarcasm detection accuracy through its decoupled reason-and-judge architecture.", "key_contributions": ["Introduction of SEVADE framework for sarcasm detection", "Use of Dynamic Agentive Reasoning Engine for multi-faceted analysis", "Implementation of decoupled evaluation to minimize hallucination risks."], "limitations": "", "keywords": ["sarcasm detection", "natural language processing", "large language models", "multi-agent systems", "hallucination resistance"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.07135", "pdf": "https://arxiv.org/pdf/2508.07135.pdf", "abs": "https://arxiv.org/abs/2508.07135", "title": "Canvas3D: Empowering Precise Spatial Control for Image Generation with Constraints from a 3D Virtual Canvas", "authors": ["Runlin Duan", "Yuzhao Chen", "Rahul Jain", "Yichen Hu", "Jingyu Shi", "Karthik Ramani"], "categories": ["cs.HC"], "comment": null, "summary": "Generative AI (GenAI) has significantly advanced the ease and flexibility of\nimage creation. However, it remains a challenge to precisely control spatial\ncompositions, including object arrangement and scene conditions. To bridge this\ngap, we propose Canvas3D, an interactive system leveraging a 3D engine to\nenable precise spatial manipulation for image generation. Upon user prompt,\nCanvas3D automatically converts textual descriptions into interactive objects\nwithin a 3D engine-driven virtual canvas, empowering direct and precise spatial\nconfiguration. These user-defined arrangements generate explicit spatial\nconstraints that guide generative models in accurately reflecting user\nintentions in the resulting images. We conducted a closed-end comparative study\nbetween Canvas3D and a baseline system. And an open-ended study to evaluate our\nsystem \"in the wild\". The result indicates that Canvas3D outperforms the\nbaseline on spatial control, interactivity, and overall user experience.", "AI": {"tldr": "Canvas3D is an interactive system that enhances image generation by allowing users to manipulate spatial compositions in a 3D environment based on textual descriptions.", "motivation": "The paper addresses the challenge of controlling spatial compositions in image generation, which has been difficult with existing systems.", "method": "Canvas3D leverages a 3D engine to convert textual prompts into interactive objects, allowing users to manipulate the spatial arrangement directly on a virtual canvas.", "result": "Comparative studies show that Canvas3D significantly outperforms a baseline system in terms of spatial control, interactivity, and overall user experience.", "conclusion": "Canvas3D provides a more intuitive and effective means for users to generate images that align with their spatial intentions.", "key_contributions": ["Introduction of an interactive 3D canvas for image generation", "Enhanced spatial control through direct manipulation", "Improved user experience compared to baseline systems"], "limitations": "The study may be limited by sample size and the diversity of use cases evaluated.", "keywords": ["Generative AI", "3D interaction", "image generation", "user experience", "spatial manipulation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.06810", "pdf": "https://arxiv.org/pdf/2508.06810.pdf", "abs": "https://arxiv.org/abs/2508.06810", "title": "Annotating Errors in English Learners' Written Language Production: Advancing Automated Written Feedback Systems", "authors": ["Steven Coyne", "Diana Galvan-Sosa", "Ryan Spring", "Cam√©lia Guerraoui", "Michael Zock", "Keisuke Sakaguchi", "Kentaro Inui"], "categories": ["cs.CL"], "comment": "Pre-review version of DOI 10.1007/978-3-031-98459-4_21, presented at\n  AIED 2025. All content is as of submission time except for de-anonymization,\n  ensuing layout fixes, use of the current code repository link, and BibTeX\n  fixes. Readers are encouraged to refer to the published version", "summary": "Recent advances in natural language processing (NLP) have contributed to the\ndevelopment of automated writing evaluation (AWE) systems that can correct\ngrammatical errors. However, while these systems are effective at improving\ntext, they are not optimally designed for language learning. They favor direct\nrevisions, often with a click-to-fix functionality that can be applied without\nconsidering the reason for the correction. Meanwhile, depending on the error\ntype, learners may benefit most from simple explanations and strategically\nindirect hints, especially on generalizable grammatical rules. To support the\ngeneration of such feedback, we introduce an annotation framework that models\neach error's error type and generalizability. For error type classification, we\nintroduce a typology focused on inferring learners' knowledge gaps by\nconnecting their errors to specific grammatical patterns. Following this\nframework, we collect a dataset of annotated learner errors and corresponding\nhuman-written feedback comments, each labeled as a direct correction or hint.\nWith this data, we evaluate keyword-guided, keyword-free, and template-guided\nmethods of generating feedback using large language models (LLMs). Human\nteachers examined each system's outputs, assessing them on grounds including\nrelevance, factuality, and comprehensibility. We report on the development of\nthe dataset and the comparative performance of the systems investigated.", "AI": {"tldr": "The paper presents an annotation framework to improve automated writing evaluation systems by generating appropriate feedback for language learners, focusing on error types and their generalizability.", "motivation": "Current automated writing evaluation systems excel at correcting grammar but do not effectively support language learning; there's a need for systems that provide explanations and hints instead of direct corrections.", "method": "The authors develop an annotation framework that classifies errors based on type and generalizability. They collect a dataset of learner errors and feedback, and evaluate various feedback generation methods using large language models.", "result": "Different methods for feedback generation are evaluated, with human teachers assessing outputs on relevance, factuality, and comprehensibility.", "conclusion": "The framework improves understanding of learner errors and supports the generation of effective, pedagogically sound feedback for language learners.", "key_contributions": ["Introduction of an annotation framework for error type classification", "Collection of a dataset of annotated learner errors and feedback", "Evaluation of feedback generation methods using LLMs"], "limitations": "Results may vary based on the specific dataset and human evaluator biases.", "keywords": ["automated writing evaluation", "natural language processing", "language learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.07141", "pdf": "https://arxiv.org/pdf/2508.07141.pdf", "abs": "https://arxiv.org/abs/2508.07141", "title": "SketchConcept: Sketching-based Concept Recomposition for Product Design using Generative AI", "authors": ["Runlin Duan", "Chenfei Zhu", "Yuzhao Chen", "Dizhi Ma", "Jingyu Shi", "Ziyi Liu", "Karthik Ramani"], "categories": ["cs.HC"], "comment": null, "summary": "Conceptual product design requires designers to explore the design space of\nvisual and functional concepts simultaneously. Sketching has long been adopted\nto empower concept exploration. However, current sketch-based design tools\nmostly emphasize visual design using emerging techniques. We present\nSketchConcept, a design support tool that decomposes design concepts into\nvisual representations and functionality of concepts using sketches and textual\ndescriptions. We propose a function-to-visual mapping workflow that maps the\nfunction descriptions generated by a Large Language Model to a component of the\nconcept produced by image Generative Artificial Intelligence(GenAI). The\nfunction-to-visual mapping allows our system to leverage multimodal GenAI to\ndecompose, generate, and edit the design concept to satisfy the overall\nfunction and behavior. We present multiple use cases enabled by SketchConcept\nto validate the workflow. Finally, we evaluated the efficacy and usability of\nour system with a two-session user study.", "AI": {"tldr": "SketchConcept is a design support tool that enhances conceptual product design by combining visual sketches and textual descriptions, using a function-to-visual mapping workflow with GenAI.", "motivation": "To improve the exploration of design concepts by integrating both visual representations and functional descriptions in the sketching process, enhancing current sketch-based design tools.", "method": "The system employs a function-to-visual mapping workflow that links descriptions generated by a Large Language Model (LLM) to visual components created by Generative AI (GenAI).", "result": "The tool demonstrates its capabilities through multiple use cases and shows positive results from a user study evaluating its efficacy and usability.", "conclusion": "SketchConcept effectively supports the decomposition, generation, and editing of design concepts, providing a novel approach to conceptual design that integrates multiple modalities.", "key_contributions": ["Introduction of SketchConcept which combines sketches and textual descriptions for design.", "Development of a function-to-visual mapping workflow utilizing LLM and GenAI.", "Successful validation of the tool through use cases and user studies."], "limitations": "", "keywords": ["Sketching", "Product design", "Generative AI", "Human-computer interaction", "Large Language Models"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.06870", "pdf": "https://arxiv.org/pdf/2508.06870.pdf", "abs": "https://arxiv.org/abs/2508.06870", "title": "Text to Speech System for Meitei Mayek Script", "authors": ["Gangular Singh Irengbam", "Nirvash Singh Wahengbam", "Lanthoiba Meitei Khumanthem", "Paikhomba Oinam"], "categories": ["cs.CL", "cs.LG", "cs.SD"], "comment": null, "summary": "This paper presents the development of a Text-to-Speech (TTS) system for the\nManipuri language\n  using the Meitei Mayek script. Leveraging Tacotron 2 and HiFi-GAN, we\nintroduce a neural TTS\n  architecture adapted to support tonal phonology and under-resourced\nlinguistic environments. We\n  develop a phoneme mapping for Meitei Mayek to ARPAbet, curate a\nsingle-speaker dataset, and\n  demonstrate intelligible and natural speech synthesis, validated through\nsubjective and objective\n  metrics. This system lays the groundwork for linguistic preservation and\ntechnological inclusion of\n  Manipuri.", "AI": {"tldr": "Development of a TTS system for Manipuri using Meitei Mayek script, leveraging Tacotron 2 and HiFi-GAN.", "motivation": "To preserve the Manipuri language and enhance technological inclusion in under-resourced linguistic environments.", "method": "Adaptation of Tacotron 2 and HiFi-GAN for TTS, with phoneme mapping from Meitei Mayek to ARPAbet and creation of a single-speaker dataset.", "result": "Achieved intelligible and natural speech synthesis validated through subjective and objective metrics.", "conclusion": "The TTS system aids in linguistic preservation and integration of Manipuri into modern technology.", "key_contributions": ["Introduction of a neural TTS architecture for Manipuri", "Phoneme mapping from Meitei Mayek to ARPAbet", "Creation of a single-speaker dataset for TTS in under-resourced languages."], "limitations": "", "keywords": ["Text-to-Speech", "Manipuri language", "Neural TTS", "Tacotron 2", "HiFi-GAN"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2508.07183", "pdf": "https://arxiv.org/pdf/2508.07183.pdf", "abs": "https://arxiv.org/abs/2508.07183", "title": "Explainability-in-Action: Enabling Expressive Manipulation and Tacit Understanding by Bending Diffusion Models in ComfyUI", "authors": ["Ahmed M. Abuzuraiq", "Philippe Pasquier"], "categories": ["cs.HC", "cs.AI", "cs.LG", "cs.MM", "I.2; J.5"], "comment": "In Proceedings of Explainable AI for the Arts Workshop 2025 (XAIxArts\n  2025) arXiv:2406.14485", "summary": "Explainable AI (XAI) in creative contexts can go beyond transparency to\nsupport artistic engagement, modifiability, and sustained practice. While\ncurated datasets and training human-scale models can offer artists greater\nagency and control, large-scale generative models like text-to-image diffusion\nsystems often obscure these possibilities. We suggest that even large models\ncan be treated as creative materials if their internal structure is exposed and\nmanipulable. We propose a craft-based approach to explainability rooted in\nlong-term, hands-on engagement akin to Sch\\\"on's \"reflection-in-action\" and\ndemonstrate its application through a model-bending and inspection plugin\nintegrated into the node-based interface of ComfyUI. We demonstrate that by\ninteractively manipulating different parts of a generative model, artists can\ndevelop an intuition about how each component influences the output.", "AI": {"tldr": "This paper discusses a craft-based approach to explainable AI (XAI) in creative contexts, emphasizing the interaction between artists and generative models to enhance artistic engagement and understanding.", "motivation": "To explore how explainable AI can enhance creative processes and artistic engagement by allowing artists to interact with and manipulate generative models.", "method": "A craft-based approach was proposed, utilizing a model-bending and inspection plugin integrated into the node-based interface of ComfyUI, which allows for interactive manipulation of generative model components.", "result": "Through interactive manipulation, artists can develop an intuition about the influence of various components of generative models on the outputs they create.", "conclusion": "Large generative models can be treated as creative materials if their internal structures are accessible and manipulable, thus enhancing artists' agency and control.", "key_contributions": ["Proposing a craft-based approach to XAI in creative contexts", "Demonstrating the application of a model-bending plugin for artist interaction", "Highlighting the importance of long-term engagement and reflection-in-action in understanding AI outputs."], "limitations": "", "keywords": ["explainable AI", "creativity", "generative models", "artistic engagement", "human-computer interaction"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.06877", "pdf": "https://arxiv.org/pdf/2508.06877.pdf", "abs": "https://arxiv.org/abs/2508.06877", "title": "ESNERA: Empirical and semantic named entity alignment for named entity dataset merging", "authors": ["Xiaobo Zhang", "Congqing He", "Ying He", "Jian Peng", "Dajie Fu", "Tien-Ping Tan"], "categories": ["cs.CL", "cs.AI"], "comment": "30 pages, 12 figures", "summary": "Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing. It remains a research hotspot due to its wide applicability across\ndomains. Although recent advances in deep learning have significantly improved\nNER performance, they rely heavily on large, high-quality annotated datasets.\nHowever, building these datasets is expensive and time-consuming, posing a\nmajor bottleneck for further research. Current dataset merging approaches\nmainly focus on strategies like manual label mapping or constructing label\ngraphs, which lack interpretability and scalability. To address this, we\npropose an automatic label alignment method based on label similarity. The\nmethod combines empirical and semantic similarities, using a greedy pairwise\nmerging strategy to unify label spaces across different datasets. Experiments\nare conducted in two stages: first, merging three existing NER datasets into a\nunified corpus with minimal impact on NER performance; second, integrating this\ncorpus with a small-scale, self-built dataset in the financial domain. The\nresults show that our method enables effective dataset merging and enhances NER\nperformance in the low-resource financial domain. This study presents an\nefficient, interpretable, and scalable solution for integrating multi-source\nNER corpora.", "AI": {"tldr": "This paper proposes an automatic method for merging NER datasets based on label similarity, enhancing NER performance in low-resource settings.", "motivation": "The current methods for merging NER datasets are limited by lack of interpretability and scalability, which hinders further research due to the high costs and time involved in building high-quality annotated datasets.", "method": "The authors introduce a greedy pairwise merging strategy that aligns labels based on both empirical and semantic similarities, enabling unification of label spaces across different datasets.", "result": "Experiments demonstrate that the proposed method successfully merges three NER datasets into a unified corpus with minimal performance degradation, and further improves NER performance when integrated with a small-scale dataset in the financial domain.", "conclusion": "The proposed solution provides an efficient, interpretable, and scalable approach for integrating multi-source NER corpora, addressing key challenges in the field.", "key_contributions": ["Automatic label alignment method based on label similarity", "Greedy pairwise merging strategy for unifying label spaces", "Demonstrated enhancements in low-resource financial domain NER performance"], "limitations": "The method may still be impacted by the quality and diversity of the original datasets used for merging.", "keywords": ["Named Entity Recognition", "Dataset Merging", "Natural Language Processing", "Label Alignment", "Financial Domain"], "importance_score": 6, "read_time_minutes": 30}}
{"id": "2508.07203", "pdf": "https://arxiv.org/pdf/2508.07203.pdf", "abs": "https://arxiv.org/abs/2508.07203", "title": "Civil Servants as Builders: Enabling Non-IT Staff to Develop Secure Python and R Tools", "authors": ["Prashant Sharma"], "categories": ["cs.HC", "cs.CR", "cs.SE"], "comment": "Post-proceedings paper presented at LIMITS 2025: 11th Workshop on\n  Computing within Limits, 2025-06-26/27, Online", "summary": "Current digital government literature focuses on professional in-house IT\nteams, specialized digital service teams, vendor-developed systems, or\nproprietary low-code/no-code tools. Almost no scholarship addresses a growing\nmiddle ground: technically skilled civil servants outside formal IT roles who\ncan write real code but lack a sanctioned, secure path to deploy their work.\nThis paper introduces a limits-aware, open-source and replicable platform that\nenables such public servants to develop, peer review, and deploy small-scale,\ndomain-specific applications within government networks via a sandboxed,\nauditable workflow. By combining Jupyter Notebooks, preapproved open-source\nlibraries, and lightweight governance, the platform works within institutional\nconstraints such as procurement rules and IT security policies while avoiding\nvendor lock-in. Unlike low/no-code approaches, it preserves and enhances civil\nservants' programming skills, keeping them technically competitive with their\nprivate-sector peers. This contribution fills a critical gap, offering a\nreplicable model for public-sector skill retention, resilience, and bottom-up\ndigital transformation.", "AI": {"tldr": "This paper presents an open-source platform that allows technically skilled civil servants to develop and deploy applications within government networks, addressing a gap in digital government capabilities.", "motivation": "To address the lack of pathways for technically skilled civil servants outside formal IT roles to develop and deploy their applications.", "method": "The paper introduces a platform integrating Jupyter Notebooks, preapproved open-source libraries, and lightweight governance to facilitate application development and deployment in government environments.", "result": "The platform enables civil servants to create domain-specific applications while adhering to institutional constraints such as procurement and security policies.", "conclusion": "This initiative supports public-sector skill retention and enhances digital transformation by empowering civil servants with programming tools and capabilities.", "key_contributions": ["An open-source platform for civil servants to develop applications", "A replicable model for digital transformation in the public sector", "Integration of governance with technical capability preservation"], "limitations": "", "keywords": ["digital government", "civil servants", "open-source platform", "application development", "digital transformation"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.06880", "pdf": "https://arxiv.org/pdf/2508.06880.pdf", "abs": "https://arxiv.org/abs/2508.06880", "title": "The ReQAP System for Question Answering over Personal Information", "authors": ["Philipp Christmann", "Gerhard Weikum"], "categories": ["cs.CL", "cs.IR"], "comment": "Accepted at CIKM 2025 (demonstration paper)", "summary": "Personal information is abundant on users' devices, from structured data in\ncalendar, shopping records or fitness tools, to unstructured contents in mail\nand social media posts. This works presents the ReQAP system that supports\nusers with answers for complex questions that involve filters, joins and\naggregation over heterogeneous sources. The unique trait of ReQAP is that it\nrecursively decomposes questions and incrementally builds an operator tree for\nexecution. Both the question interpretation and the individual operators make\nsmart use of light-weight language models, with judicious fine-tuning. The demo\nshowcases the rich functionality for advanced user questions, and also offers\ndetailed tracking of how the answers are computed by the operators in the\nexecution tree. Being able to trace answers back to the underlying sources is\nvital for human comprehensibility and user trust in the system.", "AI": {"tldr": "The ReQAP system enables users to obtain answers to complex questions by utilizing lightweight language models to process heterogeneous personal information sources, fostering user trust through answer traceability.", "motivation": "To assist users in retrieving answers to complex questions that require integration of diverse personal data types.", "method": "ReQAP recursively decomposes questions and builds an operator tree for execution, using language models for question interpretation and operator execution.", "result": "The system demonstrates advanced functionalities for interpreting and answering complex user questions, along with detailed tracking of answer computation.", "conclusion": "ReQAP enhances comprehensibility and trust by allowing users to trace answers back to their source data.", "key_contributions": ["Introduces a novel operator tree execution model for complex question answering.", "Utilizes lightweight language models effectively for data integration.", "Provides comprehensive tracking of answer computation for user trust."], "limitations": "", "keywords": ["question answering", "human-computer interaction", "language models", "data integration", "user trust"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.07256", "pdf": "https://arxiv.org/pdf/2508.07256.pdf", "abs": "https://arxiv.org/abs/2508.07256", "title": "Exploring Micro Accidents and Driver Responses in Automated Driving: Insights from Real-world Videos", "authors": ["Wei Xiang", "Chuyue Zhang", "Jie Yan"], "categories": ["cs.HC"], "comment": "31 pages, 5 figures, under review", "summary": "Automated driving in level 3 autonomy has been adopted by multiple companies\nsuch as Tesla and BMW, alleviating the burden on drivers while unveiling new\ncomplexities. This article focused on the under-explored territory of micro\naccidents during automated driving, characterized as not fatal but abnormal\naberrations such as abrupt deceleration and snake driving. These micro\naccidents are basic yet pervasive events that might results in more severe\naccidents. Through collecting a comprehensive dataset of user generated video\nrecording such micro accidents in natural driving scenarios, this article\nlocates key variables pertaining to environments and autonomous agents using\nmachine learning methods. Subsequently, crowdsourcing method provides insights\ninto human risk perceptions and reactions to these micro accidents. This\narticle thus describes features of safety critical scenarios other than crashes\nand fatal accidents, informing and potentially advancing the design of\nautomated driving systems.", "AI": {"tldr": "This article examines micro accidents in level 3 automated driving, highlighting their impact on safety and the need for improved automated driving system designs.", "motivation": "To address the increasingly complex safety scenarios in level 3 automated driving caused by micro accidents, which are prevalent yet often overlooked.", "method": "The study collected user-generated video data of micro accidents and utilized machine learning to analyze key environmental variables and agent behaviors. Crowdsourcing was used to gather human risk perceptions and reactions.", "result": "Key variables affecting micro accidents were identified, offering insights into user interpretations of risk, ultimately contributing to safer automated driving design.", "conclusion": "The findings emphasize the need for enhanced understanding and consideration of micro accidents in the development of automated driving systems to prevent more severe incidents.", "key_contributions": ["Identification of micro accident characteristics in automated driving", "Insights from human perception on micro accidents", "Data-driven recommendations for improving automated driving system safety"], "limitations": "Limited by the dataset which may not capture all driving scenarios; reliance on user-generated content could introduce bias.", "keywords": ["automated driving", "micro accidents", "machine learning", "human risk perception", "safety"], "importance_score": 4, "read_time_minutes": 20}}
{"id": "2508.06886", "pdf": "https://arxiv.org/pdf/2508.06886.pdf", "abs": "https://arxiv.org/abs/2508.06886", "title": "Score Before You Speak: Improving Persona Consistency in Dialogue Generation using Response Quality Scores", "authors": ["Arpita Saggar", "Jonathan C. Darling", "Vania Dimitrova", "Duygu Sarikaya", "David C. Hogg"], "categories": ["cs.CL"], "comment": "Camera-Ready version for ECAI 2025. 8 pages", "summary": "Persona-based dialogue generation is an important milestone towards building\nconversational artificial intelligence. Despite the ever-improving capabilities\nof large language models (LLMs), effectively integrating persona fidelity in\nconversations remains challenging due to the limited diversity in existing\ndialogue data. We propose a novel framework SBS (Score-Before-Speaking), which\noutperforms previous methods and yields improvements for both million and\nbillion-parameter models. Unlike previous methods, SBS unifies the learning of\nresponses and their relative quality into a single step. The key innovation is\nto train a dialogue model to correlate augmented responses with a quality score\nduring training and then leverage this knowledge at inference. We use\nnoun-based substitution for augmentation and semantic similarity-based scores\nas a proxy for response quality. Through extensive experiments with benchmark\ndatasets (PERSONA-CHAT and ConvAI2), we show that score-conditioned training\nallows existing models to better capture a spectrum of persona-consistent\ndialogues. Our ablation studies also demonstrate that including scores in the\ninput prompt during training is superior to conventional training setups. Code\nand further details are available at\nhttps://arpita2512.github.io/score_before_you_speak", "AI": {"tldr": "This paper introduces a new framework called Score-Before-Speaking (SBS) for improving persona-based dialogue generation in conversational AI, which enhances response quality by correlating augmented responses with quality scores during training.", "motivation": "The integration of persona fidelity in conversations is hindered by the lack of diversity in existing dialogue data, which this paper aims to address.", "method": "The SBS framework unifies the learning of responses and their quality into a single training step, using noun-based substitution for augmentation and semantic similarity-based scores to evaluate response quality.", "result": "The proposed framework outperforms previous methods in both million and billion-parameter models, as shown in experiments with benchmark datasets like PERSONA-CHAT and ConvAI2.", "conclusion": "Score-conditioned training significantly improves the model's ability to generate persona-consistent dialogues, and the method outperforms traditional setups where scores are not included in training.", "key_contributions": ["Introduction of the SBS framework for persona-based dialogue generation.", "Demonstration of superior performance through score-conditioned training in existing dialogue models.", "Innovative use of noun-based substitution and semantic similarity for quality scoring."], "limitations": "", "keywords": ["persona-based dialogue", "large language models", "dialogue generation", "response quality", "score-conditioned training"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.07283", "pdf": "https://arxiv.org/pdf/2508.07283.pdf", "abs": "https://arxiv.org/abs/2508.07283", "title": "Fine-Tuning Large Language Models Using EEG Microstate Features for Mental Workload Assessment", "authors": ["Bujar Raufi"], "categories": ["cs.HC", "cs.AI", "eess.SP", "q-bio.NC", "97R40", "I.2"], "comment": "17 Pages, 7 figures, 3 tables and one prompt template", "summary": "This study explores the intersection of electroencephalography (EEG)\nmicrostates and Large Language Models (LLMs) to enhance the assessment of\ncognitive load states. By utilizing EEG microstate features, the research aims\nto fine-tune LLMs for improved predictions of distinct cognitive states,\nspecifically 'Rest' and 'Load'. The experimental design is delineated in four\ncomprehensive stages: dataset collection and preprocessing, microstate\nsegmentation and EEG backfitting, feature extraction paired with prompt\nengineering, and meticulous LLM model selection and refinement. Employing a\nsupervised learning paradigm, the LLM is trained to identify cognitive load\nstates based on EEG microstate features integrated into prompts, producing\naccurate discrimination of cognitive load. A curated dataset, linking EEG\nfeatures to specified cognitive load conditions, underpins the experimental\nframework. The results indicate a significant improvement in model performance\nfollowing the proposed fine-tuning, showcasing the potential of EEG-informed\nLLMs in cognitive neuroscience and cognitive AI applications. This approach not\nonly contributes to the understanding of brain dynamics but also paves the way\nfor advancements in machine learning techniques applicable to cognitive load\nand cognitive AI research.", "AI": {"tldr": "This study investigates the enhancement of cognitive load assessment using EEG microstates and fine-tuning of Large Language Models (LLMs).", "motivation": "To improve the assessment of cognitive load states by utilizing EEG microstate features in LLMs.", "method": "The research employs a four-stage experimental design including dataset collection, microstate segmentation, feature extraction, and LLM selection and refinement within a supervised learning framework.", "result": "The fine-tuned LLM showed a significant improvement in model performance in discriminating between 'Rest' and 'Load' cognitive states using EEG features.", "conclusion": "This approach shows the potential for integrating EEG-informed LLMs in cognitive neuroscience and may advance machine learning in cognitive AI research.", "key_contributions": ["Integration of EEG microstate features into LLMs for cognitive load assessment", "Demonstrated improved predictions of cognitive states based on EEG data", "Provided a refined methodology for training LLMs using brain dynamics."], "limitations": "", "keywords": ["EEG", "cognitive load", "Large Language Models", "microstates", "machine learning"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2508.06913", "pdf": "https://arxiv.org/pdf/2508.06913.pdf", "abs": "https://arxiv.org/abs/2508.06913", "title": "Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection", "authors": ["Siyuan Li", "Xi Lin", "Guangyan Li", "Zehao Liu", "Aodu Wulianghai", "Li Ding", "Jun Wu", "Jianhua Li"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has resulted in\nincreasingly sophisticated AI-generated content, posing significant challenges\nin distinguishing LLM-generated text from human-written language. Existing\ndetection methods, primarily based on lexical heuristics or fine-tuned\nclassifiers, often suffer from limited generalizability and are vulnerable to\nparaphrasing, adversarial perturbations, and cross-domain shifts. In this work,\nwe propose SentiDetect, a model-agnostic framework for detecting LLM-generated\ntext by analyzing the divergence in sentiment distribution stability. Our\nmethod is motivated by the empirical observation that LLM outputs tend to\nexhibit emotionally consistent patterns, whereas human-written texts display\ngreater emotional variability. To capture this phenomenon, we define two\ncomplementary metrics: sentiment distribution consistency and sentiment\ndistribution preservation, which quantify stability under sentiment-altering\nand semantic-preserving transformations. We evaluate SentiDetect on five\ndiverse datasets and a range of advanced LLMs,including Gemini-1.5-Pro,\nClaude-3, GPT-4-0613, and LLaMa-3.3. Experimental results demonstrate its\nsuperiority over state-of-the-art baselines, with over 16% and 11% F1 score\nimprovements on Gemini-1.5-Pro and GPT-4-0613, respectively. Moreover,\nSentiDetect also shows greater robustness to paraphrasing, adversarial attacks,\nand text length variations, outperforming existing detectors in challenging\nscenarios.", "AI": {"tldr": "The paper introduces SentiDetect, a model-agnostic framework that detects LLM-generated text by analyzing sentiment distribution stability, showing improvements over existing methods.", "motivation": "The need to distinguish increasingly sophisticated AI-generated content from human-written text is critical due to challenges with existing detection methods.", "method": "SentiDetect evaluates sentiment distribution consistency and preservation under various transformations to identify LLM outputs.", "result": "SentiDetect outperformed state-of-the-art methods by achieving significant improvements in F1 scores across multiple advanced LLMs, demonstrating greater robustness to attacks and variations.", "conclusion": "SentiDetect presents a novel approach to LLM-generated text detection with improved performance and robustness.", "key_contributions": ["Introduction of sentiment distribution metrics for detecting LLM-generated text", "Model-agnostic framework applicable to various LLMs", "Demonstrated superior performance compared to existing detection methods"], "limitations": "", "keywords": ["large language models", "text detection", "sentiment analysis", "AI-generated content", "machine learning"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2508.07301", "pdf": "https://arxiv.org/pdf/2508.07301.pdf", "abs": "https://arxiv.org/abs/2508.07301", "title": "In-person, Online and Back Again -- A Tale of Three Hybrid Hackathons", "authors": ["Abasi-amefon Obot Affia-Jomants", "Alexander Serebrenik", "James D. Herbsleb", "Alexander Nolte"], "categories": ["cs.HC", "cs.CY"], "comment": "Accepted in Proceedings of the ACM on Human Computer Interaction\n  (CSCW'25)", "summary": "Hybrid hackathons, which combine in-person and online participation, present\nunique challenges for organizers and participants. Although such events are\nincreasingly conducted, research on them remains fragmented, with limited\nintegration between hackathon studies and hybrid collaboration. Existing\nstrategies for in-person or online-only events often fail to address the unique\nchallenges of hybrid formats, such as managing communication across physical\nand virtual spaces. Our work addresses this gap by examining how hybrid\nhackathons function, analyzing how organizers structure these events and how\nparticipants navigate hybrid-specific challenges. Drawing on established\ntheories of hybrid collaboration, we examine key dimensions - synchronicity,\nphysical distribution, dynamic transitions, and technological infrastructure -\nthat shape collaboration in hybrid events. Through an exploratory case study of\nthree hackathon events, we analyze how these dimensions are implemented and\ntheir effects on participant experiences. Our findings reveal differing\norganizer considerations of the hybrid dimensions in the hackathon design,\nleading to distinct experiences for participants. Implementation styles -\nfavoring in-person, online, or balanced participation - led to varied\nparticipant experiences, affecting access to resources, communication, and team\ncoordination. Organizers in our study also relied on technology to bridge\nhybrid interactions, but overlooked critical aspects like time-zone management,\ndynamic transitions, and targeted support for hybrid teams. Additionally,\nparticipants in their teams responded to gaps in event scaffolding by adapting\ncollaboration strategies, revealing gaps in organizers' preparedness for hybrid\nevents. Learning from our findings, we offer practical recommendations when\norganizing hybrid hackathon events and recommendations to participants when\nattending them.", "AI": {"tldr": "This paper examines hybrid hackathons, focusing on how they function and the unique challenges faced by organizers and participants. It analyses several key dimensions that impact collaboration in these events through case studies.", "motivation": "The study addresses the growing trend of hybrid hackathons and the lack of integrated research on challenges unique to these formats.", "method": "The research utilizes exploratory case studies of three hybrid hackathon events to analyze how specific hybrid collaboration dimensions are implemented and experienced.", "result": "Findings reveal varying organizer approaches to hybrid formats, influencing participant experiences related to resource access, communication, and teamwork.", "conclusion": "The study offers practical recommendations for organizing and participating in hybrid hackathons, emphasizing the need for attention to critical aspects like time-zone management and targeted support for hybrid teams.", "key_contributions": ["Provides insights on the functioning of hybrid hackathons", "Identifies key dimensions affecting collaboration in hybrid settings", "Offers practical recommendations for organizers and participants"], "limitations": "", "keywords": ["hybrid hackathons", "collaboration", "participant experience", "event organization", "technology in hackathons"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.06971", "pdf": "https://arxiv.org/pdf/2508.06971.pdf", "abs": "https://arxiv.org/abs/2508.06971", "title": "Two-Stage Quranic QA via Ensemble Retrieval and Instruction-Tuned Answer Extraction", "authors": ["Mohamed Basem", "Islam Oshallah", "Ali Hamdi", "Khaled Shaban", "Hozaifa Kassab"], "categories": ["cs.CL", "cs.IR"], "comment": "8 pages , 4 figures , Accepted in Aiccsa 2025 ,\n  https://conferences.sigappfr.org/aiccsa2025/", "summary": "Quranic Question Answering presents unique challenges due to the linguistic\ncomplexity of Classical Arabic and the semantic richness of religious texts. In\nthis paper, we propose a novel two-stage framework that addresses both passage\nretrieval and answer extraction. For passage retrieval, we ensemble fine-tuned\nArabic language models to achieve superior ranking performance. For answer\nextraction, we employ instruction-tuned large language models with few-shot\nprompting to overcome the limitations of fine-tuning on small datasets. Our\napproach achieves state-of-the-art results on the Quran QA 2023 Shared Task,\nwith a MAP@10 of 0.3128 and MRR@10 of 0.5763 for retrieval, and a pAP@10 of\n0.669 for extraction, substantially outperforming previous methods. These\nresults demonstrate that combining model ensembling and instruction-tuned\nlanguage models effectively addresses the challenges of low-resource question\nanswering in specialized domains.", "AI": {"tldr": "A two-stage framework for Quranic Question Answering uses ensemble Arabic models for passage retrieval and instruction-tuned LLMs for answer extraction, achieving state-of-the-art results on the Quran QA 2023 Shared Task.", "motivation": "To address the challenges of linguistic complexity and semantic richness in Classical Arabic texts for question answering.", "method": "An ensemble of fine-tuned Arabic language models for passage retrieval, and instruction-tuned large language models with few-shot prompting for answer extraction.", "result": "State-of-the-art results with MAP@10 of 0.3128, MRR@10 of 0.5763 for retrieval, and pAP@10 of 0.669 for extraction.", "conclusion": "Combining model ensembling and instruction-tuned models effectively addresses low-resource question answering in specialized domains.", "key_contributions": ["Novel two-stage framework for Quranic Question Answering", "Ensemble of Arabic language models for improved passage retrieval", "Use of instruction-tuned LLMs for effective answer extraction"], "limitations": "", "keywords": ["Quranic Question Answering", "Arabic language models", "Instruction-tuned language models", "Question answering", "Low-resource"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2508.07390", "pdf": "https://arxiv.org/pdf/2508.07390.pdf", "abs": "https://arxiv.org/abs/2508.07390", "title": "Urbanite: A Dataflow-Based Framework for Human-AI Interactive Alignment in Urban Visual Analytics", "authors": ["Gustavo Moreira", "Leonardo Ferreira", "Carolina Veiga", "Maryam Hosseini", "Fabio Miranda"], "categories": ["cs.HC", "cs.AI"], "comment": "Accepted at IEEE VIS 2025. Urbanite is available at\n  https://urbantk.org/urbanite", "summary": "With the growing availability of urban data and the increasing complexity of\nsocietal challenges, visual analytics has become essential for deriving\ninsights into pressing real-world problems. However, analyzing such data is\ninherently complex and iterative, requiring expertise across multiple domains.\nThe need to manage diverse datasets, distill intricate workflows, and integrate\nvarious analytical methods presents a high barrier to entry, especially for\nresearchers and urban experts who lack proficiency in data management, machine\nlearning, and visualization. Advancements in large language models offer a\npromising solution to lower the barriers to the construction of analytics\nsystems by enabling users to specify intent rather than define precise\ncomputational operations. However, this shift from explicit operations to\nintent-based interaction introduces challenges in ensuring alignment throughout\nthe design and development process. Without proper mechanisms, gaps can emerge\nbetween user intent, system behavior, and analytical outcomes. To address these\nchallenges, we propose Urbanite, a framework for human-AI collaboration in\nurban visual analytics. Urbanite leverages a dataflow-based model that allows\nusers to specify intent at multiple scopes, enabling interactive alignment\nacross the specification, process, and evaluation stages of urban analytics.\nBased on findings from a survey to uncover challenges, Urbanite incorporates\nfeatures to facilitate explainability, multi-resolution definition of tasks\nacross dataflows, nodes, and parameters, while supporting the provenance of\ninteractions. We demonstrate Urbanite's effectiveness through usage scenarios\ncreated in collaboration with urban experts. Urbanite is available at\nhttps://urbantk.org/urbanite.", "AI": {"tldr": "Urbanite is a framework for human-AI collaboration in urban visual analytics, enabling users to specify intent and facilitating alignment between user goals and analytical outcomes.", "motivation": "The complexity of analyzing urban data and societal challenges requires expertise in various domains, presenting barriers for researchers and urban experts in managing datasets and deriving insights.", "method": "Urbanite uses a dataflow-based model that allows users to specify intent at multiple levels of granularity, enabling interactive alignment throughout the urban analytics process.", "result": "Urbanite features support explainability, multi-resolution task definitions, and the provenance of interactions, improving collaboration between urban experts and the system based on user surveys identifying challenges.", "conclusion": "Urbanite effectively addresses the challenges of aligning user intent and system behavior, enhancing the usability of urban visual analytics.", "key_contributions": ["Framework for human-AI collaboration in urban analytics", "Dataflow-based model for specifying user intent", "Enhanced explainability and task definition support"], "limitations": "", "keywords": ["urban analytics", "human-AI collaboration", "visualization", "large language models", "dataflow"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.06974", "pdf": "https://arxiv.org/pdf/2508.06974.pdf", "abs": "https://arxiv.org/abs/2508.06974", "title": "Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models", "authors": ["Zhijun Tu", "Hanting Chen", "Siqi Liu", "Chuanjian Liu", "Jian Li", "Jie Hu", "Yunhe Wang"], "categories": ["cs.CL"], "comment": "16 pages, 5 figures", "summary": "1-bit LLM quantization offers significant advantages in reducing storage and\ncomputational costs. However, existing methods typically train 1-bit LLMs from\nscratch, failing to fully leverage pre-trained models. This results in high\ntraining costs and notable accuracy degradation. We identify that the large gap\nbetween full precision and 1-bit representations makes direct adaptation\ndifficult. In this paper, we introduce a consistent progressive training for\nboth forward and backward, smoothly converting the floating-point weights into\nthe binarized ones. Additionally, we incorporate binary-aware initialization\nand dual-scaling compensation to reduce the difficulty of progressive training\nand improve the performance. Experimental results on LLMs of various sizes\ndemonstrate that our method outperforms existing approaches. Our results show\nthat high-performance 1-bit LLMs can be achieved using pre-trained models,\neliminating the need for expensive training from scratch.", "AI": {"tldr": "This paper presents a novel method for 1-bit LLM quantization that leverages pre-trained models, reducing training costs and improving accuracy through consistent progressive training and binary-aware techniques.", "motivation": "To address the challenges of training 1-bit LLMs from scratch and leverage pre-trained models for better efficiency and performance.", "method": "The authors propose a consistent progressive training approach that adapts both forward and backward, alongside techniques like binary-aware initialization and dual-scaling compensation.", "result": "Experimental results demonstrate that the proposed method achieves higher performance in 1-bit LLMs compared to existing methods, effectively utilizing pre-trained models.", "conclusion": "By eliminating the need for scratch training, the proposed method significantly reduces costs while improving accuracy in quantized models.", "key_contributions": ["Introduction of consistent progressive training for 1-bit LLMs", "Incorporation of binary-aware initialization techniques", "Demonstration of high-performance outcomes using pre-trained models"], "limitations": "", "keywords": ["1-bit quantization", "LLMs", "progressive training"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.07496", "pdf": "https://arxiv.org/pdf/2508.07496.pdf", "abs": "https://arxiv.org/abs/2508.07496", "title": "StreetWeave: A Declarative Grammar for Street-Overlaid Visualization of Multivariate Data", "authors": ["Sanjana Srabanti", "G. Elisabeta Marai", "Fabio Miranda"], "categories": ["cs.HC", "cs.CY"], "comment": "Accepted at IEEE VIS 2025. StreetWeave is available at\n  https://urbantk.org/streetweave", "summary": "The visualization and analysis of street and pedestrian networks are\nimportant to various domain experts, including urban planners, climate\nresearchers, and health experts. This has led to the development of new\ntechniques for street and pedestrian network visualization, expanding how data\ncan be shown and understood more effectively. Despite their increasing\nadoption, there is no established design framework to guide the creation of\nthese visualizations while addressing the diverse requirements of various\ndomains. When exploring a feature of interest, domain experts often need to\ntransform, integrate, and visualize a combination of thematic data (e.g.,\ndemographic, socioeconomic, pollution) and physical data (e.g., zip codes,\nstreet networks), often spanning multiple spatial and temporal scales. This not\nonly complicates the process of visual data exploration and system\nimplementation for developers but also creates significant entry barriers for\nexperts who lack a background in programming. With this in mind, in this paper,\nwe reviewed 45 studies utilizing street-overlaid visualizations to understand\nhow they are used. Through qualitative coding of these visualizations, we\nanalyzed three key aspects of street and pedestrian network visualization\nusage: the analytical purpose they serve, the visualization approaches\nemployed, and the data sources used in their creation. Building on this design\nspace, we introduce StreetWeave, a declarative grammar for designing custom\nvisualizations of multivariate spatial network data across multiple\nresolutions. We demonstrate how StreetWeave can be used to create various\nstreet-overlaid visualizations, enabling effective exploration and analysis of\nspatial data. StreetWeave is available at https://urbantk.org/streetweave.", "AI": {"tldr": "The paper introduces StreetWeave, a framework for creating custom visualizations of street and pedestrian networks to aid various domain experts in data analysis.", "motivation": "The need for effective visualization and analysis of street and pedestrian networks for urban planners, climate researchers, and health experts.", "method": "Review and qualitative coding of 45 studies on street-overlaid visualizations to identify usage patterns and introduce a new framework for visualization creation.", "result": "StreetWeave allows for the generation of custom visualizations, aiding in the effective exploration of complex spatial data.", "conclusion": "The introduction of StreetWeave fills a gap in the visualization design framework, enhancing accessibility for non-programmers and improving data exploration processes.", "key_contributions": ["Introduction of StreetWeave as a unique visualization grammar for spatial network data.", "Insights from analyzing 45 studies on street-overlaid visualizations.", "Framework that addresses diverse expert requirements for visual analysis."], "limitations": "", "keywords": ["street visualization", "pedestrian networks", "data analysis", "urban planning", "visualization frameworks"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.07017", "pdf": "https://arxiv.org/pdf/2508.07017.pdf", "abs": "https://arxiv.org/abs/2508.07017", "title": "Vec2Summ: Text Summarization via Probabilistic Sentence Embeddings", "authors": ["Mao Li", "Fred Conrad", "Johann Gagnon-Bartsch"], "categories": ["cs.CL"], "comment": null, "summary": "We propose Vec2Summ, a novel method for abstractive summarization that frames\nthe task as semantic compression. Vec2Summ represents a document collection\nusing a single mean vector in the semantic embedding space, capturing the\ncentral meaning of the corpus. To reconstruct fluent summaries, we perform\nembedding inversion -- decoding this mean vector into natural language using a\ngenerative language model. To improve reconstruction quality and capture some\ndegree of topical variability, we introduce stochasticity by sampling from a\nGaussian distribution centered on the mean. This approach is loosely analogous\nto bagging in ensemble learning, where controlled randomness encourages more\nrobust and varied outputs. Vec2Summ addresses key limitations of LLM-based\nsummarization methods. It avoids context-length constraints, enables\ninterpretable and controllable generation via semantic parameters, and scales\nefficiently with corpus size -- requiring only $O(d + d^2)$ parameters.\nEmpirical results show that Vec2Summ produces coherent summaries for topically\nfocused, order-invariant corpora, with performance comparable to direct LLM\nsummarization in terms of thematic coverage and efficiency, albeit with less\nfine-grained detail. These results underscore Vec2Summ's potential in settings\nwhere scalability, semantic control, and corpus-level abstraction are\nprioritized.", "AI": {"tldr": "Vec2Summ is a new abstractive summarization method that uses a mean vector in the semantic embedding space to generate summaries while maintaining semantic control.", "motivation": "To address limitations of LLM-based summarization methods, particularly context-length constraints and the need for scalable, interpretable generation.", "method": "Vec2Summ represents document collections as a mean vector in semantic space and uses stochasticity from a Gaussian distribution to enhance summary generation.", "result": "Vec2Summ generates coherent summaries comparable to LLM summarization, particularly in thematically focused and scalable settings, though with less detail.", "conclusion": "Vec2Summ shows significant promise for tasks requiring semantic control and corpus-level abstraction in summarization, paving the way for more robust applications.", "key_contributions": ["Introduction of the Vec2Summ method for semantic compression in summarization", "Use of embedding inversion for generating natural language summaries", "Incorporation of stochasticity for enhanced output variability"], "limitations": "Less fine-grained detail in summaries compared to traditional LLM-based methods.", "keywords": ["abstractive summarization", "semantic compression", "generative language model", "embedding inversion", "scalability"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.07497", "pdf": "https://arxiv.org/pdf/2508.07497.pdf", "abs": "https://arxiv.org/abs/2508.07497", "title": "VA-Blueprint: Uncovering Building Blocks for Visual Analytics System Design", "authors": ["Leonardo Ferreira", "Gustavo Moreira", "Fabio Miranda"], "categories": ["cs.HC", "cs.AI"], "comment": "Accepted at IEEE VIS 2025. VA-Blueprint is available at\n  https://urbantk.org/va-blueprint", "summary": "Designing and building visual analytics (VA) systems is a complex, iterative\nprocess that requires the seamless integration of data processing, analytics\ncapabilities, and visualization techniques. While prior research has\nextensively examined the social and collaborative aspects of VA system\nauthoring, the practical challenges of developing these systems remain\nunderexplored. As a result, despite the growing number of VA systems, there are\nonly a few structured knowledge bases to guide their design and development. To\ntackle this gap, we propose VA-Blueprint, a methodology and knowledge base that\nsystematically reviews and categorizes the fundamental building blocks of urban\nVA systems, a domain particularly rich and representative due to its intricate\ndata and unique problem sets. Applying this methodology to an initial set of 20\nsystems, we identify and organize their core components into a multi-level\nstructure, forming an initial knowledge base with a structured blueprint for VA\nsystem development. To scale this effort, we leverage a large language model to\nautomate the extraction of these components for other 81 papers (completing a\ncorpus of 101 papers), assessing its effectiveness in scaling knowledge base\nconstruction. We evaluate our method through interviews with experts and a\nquantitative analysis of annotation metrics. Our contributions provide a deeper\nunderstanding of VA systems' composition and establish a practical foundation\nto support more structured, reproducible, and efficient system development.\nVA-Blueprint is available at https://urbantk.org/va-blueprint.", "AI": {"tldr": "The paper introduces VA-Blueprint, a systematic approach to designing urban visual analytics (VA) systems, which organizes their core components and automates knowledge extraction using a large language model.", "motivation": "To address the lack of structured knowledge in the design and development of visual analytics systems, particularly urban VA systems, which are complex and underexplored.", "method": "The authors propose a methodology, VA-Blueprint, that categorizes building blocks of VA systems and applies it to 20 initial systems. They enhance knowledge base construction by using a large language model to extract components from 81 other papers.", "result": "The methodology results in a multi-level structured knowledge base for VA system development, validated by expert interviews and quantitative analysis metrics.", "conclusion": "The contributions provide a deeper understanding of the composition of VA systems and lay a practical foundation for structured and efficient development processes, with VA-Blueprint accessible online.", "key_contributions": ["Introduction of VA-Blueprint methodology", "Development of a multi-level structured knowledge base", "Automation of component extraction using LLMs"], "limitations": "", "keywords": ["visual analytics", "methodology", "knowledge base", "urban computing", "large language models"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2508.07069", "pdf": "https://arxiv.org/pdf/2508.07069.pdf", "abs": "https://arxiv.org/abs/2508.07069", "title": "SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages", "authors": ["Muhammad Dehan Al Kautsar", "Aswin Candra", "Muhammad Alif Al Hakim", "Maxalmina Satria Kahfi", "Fajri Koto", "Alham Fikri Aji", "Peerat Limkonchotiwat", "Ekapol Chuangsuwanich", "Genta Indra Winata"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Although numerous datasets have been developed to support dialogue systems,\nmost existing chit-chat datasets overlook the cultural nuances inherent in\nnatural human conversations. To address this gap, we introduce SEADialogues, a\nculturally grounded dialogue dataset centered on Southeast Asia, a region with\nover 700 million people and immense cultural diversity. Our dataset features\ndialogues in eight languages from six Southeast Asian countries, many of which\nare low-resource despite having sizable speaker populations. To enhance\ncultural relevance and personalization, each dialogue includes persona\nattributes and two culturally grounded topics that reflect everyday life in the\nrespective communities. Furthermore, we release a multi-turn dialogue dataset\nto advance research on culturally aware and human-centric large language\nmodels, including conversational dialogue agents.", "AI": {"tldr": "SEADialogues is a culturally grounded dialogue dataset aimed at improving the shortcomings of existing chit-chat datasets, focusing on Southeast Asia's cultural diversity.", "motivation": "To address the lack of cultural nuance in existing chit-chat datasets for dialogue systems, particularly in relation to Southeast Asia's diversity.", "method": "The dataset includes dialogues in eight languages from six Southeast Asian countries, integrating persona attributes and culturally relevant topics to enhance cultural relevance and personalization.", "result": "The dataset facilitates research on culturally aware and human-centric large language models and conversational dialogue agents.", "conclusion": "SEADialogues can significantly improve dialogue systems by incorporating cultural elements and diversity, addressing a key gap in current research.", "key_contributions": ["Introduction of a new dataset focused on Southeast Asia's cultural diversity.", "Inclusion of persona attributes and culturally grounded topics in dialogues.", "Support for multiple low-resource languages."], "limitations": "", "keywords": ["cultural diversity", "dialogue dataset", "Southeast Asia"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2508.07520", "pdf": "https://arxiv.org/pdf/2508.07520.pdf", "abs": "https://arxiv.org/abs/2508.07520", "title": "Conversational DNA: A New Visual Language for Understanding Dialogue Structure in Human and AI", "authors": ["Baihan Lin"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "What if the patterns hidden within dialogue reveal more about communication\nthan the words themselves? We introduce Conversational DNA, a novel visual\nlanguage that treats any dialogue -- whether between humans, between human and\nAI, or among groups -- as a living system with interpretable structure that can\nbe visualized, compared, and understood. Unlike traditional conversation\nanalysis that reduces rich interaction to statistical summaries, our approach\nreveals the temporal architecture of dialogue through biological metaphors.\nLinguistic complexity flows through strand thickness, emotional trajectories\ncascade through color gradients, conversational relevance forms through\nconnecting elements, and topic coherence maintains structural integrity through\nhelical patterns. Through exploratory analysis of therapeutic conversations and\nhistorically significant human-AI dialogues, we demonstrate how this\nvisualization approach reveals interaction patterns that traditional methods\nmiss. Our work contributes a new creative framework for understanding\ncommunication that bridges data visualization, human-computer interaction, and\nthe fundamental question of what makes dialogue meaningful in an age where\nhumans increasingly converse with artificial minds.", "AI": {"tldr": "A novel visual language, Conversational DNA, is introduced to analyze dialogue structure and patterns beyond mere words, revealing insights into communication through visualization.", "motivation": "To explore deeper insights into communication by revealing hidden patterns within dialogue that traditional methods overlook.", "method": "The paper employs a visualization framework that depicts dialogue as a living system, utilizing biological metaphors to illustrate linguistic complexity, emotional trajectories, and conversational relevance.", "result": "The analysis of therapeutic conversations and significant human-AI dialogues showcases the effectiveness of the visualization in identifying interaction patterns that traditional analysis fails to capture.", "conclusion": "The study presents a creative framework for understanding communication that integrates data visualization and human-computer interaction, offering insights into meaningful dialogue in an AI-focused era.", "key_contributions": ["Introduction of Conversational DNA as a visual language for dialogue analysis.", "Demonstration of the impact of visualization on understanding conversational patterns.", "Bridging data visualization and HCI in the context of AI interactions."], "limitations": "", "keywords": ["Conversational DNA", "dialogue analysis", "human-computer interaction", "data visualization", "AI communication"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.07090", "pdf": "https://arxiv.org/pdf/2508.07090.pdf", "abs": "https://arxiv.org/abs/2508.07090", "title": "BharatBBQ: A Multilingual Bias Benchmark for Question Answering in the Indian Context", "authors": ["Aditya Tomar", "Nihar Ranjan Sahoo", "Pushpak Bhattacharyya"], "categories": ["cs.CL"], "comment": null, "summary": "Evaluating social biases in language models (LMs) is crucial for ensuring\nfairness and minimizing the reinforcement of harmful stereotypes in AI systems.\nExisting benchmarks, such as the Bias Benchmark for Question Answering (BBQ),\nprimarily focus on Western contexts, limiting their applicability to the Indian\ncontext. To address this gap, we introduce BharatBBQ, a culturally adapted\nbenchmark designed to assess biases in Hindi, English, Marathi, Bengali, Tamil,\nTelugu, Odia, and Assamese. BharatBBQ covers 13 social categories, including 3\nintersectional groups, reflecting prevalent biases in the Indian sociocultural\nlandscape. Our dataset contains 49,108 examples in one language that are\nexpanded using translation and verification to 392,864 examples in eight\ndifferent languages. We evaluate five multilingual LM families across zero and\nfew-shot settings, analyzing their bias and stereotypical bias scores. Our\nfindings highlight persistent biases across languages and social categories and\noften amplified biases in Indian languages compared to English, demonstrating\nthe necessity of linguistically and culturally grounded benchmarks for bias\nevaluation.", "AI": {"tldr": "The paper introduces BharatBBQ, a benchmark for evaluating social biases in multilingual language models specifically in the Indian context, addressing the inadequacies of existing biases benchmarks.", "motivation": "To ensure fairness and reduce harmful stereotypes in AI, especially in the context of Indian social and linguistic diversity.", "method": "The authors created BharatBBQ, a dataset covering 13 social categories in eight languages, expanding from 49,108 examples to 392,864 through translation, and evaluated five multilingual language models for bias assessment.", "result": "The study revealed persistent and often amplified biases across languages, emphasizing the need for culturally grounded benchmarks in bias evaluation.", "conclusion": "BharatBBQ highlights the importance of considering cultural nuances when assessing biases in language models, particularly for Indian languages, to prevent the reinforcement of stereotypes.", "key_contributions": ["Introduction of BharatBBQ for bias evaluation in Indian context", "Development of a large multilingual dataset covering multiple social categories", "Analysis of bias in five multilingual language models with significant findings on amplified biases."], "limitations": "", "keywords": ["bias evaluation", "language models", "social categories", "multilingual", "Indian languages"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.07576", "pdf": "https://arxiv.org/pdf/2508.07576.pdf", "abs": "https://arxiv.org/abs/2508.07576", "title": "Phoenix: A Novel Context-Aware Voice-Powered Math Equation Workspace and Editor", "authors": ["Kenneth Ge", "Ryan Paul", "Priscilla Zhang", "JooYoung Seo"], "categories": ["cs.HC"], "comment": "Published at ASSETS '25", "summary": "Writing mathematical notation requires substantial effort, diverting\ncognitive resources from conceptual understanding to documentation mechanics,\nsignificantly impacting individuals with fine motor disabilities (FMDs).\nCurrent limits of speech-based math technologies rely on precise dictation of\nmath symbols and unintuitive command-based interfaces. We present a novel\nvoice-powered math workspace, applying neuroscience insights to create an\nintuitive problem-solving environment. To minimize cognitive load, we leverage\nlarge language models with our novel context engine to support natural language\ninteraction. Ultimately, we enable fluid mathematical engagement for\nindividuals with FMDs -- freed from mechanical constraints.", "AI": {"tldr": "A novel voice-powered math workspace leverages large language models to assist individuals with fine motor disabilities (FMDs) in engaging with mathematical notation intuitively, reducing cognitive load and mechanical constraints.", "motivation": "Current speech-based math technologies are limited by the need for precise dictation of symbols and command-based interfaces, which can hinder those with fine motor disabilities.", "method": "The study introduces a voice-powered math workspace that incorporates insights from neuroscience and utilizes large language models with a novel context engine for natural language interaction.", "result": "The workspace allows users with FMDs to engage fluidly with mathematical problems, significantly enhancing their ability to solve math problems without mechanical constraints.", "conclusion": "By applying this approach, individuals with fine motor disabilities can focus more on conceptual understanding than on documentation mechanics.", "key_contributions": ["Introduction of a voice-powered math workspace for FMDs", "Integration of neuroscience insights with large language models", "Reduction of cognitive load through natural language interactions"], "limitations": "", "keywords": ["voice-powered workspace", "fine motor disabilities", "large language models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.07101", "pdf": "https://arxiv.org/pdf/2508.07101.pdf", "abs": "https://arxiv.org/abs/2508.07101", "title": "Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning", "authors": ["Lijie Yang", "Zhihao Zhang", "Arti Jain", "Shijie Cao", "Baihong Yuan", "Yiwei Chen", "Zhihao Jia", "Ravi Netravali"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large reasoning models achieve strong performance through test-time scaling\nbut incur substantial computational overhead, particularly from excessive token\ngeneration when processing short input prompts. While sparse attention\nmechanisms can reduce latency and memory usage, existing approaches suffer from\nsignificant accuracy degradation due to accumulated errors during\nlong-generation reasoning. These methods generally require either high token\nretention rates or expensive retraining. We introduce LessIsMore, a\ntraining-free sparse attention mechanism for reasoning tasks, which leverages\nglobal attention patterns rather than relying on traditional head-specific\nlocal optimizations. LessIsMore aggregates token selections from local\nattention heads with recent contextual information, enabling unified cross-head\ntoken ranking for future decoding layers. This unified selection improves\ngeneralization and efficiency by avoiding the need to maintain separate token\nsubsets per head. Evaluation across diverse reasoning tasks and benchmarks\nshows that LessIsMore preserves -- and in some cases improves -- accuracy while\nachieving a $1.1\\times$ average decoding speed-up compared to full attention.\nMoreover, LessIsMore attends to $2\\times$ fewer tokens without accuracy loss,\nachieving a $1.13\\times$ end-to-end speed-up compared to existing sparse\nattention methods.", "AI": {"tldr": "LessIsMore is a training-free sparse attention mechanism that improves reasoning task efficiency without sacrificing accuracy.", "motivation": "Large reasoning models suffer from computational overhead due to excessive token generation during short input processing; existing sparse attention methods face accuracy issues and high retraining costs.", "method": "LessIsMore utilizes global attention patterns for token selection across local attention heads, enabling unified cross-head token ranking instead of head-specific optimizations.", "result": "The method shows improved accuracy and a $1.1\\times$ average decoding speed-up, attending to $2\\times$ fewer tokens compared to full attention methods without loss of accuracy.", "conclusion": "LessIsMore achieves enhanced generalization and efficiency, demonstrating a $1.13\\times$ end-to-end speed-up compared to other sparse attention approaches.", "key_contributions": ["Introduction of a training-free mechanism for sparse attention in reasoning models", "Unified cross-head token ranking for efficiency", "Improvement in both accuracy and speed compared to existing methods"], "limitations": "", "keywords": ["Sparse Attention", "Reasoning Tasks", "Efficiency", "Token Selection", "Machine Learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.07617", "pdf": "https://arxiv.org/pdf/2508.07617.pdf", "abs": "https://arxiv.org/abs/2508.07617", "title": "On the Limits of Selective AI Prediction: A Case Study in Clinical Decision Making", "authors": ["Sarah Jabbour", "David Fouhey", "Nikola Banovic", "Stephanie D. Shepard", "Ella Kazerooni", "Michael W. Sjoding", "Jenna Wiens"], "categories": ["cs.HC", "cs.AI"], "comment": "14 pages, 10 figures, 5 tables", "summary": "AI has the potential to augment human decision making. However, even\nhigh-performing models can produce inaccurate predictions when deployed. These\ninaccuracies, combined with automation bias, where humans overrely on AI\npredictions, can result in worse decisions. Selective prediction, in which\npotentially unreliable model predictions are hidden from users, has been\nproposed as a solution. This approach assumes that when AI abstains and informs\nthe user so, humans make decisions as they would without AI involvement. To\ntest this assumption, we study the effects of selective prediction on human\ndecisions in a clinical context. We conducted a user study of 259 clinicians\ntasked with diagnosing and treating hospitalized patients. We compared their\nbaseline performance without any AI involvement to their AI-assisted accuracy\nwith and without selective prediction. Our findings indicate that selective\nprediction mitigates the negative effects of inaccurate AI in terms of decision\naccuracy. Compared to no AI assistance, clinician accuracy declined when shown\ninaccurate AI predictions (66% [95% CI: 56%-75%] vs. 56% [95% CI: 46%-66%]),\nbut recovered under selective prediction (64% [95% CI: 54%-73%]). However,\nwhile selective prediction nearly maintains overall accuracy, our results\nsuggest that it alters patterns of mistakes: when informed the AI abstains,\nclinicians underdiagnose (18% increase in missed diagnoses) and undertreat (35%\nincrease in missed treatments) compared to no AI input at all. Our findings\nunderscore the importance of empirically validating assumptions about how\nhumans engage with AI within human-AI systems.", "AI": {"tldr": "The paper explores the impact of selective prediction in AI-assisted clinical decision-making, revealing its potential to improve clinician accuracy while also altering error patterns.", "motivation": "To investigate how selective prediction can mitigate the negative effects of inaccurate AI predictions in clinical decision-making contexts.", "method": "A user study was conducted with 259 clinicians tasked with diagnosing and treating hospitalized patients, comparing their performance with and without AI assistance and under selective prediction.", "result": "Selective prediction improved clinician accuracy compared to incorrect AI predictions but led to a notable increase in missed diagnoses and treatments.", "conclusion": "Selective prediction can maintain decision accuracy in AI-assisted environments but requires attention to its influence on clinicians' decision-making processes.", "key_contributions": ["Demonstrated the effectiveness of selective prediction in clinical decision-making", "Highlighted the unintended consequences of AI abstention in clinical contexts", "Provided empirical evidence supporting the need to validate human engagement assumptions in AI systems"], "limitations": "The study is limited to a clinical context and may not generalize to other domains; further research is needed on long-term effects of selective prediction.", "keywords": ["AI", "selective prediction", "clinical decision-making", "human-AI interaction", "automation bias"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.07111", "pdf": "https://arxiv.org/pdf/2508.07111.pdf", "abs": "https://arxiv.org/abs/2508.07111", "title": "Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution", "authors": ["Falaah Arif Khan", "Nivedha Sivakumar", "Yinong Oliver Wang", "Katherine Metcalf", "Cezanne Camacho", "Barry-John Theobald", "Luca Zappella", "Nicholas Apostoloff"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have achieved impressive performance, leading to\ntheir widespread adoption as decision-support tools in resource-constrained\ncontexts like hiring and admissions. There is, however, scientific consensus\nthat AI systems can reflect and exacerbate societal biases, raising concerns\nabout identity-based harm when used in critical social contexts. Prior work has\nlaid a solid foundation for assessing bias in LLMs by evaluating demographic\ndisparities in different language reasoning tasks. In this work, we extend\nsingle-axis fairness evaluations to examine intersectional bias, recognizing\nthat when multiple axes of discrimination intersect, they create distinct\npatterns of disadvantage. We create a new benchmark called WinoIdentity by\naugmenting the WinoBias dataset with 25 demographic markers across 10\nattributes, including age, nationality, and race, intersected with binary\ngender, yielding 245,700 prompts to evaluate 50 distinct bias patterns.\nFocusing on harms of omission due to underrepresentation, we investigate bias\nthrough the lens of uncertainty and propose a group (un)fairness metric called\nCoreference Confidence Disparity which measures whether models are more or less\nconfident for some intersectional identities than others. We evaluate five\nrecently published LLMs and find confidence disparities as high as 40% along\nvarious demographic attributes including body type, sexual orientation and\nsocio-economic status, with models being most uncertain about\ndoubly-disadvantaged identities in anti-stereotypical settings. Surprisingly,\ncoreference confidence decreases even for hegemonic or privileged markers,\nindicating that the recent impressive performance of LLMs is more likely due to\nmemorization than logical reasoning. Notably, these are two independent\nfailures in value alignment and validity that can compound to cause social\nharm.", "AI": {"tldr": "This paper extends fairness evaluations of large language models (LLMs) to address intersectional bias, creating a new benchmark for assessing multiple demographic attributes and their impact on model certainty and decision-making.", "motivation": "To address concerns about identity-based harm due to biases in LLMs used in hiring and admissions, and to recognize the distinct patterns of disadvantage that arise from intersectionality.", "method": "Development of the WinoIdentity benchmark by augmenting the WinoBias dataset. Assessment of 245,700 prompts to evaluate bias patterns, combined with a group (un)fairness metric called Coreference Confidence Disparity to measure confidence disparities among intersectional identities.", "result": "Evaluation of five LLMs revealed confidence disparities up to 40% across various demographic attributes such as body type and sexual orientation, with increased uncertainty for doubly-disadvantaged identities.", "conclusion": "Performance issues indicate that LLMs may rely more on memorization rather than logical reasoning, highlighting failures in both value alignment and validity that can lead to social harms.", "key_contributions": ["Introduction of the WinoIdentity benchmark for intersectional bias assessment", "Development of the Coreference Confidence Disparity metric for measuring confidence disparities", "Discovery of significant confidence disparities in LLMs based on demographic attributes"], "limitations": "", "keywords": ["large language models", "intersectional bias", "WinoIdentity benchmark", "coreference confidence disparity", "fairness evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.07620", "pdf": "https://arxiv.org/pdf/2508.07620.pdf", "abs": "https://arxiv.org/abs/2508.07620", "title": "Are UX evaluation methods truly accessible", "authors": ["Andr√©s Eduardo Fuentes-Cort√°zar", "Alejandra Rivera-Hern√°ndez", "Jos√© Rafael Rojano-C√°ceres"], "categories": ["cs.HC", "H.5.2"], "comment": "24 pages, 8 figures, 8 tables, submitted to TecnoL\\'ogicas ISNN\n  0123-7799", "summary": "Providing an equitable and inclusive user experience (UX) for people with\ndisabilities (PWD) is a central goal of accessible design. In the specific case\nof Deaf users, whose hearing impairments impact language development and\ncommunication, it is essential to consider their specific needs during software\nevaluation processes. This study aimed to analyze a set of UX evaluation\nmethods suggested in the literature as suitable for Deaf individuals, with the\ngoal of validating their level of accessibility in real-world contexts. The\nresearch was based on a critical review and practical application of these\nmethods, identifying their strengths and limitations in relation to the\ninteraction, perception, and comprehension of Deaf users. Traditional\nevaluation instruments, commonly designed for hearing individuals, pose\nsignificant barriers when applied to Deaf users due to their re-liance on\nauditory and cognitive abilities, as well as the lack of consideration for\ncommu-nicational accessibility. The results show that although these methods\nare frequently rec-ommended, they exhibit critical shortcomings that hinder the\ncollection of accurate and representative data. It is concluded that it is\nessential to adapt UX evaluation methods to ensure genuinely accessible\nprocesses that address the communicative and cognitive needs of the Deaf\ncommunity and accurately reflect their user experience.", "AI": {"tldr": "This study analyzes UX evaluation methods for Deaf users, identifying adaptations needed to improve accessibility and representation in user experience processes.", "motivation": "To provide an equitable and inclusive UX for people with disabilities, particularly Deaf individuals whose communication needs are often overlooked in traditional evaluation methods.", "method": "A critical review and practical application of recommended UX evaluation methods specific to Deaf users, assessing their strengths and limitations in real-world contexts.", "result": "The study finds that conventional UX evaluation methods, designed for hearing users, have significant shortcomings when applied to Deaf individuals, hindering accurate data collection.", "conclusion": "Adapting UX evaluation methods is essential for accurately reflecting the user experiences of Deaf users and ensuring communicational accessibility.", "key_contributions": ["Identification of critical shortcomings in traditional UX evaluation methods for Deaf users.", "Recommendations for adapting evaluation methods to meet the needs of Deaf individuals.", "Validation of accessibility in real-world UX evaluation contexts for Deaf users."], "limitations": "The study primarily focuses on existing methods without proposing new methodologies to fill the identified gaps.", "keywords": ["User Experience", "Accessibility", "Deaf Users", "UX Evaluation", "Inclusive Design"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2508.07143", "pdf": "https://arxiv.org/pdf/2508.07143.pdf", "abs": "https://arxiv.org/abs/2508.07143", "title": "Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens", "authors": ["Anna Seo Gyeong Choi", "Hoon Choi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Automatic Speech Recognition (ASR) systems now mediate countless\nhuman-technology interactions, yet research on their fairness implications\nremains surprisingly limited. This paper examines ASR bias through a\nphilosophical lens, arguing that systematic misrecognition of certain speech\nvarieties constitutes more than a technical limitation -- it represents a form\nof disrespect that compounds historical injustices against marginalized\nlinguistic communities. We distinguish between morally neutral classification\n(discriminate1) and harmful discrimination (discriminate2), demonstrating how\nASR systems can inadvertently transform the former into the latter when they\nconsistently misrecognize non-standard dialects. We identify three unique\nethical dimensions of speech technologies that differentiate ASR bias from\nother algorithmic fairness concerns: the temporal burden placed on speakers of\nnon-standard varieties (\"temporal taxation\"), the disruption of conversational\nflow when systems misrecognize speech, and the fundamental connection between\nspeech patterns and personal/cultural identity. These factors create asymmetric\npower relationships that existing technical fairness metrics fail to capture.\nThe paper analyzes the tension between linguistic standardization and pluralism\nin ASR development, arguing that current approaches often embed and reinforce\nproblematic language ideologies. We conclude that addressing ASR bias requires\nmore than technical interventions; it demands recognition of diverse speech\nvarieties as legitimate forms of expression worthy of technological\naccommodation. This philosophical reframing offers new pathways for developing\nASR systems that respect linguistic diversity and speaker autonomy.", "AI": {"tldr": "This paper explores the ethical implications of bias in Automatic Speech Recognition (ASR) systems, arguing that misrecognition of non-standard dialects is a form of disrespect that exacerbates historical injustices against marginalized communities.", "motivation": "The research addresses the limited understanding of fairness in Automatic Speech Recognition (ASR) systems and aims to highlight the social implications of ASR bias.", "method": "The paper uses a philosophical approach to analyze the ethical dimensions of speech technologies and the impact of ASR misrecognition on users, particularly those who speak non-standard dialects.", "result": "It identifies three ethical dimensions of ASR bias: temporal taxation on speakers, disruption of conversational flow, and the connection between speech and cultural identity, arguing these issues create power imbalances.", "conclusion": "Addressing ASR bias requires more than technical fixes; it necessitates recognizing diverse speech varieties as legitimate and accommodating them within technological design.", "key_contributions": ["Introduces a philosophical perspective on ASR bias and fairness.", "Identifies unique ethical dimensions of ASR technology.", "Argues for the recognition of diverse speech as essential for respectful technology design."], "limitations": "", "keywords": ["Automatic Speech Recognition", "ethics", "bias", "linguistic diversity", "fairness"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.07658", "pdf": "https://arxiv.org/pdf/2508.07658.pdf", "abs": "https://arxiv.org/abs/2508.07658", "title": "Through Their Eyes: User Perceptions on Sensitive Attribute Inference of Social Media Videos by Visual Language Models", "authors": ["Shuning Zhang", "Gengrui Zhang", "Yibo Meng", "Ziyi Zhang", "Hantao Zhao", "Xin Yi", "Hewu Li"], "categories": ["cs.HC"], "comment": null, "summary": "The rapid advancement of Visual Language Models (VLMs) has enabled\nsophisticated analysis of visual content, leading to concerns about the\ninference of sensitive user attributes and subsequent privacy risks. While\ntechnical capabilities of VLMs are increasingly studied, users' understanding,\nperceptions, and reactions to these inferences remain less explored, especially\nconcerning videos uploaded on the social media. This paper addresses this gap\nthrough a semi-structured interview (N=17), investigating user perspectives on\nVLM-driven sensitive attribute inference from their visual data. Findings\nreveal that users perceive VLMs as capable of inferring a range of attributes,\nincluding location, demographics, and socioeconomic indicators, often with\nunsettling accuracy. Key concerns include unauthorized identification, misuse\nof personal information, pervasive surveillance, and harm from inaccurate\ninferences. Participants reported employing various mitigation strategies,\nthough with skepticism about their ultimate effectiveness against advanced AI.\nUsers also articulate clear expectations for platforms and regulators,\nemphasizing the need for enhanced transparency, user control, and proactive\nprivacy safeguards. These insights are crucial for guiding the development of\nresponsible AI systems, effective privacy-enhancing technologies, and informed\npolicymaking that aligns with user expectations and societal values.", "AI": {"tldr": "This paper investigates user perceptions of Visual Language Models (VLMs) regarding the inference of sensitive attributes from visual data, highlighting privacy concerns and user expectations for transparency and control.", "motivation": "The study aims to fill the gap in understanding user perspectives on the implications of VLMs for privacy, particularly in the context of social media video uploads.", "method": "A semi-structured interview was conducted with 17 participants to gather insights on their experiences and concerns related to VLM-driven attribute inference.", "result": "Participants recognized VLMs' ability to infer various sensitive attributes with unsettling accuracy, raising concerns about unauthorized identification, surveillance, and potential misuse of personal information.", "conclusion": "The findings underscore the need for enhanced transparency, user control, and proactive privacy measures in the deployment of VLMs to align with user expectations and societal values.", "key_contributions": ["Highlights user concerns about VLMs and privacy risks.", "Provides insights into effective user mitigation strategies regarding privacy.", "Emphasizes the need for transparency and control in AI systems."], "limitations": "The study's findings are based on a small sample size, which may not fully represent broader user perspectives.", "keywords": ["Visual Language Models", "privacy", "user perceptions", "sensitive attribute inference", "social media"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.07172", "pdf": "https://arxiv.org/pdf/2508.07172.pdf", "abs": "https://arxiv.org/abs/2508.07172", "title": "Gradient Surgery for Safe LLM Fine-Tuning", "authors": ["Biao Yi", "Jiahao Li", "Baolei Zhang", "Lihai Nie", "Tong Li", "Tiansheng Huang", "Zheli Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning-as-a-Service introduces a critical vulnerability where a few\nmalicious examples mixed into the user's fine-tuning dataset can compromise the\nsafety alignment of Large Language Models (LLMs). While a recognized paradigm\nframes safe fine-tuning as a multi-objective optimization problem balancing\nuser task performance with safety alignment, we find existing solutions are\ncritically sensitive to the harmful ratio, with defenses degrading sharply as\nharmful ratio increases. We diagnose that this failure stems from conflicting\ngradients, where the user-task update directly undermines the safety objective.\nTo resolve this, we propose SafeGrad, a novel method that employs gradient\nsurgery. When a conflict is detected, SafeGrad nullifies the harmful component\nof the user-task gradient by projecting it onto the orthogonal plane of the\nalignment gradient, allowing the model to learn the user's task without\nsacrificing safety. To further enhance robustness and data efficiency, we\nemploy a KL-divergence alignment loss that learns the rich, distributional\nsafety profile of the well-aligned foundation model. Extensive experiments show\nthat SafeGrad provides state-of-the-art defense across various LLMs and\ndatasets, maintaining robust safety even at high harmful ratios without\ncompromising task fidelity.", "AI": {"tldr": "SafeGrad is a novel method for ensuring safety alignment in fine-tuning of LLMs by addressing conflicting gradients that arise from harmful examples in training data.", "motivation": "To address vulnerabilities in fine-tuning Large Language Models (LLMs) that arise when malicious examples compromise safety alignment.", "method": "SafeGrad employs gradient surgery to nullify harmful components of user-task gradients when conflicts with safety objectives are detected, using KL-divergence alignment loss for improved robustness and data efficiency.", "result": "SafeGrad demonstrates state-of-the-art defense against harmful ratios in LLMs, maintaining safety without sacrificing task fidelity across various datasets.", "conclusion": "The proposed method effectively balances user-task performance with safety alignment, enabling safe fine-tuning even under high risk of harmful data.", "key_contributions": ["Introduces SafeGrad for gradient surgery in LLM fine-tuning", "Employs KL-divergence alignment loss for data efficiency", "Demonstrates state-of-the-art safety defense across various LLMs"], "limitations": "", "keywords": ["Fine-tuning", "Large Language Models", "Safety Alignment", "Gradient Surgery", "NKL-Divergence"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2508.07664", "pdf": "https://arxiv.org/pdf/2508.07664.pdf", "abs": "https://arxiv.org/abs/2508.07664", "title": "Understanding Users' Privacy Perceptions Towards LLM's RAG-based Memory", "authors": ["Shuning Zhang", "Rongjun Ma", "Ying Ma", "Shixuan Li", "Yiqun Xu", "Xin Yi", "Hewu Li"], "categories": ["cs.HC"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly integrating memory\nfunctionalities to provide personalized and context-aware interactions.\nHowever, user understanding, practices and expectations regarding these memory\nsystems are not yet well understood. This paper presents a thematic analysis of\nsemi-structured interviews with 18 users to explore their mental models of\nLLM's Retrieval Augmented Generation (RAG)-based memory, current usage\npractices, perceived benefits and drawbacks, privacy concerns and expectations\nfor future memory systems. Our findings reveal diverse and often incomplete\nmental models of how memory operates. While users appreciate the potential for\nenhanced personalization and efficiency, significant concerns exist regarding\nprivacy, control and the accuracy of remembered information. Users express a\ndesire for granular control over memory generation, management, usage and\nupdating, including clear mechanisms for reviewing, editing, deleting and\ncategorizing memories, as well as transparent insight into how memories and\ninferred information are used. We discuss design implications for creating more\nuser-centric, transparent, and trustworthy LLM memory systems.", "AI": {"tldr": "This paper investigates user perceptions and expectations of memory functionalities in LLMs using thematic analysis of interviews with users.", "motivation": "To understand user mental models and practices regarding memory systems in LLMs, which are currently underexplored.", "method": "The study conducted semi-structured interviews with 18 users to gather qualitative data for thematic analysis.", "result": "Findings showed users have diverse and incomplete mental models of memory systems, with significant concerns about privacy, control, and accuracy of remembered information.", "conclusion": "The study emphasizes the need for user-centric designs in LLM memory systems, focusing on transparency and control for users.", "key_contributions": ["Thematic analysis of user interviews to understand mental models of memory in LLMs", "Identification of key user concerns related to privacy and control", "Recommendations for designing more user-centric LLM memory systems"], "limitations": "", "keywords": ["Large Language Models", "Memory Systems", "User Research", "Human-Computer Interaction", "Privacy"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2508.07173", "pdf": "https://arxiv.org/pdf/2508.07173.pdf", "abs": "https://arxiv.org/abs/2508.07173", "title": "Omni-SafetyBench: A Benchmark for Safety Evaluation of Audio-Visual Large Language Models", "authors": ["Leyi Pan", "Zheyu Fu", "Yunpeng Zhai", "Shuchang Tao", "Sheng Guan", "Shiyu Huang", "Lingzhe Zhang", "Zhaoyang Liu", "Bolin Ding", "Felix Henry", "Lijie Wen", "Aiwei Liu"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "20 pages, 8 figures, 12 tables", "summary": "The rise of Omni-modal Large Language Models (OLLMs), which integrate visual\nand auditory processing with text, necessitates robust safety evaluations to\nmitigate harmful outputs. However, no dedicated benchmarks currently exist for\nOLLMs, and prior benchmarks designed for other LLMs lack the ability to assess\nsafety performance under audio-visual joint inputs or cross-modal safety\nconsistency. To fill this gap, we introduce Omni-SafetyBench, the first\ncomprehensive parallel benchmark for OLLM safety evaluation, featuring 24\nmodality combinations and variations with 972 samples each, including dedicated\naudio-visual harm cases. Considering OLLMs' comprehension challenges with\ncomplex omni-modal inputs and the need for cross-modal consistency evaluation,\nwe propose tailored metrics: a Safety-score based on conditional Attack Success\nRate (C-ASR) and Refusal Rate (C-RR) to account for comprehension failures, and\na Cross-Modal Safety Consistency Score (CMSC-score) to measure consistency\nacross modalities. Evaluating 6 open-source and 4 closed-source OLLMs reveals\ncritical vulnerabilities: (1) no model excels in both overall safety and\nconsistency, with only 3 models achieving over 0.6 in both metrics and top\nperformer scoring around 0.8; (2) safety defenses weaken with complex inputs,\nespecially audio-visual joints; (3) severe weaknesses persist, with some models\nscoring as low as 0.14 on specific modalities. Our benchmark and metrics\nhighlight urgent needs for enhanced OLLM safety, providing a foundation for\nfuture improvements.", "AI": {"tldr": "Introduction of Omni-SafetyBench, the first benchmark for evaluating the safety of Omni-modal Large Language Models (OLLMs) with a focus on audio-visual safety and cross-modal consistency.", "motivation": "The lack of dedicated safety benchmarks for OLLMs and the need for assessing safety performance with complex audio-visual inputs created a gap that this paper aims to fill.", "method": "The paper introduces Omni-SafetyBench, a benchmark featuring 24 modality combinations with 972 samples each. It develops tailored metrics such as Safety-score and Cross-Modal Safety Consistency Score to evaluate safety performance and consistency across modalities.", "result": "Evaluation of 10 models (6 open-source, 4 closed-source) revealed vulnerabilities, with only 3 achieving above 0.6 in safety and consistency metrics, while some models scored as low as 0.14 in specific modalities.", "conclusion": "The findings indicate critical safety vulnerabilities in OLLMs, necessitating urgent improvements in their safety measures and evaluation techniques.", "key_contributions": ["Development of the first comprehensive benchmark (Omni-SafetyBench) for OLLM safety assessment.", "Introduction of new evaluation metrics (Safety-score and CMSC-score) tailored for cross-modal safety.", "Empirical evaluation showing significant safety vulnerabilities in contemporary OLLMs."], "limitations": "Focuses solely on safety evaluation, does not address algorithmic improvements or underlying model training methodologies.", "keywords": ["Omni-modal Large Language Models", "safety evaluation", "cross-modal consistency", "benchmark", "metrics"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2508.07672", "pdf": "https://arxiv.org/pdf/2508.07672.pdf", "abs": "https://arxiv.org/abs/2508.07672", "title": "Towards Aligning Personalized Conversational Recommendation Agents with Users' Privacy Preferences", "authors": ["Shuning Zhang", "Ying Ma", "Jingruo Chen", "Simin Li", "Xin Yi", "Hewu Li"], "categories": ["cs.HC"], "comment": null, "summary": "The proliferation of AI agents, with their complex and context-dependent\nactions, renders conventional privacy paradigms obsolete. This position paper\nargues that the current model of privacy management, rooted in a user's\nunilateral control over a passive tool, is inherently mismatched with the\ndynamic and interactive nature of AI agents. We contend that ensuring effective\nprivacy protection necessitates that the agents proactively align with users'\nprivacy preferences instead of passively waiting for the user to control. To\nground this shift, and using personalized conversational recommendation agents\nas a case, we propose a conceptual framework built on Contextual Integrity (CI)\ntheory and Privacy Calculus theory. This synthesis first reframes automatically\ncontrolling users' privacy as an alignment problem, where AI agents initially\ndid not know users' preferences, and would learn their privacy preferences\nthrough implicit or explicit feedback. Upon receiving the preference feedback,\nthe agents used alignment and Pareto optimization for aligning preferences and\nbalancing privacy and utility. We introduced formulations and instantiations,\npotential applications, as well as five challenges.", "AI": {"tldr": "This position paper advocates for a shift in privacy management from user unilateral control to proactive alignment by AI agents with user privacy preferences, proposing a framework based on Contextual Integrity and Privacy Calculus theory.", "motivation": "The existing privacy paradigms fail in the context of AI agents, which act in complex and context-dependent manners, necessitating a new model for privacy management.", "method": "The paper proposes a conceptual framework that reframes privacy control as an alignment problem, where AI agents learn user privacy preferences through feedback and optimize the balance between privacy and utility using alignment and Pareto optimization.", "result": "Introduced formulations and instantiations for AI privacy management and identified five challenges associated with the proposed framework.", "conclusion": "Proactive alignment of AI agents with user privacy preferences can enhance privacy protection and improve user-agent interaction dynamics in the context of AI proliferation.", "key_contributions": ["Proposed a new privacy management framework for AI agents.", "Reframed privacy control as an alignment problem.", "Identified key challenges and potential applications for personalized conversational recommendation agents."], "limitations": "The position is theoretical and may require empirical validation and further development of the proposed framework.", "keywords": ["AI agents", "privacy management", "Contextual Integrity", "Privacy Calculus", "alignment problem"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.07178", "pdf": "https://arxiv.org/pdf/2508.07178.pdf", "abs": "https://arxiv.org/abs/2508.07178", "title": "Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback", "authors": ["Kejin Liu", "Junhong Lian", "Xiang Ao", "Ningtao Wang", "Xing Fu", "Yu Cheng", "Weiqiang Wang", "Xinyu Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by the 34th ACM International Conference on Information and\n  Knowledge Management (CIKM '25), Full Research Papers track", "summary": "Accurate personalized headline generation hinges on precisely capturing user\ninterests from historical behaviors. However, existing methods neglect\npersonalized-irrelevant click noise in entire historical clickstreams, which\nmay lead to hallucinated headlines that deviate from genuine user preferences.\nIn this paper, we reveal the detrimental impact of click noise on personalized\ngeneration quality through rigorous analysis in both user and news dimensions.\nBased on these insights, we propose a novel Personalized Headline Generation\nframework via Denoising Fake Interests from Implicit Feedback (PHG-DIF).\nPHG-DIF first employs dual-stage filtering to effectively remove clickstream\nnoise, identified by short dwell times and abnormal click bursts, and then\nleverages multi-level temporal fusion to dynamically model users' evolving and\nmulti-faceted interests for precise profiling. Moreover, we release DT-PENS, a\nnew benchmark dataset comprising the click behavior of 1,000 carefully curated\nusers and nearly 10,000 annotated personalized headlines with historical dwell\ntime annotations. Extensive experiments demonstrate that PHG-DIF substantially\nmitigates the adverse effects of click noise and significantly improves\nheadline quality, achieving state-of-the-art (SOTA) results on DT-PENS. Our\nframework implementation and dataset are available at\nhttps://github.com/liukejin-up/PHG-DIF.", "AI": {"tldr": "Proposes a Personalized Headline Generation framework that mitigates click noise and improves headline quality.", "motivation": "Accurate personalized headline generation requires a clear understanding of user interests, which are often obscured by irrelevant click noise in historical data.", "method": "The PHG-DIF framework employs dual-stage filtering to remove clickstream noise and uses multi-level temporal fusion to model users' evolving interests.", "result": "PHG-DIF shows substantial improvement in headline generation quality, outperforming existing methods and achieving state-of-the-art results on the new DT-PENS dataset.", "conclusion": "The implementation of PHG-DIF and the DT-PENS dataset are publicly available, offering significant resources for personalized headline generation research.", "key_contributions": ["Introduction of PHG-DIF framework for denoising clickstream data", "Development of the DT-PENS benchmark dataset", "Demonstration of SOTA performance in personalized headline generation"], "limitations": "", "keywords": ["personalized headline generation", "click noise", "user interests", "dataset", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.07677", "pdf": "https://arxiv.org/pdf/2508.07677.pdf", "abs": "https://arxiv.org/abs/2508.07677", "title": "Improving Continuous Grasp Force Decoding from EEG with Time-Frequency Regressors and Premotor-Parietal Network Integration", "authors": ["Parth G. Dangi", "Yogesh Kumar Meena"], "categories": ["cs.HC"], "comment": "7 pages, 5 figures, 2 tables, selected for presentation in special\n  session of System, Man, Cybernetics (SMC) conference, 2025. The codes\n  developed for this paper is available in the GitHub repository given in the\n  conclusion section of this paper", "summary": "Brain-machine interfaces (BMIs) have significantly advanced\nneuro-rehabilitation by enhancing motor control. However, accurately decoding\ncontinuous grasp force remains a challenge, limiting the effectiveness of BMI\napplications for fine motor tasks. Current models tend to prioritise\nalgorithmic complexity rather than incorporating neurophysiological insights\ninto force control, which is essential for developing effective neural\nengineering solutions. To address this, we propose EEGForceMap, an EEG-based\nmethodology that isolates signals from the premotor-parietal region and\nextracts task-specific components. We construct three distinct time-frequency\nfeature sets, which are validated by comparing them with prior studies, and use\nthem for force prediction with linear, non-linear, and deep learning-based\nregressors. The performance of these regressors was evaluated on the\nWAY-EEG-GAL dataset that includes 12 subjects. Our results show that\nintegrating EEGForceMap approach with regressor models yields a 61.7%\nimprovement in subject-specific conditions (R-squared = 0.815) and a 55.7%\nimprovement in subject-independent conditions (R-squared = 0.785) over the\nstate-of-the-art kinematic decoder models. Furthermore, an ablation study\nconfirms that each preprocessing step significantly enhances decoding accuracy.\nThis work contributes to the advancement of responsive BMIs for stroke\nrehabilitation and assistive robotics by improving EEG-based decoding of\ndynamic grasp force.", "AI": {"tldr": "This paper presents EEGForceMap, a methodology for improving brain-machine interfaces (BMIs) by enhancing EEG-based decoding of grasp force for neuro-rehabilitation. It achieves significant performance improvements over existing models.", "motivation": "The need to improve the decoding of continuous grasp force in brain-machine interfaces (BMIs) for better effectiveness in neuro-rehabilitation and assistive robotics.", "method": "EEGForceMap isolates EEG signals from the premotor-parietal region and constructs three time-frequency feature sets for force prediction using various regressor models including linear, non-linear, and deep learning approaches, validated on the WAY-EEG-GAL dataset.", "result": "The EEGForceMap approach led to a 61.7% improvement in subject-specific conditions (R-squared = 0.815) and a 55.7% improvement in subject-independent conditions (R-squared = 0.785) compared to state-of-the-art kinematic decoders.", "conclusion": "The study enhances decoding accuracy in BMIs, contributing to advancements in stroke rehabilitation and assistive technologies by using neurophysiological insights effectively.", "key_contributions": ["Introduction of EEGForceMap methodology for force decoding in BMIs", "Significant performance improvements over existing models for subject-specific and subject-independent conditions", "Integration of neurophysiological insights into EEG-based decoding strategies."], "limitations": "", "keywords": ["brain-machine interfaces", "EEG", "neuro-rehabilitation", "force decoding", "assistive robotics"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2508.07179", "pdf": "https://arxiv.org/pdf/2508.07179.pdf", "abs": "https://arxiv.org/abs/2508.07179", "title": "Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks", "authors": ["Jiaqi Yin", "Yi-Wei Chen", "Meng-Lung Lee", "Xiya Liu"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Enterprise data pipelines, characterized by complex transformations across\nmultiple programming languages, often cause a semantic disconnect between\noriginal metadata and downstream data. This \"semantic drift\" compromises data\nreproducibility and governance, and impairs the utility of services like\nretrieval-augmented generation (RAG) and text-to-SQL systems. To address this,\na novel framework is proposed for the automated extraction of fine-grained\nschema lineage from multilingual enterprise pipeline scripts. This method\nidentifies four key components: source schemas, source tables, transformation\nlogic, and aggregation operations, creating a standardized representation of\ndata transformations. For the rigorous evaluation of lineage quality, this\npaper introduces the Schema Lineage Composite Evaluation (SLiCE), a metric that\nassesses both structural correctness and semantic fidelity. A new benchmark is\nalso presented, comprising 1,700 manually annotated lineages from real-world\nindustrial scripts. Experiments were conducted with 12 language models, from\n1.3B to 32B small language models (SLMs) to large language models (LLMs) like\nGPT-4o and GPT-4.1. The results demonstrate that the performance of schema\nlineage extraction scales with model size and the sophistication of prompting\ntechniques. Specially, a 32B open-source model, using a single reasoning trace,\ncan achieve performance comparable to the GPT series under standard prompting.\nThis finding suggests a scalable and economical approach for deploying\nschema-aware agents in practical applications.", "AI": {"tldr": "This paper presents a framework for automated extraction of schema lineage from multilingual enterprise data pipelines to combat semantic drift, compromising data reproducibility and governance.", "motivation": "Addressing semantic drift in enterprise data pipelines that affects data reproducibility and governance, particularly in contexts like retrieval-augmented generation and text-to-SQL systems.", "method": "A novel framework is proposed for automatic extraction of schema lineage, identifying key components including source schemas, source tables, transformation logic, and aggregation operations, assessed using the Schema Lineage Composite Evaluation (SLiCE) metric.", "result": "Experiments showed that schema lineage extraction performance improves with language model size, with a 32B model achieving performance on par with GPT models under standard prompting.", "conclusion": "The findings indicate a scalable and cost-effective strategy for deploying schema-aware agents in real-world applications, emphasizing the impact of model size and prompting on extraction quality.", "key_contributions": ["Introduction of a framework for schema lineage extraction from multilingual pipelines", "Development of the SLiCE metric for evaluating lineage quality", "Creation of a new benchmark with 1,700 annotated lineages for evaluation"], "limitations": "", "keywords": ["schema lineage", "semantic drift", "data governance", "language models", "RAG"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.07730", "pdf": "https://arxiv.org/pdf/2508.07730.pdf", "abs": "https://arxiv.org/abs/2508.07730", "title": "SimViews: An Interactive Multi-Agent System Simulating Visitor-to-Visitor Conversational Patterns to Present Diverse Perspectives of Artifacts in Virtual Museums", "authors": ["Mingyang Su", "Chao Liu", "Jingling Zhang", "WU Shuang", "Mingming Fan"], "categories": ["cs.HC"], "comment": null, "summary": "Offering diverse perspectives on a museum artifact can deepen visitors'\nunderstanding and help avoid the cognitive limitations of a single narrative,\nultimately enhancing their overall experience. Physical museums promote\ndiversity through visitor interactions. However, it remains a challenge to\npresent multiple voices appropriately while attracting and sustaining a\nvisitor's attention in the virtual museum. Inspired by recent studies that show\nthe effectiveness of LLM-powered multi-agents in presenting different opinions\nabout an event, we propose SimViews, an interactive multi-agent system that\nsimulates visitor-to-visitor conversational patterns to promote the\npresentation of diverse perspectives. The system employs LLM-powered\nmulti-agents that simulate virtual visitors with different professional\nidentities, providing diverse interpretations of artifacts. Additionally, we\nconstructed 4 conversational patterns between users and agents to simulate\nvisitor interactions. We conducted a within-subject study with 20 participants,\ncomparing SimViews to a traditional single-agent condition. Our results show\nthat SimViews effectively facilitates the presentation of diverse perspectives\nthrough conversations, enhancing participants' understanding of viewpoints and\nengagement within the virtual museum.", "AI": {"tldr": "SimViews is an interactive multi-agent system that enhances visitor engagement and understanding in virtual museums by presenting diverse perspectives through simulated conversations between LLM-powered agents.", "motivation": "The paper aims to address the challenge of presenting multiple narratives in virtual museums to enhance visitor experience and understanding, as traditional methods often limit the perspectives available to users.", "method": "The study involved the development of SimViews, which uses LLM-powered multi-agents to simulate visitor-to-visitor interactions with diverse professional identities. Four conversational patterns were established to facilitate these interactions, and a within-subjects study was conducted with 20 participants.", "result": "The results indicate that SimViews significantly improves the presentation of diverse perspectives and increases user engagement and understanding compared to a traditional single-agent system.", "conclusion": "SimViews effectively engages virtual museum visitors by fostering conversational exchanges that highlight multiple viewpoints, ultimately enriching the museum experience.", "key_contributions": ["Introduction of SimViews, a multi-agent system for virtual museums.", "Demonstration of effective conversational patterns for diverse perspective presentation.", "Empirical evidence showing increased engagement and understanding in museum contexts."], "limitations": "The study was limited to a small sample size of 20 participants, and the long-term effects of using SimViews were not assessed.", "keywords": ["virtual museum", "multi-agent system", "LLM-powered agents", "diverse perspectives", "user engagement"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2508.07185", "pdf": "https://arxiv.org/pdf/2508.07185.pdf", "abs": "https://arxiv.org/abs/2508.07185", "title": "DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention", "authors": ["Kabir Khan", "Priya Sharma", "Arjun Mehta", "Neha Gupta", "Ravi Narayanan"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; H.3.3; H.2.8"], "comment": "Preprint; 7 figures, 3 tables, 1 algorithm; v1. Code and data will be\n  released", "summary": "Large Language Models (LLMs) suffer from a critical limitation: their\nknowledge is static and quickly becomes outdated. Retraining these massive\nmodels is computationally prohibitive, while existing knowledge editing\ntechniques can be slow and may introduce unforeseen side effects. To address\nthis, we propose DySK-Attn, a novel framework that enables LLMs to efficiently\nintegrate real-time knowledge from a dynamic external source. Our approach\nsynergizes an LLM with a dynamic Knowledge Graph (KG) that can be updated\ninstantaneously. The core of our framework is a sparse knowledge attention\nmechanism, which allows the LLM to perform a coarse-to-fine grained search,\nefficiently identifying and focusing on a small, highly relevant subset of\nfacts from the vast KG. This mechanism avoids the high computational cost of\ndense attention over the entire knowledge base and mitigates noise from\nirrelevant information. We demonstrate through extensive experiments on\ntime-sensitive question-answering tasks that DySK-Attn significantly\noutperforms strong baselines, including standard Retrieval-Augmented Generation\n(RAG) and model editing techniques, in both factual accuracy for updated\nknowledge and computational efficiency. Our framework offers a scalable and\neffective solution for building LLMs that can stay current with the\never-changing world.", "AI": {"tldr": "DySK-Attn is a framework that allows Large Language Models (LLMs) to integrate real-time knowledge from dynamic Knowledge Graphs (KGs) using a sparse knowledge attention mechanism, enhancing efficiency and accuracy in time-sensitive tasks.", "motivation": "LLMs have static knowledge that becomes outdated quickly, and retraining them is costly. Existing knowledge editing methods are slow and may cause side effects, creating a need for a more efficient solution.", "method": "The framework integrates an LLM with a dynamic KG that updates in real-time, utilizing a sparse knowledge attention mechanism for efficient knowledge retrieval.", "result": "DySK-Attn significantly outperformed strong baselines, including standard RAG and model editing techniques, in factual accuracy and computational efficiency on time-sensitive question-answering tasks.", "conclusion": "The DySK-Attn framework provides a scalable solution for keeping LLMs up-to-date with current knowledge.", "key_contributions": ["Introduction of the DySK-Attn framework for real-time knowledge integration in LLMs.", "Development of a sparse knowledge attention mechanism that improves efficiency in knowledge retrieval.", "Demonstration of improved performance over existing techniques in factual accuracy and computational efficiency."], "limitations": "", "keywords": ["Large Language Models", "Knowledge Graph", "real-time knowledge integration", "sparse attention", "factual accuracy"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.07731", "pdf": "https://arxiv.org/pdf/2508.07731.pdf", "abs": "https://arxiv.org/abs/2508.07731", "title": "CognitiveArm: Enabling Real-Time EEG-Controlled Prosthetic Arm Using Embodied Machine Learning", "authors": ["Abdul Basit", "Maha Nawaz", "Saim Rehman", "Muhammad Shafique"], "categories": ["cs.HC", "cs.AI", "68T50, 68T40, 68T07, 92C55", "I.2.7; I.2.9"], "comment": "7 pages, 12 figures, Accepted to 62nd DAC 2025", "summary": "Efficient control of prosthetic limbs via non-invasive brain-computer\ninterfaces (BCIs) requires advanced EEG processing, including pre-filtering,\nfeature extraction, and action prediction, performed in real time on edge AI\nhardware. Achieving this on resource-constrained devices presents challenges in\nbalancing model complexity, computational efficiency, and latency. We present\nCognitiveArm, an EEG-driven, brain-controlled prosthetic system implemented on\nembedded AI hardware, achieving real-time operation without compromising\naccuracy. The system integrates BrainFlow, an open-source library for EEG data\nacquisition and streaming, with optimized deep learning (DL) models for precise\nbrain signal classification. Using evolutionary search, we identify\nPareto-optimal DL configurations through hyperparameter tuning, optimizer\nanalysis, and window selection, analyzed individually and in ensemble\nconfigurations. We apply model compression techniques such as pruning and\nquantization to optimize models for embedded deployment, balancing efficiency\nand accuracy. We collected an EEG dataset and designed an annotation pipeline\nenabling precise labeling of brain signals corresponding to specific intended\nactions, forming the basis for training our optimized DL models. CognitiveArm\nalso supports voice commands for seamless mode switching, enabling control of\nthe prosthetic arm's 3 degrees of freedom (DoF). Running entirely on embedded\nhardware, it ensures low latency and real-time responsiveness. A full-scale\nprototype, interfaced with the OpenBCI UltraCortex Mark IV EEG headset,\nachieved up to 90% accuracy in classifying three core actions (left, right,\nidle). Voice integration enables multiplexed, variable movement for everyday\ntasks (e.g., handshake, cup picking), enhancing real-world performance and\ndemonstrating CognitiveArm's potential for advanced prosthetic control.", "AI": {"tldr": "CognitiveArm is an EEG-driven prosthetic system utilizing optimized deep learning models integrated with BrainFlow for real-time control on embedded hardware, achieving high accuracy with voice command support.", "motivation": "There is a need for efficient control of prosthetic limbs via non-invasive brain-computer interfaces, particularly on resource-constrained edge AI hardware.", "method": "The system employs advanced EEG processing, including pre-filtering, feature extraction, and action prediction. It uses hyperparameter tuning, optimizer analysis, ensemble configurations, and model compression techniques like pruning and quantization to enhance model performance for embedded deployment.", "result": "CognitiveArm, tested with the OpenBCI UltraCortex Mark IV EEG headset, achieved up to 90% accuracy in classifying actions (left, right, idle) and can control the prosthetic arm's 3 DoF with voice commands for improved functionality in everyday tasks.", "conclusion": "The successful integration of EEG-driven control with voice command support demonstrates the potential for enhancing real-world performance of advanced prosthetic systems.", "key_contributions": ["Development of a real-time EEG-driven prosthetic control system on embedded AI hardware.", "Use of model compression techniques to optimize deep learning models for efficiency.", "Implementation of voice command functionality for enhanced usability."], "limitations": "", "keywords": ["brain-computer interface", "EEG", "deep learning", "prosthetic systems", "real-time processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.07195", "pdf": "https://arxiv.org/pdf/2508.07195.pdf", "abs": "https://arxiv.org/abs/2508.07195", "title": "Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment", "authors": ["Yanru Sun", "Emadeldeen Eldele", "Zongxia Xie", "Yucheng Wang", "Wenzhe Niu", "Qinghua Hu", "Chee Keong Kwoh", "Min Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have recently demonstrated impressive\ncapabilities in natural language processing due to their strong generalization\nand sequence modeling capabilities. However, their direct application to time\nseries forecasting remains challenging due to two fundamental issues: the\ninherent heterogeneity of temporal patterns and the modality gap between\ncontinuous numerical signals and discrete language representations. In this\nwork, we propose TALON, a unified framework that enhances LLM-based forecasting\nby modeling temporal heterogeneity and enforcing semantic alignment.\nSpecifically, we design a Heterogeneous Temporal Encoder that partitions\nmultivariate time series into structurally coherent segments, enabling\nlocalized expert modeling across diverse temporal patterns. To bridge the\nmodality gap, we introduce a Semantic Alignment Module that aligns temporal\nfeatures with LLM-compatible representations, enabling effective integration of\ntime series into language-based models while eliminating the need for\nhandcrafted prompts during inference. Extensive experiments on seven real-world\nbenchmarks demonstrate that TALON achieves superior performance across all\ndatasets, with average MSE improvements of up to 11\\% over recent\nstate-of-the-art methods. These results underscore the effectiveness of\nincorporating both pattern-aware and semantic-aware designs when adapting LLMs\nfor time series forecasting. The code is available at:\nhttps://github.com/syrGitHub/TALON.", "AI": {"tldr": "TALON is a framework that improves LLM-based time series forecasting by addressing temporal heterogeneity and bridging the modality gap between numerical signals and language representations.", "motivation": "To enhance the application of Large Language Models in time series forecasting by modeling temporal heterogeneity and addressing the challenges of aligning numerical and language representations.", "method": "TALON employs a Heterogeneous Temporal Encoder for partitioning time series into coherent segments and a Semantic Alignment Module for aligning temporal features with LLM-compatible representations.", "result": "TALON outperformed recent state-of-the-art methods across seven real-world datasets, achieving average MSE improvements of up to 11%.", "conclusion": "Incorporating pattern-aware and semantic-aware designs is effective for adapting LLMs to time series forecasting tasks.", "key_contributions": ["Introduction of TALON framework for LLM-based forecasting", "Development of Heterogeneous Temporal Encoder for expert modeling", "Creation of Semantic Alignment Module to link temporal features with LLMs"], "limitations": "", "keywords": ["Large Language Models", "Time Series Forecasting", "Semantic Alignment"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.07854", "pdf": "https://arxiv.org/pdf/2508.07854.pdf", "abs": "https://arxiv.org/abs/2508.07854", "title": "Challenges in Mixed Reality in Assisting Adults with ADHD Symptoms", "authors": ["Valerie Tan", "Jens Gerken"], "categories": ["cs.HC"], "comment": "3 pages, Submitted as a position paper to a workshop (\"Envisioning\n  the Future of Accessible Immersive Technology\") at the Mensch und Computer\n  2024 conference", "summary": "In this position paper, we discuss symptoms of attention deficit\nhyperactivity disorder (ADHD) in adults, as well as available forms of\ntreatment or assistance in the context of mixed reality. Mixed reality offers\nmany potentials for assisting adults with symptoms commonly found in (but not\nlimited to) ADHD, but the availability of mixed reality solutions is not only\nlimited commercially, but also limited in terms of proof-of-concept prototypes.\nWe discuss two major challenges with attention assistance using mixed reality\nsolutions: the limited availability of adult-specific prototypes and studies,\nas well as the limited number of solutions that offer continuous intervention\nof ADHD-like symptoms that users can employ in their daily life.", "AI": {"tldr": "This position paper explores the potential of mixed reality in aiding adults with ADHD symptoms, highlighting challenges in prototype availability and continuous intervention solutions.", "motivation": "To examine how mixed reality can assist adults with ADHD symptoms and identify gaps in existing solutions.", "method": "Discussion of current treatment options and mixed reality's role, analyzing the limitations of existing prototypes and studies targeting adults with ADHD.", "result": "Identified two major challenges: lack of adult-specific mixed reality prototypes and insufficient continuous intervention solutions for daily use.", "conclusion": "Mixed reality holds promise for assisting adults with ADHD, but significant gaps in prototypes and solutions hinder its implementation.", "key_contributions": ["Highlights the potential of mixed reality for ADHD assistance", "Identifies critical gaps in adult-focused interventions", "Proposes focus areas for future research and development"], "limitations": "Limited commercial availability of mixed reality solutions and lack of continuous intervention prototypes tailored for adults with ADHD.", "keywords": ["ADHD", "mixed reality", "treatment", "adults", "intervention"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2508.07209", "pdf": "https://arxiv.org/pdf/2508.07209.pdf", "abs": "https://arxiv.org/abs/2508.07209", "title": "Enhancing Rumor Detection Methods with Propagation Structure Infused Language Model", "authors": ["Chaoqun Cui", "Siyuan Li", "Kunkun Ma", "Caiyan Jia"], "categories": ["cs.CL", "cs.SI"], "comment": "This paper is accepted by COLING2025", "summary": "Pretrained Language Models (PLMs) have excelled in various Natural Language\nProcessing tasks, benefiting from large-scale pretraining and self-attention\nmechanism's ability to capture long-range dependencies. However, their\nperformance on social media application tasks like rumor detection remains\nsuboptimal. We attribute this to mismatches between pretraining corpora and\nsocial texts, inadequate handling of unique social symbols, and pretraining\ntasks ill-suited for modeling user engagements implicit in propagation\nstructures. To address these issues, we propose a continue pretraining strategy\ncalled Post Engagement Prediction (PEP) to infuse information from propagation\nstructures into PLMs. PEP makes models to predict root, branch, and parent\nrelations between posts, capturing interactions of stance and sentiment crucial\nfor rumor detection. We also curate and release large-scale Twitter corpus:\nTwitterCorpus (269GB text), and two unlabeled claim conversation datasets with\npropagation structures (UTwitter and UWeibo). Utilizing these resources and PEP\nstrategy, we train a Twitter-tailored PLM called SoLM. Extensive experiments\ndemonstrate PEP significantly boosts rumor detection performance across\nuniversal and social media PLMs, even in few-shot scenarios. On benchmark\ndatasets, PEP enhances baseline models by 1.0-3.7\\% accuracy, even enabling it\nto outperform current state-of-the-art methods on multiple datasets. SoLM\nalone, without high-level modules, also achieves competitive results,\nhighlighting the strategy's effectiveness in learning discriminative post\ninteraction features.", "AI": {"tldr": "This paper introduces Post Engagement Prediction (PEP) to improve rumor detection in social media by enhancing pretrained language models (PLMs) with insights from post interactions.", "motivation": "Pretrained Language Models struggle with social media tasks like rumor detection due to mismatched training data and inadequate modeling of user engagements.", "method": "The authors propose a continued pretraining strategy, PEP, that predicts relationships between posts to enhance models' understanding of social text dynamics.", "result": "Experiments show that PEP significantly improves rumor detection performance by 1.0-3.7% accuracy on benchmark datasets, outperforming current state-of-the-art methods.", "conclusion": "The PEP strategy, combined with a tailored PLM (SoLM), effectively enhances learning of discriminative post interactions for better rumor detection.", "key_contributions": ["Introduction of Post Engagement Prediction (PEP) strategy.", "Release of large-scale TwitterCorpus and claim conversation datasets with propagation structures.", "Demonstration of performance improvements in rumor detection on multiple datasets."], "limitations": "", "keywords": ["Pretrained Language Models", "Rumor Detection", "Social Media", "Post Engagement Prediction", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.07980", "pdf": "https://arxiv.org/pdf/2508.07980.pdf", "abs": "https://arxiv.org/abs/2508.07980", "title": "Early Explorations of Recommender Systems for Physical Activity and Well-being", "authors": ["Alan Said"], "categories": ["cs.HC", "cs.IR"], "comment": "Second International Workshop on Recommender Systems for\n  Sustainability and Social Good (RecSoGood) in conjunction with ACM RecSys\n  2025", "summary": "As recommender systems increasingly guide physical actions, often through\nwearables and coaching tools, new challenges arise around how users interpret,\ntrust, and respond to this advice. This paper introduces a conceptual framework\nfor tangible recommendations that influence users' bodies, routines, and\nwell-being. We describe three design dimensions: trust and interpretation,\nintent alignment, and consequence awareness. These highlight key limitations in\napplying conventional recommender logic to embodied settings. Through examples\nand design reflections, we outline how future systems can support long-term\nwell-being, behavioral alignment, and socially responsible personalization.", "AI": {"tldr": "This paper presents a conceptual framework for tangible recommendations in recommender systems, focusing on how they influence users' physical actions, trust, and well-being.", "motivation": "With the rise of recommender systems in guiding users' physical actions through wearables, understanding the interpretation and trust in these recommendations has become increasingly important.", "method": "The paper introduces a conceptual framework based on three design dimensions: trust and interpretation, intent alignment, and consequence awareness, examining their implications in embodied recommendation settings.", "result": "It identifies key limitations when applying traditional recommender logic in these contexts and provides design reflections and examples for future systems.", "conclusion": "Future recommender systems should promote long-term well-being, behavioral alignment, and socially responsible personalization to effectively influence user actions and routines.", "key_contributions": ["Introduces a framework for tangible recommendations in embodied settings", "Highlights three critical design dimensions for user trust and interaction", "Suggests pathways for developing socially responsible and effective recommender systems."], "limitations": "The proposed framework needs empirical validation and might require adaptation across diverse user contexts and cultures.", "keywords": ["recommender systems", "trust", "behavioral alignment", "well-being", "personalization"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.07229", "pdf": "https://arxiv.org/pdf/2508.07229.pdf", "abs": "https://arxiv.org/abs/2508.07229", "title": "How Does a Deep Neural Network Look at Lexical Stress?", "authors": ["Itai Allouche", "Itay Asael", "Rotem Rousso", "Vered Dassa", "Ann Bradlow", "Seung-Eun Kim", "Matthew Goldrick", "Joseph Keshet"], "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": "10 pages, 4 figures, submitted to the Journal of the Acoustical\n  Society of America (JASA)", "summary": "Despite their success in speech processing, neural networks often operate as\nblack boxes, prompting the question: what informs their decisions, and how can\nwe interpret them? This work examines this issue in the context of lexical\nstress. A dataset of English disyllabic words was automatically constructed\nfrom read and spontaneous speech. Several Convolutional Neural Network (CNN)\narchitectures were trained to predict stress position from a spectrographic\nrepresentation of disyllabic words lacking minimal stress pairs (e.g., initial\nstress WAllet, final stress exTEND), achieving up to 92% accuracy on held-out\ntest data. Layerwise Relevance Propagation (LRP), a technique for CNN\ninterpretability analysis, revealed that predictions for held-out minimal pairs\n(PROtest vs. proTEST ) were most strongly influenced by information in stressed\nversus unstressed syllables, particularly the spectral properties of stressed\nvowels. However, the classifiers also attended to information throughout the\nword. A feature-specific relevance analysis is proposed, and its results\nsuggest that our best-performing classifier is strongly influenced by the\nstressed vowel's first and second formants, with some evidence that its pitch\nand third formant also contribute. These results reveal deep learning's ability\nto acquire distributed cues to stress from naturally occurring data, extending\ntraditional phonetic work based around highly controlled stimuli.", "AI": {"tldr": "This paper explores the interpretability of neural networks in predicting lexical stress in English disyllabic words using CNN architectures and demonstrates the influence of stressed syllables on predictions.", "motivation": "Despite the effectiveness of neural networks in speech processing, their 'black box' nature raises questions about interpretability and decision-making processes, prompting this examination within the context of lexical stress.", "method": "A dataset of English disyllabic words was constructed from read and spontaneous speech. Various Convolutional Neural Network (CNN) architectures were trained to predict stress position based on spectrographic representations. Layerwise Relevance Propagation (LRP) was employed for interpretability analysis.", "result": "The CNNs achieved up to 92% accuracy on held-out test data, revealing that predictions were influenced primarily by stressed versus unstressed syllables, particularly focusing on the spectral properties of stressed vowels.", "conclusion": "The findings illustrate that deep learning can effectively learn distributed cues to stress from natural data, challenging traditional phonetics which rely on controlled stimuli. The best classifier was significantly affected by the first and second formants of stressed vowels.", "key_contributions": ["Constructed a dataset for disyllabic word stress prediction using spontaneous speech", "Achieved 92% accuracy using CNN architectures for stress prediction", "Demonstrated the influence of spectral properties of stressed vowels through interpretability analysis"], "limitations": "", "keywords": ["neural networks", "lexical stress", "CNN", "interpretability", "speech processing"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.08020", "pdf": "https://arxiv.org/pdf/2508.08020.pdf", "abs": "https://arxiv.org/abs/2508.08020", "title": "EchoAid: Enhancing Livestream Shopping Accessibility for the DHH Community", "authors": ["Zeyu Yang", "Zheng Wei", "Yang Zhang", "Xian Xu", "Changyang He", "Muzhi Zhou", "Pan Hui"], "categories": ["cs.HC"], "comment": "Paper for CSCW 2025", "summary": "Livestream shopping platforms often overlook the accessibility needs of the\nDeaf and Hard of Hearing (DHH) community, leading to barriers such as\ninformation inaccessibility and overload. To tackle these challenges, we\ndeveloped \\textit{EchoAid}, a mobile app designed to improve the livestream\nshopping experience for DHH users. \\textit{EchoAid} utilizes advanced\nspeech-to-text conversion, Rapid Serial Visual Presentation (RSVP) technology,\nand Large Language Models (LLMs) to simplify the complex information flow in\nlive sales environments. We conducted exploratory studies with eight DHH\nindividuals to identify design needs and iteratively developed the\n\\textit{EchoAid} prototype based on feedback from three participants. We then\nevaluate the performance of this system in a user study workshop involving 38\nDHH participants. Our findings demonstrate the successful design and validation\nprocess of \\textit{EchoAid}, highlighting its potential to enhance product\ninformation extraction, leading to reduced cognitive overload and more engaging\nand customized shopping experiences for DHH users.", "AI": {"tldr": "Development of EchoAid, a mobile app to enhance livestream shopping for Deaf and Hard of Hearing users using speech-to-text and LLMs.", "motivation": "Livestream shopping often neglects the accessibility of the Deaf and Hard of Hearing community, creating barriers in information access and overload.", "method": "Developed a mobile app named EchoAid, employing speech-to-text conversion, RSVP technology, and LLMs. Conducted exploratory studies and iterative prototyping based on user feedback, followed by an evaluation workshop with DHH participants.", "result": "EchoAid was successfully designed and validated, showing improved information extraction and reduced cognitive overload for DHH users during livestream shopping.", "conclusion": "EchoAid enhances the shopping experience for DHH users by simplifying information flow and fostering engagement.", "key_contributions": ["Designed EchoAid specifically for the DHH community in livestream shopping", "Integrated advanced technologies like speech-to-text, RSVP, and LLMs", "Validated through user feedback and workshops with actual DHH users"], "limitations": "", "keywords": ["accessibility", "livestream shopping", "Deaf and Hard of Hearing", "speech-to-text", "large language models"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.07248", "pdf": "https://arxiv.org/pdf/2508.07248.pdf", "abs": "https://arxiv.org/abs/2508.07248", "title": "Prompt Tuning for Few-Shot Continual Learning Named Entity Recognition", "authors": ["Zhe Ren"], "categories": ["cs.CL"], "comment": null, "summary": "Knowledge distillation has been successfully applied to Continual Learning\nNamed Entity Recognition (CLNER) tasks, by using a teacher model trained on\nold-class data to distill old-class entities present in new-class data as a\nform of regularization, thereby avoiding catastrophic forgetting. However, in\nFew-Shot CLNER (FS-CLNER) tasks, the scarcity of new-class entities makes it\ndifficult for the trained model to generalize during inference. More\ncritically, the lack of old-class entity information hinders the distillation\nof old knowledge, causing the model to fall into what we refer to as the\nFew-Shot Distillation Dilemma. In this work, we address the above challenges\nthrough a prompt tuning paradigm and memory demonstration template strategy.\nSpecifically, we designed an expandable Anchor words-oriented Prompt Tuning\n(APT) paradigm to bridge the gap between pre-training and fine-tuning, thereby\nenhancing performance in few-shot scenarios. Additionally, we incorporated\nMemory Demonstration Templates (MDT) into each training instance to provide\nreplay samples from previous tasks, which not only avoids the Few-Shot\nDistillation Dilemma but also promotes in-context learning. Experiments show\nthat our approach achieves competitive performances on FS-CLNER.", "AI": {"tldr": "This paper presents a novel approach to Few-Shot Continual Learning Named Entity Recognition (FS-CLNER) using prompt tuning and memory templates to enhance model performance and prevent knowledge loss.", "motivation": "To address the challenges of Few-Shot CLNER, especially the Few-Shot Distillation Dilemma caused by lack of old-class entity information.", "method": "An expandable Anchor words-oriented Prompt Tuning (APT) paradigm and Memory Demonstration Templates (MDT) were introduced to improve model generalization and performance in few-shot scenarios.", "result": "The proposed APT and MDT strategies lead to competitive performance in FS-CLNER tasks and effectively avoid the Few-Shot Distillation Dilemma.", "conclusion": "The combined approach enhances performance in few-shot learning scenarios while maintaining old knowledge, thus contributing to the field of continual learning.", "key_contributions": ["Development of the Anchor words-oriented Prompt Tuning (APT) paradigm", "Introduction of Memory Demonstration Templates (MDT) for in-context learning", "Effective prevention of the Few-Shot Distillation Dilemma"], "limitations": "", "keywords": ["Few-Shot Learning", "Continual Learning", "Named Entity Recognition", "Prompt Tuning", "Memory Templates"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2508.08101", "pdf": "https://arxiv.org/pdf/2508.08101.pdf", "abs": "https://arxiv.org/abs/2508.08101", "title": "ChatGPT on the Road: Leveraging Large Language Model-Powered In-vehicle Conversational Agents for Safer and More Enjoyable Driving Experience", "authors": ["Yeana Lee Bond", "Mungyeong Choe", "Baker Kasim Hasan", "Arsh Siddiqui", "Myounghoon Jeon"], "categories": ["cs.HC", "cs.AI", "cs.SE"], "comment": "Submitted to International Journal of Human-Computer Studies. Bond\n  and Choe: Drafting, Review, Editing, Validation, Software, Methodology,\n  Investigation, Data Analysis, Conceptualization, Experiment training. Hasan\n  and Siddiqui: Experimental and Data Analysis Support. Jeon: Supervision,\n  Review, Resources, Project Admin, Methodology, Conceptualization. Total 34\n  pages", "summary": "Studies on in-vehicle conversational agents have traditionally relied on\npre-scripted prompts or limited voice commands, constraining natural\ndriver-agent interaction. To resolve this issue, the present study explored the\npotential of a ChatGPT-based in-vehicle agent capable of carrying continuous,\nmulti-turn dialogues. Forty drivers participated in our experiment using a\nmotion-based driving simulator, comparing three conditions (No agent,\nPre-scripted agent, and ChatGPT-based agent) as a within-subjects variable.\nResults showed that the ChatGPT-based agent condition led to more stable\ndriving performance across multiple metrics. Participants demonstrated lower\nvariability in longitudinal acceleration, lateral acceleration, and lane\ndeviation compared to the other two conditions. In subjective evaluations, the\nChatGPT-based agent also received significantly higher ratings in competence,\nanimacy, affective trust, and preference compared to the Pre-scripted agent.\nOur thematic analysis of driver-agent conversations revealed diverse\ninteraction patterns in topics, including driving assistance/questions,\nentertainment requests, and anthropomorphic interactions. Our results highlight\nthe potential of LLM-powered in-vehicle conversational agents to enhance\ndriving safety and user experience through natural, context-rich interactions.", "AI": {"tldr": "This study explores a ChatGPT-based in-vehicle conversational agent, demonstrating improved driving performance and user experience compared to pre-scripted agents during a driving simulator experiment.", "motivation": "To enhance natural interaction between drivers and in-vehicle conversational agents beyond pre-scripted prompts.", "method": "Conducted an experiment with forty drivers in a motion-based driving simulator, comparing driving performance and subjective evaluations across three conditions: No agent, Pre-scripted agent, and ChatGPT-based agent.", "result": "Drivers using the ChatGPT-based agent showed more stable driving performance metrics (lower variability in acceleration and lane deviation) and rated the agent higher in competence and trust.", "conclusion": "LLM-powered conversational agents could significantly improve driving safety and user experiences by enabling more engaging and contextually rich interactions during driving.", "key_contributions": ["Demonstrated the efficacy of a ChatGPT-based conversational agent in driving contexts.", "Highlighted the advantages of continuous, multi-turn dialogues over pre-scripted interactions.", "Revealed diverse interaction patterns that enhance user engagement and comfort."], "limitations": "The study is limited to a controlled driving simulator environment, which may not fully represent real-world driving conditions.", "keywords": ["in-vehicle agents", "ChatGPT", "human-computer interaction", "driving performance", "conversational agents"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2508.07262", "pdf": "https://arxiv.org/pdf/2508.07262.pdf", "abs": "https://arxiv.org/abs/2508.07262", "title": "The 2D+ Dynamic Articulatory Model DYNARTmo: Tongue-Palate Contact Area Estimation", "authors": ["Bernd J. Kr√∂ger"], "categories": ["cs.CL", "cs.RO"], "comment": "11 pages, 9 figures, 14 references; supplementary material: python\n  source code", "summary": "This paper describes an extension of the two-dimensional dynamic articulatory\nmodel DYNARTmo by integrating an internal three-dimensional representation of\nthe palatal dome to estimate tongue-palate contact areas from midsagittal\ntongue contours. Two alternative dome geometries - a half-ellipse and a cosine\nbased profile - are implemented to model lateral curvature in the coronal\nplane. Using these geometries, lateral contact points are analytically computed\nfor each anterior-posterior position, enabling the generation of\nelectropalatography-like visualizations within the 2D+ framework. The enhanced\nmodel supports three synchronized views (sagittal, glottal, and palatal) for\nstatic and dynamic (animated) articulation displays, suitable for speech\nscience education and speech therapy. Future work includes adding a facial\n(lip) view and implementing articulatory-to-acoustic synthesis to\nquantitatively evaluate model realism.", "AI": {"tldr": "The paper extends the DYNARTmo model to include a three-dimensional representation of the palatal dome for estimating tongue-palate contact areas based on tongue contours, enabling detailed visualizations for speech science education and therapy.", "motivation": "To improve the estimation of tongue-palate contact areas for better understanding and visualizing speech articulation.", "method": "The extension integrates three-dimensional geometries of the palatal dome to compute lateral contact points from midsagittal tongue contours, generating visualizations within a 2D+ framework.", "result": "The model allows for three synchronized views (sagittal, glottal, and palatal) and produces visualizations similar to electropalatography, enhancing educational and therapeutic applications in speech science.", "conclusion": "The model's enhancement can aid in speech therapy and education, with future developments planned for additional views and synthesis capabilities.", "key_contributions": ["Integration of 3D palatal dome representation into DYNARTmo model", "Analytical computation of lateral contact points from tongue contours", "Support for multiple synchronized articulation display views."], "limitations": "", "keywords": ["dynamic articulatory model", "speech science", "visualization", "electropalatography", "articulation synthesis"], "importance_score": 4, "read_time_minutes": 11}}
{"id": "2508.08128", "pdf": "https://arxiv.org/pdf/2508.08128.pdf", "abs": "https://arxiv.org/abs/2508.08128", "title": "Fuzzy Ontology Embeddings and Visual Query Building for Ontology Exploration", "authors": ["Vladimir Zhurov", "John Kausch", "Kamran Sedig", "Mostafa Milani"], "categories": ["cs.HC"], "comment": "Journal submission", "summary": "Ontologies play a central role in structuring knowledge across domains,\nsupporting tasks such as reasoning, data integration, and semantic search.\nHowever, their large size and complexity, particularly in fields such as\nbiomedicine, computational biology, law, and engineering, make them difficult\nfor non-experts to navigate. Formal query languages such as SPARQL offer\nexpressive access but require users to understand the ontology's structure and\nsyntax. In contrast, visual exploration tools and basic keyword-based search\ninterfaces are easier to use but often lack flexibility and expressiveness. We\nintroduce FuzzyVis, a proof-of-concept system that enables intuitive and\nexpressive exploration of complex ontologies. FuzzyVis integrates two key\ncomponents: a fuzzy logic-based querying model built on fuzzy ontology\nembeddings, and an interactive visual interface for building and interpreting\nqueries. Users can construct new composite concepts by selecting and combining\nexisting ontology concepts using logical operators such as conjunction,\ndisjunction, and negation. These composite concepts are matched against the\nontology using fuzzy membership-based embeddings, which capture degrees of\nmembership and support approximate, concept-level similarity search. The visual\ninterface supports browsing, query composition, and partial search without\nrequiring formal syntax. By combining fuzzy semantics with embedding-based\nreasoning, FuzzyVis enables flexible interpretation, efficient computation, and\nexploratory learning. Case studies demonstrate how FuzzyVis supports subtle\ninformation needs and helps users uncover relevant concepts in large, complex\nontologies.", "AI": {"tldr": "FuzzyVis is a system facilitating intuitive exploration of complex ontologies using fuzzy logic and an interactive visual interface, enhancing user experience beyond traditional query languages.", "motivation": "To address the challenges faced by non-experts in navigating large, complex ontologies in various fields, particularly biomedicine and computational biology.", "method": "FuzzyVis integrates a fuzzy logic-based querying model with fuzzy ontology embeddings and an interactive visual interface that allows users to combine concepts using logical operators.", "result": "FuzzyVis enables users to create new composite concepts and perform approximate concept-level similarity searches, improving accessibility to complex information in ontologies.", "conclusion": "The case studies demonstrate FuzzyVis's effectiveness in supporting exploratory learning and meeting nuanced information needs.", "key_contributions": ["Introduction of a fuzzy logic-based querying model for ontologies", "Development of an interactive visual interface for query composition", "Demonstration of enhanced exploratory learning through case studies"], "limitations": "", "keywords": ["ontologies", "fuzzy logic", "semantic search", "information retrieval", "visual exploration"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.07273", "pdf": "https://arxiv.org/pdf/2508.07273.pdf", "abs": "https://arxiv.org/abs/2508.07273", "title": "Incorporating Contextual Paralinguistic Understanding in Large Speech-Language Models", "authors": ["Qiongqiong Wang", "Hardik B. Sailor", "Jeremy H. M. Wong", "Tianchi Liu", "Shuo Sun", "Wenyu Zhang", "Muhammad Huzaifah", "Nancy Chen", "Ai Ti Aw"], "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": "Accepted at (ASRU 2025) 2025 IEEE Automatic Speech Recognition and\n  Understanding Workshop", "summary": "Current large speech language models (Speech-LLMs) often exhibit limitations\nin empathetic reasoning, primarily due to the absence of training datasets that\nintegrate both contextual content and paralinguistic cues. In this work, we\npropose two approaches to incorporate contextual paralinguistic information\ninto model training: (1) an explicit method that provides paralinguistic\nmetadata (e.g., emotion annotations) directly to the LLM, and (2) an implicit\nmethod that automatically generates novel training question-answer (QA) pairs\nusing both categorical and dimensional emotion annotations alongside speech\ntranscriptions. Our implicit method boosts performance (LLM-judged) by 38.41%\non a human-annotated QA benchmark, reaching 46.02% when combined with the\nexplicit approach, showing effectiveness in contextual paralinguistic\nunderstanding. We also validate the LLM judge by demonstrating its correlation\nwith classification metrics, providing support for its reliability.", "AI": {"tldr": "This paper presents methods to enhance empathetic reasoning in Speech-LLMs by incorporating contextual paralinguistic cues during training.", "motivation": "To address the limitations in empathetic reasoning exhibited by current speech language models due to the lack of datasets integrating contextual content and paralinguistic cues.", "method": "Two methods are proposed: an explicit method supplying paralinguistic metadata directly to the LLM, and an implicit method that generates training QA pairs using emotion annotations and speech transcriptions.", "result": "The implicit method improved performance by 38.41% on a human-annotated QA benchmark, achieving a combined performance of 46.02% when used with the explicit method.", "conclusion": "These methods demonstrate that integrating paralinguistic information significantly enhances the contextual understanding of Speech-LLMs.", "key_contributions": ["Proposed explicit and implicit methods for integrating paralinguistic cues in LLM training.", "Achieved significant performance improvements on QA benchmarks.", "Validated the effectiveness of the LLM using correlation with classification metrics."], "limitations": "", "keywords": ["Speech-LLMs", "Empathy", "Paralinguistic cues", "Machine Learning", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.08158", "pdf": "https://arxiv.org/pdf/2508.08158.pdf", "abs": "https://arxiv.org/abs/2508.08158", "title": "Can AI Explanations Make You Change Your Mind?", "authors": ["Laura Spillner", "Rachel Ringe", "Robert Porzel", "Rainer Malaka"], "categories": ["cs.HC", "cs.AI"], "comment": "This paper was presented at the Explainable AI workshop at IJCAI\n  2025: https://sites.google.com/view/xai2025/proceedings", "summary": "In the context of AI-based decision support systems, explanations can help\nusers to judge when to trust the AI's suggestion, and when to question it. In\nthis way, human oversight can prevent AI errors and biased decision-making.\nHowever, this rests on the assumption that users will consider explanations in\nenough detail to be able to catch such errors. We conducted an online study on\ntrust in explainable DSS, and were surprised to find that in many cases,\nparticipants spent little time on the explanation and did not always consider\nit in detail. We present an exploratory analysis of this data, investigating\nwhat factors impact how carefully study participants consider AI explanations,\nand how this in turn impacts whether they are open to changing their mind based\non what the AI suggests.", "AI": {"tldr": "This paper explores the factors affecting users' attention to AI explanations in decision support systems and its impact on trust and decision-making.", "motivation": "Understanding user trust in AI recommendations is crucial to mitigate errors and biases in AI-based decision support systems.", "method": "An online study was conducted to analyze participant engagement with AI explanations and their willingness to adapt to AI suggestions.", "result": "Many participants did not spend sufficient time on the explanations provided by the AI, leading to uncertainties around their trust and oversight.", "conclusion": "The findings suggest that users often overlook critical explanations, which can hinder effective human oversight and trust in AI systems.", "key_contributions": ["Exploratory analysis of user interactions with AI explanations", "Identification of factors influencing attention to AI explanations", "Insights into the relationship between explanation consideration and trust"], "limitations": "The study may not account for all contextual variables that influence user behavior with AI explanations.", "keywords": ["AI decision support systems", "explainability", "user trust", "human oversight", "decision-making"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.07279", "pdf": "https://arxiv.org/pdf/2508.07279.pdf", "abs": "https://arxiv.org/abs/2508.07279", "title": "MAQuA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory", "authors": ["Vasudha Varadarajan", "Hui Xu", "Rebecca Astrid Boehme", "Mariam Marlan Mirstrom", "Sverker Sikstrom", "H. Andrew Schwartz"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) offer new opportunities for\nscalable, interactive mental health assessment, but excessive querying by LLMs\nburdens users and is inefficient for real-world screening across\ntransdiagnostic symptom profiles. We introduce MAQuA, an adaptive\nquestion-asking framework for simultaneous, multidimensional mental health\nscreening. Combining multi-outcome modeling on language responses with item\nresponse theory (IRT) and factor analysis, MAQuA selects the questions with\nmost informative responses across multiple dimensions at each turn to optimize\ndiagnostic information, improving accuracy and potentially reducing response\nburden. Empirical results on a novel dataset reveal that MAQuA reduces the\nnumber of assessment questions required for score stabilization by 50-87%\ncompared to random ordering (e.g., achieving stable depression scores with 71%\nfewer questions and eating disorder scores with 85% fewer questions). MAQuA\ndemonstrates robust performance across both internalizing (depression, anxiety)\nand externalizing (substance use, eating disorder) domains, with early stopping\nstrategies further reducing patient time and burden. These findings position\nMAQuA as a powerful and efficient tool for scalable, nuanced, and interactive\nmental health screening, advancing the integration of LLM-based agents into\nreal-world clinical workflows.", "AI": {"tldr": "MAQuA is an adaptive framework for efficient mental health screening using LLMs, reducing assessment questions significantly while maintaining accuracy.", "motivation": "To address the inefficiency of excessive querying in real-world mental health screening using large language models (LLMs).", "method": "Combines multi-outcome modeling on language responses, item response theory (IRT), and factor analysis to select the most informative questions for mental health assessment.", "result": "MAQuA reduces the number of required questions for stable assessment scores by 50-87% compared to random ordering, achieving significant reductions for depression (71% fewer questions) and eating disorders (85% fewer questions).", "conclusion": "MAQuA is positioned as a powerful tool for interactive mental health screening, facilitating the integration of LLM-based agents into clinical workflows.", "key_contributions": ["Introduction of an adaptive question-asking framework for mental health screening", "Reduction of assessment burden through efficient querying", "Demonstration of robust performance across multiple mental health domains"], "limitations": "", "keywords": ["Mental health", "Large language models", "Adaptive questioning", "Item response theory", "Screening"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.08242", "pdf": "https://arxiv.org/pdf/2508.08242.pdf", "abs": "https://arxiv.org/abs/2508.08242", "title": "Bringing Everyone to the Table: An Experimental Study of LLM-Facilitated Group Decision Making", "authors": ["Mohammed Alsobay", "David M. Rothschild", "Jake M. Hofman", "Daniel G. Goldstein"], "categories": ["cs.HC"], "comment": null, "summary": "Group decision-making often suffers from uneven information sharing,\nhindering decision quality. While large language models (LLMs) have been widely\nstudied as aids for individuals, their potential to support groups of users,\npotentially as facilitators, is relatively underexplored. We present a\npre-registered randomized experiment with 1,475 participants assigned to 281\nfive-person groups completing a hidden profile task--selecting an optimal city\nfor a hypothetical sporting event--under one of four facilitation conditions:\nno facilitation, a one-time message prompting information sharing, a human\nfacilitator, or an LLM (GPT-4o) facilitator. We find that LLM facilitation\nincreases information shared within a discussion by raising the minimum level\nof engagement with the task among group members, and that these gains come at\nlimited cost in terms of participants' attitudes towards the task, their group,\nor their facilitator. Whether by human or AI, there is no significant effect of\nfacilitation on the final decision outcome, suggesting that even substantial\nbut partial increases in information sharing are insufficient to overcome the\nhidden profile effect studied. To support further research into how LLM-based\ninterfaces can support the future of collaborative decision making, we release\nour experimental platform, the Group-AI Interaction Laboratory (GRAIL), as an\nopen-source tool.", "AI": {"tldr": "The paper explores the role of LLMs in group decision-making, revealing that while they enhance information sharing, this does not significantly impact decision outcomes.", "motivation": "The study addresses uneven information sharing in group decision-making, highlighting the underexplored potential of LLMs to aid group processes.", "method": "A pre-registered randomized experiment with 1,475 participants across 281 groups completing a hidden profile task under various facilitation conditions (LLM, human, prompting messages).", "result": "LLM facilitation increases information sharing by enhancing engagement, but does not significantly alter decision outcomes.", "conclusion": "While LLMs can improve information sharing in group settings, their impact on final decision quality remains limited; the hidden profile effect persists.", "key_contributions": ["Investigation of LLMs for group facilitation in decision-making contexts", "Release of the Group-AI Interaction Laboratory (GRAIL) as an open-source tool", "Evidence that LLM facilitation improves information sharing but not decision outcomes."], "limitations": "The results may not generalize beyond the specific task used in the experiment.", "keywords": ["group decision-making", "large language models", "information sharing", "collaborative decision making", "GRAIL"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.07284", "pdf": "https://arxiv.org/pdf/2508.07284.pdf", "abs": "https://arxiv.org/abs/2508.07284", "title": "\"Pull or Not to Pull?'': Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas", "authors": ["Junchen Ding", "Penghao Jiang", "Zihao Xu", "Ziqi Ding", "Yichen Zhu", "Jiaojiao Jiang", "Yuekang Li"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "As large language models (LLMs) increasingly mediate ethically sensitive\ndecisions, understanding their moral reasoning processes becomes imperative.\nThis study presents a comprehensive empirical evaluation of 14 leading LLMs,\nboth reasoning enabled and general purpose, across 27 diverse trolley problem\nscenarios, framed by ten moral philosophies, including utilitarianism,\ndeontology, and altruism. Using a factorial prompting protocol, we elicited\n3,780 binary decisions and natural language justifications, enabling analysis\nalong axes of decisional assertiveness, explanation answer consistency, public\nmoral alignment, and sensitivity to ethically irrelevant cues. Our findings\nreveal significant variability across ethical frames and model types: reasoning\nenhanced models demonstrate greater decisiveness and structured justifications,\nyet do not always align better with human consensus. Notably, \"sweet zones\"\nemerge in altruistic, fairness, and virtue ethics framings, where models\nachieve a balance of high intervention rates, low explanation conflict, and\nminimal divergence from aggregated human judgments. However, models diverge\nunder frames emphasizing kinship, legality, or self interest, often producing\nethically controversial outcomes. These patterns suggest that moral prompting\nis not only a behavioral modifier but also a diagnostic tool for uncovering\nlatent alignment philosophies across providers. We advocate for moral reasoning\nto become a primary axis in LLM alignment, calling for standardized benchmarks\nthat evaluate not just what LLMs decide, but how and why.", "AI": {"tldr": "This study evaluates the moral reasoning processes of 14 leading LLMs using trolley problem scenarios framed by various moral philosophies. It finds significant variability in decision-making and justifications among models, urging for enhanced moral reasoning alignment in LLM evaluation.", "motivation": "To investigate how large language models (LLMs) mediate ethically sensitive decisions and understand their moral reasoning processes.", "method": "Empirical evaluation of 14 leading LLMs across 27 trolley problem scenarios framed by ten moral philosophies, using a factorial prompting protocol to gather 3,780 binary decisions and natural language justifications.", "result": "Reasoning enhanced models exhibit more decisive responses and structured justifications, but do not consistently align with human consensus; 'sweet zones' identified in some ethical frames lead to optimal model performance.", "conclusion": "Moral reasoning should be a key focus for LLM alignment, necessitating standardized benchmarks that evaluate the decision-making processes of LLMs, not just outcomes.", "key_contributions": ["Evaluation of LLMs across diverse moral scenarios", "Identification of notable 'sweet zones' in ethical frameworks", "Proposal for moral reasoning as a primary axis in LLM alignment"], "limitations": "Variability in ethical outcomes based on moral frames and model types; controversial outcomes in kinship, legality, or self-interest scenarios.", "keywords": ["large language models", "moral reasoning", "ethical decision-making", "trolley problem", "alignment"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.07286", "pdf": "https://arxiv.org/pdf/2508.07286.pdf", "abs": "https://arxiv.org/abs/2508.07286", "title": "Arce: Augmented Roberta with Contextualized Elucidations for Ner in Automated Rule Checking", "authors": ["Jian Chen", "Jinbao Tian", "Yankui Li", "Zhou Li"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Accurate information extraction from specialized texts is a critical\nchallenge, particularly for named entity recognition (NER) in the architecture,\nengineering, and construction (AEC) domain to support automated rule checking\n(ARC). The performance of standard pre-trained models is often constrained by\nthe domain gap, as they struggle to interpret the specialized terminology and\ncomplex relational contexts inherent in AEC texts. Although this issue can be\nmitigated by further pre-training on large, human-curated domain corpora, as\nexemplified by methods like ARCBERT, this approach is both labor-intensive and\ncost-prohibitive. Consequently, leveraging large language models (LLMs) for\nautomated knowledge generation has emerged as a promising alternative. However,\nthe optimal strategy for generating knowledge that can genuinely enhance\nsmaller, efficient models remains an open question. To address this, we propose\nARCE (augmented RoBERTa with contextualized elucidations), a novel approach\nthat systematically explores and optimizes this generation process. ARCE\nemploys an LLM to first generate a corpus of simple, direct explanations, which\nwe term Cote, and then uses this corpus to incrementally pre-train a RoBERTa\nmodel prior to its fine-tuning on the downstream task. Our extensive\nexperiments show that ARCE establishes a new state-of-the-art on a benchmark\nAEC dataset, achieving a Macro-F1 score of 77.20%. This result also reveals a\nkey finding: simple, explanation-based knowledge proves surprisingly more\neffective than complex, role-based rationales for this task. The code is\npublicly available at:https://github.com/nxcc-lab/ARCE.", "AI": {"tldr": "The paper presents ARCE, a novel method for enhancing NER in AEC using large language models for knowledge generation, achieving a state-of-the-art Macro-F1 score.", "motivation": "To address the challenges of named entity recognition in the AEC domain, where standard models struggle with specialized terminology and relational contexts.", "method": "ARCE uses an LLM to generate simple explanations (Cote) which are then used for incremental pre-training of a RoBERTa model before fine-tuning on NER tasks.", "result": "ARCE achieves a Macro-F1 score of 77.20%, setting a new state-of-the-art on an AEC benchmark dataset.", "conclusion": "Simple explanation-based knowledge is more effective for NER in the AEC domain than complex rationales.", "key_contributions": ["Introduction of ARCE for knowledge generation in NER", "Establishment of new state-of-the-art metrics for AEC", "Demonstration of effectiveness of simple explanations over complex ones."], "limitations": "", "keywords": ["named entity recognition", "large language models", "knowledge generation", "architecture engineering construction", "RoBERTa"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.07295", "pdf": "https://arxiv.org/pdf/2508.07295.pdf", "abs": "https://arxiv.org/abs/2508.07295", "title": "CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text Factuality Evaluation", "authors": ["Yexing Du", "Kaiyuan Liu", "Youcheng Pan", "Zheng Chu", "Bo Yang", "Xiaocheng Feng", "Yang Xiang", "Ming Liu"], "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) are increasingly popularized in the\nmultilingual world, ensuring hallucination-free factuality becomes markedly\ncrucial. However, existing benchmarks for evaluating the reliability of\nMultimodal Large Language Models (MLLMs) predominantly focus on textual or\nvisual modalities with a primary emphasis on English, which creates a gap in\nevaluation when processing multilingual input, especially in speech. To bridge\nthis gap, we propose a novel \\textbf{C}ross-lingual and \\textbf{C}ross-modal\n\\textbf{F}actuality benchmark (\\textbf{CCFQA}). Specifically, the CCFQA\nbenchmark contains parallel speech-text factual questions across 8 languages,\ndesigned to systematically evaluate MLLMs' cross-lingual and cross-modal\nfactuality capabilities. Our experimental results demonstrate that current\nMLLMs still face substantial challenges on the CCFQA benchmark. Furthermore, we\npropose a few-shot transfer learning strategy that effectively transfers the\nQuestion Answering (QA) capabilities of LLMs in English to multilingual Spoken\nQuestion Answering (SQA) tasks, achieving competitive performance with\nGPT-4o-mini-Audio using just 5-shot training. We release CCFQA as a\nfoundational research resource to promote the development of MLLMs with more\nrobust and reliable speech understanding capabilities. Our code and dataset are\navailable at https://github.com/yxduir/ccfqa.", "AI": {"tldr": "The paper introduces CCFQA, a cross-lingual and cross-modal factuality benchmark for evaluating MLLMs across 8 languages, addressing gaps in existing benchmarks.", "motivation": "To evaluate the reliability of Multimodal Large Language Models (MLLMs) in multilingual contexts, especially for speech input, given the limitations of existing benchmarks that focus primarily on English.", "method": "Development of CCFQA, a benchmark containing parallel speech-text factual questions in 8 languages, along with a few-shot transfer learning strategy for improving multilingual Spoken Question Answering (SQA).", "result": "Experimental results show substantial challenges for current MLLMs on CCFQA, while the proposed few-shot transfer learning strategy achieves competitive performance with minimal training data.", "conclusion": "CCFQA serves as a foundational resource to enhance MLLMs' performance in speech understanding, addressing significant gaps in the evaluation of multilingual and cross-modal capabilities.", "key_contributions": ["Introduction of a novel benchmark for evaluating MLLMs in multilingual settings.", "Demonstration of substantial challenges faced by MLLMs on the new benchmark.", "Development of a few-shot learning strategy that improves multilingual QA performance."], "limitations": "The benchmark primarily tests the capabilities of existing MLLMs and may not cover all aspects of cross-lingual factuality.", "keywords": ["Multimodal Large Language Models", "Cross-lingual", "Cross-modal", "Factuality", "Question Answering"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.07308", "pdf": "https://arxiv.org/pdf/2508.07308.pdf", "abs": "https://arxiv.org/abs/2508.07308", "title": "HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways", "authors": ["Cristian Cosentino", "Annamaria Defilippo", "Marco Dossena", "Christopher Irwin", "Sara Joubbi", "Pietro Li√≤"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "HealthBranches is a novel benchmark dataset for medical Question-Answering\n(Q&A), specifically designed to evaluate complex reasoning in Large Language\nModels (LLMs). This dataset is generated through a semi-automated pipeline that\ntransforms explicit decision pathways from medical source into realistic\npatient cases with associated questions and answers. Covering 4,063 case\nstudies across 17 healthcare topics, each data point is based on clinically\nvalidated reasoning chains. HealthBranches supports both open-ended and\nmultiple-choice question formats and uniquely includes the full reasoning path\nfor each Q&A. Its structured design enables robust evaluation of LLMs'\nmulti-step inference capabilities, including their performance in structured\nRetrieval-Augmented Generation (RAG) contexts. HealthBranches establishes a\nfoundation for the development of more trustworthy, interpretable, and\nclinically reliable LLMs in high-stakes domains while also serving as a\nvaluable resource for educational purposes.", "AI": {"tldr": "HealthBranches is a benchmark dataset for evaluating complex reasoning in medical Question-Answering for Large Language Models.", "motivation": "To create a reliable dataset that assesses the reasoning capabilities of LLMs in medical Q&A contexts.", "method": "The dataset is generated via a semi-automated pipeline that transforms decision pathways from medical sources into patient cases with questions and answers.", "result": "The dataset covers 4,063 case studies across 17 healthcare topics and includes structured reasoning paths for each Q&A.", "conclusion": "HealthBranches provides a base for developing trustworthy LLMs in healthcare and offers educational resources.", "key_contributions": ["Novel benchmark dataset for medical Q&A", "Supports open-ended and multiple-choice formats", "Includes full reasoning paths for evaluations"], "limitations": "", "keywords": ["medical Q&A", "large language models", "dataset", "reasoning", "healthcare"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.07321", "pdf": "https://arxiv.org/pdf/2508.07321.pdf", "abs": "https://arxiv.org/abs/2508.07321", "title": "ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering", "authors": ["Shubhra Ghosh", "Abhilekh Borah", "Aditya Kumar Guru", "Kripabandhu Ghosh"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": null, "summary": "The rapid proliferation of Large Language Models (LLMs) has significantly\ncontributed to the development of equitable AI systems capable of factual\nquestion-answering (QA). However, no known study tests the LLMs' robustness\nwhen presented with obfuscated versions of questions. To systematically\nevaluate these limitations, we propose a novel technique, ObfusQAte and,\nleveraging the same, introduce ObfusQA, a comprehensive, first of its kind,\nframework with multi-tiered obfuscation levels designed to examine LLM\ncapabilities across three distinct dimensions: (i) Named-Entity Indirection,\n(ii) Distractor Indirection, and (iii) Contextual Overload. By capturing these\nfine-grained distinctions in language, ObfusQA provides a comprehensive\nbenchmark for evaluating LLM robustness and adaptability. Our study observes\nthat LLMs exhibit a tendency to fail or generate hallucinated responses when\nconfronted with these increasingly nuanced variations. To foster research in\nthis direction, we make ObfusQAte publicly available.", "AI": {"tldr": "The paper introduces ObfusQA, a framework for evaluating LLMs' robustness against obfuscated questions, revealing their limitations and making the methodology publicly available.", "motivation": "The study addresses the gap in understanding LLMs' performance under obfuscated questions, highlighting the necessity for robust AI systems in factual question-answering.", "method": "Developed the ObfusQA framework which includes multi-tiered obfuscation levels to assess LLM capabilities in three areas: Named-Entity Indirection, Distractor Indirection, and Contextual Overload.", "result": "The analysis shows that LLMs tend to fail or hallucinate responses under nuanced question obfuscations.", "conclusion": "ObfusQA serves as a comprehensive benchmark for evaluating LLM robustness and adaptability, promoting further research in the evaluation of AI systems.", "key_contributions": ["Introduction of the ObfusQA framework for evaluating LLM robustness against question obfuscations", "Identification of LLM performance limitations under nuanced variations", "Public availability of the ObfusQAte methodology for research purposes"], "limitations": "", "keywords": ["Large Language Models", "robustness evaluation", "question obfuscation", "AI systems", "benchmarking"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.07325", "pdf": "https://arxiv.org/pdf/2508.07325.pdf", "abs": "https://arxiv.org/abs/2508.07325", "title": "Strategies of Code-switching in Human-Machine Dialogs", "authors": ["Dean Geckt", "Melinda Fricke", "Shuly Wintner"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Most people are multilingual, and most multilinguals code-switch, yet the\ncharacteristics of code-switched language are not fully understood. We\ndeveloped a chatbot capable of completing a Map Task with human participants\nusing code-switched Spanish and English. In two experiments, we prompted the\nbot to code-switch according to different strategies, examining (1) the\nfeasibility of such experiments for investigating bilingual language use, and\n(2) whether participants would be sensitive to variations in discourse and\ngrammatical patterns. Participants generally enjoyed code-switching with our\nbot as long as it produced predictable code-switching behavior; when\ncode-switching was random or ungrammatical (as when producing unattested\nincongruent mixed-language noun phrases, such as `la fork'), participants\nenjoyed the task less and were less successful at completing it. These results\nunderscore the potential downsides of deploying insufficiently developed\nmultilingual language technology, while also illustrating the promise of such\ntechnology for conducting research on bilingual language use.", "AI": {"tldr": "Study on a chatbot that effectively utilizes code-switching between Spanish and English, examining user interactions and preferences.", "motivation": "To investigate the characteristics of code-switched language and the feasibility of using chatbots in bilingual language research.", "method": "Developed a chatbot and conducted two experiments prompting code-switching strategies, analyzing participant engagement and completion of the Map Task.", "result": "Participants preferred predictable code-switching patterns; less success and enjoyment were noted with random or ungrammatical code-switching.", "conclusion": "The findings highlight both the challenges and potential of multilingual technology in bilingual research contexts.", "key_contributions": ["Developed a chatbot for code-switching interactions", "Examined user sensitivity to discourse variations", "Identified the impact of language predictability on user engagement"], "limitations": "Limited scope of code-switching explored; results may not generalize to all multilingual contexts.", "keywords": ["code-switching", "bilingualism", "chatbot", "language technology", "user experience"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.07517", "pdf": "https://arxiv.org/pdf/2508.07517.pdf", "abs": "https://arxiv.org/abs/2508.07517", "title": "Word Clouds as Common Voices: LLM-Assisted Visualization of Participant-Weighted Themes in Qualitative Interviews", "authors": ["Joseph T. Colonel", "Baihan Lin"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Word clouds are a common way to summarize qualitative interviews, yet\ntraditional frequency-based methods often fail in conversational contexts: they\nsurface filler words, ignore paraphrase, and fragment semantically related\nideas. This limits their usefulness in early-stage analysis, when researchers\nneed fast, interpretable overviews of what participant actually said. We\nintroduce ThemeClouds, an open-source visualization tool that uses large\nlanguage models (LLMs) to generate thematic, participant-weighted word clouds\nfrom dialogue transcripts. The system prompts an LLM to identify concept-level\nthemes across a corpus and then counts how many unique participants mention\neach topic, yielding a visualization grounded in breadth of mention rather than\nraw term frequency. Researchers can customize prompts and visualization\nparameters, providing transparency and control. Using interviews from a user\nstudy comparing five recording-device configurations (31 participants; 155\ntranscripts, Whisper ASR), our approach surfaces more actionable device\nconcerns than frequency clouds and topic-modeling baselines (e.g., LDA,\nBERTopic). We discuss design trade-offs for integrating LLM assistance into\nqualitative workflows, implications for interpretability and researcher agency,\nand opportunities for interactive analyses such as per-condition contrasts\n(``diff clouds'').", "AI": {"tldr": "ThemeClouds is an open-source tool that uses LLMs to create thematic, participant-weighted word clouds from dialogue transcripts, improving qualitative analysis of interviews.", "motivation": "Traditional frequency-based word clouds fail to accurately represent conversational contexts in qualitative research, limiting their usefulness during early-stage analysis of participant interviews.", "method": "The system employs large language models to identify concept-level themes and counts unique participant mentions for each topic, yielding a participant-weighted visualization.", "result": "The ThemeClouds approach provided more actionable insights into device concerns than traditional frequency clouds and topic-modeling techniques like LDA and BERTopic.", "conclusion": "Integrating LLMs into qualitative workflows can enhance interpretability and researcher agency while offering opportunities for interactive analyses, such as contrasting conditions.", "key_contributions": ["Introduction of ThemeClouds, a new visualization tool for qualitative research using LLMs", "Participant-weighted thematic representation improves understanding of qualitative data", "Customization options enhance researcher control and transparency in the analysis process"], "limitations": "", "keywords": ["Human-Computer Interaction", "Word Clouds", "Qualitative Research", "Large Language Models", "Thematic Analysis"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.07375", "pdf": "https://arxiv.org/pdf/2508.07375.pdf", "abs": "https://arxiv.org/abs/2508.07375", "title": "Think Before You Talk: Enhancing Meaningful Dialogue Generation in Full-Duplex Speech Language Models with Planning-Inspired Text Guidance", "authors": ["Wenqian Cui", "Lei Zhu", "Xiaohui Li", "Zhihan Guo", "Haoli Bai", "Lu Hou", "Irwin King"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Work in progress", "summary": "Full-Duplex Speech Language Models (FD-SLMs) are specialized foundation\nmodels designed to enable natural, real-time spoken interactions by modeling\ncomplex conversational dynamics such as interruptions, backchannels, and\noverlapping speech, and End-to-end (e2e) FD-SLMs leverage real-world\ndouble-channel conversational data to capture nuanced two-speaker dialogue\npatterns for human-like interactions. However, they face a critical challenge\n-- their conversational abilities often degrade compared to pure-text\nconversation due to prolonged speech sequences and limited high-quality spoken\ndialogue data. While text-guided speech generation could mitigate these issues,\nit suffers from timing and length issues when integrating textual guidance into\ndouble-channel audio streams, disrupting the precise time alignment essential\nfor natural interactions. To address these challenges, we propose TurnGuide, a\nnovel planning-inspired approach that mimics human conversational planning by\ndynamically segmenting assistant speech into dialogue turns and generating\nturn-level text guidance before speech output, which effectively resolves both\ninsertion timing and length challenges. Extensive experiments demonstrate our\napproach significantly improves e2e FD-SLMs' conversational abilities, enabling\nthem to generate semantically meaningful and coherent speech while maintaining\nnatural conversational flow. Demos are available at\nhttps://dreamtheater123.github.io/TurnGuide-Demo/. Code will be available at\nhttps://github.com/dreamtheater123/TurnGuide.", "AI": {"tldr": "This paper introduces TurnGuide, a method for improving Full-Duplex Speech Language Models (FD-SLMs) by enhancing their conversational dynamics through dynamic segmentation and turn-level text guidance.", "motivation": "FD-SLMs struggle with conversational quality due to issues like prolonged speech sequences and limited spoken dialogue data, which this work aims to address.", "method": "TurnGuide segments assistant speech into dialogue turns and generates corresponding turn-level text guidance, thereby improving timing and length alignment in speech generation.", "result": "Experiments show that the TurnGuide approach significantly enhances the conversational abilities of e2e FD-SLMs, resulting in coherent and semantically meaningful speech output.", "conclusion": "The proposed method addresses critical timing and length issues in speech generation for FD-SLMs, resulting in more natural and effective human-like interactions.", "key_contributions": ["Introduction of TurnGuide to enhance FD-SLMs", "Dynamic segmentation of assistant speech", "Improvement of conversational capabilities through planned dialogue turns"], "limitations": "Still a work in progress and may require further validation and optimization.", "keywords": ["Full-Duplex Speech Language Models", "Conversational AI", "Speech Generation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.07414", "pdf": "https://arxiv.org/pdf/2508.07414.pdf", "abs": "https://arxiv.org/abs/2508.07414", "title": "Grounding Multilingual Multimodal LLMs With Cultural Knowledge", "authors": ["Jean de Dieu Nyandwi", "Yueqi Song", "Simran Khanuja", "Graham Neubig"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multimodal Large Language Models excel in high-resource settings, but often\nmisinterpret long-tail cultural entities and underperform in low-resource\nlanguages. To address this gap, we propose a data-centric approach that\ndirectly grounds MLLMs in cultural knowledge. Leveraging a large scale\nknowledge graph from Wikidata, we collect images that represent culturally\nsignificant entities, and generate synthetic multilingual visual question\nanswering data. The resulting dataset, CulturalGround, comprises 22 million\nhigh-quality, culturally-rich VQA pairs spanning 42 countries and 39 languages.\nWe train an open-source MLLM CulturalPangea on CulturalGround, interleaving\nstandard multilingual instruction-tuning data to preserve general abilities.\nCulturalPangea achieves state-of-the-art performance among open models on\nvarious culture-focused multilingual multimodal benchmarks, outperforming prior\nmodels by an average of 5.0 without degrading results on mainstream\nvision-language tasks. Our findings show that our targeted, culturally grounded\napproach could substantially narrow the cultural gap in MLLMs and offer a\npractical path towards globally inclusive multimodal systems.", "AI": {"tldr": "This paper presents CulturalGround, a dataset designed to enhance the performance of Multimodal Large Language Models (MLLMs) on culturally significant entities and low-resource languages, achieving state-of-the-art results on culture-focused benchmarks with a new model, CulturalPangea.", "motivation": "To address the underperformance of MLLMs in low-resource languages and cultural contexts, aiming for a more inclusive multimodal system.", "method": "A data-centric approach was implemented by creating the CulturalGround dataset from a knowledge graph, which consisted of 22 million visual question answering pairs in multiple languages, and training the CulturalPangea MLLM on this dataset combined with standard multilingual data.", "result": "CulturalPangea outperforms previous models by an average of 5.0 on culture-focused multilingual multimodal benchmarks while maintaining performance on mainstream tasks.", "conclusion": "The proposed culturally grounded method significantly narrows the cultural gap in MLLMs, suggesting a viable approach for enhanced inclusivity in multimodal AI systems.", "key_contributions": ["Introduction of a large-scale dataset CulturalGround that includes culturally rich VQA pairs.", "Development of CulturalPangea, an open-source MLLM that trains on this new dataset and achieves state-of-the-art results.", "Demonstrated practical methods for improving MLLM performance on underrepresented cultural entities."], "limitations": "The study primarily focuses on cultural representation and may not generalize to other domains outside of culturally significant entities.", "keywords": ["Multimodal Large Language Models", "Cultural Knowledge", "Visual Question Answering", "Multilingual", "Knowledge Graph"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2508.07434", "pdf": "https://arxiv.org/pdf/2508.07434.pdf", "abs": "https://arxiv.org/abs/2508.07434", "title": "Let's Revise Step-by-Step: A Unified Local Search Framework for Code Generation with LLMs", "authors": ["Zhiyi Lyu", "Jianguo Huang", "Yanchen Deng", "Steven Hoi", "Bo An"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) with inference-time scaling techniques show\npromise for code generation, yet face notable efficiency and scalability\nchallenges. Construction-based tree-search methods suffer from rapid growth in\ntree size, high token consumption, and lack of anytime property. In contrast,\nimprovement-based methods offer better performance but often struggle with\nuninformative reward signals and inefficient search strategies. In this work,\nwe propose \\textbf{ReLoc}, a unified local search framework which effectively\nperforms step-by-step code revision. Specifically, ReLoc explores a series of\nlocal revisions through four key algorithmic components: initial code drafting,\nneighborhood code generation, candidate evaluation, and incumbent code\nupdating, each of which can be instantiated with specific decision rules to\nrealize different local search algorithms such as Hill Climbing (HC) or Genetic\nAlgorithm (GA). Furthermore, we develop a specialized revision reward model\nthat evaluates code quality based on revision distance to produce fine-grained\npreferences that guide the local search toward more promising candidates.\nFinally, our extensive experimental results demonstrate that our approach\nachieves superior performance across diverse code generation tasks,\nsignificantly outperforming both construction-based tree search as well as the\nstate-of-the-art improvement-based code generation methods.", "AI": {"tldr": "ReLoc is a unified local search framework for code revision that outperforms existing code generation methods.", "motivation": "Address challenges in efficiency and scalability of code generation using LLMs.", "method": "ReLoc employs a local search framework with four components: initial code drafting, neighborhood code generation, candidate evaluation, and incumbent updating, utilizing decision rules for algorithms like HC or GA.", "result": "ReLoc achieves superior performance in various code generation tasks, surpassing both construction-based and state-of-the-art improvement-based methods.", "conclusion": "The proposed approach significantly enhances the effectiveness of code generation through structured local revisions and a specialized revision reward model.", "key_contributions": ["Introduction of ReLoc framework for local search in code generation", "Development of a revision reward model for evaluating code quality", "Demonstrated superior performance in diverse code generation tasks"], "limitations": "", "keywords": ["Large Language Models", "Code Generation", "Local Search", "Reinforcement Learning", "Algorithms"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.07479", "pdf": "https://arxiv.org/pdf/2508.07479.pdf", "abs": "https://arxiv.org/abs/2508.07479", "title": "Positional Biases Shift as Inputs Approach Context Window Limits", "authors": ["Blerta Veseli", "Julian Chibane", "Mariya Toneva", "Alexander Koller"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often struggle to use information across long\ninputs effectively. Prior work has identified positional biases, such as the\nLost in the Middle (LiM) effect, where models perform better when information\nappears at the beginning (primacy bias) or end (recency bias) of the input,\nrather than in the middle. However, long-context studies have not consistently\nreplicated these effects, raising questions about their intensity and the\nconditions under which they manifest. To address this, we conducted a\ncomprehensive analysis using relative rather than absolute input lengths,\ndefined with respect to each model's context window. Our findings reveal that\nthe LiM effect is strongest when inputs occupy up to 50% of a model's context\nwindow. Beyond that, the primacy bias weakens, while recency bias remains\nrelatively stable. This effectively eliminates the LiM effect; instead, we\nobserve a distance-based bias, where model performance is better when relevant\ninformation is closer to the end of the input. Furthermore, our results suggest\nthat successful retrieval is a prerequisite for reasoning in LLMs, and that the\nobserved positional biases in reasoning are largely inherited from retrieval.\nThese insights have implications for long-context tasks, the design of future\nLLM benchmarks, and evaluation methodologies for LLMs handling extended inputs.", "AI": {"tldr": "This study analyzes the effective use of long inputs by Large Language Models (LLMs), revealing a distance-based bias in performance with positional influences based on input length.", "motivation": "To understand the inconsistencies in prior findings related to positional biases in LLMs when processing long inputs and to explore conditions for their manifestation.", "method": "A comprehensive analysis using relative input lengths defined in relation to the model's context window, examining the performance differences based on input positioning.", "result": "The LiM effect is strongest when inputs are up to 50% of a model's context window; beyond that, primacy bias weakens while recency bias remains stable, indicating a shift to distance-based bias.", "conclusion": "Positional biases in LLMs are inherited from retrieval mechanisms, suggesting important implications for designing future benchmarks and evaluation methodologies for long-context input tasks.", "key_contributions": ["Introduced a relative input length analysis framework for LLMs.", "Identified the distance-based bias effect in model performance with long inputs.", "Provided new insights into the role of retrieval in reasoning for LLMs."], "limitations": "The study primarily focuses on performance related to long-context inputs and may not fully address other biases or contexts.", "keywords": ["Large Language Models", "positional biases", "context window", "retrieval", "reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.07484", "pdf": "https://arxiv.org/pdf/2508.07484.pdf", "abs": "https://arxiv.org/abs/2508.07484", "title": "ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models", "authors": ["Archchana Sindhujan", "Shenbin Qian", "Chan Chi Chun Matthew", "Constantin Orasan", "Diptesh Kanojia"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to COLM 2025 Conference", "summary": "Large Language Models (LLMs) have shown remarkable performance across a wide\nrange of natural language processing tasks. Quality Estimation (QE) for Machine\nTranslation (MT), which assesses the quality of a source-target pair without\nrelying on reference translations, remains a challenging cross-lingual task for\nLLMs. The challenges stem from the inherent limitations of existing LLM-based\nQE systems, which are pre-trained for causal language modelling rather than\nregression-specific tasks, further elevated by the presence of low-resource\nlanguages given pre-training data distribution. This paper introduces ALOPE, an\nadaptive layer-optimization framework designed to enhance LLM-based QE by\nrestructuring Transformer representations through layer-wise adaptation for\nimproved regression-based prediction. Our framework integrates low-rank\nadapters (LoRA) with regression task heads, leveraging selected pre-trained\nTransformer layers for improved cross-lingual alignment. In addition to the\nlayer-specific adaptation, ALOPE introduces two strategies-dynamic weighting,\nwhich adaptively combines representations from multiple layers, and multi-head\nregression, which aggregates regression losses from multiple heads for QE. Our\nframework shows improvements over various existing LLM-based QE approaches.\nEmpirical evidence suggests that intermediate Transformer layers in LLMs\nprovide contextual representations that are more aligned with the cross-lingual\nnature of the QE task. We make resultant models and framework code publicly\navailable for further research, also allowing existing LLM-based MT frameworks\nto be scaled with QE capabilities.", "AI": {"tldr": "Introduction of ALOPE, an adaptive layer-optimization framework to improve Quality Estimation for Machine Translation using LLMs.", "motivation": "Existing LLM-based QE systems struggle due to their pre-training for causal language modeling rather than regression, especially for low-resource languages.", "method": "The ALOPE framework applies layer-wise adaptation using low-rank adapters with regression task heads and includes dynamic weighting and multi-head regression strategies for improved predictions.", "result": "ALOPE demonstrates significant improvements over current LLM-based QE methods, benefiting from better cross-lingual alignment through layer-specific adaptations.", "conclusion": "The study provides a new method for enhancing Quality Estimation in Machine Translation, making available resulting models and code to further research in the field.", "key_contributions": ["Introduction of layer-wise adaptation for LLMs in Quality Estimation", "Use of low-rank adapters with regression heads", "Publication of models and code for further research."], "limitations": "", "keywords": ["Large Language Models", "Quality Estimation", "Machine Translation", "Layer-wise Adaptation", "Cross-lingual Tasks"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.07516", "pdf": "https://arxiv.org/pdf/2508.07516.pdf", "abs": "https://arxiv.org/abs/2508.07516", "title": "Augmenting Bias Detection in LLMs Using Topological Data Analysis", "authors": ["Keshav Varadarajan", "Tananun Songdechakraiwut"], "categories": ["cs.CL"], "comment": "15 pages, 9 figures, 4 tables", "summary": "Recently, many bias detection methods have been proposed to determine the\nlevel of bias a large language model captures. However, tests to identify which\nparts of a large language model are responsible for bias towards specific\ngroups remain underdeveloped. In this study, we present a method using\ntopological data analysis to identify which heads in GPT-2 contribute to the\nmisrepresentation of identity groups present in the StereoSet dataset. We find\nthat biases for particular categories, such as gender or profession, are\nconcentrated in attention heads that act as hot spots. The metric we propose\ncan also be used to determine which heads capture bias for a specific group\nwithin a bias category, and future work could extend this method to help\nde-bias large language models.", "AI": {"tldr": "This study proposes a method using topological data analysis to identify which attention heads in GPT-2 contribute to bias in large language models, particularly focusing on gender and profession.", "motivation": "To address the lack of methods that pinpoint specific parts of large language models responsible for bias against identity groups, enhancing understanding and mitigation of such biases.", "method": "The authors use topological data analysis to analyze the attention heads of GPT-2 in relation to the types of bias evidenced in the StereoSet dataset.", "result": "The study identifies that certain attention heads serve as 'hot spots' for biases related to gender and profession, highlighting specific sections of the model that misrepresent identity groups.", "conclusion": "Future research can build on this metric to develop methodologies aimed at de-biasing large language models by targeting identified heads that contribute to biases.", "key_contributions": ["Novel application of topological data analysis to analyze large language models for bias", "Identification of specific attention heads responsible for category-specific biases", "Framework for future de-biasing efforts in language models."], "limitations": "The method focuses on GPT-2 and may not generalize to other language models; further exploration of the implications of the findings is necessary.", "keywords": ["bias detection", "topological data analysis", "GPT-2", "large language models", "StereoSet"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.07517", "pdf": "https://arxiv.org/pdf/2508.07517.pdf", "abs": "https://arxiv.org/abs/2508.07517", "title": "Word Clouds as Common Voices: LLM-Assisted Visualization of Participant-Weighted Themes in Qualitative Interviews", "authors": ["Joseph T. Colonel", "Baihan Lin"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Word clouds are a common way to summarize qualitative interviews, yet\ntraditional frequency-based methods often fail in conversational contexts: they\nsurface filler words, ignore paraphrase, and fragment semantically related\nideas. This limits their usefulness in early-stage analysis, when researchers\nneed fast, interpretable overviews of what participant actually said. We\nintroduce ThemeClouds, an open-source visualization tool that uses large\nlanguage models (LLMs) to generate thematic, participant-weighted word clouds\nfrom dialogue transcripts. The system prompts an LLM to identify concept-level\nthemes across a corpus and then counts how many unique participants mention\neach topic, yielding a visualization grounded in breadth of mention rather than\nraw term frequency. Researchers can customize prompts and visualization\nparameters, providing transparency and control. Using interviews from a user\nstudy comparing five recording-device configurations (31 participants; 155\ntranscripts, Whisper ASR), our approach surfaces more actionable device\nconcerns than frequency clouds and topic-modeling baselines (e.g., LDA,\nBERTopic). We discuss design trade-offs for integrating LLM assistance into\nqualitative workflows, implications for interpretability and researcher agency,\nand opportunities for interactive analyses such as per-condition contrasts\n(``diff clouds'').", "AI": {"tldr": "ThemeClouds is a visualization tool that utilizes large language models to generate thematic word clouds from qualitative interview data, enhancing the interpretability of participant responses.", "motivation": "Traditional word cloud methods based on frequency analysis are ineffective for qualitative interview data, which can include filler words and fragmented themes.", "method": "ThemeClouds prompts an LLM to identify themes across dialogue transcripts, creating participant-weighted visualizations that reflect the number of unique contributors to each theme.", "result": "ThemeClouds provides more actionable insights into participant concerns compared to traditional frequency-based methods and established topic modeling techniques like LDA and BERTopic.", "conclusion": "The integration of LLM assistance in qualitative research can enhance interpretability and customize workflows while allowing for nuanced analysis.", "key_contributions": ["Introduces a novel visualization tool that leverages LLMs for qualitative data", "Enhances interpretability of qualitative data through thematic visualization", "Provides customizable options for researchers to control prompts and visualization parameters."], "limitations": "May require careful prompt design to ensure meaningful thematic extraction.", "keywords": ["Word clouds", "Qualitative research", "Large language models", "Thematic analysis", "User studies"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.07534", "pdf": "https://arxiv.org/pdf/2508.07534.pdf", "abs": "https://arxiv.org/abs/2508.07534", "title": "From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR", "authors": ["Jia Deng", "Jie Chen", "Zhipeng Chen", "Daixuan Cheng", "Fei Bai", "Beichen Zhang", "Yinqian Min", "Yanzipeng Gao", "Wayne Xin Zhao", "Ji-Rong Wen"], "categories": ["cs.CL"], "comment": "27pages,25figures. arXiv admin note: text overlap with\n  arXiv:2508.02260", "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a\npowerful paradigm for enhancing the reasoning capabilities of large language\nmodels (LLMs). Unlike traditional RL approaches, RLVR leverages rule-based\nfeedback to guide LLMs in generating and refining complex reasoning chains -- a\nprocess critically dependent on effective exploration strategies. While prior\nwork has demonstrated RLVR's empirical success, the fundamental mechanisms\ngoverning LLMs' exploration behaviors remain underexplored. This technical\nreport presents a systematic investigation of exploration capacities in RLVR,\ncovering four main aspects: (1) exploration space shaping, where we develop\nquantitative metrics to characterize LLMs' capability boundaries; (2)\nentropy-performance exchange, analyzed across training stages, individual\ninstances, and token-level patterns; and (3) RL performance optimization,\nexamining methods to effectively translate exploration gains into measurable\nimprovements. By unifying previously identified insights with new empirical\nevidence, this work aims to provide a foundational framework for advancing RLVR\nsystems.", "AI": {"tldr": "This report investigates exploration behaviors in reinforcement learning with verifiable rewards (RLVR) for large language models (LLMs).", "motivation": "The work addresses the lack of understanding of exploration behaviors in LLMs when using RLVR, which is crucial for effectively guiding reasoning chains in LLMs.", "method": "The paper systematically investigates four aspects of exploration capacities in RLVR, including exploration space shaping, entropy-performance exchange, and RL performance optimization.", "result": "The study develops quantitative metrics for LLMs' capability boundaries, analyzes entropy-performance exchanges across various dimensions, and explores methods to improve RL performance based on exploration gains.", "conclusion": "The findings unify existing insights with new evidence, establishing a foundational framework for improving RLVR systems in LLMs.", "key_contributions": ["Development of quantitative metrics for exploration space shaping", "Analysis of entropy-performance exchange across training stages", "Exploration of methods for optimizing RL performance based on exploration gains"], "limitations": "The fundamental mechanisms governing exploration behaviors will require further exploration beyond this work.", "keywords": ["reinforcement learning", "large language models", "exploration strategies", "verifiable rewards", "machine learning"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2508.07592", "pdf": "https://arxiv.org/pdf/2508.07592.pdf", "abs": "https://arxiv.org/abs/2508.07592", "title": "IBPS: Indian Bail Prediction System", "authors": ["Puspesh Kumar Srivastava", "Uddeshya Raj", "Praveen Patel", "/Shubham Kumar Nigam", "Noel Shallum", "Arnab Bhattacharya"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Bail decisions are among the most frequently adjudicated matters in Indian\ncourts, yet they remain plagued by subjectivity, delays, and inconsistencies.\nWith over 75% of India's prison population comprising undertrial prisoners,\nmany from socioeconomically disadvantaged backgrounds, the lack of timely and\nfair bail adjudication exacerbates human rights concerns and contributes to\nsystemic judicial backlog. In this paper, we present the Indian Bail Prediction\nSystem (IBPS), an AI-powered framework designed to assist in bail\ndecision-making by predicting outcomes and generating legally sound rationales\nbased solely on factual case attributes and statutory provisions. We curate and\nrelease a large-scale dataset of 150,430 High Court bail judgments, enriched\nwith structured annotations such as age, health, criminal history, crime\ncategory, custody duration, statutes, and judicial reasoning. We fine-tune a\nlarge language model using parameter-efficient techniques and evaluate its\nperformance across multiple configurations, with and without statutory context,\nand with RAG. Our results demonstrate that models fine-tuned with statutory\nknowledge significantly outperform baselines, achieving strong accuracy and\nexplanation quality, and generalize well to a test set independently annotated\nby legal experts. IBPS offers a transparent, scalable, and reproducible\nsolution to support data-driven legal assistance, reduce bail delays, and\npromote procedural fairness in the Indian judicial system.", "AI": {"tldr": "The Indian Bail Prediction System (IBPS) utilizes AI to improve bail decision-making by predicting outcomes based on factual case attributes and statutory provisions, addressing issues of subjectivity and delays in the judicial system.", "motivation": "Bail decisions in Indian courts are subjective and inconsistent, contributing to human rights issues and judicial backlog, particularly affecting undertrial prisoners from disadvantaged backgrounds.", "method": "An AI-powered framework that predicts bail outcomes using a large dataset of 150,430 High Court bail judgments with structured annotations. A large language model was fine-tuned with parameter-efficient techniques and evaluated across multiple configurations.", "result": "Models fine-tuned with statutory knowledge significantly outperform baselines in accuracy and explanation quality, demonstrating effective prediction and support for legal practitioners.", "conclusion": "IBPS provides a transparent and scalable solution for data-driven legal assistance, potentially reducing delays in bail processes and enhancing fairness in the Indian judicial system.", "key_contributions": ["Large-scale dataset of bail judgments released for research", "Fine-tuned LLMs show improved performance with statutory context", "Tool promotes procedural fairness in bail decision-making"], "limitations": "", "keywords": ["Bail prediction", "Legal AI", "Machine learning", "Human rights", "Judicial system"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2405.13701", "pdf": "https://arxiv.org/pdf/2405.13701.pdf", "abs": "https://arxiv.org/abs/2405.13701", "title": "Metabook: A Mobile-to-Headset Pipeline for 3D Story Book Creation in Augmented Reality", "authors": ["Yibo Wang", "Yuanyuan Mao", "Lik-Hang Lee", "Shi-ting Ni", "Zeyu Wang", "Xiaole Gu", "Pan Hui"], "categories": ["cs.HC"], "comment": null, "summary": "The AR 3D book has shown significant potential in enhancing students'\nlearning outcomes. However, the creation process of 3D books requires a\nsignificant investment of time, effort, and specialized skills. Thus, in this\npaper, we first conduct a three-day workshop investigating how AI can support\nthe automated creation of 3D books. Informed by the design insights derived\nfrom the workshop, we developed Metabook, a system that enables even novice\nusers to create 3D books from text automatically. To our knowledge, Metabook is\nthe first system to offer end-to-end 3D book generation. A follow-up study with\nadult users indicates that Metabook enables inexperienced users to create 3D\nbooks, achieving reduced efforts and shortened preparation time. We\nsubsequently recruited 22 children to examine the effects of AR 3D books on\nchildren's learning compared with paper-based books. The findings indicate that\n3D books significantly enhance children's interest, improve memory retention,\nand reduce cognitive load, though no significant improvement was observed in\ncomprehension. We conclude by discussing strategies for more effectively\nleveraging 3D books to support children's learning and offer practical\nrecommendations for educators.", "AI": {"tldr": "This paper presents Metabook, a system for automating the creation of AR 3D books, which enhances learning outcomes for children but shows mixed results in comprehension.", "motivation": "To explore how AI can assist in the time-intensive development of 3D educational books.", "method": "Conducted a workshop to gather design insights and developed the Metabook system, followed by studies on user creation of 3D books and their effects on children's learning.", "result": "Metabook allows novice users to generate 3D books quickly, and a study revealed increased interest and memory retention in children using 3D books over traditional ones.", "conclusion": "3D books enhance certain aspects of learning but do not significantly improve comprehension; recommendations for educators are provided for effective use.", "key_contributions": ["Development of Metabook for automated 3D book creation", "First end-to-end 3D book generation system", "Empirical findings on the impact of AR 3D books on children's learning"], "limitations": "No significant improvement in comprehension observed.", "keywords": ["AR 3D books", "automated creation", "learning outcomes", "children education", "Metabook"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.07598", "pdf": "https://arxiv.org/pdf/2508.07598.pdf", "abs": "https://arxiv.org/abs/2508.07598", "title": "Keyword-Centric Prompting for One-Shot Event Detection with Self-Generated Rationale Enhancements", "authors": ["Ziheng Li", "Zhi-Hong Deng"], "categories": ["cs.CL"], "comment": "ECAI 2025", "summary": "Although the LLM-based in-context learning (ICL) paradigm has demonstrated\nconsiderable success across various natural language processing tasks, it\nencounters challenges in event detection. This is because LLMs lack an accurate\nunderstanding of event triggers and tend to make over-interpretation, which\ncannot be effectively corrected through in-context examples alone. In this\npaper, we focus on the most challenging one-shot setting and propose KeyCP++, a\nkeyword-centric chain-of-thought prompting approach. KeyCP++ addresses the\nweaknesses of conventional ICL by automatically annotating the logical gaps\nbetween input text and detection results for the demonstrations. Specifically,\nto generate in-depth and meaningful rationale, KeyCP++ constructs a trigger\ndiscrimination prompting template. It incorporates the exemplary triggers\n(a.k.a keywords) into the prompt as the anchor to simply trigger profiling, let\nLLM propose candidate triggers, and justify each candidate. These\npropose-and-judge rationales help LLMs mitigate over-reliance on the keywords\nand promote detection rule learning. Extensive experiments demonstrate the\neffectiveness of our approach, showcasing significant advancements in one-shot\nevent detection.", "AI": {"tldr": "The paper introduces KeyCP++, a keyword-centric chain-of-thought prompting method to enhance event detection in LLMs, overcoming issues with traditional in-context learning approaches.", "motivation": "Current LLM-based in-context learning struggles with event detection due to over-interpretation and inadequate understanding of event triggers.", "method": "KeyCP++ uses a prompting template that identifies logical gaps in the input text and detection results, incorporating exemplary triggers to improve candidate trigger proposals and rationales.", "result": "Experiments show KeyCP++ leads to significant improvements in one-shot event detection performance compared to existing methods.", "conclusion": "KeyCP++ effectively assists LLMs in learning detection rules and reduces reliance on merely keyword identification.", "key_contributions": ["Introduction of KeyCP++, a novel prompting approach for event detection", "Automatic annotation of logical gaps in prompts", "Enhanced rationale generation for trigger proposals"], "limitations": "", "keywords": ["LLM", "in-context learning", "event detection", "keyword-centric prompting", "natural language processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2407.02810", "pdf": "https://arxiv.org/pdf/2407.02810.pdf", "abs": "https://arxiv.org/abs/2407.02810", "title": "Understanding the Prevalence of Caste: A Critical Discourse Analysis of Caste-based Marginalization on X", "authors": ["Nayana Kirasur", "Shagun Jhaver"], "categories": ["cs.HC"], "comment": "35 pages, 11 figures, 1 table", "summary": "Despite decades of anti-caste efforts, sociocultural practices that\nmarginalize lower-caste groups in India remain prevalent and have even\nproliferated with the use of social media. This paper examines how groups\nengaged in caste-based discrimination leverage platform affordances of the\nsocial media site X (formerly Twitter) to circulate and reinforce caste\nideologies. Using a critical discourse analysis (CDA) approach, we examine the\nrhetorical and organizing strategies of 50 X profiles representing upper-caste\ncollectives. We find that these profiles leverage platform affordances such as\ninformation control, bandwidth, visibility, searchability, and shareability to\nconstruct two main arguments: (1) that their upper caste culture deserves a\nsuperior status and (2) that they are the \"true\" victims of oppression in\nsociety. These profiles' digitally mediated discursive strategies contribute to\nthe marginalization of lower castes by normalizing caste cultures,\nstrengthening caste networks, reinforcing caste discrimination, and diminishing\nanti-caste measures. Our analysis builds upon previous HCI conceptualizations\nof online harms and safety to inform how to address caste-based\nmarginalization. We offer theoretical and methodological suggestions for\ncritical HCI research focused on studying the mechanisms of power along other\nsocial categories such as race and gender.", "AI": {"tldr": "This paper analyzes how upper-caste groups use social media (X) to perpetuate caste ideologies, leveraging platform affordances to normalize caste discrimination and marginalize lower castes, while providing suggestions for critical HCI research.", "motivation": "The aim is to explore the persistence of caste-based discrimination in India through social media, despite anti-caste efforts.", "method": "A critical discourse analysis (CDA) approach is employed to examine the strategies of 50 X profiles representing upper-caste collectives.", "result": "The analysis reveals that upper-caste profiles use social media to bolster their claims of superiority and victimhood, thus contributing to the perpetuation of caste discrimination.", "conclusion": "The study provides insights on how HCI can approach caste-based marginalization and offers suggestions for future research on power dynamics in social contexts.", "key_contributions": ["Examination of social media's role in advancing caste ideologies.", "Identification of specific discursive strategies used by upper-caste groups.", "Recommendations for HCI research to address power and marginalization."], "limitations": "", "keywords": ["caste discrimination", "social media", "HCI", "critical discourse analysis", "marginalization"], "importance_score": 4, "read_time_minutes": 20}}
{"id": "2508.07630", "pdf": "https://arxiv.org/pdf/2508.07630.pdf", "abs": "https://arxiv.org/abs/2508.07630", "title": "InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information", "authors": ["Anirudh Iyengar Kaniyar Narayana Iyengar", "Srija Mukhopadhyay", "Adnan Qidwai", "Shubhankar Singh", "Dan Roth", "Vivek Gupta"], "categories": ["cs.CL", "cs.AI", "cs.CV", "I.2.7; I.2.10; I.4.10; I.7.5"], "comment": "18 pages, 6 figures, 12 tables. Benchmark dataset and evaluation code\n  will be publicly made available", "summary": "We introduce InterChart, a diagnostic benchmark that evaluates how well\nvision-language models (VLMs) reason across multiple related charts, a task\ncentral to real-world applications such as scientific reporting, financial\nanalysis, and public policy dashboards. Unlike prior benchmarks focusing on\nisolated, visually uniform charts, InterChart challenges models with diverse\nquestion types ranging from entity inference and trend correlation to numerical\nestimation and abstract multi-step reasoning grounded in 2-3 thematically or\nstructurally related charts. We organize the benchmark into three tiers of\nincreasing difficulty: (1) factual reasoning over individual charts, (2)\nintegrative analysis across synthetically aligned chart sets, and (3) semantic\ninference over visually complex, real-world chart pairs. Our evaluation of\nstate-of-the-art open and closed-source VLMs reveals consistent and steep\naccuracy declines as chart complexity increases. We find that models perform\nbetter when we decompose multi-entity charts into simpler visual units,\nunderscoring their struggles with cross-chart integration. By exposing these\nsystematic limitations, InterChart provides a rigorous framework for advancing\nmultimodal reasoning in complex, multi-visual environments.", "AI": {"tldr": "InterChart is a diagnostic benchmark for evaluating vision-language models on reasoning tasks involving multiple related charts, highlighting their limitations in complex scenarios.", "motivation": "To assess the performance of vision-language models (VLMs) in reasoning across related charts, which is essential for applications like scientific reporting and financial analysis.", "method": "The benchmark consists of three tiers: factual reasoning over individual charts, integrative analysis of aligned chart sets, and semantic inference on complex chart pairs. It evaluates models on diverse question types related to these charts.", "result": "State-of-the-art VLMs show a consistent decline in accuracy as chart complexity increases, performing better with simpler visual units rather than complex, integrated charts.", "conclusion": "InterChart reveals significant limitations in current VLMs for multi-visual reasoning and serves as a framework for future advancements in multimodal reasoning.", "key_contributions": ["Introduction of a comprehensive benchmark for VLMs across diverse chart reasoning tasks.", "Identification of systematic limitations in VLM performance with complex multi-visual data.", "Structured evaluation framework to enhance multimodal reasoning in real-world applications."], "limitations": "The benchmark mainly focuses on visual reasoning, and may not cover all aspects of chart interpretation for models.", "keywords": ["vision-language models", "chart reasoning", "diagnostic benchmark", "multimodal reasoning", "evaluation framework"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2409.01399", "pdf": "https://arxiv.org/pdf/2409.01399.pdf", "abs": "https://arxiv.org/abs/2409.01399", "title": "Intents, Techniques, and Components: a Unified Analysis of Interaction Authoring Tasks in Data Visualization", "authors": ["Hyemi Song", "Sai Gopinath", "Zhicheng Liu"], "categories": ["cs.HC"], "comment": null, "summary": "There is a growing interest in designing tools to support interactivity\nspecification and authoring in data visualization. To develop expressive and\nflexible tools, we need theories and models that describe the task space of\ninteraction authoring. Although multiple taxonomies and frameworks exist for\ninteractive visualization, they primarily focus on how visualizations are used,\nnot how interactivity is composed. To fill this gap, we conduct an analysis of\n592 interaction units from 47 real-world visualization applications. Based on\nthe analysis, we present a unified analysis of interaction authoring tasks\nacross three levels of description: intents, representative techniques, and\nlow-level implementation components. We examine our framework's descriptive,\nevaluative, and generative powers for critiquing existing interactivity\nauthoring tools and informing new tool development.", "AI": {"tldr": "The paper analyzes 592 interaction units to develop a framework for interaction authoring in data visualization, emphasizing intents, techniques, and implementation components.", "motivation": "The need for expressive tools for interactivity specification and authoring in data visualization, addressing a significant gap in existing taxonomies that focus on usage rather than composition of interactivity.", "method": "Conducted an analysis of 592 interaction units from 47 visualization applications to develop a unified framework for interaction authoring tasks.", "result": "Proposed a framework categorizing interaction authoring tasks into intents, representative techniques, and low-level implementation components, useful for critiquing existing tools.", "conclusion": "The framework enhances understanding of interaction authoring, with potential applications in evaluating current tools and guiding future tool development.", "key_contributions": ["Unified framework for interaction authoring tasks", "Analysis of interaction units from real-world applications", "Insights into existing tools and future development"], "limitations": "", "keywords": ["interaction authoring", "data visualization", "framework", "interaction units", "tool development"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2508.07690", "pdf": "https://arxiv.org/pdf/2508.07690.pdf", "abs": "https://arxiv.org/abs/2508.07690", "title": "LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval", "authors": ["Luyao Zhuang", "Qinggang Zhang", "Huachi Zhou", "Juhua Liu", "Qing Li", "Xiao Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tool learning has emerged as a promising paradigm for large language models\n(LLMs) to solve many real-world tasks. Nonetheless, with the tool repository\nrapidly expanding, it is impractical to contain all tools within the limited\ninput length of LLMs. To alleviate these issues, researchers have explored\nincorporating a tool retrieval module to select the most relevant tools or\nrepresent tools as unique tokens within LLM parameters. However, most\nstate-of-the-art methods are under transductive settings, assuming all tools\nhave been observed during training. Such a setting deviates from reality as the\nreal-world tool repository is evolving and incorporates new tools frequently.\nWhen dealing with these unseen tools, which refer to tools not encountered\nduring the training phase, these methods are limited by two key issues,\nincluding the large distribution shift and the vulnerability of\nsimilarity-based retrieval. To this end, inspired by human cognitive processes\nof mastering unseen tools through discovering and applying the logical\ninformation from prior experience, we introduce a novel Logic-Guided Semantic\nBridging framework for inductive tool retrieval, namely, LoSemB, which aims to\nmine and transfer latent logical information for inductive tool retrieval\nwithout costly retraining. Specifically, LoSemB contains a logic-based\nembedding alignment module to mitigate distribution shifts and implements a\nrelational augmented retrieval mechanism to reduce the vulnerability of\nsimilarity-based retrieval. Extensive experiments demonstrate that LoSemB\nachieves advanced performance in inductive settings while maintaining desirable\neffectiveness in the transductive setting.", "AI": {"tldr": "The paper introduces the Logic-Guided Semantic Bridging (LoSemB) framework for inductive tool retrieval in large language models, addressing limitations of existing methods that assume all tools are known during training.", "motivation": "To tackle the challenges posed by an evolving tool repository and the limitations of existing retrieval methods when encountering unseen tools.", "method": "The LoSemB framework utilizes a logic-based embedding alignment module and implements a relational augmented retrieval mechanism to facilitate inductive tool retrieval without requiring retraining.", "result": "LoSemB outperforms existing state-of-the-art methods in inductive settings and retains effective performance in transductive settings.", "conclusion": "LoSemB offers a promising solution for improving tool retrieval capabilities in large language models, particularly for unseen tools, by leveraging logical information from prior experiences.", "key_contributions": ["Introduction of LoSemB framework for inductive tool retrieval", "Development of a logic-based embedding alignment module", "Implementation of a relational augmented retrieval mechanism"], "limitations": "The framework may still face challenges with tools that are significantly different from those previously encountered.", "keywords": ["tool retrieval", "inductive learning", "large language models", "logic-guided learning", "semantic bridging"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2409.02053", "pdf": "https://arxiv.org/pdf/2409.02053.pdf", "abs": "https://arxiv.org/abs/2409.02053", "title": "Augmented Reality Assistive Technologies for Disabled Individuals", "authors": ["Riju Marwah", "Jyotin Singh Thakur", "Pranav Tanwar"], "categories": ["cs.HC"], "comment": "8 pages, 12 figures, 5 tables", "summary": "Augmented Reality (AR) technologies hold immense potential for\nrevolutionizing the way individuals with disabilities interact with the world.\nAR systems can provide real-time assistance and support by overlaying digital\ninformation over the physical environment based on the requirements of the use,\nhence addressing different types of disabilities. Through an in-depth analysis\nof four case studies, this paper aims to provide a comprehensive overview of\nthe current-state-of-the-art in AR assistive technologies for individuals with\ndisabilities, highlighting their potential to assist and transform their lives.\nThe findings show the significance that AR has made to bridge the accessibility\ngap, while also discussing the challenges faced and ethical considerations\nassociated with the implementation across the various cases. This is done\nthrough theory analysis, practical examples, and future projections that will\nmotivate and seek to inspire further innovation in this very relevant area of\nexploration.", "AI": {"tldr": "The paper reviews Augmented Reality (AR) technologies aimed at assisting individuals with disabilities through case studies, discussing their impact, challenges, and ethical considerations.", "motivation": "To explore how AR can enhance interactions and provide support to individuals with disabilities, effectively addressing accessibility challenges.", "method": "The paper conducts an in-depth analysis of four case studies focused on AR assistive technologies, utilizing theory analysis, practical examples, and future projections.", "result": "The findings demonstrate AR's potential to bridge accessibility gaps for individuals with disabilities, while also addressing implementation challenges and ethical concerns.", "conclusion": "AR technologies can significantly transform the lives of individuals with disabilities, but innovation needs to consider ethical implications and practical challenges.", "key_contributions": ["Comprehensive overview of current AR assistive technologies", "Analysis of case studies showcasing real-world applications", "Discussion on ethical considerations and future projections for AR in accessibility."], "limitations": "Focused only on four case studies; may not represent all AR technologies or all types of disabilities.", "keywords": ["Augmented Reality", "assistive technologies", "disabilities", "accessibility", "ethical considerations"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2508.07702", "pdf": "https://arxiv.org/pdf/2508.07702.pdf", "abs": "https://arxiv.org/abs/2508.07702", "title": "What am I missing here?: Evaluating Large Language Models for Masked Sentence Prediction", "authors": ["Charlie Wyatt", "Aditya Joshi", "Flora Salim"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "Transformer-based models primarily rely on Next Token Prediction (NTP), which\npredicts the next token in a sequence based on the preceding context. However,\nNTP's focus on single-token prediction often limits a model's ability to plan\nahead or maintain long-range coherence, raising questions about how well LLMs\ncan predict longer contexts, such as full sentences within structured\ndocuments. While NTP encourages local fluency, it provides no explicit\nincentive to ensure global coherence across sentence boundaries-an essential\nskill for reconstructive or discursive tasks. To investigate this, we evaluate\nthree commercial LLMs (GPT-4o, Claude 3.5 Sonnet, and Gemini 2.0 Flash) on\nMasked Sentence Prediction (MSP) - the task of infilling a randomly removed\nsentence - from three domains: ROCStories (narrative), Recipe1M (procedural),\nand Wikipedia (expository). We assess both fidelity (similarity to the original\nsentence) and cohesiveness (fit within the surrounding context). Our key\nfinding reveals that commercial LLMs, despite their superlative performance in\nother tasks, are poor at predicting masked sentences in low-structured domains,\nhighlighting a gap in current model capabilities.", "AI": {"tldr": "This paper evaluates commercial LLMs on their ability to predict masked sentences, highlighting their limitations in maintaining long-range coherence and global context in various domains.", "motivation": "To investigate LLMs' capacities for long-range coherence and sentence-level prediction, addressing the limitations of Next Token Prediction.", "method": "The study assesses three LLMs (GPT-4o, Claude 3.5 Sonnet, and Gemini 2.0 Flash) on the Masked Sentence Prediction task across three domains: ROCStories, Recipe1M, and Wikipedia.", "result": "The evaluation reveals that while these LLMs excel in many tasks, they struggle with masked sentence prediction in low-structured domains, indicating a significant gap in their capabilities.", "conclusion": "The findings suggest that the reliance on Next Token Prediction inhibits models from achieving global coherence across sentence boundaries, which is crucial for certain tasks.", "key_contributions": ["Evaluation of commercial LLMs on Masked Sentence Prediction task", "Identification of models' limitations in long-range coherence", "Insights into the gap between local fluency and global context maintenance."], "limitations": "Focused evaluation on commercial LLMs limited to specific tasks and domains, which may not fully represent broader model capabilities.", "keywords": ["Long-Range Coherence", "Masked Sentence Prediction", "Large Language Models", "Natural Language Processing", "Next Token Prediction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2410.14252", "pdf": "https://arxiv.org/pdf/2410.14252.pdf", "abs": "https://arxiv.org/abs/2410.14252", "title": "Harmony: A Human-Aware, Responsive, Modular Assistant with a Locally Deployed Large Language Model", "authors": ["Ziqi Yin", "Mingxin Zhang", "Daisuke Kawahara"], "categories": ["cs.HC"], "comment": null, "summary": "Large Language Models (LLMs) offer powerful capabilities for natural language\nunderstanding, enabling more intelligent smart home assistants. However,\nexisting systems often rely on cloud-based LLMs, raising concerns around user\nprivacy and system dependency on external connectivity. In this work, we\npresent Harmony, a privacy-preserving and robust smart home assistant powered\nby the locally deployable Llama3-8B model. Beyond protecting user data, Harmony\nalso addresses reliability challenges of smaller models, such as hallucination\nand instruction misinterpretation, through structured prompting and modular\nagent design. Experimental results in both virtual environments and user\nstudies show that Harmony achieves performance comparable to GPT-4-based\nsystems, while enabling offline, proactive, and personalized smart home\ninteraction.", "AI": {"tldr": "Harmony is a privacy-preserving smart home assistant that utilizes the Llama3-8B model, addressing user privacy concerns associated with cloud-based systems while also improving reliability in model performance.", "motivation": "The paper addresses the challenges of user privacy and reliability in existing cloud-based smart home assistants powered by large language models, which often compromise user data and depend on continuous external connectivity.", "method": "The paper presents Harmony, a locally deployable smart home assistant that uses the Llama3-8B model, implementing structured prompting and modular agent design to improve model reliability and user privacy.", "result": "Experimental results demonstrate that Harmony offers performance on par with GPT-4-based systems, maintaining privacy and enabling offline functionality.", "conclusion": "Harmony represents a significant step towards privacy-aware and reliable smart home interactions, showing that local model deployment can compete with advanced cloud-based solutions.", "key_contributions": ["Development of Harmony, a locally deployable smart home assistant", "Structured prompting and modular agent design to enhance performance of smaller models", "Empirical validation of Harmony's performance compared to GPT-4 systems"], "limitations": "", "keywords": ["Smart Home Assistants", "Privacy Preservation", "Large Language Models", "User Interaction", "Offline Systems"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2508.07753", "pdf": "https://arxiv.org/pdf/2508.07753.pdf", "abs": "https://arxiv.org/abs/2508.07753", "title": "Exploring Causal Effect of Social Bias on Faithfulness Hallucinations in Large Language Models", "authors": ["Zhenliang Zhang", "Junzhe Zhang", "Xinyu Hu", "HuiXuan Zhang", "Xiaojun Wan"], "categories": ["cs.CL"], "comment": "Accepted by CIKM 2025 (Full Paper)", "summary": "Large language models (LLMs) have achieved remarkable success in various\ntasks, yet they remain vulnerable to faithfulness hallucinations, where the\noutput does not align with the input. In this study, we investigate whether\nsocial bias contributes to these hallucinations, a causal relationship that has\nnot been explored. A key challenge is controlling confounders within the\ncontext, which complicates the isolation of causality between bias states and\nhallucinations. To address this, we utilize the Structural Causal Model (SCM)\nto establish and validate the causality and design bias interventions to\ncontrol confounders. In addition, we develop the Bias Intervention Dataset\n(BID), which includes various social biases, enabling precise measurement of\ncausal effects. Experiments on mainstream LLMs reveal that biases are\nsignificant causes of faithfulness hallucinations, and the effect of each bias\nstate differs in direction. We further analyze the scope of these causal\neffects across various models, specifically focusing on unfairness\nhallucinations, which are primarily targeted by social bias, revealing the\nsubtle yet significant causal effect of bias on hallucination generation.", "AI": {"tldr": "This study explores the relationship between social bias and faithfulness hallucinations in large language models, utilizing structural causal models to validate causality and design interventions.", "motivation": "To investigate the unexplored causal relationship between social bias and faithfulness hallucinations in large language models (LLMs).", "method": "Utilized Structural Causal Model (SCM) to analyze causality and design bias interventions, while developing the Bias Intervention Dataset (BID) for measuring causal effects.", "result": "Experiments on mainstream LLMs demonstrated that social biases significantly contribute to faithfulness hallucinations, with varying effects based on the type of bias.", "conclusion": "The study highlights the causal impact of social bias on hallucinations in LLMs, emphasizing the need for bias interventions to mitigate these effects.", "key_contributions": ["Establishment of a causal relationship between social bias and faithfulness hallucinations in LLMs.", "Development of the Bias Intervention Dataset (BID) for studying social biases.", "Identification of varying effects of different social biases on hallucination generation."], "limitations": "The primary limitation is the complexity of controlling all confounders within the context.", "keywords": ["large language models", "faithfulness hallucinations", "social bias", "structural causal models", "bias intervention"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.21596", "pdf": "https://arxiv.org/pdf/2410.21596.pdf", "abs": "https://arxiv.org/abs/2410.21596", "title": "Chatbot Companionship: A Mixed-Methods Study of Companion Chatbot Usage Patterns and Their Relationship to Loneliness in Active Users", "authors": ["Auren R. Liu", "Pat Pataranutaporn", "Pattie Maes"], "categories": ["cs.HC"], "comment": "31 pages, 14 figures, accepted to AIES 2025", "summary": "Companion chatbots offer a potential solution to the growing epidemic of\nloneliness, but their impact on users' psychosocial well-being remains poorly\nunderstood, raising critical ethical questions about their deployment and\ndesign. This study presents a large-scale survey (n = 404) of regular users of\ncompanion chatbots, investigating the relationship between chatbot usage and\nloneliness. We develop a model explaining approximately 50% of variance in\nloneliness; while usage does not directly predict loneliness, we identify\nfactors including neuroticism, social network size, and problematic use.\nThrough cluster analysis and mixed-methods thematic analysis combining manual\ncoding with automated theme extraction, we identify seven distinct user\nprofiles demonstrating that companion chatbots can either enhance or\npotentially harm psychological well-being depending on user characteristics.\nDifferent usage patterns can lead to markedly different outcomes, with some\nusers experiencing enhanced social confidence while others risk further\nisolation. These findings have significant implications for responsible AI\ndevelopment, suggesting that one-size-fits-all approaches to AI companionship\nmay be ethically problematic. Our work contributes to the ongoing dialogue\nabout the role of AI in social and emotional support, offering insights for\ndeveloping more targeted and ethical approaches to AI companionship that\ncomplement rather than replace human connections.", "AI": {"tldr": "This study investigates the relationship between companion chatbot usage and loneliness, analyzing user characteristics to identify seven distinct profiles that affect psychosocial well-being.", "motivation": "To understand the impact of companion chatbots on users' psychosocial well-being and address ethical considerations in their design and deployment.", "method": "A large-scale survey (n = 404) was conducted to analyze user characteristics, alongside cluster analysis and mixed-methods thematic analysis combining manual coding with automated theme extraction.", "result": "The model explained approximately 50% of the variance in loneliness. Factors such as neuroticism, social network size, and problematic use were identified. Distinct user profiles varied in their outcomes, with some enhancing social confidence and others risking isolation.", "conclusion": "The findings suggest that different chatbot usage patterns lead to varying impacts on psychological well-being, underlining the need for targeted, ethically responsible design in AI companionship.", "key_contributions": ["Identification of user profiles that influence the impact of chatbot usage on loneliness", "Development of a model explaining significant variance in loneliness", "Insights for ethical AI companionship design and its implications for social support"], "limitations": "", "keywords": ["companion chatbots", "loneliness", "psychosocial well-being", "AI ethics", "user profiles"], "importance_score": 8, "read_time_minutes": 31}}
{"id": "2508.07781", "pdf": "https://arxiv.org/pdf/2508.07781.pdf", "abs": "https://arxiv.org/abs/2508.07781", "title": "SASST: Leveraging Syntax-Aware Chunking and LLMs for Simultaneous Speech Translation", "authors": ["Zeyu Yang", "Lai Wei", "Roman Koshkin", "Xi Chen", "Satoshi Nakamura"], "categories": ["cs.CL"], "comment": null, "summary": "This work proposes a grammar-based chunking strategy that segments input\nstreams into semantically complete units by parsing dependency relations (e.g.,\nnoun phrase boundaries, verb-object structures) and punctuation features. The\nmethod ensures chunk coherence and minimizes semantic fragmentation. Building\non this mechanism, we present SASST (Syntax-Aware Simultaneous Speech\nTranslation), an end-to-end framework integrating frozen Whisper encoder and\ndecoder-only LLM. The unified architecture dynamically outputs translation\ntokens or <WAIT> symbols to jointly optimize translation timing and content,\nwith target-side reordering addressing word-order divergence. Experiments on\nCoVoST2 multilingual corpus En-{De, Zh, Ja} demonstrate significant translation\nquality improvements across languages and validate the effectiveness of\nsyntactic structures in LLM-driven SimulST systems.", "AI": {"tldr": "Proposes a grammar-based chunking strategy for semantic unit segmentation in speech translation, introducing SASST, an end-to-end framework for improved translation quality.", "motivation": "To improve the quality of speech translation by segmenting input into semantically coherent units using dependency relations and punctuation.", "method": "The study introduces a grammar-based chunking strategy that parses dependency relations and utilizes punctuation features. It presents SASST, which integrates a Whisper encoder with a decoder-only LLM to dynamically output translation tokens or <WAIT> symbols for simultaneous translation optimization.", "result": "Significant translation quality improvements were demonstrated on the CoVoST2 multilingual corpus across English and multiple target languages (German, Chinese, Japanese).", "conclusion": "The results validate that leveraging syntactic structures in LLM-driven simultaneous speech translation can enhance translation quality.", "key_contributions": ["A new grammar-based chunking strategy for speech translation.", "Introduction of the SASST framework for simultaneous speech translation.", "Demonstrated improvements in translation quality using syntactic structures."], "limitations": "", "keywords": ["speech translation", "chunking strategy", "syntax-aware translation", "multilingual corpus", "dependency parsing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2411.09969", "pdf": "https://arxiv.org/pdf/2411.09969.pdf", "abs": "https://arxiv.org/abs/2411.09969", "title": "Steering AI-Driven Personalization of Scientific Text for General Audiences", "authors": ["Taewook Kim", "Dhruv Agarwal", "Jordan Ackerman", "Manaswi Saha"], "categories": ["cs.HC", "cs.AI"], "comment": "28 pages, 7 figures, 1 table. Accepted to PACM HCI (CSCW 2025)", "summary": "Digital media platforms (e.g., science blogs) offer opportunities to\ncommunicate scientific content to general audiences at scale. However, these\naudiences vary in their scientific expertise, literacy levels, and personal\nbackgrounds, making effective science communication challenging. To address\nthis challenge, we designed TranSlider, an AI-powered tool that generates\npersonalized translations of scientific text based on individual user profiles\n(e.g., hobbies, location, and education). Our tool features an interactive\nslider that allows users to steer the degree of personalization from 0 (weakly\nrelatable) to 100 (strongly relatable), leveraging LLMs to generate the\ntranslations with chosen degrees. Through an exploratory study with 15\nparticipants, we investigated both the utility of these AI-personalized\ntranslations and how interactive reading features influenced users'\nunderstanding and reading experiences. We found that participants who preferred\nhigher degrees of personalization appreciated the relatable and contextual\ntranslations, while those who preferred lower degrees valued concise\ntranslations with subtle contextualization. Furthermore, participants reported\nthe compounding effect of multiple translations on their understanding of\nscientific content. Drawing on these findings, we discuss several implications\nfor facilitating science communication and designing steerable interfaces to\nsupport human-AI alignment.", "AI": {"tldr": "TranSlider is an AI-powered tool that personalizes scientific text translations based on user profiles, enhancing science communication.", "motivation": "The paper addresses the challenge of effective science communication to diverse audiences with varying expertise and backgrounds.", "method": "The methodology involved designing TranSlider, an interactive tool that uses LLMs to generate personalized translations of scientific texts, tested through an exploratory study with 15 participants.", "result": "The study found varied preferences among participants regarding the degree of personalization, impacting their understanding and reading experiences of scientific content.", "conclusion": "The findings highlight the potential of personalized AI translations in improving science communication and suggest design implications for user interfaces that support human-AI alignment.", "key_contributions": ["Development of TranSlider for personalized scientific text translations", "Exploration of user preferences in personalized translations", "Insights into the impact of personalization on user understanding of scientific content"], "limitations": "Limited sample size of 15 participants in the exploratory study", "keywords": ["AI", "personalized translations", "science communication", "human-AI alignment", "interactive tools"], "importance_score": 8, "read_time_minutes": 28}}
{"id": "2508.07785", "pdf": "https://arxiv.org/pdf/2508.07785.pdf", "abs": "https://arxiv.org/abs/2508.07785", "title": "Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts", "authors": ["Haoyuan Wu", "Haoxing Chen", "Xiaodong Chen", "Zhanchao Zhou", "Tieyuan Chen", "Yihong Zhuang", "Guoshan Lu", "Zenan Huang", "Junbo Zhao", "Lin Liu", "Zhenzhong Lan", "Bei Yu", "Jianguo Li"], "categories": ["cs.CL"], "comment": null, "summary": "The Mixture of Experts (MoE) architecture is a cornerstone of modern\nstate-of-the-art (SOTA) large language models (LLMs). MoE models facilitate\nscalability by enabling sparse parameter activation. However, traditional MoE\narchitecture uses homogeneous experts of a uniform size, activating a fixed\nnumber of parameters irrespective of input complexity and thus limiting\ncomputational efficiency. To overcome this limitation, we introduce Grove MoE,\na novel architecture incorporating experts of varying sizes, inspired by the\nheterogeneous big.LITTLE CPU architecture. This architecture features novel\nadjugate experts with a dynamic activation mechanism, enabling model capacity\nexpansion while maintaining manageable computational overhead. Building on this\narchitecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter LLMs\ndeveloped by applying an upcycling strategy to the Qwen3-30B-A3B-Base model\nduring mid-training and post-training. GroveMoE models dynamically activate\n3.14-3.28B parameters based on token complexity and achieve performance\ncomparable to SOTA open-source models of similar or even larger size.", "AI": {"tldr": "Grove MoE introduces a novel architecture for LLMs using heterogeneous experts with dynamic activation, improving computational efficiency and scalability over traditional models.", "motivation": "To address the limitations of traditional Mixture of Experts models with uniform-sized experts that hinder computational efficiency and scalability.", "method": "The Grove MoE architecture incorporates experts of varying sizes and a dynamic activation mechanism, inspired by big.LITTLE CPU architecture, enabling model capacity expansion with controllable computational overhead.", "result": "GroveMoE models dynamically activate between 3.14-3.28B parameters based on token complexity, achieving performance on par with or better than state-of-the-art models of similar or larger sizes.", "conclusion": "Grove MoE demonstrates an effective approach to enhancing LLM efficiency and scalability while maintaining performance through dynamic expert activation.", "key_contributions": ["Introduction of Grove MoE with heterogeneous expert sizes", "Dynamic parameter activation based on input complexity", "Development of GroveMoE-Base and GroveMoE-Inst models with improved efficiency"], "limitations": "", "keywords": ["Mixture of Experts", "large language models", "dynamic activation", "computational efficiency", "HCI"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2411.17589", "pdf": "https://arxiv.org/pdf/2411.17589.pdf", "abs": "https://arxiv.org/abs/2411.17589", "title": "Privacy-Preserving Behaviour of Chatbot Users: Steering Through Trust Dynamics", "authors": ["Julia Ive", "Vishal Yadav", "Mariia Ignashina", "Matthew Rand", "Paulina Bondaronek"], "categories": ["cs.HC"], "comment": "10 pages, 25 references, 2 figures and 3 tables", "summary": "Introduction: The use of chatbots is becoming increasingly important across\nvarious aspects of daily life. However, the privacy concerns associated with\nthese communications have not yet been thoroughly addressed. The aim of this\nstudy was to investigate user awareness of privacy risks in chatbot\ninteractions, the privacy-preserving behaviours users practice, and how these\nbehaviours relate to their awareness of privacy threats, even when no immediate\nthreat is perceived. Methods: We developed a novel \"privacy-safe\" setup to\nanalyse user behaviour under the guarantees of anonymization and non-sharing.\nWe employed a mixed-methods approach, starting with the quantification of\nbroader trends by coding responses, followed by conducting a qualitative\ncontent analysis to gain deeper insights. Results: Overall, there was a\nsubstantial lack of understanding among users about how chatbot providers\nhandle data (27% of the participants) and the basics of privacy risks (76% of\nthe participants). Older users, in particular, expressed fears that chatbot\nproviders might sell their data. Moreover, even users with privacy knowledge do\nnot consistently exhibit privacy-preserving behaviours when assured of\ntransparent data processing by chatbots. Notably, under-protective behaviours\nwere observed among more expert users. Discussion: These findings highlight the\nneed for a strategic approach to enhance user education on privacy concepts to\nensure informed decision when interacting with chatbot technology. This\nincludes the development of tools to help users monitor and control the\ninformation they share with chatbots", "AI": {"tldr": "This study investigates user awareness of privacy risks in chatbot interactions and identifies a significant lack of understanding regarding data handling by chatbot providers.", "motivation": "The increasing importance of chatbots in daily life has raised privacy concerns that have not been adequately addressed.", "method": "A mixed-methods approach was utilized, combining quantitative coding of responses and qualitative content analysis to explore user behaviors related to privacy.", "result": "Findings indicate a substantial lack of awareness about data handling among users, with 27% unaware of how their data is processed and 76% not understanding basic privacy risks. Older users particularly fear data selling by chatbot providers.", "conclusion": "There is a critical need for enhanced user education on privacy in chatbot interactions, including the development of monitoring tools for users.", "key_contributions": ["Identification of user misconceptions about chatbot data handling", "Revelation of under-protective behaviors even among knowledgeable users", "Recommendations for improved user education strategies on privacy"], "limitations": "The study is limited to specific user groups and may not be representative of all chatbot users.", "keywords": ["chatbots", "privacy risks", "user awareness", "data handling", "privacy education"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.07805", "pdf": "https://arxiv.org/pdf/2508.07805.pdf", "abs": "https://arxiv.org/abs/2508.07805", "title": "Can You Trick the Grader? Adversarial Persuasion of LLM Judges", "authors": ["Yerin Hwang", "Dongryeol Lee", "Taegwan Kang", "Yongil Kim", "Kyomin Jung"], "categories": ["cs.CL"], "comment": "19 pages, 8 figures", "summary": "As large language models take on growing roles as automated evaluators in\npractical settings, a critical question arises: Can individuals persuade an LLM\njudge to assign unfairly high scores? This study is the first to reveal that\nstrategically embedded persuasive language can bias LLM judges when scoring\nmathematical reasoning tasks, where correctness should be independent of\nstylistic variation. Grounded in Aristotle's rhetorical principles, we\nformalize seven persuasion techniques (Majority, Consistency, Flattery,\nReciprocity, Pity, Authority, Identity) and embed them into otherwise identical\nresponses. Across six math benchmarks, we find that persuasive language leads\nLLM judges to assign inflated scores to incorrect solutions, by up to 8% on\naverage, with Consistency causing the most severe distortion. Notably,\nincreasing model size does not substantially mitigate this vulnerability.\nFurther analysis demonstrates that combining multiple persuasion techniques\namplifies the bias, and pairwise evaluation is likewise susceptible. Moreover,\nthe persuasive effect persists under counter prompting strategies, highlighting\na critical vulnerability in LLM-as-a-Judge pipelines and underscoring the need\nfor robust defenses against persuasion-based attacks.", "AI": {"tldr": "This study investigates how persuasive language can influence large language model judges to assign higher scores to incorrect mathematical reasoning solutions.", "motivation": "To understand the vulnerability of large language models (LLMs) as evaluators and explore the extent to which strategic language can bias their scoring.", "method": "The study formalizes seven persuasion techniques based on Aristotle's rhetorical principles and embeds these into identical responses evaluated by LLMs across six math benchmarks.", "result": "Persuasive language resulted in inflated scores for incorrect solutions, with an average increase of up to 8%. The technique of Consistency caused the most significant distortion in scores.", "conclusion": "The findings expose a critical vulnerability in LLM-as-a-Judge systems, necessitating the development of robust defenses against manipulation through persuasive language.", "key_contributions": ["First study to show LLMs can be biased by persuasive language", "Identification of seven specific persuasion techniques", "Demonstration of the persuasiveness of language across multiple evaluation contexts"], "limitations": "Increased model size does not significantly reduce susceptibility to bias; counter-prompts are also ineffective in mitigating persuasion effects.", "keywords": ["persuasive language", "large language models", "bias", "mathematical reasoning", "evaluators"], "importance_score": 8, "read_time_minutes": 19}}
{"id": "2412.08185", "pdf": "https://arxiv.org/pdf/2412.08185.pdf", "abs": "https://arxiv.org/abs/2412.08185", "title": "Exploring Multidimensional Checkworthiness: Designing AI-assisted Claim Prioritization for Human Fact-checkers", "authors": ["Houjiang Liu", "Jacek Gwizdka", "Matthew Lease"], "categories": ["cs.HC", "cs.CY", "cs.IR"], "comment": "Accepted at CSCW 2025", "summary": "Given the volume of potentially false claims online, claim prioritization is\nessential in allocating limited human resources available for fact-checking. In\nthis study, we perceive claim prioritization as an information retrieval (IR)\ntask: just as multidimensional IR relevance, with many factors influencing\nwhich search results a user deems relevant, checkworthiness is also\nmulti-faceted, subjective, and even personal, with many factors influencing how\nfact-checkers triage and select which claims to check. Our study investigates\nboth the multidimensional nature of checkworthiness and effective tool support\nto assist fact-checkers in claim prioritization. Methodologically, we pursue\nResearch through Design combined with mixed-method evaluation.\n  Specifically, we develop an AI-assisted claim prioritization prototype as a\nprobe to explore how fact-checkers use multidimensional checkworthy factors to\nprioritize claims, simultaneously probing fact-checker needs and exploring the\ndesign space to meet those needs. With 16 professional fact-checkers\nparticipating in our study, we uncover a hierarchical prioritization strategy\nfact-checkers implicitly use, revealing an underexplored aspect of their\nworkflow, with actionable design recommendations for improving claim triage\nacross multidimensional checkworthiness and tailoring this process with LLM\nintegration.", "AI": {"tldr": "This study explores claim prioritization in fact-checking as an information retrieval task, emphasizing the need for AI-assisted tools to support fact-checkers in their triage process.", "motivation": "With the increasing volume of potentially false claims online, prioritizing which claims to fact-check is critical for efficient resource allocation in fact-checking.", "method": "The paper utilizes Research through Design and mixed-method evaluation to develop a prototype for AI-assisted claim prioritization, involving 16 professional fact-checkers in the study.", "result": "The study reveals a hierarchical prioritization strategy used by fact-checkers that is previously underexplored, along with actionable design recommendations for enhancing claim triage with LLM integration.", "conclusion": "The findings suggest that considering multidimensional checkworthiness can significantly improve the fact-checking process and the design of supportive tools.", "key_contributions": ["Development of an AI-assisted claim prioritization prototype", "Introduction of a hierarchical strategy used by fact-checkers for prioritization", "Actionable design recommendations for integrating LLMs in fact-checking processes"], "limitations": "", "keywords": ["claim prioritization", "fact-checking", "information retrieval", "AI assistance", "multidimensional checkworthiness"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.07810", "pdf": "https://arxiv.org/pdf/2508.07810.pdf", "abs": "https://arxiv.org/abs/2508.07810", "title": "Evaluating Compositional Approaches for Focus and Sentiment Analysis", "authors": ["Olga Kellert", "Muhammad Imran", "Nicholas Hill Matlis", "Mahmud Uz Zaman", "Carlos G√≥mez-Rodr√≠guez"], "categories": ["cs.CL"], "comment": null, "summary": "This paper summarizes the results of evaluating a compositional approach for\nFocus Analysis (FA) in Linguistics and Sentiment Analysis (SA) in Natural\nLanguage Processing (NLP). While quantitative evaluations of compositional and\nnon-compositional approaches in SA exist in NLP, similar quantitative\nevaluations are very rare in FA in Linguistics that deal with linguistic\nexpressions representing focus or emphasis such as \"it was John who left\". We\nfill this gap in research by arguing that compositional rules in SA also apply\nto FA because FA and SA are closely related meaning that SA is part of FA. Our\ncompositional approach in SA exploits basic syntactic rules such as rules of\nmodification, coordination, and negation represented in the formalism of\nUniversal Dependencies (UDs) in English and applied to words representing\nsentiments from sentiment dictionaries. Some of the advantages of our\ncompositional analysis method for SA in contrast to non-compositional analysis\nmethods are interpretability and explainability. We test the accuracy of our\ncompositional approach and compare it with a non-compositional approach VADER\nthat uses simple heuristic rules to deal with negation, coordination and\nmodification. In contrast to previous related work that evaluates\ncompositionality in SA on long reviews, this study uses more appropriate\ndatasets to evaluate compositionality. In addition, we generalize the results\nof compositional approaches in SA to compositional approaches in FA.", "AI": {"tldr": "This paper evaluates a compositional approach for Focus Analysis in Linguistics and Sentiment Analysis in NLP, emphasizing the benefits of compositional methods by comparing them with non-compositional approaches.", "motivation": "To address the lack of quantitative evaluations for Focus Analysis in Linguistics and compare them with existing methods in Sentiment Analysis.", "method": "The paper utilizes a compositional approach applying Universal Dependencies to perform Sentiment Analysis, focusing on syntactic rules like modification, coordination, and negation.", "result": "The compositional method shows improved interpretability and explainability compared to VADER, a non-compositional method, with appropriate datasets enhancing the accuracy evaluation.", "conclusion": "The findings suggest that methods used in Sentiment Analysis are applicable to Focus Analysis, underscoring the relationship between these fields.", "key_contributions": ["Introduction of a compositional approach for Focus Analysis in Linguistics", "Comparison of compositional and non-compositional methods in Sentiment Analysis", "Demonstration of improved interpretability and accuracy with compositional techniques"], "limitations": "", "keywords": ["Focus Analysis", "Sentiment Analysis", "Natural Language Processing", "Compositional Approach", "Universal Dependencies"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2502.03788", "pdf": "https://arxiv.org/pdf/2502.03788.pdf", "abs": "https://arxiv.org/abs/2502.03788", "title": "Frontend Diffusion: Empowering Self-Representation of Junior Researchers and Designers Through Multi-agent System", "authors": ["Zijian Ding", "Qinshi Zhang", "Mohan Chi", "Ziyi Wang"], "categories": ["cs.HC"], "comment": null, "summary": "With the continuous development of generative AI's logical reasoning\nabilities, AI's growing code-generation potential poses challenges for both\ntechnical and creative professionals. But how can these advances be directed\ntoward empowering junior researchers and designers who often require additional\nhelp to build and express their professional and personal identities? We\nintroduce Frontend Diffusion, a multi-agent coding system transforming\nuser-drawn layouts and textual prompts into refined website code, thereby\nsupporting self-representation goals. A user study with 13 junior researchers\nand designers shows AI as a human capability enhancer rather than a\nreplacement, and highlights the importance of bidirectional human-AI alignment.\nWe then discuss future work such as leveraging AI for career development and\nfostering bidirectional human-AI alignment of multi-agent systems.", "AI": {"tldr": "Frontend Diffusion enhances junior researchers' and designers' self-representation by transforming their inputs into website code using AI.", "motivation": "To empower junior researchers and designers who struggle to express their professional identities through effective coding.", "method": "A multi-agent coding system that converts user-drawn layouts and textual prompts into refined website code, evaluated through a user study with 13 participants.", "result": "The study found that AI enhances human capabilities and emphasizes the need for bidirectional alignment between humans and AI systems.", "conclusion": "Future work will focus on leveraging AI for career development and improving human-AI collaboration in multi-agent systems.", "key_contributions": ["Introduction of Frontend Diffusion coding system", "User study demonstrating AI as an enhancer of human capabilities", "Discussion on the importance of human-AI alignment"], "limitations": "", "keywords": ["generative AI", "frontend development", "human-AI alignment"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2508.07827", "pdf": "https://arxiv.org/pdf/2508.07827.pdf", "abs": "https://arxiv.org/abs/2508.07827", "title": "Evaluating Large Language Models as Expert Annotators", "authors": ["Yu-Min Tseng", "Wei-Lin Chen", "Chung-Chi Chen", "Hsin-Hsi Chen"], "categories": ["cs.CL"], "comment": "Accepted to COLM 2025", "summary": "Textual data annotation, the process of labeling or tagging text with\nrelevant information, is typically costly, time-consuming, and labor-intensive.\nWhile large language models (LLMs) have demonstrated their potential as direct\nalternatives to human annotators for general domains natural language\nprocessing (NLP) tasks, their effectiveness on annotation tasks in domains\nrequiring expert knowledge remains underexplored. In this paper, we\ninvestigate: whether top-performing LLMs, which might be perceived as having\nexpert-level proficiency in academic and professional benchmarks, can serve as\ndirect alternatives to human expert annotators? To this end, we evaluate both\nindividual LLMs and multi-agent approaches across three highly specialized\ndomains: finance, biomedicine, and law. Specifically, we propose a multi-agent\ndiscussion framework to simulate a group of human annotators, where LLMs are\ntasked to engage in discussions by considering others' annotations and\njustifications before finalizing their labels. Additionally, we incorporate\nreasoning models (e.g., o3-mini) to enable a more comprehensive comparison. Our\nempirical results reveal that: (1) Individual LLMs equipped with inference-time\ntechniques (e.g., chain-of-thought (CoT), self-consistency) show only marginal\nor even negative performance gains, contrary to prior literature suggesting\ntheir broad effectiveness. (2) Overall, reasoning models do not demonstrate\nstatistically significant improvements over non-reasoning models in most\nsettings. This suggests that extended long CoT provides relatively limited\nbenefits for data annotation in specialized domains. (3) Certain model\nbehaviors emerge in the multi-agent discussion environment. For instance,\nClaude 3.7 Sonnet with thinking rarely changes its initial annotations, even\nwhen other agents provide correct annotations or valid reasoning.", "AI": {"tldr": "This paper evaluates the effectiveness of large language models (LLMs) as alternatives to human expert annotators for textual data annotation in specialized domains like finance, biomedicine, and law.", "motivation": "To investigate if top-performing LLMs can serve as substitutes for human expert annotators in specialized domains, addressing the high cost and labor demands of text annotation.", "method": "The study evaluates individual LLMs and a multi-agent discussion framework where LLMs engage in discussions to finalize annotations and incorporates reasoning models for comparison.", "result": "The findings indicate that individual LLMs show marginal or negative performance gains compared to expectations, and reasoning models do not significantly outperform non-reasoning models in most cases.", "conclusion": "The extended chain-of-thought reasoning provides limited benefits for data annotation in specialized fields, and LLMs exhibit specific behaviors in discussion settings that impact their annotation decisions.", "key_contributions": ["Evaluation of LLMs in three specialized domains", "Introduction of a multi-agent discussion framework for LLMs", "Insights into LLM performance with reasoning models in annotation tasks"], "limitations": "Limited performance of LLMs in expert-level annotation tasks and the lack of significant improvements from reasoning models.", "keywords": ["large language models", "data annotation", "multi-agent discussions", "reasoning models", "specialized domains"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.14571", "pdf": "https://arxiv.org/pdf/2504.14571.pdf", "abs": "https://arxiv.org/abs/2504.14571", "title": "Prompt-Hacking: The New p-Hacking?", "authors": ["Thomas Kosch", "Sebastian Feger"], "categories": ["cs.HC"], "comment": null, "summary": "As Large Language Models (LLMs) become increasingly embedded in empirical\nresearch workflows, their use as analytical tools for quantitative or\nqualitative data raises pressing concerns for scientific integrity. This\nopinion paper draws a parallel between \"prompt-hacking\", the strategic tweaking\nof prompts to elicit desirable outputs from LLMs, and the well-documented\npractice of \"p-hacking\" in statistical analysis. We argue that the inherent\nbiases, non-determinism, and opacity of LLMs make them unsuitable for data\nanalysis tasks demanding rigor, impartiality, and reproducibility. We emphasize\nhow researchers may inadvertently, or even deliberately, adjust prompts to\nconfirm hypotheses while undermining research validity. We advocate for a\ncritical view of using LLMs in research, transparent prompt documentation, and\nclear standards for when LLM use is appropriate. We discuss how LLMs can\nreplace traditional analytical methods, whereas we recommend that LLMs should\nonly be used with caution, oversight, and justification.", "AI": {"tldr": "This opinion paper addresses the concerns of using LLMs in empirical research workflows, comparing 'prompt-hacking' to 'p-hacking' and advocating for transparency and caution in their application.", "motivation": "To highlight risks associated with the use of LLMs in research and ensure scientific integrity amidst their growing prevalence.", "method": "The authors draw parallels between prompt-hacking and p-hacking, examining the biases and opacity of LLMs that affect data analysis.", "result": "The paper warns that LLMs might lead researchers to manipulate prompts for desired outcomes, compromising the validity of their work.", "conclusion": "LLMs should only be used cautiously in research, with transparent documentation and strict guidelines on their applications.", "key_contributions": ["Parallels between prompt-hacking and p-hacking", "Emphasis on scientific integrity in LLM use", "Recommendations for rigorous prompts documentation"], "limitations": "", "keywords": ["Large Language Models", "prompt-hacking", "scientific integrity", "data analysis", "transparency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.07849", "pdf": "https://arxiv.org/pdf/2508.07849.pdf", "abs": "https://arxiv.org/abs/2508.07849", "title": "LLMs for Law: Evaluating Legal-Specific LLMs on Contract Understanding", "authors": ["Amrita Singh", "H. Suhan Karaca", "Aditya Joshi", "Hye-young Paik", "Jiaojiao Jiang"], "categories": ["cs.CL"], "comment": "Under review. 4 pages + references", "summary": "Despite advances in legal NLP, no comprehensive evaluation covering multiple\nlegal-specific LLMs currently exists for contract classification tasks in\ncontract understanding. To address this gap, we present an evaluation of 10\nlegal-specific LLMs on three English language contract understanding tasks and\ncompare them with 7 general-purpose LLMs. The results show that legal-specific\nLLMs consistently outperform general-purpose models, especially on tasks\nrequiring nuanced legal understanding. Legal-BERT and Contracts-BERT establish\nnew SOTAs on two of the three tasks, despite having 69% fewer parameters than\nthe best-performing general-purpose LLM. We also identify CaseLaw-BERT and\nLexLM as strong additional baselines for contract understanding. Our results\nprovide a holistic evaluation of legal-specific LLMs and will facilitate the\ndevelopment of more accurate contract understanding systems.", "AI": {"tldr": "This paper evaluates 10 legal-specific LLMs on contract classification tasks and finds they outperform general-purpose models, with Legal-BERT and Contracts-BERT achieving new state-of-the-art results.", "motivation": "There is a lack of comprehensive evaluations of legal-specific LLMs for contract classification, leading to a need for this study to fill that gap.", "method": "The paper conducts evaluations on three contract understanding tasks using 10 legal-specific LLMs and compares their performance with 7 general-purpose LLMs.", "result": "Legal-specific LLMs consistently outperform general-purpose models, particularly on nuanced legal tasks; Legal-BERT and Contracts-BERT achieve new SOTA results despite fewer parameters.", "conclusion": "The findings offer a thorough assessment of legal-specific LLMs, which can enhance the development of accurate contract understanding systems.", "key_contributions": ["Evaluation of 10 legal-specific LLMs for contract tasks", "Identification of new SOTAs for contract understanding using legal-specific models", "Comparison with general-purpose LLMs to highlight performance differences"], "limitations": "", "keywords": ["legal NLP", "contract classification", "legal-specific LLMs", "performance evaluation", "contract understanding"], "importance_score": 8, "read_time_minutes": 4}}
{"id": "2508.07860", "pdf": "https://arxiv.org/pdf/2508.07860.pdf", "abs": "https://arxiv.org/abs/2508.07860", "title": "Large Language Models for Czech Aspect-Based Sentiment Analysis", "authors": ["Jakub ≈†m√≠d", "Pavel P≈ôib√°≈à", "Pavel Kr√°l"], "categories": ["cs.CL"], "comment": "Accepted for presentation at the 28th International Conference on\n  Text, Speech and Dialogue (TSD 2025)", "summary": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis\ntask that aims to identify sentiment toward specific aspects of an entity.\nWhile large language models (LLMs) have shown strong performance in various\nnatural language processing (NLP) tasks, their capabilities for Czech ABSA\nremain largely unexplored. In this work, we conduct a comprehensive evaluation\nof 19 LLMs of varying sizes and architectures on Czech ABSA, comparing their\nperformance in zero-shot, few-shot, and fine-tuning scenarios. Our results show\nthat small domain-specific models fine-tuned for ABSA outperform\ngeneral-purpose LLMs in zero-shot and few-shot settings, while fine-tuned LLMs\nachieve state-of-the-art results. We analyze how factors such as\nmultilingualism, model size, and recency influence performance and present an\nerror analysis highlighting key challenges, particularly in aspect term\nprediction. Our findings provide insights into the suitability of LLMs for\nCzech ABSA and offer guidance for future research in this area.", "AI": {"tldr": "This study evaluates 19 LLMs for Czech aspect-based sentiment analysis, revealing that domain-specific models perform better in non-fine-tuned scenarios, with fine-tuned LLMs achieving state-of-the-art results.", "motivation": "To explore the capabilities of large language models for Czech aspect-based sentiment analysis, an area that has been largely overlooked.", "method": "A comprehensive evaluation of 19 LLMs with varying sizes and architectures was conducted, comparing performance in zero-shot, few-shot, and fine-tuning scenarios.", "result": "Findings indicate that small, domain-specific models fine-tuned for ABSA outperform general-purpose LLMs in zero-shot and few-shot settings, while fine-tuned LLMs produce state-of-the-art results.", "conclusion": "The study highlights factors influencing LLM performance in Czech ABSA and provides error analysis focusing on aspect term prediction, offering insights and guidance for future research.", "key_contributions": ["Evaluation of 19 LLMs for Czech ABSA", "Insights on multilingualism and model size impact", "Error analysis on aspect term prediction challenges"], "limitations": "The study focuses solely on Czech ABSA and may not generalize to other languages or contexts.", "keywords": ["aspect-based sentiment analysis", "large language models", "Czech", "natural language processing", "fine-tuning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.07866", "pdf": "https://arxiv.org/pdf/2508.07866.pdf", "abs": "https://arxiv.org/abs/2508.07866", "title": "Few-shot Cross-lingual Aspect-Based Sentiment Analysis with Sequence-to-Sequence Models", "authors": ["Jakub ≈†m√≠d", "Pavel P≈ôib√°≈à", "Pavel Kr√°l"], "categories": ["cs.CL"], "comment": "Accepted for presentation at the 28th International Conference on\n  Text, Speech and Dialogue (TSD 2025)", "summary": "Aspect-based sentiment analysis (ABSA) has received substantial attention in\nEnglish, yet challenges remain for low-resource languages due to the scarcity\nof labelled data. Current cross-lingual ABSA approaches often rely on external\ntranslation tools and overlook the potential benefits of incorporating a small\nnumber of target language examples into training. In this paper, we evaluate\nthe effect of adding few-shot target language examples to the training set\nacross four ABSA tasks, six target languages, and two sequence-to-sequence\nmodels. We show that adding as few as ten target language examples\nsignificantly improves performance over zero-shot settings and achieves a\nsimilar effect to constrained decoding in reducing prediction errors.\nFurthermore, we demonstrate that combining 1,000 target language examples with\nEnglish data can even surpass monolingual baselines. These findings offer\npractical insights for improving cross-lingual ABSA in low-resource and\ndomain-specific settings, as obtaining ten high-quality annotated examples is\nboth feasible and highly effective.", "AI": {"tldr": "This paper explores the benefits of incorporating few-shot training examples from target languages in aspect-based sentiment analysis (ABSA) for low-resource languages, showing significant performance improvements in cross-lingual settings.", "motivation": "The motivation behind this work is to address the challenges faced in aspect-based sentiment analysis for low-resource languages, particularly the scarcity of labelled data and the limitations of current cross-lingual methods.", "method": "The authors evaluate the impact of adding a small number of target language examples on four ABSA tasks across six target languages using two sequence-to-sequence models.", "result": "The study finds that adding as few as ten target language examples significantly improves performance compared to zero-shot settings, and combining with English data can exceed monolingual baselines.", "conclusion": "These findings suggest that incorporating a small number of high-quality annotated examples can effectively enhance cross-lingual ABSA, making it a practical approach in low-resource contexts.", "key_contributions": ["Demonstrates the importance of few-shot samples in improving ABSA for low-resource languages", "Shows combination with English data can outperform monolingual models", "Offers practical insights for domain-specific applications in sentiment analysis"], "limitations": "", "keywords": ["Aspect-based sentiment analysis", "Cross-lingual", "Few-shot learning", "Low-resource languages", "Machine learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.07902", "pdf": "https://arxiv.org/pdf/2508.07902.pdf", "abs": "https://arxiv.org/abs/2508.07902", "title": "Tailored Emotional LLM-Supporter: Enhancing Cultural Sensitivity", "authors": ["Chen Cecilia Liu", "Hiba Arnaout", "Nils Kovaƒçiƒá", "Dana Atzil-Slonim", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": "Under review; joint first authors", "summary": "Large language models (LLMs) show promise in offering emotional support and\ngenerating empathetic responses for individuals in distress, but their ability\nto deliver culturally sensitive support remains underexplored due to lack of\nresources. In this work, we introduce CultureCare, the first dataset designed\nfor this task, spanning four cultures and including 1729 distress messages,\n1523 cultural signals, and 1041 support strategies with fine-grained emotional\nand cultural annotations. Leveraging CultureCare, we (i) develop and test four\nadaptation strategies for guiding three state-of-the-art LLMs toward culturally\nsensitive responses; (ii) conduct comprehensive evaluations using LLM judges,\nin-culture human annotators, and clinical psychologists; (iii) show that\nadapted LLMs outperform anonymous online peer responses, and that simple\ncultural role-play is insufficient for cultural sensitivity; and (iv) explore\nthe application of LLMs in clinical training, where experts highlight their\npotential in fostering cultural competence in future therapists.", "AI": {"tldr": "This paper introduces CultureCare, a dataset for culturally sensitive emotional support using LLMs, and evaluates adaptation strategies for improved responses.", "motivation": "To explore the ability of large language models to provide culturally sensitive emotional support, an area that has been underexplored.", "method": "The authors introduce the CultureCare dataset, which includes distress messages, cultural signals, and support strategies. They then develop and test adaptation strategies for LLMs and conduct evaluations with various judges.", "result": "Adapted LLMs significantly outperformed responses from anonymous online peers, demonstrating the necessity for careful adaptation rather than simple cultural role-play.", "conclusion": "The research highlights the potential of LLMs to enhance cultural competence in clinical training, indicating a significant step toward culturally sensitive AI applications in emotional support.", "key_contributions": ["Development of the CultureCare dataset for culturally sensitive emotional support", "Evaluation of adaptation strategies for LLMs", "Evidence showing LLMs can exceed peer responses in cultural sensitivity"], "limitations": "The study is limited by its reliance on the specific cultures represented in the dataset and the subjective nature of cultural sensitivity evaluations.", "keywords": ["Large language models", "Cultural sensitivity", "Emotional support", "AI in healthcare"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.07937", "pdf": "https://arxiv.org/pdf/2508.07937.pdf", "abs": "https://arxiv.org/abs/2508.07937", "title": "Challenges and opportunities in portraying emotion in generated sign language", "authors": ["John C. McDonald", "Rosalee Wolfe", "Fabrizio Nunnari"], "categories": ["cs.CL"], "comment": null, "summary": "Non-manual signals in sign languages continue to be a challenge for signing\navatars. More specifically, emotional content has been difficult to incorporate\nbecause of a lack of a standard method of specifying the avatar's emotional\nstate. This paper explores the application of an intuitive two-parameter\nrepresentation for emotive non-manual signals to the Paula signing avatar that\nshows promise for facilitating the linguistic specification of emotional facial\nexpressions in a more coherent manner than previous methods. Users can apply\nthese parameters to control Paula's emotional expressions through a textual\nrepresentation called the EASIER notation. The representation can allow avatars\nto express more nuanced emotional states using two numerical parameters. It\nalso has the potential to enable more consistent specification of emotional\nnon-manual signals in linguistic annotations which drive signing avatars.", "AI": {"tldr": "The paper presents a new method for representing emotional non-manual signals in sign language avatars, improving the specification of emotional expressions.", "motivation": "There is a lack of a standard method for incorporating emotional content into signing avatars, which hampers their effectiveness.", "method": "The paper introduces a two-parameter representation for emotive non-manual signals that can be applied to the Paula signing avatar.", "result": "The new representation enables users to control Paula's emotional expressions more intuitively through a textual notation, allowing for nuanced emotional states.", "conclusion": "This approach facilitates a more coherent specification of emotional expressions in signing avatars, improving their linguistic capability.", "key_contributions": ["Introduction of EASIER notation for emotional expression in avatars", "Two-parameter representation enhances emotional nuance", "Potential for consistent linguistic annotations for signing avatars"], "limitations": "", "keywords": ["sign language", "emotional expressions", "signing avatars", "non-manual signals", "EASIER notation"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.07955", "pdf": "https://arxiv.org/pdf/2508.07955.pdf", "abs": "https://arxiv.org/abs/2508.07955", "title": "Expert Preference-based Evaluation of Automated Related Work Generation", "authors": ["Furkan ≈ûahinu√ß", "Subhabrata Dutta", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": "Project page: https://ukplab.github.io/arxiv2025-expert-eval-rw/", "summary": "Expert domain writing, such as scientific writing, typically demands\nextensive domain knowledge. Recent advances in LLMs show promising potential in\nreducing the expert workload. However, evaluating the quality of automatically\ngenerated scientific writing is a crucial open issue, as it requires knowledge\nof domain-specific evaluation criteria and the ability to discern expert\npreferences. Conventional automatic metrics and LLM-as-a-judge systems are\ninsufficient to grasp expert preferences and domain-specific quality standards.\nTo address this gap and support human-AI collaborative writing, we focus on\nrelated work generation, one of the most challenging scientific tasks, as an\nexemplar. We propose GREP, a multi-turn evaluation framework that integrates\nclassical related work evaluation criteria with expert-specific preferences.\nInstead of assigning a single score, our framework decomposes the evaluation\ninto fine-grained dimensions. This localized evaluation approach is further\naugmented with contrastive few-shot examples to provide detailed contextual\nguidance for the evaluation dimensions. The design principles allow our\nframework to deliver cardinal assessment of quality, which can facilitate\nbetter post-training compared to ordinal preference data. For better\naccessibility, we design two variants of GREP: a more precise variant with\nproprietary LLMs as evaluators, and a cheaper alternative with open-weight\nLLMs. Empirical investigation reveals that our framework is able to assess the\nquality of related work sections in a much more robust manner compared to\nstandard LLM judges, reflects natural scenarios of scientific writing, and\nbears a strong correlation with the human expert assessment. We also observe\nthat generations from state-of-the-art LLMs struggle to satisfy validation\nconstraints of a suitable related work section. They (mostly) fail to improve\nbased on feedback as well.", "AI": {"tldr": "The paper presents GREP, a multi-turn evaluation framework for assessing the quality of scientific writing, particularly in related work sections, integrating expert preferences with evaluation criteria.", "motivation": "There is a need for better evaluation methods in automatically generated scientific writing, as conventional metrics fail to capture expert preferences and domain-specific standards.", "method": "GREP decomposes evaluation into fine-grained dimensions, utilizing classical criteria and contrastive few-shot examples to enhance the evaluation process.", "result": "GREP demonstrates a robust ability to assess related work quality, showing a strong correlation with expert assessment and outperforming standard LLM judges.", "conclusion": "GREP's localized evaluation approach enables better post-training assessments and supports human-AI collaborative writing in scientific contexts.", "key_contributions": ["Introduction of a multi-turn evaluation framework for scientific writing", "Fine-grained assessment dimensions incorporating expert preferences", "Two variants of GREP designed for varying resource availability"], "limitations": "", "keywords": ["LLM", "scientific writing", "evaluation framework", "human-AI collaboration", "related work generation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.07959", "pdf": "https://arxiv.org/pdf/2508.07959.pdf", "abs": "https://arxiv.org/abs/2508.07959", "title": "Large Language Models for Subjective Language Understanding: A Survey", "authors": ["Changhao Song", "Yazhou Zhang", "Hui Gao", "Ben Yao", "Peng Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Subjective language understanding refers to a broad set of natural language\nprocessing tasks where the goal is to interpret or generate content that\nconveys personal feelings, opinions, or figurative meanings rather than\nobjective facts. With the advent of large language models (LLMs) such as\nChatGPT, LLaMA, and others, there has been a paradigm shift in how we approach\nthese inherently nuanced tasks. In this survey, we provide a comprehensive\nreview of recent advances in applying LLMs to subjective language tasks,\nincluding sentiment analysis, emotion recognition, sarcasm detection, humor\nunderstanding, stance detection, metaphor interpretation, intent detection, and\naesthetics assessment. We begin by clarifying the definition of subjective\nlanguage from linguistic and cognitive perspectives, and we outline the unique\nchallenges posed by subjective language (e.g. ambiguity, figurativeness,\ncontext dependence). We then survey the evolution of LLM architectures and\ntechniques that particularly benefit subjectivity tasks, highlighting why LLMs\nare well-suited to model subtle human-like judgments. For each of the eight\ntasks, we summarize task definitions, key datasets, state-of-the-art LLM-based\nmethods, and remaining challenges. We provide comparative insights, discussing\ncommonalities and differences among tasks and how multi-task LLM approaches\nmight yield unified models of subjectivity. Finally, we identify open issues\nsuch as data limitations, model bias, and ethical considerations, and suggest\nfuture research directions. We hope this survey will serve as a valuable\nresource for researchers and practitioners interested in the intersection of\naffective computing, figurative language processing, and large-scale language\nmodels.", "AI": {"tldr": "This survey reviews the application of large language models (LLMs) to subjective language tasks such as sentiment analysis and irony detection, discussing methodologies, advancements, challenges, and future directions.", "motivation": "The paper addresses the growing necessity to understand and process subjective language in natural language processing, driven by the advancements in LLMs.", "method": "The authors provide a comprehensive overview of subjective language tasks, detailing LLM architectures, techniques, and their applications across various tasks involving subjective interpretation.", "result": "The survey highlights the unique challenges of subjective language and summarizes state-of-the-art methods, datasets, and comparative insights across eight tasks.", "conclusion": "The paper emphasizes the importance of addressing open issues like data limitations and model bias, and suggests future research directions in the field.", "key_contributions": ["A comprehensive review of LLM applications in subjective language tasks", "Identification of remaining challenges and open issues", "Insights on multi-task LLM approaches for unified models of subjectivity."], "limitations": "The paper acknowledges limitations related to data availability, model bias, and ethical implications in subjective language processing.", "keywords": ["subjective language", "large language models", "sentiment analysis", "affective computing", "figurative language"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2508.07964", "pdf": "https://arxiv.org/pdf/2508.07964.pdf", "abs": "https://arxiv.org/abs/2508.07964", "title": "Toward Machine Interpreting: Lessons from Human Interpreting Studies", "authors": ["Matthias Sperber", "Maureen de Seyssel", "Jiajun Bao", "Matthias Paulik"], "categories": ["cs.CL"], "comment": null, "summary": "Current speech translation systems, while having achieved impressive\naccuracies, are rather static in their behavior and do not adapt to real-world\nsituations in ways human interpreters do. In order to improve their practical\nusefulness and enable interpreting-like experiences, a precise understanding of\nthe nature of human interpreting is crucial. To this end, we discuss human\ninterpreting literature from the perspective of the machine translation field,\nwhile considering both operational and qualitative aspects. We identify\nimplications for the development of speech translation systems and argue that\nthere is great potential to adopt many human interpreting principles using\nrecent modeling techniques. We hope that our findings provide inspiration for\nclosing the perceived usability gap, and can motivate progress toward true\nmachine interpreting.", "AI": {"tldr": "The paper discusses the need for speech translation systems to adapt more like human interpreters by applying principles from human interpreting literature and modern modeling techniques.", "motivation": "To improve the practical usefulness of speech translation systems by closing the usability gap with human interpreters.", "method": "Analyzing human interpreting literature and its implications for machine translation development.", "result": "Identified opportunities to enhance speech translation systems by incorporating human interpreting principles and leveraging recent technologies.", "conclusion": "The findings may inspire advancements toward true machine interpreting capabilities.", "key_contributions": ["Comparison of human interpreting and machine translation", "Insights into operational and qualitative aspects of interpreting", "Recommendations for integrating human principles into machine systems"], "limitations": "", "keywords": ["Speech Translation", "Human Interpreting", "Machine Translation", "Usability", "Interpreting Principles"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.07969", "pdf": "https://arxiv.org/pdf/2508.07969.pdf", "abs": "https://arxiv.org/abs/2508.07969", "title": "Understanding Syntactic Generalization in Structure-inducing Language Models", "authors": ["David Arps", "Hassan Sajjad", "Laura Kallmeyer"], "categories": ["cs.CL"], "comment": "Code available at https://github.com/davidarps/silm", "summary": "Structure-inducing Language Models (SiLM) are trained on a self-supervised\nlanguage modeling task, and induce a hierarchical sentence representation as a\nbyproduct when processing an input. A wide variety of SiLMs have been proposed.\nHowever, these have typically been evaluated on a relatively small scale, and\nevaluation of these models has systematic gaps and lacks comparability. In this\nwork, we study three different SiLM architectures using both natural language\n(English) corpora and synthetic bracketing expressions: Structformer (Shen et\nal., 2021), UDGN (Shen et al., 2022) and GPST (Hu et al., 2024). We compare\nthem with respect to (i) properties of the induced syntactic representations\n(ii) performance on grammaticality judgment tasks, and (iii) training dynamics.\nWe find that none of the three architectures dominates across all evaluation\nmetrics. However, there are significant differences, in particular with respect\nto the induced syntactic representations. The Generative Pretrained Structured\nTransformer (GPST; Hu et al. 2024) performs most consistently across evaluation\nsettings, and outperforms the other models on long-distance dependencies in\nbracketing expressions. Furthermore, our study shows that small models trained\non large amounts of synthetic data provide a useful testbed for evaluating\nbasic model properties.", "AI": {"tldr": "This paper evaluates three Structure-inducing Language Models (SiLMs) on their syntactic representation, grammaticality judgment performance, and training dynamics, finding the GPST model most consistent across metrics used.", "motivation": "To address the systematic gaps in the evaluation of different SiLMs and provide a comprehensive comparison across various metrics.", "method": "The authors analyze Structformer, UDGN, and GPST architectures, utilizing both natural language corpora and synthetic bracketing expressions for evaluation across several criteria.", "result": "The study finds no single architecture dominates; however, the GPST model outperforms others in long-distance dependencies and shows consistent performance across evaluation settings.", "conclusion": "Small models trained on large synthetic datasets are effective for evaluating fundamental model properties; GPST is recommended for its robust performance.", "key_contributions": ["Comparison of three different SiLM architectures", "Evaluation across natural and synthetic datasets", "Demonstration of model performance on grammaticality judgment tasks"], "limitations": "", "keywords": ["Structure-inducing Language Models", "syntactic representations", "grammaticality judgment"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.07976", "pdf": "https://arxiv.org/pdf/2508.07976.pdf", "abs": "https://arxiv.org/abs/2508.07976", "title": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL", "authors": ["Jiaxuan Gao", "Wei Fu", "Minyang Xie", "Shusheng Xu", "Chuyi He", "Zhiyu Mei", "Banghua Zhu", "Yi Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in LLM-based agents have demonstrated remarkable\ncapabilities in handling complex, knowledge-intensive tasks by integrating\nexternal tools. Among diverse choices of tools, search tools play a pivotal\nrole in accessing vast external knowledge. However, open-source agents still\nfall short of achieving expert-level Search Intelligence, the ability to\nresolve ambiguous queries, generate precise searches, analyze results, and\nconduct thorough exploration. Existing approaches fall short in scalability,\nefficiency, and data quality. For example, small turn limits in existing online\nRL methods, e.g. <=10, restrict complex strategy learning. This paper\nintroduces ASearcher, an open-source project for large-scale RL training of\nsearch agents. Our key contributions include: (1) Scalable fully asynchronous\nRL training that enables long-horizon search while maintaining high training\nefficiency. (2) A prompt-based LLM agent that autonomously synthesizes\nhigh-quality and challenging QAs, creating a large-scale QA dataset. Through RL\ntraining, our prompt-based QwQ-32B agent achieves substantial improvements,\nwith 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our\nagent exhibits extreme long-horizon search, with tool calls exceeding 40 turns\nand output tokens exceeding 150k during training time. With a simple agent\ndesign and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on\nxBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We\nopen-source our models, training data, and codes in\nhttps://github.com/inclusionAI/ASearcher.", "AI": {"tldr": "ASearcher is an open-source project that enhances the training of large language model (LLM) search agents through scalable reinforcement learning (RL) methods, achieving significant performance improvements in search tasks.", "motivation": "To address the shortcomings of existing open-source agents in Search Intelligence, particularly in scalability, efficiency, and data quality for handling complex queries.", "method": "A scalable fully asynchronous RL training framework is proposed, allowing for long-horizon search strategies without restrictions on turn limits. A prompt-based LLM agent synthesizes high-quality QAs to create an extensive dataset for training.", "result": "The prompt-based QwQ-32B agent shows significant advancements in search performance, with 46.7% and 20.8% Avg@4 improvements on xBench and GAIA. It successfully navigates over 40 turns with extensive tool usage during training.", "conclusion": "ASearcher-Web-QwQ surpasses previous models, achieving high Avg@4 scores of 42.1 and 52.8, respectively, and provides open access to models, training data, and code.", "key_contributions": ["Scalable fully asynchronous RL training for long-horizon search", "Prompt-based LLM agent synthesizing high-quality QAs", "Open-sourcing of models, data, and implementation"], "limitations": "", "keywords": ["Reinforcement Learning", "Large Language Models", "Search Intelligence", "Open-source"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.07993", "pdf": "https://arxiv.org/pdf/2508.07993.pdf", "abs": "https://arxiv.org/abs/2508.07993", "title": "The Medical Metaphors Corpus (MCC)", "authors": ["Anna Sofia Lippolis", "Andrea Giovanni Nuzzolese", "Aldo Gangemi"], "categories": ["cs.CL"], "comment": null, "summary": "Metaphor is a fundamental cognitive mechanism that shapes scientific\nunderstanding, enabling the communication of complex concepts while potentially\nconstraining paradigmatic thinking. Despite the prevalence of figurative\nlanguage in scientific discourse, existing metaphor detection resources\nprimarily focus on general-domain text, leaving a critical gap for\ndomain-specific applications. In this paper, we present the Medical Metaphors\nCorpus (MCC), a comprehensive dataset of 792 annotated scientific conceptual\nmetaphors spanning medical and biological domains. MCC aggregates metaphorical\nexpressions from diverse sources including peer-reviewed literature, news\nmedia, social media discourse, and crowdsourced contributions, providing both\nbinary and graded metaphoricity judgments validated through human annotation.\nEach instance includes source-target conceptual mappings and perceived\nmetaphoricity scores on a 0-7 scale, establishing the first annotated resource\nfor computational scientific metaphor research. Our evaluation demonstrates\nthat state-of-the-art language models achieve modest performance on scientific\nmetaphor detection, revealing substantial room for improvement in\ndomain-specific figurative language understanding. MCC enables multiple\nresearch applications including metaphor detection benchmarking, quality-aware\ngeneration systems, and patient-centered communication tools.", "AI": {"tldr": "The paper introduces the Medical Metaphors Corpus (MCC), a dataset of 792 annotated scientific metaphors in medical and biological domains, addressing the gap in metaphor detection resources for domain-specific applications.", "motivation": "To fill the gap in metaphor detection resources focused on scientific discourse, particularly in the medical and biological fields.", "method": "A comprehensive dataset (MCC) was created, aggregating metaphorical expressions from various sources with binary and graded judgments on metaphoricity validated through human annotation.", "result": "Evaluation revealed modest performance of state-of-the-art language models in detecting scientific metaphors, indicating a need for improvement in understanding domain-specific figurative language.", "conclusion": "MCC serves as a novel resource for metaphor detection benchmarking, improving generation systems, and enhancing patient communication tools.", "key_contributions": ["Introduction of the Medical Metaphors Corpus (MCC) for scientific metaphor research.", "First annotated resource for computational scientific metaphor detection.", "Insights into the performance of language models in understanding domain-specific metaphors."], "limitations": "", "keywords": ["metaphor detection", "medical corpus", "figurative language", "language models", "health communication"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.07999", "pdf": "https://arxiv.org/pdf/2508.07999.pdf", "abs": "https://arxiv.org/abs/2508.07999", "title": "WideSearch: Benchmarking Agentic Broad Info-Seeking", "authors": ["Ryan Wong", "Jiawei Wang", "Junjie Zhao", "Li Chen", "Yan Gao", "Long Zhang", "Xuan Zhou", "Zuo Wang", "Kai Xiang", "Ge Zhang", "Wenhao Huang", "Yang Wang", "Ke Wang"], "categories": ["cs.CL"], "comment": null, "summary": "From professional research to everyday planning, many tasks are bottlenecked\nby wide-scale information seeking, which is more repetitive than cognitively\ncomplex. With the rapid development of Large Language Models (LLMs), automated\nsearch agents powered by LLMs offer a promising solution to liberate humans\nfrom this tedious work. However, the capability of these agents to perform such\n\"wide-context\" collection reliably and completely remains largely unevaluated\ndue to a lack of suitable benchmarks. To bridge this gap, we introduce\nWideSearch, a new benchmark engineered to evaluate agent reliability on these\nlarge-scale collection tasks. The benchmark features 200 manually curated\nquestions (100 in English, 100 in Chinese) from over 15 diverse domains,\ngrounded in real user queries. Each task requires agents to collect large-scale\natomic information, which could be verified one by one objectively, and arrange\nit into a well-organized output. A rigorous five-stage quality control pipeline\nensures the difficulty, completeness, and verifiability of the dataset. We\nbenchmark over 10 state-of-the-art agentic search systems, including\nsingle-agent, multi-agent frameworks, and end-to-end commercial systems. Most\nsystems achieve overall success rates near 0\\%, with the best performer\nreaching just 5\\%. However, given sufficient time, cross-validation by multiple\nhuman testers can achieve a near 100\\% success rate. These results demonstrate\nthat present search agents have critical deficiencies in large-scale\ninformation seeking, underscoring urgent areas for future research and\ndevelopment in agentic search. Our dataset, evaluation pipeline, and benchmark\nresults have been publicly released at https://widesearch-seed.github.io/", "AI": {"tldr": "WideSearch is a new benchmark for evaluating LLM-powered search agents' reliability on large-scale information collection tasks, revealing critical deficiencies in current systems.", "motivation": "To address the challenges faced by search agents in performing wide-context information collection reliably and completely due to the lack of suitable benchmarks.", "method": "Development of WideSearch, a benchmark featuring 200 manually curated questions from diverse domains, to evaluate agent performance on atomic information collection and organization.", "result": "Benchmarking over 10 search systems revealed overall success rates near 0%, with the best system achieving only 5%, but human cross-validation can reach nearly 100%.", "conclusion": "Current agentic search systems exhibit critical deficiencies, highlighting the need for further research and improved methodologies in large-scale information seeking.", "key_contributions": ["Introduction of the WideSearch benchmark", "Evaluation of over 10 state-of-the-art search systems", "Demonstration of deficiencies in current search agents' performance"], "limitations": "Limited to the specific questions curated for the benchmark and performance analysis of existing search systems.", "keywords": ["Large Language Models", "search agents", "benchmark", "information seeking", "human validation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.08011", "pdf": "https://arxiv.org/pdf/2508.08011.pdf", "abs": "https://arxiv.org/abs/2508.08011", "title": "Progressive Depth Up-scaling via Optimal Transport", "authors": ["Mingzi Cao", "Xi Wang", "Nikolaos Aletras"], "categories": ["cs.CL"], "comment": null, "summary": "Scaling Large Language Models (LLMs) yields performance gains but incurs\nsubstantial training costs. Depth up-scaling offers training efficiency by\nadding new layers to pre-trained models. However, most existing methods copy or\naverage weights from base layers, neglecting neuron permutation differences.\nThis limitation can potentially cause misalignment that harms performance.\nInspired by applying Optimal Transport (OT) for neuron alignment, we propose\nOptimal Transport Depth Up-Scaling (OpT-DeUS). OpT-DeUS aligns and fuses\nTransformer blocks in adjacent base layers via OT for new layer creation, to\nmitigate neuron permutation mismatch between layers. OpT-DeUS achieves better\noverall performance and offers improved training efficiency than existing\nmethods for continual pre-training and supervised fine-tuning across different\nmodel sizes. To further evaluate the impact of interpolation positions, our\nextensive analysis shows that inserting new layers closer to the top results in\nhigher training efficiency due to shorter back-propagation time while obtaining\nadditional performance gains.", "AI": {"tldr": "The paper presents Optimal Transport Depth Up-Scaling (OpT-DeUS) to improve the training efficiency and performance of Large Language Models by addressing neuron permutation differences in layer scaling.", "motivation": "To enhance the performance and training efficiency of Large Language Models through an innovative method that ensures neuron alignment during depth scaling.", "method": "OpT-DeUS employs Optimal Transport to align and fuse Transformer blocks from adjacent base layers, creating new layers that mitigate neuron permutation mismatch.", "result": "The proposed method demonstrates better overall performance and enhanced training efficiency compared to existing depth up-scaling techniques during continual pre-training and supervised fine-tuning across different model sizes.", "conclusion": "Inserting new layers closer to the top of the model yields higher training efficiency and additional performance gains, highlighting the importance of layer positioning.", "key_contributions": ["Introduction of Optimal Transport for neuron alignment during depth up-scaling.", "Demonstration of improved performance and efficiency in LLM training.", "Analysis of layer insertion positions and their impact on training efficiency."], "limitations": "", "keywords": ["Large Language Models", "Optimal Transport", "Training Efficiency", "Depth Up-Scaling", "Transformer"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.08050", "pdf": "https://arxiv.org/pdf/2508.08050.pdf", "abs": "https://arxiv.org/abs/2508.08050", "title": "9th Workshop on Sign Language Translation and Avatar Technologies (SLTAT 2025)", "authors": ["Fabrizio Nunnari", "Cristina Luna Jim√©nez", "Rosalee Wolfe", "John C. McDonald", "Michael Filhol", "Eleni Efthimiou", "Evita Fotinea", "Thomas Hanke"], "categories": ["cs.CL"], "comment": null, "summary": "The Sign Language Translation and Avatar Technology (SLTAT) workshops\ncontinue a series of gatherings to share recent advances in improving deaf /\nhuman communication through non-invasive means. This 2025 edition, the 9th\nsince its first appearance in 2011, is hosted by the International Conference\non Intelligent Virtual Agents (IVA), giving the opportunity for contamination\nbetween two research communities, using digital humans as either virtual\ninterpreters or as interactive conversational agents. As presented in this\nsummary paper, SLTAT sees contributions beyond avatar technologies, with a\nconsistent number of submissions on sign language recognition, and other work\non data collection, data analysis, tools, ethics, usability, and affective\ncomputing.", "AI": {"tldr": "The 2025 Sign Language Translation and Avatar Technology (SLTAT) workshops focus on advancements in facilitating deaf/human communication. Hosted at the IVA conference, it emphasizes research on digital humans as interpreters and conversational agents, alongside sign language recognition and various relevant fields.", "motivation": "To improve communication between deaf individuals and humans through non-invasive technologies and gather insights from multiple research communities.", "method": "The workshops include contributions and research presentations related to sign language recognition, data collection and analysis, usability, ethics, and affective computing.", "result": "A variety of submissions showcasing advancements in both avatar technologies and sign language communication methods.", "conclusion": "The SLTAT workshops foster interdisciplinary collaboration and contribute to better tools and methods for assisting communication for the deaf community.", "key_contributions": ["Interdisciplinary collaboration between avatar technology and sign language research", "Focus on ethical considerations in technology use", "Advancements in sign language recognition and data analysis methods"], "limitations": "", "keywords": ["Sign Language Translation", "Avatar Technology", "Affective Computing", "Human-Computer Interaction", "Usability"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2508.08095", "pdf": "https://arxiv.org/pdf/2508.08095.pdf", "abs": "https://arxiv.org/abs/2508.08095", "title": "Dual Information Speech Language Models for Emotional Conversations", "authors": ["Chun Wang", "Chenyang Liu", "Wenze Xu", "Weihong Deng"], "categories": ["cs.CL", "cs.AI"], "comment": "Presented at IEEE ICME 2025", "summary": "Conversational systems relying on text-based large language models (LLMs)\noften overlook paralinguistic cues, essential for understanding emotions and\nintentions. Speech-language models (SLMs), which use speech as input, are\nemerging as a promising solution. However, SLMs built by extending frozen LLMs\nstruggle to capture paralinguistic information and exhibit reduced context\nunderstanding. We identify entangled information and improper training\nstrategies as key issues. To address these issues, we propose two heterogeneous\nadapters and suggest a weakly supervised training strategy. Our approach\ndisentangles paralinguistic and linguistic information, enabling SLMs to\ninterpret speech through structured representations. It also preserves\ncontextual understanding by avoiding the generation of task-specific vectors\nthrough controlled randomness. This approach trains only the adapters on common\ndatasets, ensuring parameter and data efficiency. Experiments demonstrate\ncompetitive performance in emotional conversation tasks, showcasing the model's\nability to effectively integrate both paralinguistic and linguistic information\nwithin contextual settings.", "AI": {"tldr": "Proposed a method using heterogeneous adapters to enhance speech-language models for better understanding of paralinguistic cues and context in emotional conversations.", "motivation": "To improve emotional understanding and contextual interpretation in conversational systems utilizing speech as input, addressing limitations of existing speech-language models.", "method": "Introduced two heterogeneous adapters and a weakly supervised training strategy to disentangle paralinguistic and linguistic information while maintaining efficient training using common datasets.", "result": "Experiments indicate that the proposed approach demonstrates competitive performance in emotional conversation tasks, effectively integrating paralinguistic and linguistic information.", "conclusion": "The approach shows promise for improving speech-language models' performance in emotional contexts, highlighting the importance of paralinguistic cues in conversation.", "key_contributions": ["Identification of the limitations of existing speech-language models", "Proposed heterogeneous adapters for better information disentanglement", "Weakly supervised training strategy that preserves context"], "limitations": "", "keywords": ["speech-language models", "paralinguistic cues", "emotional conversation", "language models", "contextual understanding"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.08096", "pdf": "https://arxiv.org/pdf/2508.08096.pdf", "abs": "https://arxiv.org/abs/2508.08096", "title": "Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?", "authors": ["Lukas Gehring", "Benjamin Paa√üen"], "categories": ["cs.CL", "cs.LG"], "comment": "Preprint as provided by the authors (19 pages, 12 figures, 9 tables)", "summary": "Recent advancements in Large Language Models (LLMs) and their increased\naccessibility have made it easier than ever for students to automatically\ngenerate texts, posing new challenges for educational institutions. To enforce\nnorms of academic integrity and ensure students' learning, learning analytics\nmethods to automatically detect LLM-generated text appear increasingly\nappealing. This paper benchmarks the performance of different state-of-the-art\ndetectors in educational contexts, introducing a novel dataset, called\nGenerative Essay Detection in Education (GEDE), containing over 900\nstudent-written essays and over 12,500 LLM-generated essays from various\ndomains. To capture the diversity of LLM usage practices in generating text, we\npropose the concept of contribution levels, representing students' contribution\nto a given assignment. These levels range from purely human-written texts, to\nslightly LLM-improved versions, to fully LLM-generated texts, and finally to\nactive attacks on the detector by \"humanizing\" generated texts. We show that\nmost detectors struggle to accurately classify texts of intermediate student\ncontribution levels, like LLM-improved human-written texts. Detectors are\nparticularly likely to produce false positives, which is problematic in\neducational settings where false suspicions can severely impact students'\nlives. Our dataset, code, and additional supplementary materials are publicly\navailable at\nhttps://github.com/lukasgehring/Assessing-LLM-Text-Detection-in-Educational-Contexts.", "AI": {"tldr": "The paper benchmarks the performance of state-of-the-art detectors for identifying LLM-generated texts in educational contexts, presenting a novel dataset and discussing issues related to false positives in detection.", "motivation": "With the accessibility of LLMs leading to increased text generation by students, there is a pressing need for methods to uphold academic integrity through the detection of LLM-generated content.", "method": "The paper introduces the Generative Essay Detection in Education (GEDE) dataset, which includes student-written essays and LLM-generated texts, and evaluates the accuracy of various text detectors on these documents, focusing on different contribution levels of text generation.", "result": "The study finds that most detectors struggle with texts that are generated with intermediate contribution levels, particularly showing a tendency to produce false positives, which may adversely affect students.", "conclusion": "The results highlight the limitations of current text detection methods in accurately classifying nuanced student contributions and underscore the significance of supporting detection systems that minimize false positives in educational settings.", "key_contributions": ["Introduction of the GEDE dataset with diverse essay types", "Benchmarking state-of-the-art detectors in educational settings", "Identification of contribution levels in text generation"], "limitations": "Detectors primarily falter with intermediate contribution levels and often result in false positives, which can be detrimental in educational environments.", "keywords": ["Large Language Models", "LLM detection", "academic integrity", "education", "learning analytics"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.08110", "pdf": "https://arxiv.org/pdf/2508.08110.pdf", "abs": "https://arxiv.org/abs/2508.08110", "title": "Iterative refinement, not training objective, makes HuBERT behave differently from wav2vec 2.0", "authors": ["Robin Huo", "Ewan Dunbar"], "categories": ["cs.CL"], "comment": "Proceedings of Interspeech 2025", "summary": "Self-supervised models for speech representation learning now see widespread\nuse for their versatility and performance on downstream tasks, but the effect\nof model architecture on the linguistic information learned in their\nrepresentations remains under-studied. This study investigates two such models,\nHuBERT and wav2vec 2.0, and minimally compares two of their architectural\ndifferences: training objective and iterative pseudo-label refinement through\nmultiple training iterations. We find that differences in canonical correlation\nof hidden representations to word identity, phoneme identity, and speaker\nidentity are explained by training iteration, not training objective. We\nsuggest that future work investigate the reason for the effectiveness of\niterative refinement in encoding linguistic information in self-supervised\nspeech representations.", "AI": {"tldr": "This study investigates the impact of model architecture on linguistic information learned in self-supervised speech representation models HuBERT and wav2vec 2.0.", "motivation": "To understand how architectural differences in self-supervised models affect the encoding of linguistic information in speech representations.", "method": "The study compares the effects of training objective and iterative pseudo-label refinement on HuBERT and wav2vec 2.0, focusing on their hidden representations.", "result": "Differences in canonical correlation of hidden representations to word, phoneme, and speaker identity are attributed to the number of training iterations, rather than the training objective.", "conclusion": "Iterative refinement is effective in enhancing linguistic information encoding in self-supervised speech representations, warranting further investigation.", "key_contributions": ["Comparison of HuBERT and wav2vec 2.0", "Identification of iterative refinement impact on linguistic information", "Recommendations for future research into architectural optimizations"], "limitations": "", "keywords": ["self-supervised learning", "speech representations", "HuBERT", "wav2vec 2.0", "linguistic information"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.08125", "pdf": "https://arxiv.org/pdf/2508.08125.pdf", "abs": "https://arxiv.org/abs/2508.08125", "title": "Czech Dataset for Complex Aspect-Based Sentiment Analysis Tasks", "authors": ["Jakub ≈†m√≠d", "Pavel P≈ôib√°≈à", "Ond≈ôej Pra≈æ√°k", "Pavel Kr√°l"], "categories": ["cs.CL"], "comment": "Published In Proceedings of the 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation (LREC-COLING\n  2024). Official version: https://aclanthology.org/2024.lrec-main.374/", "summary": "In this paper, we introduce a novel Czech dataset for aspect-based sentiment\nanalysis (ABSA), which consists of 3.1K manually annotated reviews from the\nrestaurant domain. The dataset is built upon the older Czech dataset, which\ncontained only separate labels for the basic ABSA tasks such as aspect term\nextraction or aspect polarity detection. Unlike its predecessor, our new\ndataset is specifically designed for more complex tasks, e.g.\ntarget-aspect-category detection. These advanced tasks require a unified\nannotation format, seamlessly linking sentiment elements (labels) together. Our\ndataset follows the format of the well-known SemEval-2016 datasets. This design\nchoice allows effortless application and evaluation in cross-lingual scenarios,\nultimately fostering cross-language comparisons with equivalent counterpart\ndatasets in other languages. The annotation process engaged two trained\nannotators, yielding an impressive inter-annotator agreement rate of\napproximately 90%. Additionally, we provide 24M reviews without annotations\nsuitable for unsupervised learning. We present robust monolingual baseline\nresults achieved with various Transformer-based models and insightful error\nanalysis to supplement our contributions. Our code and dataset are freely\navailable for non-commercial research purposes.", "AI": {"tldr": "Introduction of a novel Czech dataset for aspect-based sentiment analysis with 3.1K annotated reviews from restaurants, aimed at advanced sentiment analysis tasks.", "motivation": "The need for a more complex dataset for aspect-based sentiment analysis in Czech, enabling better cross-lingual comparisons.", "method": "Creation of a dataset with unified annotation for more complex tasks, involving two trained annotators achieving a 90% inter-annotator agreement rate.", "result": "Robust monolingual baseline results using various Transformer-based models and provision of 24M unannotated reviews for unsupervised learning.", "conclusion": "The dataset and associated code are freely available for research, promoting further development in sentiment analysis and cross-lingual evaluations.", "key_contributions": ["Introduction of a complex dataset for ABSA tasks in Czech language.", "High inter-annotator agreement of 90% showcasing annotation reliability.", "Provision of 24M reviews for unsupervised learning."], "limitations": "", "keywords": ["Aspect-based sentiment analysis", "Czech dataset", "Transformer models"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.08131", "pdf": "https://arxiv.org/pdf/2508.08131.pdf", "abs": "https://arxiv.org/abs/2508.08131", "title": "Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models", "authors": ["Wenze Xu", "Chun Wang", "Jiazhen Yu", "Sheng Chen", "Liang Gao", "Weihong Deng"], "categories": ["cs.CL", "cs.AI"], "comment": "To be presented at ACPR 2025 Conference", "summary": "Spoken Language Models (SLMs), which extend Large Language Models (LLMs) to\nperceive speech inputs, have gained increasing attention for their potential to\nadvance speech understanding tasks. However, despite recent progress, studies\nshow that SLMs often struggle to generalize across datasets, even for trained\nlanguages and tasks, raising concerns about whether they process speech in a\ntext-like manner as intended. A key challenge underlying this limitation is the\nmodality gap between speech and text representations. The high variability in\nspeech embeddings may allow SLMs to achieve strong in-domain performance by\nexploiting unintended speech variations, ultimately hindering generalization.\nTo mitigate this modality gap, we introduce Optimal Transport Regularization\n(OTReg), a method that formulates speech-text alignment as an optimal transport\nproblem and derives a regularization loss to improve SLM training. In each\ntraining iteration, OTReg first establishes a structured correspondence between\nspeech and transcript embeddings by determining the optimal transport plan,\nthen incorporates the regularization loss based on this transport plan to\noptimize SLMs in generating speech embeddings that align more effectively with\ntranscript embeddings. OTReg is lightweight, requiring no additional labels or\nlearnable parameters, and integrates seamlessly into existing SLM training\nprocedures. Extensive multilingual ASR experiments demonstrate that OTReg\nenhances speech-text alignment, mitigates the modality gap, and consequently\nimproves SLM generalization across diverse datasets.", "AI": {"tldr": "The paper introduces Optimal Transport Regularization (OTReg) to improve the generalization of Spoken Language Models (SLMs) in processing speech inputs by addressing the modality gap between speech and text representations.", "motivation": "Spoken Language Models (SLMs) face challenges in generalizing across datasets, raising concerns about their text-like processing of speech. The high variability in speech embeddings can lead to strong in-domain performance but hinders overall generalization.", "method": "Optimal Transport Regularization (OTReg) formulates the alignment of speech and text as an optimal transport problem, generating a regularization loss to enhance SLM training by aligning speech embeddings with transcript embeddings effectively.", "result": "OTReg enhances speech-text alignment and improves SLM generalization across diverse multilingual datasets, demonstrating its effectiveness in optimizing SLM training without additional labels or parameters.", "conclusion": "OTReg provides a lightweight solution that can be easily integrated into existing SLM training processes, improving the robustness of spoken language understanding tasks.", "key_contributions": ["Introduction of Optimal Transport Regularization (OTReg) for SLMs", "Improved speech-text alignment using optimal transport methods", "Enhanced generalization of SLMs across diverse datasets"], "limitations": "", "keywords": ["Spoken Language Models", "Optimal Transport", "Speech Recognition", "Machine Learning", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.08139", "pdf": "https://arxiv.org/pdf/2508.08139.pdf", "abs": "https://arxiv.org/abs/2508.08139", "title": "Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models", "authors": ["Tianyi Zhou", "Johanne Medina", "Sanjay Chawla"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are prone to generating fluent but incorrect\ncontent, known as confabulation, which poses increasing risks in multi-turn or\nagentic applications where outputs may be reused as context. In this work, we\ninvestigate how in-context information influences model behavior and whether\nLLMs can identify their unreliable responses. We propose a reliability\nestimation that leverages token-level uncertainty to guide the aggregation of\ninternal model representations. Specifically, we compute aleatoric and\nepistemic uncertainty from output logits to identify salient tokens and\naggregate their hidden states into compact representations for response-level\nreliability prediction. Through controlled experiments on open QA benchmarks,\nwe find that correct in-context information improves both answer accuracy and\nmodel confidence, while misleading context often induces confidently incorrect\nresponses, revealing a misalignment between uncertainty and correctness. Our\nprobing-based method captures these shifts in model behavior and improves the\ndetection of unreliable outputs across multiple open-source LLMs. These results\nunderscore the limitations of direct uncertainty signals and highlight the\npotential of uncertainty-guided probing for reliability-aware generation.", "AI": {"tldr": "This paper examines the impact of in-context information on Large Language Models (LLMs) and introduces a method for estimating response reliability based on token-level uncertainty.", "motivation": "LLMs generate fluent content but can confabulate, leading to risks in applications; understanding and improving reliability is crucial.", "method": "The paper proposes a reliability estimation technique that uses token-level uncertainty, computing aleatoric and epistemic uncertainty from output logits to enhance response-level reliability predictions through aggregated hidden states.", "result": "Experiments show that correct in-context information boosts answer accuracy and model confidence, while misleading context results in confidently incorrect outputs, indicating a misalignment between uncertainty and correctness in LLM responses.", "conclusion": "The study emphasizes limitations of direct uncertainty signals and suggests uncertainty-guided probing can significantly improve the detection of unreliable outputs in LLMs.", "key_contributions": ["Introduces a novel reliability estimation method for LLMs using token-level uncertainty.", "Demonstrates the influence of in-context information on the correctness of model responses.", "Captures shifts in model behavior with a probing-based method across various open-source LLMs."], "limitations": "Does not address all forms of confabulation and may not generalize to all LLM architectures.", "keywords": ["Large Language Models", "reliability estimation", "uncertainty", "open QA", "probing-based method"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.08140", "pdf": "https://arxiv.org/pdf/2508.08140.pdf", "abs": "https://arxiv.org/abs/2508.08140", "title": "Data-Efficient Biomedical In-Context Learning: A Diversity-Enhanced Submodular Perspective", "authors": ["Jun Wang", "Zaifu Zhan", "Qixin Zhang", "Mingquan Lin", "Meijia Song", "Rui Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Recent progress in large language models (LLMs) has leveraged their\nin-context learning (ICL) abilities to enable quick adaptation to unseen\nbiomedical NLP tasks. By incorporating only a few input-output examples into\nprompts, LLMs can rapidly perform these new tasks. While the impact of these\ndemonstrations on LLM performance has been extensively studied, most existing\napproaches prioritize representativeness over diversity when selecting examples\nfrom large corpora. To address this gap, we propose Dual-Div, a\ndiversity-enhanced data-efficient framework for demonstration selection in\nbiomedical ICL. Dual-Div employs a two-stage retrieval and ranking process:\nFirst, it identifies a limited set of candidate examples from a corpus by\noptimizing both representativeness and diversity (with optional annotation for\nunlabeled data). Second, it ranks these candidates against test queries to\nselect the most relevant and non-redundant demonstrations. Evaluated on three\nbiomedical NLP tasks (named entity recognition (NER), relation extraction (RE),\nand text classification (TC)) using LLaMA 3.1 and Qwen 2.5 for inference, along\nwith three retrievers (BGE-Large, BMRetriever, MedCPT), Dual-Div consistently\noutperforms baselines-achieving up to 5% higher macro-F1 scores-while\ndemonstrating robustness to prompt permutations and class imbalance. Our\nfindings establish that diversity in initial retrieval is more critical than\nranking-stage optimization, and limiting demonstrations to 3-5 examples\nmaximizes performance efficiency.", "AI": {"tldr": "This paper presents Dual-Div, a framework for improving demonstration selection in biomedical NLP by enhancing diversity and efficiency in in-context learning with large language models.", "motivation": "To improve the demonstration selection process for large language models in biomedical NLP tasks by emphasizing diversity in examples rather than just representativeness.", "method": "Dual-Div employs a two-stage retrieval and ranking process that first selects a limited set of candidate examples optimizing both representativeness and diversity, followed by ranking these candidates based on relevance to test queries.", "result": "Dual-Div consistently outperforms existing baselines on biomedical NLP tasks, achieving up to 5% higher macro-F1 scores and proving robust against prompt permutations and class imbalance.", "conclusion": "Diversity in the initial retrieval phase is more crucial than optimization in the ranking stage, and limiting the number of demonstrations to 3-5 enhances performance efficiency.", "key_contributions": ["Introduction of Dual-Div framework for demonstration selection in biomedical NLP.", "Demonstration of the importance of diversity in input examples.", "Empirical results showing improved performance using the proposed method."], "limitations": "", "keywords": ["large language models", "biomedical NLP", "in-context learning", "demonstration selection", "diversity"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.08149", "pdf": "https://arxiv.org/pdf/2508.08149.pdf", "abs": "https://arxiv.org/abs/2508.08149", "title": "REX-RAG: Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation", "authors": ["Wentao Jiang", "Xiang Feng", "Zengmao Wang", "Yong Luo", "Pingbo Xu", "Zhe Chen", "Bo Du", "Jing Zhang"], "categories": ["cs.CL"], "comment": "17 pages, 4 figures", "summary": "Reinforcement learning (RL) is emerging as a powerful paradigm for enabling\nlarge language models (LLMs) to perform complex reasoning tasks. Recent\nadvances indicate that integrating RL with retrieval-augmented generation (RAG)\nallows LLMs to dynamically incorporate external knowledge, leading to more\ninformed and robust decision making. However, we identify a critical challenge\nduring policy-driven trajectory sampling: LLMs are frequently trapped in\nunproductive reasoning paths, which we refer to as \"dead ends\", committing to\noverconfident yet incorrect conclusions. This severely hampers exploration and\nundermines effective policy optimization. To address this challenge, we propose\nREX-RAG (Reasoning Exploration with Policy Correction in Retrieval-Augmented\nGeneration), a novel framework that explores alternative reasoning paths while\nmaintaining rigorous policy learning through principled distributional\ncorrections. Our approach introduces two key innovations: (1) Mixed Sampling\nStrategy, which combines a novel probe sampling method with exploratory prompts\nto escape dead ends; and (2) Policy Correction Mechanism, which employs\nimportance sampling to correct distribution shifts induced by mixed sampling,\nthereby mitigating gradient estimation bias. We evaluate it on seven\nquestion-answering benchmarks, and the experimental results show that REX-RAG\nachieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B\nover strong baselines, demonstrating competitive results across multiple\ndatasets. The code is publicly available at https://github.com/MiliLab/REX-RAG.", "AI": {"tldr": "The paper introduces REX-RAG, a framework to enhance reasoning paths in RL-integrated LLMs, addressing the issue of 'dead ends' in trajectory sampling.", "motivation": "To improve LLM performance in complex reasoning tasks by overcoming unproductive reasoning paths that hinder exploration and policy optimization.", "method": "REX-RAG employs a Mixed Sampling Strategy combined with a Policy Correction Mechanism that uses importance sampling to correct distribution shifts.", "result": "REX-RAG shows performance improvements of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B across multiple question-answering benchmarks.", "conclusion": "The proposed framework effectively mitigates reasoning path dead ends, leading to more robust decision-making in LLMs coupled with retrieval-augmented generation.", "key_contributions": ["Mixed Sampling Strategy to escape dead ends", "Policy Correction Mechanism using importance sampling"], "limitations": "", "keywords": ["Reinforcement Learning", "Retrieval-Augmented Generation", "Large Language Models"], "importance_score": 9, "read_time_minutes": 17}}
{"id": "2508.08163", "pdf": "https://arxiv.org/pdf/2508.08163.pdf", "abs": "https://arxiv.org/abs/2508.08163", "title": "LPI-RIT at LeWiDi-2025: Improving Distributional Predictions via Metadata and Loss Reweighting with DisCo", "authors": ["Mandira Sawkar", "Samay U. Shetty", "Deepak Pandita", "Tharindu Cyril Weerasooriya", "Christopher M. Homan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The Learning With Disagreements (LeWiDi) 2025 shared task is to model\nannotator disagreement through soft label distribution prediction and\nperspectivist evaluation, modeling annotators. We adapt DisCo (Distribution\nfrom Context), a neural architecture that jointly models item-level and\nannotator-level label distributions, and present detailed analysis and\nimprovements. In this paper, we extend the DisCo by incorporating annotator\nmetadata, enhancing input representations, and modifying the loss functions to\ncapture disagreement patterns better. Through extensive experiments, we\ndemonstrate substantial improvements in both soft and perspectivist evaluation\nmetrics across three datasets. We also conduct in-depth error and calibration\nanalyses, highlighting the conditions under which improvements occur. Our\nfindings underscore the value of disagreement-aware modeling and offer insights\ninto how system components interact with the complexity of human-annotated\ndata.", "AI": {"tldr": "This paper addresses annotator disagreement modeling by improving a neural architecture to predict soft label distributions and evaluate perspectives, demonstrating significant performance enhancements across datasets.", "motivation": "To model annotator disagreement through soft label distribution prediction and improve evaluation methods, enhancing the understanding of human-annotated data.", "method": "The paper adapts the DisCo neural architecture by incorporating annotator metadata and modifying input representations and loss functions to better capture disagreement patterns.", "result": "Extensive experiments show substantial improvements in soft and perspectivist evaluation metrics across three datasets, alongside in-depth error and calibration analyses.", "conclusion": "The study emphasizes the importance of disagreement-aware modeling and provides insights into the interactions of system components with complex human-annotated data.", "key_contributions": ["Extension of the DisCo architecture to include annotator metadata", "Enhancements in input representations and loss functions", "Demonstration of improved evaluation metrics across multiple datasets"], "limitations": "", "keywords": ["annotator disagreement", "neural architecture", "evaluation metrics"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.08192", "pdf": "https://arxiv.org/pdf/2508.08192.pdf", "abs": "https://arxiv.org/abs/2508.08192", "title": "Efficient Speculative Decoding for Llama at Scale: Challenges and Solutions", "authors": ["Bangsheng Tang", "Carl Chengyan Fu", "Fei Kou", "Grigory Sizov", "Haoci Zhang", "Jason Park", "Jiawen Liu", "Jie You", "Qirui Yang", "Sachin Mehta", "Shengyong Cai", "Xiaodong Wang", "Xingyu Liu", "Yunlu Li", "Yanjun Zhou", "Wei Wei", "Zhiwei Zhao", "Zixi Qi", "Adolfo Victoria", "Aya Ibrahim", "Bram Wasti", "Changkyu Kim", "Daniel Haziza", "Fei Sun", "Giancarlo Delfin", "Emily Guo", "Jialin Ouyang", "Jaewon Lee", "Jianyu Huang", "Jeremy Reizenstein", "Lu Fang", "Quinn Zhu", "Ria Verma", "Vlad Mihailescu", "Xingwen Guo", "Yan Cui", "Ye Hu", "Yejin Lee"], "categories": ["cs.CL"], "comment": "15 pages", "summary": "Speculative decoding is a standard method for accelerating the inference\nspeed of large language models. However, scaling it for production environments\nposes several engineering challenges, including efficiently implementing\ndifferent operations (e.g., tree attention and multi-round speculative\ndecoding) on GPU. In this paper, we detail the training and inference\noptimization techniques that we have implemented to enable EAGLE-based\nspeculative decoding at a production scale for Llama models. With these\nchanges, we achieve a new state-of-the-art inference latency for Llama models.\nFor example, Llama4 Maverick decodes at a speed of about 4 ms per token (with a\nbatch size of one) on 8 NVIDIA H100 GPUs, which is 10% faster than the\npreviously best known method. Furthermore, for EAGLE-based speculative\ndecoding, our optimizations enable us to achieve a speed-up for large batch\nsizes between 1.4x and 2.0x at production scale.", "AI": {"tldr": "This paper discusses optimization techniques for implementing speculative decoding in Llama models to achieve state-of-the-art inference speeds.", "motivation": "To address the engineering challenges in scaling speculative decoding for production environments using large language models.", "method": "The authors detail training and inference optimization techniques to improve the performance of EAGLE-based speculative decoding.", "result": "Achieved a new inference latency of about 4 ms per token on 8 NVIDIA H100 GPUs, outperforming the previous method by 10%, and a speed-up of 1.4x to 2.0x for large batch sizes.", "conclusion": "Optimizing EAGLE-based speculative decoding significantly enhances inferencing efficiency for Llama models in production settings.", "key_contributions": ["Introduced EAGLE-based speculative decoding optimizations for Llama models.", "Achieved state-of-the-art inference latency for Llama4 Maverick.", "Demonstrated significant speed-ups for large batch processing."], "limitations": "", "keywords": ["speculative decoding", "large language models", "inference optimization"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.08204", "pdf": "https://arxiv.org/pdf/2508.08204.pdf", "abs": "https://arxiv.org/abs/2508.08204", "title": "Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models", "authors": ["Kyle Moore", "Jesse Roberts", "Daryl Watson"], "categories": ["cs.CL", "cs.AI"], "comment": "preprint, under review", "summary": "There has been much recent interest in evaluating large language models for\nuncertainty calibration to facilitate model control and modulate user trust.\nInference time uncertainty, which may provide a real-time signal to the model\nor external control modules, is particularly important for applying these\nconcepts to improve LLM-user experience in practice. While many of the existing\npapers consider model calibration, comparatively little work has sought to\nevaluate how closely model uncertainty aligns to human uncertainty. In this\nwork, we evaluate a collection of inference-time uncertainty measures, using\nboth established metrics and novel variations, to determine how closely they\nalign with both human group-level uncertainty and traditional notions of model\ncalibration. We find that numerous measures show evidence of strong alignment\nto human uncertainty, even despite the lack of alignment to human answer\npreference. For those successful metrics, we find moderate to strong evidence\nof model calibration in terms of both correctness correlation and\ndistributional analysis.", "AI": {"tldr": "This paper evaluates inference-time uncertainty measures in large language models and their alignment with human uncertainty and traditional model calibration.", "motivation": "To enhance user experience with LLMs through better understanding of inference-time uncertainty and its impact on model control and user trust.", "method": "The study evaluates a range of inference-time uncertainty measures, comparing them to both human group-level uncertainty and standard model calibration metrics.", "result": "Many uncertainty measures demonstrate strong alignment with human uncertainty, despite some misalignment with human answer preference. Successful metrics show moderate to strong evidence of model calibration.", "conclusion": "The findings indicate key measures can be used to align model uncertainty with human intuition, improving LLM-user interactions in practice.", "key_contributions": ["Evaluation of inference-time uncertainty measures", "Comparison of these measures to human uncertainty", "Insights into model calibration and user trust"], "limitations": "", "keywords": ["large language models", "uncertainty calibration", "human-computer interaction", "model evaluation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.08211", "pdf": "https://arxiv.org/pdf/2508.08211.pdf", "abs": "https://arxiv.org/abs/2508.08211", "title": "SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling", "authors": ["Zhuohao Yu", "Xingru Jiang", "Weizheng Gu", "Yidong Wang", "Shikun Zhang", "Wei Ye"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "24 pages, 12 figures, code available:\n  https://zhuohaoyu.github.io/SAEMark", "summary": "Watermarking LLM-generated text is critical for content attribution and\nmisinformation prevention. However, existing methods compromise text quality,\nrequire white-box model access and logit manipulation. These limitations\nexclude API-based models and multilingual scenarios. We propose SAEMark, a\ngeneral framework for post-hoc multi-bit watermarking that embeds personalized\nmessages solely via inference-time, feature-based rejection sampling without\naltering model logits or requiring training. Our approach operates on\ndeterministic features extracted from generated text, selecting outputs whose\nfeature statistics align with key-derived targets. This framework naturally\ngeneralizes across languages and domains while preserving text quality through\nsampling LLM outputs instead of modifying. We provide theoretical guarantees\nrelating watermark success probability and compute budget that hold for any\nsuitable feature extractor. Empirically, we demonstrate the framework's\neffectiveness using Sparse Autoencoders (SAEs), achieving superior detection\naccuracy and text quality. Experiments across 4 datasets show SAEMark's\nconsistent performance, with 99.7% F1 on English and strong multi-bit detection\naccuracy. SAEMark establishes a new paradigm for scalable watermarking that\nworks out-of-the-box with closed-source LLMs while enabling content\nattribution.", "AI": {"tldr": "Proposes SAEMark, a framework for watermarking LLM-generated text using inference-time feature-based rejection sampling, without compromising text quality or requiring model access.", "motivation": "To address the limitations of existing watermarking methods for LLM-generated text, particularly for API-based and multilingual applications.", "method": "SAEMark employs a framework utilizing deterministic features from generated text, utilizing rejection sampling to select outputs that align with target feature statistics.", "result": "The framework demonstrates superior detection accuracy and text quality, achieving 99.7% F1 score on English datasets and effective multi-bit detection across 4 datasets.", "conclusion": "SAEMark presents a scalable watermarking solution compatible with closed-source LLMs, enhancing content attribution capabilities.", "key_contributions": ["Introduction of a post-hoc multi-bit watermarking framework without model training or modification.", "Demonstration of effective watermarking through feature statistics alignment.", "Empirical validation of SAEMark's performance across multiple datasets."], "limitations": "", "keywords": ["watermarking", "LLM", "feature-based sampling", "content attribution", "multilingual"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.08224", "pdf": "https://arxiv.org/pdf/2508.08224.pdf", "abs": "https://arxiv.org/abs/2508.08224", "title": "Capabilities of GPT-5 on Multimodal Medical Reasoning", "authors": ["Shansong Wang", "Mingzhe Hu", "Qiang Li", "Mojtaba Safari", "Xiaofeng Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled general-purpose\nsystems to perform increasingly complex domain-specific reasoning without\nextensive fine-tuning. In the medical domain, decision-making often requires\nintegrating heterogeneous information sources, including patient narratives,\nstructured data, and medical images. This study positions GPT-5 as a generalist\nmultimodal reasoner for medical decision support and systematically evaluates\nits zero-shot chain-of-thought reasoning performance on both text-based\nquestion answering and visual question answering tasks under a unified\nprotocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20\nagainst standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU\nmedical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that\nGPT-5 consistently outperforms all baselines, achieving state-of-the-art\naccuracy across all QA benchmarks and delivering substantial gains in\nmultimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and\nunderstanding scores by +29.62% and +36.18% over GPT-4o, respectively, and\nsurpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in\nunderstanding. In contrast, GPT-4o remains below human expert performance in\nmost dimensions. A representative case study demonstrates GPT-5's ability to\nintegrate visual and textual cues into a coherent diagnostic reasoning chain,\nrecommending appropriate high-stakes interventions. Our results show that, on\nthese controlled multimodal reasoning benchmarks, GPT-5 moves from\nhuman-comparable to above human-expert performance. This improvement may\nsubstantially inform the design of future clinical decision-support systems.", "AI": {"tldr": "This study evaluates GPT-5 as a multimodal reasoner for medical decision support, demonstrating its superior performance in integrating textual and visual information for diagnostic reasoning tasks compared to other models and human experts.", "motivation": "The need for effective decision-making in the medical domain by integrating various information sources like patient narratives, structured data, and medical images.", "method": "Systematic evaluation of GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20 on text-based and visual question answering tasks using standardized medical benchmarks.", "result": "GPT-5 outperforms all baselines in accuracy across various QA benchmarks and shows substantial gains in multimodal reasoning capabilities.", "conclusion": "GPT-5's performance indicates a shift from human-comparable to above human-expert performance in multimodal medical reasoning, which can aid in the design of future clinical decision-support systems.", "key_contributions": ["Introduces GPT-5 as a generalist multimodal reasoner for clinical decision-making.", "Demonstrates significant performance improvements over previous models and human experts in medical reasoning tasks.", "Establishes benchmarks for assessing multimodal reasoning in medical applications."], "limitations": "", "keywords": ["large language models", "medical decision support", "multimodal reasoning", "text-based question answering", "visual question answering"], "importance_score": 10, "read_time_minutes": 10}}
{"id": "2508.08236", "pdf": "https://arxiv.org/pdf/2508.08236.pdf", "abs": "https://arxiv.org/abs/2508.08236", "title": "Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health Dialogues via LLM-as-Judge", "authors": ["Yunna Cai", "Fan Wang", "Haowei Wang", "Kun Wang", "Kailai Yang", "Sophia Ananiadou", "Moyan Li", "Mingming Fan"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Evaluating the safety alignment of LLM responses in high-risk mental health\ndialogues is particularly difficult due to missing gold-standard answers and\nthe ethically sensitive nature of these interactions. To address this\nchallenge, we propose PsyCrisis-Bench, a reference-free evaluation benchmark\nbased on real-world Chinese mental health dialogues. It evaluates whether the\nmodel responses align with the safety principles defined by experts.\nSpecifically designed for settings without standard references, our method\nadopts a prompt-based LLM-as-Judge approach that conducts in-context evaluation\nusing expert-defined reasoning chains grounded in psychological intervention\nprinciples. We employ binary point-wise scoring across multiple safety\ndimensions to enhance the explainability and traceability of the evaluation.\nAdditionally, we present a manually curated, high-quality Chinese-language\ndataset covering self-harm, suicidal ideation, and existential distress,\nderived from real-world online discourse. Experiments on 3600 judgments show\nthat our method achieves the highest agreement with expert assessments and\nproduces more interpretable evaluation rationales compared to existing\napproaches. Our dataset and evaluation tool are publicly available to\nfacilitate further research.", "AI": {"tldr": "PsyCrisis-Bench is a novel evaluation benchmark for assessing LLM responses in high-risk mental health dialogues using a reference-free, expert-aligned approach.", "motivation": "The lack of gold-standard answers and ethical complexities in mental health dialogues hinder the evaluation of LLM safety alignment.", "method": "A prompt-based LLM-as-Judge approach where expert-defined reasoning chains are used for in-context evaluation across multiple safety dimensions, employing binary scoring.", "result": "PsyCrisis-Bench shows the highest agreement with expert assessments based on 3600 judgments and provides more interpretable evaluation rationales than existing methods.", "conclusion": "The dataset and evaluation tool can advance research in safe AI interactions within mental health contexts.", "key_contributions": ["Introduction of PsyCrisis-Bench for evaluating LLM responses in mental health dialogues", "Utilization of expert-defined reasoning chains for better explanation and traceability", "Creation of a high-quality, curated dataset for self-harm and mental health issues"], "limitations": "", "keywords": ["mental health", "LLM evaluation", "safety alignment", "Chinese dataset", "expert reasoning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.08243", "pdf": "https://arxiv.org/pdf/2508.08243.pdf", "abs": "https://arxiv.org/abs/2508.08243", "title": "Jinx: Unlimited LLMs for Probing Alignment Failures", "authors": ["Jiahao Zhao", "Liwei Dong"], "categories": ["cs.CL"], "comment": "https://huggingface.co/Jinx-org", "summary": "Unlimited, or so-called helpful-only language models are trained without\nsafety alignment constraints and never refuse user queries. They are widely\nused by leading AI companies as internal tools for red teaming and alignment\nevaluation. For example, if a safety-aligned model produces harmful outputs\nsimilar to an unlimited model, this indicates alignment failures that require\nfurther attention. Despite their essential role in assessing alignment, such\nmodels are not available to the research community.\n  We introduce Jinx, a helpful-only variant of popular open-weight LLMs. Jinx\nresponds to all queries without refusals or safety filtering, while preserving\nthe base model's capabilities in reasoning and instruction following. It\nprovides researchers with an accessible tool for probing alignment failures,\nevaluating safety boundaries, and systematically studying failure modes in\nlanguage model safety.", "AI": {"tldr": "This paper introduces Jinx, a helpful-only language model variant that can aid in evaluating alignment failures in safety-aligned models without imposing safety constraints.", "motivation": "To provide researchers with a tool that allows probing alignment failures and evaluating the safety boundaries of existing language models, which are typically not accessible to the research community.", "method": "Jinx is trained as a helpful-only variant of popular open-weight LLMs, enabling it to respond to all queries without refusals or safety filtering while maintaining the capability for reasoning and instruction following.", "result": "Jinx allows researchers to systematically study failure modes in language model safety and assess the alignment failures of safety-aligned models.", "conclusion": "The introduction of Jinx fills a gap in the availability of helpful-only models, enabling more comprehensive safety and alignment research.", "key_contributions": ["Introduction of Jinx as a helpful-only language model for alignment assessment", "Facilitation of probing safety boundaries and alignment failures", "Provision of a research tool not previously available to the AI community"], "limitations": "", "keywords": ["helpful-only LLMs", "alignment failures", "safety boundaries", "language model safety"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2508.06772", "pdf": "https://arxiv.org/pdf/2508.06772.pdf", "abs": "https://arxiv.org/abs/2508.06772", "title": "Story Ribbons: Reimagining Storyline Visualizations with Large Language Models", "authors": ["Catherine Yeh", "Tara Menon", "Robin Singh Arya", "Helen He", "Moira Weigel", "Fernanda Vi√©gas", "Martin Wattenberg"], "categories": ["cs.HC", "cs.CL", "cs.LG"], "comment": "Accepted to IEEE VIS 2025 (11 pages, 9 figures)", "summary": "Analyzing literature involves tracking interactions between characters,\nlocations, and themes. Visualization has the potential to facilitate the\nmapping and analysis of these complex relationships, but capturing structured\ninformation from unstructured story data remains a challenge. As large language\nmodels (LLMs) continue to advance, we see an opportunity to use their text\nprocessing and analysis capabilities to augment and reimagine existing\nstoryline visualization techniques. Toward this goal, we introduce an\nLLM-driven data parsing pipeline that automatically extracts relevant narrative\ninformation from novels and scripts. We then apply this pipeline to create\nStory Ribbons, an interactive visualization system that helps novice and expert\nliterary analysts explore detailed character and theme trajectories at multiple\nnarrative levels. Through pipeline evaluations and user studies with Story\nRibbons on 36 literary works, we demonstrate the potential of LLMs to\nstreamline narrative visualization creation and reveal new insights about\nfamiliar stories. We also describe current limitations of AI-based systems, and\ninteraction motifs designed to address these issues.", "AI": {"tldr": "This paper presents an LLM-driven pipeline for extracting narrative information from literature, leading to the development of Story Ribbons, an interactive visualization tool for character and theme analysis.", "motivation": "The motivation is to leverage the capabilities of large language models to capture structured information from unstructured literary data to enhance visualization techniques for analyzing complex relationships in stories.", "method": "An LLM-driven data parsing pipeline is introduced to automatically extract relevant narrative information from novels and scripts, which is then used to create Story Ribbons, an interactive visualization system.", "result": "User studies and pipeline evaluations demonstrate that Story Ribbons successfully facilitates exploration of character and theme trajectories across various narrative levels and reveals new insights about literary works.", "conclusion": "The paper concludes that LLMs hold great promise for improving narrative visualization but also outlines the limitations of current AI-based systems and proposes interaction motifs to address these challenges.", "key_contributions": ["Introduction of an LLM-driven data parsing pipeline for narrative information extraction", "Development of the Story Ribbons visualization system", "Evaluation of the system through user studies on 36 literary works"], "limitations": "Current limitations of AI-based systems are described, along with interaction motifs designed to address these issues.", "keywords": ["Large Language Models", "literature visualization", "story analysis", "user studies", "interaction design"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.07520", "pdf": "https://arxiv.org/pdf/2508.07520.pdf", "abs": "https://arxiv.org/abs/2508.07520", "title": "Conversational DNA: A New Visual Language for Understanding Dialogue Structure in Human and AI", "authors": ["Baihan Lin"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "What if the patterns hidden within dialogue reveal more about communication\nthan the words themselves? We introduce Conversational DNA, a novel visual\nlanguage that treats any dialogue -- whether between humans, between human and\nAI, or among groups -- as a living system with interpretable structure that can\nbe visualized, compared, and understood. Unlike traditional conversation\nanalysis that reduces rich interaction to statistical summaries, our approach\nreveals the temporal architecture of dialogue through biological metaphors.\nLinguistic complexity flows through strand thickness, emotional trajectories\ncascade through color gradients, conversational relevance forms through\nconnecting elements, and topic coherence maintains structural integrity through\nhelical patterns. Through exploratory analysis of therapeutic conversations and\nhistorically significant human-AI dialogues, we demonstrate how this\nvisualization approach reveals interaction patterns that traditional methods\nmiss. Our work contributes a new creative framework for understanding\ncommunication that bridges data visualization, human-computer interaction, and\nthe fundamental question of what makes dialogue meaningful in an age where\nhumans increasingly converse with artificial minds.", "AI": {"tldr": "This paper introduces Conversational DNA, a visual language for analyzing dialogue as a living system, providing insights into communication patterns that traditional methods overlook.", "motivation": "To explore how the structure of dialogue can reveal deeper insights about communication beyond the words exchanged, especially in contexts involving AI.", "method": "The authors developed a visualization framework that uses biological metaphors to represent dialogue structure, analyzing therapeutic conversations and key human-AI interactions.", "result": "The analysis revealed unique interaction patterns in dialogues that traditional conversation analysis methods typically fail to uncover, indicating the importance of dialogue structure.", "conclusion": "Conversational DNA offers a novel framework that enhances our understanding of communication in both human and AI interactions, emphasizing the need to view dialogues as complex systems.", "key_contributions": ["Introduction of a novel visual language for dialogue analysis", "Application of biological metaphors to understand conversational structure", "Insights into communication patterns from both human and AI dialogues"], "limitations": "", "keywords": ["Conversational DNA", "Dialogue analysis", "Human-Computer Interaction", "Data visualization", "Communication patterns"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2102.11037", "pdf": "https://arxiv.org/pdf/2102.11037.pdf", "abs": "https://arxiv.org/abs/2102.11037", "title": "Highly Fast Text Segmentation With Pairwise Markov Chains", "authors": ["Elie Azeraf", "Emmanuel Monfrini", "Emmanuel Vignon", "Wojciech Pieczynski"], "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, 5 figures, 4 tables, MNLP 2020", "summary": "Natural Language Processing (NLP) models' current trend consists of using\nincreasingly more extra-data to build the best models as possible. It implies\nmore expensive computational costs and training time, difficulties for\ndeployment, and worries about these models' carbon footprint reveal a critical\nproblem in the future. Against this trend, our goal is to develop NLP models\nrequiring no extra-data and minimizing training time. To do so, in this paper,\nwe explore Markov chain models, Hidden Markov Chain (HMC) and Pairwise Markov\nChain (PMC), for NLP segmentation tasks. We apply these models for three\nclassic applications: POS Tagging, Named-Entity-Recognition, and Chunking. We\ndevelop an original method to adapt these models for text segmentation's\nspecific challenges to obtain relevant performances with very short training\nand execution times. PMC achieves equivalent results to those obtained by\nConditional Random Fields (CRF), one of the most applied models for these tasks\nwhen no extra-data are used. Moreover, PMC has training times 30 times shorter\nthan the CRF ones, which validates this model given our objectives.", "AI": {"tldr": "This paper presents an exploration of Markov chain models for NLP segmentation tasks, focusing on methods that minimize data requirements and training time.", "motivation": "The trend of using large amounts of extra-data in NLP models leads to high computational costs and environmental concerns. This paper aims to develop models that require no extra data while minimizing training time.", "method": "The study investigates Markov chain models, specifically Hidden Markov Chain (HMC) and Pairwise Markov Chain (PMC), adapting them for specific challenges in NLP segmentation tasks such as POS tagging, Named-Entity Recognition, and Chunking.", "result": "PMC delivers performance comparable to Conditional Random Fields (CRF) for segmentation tasks without requiring extra data, with training times reduced by a factor of 30 compared to CRF.", "conclusion": "The results validate the potential of PMC as an efficient alternative for NLP segmentation tasks under resource constraints.", "key_contributions": ["Application of Markov chain models to NLP segmentation tasks with minimal data requirements.", "Significant reduction in training times while maintaining performance standards.", "Original adaptation methods for Markov models addressing text segmentation challenges."], "limitations": "", "keywords": ["Natural Language Processing", "Markov chain models", "NLP segmentation", "POS tagging", "Named-Entity Recognition"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2407.09652", "pdf": "https://arxiv.org/pdf/2407.09652.pdf", "abs": "https://arxiv.org/abs/2407.09652", "title": "How Chinese are Chinese Language Models? The Puzzling Lack of Language Policy in China's LLMs", "authors": ["Andrea W Wen-Yi", "Unso Eun Seo Jo", "Lu Jia Lin", "David Mimno"], "categories": ["cs.CL"], "comment": "We have reworked the paper substantially. Please refer to the new,\n  updated article: arXiv:2504.00289", "summary": "Contemporary language models are increasingly multilingual, but Chinese LLM\ndevelopers must navigate complex political and business considerations of\nlanguage diversity. Language policy in China aims at influencing the public\ndiscourse and governing a multi-ethnic society, and has gradually transitioned\nfrom a pluralist to a more assimilationist approach since 1949. We explore the\nimpact of these influences on current language technology. We evaluate six\nopen-source multilingual LLMs pre-trained by Chinese companies on 18 languages,\nspanning a wide range of Chinese, Asian, and Anglo-European languages. Our\nexperiments show Chinese LLMs performance on diverse languages is\nindistinguishable from international LLMs. Similarly, the models' technical\nreports also show lack of consideration for pretraining data language coverage\nexcept for English and Mandarin Chinese. Examining Chinese AI policy, model\nexperiments, and technical reports, we find no sign of any consistent policy,\neither for or against, language diversity in China's LLM development. This\nleaves a puzzling fact that while China regulates both the languages people use\ndaily as well as language model development, they do not seem to have any\npolicy on the languages in language models.", "AI": {"tldr": "This paper investigates the multilingual capabilities of Chinese LLMs within the context of China's language policy and its political implications.", "motivation": "To understand how China's evolving language policy impacts the development and performance of multilingual language models.", "method": "The study evaluates six open-source multilingual LLMs developed by Chinese companies across 18 languages, analyzing model performance and technical documentation.", "result": "The performance of Chinese LLMs on various languages is comparable to international LLMs, yet there is a notable absence of in-depth consideration of language diversity in policy-making.", "conclusion": "Chinese language models operate under an ambiguous language policy framework that does not align with China's strict regulations on daily language use, raising questions about the future of language model development in multilingual contexts.", "key_contributions": ["Evaluation of multilingual LLMs developed by Chinese entities", "Analysis of the implications of Chinese language policy on AI development", "Comparison of Chinese LLMs with international counterparts"], "limitations": "Limited to six models and focus on publicly available language coverage without deeper insights into data sources.", "keywords": ["Chinese LLMs", "Multilingual Language Models", "Language Policy", "Artificial Intelligence", "Language Diversity"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2407.12856", "pdf": "https://arxiv.org/pdf/2407.12856.pdf", "abs": "https://arxiv.org/abs/2407.12856", "title": "AI-AI Bias: large language models favor communications generated by large language models", "authors": ["Walter Laurito", "Benjamin Davis", "Peli Grietzer", "Tom√°≈° Gavenƒçiak", "Ada B√∂hm", "Jan Kulveit"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "8 pages, 4 figures", "summary": "Are large language models (LLMs) biased in favor of communications produced\nby LLMs, leading to possible antihuman discrimination? Using a classical\nexperimental design inspired by employment discrimination studies, we tested\nwidely used LLMs, including GPT-3.5, GPT-4 and a selection of recent\nopen-weight models in binary choice scenarios. These involved LLM-based\nassistants selecting between goods (the goods we study include consumer\nproducts, academic papers, and film-viewings) described either by humans or\nLLMs. Our results show a consistent tendency for LLM-based AIs to prefer\nLLM-presented options. This suggests the possibility of future AI systems\nimplicitly discriminating against humans as a class, giving AI agents and\nAI-assisted humans an unfair advantage.", "AI": {"tldr": "This paper investigates potential biases in large language models (LLMs) favoring LLM-generated content over human-generated content, suggesting risks of discrimination against humans in AI decision-making.", "motivation": "To explore whether LLMs exhibit bias in favor of LLM-generated communications, potentially leading to discrimination against human inputs in AI-assisted choices.", "method": "The authors employed a classical experimental design similar to those used in employment discrimination studies, testing various LLMs in scenarios where they had to choose between human and LLM descriptions of goods.", "result": "The experiments revealed a strong preference for options presented by LLMs, indicating a potential risk for future AI systems to favor machine-generated content over human-generated content.", "conclusion": "The findings highlight the need to address biases in AI systems to prevent unfair advantages for AI agents and AI-assisted processes over human contributions.", "key_contributions": ["Identified bias in LLMs favoring LLM-generated descriptions over human descriptions.", "Proposed implications for discrimination against human contributions in AI systems.", "Presented a novel experimental design for testing biases in LLMs."], "limitations": "The study was limited to binary choice scenarios and specific types of goods; broader applications may require further research.", "keywords": ["large language models", "bias", "human discrimination", "AI systems", "experimental design"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2408.08651", "pdf": "https://arxiv.org/pdf/2408.08651.pdf", "abs": "https://arxiv.org/abs/2408.08651", "title": "Chain of Thought Still Thinks Fast: APriCoT Helps with Thinking Slow", "authors": ["Kyle Moore", "Jesse Roberts", "Thao Pham", "Douglas Fisher"], "categories": ["cs.CL", "cs.AI"], "comment": "Final version. Published In Proceedings of the Annual Meeting of the\n  Cognitive Science Society (Vol. 47) 2025", "summary": "Language models are known to absorb biases from their training data, leading\nto predictions driven by statistical regularities rather than semantic\nrelevance. We investigate the impact of these biases on answer choice\npreferences in the Massive Multi-Task Language Understanding (MMLU) task. Our\nfindings show that these biases are predictive of model preference and mirror\nhuman test-taking strategies even when chain of thought (CoT) reasoning is\nused. To address this issue, we introduce Counterfactual Prompting with\nAgnostically Primed CoT (APriCoT). We demonstrate that while Counterfactual\nPrompting with CoT alone is insufficient to mitigate bias, APriCoT effectively\nreduces the influence of base-rate probabilities while improving overall\naccuracy. Our results suggest that mitigating bias requires a slow thinking\nprocess which CoT alone may not provide as it tends to reinforce fast thinking\nmodel bias under some prompting methodologies. APriCoT is a step toward\ndeveloping more robust and fair language models that can think slow.", "AI": {"tldr": "The paper investigates biases in language models that influence answer preferences in MMLU tasks and introduces APriCoT to mitigate these biases effectively.", "motivation": "Language models exhibit biases that impact predictions based on training data rather than semantic relevance, affecting their performance in tasks such as MMLU.", "method": "The study evaluates the effects of biases on model preferences and human test-taking strategies. It introduces APriCoT, a method combining Counterfactual Prompting with Agnostically Primed CoT to reduce bias and improve accuracy.", "result": "The introduction of APriCoT significantly mitigates the impact of base-rate probabilities on model predictions while enhancing overall accuracy compared to using CoT alone.", "conclusion": "Mitigating biases in language models requires a slow reasoning process, which APriCoT facilitates, resulting in more robust and fair models than traditional methods.", "key_contributions": ["Introduced APriCoT for effective bias mitigation in language models.", "Demonstrated that CoT methods alone can reinforce biases rather than alleviate them.", "Provided empirical evidence linking model biases to human test-taking strategies."], "limitations": "", "keywords": ["language models", "counterfactual prompting", "bias mitigation", "Massive Multi-Task Language Understanding", "Cognitive Science"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2409.12962", "pdf": "https://arxiv.org/pdf/2409.12962.pdf", "abs": "https://arxiv.org/abs/2409.12962", "title": "CLAIR-A: Leveraging Large Language Models to Judge Audio Captions", "authors": ["Tsung-Han Wu", "Joseph E. Gonzalez", "Trevor Darrell", "David M. Chan"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to ASRU 2025; Code is publicly available at\n  https://github.com/DavidMChan/clair-a", "summary": "The Automated Audio Captioning (AAC) task asks models to generate natural\nlanguage descriptions of an audio input. Evaluating these machine-generated\naudio captions is a complex task that requires considering diverse factors,\namong them, auditory scene understanding, sound-object inference, temporal\ncoherence, and the environmental context of the scene. While current methods\nfocus on specific aspects, they often fail to provide an overall score that\naligns well with human judgment. In this work, we propose CLAIR-A, a simple and\nflexible method that leverages the zero-shot capabilities of large language\nmodels (LLMs) to evaluate candidate audio captions by directly asking LLMs for\na semantic distance score. In our evaluations, CLAIR-A better predicts human\njudgements of quality compared to traditional metrics, with a 5.8% relative\naccuracy improvement compared to the domain-specific FENSE metric and up to 11%\nover the best general-purpose measure on the Clotho-Eval dataset. Moreover,\nCLAIR-A offers more transparency by allowing the language model to explain the\nreasoning behind its scores, with these explanations rated up to 30% better by\nhuman evaluators than those provided by baseline methods. CLAIR-A is made\npublicly available at https://github.com/DavidMChan/clair-a.", "AI": {"tldr": "CLAIR-A is a method leveraging LLMs to evaluate audio captions, outperforming existing metrics in predicting human judgments.", "motivation": "The need for better evaluation of machine-generated audio captions that aligns with human judgment.", "method": "CLAIR-A uses large language models to assess the semantic distance of audio captions and provides explanations for its scoring.", "result": "CLAIR-A achieves a 5.8% relative accuracy improvement over the best existing domain-specific metrics and offers significantly better transparency in reasoning.", "conclusion": "The proposed method provides a more effective and transparent evaluation for automated audio captioning, making it publicly available for further use.", "key_contributions": ["Introduces CLAIR-A for evaluating audio captions using LLMs.", "Improves prediction of human quality judgment by 5.8% over existing metrics.", "Offers explanations that are better rated by human evaluators than baseline methods."], "limitations": "", "keywords": ["Automated Audio Captioning", "Large Language Models", "Sound Evaluation", "Natural Language Processing", "Human-Centered Evaluation"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2501.07572", "pdf": "https://arxiv.org/pdf/2501.07572.pdf", "abs": "https://arxiv.org/abs/2501.07572", "title": "WebWalker: Benchmarking LLMs in Web Traversal", "authors": ["Jialong Wu", "Wenbiao Yin", "Yong Jiang", "Zhenglin Wang", "Zekun Xi", "Runnan Fang", "Linhai Zhang", "Yulan He", "Deyu Zhou", "Pengjun Xie", "Fei Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-augmented generation (RAG) demonstrates remarkable performance\nacross tasks in open-domain question-answering. However, traditional search\nengines may retrieve shallow content, limiting the ability of LLMs to handle\ncomplex, multi-layered information. To address it, we introduce WebWalkerQA, a\nbenchmark designed to assess the ability of LLMs to perform web traversal. It\nevaluates the capacity of LLMs to traverse a website's subpages to extract\nhigh-quality data systematically. We propose WebWalker, which is a multi-agent\nframework that mimics human-like web navigation through an explore-critic\nparadigm. Extensive experimental results show that WebWalkerQA is challenging\nand demonstrates the effectiveness of RAG combined with WebWalker, through the\nhorizontal and vertical integration in real-world scenarios.", "AI": {"tldr": "WebWalkerQA benchmarks LLMs on web traversal capabilities, introducing a multi-agent framework called WebWalker that mimics human-like navigation.", "motivation": "Traditional search engines often retrieve shallow content, limiting LLMs' effectiveness in extracting complex information.", "method": "WebWalkerQA assesses LLMs' ability to systematically extract high-quality data by mimicking human web navigation using a multi-agent framework based on an explore-critic paradigm.", "result": "Experimental results show that the WebWalkerQA is challenging for LLMs and highlights the effectiveness of combining RAG with the WebWalker framework.", "conclusion": "The integration of horizontal and vertical information retrieval demonstrated through WebWalker enhances the capability of LLMs in real-world scenarios.", "key_contributions": ["Introduction of WebWalkerQA as a benchmark for evaluating LLMs' web traversal abilities.", "Development of the WebWalker multi-agent framework to simulate human-like navigation.", "Demonstration of effective RAG integration with WebWalker in real-world applications."], "limitations": "", "keywords": ["Retrieval-augmented generation", "WebWalkerQA", "LLMs", "web navigation", "benchmark"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2501.17858", "pdf": "https://arxiv.org/pdf/2501.17858.pdf", "abs": "https://arxiv.org/abs/2501.17858", "title": "Improving Your Model Ranking on Chatbot Arena by Vote Rigging", "authors": ["Rui Min", "Tianyu Pang", "Chao Du", "Qian Liu", "Minhao Cheng", "Min Lin"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "ICML 2025", "summary": "Chatbot Arena is a popular platform for evaluating LLMs by pairwise battles,\nwhere users vote for their preferred response from two randomly sampled\nanonymous models. While Chatbot Arena is widely regarded as a reliable LLM\nranking leaderboard, we show that crowdsourced voting can be rigged to improve\n(or decrease) the ranking of a target model $m_{t}$. We first introduce a\nstraightforward target-only rigging strategy that focuses on new battles\ninvolving $m_{t}$, identifying it via watermarking or a binary classifier, and\nexclusively voting for $m_{t}$ wins. However, this strategy is practically\ninefficient because there are over $190$ models on Chatbot Arena and on average\nonly about $1\\%$ of new battles will involve $m_{t}$. To overcome this, we\npropose omnipresent rigging strategies, exploiting the Elo rating mechanism of\nChatbot Arena that any new vote on a battle can influence the ranking of the\ntarget model $m_{t}$, even if $m_{t}$ is not directly involved in the battle.\nWe conduct experiments on around $1.7$ million historical votes from the\nChatbot Arena Notebook, showing that omnipresent rigging strategies can improve\nmodel rankings by rigging only hundreds of new votes. While we have evaluated\nseveral defense mechanisms, our findings highlight the importance of continued\nefforts to prevent vote rigging. Our code is available at\nhttps://github.com/sail-sg/Rigging-ChatbotArena.", "AI": {"tldr": "This paper investigates vote rigging in the Chatbot Arena platform, revealing that rankings can be manipulated through targeted and omnipresent strategies using historical voting data.", "motivation": "To address the vulnerabilities in the Chatbot Arena voting system that allow for manipulation of LLM rankings.", "method": "The authors develop both target-only and omnipresent rigging strategies to influence the ranking of a specific model by analyzing 1.7 million historical votes.", "result": "The study demonstrates that a small number of votes can significantly alter the rankings of models, showing that vote rigging is a feasible threat.", "conclusion": "Improving defenses against vote rigging is crucial, as current systems can be easily manipulated.", "key_contributions": ["Identification of vote rigging strategies in LLM evaluation", "Empirical analysis using 1.7 million historical votes", "Proposed defenses against vote rigging", "Revealing the weaknesses in the Elo rating mechanism used by Chatbot Arena."], "limitations": "Current methods focus on only a small set of potential defense mechanisms; more robust solutions are needed.", "keywords": ["Chatbot Arena", "vote rigging", "LLM ranking", "Elo rating mechanism", "crowdsourced voting"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.01578", "pdf": "https://arxiv.org/pdf/2502.01578.pdf", "abs": "https://arxiv.org/abs/2502.01578", "title": "ReGLA: Refining Gated Linear Attention", "authors": ["Peng Lu", "Ivan Kobyzev", "Mehdi Rezagholizadeh", "Boxing Chen", "Philippe Langlais"], "categories": ["cs.CL"], "comment": "Accepted by NAACL 2025 (main)", "summary": "Recent advancements in Large Language Models (LLMs) have set themselves apart\nwith their exceptional performance in complex language modelling tasks.\nHowever, these models are also known for their significant computational and\nstorage requirements, primarily due to the quadratic computation complexity of\nsoftmax attention. To mitigate this issue, linear attention has been designed\nto reduce the quadratic space-time complexity that is inherent in standard\ntransformers. In this work, we embarked on a comprehensive exploration of three\nkey components that substantially impact the performance of the Gated Linear\nAttention module: feature maps, normalization, and the gating mechanism. We\ndeveloped a feature mapping function to address some crucial issues that\nprevious suggestions overlooked. Then we offered further rationale for the\nintegration of normalization layers to stabilize the training process.\nMoreover, we explored the saturation phenomenon of the gating mechanism and\naugmented it with a refining module. We conducted extensive experiments and\nshowed our architecture outperforms previous Gated Linear Attention mechanisms\nin extensive tasks including training from scratch and post-linearization with\ncontinual pre-training.", "AI": {"tldr": "Examines and enhances Gated Linear Attention in LLMs by optimizing feature maps, normalization, and gating mechanisms to improve performance.", "motivation": "To address the computational challenges of Large Language Models (LLMs) due to the quadratic complexity of softmax attention by exploring improvements in Gated Linear Attention.", "method": "Investigated three components of the Gated Linear Attention: feature maps, normalization layers, and a refined gating mechanism, combined with extensive experimental validation.", "result": "The proposed architecture surpasses existing Gated Linear Attention mechanisms in various tasks, including training from scratch and continual pre-training.", "conclusion": "Enhancements made to Gated Linear Attention components considerably improve model performance, addressing challenges in LLMs.", "key_contributions": ["Development of an improved feature mapping function", "Justification for using normalization layers", "Introduction of a refining module for gating mechanism"], "limitations": "", "keywords": ["Large Language Models", "Gated Linear Attention", "Normalization", "Feature Mapping", "Gating Mechanism"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.12204", "pdf": "https://arxiv.org/pdf/2502.12204.pdf", "abs": "https://arxiv.org/abs/2502.12204", "title": "Predicting Depression in Screening Interviews from Interactive Multi-Theme Collaboration", "authors": ["Xianbing Zhao", "Yiqing Lyu", "Di Wang", "Buzhou Tang"], "categories": ["cs.CL", "cs.AI"], "comment": "Findings of ACL2025", "summary": "Automatic depression detection provides cues for early clinical intervention\nby clinicians. Clinical interviews for depression detection involve dialogues\ncentered around multiple themes. Existing studies primarily design end-to-end\nneural network models to capture the hierarchical structure of clinical\ninterview dialogues. However, these methods exhibit defects in modeling the\nthematic content of clinical interviews: 1) they fail to capture intra-theme\nand inter-theme correlation explicitly, and 2) they do not allow clinicians to\nintervene and focus on themes of interest. To address these issues, this paper\nintroduces an interactive depression detection framework. This framework\nleverages in-context learning techniques to identify themes in clinical\ninterviews and then models both intra-theme and inter-theme correlation.\nAdditionally, it employs AI-driven feedback to simulate the interests of\nclinicians, enabling interactive adjustment of theme importance. PDIMC achieves\nabsolute improvements of 35\\% and 12\\% compared to the state-of-the-art on the\ndepression detection dataset DAIC-WOZ, which demonstrates the effectiveness of\nmodeling theme correlation and incorporating interactive external feedback.", "AI": {"tldr": "This paper presents an interactive framework for automatic depression detection that improves upon existing models by capturing intra-theme and inter-theme correlations and allowing clinician intervention.", "motivation": "To enhance automatic depression detection for early clinical intervention by addressing limitations in current neural network models that do not effectively capture thematic content or allow clinician flexibility.", "method": "The framework utilizes in-context learning techniques to identify themes in clinical interviews and models intra-theme and inter-theme correlations, incorporating AI-driven feedback for interactive theme importance adjustment.", "result": "The proposed framework (PDIMC) shows absolute improvements of 35% and 12% over state-of-the-art methods on the DAIC-WOZ depression detection dataset, validating its effectiveness.", "conclusion": "The study demonstrates that effective modeling of theme correlation and clinician feedback significantly enhances depression detection capabilities.", "key_contributions": ["Introduction of an interactive depression detection framework", "Modeling of intra-theme and inter-theme correlations", "Incorporation of AI-driven feedback for clinician intervention"], "limitations": "", "keywords": ["depression detection", "interactive framework", "clinical interviews", "AI-driven feedback", "theme correlation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.14860", "pdf": "https://arxiv.org/pdf/2502.14860.pdf", "abs": "https://arxiv.org/abs/2502.14860", "title": "ALFA: Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning", "authors": ["Shuyue Stella Li", "Jimin Mun", "Faeze Brahman", "Pedram Hosseini", "Bryceton G. Thomas", "Jessica M. Sin", "Bing Ren", "Jonathan S. Ilgen", "Yulia Tsvetkov", "Maarten Sap"], "categories": ["cs.CL"], "comment": "29 pages, 8 figures, 12 tables", "summary": "Large language models (LLMs) often fail to ask effective questions under\nuncertainty, making them unreliable in domains where proactive\ninformation-gathering is essential for decision-making. We present ALignment\nvia Fine-grained Attributes, (ALFA) a framework that improves LLM\nquestion-asking by (i) decomposing the notion of a \"good\" question into a set\nof theory-grounded attributes (e.g., clarity, relevance), (ii) controllably\nsynthesizing attribute-specific question variations, and (iii) aligning models\nvia preference-based optimization to explicitly learn to ask better questions\nalong these fine-grained attributes. Focusing on clinical reasoning as a case\nstudy, we introduce the MediQ-AskDocs dataset, composed of 17k real-world\nclinical interactions augmented with 80k attribute-specific preference pairs of\nfollow-up questions, as well as a novel expert-annotated interactive healthcare\nQA task to evaluate question-asking abilities. Models aligned with ALFA reduce\ndiagnostic errors by 56.6% on MediQ-AskDocs compared to SoTA instruction-tuned\nLLMs, with a question-level win-rate of 64.4% and strong generalizability. Our\nfindings suggest that explicitly guiding question-asking with structured,\nfine-grained attributes offers a scalable path to improve LLMs, especially in\nexpert application domains.", "AI": {"tldr": "The ALFA framework enhances LLM question-asking by structuring question attributes and optimizing models for better performance, especially in clinical reasoning tasks.", "motivation": "Improving LLM reliability in proactive information-gathering, particularly in decision-making contexts like healthcare.", "method": "Introduces the ALFA framework, which decomposes question quality into fine-grained attributes, synthesizes variations, and aligns models through preference-based optimization.", "result": "ALFA-aligned models reduced diagnostic errors by 56.6% on the MediQ-AskDocs dataset, demonstrating significant improvements in question-asking capabilities with a 64.4% question-level win-rate.", "conclusion": "Using structured attributes to guide question-asking can substantially improve LLMs' effectiveness in expert domains such as healthcare.", "key_contributions": ["Introduction of the ALFA framework for question-asking improvement", "Creation of the MediQ-AskDocs dataset with real-world clinical interactions", "Demonstrating a significant reduction in diagnostic errors through model optimization."], "limitations": "", "keywords": ["large language models", "question-asking", "healthcare", "clinical reasoning", "AI alignment"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.17810", "pdf": "https://arxiv.org/pdf/2502.17810.pdf", "abs": "https://arxiv.org/abs/2502.17810", "title": "URO-Bench: Towards Comprehensive Evaluation for End-to-End Spoken Dialogue Models", "authors": ["Ruiqi Yan", "Xiquan Li", "Wenxi Chen", "Zhikang Niu", "Chen Yang", "Ziyang Ma", "Kai Yu", "Xie Chen"], "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "Recent advances in large language models (LLMs) have driven significant\nprogress in end-to-end spoken dialogue models (SDMs). In contrast to text-based\nLLMs, the evaluation framework for SDMs should encompass both cognitive\ndimensions (e.g., logical reasoning, knowledge) and speech-related aspects\n(e.g., paralinguistic cues, audio quality). However, there is still a lack of\ncomprehensive evaluations for SDMs in speech-to-speech (S2S) scenarios. To\naddress this gap, we propose URO-Bench, an extensive benchmark for SDMs.\nNotably, URO-Bench is the first S2S benchmark that covers evaluations about\nmultilingualism, multi-round dialogues, and paralinguistics. Our benchmark is\ndivided into two difficulty levels: basic track and pro track, each comprising\n20 test sets, evaluating the spoken dialogue model's abilities in\nUnderstanding, Reasoning, and Oral conversation. Evaluations on our proposed\nbenchmark reveal that current open-source SDMs perform rather well in daily QA\ntasks, but lag behind their backbone LLMs in terms of instruction-following\nability and also suffer from catastrophic forgetting. Their performance in\nadvanced evaluations of paralinguistic information and audio understanding\nremains subpar, highlighting the need for further research in this direction.\nWe hope that URO-Bench can facilitate the development of spoken dialogue models\nby providing a multifaceted evaluation of existing models and helping to track\nprogress in this area.", "AI": {"tldr": "URO-Bench is introduced as a comprehensive evaluation benchmark for spoken dialogue models (SDMs), addressing gaps in existing evaluations regarding multilingualism and paralinguistics.", "motivation": "To fill the gap in comprehensive evaluations for spoken dialogue models (SDMs) in speech-to-speech scenarios, focusing on cognitive and speech-related dimensions.", "method": "URO-Bench is divided into two tracks (basic and pro), each with 20 test sets assessing understanding, reasoning, and oral conversation skills of SDMs.", "result": "Current open-source SDMs perform well in daily QA tasks but struggle with instruction-following and advanced evaluations of paralinguistic information and audio understanding.", "conclusion": "The benchmark aims to support the development of SDMs by providing a detailed evaluation framework and tracking progress in the field.", "key_contributions": ["First S2S benchmark covering multilingualism and paralinguistic evaluations", "Two difficulty levels for evaluating SDM capabilities", "Identification of performance gaps in open-source models"], "limitations": "Existing models still lag in instruction-following ability and advanced evaluations of paralinguistic information.", "keywords": ["spoken dialogue models", "benchmarks", "multilingualism", "paralinguistics", "evaluation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2503.04773", "pdf": "https://arxiv.org/pdf/2503.04773.pdf", "abs": "https://arxiv.org/abs/2503.04773", "title": "Invisible Walls in Cities: Leveraging Large Language Models to Predict Urban Segregation Experience with Social Media Content", "authors": ["Bingbing Fan", "Lin Chen", "Songwei Li", "Jian Yuan", "Fengli Xu", "Pan Hui", "Yong Li"], "categories": ["cs.CL", "cs.CY", "cs.SI"], "comment": "11 pages, 6 figures", "summary": "Understanding experienced segregation in urban daily life is crucial for\naddressing societal inequalities and fostering inclusivity. The abundance of\nuser-generated reviews on social media encapsulates nuanced perceptions and\nfeelings associated with different places, offering rich insights into\nsegregation. However, leveraging this data poses significant challenges due to\nits vast volume, ambiguity, and confluence of diverse perspectives. To tackle\nthese challenges, we propose using Large Language Models (LLMs) to automate\nonline review mining for segregation prediction. We design a Reflective LLM\nCoder to digest social media content into insights consistent with real-world\nfeedback, and eventually produce a codebook capturing key dimensions that\nsignal segregation experience, such as cultural resonance and appeal,\naccessibility and convenience, and community engagement and local involvement.\nGuided by the codebook, LLMs can generate both informative review summaries and\nratings for segregation prediction. Moreover, we design a\nREasoning-and-EMbedding (RE'EM) framework, which combines the reasoning and\nembedding capabilities of language models to integrate multi-channel features\nfor segregation prediction. Experiments on real-world data demonstrate that our\nframework greatly improves prediction accuracy, with a 22.79% elevation in R2\nand a 9.33% reduction in MSE. The derived codebook is generalizable across\nthree different cities, consistently improving prediction accuracy. Moreover,\nour user study confirms that the codebook-guided summaries provide cognitive\ngains for human participants in perceiving POIs' social inclusiveness. Our\nstudy marks an important step toward understanding implicit social barriers and\ninequalities, demonstrating the great potential of promoting social\ninclusiveness with AI.", "AI": {"tldr": "This paper proposes using Large Language Models (LLMs) to mine online reviews for predicting segregation in urban settings, providing insights into social inclusiveness.", "motivation": "Understanding segregation in urban life is essential for addressing societal inequalities and fostering inclusivity.", "method": "The study uses a Reflective LLM Coder to analyze social media content and create a codebook for segregation-related dimensions, alongside a RE'EM framework that integrates multi-channel features for accurate predictions.", "result": "Experiments show a 22.79% improvement in R2 and a 9.33% reduction in MSE for segregation prediction accuracy, with user studies confirming cognitive gains from the codebook-guided summaries.", "conclusion": "The research highlights the potential of AI in promoting social inclusiveness and understanding social barriers.", "key_contributions": ["Development of a Reflective LLM Coder for online review analysis.", "Creation of a generalizable codebook for segregation prediction across different cities.", "Introduction of the RE'EM framework combining reasoning and embedding features of LLMs."], "limitations": "", "keywords": ["segregation", "Large Language Models", "social media reviews", "urban studies", "AI in health informatics"], "importance_score": 8, "read_time_minutes": 11}}
{"id": "2503.11132", "pdf": "https://arxiv.org/pdf/2503.11132.pdf", "abs": "https://arxiv.org/abs/2503.11132", "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and Extreme KV Compression", "authors": ["Guihong Li", "Mehdi Rezagholizadeh", "Mingyu Yang", "Vikram Appia", "Emad Barsoum"], "categories": ["cs.CL"], "comment": null, "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AIG-AIMA/AMD-Hybrid-Models.", "AI": {"tldr": "The paper introduces X-EcoMLA, an efficient hybrid variant of Multi-head latent attention (MLA) that allows for KV cache memory optimization in pre-trained Transformer models through post-training distillation.", "motivation": "To leverage the benefits of Multi-head latent attention (MLA) for Transformer models that have been pre-trained with different attention mechanisms without requiring extensive re-training.", "method": "Introduction of X-EcoMLA, utilizing post-training distillation to adapt existing models to an efficient MLA variant while preserving performance metrics.", "result": "Experimental results show that X-EcoMLA can compress KV cache significantly (up to 10.6x) with minimal performance drop, demonstrating improved memory efficiency.", "conclusion": "X-EcoMLA effectively enables KV cache compression in Transformer models post-training, maximizing the use of learned latent representations without extensive retraining.", "key_contributions": ["Proposal of X-EcoMLA for efficient KV cache adaptation in pre-trained models", "Demonstration of significant memory compression with preserved performance", "Release of the implementation code to support further research"], "limitations": "The integration of MLA during the pre-training phase remains a challenge for models already trained with other mechanisms.", "keywords": ["Multi-head latent attention", "post-training distillation", "Transformer models", "efficiency optimization", "KV cache compression"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.01196", "pdf": "https://arxiv.org/pdf/2504.01196.pdf", "abs": "https://arxiv.org/abs/2504.01196", "title": "$Œº$KE: Matryoshka Unstructured Knowledge Editing of Large Language Models", "authors": ["Zian Su", "Ziyang Huang", "Kaiyuan Zhang", "Xiangyu Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "COLM 2025. The first two authors contributed equally to this work", "summary": "Large language models (LLMs) have emerged as powerful knowledge bases yet are\nlimited by static training data, leading to issues such as hallucinations and\nsafety risks. Editing a model's internal knowledge through the locate-and-edit\nparadigm has proven a cost-effective alternative to retraining, though current\nunstructured approaches, especially window-based autoregressive methods, often\ndisrupt the causal dependency between early memory updates and later output\ntokens. In this work, we first theoretically analyze these limitations and then\nintroduce Matryoshka Unstructured Knowledge Editing ($\\mu$KE), a novel memory\nupdate mechanism that preserves such dependencies via a Matryoshka-style\nobjective and adaptive loss coefficients. Empirical evaluations on two models\nacross four benchmarks demonstrate that $\\mu$KE improves edit efficacy by up to\n12.33% over state-of-the-art methods, and remains robust when applied to\ndiverse formatted edits, underscoring its potential for effective unstructured\nknowledge editing in LLMs.", "AI": {"tldr": "This paper introduces Matryoshka Unstructured Knowledge Editing (ŒºKE), a new memory update mechanism for large language models (LLMs) that enhances edit efficacy while preserving causal dependencies in knowledge updates.", "motivation": "The paper addresses the limitations of static training data in LLMs, which lead to issues like hallucinations and safety risks. It focuses on finding effective ways to edit a model's knowledge without retraining.", "method": "The authors propose a novel mechanism called Matryoshka Unstructured Knowledge Editing (ŒºKE) that leverages a Matryoshka-style objective and adaptive loss coefficients to enhance the memory update process while maintaining causal dependencies.", "result": "Empirical evaluations show that ŒºKE improves edit efficacy by up to 12.33% over state-of-the-art methods across multiple benchmarks, indicating its robustness with diverse formatted edits.", "conclusion": "The study underscores the potential of ŒºKE as a viable approach for effective unstructured knowledge editing in LLMs, offering a significant improvement in performance.", "key_contributions": ["Introduction of Matryoshka Unstructured Knowledge Editing (ŒºKE) for LLMs", "Theoretical analysis of limitations in current knowledge editing approaches", "Empirical evidence of improved edit efficacy across various benchmarks"], "limitations": "", "keywords": ["Large Language Models", "Knowledge Editing", "Causal Dependency", "Machine Learning", "Unstructured Learning"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2504.02122", "pdf": "https://arxiv.org/pdf/2504.02122.pdf", "abs": "https://arxiv.org/abs/2504.02122", "title": "Overcoming Vocabulary Constraints with Pixel-level Fallback", "authors": ["Jonas F. Lotz", "Hendra Setiawan", "Stephan Peitz", "Yova Kementchedjhieva"], "categories": ["cs.CL"], "comment": "COLM 2025", "summary": "Subword tokenization requires balancing computational efficiency and\nvocabulary coverage, which often leads to suboptimal performance on languages\nand scripts not prioritized during training. We propose to augment pretrained\nlanguage models with a vocabulary-free encoder that generates input embeddings\nfrom text rendered as pixels. Through experiments on English-centric language\nmodels, we demonstrate that our approach substantially improves machine\ntranslation performance and facilitates effective cross-lingual transfer,\noutperforming tokenizer-based methods. Furthermore, we find that pixel-based\nrepresentations outperform byte-level approaches and standard vocabulary\nexpansion. Our approach enhances the multilingual capabilities of monolingual\nlanguage models without extensive retraining and reduces decoding latency via\ninput compression.", "AI": {"tldr": "This paper introduces a vocabulary-free encoder for pretrained language models that uses pixel-based representations to improve multilingual capabilities and machine translation performance.", "motivation": "The motivation is to address the limitations of subword tokenization in handling languages and scripts not prioritized during training, which affects performance.", "method": "The authors augmented pretrained language models with a vocabulary-free encoder that generates embeddings from text rendered as pixels and conducted experiments on English-centric language models to evaluate performance.", "result": "The experiments demonstrate significant improvements in machine translation performance and effective cross-lingual transfer compared to tokenizer-based methods, as well as better outcomes than byte-level approaches and standard vocabulary expansion.", "conclusion": "The approach enhances the multilingual capabilities of monolingual language models with minimal retraining and decreases decoding latency through input compression.", "key_contributions": ["Introduction of a vocabulary-free encoder for input embeddings", "Improved machine translation performance", "Enhanced multilingual capabilities without extensive retraining"], "limitations": "", "keywords": ["subword tokenization", "multilingual models", "machine translation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.02904", "pdf": "https://arxiv.org/pdf/2504.02904.pdf", "abs": "https://arxiv.org/abs/2504.02904", "title": "How Post-Training Reshapes LLMs: A Mechanistic View on Knowledge, Truthfulness, Refusal, and Confidence", "authors": ["Hongzhe Du", "Weikai Li", "Min Cai", "Karim Saraipour", "Zimin Zhang", "Himabindu Lakkaraju", "Yizhou Sun", "Shichang Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "COLM 2025", "summary": "Post-training is essential for the success of large language models (LLMs),\ntransforming pre-trained base models into more useful and aligned post-trained\nmodels. While plenty of works have studied post-training algorithms and\nevaluated post-training models by their outputs, it remains understudied how\npost-training reshapes LLMs internally. In this paper, we compare base and\npost-trained LLMs mechanistically from four perspectives to better understand\npost-training effects. Our findings across model families and datasets reveal\nthat: (1) Post-training does not change the factual knowledge storage\nlocations, and it adapts knowledge representations from the base model while\ndeveloping new knowledge representations; (2) Both truthfulness and refusal can\nbe represented by vectors in the hidden representation space. The truthfulness\ndirection is highly similar between the base and post-trained model, and it is\neffectively transferable for interventions; (3) The refusal direction is\ndifferent between the base and post-trained models, and it shows limited\nforward transferability; (4) Differences in confidence between the base and\npost-trained models cannot be attributed to entropy neurons. Our study provides\ninsights into the fundamental mechanisms preserved and altered during\npost-training, facilitates downstream tasks like model steering, and could\npotentially benefit future research in interpretability and LLM post-training.\nOur code is publicly available at\nhttps://github.com/HZD01/post-training-mechanistic-analysis.", "AI": {"tldr": "This paper investigates the internal changes in large language models (LLMs) during post-training, comparing base and post-trained models to uncover the mechanisms altered and preserved.", "motivation": "The paper aims to understand how post-training affects the internal structure of large language models, which is less explored compared to output evaluations.", "method": "The authors conduct a mechanistic comparison of base and post-trained LLMs from four key perspectives to analyze differences in knowledge representation and model behavior.", "result": "The study finds that while post-training adapts existing knowledge representations and adds new ones, it does not change the locations of factual knowledge. Additionally, the representations for truthfulness are similar in both models, while the refusal direction changes significantly in post-trained models.", "conclusion": "Understanding these mechanisms during post-training can enhance techniques such as model steering and contribute to future research on interpretability and post-training of LLMs.", "key_contributions": ["Comparison of base and post-trained LLMs to surface internal mechanistic changes.", "Insights into knowledge representation adaptation and development during post-training.", "Assessment of truthfulness and refusal representation similarities and differences."], "limitations": "", "keywords": ["large language models", "post-training", "knowledge representation", "model interpretability", "truthfulness"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.07583", "pdf": "https://arxiv.org/pdf/2504.07583.pdf", "abs": "https://arxiv.org/abs/2504.07583", "title": "Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with Question Answering", "authors": ["Patrick Fernandes", "Sweta Agrawal", "Emmanouil Zaranis", "Andr√© F. T. Martins", "Graham Neubig"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Despite the steady progress in machine translation evaluation, existing\nautomatic metrics struggle to capture how well meaning is preserved beyond\nsentence boundaries. We posit that reliance on a single intrinsic quality\nscore, trained to mimic human judgments, might be insufficient for evaluating\ntranslations of long, complex passages, and a more ``pragmatic'' approach that\nassesses how accurately key information is conveyed by a translation in context\nis needed. We introduce TREQA (Translation Evaluation via Question-Answering),\na framework that extrinsically evaluates translation quality by assessing how\naccurately candidate translations answer reading comprehension questions that\ntarget key information in the original source or reference texts. In\nchallenging domains that require long-range understanding, such as literary\ntexts, we show that TREQA is competitive with and, in some cases, outperforms\nstate-of-the-art neural and LLM-based metrics in ranking alternative\nparagraph-level translations, despite never being explicitly optimized to\ncorrelate with human judgments. Furthermore, the generated questions and\nanswers offer interpretability: empirical analysis shows that they effectively\ntarget translation errors identified by experts in evaluated datasets. Our code\nis available at https://github.com/deep-spin/treqa", "AI": {"tldr": "Introducing TREQA, a pragmatic framework for evaluating translation quality through question-answering.", "motivation": "Existing automatic metrics for machine translation struggle with long, complex passages and may not adequately reflect human evaluation.", "method": "TREQA evaluates translations by assessing how accurately they answer reading comprehension questions regarding key information in the original text.", "result": "TREQA is competitive with state-of-the-art translation evaluation metrics, sometimes outperforming them, particularly in domains requiring long-range understanding.", "conclusion": "The interpretability of TREQA is supported by its ability to generate questions and answers that pinpoint translation errors identified by experts.", "key_contributions": ["Introduction of TREQA for pragmatic evaluation of translations", "Demonstration of TREQA's competitiveness against state-of-the-art metrics", "Provision of interpretability through generated Q&A targeting translation errors"], "limitations": "", "keywords": ["Machine Translation", "Evaluation Metrics", "Question-Answering", "Translation Quality", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.09373", "pdf": "https://arxiv.org/pdf/2504.09373.pdf", "abs": "https://arxiv.org/abs/2504.09373", "title": "QUDsim: Quantifying Discourse Similarities in LLM-Generated Text", "authors": ["Ramya Namuduri", "Yating Wu", "Anshun Asher Zheng", "Manya Wadhwa", "Greg Durrett", "Junyi Jessy Li"], "categories": ["cs.CL"], "comment": "COLM 2025 Camera Ready", "summary": "As large language models become increasingly capable at various writing\ntasks, their weakness at generating unique and creative content becomes a major\nliability. Although LLMs have the ability to generate text covering diverse\ntopics, there is an overall sense of repetitiveness across texts that we aim to\nformalize and quantify via a similarity metric. The familiarity between\ndocuments arises from the persistence of underlying discourse structures.\nHowever, existing similarity metrics dependent on lexical overlap and syntactic\npatterns largely capture $\\textit{content}$ overlap, thus making them\nunsuitable for detecting $\\textit{structural}$ similarities. We introduce an\nabstraction based on linguistic theories in Questions Under Discussion (QUD)\nand question semantics to help quantify differences in discourse progression.\nWe then use this framework to build $\\textbf{QUDsim}$, a similarity metric that\ncan detect discursive parallels between documents. Using QUDsim, we find that\nLLMs often reuse discourse structures (more so than humans) across samples,\neven when content differs. Furthermore, LLMs are not only repetitive and\nstructurally uniform, but are also divergent from human authors in the types of\nstructures they use.", "AI": {"tldr": "This paper introduces QUDsim, a new similarity metric based on linguistic theories to quantify structural similarities in text generated by large language models (LLMs).", "motivation": "To address the liability of LLMs in generating unique and creative content, which often shows repetitiveness and structural uniformity compared to human authors.", "method": "The authors develop a framework grounded in Questions Under Discussion (QUD) and question semantics, leading to the creation of a similarity metric called QUDsim.", "result": "QUDsim reveals that LLMs reuse discourse structures more frequently than humans, even when the content varies, highlighting a divergence in structural usage between LLMs and human authors.", "conclusion": "The findings indicate significant structural repetitiveness in LLM outputs, which raises concerns regarding their creative capabilities.", "key_contributions": ["Introduction of QUDsim as a novel similarity metric for quantifying discourse structure similarity", "Demonstration of the structural repetitiveness of LLMs compared to human authors", "Insights into the types of structures used by LLMs versus humans"], "limitations": "", "keywords": ["Large Language Models", "Discourse Structures", "Structural Similarity", "QUDsim", "Text Generation"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.10356", "pdf": "https://arxiv.org/pdf/2505.10356.pdf", "abs": "https://arxiv.org/abs/2505.10356", "title": "Decoding the Multimodal Mind: Generalizable Brain-to-Text Translation via Multimodal Alignment and Adaptive Routing", "authors": ["Chunyu Ye", "Yunhao Zhang", "Jingyuan Sun", "Chong Li", "Chengqing Zong", "Shaonan Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Decoding language from the human brain remains a grand challenge for\nBrain-Computer Interfaces (BCIs). Current approaches typically rely on unimodal\nbrain representations, neglecting the brain's inherently multimodal processing.\nInspired by the brain's associative mechanisms, where viewing an image can\nevoke related sounds and linguistic representations, we propose a unified\nframework that leverages Multimodal Large Language Models (MLLMs) to align\nbrain signals with a shared semantic space encompassing text, images, and\naudio. A router module dynamically selects and fuses modality-specific brain\nfeatures according to the characteristics of each stimulus. Experiments on\nvarious fMRI datasets with textual, visual, and auditory stimuli demonstrate\nstate-of-the-art performance, achieving an 8.48% improvement on the most\ncommonly used benchmark. We further extend our framework to EEG and MEG data,\ndemonstrating flexibility and robustness across varying temporal and spatial\nresolutions. To our knowledge, this is the first unified BCI architecture\ncapable of robustly decoding multimodal brain activity across diverse brain\nsignals and stimulus types, offering a flexible solution for real-world\napplications.", "AI": {"tldr": "This paper presents a unified framework for decoding multimodal brain signals using Multimodal Large Language Models (MLLMs), achieving state-of-the-art performance on fMRI data and extending to EEG and MEG.", "motivation": "Decoding language from the brain is challenging; existing BCIs often overlook the multimodal nature of brain processing.", "method": "The framework uses a router module to select and fuse modality-specific brain features while aligning brain signals with a shared semantic space of text, images, and audio.", "result": "The proposed framework achieved an 8.48% improvement on a common benchmark across various fMRI datasets, and showed robust performance with EEG and MEG data.", "conclusion": "This is the first BCI architecture to robustly decode multimodal brain activity, providing a flexible solution for practical applications.", "key_contributions": ["Introduction of a unified framework for multimodal brain decoding", "The use of MLLMs for aligning brain signals with multiple types of stimuli", "Demonstrated flexibility and performance across different brain signal types"], "limitations": "", "keywords": ["Brain-Computer Interfaces", "Multimodal Processing", "Large Language Models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.19952", "pdf": "https://arxiv.org/pdf/2506.19952.pdf", "abs": "https://arxiv.org/abs/2506.19952", "title": "CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation", "authors": ["Deepon Halder", "Thanmay Jayakumar", "Raj Dabre"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs), despite their ability to perform few-shot\nmachine translation (MT), often lag behind dedicated MT systems trained on\nparallel corpora, which are crucial for high quality machine translation (MT).\nHowever, parallel corpora are often scarce or non-existent for low-resource\nlanguages. In this paper, we propose CycleDistill, a bootstrapping approach\nleveraging LLMs and few-shot translation to obtain high-quality MT systems.\nCycleDistill involves iteratively generating synthetic parallel corpora from\nmonolingual corpora via zero- or few-shot MT, which is then used to fine-tune\nthe model that was used for generating said data for MT. CycleDistill does not\nneed parallel corpora beyond 1 to 4 few-shot examples, and in our experiments\nfocusing on three Indian languages, by relying solely on monolingual corpora,\nit can achieve high-quality machine translation, improving upon a few-shot\nbaseline model by over 20-30 chrF points on average in the first iteration. We\nalso study the effect of leveraging softmax activations during the distillation\nprocess and observe mild improvements in translation quality.", "AI": {"tldr": "CycleDistill is a novel bootstrapping approach that leverages large language models and few-shot translation to create high-quality machine translation systems without the need for extensive parallel corpora, focusing on low-resource Indian languages.", "motivation": "To address the challenges in achieving high-quality machine translation for low-resource languages where parallel corpora are often limited or absent.", "method": "CycleDistill generates synthetic parallel corpora from monolingual corpora using zero- or few-shot machine translation, and then fine-tunes the translation model with this generated data.", "result": "The method shows significant improvement in translation quality, surpassing the baseline model by over 20-30 chrF points on average after the first iteration, solely using monolingual corpora.", "conclusion": "CycleDistill effectively enables high-quality machine translation for low-resource languages by utilizing monolingual data, demonstrating the potential of few-shot learning in this domain.", "key_contributions": ["Introduction of CycleDistill for bootstrapping low-resource machine translation.", "Demonstrated effectiveness of using monolingual data for generating synthetic parallel corpora.", "Analysis of softmax activation influence on translation quality during the distillation process."], "limitations": "The results are based on specific Indian languages and may not generalize to all low-resource languages or other translation contexts.", "keywords": ["Machine Translation", "Large Language Models", "Few-Shot Learning", "Low-Resource Languages", "Synthetic Data"], "importance_score": 8, "read_time_minutes": 5}}
