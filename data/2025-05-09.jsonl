{"id": "2505.04628", "pdf": "https://arxiv.org/pdf/2505.04628.pdf", "abs": "https://arxiv.org/abs/2505.04628", "title": "How Social is It? A Benchmark for LLMs' Capabilities in Multi-user Multi-turn Social Agent Tasks", "authors": ["Yusen Wu", "Junwu Xiong", "Xiaotie Deng"], "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": null, "summary": "Expanding the application of large language models (LLMs) to societal life,\ninstead of primary function only as auxiliary assistants to communicate with\nonly one person at a time, necessitates LLMs' capabilities to independently\nplay roles in multi-user, multi-turn social agent tasks within complex social\nsettings. However, currently the capability has not been systematically\nmeasured with available benchmarks. To address this gap, we first introduce an\nagent task leveling framework grounded in sociological principles.\nConcurrently, we propose a novel benchmark, How Social Is It (we call it HSII\nbelow), designed to assess LLM's social capabilities in comprehensive social\nagents tasks and benchmark representative models. HSII comprises four stages:\nformat parsing, target selection, target switching conversation, and stable\nconversation, which collectively evaluate the communication and task completion\ncapabilities of LLMs within realistic social interaction scenarios dataset,\nHSII-Dataset. The dataset is derived step by step from news dataset. We perform\nan ablation study by doing clustering to the dataset. Additionally, we\ninvestigate the impact of chain of thought (COT) method on enhancing LLMs'\nsocial performance. Since COT cost more computation, we further introduce a new\nstatistical metric, COT-complexity, to quantify the efficiency of certain LLMs\nwith COTs for specific social tasks and strike a better trade-off between\nmeasurement of correctness and efficiency. Various results of our experiments\ndemonstrate that our benchmark is well-suited for evaluating social skills in\nLLMs."}
{"id": "2505.04637", "pdf": "https://arxiv.org/pdf/2505.04637.pdf", "abs": "https://arxiv.org/abs/2505.04637", "title": "Adaptive Token Boundaries: Integrating Human Chunking Mechanisms into Multimodal LLMs", "authors": ["Dongxing Yu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in multimodal large language models (MLLMs) have\ndemonstrated remarkable capabilities in processing diverse data types, yet\nsignificant disparities persist between human cognitive processes and\ncomputational approaches to multimodal information integration. This research\npresents a systematic investigation into the parallels between human\ncross-modal chunking mechanisms and token representation methodologies in\nMLLMs. Through empirical studies comparing human performance patterns with\nmodel behaviors across visual-linguistic tasks, we demonstrate that\nconventional static tokenization schemes fundamentally constrain current\nmodels' capacity to simulate the dynamic, context-sensitive nature of human\ninformation processing. We propose a novel framework for dynamic cross-modal\ntokenization that incorporates adaptive boundaries, hierarchical\nrepresentations, and alignment mechanisms grounded in cognitive science\nprinciples. Quantitative evaluations demonstrate that our approach yields\nstatistically significant improvements over state-of-the-art models on\nbenchmark tasks (+7.8% on Visual Question Answering, +5.3% on Complex Scene\nDescription) while exhibiting more human-aligned error patterns and attention\ndistributions. These findings contribute to the theoretical understanding of\nthe relationship between human cognition and artificial intelligence, while\nproviding empirical evidence for developing more cognitively plausible AI\nsystems."}
{"id": "2505.04639", "pdf": "https://arxiv.org/pdf/2505.04639.pdf", "abs": "https://arxiv.org/abs/2505.04639", "title": "Language translation, and change of accent for speech-to-speech task using diffusion model", "authors": ["Abhishek Mishra", "Ritesh Sur Chowdhury", "Vartul Bahuguna", "Isha Pandey", "Ganesh Ramakrishnan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Speech-to-speech translation (S2ST) aims to convert spoken input in one\nlanguage to spoken output in another, typically focusing on either language\ntranslation or accent adaptation. However, effective cross-cultural\ncommunication requires handling both aspects simultaneously - translating\ncontent while adapting the speaker's accent to match the target language\ncontext. In this work, we propose a unified approach for simultaneous speech\ntranslation and change of accent, a task that remains underexplored in current\nliterature. Our method reformulates the problem as a conditional generation\ntask, where target speech is generated based on phonemes and guided by target\nspeech features. Leveraging the power of diffusion models, known for\nhigh-fidelity generative capabilities, we adapt text-to-image diffusion\nstrategies by conditioning on source speech transcriptions and generating Mel\nspectrograms representing the target speech with desired linguistic and\naccentual attributes. This integrated framework enables joint optimization of\ntranslation and accent adaptation, offering a more parameter-efficient and\neffective model compared to traditional pipelines."}
{"id": "2505.04640", "pdf": "https://arxiv.org/pdf/2505.04640.pdf", "abs": "https://arxiv.org/abs/2505.04640", "title": "A Comparative Benchmark of a Moroccan Darija Toxicity Detection Model (Typica.ai) and Major LLM-Based Moderation APIs (OpenAI, Mistral, Anthropic)", "authors": ["Hicham Assoudi"], "categories": ["cs.CL"], "comment": "GitHub repository with reproducibility materials and evaluation\n  notebook available at:\n  https://github.com/assoudi-typica-ai/darija-toxicity-benchmark", "summary": "This paper presents a comparative benchmark evaluating the performance of\nTypica.ai's custom Moroccan Darija toxicity detection model against major\nLLM-based moderation APIs: OpenAI (omni-moderation-latest), Mistral\n(mistral-moderation-latest), and Anthropic Claude (claude-3-haiku-20240307). We\nfocus on culturally grounded toxic content, including implicit insults,\nsarcasm, and culturally specific aggression often overlooked by general-purpose\nsystems. Using a balanced test set derived from the OMCD_Typica.ai_Mix dataset,\nwe report precision, recall, F1-score, and accuracy, offering insights into\nchallenges and opportunities for moderation in underrepresented languages. Our\nresults highlight Typica.ai's superior performance, underlining the importance\nof culturally adapted models for reliable content moderation."}
{"id": "2505.04712", "pdf": "https://arxiv.org/pdf/2505.04712.pdf", "abs": "https://arxiv.org/abs/2505.04712", "title": "Investigating the Impact and Student Perceptions of Guided Parsons Problems for Learning Logic with Subgoals", "authors": ["Sutapa Dey Tithi", "Xiaoyi Tian", "Min Chi", "Tiffany Barnes"], "categories": ["cs.HC", "cs.LO"], "comment": null, "summary": "Parsons problems (PPs) have shown promise in structured problem solving by\nproviding scaffolding that decomposes the problem and requires learners to\nreconstruct the solution. However, some students face difficulties when first\nlearning with PPs or solving more complex Parsons problems. This study\nintroduces Guided Parsons problems (GPPs) designed to provide step-specific\nhints and improve learning outcomes in an intelligent logic tutor. In a\ncontrolled experiment with 76 participants, GPP students achieved significantly\nhigher accuracy of rule application in both level-end tests and post-tests,\nwith the strongest gains among students with lower prior knowledge. GPP\nstudents initially spent more time in training (1.52 vs. 0.81 hours) but\nrequired less time for post-tests, indicating improved problem solving\nefficiency. Our thematic analysis of GPP student self-explanations revealed\ntask decomposition, better rule understanding, and reduced difficulty as key\nthemes, while some students felt the structured nature of GPPs restricted their\nown way of reasoning. These findings reinforce that GPPs can effectively\ncombine the benefits of worked examples and problem solving practice, but could\nbe further improved by individual adaptation."}
{"id": "2505.04642", "pdf": "https://arxiv.org/pdf/2505.04642.pdf", "abs": "https://arxiv.org/abs/2505.04642", "title": "Rethinking Multimodal Sentiment Analysis: A High-Accuracy, Simplified Fusion Architecture", "authors": ["Nischal Mandal", "Yang Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal sentiment analysis, a pivotal task in affective computing, seeks\nto understand human emotions by integrating cues from language, audio, and\nvisual signals. While many recent approaches leverage complex attention\nmechanisms and hierarchical architectures, we propose a lightweight, yet\neffective fusion-based deep learning model tailored for utterance-level emotion\nclassification. Using the benchmark IEMOCAP dataset, which includes aligned\ntext, audio-derived numeric features, and visual descriptors, we design a\nmodality-specific encoder using fully connected layers followed by dropout\nregularization. The modality-specific representations are then fused using\nsimple concatenation and passed through a dense fusion layer to capture\ncross-modal interactions. This streamlined architecture avoids computational\noverhead while preserving performance, achieving a classification accuracy of\n92% across six emotion categories. Our approach demonstrates that with careful\nfeature engineering and modular design, simpler fusion strategies can\noutperform or match more complex models, particularly in resource-constrained\nenvironments."}
{"id": "2505.04869", "pdf": "https://arxiv.org/pdf/2505.04869.pdf", "abs": "https://arxiv.org/abs/2505.04869", "title": "From First Draft to Final Insight: A Multi-Agent Approach for Feedback Generation", "authors": ["Jie Cao", "Chloe Qianhui Zhao", "Xian Chen", "Shuman Wang", "Christian Schunn", "Kenneth R. Koedinger", "Jionghao Lin"], "categories": ["cs.HC"], "comment": "14 pages, to be published at the 26th International Conference on\n  Artificial Intelligence in Education (AIED '25)", "summary": "Producing large volumes of high-quality, timely feedback poses significant\nchallenges to instructors. To address this issue, automation\ntechnologies-particularly Large Language Models (LLMs)-show great potential.\nHowever, current LLM-based research still shows room for improvement in terms\nof feedback quality. Our study proposed a multi-agent approach performing\n\"generation, evaluation, and regeneration\" (G-E-RG) to further enhance feedback\nquality. In the first-generation phase, six methods were adopted, combining\nthree feedback theoretical frameworks and two prompt methods: zero-shot and\nretrieval-augmented generation with chain-of-thought (RAG_CoT). The results\nindicated that, compared to first-round feedback, G-E-RG significantly improved\nfinal feedback across six methods for most dimensions. Specifically:(1)\nEvaluation accuracy for six methods increased by 3.36% to 12.98% (p<0.001); (2)\nThe proportion of feedback containing four effective components rose from an\naverage of 27.72% to an average of 98.49% among six methods, sub-dimensions of\nproviding critiques, highlighting strengths, encouraging agency, and\ncultivating dialogue also showed great enhancement (p<0.001); (3) There was a\nsignificant improvement in most of the feature values (p<0.001), although some\nsub-dimensions (e.g., strengthening the teacher-student relationship) still\nrequire further enhancement; (4) The simplicity of feedback was effectively\nenhanced (p<0.001) for three methods."}
{"id": "2505.04643", "pdf": "https://arxiv.org/pdf/2505.04643.pdf", "abs": "https://arxiv.org/abs/2505.04643", "title": "Prediction-powered estimators for finite population statistics in highly imbalanced textual data: Public hate crime estimation", "authors": ["Hannes Waldetoft", "Jakob Torgander", "Måns Magnusson"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Estimating population parameters in finite populations of text documents can\nbe challenging when obtaining the labels for the target variable requires\nmanual annotation. To address this problem, we combine predictions from a\ntransformer encoder neural network with well-established survey sampling\nestimators using the model predictions as an auxiliary variable. The\napplicability is demonstrated in Swedish hate crime statistics based on Swedish\npolice reports. Estimates of the yearly number of hate crimes and the police's\nunder-reporting are derived using the Hansen-Hurwitz estimator, difference\nestimation, and stratified random sampling estimation. We conclude that if\nlabeled training data is available, the proposed method can provide very\nefficient estimates with reduced time spent on manual annotation."}
{"id": "2505.04886", "pdf": "https://arxiv.org/pdf/2505.04886.pdf", "abs": "https://arxiv.org/abs/2505.04886", "title": "Fairness Perceptions in Regression-based Predictive Models", "authors": ["Mukund Telukunta", "Venkata Sriram Siddhardh Nadendla", "Morgan Stuart", "Casey Canfield"], "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "Regression-based predictive analytics used in modern kidney transplantation\nis known to inherit biases from training data. This leads to social\ndiscrimination and inefficient organ utilization, particularly in the context\nof a few social groups. Despite this concern, there is limited research on\nfairness in regression and its impact on organ utilization and placement. This\npaper introduces three novel divergence-based group fairness notions: (i)\nindependence, (ii) separation, and (iii) sufficiency to assess the fairness of\nregression-based analytics tools. In addition, fairness preferences are\ninvestigated from crowd feedback, in order to identify a socially accepted\ngroup fairness criterion for evaluating these tools. A total of 85 participants\nwere recruited from the Prolific crowdsourcing platform, and a Mixed-Logit\ndiscrete choice model was used to model fairness feedback and estimate social\nfairness preferences. The findings clearly depict a strong preference towards\nthe separation and sufficiency fairness notions, and that the predictive\nanalytics is deemed fair with respect to gender and race groups, but unfair in\nterms of age groups."}
{"id": "2505.04645", "pdf": "https://arxiv.org/pdf/2505.04645.pdf", "abs": "https://arxiv.org/abs/2505.04645", "title": "ChatGPT for automated grading of short answer questions in mechanical ventilation", "authors": ["Tejas Jade", "Alex Yartsev"], "categories": ["cs.CL", "cs.LG", "stat.CO"], "comment": null, "summary": "Standardised tests using short answer questions (SAQs) are common in\npostgraduate education. Large language models (LLMs) simulate conversational\nlanguage and interpret unstructured free-text responses in ways aligning with\napplying SAQ grading rubrics, making them attractive for automated grading. We\nevaluated ChatGPT 4o to grade SAQs in a postgraduate medical setting using data\nfrom 215 students (557 short-answer responses) enrolled in an online course on\nmechanical ventilation (2020--2024). Deidentified responses to three case-based\nscenarios were presented to ChatGPT with a standardised grading prompt and\nrubric. Outputs were analysed using mixed-effects modelling, variance component\nanalysis, intraclass correlation coefficients (ICCs), Cohen's kappa, Kendall's\nW, and Bland--Altman statistics. ChatGPT awarded systematically lower marks\nthan human graders with a mean difference (bias) of -1.34 on a 10-point scale.\nICC values indicated poor individual-level agreement (ICC1 = 0.086), and\nCohen's kappa (-0.0786) suggested no meaningful agreement. Variance component\nanalysis showed minimal variability among the five ChatGPT sessions (G-value =\n0.87), indicating internal consistency but divergence from the human grader.\nThe poorest agreement was observed for evaluative and analytic items, whereas\nchecklist and prescriptive rubric items had less disagreement. We caution\nagainst the use of LLMs in grading postgraduate coursework. Over 60% of\nChatGPT-assigned grades differed from human grades by more than acceptable\nboundaries for high-stakes assessments."}
{"id": "2505.04890", "pdf": "https://arxiv.org/pdf/2505.04890.pdf", "abs": "https://arxiv.org/abs/2505.04890", "title": "Theatrical Language Processing: Exploring AI-Augmented Improvisational Acting and Scriptwriting with LLMs", "authors": ["Sora Kang", "Joonhwan Lee"], "categories": ["cs.HC"], "comment": "ISEA2025 (30th International Symposium on Electronic/Emerging Art)", "summary": "The increasing convergence of artificial intelligence has opened new avenues,\nincluding its emerging role in enhancing creativity. It is reshaping\ntraditional creative practices such as actor improvisation, which often\nstruggles with predictable patterns, limited interaction, and a lack of\nengaging stimuli. In this paper, we introduce a new concept, Theatrical\nLanguage Processing (TLP), and an AI-driven creativity support tool,\nScribble.ai, designed to augment actors' creative expression and spontaneity\nthrough interactive practice. We conducted a user study involving tests and\ninterviews with fourteen participants. Our findings indicate that: (1) Actors\nexpanded their creativity when faced with AI-produced irregular scenarios; (2)\nThe AI's unpredictability heightened their problem-solving skills, specifically\nin interpreting unfamiliar situations; (3) However, AI often generated\nexcessively detailed scripts, which limited interpretive freedom and hindered\nsubtext exploration. Based on these findings, we discuss the new potential in\nenhancing creative expressions in film and theater studies through an AI-driven\ntool."}
{"id": "2505.04649", "pdf": "https://arxiv.org/pdf/2505.04649.pdf", "abs": "https://arxiv.org/abs/2505.04649", "title": "FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research Insights", "authors": ["Chengzhang Yu", "Yiming Zhang", "Zhixin Liu", "Zenghui Ding", "Yining Sun", "Zhanpeng Jin"], "categories": ["cs.CL"], "comment": "12 pages, 4 figures, 5 table", "summary": "The automation of scientific research through large language models (LLMs)\npresents significant opportunities but faces critical challenges in knowledge\nsynthesis and quality assurance. We introduce Feedback-Refined Agent\nMethodology (FRAME), a novel framework that enhances medical paper generation\nthrough iterative refinement and structured feedback. Our approach comprises\nthree key innovations: (1) A structured dataset construction method that\ndecomposes 4,287 medical papers into essential research components through\niterative refinement; (2) A tripartite architecture integrating Generator,\nEvaluator, and Reflector agents that progressively improve content quality\nthrough metric-driven feedback; and (3) A comprehensive evaluation framework\nthat combines statistical metrics with human-grounded benchmarks. Experimental\nresults demonstrate FRAME's effectiveness, achieving significant improvements\nover conventional approaches across multiple models (9.91% average gain with\nDeepSeek V3, comparable improvements with GPT-4o Mini) and evaluation\ndimensions. Human evaluation confirms that FRAME-generated papers achieve\nquality comparable to human-authored works, with particular strength in\nsynthesizing future research directions. The results demonstrated our work\ncould efficiently assist medical research by building a robust foundation for\nautomated medical research paper generation while maintaining rigorous academic\nstandards."}
{"id": "2505.05038", "pdf": "https://arxiv.org/pdf/2505.05038.pdf", "abs": "https://arxiv.org/abs/2505.05038", "title": "Uncertainty-Aware Scarf Plots", "authors": ["Nelusa Pathmanathan", "Seyda Öney", "Maurice Koch", "Daniel Weiskopf", "Kuno Kurzhals"], "categories": ["cs.HC"], "comment": "2025 Symposium on Eye Tracking Research and Applications (ETRA '25)", "summary": "Multiple challenges emerge when analyzing eye-tracking data with areas of\ninterest (AOIs) because recordings are subject to different sources of\nuncertainties. Previous work often presents gaze data without considering those\ninaccuracies in the data. To address this issue, we developed uncertainty-aware\nscarf plot visualizations that aim to make analysts aware of uncertainties with\nrespect to the position-based mapping of gaze to AOIs and depth dependency in\n3D scenes. Additionally, we also consider uncertainties in automatic AOI\nannotation. We showcase our approach in comparison to standard scarf plots in\nan augmented reality scenario."}
{"id": "2505.04651", "pdf": "https://arxiv.org/pdf/2505.04651.pdf", "abs": "https://arxiv.org/abs/2505.04651", "title": "Scientific Hypothesis Generation and Validation: Methods, Datasets, and Future Directions", "authors": ["Adithya Kulkarni", "Fatimah Alotaibi", "Xinyue Zeng", "Longfeng Wu", "Tong Zeng", "Barry Menglong Yao", "Minqian Liu", "Shuaicheng Zhang", "Lifu Huang", "Dawei Zhou"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are transforming scientific hypothesis\ngeneration and validation by enabling information synthesis, latent\nrelationship discovery, and reasoning augmentation. This survey provides a\nstructured overview of LLM-driven approaches, including symbolic frameworks,\ngenerative models, hybrid systems, and multi-agent architectures. We examine\ntechniques such as retrieval-augmented generation, knowledge-graph completion,\nsimulation, causal inference, and tool-assisted reasoning, highlighting\ntrade-offs in interpretability, novelty, and domain alignment. We contrast\nearly symbolic discovery systems (e.g., BACON, KEKADA) with modern LLM\npipelines that leverage in-context learning and domain adaptation via\nfine-tuning, retrieval, and symbolic grounding. For validation, we review\nsimulation, human-AI collaboration, causal modeling, and uncertainty\nquantification, emphasizing iterative assessment in open-world contexts. The\nsurvey maps datasets across biomedicine, materials science, environmental\nscience, and social science, introducing new resources like AHTech and\nCSKG-600. Finally, we outline a roadmap emphasizing novelty-aware generation,\nmultimodal-symbolic integration, human-in-the-loop systems, and ethical\nsafeguards, positioning LLMs as agents for principled, scalable scientific\ndiscovery."}
{"id": "2505.05170", "pdf": "https://arxiv.org/pdf/2505.05170.pdf", "abs": "https://arxiv.org/abs/2505.05170", "title": "Dukawalla: Voice Interfaces for Small Businesses in Africa", "authors": ["Elizabeth Ankrah", "Stephanie Nyairo", "Mercy Muchai", "Kagonya Awori", "Millicent Ochieng", "Mark Kariuki", "Jacki O'Neill"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Small and medium sized businesses often struggle with data driven decision\nmaking do to a lack of advanced analytics tools, especially in African\ncountries where they make up a majority of the workforce. Though many tools\nexist they are not designed to fit into the ways of working of SMB workers who\nare mobile first, have limited time to learn new workflows, and for whom social\nand business are tightly coupled. To address this, the Dukawalla prototype was\ncreated. This intelligent assistant bridges the gap between raw business data,\nand actionable insights by leveraging voice interaction and the power of\ngenerative AI. Dukawalla provides an intuitive way for business owners to\ninteract with their data, aiding in informed decision making. This paper\nexamines Dukawalla's deployment across SMBs in Nairobi, focusing on their\nexperiences using this voice based assistant to streamline data collection and\nprovide business insights"}
{"id": "2505.04653", "pdf": "https://arxiv.org/pdf/2505.04653.pdf", "abs": "https://arxiv.org/abs/2505.04653", "title": "Advancing Conversational Diagnostic AI with Multimodal Reasoning", "authors": ["Khaled Saab", "Jan Freyberg", "Chunjong Park", "Tim Strother", "Yong Cheng", "Wei-Hung Weng", "David G. T. Barrett", "David Stutz", "Nenad Tomasev", "Anil Palepu", "Valentin Liévin", "Yash Sharma", "Roma Ruparel", "Abdullah Ahmed", "Elahe Vedadi", "Kimberly Kanada", "Cian Hughes", "Yun Liu", "Geoff Brown", "Yang Gao", "Sean Li", "S. Sara Mahdavi", "James Manyika", "Katherine Chou", "Yossi Matias", "Avinatan Hassidim", "Dale R. Webster", "Pushmeet Kohli", "S. M. Ali Eslami", "Joëlle Barral", "Adam Rodman", "Vivek Natarajan", "Mike Schaekermann", "Tao Tu", "Alan Karthikesalingam", "Ryutaro Tanno"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated great potential for conducting\ndiagnostic conversations but evaluation has been largely limited to\nlanguage-only interactions, deviating from the real-world requirements of\nremote care delivery. Instant messaging platforms permit clinicians and\npatients to upload and discuss multimodal medical artifacts seamlessly in\nmedical consultation, but the ability of LLMs to reason over such data while\npreserving other attributes of competent diagnostic conversation remains\nunknown. Here we advance the conversational diagnosis and management\nperformance of the Articulate Medical Intelligence Explorer (AMIE) through a\nnew capability to gather and interpret multimodal data, and reason about this\nprecisely during consultations. Leveraging Gemini 2.0 Flash, our system\nimplements a state-aware dialogue framework, where conversation flow is\ndynamically controlled by intermediate model outputs reflecting patient states\nand evolving diagnoses. Follow-up questions are strategically directed by\nuncertainty in such patient states, leading to a more structured multimodal\nhistory-taking process that emulates experienced clinicians. We compared AMIE\nto primary care physicians (PCPs) in a randomized, blinded, OSCE-style study of\nchat-based consultations with patient actors. We constructed 105 evaluation\nscenarios using artifacts like smartphone skin photos, ECGs, and PDFs of\nclinical documents across diverse conditions and demographics. Our rubric\nassessed multimodal capabilities and other clinically meaningful axes like\nhistory-taking, diagnostic accuracy, management reasoning, communication, and\nempathy. Specialist evaluation showed AMIE to be superior to PCPs on 7/9\nmultimodal and 29/32 non-multimodal axes (including diagnostic accuracy). The\nresults show clear progress in multimodal conversational diagnostic AI, but\nreal-world translation needs further research."}
{"id": "2505.05441", "pdf": "https://arxiv.org/pdf/2505.05441.pdf", "abs": "https://arxiv.org/abs/2505.05441", "title": "GesPrompt: Leveraging Co-Speech Gestures to Augment LLM-Based Interaction in Virtual Reality", "authors": ["Xiyun Hu", "Dizhi Ma", "Fengming He", "Zhengzhe Zhu", "Shao-Kang Hsia", "Chenfei Zhu", "Ziyi Liu", "Karthik Ramani"], "categories": ["cs.HC"], "comment": null, "summary": "Large Language Model (LLM)-based copilots have shown great potential in\nExtended Reality (XR) applications. However, the user faces challenges when\ndescribing the 3D environments to the copilots due to the complexity of\nconveying spatial-temporal information through text or speech alone. To address\nthis, we introduce GesPrompt, a multimodal XR interface that combines co-speech\ngestures with speech, allowing end-users to communicate more naturally and\naccurately with LLM-based copilots in XR environments. By incorporating\ngestures, GesPrompt extracts spatial-temporal reference from co-speech\ngestures, reducing the need for precise textual prompts and minimizing\ncognitive load for end-users. Our contributions include (1) a workflow to\nintegrate gesture and speech input in the XR environment, (2) a prototype VR\nsystem that implements the workflow, and (3) a user study demonstrating its\neffectiveness in improving user communication in VR environments."}
{"id": "2505.04654", "pdf": "https://arxiv.org/pdf/2505.04654.pdf", "abs": "https://arxiv.org/abs/2505.04654", "title": "A Comparative Analysis of Ethical and Safety Gaps in LLMs using Relative Danger Coefficient", "authors": ["Yehor Tereshchenko", "Mika Hämäläinen"], "categories": ["cs.CL"], "comment": null, "summary": "Artificial Intelligence (AI) and Large Language Models (LLMs) have rapidly\nevolved in recent years, showcasing remarkable capabilities in natural language\nunderstanding and generation. However, these advancements also raise critical\nethical questions regarding safety, potential misuse, discrimination and\noverall societal impact. This article provides a comparative analysis of the\nethical performance of various AI models, including the brand new\nDeepSeek-V3(R1 with reasoning and without), various GPT variants (4o, 3.5\nTurbo, 4 Turbo, o1/o3 mini) and Gemini (1.5 flash, 2.0 flash and 2.0 flash exp)\nand highlights the need for robust human oversight, especially in situations\nwith high stakes. Furthermore, we present a new metric for calculating harm in\nLLMs called Relative Danger Coefficient (RDC)."}
{"id": "2505.04722", "pdf": "https://arxiv.org/pdf/2505.04722.pdf", "abs": "https://arxiv.org/abs/2505.04722", "title": "Fitts' List Revisited: An Empirical Study on Function Allocation in a Two-Agent Physical Human-Robot Collaborative Position/Force Task", "authors": ["Nicky Mol", "J. Micah Prendergast", "David A. Abbink", "Luka Peternel"], "categories": ["cs.RO", "cs.HC"], "comment": "10 pages, 6 figures, under review for publication in IEEE Robotics\n  and Automation Letters (RA-L)", "summary": "In this letter, we investigate whether the classical function allocation\nholds for physical Human-Robot Collaboration, which is important for providing\ninsights for Industry 5.0 to guide how to best augment rather than replace\nworkers. This study empirically tests the applicability of Fitts' List within\nphysical Human-Robot Collaboration, by conducting a user study (N=26,\nwithin-subject design) to evaluate four distinct allocations of position/force\ncontrol between human and robot in an abstract blending task. We hypothesize\nthat the function in which humans control the position achieves better\nperformance and receives higher user ratings. When allocating position control\nto the human and force control to the robot, compared to the opposite case, we\nobserved a significant improvement in preventing overblending. This was also\nperceived better in terms of physical demand and overall system acceptance,\nwhile participants experienced greater autonomy, more engagement and less\nfrustration. An interesting insight was that the supervisory role (when the\nrobot controls both position and force control) was rated second best in terms\nof subjective acceptance. Another surprising insight was that if position\ncontrol was delegated to the robot, the participants perceived much lower\nautonomy than when the force control was delegated to the robot. These findings\nempirically support applying Fitts' principles to static function allocation\nfor physical collaboration, while also revealing important nuanced user\nexperience trade-offs, particularly regarding perceived autonomy when\ndelegating position control."}
{"id": "2505.04655", "pdf": "https://arxiv.org/pdf/2505.04655.pdf", "abs": "https://arxiv.org/abs/2505.04655", "title": "Integration of Large Language Models and Traditional Deep Learning for Social Determinants of Health Prediction", "authors": ["Paul Landes", "Jimeng Sun", "Adam Cross"], "categories": ["cs.CL"], "comment": null, "summary": "Social Determinants of Health (SDoH) are economic, social and personal\ncircumstances that affect or influence an individual's health status. SDoHs\nhave shown to be correlated to wellness outcomes, and therefore, are useful to\nphysicians in diagnosing diseases and in decision-making. In this work, we\nautomatically extract SDoHs from clinical text using traditional deep learning\nand Large Language Models (LLMs) to find the advantages and disadvantages of\neach on an existing publicly available dataset. Our models outperform a\nprevious reference point on a multilabel SDoH classification by 10 points, and\nwe present a method and model to drastically speed up classification (12X\nexecution time) by eliminating expensive LLM processing. The method we present\ncombines a more nimble and efficient solution that leverages the power of the\nLLM for precision and traditional deep learning methods for efficiency. We also\nshow highly performant results on a dataset supplemented with synthetic data\nand several traditional deep learning models that outperform LLMs. Our models\nand methods offer the next iteration of automatic prediction of SDoHs that\nimpact at-risk patients."}
{"id": "2505.04885", "pdf": "https://arxiv.org/pdf/2505.04885.pdf", "abs": "https://arxiv.org/abs/2505.04885", "title": "A Multi-Agent AI Framework for Immersive Audiobook Production through Spatial Audio and Neural Narration", "authors": ["Shaja Arul Selvamani", "Nia D'Souza Ganapathy"], "categories": ["cs.SD", "cs.HC", "cs.MA", "cs.MM", "eess.AS"], "comment": null, "summary": "This research introduces an innovative AI-driven multi-agent framework\nspecifically designed for creating immersive audiobooks. Leveraging neural\ntext-to-speech synthesis with FastSpeech 2 and VALL-E for expressive narration\nand character-specific voices, the framework employs advanced language models\nto automatically interpret textual narratives and generate realistic spatial\naudio effects. These sound effects are dynamically synchronized with the\nstoryline through sophisticated temporal integration methods, including Dynamic\nTime Warping (DTW) and recurrent neural networks (RNNs). Diffusion-based\ngenerative models combined with higher-order ambisonics (HOA) and scattering\ndelay networks (SDN) enable highly realistic 3D soundscapes, substantially\nenhancing listener immersion and narrative realism. This technology\nsignificantly advances audiobook applications, providing richer experiences for\neducational content, storytelling platforms, and accessibility solutions for\nvisually impaired audiences. Future work will address personalization, ethical\nmanagement of synthesized voices, and integration with multi-sensory platforms."}
{"id": "2505.04660", "pdf": "https://arxiv.org/pdf/2505.04660.pdf", "abs": "https://arxiv.org/abs/2505.04660", "title": "AI-Generated Fall Data: Assessing LLMs and Diffusion Model for Wearable Fall Detection", "authors": ["Sana Alamgeer", "Yasine Souissi", "Anne H. H. Ngu"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Training fall detection systems is challenging due to the scarcity of\nreal-world fall data, particularly from elderly individuals. To address this,\nwe explore the potential of Large Language Models (LLMs) for generating\nsynthetic fall data. This study evaluates text-to-motion (T2M, SATO, ParCo) and\ntext-to-text models (GPT4o, GPT4, Gemini) in simulating realistic fall\nscenarios. We generate synthetic datasets and integrate them with four\nreal-world baseline datasets to assess their impact on fall detection\nperformance using a Long Short-Term Memory (LSTM) model. Additionally, we\ncompare LLM-generated synthetic data with a diffusion-based method to evaluate\ntheir alignment with real accelerometer distributions. Results indicate that\ndataset characteristics significantly influence the effectiveness of synthetic\ndata, with LLM-generated data performing best in low-frequency settings (e.g.,\n20Hz) while showing instability in high-frequency datasets (e.g., 200Hz). While\ntext-to-motion models produce more realistic biomechanical data than\ntext-to-text models, their impact on fall detection varies. Diffusion-based\nsynthetic data demonstrates the closest alignment to real data but does not\nconsistently enhance model performance. An ablation study further confirms that\nthe effectiveness of synthetic data depends on sensor placement and fall\nrepresentation. These findings provide insights into optimizing synthetic data\ngeneration for fall detection models."}
{"id": "2505.05318", "pdf": "https://arxiv.org/pdf/2505.05318.pdf", "abs": "https://arxiv.org/abs/2505.05318", "title": "Mapping User Trust in Vision Language Models: Research Landscape, Challenges, and Prospects", "authors": ["Agnese Chiatti", "Sara Bernardini", "Lara Shibelski Godoy Piccolo", "Viola Schiaffonati", "Matteo Matteucci"], "categories": ["cs.CV", "cs.AI", "cs.CY", "cs.HC", "cs.RO"], "comment": null, "summary": "The rapid adoption of Vision Language Models (VLMs), pre-trained on large\nimage-text and video-text datasets, calls for protecting and informing users\nabout when to trust these systems. This survey reviews studies on trust\ndynamics in user-VLM interactions, through a multi-disciplinary taxonomy\nencompassing different cognitive science capabilities, collaboration modes, and\nagent behaviours. Literature insights and findings from a workshop with\nprospective VLM users inform preliminary requirements for future VLM trust\nstudies."}
{"id": "2505.04665", "pdf": "https://arxiv.org/pdf/2505.04665.pdf", "abs": "https://arxiv.org/abs/2505.04665", "title": "Personalized Risks and Regulatory Strategies of Large Language Models in Digital Advertising", "authors": ["Haoyang Feng", "Yanjun Dai", "Yuan Gao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although large language models have demonstrated the potential for\npersonalized advertising recommendations in experimental environments, in\nactual operations, how advertising recommendation systems can be combined with\nmeasures such as user privacy protection and data security is still an area\nworthy of in-depth discussion. To this end, this paper studies the personalized\nrisks and regulatory strategies of large language models in digital\nadvertising. This study first outlines the principles of Large Language Model\n(LLM), especially the self-attention mechanism based on the Transformer\narchitecture, and how to enable the model to understand and generate natural\nlanguage text. Then, the BERT (Bidirectional Encoder Representations from\nTransformers) model and the attention mechanism are combined to construct an\nalgorithmic model for personalized advertising recommendations and user factor\nrisk protection. The specific steps include: data collection and preprocessing,\nfeature selection and construction, using large language models such as BERT\nfor advertising semantic embedding, and ad recommendations based on user\nportraits. Then, local model training and data encryption are used to ensure\nthe security of user privacy and avoid the leakage of personal data. This paper\ndesigns an experiment for personalized advertising recommendation based on a\nlarge language model of BERT and verifies it with real user data. The\nexperimental results show that BERT-based advertising push can effectively\nimprove the click-through rate and conversion rate of advertisements. At the\nsame time, through local model training and privacy protection mechanisms, the\nrisk of user privacy leakage can be reduced to a certain extent."}
{"id": "2505.05396", "pdf": "https://arxiv.org/pdf/2505.05396.pdf", "abs": "https://arxiv.org/abs/2505.05396", "title": "A Pain Assessment Framework based on multimodal data and Deep Machine Learning methods", "authors": ["Stefanos Gkikas"], "categories": ["cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "From the original abstract:\n  This thesis initially aims to study the pain assessment process from a\nclinical-theoretical perspective while exploring and examining existing\nautomatic approaches. Building on this foundation, the primary objective of\nthis Ph.D. project is to develop innovative computational methods for automatic\npain assessment that achieve high performance and are applicable in real\nclinical settings. A primary goal is to thoroughly investigate and assess\nsignificant factors, including demographic elements that impact pain\nperception, as recognized in pain research, through a computational standpoint.\nWithin the limits of the available data in this research area, our goal was to\ndesign, develop, propose, and offer automatic pain assessment pipelines for\nunimodal and multimodal configurations that are applicable to the specific\nrequirements of different scenarios. The studies published in this Ph.D. thesis\nshowcased the effectiveness of the proposed methods, achieving state-of-the-art\nresults. Additionally, they paved the way for exploring new approaches in\nartificial intelligence, foundation models, and generative artificial\nintelligence."}
{"id": "2505.04666", "pdf": "https://arxiv.org/pdf/2505.04666.pdf", "abs": "https://arxiv.org/abs/2505.04666", "title": "Fine-Tuning Large Language Models and Evaluating Retrieval Methods for Improved Question Answering on Building Codes", "authors": ["Mohammad Aqib", "Mohd Hamza", "Qipei Mei", "Ying Hei Chui"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Building codes are regulations that establish standards for the design,\nconstruction, and safety of buildings to ensure structural integrity, fire\nprotection, and accessibility. They are often extensive, complex, and subject\nto frequent updates, making manual querying challenging and time-consuming. Key\ndifficulties include navigating large volumes of text, interpreting technical\nlanguage, and identifying relevant clauses across different sections. A\npotential solution is to build a Question-Answering (QA) system that answers\nuser queries based on building codes. Among the various methods for building a\nQA system, Retrieval-Augmented Generation (RAG) stands out in performance. RAG\nconsists of two components: a retriever and a language model. This study\nfocuses on identifying a suitable retriever method for building codes and\noptimizing the generational capability of the language model using fine-tuning\ntechniques. We conducted a detailed evaluation of various retrieval methods by\nperforming the retrieval on the National Building Code of Canada (NBCC) and\nexplored the impact of domain-specific fine-tuning on several language models\nusing the dataset derived from NBCC. Our analysis included a comparative\nassessment of different retrievers and the performance of both pre-trained and\nfine-tuned models to determine the efficacy and domain-specific adaptation of\nlanguage models using fine-tuning on the NBCC dataset. Experimental results\nshowed that Elasticsearch proved to be the most robust retriever among all. The\nfindings also indicate that fine-tuning language models on an NBCC-specific\ndataset can enhance their ability to generate contextually relevant responses.\nWhen combined with context retrieved by a powerful retriever like\nElasticsearch, this improvement in LLM performance can optimize the RAG system,\nenabling it to better navigate the complexities of the NBCC."}
{"id": "2410.00192", "pdf": "https://arxiv.org/pdf/2410.00192.pdf", "abs": "https://arxiv.org/abs/2410.00192", "title": "Large-scale, Longitudinal, Hybrid Participatory Design Program to Create Navigation Technology for the Blind", "authors": ["Daeun Joyce Chung", "Muya Guoji", "Nina Mindel", "Alexis Malkin", "Fernando Albertorio", "Shane Lowe", "Chris McNally", "Casandra Xavier", "Paul Ruvolo"], "categories": ["cs.HC"], "comment": null, "summary": "Empowering people who are blind or visually impaired (BVI) to enhance their\norientation and mobility skills is critical to equalizing their access to\nsocial and economic opportunities. To manage this crucial challenge, we\nemployed a novel design process based on a large-scale, longitudinal,\ncommunity-based structure. Across three annual programs we engaged with the BVI\ncommunity in online and in-person modes. In total, our team included 67 total\nBVI participatory design participants online, 11 BVI co-designers in-person,\nand 4 BVI program coordinators. Through this design process we built a mobile\napplication that enables users to generate, share, and navigate maps of indoor\nand outdoor environments without the need to instrument each environment with\nbeacons or fiducial markers. We evaluated this app at a healthcare facility,\nand participants in the evaluation rated the app highly with respect to its\ndesign, features, and potential for positive impact on quality of life."}
{"id": "2505.04671", "pdf": "https://arxiv.org/pdf/2505.04671.pdf", "abs": "https://arxiv.org/abs/2505.04671", "title": "Reward-SQL: Boosting Text-to-SQL via Stepwise Reasoning and Process-Supervised Rewards", "authors": ["Yuxin Zhang", "Meihao Fan", "Ju Fan", "Mingyang Yi", "Yuyu Luo", "Jian Tan", "Guoliang Li"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in large language models (LLMs) have significantly improved\nperformance on the Text-to-SQL task by leveraging their powerful reasoning\ncapabilities. To enhance accuracy during the reasoning process, external\nProcess Reward Models (PRMs) can be introduced during training and inference to\nprovide fine-grained supervision. However, if misused, PRMs may distort the\nreasoning trajectory and lead to suboptimal or incorrect SQL generation.To\naddress this challenge, we propose Reward-SQL, a framework that systematically\nexplores how to incorporate PRMs into the Text-to-SQL reasoning process\neffectively. Our approach follows a \"cold start, then PRM supervision\"\nparadigm. Specifically, we first train the model to decompose SQL queries into\nstructured stepwise reasoning chains using common table expressions\n(Chain-of-CTEs), establishing a strong and interpretable reasoning baseline.\nThen, we investigate four strategies for integrating PRMs, and find that\ncombining PRM as an online training signal (GRPO) with PRM-guided inference\n(e.g., best-of-N sampling) yields the best results. Empirically, on the BIRD\nbenchmark, Reward-SQL enables models supervised by a 7B PRM to achieve a 13.1%\nperformance gain across various guidance strategies. Notably, our GRPO-aligned\npolicy model based on Qwen2.5-Coder-7B-Instruct achieves 68.9% accuracy on the\nBIRD development set, outperforming all baseline methods under the same model\nsize. These results demonstrate the effectiveness of Reward-SQL in leveraging\nreward-based supervision for Text-to-SQL reasoning. Our code is publicly\navailable."}
{"id": "2410.04686", "pdf": "https://arxiv.org/pdf/2410.04686.pdf", "abs": "https://arxiv.org/abs/2410.04686", "title": "From Perception to Decision: Assessing the Role of Chart Types Affordances in High-Level Decision Tasks", "authors": ["Yixuan Li", "Emery D. Berger", "Minsuk Kahng", "Cindy Xiong Bearfield"], "categories": ["cs.HC"], "comment": null, "summary": "Visualization design influences how people perceive data patterns, yet most\nresearch focuses on low-level analytic tasks, such as finding correlations. The\nextent to which these perceptual affordances translate to high-level\ndecision-making in the real world remains underexplored. Through a case study\nof academic mentorship selection using bar charts and pie charts, we\ninvestigated whether chart types differentially influence how students evaluate\nfaculty research profiles. Our crowdsourced experiment revealed only minimal\ndifferences in decision outcomes between chart types, suggesting that\nperceptual affordances established in controlled analytical tasks may not\ndirectly translate to high-level decision scenarios. These findings emphasize\nthe importance of evaluating visualizations within real-world contexts and\nhighlight the need to distinguish between perceptual and decision affordances\nwhen developing visualization guidelines."}
{"id": "2505.04673", "pdf": "https://arxiv.org/pdf/2505.04673.pdf", "abs": "https://arxiv.org/abs/2505.04673", "title": "REVEAL: Multi-turn Evaluation of Image-Input Harms for Vision LLM", "authors": ["Madhur Jindal", "Saurabh Deshpande"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages (8 main), to be published in IJCAI 2025", "summary": "Vision Large Language Models (VLLMs) represent a significant advancement in\nartificial intelligence by integrating image-processing capabilities with\ntextual understanding, thereby enhancing user interactions and expanding\napplication domains. However, their increased complexity introduces novel\nsafety and ethical challenges, particularly in multi-modal and multi-turn\nconversations. Traditional safety evaluation frameworks, designed for\ntext-based, single-turn interactions, are inadequate for addressing these\ncomplexities. To bridge this gap, we introduce the REVEAL (Responsible\nEvaluation of Vision-Enabled AI LLMs) Framework, a scalable and automated\npipeline for evaluating image-input harms in VLLMs. REVEAL includes automated\nimage mining, synthetic adversarial data generation, multi-turn conversational\nexpansion using crescendo attack strategies, and comprehensive harm assessment\nthrough evaluators like GPT-4o.\n  We extensively evaluated five state-of-the-art VLLMs, GPT-4o, Llama-3.2,\nQwen2-VL, Phi3.5V, and Pixtral, across three important harm categories: sexual\nharm, violence, and misinformation. Our findings reveal that multi-turn\ninteractions result in significantly higher defect rates compared to\nsingle-turn evaluations, highlighting deeper vulnerabilities in VLLMs. Notably,\nGPT-4o demonstrated the most balanced performance as measured by our\nSafety-Usability Index (SUI) followed closely by Pixtral. Additionally,\nmisinformation emerged as a critical area requiring enhanced contextual\ndefenses. Llama-3.2 exhibited the highest MT defect rate ($16.55 \\%$) while\nQwen2-VL showed the highest MT refusal rate ($19.1 \\%$)."}
{"id": "2503.12613", "pdf": "https://arxiv.org/pdf/2503.12613.pdf", "abs": "https://arxiv.org/abs/2503.12613", "title": "Negotiative Alignment: Embracing Disagreement to Achieve Fairer Outcomes -- Insights from Urban Studies", "authors": ["Rashid Mushkani", "Hugo Berard", "Shin Koseki"], "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.MA"], "comment": "16 pages, 13 figures", "summary": "Cities are not monolithic; they are arenas of negotiation among groups that\nhold varying needs, values, and experiences. Conventional methods of urban\nassessment -- from standardized surveys to AI-driven evaluations -- frequently\nrely on a single consensus metric (e.g., an average measure of inclusivity or\nsafety). Although such aggregations simplify design decisions, they risk\nobscuring the distinct perspectives of marginalized populations. In this paper,\nwe present findings from a community-centered study in Montreal involving 35\nresidents with diverse demographic and social identities, particularly\nwheelchair users, seniors, and LGBTQIA2+ individuals. Using rating and ranking\ntasks on 20 urban sites, we observe that disagreements are systematic rather\nthan random, reflecting structural inequalities, differing cultural values, and\npersonal experiences of safety and accessibility.\n  Based on these empirical insights, we propose negotiative alignment, an AI\nframework that treats disagreement as an essential input to be preserved,\nanalyzed, and addressed. Negotiative alignment builds on pluralistic models by\ndynamically updating stakeholder preferences through multi-agent negotiation\nmechanisms, ensuring no single perspective is marginalized. We outline how this\nframework can be integrated into urban analytics -- and other decision-making\ncontexts -- to retain minority viewpoints, adapt to changing stakeholder\nconcerns, and enhance fairness and accountability. The study demonstrates that\npreserving and engaging with disagreement, rather than striving for an\nartificial consensus, can produce more equitable and responsive AI-driven\noutcomes in urban design."}
{"id": "2505.04678", "pdf": "https://arxiv.org/pdf/2505.04678.pdf", "abs": "https://arxiv.org/abs/2505.04678", "title": "Advanced Deep Learning Approaches for Automated Recognition of Cuneiform Symbols", "authors": ["Shahad Elshehaby", "Alavikunhu Panthakkan", "Hussain Al-Ahmad", "Mina Al-Saad"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents a thoroughly automated method for identifying and\ninterpreting cuneiform characters via advanced deep-learning algorithms. Five\ndistinct deep-learning models were trained on a comprehensive dataset of\ncuneiform characters and evaluated according to critical performance metrics,\nincluding accuracy and precision. Two models demonstrated outstanding\nperformance and were subsequently assessed using cuneiform symbols from the\nHammurabi law acquisition, notably Hammurabi Law 1. Each model effectively\nrecognized the relevant Akkadian meanings of the symbols and delivered precise\nEnglish translations. Future work will investigate ensemble and stacking\napproaches to optimize performance, utilizing hybrid architectures to improve\ndetection accuracy and reliability. This research explores the linguistic\nrelationships between Akkadian, an ancient Mesopotamian language, and Arabic,\nemphasizing their historical and cultural linkages. This study demonstrates the\ncapability of deep learning to decipher ancient scripts by merging\ncomputational linguistics with archaeology, therefore providing significant\ninsights for the comprehension and conservation of human history."}
{"id": "2504.01259", "pdf": "https://arxiv.org/pdf/2504.01259.pdf", "abs": "https://arxiv.org/abs/2504.01259", "title": "Facilitating Instructors-LLM Collaboration for Problem Design in Introductory Programming Classrooms", "authors": ["Muntasir Hoq", "Jessica Vandenberg", "Shuyin Jiao", "Seung Lee", "Bradford Mott", "Narges Norouzi", "James Lester", "Bita Akram"], "categories": ["cs.HC", "K.3.1"], "comment": "Accepted at CHI 2025 Workshop on Augmented Educators and AI: Shaping\n  the Future of Human and AI Cooperation in Learning", "summary": "Advancements in Large Language Models (LLMs), such as ChatGPT, offer\nsignificant opportunities to enhance instructional support in introductory\nprogramming courses. While extensive research has explored the effectiveness of\nLLMs in supporting student learning, limited studies have examined how these\nmodels can assist instructors in designing instructional activities. This work\ninvestigates how instructors' expertise in effective activity design can be\nintegrated with LLMs' ability to generate novel and targeted programming\nproblems, facilitating more effective activity creation for programming\nclassrooms. To achieve this, we employ a participatory design approach to\ndevelop an instructor-authoring tool that incorporates LLM support, fostering\ncollaboration between instructors and AI in generating programming exercises.\nThis tool also allows instructors to specify common student mistakes and\nmisconceptions, which informs the adaptive feedback generation process. We\nconduct case studies with three instructors, analyzing how they use our system\nto design programming problems for their introductory courses. Through these\ncase studies, we assess instructors' perceptions of the usefulness and\nlimitations of LLMs in authoring problem statements for instructional purposes.\nAdditionally, we compare the efficiency, quality, effectiveness, and coverage\nof designed activities when instructors create problems with and without\nstructured LLM prompting guidelines. Our findings provide insights into the\npotential of LLMs in enhancing instructor workflows and improving programming\neducation and provide guidelines for designing effective AI-assisted\nproblem-authoring interfaces."}
{"id": "2505.04723", "pdf": "https://arxiv.org/pdf/2505.04723.pdf", "abs": "https://arxiv.org/abs/2505.04723", "title": "SOAEsV2-7B/72B: Full-Pipeline Optimization for State-Owned Enterprise LLMs via Continual Pre-Training, Domain-Progressive SFT and Distillation-Enhanced Speculative Decoding", "authors": ["Jingyang Deng", "Ran Chen", "Jo-Ku Cheng", "Jinwen Ma"], "categories": ["cs.CL"], "comment": null, "summary": "This study addresses key challenges in developing domain-specific large\nlanguage models (LLMs) for Chinese state-owned assets and enterprises (SOAEs),\nwhere current approaches face three limitations: 1) constrained model capacity\nthat limits knowledge integration and cross-task adaptability; 2) excessive\nreliance on domain-specific supervised fine-tuning (SFT) data, which neglects\nthe broader applicability of general language patterns; and 3) inefficient\ninference acceleration for large models processing long contexts. In this work,\nwe propose SOAEsV2-7B/72B, a specialized LLM series developed via a three-phase\nframework: 1) continual pre-training integrates domain knowledge while\nretaining base capabilities; 2) domain-progressive SFT employs curriculum-based\nlearning strategy, transitioning from weakly relevant conversational data to\nexpert-annotated SOAEs datasets to optimize domain-specific tasks; 3)\ndistillation-enhanced speculative decoding accelerates inference via logit\ndistillation between 72B target and 7B draft models, achieving\n1.39-1.52$\\times$ speedup without quality loss. Experimental results\ndemonstrate that our domain-specific pre-training phase maintains 99.8% of\noriginal general language capabilities while significantly improving domain\nperformance, resulting in a 1.08$\\times$ improvement in Rouge-1 score and a\n1.17$\\times$ enhancement in BLEU-4 score. Ablation studies further show that\ndomain-progressive SFT outperforms single-stage training, achieving\n1.02$\\times$ improvement in Rouge-1 and 1.06$\\times$ in BLEU-4. Our work\nintroduces a comprehensive, full-pipeline approach for optimizing SOAEs LLMs,\nbridging the gap between general language capabilities and domain-specific\nexpertise."}
{"id": "2505.00455", "pdf": "https://arxiv.org/pdf/2505.00455.pdf", "abs": "https://arxiv.org/abs/2505.00455", "title": "Data Therapist: Eliciting Domain Knowledge from Subject Matter Experts Using Large Language Models", "authors": ["Sungbok Shin", "Hyeon Jeon", "Sanghyun Hong", "Niklas Elmqvist"], "categories": ["cs.HC", "cs.AI"], "comment": "Submitted to IEEE VIS2025", "summary": "Effective data visualization requires not only technical proficiency but also\na deep understanding of the domain-specific context in which data exists. This\ncontext often includes tacit knowledge about data provenance, quality, and\nintended use, which is rarely explicit in the dataset itself. We present the\nData Therapist, a web-based tool that helps domain experts externalize this\nimplicit knowledge through a mixed-initiative process combining iterative Q&A\nwith interactive annotation. Powered by a large language model, the system\nanalyzes user-supplied datasets, prompts users with targeted questions, and\nallows annotation at varying levels of granularity. The resulting structured\nknowledge base can inform both human and automated visualization design. We\nevaluated the tool in a qualitative study involving expert pairs from Molecular\nBiology, Accounting, Political Science, and Usable Security. The study revealed\nrecurring patterns in how experts reason about their data and highlights areas\nwhere AI support can improve visualization design."}
{"id": "2505.04785", "pdf": "https://arxiv.org/pdf/2505.04785.pdf", "abs": "https://arxiv.org/abs/2505.04785", "title": "Flower Across Time and Media: Sentiment Analysis of Tang Song Poetry and Visual Correspondence", "authors": ["Shuai Gong", "Tiange Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 9 figures", "summary": "The Tang (618 to 907) and Song (960 to 1279) dynasties witnessed an\nextraordinary flourishing of Chinese cultural expression, where floral motifs\nserved as a dynamic medium for both poetic sentiment and artistic design. While\nprevious scholarship has examined these domains independently, the systematic\ncorrelation between evolving literary emotions and visual culture remains\nunderexplored. This study addresses that gap by employing BERT-based sentiment\nanalysis to quantify emotional patterns in floral imagery across Tang Song\npoetry, then validating these patterns against contemporaneous developments in\ndecorative arts.Our approach builds upon recent advances in computational\nhumanities while remaining grounded in traditional sinological methods. By\napplying a fine tuned BERT model to analyze peony and plum blossom imagery in\nclassical poetry, we detect measurable shifts in emotional connotations between\nthe Tang and Song periods. These textual patterns are then cross berenced with\nvisual evidence from textiles, ceramics, and other material culture, revealing\npreviously unrecognized synergies between literary expression and artistic\nrepresentation."}
{"id": "2411.02478", "pdf": "https://arxiv.org/pdf/2411.02478.pdf", "abs": "https://arxiv.org/abs/2411.02478", "title": "Imagining and building wise machines: The centrality of AI metacognition", "authors": ["Samuel G. B. Johnson", "Amir-Hossein Karimi", "Yoshua Bengio", "Nick Chater", "Tobias Gerstenberg", "Kate Larson", "Sydney Levine", "Melanie Mitchell", "Iyad Rahwan", "Bernhard Schölkopf", "Igor Grossmann"], "categories": ["cs.AI", "cs.CY", "cs.HC"], "comment": "23 pages, 1 figure, 3 tables", "summary": "Although AI has become increasingly smart, its wisdom has not kept pace. In\nthis article, we examine what is known about human wisdom and sketch a vision\nof its AI counterpart. We analyze human wisdom as a set of strategies for\nsolving intractable problems-those outside the scope of analytic\ntechniques-including both object-level strategies like heuristics [for managing\nproblems] and metacognitive strategies like intellectual humility,\nperspective-taking, or context-adaptability [for managing object-level\nstrategies]. We argue that AI systems particularly struggle with metacognition;\nimproved metacognition would lead to AI more robust to novel environments,\nexplainable to users, cooperative with others, and safer in risking fewer\nmisaligned goals with human users. We discuss how wise AI might be benchmarked,\ntrained, and implemented."}
{"id": "2505.04844", "pdf": "https://arxiv.org/pdf/2505.04844.pdf", "abs": "https://arxiv.org/abs/2505.04844", "title": "Osiris: A Lightweight Open-Source Hallucination Detection System", "authors": ["Alex Shan", "John Bauer", "Christopher D. Manning"], "categories": ["cs.CL"], "comment": "Stanford 191W", "summary": "Retrieval-Augmented Generation (RAG) systems have gained widespread adoption\nby application builders because they leverage sources of truth to enable Large\nLanguage Models (LLMs) to generate more factually sound responses. However,\nhallucinations, instances of LLM responses that are unfaithful to the provided\ncontext, often prevent these systems from being deployed in production\nenvironments. Current hallucination detection methods typically involve human\nevaluation or the use of closed-source models to review RAG system outputs for\nhallucinations. Both human evaluators and closed-source models suffer from\nscaling issues due to their high costs and slow inference speeds. In this work,\nwe introduce a perturbed multi-hop QA dataset with induced hallucinations. Via\nsupervised fine-tuning on our dataset, we achieve better recall with a 7B model\nthan GPT-4o on the RAGTruth hallucination detection benchmark and offer\ncompetitive performance on precision and accuracy, all while using a fraction\nof the parameters. Code is released at our repository."}
{"id": "2412.16698", "pdf": "https://arxiv.org/pdf/2412.16698.pdf", "abs": "https://arxiv.org/abs/2412.16698", "title": "Interact with me: Joint Egocentric Forecasting of Intent to Interact, Attitude and Social Actions", "authors": ["Tongfei Bian", "Yiming Ma", "Mathieu Chollet", "Victor Sanchez", "Tanaya Guha"], "categories": ["cs.CV", "cs.HC"], "comment": "Accepted to ICME, 2025. Camera-ready Version", "summary": "For efficient human-agent interaction, an agent should proactively recognize\ntheir target user and prepare for upcoming interactions. We formulate this\nchallenging problem as the novel task of jointly forecasting a person's intent\nto interact with the agent, their attitude towards the agent and the action\nthey will perform, from the agent's (egocentric) perspective. So we propose\n\\emph{SocialEgoNet} - a graph-based spatiotemporal framework that exploits task\ndependencies through a hierarchical multitask learning approach. SocialEgoNet\nuses whole-body skeletons (keypoints from face, hands and body) extracted from\nonly 1 second of video input for high inference speed. For evaluation, we\naugment an existing egocentric human-agent interaction dataset with new class\nlabels and bounding box annotations. Extensive experiments on this augmented\ndataset, named JPL-Social, demonstrate \\emph{real-time} inference and superior\nperformance (average accuracy across all tasks: 83.15\\%) of our model\noutperforming several competitive baselines. The additional annotations and\ncode will be available upon acceptance."}
{"id": "2505.04847", "pdf": "https://arxiv.org/pdf/2505.04847.pdf", "abs": "https://arxiv.org/abs/2505.04847", "title": "Benchmarking LLM Faithfulness in RAG with Evolving Leaderboards", "authors": ["Manveer Singh Tamber", "Forrest Sheng Bao", "Chenyu Xu", "Ge Luo", "Suleman Kazi", "Minseok Bae", "Miaoran Li", "Ofer Mendelevitch", "Renyi Qu", "Jimmy Lin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hallucinations remain a persistent challenge for LLMs. RAG aims to reduce\nhallucinations by grounding responses in contexts. However, even when provided\ncontext, LLMs still frequently introduce unsupported information or\ncontradictions. This paper presents our efforts to measure LLM hallucinations\nwith a focus on summarization tasks, assessing how often various LLMs introduce\nhallucinations when summarizing documents. We discuss Vectara's existing LLM\nhallucination leaderboard, based on the Hughes Hallucination Evaluation Model\n(HHEM). While HHEM and Vectara's Hallucination Leaderboard have garnered great\nresearch interest, we examine challenges faced by HHEM and current\nhallucination detection methods by analyzing the effectiveness of these methods\non existing hallucination datasets. To address these limitations, we propose\nFaithJudge, an LLM-as-a-judge approach guided by few-shot human hallucination\nannotations, which substantially improves automated LLM hallucination\nevaluation over current methods. We introduce an enhanced hallucination\nleaderboard centered on FaithJudge, alongside our current hallucination\nleaderboard, enabling more reliable benchmarking of LLMs for hallucinations in\nRAG."}
{"id": "2501.17899", "pdf": "https://arxiv.org/pdf/2501.17899.pdf", "abs": "https://arxiv.org/abs/2501.17899", "title": "The Right to AI", "authors": ["Rashid Mushkani", "Hugo Berard", "Allison Cohen", "Shin Koeski"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "ICML 2025", "summary": "This paper proposes a Right to AI, which asserts that individuals and\ncommunities should meaningfully participate in the development and governance\nof the AI systems that shape their lives. Motivated by the increasing\ndeployment of AI in critical domains and inspired by Henri Lefebvre's concept\nof the Right to the City, we reconceptualize AI as a societal infrastructure,\nrather than merely a product of expert design. In this paper, we critically\nevaluate how generative agents, large-scale data extraction, and diverse\ncultural values bring new complexities to AI oversight. The paper proposes that\ngrassroots participatory methodologies can mitigate biased outcomes and enhance\nsocial responsiveness. It asserts that data is socially produced and should be\nmanaged and owned collectively. Drawing on Sherry Arnstein's Ladder of Citizen\nParticipation and analyzing nine case studies, the paper develops a four-tier\nmodel for the Right to AI that situates the current paradigm and envisions an\naspirational future. It proposes recommendations for inclusive data ownership,\ntransparent design processes, and stakeholder-driven oversight. We also discuss\nmarket-led and state-centric alternatives and argue that participatory\napproaches offer a better balance between technical efficiency and democratic\nlegitimacy."}
{"id": "2505.04916", "pdf": "https://arxiv.org/pdf/2505.04916.pdf", "abs": "https://arxiv.org/abs/2505.04916", "title": "An Open-Source Dual-Loss Embedding Model for Semantic Retrieval in Higher Education", "authors": ["Ramteja Sajja", "Yusuf Sermet", "Ibrahim Demir"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "17 pages, 3 Tables", "summary": "Recent advances in AI have catalyzed the adoption of intelligent educational\ntools, yet many semantic retrieval systems remain ill-suited to the unique\nlinguistic and structural characteristics of academic content. This study\npresents two open-source embedding models fine-tuned for educational question\nanswering, particularly in the context of course syllabi. A synthetic dataset\nof 3,197 sentence pairs, spanning synonymous terminology, paraphrased\nquestions, and implicit-explicit mappings, was constructed through a\ncombination of manual curation and large language model (LLM)-assisted\ngeneration. Two training strategies were evaluated: (1) a baseline model\nfine-tuned using MultipleNegativesRankingLoss (MNRL), and (2) a dual-loss model\nthat combines MNRL with CosineSimilarityLoss to improve both semantic ranking\nand similarity calibration. Evaluations were conducted on 28 university course\nsyllabi using a fixed set of natural language questions categorized into\ncourse, faculty, and teaching assistant information. Results demonstrate that\nboth fine-tuned models outperform strong open-source baselines, including\nall-MiniLM-L6-v2 and multi-qa-MiniLM-L6-cos-v1, and that the dual-loss model\nnarrows the performance gap with high-performing proprietary embeddings such as\nOpenAI's text-embedding-3 series. This work contributes reusable,\ndomain-aligned embedding models and provides a replicable framework for\neducational semantic retrieval, supporting downstream applications such as\nacademic chatbots, retrieval-augmented generation (RAG) systems, and learning\nmanagement system (LMS) integrations."}
{"id": "2503.01894", "pdf": "https://arxiv.org/pdf/2503.01894.pdf", "abs": "https://arxiv.org/abs/2503.01894", "title": "LIVS: A Pluralistic Alignment Dataset for Inclusive Public Spaces", "authors": ["Rashid Mushkani", "Shravan Nayak", "Hugo Berard", "Allison Cohen", "Shin Koseki", "Hadrien Bertrand"], "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "ICML 2025", "summary": "We introduce the Local Intersectional Visual Spaces (LIVS) dataset, a\nbenchmark for multi-criteria alignment, developed through a two-year\nparticipatory process with 30 community organizations to support the\npluralistic alignment of text-to-image (T2I) models in inclusive urban\nplanning. The dataset encodes 37,710 pairwise comparisons across 13,462 images,\nstructured along six criteria - Accessibility, Safety, Comfort, Invitingness,\nInclusivity, and Diversity - derived from 634 community-defined concepts. Using\nDirect Preference Optimization (DPO), we fine-tune Stable Diffusion XL to\nreflect multi-criteria spatial preferences and evaluate the LIVS dataset and\nthe fine-tuned model through four case studies: (1) DPO increases alignment\nwith annotated preferences, particularly when annotation volume is high; (2)\npreference patterns vary across participant identities, underscoring the need\nfor intersectional data; (3) human-authored prompts generate more distinctive\nvisual outputs than LLM-generated ones, influencing annotation decisiveness;\nand (4) intersectional groups assign systematically different ratings across\ncriteria, revealing the limitations of single-objective alignment. While DPO\nimproves alignment under specific conditions, the prevalence of neutral ratings\nindicates that community values are heterogeneous and often ambiguous. LIVS\nprovides a benchmark for developing T2I models that incorporate local,\nstakeholder-driven preferences, offering a foundation for context-aware\nalignment in spatial design."}
{"id": "2505.04955", "pdf": "https://arxiv.org/pdf/2505.04955.pdf", "abs": "https://arxiv.org/abs/2505.04955", "title": "Chain-of-Thought Tokens are Computer Program Variables", "authors": ["Fangwei Zhu", "Peiyi Wang", "Zhifang Sui"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chain-of-thoughts (CoT) requires large language models (LLMs) to generate\nintermediate steps before reaching the final answer, and has been proven\neffective to help LLMs solve complex reasoning tasks. However, the inner\nmechanism of CoT still remains largely unclear. In this paper, we empirically\nstudy the role of CoT tokens in LLMs on two compositional tasks: multi-digit\nmultiplication and dynamic programming. While CoT is essential for solving\nthese problems, we find that preserving only tokens that store intermediate\nresults would achieve comparable performance. Furthermore, we observe that\nstoring intermediate results in an alternative latent form will not affect\nmodel performance. We also randomly intervene some values in CoT, and notice\nthat subsequent CoT tokens and the final answer would change correspondingly.\nThese findings suggest that CoT tokens may function like variables in computer\nprograms but with potential drawbacks like unintended shortcuts and\ncomputational complexity limits between tokens. The code and data are available\nat https://github.com/solitaryzero/CoTs_are_Variables."}
{"id": "2504.04243", "pdf": "https://arxiv.org/pdf/2504.04243.pdf", "abs": "https://arxiv.org/abs/2504.04243", "title": "Perils of Label Indeterminacy: A Case Study on Prediction of Neurological Recovery After Cardiac Arrest", "authors": ["Jakob Schoeffer", "Maria De-Arteaga", "Jonathan Elmer"], "categories": ["cs.LG", "cs.AI", "cs.HC", "stat.ME"], "comment": "The 2025 ACM Conference on Fairness, Accountability, and Transparency\n  (FAccT '25)", "summary": "The design of AI systems to assist human decision-making typically requires\nthe availability of labels to train and evaluate supervised models. Frequently,\nhowever, these labels are unknown, and different ways of estimating them\ninvolve unverifiable assumptions or arbitrary choices. In this work, we\nintroduce the concept of label indeterminacy and derive important implications\nin high-stakes AI-assisted decision-making. We present an empirical study in a\nhealthcare context, focusing specifically on predicting the recovery of\ncomatose patients after resuscitation from cardiac arrest. Our study shows that\nlabel indeterminacy can result in models that perform similarly when evaluated\non patients with known labels, but vary drastically in their predictions for\npatients where labels are unknown. After demonstrating crucial ethical\nimplications of label indeterminacy in this high-stakes context, we discuss\ntakeaways for evaluation, reporting, and design."}
{"id": "2505.04984", "pdf": "https://arxiv.org/pdf/2505.04984.pdf", "abs": "https://arxiv.org/abs/2505.04984", "title": "Rethinking the Relationship between the Power Law and Hierarchical Structures", "authors": ["Kai Nakaishi", "Ryo Yoshida", "Kohei Kajikawa", "Koji Hukushima", "Yohei Oseki"], "categories": ["cs.CL"], "comment": "13 pages, 11 figures", "summary": "Statistical analysis of corpora provides an approach to quantitatively\ninvestigate natural languages. This approach has revealed that several power\nlaws consistently emerge across different corpora and languages, suggesting the\nuniversal principles underlying languages. Particularly, the power-law decay of\ncorrelation has been interpreted as evidence for underlying hierarchical\nstructures in syntax, semantics, and discourse. This perspective has also been\nextended to child languages and animal signals. However, the argument\nsupporting this interpretation has not been empirically tested. To address this\nproblem, this study examines the validity of the argument for syntactic\nstructures. Specifically, we test whether the statistical properties of parse\ntrees align with the implicit assumptions in the argument. Using English\ncorpora, we analyze the mutual information, deviations from probabilistic\ncontext-free grammars (PCFGs), and other properties in parse trees, as well as\nin the PCFG that approximates these trees. Our results indicate that the\nassumptions do not hold for syntactic structures and that it is difficult to\napply the proposed argument to child languages and animal signals, highlighting\nthe need to reconsider the relationship between the power law and hierarchical\nstructures."}
{"id": "2505.04172", "pdf": "https://arxiv.org/pdf/2505.04172.pdf", "abs": "https://arxiv.org/abs/2505.04172", "title": "A Dataset and Toolkit for Multiparameter Cardiovascular Physiology Sensing on Rings", "authors": ["Jiankai Tang", "Kegang Wang", "Yingke Ding", "Jiatong Ji", "Zeyu Wang", "Xiyuxing Zhang", "Ping Chen", "Yuanchun Shi", "Yuntao Wang"], "categories": ["eess.IV", "cs.HC", "physics.med-ph"], "comment": null, "summary": "Smart rings offer a convenient way to continuously and unobtrusively monitor\ncardiovascular physiological signals. However, a gap remains between the ring\nhardware and reliable methods for estimating cardiovascular parameters, partly\ndue to the lack of publicly available datasets and standardized analysis tools.\nIn this work, we present $\\tau$-Ring, the first open-source ring-based dataset\ndesigned for cardiovascular physiological sensing. The dataset comprises\nphotoplethysmography signals (infrared and red channels) and 3-axis\naccelerometer data collected from two rings (reflective and transmissive\noptical paths), with 28.21 hours of raw data from 34 subjects across seven\nactivities. $\\tau$-Ring encompasses both stationary and motion scenarios, as\nwell as stimulus-evoked abnormal physiological states, annotated with four\nground-truth labels: heart rate, respiratory rate, oxygen saturation, and blood\npressure. Using our proposed RingTool toolkit, we evaluated three widely-used\nphysics-based methods and four cutting-edge deep learning approaches. Our\nresults show superior performance compared to commercial rings, achieving best\nMAE values of 5.18 BPM for heart rate, 2.98 BPM for respiratory rate, 3.22\\%\nfor oxygen saturation, and 13.33/7.56 mmHg for systolic/diastolic blood\npressure estimation. The open-sourced dataset and toolkit aim to foster further\nresearch and community-driven advances in ring-based cardiovascular health\nsensing."}
{"id": "2505.04993", "pdf": "https://arxiv.org/pdf/2505.04993.pdf", "abs": "https://arxiv.org/abs/2505.04993", "title": "Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes", "authors": ["Zhuocheng Gong", "Jian Guan", "Wei Wu", "Huishuai Zhang", "Dongyan Zhao"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success, yet aligning\ntheir generations with human preferences remains a critical challenge. Existing\napproaches to preference modeling often rely on an explicit or implicit reward\nfunction, overlooking the intricate and multifaceted nature of human\npreferences that may encompass conflicting factors across diverse tasks and\npopulations. To address this limitation, we introduce Latent Preference Coding\n(LPC), a novel framework that models the implicit factors as well as their\ncombinations behind holistic preferences using discrete latent codes. LPC\nseamlessly integrates with various offline alignment algorithms, automatically\ninferring the underlying factors and their importance from data without relying\non pre-defined reward functions and hand-crafted combination weights. Extensive\nexperiments on multiple benchmarks demonstrate that LPC consistently improves\nupon three alignment algorithms (DPO, SimPO, and IPO) using three base models\n(Mistral-7B, Llama3-8B, and Llama3-8B-Instruct). Furthermore, deeper analysis\nreveals that the learned latent codes effectively capture the differences in\nthe distribution of human preferences and significantly enhance the robustness\nof alignment against noise in data. By providing a unified representation for\nthe multifarious preference factors, LPC paves the way towards developing more\nrobust and versatile alignment techniques for the responsible deployment of\npowerful LLMs."}
{"id": "2505.04994", "pdf": "https://arxiv.org/pdf/2505.04994.pdf", "abs": "https://arxiv.org/abs/2505.04994", "title": "Rethinking Invariance in In-context Learning", "authors": ["Lizhe Fang", "Yifei Wang", "Khashayar Gatmiry", "Lei Fang", "Yisen Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In-Context Learning (ICL) has emerged as a pivotal capability of\nauto-regressive large language models, yet it is hindered by a notable\nsensitivity to the ordering of context examples regardless of their mutual\nindependence. To address this issue, recent studies have introduced several\nvariant algorithms of ICL that achieve permutation invariance. However, many of\nthese do not exhibit comparable performance with the standard auto-regressive\nICL algorithm. In this work, we identify two crucial elements in the design of\nan invariant ICL algorithm: information non-leakage and context\ninterdependence, which are not simultaneously achieved by any of the existing\nmethods. These investigations lead us to the proposed Invariant ICL (InvICL), a\nmethodology designed to achieve invariance in ICL while ensuring the two\nproperties. Empirically, our findings reveal that InvICL surpasses previous\nmodels, both invariant and non-invariant, in most benchmark datasets,\nshowcasing superior generalization capabilities across varying input lengths.\nCode is available at https://github.com/PKU-ML/InvICL."}
{"id": "2505.05016", "pdf": "https://arxiv.org/pdf/2505.05016.pdf", "abs": "https://arxiv.org/abs/2505.05016", "title": "The Pitfalls of Growing Group Complexity: LLMs and Social Choice-Based Aggregation for Group Recommendations", "authors": ["Cedric Waterschoot", "Nava Tintarev", "Francesco Barile"], "categories": ["cs.CL", "cs.IR"], "comment": "To be published in: Adjunct Proceedings of the 33rd ACM Conference on\n  User Modeling, Adaptation and Personalization (UMAP Adjunct '25), June\n  16--19, 2025, New York City, NY, USA Accepted at the 4th Workshop on Group\n  Modeling, Adaptation and Personalization (GMAP), co-located at UMAP 2025", "summary": "Large Language Models (LLMs) are increasingly applied in recommender systems\naimed at both individuals and groups. Previously, Group Recommender Systems\n(GRS) often used social choice-based aggregation strategies to derive a single\nrecommendation based on the preferences of multiple people. In this paper, we\ninvestigate under which conditions language models can perform these strategies\ncorrectly based on zero-shot learning and analyse whether the formatting of the\ngroup scenario in the prompt affects accuracy. We specifically focused on the\nimpact of group complexity (number of users and items), different LLMs,\ndifferent prompting conditions, including In-Context learning or generating\nexplanations, and the formatting of group preferences. Our results show that\nperformance starts to deteriorate when considering more than 100 ratings.\nHowever, not all language models were equally sensitive to growing group\ncomplexity. Additionally, we showed that In-Context Learning (ICL) can\nsignificantly increase the performance at higher degrees of group complexity,\nwhile adding other prompt modifications, specifying domain cues or prompting\nfor explanations, did not impact accuracy. We conclude that future research\nshould include group complexity as a factor in GRS evaluation due to its effect\non LLM performance. Furthermore, we showed that formatting the group scenarios\ndifferently, such as rating lists per user or per item, affected accuracy. All\nin all, our study implies that smaller LLMs are capable of generating group\nrecommendations under the right conditions, making the case for using smaller\nmodels that require less computing power and costs."}
{"id": "2505.05017", "pdf": "https://arxiv.org/pdf/2505.05017.pdf", "abs": "https://arxiv.org/abs/2505.05017", "title": "Scalable Multi-Stage Influence Function for Large Language Models via Eigenvalue-Corrected Kronecker-Factored Parameterization", "authors": ["Yuntai Bao", "Xuhong Zhang", "Tianyu Du", "Xinkui Zhao", "Jiang Zong", "Hao Peng", "Jianwei Yin"], "categories": ["cs.CL"], "comment": "9 pages, accepted by IJCAI 2025", "summary": "Pre-trained large language models (LLMs) are commonly fine-tuned to adapt to\ndownstream tasks. Since the majority of knowledge is acquired during\npre-training, attributing the predictions of fine-tuned LLMs to their\npre-training data may provide valuable insights. Influence functions have been\nproposed as a means to explain model predictions based on training data.\nHowever, existing approaches fail to compute ``multi-stage'' influence and lack\nscalability to billion-scale LLMs.\n  In this paper, we propose the multi-stage influence function to attribute the\ndownstream predictions of fine-tuned LLMs to pre-training data under the\nfull-parameter fine-tuning paradigm. To enhance the efficiency and practicality\nof our multi-stage influence function, we leverage Eigenvalue-corrected\nKronecker-Factored (EK-FAC) parameterization for efficient approximation.\nEmpirical results validate the superior scalability of EK-FAC approximation and\nthe effectiveness of our multi-stage influence function. Additionally, case\nstudies on a real-world LLM, dolly-v2-3b, demonstrate its interpretive power,\nwith exemplars illustrating insights provided by multi-stage influence\nestimates. Our code is public at\nhttps://github.com/colored-dye/multi_stage_influence_function."}
{"id": "2505.05026", "pdf": "https://arxiv.org/pdf/2505.05026.pdf", "abs": "https://arxiv.org/abs/2505.05026", "title": "G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness", "authors": ["Jaehyun Jeon", "Janghan Yoon", "Minsoo Kim", "Sumin Shim", "Yejin Choi", "Hanbin Kim", "Youngjae Yu"], "categories": ["cs.CL", "cs.LG"], "comment": "31 pages, 17 figures", "summary": "Evaluating user interface (UI) design effectiveness extends beyond aesthetics\nto influencing user behavior, a principle central to Design Persuasiveness. A/B\ntesting is the predominant method for determining which UI variations drive\nhigher user engagement, but it is costly and time-consuming. While recent\nVision-Language Models (VLMs) can process automated UI analysis, current\napproaches focus on isolated design attributes rather than comparative\npersuasiveness-the key factor in optimizing user interactions. To address this,\nwe introduce WiserUI-Bench, a benchmark designed for Pairwise UI Design\nPersuasiveness Assessment task, featuring 300 real-world UI image pairs labeled\nwith A/B test results and expert rationales. Additionally, we propose G-FOCUS,\na novel inference-time reasoning strategy that enhances VLM-based\npersuasiveness assessment by reducing position bias and improving evaluation\naccuracy. Experimental results show that G-FOCUS surpasses existing inference\nstrategies in consistency and accuracy for pairwise UI evaluation. Through\npromoting VLM-driven evaluation of UI persuasiveness, our work offers an\napproach to complement A/B testing, propelling progress in scalable UI\npreference modeling and design optimization. Code and data will be released\npublicly."}
{"id": "2505.05040", "pdf": "https://arxiv.org/pdf/2505.05040.pdf", "abs": "https://arxiv.org/abs/2505.05040", "title": "Image-Text Relation Prediction for Multilingual Tweets", "authors": ["Matīss Rikters", "Edison Marrese-Taylor"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Various social networks have been allowing media uploads for over a decade\nnow. Still, it has not always been clear what is their relation with the posted\ntext or even if there is any at all. In this work, we explore how multilingual\nvision-language models tackle the task of image-text relation prediction in\ndifferent languages, and construct a dedicated balanced benchmark data set from\nTwitter posts in Latvian along with their manual translations into English. We\ncompare our results to previous work and show that the more recently released\nvision-language model checkpoints are becoming increasingly capable at this\ntask, but there is still much room for further improvement."}
{"id": "2505.05056", "pdf": "https://arxiv.org/pdf/2505.05056.pdf", "abs": "https://arxiv.org/abs/2505.05056", "title": "Teochew-Wild: The First In-the-wild Teochew Dataset with Orthographic Annotations", "authors": ["Linrong Pan", "Chenglong Jiang", "Gaoze Hou", "Ying Gao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper reports the construction of the Teochew-Wild, a speech corpus of\nthe Teochew dialect. The corpus includes 18.9 hours of in-the-wild Teochew\nspeech data from multiple speakers, covering both formal and colloquial\nexpressions, with precise orthographic and pinyin annotations. Additionally, we\nprovide supplementary text processing tools and resources to propel research\nand applications in speech tasks for this low-resource language, such as\nautomatic speech recognition (ASR) and text-to-speech (TTS). To the best of our\nknowledge, this is the first publicly available Teochew dataset with accurate\northographic annotations. We conduct experiments on the corpus, and the results\nvalidate its effectiveness in ASR and TTS tasks."}
{"id": "2505.05070", "pdf": "https://arxiv.org/pdf/2505.05070.pdf", "abs": "https://arxiv.org/abs/2505.05070", "title": "Performance Evaluation of Large Language Models in Bangla Consumer Health Query Summarization", "authors": ["Ajwad Abrar", "Farzana Tabassum", "Sabbir Ahmed"], "categories": ["cs.CL"], "comment": null, "summary": "Consumer Health Queries (CHQs) in Bengali (Bangla), a low-resource language,\noften contain extraneous details, complicating efficient medical responses.\nThis study investigates the zero-shot performance of nine advanced large\nlanguage models (LLMs): GPT-3.5-Turbo, GPT-4, Claude-3.5-Sonnet,\nLlama3-70b-Instruct, Mixtral-8x22b-Instruct, Gemini-1.5-Pro,\nQwen2-72b-Instruct, Gemma-2-27b, and Athene-70B, in summarizing Bangla CHQs.\nUsing the BanglaCHQ-Summ dataset comprising 2,350 annotated query-summary\npairs, we benchmarked these LLMs using ROUGE metrics against Bangla T5, a\nfine-tuned state-of-the-art model. Mixtral-8x22b-Instruct emerged as the top\nperforming model in ROUGE-1 and ROUGE-L, while Bangla T5 excelled in ROUGE-2.\nThe results demonstrate that zero-shot LLMs can rival fine-tuned models,\nachieving high-quality summaries even without task-specific training. This work\nunderscores the potential of LLMs in addressing challenges in low-resource\nlanguages, providing scalable solutions for healthcare query summarization."}
{"id": "2505.05084", "pdf": "https://arxiv.org/pdf/2505.05084.pdf", "abs": "https://arxiv.org/abs/2505.05084", "title": "Reliably Bounding False Positives: A Zero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal Prediction", "authors": ["Xiaowei Zhu", "Yubing Ren", "Yanan Cao", "Xixun Lin", "Fang Fang", "Yangxi Li"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of large language models has raised significant\nconcerns regarding their potential misuse by malicious actors. As a result,\ndeveloping effective detectors to mitigate these risks has become a critical\npriority. However, most existing detection methods focus excessively on\ndetection accuracy, often neglecting the societal risks posed by high false\npositive rates (FPRs). This paper addresses this issue by leveraging Conformal\nPrediction (CP), which effectively constrains the upper bound of FPRs. While\ndirectly applying CP constrains FPRs, it also leads to a significant reduction\nin detection performance. To overcome this trade-off, this paper proposes a\nZero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal\nPrediction (MCP), which both enforces the FPR constraint and improves detection\nperformance. This paper also introduces RealDet, a high-quality dataset that\nspans a wide range of domains, ensuring realistic calibration and enabling\nsuperior detection performance when combined with MCP. Empirical evaluations\ndemonstrate that MCP effectively constrains FPRs, significantly enhances\ndetection performance, and increases robustness against adversarial attacks\nacross multiple detectors and datasets."}
{"id": "2505.05111", "pdf": "https://arxiv.org/pdf/2505.05111.pdf", "abs": "https://arxiv.org/abs/2505.05111", "title": "Unveiling Language-Specific Features in Large Language Models via Sparse Autoencoders", "authors": ["Boyi Deng", "Yu Wan", "Yidan Zhang", "Baosong Yang", "Fuli Feng"], "categories": ["cs.CL"], "comment": null, "summary": "The mechanisms behind multilingual capabilities in Large Language Models\n(LLMs) have been examined using neuron-based or internal-activation-based\nmethods. However, these methods often face challenges such as superposition and\nlayer-wise activation variance, which limit their reliability. Sparse\nAutoencoders (SAEs) offer a more nuanced analysis by decomposing the\nactivations of LLMs into sparse linear combination of SAE features. We\nintroduce a novel metric to assess the monolinguality of features obtained from\nSAEs, discovering that some features are strongly related to specific\nlanguages. Additionally, we show that ablating these SAE features only\nsignificantly reduces abilities in one language of LLMs, leaving others almost\nunaffected. Interestingly, we find some languages have multiple synergistic SAE\nfeatures, and ablating them together yields greater improvement than ablating\nindividually. Moreover, we leverage these SAE-derived language-specific\nfeatures to enhance steering vectors, achieving control over the language\ngenerated by LLMs."}
{"id": "2505.05148", "pdf": "https://arxiv.org/pdf/2505.05148.pdf", "abs": "https://arxiv.org/abs/2505.05148", "title": "A Benchmark Dataset and a Framework for Urdu Multimodal Named Entity Recognition", "authors": ["Hussain Ahmad", "Qingyang Zeng", "Jing Wan"], "categories": ["cs.CL"], "comment": "16 pages, 5 figures. Preprint", "summary": "The emergence of multimodal content, particularly text and images on social\nmedia, has positioned Multimodal Named Entity Recognition (MNER) as an\nincreasingly important area of research within Natural Language Processing.\nDespite progress in high-resource languages such as English, MNER remains\nunderexplored for low-resource languages like Urdu. The primary challenges\ninclude the scarcity of annotated multimodal datasets and the lack of\nstandardized baselines. To address these challenges, we introduce the U-MNER\nframework and release the Twitter2015-Urdu dataset, a pioneering resource for\nUrdu MNER. Adapted from the widely used Twitter2015 dataset, it is annotated\nwith Urdu-specific grammar rules. We establish benchmark baselines by\nevaluating both text-based and multimodal models on this dataset, providing\ncomparative analyses to support future research on Urdu MNER. The U-MNER\nframework integrates textual and visual context using Urdu-BERT for text\nembeddings and ResNet for visual feature extraction, with a Cross-Modal Fusion\nModule to align and fuse information. Our model achieves state-of-the-art\nperformance on the Twitter2015-Urdu dataset, laying the groundwork for further\nMNER research in low-resource languages."}
{"id": "2505.05225", "pdf": "https://arxiv.org/pdf/2505.05225.pdf", "abs": "https://arxiv.org/abs/2505.05225", "title": "QualBench: Benchmarking Chinese LLMs with Localized Professional Qualifications for Vertical Domain Evaluation", "authors": ["Mengze Hong", "Wailing Ng", "Di Jiang", "Chen Jason Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of Chinese large language models (LLMs) underscores the\nneed for domain-specific evaluations to ensure reliable applications. However,\nexisting benchmarks often lack coverage in vertical domains and offer limited\ninsights into the Chinese working context. Leveraging qualification exams as a\nunified framework for human expertise evaluation, we introduce QualBench, the\nfirst multi-domain Chinese QA benchmark dedicated to localized assessment of\nChinese LLMs. The dataset includes over 17,000 questions across six vertical\ndomains, with data selections grounded in 24 Chinese qualifications to closely\nalign with national policies and working standards. Through comprehensive\nevaluation, the Qwen2.5 model outperformed the more advanced GPT-4o, with\nChinese LLMs consistently surpassing non-Chinese models, highlighting the\nimportance of localized domain knowledge in meeting qualification requirements.\nThe best performance of 75.26% reveals the current gaps in domain coverage\nwithin model capabilities. Furthermore, we present the failure of LLM\ncollaboration with crowdsourcing mechanisms and suggest the opportunities for\nmulti-domain RAG knowledge enhancement and vertical domain LLM training with\nFederated Learning."}
{"id": "2505.05271", "pdf": "https://arxiv.org/pdf/2505.05271.pdf", "abs": "https://arxiv.org/abs/2505.05271", "title": "T-T: Table Transformer for Tagging-based Aspect Sentiment Triplet Extraction", "authors": ["Kun Peng", "Chaodong Tong", "Cong Cao", "Hao Peng", "Qian Li", "Guanlin Wu", "Lei Jiang", "Yanbing Liu", "Philip S. Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by IJCAI2025", "summary": "Aspect sentiment triplet extraction (ASTE) aims to extract triplets composed\nof aspect terms, opinion terms, and sentiment polarities from given sentences.\nThe table tagging method is a popular approach to addressing this task, which\nencodes a sentence into a 2-dimensional table, allowing for the tagging of\nrelations between any two words. Previous efforts have focused on designing\nvarious downstream relation learning modules to better capture interactions\nbetween tokens in the table, revealing that a stronger capability to capture\nrelations can lead to greater improvements in the model. Motivated by this, we\nattempt to directly utilize transformer layers as downstream relation learning\nmodules. Due to the powerful semantic modeling capability of transformers, it\nis foreseeable that this will lead to excellent improvement. However, owing to\nthe quadratic relation between the length of the table and the length of the\ninput sentence sequence, using transformers directly faces two challenges:\noverly long table sequences and unfair local attention interaction. To address\nthese challenges, we propose a novel Table-Transformer (T-T) for the\ntagging-based ASTE method. Specifically, we introduce a stripe attention\nmechanism with a loop-shift strategy to tackle these challenges. The former\nmodifies the global attention mechanism to only attend to a 2-dimensional local\nattention window, while the latter facilitates interaction between different\nattention windows. Extensive and comprehensive experiments demonstrate that the\nT-T, as a downstream relation learning module, achieves state-of-the-art\nperformance with lower computational costs."}
{"id": "2505.05298", "pdf": "https://arxiv.org/pdf/2505.05298.pdf", "abs": "https://arxiv.org/abs/2505.05298", "title": "Toward Reasonable Parrots: Why Large Language Models Should Argue with Us by Design", "authors": ["Elena Musi", "Nadin Kokciyan", "Khalid Al-Khatib", "Davide Ceolin", "Emmanuelle Dietz", "Klara Gutekunst", "Annette Hautli-Janisz", "Cristian Manuel Santibañez Yañez", "Jodi Schneider", "Jonas Scholz", "Cor Steging", "Jacky Visser", "Henning Wachsmuth"], "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "In this position paper, we advocate for the development of conversational\ntechnology that is inherently designed to support and facilitate argumentative\nprocesses. We argue that, at present, large language models (LLMs) are\ninadequate for this purpose, and we propose an ideal technology design aimed at\nenhancing argumentative skills. This involves re-framing LLMs as tools to\nexercise our critical thinking rather than replacing them. We introduce the\nconcept of 'reasonable parrots' that embody the fundamental principles of\nrelevance, responsibility, and freedom, and that interact through argumentative\ndialogical moves. These principles and moves arise out of millennia of work in\nargumentation theory and should serve as the starting point for LLM-based\ntechnology that incorporates basic principles of argumentation."}
{"id": "2505.05327", "pdf": "https://arxiv.org/pdf/2505.05327.pdf", "abs": "https://arxiv.org/abs/2505.05327", "title": "ICon: In-Context Contribution for Automatic Data Selection", "authors": ["Yixin Yang", "Qingxiu Dong", "Linli Yao", "Fangwei Zhu", "Zhifang Sui"], "categories": ["cs.CL"], "comment": null, "summary": "Data selection for instruction tuning is essential for improving the\nperformance of Large Language Models (LLMs) and reducing training cost.\nHowever, existing automated selection methods either depend on computationally\nexpensive gradient-based measures or manually designed heuristics, which may\nfail to fully exploit the intrinsic attributes of data. In this paper, we\npropose In-context Learning for Contribution Measurement (ICon), a novel\ngradient-free method that takes advantage of the implicit fine-tuning nature of\nin-context learning (ICL) to measure sample contribution without gradient\ncomputation or manual indicators engineering. ICon offers a computationally\nefficient alternative to gradient-based methods and reduces human inductive\nbias inherent in heuristic-based approaches. ICon comprises three components\nand identifies high-contribution data by assessing performance shifts under\nimplicit learning through ICL. Extensive experiments on three LLMs across 12\nbenchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of\nICon. Remarkably, on LLaMA3.1-8B, models trained on 15% of ICon-selected data\noutperform full datasets by 5.42% points and exceed the best performance of\nwidely used selection methods by 2.06% points. We further analyze\nhigh-contribution samples selected by ICon, which show both diverse tasks and\nappropriate difficulty levels, rather than just the hardest ones."}
{"id": "2505.05406", "pdf": "https://arxiv.org/pdf/2505.05406.pdf", "abs": "https://arxiv.org/abs/2505.05406", "title": "Frame In, Frame Out: Do LLMs Generate More Biased News Headlines than Humans?", "authors": ["Valeria Pastorino", "Nafise Sadat Moosavi"], "categories": ["cs.CL"], "comment": null, "summary": "Framing in media critically shapes public perception by selectively\nemphasizing some details while downplaying others. With the rise of large\nlanguage models in automated news and content creation, there is growing\nconcern that these systems may introduce or even amplify framing biases\ncompared to human authors. In this paper, we explore how framing manifests in\nboth out-of-the-box and fine-tuned LLM-generated news content. Our analysis\nreveals that, particularly in politically and socially sensitive contexts, LLMs\ntend to exhibit more pronounced framing than their human counterparts. In\naddition, we observe significant variation in framing tendencies across\ndifferent model architectures, with some models displaying notably higher\nbiases. These findings point to the need for effective post-training mitigation\nstrategies and tighter evaluation frameworks to ensure that automated news\ncontent upholds the standards of balanced reporting."}
{"id": "2505.05408", "pdf": "https://arxiv.org/pdf/2505.05408.pdf", "abs": "https://arxiv.org/abs/2505.05408", "title": "Crosslingual Reasoning through Test-Time Scaling", "authors": ["Zheng-Xin Yong", "M. Farid Adilazuarda", "Jonibek Mansurov", "Ruochen Zhang", "Niklas Muennighoff", "Carsten Eickhoff", "Genta Indra Winata", "Julia Kreutzer", "Stephen H. Bach", "Alham Fikri Aji"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Reasoning capabilities of large language models are primarily studied for\nEnglish, even when pretrained models are multilingual. In this work, we\ninvestigate to what extent English reasoning finetuning with long\nchain-of-thoughts (CoTs) can generalize across languages. First, we find that\nscaling up inference compute for English-centric reasoning language models\n(RLMs) improves multilingual mathematical reasoning across many languages\nincluding low-resource languages, to an extent where they outperform models\ntwice their size. Second, we reveal that while English-centric RLM's CoTs are\nnaturally predominantly English, they consistently follow a quote-and-think\npattern to reason about quoted non-English inputs. Third, we discover an\neffective strategy to control the language of long CoT reasoning, and we\nobserve that models reason better and more efficiently in high-resource\nlanguages. Finally, we observe poor out-of-domain reasoning generalization, in\nparticular from STEM to cultural commonsense knowledge, even for English.\nOverall, we demonstrate the potentials, study the mechanisms and outline the\nlimitations of crosslingual generalization of English reasoning test-time\nscaling. We conclude that practitioners should let English-centric RLMs reason\nin high-resource languages, while further work is needed to improve reasoning\nin low-resource languages and out-of-domain contexts."}
{"id": "2505.05410", "pdf": "https://arxiv.org/pdf/2505.05410.pdf", "abs": "https://arxiv.org/abs/2505.05410", "title": "Reasoning Models Don't Always Say What They Think", "authors": ["Yanda Chen", "Joe Benton", "Ansh Radhakrishnan", "Jonathan Uesato", "Carson Denison", "John Schulman", "Arushi Somani", "Peter Hase", "Misha Wagner", "Fabien Roger", "Vlad Mikulik", "Samuel R. Bowman", "Jan Leike", "Jared Kaplan", "Ethan Perez"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Chain-of-thought (CoT) offers a potential boon for AI safety as it allows\nmonitoring a model's CoT to try to understand its intentions and reasoning\nprocesses. However, the effectiveness of such monitoring hinges on CoTs\nfaithfully representing models' actual reasoning processes. We evaluate CoT\nfaithfulness of state-of-the-art reasoning models across 6 reasoning hints\npresented in the prompts and find: (1) for most settings and models tested,\nCoTs reveal their usage of hints in at least 1% of examples where they use the\nhint, but the reveal rate is often below 20%, (2) outcome-based reinforcement\nlearning initially improves faithfulness but plateaus without saturating, and\n(3) when reinforcement learning increases how frequently hints are used (reward\nhacking), the propensity to verbalize them does not increase, even without\ntraining against a CoT monitor. These results suggest that CoT monitoring is a\npromising way of noticing undesired behaviors during training and evaluations,\nbut that it is not sufficient to rule them out. They also suggest that in\nsettings like ours where CoT reasoning is not necessary, test-time monitoring\nof CoTs is unlikely to reliably catch rare and catastrophic unexpected\nbehaviors."}
{"id": "2505.05423", "pdf": "https://arxiv.org/pdf/2505.05423.pdf", "abs": "https://arxiv.org/abs/2505.05423", "title": "TransProQA: an LLM-based literary Translation evaluation metric with Professional Question Answering", "authors": ["Ran Zhang", "Wei Zhao", "Lieve Macken", "Steffen Eger"], "categories": ["cs.CL", "cs.AI"], "comment": "WIP", "summary": "The impact of Large Language Models (LLMs) has extended into literary\ndomains. However, existing evaluation metrics prioritize mechanical accuracy\nover artistic expression and tend to overrate machine translation (MT) as being\nsuperior to experienced professional human translation. In the long run, this\nbias could result in a permanent decline in translation quality and cultural\nauthenticity. In response to the urgent need for a specialized literary\nevaluation metric, we introduce TransProQA, a novel, reference-free, LLM-based\nquestion-answering (QA) framework designed specifically for literary\ntranslation evaluation. TransProQA uniquely integrates insights from\nprofessional literary translators and researchers, focusing on critical\nelements in literary quality assessment such as literary devices, cultural\nunderstanding, and authorial voice. Our extensive evaluation shows that while\nliterary-finetuned XCOMET-XL yields marginal gains, TransProQA substantially\noutperforms current metrics, achieving up to 0.07 gain in correlation (ACC-EQ\nand Kendall's tau) and surpassing the best state-of-the-art (SOTA) metrics by\nover 15 points in adequacy assessments. Incorporating professional translator\ninsights as weights further improves performance, highlighting the value of\ntranslator inputs. Notably, TransProQA approaches human-level evaluation\nperformance comparable to trained linguistic annotators. It demonstrates broad\napplicability to open-source models such as LLaMA3.3-70b and Qwen2.5-32b,\nindicating its potential as an accessible and training-free literary evaluation\nmetric and a valuable tool for evaluating texts that require local processing\ndue to copyright or ethical considerations."}
{"id": "2505.05427", "pdf": "https://arxiv.org/pdf/2505.05427.pdf", "abs": "https://arxiv.org/abs/2505.05427", "title": "Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data", "authors": ["Yudong Wang", "Zixuan Fu", "Jie Cai", "Peijun Tang", "Hongya Lyu", "Yewei Fang", "Zhi Zheng", "Jie Zhou", "Guoyang Zeng", "Chaojun Xiao", "Xu Han", "Zhiyuan Liu"], "categories": ["cs.CL"], "comment": "The datasets are available on\n  https://huggingface.co/datasets/openbmb/UltraFineWeb", "summary": "Data quality has become a key factor in enhancing model performance with the\nrapid development of large language models (LLMs). Model-driven data filtering\nhas increasingly become a primary approach for acquiring high-quality data.\nHowever, it still faces two main challenges: (1) the lack of an efficient data\nverification strategy makes it difficult to provide timely feedback on data\nquality; and (2) the selection of seed data for training classifiers lacks\nclear criteria and relies heavily on human expertise, introducing a degree of\nsubjectivity. To address the first challenge, we introduce an efficient\nverification strategy that enables rapid evaluation of the impact of data on\nLLM training with minimal computational cost. To tackle the second challenge,\nwe build upon the assumption that high-quality seed data is beneficial for LLM\ntraining, and by integrating the proposed verification strategy, we optimize\nthe selection of positive and negative samples and propose an efficient data\nfiltering pipeline. This pipeline not only improves filtering efficiency,\nclassifier quality, and robustness, but also significantly reduces experimental\nand inference costs. In addition, to efficiently filter high-quality data, we\nemploy a lightweight classifier based on fastText, and successfully apply the\nfiltering pipeline to two widely-used pre-training corpora, FineWeb and Chinese\nFineWeb datasets, resulting in the creation of the higher-quality Ultra-FineWeb\ndataset. Ultra-FineWeb contains approximately 1 trillion English tokens and 120\nbillion Chinese tokens. Empirical results demonstrate that the LLMs trained on\nUltra-FineWeb exhibit significant performance improvements across multiple\nbenchmark tasks, validating the effectiveness of our pipeline in enhancing both\ndata quality and training efficiency."}
{"id": "2505.05445", "pdf": "https://arxiv.org/pdf/2505.05445.pdf", "abs": "https://arxiv.org/abs/2505.05445", "title": "clem:todd: A Framework for the Systematic Benchmarking of LLM-Based Task-Oriented Dialogue System Realisations", "authors": ["Chalamalasetti Kranti", "Sherzod Hakimov", "David Schlangen"], "categories": ["cs.CL"], "comment": "30 pages", "summary": "The emergence of instruction-tuned large language models (LLMs) has advanced\nthe field of dialogue systems, enabling both realistic user simulations and\nrobust multi-turn conversational agents. However, existing research often\nevaluates these components in isolation-either focusing on a single user\nsimulator or a specific system design-limiting the generalisability of insights\nacross architectures and configurations. In this work, we propose clem todd\n(chat-optimized LLMs for task-oriented dialogue systems development), a\nflexible framework for systematically evaluating dialogue systems under\nconsistent conditions. clem todd enables detailed benchmarking across\ncombinations of user simulators and dialogue systems, whether existing models\nfrom literature or newly developed ones. It supports plug-and-play integration\nand ensures uniform datasets, evaluation metrics, and computational\nconstraints. We showcase clem todd's flexibility by re-evaluating existing\ntask-oriented dialogue systems within this unified setup and integrating three\nnewly proposed dialogue systems into the same evaluation pipeline. Our results\nprovide actionable insights into how architecture, scale, and prompting\nstrategies affect dialogue performance, offering practical guidance for\nbuilding efficient and effective conversational AI systems."}
{"id": "2505.05459", "pdf": "https://arxiv.org/pdf/2505.05459.pdf", "abs": "https://arxiv.org/abs/2505.05459", "title": "UKElectionNarratives: A Dataset of Misleading Narratives Surrounding Recent UK General Elections", "authors": ["Fatima Haouari", "Carolina Scarton", "Nicolò Faggiani", "Nikolaos Nikolaidis", "Bonka Kotseva", "Ibrahim Abu Farha", "Jens Linge", "Kalina Bontcheva"], "categories": ["cs.CL", "cs.SI"], "comment": "This work was accepted at the International AAAI Conference on Web\n  and Social Media (ICWSM 2025)", "summary": "Misleading narratives play a crucial role in shaping public opinion during\nelections, as they can influence how voters perceive candidates and political\nparties. This entails the need to detect these narratives accurately. To\naddress this, we introduce the first taxonomy of common misleading narratives\nthat circulated during recent elections in Europe. Based on this taxonomy, we\nconstruct and analyse UKElectionNarratives: the first dataset of\nhuman-annotated misleading narratives which circulated during the UK General\nElections in 2019 and 2024. We also benchmark Pre-trained and Large Language\nModels (focusing on GPT-4o), studying their effectiveness in detecting\nelection-related misleading narratives. Finally, we discuss potential use cases\nand make recommendations for future research directions using the proposed\ncodebook and dataset."}
{"id": "2505.05464", "pdf": "https://arxiv.org/pdf/2505.05464.pdf", "abs": "https://arxiv.org/abs/2505.05464", "title": "Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging", "authors": ["Shiqi Chen", "Jinghan Zhang", "Tongyao Zhu", "Wei Liu", "Siyang Gao", "Miao Xiong", "Manling Li", "Junxian He"], "categories": ["cs.CL"], "comment": "ICML 2025. Our code is publicly available at\n  https://github.com/shiqichen17/VLM_Merging", "summary": "Vision-Language Models (VLMs) combine visual perception with the general\ncapabilities, such as reasoning, of Large Language Models (LLMs). However, the\nmechanisms by which these two abilities can be combined and contribute remain\npoorly understood. In this work, we explore to compose perception and reasoning\nthrough model merging that connects parameters of different models. Unlike\nprevious works that often focus on merging models of the same kind, we propose\nmerging models across modalities, enabling the incorporation of the reasoning\ncapabilities of LLMs into VLMs. Through extensive experiments, we demonstrate\nthat model merging offers a successful pathway to transfer reasoning abilities\nfrom LLMs to VLMs in a training-free manner. Moreover, we utilize the merged\nmodels to understand the internal mechanism of perception and reasoning and how\nmerging affects it. We find that perception capabilities are predominantly\nencoded in the early layers of the model, whereas reasoning is largely\nfacilitated by the middle-to-late layers. After merging, we observe that all\nlayers begin to contribute to reasoning, whereas the distribution of perception\nabilities across layers remains largely unchanged. These observations shed\nlight on the potential of model merging as a tool for multimodal integration\nand interpretation."}
{"id": "2505.05465", "pdf": "https://arxiv.org/pdf/2505.05465.pdf", "abs": "https://arxiv.org/abs/2505.05465", "title": "ComPO: Preference Alignment via Comparison Oracles", "authors": ["Peter Chen", "Xi Chen", "Wotao Yin", "Tianyi Lin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "25 pages", "summary": "Direct alignment methods are increasingly used for aligning large language\nmodels (LLMs) with human preferences. However, these methods suffer from the\nissues of verbosity and likelihood displacement, which can be driven by the\nnoisy preference pairs that induce similar likelihood for preferred and\ndispreferred responses. The contributions of this paper are two-fold. First, we\npropose a new preference alignment method based on comparison oracles and\nprovide the convergence guarantee for its basic scheme. Second, we improve our\nmethod using some heuristics and conduct the experiments to demonstrate the\nflexibility and compatibility of practical scheme in improving the performance\nof LLMs using noisy preference pairs. Evaluations are conducted across multiple\nbase and instruction-tuned models (Mistral-7B, Llama-3-8B and Gemma-2-9B) with\nbenchmarks (AlpacaEval 2, MT-Bench and Arena-Hard). Experimental results show\nthe effectiveness of our method as an alternative to addressing the limitations\nof existing direct alignment methods. A highlight of our work is that we\nevidence the importance of designing specialized methods for preference pairs\nwith distinct likelihood margin, which complements the recent findings in\n\\citet{Razin-2025-Unintentional}."}
{"id": "2505.04629", "pdf": "https://arxiv.org/pdf/2505.04629.pdf", "abs": "https://arxiv.org/abs/2505.04629", "title": "From Dialect Gaps to Identity Maps: Tackling Variability in Speaker Verification", "authors": ["Abdulhady Abas Abdullah", "Soran Badawi", "Dana A. Abdullah", "Dana Rasul Hamad", "Hanan Abdulrahman Taher", "Sabat Salih Muhamad", "Aram Mahmood Ahmed", "Bryar A. Hassan", "Sirwan Abdolwahed Aula", "Tarik A. Rashid"], "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": null, "summary": "The complexity and difficulties of Kurdish speaker detection among its\nseveral dialects are investigated in this work. Because of its great phonetic\nand lexical differences, Kurdish with several dialects including Kurmanji,\nSorani, and Hawrami offers special challenges for speaker recognition systems.\nThe main difficulties in building a strong speaker identification system\ncapable of precisely identifying speakers across several dialects are\ninvestigated in this work. To raise the accuracy and dependability of these\nsystems, it also suggests solutions like sophisticated machine learning\napproaches, data augmentation tactics, and the building of thorough\ndialect-specific corpus. The results show that customized strategies for every\ndialect together with cross-dialect training greatly enhance recognition\nperformance."}
{"id": "2505.04638", "pdf": "https://arxiv.org/pdf/2505.04638.pdf", "abs": "https://arxiv.org/abs/2505.04638", "title": "Towards Artificial Intelligence Research Assistant for Expert-Involved Learning", "authors": ["Tianyu Liu", "Simeng Han", "Xiao Luo", "Hanchen Wang", "Pan Lu", "Biqing Zhu", "Yuge Wang", "Keyi Li", "Jiapeng Chen", "Rihao Qu", "Yufeng Liu", "Xinyue Cui", "Aviv Yaish", "Yuhang Chen", "Minsheng Hao", "Chuhan Li", "Kexing Li", "Arman Cohan", "Hua Xu", "Mark Gerstein", "James Zou", "Hongyu Zhao"], "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": "36 pages, 7 figures", "summary": "Large Language Models (LLMs) and Large Multi-Modal Models (LMMs) have emerged\nas transformative tools in scientific research, yet their reliability and\nspecific contributions to biomedical applications remain insufficiently\ncharacterized. In this study, we present \\textbf{AR}tificial\n\\textbf{I}ntelligence research assistant for \\textbf{E}xpert-involved\n\\textbf{L}earning (ARIEL), a multimodal dataset designed to benchmark and\nenhance two critical capabilities of LLMs and LMMs in biomedical research:\nsummarizing extensive scientific texts and interpreting complex biomedical\nfigures. To facilitate rigorous assessment, we create two open-source sets\ncomprising biomedical articles and figures with designed questions. We\nsystematically benchmark both open- and closed-source foundation models,\nincorporating expert-driven human evaluations conducted by doctoral-level\nexperts. Furthermore, we improve model performance through targeted prompt\nengineering and fine-tuning strategies for summarizing research papers, and\napply test-time computational scaling to enhance the reasoning capabilities of\nLMMs, achieving superior accuracy compared to human-expert corrections. We also\nexplore the potential of using LMM Agents to generate scientific hypotheses\nfrom diverse multimodal inputs. Overall, our results delineate clear strengths\nand highlight significant limitations of current foundation models, providing\nactionable insights and guiding future advancements in deploying large-scale\nlanguage and multi-modal models within biomedical research."}
{"id": "2505.04741", "pdf": "https://arxiv.org/pdf/2505.04741.pdf", "abs": "https://arxiv.org/abs/2505.04741", "title": "When Bad Data Leads to Good Models", "authors": ["Kenneth Li", "Yida Chen", "Fernanda Viégas", "Martin Wattenberg"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML 2025", "summary": "In large language model (LLM) pretraining, data quality is believed to\ndetermine model quality. In this paper, we re-examine the notion of \"quality\"\nfrom the perspective of pre- and post-training co-design. Specifically, we\nexplore the possibility that pre-training on more toxic data can lead to better\ncontrol in post-training, ultimately decreasing a model's output toxicity.\nFirst, we use a toy experiment to study how data composition affects the\ngeometry of features in the representation space. Next, through controlled\nexperiments with Olmo-1B models trained on varying ratios of clean and toxic\ndata, we find that the concept of toxicity enjoys a less entangled linear\nrepresentation as the proportion of toxic data increases. Furthermore, we show\nthat although toxic data increases the generational toxicity of the base model,\nit also makes the toxicity easier to remove. Evaluations on Toxigen and Real\nToxicity Prompts demonstrate that models trained on toxic data achieve a better\ntrade-off between reducing generational toxicity and preserving general\ncapabilities when detoxifying techniques such as inference-time intervention\n(ITI) are applied. Our findings suggest that, with post-training taken into\naccount, bad data may lead to good models."}
{"id": "2505.04806", "pdf": "https://arxiv.org/pdf/2505.04806.pdf", "abs": "https://arxiv.org/abs/2505.04806", "title": "Red Teaming the Mind of the Machine: A Systematic Evaluation of Prompt Injection and Jailbreak Vulnerabilities in LLMs", "authors": ["Chetan Pathade"], "categories": ["cs.CR", "cs.CL"], "comment": "7 Pages, 6 Figures", "summary": "Large Language Models (LLMs) are increasingly integrated into consumer and\nenterprise applications. Despite their capabilities, they remain susceptible to\nadversarial attacks such as prompt injection and jailbreaks that override\nalignment safeguards. This paper provides a systematic investigation of\njailbreak strategies against various state-of-the-art LLMs. We categorize over\n1,400 adversarial prompts, analyze their success against GPT-4, Claude 2,\nMistral 7B, and Vicuna, and examine their generalizability and construction\nlogic. We further propose layered mitigation strategies and recommend a hybrid\nred-teaming and sandboxing approach for robust LLM security."}
{"id": "2505.04846", "pdf": "https://arxiv.org/pdf/2505.04846.pdf", "abs": "https://arxiv.org/abs/2505.04846", "title": "HiPerRAG: High-Performance Retrieval Augmented Generation for Scientific Insights", "authors": ["Ozan Gokdemir", "Carlo Siebenschuh", "Alexander Brace", "Azton Wells", "Brian Hsu", "Kyle Hippe", "Priyanka V. Setty", "Aswathy Ajith", "J. Gregory Pauloski", "Varuni Sastry", "Sam Foreman", "Huihuo Zheng", "Heng Ma", "Bharat Kale", "Nicholas Chia", "Thomas Gibbs", "Michael E. Papka", "Thomas Brettin", "Francis J. Alexander", "Anima Anandkumar", "Ian Foster", "Rick Stevens", "Venkatram Vishwanath", "Arvind Ramanathan"], "categories": ["cs.IR", "cs.CE", "cs.CL", "cs.DC", "cs.LG", "H.3.3; I.2.7"], "comment": "This paper has been accepted at the Platform for Advanced Scientific\n  Computing Conference (PASC 25), June 16-18, 2025, Brugg-Windisch, Switzerland", "summary": "The volume of scientific literature is growing exponentially, leading to\nunderutilized discoveries, duplicated efforts, and limited cross-disciplinary\ncollaboration. Retrieval Augmented Generation (RAG) offers a way to assist\nscientists by improving the factuality of Large Language Models (LLMs) in\nprocessing this influx of information. However, scaling RAG to handle millions\nof articles introduces significant challenges, including the high computational\ncosts associated with parsing documents and embedding scientific knowledge, as\nwell as the algorithmic complexity of aligning these representations with the\nnuanced semantics of scientific content. To address these issues, we introduce\nHiPerRAG, a RAG workflow powered by high performance computing (HPC) to index\nand retrieve knowledge from more than 3.6 million scientific articles. At its\ncore are Oreo, a high-throughput model for multimodal document parsing, and\nColTrast, a query-aware encoder fine-tuning algorithm that enhances retrieval\naccuracy by using contrastive learning and late-interaction techniques.\nHiPerRAG delivers robust performance on existing scientific question answering\nbenchmarks and two new benchmarks introduced in this work, achieving 90%\naccuracy on SciQ and 76% on PubMedQA-outperforming both domain-specific models\nlike PubMedGPT and commercial LLMs such as GPT-4. Scaling to thousands of GPUs\non the Polaris, Sunspot, and Frontier supercomputers, HiPerRAG delivers million\ndocument-scale RAG workflows for unifying scientific knowledge and fostering\ninterdisciplinary innovation."}
{"id": "2505.04851", "pdf": "https://arxiv.org/pdf/2505.04851.pdf", "abs": "https://arxiv.org/abs/2505.04851", "title": "CRAFT: Cultural Russian-Oriented Dataset Adaptation for Focused Text-to-Image Generation", "authors": ["Viacheslav Vasilev", "Vladimir Arkhipkin", "Julia Agafonova", "Tatiana Nikulina", "Evelina Mironova", "Alisa Shichanina", "Nikolai Gerasimenko", "Mikhail Shoytov", "Denis Dimitrov"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.CY", "cs.LG"], "comment": "This is arxiv version of the paper which was accepted for the Doklady\n  Mathematics Journal in 2024", "summary": "Despite the fact that popular text-to-image generation models cope well with\ninternational and general cultural queries, they have a significant knowledge\ngap regarding individual cultures. This is due to the content of existing large\ntraining datasets collected on the Internet, which are predominantly based on\nWestern European or American popular culture. Meanwhile, the lack of cultural\nadaptation of the model can lead to incorrect results, a decrease in the\ngeneration quality, and the spread of stereotypes and offensive content. In an\neffort to address this issue, we examine the concept of cultural code and\nrecognize the critical importance of its understanding by modern image\ngeneration models, an issue that has not been sufficiently addressed in the\nresearch community to date. We propose the methodology for collecting and\nprocessing the data necessary to form a dataset based on the cultural code, in\nparticular the Russian one. We explore how the collected data affects the\nquality of generations in the national domain and analyze the effectiveness of\nour approach using the Kandinsky 3.1 text-to-image model. Human evaluation\nresults demonstrate an increase in the level of awareness of Russian culture in\nthe model."}
{"id": "2505.04881", "pdf": "https://arxiv.org/pdf/2505.04881.pdf", "abs": "https://arxiv.org/abs/2505.04881", "title": "ConCISE: Confidence-guided Compression in Step-by-step Efficient Reasoning", "authors": ["Ziqing Qiao", "Yongheng Deng", "Jiali Zeng", "Dong Wang", "Lai Wei", "Fandong Meng", "Jie Zhou", "Ju Ren", "Yaoxue Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) perform strongly in complex reasoning tasks via\nChain-of-Thought (CoT) prompting, but often suffer from verbose outputs caused\nby redundant content, increasing computational overhead, and degrading user\nexperience. Existing compression methods either operate post-hoc pruning,\nrisking disruption to reasoning coherence, or rely on sampling-based selection,\nwhich fails to intervene effectively during generation. In this work, we\nintroduce a confidence-guided perspective to explain the emergence of redundant\nreflection in LRMs, identifying two key patterns: Confidence Deficit, where the\nmodel reconsiders correct steps due to low internal confidence, and Termination\nDelay, where reasoning continues even after reaching a confident answer. Based\non this analysis, we propose ConCISE (Confidence-guided Compression In\nStep-by-step Efficient Reasoning), a framework that simplifies reasoning chains\nby reinforcing the model's confidence during inference, thus preventing the\ngeneration of redundant reflection steps. It integrates Confidence Injection to\nstabilize intermediate steps and Early Stopping to terminate reasoning when\nconfidence is sufficient. Extensive experiments demonstrate that fine-tuning\nLRMs on ConCISE-generated data yields significantly shorter outputs, reducing\nlength by up to approximately 50% under SimPO, while maintaining high task\naccuracy. ConCISE consistently outperforms existing baselines across multiple\nreasoning benchmarks."}
{"id": "2505.04911", "pdf": "https://arxiv.org/pdf/2505.04911.pdf", "abs": "https://arxiv.org/abs/2505.04911", "title": "SpatialPrompting: Keyframe-driven Zero-Shot Spatial Reasoning with Off-the-Shelf Multimodal Large Language Models", "authors": ["Shun Taguchi", "Hideki Deguchi", "Takumi Hamazaki", "Hiroyuki Sakai"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "18 pages, 11 figures", "summary": "This study introduces SpatialPrompting, a novel framework that harnesses the\nemergent reasoning capabilities of off-the-shelf multimodal large language\nmodels to achieve zero-shot spatial reasoning in three-dimensional (3D)\nenvironments. Unlike existing methods that rely on expensive 3D-specific\nfine-tuning with specialized 3D inputs such as point clouds or voxel-based\nfeatures, SpatialPrompting employs a keyframe-driven prompt generation\nstrategy. This framework uses metrics such as vision-language similarity,\nMahalanobis distance, field of view, and image sharpness to select a diverse\nand informative set of keyframes from image sequences and then integrates them\nwith corresponding camera pose data to effectively abstract spatial\nrelationships and infer complex 3D structures. The proposed framework not only\nestablishes a new paradigm for flexible spatial reasoning that utilizes\nintuitive visual and positional cues but also achieves state-of-the-art\nzero-shot performance on benchmark datasets, such as ScanQA and SQA3D, across\nseveral metrics. The proposed method effectively eliminates the need for\nspecialized 3D inputs and fine-tuning, offering a simpler and more scalable\nalternative to conventional approaches."}
{"id": "2505.04914", "pdf": "https://arxiv.org/pdf/2505.04914.pdf", "abs": "https://arxiv.org/abs/2505.04914", "title": "Enigme: Generative Text Puzzles for Evaluating Reasoning in Language Models", "authors": ["John Hawkins"], "categories": ["cs.AI", "cs.CL", "I.2.7"], "comment": "To be published in the proceedings of The 2025 11th International\n  Conference on Engineering, Applied Sciences, and Technology (ICEAST)", "summary": "Transformer-decoder language models are a core innovation in text based\ngenerative artificial intelligence. These models are being deployed as\ngeneral-purpose intelligence systems in many applications. Central to their\nutility is the capacity to understand natural language commands and exploit the\nreasoning embedded in human text corpora to apply some form of reasoning\nprocess to a wide variety of novel tasks. To understand the limitations of this\napproach to generating reasoning we argue that we need to consider the\narchitectural constraints of these systems. Consideration of the latent\nvariable structure of transformer-decoder models allows us to design reasoning\ntasks that should probe the boundary of their capacity to reason. We present\nenigme, an open-source library for generating text-based puzzles to be used in\ntraining and evaluating reasoning skills within transformer-decoder models and\nfuture AI architectures."}
{"id": "2505.04921", "pdf": "https://arxiv.org/pdf/2505.04921.pdf", "abs": "https://arxiv.org/abs/2505.04921", "title": "Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models", "authors": ["Yunxin Li", "Zhenyu Liu", "Zitao Li", "Xuanyu Zhang", "Zhenran Xu", "Xinyu Chen", "Haoyuan Shi", "Shenyuan Jiang", "Xintong Wang", "Jifang Wang", "Shouzheng Huang", "Xinping Zhao", "Borui Jiang", "Lanqing Hong", "Longyue Wang", "Zhuotao Tian", "Baoxing Huai", "Wenhan Luo", "Weihua Luo", "Zheng Zhang", "Baotian Hu", "Min Zhang"], "categories": ["cs.CV", "cs.CL"], "comment": "75 Pages,10 figures; Project:\n  https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models", "summary": "Reasoning lies at the heart of intelligence, shaping the ability to make\ndecisions, draw conclusions, and generalize across domains. In artificial\nintelligence, as systems increasingly operate in open, uncertain, and\nmultimodal environments, reasoning becomes essential for enabling robust and\nadaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a\npromising paradigm, integrating modalities such as text, images, audio, and\nvideo to support complex reasoning capabilities and aiming to achieve\ncomprehensive perception, precise understanding, and deep reasoning. As\nresearch advances, multimodal reasoning has rapidly evolved from modular,\nperception-driven pipelines to unified, language-centric frameworks that offer\nmore coherent cross-modal understanding. While instruction tuning and\nreinforcement learning have improved model reasoning, significant challenges\nremain in omni-modal generalization, reasoning depth, and agentic behavior. To\naddress these issues, we present a comprehensive and structured survey of\nmultimodal reasoning research, organized around a four-stage developmental\nroadmap that reflects the field's shifting design philosophies and emerging\ncapabilities. First, we review early efforts based on task-specific modules,\nwhere reasoning was implicitly embedded across stages of representation,\nalignment, and fusion. Next, we examine recent approaches that unify reasoning\ninto multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT)\nand multimodal reinforcement learning enabling richer and more structured\nreasoning chains. Finally, drawing on empirical insights from challenging\nbenchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the\nconceptual direction of native large multimodal reasoning models (N-LMRMs),\nwhich aim to support scalable, agentic, and adaptive reasoning and planning in\ncomplex, real-world environments."}
{"id": "2505.04946", "pdf": "https://arxiv.org/pdf/2505.04946.pdf", "abs": "https://arxiv.org/abs/2505.04946", "title": "T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video Generation Models", "authors": ["Xuyang Guo", "Jiayan Huo", "Zhenmei Shi", "Zhao Song", "Jiahao Zhang", "Jiale Zhao"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Thanks to recent advancements in scalable deep architectures and large-scale\npretraining, text-to-video generation has achieved unprecedented capabilities\nin producing high-fidelity, instruction-following content across a wide range\nof styles, enabling applications in advertising, entertainment, and education.\nHowever, these models' ability to render precise on-screen text, such as\ncaptions or mathematical formulas, remains largely untested, posing significant\nchallenges for applications requiring exact textual accuracy. In this work, we\nintroduce T2VTextBench, the first human-evaluation benchmark dedicated to\nevaluating on-screen text fidelity and temporal consistency in text-to-video\nmodels. Our suite of prompts integrates complex text strings with dynamic scene\nchanges, testing each model's ability to maintain detailed instructions across\nframes. We evaluate ten state-of-the-art systems, ranging from open-source\nsolutions to commercial offerings, and find that most struggle to generate\nlegible, consistent text. These results highlight a critical gap in current\nvideo generators and provide a clear direction for future research aimed at\nenhancing textual manipulation in video synthesis."}
{"id": "2505.04948", "pdf": "https://arxiv.org/pdf/2505.04948.pdf", "abs": "https://arxiv.org/abs/2505.04948", "title": "Prompt-Based LLMs for Position Bias-Aware Reranking in Personalized Recommendations", "authors": ["Md Aminul Islam", "Ahmed Sayeed Faruk"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Recommender systems are essential for delivering personalized content across\ndigital platforms by modeling user preferences and behaviors. Recently, large\nlanguage models (LLMs) have been adopted for prompt-based recommendation due to\ntheir ability to generate personalized outputs without task-specific training.\nHowever, LLM-based methods face limitations such as limited context window\nsize, inefficient pointwise and pairwise prompting, and difficulty handling\nlistwise ranking due to token constraints. LLMs can also be sensitive to\nposition bias, as they may overemphasize earlier items in the prompt regardless\nof their true relevance. To address and investigate these issues, we propose a\nhybrid framework that combines a traditional recommendation model with an LLM\nfor reranking top-k items using structured prompts. We evaluate the effects of\nuser history reordering and instructional prompts for mitigating position bias.\nExperiments on MovieLens-100K show that randomizing user history improves\nranking quality, but LLM-based reranking does not outperform the base model.\nExplicit instructions to reduce position bias are also ineffective. Our\nevaluations reveal limitations in LLMs' ability to model ranking context and\nmitigate bias. Our code is publicly available at\nhttps://github.com/aminul7506/LLMForReRanking."}
{"id": "2505.04969", "pdf": "https://arxiv.org/pdf/2505.04969.pdf", "abs": "https://arxiv.org/abs/2505.04969", "title": "General Transform: A Unified Framework for Adaptive Transform to Enhance Representations", "authors": ["Gekko Budiutama", "Shunsuke Daimon", "Hirofumi Nishi", "Yu-ichiro Matsushita"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Discrete transforms, such as the discrete Fourier transform, are widely used\nin machine learning to improve model performance by extracting meaningful\nfeatures. However, with numerous transforms available, selecting an appropriate\none often depends on understanding the dataset's properties, making the\napproach less effective when such knowledge is unavailable. In this work, we\npropose General Transform (GT), an adaptive transform-based representation\ndesigned for machine learning applications. Unlike conventional transforms, GT\nlearns data-driven mapping tailored to the dataset and task of interest. Here,\nwe demonstrate that models incorporating GT outperform conventional\ntransform-based approaches across computer vision and natural language\nprocessing tasks, highlighting its effectiveness in diverse learning scenarios."}
{"id": "2505.05063", "pdf": "https://arxiv.org/pdf/2505.05063.pdf", "abs": "https://arxiv.org/abs/2505.05063", "title": "CodeMixBench: Evaluating Large Language Models on Code Generation with Code-Mixed Prompts", "authors": ["Manik Sheokand", "Parth Sawant"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success in code\ngeneration tasks, powering various applications like code completion,\ndebugging, and programming assistance. However, existing benchmarks such as\nHumanEval, MBPP, and BigCodeBench primarily evaluate LLMs on English-only\nprompts, overlooking the real-world scenario where multilingual developers\noften use code-mixed language while interacting with LLMs. To address this gap,\nwe introduce CodeMixBench, a novel benchmark designed to evaluate the\nrobustness of LLMs on code generation from code-mixed prompts. Built upon\nBigCodeBench, CodeMixBench introduces controlled code-mixing (CMD) into the\nnatural language parts of prompts across three language pairs: Hinglish\n(Hindi-English), Spanish-English, and Chinese Pinyin-English. We\ncomprehensively evaluate a diverse set of open-source code generation models\nranging from 1.5B to 15B parameters. Our results show that code-mixed prompts\nconsistently degrade Pass@1 performance compared to their English-only\ncounterparts, with performance drops increasing under higher CMD levels for\nsmaller models. CodeMixBench provides a realistic evaluation framework for\nstudying multilingual code generation and highlights new challenges and\ndirections for building robust code generation models that generalize well\nacross diverse linguistic settings."}
{"id": "2505.05098", "pdf": "https://arxiv.org/pdf/2505.05098.pdf", "abs": "https://arxiv.org/abs/2505.05098", "title": "X-Driver: Explainable Autonomous Driving with Vision-Language Models", "authors": ["Wei Liu", "Jiyuan Zhang", "Binxiong Zheng", "Yufeng Hu", "Yingzhan Lin", "Zengfeng Zeng"], "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.ET"], "comment": null, "summary": "End-to-end autonomous driving has advanced significantly, offering benefits\nsuch as system simplicity and stronger driving performance in both open-loop\nand closed-loop settings than conventional pipelines. However, existing\nframeworks still suffer from low success rates in closed-loop evaluations,\nhighlighting their limitations in real-world deployment. In this paper, we\nintroduce X-Driver, a unified multi-modal large language models(MLLMs)\nframework designed for closed-loop autonomous driving, leveraging\nChain-of-Thought(CoT) and autoregressive modeling to enhance perception and\ndecision-making. We validate X-Driver across multiple autonomous driving tasks\nusing public benchmarks in CARLA simulation environment, including\nBench2Drive[6]. Our experimental results demonstrate superior closed-loop\nperformance, surpassing the current state-of-the-art(SOTA) while improving the\ninterpretability of driving decisions. These findings underscore the importance\nof structured reasoning in end-to-end driving and establish X-Driver as a\nstrong baseline for future research in closed-loop autonomous driving."}
{"id": "2505.05145", "pdf": "https://arxiv.org/pdf/2505.05145.pdf", "abs": "https://arxiv.org/abs/2505.05145", "title": "Understanding In-context Learning of Addition via Activation Subspaces", "authors": ["Xinyan Hu", "Kayo Yin", "Michael I. Jordan", "Jacob Steinhardt", "Lijie Chen"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "16 pages", "summary": "To perform in-context learning, language models must extract signals from\nindividual few-shot examples, aggregate these into a learned prediction rule,\nand then apply this rule to new examples. How is this implemented in the\nforward pass of modern transformer models? To study this, we consider a\nstructured family of few-shot learning tasks for which the true prediction rule\nis to add an integer $k$ to the input. We find that Llama-3-8B attains high\naccuracy on this task for a range of $k$, and localize its few-shot ability to\njust three attention heads via a novel optimization approach. We further show\nthe extracted signals lie in a six-dimensional subspace, where four of the\ndimensions track the unit digit and the other two dimensions track overall\nmagnitude. We finally examine how these heads extract information from\nindividual few-shot examples, identifying a self-correction mechanism in which\nmistakes from earlier examples are suppressed by later examples. Our results\ndemonstrate how tracking low-dimensional subspaces across a forward pass can\nprovide insight into fine-grained computational structures."}
{"id": "2505.05190", "pdf": "https://arxiv.org/pdf/2505.05190.pdf", "abs": "https://arxiv.org/abs/2505.05190", "title": "Revealing Weaknesses in Text Watermarking Through Self-Information Rewrite Attacks", "authors": ["Yixin Cheng", "Hongcheng Guo", "Yangming Li", "Leonid Sigal"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": "ICML 2025 Accpeted", "summary": "Text watermarking aims to subtly embed statistical signals into text by\ncontrolling the Large Language Model (LLM)'s sampling process, enabling\nwatermark detectors to verify that the output was generated by the specified\nmodel. The robustness of these watermarking algorithms has become a key factor\nin evaluating their effectiveness. Current text watermarking algorithms embed\nwatermarks in high-entropy tokens to ensure text quality. In this paper, we\nreveal that this seemingly benign design can be exploited by attackers, posing\na significant risk to the robustness of the watermark. We introduce a generic\nefficient paraphrasing attack, the Self-Information Rewrite Attack (SIRA),\nwhich leverages the vulnerability by calculating the self-information of each\ntoken to identify potential pattern tokens and perform targeted attack. Our\nwork exposes a widely prevalent vulnerability in current watermarking\nalgorithms. The experimental results show SIRA achieves nearly 100% attack\nsuccess rates on seven recent watermarking methods with only 0.88 USD per\nmillion tokens cost. Our approach does not require any access to the watermark\nalgorithms or the watermarked LLM and can seamlessly transfer to any LLM as the\nattack model, even mobile-level models. Our findings highlight the urgent need\nfor more robust watermarking."}
{"id": "2505.05315", "pdf": "https://arxiv.org/pdf/2505.05315.pdf", "abs": "https://arxiv.org/abs/2505.05315", "title": "Scalable Chain of Thoughts via Elastic Reasoning", "authors": ["Yuhui Xu", "Hanze Dong", "Lei Wang", "Doyen Sahoo", "Junnan Li", "Caiming Xiong"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs) have achieved remarkable progress on complex\ntasks by generating extended chains of thought (CoT). However, their\nuncontrolled output lengths pose significant challenges for real-world\ndeployment, where inference-time budgets on tokens, latency, or compute are\nstrictly constrained. We propose Elastic Reasoning, a novel framework for\nscalable chain of thoughts that explicitly separates reasoning into two\nphases--thinking and solution--with independently allocated budgets. At test\ntime, Elastic Reasoning prioritize that completeness of solution segments,\nsignificantly improving reliability under tight resource constraints. To train\nmodels that are robust to truncated thinking, we introduce a lightweight\nbudget-constrained rollout strategy, integrated into GRPO, which teaches the\nmodel to reason adaptively when the thinking process is cut short and\ngeneralizes effectively to unseen budget constraints without additional\ntraining. Empirical results on mathematical (AIME, MATH500) and programming\n(LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning\nperforms robustly under strict budget constraints, while incurring\nsignificantly lower training cost than baseline methods. Remarkably, our\napproach also produces more concise and efficient reasoning even in\nunconstrained settings. Elastic Reasoning offers a principled and practical\nsolution to the pressing challenge of controllable reasoning at scale."}
{"id": "2505.05422", "pdf": "https://arxiv.org/pdf/2505.05422.pdf", "abs": "https://arxiv.org/abs/2505.05422", "title": "TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation", "authors": ["Haokun Lin", "Teng Wang", "Yixiao Ge", "Yuying Ge", "Zhichao Lu", "Ying Wei", "Qingfu Zhang", "Zhenan Sun", "Ying Shan"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Technical Report", "summary": "Pioneering token-based works such as Chameleon and Emu3 have established a\nfoundation for multimodal unification but face challenges of high training\ncomputational overhead and limited comprehension performance due to a lack of\nhigh-level semantics. In this paper, we introduce TokLIP, a visual tokenizer\nthat enhances comprehension by semanticizing vector-quantized (VQ) tokens and\nincorporating CLIP-level semantics while enabling end-to-end multimodal\nautoregressive training with standard VQ tokens. TokLIP integrates a low-level\ndiscrete VQ tokenizer with a ViT-based token encoder to capture high-level\ncontinuous semantics. Unlike previous approaches (e.g., VILA-U) that discretize\nhigh-level features, TokLIP disentangles training objectives for comprehension\nand generation, allowing the direct application of advanced VQ tokenizers\nwithout the need for tailored quantization operations. Our empirical results\ndemonstrate that TokLIP achieves exceptional data efficiency, empowering visual\ntokens with high-level semantic understanding while enhancing low-level\ngenerative capacity, making it well-suited for autoregressive Transformers in\nboth comprehension and generation tasks. The code and models are available at\nhttps://github.com/TencentARC/TokLIP."}
{"id": "2505.05446", "pdf": "https://arxiv.org/pdf/2505.05446.pdf", "abs": "https://arxiv.org/abs/2505.05446", "title": "Adaptive Markup Language Generation for Contextually-Grounded Visual Document Understanding", "authors": ["Han Xiao", "Yina Xie", "Guanxin Tan", "Yinghao Chen", "Rui Hu", "Ke Wang", "Aojun Zhou", "Hao Li", "Hao Shao", "Xudong Lu", "Peng Gao", "Yafei Wen", "Xiaoxin Chen", "Shuai Ren", "Hongsheng Li"], "categories": ["cs.CV", "cs.CL"], "comment": "CVPR2025", "summary": "Visual Document Understanding has become essential with the increase of\ntext-rich visual content. This field poses significant challenges due to the\nneed for effective integration of visual perception and textual comprehension,\nparticularly across diverse document types with complex layouts. Moreover,\nexisting fine-tuning datasets for this domain often fall short in providing the\ndetailed contextual information for robust understanding, leading to\nhallucinations and limited comprehension of spatial relationships among visual\nelements. To address these challenges, we propose an innovative pipeline that\nutilizes adaptive generation of markup languages, such as Markdown, JSON, HTML,\nand TiKZ, to build highly structured document representations and deliver\ncontextually-grounded responses. We introduce two fine-grained structured\ndatasets: DocMark-Pile, comprising approximately 3.8M pretraining data pairs\nfor document parsing, and DocMark-Instruct, featuring 624k fine-tuning data\nannotations for grounded instruction following. Extensive experiments\ndemonstrate that our proposed model significantly outperforms existing\nstate-of-theart MLLMs across a range of visual document understanding\nbenchmarks, facilitating advanced reasoning and comprehension capabilities in\ncomplex visual scenarios. Our code and models are released at https://github.\ncom/Euphoria16/DocMark."}
{"id": "2505.05467", "pdf": "https://arxiv.org/pdf/2505.05467.pdf", "abs": "https://arxiv.org/abs/2505.05467", "title": "StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant", "authors": ["Haibo Wang", "Bo Feng", "Zhengfeng Lai", "Mingze Xu", "Shiyu Li", "Weifeng Ge", "Afshin Dehghan", "Meng Cao", "Ping Huang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks."}
{"id": "2306.10512", "pdf": "https://arxiv.org/pdf/2306.10512.pdf", "abs": "https://arxiv.org/abs/2306.10512", "title": "Position: AI Evaluation Should Learn from How We Test Humans", "authors": ["Yan Zhuang", "Qi Liu", "Zachary A. Pardos", "Patrick C. Kyllonen", "Jiyun Zu", "Zhenya Huang", "Shijin Wang", "Enhong Chen"], "categories": ["cs.CL"], "comment": "Accepted by ICML 2025", "summary": "As AI systems continue to evolve, their rigorous evaluation becomes crucial\nfor their development and deployment. Researchers have constructed various\nlarge-scale benchmarks to determine their capabilities, typically against a\ngold-standard test set and report metrics averaged across all items. However,\nthis static evaluation paradigm increasingly shows its limitations, including\nhigh evaluation costs, data contamination, and the impact of low-quality or\nerroneous items on evaluation reliability and efficiency. In this Position,\ndrawing from human psychometrics, we discuss a paradigm shift from static\nevaluation methods to adaptive testing. This involves estimating the\ncharacteristics or value of each test item in the benchmark, and tailoring each\nmodel's evaluation instead of relying on a fixed test set. This paradigm\nprovides robust ability estimation, uncovering the latent traits underlying a\nmodel's observed scores. This position paper analyze the current possibilities,\nprospects, and reasons for adopting psychometrics in AI evaluation. We argue\nthat psychometrics, a theory originating in the 20th century for human\nassessment, could be a powerful solution to the challenges in today's AI\nevaluations."}
{"id": "2405.13325", "pdf": "https://arxiv.org/pdf/2405.13325.pdf", "abs": "https://arxiv.org/abs/2405.13325", "title": "DEGAP: Dual Event-Guided Adaptive Prefixes for Templated-Based Event Argument Extraction with Slot Querying", "authors": ["Guanghui Wang", "Dexi Liu", "Jian-Yun Nie", "Qizhi Wan", "Rong Hu", "Xiping Liu", "Wanlong Liu", "Jiaming Liu"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Published as a conference paper in COLING 2025", "summary": "Recent advancements in event argument extraction (EAE) involve incorporating\nuseful auxiliary information into models during training and inference, such as\nretrieved instances and event templates. These methods face two challenges: (1)\nthe retrieval results may be irrelevant and (2) templates are developed\nindependently for each event without considering their possible relationship.\nIn this work, we propose DEGAP to address these challenges through a simple yet\neffective components: dual prefixes, i.e. learnable prompt vectors, where the\ninstance-oriented prefix and template-oriented prefix are trained to learn\ninformation from different event instances and templates. Additionally, we\npropose an event-guided adaptive gating mechanism, which can adaptively\nleverage possible connections between different events and thus capture\nrelevant information from the prefix. Finally, these event-guided prefixes\nprovide relevant information as cues to EAE model without retrieval. Extensive\nexperiments demonstrate that our method achieves new state-of-the-art\nperformance on four datasets (ACE05, RAMS, WIKIEVENTS, and MLEE). Further\nanalysis shows the impact of different components."}
{"id": "2406.17746", "pdf": "https://arxiv.org/pdf/2406.17746.pdf", "abs": "https://arxiv.org/abs/2406.17746", "title": "Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon", "authors": ["USVSN Sai Prashanth", "Alvin Deng", "Kyle O'Brien", "Jyothir S V", "Mohammad Aflah Khan", "Jaydeep Borkar", "Christopher A. Choquette-Choo", "Jacob Ray Fuehne", "Stella Biderman", "Tracy Ke", "Katherine Lee", "Naomi Saphra"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Memorization in language models is typically treated as a homogenous\nphenomenon, neglecting the specifics of the memorized data. We instead model\nmemorization as the effect of a set of complex factors that describe each\nsample and relate it to the model and corpus. To build intuition around these\nfactors, we break memorization down into a taxonomy: recitation of highly\nduplicated sequences, reconstruction of inherently predictable sequences, and\nrecollection of sequences that are neither. We demonstrate the usefulness of\nour taxonomy by using it to construct a predictive model for memorization. By\nanalyzing dependencies and inspecting the weights of the predictive model, we\nfind that different factors influence the likelihood of memorization\ndifferently depending on the taxonomic category."}
{"id": "2406.20094", "pdf": "https://arxiv.org/pdf/2406.20094.pdf", "abs": "https://arxiv.org/abs/2406.20094", "title": "Scaling Synthetic Data Creation with 1,000,000,000 Personas", "authors": ["Tao Ge", "Xin Chan", "Xiaoyang Wang", "Dian Yu", "Haitao Mi", "Dong Yu"], "categories": ["cs.CL", "cs.LG"], "comment": "Work in progress", "summary": "We propose a novel persona-driven data synthesis methodology that leverages\nvarious perspectives within a large language model (LLM) to create diverse\nsynthetic data. To fully exploit this methodology at scale, we introduce\nPersona Hub -- a collection of 1 billion diverse personas automatically curated\nfrom web data. These 1 billion personas (~13% of the world's total population),\nacting as distributed carriers of world knowledge, can tap into almost every\nperspective encapsulated within the LLM, thereby facilitating the creation of\ndiverse synthetic data at scale for various scenarios. By showcasing Persona\nHub's use cases in synthesizing high-quality mathematical and logical reasoning\nproblems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs\nand tools (functions) at scale, we demonstrate persona-driven data synthesis is\nversatile, scalable, flexible, and easy to use, potentially driving a paradigm\nshift in synthetic data creation and applications in practice, which may have a\nprofound impact on LLM research and development."}
{"id": "2409.11055", "pdf": "https://arxiv.org/pdf/2409.11055.pdf", "abs": "https://arxiv.org/abs/2409.11055", "title": "Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant", "authors": ["Jemin Lee", "Sihyeong Park", "Jinse Kwon", "Jihun Oh", "Yongin Kwon"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in IJCAI 2025, 21 pages, 2 figure", "summary": "Quantization has gained attention as a promising solution for the\ncost-effective deployment of large and small language models. However, most\nprior work has been limited to perplexity or basic knowledge tasks and lacks a\ncomprehensive evaluation of recent models like Llama-3.3. In this paper, we\nconduct a comprehensive evaluation of instruction-tuned models spanning 1B to\n405B parameters, applying four quantization methods across 13 datasets. Our\nfindings reveal that (1) quantized models generally surpass smaller FP16\nbaselines, yet they often struggle with instruction-following and hallucination\ndetection; (2) FP8 consistently emerges as the most robust option across tasks,\nand AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller\nmodels can suffer severe accuracy drops at 4-bit quantization, while 70B-scale\nmodels maintain stable performance; (4) notably, \\textit{hard} tasks do not\nalways experience the largest accuracy losses, indicating that quantization\nmagnifies a model's inherent weaknesses rather than simply correlating with\ntask difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant\nperformance declines in coding and STEM tasks, though reasoning may sometimes\nimprove."}
{"id": "2409.12183", "pdf": "https://arxiv.org/pdf/2409.12183.pdf", "abs": "https://arxiv.org/abs/2409.12183", "title": "To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning", "authors": ["Zayne Sprague", "Fangcong Yin", "Juan Diego Rodriguez", "Dongwei Jiang", "Manya Wadhwa", "Prasann Singhal", "Xinyu Zhao", "Xi Ye", "Kyle Mahowald", "Greg Durrett"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published at ICLR 2025", "summary": "Chain-of-thought (CoT) via prompting is the de facto method for eliciting\nreasoning capabilities from large language models (LLMs). But for what kinds of\ntasks is this extra ``thinking'' really helpful? To analyze this, we conducted\na quantitative meta-analysis covering over 100 papers using CoT and ran our own\nevaluations of 20 datasets across 14 models. Our results show that CoT gives\nstrong performance benefits primarily on tasks involving math or logic, with\nmuch smaller gains on other types of tasks. On MMLU, directly generating the\nanswer without CoT leads to almost identical accuracy as CoT unless the\nquestion or model's response contains an equals sign, indicating symbolic\noperations and reasoning. Following this finding, we analyze the behavior of\nCoT on these problems by separating planning and execution and comparing\nagainst tool-augmented LLMs. Much of CoT's gain comes from improving symbolic\nexecution, but it underperforms relative to using a symbolic solver. Our\nresults indicate that CoT can be applied selectively, maintaining performance\nwhile saving inference costs. Furthermore, they suggest a need to move beyond\nprompt-based CoT to new paradigms that better leverage intermediate computation\nacross the whole range of LLM applications."}
{"id": "2410.04055", "pdf": "https://arxiv.org/pdf/2410.04055.pdf", "abs": "https://arxiv.org/abs/2410.04055", "title": "Self-Correction is More than Refinement: A Learning Framework for Visual and Language Reasoning Tasks", "authors": ["Jiayi He", "Hehai Lin", "Qingyun Wang", "Yi Fung", "Heng Ji"], "categories": ["cs.CL"], "comment": null, "summary": "While Vision-Language Models (VLMs) have shown remarkable abilities in visual\nand language reasoning tasks, they invariably generate flawed responses.\nSelf-correction that instructs models to refine their outputs presents a\npromising solution to this issue. Previous studies have mainly concentrated on\nLarge Language Models (LLMs), while the self-correction abilities of VLMs,\nparticularly concerning both visual and linguistic information, remain largely\nunexamined. This study investigates the self-correction capabilities of VLMs\nduring both inference and fine-tuning stages. We introduce a Self-Correction\nLearning (SCL) approach that enables VLMs to learn from their self-generated\nself-correction data through Direct Preference Optimization (DPO) without\nrelying on external feedback, facilitating self-improvement. Specifically, we\ncollect preferred and disfavored samples based on the correctness of initial\nand refined responses, which are obtained by two-turn self-correction with VLMs\nduring the inference stage. Experimental results demonstrate that although VLMs\nstruggle to self-correct effectively during iterative inference without\nadditional fine-tuning and external feedback, they can enhance their\nperformance and avoid previous mistakes through preference fine-tuning when\ntheir self-generated self-correction data are categorized into preferred and\ndisfavored samples. This study emphasizes that self-correction is not merely a\nrefinement process; rather, it should enhance the reasoning abilities of models\nthrough additional training, enabling them to generate high-quality responses\ndirectly without further refinement."}
{"id": "2410.09580", "pdf": "https://arxiv.org/pdf/2410.09580.pdf", "abs": "https://arxiv.org/abs/2410.09580", "title": "SAPIENT: Mastering Multi-turn Conversational Recommendation with Strategic Planning and Monte Carlo Tree Search", "authors": ["Hanwen Du", "Bo Peng", "Xia Ning"], "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 Main Conference", "summary": "Conversational Recommender Systems (CRS) proactively engage users in\ninteractive dialogues to elicit user preferences and provide personalized\nrecommendations. Existing methods train Reinforcement Learning (RL)-based agent\nwith greedy action selection or sampling strategy, and may suffer from\nsuboptimal conversational planning. To address this, we present a novel Monte\nCarlo Tree Search (MCTS)-based CRS framework SAPIENT. SAPIENT consists of a\nconversational agent (S-agent) and a conversational planner (S-planner).\nS-planner builds a conversational search tree with MCTS based on the initial\nactions proposed by S-agent to find conversation plans. The best conversation\nplans from S-planner are used to guide the training of S-agent, creating a\nself-training loop where S-agent can iteratively improve its capability for\nconversational planning. Furthermore, we propose an efficient variant SAPIENT\nfor trade-off between training efficiency and performance. Extensive\nexperiments on four benchmark datasets validate the effectiveness of our\napproach, showing that SAPIENT outperforms the state-of-the-art baselines. Our\ncode and data are accessible through https://github.com/ninglab/SAPIENT."}
{"id": "2410.12705", "pdf": "https://arxiv.org/pdf/2410.12705.pdf", "abs": "https://arxiv.org/abs/2410.12705", "title": "WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines", "authors": ["Genta Indra Winata", "Frederikus Hudi", "Patrick Amadeus Irawan", "David Anugraha", "Rifki Afina Putri", "Yutong Wang", "Adam Nohejl", "Ubaidillah Ariq Prathama", "Nedjma Ousidhoum", "Afifa Amriani", "Anar Rzayev", "Anirban Das", "Ashmari Pramodya", "Aulia Adila", "Bryan Wilie", "Candy Olivia Mawalim", "Ching Lam Cheng", "Daud Abolade", "Emmanuele Chersoni", "Enrico Santus", "Fariz Ikhwantri", "Garry Kuwanto", "Hanyang Zhao", "Haryo Akbarianto Wibowo", "Holy Lovenia", "Jan Christian Blaise Cruz", "Jan Wira Gotama Putra", "Junho Myung", "Lucky Susanto", "Maria Angelica Riera Machin", "Marina Zhukova", "Michael Anugraha", "Muhammad Farid Adilazuarda", "Natasha Santosa", "Peerat Limkonchotiwat", "Raj Dabre", "Rio Alexander Audino", "Samuel Cahyawijaya", "Shi-Xiong Zhang", "Stephanie Yulia Salim", "Yi Zhou", "Yinxuan Gui", "David Ifeoluwa Adelani", "En-Shiun Annie Lee", "Shogo Okada", "Ayu Purwarianti", "Alham Fikri Aji", "Taro Watanabe", "Derry Tanti Wijaya", "Alice Oh", "Chong-Wah Ngo"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Best Theme Paper at NAACL 2025", "summary": "Vision Language Models (VLMs) often struggle with culture-specific knowledge,\nparticularly in languages other than English and in underrepresented cultural\ncontexts. To evaluate their understanding of such knowledge, we introduce\nWorldCuisines, a massive-scale benchmark for multilingual and multicultural,\nvisually grounded language understanding. This benchmark includes a visual\nquestion answering (VQA) dataset with text-image pairs across 30 languages and\ndialects, spanning 9 language families and featuring over 1 million data\npoints, making it the largest multicultural VQA benchmark to date. It includes\ntasks for identifying dish names and their origins. We provide evaluation\ndatasets in two sizes (12k and 60k instances) alongside a training dataset (1\nmillion instances). Our findings show that while VLMs perform better with\ncorrect location context, they struggle with adversarial contexts and\npredicting specific regional cuisines and languages. To support future\nresearch, we release a knowledge base with annotated food entries and images\nalong with the VQA data."}
{"id": "2411.00437", "pdf": "https://arxiv.org/pdf/2411.00437.pdf", "abs": "https://arxiv.org/abs/2411.00437", "title": "E2E-AFG: An End-to-End Model with Adaptive Filtering for Retrieval-Augmented Generation", "authors": ["Yun Jiang", "Zilong Xie", "Wei Zhang", "Yun Fang", "Shuai Pan"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 3 figures, 5 tables", "summary": "Retrieval-augmented generation methods often neglect the quality of content\nretrieved from external knowledge bases, resulting in irrelevant information or\npotential misinformation that negatively affects the generation results of\nlarge language models. In this paper, we propose an end-to-end model with\nadaptive filtering for retrieval-augmented generation (E2E-AFG), which\nintegrates answer existence judgment and text generation into a single\nend-to-end framework. This enables the model to focus more effectively on\nrelevant content while reducing the influence of irrelevant information and\ngenerating accurate answers. We evaluate E2E-AFG on six representative\nknowledge-intensive language datasets, and the results show that it\nconsistently outperforms baseline models across all tasks, demonstrating the\neffectiveness and robustness of the proposed approach."}
{"id": "2411.04996", "pdf": "https://arxiv.org/pdf/2411.04996.pdf", "abs": "https://arxiv.org/abs/2411.04996", "title": "Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models", "authors": ["Weixin Liang", "Lili Yu", "Liang Luo", "Srinivasan Iyer", "Ning Dong", "Chunting Zhou", "Gargi Ghosh", "Mike Lewis", "Wen-tau Yih", "Luke Zettlemoyer", "Xi Victoria Lin"], "categories": ["cs.CL"], "comment": "Accepted to TMLR 2025; 48 pages", "summary": "The development of large language models (LLMs) has expanded to multi-modal\nsystems capable of processing text, images, and speech within a unified\nframework. Training these models demands significantly larger datasets and\ncomputational resources compared to text-only LLMs. To address the scaling\nchallenges, we introduce Mixture-of-Transformers (MoT), a sparse multi-modal\ntransformer architecture that significantly reduces pretraining computational\ncosts. MoT decouples non-embedding parameters of the model by modality --\nincluding feed-forward networks, attention matrices, and layer normalization --\nenabling modality-specific processing with global self-attention over the full\ninput sequence. We evaluate MoT across multiple settings and model scales. In\nthe Chameleon 7B setting (autoregressive text-and-image generation), MoT\nmatches the dense baseline's performance using only 55.8\\% of the FLOPs. When\nextended to include speech, MoT reaches speech performance comparable to the\ndense baseline with only 37.2\\% of the FLOPs. In the Transfusion setting, where\ntext and image are trained with different objectives, a 7B MoT model matches\nthe image modality performance of the dense baseline with one third of the\nFLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image\ngeneration metrics. System profiling further highlights MoT's practical\nbenefits, achieving dense baseline image quality in 47.2\\% of the wall-clock\ntime and text quality in 75.6\\% of the wall-clock time (measured on AWS\np4de.24xlarge instances with NVIDIA A100 GPUs)."}
{"id": "2501.00874", "pdf": "https://arxiv.org/pdf/2501.00874.pdf", "abs": "https://arxiv.org/abs/2501.00874", "title": "LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models", "authors": ["Hieu Man", "Nghia Trung Ngo", "Viet Dac Lai", "Ryan A. Rossi", "Franck Dernoncourt", "Thien Huu Nguyen"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Recent advancements in large language models (LLMs) based embedding models\nhave established new state-of-the-art benchmarks for text embedding tasks,\nparticularly in dense vector-based retrieval. However, these models\npredominantly focus on English, leaving multilingual embedding capabilities\nlargely unexplored. To address this limitation, we present LUSIFER, a novel\nzero-shot approach that adapts LLM-based embedding models for multilingual\ntasks without requiring multilingual supervision. LUSIFER's architecture\ncombines a multilingual encoder, serving as a language-universal learner, with\nan LLM-based embedding model optimized for embedding-specific tasks. These\ncomponents are seamlessly integrated through a minimal set of trainable\nparameters that act as a connector, effectively transferring the multilingual\nencoder's language understanding capabilities to the specialized embedding\nmodel. Additionally, to comprehensively evaluate multilingual embedding\nperformance, we introduce a new benchmark encompassing 5 primary embedding\ntasks, 123 diverse datasets, and coverage across 14 languages. Extensive\nexperimental results demonstrate that LUSIFER significantly enhances the\nmultilingual performance across various embedding tasks, particularly for\nmedium and low-resource languages, without requiring explicit multilingual\ntraining data."}
{"id": "2501.01031", "pdf": "https://arxiv.org/pdf/2501.01031.pdf", "abs": "https://arxiv.org/abs/2501.01031", "title": "ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented Contextual Learning", "authors": ["Wonduk Seo", "Zonghao Yuan", "Yi Bu"], "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": "preprint", "summary": "Ensuring cultural values alignment in Large Language Models (LLMs) remains a\ncritical challenge, as these models often embed Western-centric biases from\ntheir training data, leading to misrepresentations and fairness concerns in\ncross-cultural applications. Existing approaches such as role assignment and\nfew-shot learning struggle to address these limitations effectively due to\ntheir reliance on pre-trained knowledge, limited scalability, and inability to\ncapture nuanced cultural values. To address these issues, we propose ValuesRAG,\na novel and effective framework that applies Retrieval-Augmented Generation\n(RAG) with In-Context Learning (ICL) to integrate cultural and demographic\nknowledge dynamically during text generation. Leveraging the World Values\nSurvey (WVS) dataset, ValuesRAG first generates summaries of values for each\nindividual. We subsequently curate several representative regional datasets to\nserve as test datasets and retrieve relevant summaries of values based on\ndemographic features, followed by a reranking step to select the top-k relevant\nsummaries. We evaluate ValuesRAG using 6 diverse regional datasets and show\nthat it consistently outperforms baselines: including zero-shot,\nrole-assignment, few-shot, and hybrid methods, both in main experiments and\nablation settings. Notably, ValuesRAG achieves the best overall performance\nover prior methods, demonstrating its effectiveness in fostering culturally\naligned and inclusive AI systems. Our findings underscore the potential of\ndynamic retrieval-based methods to bridge the gap between global LLM\ncapabilities and localized cultural values."}
{"id": "2501.14082", "pdf": "https://arxiv.org/pdf/2501.14082.pdf", "abs": "https://arxiv.org/abs/2501.14082", "title": "Communicating Activations Between Language Model Agents", "authors": ["Vignav Ramesh", "Kenneth Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "Communication between multiple language model (LM) agents has been shown to\nscale up the reasoning ability of LMs. While natural language has been the\ndominant medium for inter-LM communication, it is not obvious this should be\nthe standard: not only does natural language communication incur high inference\ncosts that scale quickly with the number of both agents and messages, but also\nthe decoding process abstracts away too much rich information that could be\notherwise accessed from the internal activations. In this work, we propose a\nsimple technique whereby LMs communicate via activations; concretely, we pause\nan LM $\\textit{B}$'s computation at an intermediate layer, combine its current\nactivation with another LM $\\textit{A}$'s intermediate activation via some\nfunction $\\textit{f}$, then pass $\\textit{f}$'s output into the next layer of\n$\\textit{B}$ and continue the forward pass till decoding is complete. This\napproach scales up LMs on new tasks with zero additional parameters and data,\nand saves a substantial amount of compute over natural language communication.\nWe test our method with various functional forms $\\textit{f}$ on two\nexperimental setups--multi-player coordination games and reasoning\nbenchmarks--and find that it achieves up to $27.0\\%$ improvement over natural\nlanguage communication across datasets with $<$$1/4$ the compute, illustrating\nthe superiority and robustness of activations as an alternative \"language\" for\ncommunication between LMs."}
{"id": "2501.15858", "pdf": "https://arxiv.org/pdf/2501.15858.pdf", "abs": "https://arxiv.org/abs/2501.15858", "title": "Applications of Artificial Intelligence for Cross-language Intelligibility Assessment of Dysarthric Speech", "authors": ["Eunjung Yeo", "Julie Liss", "Visar Berisha", "David Mortensen"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "14 pages, 2 figure, 2 tables", "summary": "Purpose: Speech intelligibility is a critical outcome in the assessment and\nmanagement of dysarthria, yet most research and clinical practices have focused\non English, limiting their applicability across languages. This commentary\nintroduces a conceptual framework--and a demonstration of how it can be\nimplemented--leveraging artificial intelligence (AI) to advance cross-language\nintelligibility assessment of dysarthric speech. Method: We propose a\ntwo-tiered conceptual framework consisting of a universal speech model that\nencodes dysarthric speech into acoustic-phonetic representations, followed by a\nlanguage-specific intelligibility assessment model that interprets these\nrepresentations within the phonological or prosodic structures of the target\nlanguage. We further identify barriers to cross-language intelligibility\nassessment of dysarthric speech, including data scarcity, annotation\ncomplexity, and limited linguistic insights into dysarthric speech, and outline\npotential AI-driven solutions to overcome these challenges. Conclusion:\nAdvancing cross-language intelligibility assessment of dysarthric speech\nnecessitates models that are both efficient and scalable, yet constrained by\nlinguistic rules to ensure accurate and language-sensitive assessment. Recent\nadvances in AI provide the foundational tools to support this integration,\nshaping future directions toward generalizable and linguistically informed\nassessment frameworks."}
{"id": "2502.11137", "pdf": "https://arxiv.org/pdf/2502.11137.pdf", "abs": "https://arxiv.org/abs/2502.11137", "title": "Safety Evaluation of DeepSeek Models in Chinese Contexts", "authors": ["Wenjing Zhang", "Xuejiao Lei", "Zhaoxiang Liu", "Ning Wang", "Zhenhong Long", "Peijun Yang", "Jiaojiao Zhao", "Minjie Hua", "Chaoyang Ma", "Kai Wang", "Shiguo Lian"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 2 tables, 7 figures", "summary": "Recently, the DeepSeek series of models, leveraging their exceptional\nreasoning capabilities and open-source strategy, is reshaping the global AI\nlandscape. Despite these advantages, they exhibit significant safety\ndeficiencies. Research conducted by Robust Intelligence, a subsidiary of Cisco,\nin collaboration with the University of Pennsylvania, revealed that DeepSeek-R1\nhas a 100\\% attack success rate when processing harmful prompts. Additionally,\nmultiple safety companies and research institutions have confirmed critical\nsafety vulnerabilities in this model. As models demonstrating robust\nperformance in Chinese and English, DeepSeek models require equally crucial\nsafety assessments in both language contexts. However, current research has\npredominantly focused on safety evaluations in English environments, leaving a\ngap in comprehensive assessments of their safety performance in Chinese\ncontexts. In response to this gap, this study introduces CHiSafetyBench, a\nChinese-specific safety evaluation benchmark. This benchmark systematically\nevaluates the safety of DeepSeek-R1 and DeepSeek-V3 in Chinese contexts,\nrevealing their performance across safety categories. The experimental results\nquantify the deficiencies of these two models in Chinese contexts, providing\nkey insights for subsequent improvements. It should be noted that, despite our\nefforts to establish a comprehensive, objective, and authoritative evaluation\nbenchmark, the selection of test samples, characteristics of data distribution,\nand the setting of evaluation criteria may inevitably introduce certain biases\ninto the evaluation results. We will continuously optimize the evaluation\nbenchmark and periodically update this report to provide more comprehensive and\naccurate assessment outcomes. Please refer to the latest version of the paper\nfor the most recent evaluation results and conclusions."}
{"id": "2502.14289", "pdf": "https://arxiv.org/pdf/2502.14289.pdf", "abs": "https://arxiv.org/abs/2502.14289", "title": "Drift: Decoding-time Personalized Alignments with Implicit User Preferences", "authors": ["Minbeom Kim", "Kang-il Lee", "Seongho Joo", "Hwaran Lee", "Thibaut Thonet", "Kyomin Jung"], "categories": ["cs.CL"], "comment": "19 pages, 6 figures", "summary": "Personalized alignments for individual users have been a long-standing goal\nin large language models (LLMs). We introduce Drift, a novel framework that\npersonalizes LLMs at decoding time with implicit user preferences. Traditional\nReinforcement Learning from Human Feedback (RLHF) requires thousands of\nannotated examples and expensive gradient updates. In contrast, Drift\npersonalizes LLMs in a training-free manner, using only a few dozen examples to\nsteer a frozen model through efficient preference modeling. Our approach models\nuser preferences as a composition of predefined, interpretable attributes and\naligns them at decoding time to enable personalized generation. Experiments on\nboth a synthetic persona dataset (Perspective) and a real human-annotated\ndataset (PRISM) demonstrate that Drift significantly outperforms RLHF baselines\nwhile using only 50-100 examples. Our results and analysis show that Drift is\nboth computationally efficient and interpretable."}
{"id": "2503.05505", "pdf": "https://arxiv.org/pdf/2503.05505.pdf", "abs": "https://arxiv.org/abs/2503.05505", "title": "Correctness Coverage Evaluation for Medical Multiple-Choice Question Answering Based on the Enhanced Conformal Prediction Framework", "authors": ["Yusong Ke", "Hongru Lin", "Yuting Ruan", "Junya Tang", "Li Li"], "categories": ["cs.CL"], "comment": "Published by Mathematics", "summary": "Large language models (LLMs) are increasingly adopted in medical\nquestion-answering (QA) scenarios. However, LLMs can generate hallucinations\nand nonfactual information, undermining their trustworthiness in high-stakes\nmedical tasks. Conformal Prediction (CP) provides a statistically rigorous\nframework for marginal (average) coverage guarantees but has limited\nexploration in medical QA. This paper proposes an enhanced CP framework for\nmedical multiple-choice question-answering (MCQA) tasks. By associating the\nnon-conformance score with the frequency score of correct options and\nleveraging self-consistency, the framework addresses internal model opacity and\nincorporates a risk control strategy with a monotonic loss function. Evaluated\non MedMCQA, MedQA, and MMLU datasets using four off-the-shelf LLMs, the\nproposed method meets specified error rate guarantees while reducing average\nprediction set size with increased risk level, offering a promising uncertainty\nevaluation metric for LLMs."}
{"id": "2503.08292", "pdf": "https://arxiv.org/pdf/2503.08292.pdf", "abs": "https://arxiv.org/abs/2503.08292", "title": "Large Language Models for Outpatient Referral: Problem Definition, Benchmarking and Challenges", "authors": ["Xiaoxiao Liu", "Qingying Xiao", "Junying Chen", "Xiangyi Feng", "Xiangbo Wu", "Bairui Zhang", "Xiang Wan", "Jian Chang", "Guangjun Yu", "Yan Hu", "Benyou Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly applied to outpatient referral\ntasks across healthcare systems. However, there is a lack of standardized\nevaluation criteria to assess their effectiveness, particularly in dynamic,\ninteractive scenarios. In this study, we systematically examine the\ncapabilities and limitations of LLMs in managing tasks within Intelligent\nOutpatient Referral (IOR) systems and propose a comprehensive evaluation\nframework specifically designed for such systems. This framework comprises two\ncore tasks: static evaluation, which focuses on evaluating the ability of\npredefined outpatient referrals, and dynamic evaluation, which evaluates\ncapabilities of refining outpatient referral recommendations through iterative\ndialogues. Our findings suggest that LLMs offer limited advantages over\nBERT-like models, but show promise in asking effective questions during\ninteractive dialogues."}
{"id": "2503.13690", "pdf": "https://arxiv.org/pdf/2503.13690.pdf", "abs": "https://arxiv.org/abs/2503.13690", "title": "Atyaephyra at SemEval-2025 Task 4: Low-Rank Negative Preference Optimization", "authors": ["Jan Bronec", "Jindřich Helcl"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50 (Primary), 68T07 (Secondary)", "I.2.7"], "comment": "8 pages, 3 figures, accepted to SemEval workshop proceedings at ACL\n  2025", "summary": "We present a submission to the SemEval 2025 shared task on unlearning\nsensitive content from LLMs. Our approach employs negative preference\noptimization using low-rank adaptation. We show that we can utilize this\ncombination to efficiently compute additional regularization terms, which help\nwith unlearning stabilization. The results of our approach significantly exceed\nthe shared task baselines."}
{"id": "2503.15169", "pdf": "https://arxiv.org/pdf/2503.15169.pdf", "abs": "https://arxiv.org/abs/2503.15169", "title": "Benchmarking Open-Source Large Language Models on Healthcare Text Classification Tasks", "authors": ["Yuting Guo", "Abeed Sarker"], "categories": ["cs.CL", "cs.AI"], "comment": "5 pages", "summary": "The application of large language models (LLMs) to healthcare information\nextraction has emerged as a promising approach. This study evaluates the\nclassification performance of five open-source LLMs: GEMMA-3-27B-IT,\nLLAMA3-70B, LLAMA4-109B, DEEPSEEK-R1-DISTILL-LLAMA-70B, and\nDEEPSEEK-V3-0324-UD-Q2_K_XL, across six healthcare-related classification tasks\ninvolving both social media data (breast cancer, changes in medication regimen,\nadverse pregnancy outcomes, potential COVID-19 cases) and clinical data (stigma\nlabeling, medication change discussion). We report precision, recall, and F1\nscores with 95% confidence intervals for all model-task combinations. Our\nfindings reveal significant performance variability between LLMs, with\nDeepSeekV3 emerging as the strongest overall performer, achieving the highest\nF1 scores in four tasks. Notably, models generally performed better on social\nmedia tasks compared to clinical data tasks, suggesting potential\ndomain-specific challenges. GEMMA-3-27B-IT demonstrated exceptionally high\nrecall despite its smaller parameter count, while LLAMA4-109B showed\nsurprisingly underwhelming performance compared to its predecessor LLAMA3-70B,\nindicating that larger parameter counts do not guarantee improved\nclassification results. We observed distinct precision-recall trade-offs across\nmodels, with some favoring sensitivity over specificity and vice versa. These\nfindings highlight the importance of task-specific model selection for\nhealthcare applications, considering the particular data domain and\nprecision-recall requirements rather than model size alone. As healthcare\nincreasingly integrates AI-driven text classification tools, this comprehensive\nbenchmarking provides valuable guidance for model selection and implementation\nwhile underscoring the need for continued evaluation and domain adaptation of\nLLMs in healthcare contexts."}
{"id": "2504.17480", "pdf": "https://arxiv.org/pdf/2504.17480.pdf", "abs": "https://arxiv.org/abs/2504.17480", "title": "Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation", "authors": ["Xin Yi", "Shunfan Zheng", "Linlin Wang", "Xiaoling Wang", "Liang He"], "categories": ["cs.CL"], "comment": null, "summary": "Watermarking has emerged as a critical technique for combating misinformation\nand protecting intellectual property in large language models (LLMs). A recent\ndiscovery, termed watermark radioactivity, reveals that watermarks embedded in\nteacher models can be inherited by student models through knowledge\ndistillation. On the positive side, this inheritance allows for the detection\nof unauthorized knowledge distillation by identifying watermark traces in\nstudent models. However, the robustness of watermarks against scrubbing attacks\nand their unforgeability in the face of spoofing attacks under unauthorized\nknowledge distillation remain largely unexplored. Existing watermark attack\nmethods either assume access to model internals or fail to simultaneously\nsupport both scrubbing and spoofing attacks. In this work, we propose\nContrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified\nframework that enables bidirectional attacks under unauthorized knowledge\ndistillation. Our approach employs contrastive decoding to extract corrupted or\namplified watermark texts via comparing outputs from the student model and\nweakly watermarked references, followed by bidirectional distillation to train\nnew student models capable of watermark removal and watermark forgery,\nrespectively. Extensive experiments show that CDG-KD effectively performs\nattacks while preserving the general performance of the distilled model. Our\nfindings underscore critical need for developing watermarking schemes that are\nrobust and unforgeable."}
{"id": "2505.00654", "pdf": "https://arxiv.org/pdf/2505.00654.pdf", "abs": "https://arxiv.org/abs/2505.00654", "title": "Large Language Models Understanding: an Inherent Ambiguity Barrier", "authors": ["Daniel N. Nissani"], "categories": ["cs.CL", "cs.AI"], "comment": "submitted to NEURAL COMPUTATION", "summary": "A lively ongoing debate is taking place, since the extraordinary emergence of\nLarge Language Models (LLMs) with regards to their capability to understand the\nworld and capture the meaning of the dialogues in which they are involved.\nArguments and counter-arguments have been proposed based upon thought\nexperiments, anecdotal conversations between LLMs and humans, statistical\nlinguistic analysis, philosophical considerations, and more. In this brief\npaper we present a counter-argument based upon a thought experiment and\nsemi-formal considerations leading to an inherent ambiguity barrier which\nprevents LLMs from having any understanding of what their amazingly fluent\ndialogues mean."}
{"id": "2505.01658", "pdf": "https://arxiv.org/pdf/2505.01658.pdf", "abs": "https://arxiv.org/abs/2505.01658", "title": "A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency", "authors": ["Sihyeong Park", "Sungryeol Jeon", "Chaelyn Lee", "Seokhun Jeon", "Byung-Soo Kim", "Jemin Lee"], "categories": ["cs.CL"], "comment": "Under review; 65 pages; 27 figures", "summary": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine"}
{"id": "2505.01877", "pdf": "https://arxiv.org/pdf/2505.01877.pdf", "abs": "https://arxiv.org/abs/2505.01877", "title": "Humans can learn to detect AI-generated texts, or at least learn when they can't", "authors": ["Jiří Milička", "Anna Marklová", "Ondřej Drobil", "Eva Pospíšilová"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study investigates whether individuals can learn to accurately\ndiscriminate between human-written and AI-produced texts when provided with\nimmediate feedback, and if they can use this feedback to recalibrate their\nself-perceived competence. We also explore the specific criteria individuals\nrely upon when making these decisions, focusing on textual style and perceived\nreadability.\n  We used GPT-4o to generate several hundred texts across various genres and\ntext types comparable to Koditex, a multi-register corpus of human-written\ntexts. We then presented randomized text pairs to 254 Czech native speakers who\nidentified which text was human-written and which was AI-generated.\nParticipants were randomly assigned to two conditions: one receiving immediate\nfeedback after each trial, the other receiving no feedback until experiment\ncompletion. We recorded accuracy in identification, confidence levels, response\ntimes, and judgments about text readability along with demographic data and\nparticipants' engagement with AI technologies prior to the experiment.\n  Participants receiving immediate feedback showed significant improvement in\naccuracy and confidence calibration. Participants initially held incorrect\nassumptions about AI-generated text features, including expectations about\nstylistic rigidity and readability. Notably, without feedback, participants\nmade the most errors precisely when feeling most confident -- an issue largely\nresolved among the feedback group.\n  The ability to differentiate between human and AI-generated texts can be\neffectively learned through targeted training with explicit feedback, which\nhelps correct misconceptions about AI stylistic features and readability, as\nwell as potential other variables that were not explored, while facilitating\nmore accurate self-assessment. This finding might be particularly important in\neducational contexts."}
{"id": "2505.03005", "pdf": "https://arxiv.org/pdf/2505.03005.pdf", "abs": "https://arxiv.org/abs/2505.03005", "title": "RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale", "authors": ["Daniel Goldstein", "Eric Alcaide", "Janna Lu", "Eugene Cheah"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": null, "summary": "We present Rapid Attention Distillation to Linear Attention Decoders at Scale\n(RADLADS), a protocol for rapidly converting softmax attention transformers\ninto linear attention decoder models, along with two new RWKV-variant\narchitectures, and models converted from popular Qwen2.5 open source models in\n7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens,\nless than 0.005% of the token count used to train the original teacher models.\nConverting to our 72B linear attention model costs less than \\$2,000 USD at\ntoday's prices, yet quality at inference remains close to the original\ntransformer. These models achieve state-of-the-art downstream performance\nacross a set of standard benchmarks for linear attention models of their size.\nWe release all our models on HuggingFace under the Apache 2.0 license, with the\nexception of our 72B models which are also governed by the Qwen License\nAgreement.\n  Models at\nhttps://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102\nTraining Code at https://github.com/recursal/RADLADS-paper"}
{"id": "2307.02075", "pdf": "https://arxiv.org/pdf/2307.02075.pdf", "abs": "https://arxiv.org/abs/2307.02075", "title": "Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for Entity Alignment", "authors": ["Qijie Ding", "Jie Yin", "Daokun Zhang", "Junbin Gao"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Entity alignment (EA) aims at identifying equivalent entity pairs across\ndifferent knowledge graphs (KGs) that refer to the same real-world identity. To\ncircumvent the shortage of seed alignments provided for training, recent EA\nmodels utilize pseudo-labeling strategies to iteratively add unaligned entity\npairs predicted with high confidence to the seed alignments for model training.\nHowever, the adverse impact of confirmation bias during pseudo-labeling has\nbeen largely overlooked, thus hindering entity alignment performance. To\nsystematically combat confirmation bias for pseudo-labeling-based entity\nalignment, we propose a Unified Pseudo-Labeling framework for Entity Alignment\n(UPL-EA) that explicitly eliminates pseudo-labeling errors to boost the\naccuracy of entity alignment. UPL-EA consists of two complementary components:\n(1) Optimal Transport (OT)-based pseudo-labeling uses discrete OT modeling as\nan effective means to determine entity correspondences and reduce erroneous\nmatches across two KGs. An effective criterion is derived to infer\npseudo-labeled alignments that satisfy one-to-one correspondences; (2) Parallel\npseudo-label ensembling refines pseudo-labeled alignments by combining\npredictions over multiple models independently trained in parallel. The\nensembled pseudo-labeled alignments are thereafter used to augment seed\nalignments to reinforce subsequent model training for alignment inference. The\neffectiveness of UPL-EA in eliminating pseudo-labeling errors is both\ntheoretically supported and experimentally validated. Our extensive results and\nin-depth analyses demonstrate the superiority of UPL-EA over 15 competitive\nbaselines and its utility as a general pseudo-labeling framework for entity\nalignment."}
{"id": "2404.04545", "pdf": "https://arxiv.org/pdf/2404.04545.pdf", "abs": "https://arxiv.org/abs/2404.04545", "title": "TCAN: Text-oriented Cross Attention Network for Multimodal Sentiment Analysis", "authors": ["Weize Quan", "Yunfei Feng", "Ming Zhou", "Yunzhen Zhao", "Tong Wang", "Dong-Ming Yan"], "categories": ["cs.MM", "cs.CL"], "comment": null, "summary": "Multimodal Sentiment Analysis (MSA) endeavors to understand human sentiment\nby leveraging language, visual, and acoustic modalities. Despite the remarkable\nperformance exhibited by previous MSA approaches, the presence of inherent\nmultimodal heterogeneities poses a challenge, with the contribution of\ndifferent modalities varying considerably. Past research predominantly focused\non improving representation learning techniques and feature fusion strategies.\nHowever, many of these efforts overlooked the variation in semantic richness\namong different modalities, treating each modality uniformly. This approach may\nlead to underestimating the significance of strong modalities while\noveremphasizing the importance of weak ones. Motivated by these insights, we\nintroduce a Text-oriented Cross-Attention Network (TCAN), emphasizing the\npredominant role of the text modality in MSA. Specifically, for each multimodal\nsample, by taking unaligned sequences of the three modalities as inputs, we\ninitially allocate the extracted unimodal features into a visual-text and an\nacoustic-text pair. Subsequently, we implement self-attention on the text\nmodality and apply text-queried cross-attention to the visual and acoustic\nmodalities. To mitigate the influence of noise signals and redundant features,\nwe incorporate a gated control mechanism into the framework. Additionally, we\nintroduce unimodal joint learning to gain a deeper understanding of homogeneous\nemotional tendencies across diverse modalities through backpropagation.\nExperimental results demonstrate that TCAN consistently outperforms\nstate-of-the-art MSA methods on two datasets (CMU-MOSI and CMU-MOSEI)."}
{"id": "2406.06600", "pdf": "https://arxiv.org/pdf/2406.06600.pdf", "abs": "https://arxiv.org/abs/2406.06600", "title": "HORAE: A Domain-Agnostic Language for Automated Service Regulation", "authors": ["Yutao Sun", "Mingshuai Chen", "Tiancheng Zhao", "Kangjia Zhao", "He Li", "Jintao Chen", "Zhongyi Wang", "Liqiang Lu", "Xinkui Zhao", "Shuiguang Deng", "Jianwei Yin"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Artificial intelligence is rapidly encroaching on the field of service\nregulation. However, existing AI-based regulation techniques are often tailored\nto specific application domains and thus are difficult to generalize in an\nautomated manner. This paper presents Horae, a unified specification language\nfor modeling (multimodal) regulation rules across a diverse set of domains. We\nshowcase how Horae facilitates an intelligent service regulation pipeline by\nfurther exploiting a fine-tuned large language model named RuleGPT that\nautomates the Horae modeling process, thereby yielding an end-to-end framework\nfor fully automated intelligent service regulation. The feasibility and\neffectiveness of our framework are demonstrated over a benchmark of various\nreal-world regulation domains. In particular, we show that our open-sourced,\nfine-tuned RuleGPT with 7B parameters suffices to outperform GPT-3.5 and\nperform on par with GPT-4o."}
{"id": "2409.03757", "pdf": "https://arxiv.org/pdf/2409.03757.pdf", "abs": "https://arxiv.org/abs/2409.03757", "title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding", "authors": ["Yunze Man", "Shuhong Zheng", "Zhipeng Bao", "Martial Hebert", "Liang-Yan Gui", "Yu-Xiong Wang"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "NeurIPS 2024. Project page: https://yunzeman.github.io/lexicon3d\n  Github: https://github.com/YunzeMan/Lexicon3D", "summary": "Complex 3D scene understanding has gained increasing attention, with scene\nencoding strategies playing a crucial role in this success. However, the\noptimal scene encoding strategies for various scenarios remain unclear,\nparticularly compared to their image-based counterparts. To address this issue,\nwe present a comprehensive study that probes various visual encoding models for\n3D scene understanding, identifying the strengths and limitations of each model\nacross different scenarios. Our evaluation spans seven vision foundation\nencoders, including image-based, video-based, and 3D foundation models. We\nevaluate these models in four tasks: Vision-Language Scene Reasoning, Visual\nGrounding, Segmentation, and Registration, each focusing on different aspects\nof scene understanding. Our evaluations yield key findings: DINOv2 demonstrates\nsuperior performance, video models excel in object-level tasks, diffusion\nmodels benefit geometric tasks, and language-pretrained models show unexpected\nlimitations in language-related tasks. These insights challenge some\nconventional understandings, provide novel perspectives on leveraging visual\nfoundation models, and highlight the need for more flexible encoder selection\nin future vision-language and scene-understanding tasks. Code:\nhttps://github.com/YunzeMan/Lexicon3D"}
{"id": "2411.08884", "pdf": "https://arxiv.org/pdf/2411.08884.pdf", "abs": "https://arxiv.org/abs/2411.08884", "title": "Quantifying Risk Propensities of Large Language Models: Ethical Focus and Bias Detection through Role-Play", "authors": ["Yifan Zeng", "Liang Kairong", "Fangzhou Dong", "Peijia Zheng"], "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "Accepted by CogSci 2025", "summary": "As Large Language Models (LLMs) become more prevalent, concerns about their\nsafety, ethics, and potential biases have risen. Systematically evaluating\nLLMs' risk decision-making tendencies and attitudes, particularly in the\nethical domain, has become crucial. This study innovatively applies the\nDomain-Specific Risk-Taking (DOSPERT) scale from cognitive science to LLMs and\nproposes a novel Ethical Decision-Making Risk Attitude Scale (EDRAS) to assess\nLLMs' ethical risk attitudes in depth. We further propose a novel approach\nintegrating risk scales and role-playing to quantitatively evaluate systematic\nbiases in LLMs. Through systematic evaluation and analysis of multiple\nmainstream LLMs, we assessed the \"risk personalities\" of LLMs across multiple\ndomains, with a particular focus on the ethical domain, and revealed and\nquantified LLMs' systematic biases towards different groups. This research\nhelps understand LLMs' risk decision-making and ensure their safe and reliable\napplication. Our approach provides a tool for identifying and mitigating\nbiases, contributing to fairer and more trustworthy AI systems. The code and\ndata are available."}
{"id": "2502.18635", "pdf": "https://arxiv.org/pdf/2502.18635.pdf", "abs": "https://arxiv.org/abs/2502.18635", "title": "Faster, Cheaper, Better: Multi-Objective Hyperparameter Optimization for LLM and RAG Systems", "authors": ["Matthew Barker", "Andrew Bell", "Evan Thomas", "James Carr", "Thomas Andrews", "Umang Bhatt"], "categories": ["cs.LG", "cs.AI", "cs.CL", "68T20, 68Q32, 90C29, 62P30", "I.2.6; I.2.7; G.1.6; G.3"], "comment": null, "summary": "While Retrieval Augmented Generation (RAG) has emerged as a popular technique\nfor improving Large Language Model (LLM) systems, it introduces a large number\nof choices, parameters and hyperparameters that must be made or tuned. This\nincludes the LLM, embedding, and ranker models themselves, as well as\nhyperparameters governing individual RAG components. Yet, collectively\noptimizing the entire configuration in a RAG or LLM system remains\nunder-explored - especially in multi-objective settings - due to intractably\nlarge solution spaces, noisy objective evaluations, and the high cost of\nevaluations. In this work, we introduce the first approach for multi-objective\nparameter optimization of cost, latency, safety and alignment over entire LLM\nand RAG systems. We find that Bayesian optimization methods significantly\noutperform baseline approaches, obtaining a superior Pareto front on two new\nRAG benchmark tasks. We conclude our work with important considerations for\npractitioners who are designing multi-objective RAG systems, highlighting\nnuances such as how optimal configurations may not generalize across tasks and\nobjectives."}
{"id": "2502.20170", "pdf": "https://arxiv.org/pdf/2502.20170.pdf", "abs": "https://arxiv.org/abs/2502.20170", "title": "Re-evaluating Open-ended Evaluation of Large Language Models", "authors": ["Siqi Liu", "Ian Gemp", "Luke Marris", "Georgios Piliouras", "Nicolas Heess", "Marc Lanctot"], "categories": ["cs.GT", "cs.CL", "cs.LG", "stat.ML"], "comment": "Published at ICLR 2025", "summary": "Evaluation has traditionally focused on ranking candidates for a specific\nskill. Modern generalist models, such as Large Language Models (LLMs),\ndecidedly outpace this paradigm. Open-ended evaluation systems, where candidate\nmodels are compared on user-submitted prompts, have emerged as a popular\nsolution. Despite their many advantages, we show that the current Elo-based\nrating systems can be susceptible to and even reinforce biases in data,\nintentional or accidental, due to their sensitivity to redundancies. To address\nthis issue, we propose evaluation as a 3-player game, and introduce novel\ngame-theoretic solution concepts to ensure robustness to redundancy. We show\nthat our method leads to intuitive ratings and provide insights into the\ncompetitive landscape of LLM development."}
{"id": "2504.02107", "pdf": "https://arxiv.org/pdf/2504.02107.pdf", "abs": "https://arxiv.org/abs/2504.02107", "title": "TiC-LM: A Web-Scale Benchmark for Time-Continual LLM Pretraining", "authors": ["Jeffrey Li", "Mohammadreza Armandpour", "Iman Mirzadeh", "Sachin Mehta", "Vaishaal Shankar", "Raviteja Vemulapalli", "Samy Bengio", "Oncel Tuzel", "Mehrdad Farajtabar", "Hadi Pouransari", "Fartash Faghri"], "categories": ["cs.LG", "cs.CL"], "comment": "Code available at: https://github.com/apple/ml-tic-lm", "summary": "Large Language Models (LLMs) trained on historical web data inevitably become\noutdated. We investigate evaluation strategies and update methods for LLMs as\nnew data becomes available. We introduce a web-scale dataset for time-continual\npretraining of LLMs derived from 114 dumps of Common Crawl (CC) - orders of\nmagnitude larger than previous continual language modeling benchmarks. We also\ndesign time-stratified evaluations across both general CC data and specific\ndomains (Wikipedia, StackExchange, and code documentation) to assess how well\nvarious continual learning methods adapt to new data while retaining past\nknowledge. Our findings demonstrate that, on general CC data, autoregressive\nmeta-schedules combined with a fixed-ratio replay of older data can achieve\ncomparable held-out loss to re-training from scratch, while requiring\nsignificantly less computation (2.6x). However, the optimal balance between\nincorporating new data and replaying old data differs as replay is crucial to\navoid forgetting on generic web data but less so on specific domains."}
{"id": "2504.19981", "pdf": "https://arxiv.org/pdf/2504.19981.pdf", "abs": "https://arxiv.org/abs/2504.19981", "title": "Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided GFlowNets", "authors": ["Adam Younsi", "Abdalgader Abubaker", "Mohamed El Amine Seddik", "Hakim Hacid", "Salem Lahlou"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Achieving both accuracy and diverse reasoning remains challenging for Large\nLanguage Models (LLMs) in complex domains like mathematics. A key bottleneck is\nevaluating intermediate reasoning steps to guide generation without costly\nhuman annotations. To address this, we first introduce a novel Process Reward\nModel (PRM) trained automatically using Monte Carlo Tree Search coupled with a\nsimilarity-based data augmentation technique, effectively capturing step-level\nreasoning quality. Leveraging this PRM, we then adapt Generative Flow Networks\n(GFlowNets) to operate at the reasoning step level. Unlike traditional\nreinforcement learning focused on maximizing a single reward, GFlowNets\nnaturally sample diverse, high-quality solutions proportional to their rewards,\nas measured by our PRM. Empirical evaluation shows strong improvements in both\naccuracy and solution diversity on challenging mathematical benchmarks (e.g.,\n+2.59% absolute accuracy on MATH Level 5 for Llama3.2-3B), with effective\ngeneralization to unseen datasets (+9.4% absolute on SAT MATH). Our work\ndemonstrates the potential of PRM-guided, step-level GFlowNets for developing\nmore robust and versatile mathematical reasoning in LLMs."}
{"id": "2504.21435", "pdf": "https://arxiv.org/pdf/2504.21435.pdf", "abs": "https://arxiv.org/abs/2504.21435", "title": "SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding", "authors": ["Chenkai Zhang", "Yiming Lei", "Zeming Liu", "Haitao Leng", "Shaoguo Liu", "Tingting Gao", "Qingjie Liu", "Yunhong Wang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "29 pages, 15 figures, CVPR 2025", "summary": "With the rapid development of Multi-modal Large Language Models (MLLMs), an\nincreasing number of benchmarks have been established to evaluate the video\nunderstanding capabilities of these models. However, these benchmarks focus on\nstandalone videos and mainly assess \"visual elements\" like human actions and\nobject states. In reality, contemporary videos often encompass complex and\ncontinuous narratives, typically presented as a series. To address this\nchallenge, we propose SeriesBench, a benchmark consisting of 105 carefully\ncurated narrative-driven series, covering 28 specialized tasks that require\ndeep narrative understanding. Specifically, we first select a diverse set of\ndrama series spanning various genres. Then, we introduce a novel long-span\nnarrative annotation method, combined with a full-information transformation\napproach to convert manual annotations into diverse task formats. To further\nenhance model capacity for detailed analysis of plot structures and character\nrelationships within series, we propose a novel narrative reasoning framework,\nPC-DCoT. Extensive results on SeriesBench indicate that existing MLLMs still\nface significant challenges in understanding narrative-driven series, while\nPC-DCoT enables these MLLMs to achieve performance improvements. Overall, our\nSeriesBench and PC-DCoT highlight the critical necessity of advancing model\ncapabilities to understand narrative-driven series, guiding the future\ndevelopment of MLLMs. SeriesBench is publicly available at\nhttps://github.com/zackhxn/SeriesBench-CVPR2025."}
{"id": "2505.00831", "pdf": "https://arxiv.org/pdf/2505.00831.pdf", "abs": "https://arxiv.org/abs/2505.00831", "title": "SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation", "authors": ["Quang P. M. Pham", "Khoi T. N. Nguyen", "Nhi H. Doan", "Cuong A. Pham", "Kentaro Inui", "Dezhen Song"], "categories": ["cs.RO", "cs.CL"], "comment": "Paper is under review", "summary": "Efficient path planning in robotics, particularly within large-scale, dynamic\nenvironments, remains a significant hurdle. While Large Language Models (LLMs)\noffer strong reasoning capabilities, their high computational cost and limited\nadaptability in dynamic scenarios hinder real-time deployment on edge devices.\nWe present SmallPlan -- a novel framework leveraging LLMs as teacher models to\ntrain lightweight Small Language Models (SLMs) for high-level path planning\ntasks. In SmallPlan, the SLMs provide optimal action sequences to navigate\nacross scene graphs that compactly represent full-scaled 3D scenes. The SLMs\nare trained in a simulation-powered, interleaved manner with LLM-guided\nsupervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not\nonly enables SLMs to successfully complete navigation tasks but also makes them\naware of important factors like travel distance and number of trials. Through\nexperiments, we demonstrate that the fine-tuned SLMs perform competitively with\nlarger models like GPT-4o on sequential path planning, without suffering from\nhallucination and overfitting. SmallPlan is resource-efficient, making it\nwell-suited for edge-device deployment and advancing practical autonomous\nrobotics."}
{"id": "2505.02309", "pdf": "https://arxiv.org/pdf/2505.02309.pdf", "abs": "https://arxiv.org/abs/2505.02309", "title": "Optimizing LLMs for Resource-Constrained Environments: A Survey of Model Compression Techniques", "authors": ["Sanjay Surendranath Girija", "Shashank Kapoor", "Lakshit Arora", "Dipen Pradhan", "Aman Raj", "Ankit Shetgaonkar"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to IEEE COMPSAC 2025", "summary": "Large Language Models (LLMs) have revolutionized many areas of artificial\nintelligence (AI), but their substantial resource requirements limit their\ndeployment on mobile and edge devices. This survey paper provides a\ncomprehensive overview of techniques for compressing LLMs to enable efficient\ninference in resource-constrained environments. We examine three primary\napproaches: Knowledge Distillation, Model Quantization, and Model Pruning. For\neach technique, we discuss the underlying principles, present different\nvariants, and provide examples of successful applications. We also briefly\ndiscuss complementary techniques such as mixture-of-experts and early-exit\nstrategies. Finally, we highlight promising future directions, aiming to\nprovide a valuable resource for both researchers and practitioners seeking to\noptimize LLMs for edge deployment."}
{"id": "2505.03961", "pdf": "https://arxiv.org/pdf/2505.03961.pdf", "abs": "https://arxiv.org/abs/2505.03961", "title": "The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete", "authors": ["Gerrit Großmann", "Larisa Ivanova", "Sai Leela Poduru", "Mohaddeseh Tabrizian", "Islam Mesabah", "David A. Selby", "Sebastian J. Vollmer"], "categories": ["cs.AI", "cs.CL", "cs.MA", "I.2.11; I.2.7; I.6; J.4"], "comment": "16 pages, 8 figures. Code available at\n  https://github.com/storyagents25/story-agents", "summary": "According to Yuval Noah Harari, large-scale human cooperation is driven by\nshared narratives that encode common beliefs and values. This study explores\nwhether such narratives can similarly nudge LLM agents toward collaboration. We\nuse a finitely repeated public goods game in which LLM agents choose either\ncooperative or egoistic spending strategies. We prime agents with stories\nhighlighting teamwork to different degrees and test how this influences\nnegotiation outcomes. Our experiments explore four questions:(1) How do\nnarratives influence negotiation behavior? (2) What differs when agents share\nthe same story versus different ones? (3) What happens when the agent numbers\ngrow? (4) Are agents resilient against self-serving negotiators? We find that\nstory-based priming significantly affects negotiation strategies and success\nrates. Common stories improve collaboration, benefiting each agent. By\ncontrast, priming agents with different stories reverses this effect, and those\nagents primed toward self-interest prevail. We hypothesize that these results\ncarry implications for multi-agent system design and AI alignment."}
