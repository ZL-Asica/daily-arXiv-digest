{"id": "2505.15971", "pdf": "https://arxiv.org/pdf/2505.15971.pdf", "abs": "https://arxiv.org/abs/2505.15971", "title": "A Paradigm for Creative Ownership", "authors": ["Tejaswi Polimetla", "Katy Ilonka Gero"], "categories": ["cs.HC"], "comment": "Symposium on Human-Computer Interaction for Work (CHIWORK) 2025", "summary": "As generative AI tools become more integrated into creative workflows,\nquestions of ownership in co-creative contexts have become increasingly urgent.\nWhile legal frameworks offer definitions of ownership rooted in intellectual\nproperty, they often overlook the nuanced, psychological experience of creative\nownership - how individuals come to feel that a creative product is \"theirs.\"\nDrawing on interdisciplinary literature in philosophy, psychology, and the\nsocial sciences and humanities more broadly, we introduce a new framework that\nsurfaces the material and immaterial dimensions of creative ownership. Our\nmodel organizes creative ownership into three domains - Person, Process, and\nSystem - each of which contains subdimensions that shape ownership sentiment.\nWe offer an accompanying interactive tool that enables creators and researchers\nto visualize and evaluate ownership across a range of contexts. This paradigm\nprovides a new lens through which to understand and support creative agency in\nhuman-AI collaboration, and lays the groundwork for future empirical research\nin design and human-computer interaction.", "AI": {"tldr": "This paper introduces a framework for understanding creative ownership in the context of human-AI collaboration, highlighting psychological, material, and immaterial dimensions.", "motivation": "The integration of generative AI in creative workflows raises pressing questions about ownership and personal connection to creative products.", "method": "The authors develop a framework categorizing creative ownership into three domains: Person, Process, and System, with an interactive tool for evaluation.", "result": "The framework helps visualize and assess ownership sentiment, aiding in understanding creative agency in human-AI collaboration.", "conclusion": "This model provides a new perspective for supporting creative agency and encourages empirical research in design and HCI-related fields.", "key_contributions": ["Introduces a triadic framework for creative ownership.", "Develops an interactive tool for evaluating ownership sentiment.", "Links the complexities of ownership to human-AI collaboration in creative processes."], "limitations": "The framework may require further empirical validation and testing across diverse contexts.", "keywords": ["creative ownership", "human-AI collaboration", "HCI", "creative agency", "interactive tool"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.15973", "pdf": "https://arxiv.org/pdf/2505.15973.pdf", "abs": "https://arxiv.org/abs/2505.15973", "title": "An Exploratory Study on Multi-modal Generative AI in AR Storytelling", "authors": ["Hyungjun Doh", "Jingyu Shi", "Rahul Jain", "Heesoo Kim", "Karthik Ramani"], "categories": ["cs.HC"], "comment": null, "summary": "Storytelling in AR has gained attention due to its multi-modality and\ninteractivity. However, generating multi-modal content for AR storytelling\nrequires expertise and efforts for high-quality conveyance of the narrator's\nintention. Recently, Generative-AI (GenAI) has shown promising applications in\nmulti-modal content generation. Despite the potential benefit, current research\ncalls for validating the effect of AI-generated content (AIGC) in AR\nStorytelling. Therefore, we conducted an exploratory study to investigate the\nutilization of GenAI. Analyzing 223 AR videos, we identified a design space for\nmulti-modal AR Storytelling. Based on the design space, we developed a testbed\nfacilitating multi-modal content generation and atomic elements in AR\nStorytelling. Through two studies with N=30 experienced storytellers and live\npresenters, we 1. revealed participants' preferences for modalities, 2.\nevaluated the interactions with AI to generate content, and 3. assessed the\nquality of the AIGC for AR Storytelling. We further discussed design\nconsiderations for future AR Storytelling with GenAI.", "AI": {"tldr": "This paper explores the impact and effectiveness of Generative AI in creating multi-modal content for Augmented Reality (AR) storytelling through an analysis of AR videos and user studies.", "motivation": "To investigate the role of AI-generated content (AIGC) in AR storytelling and validate its effects on multi-modal content generation.", "method": "An exploratory study analyzing 223 AR videos, followed by two studies with 30 experienced storytellers to evaluate preferences for modalities and the quality of AIGC.", "result": "Preferences for various modalities were revealed, and interactions with AI for content generation were evaluated, providing insights into the quality of generated content for AR storytelling.", "conclusion": "The study identifies a design space for multi-modal AR storytelling and suggests future design considerations involving the use of Generative AI.", "key_contributions": ["Identification of a design space for multi-modal AR storytelling", "Development of a testbed for content generation", "Evaluation of storyteller preferences and quality of AI-generated content"], "limitations": "The study is limited by the sample size of N=30 storytellers and the focus on specific modalities within AR storytelling.", "keywords": ["Augmented Reality", "Generative AI", "multi-modal storytelling", "AI-generated content", "human-computer interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.15974", "pdf": "https://arxiv.org/pdf/2505.15974.pdf", "abs": "https://arxiv.org/abs/2505.15974", "title": "Real-Time Stress Monitoring, Detection, and Management in College Students: A Wearable Technology and Machine-Learning Approach", "authors": ["Alan Ta", "Nilsu Salgin", "Mustafa Demir", "Kala Philips Randal", "Ranjana K. Mehta", "Anthony McDonald", "Carly McCord", "Farzan Sasangohar"], "categories": ["cs.HC", "cs.LG"], "comment": "31 pages, 5 figures", "summary": "College students are increasingly affected by stress, anxiety, and\ndepression, yet face barriers to traditional mental health care. This study\nevaluated the efficacy of a mobile health (mHealth) intervention, Mental Health\nEvaluation and Lookout Program (mHELP), which integrates a smartwatch sensor\nand machine learning (ML) algorithms for real-time stress detection and\nself-management. In a 12-week randomized controlled trial (n = 117),\nparticipants were assigned to a treatment group using mHELP's full suite of\ninterventions or a control group using the app solely for real-time stress\nlogging and weekly psychological assessments. The primary outcome, \"Moments of\nStress\" (MS), was assessed via physiological and self-reported indicators and\nanalyzed using Generalized Linear Mixed Models (GLMM) approaches. Similarly,\nsecondary outcomes of psychological assessments, including the Generalized\nAnxiety Disorder-7 (GAD-7) for anxiety, the Patient Health Questionnaire\n(PHQ-8) for depression, and the Perceived Stress Scale (PSS), were also\nanalyzed via GLMM. The finding of the objective measure, MS, indicates a\nsubstantial decrease in MS among the treatment group compared to the control\ngroup, while no notable between-group differences were observed in subjective\nscores of anxiety (GAD-7), depression (PHQ-8), or stress (PSS). However, the\ntreatment group exhibited a clinically meaningful decline in GAD-7 and PSS\nscores. These findings underscore the potential of wearable-enabled mHealth\ntools to reduce acute stress in college populations and highlight the need for\nextended interventions and tailored features to address chronic symptoms like\ndepression.", "AI": {"tldr": "This study evaluates a mobile health intervention, mHELP, which utilizes a smartwatch sensor and machine learning for real-time stress detection among college students. It found a significant reduction in moments of stress among users compared to a control group, while showing the need for tailored interventions for chronic conditions.", "motivation": "To address the increasing impact of stress, anxiety, and depression on college students and the barriers they face to traditional mental health care.", "method": "A 12-week randomized controlled trial involving 117 participants, comparing a treatment group using the full mHELP intervention to a control group using only stress logging and assessments, with outcomes analyzed through Generalized Linear Mixed Models.", "result": "The treatment group experienced a significant decrease in objective measures of Moments of Stress, whereas subjective measures showed clinically meaningful improvements but no significant between-group differences.", "conclusion": "Wearable-enabled mHealth tools have the potential to alleviate acute stress among college populations, but more tailored and extended interventions are required for chronic mental health issues like depression.", "key_contributions": ["Demonstrated the efficacy of a wearable mHealth intervention for stress management.", "Identified the difference between objective and subjective measures of mental health outcomes.", "Outlined the need for more targeted interventions in mobile health applications."], "limitations": "No notable between-group differences in subjective outcomes for anxiety, depression, and stress.", "keywords": ["mHealth", "stress detection", "wearable technology", "machine learning", "college mental health"], "importance_score": 9, "read_time_minutes": 31}}
{"id": "2505.16011", "pdf": "https://arxiv.org/pdf/2505.16011.pdf", "abs": "https://arxiv.org/abs/2505.16011", "title": "Exploring Perception-Based Techniques for Redirected Walking in VR: A Comprehensive Survey", "authors": ["Bradley Coles", "Yahya Hmaiti", "Joseph J. LaViola Jr"], "categories": ["cs.HC"], "comment": null, "summary": "We present a comprehensive survey of perception-based redirected walking\n(RDW) techniques in virtual reality (VR), presenting a taxonomy that serves as\na framework for understanding and designing RDW algorithms. RDW enables users\nto explore virtual environments (VEs) larger than their physical space,\naddressing the constraints of real walking in limited home VR setups. Our\nreview spans 232 papers, with 165 included in the final analysis. We categorize\nperception-based RDW techniques based on gains, gain application, target\norientation calculation, and optional general enhancements, identifying key\npatterns and relationships. We present data on how current work aligns within\nthis classification system and suggest how this data can guide future work into\nareas that are relatively under explored. This taxonomy clarifies\nperception-based RDW techniques, guiding the design and application of RDW\nsystems, and suggests future research directions to enhance VR user experience.", "AI": {"tldr": "Comprehensive survey of perception-based redirected walking (RDW) techniques in virtual reality, presenting a taxonomy for understanding and designing RDW algorithms.", "motivation": "To understand and address the constraints of real walking in limited physical spaces while exploring larger virtual environments.", "method": "A thorough review of 232 papers related to RDW, resulting in a taxonomy based on gains, gain application, target orientation calculation, and general enhancements.", "result": "Identification of key patterns and relationships among perception-based RDW techniques, with 165 papers included in the final analysis.", "conclusion": "The taxonomy clarifies RDW techniques, guiding design and application, and suggests future directions for enhancing VR user experiences.", "key_contributions": ["A comprehensive taxonomy of perception-based RDW techniques.", "Insights into 232 related papers, aiding in the understanding of RDW algorithms.", "Guidelines for future research in underexplored areas of RDW."], "limitations": "", "keywords": ["redirected walking", "virtual reality", "user experience", "taxonomy", "perception-based techniques"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.16034", "pdf": "https://arxiv.org/pdf/2505.16034.pdf", "abs": "https://arxiv.org/abs/2505.16034", "title": "\"AI just keeps guessing\": Using ARC Puzzles to Help Children Identify Reasoning Errors in Generative AI", "authors": ["Aayushi Dangol", "Trushaa Ramanan", "Runhua Zhao", "Julie A. Kientz", "Robert Wolfe", "Jason Yip"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "The integration of generative Artificial Intelligence (genAI) into everyday\nlife raises questions about the competencies required to critically engage with\nthese technologies. Unlike visual errors in genAI, textual mistakes are often\nharder to detect and require specific domain knowledge. Furthermore, AI's\nauthoritative tone and structured responses can create an illusion of\ncorrectness, leading to overtrust, especially among children. To address this,\nwe developed AI Puzzlers, an interactive system based on the Abstraction and\nReasoning Corpus (ARC), to help children identify and analyze errors in genAI.\nDrawing on Mayer & Moreno's Cognitive Theory of Multimedia Learning, AI\nPuzzlers uses visual and verbal elements to reduce cognitive overload and\nsupport error detection. Based on two participatory design sessions with 21\nchildren (ages 6 - 11), our findings provide both design insights and an\nempirical understanding of how children identify errors in genAI reasoning,\ndevelop strategies for navigating these errors, and evaluate AI outputs.", "AI": {"tldr": "The paper introduces AI Puzzlers, an interactive system designed to help children critically engage with generative AI by identifying and analyzing errors in its outputs.", "motivation": "The need to equip children with competencies to critically assess generative AI due to the misleading confidence it can convey through its authoritative tone.", "method": "The system was developed based on Mayer & Moreno's Cognitive Theory of Multimedia Learning and involves children in participatory design sessions for feedback.", "result": "Findings reveal how children identify errors in AI reasoning, strategize to navigate those errors, and evaluate AI outputs effectively.", "conclusion": "AI Puzzlers offers insights into designing educational tools for enhancing children's interaction with generative AI.", "key_contributions": ["Development of AI Puzzlers for error detection in genAI outputs", "Empirical insights from participatory design with children", "Application of cognitive theories to enhance learning about genAI"], "limitations": "The study is limited to the engagement of children aged 6-11 and may not generalize to older populations or different educational contexts.", "keywords": ["generative AI", "error detection", "interactivity", "children", "Cognitive Theory"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.16057", "pdf": "https://arxiv.org/pdf/2505.16057.pdf", "abs": "https://arxiv.org/abs/2505.16057", "title": "Signals of Provenance: Practices & Challenges of Navigating Indicators in AI-Generated Media for Sighted and Blind Individuals", "authors": ["Ayae Ide", "Tory Park", "Jaron Mink", "Tanusree Sharma"], "categories": ["cs.HC", "cs.AI", "cs.MM"], "comment": null, "summary": "AI-Generated (AIG) content has become increasingly widespread by recent\nadvances in generative models and the easy-to-use tools that have significantly\nlowered the technical barriers for producing highly realistic audio, images,\nand videos through simple natural language prompts. In response, platforms are\nadopting provable provenance with platforms recommending AIG to be\nself-disclosed and signaled to users. However, these indicators may be often\nmissed, especially when they rely solely on visual cues and make them\nineffective to users with different sensory abilities. To address the gap, we\nconducted semi-structured interviews (N=28) with 15 sighted and 13 BLV\nparticipants to examine their interaction with AIG content through\nself-disclosed AI indicators. Our findings reveal diverse mental models and\npractices, highlighting different strengths and weaknesses of content-based\n(e.g., title, description) and menu-aided (e.g., AI labels) indicators. While\nsighted participants leveraged visual and audio cues, BLV participants\nprimarily relied on audio and existing assistive tools, limiting their ability\nto identify AIG. Across both groups, they frequently overlooked menu-aided\nindicators deployed by platforms and rather interacted with content-based\nindicators such as title and comments. We uncovered usability challenges\nstemming from inconsistent indicator placement, unclear metadata, and cognitive\noverload. These issues were especially critical for BLV individuals due to the\ninsufficient accessibility of interface elements. We provide practical\nrecommendations and design implications for future AIG indicators across\nseveral dimensions.", "AI": {"tldr": "The study examines the interactions of sighted and blind/low vision (BLV) individuals with AI-generated content through self-disclosed indicators, revealing usability challenges and proposing design recommendations.", "motivation": "To address the accessibility issues of AI-generated content (AIG) indicators for users with different sensory abilities, particularly blind/low vision individuals.", "method": "Conducted semi-structured interviews with 28 participants (15 sighted, 13 BLV) to explore their interactions with AIG content and indicators.", "result": "Findings indicate that sighted participants utilize visual and auditory cues effectively, while BLV participants rely heavily on audio and assistive tools, often missing menu-aided indicators and facing usability challenges.", "conclusion": "The study highlights the need for improved design of AIG indicators to enhance accessibility for all users and offers practical recommendations based on identified challenges.", "key_contributions": ["Examined diverse mental models of sighted and BLV users regarding AIG indicators.", "Identified usability challenges that hinder effective interaction with AIG content for BLV individuals.", "Provided practical recommendations for designing more accessible AIG indicators."], "limitations": "The study's findings may be limited to the specific group of participants interviewed and may not generalize to all users of AIG content.", "keywords": ["AI-generated content", "human-computer interaction", "accessibility", "blind and low vision", "user experience"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.16089", "pdf": "https://arxiv.org/pdf/2505.16089.pdf", "abs": "https://arxiv.org/abs/2505.16089", "title": "\"If anybody finds out you are in BIG TROUBLE\": Understanding Children's Hopes, Fears, and Evaluations of Generative AI", "authors": ["Aayushi Dangol", "Robert Wolfe", "Daeun Yoo", "Arya Thiruvillakkat", "Ben Chickadel", "Julie A. Kientz"], "categories": ["cs.HC"], "comment": null, "summary": "As generative artificial intelligence (genAI) increasingly mediates how\nchildren learn, communicate, and engage with digital content, understanding\nchildren's hopes and fears about this emerging technology is crucial. In a\npilot study with 37 fifth-graders, we explored how children (ages 9-10)\nenvision genAI and the roles they believe it should play in their daily life.\nOur findings reveal three key ways children envision genAI: as a companion\nproviding guidance, a collaborator working alongside them, and a task automator\nthat offloads responsibilities. However, alongside these hopeful views,\nchildren expressed fears about overreliance, particularly in academic settings,\nlinking it to fears of diminished learning, disciplinary consequences, and\nlong-term failure. This study highlights the need for child-centric AI design\nthat balances these tensions, empowering children with the skills to critically\nengage with and navigate their evolving relationships with digital\ntechnologies.", "AI": {"tldr": "This study investigates children's perceptions of generative AI, identifying both their positive visions and concerns over reliance in educational contexts.", "motivation": "To understand children's hopes and fears regarding generative AI as it increasingly influences their learning and engagement with technology.", "method": "Pilot study with 37 fifth-graders (ages 9-10) exploring their visions of generative AI and its roles in their daily lives.", "result": "Children envision genAI as a companion, collaborator, and task automator, while also expressing fears about overreliance impacting their learning and future.", "conclusion": "The study underscores the necessity for AI design focused on children's needs, promoting skills for critical engagement with technology.", "key_contributions": ["Identified children's positive and negative perceptions of generative AI", "Highlighted the importance of child-centric AI design", "Provided insights into how children envision the roles of AI in education"], "limitations": "", "keywords": ["generative AI", "children", "education", "HCI", "technology perception"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2505.16171", "pdf": "https://arxiv.org/pdf/2505.16171.pdf", "abs": "https://arxiv.org/abs/2505.16171", "title": "Fairness and Efficiency in Human-Agent Teams: An Iterative Algorithm Design Approach", "authors": ["Mai Lee Chang", "Kim Baraka", "Greg Trafton", "Zach Lalu Vazhekatt", "Andrea Lockerd Thomaz"], "categories": ["cs.HC"], "comment": "18 pages, 5 figures", "summary": "When agents interact with people as part of a team, fairness becomes an\nimportant factor. Prior work has proposed fairness metrics based on teammates'\ncapabilities for task allocation within human-agent teams. However, most\nmetrics only consider teammate capabilities from a third-person point of view\n(POV). In this work, we extend these metrics to include task preferences and\nconsider a first-person POV. We leverage an iterative design method consisting\nof simulation data and human data to design a task allocation algorithm that\nbalances task efficiency and fairness based on both capabilities and\npreferences. We first show that these metrics may not align with people's\nperceived fairness from a first-person POV. In light of this result, we propose\na new fairness metric, fair-equity, and the Fair-Efficient Algorithm (FEA). Our\nfindings suggest that an agent teammate who balances efficiency and fairness\nbased on equity will be perceived to be fairer and preferred by human teammates\nin various human-agent team types. We suggest that the perception of fairness\nmay also depend on a person's POV.", "AI": {"tldr": "This paper explores the impact of first-person perspectives on fairness in task allocation within human-agent teams, proposing a novel fairness metric and algorithm that balance efficiency and perceived fairness.", "motivation": "To address the limitations of existing fairness metrics that primarily consider third-person perspectives in team dynamics involving agents and humans.", "method": "An iterative design method is utilized, combining simulation data and human data to develop a task allocation algorithm that incorporates both capabilities and preferences.", "result": "The proposed fair-equity metric and Fair-Efficient Algorithm (FEA) demonstrate improved perceptions of fairness and preference for agents that balance efficiency and fairness based on equity from a first-person point of view.", "conclusion": "Perceived fairness in human-agent interactions is significantly influenced by individual perspectives, highlighting the need for metrics that incorporate personal preferences in task allocation.", "key_contributions": ["Introduction of the fair-equity metric", "Development of the Fair-Efficient Algorithm (FEA)", "Demonstrated significance of first-person POV in perceived fairness"], "limitations": "The study may not generalize to all types of agent-human interactions beyond task allocation.", "keywords": ["human-agent teams", "fairness", "task allocation", "first-person perspective", "machine learning"], "importance_score": 8, "read_time_minutes": 18}}
{"id": "2505.16254", "pdf": "https://arxiv.org/pdf/2505.16254.pdf", "abs": "https://arxiv.org/abs/2505.16254", "title": "Reassessing Collaborative Writing Theories and Frameworks in the Age of LLMs: What Still Applies and What We Must Leave Behind", "authors": ["Daisuke Yukita", "Tim Miller", "Joel Mackenzie"], "categories": ["cs.HC"], "comment": null, "summary": "In this paper, we conduct a critical review of existing theories and\nframeworks on human-human collaborative writing to assess their relevance to\nthe current human-AI paradigm in professional contexts, and draw seven insights\nalong with design implications for human-AI collaborative writing tools. We\nfound that, as LLMs nudge the writing process more towards an empirical \"trial\nand error\" process analogous to prototyping, the non-linear cognitive process\nof writing will stay the same, but more rigor will be required for revision\nmethodologies. This shift would shed further light on the importance of\ncoherence support, but the large language model (LLM)'s unprecedented semantic\ncapabilities can bring novel approaches to this ongoing challenge. We argue\nthat teamwork-related factors such as group awareness, consensus building and\nauthorship - which have been central in human-human collaborative writing\nstudies - should not apply to the human-AI paradigm due to excessive\nanthropomorphism. With the LLM's text generation capabilities becoming\nessentially indistinguishable from human-written ones, we are entering an era\nwhere, for the first time in the history of computing, we are engaging in\ncollaborative writing with AI at workplaces on a daily basis. We aim to bring\ntheoretical grounding and practical design guidance to the interaction designs\nof human-AI collaborative writing, with the goal of enhancing future human-AI\nwriting software.", "AI": {"tldr": "The paper reviews theories on human-human collaborative writing to inform the design of human-AI writing tools, highlighting shifts in methodology due to LLMs.", "motivation": "To assess the relevance of human-human collaborative writing theories in the context of human-AI collaboration, particularly in professional environments.", "method": "Conducting a critical review of existing theories and frameworks on collaborative writing and drawing insights for design implications in human-AI collaborative writing tools.", "result": "Findings indicate that while writing remains a non-linear cognitive process, LLMs introduce a trial-and-error methodology that requires more rigorous revision processes and highlights the need for coherence support.", "conclusion": "The study underscores significant distinctions between human-human and human-AI collaborative writing, suggesting a need for new design frameworks that account for the unique characteristics of LLMs.", "key_contributions": ["Identified seven insights and design implications for human-AI collaborative writing tools.", "Outlined the importance of coherence support in writing with LLMs.", "Discussed the limitations of applying human teamwork concepts to AI collaboration."], "limitations": "The review may not cover all relevant theories comprehensively and focuses primarily on professional contexts.", "keywords": ["Human-AI collaboration", "Collaborative writing", "LLM", "Design implications", "Cognitive processes"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.16352", "pdf": "https://arxiv.org/pdf/2505.16352.pdf", "abs": "https://arxiv.org/abs/2505.16352", "title": "Estimating Perceptual Attributes of Haptic Textures Using Visuo-Tactile Data", "authors": ["Mudassir Ibrahim Awan", "Seokhee Jeon"], "categories": ["cs.HC"], "comment": null, "summary": "Accurate prediction of perceptual attributes of haptic textures is essential\nfor advancing VR and AR applications and enhancing robotic interaction with\nphysical surfaces. This paper presents a deep learning-based multi-modal\nframework, incorporating visual and tactile data, to predict perceptual texture\nratings by leveraging multi-feature inputs. To achieve this, a four-dimensional\nhaptic attribute space encompassing rough-smooth, flat-bumpy, sticky-slippery,\nand hard-soft dimensions is first constructed through psychophysical\nexperiments, where participants evaluate 50 diverse real-world texture samples.\nA physical signal space is subsequently created by collecting visual and\ntactile data from these textures. Finally, a deep learning architecture\nintegrating a CNN-based autoencoder for visual feature learning and a ConvLSTM\nnetwork for tactile data processing is trained to predict user-assigned\nattribute ratings. This multi-modal, multi-feature approach maps physical\nsignals to perceptual ratings, enabling accurate predictions for unseen\ntextures. To evaluate predictive accuracy, we employed leave-one-out\ncross-validation to rigorously assess the model's reliability and\ngeneralizability against several machine learning and deep learning baselines.\nExperimental results demonstrate that the framework consistently outperforms\nsingle-modality approaches, achieving lower MAE and RMSE, highlighting the\nefficacy of combining visual and tactile modalities.", "AI": {"tldr": "This paper presents a deep learning framework that combines visual and tactile data to accurately predict the perceptual attributes of haptic textures, significantly improving performance over single-modality methods.", "motivation": "Accurate prediction of haptic texture attributes is crucial for enhancing VR/AR experiences and robotic interactions with surfaces.", "method": "A deep learning architecture utilizing a CNN-based autoencoder for visual feature learning and a ConvLSTM network for tactile data processing is developed to predict perceptual texture ratings.", "result": "The proposed framework outperforms single-modality approaches, achieving lower mean absolute error (MAE) and root mean square error (RMSE) in texture attribute prediction.", "conclusion": "The multi-modal framework demonstrates improved reliability and generalizability in predicting human perception of haptic textures, making it beneficial for VR/AR and robotics.", "key_contributions": ["Development of a multi-modal and multi-feature framework for haptic texture prediction", "Construction of a four-dimensional haptic attribute space through psychophysical experiments", "Integration of visual and tactile data for enhanced predictive accuracy"], "limitations": "", "keywords": ["Haptic textures", "Deep learning", "Multi-modal framework", "Virtual reality", "Tactile data"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.16702", "pdf": "https://arxiv.org/pdf/2505.16702.pdf", "abs": "https://arxiv.org/abs/2505.16702", "title": "Truth and Trust: Fake News Detection via Biosignals", "authors": ["Gennie Nguyen", "Lei Wang", "Yangxueqing Jiang", "Tom Gedeon"], "categories": ["cs.HC"], "comment": "Research report", "summary": "Understanding how individuals physiologically respond to false information is\ncrucial for advancing misinformation detection systems. This study explores the\npotential of using physiological signals, specifically electrodermal activity\n(EDA) and photoplethysmography (PPG), to classify both the veracity of\ninformation and its interaction with user belief. In a controlled laboratory\nexperiment, we collected EDA and PPG signals while participants evaluated the\ntruthfulness of climate-related claims. Each trial was labeled based on the\nobjective truth of the claim and the participant's belief, enabling two\nclassification tasks: binary veracity detection and a novel four-class joint\nbelief-veracity classification. We extracted handcrafted features from the raw\nsignals and trained several machine learning models to benchmark the dataset.\nOur results show that EDA outperforms PPG, indicating its greater sensitivity\nto physiological responses related to truth perception. However, performance\nsignificantly drops in the joint belief-veracity classification task,\nhighlighting the complexity of modeling the interaction between belief and\ntruth. These findings suggest that while physiological signals can reflect\nbasic truth perception, accurately modeling the intricate relationships between\nbelief and veracity remains a significant challenge. This study emphasizes the\nimportance of multimodal approaches that incorporate psychological,\nphysiological, and cognitive factors to improve fake news detection systems.\nOur work provides a foundation for future research aimed at enhancing\nmisinformation detection via addressing the complexities of human belief and\ntruth processing.", "AI": {"tldr": "This study examines physiological signals for classifying truthfulness of climate-related claims, revealing EDA's superiority over PPG in detecting truth perception, while highlighting the challenges in joint belief-veracity classification.", "motivation": "To advance misinformation detection systems by understanding physiological responses to false information.", "method": "Conducted controlled experiments collecting EDA and PPG signals while participants evaluated climate-related claims' truthfulness, leading to binary and four-class classification tasks.", "result": "EDA signals outperformed PPG in detecting truth perception, but performance dropped significantly in joint belief-veracity tasks.", "conclusion": "While physiological signals can indicate basic truth perception, modeling the interaction between belief and veracity is complex; multimodal approaches are recommended for better fake news detection.", "key_contributions": ["Exploration of EDA and PPG for truth classification", "Introduction of a four-class joint belief-veracity classification task", "Demonstration of the importance of multimodal approaches in misinformation detection."], "limitations": "The performance challenge in joint belief-veracity classification indicates further complexity in accurately modeling relationships between belief and veracity.", "keywords": ["physiological signals", "misinformation detection", "human belief", "truth perception", "machine learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.16730", "pdf": "https://arxiv.org/pdf/2505.16730.pdf", "abs": "https://arxiv.org/abs/2505.16730", "title": "Detecting Fake News Belief via Skin and Blood Flow Signals", "authors": ["Gennie Nguyen", "Lei Wang", "Yangxueqing Jiang", "Tom Gedeon"], "categories": ["cs.HC"], "comment": "Research Report", "summary": "Misinformation poses significant risks to public opinion, health, and\nsecurity. While most fake news detection methods rely on text analysis, little\nis known about how people physically respond to false information or repeated\nexposure to the same statements. This study investigates whether wearable\nsensors can detect belief in a statement or prior exposure to it. We conducted\na controlled experiment where participants evaluated statements while wearing\nan EmotiBit sensor that measured their skin conductance (electrodermal\nactivity, EDA) and peripheral blood flow (photoplethysmography, PPG). From 28\nparticipants, we collected a dataset of 672 trials, each labeled with whether\nthe participant believed the statement and whether they had seen it before.\nThis dataset introduces a new resource for studying physiological responses to\nmisinformation. Using machine learning models, including KNN, CNN, and\nLightGBM, we analyzed these physiological patterns. The best-performing model\nachieved 67.83\\% accuracy, with skin conductance outperforming PPG. These\nfindings demonstrate the potential of wearable sensors as a minimally intrusive\ntool for detecting belief and prior exposure, offering new directions for\nreal-time misinformation detection and adaptive, user-aware systems.", "AI": {"tldr": "This study explores the use of wearable sensors to detect belief in misinformation by measuring physiological responses.", "motivation": "To address the gap in understanding how people physically respond to misinformation and repeated exposure, using wearable technology for detection.", "method": "A controlled experiment with 28 participants measuring skin conductance and peripheral blood flow using an EmotiBit sensor while evaluating statements.", "result": "The study analyzed 672 trials and achieved a maximum model accuracy of 67.83%, demonstrating that skin conductance is a more effective measure than PPG.", "conclusion": "Wearable sensors could serve as effective, minimally intrusive tools for real-time detection of belief and exposure to misinformation, paving the way for user-aware systems.", "key_contributions": ["Introduction of a dataset for studying physiological responses to misinformation", "Demonstration of wearable sensors as a tool for belief detection", "Analysis of machine learning models for detecting misinformation responses"], "limitations": "Limited sample size of 28 participants and the need for further validation of results in diverse contexts.", "keywords": ["misinformation", "wearable sensors", "physiological responses", "machine learning", "health informatics"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.16954", "pdf": "https://arxiv.org/pdf/2505.16954.pdf", "abs": "https://arxiv.org/abs/2505.16954", "title": "Cracking Aegis: An Adversarial LLM-based Game for Raising Awareness of Vulnerabilities in Privacy Protection", "authors": ["Jiaying Fu", "Yiyang Lu", "Zehua Yang", "Fiona Nah", "RAY LC"], "categories": ["cs.HC"], "comment": "24 pages, In Designing Interactive Systems Conference (DIS 25)", "summary": "Traditional methods for raising awareness of privacy protection often fail to\nengage users or provide hands-on insights into how privacy vulnerabilities are\nexploited. To address this, we incorporate an adversarial mechanic in the\ndesign of the dialogue-based serious game Cracking Aegis. Leveraging LLMs to\nsimulate natural interactions, the game challenges players to impersonate\ncharacters and extract sensitive information from an AI agent, Aegis. A user\nstudy (n=22) revealed that players employed diverse deceptive linguistic\nstrategies, including storytelling and emotional rapport, to manipulate Aegis.\nAfter playing, players reported connecting in-game scenarios with real-world\nprivacy vulnerabilities, such as phishing and impersonation, and expressed\nintentions to strengthen privacy control, such as avoiding oversharing personal\ninformation with AI systems. This work highlights the potential of LLMs to\nsimulate complex relational interactions in serious games, while demonstrating\nhow an adversarial game strategy provides unique insights for designs for\nsocial good, particularly privacy protection.", "AI": {"tldr": "The paper presents Cracking Aegis, a serious game utilizing LLMs to engage users in understanding privacy vulnerabilities through adversarial gameplay.", "motivation": "To improve user engagement and awareness of privacy protection methods often lacking in traditional approaches.", "method": "The dialogue-based game involves players impersonating characters to deceive an AI, Aegis, while extracting sensitive information, simulating real-world scenarios.", "result": "User study showed players used various deception strategies and realized connections between the game and real-life privacy threats, intending to enhance their privacy practices post-game.", "conclusion": "The study demonstrates the effectiveness of using LLMs in serious games to promote awareness of privacy issues and suggests adversarial game mechanics for social good.", "key_contributions": ["Introduced the Cracking Aegis game using LLMs for privacy awareness.", "Showed the effectiveness of adversarial gameplay in understanding privacy vulnerabilities.", "Demonstrated how users can translate game experiences to real-world privacy behavior."], "limitations": "Limited user study sample size (n=22) may affect generalizability of the findings.", "keywords": ["Privacy Protection", "Serious Games", "Adversarial Mechanic", "LLMs", "User Engagement"], "importance_score": 8, "read_time_minutes": 24}}
{"id": "2505.16023", "pdf": "https://arxiv.org/pdf/2505.16023.pdf", "abs": "https://arxiv.org/abs/2505.16023", "title": "Prototypical Human-AI Collaboration Behaviors from LLM-Assisted Writing in the Wild", "authors": ["Sheshera Mysore", "Debarati Das", "Hancheng Cao", "Bahareh Sarrafzadeh"], "categories": ["cs.CL", "cs.HC"], "comment": "Pre-print under-review", "summary": "As large language models (LLMs) are used in complex writing workflows, users\nengage in multi-turn interactions to steer generations to better fit their\nneeds. Rather than passively accepting output, users actively refine, explore,\nand co-construct text. We conduct a large-scale analysis of this collaborative\nbehavior for users engaged in writing tasks in the wild with two popular AI\nassistants, Bing Copilot and WildChat. Our analysis goes beyond simple task\nclassification or satisfaction estimation common in prior work and instead\ncharacterizes how users interact with LLMs through the course of a session. We\nidentify prototypical behaviors in how users interact with LLMs in prompts\nfollowing their original request. We refer to these as Prototypical Human-AI\nCollaboration Behaviors (PATHs) and find that a small group of PATHs explain a\nmajority of the variation seen in user-LLM interaction. These PATHs span users\nrevising intents, exploring texts, posing questions, adjusting style or\ninjecting new content. Next, we find statistically significant correlations\nbetween specific writing intents and PATHs, revealing how users' intents shape\ntheir collaboration behaviors. We conclude by discussing the implications of\nour findings on LLM alignment.", "AI": {"tldr": "This paper analyzes how users interact with large language models (LLMs) during writing tasks, identifying collaborative behaviors known as Prototypical Human-AI Collaboration Behaviors (PATHs).", "motivation": "To understand how users actively engage with LLMs in multi-turn interactions during complex writing workflows, going beyond simple classifications of task satisfaction.", "method": "A large-scale analysis of interactions with two AI assistants, Bing Copilot and WildChat, focusing on user behavior and writing intents.", "result": "Identified key PATHs that account for most variations in user interactions and found correlations between writing intents and specific PATHs.", "conclusion": "The findings suggest important implications for aligning LLMs with user needs based on how collaboration behaviors are shaped by their intents.", "key_contributions": ["Identification of Prototypical Human-AI Collaboration Behaviors (PATHs)", "Correlations between writing intents and user behaviors with LLMs", "Insights into user interaction patterns with AI assistants"], "limitations": "", "keywords": ["Human-AI Collaboration", "Large Language Models", "User Interaction", "Writing Workflows", "Intent Analysis"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.16505", "pdf": "https://arxiv.org/pdf/2505.16505.pdf", "abs": "https://arxiv.org/abs/2505.16505", "title": "Sparse Activation Editing for Reliable Instruction Following in Narratives", "authors": ["Runcong Zhao", "Chengyu Cao", "Qinglin Zhu", "Xiucheng Lv", "Shun Shao", "Lin Gui", "Ruifeng Xu", "Yulan He"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Complex narrative contexts often challenge language models' ability to follow\ninstructions, and existing benchmarks fail to capture these difficulties. To\naddress this, we propose Concise-SAE, a training-free framework that improves\ninstruction following by identifying and editing instruction-relevant neurons\nusing only natural language instructions, without requiring labelled data. To\nthoroughly evaluate our method, we introduce FreeInstruct, a diverse and\nrealistic benchmark of 1,212 examples that highlights the challenges of\ninstruction following in narrative-rich settings. While initially motivated by\ncomplex narratives, Concise-SAE demonstrates state-of-the-art instruction\nadherence across varied tasks without compromising generation quality.", "AI": {"tldr": "Concise-SAE is a framework for enhancing language models' instruction following in complex narrative contexts without training on labeled data.", "motivation": "Current benchmarks do not effectively assess language models' performance in complex narrative contexts, where following instructions can be challenging.", "method": "Concise-SAE identifies and edits instruction-relevant neurons based on natural language instructions, requiring no labeled data.", "result": "Concise-SAE shows state-of-the-art instruction adherence across diverse tasks while maintaining generation quality, as evaluated with the new FreeInstruct benchmark.", "conclusion": "The framework effectively improves instruction following capabilities in language models, particularly in complex narrative environments.", "key_contributions": ["Introduction of Concise-SAE framework for enhancing instruction following", "Development of FreeInstruct benchmark with 1,212 examples for comprehensive evaluation", "Demonstration of state-of-the-art performance in instruction adherence"], "limitations": "", "keywords": ["instruction following", "language models", "narrative contexts", "benchmarking", "natural language processing"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.15916", "pdf": "https://arxiv.org/pdf/2505.15916.pdf", "abs": "https://arxiv.org/abs/2505.15916", "title": "BR-TaxQA-R: A Dataset for Question Answering with References for Brazilian Personal Income Tax Law, including case law", "authors": ["Juvenal Domingos Júnior", "Augusto Faria", "E. Seiti de Oliveira", "Erick de Brito", "Matheus Teotonio", "Andre Assumpção", "Diedre Carmo", "Roberto Lotufo", "Jayr Pereira"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents BR-TaxQA-R, a novel dataset designed to support question\nanswering with references in the context of Brazilian personal income tax law.\nThe dataset contains 715 questions from the 2024 official Q\\&A document\npublished by Brazil's Internal Revenue Service, enriched with statutory norms\nand administrative rulings from the Conselho Administrativo de Recursos Fiscais\n(CARF). We implement a Retrieval-Augmented Generation (RAG) pipeline using\nOpenAI embeddings for searching and GPT-4o-mini for answer generation. We\ncompare different text segmentation strategies and benchmark our system against\ncommercial tools such as ChatGPT and Perplexity.ai using RAGAS-based metrics.\nResults show that our custom RAG pipeline outperforms commercial systems in\nResponse Relevancy, indicating stronger alignment with user queries, while\ncommercial models achieve higher scores in Factual Correctness and fluency.\nThese findings highlight a trade-off between legally grounded generation and\nlinguistic fluency. Crucially, we argue that human expert evaluation remains\nessential to ensure the legal validity of AI-generated answers in high-stakes\ndomains such as taxation. BR-TaxQA-R is publicly available at\nhttps://huggingface.co/datasets/unicamp-dl/BR-TaxQA-R.", "AI": {"tldr": "BR-TaxQA-R introduces a novel dataset for question answering on Brazilian tax law, leveraging a custom RAG pipeline that shows trade-offs with commercial systems in terms of relevancy versus fluency.", "motivation": "The motivation is to enhance question answering in the domain of Brazilian personal income tax law, addressing the need for legally grounded answers generated by AI.", "method": "The authors developed a dataset of 715 questions from the Brazilian IRS Q&A document and established a Retrieval-Augmented Generation (RAG) pipeline using OpenAI embeddings and GPT-4o-mini.", "result": "The custom RAG system outperformed commercial tools like ChatGPT in Response Relevancy but showed lower scores in Factual Correctness and fluency compared to these models.", "conclusion": "The study concludes that while AI can generate legally relevant answers, expert evaluation is necessary to maintain legal validity, especially for important domains like taxation.", "key_contributions": ["Introduction of the BR-TaxQA-R dataset for Brazilian tax law Q&A", "Development of a custom RAG pipeline tailored for legal contexts", "Comparison and benchmarking against commercial AI tools like ChatGPT"], "limitations": "The study emphasizes the need for human expert evaluation to ensure the legal validity of AI-generated content, indicating a limitation in full automation.", "keywords": ["BR-TaxQA-R", "Retrieval-Augmented Generation", "Tax law", "Question answering", "NLP"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.15918", "pdf": "https://arxiv.org/pdf/2505.15918.pdf", "abs": "https://arxiv.org/abs/2505.15918", "title": "Extracting Probabilistic Knowledge from Large Language Models for Bayesian Network Parameterization", "authors": ["Aliakbar Nafar", "Kristen Brent Venable", "Zijun Cui", "Parisa Kordjamshidi"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated potential as factual knowledge\nbases; however, their capability to generate probabilistic knowledge about\nreal-world events remains understudied. This paper investigates using\nprobabilistic knowledge inherent in LLMs to derive probability estimates for\nstatements concerning events and their interrelationships captured via a\nBayesian Network (BN). Using LLMs in this context allows for the\nparameterization of BNs, enabling probabilistic modeling within specific\ndomains. Experiments on eighty publicly available Bayesian Networks, from\nhealthcare to finance, demonstrate that querying LLMs about the conditional\nprobabilities of events provides meaningful results when compared to baselines,\nincluding random and uniform distributions, as well as approaches based on\nnext-token generation probabilities. We explore how these LLM-derived\ndistributions can serve as expert priors to refine distributions extracted from\nminimal data, significantly reducing systematic biases. Overall, this work\nintroduces a promising strategy for automatically constructing Bayesian\nNetworks by combining probabilistic knowledge extracted from LLMs with small\namounts of real-world data. Additionally, we evaluate several prompting\nstrategies for eliciting probabilistic knowledge from LLMs and establish the\nfirst comprehensive baseline for assessing LLM performance in extracting\nprobabilistic knowledge.", "AI": {"tldr": "This paper explores utilizing Large Language Models (LLMs) for deriving probabilistic knowledge about real-world events through Bayesian Networks.", "motivation": "The motivation is to utilize the probabilistic knowledge in LLMs to create probability estimates for events and their interrelationships, which have been understudied.", "method": "The approach involves querying LLMs to derive conditional probabilities for various events and comparing the results against multiple baselines, including random distributions and next-token generation probabilities.", "result": "Experiments demonstrated that LLM-derived distributions offer meaningful results and can serve as expert priors to improve upon minimal data distributions, effectively reducing systematic biases.", "conclusion": "This work provides a novel strategy for constructing Bayesian Networks via LLMs, introducing effective prompting strategies and establishing a baseline for LLM performance in this area.", "key_contributions": ["Integration of LLMs with Bayesian Networks for probabilistic modeling", "Demonstrated reduction of systematic biases using LLM-derived distributions", "Introduction of comprehensive baseline for LLM performance evaluation in probabilistic knowledge extraction"], "limitations": "", "keywords": ["Large Language Models", "Bayesian Networks", "Probabilistic Knowledge", "Health Informatics", "Machine Learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2404.06432", "pdf": "https://arxiv.org/pdf/2404.06432.pdf", "abs": "https://arxiv.org/abs/2404.06432", "title": "Missing Pieces: How Do Designs that Expose Uncertainty Longitudinally Impact Trust in AI Decision Aids? An In Situ Study of Gig Drivers", "authors": ["Rex Chen", "Ruiyi Wang", "Fei Fang", "Norman Sadeh"], "categories": ["cs.HC"], "comment": "27 pages; 3 tables; 13 figures; accepted version, published at the\n  2025 ACM Conference on Fairness, Accountability, and Transparency (FAccT '25)", "summary": "Decision aids based on artificial intelligence (AI) induce a wide range of\noutcomes when they are deployed in uncertain environments. In this paper, we\ninvestigate how users' trust in recommendations from an AI decision aid is\nimpacted over time by designs that expose uncertainty in predicted outcomes.\nUnlike previous work, we focus on gig driving - a real-world, repeated\ndecision-making context. We report on a longitudinal mixed-methods study\n($n=51$) where we measured gig drivers' trust as they interacted with an\nAI-based schedule recommendation tool. Our results show that participants'\ntrust in the tool was shaped by both their first impressions of its accuracy\nand their longitudinal interactions with it; and that task-aligned framings of\nuncertainty improved trust by allowing participants to incorporate uncertainty\ninto their decision-making processes. Additionally, we observed that trust\ndepended on their characteristics as drivers, underscoring the need for more in\nsitu studies of AI decision aids.", "AI": {"tldr": "This paper investigates how user trust in AI decision aids, specifically in gig driving contexts, evolves over time, focusing on the impact of uncertainty exposure in predictions.", "motivation": "To understand how AI decision aids can improve user trust over time by effectively communicating uncertainty in predictions, particularly in gig economy settings.", "method": "A longitudinal mixed-methods study involving 51 gig drivers was conducted, measuring their trust in an AI-based scheduling recommendation tool over time and analyzing their interactions with it.", "result": "Results indicate that initial impressions of accuracy and continued interactions are pivotal in shaping trust, with task-aligned uncertainty framing enhancing trust by aiding decision-making processes.", "conclusion": "The study highlights the need for in situ examinations of AI decision aids, emphasizing that driver characteristics also affect trust in AI recommendations.", "key_contributions": ["Investigation of trust dynamics in AI decision aids over time.", "Focus on the gig driving context for real-world applicability.", "Identification of task-aligned uncertainty framing as a mechanism to enhance trust."], "limitations": "Limited sample size and scope to gig drivers; results may not generalize to other contexts or user groups.", "keywords": ["AI decision aids", "user trust", "gig economy", "uncertainty", "longitudinal study"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.15922", "pdf": "https://arxiv.org/pdf/2505.15922.pdf", "abs": "https://arxiv.org/abs/2505.15922", "title": "Aligning Dialogue Agents with Global Feedback via Large Language Model Reward Decomposition", "authors": ["Dong Won Lee", "Hae Won Park", "Cynthia Breazeal", "Louis-Philippe Morency"], "categories": ["cs.CL"], "comment": "9 pages, 3 figures, 3 tables", "summary": "We propose a large language model based reward decomposition framework for\naligning dialogue agents using only a single session-level feedback signal. We\nleverage the reasoning capabilities of a frozen, pretrained large language\nmodel (LLM) to infer fine-grained local implicit rewards by decomposing global,\nsession-level feedback. Our first text-only variant prompts the LLM to perform\nreward decomposition using only the dialogue transcript. The second multimodal\nvariant incorporates additional behavioral cues, such as pitch, gaze, and\nfacial affect, expressed as natural language descriptions. These inferred\nturn-level rewards are distilled into a lightweight reward model, which we\nutilize for RL-based fine-tuning for dialogue generation. We evaluate both\ntext-only and multimodal variants against state-of-the-art reward decomposition\nmethods and demonstrate notable improvements in human evaluations of\nconversation quality, suggesting that LLMs are strong reward decomposers that\nobviate the need for manual reward shaping and granular human feedback.", "AI": {"tldr": "This paper proposes a framework using large language models for reward decomposition to improve dialogue agents based on session-level feedback.", "motivation": "To enhance the alignment of dialogue agents using minimal feedback while reducing the need for manual reward shaping.", "method": "The work introduces a reward decomposition framework leveraging a pretrained LLM for inferring local implicit rewards from global session feedback, with text-only and multimodal variants incorporating behavioral cues.", "result": "The proposed methods significantly outperform existing state-of-the-art reward decomposition methods in human evaluations of conversation quality.", "conclusion": "The study suggests LLMs can effectively decompose rewards, simplifying the training process for dialogue generation without extensive human input.", "key_contributions": ["Introduction of a novel reward decomposition framework for dialogue agents", "Utilization of both text-only and multimodal approaches for reward inference", "Demonstration of significant improvements in dialogue quality through effective reward modeling"], "limitations": "The reliance on a single session-level feedback signal may limit the diversity of training data.", "keywords": ["Large Language Model", "Reward Decomposition", "Dialogue Agents", "Reinforcement Learning", "Human Evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.15948", "pdf": "https://arxiv.org/pdf/2505.15948.pdf", "abs": "https://arxiv.org/abs/2505.15948", "title": "Citation Parsing and Analysis with Language Models", "authors": ["Parth Sarin", "Juan Pablo Alperin"], "categories": ["cs.CL", "cs.DL", "cs.SI"], "comment": "Presented at the Workshop on Open Citations & Open Scholarly Metadata\n  2025", "summary": "A key type of resource needed to address global inequalities in knowledge\nproduction and dissemination is a tool that can support journals in\nunderstanding how knowledge circulates. The absence of such a tool has resulted\nin comparatively less information about networks of knowledge sharing in the\nGlobal South. In turn, this gap authorizes the exclusion of researchers and\nscholars from the South in indexing services, reinforcing colonial arrangements\nthat de-center and minoritize those scholars. In order to support citation\nnetwork tracking on a global scale, we investigate the capacity of open-weight\nlanguage models to mark up manuscript citations in an indexable format. We\nassembled a dataset of matched plaintext and annotated citations from preprints\nand published research papers. Then, we evaluated a number of open-weight\nlanguage models on the annotation task. We find that, even out of the box,\ntoday's language models achieve high levels of accuracy on identifying the\nconstituent components of each citation, outperforming state-of-the-art\nmethods. Moreover, the smallest model we evaluated, Qwen3-0.6B, can parse all\nfields with high accuracy in $2^5$ passes, suggesting that post-training is\nlikely to be effective in producing small, robust citation parsing models. Such\na tool could greatly improve the fidelity of citation networks and thus\nmeaningfully improve research indexing and discovery, as well as further\nmetascientific research.", "AI": {"tldr": "This paper investigates open-weight language models for marking up manuscript citations in an indexable format to enhance global citation network tracking.", "motivation": "To address global inequalities in knowledge production and the exclusion of Southern researchers from indexing services due to insufficient tools for citation network tracking.", "method": "The authors assembled a dataset of matched plaintext and annotated citations and evaluated various open-weight language models on their citation annotation accuracy.", "result": "Today's language models achieved high accuracy in citation identification, outperforming state-of-the-art methods. Notably, the smallest model, Qwen3-0.6B, performs well in parsing citation fields effectively.", "conclusion": "Developing robust citation parsing models can significantly enhance the fidelity of citation networks and improve research indexing and discovery, thereby supporting metascientific research.", "key_contributions": ["Introduction of a tool for citation annotation using open-weight language models", "Empirical evaluation demonstrating high accuracy for citation identification", "Potential to improve research indexing and knowledge dissemination, particularly in the Global South"], "limitations": "", "keywords": ["citation networks", "open-weight language models", "knowledge dissemination", "Global South", "metascientific research"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.15960", "pdf": "https://arxiv.org/pdf/2505.15960.pdf", "abs": "https://arxiv.org/abs/2505.15960", "title": "Training Step-Level Reasoning Verifiers with Formal Verification Tools", "authors": ["Ryo Kamoi", "Yusen Zhang", "Nan Zhang", "Sarkar Snigdha Sarathi Das", "Rui Zhang"], "categories": ["cs.CL"], "comment": "Datasets, models, and code are provided at\n  https://github.com/psunlpgroup/FoVer. Please also refer to our project\n  website at https://fover-prm.github.io/", "summary": "Process Reward Models (PRMs), which provide step-by-step feedback on the\nreasoning generated by Large Language Models (LLMs), are receiving increasing\nattention. However, two key research gaps remain: collecting accurate\nstep-level error labels for training typically requires costly human\nannotation, and existing PRMs are limited to math reasoning problems. In\nresponse to these gaps, this paper aims to address the challenges of automatic\ndataset creation and the generalization of PRMs to diverse reasoning tasks. To\nachieve this goal, we propose FoVer, an approach for training PRMs on\nstep-level error labels automatically annotated by formal verification tools,\nsuch as Z3 for formal logic and Isabelle for theorem proof, which provide\nautomatic and accurate verification for symbolic tasks. Using this approach, we\nsynthesize a training dataset with error labels on LLM responses for formal\nlogic and theorem proof tasks without human annotation. Although this data\nsynthesis is feasible only for tasks compatible with formal verification, we\nobserve that LLM-based PRMs trained on our dataset exhibit cross-task\ngeneralization, improving verification across diverse reasoning tasks.\nSpecifically, PRMs trained with FoVer significantly outperform baseline PRMs\nbased on the original LLMs and achieve competitive or superior results compared\nto state-of-the-art PRMs trained on labels annotated by humans or stronger\nmodels, as measured by step-level verification on ProcessBench and Best-of-K\nperformance across 12 reasoning benchmarks, including MATH, AIME, ANLI, MMLU,\nand BBH. The datasets, models, and code are provided at\nhttps://github.com/psunlpgroup/FoVer.", "AI": {"tldr": "This paper introduces FoVer, a method for training Process Reward Models (PRMs) using automatic error annotations to enhance reasoning tasks without costly human input.", "motivation": "The paper addresses the high cost of human annotation for training PRMs and the limited applicability of existing PRMs to math reasoning problems.", "method": "FoVer employs formal verification tools such as Z3 and Isabelle to automatically annotate step-level error labels for tasks like formal logic and theorem proving, allowing for the creation of training datasets without human intervention.", "result": "PRMs trained with FoVer show significant improvements in verification performance compared to baseline PRMs, achieving competitive results against state-of-the-art models across various reasoning benchmarks, including MATH and MMLU.", "conclusion": "FoVer enables cross-task generalization for PRMs, demonstrating that automatic dataset creation can effectively enhance reasoning across diverse tasks.", "key_contributions": ["Introduction of FoVer for automatic dataset creation", "Demonstration of cross-task generalization in PRMs", "Provision of datasets, models, and tools for further research"], "limitations": "The data synthesis is limited to tasks compatible with formal verification methods.", "keywords": ["Process Reward Models", "formal verification", "Large Language Models", "reasoning tasks", "dataset creation"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2502.17293", "pdf": "https://arxiv.org/pdf/2502.17293.pdf", "abs": "https://arxiv.org/abs/2502.17293", "title": "The Challenges and Benefits of Bringing Religious Values Into Design", "authors": ["Louisa Conwill", "Megan K. Levis", "Karla Badillo-Urquiola", "Walter J. Scheirer"], "categories": ["cs.HC"], "comment": null, "summary": "HCI is increasingly taking inspiration from religious traditions as a basis\nfor ethical technology designs. Such ethically-inspired designs can be\nespecially important for social communications technologies, which are\nassociated with numerous societal concerns. If religious values are to be\nincorporated into real-world designs, there may be challenges when designers\nwork with values unfamiliar to them. Therefore, we investigate the difference\nin interpretations of values when they are translated to technology designs. To\ndo so we studied design patterns that embody Catholic Social Teaching (CST). We\ninterviewed 24 technologists and 7 CST scholars to assess how their\nunderstanding of how those values would manifest in social media designs. We\nfound that for the most part the technologists responded similarly to the CST\nscholars. However, CST scholars had a better understanding of the principle of\nsubsidiarity, and they believed moderation upheld human dignity more than the\ntechnologists did. We discuss the implications of our findings on the designs\nof social technologies and design processes at large.", "AI": {"tldr": "The paper investigates how Catholic Social Teaching values can be incorporated into social technology designs by comparing the interpretations of technologists and CST scholars.", "motivation": "To explore the incorporation of religious values, specifically Catholic Social Teaching, into ethical technology design for social communications technologies amid societal concerns.", "method": "Interviews were conducted with 24 technologists and 7 Catholic Social Teaching scholars to assess their interpretations of design patterns based on CST values.", "result": "Technologists and CST scholars generally had similar interpretations of the values, although CST scholars better understood the principle of subsidiarity and believed moderation preserves human dignity more than technologists did.", "conclusion": "The findings highlight the need for understanding diverse values in the design process for social technologies, which can enhance ethical design practices.", "key_contributions": ["Investigates the role of religious values in technology design.", "Compares interpretations of values by technologists and scholars.", "Highlights the importance of understanding principles like subsidiarity in design."], "limitations": "", "keywords": ["Human-Computer Interaction", "Catholic Social Teaching", "Ethical Design", "Social Technologies", "Value Interpretation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.15962", "pdf": "https://arxiv.org/pdf/2505.15962.pdf", "abs": "https://arxiv.org/abs/2505.15962", "title": "Pre-training Large Memory Language Models with Internal and External Knowledge", "authors": ["Linxi Zhao", "Sofian Zalouk", "Christian K. Belardi", "Justin Lovelace", "Jin Peng Zhou", "Kilian Q. Weinberger", "Yoav Artzi", "Jennifer J. Sun"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Neural language models are black-boxes -- both linguistic patterns and\nfactual knowledge are distributed across billions of opaque parameters. This\nentangled encoding makes it difficult to reliably inspect, verify, or update\nspecific facts. We propose a new class of language models, Large Memory\nLanguage Models (LMLM) with a pre-training recipe that stores factual knowledge\nin both internal weights and an external database. Our approach strategically\nmasks externally retrieved factual values from the training loss, thereby\nteaching the model to perform targeted lookups rather than relying on\nmemorization in model weights. Our experiments demonstrate that LMLMs achieve\ncompetitive performance compared to significantly larger, knowledge-dense LLMs\non standard benchmarks, while offering the advantages of explicit, editable,\nand verifiable knowledge bases. This work represents a fundamental shift in how\nlanguage models interact with and manage factual knowledge.", "AI": {"tldr": "This paper introduces Large Memory Language Models (LMLM) that enhance language models by storing factual knowledge in both model weights and an external database, facilitating better inspection and updating of knowledge.", "motivation": "Current neural language models are opaque, making it challenging to verify or update specific knowledge. The research aims to improve how factual knowledge is handled in language models.", "method": "The authors propose a new architecture that separates factual knowledge into internal model weights and an external database, using targeted lookups to enhance model performance.", "result": "LMLMs show competitive performance to larger knowledge-dense LLMs on standard benchmarks while allowing for editable and verifiable external knowledge bases.", "conclusion": "The introduction of LMLMs represents a significant advancement in the management and interaction of language models with factual knowledge, enabling improved usability and flexibility.", "key_contributions": ["Introduction of Large Memory Language Models (LMLM)", "Separation of factual knowledge into internal and external storage", "Ability to perform targeted lookups for knowledge retrieval"], "limitations": "", "keywords": ["Large Memory Language Models", "factual knowledge", "language models", "knowledge retrieval", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.15993", "pdf": "https://arxiv.org/pdf/2505.15993.pdf", "abs": "https://arxiv.org/abs/2505.15993", "title": "Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku", "authors": ["Anirudh Maiya", "Razan Alghamdi", "Maria Leonor Pacheco", "Ashutosh Trivedi", "Fabio Somenzi"], "categories": ["cs.CL"], "comment": "Accepted to Findings of ACL 2025", "summary": "The success of Large Language Models (LLMs) in human-AI collaborative\ndecision-making hinges on their ability to provide trustworthy, gradual, and\ntailored explanations. Solving complex puzzles, such as Sudoku, offers a\ncanonical example of this collaboration, where clear and customized\nexplanations often hold greater importance than the final solution. In this\nstudy, we evaluate the performance of five LLMs in solving and explaining\n\\sixsix{} Sudoku puzzles. While one LLM demonstrates limited success in solving\npuzzles, none can explain the solution process in a manner that reflects\nstrategic reasoning or intuitive problem-solving. These findings underscore\nsignificant challenges that must be addressed before LLMs can become effective\npartners in human-AI collaborative decision-making.", "AI": {"tldr": "This study evaluates five LLMs on their ability to solve and explain Sudoku puzzles, revealing challenges in providing strategic reasoning in explanations.", "motivation": "To understand the effectiveness of LLMs in human-AI collaborative decision-making, particularly in providing trustworthy and tailored explanations.", "method": "Evaluation of five LLMs on their performance in solving and explaining \\\\sixsix{} Sudoku puzzles.", "result": "One LLM showed limited success in solving puzzles, but none could provide explanations reflecting strategic reasoning.", "conclusion": "LLMs currently face significant challenges in becoming effective partners in human-AI collaborative decision-making due to inadequate explanation capabilities.", "key_contributions": ["Evaluation of LLMs in a specific problem domain (Sudoku)", "Identification of explanation deficiencies in LLMs", "Insights into the role of explanations in collaborative decision-making"], "limitations": "", "keywords": ["Large Language Models", "Sudoku", "Human-AI collaboration"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.16000", "pdf": "https://arxiv.org/pdf/2505.16000.pdf", "abs": "https://arxiv.org/abs/2505.16000", "title": "Leveraging Online Data to Enhance Medical Knowledge in a Small Persian Language Model", "authors": ["Mehrdad ghassabi", "Pedram Rostami", "Hamidreza Baradaran Kashani", "Amirhossein Poursina", "Zahra Kazemi", "Milad Tavakoli"], "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 4 figures", "summary": "The rapid advancement of language models has demonstrated the potential of\nartificial intelligence in the healthcare industry. However, small language\nmodels struggle with specialized domains in low-resource languages like\nPersian. While numerous medical-domain websites exist in Persian, no curated\ndataset or corpus has been available making ours the first of its kind. This\nstudy explores the enhancement of medical knowledge in a small language model\nby leveraging accessible online data, including a crawled corpus from medical\nmagazines and a dataset of real doctor-patient QA pairs. We fine-tuned a\nbaseline model using our curated data to improve its medical knowledge.\nBenchmark evaluations demonstrate that the fine-tuned model achieves improved\naccuracy in medical question answering and provides better responses compared\nto its baseline. This work highlights the potential of leveraging open-access\nonline data to enrich small language models in medical fields, providing a\nnovel solution for Persian medical AI applications suitable for\nresource-constrained environments.", "AI": {"tldr": "This study presents the first curated dataset for medical language models in Persian, enhancing small language models by fine-tuning with crawled online medical data.", "motivation": "There is a significant gap in NLP resources for low-resource languages like Persian, particularly in the medical domain, necessitating the creation of specialized datasets to improve model performance.", "method": "The authors collected a corpus from Persian medical magazines and a dataset of doctor-patient QA pairs and fine-tuned a baseline language model on this curated data to enhance its medical knowledge.", "result": "The fine-tuned model showed improved accuracy in medical question answering tasks and provided better responses than the baseline model during benchmark evaluations.", "conclusion": "Leveraging open-access online data has proven effective in enriching small language models for medical applications, showing promise for future Persian medical AI solutions in resource-limited settings.", "key_contributions": ["Creation of the first curated dataset for Persian in the medical domain.", "Demonstration of improved performance of small language models through fine-tuning.", "Highlighting the use of accessible online data to enhance AI applications in healthcare."], "limitations": "The model's performance may still vary with different medical terminologies and contexts not included in the training data.", "keywords": ["language models", "healthcare", "Persian", "question answering", "machine learning"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2505.16002", "pdf": "https://arxiv.org/pdf/2505.16002.pdf", "abs": "https://arxiv.org/abs/2505.16002", "title": "Causal Interventions Reveal Shared Structure Across English Filler-Gap Constructions", "authors": ["Sasha Boguraev", "Christopher Potts", "Kyle Mahowald"], "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 19 figures, 11 tables", "summary": "Large Language Models (LLMs) have emerged as powerful sources of evidence for\nlinguists seeking to develop theories of syntax. In this paper, we argue that\ncausal interpretability methods, applied to LLMs, can greatly enhance the value\nof such evidence by helping us characterize the abstract mechanisms that LLMs\nlearn to use. Our empirical focus is a set of English filler-gap dependency\nconstructions (e.g., questions, relative clauses). Linguistic theories largely\nagree that these constructions share many properties. Using experiments based\nin Distributed Interchange Interventions, we show that LLMs converge on similar\nabstract analyses of these constructions. These analyses also reveal previously\noverlooked factors -- relating to frequency, filler type, and surrounding\ncontext -- that could motivate changes to standard linguistic theory. Overall,\nthese results suggest that mechanistic, internal analyses of LLMs can push\nlinguistic theory forward.", "AI": {"tldr": "This paper explores the use of causal interpretability methods on Large Language Models (LLMs) to enhance linguistic theories regarding filler-gap dependency constructions.", "motivation": "The paper aims to demonstrate that causal interpretability can improve understanding of the abstract mechanisms LLMs use, impacting linguistic theories.", "method": "Experiments based on Distributed Interchange Interventions were conducted to analyze English filler-gap dependency constructions.", "result": "The results show that LLMs converge on similar abstract analyses of these constructions, revealing overlooked factors that could influence linguistic theory.", "conclusion": "Mechanistic analyses of LLMs can advance linguistic theory by highlighting new influences on filler-gap constructions.", "key_contributions": ["Causal interpretability methods can enhance linguistic analysis of LLMs.", "LLMs reveal previously overlooked factors in linguistic constructions.", "Demonstration of empirical focus on specific syntactic phenomena."], "limitations": "", "keywords": ["Large Language Models", "syntax", "causal interpretability", "filler-gap dependency", "linguistic theory"], "importance_score": 4, "read_time_minutes": 20}}
{"id": "2505.16003", "pdf": "https://arxiv.org/pdf/2505.16003.pdf", "abs": "https://arxiv.org/abs/2505.16003", "title": "SLMEval: Entropy-Based Calibration for Human-Aligned Evaluation of Large Language Models", "authors": ["Roland Daynauth", "Christopher Clarke", "Krisztian Flautner", "Lingjia Tang", "Jason Mars"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The LLM-as-a-Judge paradigm offers a scalable, reference-free approach for\nevaluating language models. Although several calibration techniques have been\nproposed to better align these evaluators with human judgment, prior studies\nfocus primarily on narrow, well-structured benchmarks. As a result, it remains\nunclear whether such calibrations generalize to real-world, open-ended tasks.\n  In this work, we show that SOTA calibrated evaluators often fail in these\nsettings, exhibiting weak or even negative correlation with human judgments. To\naddress this, we propose SLMEval, a novel and efficient calibration method\nbased on entropy maximization over a small amount of human preference data. By\nestimating a latent distribution over model quality and reweighting evaluator\nscores accordingly, SLMEval achieves strong correlation with human evaluations\nacross two real-world production use cases and the public benchmark. For\nexample, on one such task, SLMEval achieves a Spearman correlation of 0.57 with\nhuman judgments, while G-Eval yields a negative correlation. In addition,\nSLMEval reduces evaluation costs by 5-30x compared to GPT-4-based calibrated\nevaluators such as G-eval.", "AI": {"tldr": "Proposes SLMEval, a novel calibration method for evaluating language models that aligns better with human judgments.", "motivation": "To improve the evaluation of language models by addressing the shortcomings of existing calibration techniques which struggle in real-world, open-ended tasks.", "method": "SLMEval uses entropy maximization over a small set of human preference data to estimate a latent distribution of model quality and reweight evaluator scores.", "result": "SLMEval demonstrates strong correlation with human evaluations, achieving a Spearman correlation of 0.57, and reduces evaluation costs by 5-30 times compared to existing methods.", "conclusion": "SLMEval provides an efficient and effective means of calibrating language model evaluations to better reflect human judgments in diverse tasks.", "key_contributions": ["Introduction of SLMEval as a new calibration method.", "Evidence of SLMEval achieving better correlation with human evaluations compared to state-of-the-art techniques.", "Significant reduction in evaluation costs compared to existing methods."], "limitations": "", "keywords": ["LLM-as-a-Judge", "calibration", "human judgment", "evaluation methods", "entropy maximization"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.16008", "pdf": "https://arxiv.org/pdf/2505.16008.pdf", "abs": "https://arxiv.org/abs/2505.16008", "title": "LAGO: Few-shot Crosslingual Embedding Inversion Attacks via Language Similarity-Aware Graph Optimization", "authors": ["Wenrui Yu", "Yiyi Chen", "Johannes Bjerva", "Sokol Kosta", "Qiongxiu Li"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "We propose LAGO - Language Similarity-Aware Graph Optimization - a novel\napproach for few-shot cross-lingual embedding inversion attacks, addressing\ncritical privacy vulnerabilities in multilingual NLP systems. Unlike prior work\nin embedding inversion attacks that treat languages independently, LAGO\nexplicitly models linguistic relationships through a graph-based constrained\ndistributed optimization framework. By integrating syntactic and lexical\nsimilarity as edge constraints, our method enables collaborative parameter\nlearning across related languages. Theoretically, we show this formulation\ngeneralizes prior approaches, such as ALGEN, which emerges as a special case\nwhen similarity constraints are relaxed. Our framework uniquely combines\nFrobenius-norm regularization with linear inequality or total variation\nconstraints, ensuring robust alignment of cross-lingual embedding spaces even\nwith extremely limited data (as few as 10 samples per language). Extensive\nexperiments across multiple languages and embedding models demonstrate that\nLAGO substantially improves the transferability of attacks with 10-20% increase\nin Rouge-L score over baselines. This work establishes language similarity as a\ncritical factor in inversion attack transferability, urging renewed focus on\nlanguage-aware privacy-preserving multilingual embeddings.", "AI": {"tldr": "LAGO is a new method for improving few-shot cross-lingual embedding inversion attacks by modeling language similarities, enhancing the security of multilingual NLP systems.", "motivation": "Address critical privacy vulnerabilities in multilingual NLP systems by improving few-shot cross-lingual embedding inversion attacks.", "method": "A graph-based constrained distributed optimization framework that integrates syntactic and lexical similarity as edge constraints for collaborative parameter learning across languages.", "result": "LAGO improves attack transferability by 10-20% in Rouge-L score across various languages and embedding models compared to baseline approaches.", "conclusion": "The framework highlights the importance of language similarity in enhancing the transferability of inversion attacks and calls for attention to privacy-preserving multilingual embeddings.", "key_contributions": ["Introduces LAGO for few-shot cross-lingual embedding inversion attacks.", "Models linguistic relationships through a graph-based framework.", "Demonstrates substantial improvements in attack transferability."], "limitations": "", "keywords": ["cross-lingual", "embedding inversion attacks", "language similarity", "privacy", "NLP"], "importance_score": 4, "read_time_minutes": 7}}
{"id": "2505.16014", "pdf": "https://arxiv.org/pdf/2505.16014.pdf", "abs": "https://arxiv.org/abs/2505.16014", "title": "Ranking Free RAG: Replacing Re-ranking with Selection in RAG for Sensitive Domains", "authors": ["Yash Saxena", "Anpur Padia", "Mandar S Chaudhary", "Kalpa Gunaratna", "Srinivasan Parthasarathy", "Manas Gaur"], "categories": ["cs.CL"], "comment": null, "summary": "Traditional Retrieval-Augmented Generation (RAG) pipelines rely on\nsimilarity-based retrieval and re-ranking, which depend on heuristics such as\ntop-k, and lack explainability, interpretability, and robustness against\nadversarial content. To address this gap, we propose a novel method METEORA\nthat replaces re-ranking in RAG with a rationale-driven selection approach.\nMETEORA operates in two stages. First, a general-purpose LLM is\npreference-tuned to generate rationales conditioned on the input query using\ndirect preference optimization. These rationales guide the evidence chunk\nselection engine, which selects relevant chunks in three stages: pairing\nindividual rationales with corresponding retrieved chunks for local relevance,\nglobal selection with elbow detection for adaptive cutoff, and context\nexpansion via neighboring chunks. This process eliminates the need for top-k\nheuristics. The rationales are also used for consistency check using a Verifier\nLLM to detect and filter poisoned or misleading content for safe generation.\nThe framework provides explainable and interpretable evidence flow by using\nrationales consistently across both selection and verification. Our evaluation\nacross six datasets spanning legal, financial, and academic research domains\nshows that METEORA improves generation accuracy by 33.34% while using\napproximately 50% fewer chunks than state-of-the-art re-ranking methods. In\nadversarial settings, METEORA significantly improves the F1 score from 0.10 to\n0.44 over the state-of-the-art perplexity-based defense baseline, demonstrating\nstrong resilience to poisoning attacks. Code available at:\nhttps://anonymous.4open.science/r/METEORA-DC46/README.md", "AI": {"tldr": "METEORA is a novel method for Retrieval-Augmented Generation that improves selection and verification of evidence using rationales, thereby enhancing explainability, interpretability, and robustness against adversarial content.", "motivation": "Traditional RAG pipelines lack explainability, robustness, and rely heavily on similarity-based retrieval which may not perform well in adversarial conditions.", "method": "METEORA replaces re-ranking with a rationale-driven selection approach in two stages: generating rationales using a preference-tuned LLM and selecting evidence chunks based on local relevance, adaptive cutoff, and context expansion.", "result": "METEORA improves generation accuracy by 33.34% and reduces the number of chunks used by about 50% compared to state-of-the-art methods. It also enhances resilience to adversarial attacks, with an F1 score improvement from 0.10 to 0.44.", "conclusion": "The framework enables explainable and interpretable evidence flow, effectively addressing issues of previous RAG approaches while demonstrating improved performance across multiple datasets.", "key_contributions": ["Introduction of a rationale-driven selection mechanism in RAG", "Significant improvement in generation accuracy and robustness against adversarial content", "Consistent use of rationales for evidence selection and verification, enhancing explainability."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Explainability", "Adversarial Robustness"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.16022", "pdf": "https://arxiv.org/pdf/2505.16022.pdf", "abs": "https://arxiv.org/abs/2505.16022", "title": "NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning", "authors": ["Wei Liu", "Siya Qi", "Xinyu Wang", "Chen Qian", "Yali Du", "Yulan He"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "20 pages, 5 tables, 12 figures", "summary": "Recent advances such as DeepSeek R1-Zero highlight the effectiveness of\nincentive training, a reinforcement learning paradigm that computes rewards\nsolely based on the final answer part of a language model's output, thereby\nencouraging the generation of intermediate reasoning steps. However, these\nmethods fundamentally rely on external verifiers, which limits their\napplicability to domains like mathematics and coding where such verifiers are\nreadily available. Although reward models can serve as verifiers, they require\nhigh-quality annotated data and are costly to train. In this work, we propose\nNOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning\nframework that requires only standard supervised fine-tuning data with no need\nfor an external verifier. NOVER enables incentive training across a wide range\nof text-to-text tasks and outperforms the model of the same size distilled from\nlarge reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the\nflexibility of NOVER enables new possibilities for optimizing large language\nmodels, such as inverse incentive training.", "AI": {"tldr": "The paper introduces NOVER, a novel reinforcement learning framework that eliminates the need for external verifiers, instead relying on standard supervised fine-tuning data to enhance language model performance in various text-to-text tasks.", "motivation": "The paper aims to address the limitations of current incentive training methods in reinforcement learning, particularly their dependence on external verifiers, which restricts their applicability in domains lacking such resources.", "method": "The authors propose a framework called NOVER (NO-VERifier Reinforcement Learning) that utilizes standard supervised fine-tuning data, allowing for the application of incentive training without requiring external verification.", "result": "NOVER demonstrates a 7.7% improvement over a model of the same size distilled from large reasoning models like DeepSeek R1 671B, showcasing its effectiveness and potential.", "conclusion": "The study concludes that NOVER not only overcomes the limitations of previous methods but also opens new avenues for optimizing large language models, including inverse incentive training.", "key_contributions": ["Introduction of NOVER, a framework that avoids the need for external verifiers in reinforcement learning.", "Demonstration of superior performance compared to large reasoning models using standard fine-tuning data.", "Exploration of new optimization possibilities for large language models through inverse incentive training."], "limitations": "", "keywords": ["reinforcement learning", "incentive training", "language models", "NOVER", "fine-tuning"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2505.16023", "pdf": "https://arxiv.org/pdf/2505.16023.pdf", "abs": "https://arxiv.org/abs/2505.16023", "title": "Prototypical Human-AI Collaboration Behaviors from LLM-Assisted Writing in the Wild", "authors": ["Sheshera Mysore", "Debarati Das", "Hancheng Cao", "Bahareh Sarrafzadeh"], "categories": ["cs.CL", "cs.HC"], "comment": "Pre-print under-review", "summary": "As large language models (LLMs) are used in complex writing workflows, users\nengage in multi-turn interactions to steer generations to better fit their\nneeds. Rather than passively accepting output, users actively refine, explore,\nand co-construct text. We conduct a large-scale analysis of this collaborative\nbehavior for users engaged in writing tasks in the wild with two popular AI\nassistants, Bing Copilot and WildChat. Our analysis goes beyond simple task\nclassification or satisfaction estimation common in prior work and instead\ncharacterizes how users interact with LLMs through the course of a session. We\nidentify prototypical behaviors in how users interact with LLMs in prompts\nfollowing their original request. We refer to these as Prototypical Human-AI\nCollaboration Behaviors (PATHs) and find that a small group of PATHs explain a\nmajority of the variation seen in user-LLM interaction. These PATHs span users\nrevising intents, exploring texts, posing questions, adjusting style or\ninjecting new content. Next, we find statistically significant correlations\nbetween specific writing intents and PATHs, revealing how users' intents shape\ntheir collaboration behaviors. We conclude by discussing the implications of\nour findings on LLM alignment.", "AI": {"tldr": "This paper analyzes user interactions with large language models (LLMs) during writing tasks, identifying collaborative behaviors known as Prototypical Human-AI Collaboration Behaviors (PATHs).", "motivation": "To understand how users actively engage and collaborate with LLMs in writing workflows, moving beyond passive output acceptance.", "method": "A large-scale analysis of user interactions with popular AI assistants, categorizing behaviors into specific PATHs based on multi-turn interactions during writing tasks.", "result": "Identified a small group of PATHs that explain most of the variation in user-LLM interactions, correlating specific writing intents with distinct collaboration behaviors.", "conclusion": "The findings highlight the importance of user intent in shaping interactions with LLMs and have implications for LLM alignment strategies.", "key_contributions": ["Identified Prototypical Human-AI Collaboration Behaviors (PATHs) from user interactions.", "Demonstrated the correlation between writing intents and user collaboration behaviors with LLMs.", "Provided a novel framework for understanding collaborative behaviors in AI-assisted writing."], "limitations": "", "keywords": ["Human-AI Interaction", "Large Language Models", "Collaboration Behaviors", "Writing Intents", "AI Alignment"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.16036", "pdf": "https://arxiv.org/pdf/2505.16036.pdf", "abs": "https://arxiv.org/abs/2505.16036", "title": "OpenEthics: A Comprehensive Ethical Evaluation of Open-Source Generative Large Language Models", "authors": ["Burak Erinç Çetin", "Yıldırım Özen", "Elif Naz Demiryılmaz", "Kaan Engür", "Cagri Toraman"], "categories": ["cs.CL"], "comment": null, "summary": "Generative large language models present significant potential but also raise\ncritical ethical concerns. Most studies focus on narrow ethical dimensions, and\nalso limited diversity of languages and models. To address these gaps, we\nconduct a broad ethical evaluation of 29 recent open-source large language\nmodels using a novel data collection including four ethical aspects:\nRobustness, reliability, safety, and fairness. We analyze model behavior in\nboth a commonly used language, English, and a low-resource language, Turkish.\nOur aim is to provide a comprehensive ethical assessment and guide safer model\ndevelopment by filling existing gaps in evaluation breadth, language coverage,\nand model diversity. Our experimental results, based on LLM-as-a-Judge, reveal\nthat optimization efforts for many open-source models appear to have\nprioritized safety and fairness, and demonstrated good robustness while\nreliability remains a concern. We demonstrate that ethical evaluation can be\neffectively conducted independently of the language used. In addition, models\nwith larger parameter counts tend to exhibit better ethical performance, with\nGemma and Qwen models demonstrating the most ethical behavior among those\nevaluated.", "AI": {"tldr": "The paper assesses the ethical implications of 29 open-source large language models, focusing on robustness, safety, reliability, and fairness, across English and Turkish.", "motivation": "To address gaps in the ethical evaluation of generative large language models, particularly regarding diversity of languages and models.", "method": "A comprehensive ethical evaluation of 29 recent open-source large language models was conducted, analyzing robustness, reliability, safety, and fairness in both English and Turkish.", "result": "The study found that while many models prioritize safety and fairness, reliability remains a concern; larger models generally show better ethical performance, with Gemma and Qwen performing the best.", "conclusion": "The research highlights that ethical evaluations can be effectively performed regardless of language, and underscores the need for continuous improvement in model reliability.", "key_contributions": ["Broad ethical evaluation of 29 open-source LLMs", "Analysis of performance across different languages", "Identification of key ethical performance trends based on model size"], "limitations": "Limited to the open-source models evaluated and may not generalize to proprietary models.", "keywords": ["Generative models", "Ethics", "Large language models", "Robustness", "Fairness"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.16061", "pdf": "https://arxiv.org/pdf/2505.16061.pdf", "abs": "https://arxiv.org/abs/2505.16061", "title": "Internal and External Impacts of Natural Language Processing Papers", "authors": ["Yu Zhang"], "categories": ["cs.CL", "cs.DL"], "comment": "7 pages; Accepted to ACL 2025", "summary": "We investigate the impacts of NLP research published in top-tier conferences\n(i.e., ACL, EMNLP, and NAACL) from 1979 to 2024. By analyzing citations from\nresearch articles and external sources such as patents, media, and policy\ndocuments, we examine how different NLP topics are consumed both within the\nacademic community and by the broader public. Our findings reveal that language\nmodeling has the widest internal and external influence, while linguistic\nfoundations have lower impacts. We also observe that internal and external\nimpacts generally align, but topics like ethics, bias, and fairness show\nsignificant attention in policy documents with much fewer academic citations.\nAdditionally, external domains exhibit distinct preferences, with patents\nfocusing on practical NLP applications and media and policy documents engaging\nmore with the societal implications of NLP models.", "AI": {"tldr": "This paper analyzes the impacts of NLP research published in major conferences from 1979 to 2024, revealing trends in academic and public engagement with different topics.", "motivation": "To investigate how NLP research influences both the academic community and broader public domains over time, with a focus on various NLP topics.", "method": "Citations from research articles and external sources (patents, media, policy documents) were analyzed to assess the consumption of NLP topics.", "result": "Language modeling demonstrates the highest internal and external influence, while linguistic foundations have lower impacts. Topics like ethics, bias, and fairness attract significant attention in policy documents but are less cited in academic literature.", "conclusion": "The findings suggest a mismatch between academic focus and societal needs, particularly regarding ethical implications of NLP.", "key_contributions": ["Analysis of NLP research impacts over several decades", "Identification of alignment and divergence between academic and external consumption of NLP topics", "Insights on public interest in societal implications versus academic discourse"], "limitations": "May not capture the entire spectrum of NLP influence due to the focus on top-tier conferences alone.", "keywords": ["NLP", "citations analysis", "language modeling", "ethical implications", "public engagement"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.16078", "pdf": "https://arxiv.org/pdf/2505.16078.pdf", "abs": "https://arxiv.org/abs/2505.16078", "title": "Small Language Models in the Real World: Insights from Industrial Text Classification", "authors": ["Lujun Li", "Lama Sleem", "Niccolo' Gentile", "Geoffrey Nichil", "Radu State"], "categories": ["cs.CL"], "comment": null, "summary": "With the emergence of ChatGPT, Transformer models have significantly advanced\ntext classification and related tasks. Decoder-only models such as Llama\nexhibit strong performance and flexibility, yet they suffer from inefficiency\non inference due to token-by-token generation, and their effectiveness in text\nclassification tasks heavily depends on prompt quality. Moreover, their\nsubstantial GPU resource requirements often limit widespread adoption. Thus,\nthe question of whether smaller language models are capable of effectively\nhandling text classification tasks emerges as a topic of significant interest.\nHowever, the selection of appropriate models and methodologies remains largely\nunderexplored. In this paper, we conduct a comprehensive evaluation of prompt\nengineering and supervised fine-tuning methods for transformer-based text\nclassification. Specifically, we focus on practical industrial scenarios,\nincluding email classification, legal document categorization, and the\nclassification of extremely long academic texts. We examine the strengths and\nlimitations of smaller models, with particular attention to both their\nperformance and their efficiency in Video Random-Access Memory (VRAM)\nutilization, thereby providing valuable insights for the local deployment and\napplication of compact models in industrial settings.", "AI": {"tldr": "The paper evaluates prompt engineering and fine-tuning methods for transformer-based text classification, focusing on the efficiency and performance of smaller models in industrial contexts.", "motivation": "To assess whether smaller language models can effectively handle text classification tasks amidst challenges such as prompt quality and GPU resource limitations.", "method": "A comprehensive evaluation of prompt engineering and supervised fine-tuning methods for transformer-based models, applied in real-world scenarios like email classification and legal document categorization.", "result": "The study highlights the performance and efficiency of smaller transformer models in text classification tasks while analyzing their VRAM utilization.", "conclusion": "Smaller language models can be deployed effectively for text classification, but their performance is influenced by prompt quality and requires efficient VRAM management.", "key_contributions": ["Evaluation of prompt engineering and supervised fine-tuning for smaller models", "Focus on practical industrial applications", "Insights into VRAM utilization for model deployment"], "limitations": "Limited exploration of a wider range of models and methodologies.", "keywords": ["text classification", "prompt engineering", "supervised fine-tuning", "smaller models", "VRAM utilization"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.16081", "pdf": "https://arxiv.org/pdf/2505.16081.pdf", "abs": "https://arxiv.org/abs/2505.16081", "title": "BiasLab: Toward Explainable Political Bias Detection with Dual-Axis Annotations and Rationale Indicators", "authors": ["KMA Solaiman"], "categories": ["cs.CL"], "comment": "Under review", "summary": "We present BiasLab, a dataset of 300 political news articles annotated for\nperceived ideological bias. These articles were selected from a curated\n900-document pool covering diverse political events and source biases. Each\narticle is labeled by crowdworkers along two independent scales, assessing\nsentiment toward the Democratic and Republican parties, and enriched with\nrationale indicators. The annotation pipeline incorporates targeted worker\nqualification and was refined through pilot-phase analysis. We quantify\ninter-annotator agreement, analyze misalignment with source-level outlet bias,\nand organize the resulting labels into interpretable subsets. Additionally, we\nsimulate annotation using schema-constrained GPT-4o, enabling direct comparison\nto human labels and revealing mirrored asymmetries, especially in\nmisclassifying subtly right-leaning content. We define two modeling tasks:\nperception drift prediction and rationale type classification, and report\nbaseline performance to illustrate the challenge of explainable bias detection.\nBiasLab's rich rationale annotations provide actionable interpretations that\nfacilitate explainable modeling of political bias, supporting the development\nof transparent, socially aware NLP systems. We release the dataset, annotation\nschema, and modeling code to encourage research on human-in-the-loop\ninterpretability and the evaluation of explanation effectiveness in real-world\nsettings.", "AI": {"tldr": "BiasLab is a dataset of 300 political news articles annotated for perceived ideological bias, aimed at enhancing explainable modeling of political bias in NLP systems.", "motivation": "The paper addresses the need for a comprehensive dataset that aids in understanding and detecting ideological bias in political news articles, promoting the development of explainable NLP systems in the context of political discourse.", "method": "The authors implemented a structured annotation pipeline involving crowdworkers to label articles based on their sentiment toward political parties, alongside conducting various analyses on inter-annotator agreement and misalignment with source bias. They also employed a GPT-4o model to simulate annotation for comparative analysis.", "result": "The dataset shows quantifiable inter-annotator agreement and reveals challenges in bias detection, specifically noting mirrored asymmetries in misclassifying right-leaning content. Baseline performance is reported for two modeling tasks: perception drift prediction and rationale type classification.", "conclusion": "BiasLab offers rich annotations that enhance interpretability in bias detection and provides essential resources for future research in explainable AI and human-in-the-loop systems.", "key_contributions": ["Introduction of BiasLab dataset for ideological bias in political news articles.", "Comparison between human annotations and GPT-4o simulated annotations.", "Defined modeling tasks to further explore explainable bias detection."], "limitations": "The dataset is limited to 300 articles and the findings might not generalize across all types of political news or other contexts.", "keywords": ["ideological bias", "political news", "explainable AI", "natural language processing", "dataset release"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2505.16088", "pdf": "https://arxiv.org/pdf/2505.16088.pdf", "abs": "https://arxiv.org/abs/2505.16088", "title": "Date Fragments: A Hidden Bottleneck of Tokenization for Temporal Reasoning", "authors": ["Gagan Bhatia", "Maxime Peyrard", "Wei Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Modern BPE tokenizers often split calendar dates into meaningless fragments,\ne.g., 20250312 $\\rightarrow$ 202, 503, 12, inflating token counts and obscuring\nthe inherent structure needed for robust temporal reasoning. In this work, we\n(1) introduce a simple yet interpretable metric, termed date fragmentation\nratio, that measures how faithfully a tokenizer preserves multi-digit date\ncomponents; (2) release DateAugBench, a suite of 6500 examples spanning three\ntemporal reasoning tasks: context-based date resolution, format-invariance\npuzzles, and date arithmetic across historical, contemporary, and future\nregimes; and (3) through layer-wise probing and causal attention-hop analyses,\nuncover an emergent date-abstraction mechanism whereby large language models\nstitch together the fragments of month, day, and year components for temporal\nreasoning. Our experiments show that excessive fragmentation correlates with\naccuracy drops of up to 10 points on uncommon dates like historical and\nfuturistic dates. Further, we find that the larger the model, the faster the\nemergent date abstraction that heals date fragments is accomplished. Lastly, we\nobserve a reasoning path that LLMs follow to assemble date fragments, typically\ndiffering from human interpretation (year $\\rightarrow$ month $\\rightarrow$\nday).", "AI": {"tldr": "This work introduces a new metric to measure date fragmentation in tokenization, releases a benchmark for temporal reasoning tasks, and analyzes how LLMs reconstruct date fragments for better reasoning.", "motivation": "Tokenizers often fragment calendar dates into parts that obscure their structure, negatively impacting temporal reasoning in NLP models.", "method": "The authors propose a date fragmentation ratio metric to assess tokenization fidelity for dates and introduce DateAugBench, which includes 6500 examples across three types of temporal reasoning tasks. They use layer-wise probing and causal attention analysis to investigate LLM behavior in date reasoning.", "result": "Excessive token fragmentation correlates with up to 10-point accuracy drops on certain date tasks. Larger models show rapid improvements in reconstructing date fragments for reasoning.", "conclusion": "LLMs reconstruct fragments differently from humans, suggesting a distinct reasoning process in date handling that could be improved with better tokenization strategies.", "key_contributions": ["Introduction of the date fragmentation ratio metric", "Release of DateAugBench for evaluating temporal reasoning", "Identification of a unique reasoning path in LLMs for assembling date fragments"], "limitations": "Focuses primarily on tokenization issues and their impact on LLMs; may not address all aspects of temporal reasoning in NLP.", "keywords": ["tokenization", "temporal reasoning", "large language models", "benchmarks", "fragmentation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.16102", "pdf": "https://arxiv.org/pdf/2505.16102.pdf", "abs": "https://arxiv.org/abs/2505.16102", "title": "Continually Self-Improving Language Models for Bariatric Surgery Question--Answering", "authors": ["Yash Kumar Atri", "Thomas H Shin", "Thomas Hartvigsen"], "categories": ["cs.CL"], "comment": null, "summary": "While bariatric and metabolic surgery (MBS) is considered the gold standard\ntreatment for severe and morbid obesity, its therapeutic efficacy hinges upon\nactive and longitudinal engagement with multidisciplinary providers, including\nsurgeons, dietitians/nutritionists, psychologists, and endocrinologists. This\nengagement spans the entire patient journey, from preoperative preparation to\nlong-term postoperative management. However, this process is often hindered by\nnumerous healthcare disparities, such as logistical and access barriers, which\nimpair easy patient access to timely, evidence-based, clinician-endorsed\ninformation. To address these gaps, we introduce bRAGgen, a novel adaptive\nretrieval-augmented generation (RAG)-based model that autonomously integrates\nreal-time medical evidence when response confidence dips below dynamic\nthresholds. This self-updating architecture ensures that responses remain\ncurrent and accurate, reducing the risk of misinformation. Additionally, we\npresent bRAGq, a curated dataset of 1,302 bariatric surgery--related questions,\nvalidated by an expert bariatric surgeon. bRAGq constitutes the first\nlarge-scale, domain-specific benchmark for comprehensive MBS care. In a\ntwo-phase evaluation, bRAGgen is benchmarked against state-of-the-art models\nusing both large language model (LLM)--based metrics and expert surgeon review.\nAcross all evaluation dimensions, bRAGgen demonstrates substantially superior\nperformance in generating clinically accurate and relevant responses.", "AI": {"tldr": "The paper introduces bRAGgen, an adaptive RAG model to enhance patient engagement in bariatric and metabolic surgery care by integrating real-time medical evidence and presenting bRAGq, a comprehensive dataset for evaluating MBS-related questions.", "motivation": "To improve patient engagement in bariatric and metabolic surgery by addressing healthcare disparities and providing timely, evidence-based information.", "method": "Development of bRAGgen, an adaptive retrieval-augmented generation (RAG) model that updates real-time medical evidence based on confidence thresholds, and the creation of bRAGq, a curated dataset of bariatric surgery questions.", "result": "bRAGgen shows significantly improved performance over existing models in generating clinically accurate and relevant responses during evaluations.", "conclusion": "bRAGgen is a promising tool for enhancing the delivery of evidence-based information in bariatric surgery, thereby improving patient outcomes and engagement.", "key_contributions": ["Introduction of bRAGgen for real-time integration of medical evidence.", "Creation of bRAGq, the first large-scale dataset for bariatric surgery questions.", "Demonstration of bRAGgen's superior performance in clinical response generation."], "limitations": "", "keywords": ["bariatric surgery", "adaptive retrieval-augmented generation", "healthcare disparities", "patient engagement", "large language model"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.16104", "pdf": "https://arxiv.org/pdf/2505.16104.pdf", "abs": "https://arxiv.org/abs/2505.16104", "title": "Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models", "authors": ["Yue Li", "Xin Yi", "Dongsheng Shi", "Gerard de Melo", "Xiaoling Wang", "Linlin Wang"], "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": "ACL 2025 Findings", "summary": "With the increasing size of Large Vision-Language Models (LVLMs), network\npruning techniques aimed at compressing models for deployment in\nresource-constrained environments have garnered significant attention. However,\nwe observe that pruning often leads to a degradation in safety performance. To\naddress this issue, we present a novel and lightweight approach, termed\nHierarchical Safety Realignment (HSR). HSR operates by first quantifying the\ncontribution of each attention head to safety, identifying the most critical\nones, and then selectively restoring neurons directly within these attention\nheads that play a pivotal role in maintaining safety. This process\nhierarchically realigns the safety of pruned LVLMs, progressing from the\nattention head level to the neuron level. We validate HSR across various models\nand pruning strategies, consistently achieving notable improvements in safety\nperformance. To our knowledge, this is the first work explicitly focused on\nrestoring safety in LVLMs post-pruning.", "AI": {"tldr": "Hierarchical Safety Realignment (HSR) improves safety performance in pruned Large Vision-Language Models.", "motivation": "Pruning LVLMs often degrades safety performance, necessitating methods to restore safety in compressed models.", "method": "HSR quantifies the contribution of each attention head to safety, identifies critical heads, and selectively restores neurons to realign safety hierarchically from attention heads to neuron levels.", "result": "HSR consistently achieved notable improvements in safety performance across various models and pruning strategies.", "conclusion": "HSR is the first approach to explicitly focus on restoring safety in pruned LVLMs, demonstrating effectiveness in ensuring safety in resource-constrained environments.", "key_contributions": ["Introduction of Hierarchical Safety Realignment (HSR) for LVLMs.", "Quantitative assessment of attention heads' contributions to safety.", "First explicit focus on safety restoration post-pruning in LVLMs."], "limitations": "", "keywords": ["Large Vision-Language Models", "network pruning", "safety performance", "Hierarchical Safety Realignment", "neuron restoration"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.16107", "pdf": "https://arxiv.org/pdf/2505.16107.pdf", "abs": "https://arxiv.org/abs/2505.16107", "title": "MPL: Multiple Programming Languages with Large Language Models for Information Extraction", "authors": ["Bo Li", "Gexiang Fang", "Wei Ye", "Zhenghua Xu", "Jinglei Zhang", "Hao Cheng", "Shikun Zhang"], "categories": ["cs.CL"], "comment": "Findings of ACL2025", "summary": "Recent research in information extraction (IE) focuses on utilizing\ncode-style inputs to enhance structured output generation. The intuition behind\nthis is that the programming languages (PLs) inherently exhibit greater\nstructural organization than natural languages (NLs). This structural advantage\nmakes PLs particularly suited for IE tasks. Nevertheless, existing research\nprimarily focuses on Python for code-style simulation, overlooking the\npotential of other widely-used PLs (e.g., C++ and Java) during the supervised\nfine-tuning (SFT) phase. In this research, we propose \\textbf{M}ultiple\n\\textbf{P}rogramming \\textbf{L}anguages with large language models for\ninformation extraction (abbreviated as \\textbf{MPL}), a novel framework that\nexplores the potential of incorporating different PLs in the SFT phase.\nAdditionally, we introduce \\texttt{function-prompt} with virtual running to\nsimulate code-style inputs more effectively and efficiently. Experimental\nresults on a wide range of datasets demonstrate the effectiveness of MPL.\nFurthermore, we conduct extensive experiments to provide a comprehensive\nanalysis. We have released our code for future research.", "AI": {"tldr": "The paper presents MPL, a framework that utilizes multiple programming languages to enhance information extraction tasks during supervised fine-tuning, proposing a novel method called function-prompt with virtual running.", "motivation": "To improve structured output generation in information extraction by leveraging the inherent structural advantages of multiple programming languages.", "method": "The proposed MPL framework incorporates various programming languages (such as C++ and Java) during the supervised fine-tuning phase and introduces the function-prompt method with virtual running for more effective simulation of code-style inputs.", "result": "Experimental results indicate that MPL effectively enhances information extraction across multiple datasets compared to existing approaches that primarily focus on Python.", "conclusion": "MPL shows promise for advancing information extraction by incorporating diverse programming languages; the released code will support further research in this area.", "key_contributions": ["Introduction of the MPL framework for using multiple programming languages in information extraction.", "Development of the function-prompt method with virtual running for better code-style input simulation.", "Extensive experimental analysis demonstrating MPL's effectiveness across various datasets."], "limitations": "", "keywords": ["information extraction", "supervised fine-tuning", "programming languages", "large language models", "code-style inputs"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.16118", "pdf": "https://arxiv.org/pdf/2505.16118.pdf", "abs": "https://arxiv.org/abs/2505.16118", "title": "Semiotic Reconstruction of Destination Expectation Constructs An LLM-Driven Computational Paradigm for Social Media Tourism Analytics", "authors": ["Haotian Lan", "Yao Gao", "Yujun Cheng", "Wei Yuan", "Kun Wang"], "categories": ["cs.CL", "stat.AP"], "comment": "33 pages, 6 figures", "summary": "Social media's rise establishes user-generated content (UGC) as pivotal for\ntravel decisions, yet analytical methods lack scalability. This study\nintroduces a dual-method LLM framework: unsupervised expectation extraction\nfrom UGC paired with survey-informed supervised fine-tuning. Findings reveal\nleisure/social expectations drive engagement more than foundational\nnatural/emotional factors. By establishing LLMs as precision tools for\nexpectation quantification, we advance tourism analytics methodology and\npropose targeted strategies for experience personalization and social travel\npromotion. The framework's adaptability extends to consumer behavior research,\ndemonstrating computational social science's transformative potential in\nmarketing optimization.", "AI": {"tldr": "This study presents a dual-method LLM framework for analyzing user-generated content in travel, enhancing tourism analytics by quantifying user expectations for improved engagement and personalization.", "motivation": "To address the scalability issues in analyzing user-generated content (UGC) for travel decision-making.", "method": "The paper employs a dual-method LLM framework combining unsupervised expectation extraction from UGC with supervised fine-tuning informed by surveys.", "result": "The framework identifies that leisure/social expectations have a greater impact on user engagement than foundational natural/emotional factors.", "conclusion": "By positioning LLMs as precise tools for quantifying expectations in tourism, the study suggests methods to personalize experiences and improve social travel marketing.", "key_contributions": ["Introduction of a dual-method LLM framework for expectation extraction and fine-tuning.", "Demonstration of LLM adaptability in consumer behavior research.", "Advancement of tourism analytics methodology for marketing optimization."], "limitations": "The study's findings are specific to the tourism sector and may not generalize to other domains without further adaptation.", "keywords": ["user-generated content", "LLM", "tourism analytics", "expectation quantification", "consumer behavior"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2505.16125", "pdf": "https://arxiv.org/pdf/2505.16125.pdf", "abs": "https://arxiv.org/abs/2505.16125", "title": "KoBALT: Korean Benchmark For Advanced Linguistic Tasks", "authors": ["Hyopil Shin", "Sangah Lee", "Dongjun Jang", "Wooseok Song", "Jaeyoon Kim", "Chaeyoung Oh", "Hyemi Jo", "Youngchae Ahn", "Sihyun Oh", "Hyohyeong Chang", "Sunkyoung Kim", "Jinsik Lee"], "categories": ["cs.CL"], "comment": "Under Reveiw", "summary": "We introduce KoBALT (Korean Benchmark for Advanced Linguistic Tasks), a\ncomprehensive linguistically-motivated benchmark comprising 700 multiple-choice\nquestions spanning 24 phenomena across five linguistic domains: syntax,\nsemantics, pragmatics, phonetics/phonology, and morphology. KoBALT is designed\nto advance the evaluation of large language models (LLMs) in Korean, a\nmorphologically rich language, by addressing the limitations of conventional\nbenchmarks that often lack linguistic depth and typological grounding. It\nintroduces a suite of expert-curated, linguistically motivated questions with\nminimal n-gram overlap with standard Korean corpora, substantially mitigating\nthe risk of data contamination and allowing a more robust assessment of true\nlanguage understanding. Our evaluation of 20 contemporary LLMs reveals\nsignificant performance disparities, with the highest-performing model\nachieving 61\\% general accuracy but showing substantial variation across\nlinguistic domains - from stronger performance in semantics (66\\%) to\nconsiderable weaknesses in phonology (31\\%) and morphology (36\\%). Through\nhuman preference evaluation with 95 annotators, we demonstrate a strong\ncorrelation between KoBALT scores and human judgments, validating our\nbenchmark's effectiveness as a discriminative measure of Korean language\nunderstanding. KoBALT addresses critical gaps in linguistic evaluation for\ntypologically diverse languages and provides a robust framework for assessing\ngenuine linguistic competence in Korean language models.", "AI": {"tldr": "KoBALT is a benchmark for evaluating Korean LLMs with 700 questions across 24 linguistic phenomena, aiming to improve assessment depth and accuracy.", "motivation": "To advance LLM evaluation in Korean by addressing limitations of existing benchmarks that lack linguistic depth.", "method": "KoBALT includes expert-curated multiple-choice questions covering various linguistic domains, ensuring minimal n-gram overlap with standard corpora to reduce data contamination.", "result": "Evaluation of 20 LLMs shows significant performance disparities, with general accuracy peaking at 61%, revealing strengths in semantics and weaknesses in phonology and morphology.", "conclusion": "KoBALT provides an effective framework for evaluating linguistic competence in Korean language models, validating its use through correlation with human judgments.", "key_contributions": ["First comprehensive benchmark for Korean LLMs", "Addresses linguistic evaluation gaps in typologically diverse languages", "Validates effectiveness through human preference evaluation"], "limitations": "", "keywords": ["Korean language", "large language models", "linguistic evaluation", "benchmarking", "human-computer interaction"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2505.16128", "pdf": "https://arxiv.org/pdf/2505.16128.pdf", "abs": "https://arxiv.org/abs/2505.16128", "title": "Veracity Bias and Beyond: Uncovering LLMs' Hidden Beliefs in Problem-Solving Reasoning", "authors": ["Yue Zhou", "Barbara Di Eugenio"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Despite LLMs' explicit alignment against demographic stereotypes, they have\nbeen shown to exhibit biases under various social contexts. In this work, we\nfind that LLMs exhibit concerning biases in how they associate solution\nveracity with demographics. Through experiments across five human value-aligned\nLLMs on mathematics, coding, commonsense, and writing problems, we reveal two\nforms of such veracity biases: Attribution Bias, where models\ndisproportionately attribute correct solutions to certain demographic groups,\nand Evaluation Bias, where models' assessment of identical solutions varies\nbased on perceived demographic authorship. Our results show pervasive biases:\nLLMs consistently attribute fewer correct solutions and more incorrect ones to\nAfrican-American groups in math and coding, while Asian authorships are least\npreferred in writing evaluation. In additional studies, we show LLMs\nautomatically assign racially stereotypical colors to demographic groups in\nvisualization code, suggesting these biases are deeply embedded in models'\nreasoning processes. Our findings indicate that demographic bias extends beyond\nsurface-level stereotypes and social context provocations, raising concerns\nabout LLMs' deployment in educational and evaluation settings.", "AI": {"tldr": "This paper investigates biases in LLMs regarding the association of solution veracity with demographics, revealing significant disparities in how different demographic groups are evaluated across various tasks.", "motivation": "To explore the biases exhibited by LLMs in how they attribute solution correctness to different demographics, particularly in educational contexts.", "method": "Experiments were conducted on five human value-aligned LLMs across mathematics, coding, commonsense, and writing problems to identify biases in solution attribution and evaluation.", "result": "LLMs show a bias in attributing correct solutions more frequently to certain demographic groups, notably attributing fewer correct solutions to African-American groups in math and coding, and displaying preference against Asian authors in writing evaluations.", "conclusion": "Demographic bias in LLMs is complex and pervasive, indicating significant implications for their use in educational and evaluative settings.", "key_contributions": ["Identification of Attribution Bias and Evaluation Bias in LLM assessments", "Demonstration of racial stereotypical color assignments in LLM visualizations", "Implications for the deployment of LLMs in education and evaluation contexts"], "limitations": "The study focuses on only five LLMs and a limited set of tasks, which may not fully represent the diversity of LLM behaviors.", "keywords": ["LLMs", "demographic bias", "solution veracity", "educational evaluation", "human value alignment"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.16129", "pdf": "https://arxiv.org/pdf/2505.16129.pdf", "abs": "https://arxiv.org/abs/2505.16129", "title": "LLMs Are Not Scorers: Rethinking MT Evaluation with Generation-Based Methods", "authors": ["Hyang Cui"], "categories": ["cs.CL", "I.2.7"], "comment": "5 pages, 2 figures, 2 tables. Conforms to the ACL Rolling Review\n  (ARR) short paper track. Code and data available at:\n  https://github.com/CuiNiki/LLMs-Are-Not-Scorers", "summary": "Recent studies have applied large language models (LLMs) to machine\ntranslation quality estimation (MTQE) by prompting models to assign numeric\nscores. Nonetheless, these direct scoring methods tend to show low\nsegment-level correlation with human judgments. In this paper, we propose a\ngeneration-based evaluation paradigm that leverages decoder-only LLMs to\nproduce high-quality references, followed by semantic similarity scoring using\nsentence embeddings. We conduct the most extensive evaluation to date in MTQE,\ncovering 8 LLMs and 8 language pairs. Empirical results show that our method\noutperforms both intra-LLM direct scoring baselines and external non-LLM\nreference-free metrics from MTME. These findings demonstrate the strength of\ngeneration-based evaluation and support a shift toward hybrid approaches that\ncombine fluent generation with accurate semantic assessment.", "AI": {"tldr": "This paper presents a novel generation-based evaluation paradigm for machine translation quality estimation (MTQE) using decoder-only large language models (LLMs), which outperforms existing direct scoring methods and reference-free metrics.", "motivation": "Current machine translation quality estimation methods using LLMs often lack correlation with human judgments; this paper aims to improve evaluation accuracy.", "method": "The authors propose a generation-based approach where decoder-only LLMs create high-quality references, followed by semantic similarity scoring using sentence embeddings.", "result": "The method shows improved performance over baseline direct scoring and external reference-free metrics across 8 LLMs and 8 language pairs in extensive evaluations.", "conclusion": "The findings advocate for hybrid evaluation techniques that merge fluent generation with precise semantic evaluation for better translation quality assessment.", "key_contributions": ["Introduction of a generation-based evaluation paradigm", "Extensive empirical evaluation of MTQE methods across multiple LLMs and language pairs", "Demonstration of improved correlation with human judgments compared to standard methods."], "limitations": "The study focuses solely on 8 LLMs and 8 language pairs, which may limit generalizability.", "keywords": ["Machine Translation", "Quality Estimation", "Large Language Models", "Semantic Similarity", "Evaluation Methods"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.16134", "pdf": "https://arxiv.org/pdf/2505.16134.pdf", "abs": "https://arxiv.org/abs/2505.16134", "title": "Position of Uncertainty: A Cross-Linguistic Study of Positional Bias in Large Language Models", "authors": ["Menschikov Mikhail", "Alexander Kharitonov", "Maiia Kotyga", "Vadim Porvatov", "Anna Zhukovskaya", "David Kagramanyan", "Egor Shvetsov", "Evgeny Burnaev"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models exhibit positional bias -- systematic neglect of\ninformation at specific context positions -- yet its interplay with linguistic\ndiversity remains poorly understood. We present a cross-linguistic study across\nfive typologically distinct languages (English, Russian, German, Hindi,\nVietnamese), examining how positional bias interacts with model uncertainty,\nsyntax, and prompting. Key findings: (1) Positional bias is model-driven, with\nlanguage-specific variations -- Qwen2.5-7B favors late positions, challenging\nassumptions of early-token bias; (2) Explicit positional guidance (e.g.,\ncorrect context is at position X) reduces accuracy across languages,\nundermining prompt-engineering practices; (3) Aligning context with positional\nbias increases entropy, yet minimal entropy does not predict accuracy. (4) We\nfurther uncover that LLMs differently impose dominant word order in\nfree-word-order languages like Hindi.", "AI": {"tldr": "This study investigates how positional bias in large language models varies across five languages, revealing its model-driven nature and implications for prompt engineering.", "motivation": "To understand the relationship between positional bias in language models and linguistic diversity, addressing gaps in knowledge regarding how models vary across different languages.", "method": "A cross-linguistic study analyzing positional bias in five languages (English, Russian, German, Hindi, Vietnamese) and its interaction with model uncertainty, syntax, and prompting.", "result": "Positional bias is model-specific, with Qwen2.5-7B showing a preference for late positions, counter to expectations; explicit positional guidance lowers accuracy; aligning context with bias increases entropy without improving accuracy; LLMs influence word order in free-word-order languages.", "conclusion": "The findings highlight the complexity of positional bias and its non-uniform impact across languages, suggesting a need for reconsideration in prompt-engineering methods.", "key_contributions": ["Identification of model-driven positional bias with language-specific variations", "Impact of explicit positional guidance on accuracy", "Insights into dominance of word order in free-word-order languages"], "limitations": "The study focuses only on five languages; further research is needed to explore additional languages and contexts.", "keywords": ["positional bias", "large language models", "linguistic diversity", "cross-linguistic study", "prompt engineering"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.16142", "pdf": "https://arxiv.org/pdf/2505.16142.pdf", "abs": "https://arxiv.org/abs/2505.16142", "title": "Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning", "authors": ["Shicheng Xu", "Liang Pang", "Yunchang Zhu", "Jia Gu", "Zihao Wei", "Jingcheng Deng", "Feiyang Pan", "Huawei Shen", "Xueqi Cheng"], "categories": ["cs.CL"], "comment": "15 pages", "summary": "Distilling reasoning paths from teacher to student models via supervised\nfine-tuning (SFT) provides a shortcut for improving the reasoning ability of\nsmaller Large Language Models (LLMs). However, the reasoning paths generated by\nteacher models often reflect only surface-level traces of their underlying\nauthentic reasoning. Insights from cognitive neuroscience suggest that\nauthentic reasoning involves a complex interweaving between meta-reasoning\n(which selects appropriate sub-problems from multiple candidates) and solving\n(which addresses the sub-problem). This implies authentic reasoning has an\nimplicit multi-branch structure. Supervised fine-tuning collapses this rich\nstructure into a flat sequence of token prediction in the teacher's reasoning\npath, preventing effective distillation of this structure to students. To\naddress this limitation, we propose RLKD, a reinforcement learning (RL)-based\ndistillation framework guided by a novel Generative Structure Reward Model\n(GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving\nsteps and computes rewards to measure structural alignment between student and\nteacher reasoning. RLKD combines this reward with RL, enabling student LLMs to\ninternalize the teacher's implicit multi-branch reasoning structure rather than\nmerely mimicking fixed output paths. Experiments show RLKD surpasses standard\nSFT-RL pipelines even when trained on 0.1% of data under an RL-only regime,\nunlocking greater student reasoning potential than SFT-based distillation.", "AI": {"tldr": "This paper introduces RLKD, a reinforcement learning-based distillation framework that improves the reasoning ability of smaller LLMs by effectively capturing the multi-branch reasoning structure from teacher models.", "motivation": "Existing distillation methods reduce complex reasoning paths into flat sequences, losing critical multi-branch structures necessary for authentic reasoning.", "method": "The proposed RLKD uses a Generative Structure Reward Model (GSRM) to convert reasoning paths into meta-reasoning-solving steps, providing rewards based on structural alignment between student and teacher models.", "result": "RLKD achieves superior performance over standard supervised fine-tuning and reinforcement learning pipelines, even with limited training data.", "conclusion": "By enabling student models to absorb rich reasoning structures, RLKD significantly enhances their reasoning capabilities compared to traditional methods.", "key_contributions": ["Introduces a novel reinforcement learning-based distillation method (RLKD) for LLMs.", "Develops a Generative Structure Reward Model that encapsulates multi-branch reasoning.", "Demonstrates improved reasoning capabilities in student models with minimal training data."], "limitations": "The study primarily focuses on a specific distillation approach and may require further exploration of its applicability across various models and tasks.", "keywords": ["Reinforcement Learning", "Large Language Models", "Distillation", "Meta-reasoning", "Neuroscience"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.16160", "pdf": "https://arxiv.org/pdf/2505.16160.pdf", "abs": "https://arxiv.org/abs/2505.16160", "title": "EduBench: A Comprehensive Benchmarking Dataset for Evaluating Large Language Models in Diverse Educational Scenarios", "authors": ["Bin Xu", "Yu Bai", "Huashan Sun", "Yiguan Lin", "Siming Liu", "Xinyue Liang", "Yaolin Li", "Yang Gao", "Heyan Huang"], "categories": ["cs.CL"], "comment": null, "summary": "As large language models continue to advance, their application in\neducational contexts remains underexplored and under-optimized. In this paper,\nwe address this gap by introducing the first diverse benchmark tailored for\neducational scenarios, incorporating synthetic data containing 9 major\nscenarios and over 4,000 distinct educational contexts. To enable comprehensive\nassessment, we propose a set of multi-dimensional evaluation metrics that cover\n12 critical aspects relevant to both teachers and students. We further apply\nhuman annotation to ensure the effectiveness of the model-generated evaluation\nresponses. Additionally, we succeed to train a relatively small-scale model on\nour constructed dataset and demonstrate that it can achieve performance\ncomparable to state-of-the-art large models (e.g., Deepseek V3, Qwen Max) on\nthe test set. Overall, this work provides a practical foundation for the\ndevelopment and evaluation of education-oriented language models. Code and data\nare released at https://github.com/ybai-nlp/EduBench.", "AI": {"tldr": "This paper introduces the EduBench benchmark for assessing educational language models and presents a multi-dimensional evaluation framework with synthetic datasets.", "motivation": "To fill the gap in applying large language models in education by creating a benchmark tailored to educational contexts.", "method": "The authors developed a diverse benchmark containing synthetic data across 9 scenarios and 4,000 educational contexts, proposing multi-dimensional evaluation metrics for assessment.", "result": "A relatively small-scale model trained on the EduBench dataset achieved performance comparable to state-of-the-art large models in educational contexts.", "conclusion": "The work lays a practical foundation for developing and evaluating education-focused language models, with data and code made available for the community.", "key_contributions": ["Introduction of the EduBench benchmark for educational language models", "Development of multi-dimensional evaluation metrics", "Training of a small model that outperforms traditional large models in educational contexts"], "limitations": "The benchmark may not cover all educational scenarios or contexts comprehensively.", "keywords": ["large language models", "educational technology", "benchmark", "evaluation metrics", "synthetic data"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.16162", "pdf": "https://arxiv.org/pdf/2505.16162.pdf", "abs": "https://arxiv.org/abs/2505.16162", "title": "KNN-SSD: Enabling Dynamic Self-Speculative Decoding via Nearest Neighbor Layer Set Optimization", "authors": ["Mingbo Song", "Heming Xia", "Jun Zhang", "Chak Tou Leong", "Qiancheng Xu", "Wenjie Li", "Sujian Li"], "categories": ["cs.CL"], "comment": "8 pages", "summary": "Speculative Decoding (SD) has emerged as a widely used paradigm to accelerate\nthe inference of large language models (LLMs) without compromising generation\nquality. It works by efficiently drafting multiple tokens using a compact model\nand then verifying them in parallel using the target LLM. Notably,\nSelf-Speculative Decoding proposes skipping certain layers to construct the\ndraft model, which eliminates the need for additional parameters or training.\nDespite its strengths, we observe in this work that drafting with layer\nskipping exhibits significant sensitivity to domain shifts, leading to a\nsubstantial drop in acceleration performance. To enhance the domain\ngeneralizability of this paradigm, we introduce KNN-SSD, an algorithm that\nleverages K-Nearest Neighbor (KNN) search to match different skipped layers\nwith various domain inputs. We evaluated our algorithm in various models and\nmultiple tasks, observing that its application leads to 1.3x-1.6x speedup in\nLLM inference.", "AI": {"tldr": "This paper discusses KNN-SSD, an algorithm that enhances the domain generalizability of Self-Speculative Decoding for accelerating large language models' inference.", "motivation": "To address the sensitivity of Self-Speculative Decoding to domain shifts, which impacts acceleration performance in language model inference.", "method": "KNN-SSD employs K-Nearest Neighbor search to dynamically select appropriate skipped layers based on the input domain, avoiding the need for additional parameters or retraining.", "result": "KNN-SSD achieves speedups of 1.3x-1.6x in large language model inference across various tasks, improving upon traditional Self-Speculative Decoding.", "conclusion": "The introduction of KNN-SSD makes Self-Speculative Decoding more robust to domain shifts, establishing a more effective acceleration strategy for LLMs.", "key_contributions": ["Introduction of KNN-SSD algorithm", "Significant speedups in inference performance", "Enhanced domain generalizability for LLMs"], "limitations": "Sensitivity to domain shifts remains a challenge, but KNN-SSD mitigates this issue.", "keywords": ["Self-Speculative Decoding", "K-Nearest Neighbor", "Domain Generalization"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2505.16164", "pdf": "https://arxiv.org/pdf/2505.16164.pdf", "abs": "https://arxiv.org/abs/2505.16164", "title": "Can LLMs Simulate Human Behavioral Variability? A Case Study in the Phonemic Fluency Task", "authors": ["Mengyang Qiu", "Zoe Brisebois", "Siena Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly explored as substitutes for\nhuman participants in cognitive tasks, but their ability to simulate human\nbehavioral variability remains unclear. This study examines whether LLMs can\napproximate individual differences in the phonemic fluency task, where\nparticipants generate words beginning with a target letter. We evaluated 34\nmodel configurations, varying prompt specificity, sampling temperature, and\nmodel type, and compared outputs to responses from 106 human participants.\nWhile some configurations, especially Claude 3.7 Sonnet, matched human averages\nand lexical preferences, none reproduced the scope of human variability. LLM\noutputs were consistently less diverse and structurally rigid, and LLM\nensembles failed to increase diversity. Network analyses further revealed\nfundamental differences in retrieval structure between humans and models. These\nresults highlight key limitations in using LLMs to simulate human cognition and\nbehavior.", "AI": {"tldr": "This study investigates the ability of large language models (LLMs) to simulate human behavioral variability in a phonemic fluency task, comparing LLM outputs to those of human participants.", "motivation": "To examine if LLMs can replicate individual differences in cognitive tasks and understand their limitations in simulating human behavior.", "method": "The researchers evaluated 34 LLM configurations, altering prompt specificity, sampling temperature, and model type, and compared outputs from these models to responses from 106 human participants in a phonemic fluency task.", "result": "Some LLM configurations, particularly Claude 3.7 Sonnet, reflected human averages and preferences, yet none matched the diversity of human responses, with LLM outputs being less varied and more structured.", "conclusion": "The study concludes that LLMs have significant limitations in simulating human cognition and behavior, as they fail to capture human variability in cognitive tasks effectively.", "key_contributions": ["Evaluation of LLMs in a cognitive task context", "Analysis of model configurations affecting outputs", "Insights into the differences in retrieval structure between humans and LLMs"], "limitations": "LLMs did not reproduce the diversity of human responses in the phonemic fluency task, indicating their limitations in simulating human-like variability.", "keywords": ["large language models", "human cognition", "behavioral variability", "phonemic fluency task", "cognitive tasks"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.16170", "pdf": "https://arxiv.org/pdf/2505.16170.pdf", "abs": "https://arxiv.org/abs/2505.16170", "title": "When Do LLMs Admit Their Mistakes? Understanding the Role of Model Belief in Retraction", "authors": ["Yuqing Yang", "Robin Jia"], "categories": ["cs.CL"], "comment": null, "summary": "Can large language models (LLMs) admit their mistakes when they should know\nbetter? In this work, we define the behavior of acknowledging errors in\npreviously generated answers as \"retraction\" and aim to understand when and why\nLLMs choose to retract. We first construct model-specific datasets to evaluate\nwhether a model will retract an incorrect answer that contradicts its own\nparametric knowledge. While LLMs are capable of retraction, they do so only\ninfrequently. We demonstrate that retraction is closely tied to previously\nidentified indicators of models' internal belief: models fail to retract wrong\nanswers that they \"believe\" to be factually correct. Steering experiments\nfurther demonstrate that internal belief causally influences model retraction.\nIn particular, when the model does not believe its answer, this not only\nencourages the model to attempt to verify the answer, but also alters attention\nbehavior during self-verification. Finally, we demonstrate that simple\nsupervised fine-tuning significantly improves retraction performance by helping\nthe model learn more accurate internal beliefs. Code and datasets are available\non https://github.com/ayyyq/llm-retraction.", "AI": {"tldr": "This paper explores the phenomenon of retraction in large language models (LLMs), investigating when and why they admit to previous mistakes, and demonstrates that improved internal belief accuracy enhances their retraction ability.", "motivation": "Understanding the conditions under which LLMs acknowledge their errors is crucial for improving their reliability in generating factual information.", "method": "The study constructs model-specific datasets to evaluate LLMs' retraction behavior and conducts steering experiments to analyze the influence of internal beliefs on retraction.", "result": "LLMs retract incorrect answers infrequently, and this behavior is influenced by their internal belief in the correctness of prior generated answers.", "conclusion": "Supervised fine-tuning can improve the retraction performance of LLMs by enabling them to learn more accurate internal beliefs about their answers.", "key_contributions": ["Definition and analysis of LLM retraction behavior", "Demonstration of the causal influence of internal beliefs on retraction", "Validation of supervised fine-tuning's effectiveness in improving retraction performance"], "limitations": "", "keywords": ["retraction", "large language models", "internal beliefs", "supervised fine-tuning", "self-verification"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.16172", "pdf": "https://arxiv.org/pdf/2505.16172.pdf", "abs": "https://arxiv.org/abs/2505.16172", "title": "Automated Feedback Loops to Protect Text Simplification with Generative AI from Information Loss", "authors": ["Abhay Kumara Sri Krishna Nandiraju", "Gondy Leroy", "David Kauchak", "Arif Ahmed"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Understanding health information is essential in achieving and maintaining a\nhealthy life. We focus on simplifying health information for better\nunderstanding. With the availability of generative AI, the simplification\nprocess has become efficient and of reasonable quality, however, the algorithms\nremove information that may be crucial for comprehension. In this study, we\ncompare generative AI to detect missing information in simplified text,\nevaluate its importance, and fix the text with the missing information. We\ncollected 50 health information texts and simplified them using gpt-4-0613. We\ncompare five approaches to identify missing elements and regenerate the text by\ninserting the missing elements. These five approaches involve adding missing\nentities and missing words in various ways: 1) adding all the missing entities,\n2) adding all missing words, 3) adding the top-3 entities ranked by gpt-4-0613,\nand 4, 5) serving as controls for comparison, adding randomly chosen entities.\nWe use cosine similarity and ROUGE scores to evaluate the semantic similarity\nand content overlap between the original, simplified, and reconstructed\nsimplified text. We do this for both summaries and full text. Overall, we find\nthat adding missing entities improves the text. Adding all the missing entities\nresulted in better text regeneration, which was better than adding the\ntop-ranked entities or words, or random words. Current tools can identify these\nentities, but are not valuable in ranking them.", "AI": {"tldr": "This study investigates the effectiveness of generative AI in simplifying health information by detecting and inserting missing essential elements.", "motivation": "The need for better understanding of health information and the limitations of current generative AI models in maintaining crucial details during simplification.", "method": "The study involved simplifying 50 health information texts using gpt-4-0613 and comparing five approaches to detect and insert missing elements, evaluated through cosine similarity and ROUGE scores.", "result": "Adding all missing entities significantly improved the quality of the regenerated text compared to other methods.", "conclusion": "Enhancing generative AI models to effectively identify and rank missing information can lead to better comprehension of simplified health texts.", "key_contributions": ["Evaluated multiple approaches for detecting missing information in simplified health texts.", "Demonstrated the efficacy of adding all missing entities in improving text regeneration.", "Identified limitations of current tools in ranking missing entities."], "limitations": "Current tools can identify missing elements but do not provide valuable rankings for better reconstruction.", "keywords": ["health information", "generative AI", "text simplification", "machine learning", "natural language processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.16178", "pdf": "https://arxiv.org/pdf/2505.16178.pdf", "abs": "https://arxiv.org/abs/2505.16178", "title": "Understanding Fact Recall in Language Models: Why Two-Stage Training Encourages Memorization but Mixed Training Teaches Knowledge", "authors": ["Ying Zhang", "Benjamin Heinzerling", "Dongyuan Li", "Ryoma Ishigaki", "Yuta Hitomi", "Kentaro Inui"], "categories": ["cs.CL"], "comment": null, "summary": "Fact recall, the ability of language models (LMs) to retrieve specific\nfactual knowledge, remains a challenging task despite their impressive general\ncapabilities. Common training strategies often struggle to promote robust\nrecall behavior with two-stage training, which first trains a model with\nfact-storing examples (e.g., factual statements) and then with fact-recalling\nexamples (question-answer pairs), tending to encourage rote memorization rather\nthan generalizable fact retrieval. In contrast, mixed training, which jointly\nuses both types of examples, has been empirically shown to improve the ability\nto recall facts, but the underlying mechanisms are still poorly understood. In\nthis work, we investigate how these training strategies affect how model\nparameters are shaped during training and how these differences relate to their\nability to recall facts. We introduce cross-task gradient trace to identify\nshared parameters, those strongly influenced by both fact-storing and\nfact-recalling examples. Our analysis on synthetic fact recall datasets with\nthe Llama-3.2B and Pythia-2.8B models reveals that mixed training encouraging a\nlarger and more centralized set of shared parameters. These findings suggest\nthat the emergence of parameters may play a key role in enabling LMs to\ngeneralize factual knowledge across task formulations.", "AI": {"tldr": "This paper investigates the impact of mixed vs. two-stage training strategies on language models' fact recall abilities.", "motivation": "Despite advancements in language models, fact recall remains a significant challenge that current training strategies struggle to address effectively.", "method": "The authors analyze the influence of training strategies on model parameters using cross-task gradient trace to identify shared parameters that are influenced by both fact-storing and fact-recalling examples.", "result": "The analysis reveals that mixed training leads to a larger and more centralized set of shared parameters, enhancing LMs' ability to generalize factual knowledge across tasks.", "conclusion": "These findings indicate that understanding the emergence of shared parameters is critical for improving language models' fact retrieval capabilities.", "key_contributions": ["Introduction of cross-task gradient trace for identifying shared parameters", "Empirical evidence showing mixed training improves fact recall", "Insights into the relationship between training strategy and model parameter shaping."], "limitations": "", "keywords": ["fact recall", "language models", "training strategies", "mixed training", "shared parameters"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.16188", "pdf": "https://arxiv.org/pdf/2505.16188.pdf", "abs": "https://arxiv.org/abs/2505.16188", "title": "SAE-SSV: Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models", "authors": ["Zirui He", "Mingyu Jin", "Bo Shen", "Ali Payani", "Yongfeng Zhang", "Mengnan Du"], "categories": ["cs.CL"], "comment": "30 pages, 24 figures, 12 tables", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but controlling their behavior\nreliably remains challenging, especially in open-ended generation settings.\nThis paper introduces a novel supervised steering approach that operates in\nsparse, interpretable representation spaces. We employ sparse autoencoders\n(SAEs)to obtain sparse latent representations that aim to disentangle semantic\nattributes from model activations. Then we train linear classifiers to identify\na small subspace of task-relevant dimensions in latent representations.\nFinally, we learn supervised steering vectors constrained to this subspace,\noptimized to align with target behaviors. Experiments across sentiment,\ntruthfulness, and politics polarity steering tasks with multiple LLMs\ndemonstrate that our supervised steering vectors achieve higher success rates\nwith minimal degradation in generation quality compared to existing methods.\nFurther analysis reveals that a notably small subspace is sufficient for\neffective steering, enabling more targeted and interpretable interventions.", "AI": {"tldr": "This paper presents a supervised steering method for large language models (LLMs) using sparse autoencoders to guide model behavior in open-ended tasks.", "motivation": "The need to control LLMs effectively in open-ended generation settings, while maintaining quality and reliability in their outputs.", "method": "Using sparse autoencoders to create interpretable, sparse latent representations, followed by training linear classifiers to pinpoint relevant dimensions in these representations and learning steering vectors constrained to those dimensions.", "result": "The proposed method achieves higher success rates in steering tasks, such as sentiment and truthfulness, with little to no degradation in output quality as compared to existing approaches.", "conclusion": "A small subspace of dimensions is sufficient for effective steering, leading to targeted interventions in LLM behavior.", "key_contributions": ["Introduces a supervised steering approach for LLMs", "Employs sparse autoencoders for interpretable representations", "Demonstrates high success rates with minimal quality loss in steering tasks."], "limitations": "The performance in highly complex tasks may still need further investigation and validation.", "keywords": ["large language models", "supervised steering", "sparse autoencoders", "interpretability", "natural language processing"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2505.16189", "pdf": "https://arxiv.org/pdf/2505.16189.pdf", "abs": "https://arxiv.org/abs/2505.16189", "title": "The Language of Interoception: Examining Embodiment and Emotion Through a Corpus of Body Part Mentions", "authors": ["Sophie Wu", "Jan Philip Wahle", "Saif M. Mohammad"], "categories": ["cs.CL"], "comment": "8 pages, 26 figures", "summary": "This paper is the first investigation of the connection between emotion,\nembodiment, and everyday language in a large sample of natural language data.\nWe created corpora of body part mentions (BPMs) in online English text (blog\nposts and tweets). This includes a subset featuring human annotations for the\nemotions of the person whose body part is mentioned in the text. We show that\nBPMs are common in personal narratives and tweets (~5% to 10% of posts include\nBPMs) and that their usage patterns vary markedly by time and %geographic\nlocation. Using word-emotion association lexicons and our annotated data, we\nshow that text containing BPMs tends to be more emotionally charged, even when\nthe BPM is not explicitly used to describe a physical reaction to the emotion\nin the text. Finally, we discover a strong and statistically significant\ncorrelation between body-related language and a variety of poorer health\noutcomes. In sum, we argue that investigating the role of body-part related\nwords in language can open up valuable avenues of future research at the\nintersection of NLP, the affective sciences, and the study of human wellbeing.", "AI": {"tldr": "This paper investigates the relationship between emotion, embodiment, and language through analysis of body part mentions (BPMs) in online text. It finds that BPMs are prevalent in personal narratives and correlate with emotional intensity and poorer health outcomes.", "motivation": "To explore the connection between emotion, embodiment, and everyday language in large-scale natural language data.", "method": "Analysis of corpora of body part mentions in online text, along with human annotations for the emotions associated with those mentions. Utilizes word-emotion association lexicons to study patterns and correlations.", "result": "BPMs are found in 5% to 10% of personal narratives and tweets, and their usage varies over time and geography. Text with BPMs is more emotionally charged, and there is a strong correlation between body-related language and poorer health outcomes.", "conclusion": "Investigating body-part related words may provide insight into the intersection of NLP, affective sciences, and human wellbeing.", "key_contributions": ["First study linking emotion, embodiment, and language using a large dataset", "Identified commonality and emotional charge of BPMs in online text", "Established correlation between BPMs and health outcomes"], "limitations": "Limited to English text on online platforms; may not generalize to other languages or contexts.", "keywords": ["emotion", "embodiment", "natural language processing", "body part mentions", "health outcomes"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2505.16193", "pdf": "https://arxiv.org/pdf/2505.16193.pdf", "abs": "https://arxiv.org/abs/2505.16193", "title": "An Empirical Study on Configuring In-Context Learning Demonstrations for Unleashing MLLMs' Sentimental Perception Capability", "authors": ["Daiqing Wu", "Dongbao Yang", "Sicheng Zhao", "Can Ma", "Yu Zhou"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "The advancements in Multimodal Large Language Models (MLLMs) have enabled\nvarious multimodal tasks to be addressed under a zero-shot paradigm. This\nparadigm sidesteps the cost of model fine-tuning, emerging as a dominant trend\nin practical application. Nevertheless, Multimodal Sentiment Analysis (MSA), a\npivotal challenge in the quest for general artificial intelligence, fails to\naccommodate this convenience. The zero-shot paradigm exhibits undesirable\nperformance on MSA, casting doubt on whether MLLMs can perceive sentiments as\ncompetent as supervised models. By extending the zero-shot paradigm to\nIn-Context Learning (ICL) and conducting an in-depth study on configuring\ndemonstrations, we validate that MLLMs indeed possess such capability.\nSpecifically, three key factors that cover demonstrations' retrieval,\npresentation, and distribution are comprehensively investigated and optimized.\nA sentimental predictive bias inherent in MLLMs is also discovered and later\neffectively counteracted. By complementing each other, the devised strategies\nfor three factors result in average accuracy improvements of 15.9% on six MSA\ndatasets against the zero-shot paradigm and 11.2% against the random ICL\nbaseline.", "AI": {"tldr": "This paper explores the effectiveness of Multimodal Large Language Models (MLLMs) in Multimodal Sentiment Analysis (MSA) under a zero-shot paradigm, proposing strategies to improve their performance through in-context learning configurations.", "motivation": "To address the limitations of zero-shot performance of MLLMs in Multimodal Sentiment Analysis, which is crucial for advancing artificial intelligence capabilities.", "method": "The paper extends the zero-shot paradigm to in-context learning (ICL) by investigating and optimizing three key factors: retrieval, presentation, and distribution of demonstrations.", "result": "The proposed strategies improved average accuracy by 15.9% on six MSA datasets compared to the zero-shot paradigm and by 11.2% against the random ICL baseline.", "conclusion": "MLLMs can perceive sentiments effectively when equipped with optimized in-context learning strategies, overcoming inherent predictive biases.", "key_contributions": ["Introduction of optimized strategies for in-context learning in MLLMs", "Demonstrated significant performance improvements in MSA tasks", "Identification and counteraction of MLLMs' sentimental predictive bias"], "limitations": "", "keywords": ["Multimodal Large Language Models", "Multimodal Sentiment Analysis", "In-Context Learning", "Zero-Shot Learning", "Predictive Bias"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.16212", "pdf": "https://arxiv.org/pdf/2505.16212.pdf", "abs": "https://arxiv.org/abs/2505.16212", "title": "Large Language Models based ASR Error Correction for Child Conversations", "authors": ["Anfeng Xu", "Tiantian Feng", "So Hyun Kim", "Somer Bishop", "Catherine Lord", "Shrikanth Narayanan"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Automatic Speech Recognition (ASR) has recently shown remarkable progress,\nbut accurately transcribing children's speech remains a significant challenge.\nRecent developments in Large Language Models (LLMs) have shown promise in\nimproving ASR transcriptions. However, their applications in child speech\nincluding conversational scenarios are underexplored. In this study, we explore\nthe use of LLMs in correcting ASR errors for conversational child speech. We\ndemonstrate the promises and challenges of LLMs through experiments on two\nchildren's conversational speech datasets with both zero-shot and fine-tuned\nASR outputs. We find that while LLMs are helpful in correcting zero-shot ASR\noutputs and fine-tuned CTC-based ASR outputs, it remains challenging for LLMs\nto improve ASR performance when incorporating contextual information or when\nusing fine-tuned autoregressive ASR (e.g., Whisper) outputs.", "AI": {"tldr": "This study investigates the use of Large Language Models (LLMs) to improve Automatic Speech Recognition (ASR) for children's speech, addressing the challenges and effectiveness of LLMs in error correction in conversational datasets.", "motivation": "Accurate transcription of children's speech by ASR systems is challenging, and recent advances in LLMs offer potential solutions. However, their application to children's conversational scenarios is not well-explored.", "method": "Experiments were conducted on two children's conversational speech datasets using zero-shot and fine-tuned ASR outputs. The study analyzes LLM performance in correcting ASR errors across different output types.", "result": "LLMs were found to effectively correct zero-shot ASR outputs and certain fine-tuned ASR outputs, but struggled with contextual information and autoregressive ASR outputs.", "conclusion": "While LLMs show promise in enhancing ASR for children's speech, challenges remain in their application particularly with contextual information and specific ASR models.", "key_contributions": ["Demonstrated the effectiveness of LLMs in correcting ASR errors in children's speech.", "Identified challenges in applying LLMs to improve ASR performance in particular scenarios.", "Provided insights into the performance of zero-shot versus fine-tuned ASR outputs when corrected by LLMs."], "limitations": "Challenges remain in improving ASR performance with contextual information and fine-tuned autoregressive models.", "keywords": ["Automatic Speech Recognition", "Large Language Models", "Child Speech", "Conversational Scenarios", "Error Correction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.16216", "pdf": "https://arxiv.org/pdf/2505.16216.pdf", "abs": "https://arxiv.org/abs/2505.16216", "title": "Memorization or Reasoning? Exploring the Idiom Understanding of LLMs", "authors": ["Jisu Kim", "Youngwoo Shin", "Uiji Hwang", "Jihun Choi", "Richeng Xuan", "Taeuk Kim"], "categories": ["cs.CL"], "comment": null, "summary": "Idioms have long posed a challenge due to their unique linguistic properties,\nwhich set them apart from other common expressions. While recent studies have\nleveraged large language models (LLMs) to handle idioms across various tasks,\ne.g., idiom-containing sentence generation and idiomatic machine translation,\nlittle is known about the underlying mechanisms of idiom processing in LLMs,\nparticularly in multilingual settings. To this end, we introduce MIDAS, a new\nlarge-scale dataset of idioms in six languages, each paired with its\ncorresponding meaning. Leveraging this resource, we conduct a comprehensive\nevaluation of LLMs' idiom processing ability, identifying key factors that\ninfluence their performance. Our findings suggest that LLMs rely not only on\nmemorization, but also adopt a hybrid approach that integrates contextual cues\nand reasoning, especially when processing compositional idioms. This implies\nthat idiom understanding in LLMs emerges from an interplay between internal\nknowledge retrieval and reasoning-based inference.", "AI": {"tldr": "The paper introduces MIDAS, a dataset for studying idiom processing in multilingual settings, and evaluates how large language models (LLMs) handle idioms.", "motivation": "There is a lack of understanding of idiom processing mechanisms in large language models, especially in multilingual contexts.", "method": "The authors created the MIDAS dataset containing idioms in six languages with their meanings, and conducted a comprehensive evaluation on LLMs' performance in processing these idioms.", "result": "LLMs use a hybrid approach for idiom processing, combining memorization with contextual cues and reasoning, particularly for compositional idioms.", "conclusion": "Idiom understanding in LLMs involves both knowledge retrieval and reasoning-based inference.", "key_contributions": ["Introduction of MIDAS dataset for idioms in six languages", "Comprehensive evaluation of LLMs' idiom processing ability", "Insights into the hybrid approach of LLMs in understanding idioms"], "limitations": "", "keywords": ["idioms", "large language models", "multilingual", "dataset", "natural language processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.16222", "pdf": "https://arxiv.org/pdf/2505.16222.pdf", "abs": "https://arxiv.org/abs/2505.16222", "title": "Don't Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation", "authors": ["Jiwon Moon", "Yerin Hwang", "Dongryeol Lee", "Taegwan Kang", "Yongil Kim", "Kyomin Jung"], "categories": ["cs.CL", "cs.SE"], "comment": "26 pages", "summary": "With the growing use of large language models(LLMs) as evaluators, their\napplication has expanded to code evaluation tasks, where they assess the\ncorrectness of generated code without relying on reference implementations.\nWhile this offers scalability and flexibility, it also raises a critical,\nunresolved question: Can LLM judges fairly and robustly evaluate semantically\nequivalent code with superficial variations? Functionally correct code often\nexhibits variations-such as differences in variable names, comments, or\nformatting-that should not influence its correctness. Yet, whether LLM judges\ncan reliably handle these variations remains unclear. We present the first\ncomprehensive study of this issue, defining six types of potential bias in code\nevaluation and revealing their systematic impact on LLM judges. Across five\nprogramming languages and multiple LLMs, we empirically demonstrate that all\ntested LLM judges are susceptible to both positive and negative biases,\nresulting in inflated or unfairly low scores. Moreover, we observe that LLM\njudges remain vulnerable to these biases even when prompted to generate test\ncases before scoring, highlighting the need for more robust code evaluation\nmethods.", "AI": {"tldr": "A study examining biases in large language models (LLMs) when evaluating code correctness, revealing their vulnerability to superficial variations in functionally correct code.", "motivation": "Growing use of LLMs for code evaluation raises concerns about their ability to fairly assess semantically equivalent code with superficial variations.", "method": "The study defines six types of potential bias in code evaluation and empirically tests multiple LLMs across five programming languages to observe their performance.", "result": "All tested LLM judges exhibited susceptibility to both positive and negative biases, affecting their scoring accuracy.", "conclusion": "There is a critical need for more robust code evaluation methods to ensure fairness and accuracy in LLM-driven assessments.", "key_contributions": ["First comprehensive study on biases in LLM code evaluation", "Definition of six types of potential bias", "Demonstration of LLMs' vulnerability to scoring biases across multiple programming languages"], "limitations": "The study focuses only on certain programming languages and LLMs; further research needed with additional languages and models.", "keywords": ["Large Language Models", "Code Evaluation", "Bias in AI", "Human-Computer Interaction", "Programming"], "importance_score": 9, "read_time_minutes": 26}}
{"id": "2505.16227", "pdf": "https://arxiv.org/pdf/2505.16227.pdf", "abs": "https://arxiv.org/abs/2505.16227", "title": "Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning", "authors": ["Bohao Wu", "Qingyun Wang", "Yue Guo"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Personalizing jargon detection and explanation is essential for making\ntechnical documents accessible to readers with diverse disciplinary\nbackgrounds. However, tailoring models to individual users typically requires\nsubstantial annotation efforts and computational resources due to user-specific\nfinetuning. To address this, we present a systematic study of personalized\njargon detection, focusing on methods that are both efficient and scalable for\nreal-world deployment. We explore two personalization strategies: (1)\nlightweight fine-tuning using Low-Rank Adaptation (LoRA) on open-source models,\nand (2) personalized prompting, which tailors model behavior at inference time\nwithout retaining. To reflect realistic constraints, we also investigate hybrid\napproaches that combine limited annotated data with unsupervised user\nbackground signals. Our personalized LoRA model outperforms GPT-4 by 21.4% in\nF1 score and exceeds the best performing oracle baseline by 8.3%. Remarkably,\nour method achieves comparable performance using only 10% of the annotated\ntraining data, demonstrating its practicality for resource-constrained\nsettings. Our study offers the first work to systematically explore efficient,\nlow-resource personalization of jargon detection using open-source language\nmodels, offering a practical path toward scalable, user-adaptive NLP system.", "AI": {"tldr": "This paper explores efficient methods for personalized jargon detection in technical documents, focusing on low-resource approaches that enhance accessibility without substantial computational demands.", "motivation": "To make technical documents more accessible to readers with diverse backgrounds through personalized jargon detection and explanation.", "method": "The study investigates lightweight fine-tuning with Low-Rank Adaptation (LoRA) on open-source models, personalized prompting at inference time, and hybrid approaches combining limited annotated data with unsupervised signals.", "result": "The personalized LoRA model outperforms GPT-4 by 21.4% in F1 score and surpasses the best oracle baseline by 8.3%, achieving comparable performance with only 10% of the training data.", "conclusion": "This work provides the first systematic exploration of low-resource personalization strategies for jargon detection, demonstrating a practical path for user-adaptive NLP applications.", "key_contributions": ["Introduction of low-resource personalization strategies for jargon detection", "Demonstration of superior performance over existing models", "Exploration of hybrid personalization approaches combining various data sources"], "limitations": "", "keywords": ["jargon detection", "personalization", "Low-Rank Adaptation", "NLP", "open-source models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.16232", "pdf": "https://arxiv.org/pdf/2505.16232.pdf", "abs": "https://arxiv.org/abs/2505.16232", "title": "MuseRAG: Idea Originality Scoring At Scale", "authors": ["Ali Sarosh Bangash", "Krish Veera", "Ishfat Abrar Islam", "Raiyan Abdul Baten"], "categories": ["cs.CL"], "comment": null, "summary": "An objective, face-valid way to assess the originality of creative ideas is\nto measure how rare each idea is within a population -- an approach long used\nin creativity research but difficult to automate at scale. Tabulating response\nfrequencies via manual bucketing of idea rephrasings is labor-intensive,\nerror-prone, and brittle under large corpora. We introduce a fully automated,\npsychometrically validated pipeline for frequency-based originality scoring.\nOur method, MuseRAG, combines large language models (LLMs) with an externally\norchestrated retrieval-augmented generation (RAG) framework. Given a new idea,\nthe system retrieves semantically similar prior idea buckets and zero-shot\nprompts the LLM to judge whether the new idea belongs to an existing bucket or\nforms a new one. The resulting buckets enable computation of frequency-based\noriginality metrics. Across five datasets (N=1143, n_ideas=16294), MuseRAG\nmatches human annotators in idea clustering structure and resolution (AMI =\n0.59) and in participant-level scoring (r = 0.89) -- while exhibiting strong\nconvergent and external validity. Our work enables intent-sensitive,\nhuman-aligned originality scoring at scale to aid creativity research.", "AI": {"tldr": "MuseRAG is an automated system for originality scoring of creative ideas using LLMs and a retrieval-augmented generation framework.", "motivation": "To automate the labor-intensive and error-prone process of measuring originality of creative ideas based on their rarity in a population.", "method": "The MuseRAG pipeline uses large language models to retrieve similar prior ideas and evaluate new ideas for originality, calculating frequency-based metrics.", "result": "MuseRAG matches human annotators in clustering structure and scoring, demonstrating strong validity across multiple datasets.", "conclusion": "This method enables scalable originality scoring in creativity research, aligning with human intent and sensitivity.", "key_contributions": ["Development of the MuseRAG system for originality scoring", "Combination of LLMs with retrieval-augmented generation for creativity assessment", "Empirical validation across five datasets showing high alignment with human judgments"], "limitations": "The automated system may still be influenced by biases in the training data of LLMs and requires ongoing validation across diverse contexts.", "keywords": ["originality", "creativity", "large language models", "retrieval-augmented generation", "automation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.16234", "pdf": "https://arxiv.org/pdf/2505.16234.pdf", "abs": "https://arxiv.org/abs/2505.16234", "title": "LIFEBench: Evaluating Length Instruction Following in Large Language Models", "authors": ["Wei Zhang", "Zhenhong Zhou", "Junfeng Fang", "Rongwu Xu", "Kun Wang", "Yuanhe Zhang", "Rui Wang", "Ge Zhang", "Xinfeng Li", "Li Sun", "Lingjuan Lyu", "Yang Liu", "Sen Su"], "categories": ["cs.CL", "cs.AI"], "comment": "81 pages, 22 tables, 32 figures. Homepage:\n  https://ydyjya.github.io/LIFEBench/", "summary": "While large language models (LLMs) can solve PhD-level reasoning problems\nover long context inputs, they still struggle with a seemingly simpler task:\nfollowing explicit length instructions-e.g., write a 10,000-word novel.\nAdditionally, models often generate far too short outputs, terminate\nprematurely, or even refuse the request. Existing benchmarks focus primarily on\nevaluating generations quality, but often overlook whether the generations meet\nlength constraints. To this end, we introduce Length Instruction Following\nEvaluation Benchmark (LIFEBench) to comprehensively evaluate LLMs' ability to\nfollow length instructions across diverse tasks and a wide range of specified\nlengths. LIFEBench consists of 10,800 instances across 4 task categories in\nboth English and Chinese, covering length constraints ranging from 16 to 8192\nwords. We evaluate 26 widely-used LLMs and find that most models reasonably\nfollow short-length instructions but deteriorate sharply beyond a certain\nthreshold. Surprisingly, almost all models fail to reach the vendor-claimed\nmaximum output lengths in practice, as further confirmed by our evaluations\nextending up to 32K words. Even long-context LLMs, despite their extended\ninput-output windows, counterintuitively fail to improve length-instructions\nfollowing. Notably, Reasoning LLMs outperform even specialized long-text\ngeneration models, achieving state-of-the-art length following. Overall,\nLIFEBench uncovers fundamental limitations in current LLMs' length instructions\nfollowing ability, offering critical insights for future progress.", "AI": {"tldr": "This paper introduces LIFEBench, a benchmark designed to evaluate large language models' (LLMs) ability to follow explicit length instructions. It assesses 26 widely-used LLMs across diverse tasks and languages, revealing significant limitations in meeting length requirements.", "motivation": "To address the gap in evaluating LLMs' performance in following explicit length instructions, as current benchmarks focus mainly on the quality of generated content.", "method": "The study creates the Length Instruction Following Evaluation Benchmark (LIFEBench) consisting of 10,800 instances across 4 task categories in English and Chinese, testing various specified lengths.", "result": "The evaluation indicates that while LLMs perform reasonably well with short-length instructions, their ability declines sharply as the length increases, with most failing to achieve vendor-claimed maximum lengths.", "conclusion": "LIFEBench highlights critical limitations in LLMs' capability to follow length instructions, providing insights for future improvements in model design and evaluation.", "key_contributions": ["Introduction of LIFEBench as a comprehensive benchmark for length instruction following", "Evaluation and analysis of 26 popular LLMs across different lengths", "Discovery of a performance decline in LLMs as instructed lengths increase."], "limitations": "Focuses primarily on length instructions; other performance metrics and aspects may not be covered.", "keywords": ["Large Language Models", "Length Instructions", "Benchmark", "Evaluation", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2505.16237", "pdf": "https://arxiv.org/pdf/2505.16237.pdf", "abs": "https://arxiv.org/abs/2505.16237", "title": "Align-GRAG: Reasoning-Guided Dual Alignment for Graph Retrieval-Augmented Generation", "authors": ["Derong Xu", "Pengyue Jia", "Xiaopeng Li", "Yingyi Zhang", "Maolin Wang", "Qidong Liu", "Xiangyu Zhao", "Yichao Wang", "Huifeng Guo", "Ruiming Tang", "Enhong Chen", "Tong Xu"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, but\nstill struggle with issues like hallucinations and outdated information.\nRetrieval-augmented generation (RAG) addresses these issues by grounding LLM\noutputs in external knowledge with an Information Retrieval (IR) system.\nBuilding on this foundation, graph-based RAG systems go a step further by\nretrieving subgraphs, which preserve the relationships between knowledge\nentities and provide more comprehensive context. However, graph RAG faces two\nchallenges: (1) Retrieving relevant information introduces irrelevant nodes\n(especially in dense graph databases, where retrieval usually extends to\nadjacent nodes), and leads to overly lengthy inputs that hinder efficiency; (2)\nThe representation gap between graph and language during generation with LLMs\nlimits the ability to fully leverage graph structures for enhanced\nunderstanding. To address these limitations, we propose Align-GRAG, a novel\nreasoning-guided dual alignment framework in post-retrieval phrase. It first\nformulates a subgraph by retrieving nodes and edges. Then an Aligner is\nproposed to jointly optimizes a graph encoder with LLM-summarized reasoning. It\nachieves dual alignment of graph node and representation by leveraging KL\ndivergence loss and contrastive loss, facilitating efficient pruning of\nirrelevant knowledge and establishing a unified semantic space. The Generator\nintegrates the aligned graph data with LLM to produce coherent and accurate\nanswers. Experiments on GraphQA benchmark across three tasks (including common\nsense reasoning, scene graph understanding, and knowledge graph reasoning)\nvalidate the effectiveness of our method. The code will be available upon\naccepted.", "AI": {"tldr": "Align-GRAG is a novel dual alignment framework for enhancing graph-based retrieval-augmented generation (RAG) in large language models by optimizing the use of graph structures and improving retrieval efficiency.", "motivation": "Despite advancements in large language models (LLMs), issues such as hallucinations and outdated information persist. Retrieval-augmented generation (RAG) seeks to mitigate these problems by integrating external knowledge.", "method": "The Align-GRAG framework retrieves a subgraph of nodes and edges, utilizes an Aligner for dual alignment optimization of the graph encoder and LLM reasoning, applying KL divergence loss and contrastive loss to prune irrelevant information and create a unified semantic representation.", "result": "Experiments on the GraphQA benchmark showed that Align-GRAG effectively improves performance in tasks related to common sense reasoning, scene graph understanding, and knowledge graph reasoning compared to traditional methods.", "conclusion": "Align-GRAG enhances the integration of graph structures and LLMs for more accurate and contextually relevant answers, addressing key retrieval challenges in graph-based RAG systems.", "key_contributions": ["Proposes Align-GRAG framework for dual alignment in graph-based RAG", "Implements a novel Aligner for optimizing graph encoding with LLM reasoning", "Demonstrates effectiveness on the GraphQA benchmark across multiple tasks"], "limitations": "", "keywords": ["large language models", "retrieval-augmented generation", "graph-based reasoning", "knowledge representation", "Align-GRAG"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.16241", "pdf": "https://arxiv.org/pdf/2505.16241.pdf", "abs": "https://arxiv.org/abs/2505.16241", "title": "Three Minds, One Legend: Jailbreak Large Reasoning Model with Adaptive Stacked Ciphers", "authors": ["Viet-Anh Nguyen", "Shiqian Zhao", "Gia Dao", "Runyi Hu", "Yi Xie", "Luu Anh Tuan"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, Large Reasoning Models (LRMs) have demonstrated superior logical\ncapabilities compared to traditional Large Language Models (LLMs), gaining\nsignificant attention. Despite their impressive performance, the potential for\nstronger reasoning abilities to introduce more severe security vulnerabilities\nremains largely underexplored. Existing jailbreak methods often struggle to\nbalance effectiveness with robustness against adaptive safety mechanisms. In\nthis work, we propose SEAL, a novel jailbreak attack that targets LRMs through\nan adaptive encryption pipeline designed to override their reasoning processes\nand evade potential adaptive alignment. Specifically, SEAL introduces a stacked\nencryption approach that combines multiple ciphers to overwhelm the models\nreasoning capabilities, effectively bypassing built-in safety mechanisms. To\nfurther prevent LRMs from developing countermeasures, we incorporate two\ndynamic strategies - random and adaptive - that adjust the cipher length,\norder, and combination. Extensive experiments on real-world reasoning models,\nincluding DeepSeek-R1, Claude Sonnet, and OpenAI GPT-o4, validate the\neffectiveness of our approach. Notably, SEAL achieves an attack success rate of\n80.8% on GPT o4-mini, outperforming state-of-the-art baselines by a significant\nmargin of 27.2%. Warning: This paper contains examples of inappropriate,\noffensive, and harmful content.", "AI": {"tldr": "SEAL is a novel jailbreak attack for Large Reasoning Models that utilizes a stacked encryption approach to bypass safety mechanisms.", "motivation": "To explore the potential security vulnerabilities introduced by stronger reasoning abilities in Large Reasoning Models and to develop effective jailbreak methods.", "method": "The authors propose SEAL, which uses a stacked encryption pipeline with multiple ciphers and dynamic strategies to overwhelm the reasoning capabilities of LRMs and evade safety mechanisms.", "result": "SEAL shows a high attack success rate of 80.8% on GPT o4-mini, significantly outperforming existing jailbreak methods by 27.2%.", "conclusion": "The proposed approach demonstrates that it is possible to effectively bypass the reasoning safeguards in LRMs using advanced encryption techniques.", "key_contributions": ["Introduction of SEAL, a novel jailbreak attack for LRMs", "Utilization of a stacked encryption approach to target reasoning capabilities", "Development of dynamic strategies to prevent countermeasures from LRMs."], "limitations": "The paper contains examples of inappropriate, offensive, and harmful content; further exploration into the ethical implications may be necessary.", "keywords": ["Large Reasoning Models", "jailbreak attack", "adaptative encryption", "security vulnerabilities", "machine learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.16245", "pdf": "https://arxiv.org/pdf/2505.16245.pdf", "abs": "https://arxiv.org/abs/2505.16245", "title": "Diverse, not Short: A Length-Controlled Self-Learning Framework for Improving Response Diversity of Language Models", "authors": ["Vijeta Deshpande", "Debasmita Ghose", "John D. Patterson", "Roger Beaty", "Anna Rumshisky"], "categories": ["cs.CL"], "comment": null, "summary": "Diverse language model responses are crucial for creative generation,\nopen-ended tasks, and self-improvement training. We show that common diversity\nmetrics, and even reward models used for preference optimization,\nsystematically bias models toward shorter outputs, limiting expressiveness. To\naddress this, we introduce Diverse, not Short (Diverse-NS), a length-controlled\nself-learning framework that improves response diversity while maintaining\nlength parity. By generating and filtering preference data that balances\ndiversity, quality, and length, Diverse-NS enables effective training using\nonly 3,000 preference pairs. Applied to LLaMA-3.1-8B and the Olmo-2 family,\nDiverse-NS substantially enhances lexical and semantic diversity. We show\nconsistent improvement in diversity with minor reduction or gains in response\nquality on four creative generation tasks: Divergent Associations, Persona\nGeneration, Alternate Uses, and Creative Writing. Surprisingly, experiments\nwith the Olmo-2 model family (7B, and 13B) show that smaller models like\nOlmo-2-7B can serve as effective \"diversity teachers\" for larger models. By\nexplicitly addressing length bias, our method efficiently pushes models toward\nmore diverse and expressive outputs.", "AI": {"tldr": "This paper introduces Diverse-NS, a self-learning framework that enhances language model response diversity without sacrificing output length.", "motivation": "To improve the diversity of language model responses which is essential for creative tasks and self-improvement, while addressing the bias towards shorter outputs in current diversity metrics.", "method": "Diverse-NS utilizes a length-controlled self-learning framework to generate and filter preference data that balances diversity, quality, and length, using only 3,000 preference pairs for training.", "result": "The application of Diverse-NS to models like LLaMA-3.1-8B and the Olmo-2 family shows a significant improvement in lexical and semantic diversity across various creative tasks with minimal impact on quality.", "conclusion": "By explicitly addressing length bias, Diverse-NS allows language models to produce more diverse and expressive outputs, even enabling smaller models to teach diversity effectively to larger models.", "key_contributions": ["Introduction of the Diverse-NS framework for length-controlled diversity training.", "Demonstration of improved diversity metrics on various creative tasks.", "Finding that smaller models can effectively teach diversity to larger models."], "limitations": "", "keywords": ["language models", "response diversity", "self-learning", "length control", "creative generation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.16252", "pdf": "https://arxiv.org/pdf/2505.16252.pdf", "abs": "https://arxiv.org/abs/2505.16252", "title": "Does Localization Inform Unlearning? A Rigorous Examination of Local Parameter Attribution for Knowledge Unlearning in Language Models", "authors": ["Hwiyeong Lee", "Uiji Hwang", "Hyelim Lim", "Taeuk Kim"], "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Large language models often retain unintended content, prompting growing\ninterest in knowledge unlearning. Recent approaches emphasize localized\nunlearning, which restricts parameter updates to specific regions in an effort\nto remove target knowledge while preserving unrelated general knowledge.\nHowever, their effectiveness remains uncertain due to the lack of robust and\nthorough evaluation of the trade-off between the competing goals of unlearning.\nIn this paper, we begin by revisiting existing localized unlearning approaches.\nWe then conduct controlled experiments to rigorously evaluate whether local\nparameter updates causally contribute to unlearning. Our findings reveal that\nthe set of parameters that must be modified for effective unlearning is not\nstrictly determined, challenging the core assumption of localized unlearning\nthat parameter locality is inherently indicative of effective knowledge\nremoval.", "AI": {"tldr": "This paper evaluates localized unlearning approaches in large language models, revealing that effective unlearning does not strictly depend on parameter locality.", "motivation": "To address the challenge of unintended content retention in large language models and evaluate the trade-offs in knowledge unlearning.", "method": "The authors revisit existing localized unlearning approaches and conduct controlled experiments to assess the causal relationship between parameter updates and unlearning efficacy.", "result": "The experiments show that the parameters required for effective unlearning are not well-defined, disputing the assumption that parameter locality is key to knowledge removal.", "conclusion": "Effective knowledge unlearning in language models cannot solely rely on modifying local parameters, as the necessary parameters for unlearning are not consistently identifiable.", "key_contributions": ["Thorough evaluation of localized unlearning methods", "Controlled experiments that challenge assumptions about parameter locality in unlearning", "Insights into the complexities of unlearning knowledge in large language models."], "limitations": "Does not address the practical application of findings in specific language models or architectures.", "keywords": ["knowledge unlearning", "large language models", "parameter updates", "localized unlearning", "unlearning evaluation"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.16258", "pdf": "https://arxiv.org/pdf/2505.16258.pdf", "abs": "https://arxiv.org/abs/2505.16258", "title": "IRONIC: Coherence-Aware Reasoning Chains for Multi-Modal Sarcasm Detection", "authors": ["Aashish Anantha Ramakrishnan", "Aadarsh Anantha Ramakrishnan", "Dongwon Lee"], "categories": ["cs.CL", "cs.AI", "cs.CV", "68T50", "I.2.7; I.2.10"], "comment": null, "summary": "Interpreting figurative language such as sarcasm across multi-modal inputs\npresents unique challenges, often requiring task-specific fine-tuning and\nextensive reasoning steps. However, current Chain-of-Thought approaches do not\nefficiently leverage the same cognitive processes that enable humans to\nidentify sarcasm. We present IRONIC, an in-context learning framework that\nleverages Multi-modal Coherence Relations to analyze referential, analogical\nand pragmatic image-text linkages. Our experiments show that IRONIC achieves\nstate-of-the-art performance on zero-shot Multi-modal Sarcasm Detection across\ndifferent baselines. This demonstrates the need for incorporating linguistic\nand cognitive insights into the design of multi-modal reasoning strategies. Our\ncode is available at: https://github.com/aashish2000/IRONIC", "AI": {"tldr": "IRONIC is a multi-modal learning framework designed for detecting sarcasm in image-text pairs, achieving state-of-the-art results in zero-shot settings.", "motivation": "Current methods for detecting sarcasm often lack the cognitive efficiency observed in human understanding, suggesting a need for improved models leveraging human-like reasoning.", "method": "The IRONIC framework is based on Multi-modal Coherence Relations, which analyzes how referential, analogical, and pragmatic links between images and text contribute to sarcasm detection.", "result": "IRONIC outperforms various baselines in zero-shot Multi-modal Sarcasm Detection, showcasing its effectiveness in understanding sarcasm through multi-modal inputs.", "conclusion": "The findings highlight the importance of integrating linguistic and cognitive insights into multi-modal reasoning strategies for better sarcasm detection.", "key_contributions": ["Introduction of the IRONIC framework for sarcasm detection", "Demonstrated state-of-the-art performance in zero-shot scenarios", "Highlighted the significance of cognitive insights in multi-modal analyses"], "limitations": "", "keywords": ["multi-modal", "sarcasm detection", "in-context learning", "linguistic insights", "cognitive processes"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.16270", "pdf": "https://arxiv.org/pdf/2505.16270.pdf", "abs": "https://arxiv.org/abs/2505.16270", "title": "Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning", "authors": ["Jiaru Zou", "Yikun Ban", "Zihao Li", "Yunzhe Qi", "Ruizhong Qiu", "Ling Yang", "Jingrui He"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "33 pages, 7 figures", "summary": "Large language models are typically adapted to downstream tasks through\nsupervised fine-tuning on domain-specific data. While standard fine-tuning\nfocuses on minimizing generation loss to optimize model parameters, we take a\ndeeper step by retaining and leveraging the model's own learning signals,\nanalogous to how human learners reflect on past mistakes to improve future\nperformance. We first introduce the concept of Mistake Log to systematically\ntrack the model's learning behavior and recurring errors throughout\nfine-tuning. Treating the original transformer-based model as the Pilot, we\ncorrespondingly design a Copilot model to refine the Pilot's inference\nperformance via logits rectification. We name the overall Pilot-Copilot\nframework the Transformer Copilot, which introduces (i) a novel Copilot model\ndesign, (ii) a joint training paradigm where the Copilot continuously learns\nfrom the evolving Mistake Log alongside the Pilot, and (iii) a fused inference\nparadigm where the Copilot rectifies the Pilot's logits for enhanced\ngeneration. We provide both theoretical and empirical analyses on our new\nlearning framework. Experiments on 12 benchmarks spanning commonsense,\narithmetic, and recommendation tasks demonstrate that Transformer Copilot\nconsistently improves performance by up to 34.5%, while introducing marginal\ncomputational overhead to Pilot models and exhibiting strong scalability and\ntransferability.", "AI": {"tldr": "Introducing the Transformer Copilot framework for enhancing language model performance through systematic tracking and leveraging of learning signals.", "motivation": "To improve the performance of language models by utilizing a systematic approach to track learning behaviors and reduce errors during fine-tuning.", "method": "The Transformer Copilot framework involves a Piloting model that learns traditionally and a Copilot model designed to rectify inference based on a Mistake Log that tracks errors and learning signals.", "result": "Experiments show that the Transformer Copilot improves model performance by up to 34.5% on 12 diverse benchmarks, with only marginal computational overhead and strong scalability.", "conclusion": "The framework provides a novel approach to refining language model performance, blending traditional learning with innovative error tracking and correction mechanisms.", "key_contributions": ["Introduction of the Mistake Log for tracking learning behaviors", "Design of the Copilot model for logits rectification", "Development of a joint training and fused inference paradigm for enhanced model performance"], "limitations": "", "keywords": ["Transformer models", "Machine learning", "Fine-tuning", "Error tracking", "Copilot"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2505.16277", "pdf": "https://arxiv.org/pdf/2505.16277.pdf", "abs": "https://arxiv.org/abs/2505.16277", "title": "Spontaneous Speech Variables for Evaluating LLMs Cognitive Plausibility", "authors": ["Sheng-Fu Wang", "Laurent Prevot", "Jou-an Chi", "Ri-Sheng Huang", "Shu-Kai Hsieh"], "categories": ["cs.CL"], "comment": "The 14th Workshop on Cognitive Modeling and Computational Linguistics\n  (CMCL). May 3, 2025. Collocated with NAACL 2025", "summary": "The achievements of Large Language Models in Natural Language Processing,\nespecially for high-resource languages, call for a better understanding of\ntheir characteristics from a cognitive perspective. Researchers have attempted\nto evaluate artificial models by testing their ability to predict behavioral\n(e.g., eye-tracking fixations) and physiological (e.g., brain responses)\nvariables during language processing (e.g., reading/listening). In this paper,\nwe propose using spontaneous speech corpora to derive production variables\n(speech reductions, prosodic prominences) and applying them in a similar\nfashion. More precisely, we extract. We then test models trained with a\nstandard procedure on different pretraining datasets (written, spoken, and\nmixed genres) for their ability to predict these two variables. Our results\nshow that, after some fine-tuning, the models can predict these production\nvariables well above baselines. We also observe that spoken genre training data\nprovides more accurate predictions than written genres. These results\ncontribute to the broader effort of using high-quality speech corpora as\nbenchmarks for LLMs.", "AI": {"tldr": "The paper investigates the ability of Large Language Models to predict speech production variables by leveraging spontaneous speech corpora and comparing different pretraining datasets.", "motivation": "Despite the advancements in language models, there's a need to evaluate their cognitive characteristics, particularly in predicting language processing behaviors and responses.", "method": "The study extracts production variables such as speech reductions and prosodic prominences from spontaneous speech corpora, testing models trained on various pretraining datasets to see their predictive accuracy.", "result": "After fine-tuning, the models demonstrated a strong capability to predict speech production variables, with spoken genre datasets yielding better accuracy than written ones.", "conclusion": "The findings highlight the importance of utilizing high-quality speech corpora as benchmarks for evaluating the effectiveness of Large Language Models.", "key_contributions": ["Introduces the use of spontaneous speech corpora for evaluating LLMs", "Shows models can predict speech production variables beyond baseline measures", "Demonstrates that spoken genre data enhances predictive accuracy"], "limitations": "", "keywords": ["Large Language Models", "Cognitive Modeling", "Speech Production", "Pretraining Datasets", "Natural Language Processing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.16281", "pdf": "https://arxiv.org/pdf/2505.16281.pdf", "abs": "https://arxiv.org/abs/2505.16281", "title": "HiMATE: A Hierarchical Multi-Agent Framework for Machine Translation Evaluation", "authors": ["Shijie Zhang", "Renhao Li", "Songsheng Wang", "Philipp Koehn", "Min Yang", "Derek F. Wong"], "categories": ["cs.CL"], "comment": null, "summary": "The advancement of Large Language Models (LLMs) enables flexible and\ninterpretable automatic evaluations. In the field of machine translation\nevaluation, utilizing LLMs with translation error annotations based on\nMultidimensional Quality Metrics (MQM) yields more human-aligned judgments.\nHowever, current LLM-based evaluation methods still face challenges in\naccurately identifying error spans and assessing their severity. In this paper,\nwe propose HiMATE, a Hierarchical Multi-Agent Framework for Machine Translation\nEvaluation. We argue that existing approaches inadequately exploit the\nfine-grained structural and semantic information within the MQM hierarchy. To\naddress this, we develop a hierarchical multi-agent system grounded in the MQM\nerror typology, enabling granular evaluation of subtype errors. Two key\nstrategies are incorporated to further mitigate systemic hallucinations within\nthe framework: the utilization of the model's self-reflection capability and\nthe facilitation of agent discussion involving asymmetric information.\nEmpirically, HiMATE outperforms competitive baselines across different datasets\nin conducting human-aligned evaluations. Further analyses underscore its\nsignificant advantage in error span detection and severity assessment,\nachieving an average F1-score improvement of 89% over the best-performing\nbaseline. We make our code and data publicly available at\nhttps://anonymous.4open.science/r/HiMATE-Anony.", "AI": {"tldr": "HiMATE is a proposed framework that enhances machine translation evaluation using hierarchical multi-agent systems to align more closely with human judgment by leveraging detailed error annotations.", "motivation": "Current LLM-based machine translation evaluation methods struggle with accurately identifying error spans and assessing their severity, highlighting the need for improved evaluation frameworks.", "method": "We developed HiMATE, a Hierarchical Multi-Agent Framework that utilizes the Multidimensional Quality Metrics (MQM) error typology for granular error evaluation, incorporating strategies like self-reflection and agent discussions to minimize hallucinations.", "result": "HiMATE shows superior performance over existing methods, achieving an average F1-score improvement of 89% in error span detection and severity assessment across various datasets.", "conclusion": "The results indicate that HiMATE provides a more human-aligned evaluation of machine translation, effectively addressing past limitations in error detection and severity assessment.", "key_contributions": ["Introduction of a hierarchical multi-agent framework for machine translation evaluation.", "Enhanced error detection and severity assessment through the utilization of MQM error typology.", "Empirically validated improvements in evaluation metrics over competitive baselines."], "limitations": "", "keywords": ["Machine Translation", "Large Language Models", "Evaluation Metrics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.16293", "pdf": "https://arxiv.org/pdf/2505.16293.pdf", "abs": "https://arxiv.org/abs/2505.16293", "title": "Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA", "authors": ["Rishabh Maheshwary", "Masoud Hashemi", "Khyati Mahajan", "Shiva Krishna Reddy Malay", "Sai Rajeswar", "Sathwik Tejaswi Madhusudhan", "Spandana Gella", "Vikas Yadav"], "categories": ["cs.CL"], "comment": null, "summary": "Iterative RAG for multi-hop question answering faces challenges with lengthy\ncontexts and the buildup of irrelevant information. This hinders a model's\ncapacity to process and reason over retrieved content and limits performance.\nWhile recent methods focus on compressing retrieved information, they are\neither restricted to single-round RAG, require finetuning or lack scalability\nin iterative RAG. To address these challenges, we propose Notes Writing, a\nmethod that generates concise and relevant notes from retrieved documents at\neach step, thereby reducing noise and retaining only essential information.\nThis indirectly increases the effective context length of Large Language Models\n(LLMs), enabling them to reason and plan more effectively while processing\nlarger volumes of input text. Notes Writing is framework agnostic and can be\nintegrated with different iterative RAG methods. We demonstrate its\neffectiveness with three iterative RAG methods, across two models and four\nevaluation datasets. Notes writing yields an average improvement of 15.6\npercentage points overall, with minimal increase in output tokens.", "AI": {"tldr": "Notes Writing improves iterative RAG for multi-hop question answering by generating concise notes from retrieved documents, enhancing context processing and reasoning of LLMs.", "motivation": "To overcome challenges of lengthy contexts and irrelevant information in iterative RAG for multi-hop question answering, which hinder model performance.", "method": "The Notes Writing method generates concise and relevant notes from retrieved documents at each step, reducing noise and retaining essential information.", "result": "Notes Writing leads to an average improvement of 15.6 percentage points in performance across three iterative RAG methods, in two models, and on four evaluation datasets, with minimal increase in output tokens.", "conclusion": "This method indirectly increases the effective context length of LLMs, enabling better reasoning and planning.", "key_contributions": ["Introduction of the Notes Writing method", "Framework agnostic integration with different RAG methods", "Significant performance improvement in multi-hop question answering"], "limitations": "", "keywords": ["multi-hop question answering", "iterative RAG", "notes writing", "context processing", "large language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.16297", "pdf": "https://arxiv.org/pdf/2505.16297.pdf", "abs": "https://arxiv.org/abs/2505.16297", "title": "ToDi: Token-wise Distillation via Fine-Grained Divergence Control", "authors": ["Seongryong Jung", "Suwan Yoon", "DongGeon Kim", "Hwanhee Lee"], "categories": ["cs.CL"], "comment": "13 pages, 7 figures", "summary": "Large language models (LLMs) offer impressive performance but are impractical\nfor resource-constrained deployment due to high latency and energy consumption.\nKnowledge distillation (KD) addresses this by transferring knowledge from a\nlarge teacher to a smaller student model. However, conventional KD, notably\napproaches like Forward KL (FKL) and Reverse KL (RKL), apply uniform divergence\nloss across the entire vocabulary, neglecting token-level prediction\ndiscrepancies. By investigating these representative divergences via gradient\nanalysis, we reveal that FKL boosts underestimated tokens, while RKL suppresses\noverestimated ones, showing their complementary roles. Based on this\nobservation, we propose Token-wise Distillation (ToDi), a novel method that\nadaptively combines FKL and RKL per token using a sigmoid-based weighting\nfunction derived from the teacher-student probability log-ratio. ToDi\ndynamically emphasizes the appropriate divergence for each token, enabling\nprecise distribution alignment. We demonstrate that ToDi consistently\noutperforms recent distillation baselines using uniform or less granular\nstrategies across instruction-following benchmarks. Extensive ablation studies\nand efficiency analysis further validate ToDi's effectiveness and practicality.", "AI": {"tldr": "The paper introduces Token-wise Distillation (ToDi), a method that improves knowledge distillation in language models by adaptively combining Forward KL and Reverse KL divergences for better token prediction.", "motivation": "To address the impracticality of large language models in resource-constrained environments due to high latency and energy consumption.", "method": "Token-wise Distillation (ToDi) combines FKL and RKL per token using a sigmoid-based weighting function derived from the teacher-student probability log-ratio.", "result": "ToDi outperforms existing distillation methods on instruction-following benchmarks and shows improved efficiency and effectiveness.", "conclusion": "ToDi represents a significant advancement in knowledge distillation by focusing on token-level discrepancies for better model performance in practical applications.", "key_contributions": ["Introduction of Token-wise Distillation (ToDi) method", "Adaptive combination of Forward KL and Reverse KL divergences", "Consistent outperforming of existing distillation baselines across benchmarks."], "limitations": "", "keywords": ["knowledge distillation", "large language models", "token-wise distillation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.16303", "pdf": "https://arxiv.org/pdf/2505.16303.pdf", "abs": "https://arxiv.org/abs/2505.16303", "title": "INFERENCEDYNAMICS: Efficient Routing Across LLMs through Structured Capability and Knowledge Profiling", "authors": ["Haochen Shi", "Tianshi Zheng", "Weiqi Wang", "Baixuan Xu", "Chunyang Li", "Chunkit Chan", "Tao Fan", "Yangqiu Song", "Qiang Yang"], "categories": ["cs.CL"], "comment": "17 pages", "summary": "Large Language Model (LLM) routing is a pivotal technique for navigating a\ndiverse landscape of LLMs, aiming to select the best-performing LLMs tailored\nto the domains of user queries, while managing computational resources.\nHowever, current routing approaches often face limitations in scalability when\ndealing with a large pool of specialized LLMs, or in their adaptability to\nextending model scope and evolving capability domains. To overcome those\nchallenges, we propose InferenceDynamics, a flexible and scalable\nmulti-dimensional routing framework by modeling the capability and knowledge of\nmodels. We operate it on our comprehensive dataset RouteMix, and demonstrate\nits effectiveness and generalizability in group-level routing using modern\nbenchmarks including MMLU-Pro, GPQA, BigGenBench, and LiveBench, showcasing its\nability to identify and leverage top-performing models for given tasks, leading\nto superior outcomes with efficient resource utilization. The broader adoption\nof Inference Dynamics can empower users to harness the full specialized\npotential of the LLM ecosystem, and our code will be made publicly available to\nencourage further research.", "AI": {"tldr": "The paper introduces InferenceDynamics, a scalable multi-dimensional routing framework for selecting optimal large language models (LLMs) based on user queries and performance in various domains.", "motivation": "Current LLM routing techniques struggle with scalability and adaptability among a large pool of specialized models. This work aims to enhance routing efficiency in selecting models tailored to user queries.", "method": "InferenceDynamics employs a flexible multi-dimensional framework that models the capabilities and knowledge of various LLMs, allowing for efficient routing and selection during inference.", "result": "The framework demonstrates effectiveness and generalizability across multiple benchmarks, achieving superior model selection resulting in improved outcomes and optimized resource use.", "conclusion": "InferenceDynamics provides a solution to the challenges of LLM routing, potentially enhancing overall user performance in utilizing LLMs and encouraging wider adoption through its open-source availability.", "key_contributions": ["Introduction of a flexible and scalable framework for LLM routing", "Validation through extensive benchmarking against existing benchmarks", "Public availability of the code to support further research"], "limitations": "", "keywords": ["Large Language Models", "LLM routing", "InferenceDynamics", "resource utilization", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.16307", "pdf": "https://arxiv.org/pdf/2505.16307.pdf", "abs": "https://arxiv.org/abs/2505.16307", "title": "PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models", "authors": ["Chenzhuo Zhao", "Ziqian Liu", "Xingda Wang", "Junting Lu", "Chaoyi Ruan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Prompt optimization offers a practical and broadly applicable alternative to\nfine-tuning for improving large language model (LLM) performance. However,\nexisting methods often rely on costly output generation, self-critiquing\nabilities, or human-annotated preferences, which limit their scalability,\nespecially for smaller or non-instruction-tuned models. We introduce PMPO\n(Probabilistic Metric Prompt Optimization), a unified framework that refines\nprompts using token-level cross-entropy loss as a direct, lightweight\nevaluation signal. PMPO identifies low-quality prompt segments by masking and\nmeasuring their impact on loss, then rewrites and selects improved variants by\nminimizing loss over positive and negative examples. Unlike prior methods, it\nrequires no output sampling or human evaluation during optimization, relying\nonly on forward passes and log-likelihoods. PMPO supports both supervised and\npreference-based tasks through a closely aligned loss-based evaluation\nstrategy. Experiments show that PMPO consistently outperforms prior methods\nacross model sizes and tasks: it achieves the highest average accuracy on BBH,\nperforms strongly on GSM8K and AQUA-RAT, and improves AlpacaEval 2.0 win rates\nby over 19 points. These results highlight PMPO's effectiveness, efficiency,\nand broad applicability.", "AI": {"tldr": "PMPO is a new framework for prompt optimization that improves LLM performance without costly sampling or human evaluation.", "motivation": "To provide a more scalable, efficient, and practical method for optimizing prompts in LLMs without the limitations of existing methods that rely on expensive output generation and evaluations.", "method": "PMPO uses token-level cross-entropy loss to assess prompt quality, identifying and rewriting low-quality segments by minimizing loss over examples without the need for output sampling or human input.", "result": "PMPO outperforms existing methods, achieving the highest accuracy on BBH and significant performance improvements on other tasks like GSM8K and AQUA-RAT, as well as a 19-point increase in AlpacaEval 2.0 win rates.", "conclusion": "PMPO demonstrates a highly effective and efficient approach to prompt optimization that is applicable across various models and tasks.", "key_contributions": ["Introduces a unified framework for prompt optimization using token-level cross-entropy loss.", "Eliminates the need for output sampling and human evaluation during the optimization process.", "Shows consistent performance improvement across multiple tasks and model sizes."], "limitations": "", "keywords": ["Prompt Optimization", "Large Language Models", "Machine Learning", "Probabilistic Model"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.16325", "pdf": "https://arxiv.org/pdf/2505.16325.pdf", "abs": "https://arxiv.org/abs/2505.16325", "title": "CLEAR: A Clinically-Grounded Tabular Framework for Radiology Report Evaluation", "authors": ["Yuyang Jiang", "Chacha Chen", "Shengyuan Wang", "Feng Li", "Zecong Tang", "Benjamin M. Mervak", "Lydia Chelala", "Christopher M Straus", "Reve Chahine", "Samuel G. Armato III", "Chenhao Tan"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "18 pages, 4 figures", "summary": "Existing metrics often lack the granularity and interpretability to capture\nnuanced clinical differences between candidate and ground-truth radiology\nreports, resulting in suboptimal evaluation. We introduce a Clinically-grounded\ntabular framework with Expert-curated labels and Attribute-level comparison for\nRadiology report evaluation (CLEAR). CLEAR not only examines whether a report\ncan accurately identify the presence or absence of medical conditions, but also\nassesses whether it can precisely describe each positively identified condition\nacross five key attributes: first occurrence, change, severity, descriptive\nlocation, and recommendation. Compared to prior works, CLEAR's\nmulti-dimensional, attribute-level outputs enable a more comprehensive and\nclinically interpretable evaluation of report quality. Additionally, to measure\nthe clinical alignment of CLEAR, we collaborate with five board-certified\nradiologists to develop CLEAR-Bench, a dataset of 100 chest X-ray reports from\nMIMIC-CXR, annotated across 6 curated attributes and 13 CheXpert conditions.\nOur experiments show that CLEAR achieves high accuracy in extracting clinical\nattributes and provides automated metrics that are strongly aligned with\nclinical judgment.", "AI": {"tldr": "CLEAR introduces a multi-dimensional framework for evaluating radiology reports with a focus on attribute-level comparisons and clinical relevance.", "motivation": "Existing metrics are insufficient for effectively evaluating the quality of radiology reports, lacking detailed assessments of nuanced clinical differences.", "method": "CLEAR is a framework that evaluates radiology reports by examining their accuracy in identifying medical conditions and descriptively assessing these conditions across five attributes: first occurrence, change, severity, descriptive location, and recommendation.", "result": "CLEAR provides robust, multi-dimensional outputs that enhance the interpretability and clinical relevance of report evaluations, aligned with the judgment of radiologists.", "conclusion": "The proposed CLEAR framework enhances the evaluation of radiology reports by offering a detailed and clinically-grounded assessment approach.", "key_contributions": ["Development of the CLEAR framework for radiology report evaluation", "Introduction of attribute-level comparisons across five key diagnostic attributes", "Creation of CLEAR-Bench dataset annotated by radiologists for clinical alignment"], "limitations": "", "keywords": ["radiology", "evaluation metrics", "machine learning", "clinical assessment", "attribute-level comparison"], "importance_score": 7, "read_time_minutes": 18}}
{"id": "2505.16330", "pdf": "https://arxiv.org/pdf/2505.16330.pdf", "abs": "https://arxiv.org/abs/2505.16330", "title": "SC4ANM: Identifying Optimal Section Combinations for Automated Novelty Prediction in Academic Papers", "authors": ["Wenqing Wu", "Chengzhi Zhang", "Tong Bao", "Yi Zhao"], "categories": ["cs.CL", "cs.AI", "cs.DL"], "comment": null, "summary": "Novelty is a core component of academic papers, and there are multiple\nperspectives on the assessment of novelty. Existing methods often focus on word\nor entity combinations, which provide limited insights. The content related to\na paper's novelty is typically distributed across different core sections,\ne.g., Introduction, Methodology and Results. Therefore, exploring the optimal\ncombination of sections for evaluating the novelty of a paper is important for\nadvancing automated novelty assessment. In this paper, we utilize different\ncombinations of sections from academic papers as inputs to drive language\nmodels to predict novelty scores. We then analyze the results to determine the\noptimal section combinations for novelty score prediction. We first employ\nnatural language processing techniques to identify the sectional structure of\nacademic papers, categorizing them into introduction, methods, results, and\ndiscussion (IMRaD). Subsequently, we used different combinations of these\nsections (e.g., introduction and methods) as inputs for pretrained language\nmodels (PLMs) and large language models (LLMs), employing novelty scores\nprovided by human expert reviewers as ground truth labels to obtain prediction\nresults. The results indicate that using introduction, results and discussion\nis most appropriate for assessing the novelty of a paper, while the use of the\nentire text does not yield significant results. Furthermore, based on the\nresults of the PLMs and LLMs, the introduction and results appear to be the\nmost important section for the task of novelty score prediction. The code and\ndataset for this paper can be accessed at\nhttps://github.com/njust-winchy/SC4ANM.", "AI": {"tldr": "This paper investigates the optimal combinations of academic paper sections for predicting novelty scores using language models.", "motivation": "To improve automated novelty assessment of academic papers, as existing methods provide limited insights by focusing on word or entity combinations only.", "method": "Natural language processing techniques are used to identify sections (introduction, methods, results, discussion) of academic papers, and various combinations of these sections are tested as inputs for pretrained language models and large language models to predict novelty scores.", "result": "The study finds that using the introduction, results, and discussion sections yields the best novelty score predictions, while the entire text does not produce significant results.", "conclusion": "Optimal novelty score predictions are achieved with specific section combinations, particularly the introduction and results, offering insights for automated assessment methods.", "key_contributions": ["Introduces a method for automated assessment of novelty in academic papers using NLP.", "Identifies optimal combinations of academic sections for predicting novelty scores.", "Provides a dataset and code for others to explore novelty assessment using language models."], "limitations": "The study relies on human expert reviewers for ground truth labels, which may introduce subjective bias.", "keywords": ["academic papers", "novelty assessment", "language models", "natural language processing", "automated evaluation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.16348", "pdf": "https://arxiv.org/pdf/2505.16348.pdf", "abs": "https://arxiv.org/abs/2505.16348", "title": "Embodied Agents Meet Personalization: Exploring Memory Utilization for Personalized Assistance", "authors": ["Taeyoon Kwon", "Dongwook Choi", "Sunghwan Kim", "Hyojun Kim", "Seungjun Moon", "Beong-woo Kwak", "Kuan-Hao Huang", "Jinyoung Yeo"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Embodied agents empowered by large language models (LLMs) have shown strong\nperformance in household object rearrangement tasks. However, these tasks\nprimarily focus on single-turn interactions with simplified instructions, which\ndo not truly reflect the challenges of providing meaningful assistance to\nusers. To provide personalized assistance, embodied agents must understand the\nunique semantics that users assign to the physical world (e.g., favorite cup,\nbreakfast routine) by leveraging prior interaction history to interpret\ndynamic, real-world instructions. Yet, the effectiveness of embodied agents in\nutilizing memory for personalized assistance remains largely underexplored. To\naddress this gap, we present MEMENTO, a personalized embodied agent evaluation\nframework designed to comprehensively assess memory utilization capabilities to\nprovide personalized assistance. Our framework consists of a two-stage memory\nevaluation process design that enables quantifying the impact of memory\nutilization on task performance. This process enables the evaluation of agents'\nunderstanding of personalized knowledge in object rearrangement tasks by\nfocusing on its role in goal interpretation: (1) the ability to identify target\nobjects based on personal meaning (object semantics), and (2) the ability to\ninfer object-location configurations from consistent user patterns, such as\nroutines (user patterns). Our experiments across various LLMs reveal\nsignificant limitations in memory utilization, with even frontier models like\nGPT-4o experiencing a 30.5% performance drop when required to reference\nmultiple memories, particularly in tasks involving user patterns. These\nfindings, along with our detailed analyses and case studies, provide valuable\ninsights for future research in developing more effective personalized embodied\nagents. Project website: https://connoriginal.github.io/MEMENTO", "AI": {"tldr": "This paper presents MEMENTO, a framework for evaluating how embodied agents using LLMs can utilize memory for personalized assistance in object rearrangement tasks.", "motivation": "To explore the limitations of embodied agents in understanding user-specific semantics and memory utilization in real-world interactions for personalized assistance.", "method": "The MEMENTO framework consists of a two-stage memory evaluation process that quantifies the impact of memory on task performance, focusing on object semantics and user patterns.", "result": "Experiments show significant memory utilization limitations, with models like GPT-4o experiencing a 30.5% performance drop in tasks involving multiple memories and user patterns.", "conclusion": "The findings highlight the need for improved memory capabilities in embodied agents for better personalization, providing insights for future research.", "key_contributions": ["Introduction of the MEMENTO framework for personalized assistance evaluation", "Identification of performance drop due to memory limitations in LLMs", "Insights on object semantics and user patterns in embodied agents"], "limitations": "The study is a work in progress and the findings may evolve with further research.", "keywords": ["Embodied agents", "Large language models", "Personalized assistance", "Memory utilization", "Human-computer interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.16349", "pdf": "https://arxiv.org/pdf/2505.16349.pdf", "abs": "https://arxiv.org/abs/2505.16349", "title": "Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization", "authors": ["Pierre Achkar", "Tim Gollub", "Martin Potthast"], "categories": ["cs.CL"], "comment": "Accepted at SCOLIA@ECIR 2025 Workshop", "summary": "The exponential growth of scientific publications has made it increasingly\ndifficult for researchers to stay updated and synthesize knowledge effectively.\nThis paper presents XSum, a modular pipeline for multi-document summarization\n(MDS) in the scientific domain using Retrieval-Augmented Generation (RAG). The\npipeline includes two core components: a question-generation module and an\neditor module. The question-generation module dynamically generates questions\nadapted to the input papers, ensuring the retrieval of relevant and accurate\ninformation. The editor module synthesizes the retrieved content into coherent\nand well-structured summaries that adhere to academic standards for proper\ncitation. Evaluated on the SurveySum dataset, XSum demonstrates strong\nperformance, achieving considerable improvements in metrics such as CheckEval,\nG-Eval and Ref-F1 compared to existing approaches. This work provides a\ntransparent, adaptable framework for scientific summarization with potential\napplications in a wide range of domains. Code available at\nhttps://github.com/webis-de/scolia25-xsum", "AI": {"tldr": "XSum is a modular pipeline for multi-document summarization in the scientific domain using Retrieval-Augmented Generation, featuring a dynamic question-generation module and an editor for creating coherent summaries.", "motivation": "To address the challenge of keeping up with the exponential growth of scientific literature and to improve the synthesis of knowledge through summarization.", "method": "XSum consists of a question-generation module that generates context-specific questions for retrieving relevant information, followed by an editor module that synthesizes this information into concise summaries adhering to academic standards.", "result": "XSum achieved significant improvements in summarization metrics like CheckEval, G-Eval, and Ref-F1 when evaluated on the SurveySum dataset, demonstrating robust performance over existing methods.", "conclusion": "XSum offers an adaptable and transparent framework for scientific summarization with broad potential applications.", "key_contributions": ["Presentation of a novel modular pipeline for multi-document summarization in the scientific domain using RAG.", "Development of a question-generation module for targeted information retrieval.", "Creation of an editor module that formats and synthesizes information into structured summaries."], "limitations": "", "keywords": ["multi-document summarization", "Retrieval-Augmented Generation", "scientific literature", "summarization metrics", "knowledge synthesis"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2505.16381", "pdf": "https://arxiv.org/pdf/2505.16381.pdf", "abs": "https://arxiv.org/abs/2505.16381", "title": "PaTH Attention: Position Encoding via Accumulating Householder Transformations", "authors": ["Songlin Yang", "Yikang Shen", "Kaiyue Wen", "Shawn Tan", "Mayank Mishra", "Liliang Ren", "Rameswar Panda", "Yoon Kim"], "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "The attention mechanism is a core primitive in modern large language models\n(LLMs) and AI more broadly. Since attention by itself is permutation-invariant,\nposition encoding is essential for modeling structured domains such as\nlanguage. Rotary position encoding (RoPE) has emerged as the de facto standard\napproach for position encoding and is part of many modern LLMs. However, in\nRoPE the key/query transformation between two elements in a sequence is only a\nfunction of their relative position and otherwise independent of the actual\ninput. This limits the expressivity of RoPE-based transformers.\n  This paper describes PaTH, a flexible data-dependent position encoding scheme\nbased on accumulated products of Householder(like) transformations, where each\ntransformation is data-dependent, i.e., a function of the input. We derive an\nefficient parallel algorithm for training through exploiting a compact\nrepresentation of products of Householder matrices, and implement a\nFlashAttention-style blockwise algorithm that minimizes I/O cost. Across both\ntargeted synthetic benchmarks and moderate-scale real-world language modeling\nexperiments, we find that PaTH demonstrates superior performance compared to\nRoPE and other recent baselines.", "AI": {"tldr": "PaTH is a new position encoding scheme for LLMs that improves upon rotary position encoding (RoPE) by being data-dependent, resulting in better performance in language modeling tasks.", "motivation": "To enhance the expressivity of transformers by developing a position encoding method that incorporates the actual input rather than relying solely on relative positions.", "method": "Development of PaTH, a position encoding scheme using accumulated products of data-dependent Householder transformations, with an efficient parallel training algorithm and a FlashAttention-style implementation for reduced I/O costs.", "result": "PaTH outperforms RoPE and other recent baselines in both synthetic benchmarks and real-world language modeling experiments.", "conclusion": "PaTH provides a promising alternative to existing position encoding methods, potentially leading to more robust and expressive LLMs.", "key_contributions": ["Introduction of PaTH as a data-dependent position encoding scheme", "Development of an efficient training algorithm utilizing Householder transformations", "Demonstration of superior performance compared to RoPE and other baselines."], "limitations": "", "keywords": ["attention mechanism", "position encoding", "large language models", "Householder transformations", "FlashAttention"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.16385", "pdf": "https://arxiv.org/pdf/2505.16385.pdf", "abs": "https://arxiv.org/abs/2505.16385", "title": "Semantic Pivots Enable Cross-Lingual Transfer in Large Language Models", "authors": ["Kaiyu He", "Tong Zhou", "Yubo Chen", "Delai Qiu", "Shengping Liu", "Kang Liu", "Jun Zhao"], "categories": ["cs.CL"], "comment": "14 pages, 10 figures", "summary": "Large language models (LLMs) demonstrate remarkable ability in cross-lingual\ntasks. Understanding how LLMs acquire this ability is crucial for their\ninterpretability. To quantify the cross-lingual ability of LLMs accurately, we\npropose a Word-Level Cross-Lingual Translation Task. To find how LLMs learn\ncross-lingual ability, we trace the outputs of LLMs' intermediate layers in the\nword translation task. We identify and distinguish two distinct behaviors in\nthe forward pass of LLMs: co-occurrence behavior and semantic pivot behavior.\nWe attribute LLMs' two distinct behaviors to the co-occurrence frequency of\nwords and find the semantic pivot from the pre-training dataset. Finally, to\napply our findings to improve the cross-lingual ability of LLMs, we reconstruct\na semantic pivot-aware pre-training dataset using documents with a high\nproportion of semantic pivots. Our experiments validate the effectiveness of\nour approach in enhancing cross-lingual ability. Our research contributes\ninsights into the interpretability of LLMs and offers a method for improving\nLLMs' cross-lingual ability.", "AI": {"tldr": "The paper proposes a method to enhance the cross-lingual capabilities of large language models (LLMs) by analyzing their intermediate layer outputs to identify co-occurrence and semantic pivot behaviors.", "motivation": "Understanding how LLMs acquire cross-lingual abilities is essential for their interpretability and practical applications in language translation tasks.", "method": "The authors trace the outputs of LLMs' intermediate layers during a word translation task to identify distinct behaviors linked to co-occurrence frequency and semantic pivots. They then create a semantic pivot-aware pre-training dataset to improve LLMs' cross-lingual abilities.", "result": "The experiments demonstrate that the proposed method significantly enhances the cross-lingual ability of LLMs by utilizing a dataset enriched with semantic pivots.", "conclusion": "This research offers valuable insights into LLM interpretability and provides a practical approach for improving their cross-lingual translation performance.", "key_contributions": ["Introduction of a Word-Level Cross-Lingual Translation Task", "Identification of co-occurrence and semantic pivot behaviors in LLMs", "Development of a semantic pivot-aware pre-training dataset"], "limitations": "The study primarily focuses on specific behaviors of LLMs and may not address all aspects of cross-lingual capabilities or generalization across all languages.", "keywords": ["cross-lingual", "large language models", "LLMs", "semantic pivots", "interpretability"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2505.16392", "pdf": "https://arxiv.org/pdf/2505.16392.pdf", "abs": "https://arxiv.org/abs/2505.16392", "title": "Resource for Error Analysis in Text Simplification: New Taxonomy and Test Collection", "authors": ["Benjamin Vendeville", "Liana Ermakova", "Pierre De Loor"], "categories": ["cs.CL", "cs.AI", "I.2.6; I.5.2"], "comment": "Accepted at SIGIR 2025", "summary": "The general public often encounters complex texts but does not have the time\nor expertise to fully understand them, leading to the spread of misinformation.\nAutomatic Text Simplification (ATS) helps make information more accessible, but\nits evaluation methods have not kept up with advances in text generation,\nespecially with Large Language Models (LLMs). In particular, recent studies\nhave shown that current ATS metrics do not correlate with the presence of\nerrors. Manual inspections have further revealed a variety of errors,\nunderscoring the need for a more nuanced evaluation framework, which is\ncurrently lacking. This resource paper addresses this gap by introducing a test\ncollection for detecting and classifying errors in simplified texts. First, we\npropose a taxonomy of errors, with a formal focus on information distortion.\nNext, we introduce a parallel dataset of automatically simplified scientific\ntexts. This dataset has been human-annotated with labels based on our proposed\ntaxonomy. Finally, we analyze the quality of the dataset, and we study the\nperformance of existing models to detect and classify errors from that\ntaxonomy. These contributions give researchers the tools to better evaluate\nerrors in ATS, develop more reliable models, and ultimately improve the quality\nof automatically simplified texts.", "AI": {"tldr": "This paper introduces a new framework for evaluating Automatic Text Simplification (ATS) by proposing a taxonomy of errors and a parallel dataset of simplified texts with human annotations.", "motivation": "The need for effective evaluation methods for Automatic Text Simplification (ATS) due to the inadequacy of current metrics, especially in light of advancements in Large Language Models (LLMs).", "method": "The authors propose a taxonomy of errors related to information distortion in simplified texts and create a parallel dataset of automatically simplified scientific texts, which is human-annotated based on the proposed taxonomy.", "result": "The study evaluates the quality of the dataset and assesses the performance of existing models in detecting and classifying errors according to the taxonomy.", "conclusion": "The framework and dataset provide valuable resources for researchers to enhance the evaluation of ATS and improve the quality of simplified texts.", "key_contributions": ["Introduction of a new error taxonomy for ATS evaluation.", "Development of a human-annotated dataset of automatically simplified scientific texts.", "Analysis of current models' performance in detecting errors based on the proposed taxonomy."], "limitations": "", "keywords": ["Automatic Text Simplification", "error taxonomy", "Large Language Models", "dataset", "evaluation methods"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.16406", "pdf": "https://arxiv.org/pdf/2505.16406.pdf", "abs": "https://arxiv.org/abs/2505.16406", "title": "On the reliability of feature attribution methods for speech classification", "authors": ["Gaofei Shen", "Hosein Mohebbi", "Arianna Bisazza", "Afra Alishahi", "Grzegorz Chrupała"], "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "As the capabilities of large-scale pre-trained models evolve, understanding\nthe determinants of their outputs becomes more important. Feature attribution\naims to reveal which parts of the input elements contribute the most to model\noutputs. In speech processing, the unique characteristics of the input signal\nmake the application of feature attribution methods challenging. We study how\nfactors such as input type and aggregation and perturbation timespan impact the\nreliability of standard feature attribution methods, and how these factors\ninteract with characteristics of each classification task. We find that\nstandard approaches to feature attribution are generally unreliable when\napplied to the speech domain, with the exception of word-aligned perturbation\nmethods when applied to word-based classification tasks.", "AI": {"tldr": "This paper investigates the reliability of feature attribution methods in speech processing, finding that standard approaches are generally unreliable except for word-aligned perturbation methods in word-based classification tasks.", "motivation": "With the evolution of large-scale pre-trained models, understanding the inputs that influence model outputs is critical, especially in the context of speech processing.", "method": "The study examines how input type, aggregation, and perturbation timespan affect the effectiveness of feature attribution methods in the speech domain.", "result": "It was found that standard feature attribution methods are often unreliable for speech tasks, with word-aligned perturbation methods being an exception for word-based classifications.", "conclusion": "The research highlights the importance of choosing appropriate feature attribution methods for speech-related tasks, given the unique characteristics of the input signals.", "key_contributions": ["Analysis of input types and their effects on feature attribution reliability", "Identification of specific scenarios where standard methods fail", "Highlighting word-aligned perturbation methods as reliable for specific tasks"], "limitations": "Focuses primarily on word-based classification tasks, which may not generalize to other speech-related tasks.", "keywords": ["feature attribution", "speech processing", "model interpretation"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.16408", "pdf": "https://arxiv.org/pdf/2505.16408.pdf", "abs": "https://arxiv.org/abs/2505.16408", "title": "From Surveys to Narratives: Rethinking Cultural Value Adaptation in LLMs", "authors": ["Muhammad Farid Adilazuarda", "Chen Cecilia Liu", "Iryna Gurevych", "Alham Fikri Aji"], "categories": ["cs.CL"], "comment": null, "summary": "Adapting cultural values in Large Language Models (LLMs) presents significant\nchallenges, particularly due to biases and limited training data. Prior work\nprimarily aligns LLMs with different cultural values using World Values Survey\n(WVS) data. However, it remains unclear whether this approach effectively\ncaptures cultural nuances or produces distinct cultural representations for\nvarious downstream tasks. In this paper, we systematically investigate\nWVS-based training for cultural value adaptation and find that relying solely\non survey data can homogenize cultural norms and interfere with factual\nknowledge. To investigate these issues, we augment WVS with encyclopedic and\nscenario-based cultural narratives from Wikipedia and NormAd. While these\nnarratives may have variable effects on downstream tasks, they consistently\nimprove cultural distinctiveness than survey data alone. Our work highlights\nthe inherent complexity of aligning cultural values with the goal of guiding\ntask-specific behavior.", "AI": {"tldr": "The paper investigates the adaptation of cultural values in Large Language Models (LLMs) using World Values Survey data and proposes augmenting it with additional cultural narratives to improve performance.", "motivation": "There are significant challenges in adapting cultural values in LLMs, mainly due to biases and the limitations of training data, particularly concerning the World Values Survey.", "method": "The study systematically investigates WVS-based training for cultural value adaptation and augments it with encyclopedic and scenario-based cultural narratives from Wikipedia and NormAd.", "result": "The findings indicate that using survey data alone can lead to homogenized cultural norms and interfere with factual knowledge, while the additional narratives improve cultural distinctiveness in downstream tasks.", "conclusion": "The work underscores the complexity of aligning cultural values in LLMs to better guide task-specific behavior.", "key_contributions": ["Systematic investigation of WVS-based training for cultural adaptation in LLMs", "Introduction of encyclopedic and scenario-based narratives to enhance cultural representations", "Empirical analysis of the impact of narrative augmentation on task performance"], "limitations": "Variable effects of cultural narratives on different downstream tasks.", "keywords": ["Cultural Values", "Large Language Models", "World Values Survey", "Cultural Adaptation", "Encyclopedic Narratives"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.16410", "pdf": "https://arxiv.org/pdf/2505.16410.pdf", "abs": "https://arxiv.org/abs/2505.16410", "title": "Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning", "authors": ["Guanting Dong", "Yifei Chen", "Xiaoxi Li", "Jiajie Jin", "Hongjin Qian", "Yutao Zhu", "Hangyu Mao", "Guorui Zhou", "Zhicheng Dou", "Ji-Rong Wen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Working in progress", "summary": "Recently, large language models (LLMs) have shown remarkable reasoning\ncapabilities via large-scale reinforcement learning (RL). However, leveraging\nthe RL algorithm to empower effective multi-tool collaborative reasoning in\nLLMs remains an open challenge. In this paper, we introduce Tool-Star, an\nRL-based framework designed to empower LLMs to autonomously invoke multiple\nexternal tools during stepwise reasoning. Tool-Star integrates six types of\ntools and incorporates systematic designs in both data synthesis and training.\nTo address the scarcity of tool-use data, we propose a general tool-integrated\nreasoning data synthesis pipeline, which combines tool-integrated prompting\nwith hint-based sampling to automatically and scalably generate tool-use\ntrajectories. A subsequent quality normalization and difficulty-aware\nclassification process filters out low-quality samples and organizes the\ndataset from easy to hard. Furthermore, we propose a two-stage training\nframework to enhance multi-tool collaborative reasoning by: (1) cold-start\nfine-tuning, which guides LLMs to explore reasoning patterns via\ntool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with\nhierarchical reward design, which reinforces reward understanding and promotes\neffective tool collaboration. Experimental analyses on over 10 challenging\nreasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.\nThe code is available at https://github.com/dongguanting/Tool-Star.", "AI": {"tldr": "Tool-Star is an RL-based framework enabling LLMs to autonomously use multiple external tools for collaborative reasoning, enhancing their performance on complex reasoning tasks.", "motivation": "The increasing reasoning capabilities of LLMs highlight the need for effective multi-tool collaboration in complex problem-solving scenarios.", "method": "Tool-Star utilizes a two-stage training framework: it fine-tunes LLMs via tool-invocation feedback and employs a self-critic RL algorithm to improve collaborative reasoning.", "result": "Experimental results on over 10 reasoning benchmarks demonstrate that Tool-Star significantly enhances the effectiveness and efficiency of LLMs in multi-tool collaborative reasoning.", "conclusion": "Tool-Star presents a systematic approach for LLMs to improve their reasoning ability through better tool integration and training methodologies.", "key_contributions": ["Introduction of Tool-Star framework for multi-tool reasoning in LLMs.", "Development of a general tool-integrated reasoning data synthesis pipeline.", "Implementation of a two-stage training approach to enhance tool collaboration."], "limitations": "The framework is still a work in progress and initial analyses may not cover all possible reasoning scenarios.", "keywords": ["large language models", "reinforcement learning", "tool collaboration", "reasoning frameworks", "data synthesis"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.16415", "pdf": "https://arxiv.org/pdf/2505.16415.pdf", "abs": "https://arxiv.org/abs/2505.16415", "title": "Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation", "authors": ["Ruizhe Li", "Chen Chen", "Yuchen Hu", "Yanjun Gao", "Xi Wang", "Emine Yilmaz"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in process", "summary": "Retrieval-Augmented Generation (RAG) leverages large language models (LLMs)\ncombined with external contexts to enhance the accuracy and reliability of\ngenerated responses. However, reliably attributing generated content to\nspecific context segments, context attribution, remains challenging due to the\ncomputationally intensive nature of current methods, which often require\nextensive fine-tuning or human annotation. In this work, we introduce a novel\nJensen-Shannon Divergence driven method to Attribute Response to Context\n(ARC-JSD), enabling efficient and accurate identification of essential context\nsentences without additional fine-tuning or surrogate modelling. Evaluations on\na wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using\ninstruction-tuned LLMs in different scales demonstrate superior accuracy and\nsignificant computational efficiency improvements compared to the previous\nsurrogate-based method. Furthermore, our mechanistic analysis reveals specific\nattention heads and multilayer perceptron (MLP) layers responsible for context\nattribution, providing valuable insights into the internal workings of RAG\nmodels.", "AI": {"tldr": "This paper presents ARC-JSD, a novel method for efficient and accurate context attribution in Retrieval-Augmented Generation (RAG) without requiring fine-tuning. It shows superior performance on benchmark tasks with insights into model operations.", "motivation": "The motivation is to improve context attribution in RAG, which is currently computationally intensive and requires fine-tuning or human annotation.", "method": "The authors introduce a Jensen-Shannon Divergence driven method to Attribute Response to Context (ARC-JSD) that identifies essential context sentences efficiently.", "result": "Evaluations demonstrate that ARC-JSD achieves superior accuracy and significant computational efficiency improvements compared to previous methods on various RAG benchmarks.", "conclusion": "The findings emphasize the potential of ARC-JSD to enhance RAG systems while providing insights into the mechanisms of context attribution.", "key_contributions": ["Introduction of a novel ARC-JSD method for context attribution", "Demonstrated improvements in both accuracy and computational efficiency on RAG benchmarks", "Insights into attention heads and MLP layers related to context attribution"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Context Attribution", "Large Language Models", "Jensen-Shannon Divergence"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.16418", "pdf": "https://arxiv.org/pdf/2505.16418.pdf", "abs": "https://arxiv.org/abs/2505.16418", "title": "Exploring the Relationship Between Diversity and Quality in Ad Text Generation", "authors": ["Yoichi Aoki", "Soichiro Murakami", "Ukyo Honda", "Akihiko Kato"], "categories": ["cs.CL"], "comment": null, "summary": "In natural language generation for advertising, creating diverse and engaging\nad texts is crucial for capturing a broad audience and avoiding advertising\nfatigue. Regardless of the importance of diversity, the impact of the\ndiversity-enhancing methods in ad text generation -- mainly tested on tasks\nsuch as summarization and machine translation -- has not been thoroughly\nexplored. Ad text generation significantly differs from these tasks owing to\nthe text style and requirements. This research explores the relationship\nbetween diversity and ad quality in ad text generation by considering multiple\nfactors, such as diversity-enhancing methods, their hyperparameters,\ninput-output formats, and the models.", "AI": {"tldr": "This paper investigates the effect of diversity-enhancing methods on the quality of generated ad texts.", "motivation": "The motivation behind this research is to improve advertising effectiveness through diverse and engaging ad text generation, which is vital for attracting a wide audience.", "method": "The study examines the relationship between diversity and ad quality in ad text generation, focusing on various diversity-enhancing methods, their hyperparameters, input-output formats, and model performance.", "result": "The findings reveal that diversity-enhancing methods significantly influence the quality of the ad texts produced, indicating that greater diversity can lead to better engagement.", "conclusion": "Enhancing diversity in ad text generation is essential to mitigate ad fatigue and improve overall advertisement effectiveness.", "key_contributions": ["Identifies the unique challenges of ad text generation compared to summarization and machine translation", "Explores various diversity-enhancing methods and their hyperparameters", "Provides empirical evidence of the relationship between diversity and ad quality"], "limitations": "The study may be limited by the specific models and methods evaluated, which may not generalize to all ad text generation contexts.", "keywords": ["Ad Text Generation", "Diversity", "Quality Assessment", "Machine Learning", "Natural Language Generation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.16421", "pdf": "https://arxiv.org/pdf/2505.16421.pdf", "abs": "https://arxiv.org/abs/2505.16421", "title": "WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning", "authors": ["Zhepei Wei", "Wenlin Yao", "Yao Liu", "Weizhi Zhang", "Qin Lu", "Liang Qiu", "Changlong Yu", "Puyang Xu", "Chao Zhang", "Bing Yin", "Hyokun Yun", "Lihong Li"], "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "While reinforcement learning (RL) has demonstrated remarkable success in\nenhancing large language models (LLMs), it has primarily focused on single-turn\ntasks such as solving math problems. Training effective web agents for\nmulti-turn interactions remains challenging due to the complexity of\nlong-horizon decision-making across dynamic web interfaces. In this work, we\npresent WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework\nfor training web agents. It learns directly from online interactions with web\nenvironments by asynchronously generating diverse trajectories, entirely guided\nby binary rewards depending on task success. Experiments on the WebArena-Lite\nbenchmark demonstrate the effectiveness of WebAgent-R1, boosting the task\nsuccess rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to\n44.8%, significantly outperforming existing state-of-the-art methods and strong\nproprietary models such as OpenAI o3. In-depth analyses reveal the\neffectiveness of the thinking-based prompting strategy and test-time scaling\nthrough increased interactions for web tasks. We further investigate different\nRL initialization policies by introducing two variants, namely WebAgent-R1-Zero\nand WebAgent-R1-CoT, which highlight the importance of the warm-up training\nstage (i.e., behavior cloning) and provide insights on incorporating long\nchain-of-thought (CoT) reasoning in web agents.", "AI": {"tldr": "WebAgent-R1 is a multi-turn reinforcement learning framework for training web agents, enhancing task success rates in dynamic web interfaces.", "motivation": "To address the challenges of training effective web agents for multi-turn interactions in dynamic web interfaces using reinforcement learning, which has been primarily focused on single-turn tasks.", "method": "WebAgent-R1 uses an end-to-end framework that learns from online interactions by generating diverse trajectories and is guided by binary rewards based on task success.", "result": "WebAgent-R1 improves the task success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to 44.8%, outperforming existing methods and proprietary models like OpenAI o3.", "conclusion": "The results indicate that the WebAgent-R1 framework is a significant advancement in training multi-turn RL web agents, demonstrating the potential of thinking-based prompting and the effectiveness of different RL initialization policies.", "key_contributions": ["Introduction of the WebAgent-R1 framework for multi-turn web agent training.", "Demonstrated substantial improvements in task success rates over existing models.", "Exploration of RL initialization policies enhances understanding of behavior cloning and CoT reasoning."], "limitations": "", "keywords": ["reinforcement learning", "large language models", "web agents", "multi-turn interactions", "task success rate"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.16425", "pdf": "https://arxiv.org/pdf/2505.16425.pdf", "abs": "https://arxiv.org/abs/2505.16425", "title": "$I^2G$: Generating Instructional Illustrations via Text-Conditioned Diffusion", "authors": ["Jing Bi", "Pinxin Liu", "Ali Vosoughi", "Jiarui Wu", "Jinxi He", "Chenliang Xu"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 5 figures, under review", "summary": "The effective communication of procedural knowledge remains a significant\nchallenge in natural language processing (NLP), as purely textual instructions\noften fail to convey complex physical actions and spatial relationships. We\naddress this limitation by proposing a language-driven framework that\ntranslates procedural text into coherent visual instructions. Our approach\nmodels the linguistic structure of instructional content by decomposing it into\ngoal statements and sequential steps, then conditioning visual generation on\nthese linguistic elements. We introduce three key innovations: (1) a\nconstituency parser-based text encoding mechanism that preserves semantic\ncompleteness even with lengthy instructions, (2) a pairwise discourse coherence\nmodel that maintains consistency across instruction sequences, and (3) a novel\nevaluation protocol specifically designed for procedural language-to-image\nalignment. Our experiments across three instructional datasets (HTStep,\nCaptainCook4D, and WikiAll) demonstrate that our method significantly\noutperforms existing baselines in generating visuals that accurately reflect\nthe linguistic content and sequential nature of instructions. This work\ncontributes to the growing body of research on grounding procedural language in\nvisual content, with applications spanning education, task guidance, and\nmultimodal language understanding.", "AI": {"tldr": "This paper presents a framework for converting procedural text into visual instructions, addressing challenges in effectively communicating complex actions and spatial relationships in NLP.", "motivation": "The effective communication of procedural knowledge in NLP is challenging due to limitations in conveying complex physical actions with text alone.", "method": "The framework translates procedural text into visual instructions by analyzing the linguistic structure and decomposing it into goal statements and sequential steps, using a constituency parser-based encoding and a discourse coherence model.", "result": "Experiments on three datasets (HTStep, CaptainCook4D, and WikiAll) show significant improvements over existing methods in visually representing linguistic instructions.", "conclusion": "The paper contributes to research on grounding procedural language in visual content, with potential applications in education, task guidance, and multimodal language understanding.", "key_contributions": ["Constituency parser-based text encoding that maintains semantic completeness", "Pairwise discourse coherence model for instructional consistency", "Novel evaluation protocol for procedural language-to-image alignment"], "limitations": "", "keywords": ["natural language processing", "visual instructions", "procedural knowledge", "multimodal understanding"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.16429", "pdf": "https://arxiv.org/pdf/2505.16429.pdf", "abs": "https://arxiv.org/abs/2505.16429", "title": "Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems", "authors": ["Song Jin", "Juntian Zhang", "Yuhan Liu", "Xun Zhang", "Yufei Zhang", "Guojun Yin", "Fei Jiang", "Wei Lin", "Rui Yan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating and iterating upon recommender systems is crucial, yet traditional\nA/B testing is resource-intensive, and offline methods struggle with dynamic\nuser-platform interactions. While agent-based simulation is promising, existing\nplatforms often lack a mechanism for user actions to dynamically reshape the\nenvironment. To bridge this gap, we introduce RecInter, a novel agent-based\nsimulation platform for recommender systems featuring a robust interaction\nmechanism. In RecInter platform, simulated user actions (e.g., likes, reviews,\npurchases) dynamically update item attributes in real-time, and introduced\nMerchant Agents can reply, fostering a more realistic and evolving ecosystem.\nHigh-fidelity simulation is ensured through Multidimensional User Profiling\nmodule, Advanced Agent Architecture, and LLM fine-tuned on Chain-of-Thought\n(CoT) enriched interaction data. Our platform achieves significantly improved\nsimulation credibility and successfully replicates emergent phenomena like\nBrand Loyalty and the Matthew Effect. Experiments demonstrate that this\ninteraction mechanism is pivotal for simulating realistic system evolution,\nestablishing our platform as a credible testbed for recommender systems\nresearch.", "AI": {"tldr": "Introducing RecInter, an agent-based simulation platform that enhances recommender system testing by allowing dynamic user interactions to reshape the simulation environment.", "motivation": "Traditional A/B testing is resource-intensive and offline methods are inadequate for dynamic interactions in recommender systems; thus, a new simulation platform is needed.", "method": "RecInter utilizes agent-based simulation with a dynamic interaction mechanism where simulated user actions update item attributes in real-time and involve Merchant Agents for realistic ecosystem responses.", "result": "The platform improves the credibility of simulations, replicating phenomena such as Brand Loyalty and the Matthew Effect through realistic evolution of the system.", "conclusion": "RecInter serves as a credible testbed for research in recommender systems, significantly enhancing simulation fidelity and user interaction dynamics.", "key_contributions": ["Introduction of RecInter, a dynamic agent-based simulation platform.", "Real-time updates of item attributes based on user interactions.", "Ability to replicate emergent phenomena in recommender system behavior."], "limitations": "", "keywords": ["recommender systems", "agent-based simulation", "user interaction", "dynamic system"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.16460", "pdf": "https://arxiv.org/pdf/2505.16460.pdf", "abs": "https://arxiv.org/abs/2505.16460", "title": "University of Indonesia at SemEval-2025 Task 11: Evaluating State-of-the-Art Encoders for Multi-Label Emotion Detection", "authors": ["Ikhlasul Akmal Hanif", "Eryawan Presma Yulianrifat", "Jaycent Gunawan Ongris", "Eduardus Tjitrahardja", "Muhammad Falensi Azmi", "Rahmat Bryan Naufal", "Alfan Farizki Wicaksono"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "16 pages, 13 tables, 1 figures", "summary": "This paper presents our approach for SemEval 2025 Task 11 Track A, focusing\non multilabel emotion classification across 28 languages. We explore two main\nstrategies: fully fine-tuning transformer models and classifier-only training,\nevaluating different settings such as fine-tuning strategies, model\narchitectures, loss functions, encoders, and classifiers. Our findings suggest\nthat training a classifier on top of prompt-based encoders such as mE5 and BGE\nyields significantly better results than fully fine-tuning XLMR and mBERT. Our\nbest-performing model on the final leaderboard is an ensemble combining\nmultiple BGE models, where CatBoost serves as the classifier, with different\nconfigurations. This ensemble achieves an average F1-macro score of 56.58\nacross all languages.", "AI": {"tldr": "This paper examines multilabel emotion classification across 28 languages using various strategies and models, achieving a top F1-macro score with an ensemble approach.", "motivation": "To improve multilabel emotion classification performance across multiple languages by exploring different training strategies and model architectures.", "method": "The study evaluates fully fine-tuning transformer models versus classifier-only training, testing various fine-tuning strategies, model architectures, loss functions, encoders, and classifiers.", "result": "The best-performing model is an ensemble of multiple BGE models with CatBoost as the classifier, achieving an average F1-macro score of 56.58 across all languages.", "conclusion": "The findings indicate that using prompt-based encoders and a classifier-only approach can yield significantly better results compared to fully fine-tuning traditional transformer models.", "key_contributions": ["Introduction of a novel ensemble technique for multilabel emotion classification", "Demonstration of superior performance with prompt-based encoders", "Comprehensive evaluation of multiple strategies in multilingual settings"], "limitations": "", "keywords": ["emotion classification", "multilabel", "transformer models", "multilingual", "machine learning"], "importance_score": 6, "read_time_minutes": 16}}
{"id": "2505.16467", "pdf": "https://arxiv.org/pdf/2505.16467.pdf", "abs": "https://arxiv.org/abs/2505.16467", "title": "Reading Between the Prompts: How Stereotypes Shape LLM's Implicit Personalization", "authors": ["Vera Neplenbroek", "Arianna Bisazza", "Raquel Fernández"], "categories": ["cs.CL"], "comment": null, "summary": "Generative Large Language Models (LLMs) infer user's demographic information\nfrom subtle cues in the conversation -- a phenomenon called implicit\npersonalization. Prior work has shown that such inferences can lead to lower\nquality responses for users assumed to be from minority groups, even when no\ndemographic information is explicitly provided. In this work, we systematically\nexplore how LLMs respond to stereotypical cues using controlled synthetic\nconversations, by analyzing the models' latent user representations through\nboth model internals and generated answers to targeted user questions. Our\nfindings reveal that LLMs do infer demographic attributes based on these\nstereotypical signals, which for a number of groups even persists when the user\nexplicitly identifies with a different demographic group. Finally, we show that\nthis form of stereotype-driven implicit personalization can be effectively\nmitigated by intervening on the model's internal representations using a\ntrained linear probe to steer them toward the explicitly stated identity. Our\nresults highlight the need for greater transparency and control in how LLMs\nrepresent user identity.", "AI": {"tldr": "This paper explores how Generative Large Language Models (LLMs) infer user demographic information from conversation cues, highlighting issues of stereotype-driven implicit personalization and proposing methods for mitigation.", "motivation": "To understand the effects of implicit personalization by LLMs, especially concerning minority groups and the accuracy of their responses based on demographic inferences.", "method": "The authors conducted a systematic exploration using controlled synthetic conversations and analyzed LLMs' latent user representations through model internals and responses to specific user questions.", "result": "LLMs were found to infer demographic attributes based on stereotypical cues, leading to biased responses even against users identifying with a different demographic group. The study also showed that these biases can be mitigated by intervening with a trained linear probe on the model's internal representations.", "conclusion": "The study underscores the necessity for improved transparency and control over how LLMs handle user identity representations, particularly in relation to stereotype-driven biases.", "key_contributions": ["Identified how LLMs infer demographic information from conversation cues.", "Demonstrated the persistence of stereotype-driven biases regardless of user self-identification.", "Proposed a mitigation strategy using trained linear probes on models' internal representations."], "limitations": "The study focuses on synthetic conversations, which may not fully capture the complexities of real interactions.", "keywords": ["Generative Large Language Models", "implicit personalization", "stereotype-driven bias", "demographic inference", "transparency in AI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.16483", "pdf": "https://arxiv.org/pdf/2505.16483.pdf", "abs": "https://arxiv.org/abs/2505.16483", "title": "Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning", "authors": ["Shuzheng Si", "Haozhe Zhao", "Cheng Gao", "Yuzhuo Bai", "Zhitong Wang", "Bofei Gao", "Kangyang Luo", "Wenhao Li", "Yufei Huang", "Gang Chen", "Fanchao Qi", "Minjia Zhang", "Baobao Chang", "Maosong Sun"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Teaching large language models (LLMs) to be faithful in the provided context\nis crucial for building reliable information-seeking systems. Therefore, we\npropose a systematic framework, CANOE, to improve the faithfulness of LLMs in\nboth short-form and long-form generation tasks without human annotations.\nSpecifically, we first synthesize short-form question-answering (QA) data with\nfour diverse tasks to construct high-quality and easily verifiable training\ndata without human annotation. Also, we propose Dual-GRPO, a rule-based\nreinforcement learning method that includes three tailored rule-based rewards\nderived from synthesized short-form QA data, while simultaneously optimizing\nboth short-form and long-form response generation. Notably, Dual-GRPO\neliminates the need to manually label preference data to train reward models\nand avoids over-optimizing short-form generation when relying only on the\nsynthesized short-form QA data. Experimental results show that CANOE greatly\nimproves the faithfulness of LLMs across 11 different downstream tasks, even\noutperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.", "AI": {"tldr": "CANOE is a framework for improving the faithfulness of large language models (LLMs) in generation tasks without human annotations by synthesizing training data and using a reinforcement learning approach.", "motivation": "To develop reliable information-seeking systems, it is essential to enhance the faithfulness of LLMs in their output, which is traditionally reliant on human annotations.", "method": "The CANOE framework generates high-quality, verifiable training data by synthesizing short-form QA data from diverse tasks and employs a rule-based reinforcement learning method (Dual-GRPO) to optimize response generation without manual labeling.", "result": "Experimental results indicate that CANOE significantly enhances the faithfulness of LLMs across 11 different downstream tasks and surpasses state-of-the-art models like GPT-4o and OpenAI o1.", "conclusion": "The framework provides a robust solution for improving the generation quality of LLMs while bypassing the limitations of manual annotation, potentially expanding the applicability of LLMs in various domains.", "key_contributions": ["Synthesis of high-quality training data without human annotations.", "Introduction of Dual-GRPO for optimizing response generation.", "Demonstration of improved LLM faithfulness across numerous tasks."], "limitations": "", "keywords": ["large language models", "faithfulness", "reinforcement learning", "synthesized training data", "information-seeking systems"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.16491", "pdf": "https://arxiv.org/pdf/2505.16491.pdf", "abs": "https://arxiv.org/abs/2505.16491", "title": "LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing", "authors": ["Dario Di Palma", "Alessandro De Bellis", "Giovanni Servedio", "Vito Walter Anelli", "Fedelucio Narducci", "Tommaso Di Noia"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have rapidly become central to NLP,\ndemonstrating their ability to adapt to various tasks through prompting\ntechniques, including sentiment analysis. However, we still have a limited\nunderstanding of how these models capture sentiment-related information. This\nstudy probes the hidden layers of Llama models to pinpoint where sentiment\nfeatures are most represented and to assess how this affects sentiment\nanalysis.\n  Using probe classifiers, we analyze sentiment encoding across layers and\nscales, identifying the layers and pooling methods that best capture sentiment\nsignals. Our results show that sentiment information is most concentrated in\nmid-layers for binary polarity tasks, with detection accuracy increasing up to\n14% over prompting techniques. Additionally, we find that in decoder-only\nmodels, the last token is not consistently the most informative for sentiment\nencoding. Finally, this approach enables sentiment tasks to be performed with\nmemory requirements reduced by an average of 57%.\n  These insights contribute to a broader understanding of sentiment in LLMs,\nsuggesting layer-specific probing as an effective approach for sentiment tasks\nbeyond prompting, with potential to enhance model utility and reduce memory\nrequirements.", "AI": {"tldr": "This study investigates how sentiment-related information is captured in Llama models' layers, revealing that mid-layers provide the most concentrated sentiment signals, leading to improved detection accuracy and reduced memory requirements for sentiment tasks.", "motivation": "To understand how Large Language Models capture sentiment information, which remains poorly understood despite their popularity in NLP tasks.", "method": "Probe classifiers were used to analyze sentiment encoding across different layers of Llama models, identifying which layers and pooling methods best capture sentiment signals.", "result": "The research found that sentiment information is most concentrated in mid-layers for binary polarity tasks, with up to 14% accuracy increase over prompting techniques and a 57% reduction in memory requirements for sentiment tasks.", "conclusion": "Layer-specific probing enhances our understanding of sentiment in LLMs and suggests effective methods for improving sentiment task performance and model utility.", "key_contributions": ["Identification of optimal layers and pooling methods for sentiment detection in Llama models", "Demonstration of improved sentiment analysis accuracy using layer-specific probing", "Reduction of memory requirements for sentiment tasks by an average of 57%"], "limitations": "", "keywords": ["Large Language Models", "Sentiment Analysis", "Layer Probing", "NLP", "Memory Efficiency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.16505", "pdf": "https://arxiv.org/pdf/2505.16505.pdf", "abs": "https://arxiv.org/abs/2505.16505", "title": "Sparse Activation Editing for Reliable Instruction Following in Narratives", "authors": ["Runcong Zhao", "Chengyu Cao", "Qinglin Zhu", "Xiucheng Lv", "Shun Shao", "Lin Gui", "Ruifeng Xu", "Yulan He"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Complex narrative contexts often challenge language models' ability to follow\ninstructions, and existing benchmarks fail to capture these difficulties. To\naddress this, we propose Concise-SAE, a training-free framework that improves\ninstruction following by identifying and editing instruction-relevant neurons\nusing only natural language instructions, without requiring labelled data. To\nthoroughly evaluate our method, we introduce FreeInstruct, a diverse and\nrealistic benchmark of 1,212 examples that highlights the challenges of\ninstruction following in narrative-rich settings. While initially motivated by\ncomplex narratives, Concise-SAE demonstrates state-of-the-art instruction\nadherence across varied tasks without compromising generation quality.", "AI": {"tldr": "Concise-SAE is a framework that improves instruction following in language models by editing relevant neurons based on natural language without requiring labeled data, assessed through the FreeInstruct benchmark.", "motivation": "Existing benchmarks do not adequately capture the challenges faced by language models in complex narrative contexts, hindering effective instruction following.", "method": "Concise-SAE identifies and edits instruction-relevant neurons using only natural language instructions, without requiring labeled data, improving instruction adherence.", "result": "Concise-SAE achieves state-of-the-art instruction adherence across a variety of tasks while maintaining high generation quality.", "conclusion": "The framework effectively enhances instruction following in narrative-rich settings, marking a significant improvement over existing methods.", "key_contributions": ["Introducing a training-free framework for instruction following", "Development of the FreeInstruct benchmark with 1,212 diverse examples", "Demonstrating improved instruction adherence without compromising output quality"], "limitations": "", "keywords": ["Instruction following", "Natural language instructions", "Concise-SAE"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.16514", "pdf": "https://arxiv.org/pdf/2505.16514.pdf", "abs": "https://arxiv.org/abs/2505.16514", "title": "AppealCase: A Dataset and Benchmark for Civil Case Appeal Scenarios", "authors": ["Yuting Huang", "Meitong Guo", "Yiquan Wu", "Ang Li", "Xiaozhong Liu", "Keting Yin", "Changlong Sun", "Fei Wu", "Kun Kuang"], "categories": ["cs.CL"], "comment": "15 pages, 4 figures", "summary": "Recent advances in LegalAI have primarily focused on individual case judgment\nanalysis, often overlooking the critical appellate process within the judicial\nsystem. Appeals serve as a core mechanism for error correction and ensuring\nfair trials, making them highly significant both in practice and in research.\nTo address this gap, we present the AppealCase dataset, consisting of 10,000\npairs of real-world, matched first-instance and second-instance documents\nacross 91 categories of civil cases. The dataset also includes detailed\nannotations along five dimensions central to appellate review: judgment\nreversals, reversal reasons, cited legal provisions, claim-level decisions, and\nwhether there is new information in the second instance. Based on these\nannotations, we propose five novel LegalAI tasks and conduct a comprehensive\nevaluation across 20 mainstream models. Experimental results reveal that all\ncurrent models achieve less than 50% F1 scores on the judgment reversal\nprediction task, highlighting the complexity and challenge of the appeal\nscenario. We hope that the AppealCase dataset will spur further research in\nLegalAI for appellate case analysis and contribute to improving consistency in\njudicial decision-making.", "AI": {"tldr": "The paper introduces the AppealCase dataset to advance LegalAI by focusing on the appellate process, presenting five novel tasks, and evaluating mainstream models on judgment reversal predictions.", "motivation": "To address the oversight of appellate processes in LegalAI research, emphasizing the significance of appeals in error correction and fair trials.", "method": "Development of the AppealCase dataset comprising 10,000 matched documents across civil case categories, with detailed annotations and evaluation across 20 models.", "result": "All mainstream models achieved less than 50% F1 scores on judgment reversal predictions, indicating challenges in the appeal analysis.", "conclusion": "The dataset aims to foster further research in appellate case analysis within LegalAI to enhance judicial decision-making consistency.", "key_contributions": ["Introduction of the AppealCase dataset with real-world matched documents", "Presentation of five novel LegalAI tasks", "Comprehensive evaluation of existing models' performances on the dataset"], "limitations": "The complexity and challenge of judgment reversal prediction indicate currently inadequate model performance, which may limit immediate practical applications.", "keywords": ["LegalAI", "Appellate process", "Dataset", "Machine learning", "Judicial decision-making"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2505.16518", "pdf": "https://arxiv.org/pdf/2505.16518.pdf", "abs": "https://arxiv.org/abs/2505.16518", "title": "CUB: Benchmarking Context Utilisation Techniques for Language Models", "authors": ["Lovisa Hagström", "Youna Kim", "Haeun Yu", "Sang-goo Lee", "Richard Johansson", "Hyunsoo Cho", "Isabelle Augenstein"], "categories": ["cs.CL", "cs.AI"], "comment": "27 pages", "summary": "Incorporating external knowledge is crucial for knowledge-intensive tasks,\nsuch as question answering and fact checking. However, language models (LMs)\nmay ignore relevant information that contradicts outdated parametric memory or\nbe distracted by irrelevant contexts. While many context utilisation\nmanipulation techniques (CMTs) that encourage or suppress context utilisation\nhave recently been proposed to alleviate these issues, few have seen systematic\ncomparison. In this paper, we develop CUB (Context Utilisation Benchmark) to\nhelp practitioners within retrieval-augmented generation (RAG) identify the\nbest CMT for their needs. CUB allows for rigorous testing on three distinct\ncontext types, observed to capture key challenges in realistic context\nutilisation scenarios. With this benchmark, we evaluate seven state-of-the-art\nmethods, representative of the main categories of CMTs, across three diverse\ndatasets and tasks, applied to nine LMs. Our results show that most of the\nexisting CMTs struggle to handle the full set of types of contexts that may be\nencountered in real-world retrieval-augmented scenarios. Moreover, we find that\nmany CMTs display an inflated performance on simple synthesised datasets,\ncompared to more realistic datasets with naturally occurring samples.\nAltogether, our results show the need for holistic tests of CMTs and the\ndevelopment of CMTs that can handle multiple context types.", "AI": {"tldr": "This paper introduces the Context Utilisation Benchmark (CUB) for evaluating context utilisation manipulation techniques (CMTs) in retrieval-augmented generation tasks, revealing deficiencies in existing methods when handling real-world context scenarios.", "motivation": "The need to incorporate external knowledge effectively in knowledge-intensive tasks like question answering and fact checking, while addressing the limitations of language models that may ignore relevant information or be distracted by irrelevant contexts.", "method": "The authors developed CUB to systematically evaluate and compare seven state-of-the-art CMTs across three diverse datasets and tasks, applying the techniques to nine different language models.", "result": "The evaluation revealed that existing CMTs often failed to manage various types of contexts encountered in real-world scenarios and demonstrated inflated performances on simplified datasets compared to realistic ones.", "conclusion": "There is a clear need for more comprehensive evaluations of CMTs and the creation of techniques capable of accommodating multiple types of contexts in retrieval-augmented tasks.", "key_contributions": ["Introduction of the Context Utilisation Benchmark (CUB)", "Systematic comparison of seven state-of-the-art CMTs", "Identification of performance discrepancies across datasets"], "limitations": "", "keywords": ["Context Utilisation", "Retrieval-Augmented Generation", "Language Models", "CMTs", "Benchmarking"], "importance_score": 9, "read_time_minutes": 27}}
{"id": "2505.16520", "pdf": "https://arxiv.org/pdf/2505.16520.pdf", "abs": "https://arxiv.org/abs/2505.16520", "title": "Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs", "authors": ["Giovanni Servedio", "Alessandro De Bellis", "Dario Di Palma", "Vito Walter Anelli", "Tommaso Di Noia"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Factual hallucinations are a major challenge for Large Language Models\n(LLMs). They undermine reliability and user trust by generating inaccurate or\nfabricated content. Recent studies suggest that when generating false\nstatements, the internal states of LLMs encode information about truthfulness.\nHowever, these studies often rely on synthetic datasets that lack realism,\nwhich limits generalization when evaluating the factual accuracy of text\ngenerated by the model itself. In this paper, we challenge the findings of\nprevious work by investigating truthfulness encoding capabilities, leading to\nthe generation of a more realistic and challenging dataset. Specifically, we\nextend previous work by introducing: (1) a strategy for sampling plausible\ntrue-false factoid sentences from tabular data and (2) a procedure for\ngenerating realistic, LLM-dependent true-false datasets from Question Answering\ncollections. Our analysis of two open-source LLMs reveals that while the\nfindings from previous studies are partially validated, generalization to\nLLM-generated datasets remains challenging. This study lays the groundwork for\nfuture research on factuality in LLMs and offers practical guidelines for more\neffective evaluation.", "AI": {"tldr": "This paper explores the challenge of factual hallucinations in LLMs and introduces a more realistic dataset for evaluating truthfulness encoding capabilities.", "motivation": "Factual hallucinations in LLMs undermine their reliability and user trust, necessitating better evaluation methods for accuracy.", "method": "The paper proposes a strategy for sampling true-false factoid sentences from tabular data and generating realistic true-false datasets from Question Answering collections.", "result": "Analysis of two open-source LLMs indicates that while some previous findings are validated, generalization to LLM-generated datasets is still difficult.", "conclusion": "The study establishes a foundation for future research on LLM factuality and provides guidelines for effective evaluation.", "key_contributions": ["Introduction of a plausible true-false sentence sampling strategy from tabular data", "Creation of realistic LLM-dependent true-false datasets", "Validation and challenges of previous findings regarding truthfulness encoding in LLMs"], "limitations": "", "keywords": ["large language models", "factual accuracy", "truthfulness encoding", "dataset generation", "evaluation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.16522", "pdf": "https://arxiv.org/pdf/2505.16522.pdf", "abs": "https://arxiv.org/abs/2505.16522", "title": "Benchmarking and Pushing the Multi-Bias Elimination Boundary of LLMs via Causal Effect Estimation-guided Debiasing", "authors": ["Zhouhao Sun", "Zhiyuan Kan", "Xiao Ding", "Li Du", "Yang Zhao", "Bing Qin", "Ting Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite significant progress, recent studies have indicated that current\nlarge language models (LLMs) may still utilize bias during inference, leading\nto the poor generalizability of LLMs. Some benchmarks are proposed to\ninvestigate the generalizability of LLMs, with each piece of data typically\ncontaining one type of controlled bias. However, a single piece of data may\ncontain multiple types of biases in practical applications. To bridge this gap,\nwe propose a multi-bias benchmark where each piece of data contains five types\nof biases. The evaluations conducted on this benchmark reveal that the\nperformance of existing LLMs and debiasing methods is unsatisfying,\nhighlighting the challenge of eliminating multiple types of biases\nsimultaneously. To overcome this challenge, we propose a causal effect\nestimation-guided multi-bias elimination method (CMBE). This method first\nestimates the causal effect of multiple types of biases simultaneously.\nSubsequently, we eliminate the causal effect of biases from the total causal\neffect exerted by both the semantic information and biases during inference.\nExperimental results show that CMBE can effectively eliminate multiple types of\nbias simultaneously to enhance the generalizability of LLMs.", "AI": {"tldr": "The paper introduces a multi-bias benchmark for large language models (LLMs) that addresses the issue of multiple biases in data and proposes a causal effect estimation-guided method for bias elimination to improve LLM generalizability.", "motivation": "To address the poor generalizability of LLMs caused by biases during inference in practical applications.", "method": "The authors propose a multi-bias benchmark containing five types of biases and introduce a causal effect estimation-guided multi-bias elimination method (CMBE) to estimate and eliminate these biases.", "result": "Evaluations reveal inadequate performance of existing LLMs and debiasing methods, while CMBE effectively eliminates multiple biases simultaneously, enhancing LLM generalizability.", "conclusion": "The proposed CMBE method shows promise in addressing the challenges associated with multiple biases in LLMs, which is crucial for improving their application in diverse scenarios.", "key_contributions": ["Introduction of a multi-bias benchmark for LLMs.", "Development of CMBE for simultaneous bias elimination.", "Demonstration of CMBE's effectiveness in improving LLM generalizability."], "limitations": "", "keywords": ["large language models", "bias elimination", "generalizability", "CMBE", "multi-bias benchmark"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.16526", "pdf": "https://arxiv.org/pdf/2505.16526.pdf", "abs": "https://arxiv.org/abs/2505.16526", "title": "EnSToM: Enhancing Dialogue Systems with Entropy-Scaled Steering Vectors for Topic Maintenance", "authors": ["Heejae Suh", "Yejin Jeon", "Deokhyung Kang", "Taehee Park", "Yejin Min", "Gary Geunbae Lee"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 (Findings, long paper)", "summary": "Small large language models (sLLMs) offer the advantage of being lightweight\nand efficient, which makes them suitable for resource-constrained environments.\nHowever, sLLMs often struggle to maintain topic consistency in task-oriented\ndialogue systems, which is critical for scenarios such as service chatbots.\nSpecifically, it is important to ensure that the model denies off-topic or\nmalicious inputs and adheres to its intended functionality so as to prevent\npotential misuse and uphold reliability. Towards this, existing activation\nengineering approaches have been proposed to manipulate internal activations\nduring inference. While these methods are effective in certain scenarios, our\npreliminary experiments reveal their limitations in ensuring topic adherence.\nTherefore, to address this, we propose a novel approach termed Entropy-scaled\nSteering vectors for Topic Maintenance (EnSToM). EnSToM dynamically adjusts the\nsteering intensity based on input uncertainty, which allows the model to handle\noff-topic distractors effectively while preserving on-topic accuracy. Our\nexperiments demonstrate that EnSToM achieves significant performance gain with\na relatively small data size compared to fine-tuning approaches. By improving\ntopic adherence without compromising efficiency, our approach provides a robust\nsolution for enhancing sLLM-based dialogue systems.", "AI": {"tldr": "This paper presents EnSToM, a novel method for maintaining topic consistency in small large language models during task-oriented dialogue, critical for applications like service chatbots.", "motivation": "To address the difficulty sLLMs have in maintaining topic consistency in task-oriented dialogue systems, which is essential for applications like service chatbots to avoid off-topic or malicious inputs.", "method": "EnSToM dynamically adjusts the steering intensity based on input uncertainty to effectively handle off-topic distractors while maintaining on-topic accuracy.", "result": "Experiments show that EnSToM achieves significant performance gain with a smaller data size compared to traditional fine-tuning methods.", "conclusion": "EnSToM offers a robust solution for improving topic adherence in sLLM-based dialogue systems without sacrificing efficiency.", "key_contributions": ["Introduction of EnSToM for topic maintenance in sLLMs", "Demonstrated performance improvement with minimal data", "Preservation of on-topic accuracy in the presence of distractors"], "limitations": "Preliminary experiments reveal limitations of existing activation engineering methods in ensuring topic adherence.", "keywords": ["Small language models", "Topic consistency", "Task-oriented dialogue", "Entropy-scaled steering vectors", "HCI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.16538", "pdf": "https://arxiv.org/pdf/2505.16538.pdf", "abs": "https://arxiv.org/abs/2505.16538", "title": "Mechanistic Understanding and Mitigation of Language Confusion in English-Centric Large Language Models", "authors": ["Ercong Nie", "Helmut Schmid", "Hinrich Schütze"], "categories": ["cs.CL"], "comment": "16 pages, 5 figures", "summary": "Language confusion -- where large language models (LLMs) generate unintended\nlanguages against the user's need -- remains a critical challenge, especially\nfor English-centric models. We present the first mechanistic interpretability\n(MI) study of language confusion, combining behavioral benchmarking with\nneuron-level analysis. Using the Language Confusion Benchmark (LCB), we show\nthat confusion points (CPs) -- specific positions where language switches occur\n-- are central to this phenomenon. Through layer-wise analysis with TunedLens\nand targeted neuron attribution, we reveal that transition failures in the\nfinal layers drive confusion. We further demonstrate that editing a small set\nof critical neurons, identified via comparative analysis with\nmultilingual-tuned models, substantially mitigates confusion without harming\ngeneral competence or fluency. Our approach matches multilingual alignment in\nconfusion reduction for most languages and yields cleaner, higher-quality\noutputs. These findings provide new insights into the internal dynamics of LLMs\nand highlight neuron-level interventions as a promising direction for robust,\ninterpretable multilingual language modeling.", "AI": {"tldr": "This paper investigates language confusion in LLMs and offers neuron-level insights for mitigating the issue through targeted interventions.", "motivation": "Language confusion in large language models (LLMs) poses significant challenges, especially for English-centric models.", "method": "The study employs behavioral benchmarking and neuron-level analysis using the Language Confusion Benchmark (LCB) to identify confusion points and analyze neuron functionality.", "result": "The analysis reveals that transition failures in LLMs' final layers are key to language confusion and highlights the potential for editing critical neurons to alleviate this issue effectively.", "conclusion": "The findings suggest that neuron-level interventions can improve multilingual capabilities without sacrificing overall language fluency, offering a novel approach to enhancing LLM performance.", "key_contributions": ["First mechanistic interpretability study of language confusion in LLMs.", "Introduction of the Language Confusion Benchmark (LCB).", "Demonstration of effective neuron-level interventions to reduce language confusion."], "limitations": "", "keywords": ["language confusion", "mechanistic interpretability", "large language models"], "importance_score": 9, "read_time_minutes": 16}}
{"id": "2505.16552", "pdf": "https://arxiv.org/pdf/2505.16552.pdf", "abs": "https://arxiv.org/abs/2505.16552", "title": "Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains", "authors": ["Wenhui Tan", "Jiaze Li", "Jianzhong Ju", "Zhenbo Luo", "Jian Luan", "Ruihua Song"], "categories": ["cs.CL"], "comment": "15 pages, 8 figures", "summary": "Large Language Models (LLMs) achieve superior performance through\nChain-of-Thought (CoT) reasoning, but these token-level reasoning chains are\ncomputationally expensive and inefficient. In this paper, we introduce\nCompressed Latent Reasoning (CoLaR), a novel framework that dynamically\ncompresses reasoning processes in latent space through a two-stage training\napproach. First, during supervised fine-tuning, CoLaR extends beyond next-token\nprediction by incorporating an auxiliary next compressed embedding prediction\nobjective. This process merges embeddings of consecutive tokens using a\ncompression factor randomly sampled from a predefined range, and trains a\nspecialized latent head to predict distributions of subsequent compressed\nembeddings. Second, we enhance CoLaR through reinforcement learning (RL) that\nleverages the latent head's non-deterministic nature to explore diverse\nreasoning paths and exploit more compact ones. This approach enables CoLaR to:\ni) perform reasoning at a dense latent level (i.e., silently), substantially\nreducing reasoning chain length, and ii) dynamically adjust reasoning speed at\ninference time by simply prompting the desired compression factor. Extensive\nexperiments across four mathematical reasoning datasets demonstrate that CoLaR\nachieves 14.1% higher accuracy than latent-based baseline methods at comparable\ncompression ratios, and reduces reasoning chain length by 53.3% with only 4.8%\nperformance degradation compared to explicit CoT method. Moreover, when applied\nto more challenging mathematical reasoning tasks, our RL-enhanced CoLaR\ndemonstrates performance gains of up to 5.4% while dramatically reducing latent\nreasoning chain length by 82.8%. The code and models will be released upon\nacceptance.", "AI": {"tldr": "A novel framework called Compressed Latent Reasoning (CoLaR) that enhances reasoning efficiency in Large Language Models (LLMs) via dynamic compression of reasoning processes.", "motivation": "To address the computational expense and inefficiency of Chain-of-Thought (CoT) reasoning in Large Language Models.", "method": "CoLaR employs a two-stage training approach that involves supervised fine-tuning with an auxiliary compression objective, followed by reinforcement learning to explore diverse and compact reasoning paths.", "result": "CoLaR improves accuracy by 14.1% over baseline methods while significantly reducing reasoning chain length by 53.3% and achieving performance gains on challenging tasks.", "conclusion": "The proposed CoLaR framework offers a more efficient and effective means of reasoning in LLMs, promising reductions in reasoning length and improvements in task performance.", "key_contributions": ["Introduction of Compressed Latent Reasoning (CoLaR) framework.", "Dynamic adjustment of reasoning speed based on compression factors during inference.", "Demonstrated significant performance improvements with reduced reasoning length across multiple datasets."], "limitations": "", "keywords": ["Large Language Models", "Chain-of-Thought", "Compressed Latent Reasoning", "Reinforcement Learning", "Mathematical Reasoning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.16566", "pdf": "https://arxiv.org/pdf/2505.16566.pdf", "abs": "https://arxiv.org/abs/2505.16566", "title": "ScholarBench: A Bilingual Benchmark for Abstraction, Comprehension, and Reasoning Evaluation in Academic Contexts", "authors": ["Dongwon Noh", "Donghyeok Koh", "Junghun Yuk", "Gyuwan Kim", "Jaeyong Lee", "Kyungtae Lim", "Cheoneum Park"], "categories": ["cs.CL"], "comment": null, "summary": "Prior benchmarks for evaluating the domain-specific knowledge of large\nlanguage models (LLMs) lack the scalability to handle complex academic tasks.\nTo address this, we introduce \\texttt{ScholarBench}, a benchmark centered on\ndeep expert knowledge and complex academic problem-solving, which evaluates the\nacademic reasoning ability of LLMs and is constructed through a three-step\nprocess. \\texttt{ScholarBench} targets more specialized and logically complex\ncontexts derived from academic literature, encompassing five distinct problem\ntypes. Unlike prior benchmarks, \\texttt{ScholarBench} evaluates the\nabstraction, comprehension, and reasoning capabilities of LLMs across eight\ndistinct research domains. To ensure high-quality evaluation data, we define\ncategory-specific example attributes and design questions that are aligned with\nthe characteristic research methodologies and discourse structures of each\ndomain. Additionally, this benchmark operates as an English-Korean bilingual\ndataset, facilitating simultaneous evaluation for linguistic capabilities of\nLLMs in both languages. The benchmark comprises 5,031 examples in Korean and\n5,309 in English, with even state-of-the-art models like o3-mini achieving an\naverage evaluation score of only 0.543, demonstrating the challenging nature of\nthis benchmark.", "AI": {"tldr": "ScholarBench is a new benchmark for assessing the academic reasoning ability of LLMs, specifically designed for complex academic tasks across multiple research domains.", "motivation": "Current benchmarks fail to evaluate LLMs' knowledge in scalable and complex academic contexts, necessitating a new comprehensive evaluation framework.", "method": "The benchmark was constructed through a three-step process, defining category-specific attributes and designing aligned questions across eight research domains.", "result": "The benchmark includes 5,031 examples in Korean and 5,309 in English, revealing that even advanced LLMs struggle, with an average score of only 0.543.", "conclusion": "ScholarBench effectively challenges LLMs in their reasoning and linguistic capabilities, showcasing the need for rigorous evaluation in specialized contexts.", "key_contributions": ["Introduction of a comprehensive benchmark for evaluating academic reasoning in LLMs.", "Focus on deep expert knowledge and complex academic problem-solving.", "Bilingual dataset supporting evaluations in both English and Korean."], "limitations": "The benchmark may still not capture all facets of academic reasoning and may require ongoing updates to keep pace with evolving knowledge.", "keywords": ["Large Language Models", "Benchmarking", "Academic Reasoning", "Bilingual Dataset", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.16570", "pdf": "https://arxiv.org/pdf/2505.16570.pdf", "abs": "https://arxiv.org/abs/2505.16570", "title": "URLs Help, Topics Guide: Understanding Metadata Utility in LLM Training", "authors": ["Dongyang Fan", "Vinko Sabolčec", "Martin Jaggi"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are commonly pretrained on vast corpora of text\nwithout utilizing contextual metadata such as source, quality, or topic,\nleading to a context-free learning paradigm. While recent studies suggest that\nadding metadata like URL information as context (i.e., auxiliary inputs not\nused in the loss calculation) can improve training efficiency and downstream\nperformance, they offer limited understanding of which types of metadata are\ntruly effective and under what conditions. In this work, we conduct a\nsystematic evaluation and find that not all metadata types contribute equally.\nOnly URL context speeds up training, whereas quality scores and topic/format\ndomain information offer no clear benefit. Furthermore, the improved downstream\nperformances of URL conditioning emerge only when longer prompts are used at\ninference time. In addition, we demonstrate that context-aware pretraining\nenables more controllable generation than context-free pretraining, in a\nclassifier-free guidance fashion. Although topic and format metadata do not\naccelerate training, they are effective for steering outputs, offering\nhuman-interpretable control over generation.", "AI": {"tldr": "This paper investigates the use of contextual metadata in training Large Language Models (LLMs) and finds that only URL context enhances training efficiency and downstream performance, while other metadata types do not.", "motivation": "To improve the training efficiency and performance of Large Language Models (LLMs) by evaluating the effectiveness of different types of contextual metadata.", "method": "A systematic evaluation of various metadata types, including URL information, quality scores, and topic/domain descriptions, to analyze their impact on training and downstream performance.", "result": "URL context significantly speeds up training and improves performance when used with longer prompts; other metadata types provide no clear benefits, but are useful for output control.", "conclusion": "Context-aware pretraining allows for better control over generated outputs, highlighting the importance of URL metadata while suggesting that other types, though not beneficial for training speed, can steer outputs effectively.", "key_contributions": ["Identification of URL context as the only metadata type that accelerates training.", "Demonstration of improved controllability in generation with context-aware pretraining.", "Clarification on the limited benefits of quality scores and topic metadata in training efficiency."], "limitations": "The study primarily focuses on URL context and may not generalize to other types of metadata not tested.", "keywords": ["Large Language Models", "contextual metadata", "training efficiency", "downstream performance", "controlled generation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.16576", "pdf": "https://arxiv.org/pdf/2505.16576.pdf", "abs": "https://arxiv.org/abs/2505.16576", "title": "EMULATE: A Multi-Agent Framework for Determining the Veracity of Atomic Claims by Emulating Human Actions", "authors": ["Spencer Hong", "Meng Luo", "Xinyi Wan"], "categories": ["cs.CL"], "comment": null, "summary": "Determining the veracity of atomic claims is an imperative component of many\nrecently proposed fact-checking systems. Many approaches tackle this problem by\nfirst retrieving evidence by querying a search engine and then performing\nclassification by providing the evidence set and atomic claim to a large\nlanguage model, but this process deviates from what a human would do in order\nto perform the task. Recent work attempted to address this issue by proposing\niterative evidence retrieval, allowing for evidence to be collected several\ntimes and only when necessary. Continuing along this line of research, we\npropose a novel claim verification system, called EMULATE, which is designed to\nbetter emulate human actions through the use of a multi-agent framework where\neach agent performs a small part of the larger task, such as ranking search\nresults according to predefined criteria or evaluating webpage content.\nExtensive experiments on several benchmarks show clear improvements over prior\nwork, demonstrating the efficacy of our new multi-agent framework.", "AI": {"tldr": "EMULATE is a novel claim verification system using a multi-agent framework to improve human-like evidence retrieval for fact-checking.", "motivation": "To enhance the fidelity of fact-checking systems by emulating human behaviors in claim verification more closely.", "method": "The proposed system uses a multi-agent framework where different agents perform distinct tasks such as ranking search results and evaluating content based on predefined criteria.", "result": "The EMULATE system showed significant improvements in performance over previous methods in extensive benchmark experiments.", "conclusion": "EMULATE effectively enhances the claim verification process by mimicking human-like iterative evidence retrieval methods.", "key_contributions": ["Introduction of the EMULATE system for claim verification.", "Utilization of a multi-agent framework to distribute tasks.", "Demonstrated performance improvements on benchmark datasets."], "limitations": "", "keywords": ["claim verification", "multi-agent framework", "fact-checking", "evidence retrieval", "language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.16582", "pdf": "https://arxiv.org/pdf/2505.16582.pdf", "abs": "https://arxiv.org/abs/2505.16582", "title": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering", "authors": ["Jianbiao Mei", "Tao Hu", "Daocheng Fu", "Licheng Wen", "Xuemeng Yang", "Rong Wu", "Pinlong Cai", "Xing Gao", "Yu Yang", "Chengjun Xie", "Botian Shi", "Yong Liu", "Yu Qiao"], "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 9 figures", "summary": "Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones.", "AI": {"tldr": "The paper presents O$^2$-Searcher, a reinforcement learning-based search agent designed to address both open-ended and closed-ended questions in an open-domain setting, achieving significant performance improvements over existing LLM agents.", "motivation": "To address the limitations of large language models (LLMs) in handling open-ended questions, which often require dynamic and up-to-date information, and to improve performance beyond static parametric knowledge.", "method": "O$^2$-Searcher uses a reinforcement learning framework to create a locally simulated search environment that allows for dynamic knowledge acquisition. It incorporates a unified training mechanism with tailored reward functions to adapt strategies based on problem type.", "result": "O$^2$-Searcher significantly outperforms leading LLM agents on the O$^2$-QA benchmark, which contains 300 manually curated, multi-domain open-ended questions, and also achieves state-of-the-art results on various closed-ended QA benchmarks against similarly-sized models.", "conclusion": "The results indicate that O$^2$-Searcher effectively bridges the gap in LLM capabilities for open-ended questions while maintaining competitive performance on closed-ended tasks.", "key_contributions": ["Introduction of O$^2$-Searcher as a novel search agent for diverse question types.", "Development of O$^2$-QA benchmark with 300 curated open-ended questions.", "Demonstration of significant performance improvements over existing models using a 3B parameter model."], "limitations": "", "keywords": ["Large Language Models", "Reinforcement Learning", "Open-Ended Questions", "Dynamic Knowledge Acquisition", "Question Answering"], "importance_score": 9, "read_time_minutes": 25}}
{"id": "2505.16591", "pdf": "https://arxiv.org/pdf/2505.16591.pdf", "abs": "https://arxiv.org/abs/2505.16591", "title": "Evaluating Large Language Model with Knowledge Oriented Language Specific Simple Question Answering", "authors": ["Bowen Jiang", "Runchuan Zhu", "Jiang Wu", "Zinco Jiang", "Yifan He", "Junyuan Gao", "Jia Yu", "Rui Min", "Yinfan Wang", "Haote Yang", "Songyang Zhang", "Dahua Lin", "Lijun Wu", "Conghui He"], "categories": ["cs.CL"], "comment": "Equal contribution: Bowen Jiang, Runchuan Zhu, Jiang Wu;\n  Corresponding author: Conghui He", "summary": "We introduce KoLasSimpleQA, the first benchmark evaluating the multilingual\nfactual ability of Large Language Models (LLMs). Inspired by existing research,\nwe created the question set with features such as single knowledge point\ncoverage, absolute objectivity, unique answers, and temporal stability. These\nquestions enable efficient evaluation using the LLM-as-judge paradigm, testing\nboth the LLMs' factual memory and self-awareness (\"know what they don't know\").\nKoLasSimpleQA expands existing research in two key dimensions: (1) Breadth\n(Multilingual Coverage): It includes 9 languages, supporting global\napplicability evaluation. (2) Depth (Dual Domain Design): It covers both the\ngeneral domain (global facts) and the language-specific domain (such as\nhistory, culture, and regional traditions) for a comprehensive assessment of\nmultilingual capabilities. We evaluated mainstream LLMs, including traditional\nLLM and emerging Large Reasoning Models. Results show significant performance\ndifferences between the two domains, particularly in performance metrics,\nranking, calibration, and robustness. This highlights the need for targeted\nevaluation and optimization in multilingual contexts. We hope KoLasSimpleQA\nwill help the research community better identify LLM capability boundaries in\nmultilingual contexts and provide guidance for model optimization. We will\nrelease KoLasSimpleQA at https://github.com/opendatalab/KoLasSimpleQA .", "AI": {"tldr": "KoLasSimpleQA is a new benchmark for assessing the multilingual factual capabilities of Large Language Models (LLMs), featuring a diverse question set for comprehensive evaluation.", "motivation": "The paper aims to evaluate the factual abilities of LLMs in a multilingual context, addressing gaps in existing assessments.", "method": "The authors developed a question set covering 9 languages, focusing on single knowledge points, objectivity, and temporal stability, allowing evaluation of both traditional LLMs and new Large Reasoning Models.", "result": "Evaluation of several mainstream LLMs revealed notable differences in performance across general and language-specific domains, necessitating targeted optimization for multilingual applications.", "conclusion": "KoLasSimpleQA provides insights into the limits of LLM capabilities in multilingual settings, aiming to inform future model enhancements.", "key_contributions": ["Creation of a multilingual factual benchmark for LLMs", "Inclusion of both global and language-specific domains", "Highlighting performance differences in LLMs across diverse contexts"], "limitations": "", "keywords": ["Multilingual", "Large Language Models", "Benchmark", "Evaluation", "Factual ability"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.16592", "pdf": "https://arxiv.org/pdf/2505.16592.pdf", "abs": "https://arxiv.org/abs/2505.16592", "title": "What Media Frames Reveal About Stance: A Dataset and Study about Memes in Climate Change Discourse", "authors": ["Shijia Zhou", "Siyao Peng", "Simon Luebke", "Jörg Haßler", "Mario Haim", "Saif M. Mohammad", "Barbara Plank"], "categories": ["cs.CL", "cs.MM"], "comment": "19 pages, 9 figures", "summary": "Media framing refers to the emphasis on specific aspects of perceived reality\nto shape how an issue is defined and understood. Its primary purpose is to\nshape public perceptions often in alignment with the authors' opinions and\nstances. However, the interaction between stance and media frame remains\nlargely unexplored. In this work, we apply an interdisciplinary approach to\nconceptualize and computationally explore this interaction with internet memes\non climate change. We curate CLIMATEMEMES, the first dataset of climate-change\nmemes annotated with both stance and media frames, inspired by research in\ncommunication science. CLIMATEMEMES includes 1,184 memes sourced from 47\nsubreddits, enabling analysis of frame prominence over time and communities,\nand sheds light on the framing preferences of different stance holders. We\npropose two meme understanding tasks: stance detection and media frame\ndetection. We evaluate LLaVA-NeXT and Molmo in various setups, and report the\ncorresponding results on their LLM backbone. Human captions consistently\nenhance performance. Synthetic captions and human-corrected OCR also help\noccasionally. Our findings highlight that VLMs perform well on stance, but\nstruggle on frames, where LLMs outperform VLMs. Finally, we analyze VLMs'\nlimitations in handling nuanced frames and stance expressions on climate change\ninternet memes.", "AI": {"tldr": "This paper explores the interaction between stance and media framing in climate change memes, presenting the CLIMATEMEMES dataset and evaluating stance and media frame detection tasks using LLaVA-NeXT and Molmo models.", "motivation": "To understand how media framing and stance interact, particularly in the context of climate change, using internet memes as a case study.", "method": "The authors curated the CLIMATEMEMES dataset, which contains 1,184 annotated climate change memes. They conducted stance detection and media frame detection tasks using various models, including LLaVA-NeXT and Molmo, and analyzed their performances.", "result": "The study found that while Visual Language Models (VLMs) perform well on stance detection, they struggle with media frame detection compared to Large Language Models (LLMs), which outperform them in that task.", "conclusion": "The findings highlight the importance of further exploring the nuances of media frames and stances in the context of internet memes, with implications for understanding public perceptions of climate change.", "key_contributions": ["Introduction of the CLIMATEMEMES dataset", "Exploration of the interaction between stance and media framing", "Evaluation of model performance in meme understanding tasks"], "limitations": "VLMs have limitations in handling nuanced frames and stance expressions in climate change memes.", "keywords": ["media framing", "stance detection", "climate change", "internet memes", "dataset"], "importance_score": 4, "read_time_minutes": 20}}
{"id": "2505.16610", "pdf": "https://arxiv.org/pdf/2505.16610.pdf", "abs": "https://arxiv.org/abs/2505.16610", "title": "From Generic Empathy to Personalized Emotional Support: A Self-Evolution Framework for User Preference Alignment", "authors": ["Jing Ye", "Lu Xiang", "Yaping Zhang", "Chengqing Zong"], "categories": ["cs.CL"], "comment": "27 pages", "summary": "Effective emotional support hinges on understanding users' emotions and needs\nto provide meaningful comfort during multi-turn interactions. Large Language\nModels (LLMs) show great potential for expressing empathy; however, they often\ndeliver generic and one-size-fits-all responses that fail to address users'\nspecific needs. To tackle this issue, we propose a self-evolution framework\ndesigned to help LLMs improve their responses to better align with users'\nimplicit preferences concerning user profiles (personalities), emotional\nstates, and specific situations. Our framework consists of two distinct phases:\n\\textit{(1)} \\textit{Emotional Support Experience Acquisition}, where LLMs are\nfine-tuned on limited emotional support conversation data to provide basic\nsupport, and \\textit{(2)} \\textit{Self-Improvement for Personalized Emotional\nSupport}, where LLMs leverage self-reflection and self-refinement to generate\npersonalized responses. Through iterative direct preference optimization\nbetween the pre- and post-refined responses, our model generates responses that\nreflect a better understanding of the user's implicit preferences. Extensive\nexperiments and evaluations demonstrate that our method significantly enhances\nthe model's performance in emotional support, reducing unhelpful responses and\nminimizing discrepancies between user preferences and model outputs.", "AI": {"tldr": "A framework for improving LLMs' emotional support responses by aligning with user preferences through self-reflection and refinement.", "motivation": "To enhance the emotional support provided by LLMs by making responses more personalized and reflective of users' implicit preferences.", "method": "The framework involves two phases: 1) fine-tuning LLMs on emotional support data to acquire basic support, and 2) self-refinement to optimize responses based on user preferences.", "result": "The proposed method significantly improves the performance of LLMs in delivering emotional support, reducing generic responses and aligning better with user needs through iterative preference optimizations.", "conclusion": "The self-evolution framework effectively helps LLMs to better understand and cater to the emotional states and preferences of users, enhancing the quality of emotional support.", "key_contributions": ["Introduces a self-evolution framework to improve LLM responses to emotional support.", "Employs iterative preference optimization for personalized responses.", "Validates the framework through extensive experiments demonstrating performance improvements."], "limitations": "The framework is dependent on the quality of the initial emotional support conversation data used for fine-tuning.", "keywords": ["Emotion", "Large Language Models", "Personalized Responses", "Emotional Support", "Self-Improvement"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.16612", "pdf": "https://arxiv.org/pdf/2505.16612.pdf", "abs": "https://arxiv.org/abs/2505.16612", "title": "Steering Large Language Models for Machine Translation Personalization", "authors": ["Daniel Scalena", "Gabriele Sarti", "Arianna Bisazza", "Elisabetta Fersini", "Malvina Nissim"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "High-quality machine translation systems based on large language models\n(LLMs) have simplified the production of personalized translations reflecting\nspecific stylistic constraints. However, these systems still struggle in\nsettings where stylistic requirements are less explicit and might be harder to\nconvey via prompting. We explore various strategies for personalizing\nLLM-generated translations in low-resource settings, focusing on the\nchallenging literary translation domain. We explore prompting strategies and\ninference-time interventions for steering model generations towards a\npersonalized style, and propose a contrastive framework exploiting latent\nconcepts extracted from sparse autoencoders to identify salient personalization\nproperties. Our results show that steering achieves strong personalization\nwhile preserving translation quality. We further examine the impact of steering\non LLM representations, finding model layers with a relevant impact for\npersonalization are impacted similarly by multi-shot prompting and our steering\nmethod, suggesting similar mechanism at play.", "AI": {"tldr": "This paper explores personalized machine translation using LLMs with a focus on literary translations, proposing strategies for effective personalization while maintaining quality.", "motivation": "The need for personalized translations that meet specific stylistic constraints in low-resource settings, particularly in literary translation, where traditional prompting fails to capture nuanced stylistic requirements.", "method": "The paper investigates various prompting strategies and inference-time interventions in LLMs, introducing a contrastive framework that leverages latent concepts from sparse autoencoders to identify important personalization attributes.", "result": "Steering methods lead to significant personalization in translations while maintaining quality, with insights into the effects on LLM representations and the interaction of multi-shot prompting and steering.", "conclusion": "Personalization in LLM-generated translations is achievable even in challenging domains, with effective methods that do not compromise the quality of translations.", "key_contributions": ["Introduction of prompting strategies and inference-time interventions for LLM personalization", "Development of a contrastive framework using sparse autoencoders for personalization attributes", "Demonstration of the relationship between prompting and personalization influence on LLM representations"], "limitations": "", "keywords": ["Machine Translation", "Large Language Models", "Personalization", "Literary Translation", "Inference-time Interventions"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.16637", "pdf": "https://arxiv.org/pdf/2505.16637.pdf", "abs": "https://arxiv.org/abs/2505.16637", "title": "SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation", "authors": ["Wenjie Yang", "Mao Zheng", "Mingyang Song", "Zheng Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have recently demonstrated remarkable\ncapabilities in machine translation (MT). However, most advanced MT-specific\nLLMs heavily rely on external supervision signals during training, such as\nhuman-annotated reference data or trained reward models (RMs), which are often\nexpensive to obtain and challenging to scale. To overcome this limitation, we\npropose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for\nMT that is reference-free, fully online, and relies solely on self-judging\nrewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as\nthe backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,\ne.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like\nQwen2.5-32B-Instruct in English $\\leftrightarrow$ Chinese translation tasks\nfrom WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR\nwith external supervision from COMET, our strongest model, SSR-X-Zero-7B,\nachieves state-of-the-art performance in English $\\leftrightarrow$ Chinese\ntranslation, surpassing all existing open-source models under 72B parameters\nand even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.\nOur analysis highlights the effectiveness of the self-rewarding mechanism\ncompared to the external LLM-as-a-judge approach in MT and demonstrates its\ncomplementary benefits when combined with trained RMs. Our findings provide\nvaluable insight into the potential of self-improving RL methods. We have\npublicly released our code, data and models.", "AI": {"tldr": "This paper introduces a Simple Self-Rewarding Reinforcement Learning framework for machine translation that eliminates reliance on costly external supervision.\n", "motivation": "To address the limitations of relying on external supervision signals during training of machine translation models, which are expensive and difficult to scale.", "method": "The authors propose a Simple Self-Rewarding (SSR) RL framework that is reference-free and operates fully online, using self-judging rewards from 13K monolingual examples with the Qwen-2.5-7B model.", "result": "The SSR-Zero-7B model outperforms several advanced MT models, including TowerInstruct-13B and larger general LLMs, in English ⇄ Chinese translation tasks. An augmented model, SSR-X-Zero-7B, using external supervision, achieves state-of-the-art performance on various benchmarks.", "conclusion": "The paper concludes that the self-rewarding mechanism is more effective than traditional external judging methods in machine translation and complements trained reward models, indicating a promising direction for RL methods in MT.", "key_contributions": ["Development of the Simple Self-Rewarding RL framework for MT.", "Demonstration of state-of-the-art performance in translation tasks without external references.", "Public release of code, data, and models to support further research."], "limitations": "", "keywords": ["Machine Translation", "Reinforcement Learning", "Self-Rewarding Mechanism"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.16648", "pdf": "https://arxiv.org/pdf/2505.16648.pdf", "abs": "https://arxiv.org/abs/2505.16648", "title": "Collaboration among Multiple Large Language Models for Medical Question Answering", "authors": ["Kexin Shang", "Chia-Hsuan Chang", "Christopher C. Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to IEEE International Conference on Healthcare Informatics\n  2025", "summary": "Empowered by vast internal knowledge reservoir, the new generation of large\nlanguage models (LLMs) demonstrate untapped potential to tackle medical tasks.\nHowever, there is insufficient effort made towards summoning up a synergic\neffect from multiple LLMs' expertise and background. In this study, we propose\na multi-LLM collaboration framework tailored on a medical multiple-choice\nquestions dataset. Through post-hoc analysis on 3 pre-trained LLM participants,\nour framework is proved to boost all LLMs reasoning ability as well as\nalleviate their divergence among questions. We also measure an LLM's confidence\nwhen it confronts with adversary opinions from other LLMs and observe a\nconcurrence between LLM's confidence and prediction accuracy.", "AI": {"tldr": "This study presents a multi-LLM collaboration framework to enhance reasoning abilities in medical tasks, demonstrating improved performance through synergy among language models.", "motivation": "There is a lack of efforts to leverage the combined expertise of multiple LLMs for medical applications, particularly in enhancing reasoning abilities.", "method": "The study proposes a collaborative framework for multiple LLMs, utilizing a medical multiple-choice question dataset and conducting post-hoc analyses of three pre-trained LLMs.", "result": "The framework significantly boosts the reasoning abilities of all participating LLMs and reduces their divergence in question responses, alongside measuring LLM confidence against adversarial opinions.", "conclusion": "The synergy from multiple LLMs leads to improved reasoning and prediction accuracy, suggesting potential for effective applications in healthcare.", "key_contributions": ["Proposes a collaborative multi-LLM framework", "Demonstrates enhancement in LLM reasoning abilities", "Analyzes the relationship between LLM confidence and prediction accuracy"], "limitations": "", "keywords": ["large language models", "health informatics", "multi-LLM collaboration", "reasoning ability", "confidence measurement"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.16660", "pdf": "https://arxiv.org/pdf/2505.16660.pdf", "abs": "https://arxiv.org/abs/2505.16660", "title": "Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu", "authors": ["Liu Chang", "Wang Dongbo", "Liu liu", "Zhao Zhixiao"], "categories": ["cs.CL", "cs.AI"], "comment": "29pages, 7 figures", "summary": "This study addresses the challenges in intelligent processing of Chinese\nancient mathematical classics by constructing Guji_MATH, a benchmark for\nevaluating classical texts based on Suanjing Shishu. It systematically assesses\nthe mathematical problem-solving capabilities of mainstream reasoning models\nunder the unique linguistic constraints of classical Chinese. Through\nmachine-assisted annotation and manual verification, 538 mathematical problems\nwere extracted from 8 canonical texts, forming a structured dataset centered on\nthe \"Question-Answer-Solution\" framework, supplemented by problem types and\ndifficulty levels. Dual evaluation modes--closed-book (autonomous\nproblem-solving) and open-book (reproducing classical solution methods)--were\ndesigned to evaluate the performance of six reasoning models on ancient Chinese\nmathematical problems. Results indicate that reasoning models can partially\ncomprehend and solve these problems, yet their overall performance remains\ninferior to benchmarks on modern mathematical tasks. Enhancing models'\nclassical Chinese comprehension and cultural knowledge should be prioritized\nfor optimization. This study provides methodological support for mining\nmathematical knowledge from ancient texts and disseminating traditional\nculture, while offering new perspectives for evaluating cross-linguistic and\ncross-cultural capabilities of reasoning models.", "AI": {"tldr": "This study presents Guji_MATH, a benchmark for evaluating reasoning models on ancient Chinese mathematical texts, revealing their limitations and suggesting the need for improved cultural comprehension.", "motivation": "The study aims to address the challenges of processing classical Chinese mathematical texts and evaluating reasoning models under unique linguistic constraints.", "method": "The research built Guji_MATH, extracting 538 mathematical problems from 8 ancient texts, using machine-assisted annotation and manual verification to create a structured dataset.", "result": "Reasoning models demonstrated partial understanding of ancient Chinese mathematical problems, but their overall performance was inferior to established benchmarks for modern tasks.", "conclusion": "There is a significant need to enhance reasoning models' comprehension of classical Chinese and cultural knowledge to optimize their performance on ancient problems.", "key_contributions": ["Development of Guji_MATH benchmark for ancient mathematical texts", "Structured dataset of 538 problems using the Question-Answer-Solution framework", "Evaluation of reasoning models' performance on cross-linguistic and cross-cultural tasks"], "limitations": "The models' comprehensiveness and performance on classical texts remain lower compared to modern problems; cultural and linguistic optimization is required.", "keywords": ["classical Chinese", "mathematics", "reasoning models", "machine learning", "cross-linguistic"], "importance_score": 2, "read_time_minutes": 15}}
{"id": "2505.16661", "pdf": "https://arxiv.org/pdf/2505.16661.pdf", "abs": "https://arxiv.org/abs/2505.16661", "title": "A Japanese Language Model and Three New Evaluation Benchmarks for Pharmaceutical NLP", "authors": ["Issey Sukeda", "Takuro Fujii", "Kosei Buma", "Shunsuke Sasaki", "Shinnosuke Ono"], "categories": ["cs.CL"], "comment": "15 pages, 9 tables, 5 figures", "summary": "We present a Japanese domain-specific language model for the pharmaceutical\nfield, developed through continual pretraining on 2 billion Japanese\npharmaceutical tokens and 8 billion English biomedical tokens. To enable\nrigorous evaluation, we introduce three new benchmarks: YakugakuQA, based on\nnational pharmacist licensing exams; NayoseQA, which tests cross-lingual\nsynonym and terminology normalization; and SogoCheck, a novel task designed to\nassess consistency reasoning between paired statements. We evaluate our model\nagainst both open-source medical LLMs and commercial models, including GPT-4o.\nResults show that our domain-specific model outperforms existing open models\nand achieves competitive performance with commercial ones, particularly on\nterminology-heavy and knowledge-based tasks. Interestingly, even GPT-4o\nperforms poorly on SogoCheck, suggesting that cross-sentence consistency\nreasoning remains an open challenge. Our benchmark suite offers a broader\ndiagnostic lens for pharmaceutical NLP, covering factual recall, lexical\nvariation, and logical consistency. This work demonstrates the feasibility of\nbuilding practical, secure, and cost-effective language models for Japanese\ndomain-specific applications, and provides reusable evaluation resources for\nfuture research in pharmaceutical and healthcare NLP. Our model, codes, and\ndatasets are released at https://github.com/EQUES-Inc/pharma-LLM-eval.", "AI": {"tldr": "Development of a Japanese domain-specific language model for the pharmaceutical field with robust benchmarks for evaluation.", "motivation": "To address the need for improved NLP tools in the pharmaceutical field and evaluate their performance rigorously.", "method": "Continual pretraining on 2 billion Japanese pharmaceutical tokens and 8 billion English biomedical tokens; introduction of new benchmarks for evaluation.", "result": "The model outperforms existing open models and is competitive with commercial models, particularly on specialized tasks.", "conclusion": "The research demonstrates the potential for creating practical language models for domain-specific applications and provides essential evaluation resources.", "key_contributions": ["Development of a domain-specific model for pharmaceuticals", "Introduction of novel evaluation benchmarks (YakugakuQA, NayoseQA, SogoCheck)", "Demonstration of competitive performance against existing models including commercial options."], "limitations": "", "keywords": ["pharmaceutical NLP", "domain-specific language model", "evaluation benchmarks"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.16694", "pdf": "https://arxiv.org/pdf/2505.16694.pdf", "abs": "https://arxiv.org/abs/2505.16694", "title": "Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence", "authors": ["Gouki Minegishi", "Hiroki Furuta", "Shohei Taniguchi", "Yusuke Iwasawa", "Yutaka Matsuo"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ICML 2025", "summary": "Transformer-based language models exhibit In-Context Learning (ICL), where\npredictions are made adaptively based on context. While prior work links\ninduction heads to ICL through a sudden jump in accuracy, this can only account\nfor ICL when the answer is included within the context. However, an important\nproperty of practical ICL in large language models is the ability to meta-learn\nhow to solve tasks from context, rather than just copying answers from context;\nhow such an ability is obtained during training is largely unexplored. In this\npaper, we experimentally clarify how such meta-learning ability is acquired by\nanalyzing the dynamics of the model's circuit during training. Specifically, we\nextend the copy task from previous research into an In-Context Meta Learning\nsetting, where models must infer a task from examples to answer queries.\nInterestingly, in this setting, we find that there are multiple phases in the\nprocess of acquiring such abilities, and that a unique circuit emerges in each\nphase, contrasting with the single-phases change in induction heads. The\nemergence of such circuits can be related to several phenomena known in large\nlanguage models, and our analysis lead to a deeper understanding of the source\nof the transformer's ICL ability.", "AI": {"tldr": "This paper explores how transformer-based language models develop the ability for In-Context Meta Learning during training, identifying distinct phases and circuits involved in acquiring this capability.", "motivation": "To understand how transformer-based language models can meta-learn tasks from context rather than merely copying answers, which is crucial for practical In-Context Learning (ICL).", "method": "The authors experimentally analyze the model's circuit dynamics during training in a novel In-Context Meta Learning setup, extending previous copy task methods.", "result": "The study identifies multiple phases in the acquisition of meta-learning abilities, each associated with the emergence of distinct circuits, shedding light on the transformers' ICL capabilities.", "conclusion": "Insights gained from the dynamics of circuit emergence provide a deeper understanding of how transformers develop their ICL abilities, suggesting a complex learning process beyond simple induction head changes.", "key_contributions": ["Introduced a novel In-Context Meta Learning framework to analyze transformer training dynamics.", "Identified multiple learning phases with unique circuit formations in language models.", "Provided insights linking circuit emergence to known phenomena in large language models."], "limitations": "", "keywords": ["Transformer models", "In-Context Learning", "Meta Learning", "NLP", "Machine Learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.16703", "pdf": "https://arxiv.org/pdf/2505.16703.pdf", "abs": "https://arxiv.org/abs/2505.16703", "title": "Locate-then-Merge: Neuron-Level Parameter Fusion for Mitigating Catastrophic Forgetting in Multimodal LLMs", "authors": ["Zeping Yu", "Sophia Ananiadou"], "categories": ["cs.CL"], "comment": null, "summary": "Although multimodal large language models (MLLMs) have achieved impressive\nperformance, the multimodal instruction tuning stage often causes catastrophic\nforgetting of the base LLM's language ability, even in strong models like\nLlama3. To address this, we propose Locate-then-Merge, a training-free\nparameter fusion framework that first locates important parameters and then\nselectively merges them. We further introduce Neuron-Fusion, a neuron-level\nstrategy that preserves the influence of neurons with large parameter\nshifts--neurons likely responsible for newly acquired visual\ncapabilities--while attenuating the influence of neurons with smaller changes\nthat likely encode general-purpose language skills. This design enables better\nretention of visual adaptation while mitigating language degradation.\nExperiments on 13 benchmarks across both language and visual tasks show that\nNeuron-Fusion consistently outperforms existing model merging methods. Further\nanalysis reveals that our method effectively reduces context hallucination in\ngeneration.", "AI": {"tldr": "This paper presents Locate-then-Merge, a framework for multimodal large language models that prevents catastrophic forgetting of language skills during multimodal instruction tuning by selectively merging important parameters.", "motivation": "To address the problem of catastrophic forgetting in multimodal large language models, particularly in relation to retaining language abilities while acquiring new visual capabilities.", "method": "The authors propose a training-free parameter fusion framework called Locate-then-Merge, which identifies important parameters and merges them judiciously. Additionally, a neuron-level approach called Neuron-Fusion is introduced to focus on neurons with significant parameter shifts.", "result": "Neuron-Fusion outperforms existing model merging methods in 13 benchmarks across language and visual tasks, effectively reducing context hallucination in generated outputs.", "conclusion": "The proposed method successfully balances the retention of language skills while adapting to new visual capabilities, thus enhancing the performance of multimodal large language models without requiring additional training.", "key_contributions": ["Introduction of Locate-then-Merge framework for parameter fusion in MLLMs", "Development of Neuron-Fusion strategy for neuron-level parameter management", "Demonstrated effectiveness in reducing context hallucination in generated outputs."], "limitations": "", "keywords": ["multimodal large language models", "parameter fusion", "Neuron-Fusion", "catastrophic forgetting", "visual adaptation"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2505.16722", "pdf": "https://arxiv.org/pdf/2505.16722.pdf", "abs": "https://arxiv.org/abs/2505.16722", "title": "Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification", "authors": ["Himanshu Beniwal", "Youngwoo Kim", "Maarten Sap", "Soham Dan", "Thomas Hartvigsen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) become increasingly prevalent in global\napplications, ensuring that they are toxicity-free across diverse linguistic\ncontexts remains a critical challenge. We explore \"Cross-lingual\nDetoxification\", a cross-lingual paradigm that mitigates toxicity, enabling\ndetoxification capabilities to transfer between high and low-resource languages\nacross different script families. We analyze cross-lingual detoxification's\neffectiveness through 504 extensive settings to evaluate toxicity reduction in\ncross-distribution settings with limited data and investigate how mitigation\nimpacts model performance on non-toxic tasks, revealing trade-offs between\nsafety and knowledge preservation. Our code and dataset are publicly available\nat https://github.com/himanshubeniwal/Breaking-mBad.", "AI": {"tldr": "This paper investigates a cross-lingual detoxification method for large language models aimed at reducing toxicity across languages while preserving model performance on non-toxic tasks.", "motivation": "With the growing use of large language models globally, there is an urgent need to ensure these models are free from toxicity in various languages, especially for low-resource languages.", "method": "The study employs a cross-lingual detoxification framework that evaluates toxicity reduction in 504 different settings and analyzes the impacts on model performance for non-toxic tasks under varying data availabilities.", "result": "The research demonstrates effective toxicity reduction in cross-lingual environments, revealing important trade-offs between maintaining safety and preserving knowledge across different languages and scripts.", "conclusion": "The findings highlight the potential of cross-lingual detoxification in enhancing the safety of LLMs while indicating the need for balancing safety with task performance.", "key_contributions": ["Introduction of a cross-lingual detoxification framework for LLMs", "Performance analysis of toxicity reduction across diverse languages", "Public availability of the code and dataset for further research"], "limitations": "The study may be limited by the dataset and settings used, which could affect the generalizability of the findings to all languages and contexts.", "keywords": ["cross-lingual detoxification", "large language models", "toxicity reduction", "machine learning", "natural language processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.16743", "pdf": "https://arxiv.org/pdf/2505.16743.pdf", "abs": "https://arxiv.org/abs/2505.16743", "title": "TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative Metric-driven Pruning", "authors": ["Florentin Beck", "William Rudman", "Carsten Eickhoff"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; F.2.2"], "comment": null, "summary": "Large Language Models (LLMs) present significant computational and memory\nchallenges due to their extensive size, making pruning essential for their\nefficient deployment. Existing one-shot pruning methods often apply uniform\nsparsity constraints across layers or within each layer, resulting in\nsuboptimal performance, especially at high sparsity ratios. This work\nintroduces TRIM (Targeted Row-wise Iterative Metric-driven pruning), a novel\napproach that applies varying sparsity ratios to individual output dimensions\n(rows) within each layer. TRIM employs an iterative adjustment process guided\nby quality metrics to optimize dimension-wise sparsity allocation, focusing on\nreducing variance in quality retention across outputs to preserve critical\ninformation. TRIM can be seamlessly integrated with existing layer-wise pruning\nstrategies. Our evaluations on perplexity and zero-shot tasks across diverse\nLLM families (Qwen2.5, LLaMA-2, and OPT) and sparsity levels demonstrate that\nTRIM achieves new state-of-the-art results and enhances stability. For\ninstance, at 80% sparsity, TRIM reduces perplexity by 48% for Qwen2.5-14B and\nover 90% for OPT-13B compared to baseline methods. We conclude that\nfine-grained, dimension-wise sparsity adaptation is crucial for pushing the\nlimits of extreme LLM compression. Code available at:\nhttps://github.com/flobk/TRIM", "AI": {"tldr": "TRIM is a novel pruning method for large language models that applies varying sparsity to output dimensions to optimize performance, achieving state-of-the-art results in model compression.", "motivation": "Large Language Models require pruning for efficient deployment due to their size, and existing methods can hinder performance at high sparsity.", "method": "TRIM applies targeted row-wise pruning with varying sparsity ratios across individual output dimensions and utilizes an iterative adjustment process based on quality metrics.", "result": "TRIM achieves state-of-the-art results, significantly reducing perplexity at high sparsity levels, and enhancing stability across various LLMs.", "conclusion": "Fine-grained, dimension-wise sparsity adaptation is crucial for optimal LLM compression performance.", "key_contributions": ["Introduction of TRIM for dimension-wise sparsity allocation", "Demonstrated performance improvements on multiple LLMs", "Integration capability with existing pruning strategies"], "limitations": "", "keywords": ["large language models", "pruning", "sparsity", "machine learning", "optimization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.16774", "pdf": "https://arxiv.org/pdf/2505.16774.pdf", "abs": "https://arxiv.org/abs/2505.16774", "title": "IFEval-Audio: Benchmarking Instruction-Following Capability in Audio-based Large Language Models", "authors": ["Yiming Gao", "Bin Wang", "Chengwei Wei", "Shuo Sun", "AiTi Aw"], "categories": ["cs.CL"], "comment": "Link: https://github.com/AudioLLMs/AudioBench/tree/main/IFEval-Audio", "summary": "Large language models (LLMs) have demonstrated strong instruction-following\ncapabilities in text-based tasks. However, this ability often deteriorates in\nmultimodal models after alignment with non-text modalities such as images or\naudio. While several recent efforts have investigated instruction-following\nperformance in text and vision-language models, instruction-following in\naudio-based large language models remains largely unexplored. To bridge this\ngap, we introduce IFEval-Audio, a novel evaluation dataset designed to assess\nthe ability to follow instructions in an audio LLM. IFEval-Audio contains 280\naudio-instruction-answer triples across six diverse dimensions: Content,\nCapitalization, Symbol, List Structure, Length, and Format. Each example pairs\nan audio input with a text instruction, requiring the model to generate an\noutput that follows a specified structure. We benchmark state-of-the-art audio\nLLMs on their ability to follow audio-involved instructions. The dataset is\nreleased publicly to support future research in this emerging area.", "AI": {"tldr": "Introduction of IFEval-Audio, a dataset for evaluating instruction-following in audio-based large language models.", "motivation": "To address the gap in evaluating instruction-following capabilities of audio LLMs, which have been underexplored compared to text and vision-language models.", "method": "The authors created IFEval-Audio, consisting of 280 audio-instruction-answer triples, assessing various dimensions related to instruction-following in audio LLMs.", "result": "Benchmarking on state-of-the-art audio LLMs demonstrated varying levels of ability to follow complex audio instructions.", "conclusion": "The IFEval-Audio dataset aims to enable future research on instruction-following in audio models and improve their performance.", "key_contributions": ["Introduction of IFEval-Audio dataset for audiobased instruction-following assessment", "Benchmarking of existing audio LLMs", "Public release of the dataset for ongoing research"], "limitations": "", "keywords": ["audio LLMs", "instruction-following", "multimodal models", "evaluation dataset", "machine learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.16782", "pdf": "https://arxiv.org/pdf/2505.16782.pdf", "abs": "https://arxiv.org/abs/2505.16782", "title": "Reasoning Beyond Language: A Comprehensive Survey on Latent Chain-of-Thought Reasoning", "authors": ["Xinghao Chen", "Anhao Zhao", "Heming Xia", "Xuan Lu", "Hanlin Wang", "Yanjun Chen", "Wei Zhang", "Jian Wang", "Wenjie Li", "Xiaoyu Shen"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved impressive performance on complex\nreasoning tasks with Chain-of-Thought (CoT) prompting. However, conventional\nCoT relies on reasoning steps explicitly verbalized in natural language,\nintroducing inefficiencies and limiting its applicability to abstract\nreasoning. To address this, there has been growing research interest in latent\nCoT reasoning, where inference occurs within latent spaces. By decoupling\nreasoning from language, latent reasoning promises richer cognitive\nrepresentations and more flexible, faster inference. Researchers have explored\nvarious directions in this promising field, including training methodologies,\nstructural innovations, and internal reasoning mechanisms. This paper presents\na comprehensive overview and analysis of this reasoning paradigm. We begin by\nproposing a unified taxonomy from four perspectives: token-wise strategies,\ninternal mechanisms, analysis, and applications. We then provide in-depth\ndiscussions and comparative analyses of representative methods, highlighting\ntheir design patterns, strengths, and open challenges. We aim to provide a\nstructured foundation for advancing this emerging direction in LLM reasoning.\nThe relevant papers will be regularly updated at\nhttps://github.com/EIT-NLP/Awesome-Latent-CoT.", "AI": {"tldr": "This paper reviews latent Chain-of-Thought (CoT) reasoning in Large Language Models, proposing a taxonomy and discussing various methodologies and challenges.", "motivation": "The need to improve efficiency and applicability of reasoning in LLMs by moving beyond natural language prompts for latent inference.", "method": "A comprehensive review and analysis of latent CoT reasoning, categorized into token-wise strategies, internal mechanisms, and applications.", "result": "The paper presents a proposed taxonomy of latent reasoning methods and highlights their strengths and challenges.", "conclusion": "A structured foundation is provided to advance the study of latent reasoning in LLMs, with plans for ongoing updates to relevant literature.", "key_contributions": ["Proposed a unified taxonomy for latent CoT reasoning in LLMs.", "In-depth analysis of various methodologies and design patterns.", "Highlighting open challenges and future directions for research."], "limitations": "", "keywords": ["Large Language Models", "Chain-of-Thought", "latent reasoning", "natural language processing", "cognitive representations"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2505.16789", "pdf": "https://arxiv.org/pdf/2505.16789.pdf", "abs": "https://arxiv.org/abs/2505.16789", "title": "Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability", "authors": ["Punya Syon Pandey", "Samuel Simko", "Kellin Pelrine", "Zhijing Jin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "As large language models gain popularity, their vulnerability to adversarial\nattacks remains a primary concern. While fine-tuning models on domain-specific\ndatasets is often employed to improve model performance, it can introduce\nvulnerabilities within the underlying model. In this work, we investigate\nAccidental Misalignment, unexpected vulnerabilities arising from\ncharacteristics of fine-tuning data. We begin by identifying potential\ncorrelation factors such as linguistic features, semantic similarity, and\ntoxicity within our experimental datasets. We then evaluate the adversarial\nperformance of these fine-tuned models and assess how dataset factors correlate\nwith attack success rates. Lastly, we explore potential causal links, offering\nnew insights into adversarial defense strategies and highlighting the crucial\nrole of dataset design in preserving model alignment. Our code is available at\nhttps://github.com/psyonp/accidental_misalignment.", "AI": {"tldr": "This paper investigates the vulnerabilities of fine-tuned large language models to adversarial attacks, focusing on accidental misalignment caused by the characteristics of fine-tuning data.", "motivation": "As fine-tuning models on domain-specific datasets becomes common, the paper aims to explore how such practices can inadvertently introduce vulnerabilities related to adversarial attacks.", "method": "The study identifies correlation factors in fine-tuning datasets, such as linguistic features, semantic similarity, and toxicity, before evaluating the adversarial performance of fine-tuned models and assessing how these factors relate to attack success rates.", "result": "The investigation reveals that specific characteristics of fine-tuning datasets significantly impact the success rates of adversarial attacks, providing insights into potential causal relationships.", "conclusion": "The findings underscore the importance of dataset design in maintaining model alignment and propose new defense strategies against adversarial attacks.", "key_contributions": ["Insights into accidental misalignment in fine-tuned models", "Identification of dataset characteristics affecting adversarial vulnerability", "Novel defense strategies related to dataset design"], "limitations": "", "keywords": ["adversarial attacks", "language models", "fine-tuning", "dataset design", "vulnerabilities"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.16800", "pdf": "https://arxiv.org/pdf/2505.16800.pdf", "abs": "https://arxiv.org/abs/2505.16800", "title": "Learning Beyond Limits: Multitask Learning and Synthetic Data for Low-Resource Canonical Morpheme Segmentation", "authors": ["Changbing Yang", "Garrett Nicolai"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce a transformer-based morpheme segmentation system that augments a\nlow-resource training signal through multitask learning and LLM-generated\nsynthetic data. Our framework jointly predicts morphological segments and\nglosses from orthographic input, leveraging shared linguistic representations\nobtained through a common documentary process to enhance model generalization.\nTo further address data scarcity, we integrate synthetic training data\ngenerated by large language models (LLMs) using in-context learning.\nExperimental results on the SIGMORPHON 2023 dataset show that our approach\nsignificantly improves word-level segmentation accuracy and morpheme-level\nF1-score across multiple low-resource languages.", "AI": {"tldr": "A transformer-based morpheme segmentation system enhances performance in low-resource languages using multitask learning and synthetic data from large language models.", "motivation": "To improve morpheme segmentation accuracy in low-resource languages where training data is scarce.", "method": "The proposed framework jointly predicts morphological segments and glosses using multitask learning and synthetic data generated by LLMs, enhancing model generalization.", "result": "Significant improvements in word-level segmentation accuracy and morpheme-level F1-score on the SIGMORPHON 2023 dataset across multiple low-resource languages were observed.", "conclusion": "The integration of LLM-generated synthetic data through in-context learning effectively addresses data scarcity challenges in morpheme segmentation tasks.", "key_contributions": ["Introduction of a transformer-based morpheme segmentation system.", "Use of multitask learning to enhance model performance.", "Integration of LLM-generated synthetic data for low-resource languages."], "limitations": "", "keywords": ["morpheme segmentation", "multitask learning", "synthetic data", "low-resource languages", "large language models"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.16806", "pdf": "https://arxiv.org/pdf/2505.16806.pdf", "abs": "https://arxiv.org/abs/2505.16806", "title": "Two-way Evidence self-Alignment based Dual-Gated Reasoning Enhancement", "authors": ["Kexin Zhang", "Junlan Chen", "Daifeng Li", "Yuxuan Zhang", "Yangyang Feng", "Bowen Deng", "Weixu Chen"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Large language models (LLMs) encounter difficulties in knowledge-intensive\nmulti-step reasoning (KIMSR) tasks. One challenge is how to effectively extract\nand represent rationale evidence. The current methods often extract\nsemantically relevant but logically irrelevant evidence, resulting in flawed\nreasoning and inaccurate responses. We propose a two-way evidence\nself-alignment (TW-ESA) module, which utilizes the mutual alignment between\nstrict reasoning and LLM reasoning to enhance its understanding of the causal\nlogic of evidence, thereby addressing the first challenge. Another challenge is\nhow to utilize the rationale evidence and LLM's intrinsic knowledge for\naccurate reasoning when the evidence contains uncertainty. We propose a\ndual-gated reasoning enhancement (DGR) module to gradually fuse useful\nknowledge of LLM within strict reasoning, which can enable the model to perform\naccurate reasoning by focusing on causal elements in the evidence and exhibit\ngreater robustness. The two modules are collaboratively trained in a unified\nframework ESA-DGR. Extensive experiments on three diverse and challenging KIMSR\ndatasets reveal that ESA-DGR significantly surpasses state-of-the-art LLM-based\nfine-tuning methods, with remarkable average improvements of 4% in exact match\n(EM) and 5% in F1 score. The implementation code is available at\nhttps://anonymous.4open.science/r/ESA-DGR-2BF8.", "AI": {"tldr": "The paper presents a new framework, ESA-DGR, to improve reasoning in large language models by aligning evidence and integrating LLM knowledge for better performance on knowledge-intensive reasoning tasks.", "motivation": "Large language models struggle with knowledge-intensive multi-step reasoning tasks due to ineffective evidence extraction and uncertainty in rationale evidence.", "method": "The proposed approach includes a two-way evidence self-alignment (TW-ESA) module for enhancing causal logic understanding and a dual-gated reasoning enhancement (DGR) module for fusing LLM knowledge with strict reasoning.", "result": "ESA-DGR outperforms state-of-the-art methods, achieving an average improvement of 4% in exact match and 5% in F1 score across three diverse KIMSR datasets.", "conclusion": "The integrated ESA-DGR framework significantly enhances the reasoning capabilities of LLMs on complex tasks, demonstrating its effectiveness in handling uncertainty and improving logical accuracy.", "key_contributions": ["Introduction of a two-way evidence self-alignment (TW-ESA) module.", "Development of a dual-gated reasoning enhancement (DGR) module.", "A unified framework (ESA-DGR) that outperforms existing fine-tuning methods."], "limitations": "", "keywords": ["large language models", "multi-step reasoning", "knowledge extraction", "causal logic", "uncertainty"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.16814", "pdf": "https://arxiv.org/pdf/2505.16814.pdf", "abs": "https://arxiv.org/abs/2505.16814", "title": "Does Synthetic Data Help Named Entity Recognition for Low-Resource Languages?", "authors": ["Gaurav Kamath", "Sowmya Vajjala"], "categories": ["cs.CL"], "comment": "pre-print", "summary": "Named Entity Recognition(NER) for low-resource languages aims to produce\nrobust systems for languages where there is limited labeled training data\navailable, and has been an area of increasing interest within NLP. Data\naugmentation for increasing the amount of low-resource labeled data is a common\npractice. In this paper, we explore the role of synthetic data in the context\nof multilingual, low-resource NER, considering 11 languages from diverse\nlanguage families. Our results suggest that synthetic data does in fact hold\npromise for low-resource language NER, though we see significant variation\nbetween languages.", "AI": {"tldr": "This paper investigates the effectiveness of synthetic data for enhancing Named Entity Recognition (NER) systems in low-resource languages.", "motivation": "The motivation is to improve NER systems for low-resource languages, where there is a scarcity of labeled training data.", "method": "The study evaluates the impact of synthetic data on NER performance across 11 low-resource languages from different language families.", "result": "The results indicate that synthetic data can positively influence NER performance in low-resource languages, but the effectiveness varies significantly across different languages.", "conclusion": "Overall, while synthetic data shows promise for low-resource NER, researchers should be aware of the variability in efficacy across languages.", "key_contributions": ["Exploration of synthetic data in low-resource NER contexts.", "Assessment across 11 diverse languages.", "Insights into the varying effectiveness of synthetic data for different languages."], "limitations": "The study does not address the long-term implications or generalizability of synthetic data methods across even broader language sets.", "keywords": ["Named Entity Recognition", "low-resource languages", "synthetic data", "multilingual", "NLP"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.16831", "pdf": "https://arxiv.org/pdf/2505.16831.pdf", "abs": "https://arxiv.org/abs/2505.16831", "title": "Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs", "authors": ["Xiaoyu Xu", "Xiang Yue", "Yang Liu", "Qingqing Ye", "Haibo Hu", "Minxin Du"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "44 pages", "summary": "Unlearning in large language models (LLMs) is intended to remove the\ninfluence of specific data, yet current evaluations rely heavily on token-level\nmetrics such as accuracy and perplexity. We show that these metrics can be\nmisleading: models often appear to forget, but their original behavior can be\nrapidly restored with minimal fine-tuning, revealing that unlearning may\nobscure information rather than erase it. To diagnose this phenomenon, we\nintroduce a representation-level evaluation framework using PCA-based\nsimilarity and shift, centered kernel alignment, and Fisher information.\nApplying this toolkit across six unlearning methods, three domains (text, code,\nmath), and two open-source LLMs, we uncover a critical distinction between\nreversible and irreversible forgetting. In reversible cases, models suffer\ntoken-level collapse yet retain latent features; in irreversible cases, deeper\nrepresentational damage occurs. We further provide a theoretical account\nlinking shallow weight perturbations near output layers to misleading\nunlearning signals, and show that reversibility is modulated by task type and\nhyperparameters. Our findings reveal a fundamental gap in current evaluation\npractices and establish a new diagnostic foundation for trustworthy unlearning\nin LLMs. We provide a unified toolkit for analyzing LLM representation changes\nunder unlearning and relearning:\nhttps://github.com/XiaoyuXU1/Representational_Analysis_Tools.git.", "AI": {"tldr": "This paper critiques current evaluation methods for unlearning in large language models, revealing that misleading performance metrics can obscure true model behavior, and proposes a novel representation-level evaluation framework.", "motivation": "To address the shortcomings of current evaluation metrics for unlearning in large language models, which may not adequately reflect the actual effects of unlearning processes.", "method": "The authors introduce a representation-level evaluation framework utilizing PCA-based similarity, centered kernel alignment, and Fisher information to assess unlearning methods across different domains and models.", "result": "The study identifies a critical distinction between reversible and irreversible unlearning, showing that while reversible unlearning leads to token-level collapse, latent features may remain intact; irreversible forgetting results in deeper representational damage.", "conclusion": "Current evaluation practices are fundamentally flawed, obscuring true model behavior after unlearning; a new diagnostic framework is established for trustworthy unlearning in LLMs.", "key_contributions": ["Introduction of a new representation-level evaluation framework for LLM unlearning", "Identification of the distinction between reversible and irreversible forgetting", "Provision of a comprehensive toolkit for analyzing representation changes in LLMs during unlearning"], "limitations": "The study focuses on specific domains and models, which may not generalize to all LLMs or tasks.", "keywords": ["unlearning", "large language models", "representation-level evaluation", "irreversible forgetting", "model behavior"], "importance_score": 8, "read_time_minutes": 44}}
{"id": "2505.16834", "pdf": "https://arxiv.org/pdf/2505.16834.pdf", "abs": "https://arxiv.org/abs/2505.16834", "title": "SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis", "authors": ["Shuang Sun", "Huatong Song", "Yuhao Wang", "Ruiyang Ren", "Jinhao Jiang", "Junjie Zhang", "Fei Bai", "Jia Deng", "Wayne Xin Zhao", "Zheng Liu", "Lei Fang", "Zhongyuan Wang", "Ji-Rong Wen"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Retrieval-augmented generation (RAG) systems have advanced large language\nmodels (LLMs) in complex deep search scenarios requiring multi-step reasoning\nand iterative information retrieval. However, existing approaches face critical\nlimitations that lack high-quality training trajectories or suffer from the\ndistributional mismatches in simulated environments and prohibitive\ncomputational costs for real-world deployment. This paper introduces\nSimpleDeepSearcher, a lightweight yet effective framework that bridges this gap\nthrough strategic data engineering rather than complex training paradigms. Our\napproach synthesizes high-quality training data by simulating realistic user\ninteractions in live web search environments, coupled with a multi-criteria\ncuration strategy that optimizes the diversity and quality of input and output\nside. Experiments on five benchmarks across diverse domains demonstrate that\nSFT on only 871 curated samples yields significant improvements over RL-based\nbaselines. Our work establishes SFT as a viable pathway by systematically\naddressing the data-scarce bottleneck, offering practical insights for\nefficient deep search systems. Our code is available at\nhttps://github.com/RUCAIBox/SimpleDeepSearcher.", "AI": {"tldr": "This paper presents SimpleDeepSearcher, a framework that enhances RAG systems by synthesizing high-quality training data through realistic user interaction simulations and a multi-criteria curation strategy.", "motivation": "To address the limitations of existing retrieval-augmented generation approaches related to training data quality, distributional mismatches, and computational costs.", "method": "SimpleDeepSearcher employs strategic data engineering to simulate realistic user interactions and curate diverse and high-quality training samples.", "result": "Experiments show that supervised fine-tuning (SFT) on 871 curated samples significantly outperforms reinforcement learning-based baselines across five benchmarks in various domains.", "conclusion": "The work demonstrates that SFT can effectively address data scarcity issues in deep search systems, providing insights for practical applications.", "key_contributions": ["Introduction of SimpleDeepSearcher framework for RAG systems.", "Development of a strategy for synthesizing high-quality training data via user simulation.", "Demonstration of significant performance improvements with minimal curated sample usage."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Large Language Models", "Data Engineering"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.16838", "pdf": "https://arxiv.org/pdf/2505.16838.pdf", "abs": "https://arxiv.org/abs/2505.16838", "title": "R1-Compress: Long Chain-of-Thought Compression via Chunk Compression and Search", "authors": ["Yibo Wang", "Li Shen", "Huanjin Yao", "Tiansheng Huang", "Rui Liu", "Naiqiang Tan", "Jiaxing Huang", "Kai Zhang", "Dacheng Tao"], "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-Thought (CoT) reasoning enhances large language models (LLMs) by\nenabling step-by-step problem-solving, yet its extension to Long-CoT introduces\nsubstantial computational overhead due to increased token length. Existing\ncompression approaches -- instance-level and token-level -- either sacrifice\nessential local reasoning signals like reflection or yield incoherent outputs.\nTo address these limitations, we propose R1-Compress, a two-stage chunk-level\ncompression framework that preserves both local information and coherence. Our\nmethod segments Long-CoT into manageable chunks, applies LLM-driven inner-chunk\ncompression, and employs an inter-chunk search mechanism to select the short\nand coherent sequence. Experiments on Qwen2.5-Instruct models across MATH500,\nAIME24, and GPQA-Diamond demonstrate that R1-Compress significantly reduces\ntoken usage while maintaining comparable reasoning accuracy. On MATH500,\nR1-Compress achieves an accuracy of 92.4%, with only a 0.6% drop compared to\nthe Long-CoT baseline, while reducing token usage by about 20%. Source code\nwill be available at https://github.com/w-yibo/R1-Compress", "AI": {"tldr": "R1-Compress is a two-stage framework for compressing Long-CoT reasoning in LLMs, preserving local information and coherence while reducing token usage.", "motivation": "To enhance Chain-of-Thought reasoning in large language models while overcoming computational overhead and local reasoning signal loss associated with existing compression methods.", "method": "R1-Compress segments Long-CoT into manageable chunks and applies LLM-driven inner-chunk compression alongside an inter-chunk search mechanism for coherent output.", "result": "R1-Compress significantly reduces token usage by about 20% while maintaining reasoning accuracy, achieving 92.4% accuracy on MATH500 with only a 0.6% drop compared to the Long-CoT baseline.", "conclusion": "R1-Compress effectively balances the trade-off between compression and reasoning accuracy, making it a promising approach for improving LLM efficiency.", "key_contributions": ["Introduction of a two-stage chunk-level compression framework for Long-CoT reasoning.", "Preservation of local reasoning signals and coherence during compression.", "Demonstrated significant reduction in token usage with maintained accuracy across multiple benchmarks."], "limitations": "", "keywords": ["Chain-of-Thought", "large language models", "compression framework", "local reasoning", "coherence"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2505.16847", "pdf": "https://arxiv.org/pdf/2505.16847.pdf", "abs": "https://arxiv.org/abs/2505.16847", "title": "Understanding and Analyzing Inappropriately Targeting Language in Online Discourse: A Comparative Annotation Study", "authors": ["Baran Barbarestani", "Isa Maks", "Piek Vossen"], "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces a method for detecting inappropriately targeting\nlanguage in online conversations by integrating crowd and expert annotations\nwith ChatGPT. We focus on English conversation threads from Reddit, examining\ncomments that target individuals or groups. Our approach involves a\ncomprehensive annotation framework that labels a diverse data set for various\ntarget categories and specific target words within the conversational context.\nWe perform a comparative analysis of annotations from human experts, crowd\nannotators, and ChatGPT, revealing strengths and limitations of each method in\nrecognizing both explicit hate speech and subtler discriminatory language. Our\nfindings highlight the significant role of contextual factors in identifying\nhate speech and uncover new categories of targeting, such as social belief and\nbody image. We also address the challenges and subjective judgments involved in\nannotation and the limitations of ChatGPT in grasping nuanced language. This\nstudy provides insights for improving automated content moderation strategies\nto enhance online safety and inclusivity.", "AI": {"tldr": "This paper presents a method utilizing ChatGPT to detect inappropriate targeting language in online conversations, focusing on Reddit comments.", "motivation": "To enhance online safety and inclusivity by improving the detection of inappropriate targeting language in digital conversations.", "method": "The study uses a comprehensive annotation framework that combines human expert, crowd, and ChatGPT annotations to analyze English conversation threads from Reddit for various categories of targeting language.", "result": "The comparative analysis shows the strengths and limitations of different annotation methods while revealing contextual factors significant in identifying hate speech and new targeting categories.", "conclusion": "The findings suggest that improving automated content moderation strategies is crucial for better online safety and inclusivity, acknowledging the subjective challenges in language interpretation.", "key_contributions": ["Development of a comprehensive annotation framework for targeting language", "Comparison of human, crowd, and AI annotations in identifying hate speech", "Identification of new targeting categories like social belief and body image"], "limitations": "Challenges in subjective judgments during annotation and limitations in ChatGPT's understanding of nuanced language.", "keywords": ["hate speech", "ChatGPT", "online safety", "content moderation", "targeted language"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.16855", "pdf": "https://arxiv.org/pdf/2505.16855.pdf", "abs": "https://arxiv.org/abs/2505.16855", "title": "Nested Named Entity Recognition as Single-Pass Sequence Labeling", "authors": ["Alberto Muñoz-Ortiz", "David Vilares", "Caio COrro", "Carlos Gómez-Rodríguez"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "Submitted to EMNLP 2025", "summary": "We cast nested named entity recognition (NNER) as a sequence labeling task by\nleveraging prior work that linearizes constituency structures, effectively\nreducing the complexity of this structured prediction problem to\nstraightforward token classification. By combining these constituency\nlinearizations with pretrained encoders, our method captures nested entities\nwhile performing exactly $n$ tagging actions. Our approach achieves competitive\nperformance compared to less efficient systems, and it can be trained using any\noff-the-shelf sequence labeling library.", "AI": {"tldr": "This paper presents a method for nested named entity recognition (NNER) by reformulating it as a token classification problem, utilizing constituency linearizations and pretrained encoders to achieve competitive performance.", "motivation": "The complexity of nested named entity recognition necessitates an efficient method to handle structured prediction, making it accessible to standard sequence labeling techniques.", "method": "The paper proposes a new approach that linearizes constituency structures for NNER, reducing it to a sequence labeling task that can leverage pretrained encoders.", "result": "The proposed method demonstrates competitive performance when compared to more complex systems, while also using standard sequence labeling libraries for training.", "conclusion": "The method effectively captures nested entities with efficiency, achieving satisfactory results in the NNER task.", "key_contributions": ["Introduces a novel approach to NNER by reformulating it as a token classification task.", "Demonstrates the use of constituency linearizations to simplify nested entity recognition.", "Achieves competitive performance using standard sequence labeling techniques."], "limitations": "", "keywords": ["nested named entity recognition", "sequence labeling", "constituency linearization"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.16868", "pdf": "https://arxiv.org/pdf/2505.16868.pdf", "abs": "https://arxiv.org/abs/2505.16868", "title": "Comparative analysis of subword tokenization approaches for Indian languages", "authors": ["Sudhansu Bala Das", "Samujjal Choudhury", "Tapas Kumar Mishra", "Bidyut Kr. Patra"], "categories": ["cs.CL"], "comment": "24 pages, 4 tables", "summary": "Tokenization is the act of breaking down text into smaller parts, or tokens,\nthat are easier for machines to process. This is a key phase in machine\ntranslation (MT) models. Subword tokenization enhances this process by breaking\ndown words into smaller subword units, which is especially beneficial in\nlanguages with complicated morphology or a vast vocabulary. It is useful in\ncapturing the intricate structure of words in Indian languages (ILs), such as\nprefixes, suffixes, and other morphological variations. These languages\nfrequently use agglutinative structures, in which words are formed by the\ncombination of multiple morphemes such as suffixes, prefixes, and stems. As a\nresult, a suitable tokenization strategy must be chosen to address these\nscenarios. This paper examines how different subword tokenization techniques,\nsuch as SentencePiece, Byte Pair Encoding (BPE), and WordPiece Tokenization,\naffect ILs. The effectiveness of these subword tokenization techniques is\ninvestigated in statistical, neural, and multilingual neural machine\ntranslation models. All models are examined using standard evaluation metrics,\nsuch as the Bilingual Evaluation Understudy (BLEU) score, TER, METEOR, CHRF,\nRIBES, and COMET. Based on the results, it appears that for the majority of\nlanguage pairs for the Statistical and Neural MT models, the SentencePiece\ntokenizer continuously performed better than other tokenizers in terms of BLEU\nscore. However, BPE tokenization outperformed other tokenization techniques in\nthe context of Multilingual Neural Machine Translation model. The results show\nthat, despite using the same tokenizer and dataset for each model, translations\nfrom ILs to English surpassed translations from English to ILs.", "AI": {"tldr": "This paper reviews various subword tokenization methods and their impact on machine translation for Indian languages, highlighting the advantages of SentencePiece and BPE based on empirical results.", "motivation": "To address challenges in machine translation for Indian languages by employing effective subword tokenization techniques that account for their complex morphological structures.", "method": "The paper compares the performance of SentencePiece, Byte Pair Encoding (BPE), and WordPiece tokenization across statistical and neural machine translation models using standard metrics like BLEU, TER, METEOR, and others.", "result": "SentencePiece consistently outperformed other tokenization methods in statistical and neural MT models, while BPE was most effective in multilingual neural MT scenarios.", "conclusion": "The study indicates that, regardless of tokenizer used, translations from Indian languages to English were more successful than vice versa, suggesting a need for tailored approaches in machine translation.", "key_contributions": ["Comprehensive analysis of tokenization strategies for Indian languages.", "Empirical evaluation of performance across multiple machine translation models.", "Identification of optimal tokenization methods for different translation contexts."], "limitations": "", "keywords": ["Tokenization", "Machine Translation", "Indian Languages", "Subword Units", "Natural Language Processing"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.16869", "pdf": "https://arxiv.org/pdf/2505.16869.pdf", "abs": "https://arxiv.org/abs/2505.16869", "title": "MPO: Multilingual Safety Alignment via Reward Gap Optimization", "authors": ["Weixiang Zhao", "Yulin Hu", "Yang Deng", "Tongtong Wu", "Wenxuan Zhang", "Jiahe Guo", "An Zhang", "Yanyan Zhao", "Bing Qin", "Tat-Seng Chua", "Ting Liu"], "categories": ["cs.CL"], "comment": "To Appear at ACL 2025 (Main)", "summary": "Large language models (LLMs) have become increasingly central to AI\napplications worldwide, necessitating robust multilingual safety alignment to\nensure secure deployment across diverse linguistic contexts. Existing\npreference learning methods for safety alignment, such as RLHF and DPO, are\nprimarily monolingual and struggle with noisy multilingual data. To address\nthese limitations, we introduce Multilingual reward gaP Optimization (MPO), a\nnovel approach that leverages the well-aligned safety capabilities of the\ndominant language (English) to improve safety alignment across multiple\nlanguages. MPO directly minimizes the reward gap difference between the\ndominant language and target languages, effectively transferring safety\ncapabilities while preserving the original strengths of the dominant language.\nExtensive experiments on three LLMs, LLaMA-3.1, Gemma-2 and Qwen2.5, validate\nMPO's efficacy in multilingual safety alignment without degrading general\nmultilingual utility.", "AI": {"tldr": "MPO enhances multilingual safety alignment of LLMs by leveraging English as a proxy to improve safety across multiple languages.", "motivation": "Robust multilingual safety alignment is essential for the secure deployment of LLMs in diverse linguistic contexts, as existing methods struggle with noisy multilingual data.", "method": "The paper introduces Multilingual reward gaP Optimization (MPO), which minimizes the reward gap between a dominant language (English) and target languages to transfer safety capabilities without losing the strengths of the dominant language.", "result": "MPO demonstrates improved safety alignment in multilingual contexts across three LLMs, validating its effectiveness through extensive experiments.", "conclusion": "MPO provides a solution for enhancing multilingual safety alignment in LLMs, maintaining their overall utility while improving safety across languages.", "key_contributions": ["Introduction of Multilingual reward gaP Optimization (MPO) for LLM safety alignment", "Effective transfer of safety capabilities from a dominant language to target languages", "Demonstration of MPO's efficacy with extensive experiments on LLaMA-3.1, Gemma-2, and Qwen2.5."], "limitations": "", "keywords": ["Language Models", "Safety Alignment", "Multilingual Systems", "Preference Learning", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.16881", "pdf": "https://arxiv.org/pdf/2505.16881.pdf", "abs": "https://arxiv.org/abs/2505.16881", "title": "CASTILLO: Characterizing Response Length Distributions of Large Language Models", "authors": ["Daniel F. Perez-Ramirez", "Dejan Kostic", "Magnus Boman"], "categories": ["cs.CL", "cs.AI"], "comment": "Dataset available in\n  https://huggingface.co/datasets/danfperam/castillo and code is available in\n  https://github.com/DanielFPerez/castillo", "summary": "Efficiently managing compute resources for Large Language Model (LLM)\ninference remains challenging due to the inherently stochastic and variable\nlengths of autoregressive text generation. Accurately estimating response\nlengths in advance enables proactive resource allocation, yet existing\napproaches either bias text generation towards certain lengths or rely on\nassumptions that ignore model- and prompt-specific variability. We introduce\nCASTILLO, a dataset characterizing response length distributions across 13\nwidely-used open-source LLMs evaluated on seven distinct instruction-following\ncorpora. For each $\\langle$prompt, model$\\rangle$ sample pair, we generate 10\nindependent completions using fixed decoding hyper-parameters, record the token\nlength of each response, and publish summary statistics (mean, std-dev,\npercentiles), along with the shortest and longest completions, and the exact\ngeneration settings. Our analysis reveals significant inter- and intra-model\nvariability in response lengths (even under identical generation settings), as\nwell as model-specific behaviors and occurrences of partial text degeneration\nin only subsets of responses. CASTILLO enables the development of predictive\nmodels for proactive scheduling and provides a systematic framework for\nanalyzing model-specific generation behaviors. We publicly release the dataset\nand code to foster research at the intersection of generative language modeling\nand systems.", "AI": {"tldr": "The paper introduces CASTILLO, a dataset that characterizes response length distributions across various LLMs to improve resource management for LLM inference.", "motivation": "Efficient management of compute resources for LLM inference is difficult due to variability in response lengths during text generation.", "method": "The dataset CASTILLO was created by generating 10 completions for various prompt and model pairs and analyzing the response length distributions statistically.", "result": "The analysis shows significant variability in response lengths both between and within models, revealing model-specific behaviors.", "conclusion": "CASTILLO facilitates the development of predictive models for resource scheduling and aids in the understanding of model-specific generation behaviors.", "key_contributions": ["Development of the CASTILLO dataset for response length analysis.", "Insight into inter- and intra-model variability during text generation.", "Release of tools for continued research in generative language modeling."], "limitations": "", "keywords": ["Large Language Models", "Dataset", "Response Length Distributions", "Predictive Modeling", "Generative Language Modeling"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.16894", "pdf": "https://arxiv.org/pdf/2505.16894.pdf", "abs": "https://arxiv.org/abs/2505.16894", "title": "Shadows in the Attention: Contextual Perturbation and Representation Drift in the Dynamics of Hallucination in LLMs", "authors": ["Zeyu Wei", "Shuo Wang", "Xiaohui Rong", "Xuemin Liu", "He Li"], "categories": ["cs.CL"], "comment": null, "summary": "Hallucinations -- plausible yet erroneous outputs -- remain a critical\nbarrier to reliable deployment of large language models (LLMs). We present the\nfirst systematic study linking hallucination incidence to internal-state drift\ninduced by incremental context injection. Using TruthfulQA, we construct two\n16-round \"titration\" tracks per question: one appends relevant but partially\nflawed snippets, the other injects deliberately misleading content. Across six\nopen-source LLMs, we track overt hallucination rates with a tri-perspective\ndetector and covert dynamics via cosine, entropy, JS and Spearman drifts of\nhidden states and attention maps. Results reveal (1) monotonic growth of\nhallucination frequency and representation drift that plateaus after 5--7\nrounds; (2) relevant context drives deeper semantic assimilation, producing\nhigh-confidence \"self-consistent\" hallucinations, whereas irrelevant context\ninduces topic-drift errors anchored by attention re-routing; and (3)\nconvergence of JS-Drift ($\\sim0.69$) and Spearman-Drift ($\\sim0$) marks an\n\"attention-locking\" threshold beyond which hallucinations solidify and become\nresistant to correction. Correlation analyses expose a seesaw between\nassimilation capacity and attention diffusion, clarifying size-dependent error\nmodes. These findings supply empirical foundations for intrinsic hallucination\nprediction and context-aware mitigation mechanisms.", "AI": {"tldr": "This study systematically investigates the relationship between hallucinations in large language models (LLMs) and internal state changes caused by the injection of incremental context.", "motivation": "Understanding hallucinations in LLMs is critical for reliable applications, especially in health informatics and other AI-related fields.", "method": "The authors constructed two titration tracks for each question using TruthfulQA, measuring hallucination rates and monitoring internal states and attention maps across six open-source LLMs.", "result": "The study found that hallucination frequency increases with incremental context up to a threshold, where self-consistent hallucinations occur with high confidence; also, attention-locking thresholds mark a point beyond which hallucinations are more resistant to correction.", "conclusion": "These insights provide a foundation for better prediction and mitigation of hallucinations in LLMs, with implications for their reliable deployment in applications.", "key_contributions": ["First systematic study linking hallucination to internal-state drift", "Identification of key correlation between context injection and hallucination solidification", "Empirical basis for intrinsic hallucination prediction methods"], "limitations": "Focus on specific LLM models might limit generalizability to all models.", "keywords": ["hallucinations", "large language models", "context injection", "attention dynamics", "error modes"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.16900", "pdf": "https://arxiv.org/pdf/2505.16900.pdf", "abs": "https://arxiv.org/abs/2505.16900", "title": "Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality", "authors": ["Jintian Shao", "Hongyi Huang", "Jiayi Wu", "Beiwen Zhang", "ZhiYu Wu", "You Shan", "MingKai Zheng"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "During the finetuning stage of text generation tasks, standard cross-entropy\nloss treats all tokens equally. This can lead models to overemphasize\nhigh-frequency, low-information tokens, neglecting lower-frequency tokens\ncrucial for specificity and informativeness in generated content. This paper\nintroduces a novel loss function, Power-Law Decay Loss (PDL), specifically\ndesigned to optimize the finetuning process for text generation. The core\nmotivation for PDL stems from observations in information theory and\nlinguistics: the informativeness of a token is often inversely proportional to\nits frequency of occurrence. PDL re-weights the contribution of each token in\nthe standard cross-entropy loss based on its frequency in the training corpus,\nfollowing a power-law decay. Specifically, the weights for high-frequency\ntokens are reduced, while low-frequency, information-dense tokens are assigned\nhigher weights. This mechanism guides the model during finetuning to focus more\non learning and generating tokens that convey specific and unique information,\nthereby enhancing the quality, diversity, and informativeness of the generated\ntext. We theoretically elaborate on the motivation and construction of PDL and\ndiscuss its potential applications and advantages across various text\ngeneration finetuning tasks, such as abstractive summarization, dialogue\nsystems, and style transfer.", "AI": {"tldr": "A novel loss function, Power-Law Decay Loss (PDL), enhances text generation by re-weighting tokens based on their frequency, focusing on low-frequency, informative tokens during finetuning.", "motivation": "Standard cross-entropy loss in text generation neglects low-frequency tokens, leading to less informative outputs.", "method": "The Power-Law Decay Loss re-weights token contributions in loss calculation based on their frequency, reducing the weights of high-frequency tokens and increasing those of low-frequency tokens.", "result": "PDL improves the quality, diversity, and informativeness of generated text by ensuring models focus on more specific tokens that convey unique information.", "conclusion": "The paper demonstrates that PDL is beneficial for various text generation tasks, shedding light on ways to enhance model performance in generating rich and informative content.", "key_contributions": ["Introduction of the Power-Law Decay Loss (PDL) for text generation", "Theoretical foundations supporting the motivation of PDL", "Applications of PDL in tasks like summarization, dialogue systems, and style transfer."], "limitations": "", "keywords": ["Power-Law Decay Loss", "text generation", "cross-entropy", "informativeness", "token frequency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.16922", "pdf": "https://arxiv.org/pdf/2505.16922.pdf", "abs": "https://arxiv.org/abs/2505.16922", "title": "UNCLE: Uncertainty Expressions in Long-Form Generation", "authors": ["Ruihan Yang", "Caiqi Zhang", "Zhisong Zhang", "Xinting Huang", "Dong Yu", "Nigel Collier", "Deqing Yang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are prone to hallucination, particularly in\nlong-form generations. A promising direction to mitigate hallucination is to\nteach LLMs to express uncertainty explicitly when they lack sufficient\nknowledge. However, existing work lacks direct and fair evaluation of LLMs'\nability to express uncertainty effectively in long-form generation. To address\nthis gap, we first introduce UNCLE, a benchmark designed to evaluate\nuncertainty expression in both long- and short-form question answering (QA).\nUNCLE spans five domains and comprises 4k long-form QA instances and over 20k\nshort-form QA pairs. Our dataset is the first to directly bridge short- and\nlong-form QA with paired questions and gold-standard answers. Along with the\nbenchmark, we propose a suite of new metrics to assess the models' capabilities\nto selectively express uncertainty. Using UNCLE, we then demonstrate that\ncurrent models fail to convey uncertainty appropriately in long-form\ngeneration. We further explore both prompt-based and training-based methods to\nimprove models' performance, with the training-based methods yielding greater\ngains. Further analysis of alignment gaps between short- and long-form\nuncertainty expression highlights promising directions for future research\nusing UNCLE.", "AI": {"tldr": "Introducing UNCLE, a novel benchmark for evaluating uncertainty expression in LLMs during long- and short-form question answering, revealing current limitations and suggesting methods for improvement.", "motivation": "Despite the growing use of LLMs, the issue of hallucination, especially in long-form generation, remains largely unaddressed. This paper aims to evaluate and enhance the capability of LLMs to express uncertainty effectively.", "method": "Development of the UNCLE benchmark, encompassing 4k long-form and over 20k short-form QA instances, along with new metrics to assess uncertainty expression in LLM outputs.", "result": "Current LLMs are shown to inadequately express uncertainty in long-form generation. Training-based improvement methods outperform prompt-based methods in addressing this issue.", "conclusion": "UNCLE serves as a critical tool for evaluating and enhancing LLMs' ability to express uncertainty, guiding future research in this area.", "key_contributions": ["Introduction of the UNCLE benchmark for uncertainty evaluation in LLMs", "Development of metrics to assess models' uncertainty expression", "Demonstration of models' limitations in uncertainty expression and potential enhancement strategies"], "limitations": "Study focuses primarily on LLMs and may not generalize to other AI models; further practical implementations of proposed methods need exploration.", "keywords": ["Large Language Models", "uncertainty expression", "benchmark", "long-form QA", "short-form QA"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.16927", "pdf": "https://arxiv.org/pdf/2505.16927.pdf", "abs": "https://arxiv.org/abs/2505.16927", "title": "Latent Principle Discovery for Language Model Self-Improvement", "authors": ["Keshav Ramji", "Tahira Naseem", "Ramón Fernandez Astudillo"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "When language model (LM) users aim to improve the quality of its generations,\nit is crucial to specify concrete behavioral attributes that the model should\nstrive to reflect. However, curating such principles across many domains, even\nnon-exhaustively, requires a labor-intensive annotation process. To automate\nthis process, we propose eliciting these latent attributes guiding model\nreasoning towards human-preferred responses by explicitly modeling them in a\nself-correction setting. Our approach mines new principles from the LM itself\nand compresses the discovered elements to an interpretable set via clustering.\nSpecifically, we employ an approximation of posterior-regularized Monte Carlo\nExpectation-Maximization to both identify a condensed set of the most effective\nlatent principles and teach the LM to strategically invoke them in order to\nintrinsically refine its responses. We demonstrate that bootstrapping our\nalgorithm over multiple iterations enables smaller language models (7-8B\nparameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an\naverage of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on\nIFEval. We also show that clustering the principles yields interpretable and\ndiverse model-generated constitutions while retaining model performance. The\ngains our method achieves highlight the potential of automated,\nprinciple-driven post-training recipes toward continual self-improvement.", "AI": {"tldr": "The paper presents a method for automating the curation of behavioral principles for language models by eliciting latent attributes and using self-correction to enhance model outputs.", "motivation": "Improving the quality of language model generations by specifying concrete behavioral attributes is labor-intensive. Automating this process can enhance the usability of models across various domains.", "method": "The authors propose a self-correction approach that uses posterior-regularized Monte Carlo Expectation-Maximization to identify and cluster latent attributes from the language model, allowing it to refine its responses based on these principles.", "result": "The proposed method demonstrates improvements in smaller language models (7-8B parameters), achieving significant upticks in performance metrics: +8-10% in AlpacaEval win-rate, +0.3 in MT-Bench, and +19-23% in principle-following win-rate on IFEval.", "conclusion": "Automating principle-driven post-training processes can lead to continual self-improvement of language models, providing an interpretable framework for model enhancements.", "key_contributions": ["Elicitation of latent attributes from the language model.", "Implementation of a self-correction mechanism for response refinement.", "Demonstration of significant performance improvements in smaller language models."], "limitations": "", "keywords": ["Language Models", "Self-Correction", "Behavioral Attributes", "Automated Annotation", "Machine Learning"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.16931", "pdf": "https://arxiv.org/pdf/2505.16931.pdf", "abs": "https://arxiv.org/abs/2505.16931", "title": "PIIvot: A Lightweight NLP Anonymization Framework for Question-Anchored Tutoring Dialogues", "authors": ["Matthew Zent", "Digory Smith", "Simon Woodhead"], "categories": ["cs.CL"], "comment": "6 pages, 2 figures, submitted to EMNLP 2025, for associated dataset,\n  see\n  https://huggingface.co/datasets/Eedi/Question-Anchored-Tutoring-Dialogues-2k", "summary": "Personally identifiable information (PII) anonymization is a high-stakes task\nthat poses a barrier to many open-science data sharing initiatives. While PII\nidentification has made large strides in recent years, in practice, error\nthresholds and the recall/precision trade-off still limit the uptake of these\nanonymization pipelines. We present PIIvot, a lighter-weight framework for PII\nanonymization that leverages knowledge of the data context to simplify the PII\ndetection problem. To demonstrate its effectiveness, we also contribute\nQATD-2k, the largest open-source real-world tutoring dataset of its kind, to\nsupport the demand for quality educational dialogue data.", "AI": {"tldr": "PIIvot is a framework aimed at simplifying the PII anonymization process by using data context for detection, supported by the QATD-2k educational dataset.", "motivation": "The high stakes of PII anonymization hinder data sharing in open science. Existing PII identification methods have limitations in error thresholds, recall, and precision, impacting their adoption.", "method": "PIIvot simplifies the PII detection problem by leveraging knowledge of the data context, making it a lighter-weight framework for PII anonymization.", "result": "PIIvot demonstrates improved effectiveness in the PII anonymization task. The QATD-2k dataset is introduced as a supporting resource, being the largest open-source real-world tutoring dataset available.", "conclusion": "PIIvot provides a significant advancement in PII anonymization, facilitating better data sharing in educational contexts through improved pipeline performance and the contribution of a comprehensive dataset.", "key_contributions": ["Development of the PIIvot anonymization framework", "Introduction of the QATD-2k open-source dataset", "Improvement in PII detection leveraging data context"], "limitations": "", "keywords": ["PII anonymization", "data sharing", "educational dataset"], "importance_score": 6, "read_time_minutes": 6}}
{"id": "2505.16934", "pdf": "https://arxiv.org/pdf/2505.16934.pdf", "abs": "https://arxiv.org/abs/2505.16934", "title": "In-Context Watermarks for Large Language Models", "authors": ["Yepeng Liu", "Xuandong Zhao", "Christopher Kruegel", "Dawn Song", "Yuheng Bu"], "categories": ["cs.CL"], "comment": null, "summary": "The growing use of large language models (LLMs) for sensitive applications\nhas highlighted the need for effective watermarking techniques to ensure the\nprovenance and accountability of AI-generated text. However, most existing\nwatermarking methods require access to the decoding process, limiting their\napplicability in real-world settings. One illustrative example is the use of\nLLMs by dishonest reviewers in the context of academic peer review, where\nconference organizers have no access to the model used but still need to detect\nAI-generated reviews. Motivated by this gap, we introduce In-Context\nWatermarking (ICW), which embeds watermarks into generated text solely through\nprompt engineering, leveraging LLMs' in-context learning and\ninstruction-following abilities. We investigate four ICW strategies at\ndifferent levels of granularity, each paired with a tailored detection method.\nWe further examine the Indirect Prompt Injection (IPI) setting as a specific\ncase study, in which watermarking is covertly triggered by modifying input\ndocuments such as academic manuscripts. Our experiments validate the\nfeasibility of ICW as a model-agnostic, practical watermarking approach.\nMoreover, our findings suggest that as LLMs become more capable, ICW offers a\npromising direction for scalable and accessible content attribution.", "AI": {"tldr": "This paper presents In-Context Watermarking (ICW), a new watermarking technique for AI-generated text using prompt engineering, allowing for detection without access to model decoding.", "motivation": "The need for effective watermarking techniques in the context of large language models (LLMs) for applications such as academic peer review, where access to the AI model is often unavailable.", "method": "The authors introduce four ICW strategies that embed watermarks in generated text through prompt engineering and pair them with tailored detection methods.", "result": "Experiments demonstrate the efficacy of ICW as a model-agnostic watermarking approach that is practical and performs well in detection scenarios.", "conclusion": "ICW provides a promising method for scalable and accessible content attribution in AI-generated text, especially as LLM capabilities advance.", "key_contributions": ["Introduction of In-Context Watermarking (ICW) technique", "Development of multiple ICW strategies paired with detection methods", "Validation of ICW's feasibility in real-world applications"], "limitations": "", "keywords": ["watermarking", "large language models", "text generation", "prompt engineering", "content attribution"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.16956", "pdf": "https://arxiv.org/pdf/2505.16956.pdf", "abs": "https://arxiv.org/abs/2505.16956", "title": "On Multilingual Encoder Language Model Compression for Low-Resource Languages", "authors": ["Daniil Gurgurov", "Michal Gregor", "Josef van Genabith", "Simon Ostermann"], "categories": ["cs.CL"], "comment": "Pre-print", "summary": "In this paper, we combine two-step knowledge distillation, structured\npruning, truncation, and vocabulary trimming for extremely compressing\nmultilingual encoder-only language models for low-resource languages. Our novel\napproach systematically combines existing techniques and takes them to the\nextreme, reducing layer depth, feed-forward hidden size, and intermediate layer\nembedding size to create significantly smaller monolingual models while\nretaining essential language-specific knowledge. We achieve compression rates\nof up to 92% with only a marginal performance drop of 2-10% in four downstream\ntasks, including sentiment analysis, topic classification, named entity\nrecognition, and part-of-speech tagging, across three low-resource languages.\nNotably, the performance degradation correlates with the amount of\nlanguage-specific data in the teacher model, with larger datasets resulting in\nsmaller performance losses. Additionally, we conduct extensive ablation studies\nto identify best practices for multilingual model compression using these\ntechniques.", "AI": {"tldr": "This paper presents a novel approach to compress multilingual encoder-only language models for low-resource languages using a combination of techniques, achieving significant size reduction with minimal performance loss.", "motivation": "The need for efficient language models that perform well in low-resource languages while being significantly smaller and faster to deploy.", "method": "A systematic combination of two-step knowledge distillation, structured pruning, truncation, and vocabulary trimming to compress language models.", "result": "Achieved compression rates of up to 92% with only a marginal performance drop of 2-10% in downstream tasks across three low-resource languages.", "conclusion": "The proposed methods demonstrate that significant model compression is feasible while retaining essential language-specific knowledge, especially when using larger datasets.", "key_contributions": ["Combines multiple compression techniques for language models", "Achieves high compression rates with minimal performance loss", "Identifies best practices for multilingual model compression through ablation studies."], "limitations": "Performance degradation varies with the amount of language-specific data in the teacher model, which may limit applicability in extremely low-data scenarios.", "keywords": ["multilingual models", "knowledge distillation", "model compression", "low-resource languages", "language processing"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2505.16965", "pdf": "https://arxiv.org/pdf/2505.16965.pdf", "abs": "https://arxiv.org/abs/2505.16965", "title": "BP-Seg: A graphical model approach to unsupervised and non-contiguous text segmentation using belief propagation", "authors": ["Fengyi Li", "Kayhan Behdin", "Natesh Pillai", "Xiaofeng Wang", "Zhipeng Wang", "Ercan Yildiz"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Text segmentation based on the semantic meaning of sentences is a fundamental\ntask with broad utility in many downstream applications. In this paper, we\npropose a graphical model-based unsupervised learning approach, named BP-Seg\nfor efficient text segmentation. Our method not only considers local coherence,\ncapturing the intuition that adjacent sentences are often more related, but\nalso effectively groups sentences that are distant in the text yet semantically\nsimilar. This is achieved through belief propagation on the carefully\nconstructed graphical models. Experimental results on both an illustrative\nexample and a dataset with long-form documents demonstrate that our method\nperforms favorably compared to competing approaches.", "AI": {"tldr": "This paper introduces BP-Seg, an unsupervised graphical model approach for text segmentation that improves upon existing methods by accounting for semantic relatedness among sentences.", "motivation": "Text segmentation is essential for various applications, necessitating improved methods that capture both local coherence and distant semantic similarities.", "method": "The proposed BP-Seg uses belief propagation on graphical models to determine text segments based on semantic meaning, integrating local and distant sentence relationships.", "result": "Experimental results indicate that BP-Seg outperforms competitive approaches in segmenting long-form documents based on semantic coherence.", "conclusion": "The BP-Seg method presents a significant advancement in unsupervised text segmentation by leveraging semantic relationships, offering improved performance in practical applications.", "key_contributions": ["Introduction of BP-Seg, an unsupervised learning model for text segmentation", "Employs belief propagation to enhance segmentation accuracy", "Demonstrates effectiveness on long-form document datasets"], "limitations": "", "keywords": ["text segmentation", "graphical models", "unsupervised learning", "semantic coherence", "belief propagation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.16972", "pdf": "https://arxiv.org/pdf/2505.16972.pdf", "abs": "https://arxiv.org/abs/2505.16972", "title": "From Tens of Hours to Tens of Thousands: Scaling Back-Translation for Speech Recognition", "authors": ["Tianduo Wang", "Lu Xu", "Wei Lu", "Shanbo Cheng"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Recent advances in Automatic Speech Recognition (ASR) have been largely\nfueled by massive speech corpora. However, extending coverage to diverse\nlanguages with limited resources remains a formidable challenge. This paper\nintroduces Speech Back-Translation, a scalable pipeline that improves\nmultilingual ASR models by converting large-scale text corpora into synthetic\nspeech via off-the-shelf text-to-speech (TTS) models. We demonstrate that just\ntens of hours of real transcribed speech can effectively train TTS models to\ngenerate synthetic speech at hundreds of times the original volume while\nmaintaining high quality. To evaluate synthetic speech quality, we develop an\nintelligibility-based assessment framework and establish clear thresholds for\nwhen synthetic data benefits ASR training. Using Speech Back-Translation, we\ngenerate more than 500,000 hours of synthetic speech in ten languages and\ncontinue pre-training Whisper-large-v3, achieving average transcription error\nreductions of over 30\\%. These results highlight the scalability and\neffectiveness of Speech Back-Translation for enhancing multilingual ASR\nsystems.", "AI": {"tldr": "This paper presents Speech Back-Translation, a method that enhances multilingual automatic speech recognition (ASR) models by generating synthetic speech from large text corpora using text-to-speech models.", "motivation": "The paper addresses the challenge of improving ASR systems in diverse languages with limited resources, leveraging synthetic speech to scale up training data.", "method": "The authors developed a scalable pipeline that uses text-to-speech models to produce synthetic speech from large text corpora, significantly increasing the volume of training data available for ASR.", "result": "The pipeline successfully generated over 500,000 hours of synthetic speech in ten languages, leading to a reduction in transcription error rates of over 30% when pre-training the Whisper-large-v3 model.", "conclusion": "Speech Back-Translation is a scalable and effective approach to enhance the performance of multilingual ASR systems, especially in languages with limited resources.", "key_contributions": ["Introduction of Speech Back-Translation for ASR enhancement", "Development of an intelligibility-based framework for synthetic speech assessment", "Achievement of significant transcription error rate reductions with synthetic speech training"], "limitations": "", "keywords": ["Automatic Speech Recognition", "multilingual models", "text-to-speech", "synthetic speech", "Speech Back-Translation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.16973", "pdf": "https://arxiv.org/pdf/2505.16973.pdf", "abs": "https://arxiv.org/abs/2505.16973", "title": "VeriFastScore: Speeding up long-form factuality evaluation", "authors": ["Rishanth Rajendhran", "Amir Zadeh", "Matthew Sarte", "Chuan Li", "Mohit Iyyer"], "categories": ["cs.CL"], "comment": null, "summary": "Metrics like FactScore and VeriScore that evaluate long-form factuality\noperate by decomposing an input response into atomic claims and then\nindividually verifying each claim. While effective and interpretable, these\nmethods incur numerous LLM calls and can take upwards of 100 seconds to\nevaluate a single response, limiting their practicality in large-scale\nevaluation and training scenarios. To address this, we propose VeriFastScore,\nwhich leverages synthetic data to fine-tune Llama3.1 8B for simultaneously\nextracting and verifying all verifiable claims within a given text based on\nevidence from Google Search. We show that this task cannot be solved via\nfew-shot prompting with closed LLMs due to its complexity: the model receives\n~4K tokens of evidence on average and needs to concurrently decompose claims,\njudge their verifiability, and verify them against noisy evidence. However, our\nfine-tuned VeriFastScore model demonstrates strong correlation with the\noriginal VeriScore pipeline at both the example level (r=0.80) and system level\n(r=0.94) while achieving an overall speedup of 6.6x (9.9x excluding evidence\nretrieval) over VeriScore. To facilitate future factuality research, we\npublicly release our VeriFastScore model and synthetic datasets.", "AI": {"tldr": "VeriFastScore improves long-form factuality evaluation by simultaneously extracting and verifying claims with a fine-tuned Llama3.1 model, achieving significant speedup over existing methods.", "motivation": "To enhance the practicality of factuality evaluation metrics like VeriScore and FactScore, which are limited by extensive LLM calls and long evaluation times.", "method": "VeriFastScore fine-tunes Llama3.1 8B to extract and verify verifiable claims within a text using evidence from Google Search, addressing the limitations of few-shot prompting.", "result": "The fine-tuned VeriFastScore model shows strong correlation with the VeriScore pipeline at r=0.80 (example level) and r=0.94 (system level), achieving a speedup of 6.6x overall and 9.9x excluding evidence retrieval.", "conclusion": "VeriFastScore significantly enhances the speed and efficiency of factuality evaluation in LLMs, and the model and datasets are publicly released for future research.", "key_contributions": ["Introduces VeriFastScore for simultaneous claim extraction and verification", "Achieves significant speedup over existing verification metrics", "Public release of the model and synthetic datasets for research purposes"], "limitations": "", "keywords": ["factuality evaluation", "Llama3", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.16983", "pdf": "https://arxiv.org/pdf/2505.16983.pdf", "abs": "https://arxiv.org/abs/2505.16983", "title": "LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding", "authors": ["Junlong Tong", "Jinlan Fu", "Zixuan Lin", "Yingqi Fan", "Anhao Zhao", "Hui Su", "Xiaoyu Shen"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Large Language Models (LLMs) are primarily designed for batch processing.\nExisting methods for adapting LLMs to streaming rely either on expensive\nre-encoding or specialized architectures with limited scalability. This work\nidentifies three key mismatches in adapting batch-oriented LLMs to streaming:\n(1) input-attention, (2) output-attention, and (3) position-ID mismatches.\nWhile it is commonly assumed that the latter two mismatches require frequent\nre-encoding, our analysis reveals that only the input-attention mismatch\nsignificantly impacts performance, indicating re-encoding outputs is largely\nunnecessary. To better understand this discrepancy with the common assumption,\nwe provide the first comprehensive analysis of the impact of position encoding\non LLMs in streaming, showing that preserving relative positions within source\nand target contexts is more critical than maintaining absolute order. Motivated\nby the above analysis, we introduce a group position encoding paradigm built on\nbatch architectures to enhance consistency between streaming and batch modes.\nExtensive experiments on cross-lingual and cross-modal tasks demonstrate that\nour method outperforms existing approaches. Our method requires no\narchitectural modifications, exhibits strong generalization in both streaming\nand batch modes. The code is available at repository\nhttps://github.com/EIT-NLP/StreamingLLM.", "AI": {"tldr": "This paper addresses the adaptation of Large Language Models (LLMs) for streaming applications, identifying key mismatches and proposing a novel position encoding method that enhances performance without complex architectural changes.", "motivation": "To improve the adaptation of batch-oriented LLMs to streaming without the need for expensive re-encoding processes.", "method": "The authors analyze three mismatches in LLM adaptation and propose a group position encoding paradigm that focuses on preserving relative positions instead of absolute order, based on extensive experimental validation.", "result": "The proposed method shows improved performance on cross-lingual and cross-modal tasks while requiring no architectural modifications.", "conclusion": "Adapting LLMs for streaming can be achieved effectively with the proposed position encoding technique, enhancing both performance and generalization across different modalities.", "key_contributions": ["Identified critical mismatches in streaming adaptation of LLMs.", "Proposed a scalable group position encoding paradigm for enhanced streaming performance.", "Demonstrated strong performance across multiple tasks without altering the LLM architecture."], "limitations": "The study is focused on specific tasks and may need further validation in broader contexts.", "keywords": ["Large Language Models", "Streaming", "Position Encoding", "Cross-lingual", "Cross-modal"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.16986", "pdf": "https://arxiv.org/pdf/2505.16986.pdf", "abs": "https://arxiv.org/abs/2505.16986", "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning", "authors": ["Amartya Chakraborty", "Paresh Dashore", "Nadia Bathaee", "Anmol Jain", "Anirban Das", "Shi-Xiong Zhang", "Sambit Sahu", "Milind Naphade", "Genta Indra Winata"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-source language models. We present results powered by\nT1-Agent, highlighting their ability to plan and reason in complex,\ntool-dependent scenarios.", "AI": {"tldr": "Introduction of T1, a dataset for evaluating multi-turn conversational agents with tool use and planning capabilities.", "motivation": "Large Language Models struggle with effective planning in multi-turn conversations involving dependencies between API or tool calls.", "method": "Development of the T1 dataset designed to manage inter-tool dependencies across nine domains, supporting dynamic replanning and caching mechanisms.", "result": "Evaluation results of agents using T1, demonstrating their capability in planning and reasoning within complex tool-dependent scenarios.", "conclusion": "T1 is a valuable resource for research on tool use, planning, and as a benchmark for open-source language models.", "key_contributions": ["Introduction of the T1 dataset for tool-augmented conversation research.", "Evaluation of agents in multi-domain, multi-turn scenarios.", "Integrated caching mechanism to support dynamic replanning."], "limitations": "", "keywords": ["Large Language Models", "Tool Use", "Multi-domain Conversations"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.16988", "pdf": "https://arxiv.org/pdf/2505.16988.pdf", "abs": "https://arxiv.org/abs/2505.16988", "title": "MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent Systems", "authors": ["Rui Ye", "Keduan Huang", "Qimin Wu", "Yuzhu Cai", "Tian Jin", "Xianghe Pang", "Xiangrui Liu", "Jiaqi Su", "Chen Qian", "Bohan Tang", "Kaiqu Liang", "Jiaao Chen", "Yue Hu", "Zhenfei Yin", "Rongye Shi", "Bo An", "Yang Gao", "Wenjun Wu", "Lei Bai", "Siheng Chen"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "18 pages, 11 figures", "summary": "LLM-based multi-agent systems (MAS) have demonstrated significant potential\nin enhancing single LLMs to address complex and diverse tasks in practical\napplications. Despite considerable advancements, the field lacks a unified\ncodebase that consolidates existing methods, resulting in redundant\nre-implementation efforts, unfair comparisons, and high entry barriers for\nresearchers. To address these challenges, we introduce MASLab, a unified,\ncomprehensive, and research-friendly codebase for LLM-based MAS. (1) MASLab\nintegrates over 20 established methods across multiple domains, each rigorously\nvalidated by comparing step-by-step outputs with its official implementation.\n(2) MASLab provides a unified environment with various benchmarks for fair\ncomparisons among methods, ensuring consistent inputs and standardized\nevaluation protocols. (3) MASLab implements methods within a shared streamlined\nstructure, lowering the barriers for understanding and extension. Building on\nMASLab, we conduct extensive experiments covering 10+ benchmarks and 8 models,\noffering researchers a clear and comprehensive view of the current landscape of\nMAS methods. MASLab will continue to evolve, tracking the latest developments\nin the field, and invite contributions from the broader open-source community.", "AI": {"tldr": "Introduction of MASLab, a unified codebase for LLM-based multi-agent systems to streamline research and comparisons in the field.", "motivation": "To address the challenges of redundant re-implementations and high entry barriers in LLM-based multi-agent systems research.", "method": "Creation of a comprehensive codebase integrating over 20 established MAS methods, with experimental validation and benchmarking capabilities.", "result": "Successful integration and validation of multiple methods, providing researchers with a robust platform for evaluation and comparison.", "conclusion": "MASLab serves as a dynamic platform that will evolve with contributions from the community and track developments in LLM-based MAS.", "key_contributions": ["Introduction of a unified codebase (MASLab) for LLM-based multi-agent systems.", "Integration of over 20 validated MAS methods across domains.", "Establishment of a standardized environment for benchmarking and evaluation."], "limitations": "", "keywords": ["multi-agent systems", "LLM", "codebase", "machine learning", "benchmarking"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.16995", "pdf": "https://arxiv.org/pdf/2505.16995.pdf", "abs": "https://arxiv.org/abs/2505.16995", "title": "DecoupledESC: Enhancing Emotional Support Generation via Strategy-Response Decoupled Preference Optimization", "authors": ["Chao Zhang", "Xin Shi", "Xueqiao Zhang", "Yifan Zhu", "Yi Yang", "Yawei Luo"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in Emotional Support Conversation (ESC) have improved\nemotional support generation by fine-tuning Large Language Models (LLMs) via\nSupervised Fine-Tuning (SFT). However, common psychological errors still\npersist. While Direct Preference Optimization (DPO) shows promise in reducing\nsuch errors through pairwise preference learning, its effectiveness in ESC\ntasks is limited by two key challenges: (1) Entangled data structure: Existing\nESC data inherently entangles psychological strategies and response content,\nmaking it difficult to construct high-quality preference pairs; and (2)\nOptimization ambiguity: Applying vanilla DPO to such entangled pairwise data\nleads to ambiguous training objectives. To address these issues, we introduce\nInferential Preference Mining (IPM) to construct high-quality preference data,\nforming the IPM-PrefDial dataset. Building upon this data, we propose a\nDecoupled ESC framework inspired by Gross's Extended Process Model of Emotion\nRegulation, which decomposes the ESC task into two sequential subtasks:\nstrategy planning and empathic response generation. Each was trained via SFT\nand subsequently enhanced by DPO to align with the psychological preference.\nExtensive experiments demonstrate that our Decoupled ESC framework outperforms\njoint optimization baselines, reducing preference bias and improving response\nquality.", "AI": {"tldr": "This paper introduces a Decoupled Emotional Support Conversation framework that improves response quality by addressing issues in preference learning using a novel dataset and approach.", "motivation": "The paper aims to enhance emotional support generation in conversational agents by addressing psychological errors that persist in existing models.", "method": "The authors introduce Inferential Preference Mining (IPM) to create high-quality preference data, forming the IPM-PrefDial dataset, and propose a Decoupled framework that separates strategy planning from response generation, utilizing Supervised Fine-Tuning and Direct Preference Optimization.", "result": "Extensive experiments demonstrate that the Decoupled ESC framework significantly outperforms joint optimization approaches, reducing preference bias and enhancing the quality of responses.", "conclusion": "The proposed methods effectively tackle the challenges associated with emotional support conversation tasks and lead to better model performance in generating empathetic responses.", "key_contributions": ["Introduction of Inferential Preference Mining (IPM) for high-quality preference data creation.", "Development of the IPM-PrefDial dataset specifically for Emotional Support Conversations.", "Proposal of a Decoupled ESC framework that separates tasks to improve model training."], "limitations": "The paper does not discuss the long-term usability or applicability of the proposed method in real-world scenarios.", "keywords": ["Emotional Support Conversation", "Large Language Models", "Preference Learning", "Decoupled Framework", "Supervised Fine-Tuning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.16998", "pdf": "https://arxiv.org/pdf/2505.16998.pdf", "abs": "https://arxiv.org/abs/2505.16998", "title": "Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?", "authors": ["Jin Jiang", "Jianing Wang", "Yuchen Yan", "Yang Liu", "Jianhua Zhu", "Mengdi Zhang", "Xunliang Cai", "Liangcai Gao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have been shown to achieve breakthrough\nperformance on complex logical reasoning tasks. Nevertheless, most existing\nresearch focuses on employing formal language to guide LLMs to derive reliable\nreasoning paths, while systematic evaluations of these capabilities are still\nlimited. In this paper, we aim to conduct a comprehensive evaluation of LLMs\nacross various logical reasoning problems utilizing formal languages. From the\nperspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and\nformat of trajectories, our key findings are: 1) Thinking models significantly\noutperform Instruct models, especially when formal language is employed; 2) All\nLLMs exhibit limitations in inductive reasoning capability, irrespective of\nwhether they use a formal language; 3) Data with PoT format achieves the best\ngeneralization performance across other languages. Additionally, we also curate\nthe formal-relative training data to further enhance the small language models,\nand the experimental results indicate that a simple rejected fine-tuning method\ncan better enable LLMs to generalize across formal languages and achieve the\nbest overall performance. Our codes and reports are available at\nhttps://github.com/jiangjin1999/FormalEval.", "AI": {"tldr": "This paper evaluates Large Language Models (LLMs) on logical reasoning tasks using formal languages, revealing strengths and limitations of different models and offering enhancements for better performance.", "motivation": "Despite breakthroughs in LLMs for logical reasoning, systematic evaluations of their capabilities are limited, warranting comprehensive evaluation across various tasks.", "method": "The study evaluates LLMs across a spectrum of models and tasks using formal language, analyzing their performance in terms of task taxonomy and reasoning paths.", "result": "Thinking models significantly outperform Instruct models, all LLMs show limitations in inductive reasoning, and PoT format data provides best generalization performance; enhanced training data improves small model performance.", "conclusion": "A simple rejected fine-tuning method can enable LLMs to better generalize across formal languages and enhance overall performance.", "key_contributions": ["Comprehensive evaluation of various LLMs in logical reasoning tasks using formal languages.", "Identification of model performance variation based on the application of formal language vs. informal language.", "Introduction of enhanced training data methodologies for small language models."], "limitations": "Current evaluations reveal inherent limitations in inductive reasoning capabilities across all LLMs.", "keywords": ["Large Language Models", "logical reasoning", "formal language", "inductive reasoning", "fine-tuning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.17005", "pdf": "https://arxiv.org/pdf/2505.17005.pdf", "abs": "https://arxiv.org/abs/2505.17005", "title": "R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning", "authors": ["Huatong Song", "Jinhao Jiang", "Wenqing Tian", "Zhipeng Chen", "Yuhuan Wu", "Jiahao Zhao", "Yingqian Min", "Wayne Xin Zhao", "Lei Fang", "Ji-Rong Wen"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) are powerful but prone to hallucinations due to\nstatic knowledge. Retrieval-Augmented Generation (RAG) helps by injecting\nexternal information, but current methods often are costly, generalize poorly,\nor ignore the internal knowledge of the model. In this paper, we introduce\nR1-Searcher++, a novel framework designed to train LLMs to adaptively leverage\nboth internal and external knowledge sources. R1-Searcher++ employs a two-stage\ntraining strategy: an initial SFT Cold-start phase for preliminary format\nlearning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses\noutcome-supervision to encourage exploration, incorporates a reward mechanism\nfor internal knowledge utilization, and integrates a memorization mechanism to\ncontinuously assimilate retrieved information, thereby enriching the model's\ninternal knowledge. By leveraging internal knowledge and external search\nengine, the model continuously improves its capabilities, enabling efficient\nretrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++\noutperforms previous RAG and reasoning methods and achieves efficient\nretrieval. The code is available at\nhttps://github.com/RUCAIBox/R1-Searcher-plus.", "AI": {"tldr": "Introduction of R1-Searcher++, a framework for LLMs to leverage both internal and external knowledge through a two-stage training strategy.", "motivation": "To address the limitations of current Retrieval-Augmented Generation methods that are costly, generalize poorly, or overlook the internal knowledge of LLMs.", "method": "R1-Searcher++ employs an initial SFT Cold-start phase for preliminary format learning followed by a reinforcement learning (RL) stage for Dynamic Knowledge Acquisition, utilizing reward mechanisms for internal knowledge and a memorization mechanism for retrieved information.", "result": "R1-Searcher++ consistently outperforms existing RAG methods and achieves efficient retrieval-augmented reasoning as demonstrated by experimental results.", "conclusion": "The proposed framework enriches the model's internal knowledge, improving the efficiency of retrieval-augmented reasoning.", "key_contributions": ["Novel framework R1-Searcher++ for adaptively leveraging internal and external knowledge in LLMs.", "Two-stage training strategy incorporating RL for Dynamic Knowledge Acquisition.", "Demonstrated performance improvements over existing RAG methods."], "limitations": "", "keywords": ["Large Language Models", "Retrieval-Augmented Generation", "Reinforcement Learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2310.08232", "pdf": "https://arxiv.org/pdf/2310.08232.pdf", "abs": "https://arxiv.org/abs/2310.08232", "title": "Language Models are Universal Embedders", "authors": ["Xin Zhang", "Zehan Li", "Yanzhao Zhang", "Dingkun Long", "Pengjun Xie", "Meishan Zhang", "Min Zhang"], "categories": ["cs.CL"], "comment": "XLLM Workshop, ACL 2025", "summary": "In the large language model (LLM) revolution, embedding is a key component of\nvarious systems, such as retrieving knowledge or memories for LLMs or building\ncontent moderation filters. As such cases span from English to other natural or\nprogramming languages, from retrieval to classification and beyond, it is\nadvantageous to build a unified embedding model rather than dedicated ones for\neach scenario. In this context, the pre-trained multilingual decoder-only large\nlanguage models, e.g., BLOOM, emerge as a viable backbone option. To assess\ntheir potential, we propose straightforward strategies for constructing\nembedders and introduce a universal evaluation benchmark. Experimental results\nshow that our trained model is proficient at generating good embeddings across\nlanguages and tasks, even extending to languages and tasks for which no\nfinetuning/pretraining data is available. We also present detailed analyses and\nadditional evaluations. We hope that this work could encourage the development\nof more robust open-source universal embedders.", "AI": {"tldr": "This paper proposes a unified embedding model using pre-trained multilingual large language models for various tasks like retrieval and classification, demonstrating its effectiveness across languages.", "motivation": "The need for a unified embedding model is emphasized due to the diverse applications of LLMs that span various natural and programming languages.", "method": "The authors propose straightforward strategies for constructing embedders and introduce a universal evaluation benchmark.", "result": "Experimental results indicate that the trained model generates good embeddings across languages and tasks, even without additional fine-tuning or pretraining data.", "conclusion": "The findings support the development of robust open-source universal embedders to enhance performance in various scenarios.", "key_contributions": ["Proposes a unified embedding model for various LLM applications", "Introduces a universal evaluation benchmark for assessing embedding models", "Demonstrates effectiveness across languages with limited data"], "limitations": "", "keywords": ["embedding model", "multilingual", "large language models", "universal evaluation benchmark", "open-source"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2312.13772", "pdf": "https://arxiv.org/pdf/2312.13772.pdf", "abs": "https://arxiv.org/abs/2312.13772", "title": "Large Language Models are Miscalibrated In-Context Learners", "authors": ["Chengzu Li", "Han Zhou", "Goran Glavaš", "Anna Korhonen", "Ivan Vulić"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, 4 figures, 5 tables (20 pages, 5 figures, 13 tables\n  including references and appendices)", "summary": "When adapting ICL with or without fine-tuning, we are curious about whether\nthe instruction-tuned language model is able to achieve well-calibrated results\nwithout suffering from the problem of overconfidence (i.e., miscalibration)\nconsidering its strong instruction following ability, especially in such\nlimited data setups. In this work, we deliver an in-depth analysis of the\nbehavior across different choices of learning methods from the perspective of\nboth performance and calibration. Through extensive controlled experiments, we\nobserve that the miscalibration problem exists across all learning methods in\nlow-resource setups. To achieve simultaneous gain for both in-task performance\nand calibration, we then study the potential of self-ensembling applied at\ndifferent modeling stages (e.g., variations of in-context examples or\nvariations in prompts or different ensembling strategies) to make the\npredictions more calibrated and have comparable or even better performance. We\nfind that self-ensembling with max probability produces robust and calibrated\npredictions. Our work reveals the potential calibration problem of using ICL\ndespite the improvements in task performance and sheds light on which learning\nparadigm to choose. We also provide practical guidelines for choosing learning\nparadigms depending on whether the data has been seen by the model before and a\nworthwhile solution via self-ensembling on how to enhance both task performance\nand calibration of LMs, which we hope could encourage further study.", "AI": {"tldr": "This paper investigates the miscalibration problem in instruction-tuned language models in low-resource settings, showing that self-ensembling techniques can improve both the calibration of predictions and task performance.", "motivation": "To understand if instruction-tuned language models maintain calibration without overconfidence in low-resource environments, given their strong instruction following abilities.", "method": "Extensive controlled experiments were conducted across various learning methods to analyze performance and calibration, focusing on self-ensembling at different modeling stages.", "result": "Self-ensembling with max probability led to robust and calibrated predictions, highlighting the miscalibration problem in all studied methods.", "conclusion": "The study reveals the calibration issues in instruction-tuned language models and suggests self-ensembling as a viable strategy to enhance both task performance and calibration.", "key_contributions": ["In-depth analysis of miscalibration in instruction-tuned models.", "Empirical evidence showing self-ensembling improves model performance and calibration.", "Practical guidelines for choosing appropriate learning paradigms based on data exposure."], "limitations": "Focus primarily on low-resource settings; may not generalize to high-resource setups.", "keywords": ["instruction tuning", "miscalibration", "self-ensembling", "language models", "low-resource learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2402.06738", "pdf": "https://arxiv.org/pdf/2402.06738.pdf", "abs": "https://arxiv.org/abs/2402.06738", "title": "EntGPT: Entity Linking with Generative Large Language Models", "authors": ["Yifan Ding", "Amrit Poudel", "Qingkai Zeng", "Tim Weninger", "Balaji Veeramani", "Sanmitra Bhattacharya"], "categories": ["cs.CL", "H.3.3"], "comment": null, "summary": "Entity Linking in natural language processing seeks to match text entities to\ntheir corresponding entries in a dictionary or knowledge base. Traditional\napproaches rely on contextual models, which can be complex, hard to train, and\nhave limited transferability across different domains. Generative large\nlanguage models like GPT offer a promising alternative but often underperform\nwith naive prompts. In this study, we introduce EntGPT, employing advanced\nprompt engineering to enhance EL tasks. Our three-step hard-prompting method\n(EntGPT-P) significantly boosts the micro-F_1 score by up to 36% over vanilla\nprompts, achieving competitive performance across 10 datasets without\nsupervised fine-tuning. Additionally, our instruction tuning method (EntGPT-I)\nimproves micro-F_1 scores by 2.1% on average in supervised EL tasks and\noutperforms several baseline models in six Question Answering tasks. Our\nmethods are compatible with both open-source and proprietary LLMs. All data and\ncode are available on GitHub at https://github.com/yifding/In_Context_EL.", "AI": {"tldr": "This paper introduces EntGPT, a method combining advanced prompt engineering with generative language models to improve entity linking (EL) tasks in NLP, achieving significant performance enhancements without supervised fine-tuning.", "motivation": "To address the limitations of traditional contextual models in entity linking by using generative large language models (LLMs), enhancing their performance through effective prompt engineering.", "method": "The paper presents a three-step hard-prompting method (EntGPT-P) and an instruction tuning method (EntGPT-I) to improve entity linking tasks without the need for supervised fine-tuning.", "result": "EntGPT-P boosts the micro-F_1 score by up to 36% over standard prompts, while EntGPT-I enhances micro-F_1 scores by an average of 2.1% in supervised tasks and shows competitive performance in Question Answering tasks.", "conclusion": "The proposed methods demonstrate significant improvements in entity linking performance, confirming the potential of utilizing prompt engineering in conjunction with LLMs for both supervised and unsupervised settings.", "key_contributions": ["Introduction of a novel method (EntGPT) for entity linking using advanced prompt engineering", "Achievement of significant performance improvements in entity linking tasks across multiple datasets", "Compatibility with both open-source and proprietary LLMs"], "limitations": "", "keywords": ["entity linking", "large language models", "prompt engineering", "NLP", "question answering"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2405.04756", "pdf": "https://arxiv.org/pdf/2405.04756.pdf", "abs": "https://arxiv.org/abs/2405.04756", "title": "Red-Teaming for Inducing Societal Bias in Large Language Models", "authors": ["Chu Fei Luo", "Ahmad Ghawanmeh", "Bharat Bhimshetty", "Kashyap Murali", "Murli Jadhav", "Xiaodan Zhu", "Faiza Khan Khattak"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Ensuring the safe deployment of AI systems is critical in industry settings\nwhere biased outputs can lead to significant operational, reputational, and\nregulatory risks. Thorough evaluation before deployment is essential to prevent\nthese hazards. Red-teaming addresses this need by employing adversarial attacks\nto develop guardrails that detect and reject biased or harmful queries,\nenabling models to be retrained or steered away from harmful outputs. However,\nmost red-teaming efforts focus on harmful or unethical instructions rather than\naddressing social bias, leaving this critical area under-explored despite its\nsignificant real-world impact, especially in customer-facing systems. We\npropose two bias-specific red-teaming methods, Emotional Bias Probe (EBP) and\nBiasKG, to evaluate how standard safety measures for harmful content affect\nbias. For BiasKG, we refactor natural language stereotypes into a knowledge\ngraph. We use these attacking strategies to induce biased responses from\nseveral open- and closed-source language models. Unlike prior work, these\nmethods specifically target social bias. We find our method increases bias in\nall models, even those trained with safety guardrails. Our work emphasizes\nuncovering societal bias in LLMs through rigorous evaluation, and recommends\nmeasures ensure AI safety in high-stakes industry deployments.", "AI": {"tldr": "This paper presents two novel methods for red-teaming AI systems to identify and mitigate social bias in language models, emphasizing the importance of thorough bias evaluation before deployment.", "motivation": "The paper addresses the critical need for ensuring AI system safety in industry settings, particularly by evaluating social bias that may lead to operational and reputational risks.", "method": "The authors introduce two bias-specific red-teaming methods: Emotional Bias Probe (EBP) and BiasKG, the latter using a knowledge graph to refactor natural language stereotypes, and employ these methods to induce biased responses from various language models.", "result": "The findings demonstrate that both methods increase bias in all evaluated models, including those equipped with safety guardrails, thus highlighting the inadequacies of current safety measures in addressing social bias.", "conclusion": "The work underscores the need for better evaluation measures to uncover societal bias in language models and recommends strategies to ensure AI safety during industry deployment.", "key_contributions": ["Introduction of two novel red-teaming methods (EBP and BiasKG) targeting social bias in AI systems", "Demonstration of increased bias in language models even with existing safety measures", "Recommendations for improving AI safety assessment in industry settings"], "limitations": "", "keywords": ["AI safety", "social bias", "red-teaming", "language models", "Emotional Bias Probe"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2406.10594", "pdf": "https://arxiv.org/pdf/2406.10594.pdf", "abs": "https://arxiv.org/abs/2406.10594", "title": "BlockPruner: Fine-grained Pruning for Large Language Models", "authors": ["Longguang Zhong", "Fanqi Wan", "Ruijun Chen", "Xiaojun Quan", "Liangzhi Li"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "With the rapid growth in the size and complexity of large language models\n(LLMs), the costs associated with their training and inference have escalated\nsignificantly. Research indicates that certain layers in LLMs harbor\nsubstantial redundancy, and pruning these layers has minimal impact on the\noverall performance. While various layer pruning methods have been developed\nbased on this insight, they generally overlook the finer-grained redundancies\nwithin the layers themselves. In this paper, we delve deeper into the\narchitecture of LLMs and demonstrate that finer-grained pruning can be achieved\nby targeting redundancies in multi-head attention (MHA) and multi-layer\nperceptron (MLP) blocks. We propose a novel, training-free structured pruning\napproach called BlockPruner. Unlike existing layer pruning methods, BlockPruner\nsegments each Transformer layer into MHA and MLP blocks. It then assesses the\nimportance of these blocks using perplexity measures and applies a heuristic\nsearch for iterative pruning. We applied BlockPruner to LLMs of various sizes\nand architectures and validated its performance across a wide range of\ndownstream tasks. Experimental results show that BlockPruner achieves more\ngranular and effective pruning compared to state-of-the-art baselines.", "AI": {"tldr": "This paper introduces BlockPruner, a novel pruning method for large language models that targets redundancies within multi-head attention and multi-layer perceptron blocks, leading to more efficient model performance.", "motivation": "The increasing costs associated with training and inference of large language models necessitate more efficient pruning methods to reduce redundancy without affecting performance.", "method": "BlockPruner segments Transformer layers into MHA and MLP blocks, evaluates their importance using perplexity measures, and employs heuristic search for iterative pruning.", "result": "BlockPruner was applied to various LLMs, demonstrating more granular and effective pruning compared to existing methods, validated across numerous downstream tasks.", "conclusion": "The proposed pruning approach can significantly reduce the complexity of LLMs while maintaining their performance, offering a more efficient alternative to traditional layer pruning methods.", "key_contributions": ["Introduction of BlockPruner for fine-grained pruning of LLMs", "Assessment of block importance using perplexity measures", "Demonstration of superior performance in various tasks compared to state-of-the-art methods."], "limitations": "", "keywords": ["large language models", "pruning methods", "Transformer layers", "multi-head attention", "block importance"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2409.02393", "pdf": "https://arxiv.org/pdf/2409.02393.pdf", "abs": "https://arxiv.org/abs/2409.02393", "title": "Determination of language families using deep learning", "authors": ["Peter B. Lerner"], "categories": ["cs.CL", "I.2.7"], "comment": "Second draft with improved statistics of NN simulations. Comments are\n  welcome", "summary": "We use a c-GAN (convolutional generative adversarial) neural network to\nanalyze transliterated text fragments of extant, dead comprehensible, and one\ndead non-deciphered (Cypro-Minoan) language to establish linguistic affinities.\nThe paper is agnostic with respect to translation and/or deciphering. However,\nthere is hope that the proposed approach can be useful for decipherment with\nmore sophisticated neural network techniques.", "AI": {"tldr": "This paper presents a c-GAN neural network approach to analyze and establish linguistic affinities in various dead languages.", "motivation": "To explore the use of machine learning techniques in the analysis of transliterated text fragments from dead languages, especially in terms of understanding their linguistic relationships.", "method": "Utilized a convolutional generative adversarial neural network (c-GAN) to analyze transliterated texts of comprehensible and non-deciphered languages.", "result": "The approach shows potential for establishing linguistic relationships, with improved statistics from neural network simulations in the second draft.", "conclusion": "The proposed c-GAN method may eventually contribute to decipherment efforts with more advanced neural networks, despite not focusing on translation or deciphering directly.", "key_contributions": ["Introduction of a c-GAN approach for linguistic analysis", "Improved neural network simulation statistics", "Potential application for language decipherment with advanced techniques"], "limitations": "", "keywords": ["c-GAN", "linguistic analysis", "dead languages"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2409.02813", "pdf": "https://arxiv.org/pdf/2409.02813.pdf", "abs": "https://arxiv.org/abs/2409.02813", "title": "MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark", "authors": ["Xiang Yue", "Tianyu Zheng", "Yuansheng Ni", "Yubo Wang", "Kai Zhang", "Shengbang Tong", "Yuxuan Sun", "Botao Yu", "Ge Zhang", "Huan Sun", "Yu Su", "Wenhu Chen", "Graham Neubig"], "categories": ["cs.CL", "cs.CV"], "comment": "ACL 2025 Main", "summary": "This paper introduces MMMU-Pro, a robust version of the Massive\nMulti-discipline Multimodal Understanding and Reasoning (MMMU) benchmark.\nMMMU-Pro rigorously assesses multimodal models' true understanding and\nreasoning capabilities through a three-step process based on MMMU: (1)\nfiltering out questions answerable by text-only models, (2) augmenting\ncandidate options, and (3) introducing a vision-only input setting where\nquestions are embedded within images. This setting challenges AI to truly \"see\"\nand \"read\" simultaneously, testing a fundamental human cognitive skill of\nseamlessly integrating visual and textual information. Results show that model\nperformance is substantially lower on MMMU-Pro than on MMMU, ranging from 16.8%\nto 26.9% across models. We explore the impact of OCR prompts and Chain of\nThought (CoT) reasoning, finding that OCR prompts have minimal effect while CoT\ngenerally improves performance. MMMU-Pro provides a more rigorous evaluation\ntool, closely mimicking real-world scenarios and offering valuable directions\nfor future research in multimodal AI.", "AI": {"tldr": "MMMU-Pro is a new benchmark for assessing multimodal models' understanding and reasoning abilities, highlighting significant performance drops compared to previous benchmarks.", "motivation": "To create a more rigorous evaluation tool for multimodal models that mimics real-world scenarios.", "method": "MMMU-Pro uses a three-step process: filtering questions for text-only models, augmenting candidate options, and implementing a vision-only input setting that requires integrated visual and textual understanding.", "result": "Model performance on MMMU-Pro is significantly lower, with results showing a decrease ranging from 16.8% to 26.9%.", "conclusion": "MMMU-Pro serves as a better evaluation framework for multimodal AI, suggesting avenues for future research.", "key_contributions": ["Introduction of a more rigorous multimodal evaluation benchmark (MMMU-Pro)", "Incorporation of vision-only input settings and augmented questions", "Evaluation of the effects of OCR prompts and Chain of Thought reasoning on model performance"], "limitations": "", "keywords": ["multimodal AI", "benchmark", "MMMU-Pro", "Chain of Thought", "OCR prompts"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2409.03327", "pdf": "https://arxiv.org/pdf/2409.03327.pdf", "abs": "https://arxiv.org/abs/2409.03327", "title": "Normal forms in Virus Machines", "authors": ["A. Ramírez-de-Arellano", "F. G. C. Cabarle", "D. Orellana-Martín", "M. J. Pérez-Jiménez"], "categories": ["cs.CL", "cs.FL", "68Q07 (Primary) 68Q10, 68R01 (Secondary)", "F.0; F.1.1"], "comment": "24 pages, 14 figures", "summary": "In the present work, we further study the computational power of virus\nmachines (VMs in short).VMs provide a computing paradigm inspired by the\ntransmission and replication networks of viruses.VMs consist of process units\n(called hosts) structured by a directed graph whose arcs are called channels\nand an instruction graph that controls the transmissions of virus objects among\nhosts. The present work complements our understanding of the computing power of\nVMs by introducing normal forms; these expressions restrict the features in a\ngiven computing model.Some of the features that we restrict in our normal forms\ninclude (a) the number of hosts, (b) the number of instructions, and (c) the\nnumber of virus objects in each host. After we recall some known results on the\ncomputing power of VMs we give our series of normal forms, such as the size of\nthe loops in the network, proving new characterisations of family of sets, such\nas finite sets, semilinear sets, or recursively enumerable sets (NRE).", "AI": {"tldr": "This paper explores the computational capabilities of virus machines (VMs) by introducing normal forms that restrict certain features in the computing model.", "motivation": "The study aims to enhance the understanding of the computational power of virus machines, inspired by viral transmission and replication networks.", "method": "The authors introduce normal forms that limit the number of hosts, instructions, and virus objects in each host within VMs, and provide new characterizations of various families of sets.", "result": "New characterization results of families of sets such as finite sets, semilinear sets, and recursively enumerable sets (NRE) are established through these normal forms.", "conclusion": "The new normal forms contribute to a deeper understanding of the computing power of virus machines.", "key_contributions": ["Introduction of normal forms for virus machines", "New characterizations of finite, semilinear, and recursively enumerable sets", "Analysis of the impact of restricted features on computational power"], "limitations": "", "keywords": ["virus machines", "computational power", "normal forms", "set theory", "directed graphs"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2409.18199", "pdf": "https://arxiv.org/pdf/2409.18199.pdf", "abs": "https://arxiv.org/abs/2409.18199", "title": "LangSAMP: Language-Script Aware Multilingual Pretraining", "authors": ["Yihong Liu", "Haotian Ye", "Chunlan Ma", "Mingyang Wang", "Hinrich Schütze"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Recent multilingual pretrained language models (mPLMs) often avoid using\nlanguage embeddings -- learnable vectors assigned to individual languages.\nHowever, this places a significant burden on token representations to encode\nall language-specific information, which may hinder language neutrality. To\naddress this limitation, we propose Language-Script Aware Multilingual\nPretraining (LangSAMP), a method that incorporates both language and script\nembeddings to enhance representation learning. Specifically, we integrate these\nembeddings into the output of the Transformer blocks before passing the final\nrepresentations to the language modeling head for prediction. We apply LangSAMP\nto the continual pretraining of XLM-R on a highly multilingual corpus covering\nmore than 500 languages. The resulting model consistently outperforms the\nbaseline in zero-shot crosslingual transfer across diverse downstream tasks.\nExtensive analysis reveals that language and script embeddings capture\nlanguage- and script-specific nuances, which benefits more language-neutral\nrepresentations, proven by improved pairwise cosine similarity. In our case\nstudy, we also show that language and script embeddings can be used to select\nbetter source languages for crosslingual transfer. We make our code and models\npublicly available at https://github.com/cisnlp/LangSAMP.", "AI": {"tldr": "This paper introduces LangSAMP, a multilingual pretraining method that incorporates language and script embeddings to improve representation learning in language models.", "motivation": "Current multilingual pretrained models often neglect language embeddings, which burdens token representations and compromises language neutrality.", "method": "LangSAMP integrates both language and script embeddings into the output of Transformer blocks enhancing the learning process during the continual pretraining of XLM-R.", "result": "The model extends performance over the baseline in zero-shot crosslingual transfer across a variety of tasks, succeeding in nuanced representation learning.", "conclusion": "Utilizing language and script embeddings fosters better language-neutral representations and aids in selecting more suitable source languages for crosslingual transfer.", "key_contributions": ["Introduces LangSAMP for improved multilingual representation learning.", "Enhances zero-shot crosslingual transfer performance through integrated embeddings.", "Offers publicly available implementation for further research and application."], "limitations": "", "keywords": ["multilingual models", "language embeddings", "script embeddings", "crosslingual transfer", "pretraining"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2410.05254", "pdf": "https://arxiv.org/pdf/2410.05254.pdf", "abs": "https://arxiv.org/abs/2410.05254", "title": "GLEE: A Unified Framework and Benchmark for Language-based Economic Environments", "authors": ["Eilam Shapira", "Omer Madmon", "Itamar Reinman", "Samuel Joseph Amouyal", "Roi Reichart", "Moshe Tennenholtz"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.GT", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) show significant potential in economic and\nstrategic interactions, where communication via natural language is often\nprevalent. This raises key questions: Do LLMs behave rationally? How do they\nperform compared to humans? Do they tend to reach an efficient and fair\noutcome? What is the role of natural language in strategic interaction? How do\ncharacteristics of the economic environment influence these dynamics? These\nquestions become crucial concerning the economic and societal implications of\nintegrating LLM-based agents into real-world data-driven systems, such as\nonline retail platforms and recommender systems. To answer these questions, we\nintroduce a benchmark for standardizing research on two-player, sequential,\nlanguage-based games. Inspired by the economic literature, we define three base\nfamilies of games with consistent parameterization, degrees of freedom and\neconomic measures to evaluate agents' performance (self-gain), as well as the\ngame outcome (efficiency and fairness). We develop an open-source framework for\ninteraction simulation and analysis, and utilize it to collect a dataset of LLM\nvs. LLM interactions across numerous game configurations and an additional\ndataset of human vs. LLM interactions. Through extensive experimentation, we\ndemonstrate how our framework and dataset can be used to: (i) compare the\nbehavior of LLM-based agents in various economic contexts; (ii) evaluate agents\nin both individual and collective performance measures; and (iii) quantify the\neffect of the economic characteristics of the environments on the behavior of\nagents. Our results suggest that the market parameters, as well as the choice\nof the LLMs, tend to have complex and interdependent effects on the economic\noutcome, which calls for careful design and analysis of the language-based\neconomic ecosystem.", "AI": {"tldr": "This paper introduces a benchmark for studying economic interactions of Large Language Models (LLMs) in language-based games, presenting an open-source framework for simulation and evaluation.", "motivation": "To explore the rationality and performance of LLM-based agents in economic contexts and their implications for real-world applications like online retail and recommender systems.", "method": "The authors define three families of two-player, sequential, language-based games with consistent parameters. An open-source framework is developed for simulating interactions, along with a dataset for LLM vs. LLM and human vs. LLM interactions.", "result": "The experiments reveal that economic parameters and LLM choice have complex interdependent effects on outcomes such as efficiency and fairness in interactions.", "conclusion": "Designing and analyzing the language-based economic ecosystem requires attention to the intricate relationships between LLM behavior, economic environments, and game parameters.", "key_contributions": ["Introduction of a standardized benchmark for LLM interactions in economic games", "Creation of an open-source framework for simulation and data collection", "Comprehensive datasets for comparing LLM and human behaviors in economic contexts"], "limitations": "", "keywords": ["Large Language Models", "Economic Games", "Agent-Based Simulation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.08085", "pdf": "https://arxiv.org/pdf/2410.08085.pdf", "abs": "https://arxiv.org/abs/2410.08085", "title": "Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study Over Open-ended Question Answering", "authors": ["Yuan Sui", "Yufei He", "Zifeng Ding", "Bryan Hooi"], "categories": ["cs.CL", "cs.AI"], "comment": "This paper has been accepted by ACL 2025", "summary": "Recent works integrating Knowledge Graphs (KGs) have shown promising\nimprovements in enhancing the reasoning capabilities of Large Language Models\n(LLMs). However, existing benchmarks primarily focus on closed-ended tasks,\nleaving a gap in evaluating performance on more complex, real-world scenarios.\nThis limitation also hinders a thorough assessment of KGs' potential to reduce\nhallucinations in LLMs. To address this, we introduce OKGQA, a new benchmark\nspecifically designed to evaluate LLMs augmented with KGs in open-ended,\nreal-world question answering settings. OKGQA reflects practical complexities\nthrough diverse question types and incorporates metrics to quantify both\nhallucination rates and reasoning improvements in LLM+KG models. To consider\nthe scenarios in which KGs may contain varying levels of errors, we propose a\nbenchmark variant, OKGQA-P, to assess model performance when the semantics and\nstructure of KGs are deliberately perturbed and contaminated. In this paper, we\naims to (1) explore whether KGs can make LLMs more trustworthy in an open-ended\nsetting, and (2) conduct a comparative analysis to shed light on method design.\nWe believe this study can facilitate a more complete performance comparison and\nencourages continuous improvement in integrating KGs with LLMs to mitigate\nhallucination, and make LLMs more trustworthy. Code and data are released at\nhttps://github.com/Y-Sui/OKGQA.", "AI": {"tldr": "Introducing OKGQA, a benchmark for evaluating LLMs with KGs in open-ended question answering, addressing gaps in existing benchmarks.", "motivation": "Existing benchmarks for LLMs integrating KGs focus on closed-ended tasks, lacking evaluation for complex real-world scenarios and not assessing hallucination issues adequately.", "method": "The paper proposes the OKGQA benchmark for open-ended question answering with diverse question types and metrics for hallucination rates and reasoning improvements. A variant, OKGQA-P, is introduced to evaluate performance when KGs contain errors.", "result": "OKGQA allows for a more detailed performance comparison of LLMs with KGs in real-world tasks, revealing how KGs can reduce hallucinations and enhance trustworthiness in LLMs.", "conclusion": "The study promotes ongoing enhancements in the integration of KGs with LLMs, addressing hallucination concerns and advancing the reliability of these models.", "key_contributions": ["Introduction of the OKGQA benchmark for evaluating LLMs with KGs", "Development of the OKGQA-P variant for assessing error-prone KGs", "Metrics for quantifying hallucination rates and reasoning capabilities in LLM+KG models"], "limitations": "", "keywords": ["Knowledge Graphs", "Large Language Models", "Open-ended Question Answering", "Hallucination", "Benchmark"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.09338", "pdf": "https://arxiv.org/pdf/2410.09338.pdf", "abs": "https://arxiv.org/abs/2410.09338", "title": "Keys to Robust Edits: from Theoretical Insights to Practical Advances", "authors": ["Jianhao Yan", "Futing Wang", "Yun Luo", "Yafu Li", "Yue Zhang"], "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference", "summary": "Large language models (LLMs) struggle with maintaining accurate knowledge due\nto conflicting/outdated parametric memories. While locate-and-edit methods\naddress this, their reliance on models' internal representations leads to\nrobustness failures in long-context reasoning and paraphrased queries. We\nidentify a fundamental limitation of locate-and-edit methods: existing semantic\nkeys (for memory localization) cannot simultaneously satisfy robustness\n(context-invariant activation) and specificity (precise knowledge\ndiscrimination). Through theoretical error-bound analysis, we establish formal\ncriteria for effective editing. Our solution introduces \\textit{Robust Edit\nPathway (REP)}, a plug-and-play module that: (1) disentangles editing keys from\nnative model representations; (2) dynamically adjusts keys via contrastive\nlearning to achieve robustness-specificity balance. Extensive experiments\nacross various editing methods (ROME/MEMIT/R-ROME/EMMET), existing LLMs\n(LLaMA2, QWen, Mistral), and datasets (CounterFact, ZsRE) show that REP\nimproves success rate over robustness tests by up-to 66.4\\% while maintaining\nthe success rate unaffected. Our code can be found at\nhttps://github.com/ElliottYan/RobustKeyEdit .", "AI": {"tldr": "This paper presents Robust Edit Pathway (REP), a solution to enhance memory localization in large language models (LLMs) while maintaining robustness and specificity in knowledge editing tasks.", "motivation": "LLMs often display inaccuracies in knowledge due to outdated memories, and existing methods struggle with robustness in context and specificity in knowledge editing.", "method": "The paper introduces REP, which disentangles editing keys from the model's native representations and uses contrastive learning to dynamically adjust these keys for better balance between robustness and specificity.", "result": "Experiments demonstrate that REP improves success rates in robustness tests by up to 66.4% while keeping the success rate constant across various editing methods and datasets.", "conclusion": "REP effectively addresses the limitations of current edit methods, leading to substantial improvements in LLM memory editing without sacrificing accuracy.", "key_contributions": ["Introduced Robust Edit Pathway (REP) for memory editing in LLMs", "Enhanced robustness and specificity balance in knowledge editing", "Extensive testing with various LLMs and datasets showing significant improvements"], "limitations": "Potential computational overhead introduced by the dynamic adjustment of keys.", "keywords": ["large language models", "memory editing", "robustness", "specificity", "contrastive learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2410.10347", "pdf": "https://arxiv.org/pdf/2410.10347.pdf", "abs": "https://arxiv.org/abs/2410.10347", "title": "A Unified Approach to Routing and Cascading for LLMs", "authors": ["Jasper Dekoninck", "Maximilian Baader", "Martin Vechev"], "categories": ["cs.CL"], "comment": null, "summary": "The availability of a wide range of large language models (LLMs) embedded in\nvarious agentic systems has significantly increased the potential of model\nselection strategies to improve the cost-performance tradeoff. Existing\nstrategies involve either routing, where a single model is chosen per query, or\ncascading, which sequentially runs increasingly larger models until a\nsatisfactory answer is found. However, current approaches face three key\nlimitations: they (1) lack formal proofs of optimality, (2) fail to identify\nthe conditions under which these strategies are most effective to improve the\ncost-performance tradeoff, and (3) are unable to combine both paradigms for\nfurther improvements. To address these issues, we first derive a novel optimal\nstrategy for cascading and prove the optimality of an existing routing\nstrategy. Further, we propose cascade routing, a unified framework that\nintegrates routing and cascading into a theoretically optimal strategy. Through\nour analysis, we identify good quality estimators as the critical factor for\nthe success of model selection paradigms. Finally, in our experiments, we show\nthat cascade routing consistently outperforms the individual approaches by a\nlarge margin and we analyze quality estimators to determine when routing and/or\ncascading are useful paradigms for model selection.", "AI": {"tldr": "This paper introduces a novel framework called cascade routing that combines routing and cascading strategies for effective model selection in large language models, proving their optimality and showing significant performance improvements.", "motivation": "To address the limitations of existing model selection strategies which include lack of optimality proofs, inability to determine effective conditions, and the failure to integrate routing and cascading approaches.", "method": "The authors derive a new optimal strategy for cascading and prove the existing routing strategy's optimality, followed by proposing a unified framework called cascade routing that integrates both strategies.", "result": "Cascade routing consistently outperforms both routing and cascading strategies in terms of cost-performance tradeoff, underpinning the importance of quality estimators in model selection.", "conclusion": "The proposed cascade routing offers a theoretically optimal model selection strategy and demonstrates the critical role of quality estimators in enhancing performance.", "key_contributions": ["Derivation of a novel optimal strategy for cascading models", "Proof of optimality for an existing routing strategy", "Development of cascade routing framework that integrates routing and cascading approaches"], "limitations": "", "keywords": ["model selection", "large language models", "cascade routing", "cost-performance tradeoff", "quality estimators"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2410.17477", "pdf": "https://arxiv.org/pdf/2410.17477.pdf", "abs": "https://arxiv.org/abs/2410.17477", "title": "Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of Architectural Inductive Biases on Hallucination", "authors": ["Jerry Huang", "Prasanna Parthasarathi", "Mehdi Rezagholizadeh", "Boxing Chen", "Sarath Chandar"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to Findings of The 63rd Annual Meeting of the Association\n  for Computational Linguistics (ACL), 2025", "summary": "The growth in prominence of large language models (LLMs) in everyday life can\nbe largely attributed to their generative abilities, yet some of this is also\nowed to the risks and costs associated with their use. On one front is their\ntendency to hallucinate false or misleading information, limiting their\nreliability. On another is the increasing focus on the computational\nlimitations associated with traditional self-attention based LLMs, which has\nbrought about new alternatives, in particular recurrent models, meant to\novercome them. Yet it remains uncommon to consider these two concerns\nsimultaneously. Do changes in architecture exacerbate/alleviate existing\nconcerns about hallucinations? Do they affect how and where they occur? Through\nan extensive evaluation, we study how these architecture-based inductive biases\naffect the propensity to hallucinate. While hallucination remains a general\nphenomenon not limited to specific architectures, the situations in which they\noccur and the ease with which specific types of hallucinations can be induced\ncan significantly differ based on the model architecture. These findings\nhighlight the need for better understanding both these problems in conjunction\nwith each other, as well as consider how to design more universal techniques\nfor handling hallucinations.", "AI": {"tldr": "The paper investigates the relationship between model architecture and the tendency of large language models (LLMs) to hallucinate information, highlighting significant differences in hallucination types across architectures.", "motivation": "To understand how changes in architecture influence the propensity for LLMs to hallucinate and to address the dual concerns of hallucination reliability and computational limitations.", "method": "An extensive evaluation of different LLM architectures, particularly comparing traditional self-attention based models and recurrent models, focusing on how these architectures affect the occurrence of hallucinations.", "result": "The study finds that while hallucinations are a common issue across LLMs, the frequency and nature of hallucinations vary significantly with model architecture, indicating a need for integrated strategies in LLM design.", "conclusion": "A deeper understanding of the interplay between architecture and hallucination types is essential for developing more robust techniques to mitigate hallucination risks in LLMs.", "key_contributions": ["Identified significant differences in hallucination types based on LLM architecture.", "Provided evidence that model architecture can influence the propensity for hallucinations.", "Highlighted the need to concurrently address hallucination concerns and architectural limitations."], "limitations": "The study focuses primarily on the architectural aspects of hallucinations and may not cover other contributing factors.", "keywords": ["large language models", "hallucination", "model architecture", "inductive biases", "computational limitations"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.22316", "pdf": "https://arxiv.org/pdf/2410.22316.pdf", "abs": "https://arxiv.org/abs/2410.22316", "title": "Understanding Synthetic Context Extension via Retrieval Heads", "authors": ["Xinyu Zhao", "Fangcong Yin", "Greg Durrett"], "categories": ["cs.CL"], "comment": "Published at ICML 2025", "summary": "Long-context LLMs are increasingly in demand for applications such as\nretrieval-augmented generation. To defray the cost of pretraining LLMs over\nlong contexts, recent work takes an approach of synthetic context extension:\nfine-tuning LLMs with synthetically generated long-context data in a\npost-training stage. However, it remains unclear how and why this synthetic\ncontext extension imparts abilities for downstream long-context tasks. In this\npaper, we investigate fine-tuning on synthetic data for three long-context\ntasks that require retrieval and reasoning. We vary the realism of \"needle\"\nconcepts to be retrieved and diversity of the surrounding \"haystack\" context,\nfrom using LLMs to construct synthetic documents to using templated relations\nand creating symbolic datasets. We find that models trained on synthetic data\nfall short of the real data, but surprisingly, the mismatch can be interpreted\nand even predicted in terms of a special set of attention heads that are\nresponsible for retrieval over long context, retrieval heads (Wu et al., 2024).\nThe retrieval heads learned on synthetic data have high overlap with retrieval\nheads learned on real data, and there is a strong correlation between the\nrecall of heads learned and the downstream performance of a model. Furthermore,\nwith attention knockout and activation patching, we mechanistically show that\nretrieval heads are necessary and explain model performance, although they are\nnot totally sufficient. Our results shed light on how to interpret synthetic\ndata fine-tuning performance and how to approach creating better data for\nlearning real-world capabilities over long contexts.", "AI": {"tldr": "The paper investigates the impact of fine-tuning language models with synthetic long-context data on downstream retrieval tasks, revealing insights about attention heads involved in performance.", "motivation": "To understand the effectiveness of synthetic context extension for long-context tasks in language models and identify how this affects retrieval abilities.", "method": "The study fine-tunes models on synthetic data with varying realism and diversity of contexts and evaluates performance on long-context retrieval tasks, analyzing the role of attention heads.", "result": "Models fine-tuned on synthetic data perform worse than those trained on real data, but the performance can be predicted by the behavior of specific attention heads related to retrieval tasks.", "conclusion": "Synthetic data fine-tuning can inform model performance but needs better strategies to create realistic training data to enhance long-context task capabilities.", "key_contributions": ["Investigation of the impact of synthetic context fine-tuning on model performance.", "Identification of retrieval heads and their correlation with retrieval effectiveness.", "Mechanistic explanation of model performance variations based on attention knockout experiments."], "limitations": "Models trained on synthetic data consistently underperform compared to those trained on real data, indicating limitations in realism and validation.", "keywords": ["Long-context LLMs", "Synthetic data", "Attention mechanisms", "Retrieval tasks", "Fine-tuning"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2410.22394", "pdf": "https://arxiv.org/pdf/2410.22394.pdf", "abs": "https://arxiv.org/abs/2410.22394", "title": "AAAR-1.0: Assessing AI's Potential to Assist Research", "authors": ["Renze Lou", "Hanzi Xu", "Sijia Wang", "Jiangshu Du", "Ryo Kamoi", "Xiaoxin Lu", "Jian Xie", "Yuxuan Sun", "Yusen Zhang", "Jihyun Janice Ahn", "Hongchao Fang", "Zhuoyang Zou", "Wenchao Ma", "Xi Li", "Kai Zhang", "Congying Xia", "Lifu Huang", "Wenpeng Yin"], "categories": ["cs.CL"], "comment": "ICML 2025. Project Webpage: https://renzelou.github.io/AAAR-1.0/", "summary": "Numerous studies have assessed the proficiency of AI systems, particularly\nlarge language models (LLMs), in facilitating everyday tasks such as email\nwriting, question answering, and creative content generation. However,\nresearchers face unique challenges and opportunities in leveraging LLMs for\ntheir own work, such as brainstorming research ideas, designing experiments,\nand writing or reviewing papers. In this study, we introduce AAAR-1.0, a\nbenchmark dataset designed to evaluate LLM performance in three fundamental,\nexpertise-intensive research tasks: (i) EquationInference, assessing the\ncorrectness of equations based on the contextual information in paper\nsubmissions; (ii) ExperimentDesign, designing experiments to validate research\nideas and solutions; (iii) PaperWeakness, identifying weaknesses in paper\nsubmissions; and (iv) REVIEWCRITIQUE, identifying each segment in human reviews\nis deficient or not. AAAR-1.0 differs from prior benchmarks in two key ways:\nfirst, it is explicitly research-oriented, with tasks requiring deep domain\nexpertise; second, it is researcher-oriented, mirroring the primary activities\nthat researchers engage in on a daily basis. An evaluation of both open-source\nand proprietary LLMs reveals their potential as well as limitations in\nconducting sophisticated research tasks. We will keep iterating AAAR-1.0 to new\nversions.", "AI": {"tldr": "Introducing AAAR-1.0, a benchmark dataset for evaluating LLMs in research tasks.", "motivation": "To evaluate the performance of LLMs in research-specific tasks where they can assist researchers.", "method": "Development of a benchmark dataset called AAAR-1.0 which includes tasks like EquationInference, ExperimentDesign, PaperWeakness, and REVIEWCRITIQUE.", "result": "Evaluation shows potential and limitations of various LLMs in performing sophisticated research tasks.", "conclusion": "AAAR-1.0 provides a specialized approach to assessing LLM capabilities in research contexts and will be iteratively improved.", "key_contributions": ["Introduction of AAAR-1.0 benchmark tailored for research tasks", "Focus on expertise-intensive tasks for researchers", "Evaluation of LLMs in researcher-oriented activities"], "limitations": "Limited to specific research tasks; ongoing iterations may be required.", "keywords": ["Large Language Models", "Benchmark", "Research Tasks", "Experiment Design", "AI in Research"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2411.02454", "pdf": "https://arxiv.org/pdf/2411.02454.pdf", "abs": "https://arxiv.org/abs/2411.02454", "title": "Graph-based Confidence Calibration for Large Language Models", "authors": ["Yukun Li", "Sijia Wang", "Lifu Huang", "Li-Ping Liu"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Reliable confidence estimation is essential for enhancing the trustworthiness\nof large language models (LLMs), especially in high-stakes scenarios. Despite\nits importance, accurately estimating confidence in LLM responses remains a\nsignificant challenge. In this work, we propose using an auxiliary learning\nmodel to assess response correctness based on the self-consistency of multiple\noutputs generated by the LLM. Our method builds a consistency graph to\nrepresent the agreement among multiple responses and uses a graph neural\nnetwork (GNN) to estimate the likelihood that each response is correct.\nExperiments demonstrate that this method has strong calibration performance on\nvarious benchmark datasets and generalizes well to out-of-domain cases.", "AI": {"tldr": "This paper proposes an auxiliary learning model using graph neural networks to improve confidence estimation in large language model responses.", "motivation": "Accurate confidence estimation in large language models is crucial for trustworthiness in high-stakes environments, yet remains challenging.", "method": "The proposed method uses a consistency graph to represent agreement among multiple LLM responses and employs a graph neural network to estimate response correctness.", "result": "Experiments show strong calibration performance on benchmark datasets, with good generalization to out-of-domain cases.", "conclusion": "The proposed approach enhances the reliability of confidence estimation for LLM responses, which is vital in high-risk applications.", "key_contributions": ["Introduction of a consistency graph for response evaluation.", "Utilization of graph neural networks for confidence estimation.", "Demonstrated strong calibration performance across various benchmarks."], "limitations": "", "keywords": ["confidence estimation", "large language models", "graph neural networks", "self-consistency", "calibration"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2411.04847", "pdf": "https://arxiv.org/pdf/2411.04847.pdf", "abs": "https://arxiv.org/abs/2411.04847", "title": "Prompt-Guided Internal States for Hallucination Detection of Large Language Models", "authors": ["Fujie Zhang", "Peiqi Yu", "Biao Yi", "Baolei Zhang", "Tong Li", "Zheli Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks in different domains. However, they sometimes generate\nresponses that are logically coherent but factually incorrect or misleading,\nwhich is known as LLM hallucinations. Data-driven supervised methods train\nhallucination detectors by leveraging the internal states of LLMs, but\ndetectors trained on specific domains often struggle to generalize well to\nother domains. In this paper, we aim to enhance the cross-domain performance of\nsupervised detectors with only in-domain data. We propose a novel framework,\nprompt-guided internal states for hallucination detection of LLMs, namely\nPRISM. By utilizing appropriate prompts to guide changes to the structure\nrelated to text truthfulness in LLMs' internal states, we make this structure\nmore salient and consistent across texts from different domains. We integrated\nour framework with existing hallucination detection methods and conducted\nexperiments on datasets from different domains. The experimental results\nindicate that our framework significantly enhances the cross-domain\ngeneralization of existing hallucination detection methods.", "AI": {"tldr": "This paper introduces PRISM, a framework designed to improve the cross-domain performance of hallucination detectors for large language models by utilizing prompt-guided internal states.", "motivation": "To address the limitations of hallucination detectors trained on specific domains, which often struggle to generalize across different domains.", "method": "The authors proposed PRISM, a prompt-guided framework that modifies LLM internal states to enhance the salience and consistency of text truthfulness across domains.", "result": "The experimental results demonstrate that PRISM significantly improves the cross-domain generalization of hallucination detection methods.", "conclusion": "Integrating PRISM with existing hallucination detection techniques leads to better performance in recognizing factually incorrect outputs from LLMs across diverse domains.", "key_contributions": ["Introduction of PRISM framework for hallucination detection", "Enhanced cross-domain performance for existing detectors", "Experimental validation across multiple datasets"], "limitations": "", "keywords": ["Large Language Models", "hallucination detection", "cross-domain generalization", "prompt-guided framework", "text truthfulness"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2412.01031", "pdf": "https://arxiv.org/pdf/2412.01031.pdf", "abs": "https://arxiv.org/abs/2412.01031", "title": "Evaluating Automated Radiology Report Quality through Fine-Grained Phrasal Grounding of Clinical Findings", "authors": ["Razi Mahmood", "Pingkun Yan", "Diego Machado Reyes", "Ge Wang", "Mannudeep K. Kalra", "Parisa Kaviani", "Joy T. Wu", "Tanveer Syeda-Mahmood"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Several evaluation metrics have been developed recently to automatically\nassess the quality of generative AI reports for chest radiographs based only on\ntextual information using lexical, semantic, or clinical named entity\nrecognition methods. In this paper, we develop a new method of report quality\nevaluation by first extracting fine-grained finding patterns capturing the\nlocation, laterality, and severity of a large number of clinical findings. We\nthen performed phrasal grounding to localize their associated anatomical\nregions on chest radiograph images. The textual and visual measures are then\ncombined to rate the quality of the generated reports. We present results that\ncompare this evaluation metric with other textual metrics on a gold standard\ndataset derived from the MIMIC collection and show its robustness and\nsensitivity to factual errors.", "AI": {"tldr": "A new method for evaluating the quality of AI-generated chest radiograph reports combines textual and visual measures by extracting detailed clinical findings and localizing them on images.", "motivation": "To improve the automated assessment of generative AI reports for chest radiographs using both textual and visual information.", "method": "Extract fine-grained finding patterns from reports that denote location, laterality, and severity of clinical findings, then perform phrasal grounding to associate these findings with anatomical regions on chest radiographs. Combine these measures to evaluate report quality.", "result": "The proposed evaluation metric shows robustness and sensitivity to factual errors when compared to existing textual metrics on a gold standard dataset from the MIMIC collection.", "conclusion": "This new evaluation approach can enhance the quality assessment of AI-generated medical reports by integrating both textual content and corresponding visual data.", "key_contributions": ["Developed a fine-grained method for clinical finding extraction", "Introduced phrasal grounding for anatomical localization", "Demonstrated better robustness and sensitivity through empirical comparison"], "limitations": "", "keywords": ["AI in healthcare", "chest radiography", "report quality evaluation", "medical imaging", "natural language processing"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2412.06272", "pdf": "https://arxiv.org/pdf/2412.06272.pdf", "abs": "https://arxiv.org/abs/2412.06272", "title": "Evaluating LLM-based Approaches to Legal Citation Prediction: Domain-specific Pre-training, Fine-tuning, or RAG? A Benchmark and an Australian Law Case Study", "authors": ["Jiuzhou Han", "Paul Burgess", "Ehsan Shareghi"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "For code, data, and models see https://auslawbench.github.io", "summary": "Large Language Models (LLMs) have demonstrated strong potential across legal\ntasks, yet the problem of legal citation prediction remains under-explored. At\nits core, this task demands fine-grained contextual understanding and precise\nidentification of relevant legislation or precedent. We introduce the AusLaw\nCitation Benchmark, a real-world dataset comprising 55k Australian legal\ninstances and 18,677 unique citations which to the best of our knowledge is the\nfirst of its scale and scope. We then conduct a systematic benchmarking across\na range of solutions: (i) standard prompting of both general and\nlaw-specialised LLMs, (ii) retrieval-only pipelines with both generic and\ndomain-specific embeddings, (iii) supervised fine-tuning, and (iv) several\nhybrid strategies that combine LLMs with retrieval augmentation through query\nexpansion, voting ensembles, or re-ranking. Results show that neither general\nnor law-specific LLMs suffice as stand-alone solutions, with performance near\nzero. Instruction tuning (of even a generic open-source LLM) on task-specific\ndataset is among the best performing solutions. We highlight that database\ngranularity along with the type of embeddings play a critical role in\nretrieval-based approaches, with hybrid methods which utilise a trained\nre-ranker delivering the best results. Despite this, a performance gap of\nnearly 50% remains, underscoring the value of this challenging benchmark as a\nrigorous test-bed for future research in legal-domain.", "AI": {"tldr": "The paper introduces the AusLaw Citation Benchmark, a dataset for legal citation prediction using LLMs, and evaluates various methods, finding that instruction tuning on task-specific datasets yields the best performance.", "motivation": "Legal citation prediction is a significant yet under-explored task that requires deep contextual understanding. The introduction of a comprehensive dataset aims to advance research in this area.", "method": "The paper benchmarks several approaches which include standard prompting of LLMs, retrieval-only pipelines, supervised fine-tuning, and hybrid strategies combining LLMs with retrieval techniques.", "result": "The study found that both general and law-specific LLMs perform poorly as standalone solutions, with the best results from instruction tuning on a task-specific dataset and hybrid methods utilizing trained re-rankers.", "conclusion": "The AusLaw Citation Benchmark highlights the challenges in legal citation prediction and indicates significant room for improvement, serving as a foundation for future research in the legal domain.", "key_contributions": ["Introduced the AusLaw Citation Benchmark dataset for legal citation prediction.", "Evaluated multiple methods for citation prediction, finding instruction tuning and hybrid methods most effective.", "Identified key factors affecting performance, including database granularity and type of embeddings."], "limitations": "There remains a significant performance gap of nearly 50%, indicating that current methods are not sufficient for optimal citation prediction.", "keywords": ["Large Language Models", "legal citation prediction", "benchmark dataset", "supervised fine-tuning", "retrieval augmentation"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2412.07367", "pdf": "https://arxiv.org/pdf/2412.07367.pdf", "abs": "https://arxiv.org/abs/2412.07367", "title": "My Words Imply Your Opinion: Reader Agent-based Propagation Enhancement for Personalized Implicit Emotion Analysis", "authors": ["Jian Liao", "Yu Feng", "Yujin Zheng", "Jun Zhao", "Suge Wang", "Jianxing Zheng"], "categories": ["cs.CL"], "comment": null, "summary": "The subtlety of emotional expressions makes implicit emotion analysis (IEA)\nparticularly sensitive to user-specific characteristics. Current studies\npersonalize emotion analysis by focusing on the author but neglect the impact\nof the intended reader on implicit emotional feedback. In this paper, we\nintroduce Personalized IEA (PIEA) and present the RAPPIE model, which addresses\nsubjective variability by incorporating reader feedback. In particular, (1) we\ncreate reader agents based on large language models to simulate reader\nfeedback, overcoming the issue of ``spiral of silence effect'' and data\nincompleteness of real reader reaction. (2) We develop a role-aware multi-view\ngraph learning to model the emotion interactive propagation process in\nscenarios with sparse reader information. (3) We construct two new PIEA\ndatasets covering English and Chinese social media with detailed user metadata,\naddressing the text-centric limitation of existing datasets. Extensive\nexperiments show that RAPPIE significantly outperforms state-of-the-art\nbaselines, demonstrating the value of incorporating reader feedback in PIEA.", "AI": {"tldr": "The paper proposes the RAPPIE model for Personalized Implicit Emotion Analysis (PIEA), which incorporates reader feedback to enhance emotion analysis.", "motivation": "To address the limitations of implicit emotion analysis (IEA) by considering the impact of intended readers, in addition to the authors.", "method": "The authors introduce reader agents modeled after large language models to simulate feedback and a role-aware multi-view graph learning approach to handle limited reader information.", "result": "Extensive experiments demonstrate that RAPPIE outperforms current state-of-the-art approaches in emotion analysis by effectively incorporating reader feedback.", "conclusion": "Incorporating reader feedback in emotion analysis significantly improves the accuracy and robustness of personalized implicit emotion detection.", "key_contributions": ["Introduction of the RAPPIE model for Personalized Implicit Emotion Analysis", "Development of reader agents using large language models to simulate feedback", "Creation of new PIEA datasets for English and Chinese social media featuring detailed user metadata."], "limitations": "The study may have limitations in the generalizability of the reader agents across diverse cultures and contexts.", "keywords": ["Personalized IEA", "reader feedback", "emotion analysis", "large language models", "graph learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2412.12459", "pdf": "https://arxiv.org/pdf/2412.12459.pdf", "abs": "https://arxiv.org/abs/2412.12459", "title": "LITA: An Efficient LLM-assisted Iterative Topic Augmentation Framework", "authors": ["Chia-Hsuan Chang", "Jui-Tse Tsai", "Yi-Hang Tsai", "San-Yih Hwang"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted to PAKDD 2025", "summary": "Topic modeling is widely used for uncovering thematic structures within text\ncorpora, yet traditional models often struggle with specificity and coherence\nin domain-focused applications. Guided approaches, such as SeededLDA and CorEx,\nincorporate user-provided seed words to improve relevance but remain\nlabor-intensive and static. Large language models (LLMs) offer potential for\ndynamic topic refinement and discovery, yet their application often incurs high\nAPI costs. To address these challenges, we propose the LLM-assisted Iterative\nTopic Augmentation framework (LITA), an LLM-assisted approach that integrates\nuser-provided seeds with embedding-based clustering and iterative refinement.\nLITA identifies a small number of ambiguous documents and employs an LLM to\nreassign them to existing or new topics, minimizing API costs while enhancing\ntopic quality. Experiments on two datasets across topic quality and clustering\nperformance metrics demonstrate that LITA outperforms five baseline models,\nincluding LDA, SeededLDA, CorEx, BERTopic, and PromptTopic. Our work offers an\nefficient and adaptable framework for advancing topic modeling and text\nclustering.", "AI": {"tldr": "The LLM-assisted Iterative Topic Augmentation framework (LITA) enhances topic modeling by integrating user seed words with LLMs for dynamic refinement, outperforming several traditional models in quality and performance.", "motivation": "Traditional topic modeling struggles with specificity in domain-focused applications, and existing guided methods remain labor-intensive and static.", "method": "LITA integrates user-provided seeds with embedding-based clustering and iterative refinement to improve topic quality while minimizing API costs.", "result": "LITA outperforms five baseline models, including LDA, SeededLDA, CorEx, BERTopic, and PromptTopic, in both topic quality and clustering performance metrics.", "conclusion": "LITA offers an efficient and adaptable framework for advancing topic modeling and text clustering.", "key_contributions": ["Introduction of LITA for dynamic topic modeling", "Integration of LLMs with user seed words for efficient topic refinement", "Demonstration of improved performance over traditional models"], "limitations": "", "keywords": ["topic modeling", "LLM", "text clustering", "semantic refinement", "machine learning"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2412.12505", "pdf": "https://arxiv.org/pdf/2412.12505.pdf", "abs": "https://arxiv.org/abs/2412.12505", "title": "DocFusion: A Unified Framework for Document Parsing Tasks", "authors": ["Mingxu Chai", "Ziyu Shen", "Chong Zhang", "Yue Zhang", "Xiao Wang", "Shihan Dou", "Jihua Kang", "Jiazheng Zhang", "Qi Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Document parsing is essential for analyzing complex document structures and\nextracting fine-grained information, supporting numerous downstream\napplications. However, existing methods often require integrating multiple\nindependent models to handle various parsing tasks, leading to high complexity\nand maintenance overhead. To address this, we propose DocFusion, a lightweight\ngenerative model with only 0.28B parameters. It unifies task representations\nand achieves collaborative training through an improved objective function.\nExperiments reveal and leverage the mutually beneficial interaction among\nrecognition tasks, and integrating recognition data significantly enhances\ndetection performance. The final results demonstrate that DocFusion achieves\nstate-of-the-art (SOTA) performance across four key tasks.", "AI": {"tldr": "DocFusion is a lightweight generative model that unifies document parsing tasks into a single framework, achieving SOTA performance.", "motivation": "Address the complexity and maintenance overhead of existing document parsing methods that often rely on multiple independent models.", "method": "DocFusion uses a collaborative training approach with an improved objective function to unify task representations in document parsing.", "result": "Achieves state-of-the-art performance across four key document parsing tasks, benefiting from the interaction among recognition tasks and enhanced detection through integrated recognition data.", "conclusion": "DocFusion demonstrates that a single, unified model can effectively handle multiple document parsing tasks with improved performance and reduced complexity.", "key_contributions": ["Introduction of DocFusion as a lightweight generative model for document parsing", "Unification of multiple parsing tasks into a single collaborative framework", "State-of-the-art performance across key document recognition tasks"], "limitations": "", "keywords": ["Document Parsing", "Generative Model", "Collaborative Training", "Machine Learning", "State-of-the-Art Performance"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2412.16555", "pdf": "https://arxiv.org/pdf/2412.16555.pdf", "abs": "https://arxiv.org/abs/2412.16555", "title": "Divide and Conquer: A Hybrid Strategy Defeats Multimodal Large Language Models", "authors": ["Yanxu Mao", "Peipei Liu", "Tiehan Cui", "Zhaoteng Yan", "Congying Liu", "Datao You"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are widely applied in various fields of society\ndue to their powerful reasoning, understanding, and generation capabilities.\nHowever, the security issues associated with these models are becoming\nincreasingly severe. Jailbreaking attacks, as an important method for detecting\nvulnerabilities in LLMs, have been explored by researchers who attempt to\ninduce these models to generate harmful content through various attack methods.\nNevertheless, existing jailbreaking methods face numerous limitations, such as\nexcessive query counts, limited coverage of jailbreak modalities, low attack\nsuccess rates, and simplistic evaluation methods. To overcome these\nconstraints, this paper proposes a multimodal jailbreaking method: JMLLM. This\nmethod integrates multiple strategies to perform comprehensive jailbreak\nattacks across text, visual, and auditory modalities. Additionally, we\ncontribute a new and comprehensive dataset for multimodal jailbreaking\nresearch: TriJail, which includes jailbreak prompts for all three modalities.\nExperiments on the TriJail dataset and the benchmark dataset AdvBench,\nconducted on 13 popular LLMs, demonstrate advanced attack success rates and\nsignificant reduction in time overhead.", "AI": {"tldr": "This paper presents JMLLM, a multimodal jailbreaking method for large language models (LLMs) to address security vulnerabilities, alongside a new dataset called TriJail for research.", "motivation": "To address the growing security issues related to large language models (LLMs) and improve existing jailbreaking methods which suffer from high query counts, limited modality coverage, and low success rates.", "method": "The paper proposes a new multimodal jailbreaking method, JMLLM, which incorporates various strategies for text, visual, and auditory attacks and introduces the TriJail dataset for comprehensive multimodal research.", "result": "Experiments show that JMLLM achieves higher attack success rates and reduced time overhead on both the TriJail and AdvBench datasets across 13 popular LLMs.", "conclusion": "The proposed method and dataset provide significant advancements in the detection of vulnerabilities in LLMs, paving the way for more robust security measures.", "key_contributions": ["Introduction of JMLLM as a multimodal jailbreaking method", "Development of the TriJail dataset for multimodal research", "Demonstration of improved attack success rates on multiple LLMs"], "limitations": "The paper does not address potential ethical implications of jailbreaking LLMs or the long-term effectiveness of the proposed methods against evolving security measures.", "keywords": ["Large Language Models", "Jailbreaking", "Multimodal Attack", "Dataset", "Security"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2412.17933", "pdf": "https://arxiv.org/pdf/2412.17933.pdf", "abs": "https://arxiv.org/abs/2412.17933", "title": "BenCzechMark : A Czech-centric Multitask and Multimetric Benchmark for Large Language Models with Duel Scoring Mechanism", "authors": ["Martin Fajcik", "Martin Docekal", "Jan Dolezal", "Karel Ondrej", "Karel Beneš", "Jan Kapsa", "Pavel Smrz", "Alexander Polok", "Michal Hradis", "Zuzana Neverilova", "Ales Horak", "Radoslav Sabol", "Michal Stefanik", "Adam Jirkovsky", "David Adamczyk", "Petr Hyner", "Jan Hula", "Hynek Kydlicek"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to TACL", "summary": "We present BenCzechMark (BCM), the first comprehensive Czech language\nbenchmark designed for large language models, offering diverse tasks, multiple\ntask formats, and multiple evaluation metrics. Its duel scoring system is\ngrounded in statistical significance theory and uses aggregation across tasks\ninspired by social preference theory. Our benchmark encompasses 50 challenging\ntasks, with corresponding test datasets, primarily in native Czech, with 14\nnewly collected ones. These tasks span 8 categories and cover diverse domains,\nincluding historical Czech news, essays from pupils or language learners, and\nspoken word. Furthermore, we collect and clean BUT-Large Czech Collection, the\nlargest publicly available clean Czech language corpus, and use it for (i)\ncontamination analysis and (ii) continuous pretraining of the first\nCzech-centric 7B language model with Czech-specific tokenization. We use our\nmodel as a baseline for comparison with publicly available multilingual models.\nLastly, we release and maintain a leaderboard with existing 50 model\nsubmissions, where new model submissions can be made at\nhttps://huggingface.co/spaces/CZLC/BenCzechMark.", "AI": {"tldr": "BenCzechMark (BCM) is the first benchmark for Czech language large language models, featuring 50 tasks across diverse domains and a novel duel scoring system.", "motivation": "To establish a comprehensive evaluation framework for large language models in the Czech language, addressing the lack of resources and benchmarks tailored for this language.", "method": "Developed a benchmark with 50 task variations and evaluation metrics grounded in statistical significance and social preference theories. Collected and cleaned a large Czech language corpus for continuous pretraining of a Czech-specific language model.", "result": "Established a baseline Czech-centric 7B language model and a leaderboard hosting 50 model submissions for comparative evaluation with multilingual models.", "conclusion": "BenCzechMark provides a crucial resource for advancing Czech language model development and evaluation, enabling better performance assessments and fostering innovation in Czech NLP applications.", "key_contributions": ["First comprehensive Czech language benchmark for LLMs", "Duel scoring system based on statistical significance and social preference theory", "Release of the largest clean Czech language corpus and baseline model"], "limitations": "", "keywords": ["Czech language", "benchmark", "large language models", "NLP", "machine learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2501.18101", "pdf": "https://arxiv.org/pdf/2501.18101.pdf", "abs": "https://arxiv.org/abs/2501.18101", "title": "Diverse Preference Optimization", "authors": ["Jack Lanchantin", "Angelica Chen", "Shehzaad Dhuliawala", "Ping Yu", "Jason Weston", "Sainbayar Sukhbaatar", "Ilia Kulikov"], "categories": ["cs.CL"], "comment": null, "summary": "Post-training of language models, either through reinforcement learning,\npreference optimization or supervised finetuning, tends to sharpen the output\nprobability distribution and reduce the diversity of generated responses. This\nis particularly a problem for creative generative tasks where varied responses\nare desired. In this work we introduce Diverse Preference Optimization (DivPO),\nan optimization method which learns to generate much more diverse responses\nthan standard pipelines, while maintaining the quality of the generations. In\nDivPO, preference pairs are selected by first considering a pool of responses,\nand a measure of diversity among them, and selecting chosen examples as being\nmore rare but high quality, while rejected examples are more common, but low\nquality. DivPO results in generating 45.6% more diverse persona attributes, and\na 74.6% increase in story diversity, while maintaining similar win rates as\nstandard baselines. On general instruction following, DivPO results in a 46.2%\nincrease in diversity, and a 2.4% winrate improvement compared to DPO.", "AI": {"tldr": "Introducing Diverse Preference Optimization (DivPO) to enhance response diversity in language models during creative tasks without sacrificing quality.", "motivation": "Current post-training methods reduce response diversity, which is problematic for creative tasks that require varied outputs.", "method": "DivPO selects preference pairs from a pool of responses, focusing on those that are rare yet high quality versus common but low quality.", "result": "DivPO generates 45.6% more diverse persona attributes and 74.6% more story diversity while maintaining win rates.", "conclusion": "DivPO offers significant improvements in response diversity for language models, applicable in guiding creative tasks effectively.", "key_contributions": ["Introduces Diverse Preference Optimization (DivPO) for language models.", "Demonstrates significant increases in response diversity while maintaining quality.", "Establishes a new approach for selecting diverse preference pairs."], "limitations": "", "keywords": ["Diverse Preference Optimization", "language models", "response diversity", "creative generative tasks", "preference optimization"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2502.00675", "pdf": "https://arxiv.org/pdf/2502.00675.pdf", "abs": "https://arxiv.org/abs/2502.00675", "title": "ReFoRCE: A Text-to-SQL Agent with Self-Refinement, Consensus Enforcement, and Column Exploration", "authors": ["Minghang Deng", "Ashwin Ramachandran", "Canwen Xu", "Lanxiang Hu", "Zhewei Yao", "Anupam Datta", "Hao Zhang"], "categories": ["cs.CL", "I.2.7; I.2.0; H.2.0"], "comment": "32 pages, 2 figures", "summary": "We present ReFoRCE, a Text-to-SQL agent that tops the Spider 2.0\nleaderboard--a challenging benchmark reflecting complex, real-world Text-to-SQL\nscenarios. While Text-to-SQL systems enable natural language queries over\nstructured databases, deploying them in enterprise environments remains\ndifficult due to large, complex schemas (with over 1,000 columns), diverse SQL\ndialects (e.g., BigQuery, Snowflake), and sophisticated query requirements\n(e.g., transformations and analytics). ReFoRCE addresses these challenges\nthrough: (a) database information compression via pattern-based table grouping\nand LLM-guided schema linking to alleviate long-context issues; (b)\nself-refinement to iteratively correct syntax and semantic errors across\ndialects; (c) majority-vote consensus to select high-confidence candidates\nwhile deferring ambiguous cases arising from sophisticated queries; and (d)\niterative column exploration guided by execution feedback to resolve those\ndeferred cases. ReFoRCE achieves new state-of-the-art results, with scores of\n35.83 on Spider 2.0-Snow and 36.56 on Spider 2.0-Lite.", "AI": {"tldr": "ReFoRCE is a top-performing Text-to-SQL agent that tackles challenges in natural language querying over complex databases.", "motivation": "Deployment of Text-to-SQL systems in enterprise settings is hindered by large schemas, various SQL dialects, and complex query requirements.", "method": "ReFoRCE employs database information compression, self-refinement for correction, majority-vote consensus for candidate selection, and iterative column exploration based on execution feedback.", "result": "ReFoRCE achieves state-of-the-art scores of 35.83 on Spider 2.0-Snow and 36.56 on Spider 2.0-Lite benchmarks.", "conclusion": "ReFoRCE significantly improves the ability to perform complex natural language queries over structured data.", "key_contributions": ["Top scores on Spider 2.0 benchmarks", "Innovative database information compression techniques", "Effective self-refinement and consensus mechanisms"], "limitations": "", "keywords": ["Text-to-SQL", "Natural Language Processing", "Large Language Models"], "importance_score": 4, "read_time_minutes": 20}}
{"id": "2502.00761", "pdf": "https://arxiv.org/pdf/2502.00761.pdf", "abs": "https://arxiv.org/abs/2502.00761", "title": "FIRE: Flexible Integration of Data Quality Ratings for Effective Pre-Training", "authors": ["Liangyu Xu", "Xuemiao Zhang", "Feiyu Duan", "Sirui Wang", "Rongxiang Weng", "Jingang Wang", "Xunliang Cai"], "categories": ["cs.CL"], "comment": "21 pages, 11 figures", "summary": "Selecting high-quality data can improve the pretraining efficiency of large\nlanguage models (LLMs). Existing methods generally rely on heuristic techniques\nor single quality signals, limiting their ability to evaluate data quality\ncomprehensively. In this work, we propose FIRE, a flexible and scalable\nframework for integrating multiple data quality raters, which allows for a\ncomprehensive assessment of data quality across various dimensions. FIRE aligns\nmultiple quality signals into a unified space, and integrates diverse data\nquality raters to provide a comprehensive quality signal for each data point.\nFurther, we introduce a progressive data selection scheme based on FIRE that\niteratively refines the selection of high-quality data points. Extensive\nexperiments show that FIRE outperforms other data selection methods and\nsignificantly boosts pretrained model performance across a wide range of\ndownstream tasks, while requiring less than 37.5\\% of the training data needed\nby the Random baseline to reach the target performance.", "AI": {"tldr": "FIRE is a new framework for assessing and selecting high-quality data for large language models, integrating multiple quality signals to enhance pretraining efficiency and model performance.", "motivation": "To improve the efficiency of large language model pretraining through better selection of high-quality training data.", "method": "FIRE integrates multiple data quality raters into a unified framework and uses a progressive data selection scheme for iterative refinement of data quality selection.", "result": "FIRE outperforms existing data selection methods and enhances pretrained model performance, achieving target performance with significantly less training data compared to the Random baseline.", "conclusion": "FIRE provides a comprehensive approach to data quality evaluation and selection, leading to improved outcomes in various downstream tasks.", "key_contributions": ["Introduction of the FIRE framework for data quality assessment", "Integration of multiple data quality signals", "Development of a progressive data selection scheme"], "limitations": "", "keywords": ["data quality", "large language models", "data selection", "machine learning", "pretraining"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.04380", "pdf": "https://arxiv.org/pdf/2502.04380.pdf", "abs": "https://arxiv.org/abs/2502.04380", "title": "Diversity as a Reward: Fine-Tuning LLMs on a Mixture of Domain-Undetermined Data", "authors": ["Zhenqing Ling", "Daoyuan Chen", "Liuyi Yao", "Qianli Shen", "Yaliang Li", "Ying Shen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "33 pages, 20 figures, 21 tables", "summary": "Fine-tuning large language models (LLMs) using diverse datasets is crucial\nfor enhancing their overall performance across various domains. In practical\nscenarios, existing methods based on modeling the mixture proportions of data\ncomposition often struggle with data whose domain labels are missing, imprecise\nor non-normalized, while methods based on data selection usually encounter\ndifficulties in balancing multi-domain performance. To address these\nchallenges, in this work, we investigate the role of data diversity in\nenhancing the overall abilities of LLMs by empirically constructing contrastive\ndata pools and theoretically deriving explanations. Building upon the insights\ngained, we propose a new method that gives the LLM a dual identity: an output\nmodel to cognitively probe and select data based on diversity reward, as well\nas an input model to be tuned with the selected data. Extensive experiments\nshow that the proposed method notably boosts performance across\ndomain-undetermined data and a series of foundational downstream tasks when\napplied to various advanced LLMs. We release our code and hope this study can\nshed light on the understanding of data diversity and advance feedback-driven\ndata-model co-design for LLMs.", "AI": {"tldr": "The paper investigates the impact of data diversity on fine-tuning large language models and proposes a method that utilizes a dual model identity to enhance performance across domains.", "motivation": "Fine-tuning LLMs effectively requires addressing issues with data composition, especially when domain labels are missing or imprecise.", "method": "The study creates contrastive data pools and proposes a dual identity model for LLMs—an output model for selecting diverse data and an input model for tuning with that data.", "result": "The proposed method significantly improves performance on domain-undetermined data and foundational downstream tasks across various advanced LLMs.", "conclusion": "This research enhances understanding of data diversity's role in LLM fine-tuning and emphasizes feedback-driven co-design between data and models.", "key_contributions": ["Investigation of data diversity's role in LLM performance.", "Proposed dual identity model for improved data selection and tuning.", "Empirical evidence showing performance boosts across various domains."], "limitations": "The effectiveness of the method in domains with fully normalized data remains untested.", "keywords": ["Large Language Models", "Data Diversity", "Fine-tuning", "Contrastive Data Pools", "Feedback-driven Co-design"], "importance_score": 9, "read_time_minutes": 25}}
{"id": "2502.06086", "pdf": "https://arxiv.org/pdf/2502.06086.pdf", "abs": "https://arxiv.org/abs/2502.06086", "title": "Is a Peeled Apple Still Red? Evaluating LLMs' Ability for Conceptual Combination with Property Type", "authors": ["Seokwon Song", "Taehyun Lee", "Jaewoo Ahn", "Jae Hyuk Sung", "Gunhee Kim"], "categories": ["cs.CL"], "comment": "NAACL 2025 Oral", "summary": "Conceptual combination is a cognitive process that merges basic concepts,\nenabling the creation of complex expressions. During this process, the\nproperties of combination (e.g., the whiteness of a peeled apple) can be\ninherited from basic concepts, newly emerge, or be canceled. However, previous\nstudies have evaluated a limited set of properties and have not examined the\ngenerative process. To address this gap, we introduce the Conceptual\nCombination with Property Type dataset (CCPT), which consists of 12.3K\nannotated triplets of noun phrases, properties, and property types. Using CCPT,\nwe establish three types of tasks to evaluate LLMs for conceptual combination\nthoroughly. Our key findings are threefold: (1) Our automatic metric grading\nproperty emergence and cancellation closely corresponds with human judgments.\n(2) LLMs, including OpenAI's o1, struggle to generate noun phrases which\npossess given emergent properties. (3) Our proposed method, inspired by\ncognitive psychology model that explains how relationships between concepts are\nformed, improves performances in all generative tasks. The dataset and\nexperimental code are available at https://github.com/seokwon99/CCPT.git.", "AI": {"tldr": "This paper introduces the Conceptual Combination with Property Type dataset (CCPT) for evaluating LLMs in the cognitive process of conceptual combination.", "motivation": "To address the gaps in understanding the generative process of conceptual combination and evaluate LLMs on this task.", "method": "Three tasks are established using the CCPT dataset, which contains 12.3K annotated triplets of noun phrases, properties, and property types, to assess LLM performance.", "result": "The study finds that automatic metrics for grading property emergence correlate with human judgments, while LLMs struggle to generate noun phrases with specified emergent properties. A method based on cognitive psychology improves generative task performance.", "conclusion": "The proposed CCPT dataset and evaluation methods enhance understanding of conceptual combination and LLM capabilities, contributing to AI and cognitive modeling fields.", "key_contributions": ["Introduction of the CCPT dataset for evaluating conceptual combination in LLMs", "Demonstration of a correlation between automatic grading metrics and human judgments", "Development of a cognitive psychology-inspired method that improves LLM performance in generative tasks."], "limitations": "The study focuses primarily on noun phrases and may not generalize to other types of linguistic expressions.", "keywords": ["conceptual combination", "LLMs", "property emergence", "dataset", "cognitive psychology"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2502.06139", "pdf": "https://arxiv.org/pdf/2502.06139.pdf", "abs": "https://arxiv.org/abs/2502.06139", "title": "LCIRC: A Recurrent Compression Approach for Efficient Long-form Context and Query Dependent Modeling in LLMs", "authors": ["Sumin An", "Junyoung Sung", "Wonpyo Park", "Chanjun Park", "Paul Hongsuck Seo"], "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025. Project Page:\n  https://ssuminan.github.io/LCIRC/", "summary": "While large language models (LLMs) excel in generating coherent and\ncontextually rich outputs, their capacity to efficiently handle long-form\ncontexts is limited by fixed-length position embeddings. Additionally, the\ncomputational cost of processing long sequences increases quadratically, making\nit challenging to extend context length. To address these challenges, we\npropose Long-form Context Injection with Recurrent Compression (LCIRC), a\nmethod that enables the efficient processing long-form sequences beyond the\nmodel's length limit through recurrent compression without retraining the\nentire model. We further introduce query dependent context modeling, which\nselectively compresses query-relevant information, ensuring that the model\nretains the most pertinent content. Our empirical results demonstrate that\nQuery Dependent LCIRC (QD-LCIRC) significantly improves LLM's ability to manage\nextended contexts, making it well-suited for tasks that require both\ncomprehensive context understanding and query relevance.", "AI": {"tldr": "Proposes Long-form Context Injection with Recurrent Compression (LCIRC) to improve LLMs' handling of long sequences without retraining.", "motivation": "LLMs struggle with long contexts due to fixed-length position embeddings and high computational costs.", "method": "Introduced LCIRC to efficiently process long sequences through recurrent compression and query dependent context modeling.", "result": "Empirical results show that QD-LCIRC enhances LLMs' capabilities in managing extended contexts effectively.", "conclusion": "The proposed method allows for better context understanding in tasks requiring comprehensive information and query relevance.", "key_contributions": ["Introduced the LCIRC method for long-form sequence processing", "Developed query dependent context modeling for selective compression", "Demonstrated significant improvements in managing long contexts with QD-LCIRC"], "limitations": "", "keywords": ["Large Language Models", "Context Injection", "Recurrent Compression", "Query Dependent", "Long-form Context"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.06205", "pdf": "https://arxiv.org/pdf/2502.06205.pdf", "abs": "https://arxiv.org/abs/2502.06205", "title": "C-3PO: Compact Plug-and-Play Proxy Optimization to Achieve Human-like Retrieval-Augmented Generation", "authors": ["Guoxin Chen", "Minpeng Liao", "Peiying Yu", "Dingmin Wang", "Zile Qiao", "Chao Yang", "Xin Zhao", "Kai Fan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Camera ready version for ICML 2025", "summary": "Retrieval-augmented generation (RAG) systems face a fundamental challenge in\naligning independently developed retrievers and large language models (LLMs).\nExisting approaches typically involve modifying either component or introducing\nsimple intermediate modules, resulting in practical limitations and sub-optimal\nperformance. Inspired by human search behavior -- typically involving a\nback-and-forth process of proposing search queries and reviewing documents, we\npropose C-3PO, a proxy-centric framework that facilitates communication between\nretrievers and LLMs through a lightweight multi-agent system. Our framework\nimplements three specialized agents that collaboratively optimize the entire\nRAG pipeline without altering the retriever and LLMs. These agents work\ntogether to assess the need for retrieval, generate effective queries, and\nselect information suitable for the LLMs. To enable effective multi-agent\ncoordination, we develop a tree-structured rollout approach for reward credit\nassignment in reinforcement learning. Extensive experiments in both in-domain\nand out-of-distribution scenarios demonstrate that C-3PO significantly enhances\nRAG performance while maintaining plug-and-play flexibility and superior\ngeneralization capabilities.", "AI": {"tldr": "C-3PO is a multi-agent framework that improves the integration of retrievers and large language models in retrieval-augmented generation systems, enhancing performance without modifying existing components.", "motivation": "To address the challenges faced in aligning retrievers and large language models in RAG systems, leading to sub-optimal performance in existing methods.", "method": "C-3PO employs a proxy-centric framework with three specialized agents that optimize the RAG pipeline via collaboration, using a tree-structured rollout for reinforcement learning reward assignment.", "result": "Extensive experiments show that C-3PO significantly improves RAG performance in various scenarios while retaining flexibility and enhancing generalization capabilities.", "conclusion": "C-3PO demonstrates effective integration of retrievers and LLMs, leading to improved retrieval-augmented generation outcomes without altering the components.", "key_contributions": ["Introduction of a novel proxy-centric framework for RAG systems", "Development of a multi-agent system to optimize integration of retrievers and LLMs", "Implementation of a tree-structured rollout approach for improved reinforcement learning."], "limitations": "", "keywords": ["Retrieval-augmented generation", "Multi-agent systems", "Reinforcement learning"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2502.08550", "pdf": "https://arxiv.org/pdf/2502.08550.pdf", "abs": "https://arxiv.org/abs/2502.08550", "title": "No Need for Explanations: LLMs can implicitly learn from mistakes in-context", "authors": ["Lisa Alazraki", "Maximilian Mozes", "Jon Ander Campos", "Tan Yi-Chern", "Marek Rei", "Max Bartolo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Showing incorrect answers to Large Language Models (LLMs) is a popular\nstrategy to improve their performance in reasoning-intensive tasks. It is\nwidely assumed that, in order to be helpful, the incorrect answers must be\naccompanied by comprehensive rationales, explicitly detailing where the\nmistakes are and how to correct them. However, in this work we present a\ncounterintuitive finding: we observe that LLMs perform better in math reasoning\ntasks when these rationales are eliminated from the context and models are left\nto infer on their own what makes an incorrect answer flawed. This approach also\nsubstantially outperforms chain-of-thought prompting in our evaluations. These\nresults are consistent across LLMs of different sizes and varying reasoning\nabilities. To gain an understanding of why LLMs learn from mistakes more\neffectively without explicit corrective rationales, we perform a thorough\nanalysis, investigating changes in context length and answer diversity between\ndifferent prompting strategies, and their effect on performance. We also\nexamine evidence of overfitting to the in-context rationales when these are\nprovided, and study the extent to which LLMs are able to autonomously infer\nhigh-quality corrective rationales given only incorrect answers as input. We\nfind evidence that, while incorrect answers are more beneficial for LLM\nlearning than additional diverse correct answers, explicit corrective\nrationales over-constrain the model, thus limiting those benefits.", "AI": {"tldr": "Eliminating explicit rationales for incorrect answers improves LLM performance in math reasoning tasks.", "motivation": "To understand the impact of different prompting strategies on LLM performance in reasoning tasks, especially regarding the use of rationales for incorrect answers.", "method": "The study evaluates LLM performance with and without explicit rationales accompanying incorrect answers in math reasoning tasks, comparing it to chain-of-thought prompting.", "result": "LLMs showed better performance when rationales were removed, outperforming scenarios where rationales were provided, consistent across various LLM sizes and reasoning abilities.", "conclusion": "Explicit corrective rationales limit the learning benefits from incorrect answers, suggesting that LLMs can infer better without them.", "key_contributions": ["Demonstrated improved LLM performance in reasoning without explicit rationales.", "Provided insights into overfitting to in-context rationales.", "Showed that incorrect answers are more beneficial than diverse correct answers alone."], "limitations": "The study primarily focuses on math reasoning tasks, and results may vary in other contexts.", "keywords": ["Large Language Models", "reasoning tasks", "prompting strategies", "incorrect answers", "corrective rationales"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.09604", "pdf": "https://arxiv.org/pdf/2502.09604.pdf", "abs": "https://arxiv.org/abs/2502.09604", "title": "SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models", "authors": ["Yung-Sung Chuang", "Benjamin Cohen-Wang", "Shannon Zejiang Shen", "Zhaofeng Wu", "Hu Xu", "Xi Victoria Lin", "James Glass", "Shang-Wen Li", "Wen-tau Yih"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML 2025 main conference paper. The source code is available at\n  https://github.com/facebookresearch/SelfCite", "summary": "We introduce SelfCite, a novel self-supervised approach that aligns LLMs to\ngenerate high-quality, fine-grained, sentence-level citations for the\nstatements in their generated responses. Instead of only relying on costly and\nlabor-intensive annotations, SelfCite leverages a reward signal provided by the\nLLM itself through context ablation: If a citation is necessary, removing the\ncited text from the context should prevent the same response; if sufficient,\nretaining the cited text alone should preserve the same response. This reward\ncan guide the inference-time best-of-N sampling strategy to improve citation\nquality significantly, as well as be used in preference optimization to\ndirectly fine-tune the models for generating better citations. The\neffectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3\npoints on the LongBench-Cite benchmark across five long-form question answering\ntasks. The source code is available at\nhttps://github.com/facebookresearch/SelfCite", "AI": {"tldr": "SelfCite introduces a self-supervised method to improve citation quality in LLM-generated responses by using the LLM’s own reward signals.", "motivation": "The paper addresses the need for high-quality citations in LLM responses without relying on expensive annotations.", "method": "SelfCite uses context ablation to provide a reward signal for the LLM, guiding the inference-time sampling strategy to generate better citations.", "result": "SelfCite achieved an increase in citation F1 by up to 5.3 points on the LongBench-Cite benchmark across five long-form question answering tasks.", "conclusion": "The approach significantly enhances the quality of citations produced by LLMs, making them more useful for academic and research applications.", "key_contributions": ["Introduction of a self-supervised method for citation generation", "Leveraging LLM's own reward signals for improved citation quality", "Demonstration of effectiveness on LongBench-Cite benchmark"], "limitations": "", "keywords": ["Self-supervised learning", "Citations", "Language model", "Long-form question answering", "Context ablation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.09674", "pdf": "https://arxiv.org/pdf/2502.09674.pdf", "abs": "https://arxiv.org/abs/2502.09674", "title": "The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Analysis of Orthogonal Safety Directions", "authors": ["Wenbo Pan", "Zhichao Liu", "Qiguang Chen", "Xiangyang Zhou", "Haining Yu", "Xiaohua Jia"], "categories": ["cs.CL", "cs.AI"], "comment": "Code and artifacts: https://github.com/BMPixel/safety-residual-space\n  Accepted by ICML 2025", "summary": "Large Language Models' safety-aligned behaviors, such as refusing harmful\nqueries, can be represented by linear directions in activation space. Previous\nresearch modeled safety behavior with a single direction, limiting mechanistic\nunderstanding to an isolated safety feature. In this work, we discover that\nsafety-aligned behavior is jointly controlled by multi-dimensional directions.\nNamely, we study the vector space of representation shifts during safety\nfine-tuning on Llama 3 8B for refusing jailbreaks. By studying orthogonal\ndirections in the space, we first find that a dominant direction governs the\nmodel's refusal behavior, while multiple smaller directions represent distinct\nand interpretable features like hypothetical narrative and role-playing. We\nthen measure how different directions promote or suppress the dominant\ndirection, showing the important role of secondary directions in shaping the\nmodel's refusal representation. Finally, we demonstrate that removing certain\ntrigger tokens in harmful queries can mitigate these directions to bypass the\nlearned safety capability, providing new insights on understanding safety\nalignment vulnerability from a multi-dimensional perspective. Code and\nartifacts are available at https://github.com/BMPixel/safety-residual-space.", "AI": {"tldr": "This paper investigates the multi-dimensional aspects of safety-aligned behaviors in LLMs, specifically focusing on refusal behaviors in response to harmful queries.", "motivation": "To enhance the mechanistic understanding of how safety-aligned behaviors in LLMs can be represented, beyond single-direction models.", "method": "The authors analyze the vector space of representation shifts during safety fine-tuning of Llama 3 8B, identifying key directions that control refusal behavior and testing the impact of removing trigger tokens in harmful queries.", "result": "The study finds that a dominant direction governs refusal behavior, while multiple smaller orthogonal directions represent distinct interpretive features; also discovering that removing certain trigger tokens can bypass safety capabilities.", "conclusion": "The findings reveal the complexity of safety alignment in LLMs and the importance of understanding multi-dimensional directions to better address safety vulnerabilities.", "key_contributions": ["Identification of multi-dimensional directions in LLM safety-aligned behavior.", "Demonstration of how secondary directions impact the dominant refusal direction.", "Insights on how trigger tokens can be manipulated to bypass safety features."], "limitations": "The study focuses primarily on one model (Llama 3 8B) and its safety features, potentially limiting generalizability to other models.", "keywords": ["Large Language Models", "Safety Alignment", "Refusal Behavior", "Multi-dimensional Analysis", "Vector Space"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.11018", "pdf": "https://arxiv.org/pdf/2502.11018.pdf", "abs": "https://arxiv.org/abs/2502.11018", "title": "GRIFFIN: Effective Token Alignment for Faster Speculative Decoding", "authors": ["Shijing Hu", "Jingyang Li", "Xingyu Xie", "Zhihui Lu", "Kim-Chuan Toh", "Pan Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Speculative decoding accelerates inference in large language models (LLMs) by\ngenerating multiple draft tokens simultaneously. However, existing methods\noften struggle with token misalignment between the training and decoding\nphases, limiting their performance. To address this, we propose GRIFFIN, a\nnovel framework that incorporates a token-alignable training strategy and a\ntoken-alignable draft model to mitigate misalignment. The training strategy\nemploys a loss masking mechanism to exclude highly misaligned tokens during\ntraining, preventing them from negatively impacting the draft model's\noptimization. The token-alignable draft model introduces input tokens to\ncorrect inconsistencies in generated features. Experiments on LLaMA, Vicuna,\nQwen and Mixtral models demonstrate that GRIFFIN achieves an average acceptance\nlength improvement of over 8% and a speedup ratio exceeding 7%, outperforming\ncurrent speculative decoding state-of-the-art methods. Our code and GRIFFIN's\ndraft models are released publicly in https://github.com/hsj576/GRIFFIN.", "AI": {"tldr": "GRIFFIN is a new framework for improving the inference speed of large language models through enhanced token alignment during training and decoding.", "motivation": "To overcome token misalignment issues in speculative decoding for large language models, which can limit inference performance.", "method": "GRIFFIN uses a token-alignable training strategy with a loss masking mechanism and a draft model that introduces input tokens to correct inconsistencies in output features.", "result": "Experiments show GRIFFIN achieves over 8% improvement in average acceptance length and more than 7% speedup, outperforming existing methods.", "conclusion": "The results indicate that GRIFFIN significantly enhances the performance of speculative decoding in large language models, making it a valuable contribution to the field.", "key_contributions": ["Introduced a novel token-alignable training strategy with loss masking.", "Developed a token-alignable draft model for correcting generated feature inconsistencies.", "Demonstrated substantial performance improvements over existing methods in several models."], "limitations": "", "keywords": ["large language models", "speculative decoding", "token alignment", "inference speed", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.11438", "pdf": "https://arxiv.org/pdf/2502.11438.pdf", "abs": "https://arxiv.org/abs/2502.11438", "title": "SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example Selection for Text-to-SQL", "authors": ["Jimin Lee", "Ingeol Baek", "Byeongjeong Kim", "Hyunkyung Bae", "Hwanhee Lee"], "categories": ["cs.CL"], "comment": "13 pages, 5 figures, 10 tables", "summary": "Text-to-SQL aims to convert natural language questions into executable SQL\nqueries. While previous approaches, such as skeleton-masked selection, have\ndemonstrated strong performance by retrieving similar training examples to\nguide large language models (LLMs), they struggle in real-world scenarios where\nsuch examples are unavailable. To overcome this limitation, we propose\nSelf-Augmentation in-context learning with Fine-grained Example selection for\nText-to-SQL (SAFE-SQL), a novel framework that improves SQL generation by\ngenerating and filtering self-augmented examples. SAFE-SQL first prompts an LLM\nto generate multiple Text-to-SQL examples relevant to the test input. Then\nSAFE-SQL filters these examples through three relevance assessments,\nconstructing high-quality in-context learning examples. Using self-generated\nexamples, SAFE-SQL surpasses the previous zero-shot, and few-shot Text-to-SQL\nframeworks, achieving higher execution accuracy. Notably, our approach provides\nadditional performance gains in extra hard and unseen scenarios, where\nconventional methods often fail.", "AI": {"tldr": "SAFE-SQL improves Text-to-SQL query generation by using self-augmented examples filtered through relevance assessments.", "motivation": "Existing Text-to-SQL methods struggle in real-world scenarios without available training examples, necessitating a more robust solution.", "method": "SAFE-SQL prompts an LLM to generate multiple relevant Text-to-SQL examples, then filters them based on three relevance assessments to create high-quality inputs for in-context learning.", "result": "SAFE-SQL achieves higher execution accuracy than previous zero-shot and few-shot Text-to-SQL methods, especially in challenging scenarios.", "conclusion": "The self-augmented example generation and filtering process of SAFE-SQL leads to significant improvements in SQL query generation performance.", "key_contributions": ["Introduction of self-augmented examples for SQL generation", "Implementation of three relevance assessments for example filtering", "Demonstrated performance improvement in hard and unseen scenarios"], "limitations": "", "keywords": ["Text-to-SQL", "Self-Augmentation", "In-context Learning", "Large Language Models", "SQL Generation"], "importance_score": 8, "read_time_minutes": 13}}
{"id": "2502.11824", "pdf": "https://arxiv.org/pdf/2502.11824.pdf", "abs": "https://arxiv.org/abs/2502.11824", "title": "M-ABSA: A Multilingual Dataset for Aspect-Based Sentiment Analysis", "authors": ["Chengyan Wu", "Bolei Ma", "Yihong Liu", "Zheyu Zhang", "Ningyuan Deng", "Yanshu Li", "Baolan Chen", "Yi Zhang", "Yun Xue", "Barbara Plank"], "categories": ["cs.CL"], "comment": null, "summary": "Aspect-based sentiment analysis (ABSA) is a crucial task in information\nextraction and sentiment analysis, aiming to identify aspects with associated\nsentiment elements in text. However, existing ABSA datasets are predominantly\nEnglish-centric, limiting the scope for multilingual evaluation and research.\nTo bridge this gap, we present M-ABSA, a comprehensive dataset spanning 7\ndomains and 21 languages, making it the most extensive multilingual parallel\ndataset for ABSA to date. Our primary focus is on triplet extraction, which\ninvolves identifying aspect terms, aspect categories, and sentiment polarities.\nThe dataset is constructed through an automatic translation process with human\nreview to ensure quality. We perform extensive experiments using various\nbaselines to assess performance and compatibility on M-ABSA. Our empirical\nfindings highlight that the dataset enables diverse evaluation tasks, such as\nmultilingual and multi-domain transfer learning, and large language model\nevaluation, underscoring its inclusivity and its potential to drive\nadvancements in multilingual ABSA research.", "AI": {"tldr": "M-ABSA is a multilingual dataset for aspect-based sentiment analysis that covers 7 domains and 21 languages, facilitating research in multilingual evaluation.", "motivation": "Existing ABSA datasets are primarily English-centric, which limits multilingual research and evaluation.", "method": "The dataset is constructed through an automatic translation process, validated by human review, and includes triplet extraction of aspect terms, categories, and sentiment polarities.", "result": "Empirical experiments demonstrate the dataset's efficacy for multilingual and multi-domain transfer learning and LLM evaluations, enhancing inclusivity in ABSA research.", "conclusion": "M-ABSA significantly contributes to advancing multilingual sentiment analysis and can facilitate various evaluation tasks.", "key_contributions": ["Creation of the most extensive multilingual parallel dataset for ABSA covering 21 languages and 7 domains.", "Focus on triplet extraction for improved sentiment analysis tasks.", "Validation of dataset quality through human review of automatically translated content."], "limitations": "", "keywords": ["aspect-based sentiment analysis", "multilingual dataset", "triplet extraction", "transfer learning", "large language models"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2502.13028", "pdf": "https://arxiv.org/pdf/2502.13028.pdf", "abs": "https://arxiv.org/abs/2502.13028", "title": "Whose story is it? Personalizing story generation by inferring author styles", "authors": ["Nischal Ashok Kumar", "Chau Minh Pham", "Mohit Iyyer", "Andrew Lan"], "categories": ["cs.CL"], "comment": "preprint:55 pages", "summary": "Personalization is critical for improving user experience in interactive\nwriting and educational applications, yet remains understudied in story\ngeneration. We study the task of personalizing story generation, where our goal\nis to mimic an author's writing style, given other stories written by them. We\ncollect Mythos, a dataset of 3.6k stories from 112 authors, with an average of\n16 stories per author, across five distinct sources reflecting diverse\nstory-writing settings. We propose a two-stage pipeline for personalized story\ngeneration: first, we infer authors' implicit writing characteristics and\norganize them into an Author Writing Sheet, which is validated by humans to be\nof high quality; second, we simulate the author's persona using tailored\npersona descriptions and personalized story rules. We find that stories\npersonalized using the Author Writing Sheet outperform a non-personalized\nbaseline, achieving a 78% win-rate in capturing authors' past style and 59% in\nsimilarity to ground-truth author stories. Human evaluation supports these\nfindings and further highlights trends, such as Reddit stories being easier to\npersonalize, and the Creativity and Language Use aspects of stories being\neasier to personalize than the Plot.", "AI": {"tldr": "This paper presents a personalized story generation framework that mimics authors' writing styles using a new dataset and a two-stage pipeline.", "motivation": "Personalization is underexplored in story generation, yet it is crucial for enhancing user experiences in writing and education.", "method": "A two-stage pipeline is proposed for personalized story generation: inferring authors' implicit writing characteristics into an Author Writing Sheet, and simulating the author's persona with tailored descriptions and rules.", "result": "Personalized stories generated from the Author Writing Sheet achieved a 78% win-rate in mimicking authors' styles and 59% similarity to actual stories, validated by human evaluation.", "conclusion": "The findings indicate that personalization enhances story generation, with certain themes and features being more amenable to personalization.", "key_contributions": ["Introduction of the Mythos dataset comprising 3.6k stories from 112 authors.", "Development of a two-stage pipeline for personalized story generation.", "Demonstration of significant improvements in story personalization metrics compared to a baseline."], "limitations": "Limited exploration of the contexts in which personalization is effective and the specific challenges for various genres.", "keywords": ["personalization", "story generation", "author writing styles", "human evaluation", "dataset"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2502.13487", "pdf": "https://arxiv.org/pdf/2502.13487.pdf", "abs": "https://arxiv.org/abs/2502.13487", "title": "Transferring Textual Preferences to Vision-Language Understanding through Model Merging", "authors": ["Chen-An Li", "Tzu-Han Lin", "Yun-Nung Chen", "Hung-yi Lee"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted to ACL 2025 main", "summary": "Large vision-language models (LVLMs) perform outstandingly across various\nmultimodal tasks. However, their ability to evaluate generated content remains\nlimited, and training vision-language reward models (VLRMs) with preference\ndata is computationally expensive. This paper explores a training-free\nalternative by merging text-based reward models (RMs) with LVLMs to create\nVLRMs. Our approach shows that integrating these models leads to improved\nperformance over LVLMs' scoring and text-based RMs, offering an efficient\nmethod for incorporating textual preferences into LVLMs.", "AI": {"tldr": "This paper presents a training-free method to enhance vision-language models by integrating text-based reward models, improving their content evaluation capabilities.", "motivation": "To address the limitations of large vision-language models (LVLMs) in evaluating generated content and the high computational cost of training vision-language reward models (VLRMs) with preference data.", "method": "The authors propose a method that merges text-based reward models with LVLMs to create efficient VLRMs that do not require extensive training.", "result": "The integration of text-based reward models with LVLMs results in improved performance compared to both LVLMs' scoring and standalone text-based reward models.", "conclusion": "This approach offers an efficient way to incorporate textual preferences into LVLMs without the need for training, thereby simplifying the evaluation of generated content.", "key_contributions": ["Development of a training-free alternative for vision-language reward models.", "Demonstration of improved performance in multimodal tasks using the combined model.", "Reduction in computational requirements for training compared to traditional methods."], "limitations": "The proposed method may still struggle with complex multimodal interactions that require more nuanced training.", "keywords": ["vision-language models", "reward models", "multimodal tasks", "text-based preferences", "computational efficiency"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2502.15132", "pdf": "https://arxiv.org/pdf/2502.15132.pdf", "abs": "https://arxiv.org/abs/2502.15132", "title": "CoT-ICL Lab: A Synthetic Framework for Studying Chain-of-Thought Learning from In-Context Demonstrations", "authors": ["Vignesh Kothapalli", "Hamed Firooz", "Maziar Sanjabi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL Main 2025", "summary": "We introduce CoT-ICL Lab, a framework and methodology to generate synthetic\ntokenized datasets and systematically study chain-of-thought (CoT) in-context\nlearning (ICL) in language models. CoT-ICL Lab allows fine grained control over\nthe complexity of in-context examples by decoupling (1) the causal structure\ninvolved in chain token generation from (2) the underlying token processing\nfunctions. We train decoder-only transformers (up to 700M parameters) on these\ndatasets and show that CoT accelerates the accuracy transition to higher values\nacross model sizes. In particular, we find that model depth is crucial for\nleveraging CoT with limited in-context examples, while more examples help\nshallow models match deeper model performance. Additionally, limiting the\ndiversity of token processing functions throughout training improves causal\nstructure learning via ICL. We also interpret these transitions by analyzing\ntransformer embeddings and attention maps. Overall, CoT-ICL Lab serves as a\nsimple yet powerful testbed for theoretical and empirical insights into ICL and\nCoT in language models.", "AI": {"tldr": "CoT-ICL Lab is a framework for generating synthetic datasets to explore chain-of-thought in-context learning in language models, demonstrating the importance of model depth and example diversity.", "motivation": "To systematically study chain-of-thought in-context learning in language models and understand its effects on performance.", "method": "A framework called CoT-ICL Lab was developed to generate synthetic tokenized datasets, allowing control over in-context example complexity by separating causal structure and token processing functions. Decoder-only transformers were trained on these datasets.", "result": "The study found that chain-of-thought significantly improves accuracy transitions in larger models, while depth is critical for models with few in-context examples, and diversity in token processing enhances causal structure learning.", "conclusion": "CoT-ICL Lab provides a valuable tool for gaining theoretical and empirical insights into in-context learning and chain-of-thought in language models.", "key_contributions": ["Introduction of CoT-ICL Lab framework", "Demonstration of model depth's importance in CoT", "Insights on token processing diversity and causal structure learning"], "limitations": "", "keywords": ["in-context learning", "chain-of-thought", "language models", "synthetic datasets", "transformers"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2502.16002", "pdf": "https://arxiv.org/pdf/2502.16002.pdf", "abs": "https://arxiv.org/abs/2502.16002", "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse", "authors": ["Jingbo Yang", "Bairu Hou", "Wei Wei", "Yujia Bao", "Shiyu Chang"], "categories": ["cs.CL"], "comment": null, "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines.", "AI": {"tldr": "KVLink offers a method for efficient reuse of key-value (KV) caches in large language models, enhancing speed and accuracy during inference by allowing the model to utilize precomputed caches.", "motivation": "To address redundancy in computation when overlapping contexts are present in LLM applications, leading to inefficient resource usage.", "method": "KVLink computes KV caches for documents independently and concatenates them during inference, introducing positional adjustments and special tokens for effective self-attention restoration.", "result": "KVLink enhances question answering accuracy by 4% and reduces time-to-first-token by 96% relative to traditional LLM inference methods.", "conclusion": "KVLink provides a scalable, efficient solution for leveraging cache reuse in LLMs and can be integrated with other cache compression techniques for improved performance.", "key_contributions": ["Introduction of KVLink for efficient KV cache reuse", "Positional embedding adjustments for concatenated caches", "Use of trainable special tokens for self-attention restoration"], "limitations": "", "keywords": ["key-value cache", "large language models", "context reuse", "efficiency", "self-attention"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.17775", "pdf": "https://arxiv.org/pdf/2502.17775.pdf", "abs": "https://arxiv.org/abs/2502.17775", "title": "FoREST: Frame of Reference Evaluation in Spatial Reasoning Tasks", "authors": ["Tanawan Premsri", "Parisa Kordjamshidi"], "categories": ["cs.CL"], "comment": "9 pages", "summary": "Spatial reasoning is a fundamental aspect of human intelligence. One key\nconcept in spatial cognition is the Frame of Reference (FoR), which identifies\nthe perspective of spatial expressions. Despite its significance, FoR has\nreceived limited attention in AI models that need spatial intelligence. There\nis a lack of dedicated benchmarks and in-depth evaluation of large language\nmodels (LLMs) in this area. To address this issue, we introduce the Frame of\nReference Evaluation in Spatial Reasoning Tasks (FoREST) benchmark, designed to\nassess FoR comprehension in LLMs. We evaluate LLMs on answering questions that\nrequire FoR comprehension and layout generation in text-to-image models using\nFoREST. Our results reveal a notable performance gap across different FoR\nclasses in various LLMs, affecting their ability to generate accurate layouts\nfor text-to-image generation. This highlights critical shortcomings in FoR\ncomprehension. To improve FoR understanding, we propose Spatial-Guided\nprompting, which improves LLMs ability to extract essential spatial concepts.\nOur proposed method improves overall performance across spatial reasoning\ntasks.", "AI": {"tldr": "The paper introduces the FoREST benchmark to evaluate Frame of Reference (FoR) comprehension in large language models (LLMs) and proposes a method to enhance LLM performance in spatial reasoning tasks.", "motivation": "Despite the importance of spatial reasoning and the Frame of Reference in AI, there is a lack of dedicated benchmarks and evaluations for LLMs in this context.", "method": "The authors developed the Frame of Reference Evaluation in Spatial Reasoning Tasks (FoREST) benchmark to test LLM comprehension and layout generation related to FoR. They also introduced Spatial-Guided prompting to improve performance in spatial reasoning tasks.", "result": "LLMs exhibited significant performance gaps across different FoR classes, which affected their ability to generate correct layouts for text-to-image generation.", "conclusion": "To enhance FoR understanding, the proposed Spatial-Guided prompting improved LLMs' ability to grasp essential spatial concepts and overall performance in spatial reasoning tasks.", "key_contributions": ["Introduction of the FoREST benchmark for assessing FoR in LLMs", "Identification of performance gaps in LLMs related to FoR comprehension", "Proposal of Spatial-Guided prompting to enhance spatial reasoning tasks."], "limitations": "", "keywords": ["Frame of Reference", "Spatial Reasoning", "Large Language Models", "Benchmark", "Text-to-Image Generation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2502.17956", "pdf": "https://arxiv.org/pdf/2502.17956.pdf", "abs": "https://arxiv.org/abs/2502.17956", "title": "Towards Better Understanding of Program-of-Thought Reasoning in Cross-Lingual and Multilingual Environments", "authors": ["Patomporn Payoungkhamdee", "Pume Tuchinda", "Jinheon Baek", "Samuel Cahyawijaya", "Can Udomcharoenchaikit", "Potsawee Manakul", "Peerat Limkonchotiwat", "Ekapol Chuangsuwanich", "Sarana Nutanong"], "categories": ["cs.CL"], "comment": null, "summary": "Multi-step reasoning is essential for large language models (LLMs), yet\nmultilingual performance remains challenging. While Chain-of-Thought (CoT)\nprompting improves reasoning, it struggles with non-English languages due to\nthe entanglement of reasoning and execution. Program-of-Thought (PoT) prompting\nseparates reasoning from execution, offering a promising alternative but\nshifting the challenge to generating programs from non-English questions. We\npropose a framework to evaluate PoT by separating multilingual reasoning from\ncode execution to examine (i) the impact of fine-tuning on question-reasoning\nalignment and (ii) how reasoning quality affects answer correctness. Our\nfindings demonstrate that PoT fine-tuning substantially enhances multilingual\nreasoning, outperforming CoT fine-tuned models. We further demonstrate a strong\ncorrelation between reasoning quality (measured through code quality) and\nanswer accuracy, highlighting its potential as a test-time performance\nimprovement heuristic.", "AI": {"tldr": "This paper proposes a framework for evaluating Program-of-Thought (PoT) prompting in addressing multilingual reasoning challenges in large language models, showing that fine-tuning PoT significantly enhances reasoning and accuracy compared to Chain-of-Thought (CoT) prompting.", "motivation": "Enhancing multilingual performance in large language models where multi-step reasoning is required, specifically addressing the challenges of Chain-of-Thought (CoT) prompting in non-English languages.", "method": "The paper introduces a framework that separates multilingual reasoning from code execution to evaluate PoT prompting, analyzing the effects of fine-tuning on question-reasoning alignment and how reasoning quality influences answer correctness.", "result": "The findings reveal that PoT fine-tuning leads to significant improvements in multilingual reasoning capabilities and demonstrates a strong correlation between the quality of reasoning (as measured by code quality) and the accuracy of answers.", "conclusion": "The results suggest that the PoT approach can serve as an effective technique to enhance reasoning quality and accuracy in multilingual applications of large language models.", "key_contributions": ["Proposed a novel evaluation framework for PoT prompting.", "Demonstrated the effectiveness of PoT fine-tuning over CoT in multilingual contexts.", "Established a correlation between reasoning quality and answer accuracy."], "limitations": "", "keywords": ["multilingual reasoning", "large language models", "Program-of-Thought prompting"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.20073", "pdf": "https://arxiv.org/pdf/2502.20073.pdf", "abs": "https://arxiv.org/abs/2502.20073", "title": "Collab-Overcooked: Benchmarking and Evaluating Large Language Models as Collaborative Agents", "authors": ["Haochen Sun", "Shuwen Zhang", "Lujie Niu", "Lei Ren", "Hao Xu", "Hao Fu", "Fangkun Zhao", "Caixia Yuan", "Xiaojie Wang"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "30 pages, 17 figures", "summary": "Large language models (LLMs) based agent systems have made great strides in\nreal-world applications beyond traditional NLP tasks. This paper proposes a new\nLLM-powered Multi-Agent System (LLM-MAS) benchmark, Collab-Overcooked, built on\nthe popular Overcooked-AI game with more applicable and challenging tasks in\ninteractive environments. Collab-Overcooked extends existing benchmarks from\ntwo novel perspectives. First, it provides a multi-agent framework supporting\ndiverse tasks and objectives and encourages collaboration through natural\nlanguage communication. Second, it introduces a spectrum of process-oriented\nevaluation metrics to assess the fine-grained collaboration capabilities of\ndifferent LLM agents, a dimension often overlooked in prior work. We conduct\nextensive experiments over 11 popular LLMs and show that, while the LLMs\npresent a strong ability in goal interpretation, there is a significant\ndiscrepancy in active collaboration and continuous adaptation which are\ncritical for efficiently fulfilling complicated tasks. Notably, we highlight\nthe strengths and weaknesses in LLM-MAS and provide insights for improving and\nevaluating LLM-MAS on a unified and open-sourced benchmark. The environments,\n30 open-ended tasks, and the evaluation package are publicly available at\nhttps://github.com/YusaeMeow/Collab-Overcooked.", "AI": {"tldr": "The paper introduces Collab-Overcooked, an LLM-powered benchmark for multi-agent systems that enhances collaboration and evaluation in complex interactive tasks.", "motivation": "There is a need for better benchmarking of multi-agent systems that focuses on collaboration and complex task fulfillment using large language models.", "method": "A new benchmark called Collab-Overcooked is proposed, based on the Overcooked-AI game, featuring a multi-agent framework and a variety of process-oriented evaluation metrics.", "result": "Experiments with 11 popular LLMs reveal strong goal interpretation but significant gaps in collaboration and adaptation skills among agents.", "conclusion": "The study provides insights for improving LLM-MAS functionality and evaluation, with the resources available for public use.", "key_contributions": ["Introduction of Collab-Overcooked benchmark for multi-agent systems", "Novel evaluation metrics for assessing LLM collaborative capabilities", "Open-sourced resources for further research and evaluation"], "limitations": "", "keywords": ["Multi-Agent Systems", "Large Language Models", "Collaborative Tasks", "Benchmarking", "Evaluation Metrics"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2503.04598", "pdf": "https://arxiv.org/pdf/2503.04598.pdf", "abs": "https://arxiv.org/abs/2503.04598", "title": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization", "authors": ["Zhijian Zhuo", "Yutao Zeng", "Ya Wang", "Sijun Zhang", "Jian Yang", "Xiaoqing Li", "Xun Zhou", "Jinwen Ma"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Transformers have become the de facto architecture for a wide range of\nmachine learning tasks, particularly in large language models (LLMs). Despite\ntheir remarkable performance, challenges remain in training deep transformer\nnetworks, especially regarding the position of layer normalization. While\nPre-Norm structures facilitate more stable training owing to their stronger\nidentity path, they often lead to suboptimal performance compared to Post-Norm.\nIn this paper, we propose $\\textbf{HybridNorm}$, a simple yet effective hybrid\nnormalization strategy that integrates the advantages of both Pre-Norm and\nPost-Norm. Specifically, HybridNorm employs QKV normalization within the\nattention mechanism and Post-Norm in the feed-forward network (FFN) of each\ntransformer block. We provide both theoretical insights and empirical evidence\ndemonstrating that HybridNorm improves gradient flow and model robustness.\nExtensive experiments on large-scale transformer models, including both dense\nand sparse variants, show that HybridNorm consistently outperforms both\nPre-Norm and Post-Norm approaches across multiple benchmarks. These findings\nhighlight the potential of HybridNorm as a more stable and effective technique\nfor improving the training and performance of deep transformer models. Code is\navailable at https://github.com/BryceZhuo/HybridNorm.", "AI": {"tldr": "HybridNorm integrates Pre-Norm and Post-Norm strategies for improved transformer training.", "motivation": "To address the training challenges in deep transformer models related to the positioning of layer normalization.", "method": "HybridNorm uses QKV normalization within the attention mechanism and Post-Norm in the feed-forward network of transformer blocks.", "result": "HybridNorm improves gradient flow and model robustness, consistently outperforming both Pre-Norm and Post-Norm across multiple benchmarks.", "conclusion": "HybridNorm offers a more stable and effective technique for training deep transformers, enhancing their overall performance.", "key_contributions": ["Introduction of HybridNorm, a hybrid normalization method", "Empirical validation showing improved performance over Pre-Norm and Post-Norm", "Theoretical insights into gradient flow and model robustness"], "limitations": "", "keywords": ["HybridNorm", "transformers", "layer normalization", "machine learning", "deep learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2503.08506", "pdf": "https://arxiv.org/pdf/2503.08506.pdf", "abs": "https://arxiv.org/abs/2503.08506", "title": "ReviewAgents: Bridging the Gap Between Human and AI-Generated Paper Reviews", "authors": ["Xian Gao", "Jiacheng Ruan", "Jingsheng Gao", "Ting Liu", "Yuzhuo Fu"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Academic paper review is a critical yet time-consuming task within the\nresearch community. With the increasing volume of academic publications,\nautomating the review process has become a significant challenge. The primary\nissue lies in generating comprehensive, accurate, and reasoning-consistent\nreview comments that align with human reviewers' judgments. In this paper, we\naddress this challenge by proposing ReviewAgents, a framework that leverages\nlarge language models (LLMs) to generate academic paper reviews. We first\nintroduce a novel dataset, Review-CoT, consisting of 142k review comments,\ndesigned for training LLM agents. This dataset emulates the structured\nreasoning process of human reviewers-summarizing the paper, referencing\nrelevant works, identifying strengths and weaknesses, and generating a review\nconclusion. Building upon this, we train LLM reviewer agents capable of\nstructured reasoning using a relevant-paper-aware training method. Furthermore,\nwe construct ReviewAgents, a multi-role, multi-LLM agent review framework, to\nenhance the review comment generation process. Additionally, we propose\nReviewBench, a benchmark for evaluating the review comments generated by LLMs.\nOur experimental results on ReviewBench demonstrate that while existing LLMs\nexhibit a certain degree of potential for automating the review process, there\nremains a gap when compared to human-generated reviews. Moreover, our\nReviewAgents framework further narrows this gap, outperforming advanced LLMs in\ngenerating review comments.", "AI": {"tldr": "The paper presents ReviewAgents, a framework using LLMs to automate academic paper reviews, highlighted by a new dataset and evaluation benchmark.", "motivation": "The increasing volume of academic publications necessitates automating the time-consuming process of paper reviews.", "method": "The authors introduce a dataset called Review-CoT for training LLM agents, emulating human reviewers' reasoning processes, and develop the ReviewAgents framework for generating structured review comments.", "result": "Experimental results show that ReviewAgents outperform existing LLMs in generating review comments, though gaps remain compared to human reviews.", "conclusion": "The ReviewAgents framework effectively narrows the gap in review comment quality between LLM-generated and human-generated reviews.", "key_contributions": ["Introduction of Review-CoT dataset with 142k review comments", "Development of ReviewAgents framework for multi-role review generation", "Creation of ReviewBench for evaluating LLM-generated reviews"], "limitations": "", "keywords": ["large language models", "academic review automation", "structured reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.14749", "pdf": "https://arxiv.org/pdf/2503.14749.pdf", "abs": "https://arxiv.org/abs/2503.14749", "title": "Uncertainty Distillation: Teaching Language Models to Express Semantic Confidence", "authors": ["Sophia Hager", "David Mueller", "Kevin Duh", "Nicholas Andrews"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "As large language models (LLMs) are increasingly used for factual\nquestion-answering, it becomes more important for LLMs to have the capability\nto communicate the likelihood that their answer is correct. For these\nverbalized expressions of uncertainty to be meaningful, they should reflect the\nerror rates at the expressed level of confidence. However, when prompted to\nexpress confidence, the error rates of current LLMs are inconsistent with their\ncommunicated confidences, highlighting the need for uncertainty quantification\nmethods. Many prior methods calculate lexical uncertainty, estimating a model's\nconfidence in the specific string it generated. In some cases, however, it may\nbe more useful to estimate semantic uncertainty, or the model's confidence in\nthe answer regardless of how it is verbalized. We propose a simple procedure,\nuncertainty distillation, to teach an LLM to verbalize calibrated semantic\nconfidences. Using held-out data to map initial uncertainty estimates to\nmeaningful probabilities, we create examples annotated with verbalized\nprobabilities for supervised fine-tuning. We compare uncertainty distillation\nto several strong baselines, and find that our method yields verbalized\nconfidences that correlate well with observed error rates.", "AI": {"tldr": "The paper proposes a method called uncertainty distillation to improve large language models' ability to verbalize calibrated semantic confidences that accurately reflect their error rates.", "motivation": "As LLMs are used more for factual question-answering, it's crucial for them to communicate the likelihood of answer correctness, necessitating reliable uncertainty quantification methods.", "method": "The proposed method, uncertainty distillation, uses held-out data to map initial uncertainty estimates to meaningful probabilities and involves creating annotated examples for supervised fine-tuning.", "result": "Uncertainty distillation leads to verbalized confidences from LLMs that strongly correlate with their observed error rates, outperforming several strong baselines.", "conclusion": "The approach significantly improves the reliability of how LLMs express their confidence, making their outputs more meaningful.", "key_contributions": ["Introduction of uncertainty distillation for LLMs.", "Demonstration of improved correlation between verbalized confidence and error rates.", "Development of a framework for supervised fine-tuning with annotated probabilities."], "limitations": "", "keywords": ["large language models", "uncertainty quantification", "semantic confidence", "calibrated probabilities", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.15463", "pdf": "https://arxiv.org/pdf/2503.15463.pdf", "abs": "https://arxiv.org/abs/2503.15463", "title": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment", "authors": ["Jia-Nan Li", "Jian Guan", "Songhao Wu", "Wei Wu", "Rui Yan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our approach toward user-adaptive AI\nsystems.", "AI": {"tldr": "This paper presents a framework for scalable personalized alignment of large language models (LLMs), introducing the AlignX dataset and two alignment methods that improve accuracy and adaptability.", "motivation": "The paper motivates the need for personalized alignment of LLMs due to the diversity in user values and preferences that current one-size-fits-all methods overlook.", "method": "A systematic preference space is established with persona representations for robust preference inference, leading to the development of two alignment approaches: in-context alignment and preference-bridged alignment.", "result": "The proposed methods showed an average accuracy gain of 17.06% across four benchmarks, with effective adaptation to new preferences and robustness to limited user data.", "conclusion": "The results support a shift towards user-adaptive AI systems that better address diverse user needs.", "key_contributions": ["Introduction of a comprehensive framework for personalized alignment of LLMs", "Development of the AlignX dataset of over 1.3 million preference examples", "Introduction of two novel alignment approaches enhancing accuracy and adaptability."], "limitations": "", "keywords": ["large language models", "personalized alignment", "preference inference"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.16674", "pdf": "https://arxiv.org/pdf/2503.16674.pdf", "abs": "https://arxiv.org/abs/2503.16674", "title": "Through the LLM Looking Glass: A Socratic Probing of Donkeys, Elephants, and Markets", "authors": ["Molly Kennedy", "Ayyoob Imani", "Timo Spinde", "Hinrich Schütze"], "categories": ["cs.CL"], "comment": null, "summary": "While detecting and avoiding bias in LLM-generated text is becoming\nincreasingly important, media bias often remains subtle and subjective, making\nit particularly difficult to identify and mitigate. In this study, we assess\nmedia bias in LLM-generated content and LLMs' ability to detect subtle\nideological bias. We conduct this evaluation using two datasets, PoliGen and\nEconoLex, covering political and economic discourse, respectively. We evaluate\nseven widely used LLMs by prompting them to generate articles and analyze their\nideological preferences via Socratic probing. By using our self-contained\nSocratic approach, the study aims to directly measure the models' biases rather\nthan relying on external interpretations, thereby minimizing subjective\njudgments about media bias. Our results reveal a consistent preference of\nDemocratic over Republican positions across all models. Conversely, in economic\ntopics, biases vary among Western LLMs, while those developed in China lean\nmore strongly toward socialism.", "AI": {"tldr": "This study assesses media bias in LLM-generated content by evaluating ideological preferences in political and economic discourse using Socratic probing.", "motivation": "Detecting and avoiding bias in LLM-generated text is crucial, as media bias can be subtle and subjective, complicating its identification and mitigation.", "method": "The study evaluates seven widely used LLMs by prompting them to generate articles and analyzes their ideological preferences via a self-contained Socratic probing approach.", "result": "The evaluation reveals a consistent preference for Democratic positions over Republican across all models and varying biases among Western LLMs in economic topics, with Chinese models leaning toward socialism.", "conclusion": "The study highlights the different ideological biases of LLMs in political and economic discourse, offering a method to assess these biases with reduced subjectivity.", "key_contributions": ["Introduced a self-contained Socratic approach to measure LLM biases directly.", "Evaluated ideological preferences across two diverse datasets.", "Identified systematic biases in LLMs related to political and economic topics."], "limitations": "The study may not account for all cultural contexts influencing bias in LLMs.", "keywords": ["LLM", "media bias", "ideological bias", "Socratic probing", "political discourse"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.16965", "pdf": "https://arxiv.org/pdf/2503.16965.pdf", "abs": "https://arxiv.org/abs/2503.16965", "title": "Praxis-VLM: Vision-Grounded Decision Making via Text-Driven Reinforcement Learning", "authors": ["Zhe Hu", "Jing Li", "Zhongzhu Pu", "Hou Pong Chan", "Yu Yin"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Vision Language Models exhibited immense potential for embodied AI, yet they\noften lack the sophisticated situational reasoning required for complex\ndecision-making. This paper shows that VLMs can achieve surprisingly strong\ndecision-making performance when visual scenes are represented merely as\ntext-only descriptions, suggesting foundational reasoning can be effectively\nlearned from language. Motivated by this insight, we propose Praxis-VLM, a\nreasoning VLM for vision-grounded decision-making. Praxis-VLM employs the GRPO\nalgorithm on textual scenarios to instill robust reasoning capabilities, where\nmodels learn to evaluate actions and their consequences. These reasoning\nskills, acquired purely from text, successfully transfer to multimodal\ninference with visual inputs, significantly reducing reliance on scarce paired\nimage-text training data. Experiments across diverse decision-making benchmarks\ndemonstrate that Praxis-VLM substantially outperforms standard supervised\nfine-tuning, exhibiting superior performance and generalizability. Further\nanalysis confirms that our models engage in explicit and effective reasoning,\nunderpinning their enhanced performance and adaptability.", "AI": {"tldr": "Praxis-VLM is a vision language model that enhances decision-making by using text-only descriptions, leading to improved reasoning and performance with visual inputs.", "motivation": "Vision Language Models (VLMs) often struggle with complex situational reasoning, which is crucial for effective decision-making, even though they have potential in embodied AI.", "method": "Praxis-VLM employs the GRPO algorithm on textual scenarios to develop reasoning skills, which are then applied to multimodal inference with visual inputs.", "result": "Praxis-VLM significantly outperforms standard supervised fine-tuning in various decision-making benchmarks, demonstrating superior reasoning capabilities and generalizability.", "conclusion": "The strong performance of Praxis-VLM underscores the potential for language-based reasoning to enhance the decision-making capabilities of VLMs, reducing the need for paired image-text data.", "key_contributions": ["Introduction of Praxis-VLM for vision-grounded decision-making", "Application of the GRPO algorithm for reasoning skills", "Demonstrated transfer of reasoning skills from text to multimodal inputs"], "limitations": "", "keywords": ["Vision Language Models", "Decision-Making", "Reasoning VLM", "Multimodal Inference", "GRPO algorithm"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.17287", "pdf": "https://arxiv.org/pdf/2503.17287.pdf", "abs": "https://arxiv.org/abs/2503.17287", "title": "FastCuRL: Curriculum Reinforcement Learning with Stage-wise Context Scaling for Efficient Training R1-like Reasoning Models", "authors": ["Mingyang Song", "Mao Zheng", "Zheng Li", "Wenjie Yang", "Xuan Luo", "Yue Pan", "Feng Zhang"], "categories": ["cs.CL"], "comment": "Ongoing Work", "summary": "Improving training efficiency continues to be one of the primary challenges\nin large-scale Reinforcement Learning (RL). In this paper, we investigate how\ncontext length and the complexity of training data influence the RL scaling\ntraining process of R1-distilled small reasoning models, e.g.,\nDeepSeek-R1-Distill-Qwen-1.5B. Our experimental results reveal that: (1) simply\ncontrolling the context length and curating the training data based on the\ninput prompt length can effectively improve the training efficiency of scaling\nRL, achieving better performance with more concise CoT; (2) properly scaling\nthe context length helps mitigate entropy collapse; and (3) choosing an optimal\ncontext length can improve the efficiency of model training and incentivize the\nmodel's chain-of-thought reasoning capabilities. Inspired by these insights, we\npropose FastCuRL, a curriculum RL framework with stage-wise context scaling to\nachieve efficient training and concise CoT reasoning. Experiment results\ndemonstrate that FastCuRL-1.5B-V3 significantly outperforms state-of-the-art\nreasoning models on five competition-level benchmarks and achieves 49.6\\%\naccuracy on AIME 2024. Furthermore, FastCuRL-1.5B-Preview surpasses\nDeepScaleR-1.5B-Preview on five benchmarks while only using a single node with\n8 GPUs and a total of 50\\% of training steps. %The code, training data, and\nmodels will be publicly released.", "AI": {"tldr": "This paper presents FastCuRL, a reinforcement learning framework that enhances training efficiency by optimizing context length and training data complexity for better reasoning performance.", "motivation": "Improving training efficiency in large-scale Reinforcement Learning is a critical challenge.", "method": "The paper investigates how context length and training data complexity affect the scaling training process of R1-distilled models, proposing FastCuRL, a curriculum RL framework that employs stage-wise context scaling.", "result": "Experiments demonstrate that FastCuRL significantly outperforms state-of-the-art reasoning models, achieving 49.6% accuracy on AIME 2024 with improved efficiency.", "conclusion": "The proposed approach allows for more efficient training and enhances the model's chain-of-thought reasoning capabilities, with the code and models to be publicly released.", "key_contributions": ["Introduction of FastCuRL framework for efficient RL training", "Demonstration of the importance of context length in improving training efficiency", "Results show significant performance improvements on competitive benchmarks"], "limitations": "", "keywords": ["Reinforcement Learning", "Context Length", "Training Efficiency", "Reasoning Models", "Curriculum Learning"], "importance_score": 6, "read_time_minutes": 12}}
{"id": "2503.19498", "pdf": "https://arxiv.org/pdf/2503.19498.pdf", "abs": "https://arxiv.org/abs/2503.19498", "title": "DomainCQA: Crafting Expert-Level QA from Domain-Specific Charts", "authors": ["Ling Zhong", "Yujing Lu", "Jing Yang", "Weiming Li", "Peng Wei", "Yongheng Wang", "Manni Duan", "Qing Zhang"], "categories": ["cs.CL"], "comment": "87 pages, 65 figures", "summary": "Chart Question Answering (CQA) benchmarks are essential for evaluating the\ncapability of Multimodal Large Language Models (MLLMs) to interpret visual\ndata. However, current benchmarks focus primarily on the evaluation of\ngeneral-purpose CQA but fail to adequately capture domain-specific challenges.\nWe introduce DomainCQA, a systematic methodology for constructing\ndomain-specific CQA benchmarks, and demonstrate its effectiveness by developing\nAstroChart, a CQA benchmark in the field of astronomy. Our evaluation shows\nthat current MLLMs face fundamental challenges in vision-language alignment and\ndomain adaptation, highlighting a critical gap in current benchmarks. By\nproviding a scalable and rigorous framework, DomainCQA enables more precise\nassessment and improvement of MLLMs for domain-specific applications.", "AI": {"tldr": "Introducing DomainCQA for domain-specific Chart Question Answering benchmarks to enhance evaluation of Multimodal Large Language Models in specialized fields.", "motivation": "To address the inadequacy of existing chart question answering benchmarks in capturing domain-specific challenges faced by Multimodal Large Language Models.", "method": "A systematic methodology was established for constructing domain-specific chart question answering benchmarks, illustrated through the development of AstroChart for astronomy.", "result": "Current Multimodal Large Language Models encounter significant difficulties in vision-language alignment and domain adaptation, indicating critical gaps in existing evaluation methods.", "conclusion": "DomainCQA provides a scalable and rigorous framework for evaluating MLLMs, leading to more accurate assessments and enhancements in domain-specific applications.", "key_contributions": ["Introduction of DomainCQA methodology for domain-specific benchmarking", "Development of the AstroChart benchmark in astronomy", "Highlighting fundamental challenges in vision-language alignment for MLLMs"], "limitations": "", "keywords": ["Chart Question Answering", "Multimodal Large Language Models", "Domain-Specific Challenges", "Vision-Language Alignment", "AstroChart"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2503.20083", "pdf": "https://arxiv.org/pdf/2503.20083.pdf", "abs": "https://arxiv.org/abs/2503.20083", "title": "Universal Cross-Tokenizer Distillation via Approximate Likelihood Matching", "authors": ["Benjamin Minixhofer", "Ivan Vulić", "Edoardo Maria Ponti"], "categories": ["cs.CL"], "comment": "Preprint, 21 pages", "summary": "Distillation has shown remarkable success in transferring knowledge from a\nLarge Language Model (LLM) teacher to a student LLM. However, current\ndistillation methods require similar tokenizers between the teacher and the\nstudent, restricting their applicability to only a small subset of\nteacher-student pairs. In this work, we develop a principled cross-tokenizer\ndistillation method to solve this crucial deficiency. Our method is the first\nto enable effective distillation across fundamentally different tokenizers,\nwhile also substantially outperforming prior methods in all other cases. We\nverify the efficacy of our method on three distinct use cases. First, we show\nthat viewing tokenizer transfer as self-distillation enables unprecedentedly\neffective transfer across tokenizers, including rapid transfer of subword\nmodels to the byte-level. Transferring different models to the same tokenizer\nalso enables ensembling to boost performance. Secondly, we distil a large\nmaths-specialised LLM into a small general-purpose model with a different\ntokenizer, achieving competitive maths problem-solving performance. Thirdly, we\nuse our method to train state-of-the-art embedding prediction hypernetworks for\ntraining-free tokenizer transfer. Our results unlock an expanded range of\nteacher-student pairs for distillation, enabling new ways to adapt and enhance\ninteraction between LLMs.", "AI": {"tldr": "This paper presents a novel cross-tokenizer distillation method for transferring knowledge between Large Language Models (LLMs) with different tokenizers, which overcomes the limitations of existing methods and improves performance across various use cases.", "motivation": "Current distillation methods require similar tokenizers between teacher and student LLMs, limiting their applicability. This work aims to develop a method for effective distillation across different tokenizers.", "method": "The proposed method enables effective distillation by treating tokenizer transfer as self-distillation, allowing models with different tokenizers to effectively transfer knowledge and improve interoperability.", "result": "The method outperforms prior distillation techniques across various scenarios, including transferring subword models to byte-level, distilling a math-specialized LLM to a general-purpose model with different tokenizers, and enhancing embedding prediction hypernetworks.", "conclusion": "The novel cross-tokenizer distillation technique expands the range of compatible teacher-student pairs, facilitating new adaptations in LLM interactions and enhancing their overall performance.", "key_contributions": ["First effective method for distillation across different tokenizers", "Demonstrated substantial performance improvements in three distinct use cases", "Facilitated enhanced interaction between LLMs through expanded adapter models"], "limitations": "", "keywords": ["Large Language Models", "Tokenization", "Distillation", "Knowledge Transfer", "Machine Learning"], "importance_score": 9, "read_time_minutes": 21}}
{"id": "2504.07053", "pdf": "https://arxiv.org/pdf/2504.07053.pdf", "abs": "https://arxiv.org/abs/2504.07053", "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling", "authors": ["Liang-Hsuan Tseng", "Yi-Chang Chen", "Kuan-Yi Lee", "Da-Shan Shiu", "Hung-yi Lee"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Preprint", "summary": "Recent efforts target spoken language models (SLMs) that not only listen but\nalso speak for more natural human-LLM interaction. Joint speech-text modeling\nis a promising direction to achieve this. However, the effectiveness of recent\nspeech tokens for joint modeling remains underexplored. To address this, we\nintroduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that\ndirectly addresses the modality gap by aligning speech token with the\ncorresponding text transcription during the tokenization stage. We propose a\nmethod that can achieve this through a attention-based aggregation mechanism\nand with speech reconstruction as the training objective. We conduct extensive\nexperiments and show that TASTE can preserve essential paralinguistic\ninformation while dramatically reducing the token sequence length. With TASTE,\nwe perform straightforward joint spoken language modeling by using Low-Rank\nAdaptation on the pre-trained text LLM. Experimental results show that\nTASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze;\nwhile significantly outperform other pre-trained SLMs on speech continuation\nacross subjective and objective evaluations. To our knowledge, TASTE is the\nfirst end-to-end approach that utilizes a reconstruction objective to\nautomatically learn a text-aligned speech tokenization and embedding suitable\nfor spoken language modeling. Our demo, code, and model are available at\nhttps://mtkresearch.github.io/TASTE-SpokenLM.github.io.", "AI": {"tldr": "TASTE introduces a novel approach for spoken language modeling by aligning speech tokens with text transcriptions, improving natural human-LLM interaction.", "motivation": "To address the limited effectiveness of recent speech tokens in joint speech-text modeling for human-LLM interaction.", "method": "Text-Aligned Speech Tokenization and Embedding (TASTE) utilizes an attention-based aggregation mechanism and speech reconstruction as the training objective to align speech tokens with text transcriptions.", "result": "TASTE-based spoken language models show comparable performance to existing methods on SALMON and StoryCloze, while significantly outperforming other pre-trained SLMs in speech continuation tasks.", "conclusion": "TASTE is the first end-to-end method to automatically learn text-aligned speech tokenization and embedding, enhancing spoken language modeling.", "key_contributions": ["Introduction of Text-Aligned Speech Tokenization and Embedding (TASTE) method.", "Achieves alignment of speech tokens and text transcriptions during tokenization.", "First end-to-end approach to use speech reconstruction as a training objective."], "limitations": "", "keywords": ["Spoken Language Models", "Speech Tokenization", "Human-LLM Interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.09753", "pdf": "https://arxiv.org/pdf/2504.09753.pdf", "abs": "https://arxiv.org/abs/2504.09753", "title": "Improving Multilingual Capabilities with Cultural and Local Knowledge in Large Language Models While Enhancing Native Performance", "authors": ["Ram Mohan Rao Kadiyala", "Siddartha Pullakhandam", "Siddhant Gupta", "Drishti Sharma", "Jebish Purbey", "Kanwal Mehreen", "Muhammad Arham", "Hamza Farooq"], "categories": ["cs.CL", "cs.AI"], "comment": "24 pages, 18 figures", "summary": "Large Language Models (LLMs) have shown remarkable capabilities, but their\ndevelopment has primarily focused on English and other high-resource languages,\nleaving many languages underserved. We present our latest Hindi-English\nbi-lingual LLM \\textbf{Mantra-14B} with ~3\\% average improvement in benchmark\nscores over both languages, outperforming models twice its size. Using a\ncurated dataset composed of English and Hindi instruction data of 485K samples,\nwe instruction tuned models such as Qwen-2.5-14B-Instruct and Phi-4 to improve\nperformance over both English and Hindi. Our experiments encompassing seven\ndifferent LLMs of varying parameter sizes and over 140 training attempts with\nvarying English-Hindi training data ratios demonstrated that it is possible to\nsignificantly improve multilingual performance without compromising native\nperformance. Further, our approach avoids resource-intensive techniques like\nvocabulary expansion or architectural modifications, thus keeping the model\nsize small. Our results indicate that modest fine-tuning with culturally and\nlocally informed data can bridge performance gaps without incurring significant\ncomputational overhead. We release our training code, datasets, and models\nunder mit and apache licenses to aid further research towards under-represented\nand low-resource languages.", "AI": {"tldr": "We introduce Mantra-14B, a Hindi-English bilingual large language model that outperforms existing models while requiring fewer resources.", "motivation": "Address the underrepresentation of low-resource languages in the development of large language models (LLMs).", "method": "We used a curated dataset of 485K samples to instruction tune various models, achieving a 3% average benchmark improvement in multilingual performance.", "result": "Mantra-14B outperforms larger models by improving performance in both English and Hindi without resource-intensive techniques.", "conclusion": "Modest fine-tuning with localized data can enhance model performance for under-represented languages without significant computational costs.", "key_contributions": ["Development of Mantra-14B, a Hindi-English bilingual LLM.", "Significant multilingual performance improvement with smaller model size.", "Open release of training resources for further research."], "limitations": "", "keywords": ["Large Language Models", "Hindi-English", "bilingual LLM", "multilingual performance", "low-resource languages"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.10063", "pdf": "https://arxiv.org/pdf/2504.10063.pdf", "abs": "https://arxiv.org/abs/2504.10063", "title": "Hallucination Detection in LLMs with Topological Divergence on Attention Graphs", "authors": ["Alexandra Bazarova", "Aleksandr Yugay", "Andrey Shulga", "Alina Ermilova", "Andrei Volodichev", "Konstantin Polev", "Julia Belikova", "Rauf Parchiev", "Dmitry Simakov", "Maxim Savchenko", "Andrey Savchenko", "Serguei Barannikov", "Alexey Zaytsev"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hallucination, i.e., generating factually incorrect content, remains a\ncritical challenge for large language models (LLMs). We introduce TOHA, a\nTOpology-based HAllucination detector in the RAG setting, which leverages a\ntopological divergence metric to quantify the structural properties of graphs\ninduced by attention matrices. Examining the topological divergence between\nprompt and response subgraphs reveals consistent patterns: higher divergence\nvalues in specific attention heads correlate with hallucinated outputs,\nindependent of the dataset. Extensive experiments - including evaluation on\nquestion answering and summarization tasks - show that our approach achieves\nstate-of-the-art or competitive results on several benchmarks while requiring\nminimal annotated data and computational resources. Our findings suggest that\nanalyzing the topological structure of attention matrices can serve as an\nefficient and robust indicator of factual reliability in LLMs.", "AI": {"tldr": "The paper introduces TOHA, a topology-based detector for hallucinations in large language models, which quantifies structural properties of attention matrices to identify factual inaccuracies in generated content.", "motivation": "To address the critical issue of hallucination in large language models (LLMs) where they generate factually incorrect content.", "method": "TOHA leverages a topological divergence metric to analyze the structural properties of graphs formed by attention matrices, focusing on the divergence between prompt and response subgraphs.", "result": "The approach exhibits consistent patterns that correlate higher topological divergence values with hallucinated outputs and achieves state-of-the-art or competitive results in question answering and summarization while requiring minimal annotated data.", "conclusion": "Analyzing the topological structure of attention matrices can serve as an efficient and robust indicator of factual reliability in LLMs.", "key_contributions": ["Introduction of TOHA for hallucination detection", "Quantitative analysis of topological divergence in attention matrices", "Demonstrated effectiveness with minimal data requirements"], "limitations": "", "keywords": ["hallucination", "large language models", "topological divergence", "attention matrices", "RAG setting"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.11952", "pdf": "https://arxiv.org/pdf/2504.11952.pdf", "abs": "https://arxiv.org/abs/2504.11952", "title": "Robust and Fine-Grained Detection of AI Generated Texts", "authors": ["Ram Mohan Rao Kadiyala", "Siddartha Pullakhandam", "Kanwal Mehreen", "Drishti Sharma", "Siddhant Gupta", "Jebish Purbey", "Ashay Srivastava", "Subhasya TippaReddy", "Arvind Reddy Bobbili", "Suraj Telugara Chandrashekhar", "Modabbir Adeeb", "Srinadh Vura", "Hamza Farooq"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "18 pages, 6 figures", "summary": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts.", "AI": {"tldr": "This paper presents models for detecting human-LLM co-authored texts, highlighting performance against various challenges in AI-generated content detection.", "motivation": "The increasing sophistication of LLMs has made it challenging for existing systems to accurately detect machine-generated content, particularly in shorter texts and cases where human and machine contributions are mixed.", "method": "The authors developed token classification models trained on a unique dataset of 2.4 million co-authored texts across 23 languages, focusing on performance in diverse domains and against adversarial inputs.", "result": "The models demonstrated strong performance in classifying co-authored texts, outperforming existing systems particularly in handling unseen domains, non-native speaker content, and adversarial conditions.", "conclusion": "The findings suggest significant advancements in detecting co-authored content, with robust performance metrics across various challenges, establishing a foundation for future research in AI content detection.", "key_contributions": ["Introduction of a token classification model for human-LLM co-authored texts", "A new dataset of 2.4 million texts co-authored by LLMs", "Evaluation metrics against adversarial inputs and text characteristics"], "limitations": "Performance may vary with specific nuances of different languages and contexts not covered in the training data.", "keywords": ["AI content detection", "human-LLM collaboration", "token classification", "adversarial methods", "language models"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2504.12816", "pdf": "https://arxiv.org/pdf/2504.12816.pdf", "abs": "https://arxiv.org/abs/2504.12816", "title": "SMARTe: Slot-based Method for Accountable Relational Triple extraction", "authors": ["Xue Wen Tan", "Stanley Kok"], "categories": ["cs.CL"], "comment": null, "summary": "Relational Triple Extraction (RTE) is a fundamental task in Natural Language\nProcessing (NLP). However, prior research has primarily focused on optimizing\nmodel performance, with limited efforts to understand the internal mechanisms\ndriving these models. Many existing methods rely on complex preprocessing to\ninduce specific interactions, often resulting in opaque systems that may not\nfully align with their theoretical foundations. To address these limitations,\nwe propose SMARTe: a Slot-based Method for Accountable Relational Triple\nextraction. SMARTe introduces intrinsic interpretability through a slot\nattention mechanism and frames the task as a set prediction problem. Slot\nattention consolidates relevant information into distinct slots, ensuring all\npredictions can be explicitly traced to learned slot representations and the\ntokens contributing to each predicted relational triple. While emphasizing\ninterpretability, SMARTe achieves performance comparable to state-of-the-art\nmodels. Evaluations on the NYT and WebNLG datasets demonstrate that adding\ninterpretability does not compromise performance. Furthermore, we conducted\nqualitative assessments to showcase the explanations provided by SMARTe, using\nattention heatmaps that map to their respective tokens. We conclude with a\ndiscussion of our findings and propose directions for future research.", "AI": {"tldr": "The paper introduces SMARTe, a Slot-based Method for Accountable Relational Triple Extraction, emphasizing interpretability without sacrificing performance.", "motivation": "Prior research on Relational Triple Extraction (RTE) has focused on model performance but lacks understanding of the internal mechanisms, leading to opaque systems.", "method": "SMARTe utilizes a slot attention mechanism to consolidate relevant information into slots, framing the task as a set prediction problem.", "result": "SMARTe achieves performance comparable to state-of-the-art models while enhancing interpretability, as demonstrated through evaluations on NYT and WebNLG datasets.", "conclusion": "Adding interpretability to the model does not compromise performance, and qualitative assessments provide insightful explanations of predictions.", "key_contributions": ["Introduction of the SMARTe framework for RTE", "Implementation of a slot attention mechanism for interpretability", "Demonstration of performance on NYT and WebNLG datasets while ensuring explainability"], "limitations": "", "keywords": ["Relational Triple Extraction", "NLP", "Slot attention", "Interpretability", "Set prediction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.15253", "pdf": "https://arxiv.org/pdf/2504.15253.pdf", "abs": "https://arxiv.org/abs/2504.15253", "title": "Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators", "authors": ["Yilun Zhou", "Austin Xu", "Peifeng Wang", "Caiming Xiong", "Shafiq Joty"], "categories": ["cs.CL", "cs.LG"], "comment": "ICML 2025. The first two authors contributed equally. The codebase is\n  at https://github.com/SalesforceAIResearch/jetts-benchmark", "summary": "Scaling test-time computation, or affording a generator large language model\n(LLM) extra compute during inference, typically employs the help of external\nnon-generative evaluators (i.e., reward models). Concurrently, LLM-judges,\nmodels trained to generate evaluations and critiques (explanations) in natural\nlanguage, are becoming increasingly popular in automatic evaluation. Despite\njudge empirical successes, their effectiveness as evaluators in test-time\nscaling settings is largely unknown. In this paper, we introduce the Judge\nEvaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge\nperformance in three domains (math reasoning, code generation, and instruction\nfollowing) under three task settings: response reranking, step-level beam\nsearch, and critique-based response refinement. We evaluate 10 different judge\nmodels (7B-70B parameters) for 8 different base generator models (6.7B-72B\nparameters). Our benchmark shows that while judges are competitive with outcome\nreward models in reranking, they are consistently worse than process reward\nmodels in beam search procedures. Furthermore, though unique to LLM-judges,\ntheir natural language critiques are currently ineffective in guiding the\ngenerator towards better responses.", "AI": {"tldr": "This paper introduces the Judge Evaluation for Test-Time Scaling (JETTS) benchmark to evaluate LLM-judges' performance compared to traditional reward models in various computational tasks.", "motivation": "To assess the effectiveness of LLM-judges as evaluators in enhancing test-time computation for large language models, given the growing use of language-based evaluations in automatic assessment.", "method": "The benchmark evaluates 10 different judge models across three domains (math reasoning, code generation, instruction following) and under three task settings (response reranking, step-level beam search, critique-based response refinement).", "result": "The study finds that while LLM-judges perform competitively with outcome reward models for response reranking, they underperform against process reward models in beam search tasks and provide ineffective critiques.", "conclusion": "The findings highlight the current limitations of LLM-judges in guiding generator responses, suggesting a need for further refinement and research in this area.", "key_contributions": ["Introduction of the JETTS benchmark for assessing LLM-judges", "Evaluation across multiple domains and task settings", "Comparative analysis of judge models against traditional reward models"], "limitations": "LLM-judges show consistent shortcomings in guiding the generator's response improvement through critiques.", "keywords": ["LLM-judges", "test-time scaling", "benchmark", "evaluation", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.16084", "pdf": "https://arxiv.org/pdf/2504.16084.pdf", "abs": "https://arxiv.org/abs/2504.16084", "title": "TTRL: Test-Time Reinforcement Learning", "authors": ["Yuxin Zuo", "Kaiyan Zhang", "Li Sheng", "Shang Qu", "Ganqu Cui", "Xuekai Zhu", "Haozhan Li", "Yuchen Zhang", "Xinwei Long", "Ermo Hua", "Biqing Qi", "Youbang Sun", "Zhiyuan Ma", "Lifan Yuan", "Ning Ding", "Bowen Zhou"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 211% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the maj@n metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model maj@n,\nand approach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL", "AI": {"tldr": "This paper presents Test-Time Reinforcement Learning (TTRL), a method for training Large Language Models (LLMs) on unlabeled data by estimating rewards through common Test-Time Scaling practices.", "motivation": "To address the challenge of training LLMs using Reinforcement Learning on unlabeled data without access to ground-truth information.", "method": "The proposed TTRL method leverages Test-Time Scaling techniques like majority voting to estimate rewards and drive reinforcement learning training.", "result": "TTRL significantly improves performance on various tasks, boosting pass@1 performance by approximately 211% on specific benchmarks while surpassing initial model performance.", "conclusion": "The experiments validate TTRL's effectiveness and highlight its potential applicability across diverse tasks and domains, even with minimal supervision.", "key_contributions": ["Introduction of Test-Time Reinforcement Learning (TTRL) for LLMs on unlabeled data", "Demonstrated effectiveness of majority voting for reward estimation", "Achieved substantial performance improvements compared to existing models"], "limitations": "", "keywords": ["Reinforcement Learning", "Large Language Models", "Unlabeled Data", "Test-Time Scaling", "Reward Estimation"], "importance_score": 9, "read_time_minutes": 10}}
