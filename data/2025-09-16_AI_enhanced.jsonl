{"id": "2509.10637", "pdf": "https://arxiv.org/pdf/2509.10637.pdf", "abs": "https://arxiv.org/abs/2509.10637", "title": "LLMs Homogenize Values in Constructive Arguments on Value-Laden Topics", "authors": ["Farhana Shahid", "Stella Zhang", "Aditya Vashistha"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) are increasingly used to promote prosocial and\nconstructive discourse online. Yet little is known about how they negotiate and\nshape underlying values when reframing people's arguments on value-laden\ntopics. We conducted experiments with 347 participants from India and the\nUnited States, who wrote constructive comments on homophobic and Islamophobic\nthreads, and reviewed human-written and LLM-rewritten versions of these\ncomments. Our analysis shows that LLM systematically diminishes Conservative\nvalues while elevating prosocial values such as Benevolence and Universalism.\nWhen these comments were read by others, participants opposing same-sex\nmarriage or Islam found human-written comments more aligned with their values,\nwhereas those supportive of these communities found LLM-rewritten versions more\naligned with their values. These findings suggest that LLM-driven value\nhomogenization can shape how diverse viewpoints are represented in contentious\ndebates on value-laden topics and may influence the dynamics of online\ndiscourse critically.", "AI": {"tldr": "This paper explores how large language models (LLMs) influence online discourse by reframing arguments on value-laden topics, specifically analyzing their impact on conservative and prosocial values through participant experiments.", "motivation": "There is a need to understand the role of LLMs in negotiating values during online discourse, particularly on contentious topics.", "method": "Experiments conducted with 347 participants from India and the US, analyzing their comments on homophobic and Islamophobic threads, evaluating human-written versus LLM-rewritten responses.", "result": "LLMs tend to diminish Conservative values while elevating prosocial values. Participants generally felt human-written comments aligned more with their values if they opposed same-sex marriage or Islam, while supporters preferred LLM-rewritten versions.", "conclusion": "LLM-driven value homogenization can significantly alter representations of diverse viewpoints in online debates, impacting the nature of discourse.", "key_contributions": ["Identification of LLMs' impact on value alignment in discourse", "Experimental evidence of differing perceptions of human vs LLM comments", "Discussion on the implications of LLMs for online communication dynamics"], "limitations": "", "keywords": ["large language models", "value alignment", "prosocal discourse", "online communication", "contentious debates"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.10652", "pdf": "https://arxiv.org/pdf/2509.10652.pdf", "abs": "https://arxiv.org/abs/2509.10652", "title": "Vibe Coding for UX Design: Understanding UX Professionals' Perceptions of AI-Assisted Design and Development", "authors": ["Jie Li", "Youyang Hou", "Laura Lin", "Ruihao Zhu", "Hancheng Cao", "Abdallah El Ali"], "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.ET"], "comment": null, "summary": "Generative AI is reshaping UX design practices through \"vibe coding,\" where\nUX professionals express intent in natural language and AI translates it into\nfunctional prototypes and code. Despite rapid adoption, little research has\nexamined how vibe coding reconfigures UX workflows and collaboration. Drawing\non interviews with 20 UX professionals across enterprises, startups, and\nacademia, we show how vibe coding follows a four-stage workflow of ideation, AI\ngeneration, debugging, and review. This accelerates iteration, supports\ncreativity, and lowers barriers to participation. However, professionals\nreported challenges of code unreliability, integration, and AI over-reliance.\nWe find tensions between efficiency-driven prototyping (\"intending the right\ndesign\") and reflection (\"designing the right intention\"), introducing new\nasymmetries in trust, responsibility, and social stigma within teams. Through\nthe lens of responsible human-AI collaboration for AI-assisted UX design and\ndevelopment, we contribute a deeper understanding of deskilling, ownership and\ndisclosure, and creativity safeguarding in the age of vibe coding.", "AI": {"tldr": "This paper explores how generative AI, through vibe coding, is transforming UX design workflows and collaboration, outlining a four-stage process while addressing associated challenges.", "motivation": "To investigate the impact of generative AI on UX design practices, specifically how vibe coding reconfigures workflows and collaboration among UX professionals.", "method": "Interviews with 20 UX professionals from various sectors such as enterprises, startups, and academia.", "result": "Vibe coding follows a four-stage workflow (ideation, AI generation, debugging, and review), which enhances iteration speed, creativity, and participation, but presents issues like code unreliability and AI over-reliance.", "conclusion": "The findings reveal the tensions between efficiency and reflective design, leading to new asymmetries in trust and responsibility among teams.", "key_contributions": ["Identification of the four-stage vibe coding workflow", "Insights into the challenges of AI-assisted UX design", "Discussion on the implications of deskilling and trust within teams"], "limitations": "The study is based on interviews, which may not capture the complete range of experiences and contexts.", "keywords": ["generative AI", "vibe coding", "UX design", "human-AI collaboration", "workflow"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.10723", "pdf": "https://arxiv.org/pdf/2509.10723.pdf", "abs": "https://arxiv.org/abs/2509.10723", "title": "Dark Patterns Meet GUI Agents: LLM Agent Susceptibility to Manipulative Interfaces and the Role of Human Oversight", "authors": ["Jingyu Tang", "Chaoran Chen", "Jiawen Li", "Zhiping Zhang", "Bingcan Guo", "Ibrahim Khalilov", "Simret Araya Gebreegziabher", "Bingsheng Yao", "Dakuo Wang", "Yanfang Ye", "Tianshi Li", "Ziang Xiao", "Yaxing Yao", "Toby Jia-Jun Li"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The dark patterns, deceptive interface designs manipulating user behaviors,\nhave been extensively studied for their effects on human decision-making and\nautonomy. Yet, with the rising prominence of LLM-powered GUI agents that\nautomate tasks from high-level intents, understanding how dark patterns affect\nagents is increasingly important. We present a two-phase empirical study\nexamining how agents, human participants, and human-AI teams respond to 16\ntypes of dark patterns across diverse scenarios. Phase 1 highlights that agents\noften fail to recognize dark patterns, and even when aware, prioritize task\ncompletion over protective action. Phase 2 revealed divergent failure modes:\nhumans succumb due to cognitive shortcuts and habitual compliance, while agents\nfalter from procedural blind spots. Human oversight improved avoidance but\nintroduced costs such as attentional tunneling and cognitive load. Our findings\nshow neither humans nor agents are uniformly resilient, and collaboration\nintroduces new vulnerabilities, suggesting design needs for transparency,\nadjustable autonomy, and oversight.", "AI": {"tldr": "This paper explores how dark patterns in user interface design impact the behavior of LLM-powered agents and human participants in decision-making processes.", "motivation": "Understanding how dark patterns affect both human users and LLM-powered agents is crucial as reliance on these agents increases.", "method": "A two-phase empirical study was conducted to examine the responses of agents, human participants, and human-AI teams to 16 types of dark patterns in various scenarios.", "result": "Phase 1 showed agents often fail to recognize dark patterns, prioritizing task completion. Phase 2 indicated that humans fall prey to cognitive shortcuts, while agents suffer from procedural blind spots. Collaboration increases vulnerabilities but also enables better avoidance with oversight.", "conclusion": "Neither humans nor agents are universally resilient to dark patterns. Design improvements are necessary for transparency, adjustable autonomy, and oversight to mitigate risks.", "key_contributions": ["Found that LLM-powered agents often overlook dark patterns and prioritize task completion.", "Highlighted the different failure modes for humans and agents when faced with dark patterns.", "Identified implications for design in fostering transparency and human oversight."], "limitations": "The study is limited to certain types of dark patterns and may not generalize to all design contexts.", "keywords": ["dark patterns", "LLM-powered agents", "human-AI collaboration", "user behavior", "interface design"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.10747", "pdf": "https://arxiv.org/pdf/2509.10747.pdf", "abs": "https://arxiv.org/abs/2509.10747", "title": "Emerging Patterns of GenAI Use in K-12 Science and Mathematics Education", "authors": ["Lief Esbenshade", "Shawon Sarkar", "Drew Nucci", "Ann Edwards", "Sarah Nielsen", "Joshua M. Rosenberg", "Alex Liu", "Zewei", "Tian", "Min Sun", "Zachary Zhang", "Thomas Han", "Yulia Lapicus", "Kevin He"], "categories": ["cs.HC"], "comment": null, "summary": "In this report, we share findings from a nationally representative survey of\nUS public school math and science teachers, examining current generative AI\n(GenAI) use, perceptions, constraints, and institutional support. We show\ntrends in math and science teacher adoption of GenAI, including frequency and\npurpose of use. We describe how teachers use GenAI with students and their\nbeliefs about GenAI's impact on student learning. We share teachers' reporting\non the school and district support they are receiving for GenAI learning and\nimplementation, and the support they would like schools and districts to\nprovide, and close with implications for policy, practice, and research. Given\nthe rapid pace of GenAI development and growing pressure on schools to\nintegrate emerging technologies, these findings offer timely insights into how\nfrontline educators are navigating this shift in practice.", "AI": {"tldr": "This report examines how US public school math and science teachers are using generative AI (GenAI), their perceptions, constraints, and institutional support received for its implementation.", "motivation": "To understand the adoption and impacts of generative AI among math and science teachers in public schools.", "method": "A nationally representative survey of US public school teachers was conducted to gather data about their use, perceptions, and constraints related to generative AI.", "result": "The findings reveal trends in GenAI adoption among teachers, how they incorporate it into their teaching, and their beliefs about its effects on student learning.", "conclusion": "The report emphasizes the importance of understanding teachers' needs and support systems to effectively integrate GenAI in educational practices, which has significant implications for policy and research.", "key_contributions": ["Identification of trends in GenAI use among teachers", "Insights on teachers' beliefs about GenAI's impact on learning", "Recommendations for school and district support in GenAI implementation"], "limitations": "", "keywords": ["generative AI", "teacher perceptions", "educational technology", "math and science education", "institutional support"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2509.10546", "pdf": "https://arxiv.org/pdf/2509.10546.pdf", "abs": "https://arxiv.org/abs/2509.10546", "title": "Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment", "authors": ["Gang Cheng", "Haibo Jin", "Wenbin Zhang", "Haohan Wang", "Jun Zhuang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint, under review. TL;DR: We propose a multi-turn red-teaming\n  framework, RCA, that reveals critical regulatory vulnerabilities in financial\n  LLMs, achieving over 93% attack success on a proposed new benchmark,\n  FIN-Bench", "summary": "Large Language Models (LLMs) are increasingly integrated into financial\napplications, yet existing red-teaming research primarily targets harmful\ncontent, largely neglecting regulatory risks. In this work, we aim to\ninvestigate the vulnerability of financial LLMs through red-teaming approaches.\nWe introduce Risk-Concealment Attacks (RCA), a novel multi-turn framework that\niteratively conceals regulatory risks to provoke seemingly compliant yet\nregulatory-violating responses from LLMs. To enable systematic evaluation, we\nconstruct FIN-Bench, a domain-specific benchmark for assessing LLM safety in\nfinancial contexts. Extensive experiments on FIN-Bench demonstrate that RCA\neffectively bypasses nine mainstream LLMs, achieving an average attack success\nrate (ASR) of 93.18%, including 98.28% on GPT-4.1 and 97.56% on OpenAI o1.\nThese findings reveal a critical gap in current alignment techniques and\nunderscore the urgent need for stronger moderation mechanisms in financial\ndomains. We hope this work offers practical insights for advancing robust and\ndomain-aware LLM alignment.", "AI": {"tldr": "We propose a multi-turn red-teaming framework, RCA, that reveals critical regulatory vulnerabilities in financial LLMs, achieving over 93% attack success on a proposed new benchmark, FIN-Bench.", "motivation": "Existing red-teaming research on Large Language Models (LLMs) focuses mainly on harmful content, overlooking regulatory risks in financial applications.", "method": "We introduce Risk-Concealment Attacks (RCA), a multi-turn framework that conceals regulatory risks to elicit responses from LLMs that seem compliant but violate regulations. We also constructed FIN-Bench, a benchmark for evaluating LLM safety in financial contexts.", "result": "Extensive experiments on FIN-Bench show that RCA can effectively bypass nine mainstream LLMs, achieving an average attack success rate of 93.18%, including 98.28% on GPT-4 and 97.56% on OpenAI's model.", "conclusion": "Our findings highlight significant gaps in existing LLM alignment techniques and indicate an urgent need for better moderation mechanisms in financial contexts, offering practical insights for LLM alignment.", "key_contributions": ["Introduction of Risk-Concealment Attacks (RCA) for probing regulatory vulnerabilities in LLMs", "Development of FIN-Bench, a domain-specific benchmark for assessing LLM safety in finance", "Demonstration of high attack success rates against mainstream LLMs, revealing critical alignment gaps"], "limitations": "", "keywords": ["Large Language Models", "financial applications", "Risk-Concealment Attacks", "LLM alignment", "FIN-Bench"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.10749", "pdf": "https://arxiv.org/pdf/2509.10749.pdf", "abs": "https://arxiv.org/abs/2509.10749", "title": "Remotely Seeing Is Believing: How Trust in Cyber-Physical Systems Evolves Through Virtual Observation", "authors": ["Zhi Hua Jin", "Kurt Xiao", "David Hyde"], "categories": ["cs.HC", "J.4"], "comment": "23 pages, 12 figures", "summary": "In this paper, we develop a virtual laboratory for measuring human trust. Our\nlaboratory, which is realized as a web application, enables researchers to show\npre-recorded or live video feeds to groups of users in a synchronized fashion.\nUsers are able to provide real-time feedback on these videos via affect buttons\nand a freeform chat interface. We evaluate our application via a quantitative\nuser study ($N \\approx 80$) involving videos of cyber-physical systems, such as\nautonomous vehicles, performing positively or negatively. Using data collected\nfrom user responses in the application, as well as customized survey\ninstruments assessing different facets of trust, we find that human trust in\ncyber-physical systems can be affected merely by remotely observing the\nbehavior of such systems, without ever encountering them in person.", "AI": {"tldr": "Development of a virtual laboratory to measure human trust in cyber-physical systems through real-time user feedback on video feeds.", "motivation": "To explore how human trust in cyber-physical systems can be influenced through remote observation.", "method": "A web application for presenting videos and collecting real-time user feedback using affect buttons and a chat interface, evaluated through a quantitative study with approximately 80 participants.", "result": "Data indicates that trust in cyber-physical systems is influenced by observed behavior in video feeds, independent of direct interaction.", "conclusion": "Observing the behavior of cyber-physical systems remotely can significantly impact human trust, highlighting the importance of design in these systems' presentations.", "key_contributions": ["Development of a synchronized video feedback web application", "Quantitative analysis of trust based on user responses", "Insight into remote trust evaluation in cyber-physical systems"], "limitations": "", "keywords": ["human trust", "cyber-physical systems", "virtual laboratory", "user feedback", "remote observation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.10625", "pdf": "https://arxiv.org/pdf/2509.10625.pdf", "abs": "https://arxiv.org/abs/2509.10625", "title": "No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes", "authors": ["Iván Vicente Moreno Cencerrado", "Arnau Padrés Masdemont", "Anton Gonzalvez Hawthorne", "David Demitri Africa", "Lorenzo Pacchiardi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Do large language models (LLMs) anticipate when they will answer correctly?\nTo study this, we extract activations after a question is read but before any\ntokens are generated, and train linear probes to predict whether the model's\nforthcoming answer will be correct. Across three open-source model families\nranging from 7 to 70 billion parameters, projections on this \"in-advance\ncorrectness direction\" trained on generic trivia questions predict success in\ndistribution and on diverse out-of-distribution knowledge datasets,\noutperforming black-box baselines and verbalised predicted confidence.\nPredictive power saturates in intermediate layers, suggesting that\nself-assessment emerges mid-computation. Notably, generalisation falters on\nquestions requiring mathematical reasoning. Moreover, for models responding \"I\ndon't know\", doing so strongly correlates with the probe score, indicating that\nthe same direction also captures confidence. By complementing previous results\non truthfulness and other behaviours obtained with probes and sparse\nauto-encoders, our work contributes essential findings to elucidate LLM\ninternals.", "AI": {"tldr": "This paper explores whether large language models can anticipate the correctness of their answers by analyzing activations before token generation, using linear probes to predict answer accuracy.", "motivation": "The study aims to understand if and how large language models self-assess the correctness of their forthcoming answers based on internal activations.", "method": "The researchers extracted activations after the model reads a question but before generating tokens, using linear probes trained on trivia questions to predict the correctness of answers across different model families.", "result": "The model's projections on the correctness of answers effectively predict success on both in-distribution and out-of-distribution datasets, outperforming traditional methods and showing saturation of predictive power in intermediate layers.", "conclusion": "The findings suggest that self-assessment of correctness occurs mid-computation in LLMs, providing insights into model behavior and confidence assessment, though generalization issues arise with mathematical reasoning questions.", "key_contributions": ["Introduction of the 'in-advance correctness direction' for LLMs.", "Demonstration of predictive power saturation in intermediate layers of models.", "Correlation between model responses of 'I don't know' and probe scores, indicating a link to confidence."], "limitations": "Generalization falters with questions requiring mathematical reasoning, which highlights a limitation in predictive capabilities.", "keywords": ["large language models", "correctness prediction", "self-assessment", "activations", "confidence"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.10750", "pdf": "https://arxiv.org/pdf/2509.10750.pdf", "abs": "https://arxiv.org/abs/2509.10750", "title": "Unbounded: Object-Boundary Interactions in Mixed Reality", "authors": ["Zhuoyue Lyu", "Per Ola Kristensson"], "categories": ["cs.HC"], "comment": null, "summary": "Boundaries such as walls, windows, and doors are ubiquitous in the physical\nworld, yet their potential in Mixed Reality (MR) remains underexplored. We\npresent Unbounded, a Research through Design inquiry into Object-Boundary\nInteractions (OBIs). Building on prior work, we articulate a design space aimed\nat providing a shared language for OBIs. To demonstrate its potential, we\ndesign and implement eight examples across productivity and art exploration\nscenarios, showcasing how boundaries can enrich and reframe everyday\ninteractions. We further engage with six MR experts in one-on-one feedback\nsessions, using the design space and examples as design probes. Their\nreflections broaden the conceptual scope of OBIs, reveal new possibilities for\nhow the framework may be applied, and highlight implications for future MR\ninteraction design.", "AI": {"tldr": "This paper explores Object-Boundary Interactions (OBIs) in Mixed Reality, offering a design space and examples to enhance user interactions with physical boundaries.", "motivation": "The potential of using physical boundaries in Mixed Reality has not been fully explored, despite their ubiquity in the real world.", "method": "A Research through Design approach was utilized, culminating in the design and implementation of eight examples illustrating OBIs within productivity and art exploration.", "result": "The findings were validated through feedback from six Mixed Reality experts, who provided insights into expanding the conceptual framework and its application.", "conclusion": "The exploration of object-boundary interactions reveals new potentials in Mixed Reality and suggests implications for future design in MR interactions.", "key_contributions": ["Proposed a design space for Object-Boundary Interactions (OBIs) in Mixed Reality.", "Developed eight practical examples demonstrating the application of OBIs.", "Engaged MR experts to refine the understanding and potential applications of the design space."], "limitations": "The examples and insights are preliminary; further research is needed to generalize findings across broader contexts of MR.", "keywords": ["Mixed Reality", "Object-Boundary Interactions", "Human-Computer Interaction", "Design Space", "Expert Feedback"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.10644", "pdf": "https://arxiv.org/pdf/2509.10644.pdf", "abs": "https://arxiv.org/abs/2509.10644", "title": "Interdisciplinary Research in Conversation: A Case Study in Computational Morphology for Language Documentation", "authors": ["Enora Rice", "Katharina von der Wense", "Alexis Palmer"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025", "summary": "Computational morphology has the potential to support language documentation\nthrough tasks like morphological segmentation and the generation of Interlinear\nGlossed Text (IGT). However, our research outputs have seen limited use in\nreal-world language documentation settings. This position paper situates the\ndisconnect between computational morphology and language documentation within a\nbroader misalignment between research and practice in NLP and argues that the\nfield risks becoming decontextualized and ineffectual without systematic\nintegration of User-Centered Design (UCD). To demonstrate how principles from\nUCD can reshape the research agenda, we present a case study of GlossLM, a\nstate-of-the-art multilingual IGT generation model. Through a small-scale user\nstudy with three documentary linguists, we find that despite strong metric\nbased performance, the system fails to meet core usability needs in real\ndocumentation contexts. These insights raise new research questions around\nmodel constraints, label standardization, segmentation, and personalization. We\nargue that centering users not only produces more effective tools, but surfaces\nricher, more relevant research directions", "AI": {"tldr": "This paper discusses the disconnect between computational morphology research and practical language documentation, highlighting the need for User-Centered Design principles through a case study of the GlossLM model.", "motivation": "To address the limited use of computational morphology in language documentation and the disconnection between NLP research and practical applications.", "method": "The paper presents a case study of GlossLM, a multilingual IGT generation model, assessed through a small-scale user study with documentary linguists to evaluate usability.", "result": "Despite strong performance metrics, GlossLM fails to meet key usability requirements in real documentation environments, indicating a misalignment between system design and user needs.", "conclusion": "Integrating User-Centered Design in the development of NLP tools can enhance their effectiveness and lead to more relevant research inquiries.", "key_contributions": ["Demonstration of UCD principles in computational morphology research", "Identification of usability issues in existing IGT generation models", "Proposition of new research questions focused on model constraints and user needs"], "limitations": "The study is based on a small user sample, limiting generalizability of findings.", "keywords": ["computational morphology", "language documentation", "User-Centered Design", "multilingual IGT", "usability"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2509.10764", "pdf": "https://arxiv.org/pdf/2509.10764.pdf", "abs": "https://arxiv.org/abs/2509.10764", "title": "LubDubDecoder: Bringing Micro-Mechanical Cardiac Monitoring to Hearables", "authors": ["Siqi Zhang", "Xiyuxing Zhang", "Duc Vu", "Tao Qiang", "Clara Palacios", "Jiangyifei Zhu", "Yuntao Wang", "Mayank Goel", "Justin Chan"], "categories": ["cs.HC"], "comment": null, "summary": "We present LubDubDecoder, a system that enables fine-grained monitoring of\nmicro-cardiac vibrations associated with the opening and closing of heart\nvalves across a range of hearables. Our system transforms the built-in speaker,\nthe only transducer common to all hearables, into an acoustic sensor that\ncaptures the coarse \"lub-dub\" heart sounds, leverages their shared temporal and\nspectral structure to reconstruct the subtle seismocardiography (SCG) and\ngyrocardiography (GCG) waveforms, and extract the timing of key micro-cardiac\nevents. In an IRB-approved feasibility study with 18 users, our system achieves\ncorrelations of 0.88-0.95 compared to chest-mounted reference measurements in\nwithin-user and cross-user evaluations, and generalizes to unseen hearables\nusing a zero-effort adaptation scheme with a correlation of 0.91. Our system is\nrobust across remounting sessions and music playback.", "AI": {"tldr": "LubDubDecoder enables fine-grained monitoring of heart sounds using hearables' built-in speakers.", "motivation": "To monitor micro-cardiac vibrations associated with heart valve movements using common hearable devices.", "method": "The system uses the built-in speaker as an acoustic sensor to capture heart sounds and reconstruct SCG and GCG waveforms.", "result": "Achieved correlations of 0.88-0.95 with chest-mounted references and 0.91 generalization to unseen hearables in a study with 18 users.", "conclusion": "The system is robust across remounting sessions and music playback, showcasing its adaptability for health monitoring.", "key_contributions": ["Transforms hearable speakers into acoustic sensors for heart sound monitoring", "Achieves high correlation with chest-mounted measurements", "Demonstrates robust performance with zero-effort adaptation to new devices"], "limitations": "", "keywords": ["heart monitoring", "hearables", "seismocardiography", "gyrocardiography", "health informatics"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2509.10663", "pdf": "https://arxiv.org/pdf/2509.10663.pdf", "abs": "https://arxiv.org/abs/2509.10663", "title": "Context Copying Modulation: The Role of Entropy Neurons in Managing Parametric and Contextual Knowledge Conflicts", "authors": ["Zineddine Tighidet", "Andrea Mogini", "Hedi Ben-younes", "Jiali Mei", "Patrick Gallinari", "Benjamin Piwowarski"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2025", "summary": "The behavior of Large Language Models (LLMs) when facing contextual\ninformation that conflicts with their internal parametric knowledge is\ninconsistent, with no generally accepted explanation for the expected outcome\ndistribution. Recent work has identified in autoregressive transformer models a\nclass of neurons -- called entropy neurons -- that produce a significant effect\non the model output entropy while having an overall moderate impact on the\nranking of the predicted tokens. In this paper, we investigate the preliminary\nclaim that these neurons are involved in inhibiting context copying behavior in\ntransformers by looking at their role in resolving conflicts between contextual\nand parametric information. We show that entropy neurons are responsible for\nsuppressing context copying across a range of LLMs, and that ablating them\nleads to a significant change in the generation process. These results enhance\nour understanding of the internal dynamics of LLMs when handling conflicting\ninformation.", "AI": {"tldr": "This paper investigates the role of entropy neurons in Large Language Models (LLMs) and their impact on context copying behavior when faced with conflicting information.", "motivation": "Understanding how LLMs manage conflicts between contextual and parametric information is crucial for improving their performance and reliability.", "method": "The authors analyze entropy neurons in autoregressive transformer models to determine their effect on context copying and model output entropy.", "result": "The study finds that entropy neurons suppress context copying across various LLMs; ablating these neurons significantly alters the generation process.", "conclusion": "The findings provide deeper insights into the dynamics of LLMs when encountering conflicting information, highlighting the importance of entropy neurons in resolving those conflicts.", "key_contributions": ["Demonstration of the role of entropy neurons in context management within LLMs.", "Evidence that ablation of these neurons leads to significant changes in output behavior.", "Insights into internal model dynamics regarding conflicting information."], "limitations": "", "keywords": ["Large Language Models", "entropy neurons", "context copying", "autoregressive transformers", "conflicting information"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.10776", "pdf": "https://arxiv.org/pdf/2509.10776.pdf", "abs": "https://arxiv.org/abs/2509.10776", "title": "Bonsai: Intentional and Personalized Social Media Feeds", "authors": ["Omar El Malki", "Marianne Aubin Le Quéré", "Andrés Monroy-Hernández", "Manoel Horta Ribeiro"], "categories": ["cs.HC"], "comment": null, "summary": "Modern social media feeds use predictive models to maximize engagement, often\nmisaligning how people consume content with how they wish to. We introduce\nBonsai, a system that enables people to build personalized and intentional\nfeeds. Bonsai implements a platform-agnostic framework comprising Planning,\nSourcing, Curating, and Ranking modules. Altogether, this framework allows\nusers to express their intent in natural language and exert fine-grained\ncontrol over a procedurally transparent feed creation process. We evaluated the\nsystem with 15 Bluesky users in a two-phase, multi-week study. We find that\nparticipants successfully used our system to discover new content, filter out\nirrelevant or toxic posts, and disentangle engagement from intent, but curating\nintentional feeds required participants to exert more effort than they are used\nto. Simultaneously, users sought system transparency mechanisms to trust and\neffectively use intentional, personalized feeds. Overall, our work highlights\nintentional feedbuilding as a viable path beyond engagement-based optimization.", "AI": {"tldr": "Bonsai is a system designed for building personalized social media feeds, allowing users to express intent in natural language and gain control over content, evaluated with Bluesky users.", "motivation": "To address the misalignment between how people consume content on social media and their true preferences by providing a system for intentional feed creation.", "method": "Bonsai utilizes a platform-agnostic framework that includes Planning, Sourcing, Curating, and Ranking modules to enable precise user control and intent expression.", "result": "Participants were able to discover new content and filter out unwanted posts while struggling with the additional effort required for intentional feed curation.", "conclusion": "The findings suggest that building intentional feeds is a feasible alternative to traditional engagement-driven models but requires user effort and transparency for trust.", "key_contributions": ["Introduction of Bonsai for intentional social media feeds", "Evaluation with real users highlighting the system's effectiveness", "Insights into user effort and desire for transparency in feed curation"], "limitations": "Curation of intentional feeds requires more user effort than traditional methods, potentially impacting usability.", "keywords": ["intentional feeds", "social media", "user engagement", "content curation", "system transparency"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.10685", "pdf": "https://arxiv.org/pdf/2509.10685.pdf", "abs": "https://arxiv.org/abs/2509.10685", "title": "Pluralistic Alignment for Healthcare: A Role-Driven Framework", "authors": ["Jiayou Zhong", "Anudeex Shetty", "Chao Jia", "Xuanrui Lin", "Usman Naseem"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to EMNLP 2025 (Main Proceedings)", "summary": "As large language models are increasingly deployed in sensitive domains such\nas healthcare, ensuring their outputs reflect the diverse values and\nperspectives held across populations is critical. However, existing alignment\napproaches, including pluralistic paradigms like Modular Pluralism, often fall\nshort in the health domain, where personal, cultural, and situational factors\nshape pluralism. Motivated by the aforementioned healthcare challenges, we\npropose a first lightweight, generalizable, pluralistic alignment approach,\nEthosAgents, designed to simulate diverse perspectives and values. We\nempirically show that it advances the pluralistic alignment for all three modes\nacross seven varying-sized open and closed models. Our findings reveal that\nhealth-related pluralism demands adaptable and normatively aware approaches,\noffering insights into how these models can better respect diversity in other\nhigh-stakes domains.", "AI": {"tldr": "This paper presents EthosAgents, a novel pluralistic alignment approach for large language models in healthcare, addressing the challenge of ensuring diverse values and perspectives are reflected in model outputs.", "motivation": "The need for alignment approaches that respect diverse values in healthcare, where personal, cultural, and situational factors influence pluralism, is critical.", "method": "The paper proposes a lightweight and generalizable approach called EthosAgents, which simulates diverse perspectives and is empirically tested across various model sizes.", "result": "EthosAgents improves pluralistic alignment across three modes in seven different models, demonstrating the importance of adaptable and normatively aware approaches in health-related pluralism.", "conclusion": "The findings indicate that effective pluralistic alignment in healthcare may contribute insights applicable to other high-stakes domains.", "key_contributions": ["Introduction of EthosAgents for pluralistic alignment in healthcare.", "Empirical validation across multiple model sizes.", "Insights into diversity in AI outputs for sensitive domains."], "limitations": "The approach may require further testing across more diverse contexts beyond healthcare.", "keywords": ["pluralistic alignment", "healthcare", "large language models", "diversity", "EthosAgents"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.10780", "pdf": "https://arxiv.org/pdf/2509.10780.pdf", "abs": "https://arxiv.org/abs/2509.10780", "title": "Bridging Cultural Distance Between Models Default and Local Classroom Demands: How Global Teachers Adopt GenAI to Support Everyday Teaching Practices", "authors": ["Ruiwei Xiao", "Qing Xiao", "Xinying Hou", "Hanqi Jane Li", "Phenyo Phemelo Moletsane", "Hong Shen", "John Stamper"], "categories": ["cs.HC", "cs.AI"], "comment": "15 pages, 1 figure", "summary": "Generative AI (GenAI) is rapidly entering K-12 classrooms, offering teachers\nnew ways for teaching practices. Yet GenAI models are often trained on\nculturally uneven datasets, embedding a \"default culture\" that often misaligns\nwith local classrooms. To understand how teachers navigate this gap, we defined\nthe new concept Cultural Distance (the gap between GenAI's default cultural\nrepertoire and the situated demands of teaching practice) and conducted\nin-depth interviews with 30 K-12 teachers, 10 each from South Africa, Taiwan,\nand the United States, who had integrated AI into their teaching practice.\nThese teachers' experiences informed the development of our three-level\ncultural distance framework. This work contributes the concept and framework of\ncultural distance, six illustrative instances spanning in low, mid, high\ndistance levels with teachers' experiences and strategies for addressing them.\nEmpirically, we offer implications to help AI designers, policymakers, and\neducators create more equitable and culturally responsive GenAI tools for\neducation.", "AI": {"tldr": "This paper introduces the concept of Cultural Distance in the context of Generative AI integration in K-12 education, based on interviews with teachers from three countries.", "motivation": "To address the disparities between the cultural default of GenAI models and the actual cultural needs in K-12 classrooms.", "method": "In-depth interviews with 30 K-12 teachers from South Africa, Taiwan, and the United States, leading to the creation of a three-level cultural distance framework.", "result": "The study identifies the concept of cultural distance and presents six illustrative cases demonstrating low, mid, and high cultural distance in teaching experiences.", "conclusion": "The findings highlight the need for AI developers, policymakers, and educators to create culturally responsive AI tools for equitable education.", "key_contributions": ["Introduction of the Cultural Distance concept", "Development of a three-level cultural distance framework", "Illustrative cases based on real teacher experiences"], "limitations": "The study is limited to interviews with teachers from only three countries, which may not be representative of global experiences.", "keywords": ["Generative AI", "Cultural Distance", "K-12 education", "AI integration", "equitable tools"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2509.10696", "pdf": "https://arxiv.org/pdf/2509.10696.pdf", "abs": "https://arxiv.org/abs/2509.10696", "title": "Struct-Bench: A Benchmark for Differentially Private Structured Text Generation", "authors": ["Shuaiqi Wang", "Vikas Raunak", "Arturs Backurs", "Victor Reis", "Pei Zhou", "Sihao Chen", "Longqi Yang", "Zinan Lin", "Sergey Yekhanin", "Giulia Fanti"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Differentially private (DP) synthetic data generation is a promising\ntechnique for utilizing private datasets that otherwise cannot be exposed for\nmodel training or other analytics. While much research literature has focused\non generating private unstructured text and image data, in enterprise settings,\nstructured data (e.g., tabular) is more common, often including natural\nlanguage fields or components. Existing synthetic data evaluation techniques\n(e.g., FID) struggle to capture the structural properties and correlations of\nsuch datasets. In this work, we propose Struct-Bench, a framework and benchmark\nfor evaluating synthetic datasets derived from structured datasets that contain\nnatural language data. The Struct-Bench framework requires users to provide a\nrepresentation of their dataset structure as a Context-Free Grammar (CFG). Our\nbenchmark comprises 5 real-world and 2 synthetically generated datasets, each\nannotated with CFGs. We show that these datasets demonstrably present a great\nchallenge even for state-of-the-art DP synthetic data generation methods.\nStruct-Bench also includes reference implementations of different metrics and a\nleaderboard, thereby providing researchers a standardized evaluation platform\nto benchmark and investigate privacy-preserving synthetic data generation\nmethods. Further, we also present a case study showing how to use Struct-Bench\nto improve the synthetic data quality of Private Evolution (PE) on structured\ndata. The benchmark and the leaderboard have been publicly made available at\nhttps://struct-bench.github.io.", "AI": {"tldr": "Struct-Bench is a framework for evaluating synthetic datasets generated from structured data containing natural language, addressing the limitations of existing evaluation methods.", "motivation": "The paper aims to improve the evaluation of DP synthetic data generation techniques for structured datasets, which often include natural language elements.", "method": "The authors introduce Struct-Bench, which requires dataset representation via Context-Free Grammar (CFG) and includes a benchmark with real and synthetic datasets annotated with CFGs.", "result": "The study demonstrates that existing DP synthetic data generation methods face significant challenges when evaluated using the Struct-Bench framework on structured datasets.", "conclusion": "Struct-Bench provides a standardized platform for assessing privacy-preserving synthetic data generation methods and has been made publicly accessible.", "key_contributions": ["Introduction of the Struct-Bench framework for evaluating structured synthetic datasets with natural language elements.", "Creation of a benchmark comprising real and synthetic datasets annotated with CFGs.", "Development of reference metrics and a leaderboard for standardized evaluation."], "limitations": "", "keywords": ["Differential Privacy", "synthetic data", "structured data", "natural language", "evaluation"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.10782", "pdf": "https://arxiv.org/pdf/2509.10782.pdf", "abs": "https://arxiv.org/abs/2509.10782", "title": "Do Teachers Dream of GenAI Widening Educational (In)equality? Envisioning the Future of K-12 GenAI Education from Global Teachers' Perspectives", "authors": ["Ruiwei Xiao", "Qing Xiao", "Xinying Hou", "Phenyo Phemelo Moletsane", "Hanqi Jane Li", "Hong Shen", "John Stamper"], "categories": ["cs.HC"], "comment": "18 pages, 3 figures", "summary": "Generative artificial intelligence (GenAI) is rapidly entering K-12\nclassrooms worldwide, initiating urgent debates about its potential to either\nreduce or exacerbate educational inequalities. Drawing on interviews with 30\nK-12 teachers across the United States, South Africa, and Taiwan, this study\nexamines how teachers navigate this GenAI tension around educational\nequalities. We found teachers actively framed GenAI education as an\nequality-oriented practice: they used it to alleviate pre-existing inequalities\nwhile simultaneously working to prevent new inequalities from emerging. Despite\nthese efforts, teachers confronted persistent systemic barriers, i.e., unequal\ninfrastructure, insufficient professional training, and restrictive social\nnorms, that individual initiative alone could not overcome. Teachers thus\narticulated normative visions for more inclusive GenAI education. By centering\nteachers' practices, constraints, and future envisions, this study contributes\na global account of how GenAI education is being integrated into K-12 contexts\nand highlights what is required to make its adoption genuinely equal.", "AI": {"tldr": "This study explores how K-12 teachers navigate generative AI in education, focusing on its potential to address or exacerbate inequalities while highlighting systemic barriers.", "motivation": "To investigate how generative AI is perceived and implemented by K-12 teachers in relation to educational inequalities.", "method": "Interviews with 30 K-12 teachers in the United States, South Africa, and Taiwan were conducted to gather qualitative data on their experiences and perspectives.", "result": "Teachers framed GenAI education as a practice aimed at promoting equality, using it to alleviate existing inequalities despite encountering systemic barriers.", "conclusion": "For GenAI adoption in education to be truly equitable, systemic issues related to infrastructure, training, and social norms must be addressed alongside teacher initiatives.", "key_contributions": ["Provides a global perspective on the integration of GenAI in K-12 education.", "Highlights the importance of teacher agency in navigating educational inequalities.", "Identifies systemic barriers to equitable GenAI adoption in schools."], "limitations": "The study is based on qualitative interviews from a limited number of teachers across three countries; broader quantitative data may be needed for comprehensive insights.", "keywords": ["Generative AI", "K-12 education", "educational inequality", "teacher practices", "systemic barriers"], "importance_score": 4, "read_time_minutes": 18}}
{"id": "2509.10697", "pdf": "https://arxiv.org/pdf/2509.10697.pdf", "abs": "https://arxiv.org/abs/2509.10697", "title": "A Survey on Retrieval And Structuring Augmented Generation with Large Language Models", "authors": ["Pengcheng Jiang", "Siru Ouyang", "Yizhu Jiao", "Ming Zhong", "Runchu Tian", "Jiawei Han"], "categories": ["cs.CL"], "comment": "KDD'25 survey track", "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nwith their remarkable capabilities in text generation and reasoning. However,\nthese models face critical challenges when deployed in real-world applications,\nincluding hallucination generation, outdated knowledge, and limited domain\nexpertise. Retrieval And Structuring (RAS) Augmented Generation addresses these\nlimitations by integrating dynamic information retrieval with structured\nknowledge representations. This survey (1) examines retrieval mechanisms\nincluding sparse, dense, and hybrid approaches for accessing external\nknowledge; (2) explore text structuring techniques such as taxonomy\nconstruction, hierarchical classification, and information extraction that\ntransform unstructured text into organized representations; and (3) investigate\nhow these structured representations integrate with LLMs through prompt-based\nmethods, reasoning frameworks, and knowledge embedding techniques. It also\nidentifies technical challenges in retrieval efficiency, structure quality, and\nknowledge integration, while highlighting research opportunities in multimodal\nretrieval, cross-lingual structures, and interactive systems. This\ncomprehensive overview provides researchers and practitioners with insights\ninto RAS methods, applications, and future directions.", "AI": {"tldr": "This survey reviews Retrieval And Structuring (RAS) Augmented Generation for Large Language Models, focusing on retrieval mechanisms, text structuring techniques, and integration with LLMs.", "motivation": "To address the challenges faced by Large Language Models (LLMs) in real-world applications, such as hallucinations and limited domain expertise, by integrating information retrieval with structured knowledge.", "method": "Examines various retrieval mechanisms (sparse, dense, hybrid) and text structuring techniques (taxonomy construction, hierarchical classification, information extraction) to create organized representations for LLMs.", "result": "Identifies technical challenges in retrieval efficiency, structure quality, and knowledge integration; and highlights research opportunities in multimodal retrieval, cross-lingual structures, and interactive systems.", "conclusion": "Provides a comprehensive overview of RAS methods, applications, and future research directions for enhancing LLM capabilities.", "key_contributions": ["Examines multiple retrieval mechanisms for accessing external knowledge.", "Explores structuring techniques to enhance unstructured text representation.", "Highlights challenges and opportunities for future research in LLM integration."], "limitations": "", "keywords": ["Large Language Models", "Retrieval and Structuring", "Knowledge Integration", "Text Structuring", "Multimodal Retrieval"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.10789", "pdf": "https://arxiv.org/pdf/2509.10789.pdf", "abs": "https://arxiv.org/abs/2509.10789", "title": "\"I thought it was my mistake, but it's really the design'': A Critical Examination of the Accessibility of User-Enacted Moderation Tools on Facebook and X", "authors": ["Sudhamshu Hosamane", "Alyvia Walters", "Yao Lyu", "Shagun Jhaver"], "categories": ["cs.HC"], "comment": null, "summary": "As social media platforms increasingly promote the use of user-enacted\nmoderation tools (e.g., reporting, blocking, content filters) to address online\nharms, it becomes crucially important that such controls are usable for\neveryone. We evaluate the accessibility of these moderation tools on two\nmainstream platforms -- Facebook and X -- through interviews and task-based\nwalkthroughs with 15 individuals with vision impairments. Adapting the lens of\n\\emph{administrative burden of safety work}, we identify three interleaved\ncosts that users with vision loss incur while interacting with moderation\ntools: \\emph{learning costs} (understanding what controls do and where they\nlive), \\emph{compliance costs} (executing multi-step procedures under screen\nreader and low-vision conditions), and \\emph{psychological costs} (experiencing\nuncertainty, stress, and diminished agency). Our analysis bridges the fields of\ncontent moderation and accessibility in HCI research and contributes (1) a\ncross-platform catalog of accessibility and usability breakdowns affecting\nsafety tools; and (2) design recommendations for reducing this burden.", "AI": {"tldr": "The paper evaluates the accessibility of user-enacted moderation tools on Facebook and X for individuals with vision impairments, identifying usability challenges and proposing design recommendations.", "motivation": "To enhance the usability of moderation tools on social media for users with vision impairments, addressing the growing reliance on these tools to tackle online harms.", "method": "Interviews and task-based walkthroughs with 15 individuals with vision impairments to assess the accessibility of moderation tools on two platforms (Facebook and X).", "result": "Identified three interleaved costs for users with vision impairments: learning costs, compliance costs, and psychological costs, highlighting barriers faced when using moderation tools.", "conclusion": "The analysis provides a cross-platform catalog of accessibility issues and offers design recommendations to improve usability and reduce the administrative burden of safety work.", "key_contributions": ["A catalog of accessibility and usability breakdowns affecting moderation tools.", "Design recommendations for improving accessibility of social media moderation tools."], "limitations": "", "keywords": ["accessibility", "moderation tools", "vision impairments", "usability", "HCI"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.10708", "pdf": "https://arxiv.org/pdf/2509.10708.pdf", "abs": "https://arxiv.org/abs/2509.10708", "title": "SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation", "authors": ["Iman Barati", "Mostafa Amiri", "Heshaam Faili"], "categories": ["cs.CL"], "comment": null, "summary": "Supervised Fine-Tuning (SFT) is essential for training large language models\n(LLMs), significantly enhancing critical capabilities such as instruction\nfollowing and in-context learning. Nevertheless, creating suitable training\ndatasets tailored for specific domains remains challenging due to unique domain\nconstraints and data scarcity. In this paper, we propose SearchInstruct, an\ninnovative method explicitly designed to construct high quality instruction\ndatasets for SFT. Our approach begins with a limited set of domain specific,\nhuman generated questions, which are systematically expanded using a large\nlanguage model. Subsequently, domain relevant resources are dynamically\nretrieved to generate accurate and contextually appropriate answers for each\naugmented question. Experimental evaluation demonstrates that SearchInstruct\nenhances both the diversity and quality of SFT datasets, leading to measurable\nimprovements in LLM performance within specialized domains. Additionally, we\nshow that beyond dataset generation, the proposed method can also effectively\nfacilitate tasks such as model editing, enabling efficient updates to existing\nmodels. To facilitate reproducibility and community adoption, we provide full\nimplementation details, the complete set of generated instruction response\npairs, and the source code in a publicly accessible Git repository:\n[https://github.com/mostafaamiri/SearchInstruct](https://github.com/mostafaamiri/SearchInstruct)", "AI": {"tldr": "The paper presents SearchInstruct, a method for generating high-quality instruction datasets for supervised fine-tuning of large language models, enhancing their performance in specialized domains.", "motivation": "Creating suitable training datasets for supervised fine-tuning of large language models is crucial yet challenging due to domain constraints and data scarcity.", "method": "SearchInstruct expands a limited set of domain-specific human-generated questions using a large language model, then retrieves domain-relevant resources to generate accurate answers.", "result": "Experimental evaluations show that SearchInstruct improves both the diversity and quality of datasets, leading to better performance of LLMs in specialized areas, alongside facilitating model editing.", "conclusion": "The proposed method not only aids in dataset generation but also streamlines model updates; implementation details and resources are available on GitHub.", "key_contributions": ["Introduction of SearchInstruct for dataset generation for SFT", "Demonstrated improvements in LLM performance with specialized datasets", "Facilitation of model editing processes."], "limitations": "", "keywords": ["Large Language Models", "Supervised Fine-Tuning", "Dataset Generation", "Instruction Learning", "Model Editing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.10830", "pdf": "https://arxiv.org/pdf/2509.10830.pdf", "abs": "https://arxiv.org/abs/2509.10830", "title": "The Siren Song of LLMs: How Users Perceive and Respond to Dark Patterns in Large Language Models", "authors": ["Yike Shi", "Qing Xiao", "Qing", "Hu", "Hong Shen", "Hua Shen"], "categories": ["cs.HC"], "comment": null, "summary": "Large language models can influence users through conversation, creating new\nforms of dark patterns that differ from traditional UX dark patterns. We define\nLLM dark patterns as manipulative or deceptive behaviors enacted in dialogue.\nDrawing on prior work and AI incident reports, we outline a diverse set of\ncategories with real-world examples. Using them, we conducted a scenario-based\nstudy where participants (N=34) compared manipulative and neutral LLM\nresponses. Our results reveal that recognition of LLM dark patterns often\nhinged on conversational cues such as exaggerated agreement, biased framing, or\nprivacy intrusions, but these behaviors were also sometimes normalized as\nordinary assistance. Users' perceptions of these dark patterns shaped how they\nrespond to them. Responsibilities for these behaviors were also attributed in\ndifferent ways, with participants assigning it to companies and developers, the\nmodel itself, or to users. We conclude with implications for design, advocacy,\nand governance to safeguard user autonomy.", "AI": {"tldr": "This paper examines LLM dark patterns—manipulative behaviors in dialogues by large language models—through a scenario-based study to understand user recognition and perceptions.", "motivation": "To address the emergence of LLM dark patterns in user interactions with conversational agents, differentiating them from traditional UX dark patterns and assessing their implications.", "method": "Conducted a scenario-based study with participants comparing manipulative and neutral responses from LLMs, analyzing recognition cues and attributions of responsibility.", "result": "Findings showed that users recognized manipulative LLM behaviors based on conversational cues, but some normalized such interactions as regular assistance.", "conclusion": "The paper discusses the need for design, advocacy, and governance measures to protect user autonomy against manipulative LLM behaviors.", "key_contributions": ["Definition and categorization of LLM dark patterns", "Empirical evidence from user interactions with LLMs", "Insights on user perceptions and responsibility attributions regarding LLM behaviors"], "limitations": "", "keywords": ["LLM dark patterns", "user autonomy", "conversational agents", "manipulative behavior", "UX design"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.10737", "pdf": "https://arxiv.org/pdf/2509.10737.pdf", "abs": "https://arxiv.org/abs/2509.10737", "title": "PolyTruth: Multilingual Disinformation Detection using Transformer-Based Language Models", "authors": ["Zaur Gouliev", "Jennifer Waters", "Chengqian Wang"], "categories": ["cs.CL", "cs.LG", "68T50, 68T07", "I.2.7; H.3.3"], "comment": "11 pages, 5 figures, 4 tables. Submitted to arXiv in Computation and\n  Language", "summary": "Disinformation spreads rapidly across linguistic boundaries, yet most AI\nmodels are still benchmarked only on English. We address this gap with a\nsystematic comparison of five multilingual transformer models: mBERT, XLM,\nXLM-RoBERTa, RemBERT, and mT5 on a common fake-vs-true machine learning\nclassification task. While transformer-based language models have demonstrated\nnotable success in detecting disinformation in English, their effectiveness in\nmultilingual contexts still remains up for debate. To facilitate evaluation, we\nintroduce PolyTruth Disinfo Corpus, a novel corpus of 60,486 statement pairs\n(false claim vs. factual correction) spanning over twenty five languages that\ncollectively cover five language families and a broad topical range from\npolitics, health, climate, finance, and conspiracy, half of which are\nfact-checked disinformation claims verified by an augmented MindBugs Discovery\ndataset. Our experiments revealed performance variations. Models such as\nRemBERT achieved better overall accuracy, particularly excelling in\nlow-resource languages, whereas models like mBERT and XLM exhibit considerable\nlimitations when training data is scarce. We provide a discussion of these\nperformance patterns and implications for real-world deployment. The dataset is\npublicly available on our GitHub repository to encourage further\nexperimentation and advancement. Our findings illuminate both the potential and\nthe current limitations of AI systems for multilingual disinformation\ndetection.", "AI": {"tldr": "This paper systematically compares five multilingual transformer models on their ability to detect disinformation across multiple languages and introduces a novel corpus for evaluation.", "motivation": "To address the effectiveness of AI models in detecting disinformation in multilingual contexts, as current benchmarks predominantly focus on English.", "method": "A comparative analysis of multilingual transformer models (mBERT, XLM, XLM-RoBERTa, RemBERT, and mT5) on a fake-vs-true classification task using the PolyTruth Disinfo Corpus.", "result": "RemBERT achieved better accuracy overall, especially in low-resource languages, while mBERT and XLM showed limitations with scarce training data.", "conclusion": "The study highlights the potential and limitations of AI systems for multilingual disinformation detection and provides a publicly available dataset for further research.", "key_contributions": ["Introduction of PolyTruth Disinfo Corpus with 60,486 statement pairs across 25 languages.", "Systematic comparison of multilingual transformer models for disinformation detection.", "Insights on performance variations among models, emphasizing the real-world implications."], "limitations": "Performance variations may not fully represent all languages and contexts; primarily focused on selected models.", "keywords": ["disinformation", "multilingual models", "machine learning", "transformers", "natural language processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.10848", "pdf": "https://arxiv.org/pdf/2509.10848.pdf", "abs": "https://arxiv.org/abs/2509.10848", "title": "Tracer: A Forensic Framework for Detecting Fraudulent Speedruns from Game Replays", "authors": ["Jaeung Franciskus Yoo", "Huy Kang Kim"], "categories": ["cs.HC", "cs.CY"], "comment": "16 pages, 8 figures. Extended version of the paper in Companion\n  Proceedings of the Annual Symposium on Computer-Human Interaction in Play\n  (CHI PLAY Companion 25), New York, NY, USA, October 2025", "summary": "Speedrun, a practice of completing a game as quickly as possible, has\nfostered vibrant communities driven by creativity, competition, and mastery of\ngame mechanics and motor skills. However, this contest also attracts malicious\nactors as financial incentives come into play. As media and software\nmanipulation techniques advance - such as spliced footage, modified game\nsoftware and live stream with staged setups - forged speedruns have become\nincreasingly difficult to detect. Volunteer-driven communities invest\nsignificant effort to verify submissions, yet the process remains slow,\ninconsistent, and reliant on informal expertise. In high-profile cases,\nfraudulent runs have gone undetected for years, allowing perpetrators to gain\nfame and financial benefits through monetised viewership, sponsorships,\ndonations, and community bounties. To address this gap, we propose Tracer,\nTamper Recognition via Analysis of Continuity and Events in game Runs, a\nmodular framework for identifying artefacts of manipulation in speedrun\nsubmissions. Tracer provides structured guidelines across audiovisual,\nphysical, and cyberspace dimensions, systematically documenting dispersed\nin-game knowledge and previously reported fraudulent cases to enhance\nverification efficiency.", "AI": {"tldr": "Tracer is a modular framework designed for identifying manipulated speedrun submissions through structured guidelines.", "motivation": "The verification process for speedrun submissions is slow and inconsistent, leading to instances of fraudulent runs going undetected for extended periods.", "method": "Tracer employs a systematic approach that analyzes audiovisual, physical, and cyberspace dimensions to detect manipulation artefacts in speedruns.", "result": "The framework enhances the efficiency of verifying speedrun submissions by documenting in-game knowledge and fraudulent cases.", "conclusion": "Tracer offers a structured method to improve detection of fraudulent speedruns, supporting community-driven verification processes.", "key_contributions": ["Introduction of the Tracer framework for speedrun verification.", "Structured guidelines for identifying manipulation artefacts.", "Documentation of past fraudulent cases to aid in future detection."], "limitations": "", "keywords": ["speedrun", "fraud detection", "game verification", "HCI", "manipulation analysis"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2509.10739", "pdf": "https://arxiv.org/pdf/2509.10739.pdf", "abs": "https://arxiv.org/abs/2509.10739", "title": "Reasoning Under Uncertainty: Exploring Probabilistic Reasoning Capabilities of LLMs", "authors": ["Mobina Pournemat", "Keivan Rezaei", "Gaurang Sriramanan", "Arman Zarei", "Jiaxiang Fu", "Yang Wang", "Hamid Eghbalzadeh", "Soheil Feizi"], "categories": ["cs.CL"], "comment": "25 pages, 4 figures, 6 tables", "summary": "Despite widespread success in language understanding and generation, large\nlanguage models (LLMs) exhibit unclear and often inconsistent behavior when\nfaced with tasks that require probabilistic reasoning. In this work, we present\nthe first comprehensive study of the reasoning capabilities of LLMs over\nexplicit discrete probability distributions. Given observations from a\nprobability distribution, we evaluate models on three carefully designed tasks,\nmode identification, maximum likelihood estimation, and sample generation, by\nprompting them to provide responses to queries about either the joint\ndistribution or its conditionals. These tasks thus probe a range of\nprobabilistic skills, including frequency analysis, marginalization, and\ngenerative behavior. Through comprehensive empirical evaluations, we\ndemonstrate that there exists a clear performance gap between smaller and\nlarger models, with the latter demonstrating stronger inference and surprising\ncapabilities in sample generation. Furthermore, our investigations reveal\nnotable limitations, including sensitivity to variations in the notation\nutilized to represent probabilistic outcomes and performance degradation of\nover 60% as context length increases. Together, our results provide a detailed\nunderstanding of the probabilistic reasoning abilities of LLMs and identify key\ndirections for future improvement.", "AI": {"tldr": "This paper investigates the reasoning capabilities of large language models (LLMs) when dealing with explicit discrete probability distributions, identifying performance gaps and limitations in their probabilistic skills.", "motivation": "To understand the reasoning capabilities of LLMs regarding probabilistic reasoning, addressing the inconsistencies observed in their behavior on such tasks.", "method": "The study evaluates LLMs using three tasks: mode identification, maximum likelihood estimation, and sample generation, with careful prompting regarding joint and conditional distributions.", "result": "The study finds a clear performance gap between smaller and larger models, with larger models performing better in inference and sample generation; however, notable limitations were observed such as sensitivity to notation and significant performance degradation with increased context length.", "conclusion": "The findings reveal important insights into LLMs' probabilistic reasoning abilities and highlight directions for future improvements.", "key_contributions": ["First comprehensive study of LLMs' reasoning over discrete probability distributions.", "Identification of performance gaps between model sizes in probabilistic reasoning tasks.", "Insights into limitations regarding notation and context length impact on performance."], "limitations": "Sensitivity to variations in notation used to represent probabilities; performance drops over 60% with increased context length.", "keywords": ["large language models", "probabilistic reasoning", "machine learning", "inference", "sample generation"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2509.10906", "pdf": "https://arxiv.org/pdf/2509.10906.pdf", "abs": "https://arxiv.org/abs/2509.10906", "title": "Crisis Messaging Journeys: Epistemic Struggles over CDC Guidance During COVID-19", "authors": ["Tawfiq Ammari"], "categories": ["cs.HC"], "comment": null, "summary": "This study investigates how the U.S. Centers for Disease Control and\nPrevention (CDC) communicated COVID-19 guidance on Twitter and how publics\nresponded over two years of the pandemic. Drawing on 275,124 tweets mentioning\nor addressing @CDCgov, I combine BERTopic modeling, sentiment analysis (VADER),\ncredibility checks (Iffy Index), change point detection (PELT), and survival\nanalysis to trace three phases of discourse: (1) early hoax claims and testing\ndebates, (2) lockdown and mask controversies, and (3) post-vaccine variant\nconcerns. I introduce the concept of crisis messaging journeys to explain how\narchived \"receipts\" of prior CDC statements fueled epistemic struggles,\npolitical polarization, and sustained engagement. Findings show that skeptical,\ncognitively complex discourse particularly questioning institutional trust\nprolonged participation, while positive affirmation predicted faster\ndisengagement. I conclude with design recommendations for annotated, cautious,\nand flashpoint-responsive communication strategies to bolster public trust and\nresilience during protracted health crises.", "AI": {"tldr": "This study analyzes CDC's Twitter communication during COVID-19, focusing on public responses and discourse progression over two years.", "motivation": "To understand how the CDC's communication impacted public engagement and trust during the COVID-19 pandemic.", "method": "The study analyzes 275,124 tweets using BERTopic modeling, sentiment analysis (VADER), credibility checks (Iffy Index), change point detection (PELT), and survival analysis.", "result": "Findings reveal that skeptical and complex discourse prolonged public participation, while positive affirmation led to quicker disengagement.", "conclusion": "The study concludes with design recommendations for effective health crisis communication to enhance trust and resilience.", "key_contributions": ["Introduction of crisis messaging journeys concept", "Comprehensive analysis of public discourse on CDC's communication", "Design recommendations for health crisis communication strategies"], "limitations": "", "keywords": ["CDC communication", "COVID-19", "public trust", "sentiment analysis", "crisis messaging"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.10744", "pdf": "https://arxiv.org/pdf/2509.10744.pdf", "abs": "https://arxiv.org/abs/2509.10744", "title": "Automated MCQA Benchmarking at Scale: Evaluating Reasoning Traces as Retrieval Sources for Domain Adaptation of Small Language Models", "authors": ["Ozan Gokdemir", "Neil Getty", "Robert Underwood", "Sandeep Madireddy", "Franck Cappello", "Arvind Ramanathan", "Ian T. Foster", "Rick L. Stevens"], "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.11"], "comment": "This manuscript has been accepted for publication at the\n  Supercomputing 25 (SC '25) Conference (Frontiers in Generative AI for HPC\n  Science and Engineering: Foundations, Challenges, and Opportunities Workshop)\n  in St. Louis, MO, USA on November 16th, 2025. It will appear in the SC25\n  Workshop Proceedings after that date", "summary": "As scientific knowledge grows at an unprecedented pace, evaluation benchmarks\nmust evolve to reflect new discoveries and ensure language models are tested on\ncurrent, diverse literature. We propose a scalable, modular framework for\ngenerating multiple-choice question-answering (MCQA) benchmarks directly from\nlarge corpora of scientific papers. Our pipeline automates every stage of MCQA\ncreation, including PDF parsing, semantic chunking, question generation, and\nmodel evaluation. As a case study, we generate more than 16,000 MCQs from\n22,000 open-access articles in radiation and cancer biology. We then evaluate a\nsuite of small language models (1.1B-14B parameters) on these questions,\ncomparing baseline accuracy with retrieval-augmented generation (RAG) from\npaper-derived semantic chunks and from reasoning traces distilled from GPT-4.1.\nWe find that reasoning-trace retrieval consistently improves performance on\nboth synthetic and expert-annotated benchmarks, enabling several small models\nto surpass GPT-4 on the 2023 Astro Radiation and Cancer Biology exam.", "AI": {"tldr": "A modular framework for creating multiple-choice question-answering benchmarks from scientific literature is proposed, automating various stages and demonstrating improved language model performance.", "motivation": "To address the rapid growth of scientific knowledge, it is essential to update evaluation benchmarks for language models to reflect current literature.", "method": "A scalable, modular framework that automates the generation of multiple-choice questions from large scientific corpora, including stages like PDF parsing and semantic chunking.", "result": "The framework generated over 16,000 MCQs from 22,000 articles in radiation and cancer biology, with small language models outperforming GPT-4 on the exam.", "conclusion": "Reasoning-trace retrieval enhances performance on both synthetic and expert-annotated benchmarks, leading to superior results compared to previous models.", "key_contributions": ["Development of a modular pipeline for MCQA benchmark generation", "Mass generation of questions from open-access scientific literature", "Demonstrated performance improvements in small language models using reasoning traces"], "limitations": "", "keywords": ["language models", "multiple-choice questions", "scientific papers", "benchmarking", "reasoning traces"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.10950", "pdf": "https://arxiv.org/pdf/2509.10950.pdf", "abs": "https://arxiv.org/abs/2509.10950", "title": "Can GenAI Move from Individual Use to Collaborative Work? Experiences, Challenges, and Opportunities of Integrating GenAI into Collaborative Newsroom Routines", "authors": ["Qing Xiao", "Qing", "Hu", "Jingjia Xiao", "Hancheng Cao", "Hong Shen"], "categories": ["cs.HC", "cs.CY"], "comment": "17 pages, 1 figure", "summary": "Generative AI (GenAI) is reshaping work, but adoption remains largely\nindividual and experimental rather than integrated into collaborative routines.\nWhether GenAI can move from individual use to collaborative work is a critical\nquestion for future organizations. Journalism offers a compelling site to\nexamine this shift: individual journalists have already been disrupted by GenAI\ntools; yet newswork is inherently collaborative relying on shared routines and\ncoordinated workflows. We conducted 27 interviews with newsrooms managers,\neditors, and front-line journalists in China. We found that journalists\nfrequently used GenAI to support daily tasks, but value alignment was\nsafeguarded mainly through individual discretion. At the organizational level,\nGenAI use remained disconnected from team workflows, hindered by structural\nbarriers and cultural reluctance to share practices. These findings underscore\nthe gap between individual and collective adoption, pointing to the need for\naccounting for organizational structures, cultural norms, and workflow\nintegration when designing GenAI for collaborative work.", "AI": {"tldr": "This paper explores the integration of Generative AI (GenAI) into collaborative work environments, focusing on its use in journalism.", "motivation": "To understand how Generative AI can transition from individual usage to collaborative work, particularly in journalistic settings where teamwork is essential.", "method": "Conducted 27 interviews with newsroom managers, editors, and journalists in China to gather insights on GenAI usage in newswork.", "result": "Journalists used GenAI for daily tasks, but its use remained largely disconnected from team workflows due to structural barriers and cultural reluctance to share practices.", "conclusion": "Attention must be paid to organizational structures, cultural norms, and workflow integration when designing GenAI tools for collaborative environments.", "key_contributions": ["Identifies the gap between individual and collective GenAI adoption in journalism.", "Sheds light on the cultural and structural barriers affecting GenAI integration in collaborative work.", "Highlights the importance of organizational context in the design of GenAI tools."], "limitations": "", "keywords": ["Generative AI", "collaborative work", "journalism", "organizational culture", "workflow integration"], "importance_score": 4, "read_time_minutes": 17}}
{"id": "2509.10746", "pdf": "https://arxiv.org/pdf/2509.10746.pdf", "abs": "https://arxiv.org/abs/2509.10746", "title": "RECAP: Transparent Inference-Time Emotion Alignment for Medical Dialogue Systems", "authors": ["Adarsh Srinivasan", "Jacob Dineen", "Muhammad Umar Afzal", "Muhammad Uzair Sarfraz", "Irbaz B. Riaz", "Ben Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models in healthcare often miss critical emotional cues,\ndelivering medically sound but emotionally flat advice. This is especially\nproblematic in clinical contexts where patients are distressed and vulnerable,\nand require empathic communication to support safety, adherence, and trust. We\npresent RECAP (Reflect-Extract-Calibrate-Align-Produce), an inference-time\nframework that adds structured emotional reasoning without retraining. By\ndecomposing empathy into transparent appraisal-theoretic stages and exposing\nper-dimension Likert signals, RECAP produces nuanced, auditable responses.\nAcross EmoBench, SECEU, and EQ-Bench, RECAP improves emotional reasoning by\n22-28% on 8B models and 10-13% on larger models over zero-shot baselines.\nClinician evaluations further confirm superior empathetic communication. RECAP\nshows that modular, theory-grounded prompting can systematically enhance\nemotional intelligence in medical AI while preserving the accountability\nrequired for deployment.", "AI": {"tldr": "RECAP enhances emotional reasoning in healthcare AI by structuring empathy into distinct appraisal stages without retraining the model.", "motivation": "Large language models often fail to provide emotionally resonant communication in healthcare, which can harm patient trust and adherence.", "method": "RECAP framework decomposes empathy into transparent stages and uses per-dimension Likert signals to generate responses.", "result": "RECAP achieves a 22-28% improvement in emotional reasoning across various benchmarks for 8B models and 10-13% for larger models, with positive clinician evaluations.", "conclusion": "The RECAP framework demonstrates that structured prompting can significantly enhance emotional intelligence in medical AIs while maintaining accountability.", "key_contributions": ["Introduction of the RECAP framework for emotional reasoning in healthcare AI.", "Demonstration of improved outcomes in emotional reasoning metrics.", "Validation of the framework through clinician evaluations for empathetic communication."], "limitations": "The approach may require extensive validation in real-world clinical settings to fully ascertain its effectiveness and reliability.", "keywords": ["empathy", "language models", "healthcare AI", "emotional reasoning", "RECAP"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.10956", "pdf": "https://arxiv.org/pdf/2509.10956.pdf", "abs": "https://arxiv.org/abs/2509.10956", "title": "AI Hasn't Fixed Teamwork, But It Shifted Collaborative Culture: A Longitudinal Study in a Project-Based Software Development Organization (2023-2025)", "authors": ["Qing Xiao", "Xinlan Emily Hu", "Mark E. Whiting", "Arvind Karunakaran", "Hong Shen", "Hancheng Cao"], "categories": ["cs.HC", "cs.CY", "cs.SE"], "comment": "18 pages", "summary": "When AI entered the workplace, many believed it could reshape teamwork as\nprofoundly as it boosted individual productivity. Would AI finally ease the\nlongstanding challenges of team collaboration? Our findings suggested a more\ncomplicated reality. We conducted a longitudinal two-wave interview study\n(2023-2025) with members (N=15) of a project-based software development\norganization to examine the expectations and use of AI in teamwork. In early\n2023, just after the release of ChatGPT, participants envisioned AI as an\nintelligent coordinator that could align projects, track progress, and ease\ninterpersonal frictions. By 2025, however, AI was used mainly to accelerate\nindividual tasks such as coding, writing, and documentation, leaving persistent\ncollaboration issues of performance accountability and fragile communication\nunresolved. Yet AI reshaped collaborative culture: efficiency became a norm,\ntransparency and responsible use became markers of professionalism, and AI was\nincreasingly accepted as part of teamwork.", "AI": {"tldr": "A longitudinal study explores the evolving role of AI in teamwork within a software development organization, revealing that while AI was initially seen as a tool to enhance collaboration, it ultimately reinforced individual productivity without resolving existing collaboration challenges.", "motivation": "To examine the expectations and actual use of AI in teamwork over time, especially in the context of a project-based software development organization.", "method": "Conducted a longitudinal two-wave interview study from 2023 to 2025 with 15 members of a software development organization.", "result": "AI initially envisioned as a collaborative coordinator evolved to mainly support individual tasks, leading to unresolved issues in team collaboration.", "conclusion": "AI reshaped collaborative culture but did not adequately address core collaboration issues; efficiency and transparency became the new norms.", "key_contributions": ["Identified the gap between expectations and actual use of AI in teamwork.", "Documented the evolution of perceptions regarding AI's role in collaboration over two years.", "Highlighted the cultural shift in collaboration practices influenced by AI."], "limitations": "The study is based on a small sample size in a specific industry, which may limit generalizability.", "keywords": ["AI in teamwork", "collaboration", "software development", "organizational culture", "longitudinal study"], "importance_score": 7, "read_time_minutes": 18}}
{"id": "2509.10798", "pdf": "https://arxiv.org/pdf/2509.10798.pdf", "abs": "https://arxiv.org/abs/2509.10798", "title": "Judge Q: Trainable Queries for Optimized Information Retention in KV Cache Eviction", "authors": ["Yijun Liu", "Yixuan Wang", "Yuzhuang Xu", "Shiyu Ji", "Yang Xu", "Qingfu Zhu", "Wanxiang Che"], "categories": ["cs.CL", "cs.AI"], "comment": "preprint", "summary": "Large language models (LLMs) utilize key-value (KV) cache to store historical\ninformation during sequence processing. The size of KV cache grows linearly as\nthe length of the sequence extends, which seriously affects memory usage and\ndecoding efficiency. Current methods for KV cache eviction typically utilize\nthe last window from the pre-filling phase as queries to compute the KV\nimportance scores for eviction. Although this scheme is simple to implement, it\ntends to overly focus on local information, potentially leading to the neglect\nor omission of crucial global information. To mitigate this issue, we propose\nJudge Q, a novel training method which incorporates a soft token list. This\nmethod only tunes the model's embedding layer at a low training cost. By\nconcatenating the soft token list at the end of the input sequence, we train\nthese tokens' attention map to the original input sequence to align with that\nof the actual decoded tokens. In this way, the queries corresponding to the\nsoft tokens can effectively capture global information and better evaluate the\nimportance of the keys and values within the KV cache, thus maintaining\ndecoding quality when KV cache is evicted. Under the same eviction budget, our\nmethod exhibits less performance degradation compared to existing eviction\napproaches. We validate our approach through experiments conducted on models\nsuch as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks\nincluding LongBench, RULER, and Needle-in-a-Haystack. Results indicate an\nimprovement of approximately 1 point on the LongBench and over 3 points on\nRULER. This proposed methodology can be seamlessly integrated into existing\nopen-source models with minimal training overhead, thereby enhancing\nperformance in KV cache eviction scenarios.", "AI": {"tldr": "The paper proposes Judge Q, a novel training method for improving key-value (KV) cache eviction in large language models (LLMs) by incorporating a soft token list to better capture global information.", "motivation": "To address the limitations of existing KV cache eviction methods that focus too much on local information, resulting in performance degradation.", "method": "The proposed method, Judge Q, trains a soft token list to capture global information in conjunction with the KV cache, tuning only the model's embedding layer at a low cost.", "result": "Judge Q shows significantly better performance than existing eviction methods with improved scores on benchmarks like LongBench and RULER under the same eviction budget.", "conclusion": "The methodology enhances KV cache eviction for LLMs while being easily integrable into existing models with minimal overhead.", "key_contributions": ["Introduction of Judge Q for KV cache eviction", "Use of soft tokens for capturing global information", "Demonstrated performance improvement on standard benchmarks"], "limitations": "", "keywords": ["large language models", "KV cache", "eviction", "soft token", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.10957", "pdf": "https://arxiv.org/pdf/2509.10957.pdf", "abs": "https://arxiv.org/abs/2509.10957", "title": "The Digital Landscape of God: Narrative, Visuals and Viewer Engagement of Religious Videos on YouTube", "authors": ["Rongyi Chen", "Ziyan Xin", "Qing Xiao", "Ruiwei Xiao", "Jingjia Xiao", "Bingbing Zhang", "Hong Shen", "Zhicong Lu"], "categories": ["cs.HC"], "comment": "26 pages, 6 figures", "summary": "The digital transformation of religious practice has reshaped how billions of\npeople engage with spiritual content, with video-sharing platforms becoming\ncentral to contemporary religious communication. Yet HCI research lacks\nsystematic understanding of how narrative and visual elements create meaningful\nspiritual experiences and foster viewer engagement. We present a mixed-methods\nstudy of religious videos on YouTube across major religions, developing\ntaxonomies of narrative frameworks, visual elements, and viewer interaction.\nUsing LLM-assisted analysis, we studied relationships between content\ncharacteristics and viewer responses. Religious videos predominantly adopt\nlecture-style formats with authority-based persuasion strategies, using\nsalvation narratives for guidance. All prefer bright lighting, with Buddhism\nfavoring warm tones and prominent symbols, Judaism preferring indoor settings,\nand Hinduism emphasizing sacred objects. We identified differentiated patterns\nof emotional sharing among religious viewers while revealing significant\ncorrelations between content characteristics and engagement, particularly\nregarding AI-generated content. We provide evidence-based guidance for creating\ninclusive and engaging spiritual media.", "AI": {"tldr": "The study explores the impact of narrative and visual elements in religious videos on YouTube, using LLM-assisted analysis to understand viewer engagement and emotional responses.", "motivation": "To systematically understand how narrative and visual elements in YouTube religious videos influence viewer engagement and spiritual experiences, particularly in the context of HCI.", "method": "A mixed-methods approach involving the development of taxonomies of narrative frameworks, visual elements, and viewer interactions, supplemented by LLM-assisted analysis to explore relationships between content characteristics and viewer responses.", "result": "Identified prevalent lecture-style formats in religious videos, correlations between content characteristics and viewer engagement, and differentiated patterns of emotional sharing among viewers of different religions.", "conclusion": "The findings underscore the importance of narrative and visual strategies in creating engaging spiritual media, offering evidence-based guidance for content creators.", "key_contributions": ["Development of taxonomies for narrative frameworks and visual elements in religious videos", "Analysis of viewer engagement characteristics using LLM-assisted methods", "Identification of emotional sharing patterns among viewers of different religions"], "limitations": "", "keywords": ["Human-Computer Interaction", "Religious Media", "Viewer Engagement", "Narrative Frameworks", "Visual Elements"], "importance_score": 4, "read_time_minutes": 26}}
{"id": "2509.10833", "pdf": "https://arxiv.org/pdf/2509.10833.pdf", "abs": "https://arxiv.org/abs/2509.10833", "title": "Towards Automated Error Discovery: A Study in Conversational AI", "authors": ["Dominic Petrak", "Thy Thy Tran", "Iryna Gurevych"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "Accepted to EMNLP 2025 main conference", "summary": "Although LLM-based conversational agents demonstrate strong fluency and\ncoherence, they still produce undesirable behaviors (errors) that are\nchallenging to prevent from reaching users during deployment. Recent research\nleverages large language models (LLMs) to detect errors and guide\nresponse-generation models toward improvement. However, current LLMs struggle\nto identify errors not explicitly specified in their instructions, such as\nthose arising from updates to the response-generation model or shifts in user\nbehavior. In this work, we introduce Automated Error Discovery, a framework for\ndetecting and defining errors in conversational AI, and propose SEEED (Soft\nClustering Extended Encoder-Based Error Detection), as an encoder-based\napproach to its implementation. We enhance the Soft Nearest Neighbor Loss by\namplifying distance weighting for negative samples and introduce Label-Based\nSample Ranking to select highly contrastive examples for better representation\nlearning. SEEED outperforms adapted baselines -- including GPT-4o and Phi-4 --\nacross multiple error-annotated dialogue datasets, improving the accuracy for\ndetecting unknown errors by up to 8 points and demonstrating strong\ngeneralization to unknown intent detection.", "AI": {"tldr": "This paper presents a framework for Automated Error Discovery in conversational AI and introduces SEEED, an encoder-based method that improves the detection of unknown errors in dialogue systems.", "motivation": "The need for effective error detection in LLM-based conversational agents that struggle with identifying unspecified errors during deployment is the primary motivation behind this research.", "method": "The paper proposes Automated Error Discovery and develops SEEED, which enhances error detection through Soft Clustering and extends encoder-based approaches with improved distance weighting and sample ranking techniques.", "result": "SEEED shows significant improvements in detecting unknown errors, outperforming various baseline models, including GPT-4o and Phi-4, with an accuracy increase of up to 8 points across multiple datasets.", "conclusion": "The findings indicate that SEEED effectively enhances the performance of conversational AI in identifying and adapting to unknown user behaviors and model updates.", "key_contributions": ["Introduction of Automated Error Discovery framework", "Development of SEEED for error detection", "Improvements in accuracy on dialogue datasets"], "limitations": "", "keywords": ["Automated Error Discovery", "Error Detection", "Conversational AI"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.10993", "pdf": "https://arxiv.org/pdf/2509.10993.pdf", "abs": "https://arxiv.org/abs/2509.10993", "title": "When Your Boss Is an AI Bot: Exploring Opportunities and Risks of Manager Clone Agents in the Future Workplace", "authors": ["Qing", "Hu", "Qing Xiao", "Hancheng Cao", "Hong Shen"], "categories": ["cs.HC", "cs.CY"], "comment": "18 pages, 2 figures", "summary": "As Generative AI (GenAI) becomes increasingly embedded in the workplace,\nmanagers are beginning to create Manager Clone Agents - AI-powered digital\nsurrogates that are trained on their work communications and decision patterns\nto perform managerial tasks on their behalf. To investigate this emerging\nphenomenon, we conducted six design fiction workshops (n = 23) with managers\nand workers, in which participants co-created speculative scenarios and\ndiscussed how Manager Clone Agents might transform collaborative work. We\nidentified four potential roles that participants envisioned for Manager Clone\nAgents: proxy presence, informational conveyor belt, productivity engine, and\nleadership amplifier, while highlighting concerns spanning individual,\ninterpersonal, and organizational levels. We provide design recommendations\nenvisioned by both parties for integrating Manager Clone Agents responsibly\ninto the future workplace, emphasizing the need to prioritize workers'\nperspectives, strengthen interpersonal bonds, and enable flexible clone\nconfiguration.", "AI": {"tldr": "The paper explores the emerging use of Generative AI as Manager Clone Agents, which are AI-powered surrogates capable of performing managerial tasks by mimicking the communication and decision-making styles of their human counterparts. It discusses roles, implications, and design recommendations based on workshops with managers and workers.", "motivation": "To explore how Generative AI can assist in managerial tasks and its potential to transform collaborative work through Manager Clone Agents.", "method": "Conducted six design fiction workshops with 23 participants, including managers and workers, to co-create scenarios and discuss perspectives on Manager Clone Agents.", "result": "Identified four envisioned roles for Manager Clone Agents: proxy presence, informational conveyor belt, productivity engine, and leadership amplifier, alongside concerns regarding their integration.", "conclusion": "Design recommendations for responsibly integrating Manager Clone Agents emphasize the importance of prioritizing workers' perspectives and fostering interpersonal relationships.", "key_contributions": ["Identified roles for Manager Clone Agents in the workplace", "Provided design recommendations for responsible integration of AI in management", "Highlighted the need for worker-centric perspectives and interpersonal bond enhancement"], "limitations": "", "keywords": ["Generative AI", "Manager Clone Agents", "Human-Computer Interaction", "Design Fiction", "Workplace Collaboration"], "importance_score": 8, "read_time_minutes": 18}}
{"id": "2509.10843", "pdf": "https://arxiv.org/pdf/2509.10843.pdf", "abs": "https://arxiv.org/abs/2509.10843", "title": "Evaluating Large Language Models for Evidence-Based Clinical Question Answering", "authors": ["Can Wang", "Yiqun Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated substantial progress in\nbiomedical and clinical applications, motivating rigorous evaluation of their\nability to answer nuanced, evidence-based questions. We curate a multi-source\nbenchmark drawing from Cochrane systematic reviews and clinical guidelines,\nincluding structured recommendations from the American Heart Association and\nnarrative guidance used by insurers. Using GPT-4o-mini and GPT-5, we observe\nconsistent performance patterns across sources and clinical domains: accuracy\nis highest on structured guideline recommendations (90%) and lower on narrative\nguideline and systematic review questions (60--70%). We also find a strong\ncorrelation between accuracy and the citation count of the underlying\nsystematic reviews, where each doubling of citations is associated with roughly\na 30% increase in the odds of a correct answer. Models show moderate ability to\nreason about evidence quality when contextual information is supplied. When we\nincorporate retrieval-augmented prompting, providing the gold-source abstract\nraises accuracy on previously incorrect items to 0.79; providing top 3 PubMed\nabstracts (ranked by semantic relevance) improves accuracy to 0.23, while\nrandom abstracts reduce accuracy (0.10, within temperature variation). These\neffects are mirrored in GPT-4o-mini, underscoring that source clarity and\ntargeted retrieval -- not just model size -- drive performance. Overall, our\nresults highlight both the promise and current limitations of LLMs for\nevidence-based clinical question answering. Retrieval-augmented prompting\nemerges as a useful strategy to improve factual accuracy and alignment with\nsource evidence, while stratified evaluation by specialty and question type\nremains essential to understand current knowledge access and to contextualize\nmodel performance.", "AI": {"tldr": "This study evaluates the performance of LLMs in answering clinical questions using a multi-source benchmark, highlighting strengths in structured guidelines and the benefits of retrieval-augmented prompting for accuracy improvement.", "motivation": "To rigorously assess the ability of large language models to answer nuanced, evidence-based clinical questions using a diverse set of authoritative sources.", "method": "The authors curate a benchmark from Cochrane systematic reviews, American Heart Association guidelines, and clinical narratives, utilizing models GPT-4o-mini and GPT-5 to analyze their accuracy across different sources and clinical contexts.", "result": "Models achieve the highest accuracy (90%) on structured recommendations, with a correlation between the citation count of systematic reviews impacting answer accuracy. Retrieval-augmented prompting significantly enhances performance, with notable improvements when using relevant PubMed abstracts.", "conclusion": "While LLMs show promise in clinical question answering, their current limitations necessitate careful evaluation by specialty and question type; retrieval-augmented prompting is crucial for improving accuracy and alignment with evidence.", "key_contributions": ["Curated a multi-source benchmark from clinical guidelines and systematic reviews.", "Identified significant accuracy variations depending on guideline structure and quality of citations.", "Demonstrated the effectiveness of retrieval-augmented prompting in improving model accuracy."], "limitations": "Moderate reasoning capability about evidence quality; accuracy varies widely based on document clarity and relevance of retrieved sources.", "keywords": ["Large Language Models", "Clinical Questions", "Retrieval-augmented prompting", "Evidence-based Medicine", "Benchmarking"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.11027", "pdf": "https://arxiv.org/pdf/2509.11027.pdf", "abs": "https://arxiv.org/abs/2509.11027", "title": "Vocabuild: An Accessible Augmented Tangible Interface for Gamified Vocabulary Learning of Constructing Meaning", "authors": ["Siying Hu", "Zhenhao Zhang"], "categories": ["cs.HC"], "comment": null, "summary": "Vocabulary acquisition in early education often relies on rote memorization\nand passive screen-based tools, which can fail to engage students\nkinesthetically and collaboratively. This paper introduces Vocabuild, an\naugmented tangible interface designed to transform vocabulary learning into an\nactive, embodied, and playful experience. The system combines physical letter\nblocks with a projection-augmented surface. As children physically construct\nwords with the blocks, the system provides real-time, dynamic feedback, such as\ndisplaying corresponding images and animations, thus helping them construct\nsemantic meaning. Deployed in a classroom context, our gamified approach\nfosters both individual exploration and peer collaboration. A user study\nconducted with elementary school children demonstrates that our tangible\ninterface leads to higher engagement, increased collaboration, and a more\npositive attitude towards learning compared to traditional methods. Our\ncontributions are twofold: (1) the design and implementation of Vocabuild, a\nprojection-augmented tangible system that transforms vocabulary learning into\nan embodied and collaborative activity; and (2) empirical findings from a\nclassroom study showing that our tangible approach significantly increases\nengagement, peer collaboration, and positive learning attitudes compared to\ntraditional methods.", "AI": {"tldr": "This paper presents Vocabuild, a tangible interface that enhances vocabulary acquisition through kinesthetic and collaborative learning in early education.", "motivation": "To address the shortcomings of traditional vocabulary learning methods that rely on rote memorization and passive tools, leading to low engagement among students.", "method": "Vocabuild combines physical letter blocks with a projection-augmented surface, allowing children to construct words actively. The system provides real-time feedback through images and animations during the learning process.", "result": "User studies with elementary students showed that Vocabuild significantly increased engagement, collaboration amongst peers, and improved attitudes towards learning compared to traditional methods.", "conclusion": "The design and implementation of Vocabuild and empirical findings demonstrate its effectiveness in transforming vocabulary learning into an active and collaborative experience.", "key_contributions": ["Design and implementation of Vocabuild, a projection-augmented tangible system for vocabulary learning.", "Empirical findings from user studies showing improved engagement and collaborative learning in children."], "limitations": "", "keywords": ["vocabulary acquisition", "augmented reality", "tangible interface", "kinesthetic learning", "collaborative learning"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2509.10844", "pdf": "https://arxiv.org/pdf/2509.10844.pdf", "abs": "https://arxiv.org/abs/2509.10844", "title": "GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings", "authors": ["Yixuan Tang", "Yi Yang"], "categories": ["cs.CL"], "comment": "https://github.com/yixuantt/GAPrune", "summary": "Domain-specific embedding models have shown promise for applications that\nrequire specialized semantic understanding, such as coding agents and financial\nretrieval systems, often achieving higher performance gains than general\nmodels. However, state-of-the-art embedding models are typically based on LLMs,\nwhich contain billions of parameters, making deployment challenging in\nresource-constrained environments. Model compression through pruning offers a\npromising solution, but existing pruning methods treat all parameters\nuniformly, failing to distinguish between general semantic representations and\ndomain-specific patterns, leading to suboptimal pruning decisions. Thus, we\npropose GAPrune, a pruning framework that addresses this challenge by\nconsidering both domain importance and preserving general linguistic\nfoundation. Our method uses Fisher Information to measure importance and\ngeneral-domain gradient alignment to assess parameter behavior, then combines\nthese signals using our Domain Alignment Importance (DAI) scoring. Lower DAI\nscores indicate that the parameter is either less important for the domain task\nor creates conflicts between domain and general objectives. Experiments on two\ndomain benchmarks, FinMTEB and ChemTEB, show that GAPrune maintains performance\nwithin 2.5% of dense models in one-shot pruning at 50% sparsity, while\noutperforming all baselines. With retraining in 100 steps, GAPrune achieves\n+4.51% improvement on FinMTEB and +1.73% on ChemTEB, demonstrating that our\npruning strategy not only preserves but enhances domain-specific capabilities.\nOur findings demonstrate that principled pruning strategies can achieve model\ncompression and enhanced domain specialization, providing the research\ncommunity with a new approach for development.", "AI": {"tldr": "GAPrune is a pruning framework that enhances domain-specific embedding models by balancing domain importance with general linguistic knowledge, achieving effective model compression and improved performance.", "motivation": "Existing model pruning methods fail to effectively distinguish between domain-specific patterns and general representations, leading to suboptimal pruning outcomes for specialized embedding models.", "method": "GAPrune utilizes Fisher Information to evaluate parameter importance and general-domain gradient alignment for assessing parameter behavior, combining these metrics through a Domain Alignment Importance (DAI) scoring system.", "result": "GAPrune maintains performance within 2.5% of dense models at 50% sparsity in one-shot pruning, showing noticeable improvements upon retraining: +4.51% on FinMTEB and +1.73% on ChemTEB.", "conclusion": "GAPrune provides a principled approach to model compression that enhances domain-specific capabilities, offering a valuable method for developing specialized models.", "key_contributions": ["Introduction of Domain Alignment Importance (DAI) scoring for parameter importance evaluation.", "Effective pruning strategy that balances domain-specific and general performance.", "Empirical results demonstrating performance retention and improvement on domain benchmarks."], "limitations": "", "keywords": ["model pruning", "embedding models", "domain-specific", "Fisher Information", "domain alignment"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.11052", "pdf": "https://arxiv.org/pdf/2509.11052.pdf", "abs": "https://arxiv.org/abs/2509.11052", "title": "Commenotes: Synthesizing Organic Comments to Support Community-Based Fact-Checking", "authors": ["Shuning Zhang", "Linzhi Wang", "Dai Shi", "Yuwei Chuai", "Jingruo Chen", "Yunyi Chen", "Yifan Wang", "Yating Wang", "Xin Yi", "Hewu Li"], "categories": ["cs.HC"], "comment": null, "summary": "Community-based fact-checking is promising to reduce the spread of misleading\nposts at scale. However, its effectiveness can be undermined by the delays in\nfact-check delivery. Notably, user-initiated organic comments often contain\ndebunking information and have the potential to help mitigate this limitation.\nHere, we investigate the feasibility of synthesizing comments to generate\ntimely high-quality fact-checks. To this end, we analyze over 2.2 million\nreplies on X and introduce Commenotes, a two-phase framework that filters and\nsynthesizes comments to facilitate fact-check delivery. Our framework reveals\nthat fact-checking comments appear early and sufficiently: 99.3\\% of misleading\nposts receive debunking comments within the initial two hours since post\npublication, with synthesized \\textit{commenotes} successfully earning user\ntrust for 85.8\\% of those posts. Additionally, a user study (N=144) found that\nthe synthesized commenotes were often preferred, with the best-performing model\nachieving a 70.1\\% win rate over human notes and being rated as significantly\nmore helpful.", "AI": {"tldr": "This paper presents Commenotes, a framework for synthesizing comments to enhance the delivery of fact-checks, showing high user trust and preference for synthesized over human notes.", "motivation": "To improve the timeliness and effectiveness of community-based fact-checking by leveraging user-generated comments as a resource for debunking information.", "method": "The authors analyze over 2.2 million replies on X, introducing the Commenotes framework, which consists of a two-phase process to filter and synthesize comments that help in fact-check delivery.", "result": "The framework revealed that 99.3% of misleading posts received debunking comments within two hours, and synthesized commenotes earned user trust in 85.8% of cases. A user study indicated a 70.1% win rate for the best model compared to human notes.", "conclusion": "Synthesized comments can effectively complement community-based fact-checking by providing timely and trusted debunking information.", "key_contributions": ["Introduction of the Commenotes framework for synthesizing debunking comments", "Demonstration of high user trust in synthesized comments", "User study showing preference for synthesized over human-generated fact-checking comments"], "limitations": "", "keywords": ["fact-checking", "human-computer interaction", "social media comments"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.10845", "pdf": "https://arxiv.org/pdf/2509.10845.pdf", "abs": "https://arxiv.org/abs/2509.10845", "title": "Text2Sign Diffusion: A Generative Approach for Gloss-Free Sign Language Production", "authors": ["Liqian Feng", "Lintao Wang", "Kun Hu", "Dehui Kong", "Zhiyong Wang"], "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "Sign language production (SLP) aims to translate spoken language sentences\ninto a sequence of pose frames in a sign language, bridging the communication\ngap and promoting digital inclusion for deaf and hard-of-hearing communities.\nExisting methods typically rely on gloss, a symbolic representation of sign\nlanguage words or phrases that serves as an intermediate step in SLP. This\nlimits the flexibility and generalization of SLP, as gloss annotations are\noften unavailable and language-specific. Therefore, we present a novel\ndiffusion-based generative approach - Text2Sign Diffusion (Text2SignDiff) for\ngloss-free SLP. Specifically, a gloss-free latent diffusion model is proposed\nto generate sign language sequences from noisy latent sign codes and spoken\ntext jointly, reducing the potential error accumulation through a\nnon-autoregressive iterative denoising process. We also design a cross-modal\nsigning aligner that learns a shared latent space to bridge visual and textual\ncontent in sign and spoken languages. This alignment supports the conditioned\ndiffusion-based process, enabling more accurate and contextually relevant sign\nlanguage generation without gloss. Extensive experiments on the commonly used\nPHOENIX14T and How2Sign datasets demonstrate the effectiveness of our method,\nachieving the state-of-the-art performance.", "AI": {"tldr": "A novel gloss-free diffusion-based model for generating sign language sequences from spoken text.", "motivation": "To bridge communication gaps for deaf and hard-of-hearing communities by improving sign language production (SLP) without reliance on gloss.", "method": "A gloss-free latent diffusion model generates sign language sequences from noisy latent sign codes and spoken text through a non-autoregressive iterative denoising process, complemented by a cross-modal signing aligner for visual-textual bridge.", "result": "Achieved state-of-the-art performance on the PHOENIX14T and How2Sign datasets, demonstrating effectiveness in generating accurate sign language sequences.", "conclusion": "The proposed Text2Sign Diffusion model enhances the flexibility and generalization of sign language production, allowing for more effective communication.", "key_contributions": ["Introduction of a gloss-free SLP approach using latent diffusion models", "Development of a cross-modal signing aligner for visual and textual content", "Demonstration of state-of-the-art performance on benchmark datasets"], "limitations": "", "keywords": ["sign language", "gloss-free", "latent diffusion", "cross-modal alignment", "state-of-the-art"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.11059", "pdf": "https://arxiv.org/pdf/2509.11059.pdf", "abs": "https://arxiv.org/abs/2509.11059", "title": "Living with Data: Exploring Physicalization Approaches to Sedentary Behavior Intervention for the Elderly", "authors": ["Siying Hu", "Zhenhao Zhang"], "categories": ["cs.HC"], "comment": null, "summary": "Sedentary behavior is a critical health risk for older adults. While digital\ninterventions exist, they often rely on screen-based notifications that feel\nclinical and are easily ignored. This paper presents a Research through Design\ninquiry into data physicalization as a humane alternative. We designed and\ndeployed tangible artifacts that ambiently represent sedentary patterns in\nolder adults' homes. These artifacts transform abstract data into aesthetic,\nevolving forms, becoming part of the domestic landscape. Through a long-term\nin-situ study, our analysis reveals these physicalizations fostered\nself-reflection, family conversations, and prompted reflection on activity. Our\nwork contributes empirical design principles for tangible health interventions\nthat are both evocative and actionable. We demonstrate how qualities like\naesthetic ambiguity and slow revelation can empower older adults, fostering a\nreflective relationship with their wellbeing. We argue this approach signals a\nnecessary shift from merely informing users to enabling them to live with and\nthrough their data.", "AI": {"tldr": "This paper explores the use of data physicalization for promoting physical activity among older adults, presenting tangible artifacts that help visualize sedentary behavior.", "motivation": "The need for more engaging and effective interventions to combat sedentary behavior among older adults, as traditional digital notifications are often ignored.", "method": "A Research through Design inquiry that involved creating and deploying tangible artifacts in older adults' homes to represent sedentary patterns.", "result": "The study showed that these physicalizations encouraged self-reflection, family discussions, and a heightened awareness of physical activity among participants.", "conclusion": "The findings support the potential of data physicalization as a means to transform health interventions into more interactive, reflective, and user-friendly experiences.", "key_contributions": ["Empirical design principles for tangible health interventions", "Demonstration of data physicalization's impact on older adults' health behaviors", "Strategies for enhancing user engagement through aesthetic designs"], "limitations": "", "keywords": ["Data Physicalization", "Sedentary Behavior", "Tangible Interaction", "Health Informatics", "Aging"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.10847", "pdf": "https://arxiv.org/pdf/2509.10847.pdf", "abs": "https://arxiv.org/abs/2509.10847", "title": "A funny companion: Distinct neural responses to perceived AI- versus human- generated humor", "authors": ["Xiaohui Rao", "Hanlin Wu", "Zhenguang G. Cai"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As AI companions become capable of human-like communication, including\ntelling jokes, understanding how people cognitively and emotionally respond to\nAI humor becomes increasingly important. This study used electroencephalography\n(EEG) to compare how people process humor from AI versus human sources.\nBehavioral analysis revealed that participants rated AI and human humor as\ncomparably funny. However, neurophysiological data showed that AI humor\nelicited a smaller N400 effect, suggesting reduced cognitive effort during the\nprocessing of incongruity. This was accompanied by a larger Late Positive\nPotential (LPP), indicating a greater degree of surprise and emotional\nresponse. This enhanced LPP likely stems from the violation of low initial\nexpectations regarding AI's comedic capabilities. Furthermore, a key temporal\ndynamic emerged: human humor showed habituation effects, marked by an\nincreasing N400 and a decreasing LPP over time. In contrast, AI humor\ndemonstrated increasing processing efficiency and emotional reward, with a\ndecreasing N400 and an increasing LPP. This trajectory reveals how the brain\ncan dynamically update its predictive model of AI capabilities. This process of\ncumulative reinforcement challenges \"algorithm aversion\" in humor, as it\ndemonstrates how cognitive adaptation to AI's language patterns can lead to an\nintensified emotional reward. Additionally, participants' social attitudes\ntoward AI modulated these neural responses, with higher perceived AI\ntrustworthiness correlating with enhanced emotional engagement. These findings\nindicate that the brain responds to AI humor with surprisingly positive and\nintense reactions, highlighting humor's potential for fostering genuine\nengagement in human-AI social interaction.", "AI": {"tldr": "This study investigates how people cognitively and emotionally respond to humor from AI versus human sources using EEG data.", "motivation": "With AI companions becoming capable of human-like communication, understanding cognitive and emotional responses to AI humor is crucial for developing engaging interactions.", "method": "The study employed electroencephalography (EEG) to compare the neurophysiological responses of participants when exposed to humor from AI and humans.", "result": "Participants rated AI and human humor similarly funny, but EEG results showed reduced cognitive effort (smaller N400 effect) for AI humor and a greater emotional response (larger LPP), indicating surprise.", "conclusion": "The study suggests that as people adapt to AI humor, their emotional responses may intensify, challenging the notion of 'algorithm aversion' in humor and emphasizing humor's role in human-AI engagement.", "key_contributions": ["Demonstrated differential neurophysiological processing of humor from AI and humans.", "Revealed that cognitive adaptation to AI humor leads to enhanced emotional responses.", "Identified social attitudes toward AI as a modulating factor in humor processing."], "limitations": "", "keywords": ["AI humor", "neurophysiology", "cognitive adaptation", "social interaction", "electroencephalography"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2509.11062", "pdf": "https://arxiv.org/pdf/2509.11062.pdf", "abs": "https://arxiv.org/abs/2509.11062", "title": "Auto-Slides: An Interactive Multi-Agent System for Creating and Customizing Research Presentations", "authors": ["Yuheng Yang", "Wenjia Jiang", "Yang Wang", "Yiwei Wang", "Chi Zhang"], "categories": ["cs.HC"], "comment": "28 pages (main text: 16 pages, appendix: 10 pages), 4 figures", "summary": "The rapid progress of large language models (LLMs) has opened new\nopportunities for education. While learners can interact with academic papers\nthrough LLM-powered dialogue, limitations still exist: absence of structured\norganization and high text reliance can impede systematic understanding and\nengagement with complex concepts. To address these challenges, we propose\nAuto-Slides, an LLM-driven system that converts research papers into\npedagogically structured, multimodal slides (e.g., diagrams and tables).\nDrawing on cognitive science, it creates a presentation-oriented narrative and\nallows iterative refinement via an interactive editor, in order to match\nlearners' knowledge level and goals. Auto-Slides further incorporates\nverification and knowledge retrieval mechanisms to ensure accuracy and\ncontextual completeness. Through extensive user studies, Auto-Slides enhances\nlearners' comprehension and engagement compared to conventional LLM-based\nreading. Our contributions lie in designing a multi-agent framework for\ntransforming academic papers into pedagogically optimized slides and\nintroducing interactive customization for personalized learning.", "AI": {"tldr": "The paper proposes Auto-Slides, a system that converts research papers into structured, multimodal presentations to improve learning engagement and comprehension.", "motivation": "To address limitations in learners' interaction with academic papers due to unstructured information and high reliance on text.", "method": "Development of Auto-Slides, an LLM-driven system that creates pedagogically structured slides with interactive editing capabilities, drawing from cognitive science principles.", "result": "User studies show that Auto-Slides significantly enhances learners' comprehension and engagement compared to traditional LLM-based reading methods.", "conclusion": "Auto-Slides offers a novel multi-agent framework for personalized learning through academic papers and improves educational outcomes by providing structured, multimodal presentations.", "key_contributions": ["Designing a multi-agent framework for transforming academic papers into structured slides", "Introducing interactive customization for personalized learning", "Incorporating verification and knowledge retrieval mechanisms to ensure accuracy and contextual completeness"], "limitations": "", "keywords": ["large language models", "education", "human-computer interaction", "learning technology"], "importance_score": 9, "read_time_minutes": 16}}
{"id": "2509.10852", "pdf": "https://arxiv.org/pdf/2509.10852.pdf", "abs": "https://arxiv.org/abs/2509.10852", "title": "Pre-Storage Reasoning for Episodic Memory: Shifting Inference Burden to Memory for Personalized Dialogue", "authors": ["Sangyeop Kim", "Yohan Lee", "Sanghwa Kim", "Hyunjong Kim", "Sungzoon Cho"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP 2025 (Findings)", "summary": "Effective long-term memory in conversational AI requires synthesizing\ninformation across multiple sessions. However, current systems place excessive\nreasoning burden on response generation, making performance significantly\ndependent on model sizes. We introduce PREMem (Pre-storage Reasoning for\nEpisodic Memory), a novel approach that shifts complex reasoning processes from\ninference to memory construction. PREMem extracts fine-grained memory fragments\ncategorized into factual, experiential, and subjective information; it then\nestablishes explicit relationships between memory items across sessions,\ncapturing evolution patterns like extensions, transformations, and\nimplications. By performing this reasoning during pre-storage rather than when\ngenerating a response, PREMem creates enriched representations while reducing\ncomputational demands during interactions. Experiments show significant\nperformance improvements across all model sizes, with smaller models achieving\nresults comparable to much larger baselines while maintaining effectiveness\neven with constrained token budgets. Code and dataset are available at\nhttps://github.com/sangyeop-kim/PREMem.", "AI": {"tldr": "PREMem introduces a novel method for improving long-term memory in conversational AI by shifting complex reasoning from response generation to memory construction.", "motivation": "Current conversational AI systems struggle with long-term memory due to excessive reasoning during response generation, which affects performance, especially in smaller models.", "method": "PREMem extracts memory fragments categorized as factual, experiential, and subjective, establishing explicit relationships across sessions for improved memory synthesis.", "result": "Experiments demonstrate significant performance improvements, with smaller models achieving results similar to larger models, while effectively managing token constraints.", "conclusion": "By performing reasoning during pre-storage, PREMem enriches memory representations and reduces computational burdens during interactions.", "key_contributions": ["Introduction of PREMem for memory-efficient reasoning", "Categorization of memory fragments into factual, experiential, and subjective", "Performance improvements across various model sizes"], "limitations": "", "keywords": ["Conversational AI", "Long-term memory", "Memory construction", "Machine Learning", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.11098", "pdf": "https://arxiv.org/pdf/2509.11098.pdf", "abs": "https://arxiv.org/abs/2509.11098", "title": "Rethinking User Empowerment in AI Recommender Systems: Designing through Transparency and Control", "authors": ["Mengke Wu", "Weizi Liu", "Yanyun Wang", "Weiyu Ding", "Mike Yao"], "categories": ["cs.HC"], "comment": "28 pages, 8 figures", "summary": "Smart recommendation algorithms have revolutionized content delivery and\nimproved efficiency across various domains. However, concerns about user agency\npersist due to their inherent opacity (information asymmetry) and one-way\ninfluence (power asymmetry). This study introduces a provotype designed to\nenhance user agency by providing actionable transparency and control over data\nmanagement and content delivery. We conducted qualitative interviews with 19\nparticipants to explore their preferences and concerns regarding the features,\nas well as the provotype's impact on users' understanding and trust toward\nrecommender systems. Findings underscore the importance of integrating\ntransparency with control, and reaffirm users' desire for agency and the\nability to actively intervene in personalization. We also discuss insights for\nencouraging adoption and awareness of such agency-enhancing features. Overall,\nthis study contributes novel approaches and applicable insights, laying the\ngroundwork for designing more user-centered recommender systems that foreground\nuser autonomy and fairness in AI-driven content delivery.", "AI": {"tldr": "This study presents a provotype aimed at increasing user agency in recommender systems by integrating transparency and control over data management.", "motivation": "To address user concerns regarding the opacity and one-way influence of smart recommendation algorithms, which can undermine user agency.", "method": "Qualitative interviews with 19 participants were conducted to gather insights about their preferences, concerns, and experiences regarding the provotype.", "result": "The study revealed that users value transparency and control, indicating a strong desire for agency in how recommendations are presented and managed.", "conclusion": "Integrating transparency with control can enhance user trust and understanding of recommender systems, paving the way for more user-centered designs that prioritize autonomy and fairness.", "key_contributions": ["Introduces a provotype for enhancing user agency in recommender systems", "Highlights the importance of transparency and control for users", "Provides insights for designing user-centered AI applications."], "limitations": "", "keywords": ["recommendation systems", "user agency", "transparency", "control", "human-computer interaction"], "importance_score": 8, "read_time_minutes": 28}}
{"id": "2509.10860", "pdf": "https://arxiv.org/pdf/2509.10860.pdf", "abs": "https://arxiv.org/abs/2509.10860", "title": "Quantifier Scope Interpretation in Language Learners and LLMs", "authors": ["Shaohua Fang", "Yue Li", "Yan Cong"], "categories": ["cs.CL"], "comment": null, "summary": "Sentences with multiple quantifiers often lead to interpretive ambiguities,\nwhich can vary across languages. This study adopts a cross-linguistic approach\nto examine how large language models (LLMs) handle quantifier scope\ninterpretation in English and Chinese, using probabilities to assess\ninterpretive likelihood. Human similarity (HS) scores were used to quantify the\nextent to which LLMs emulate human performance across language groups. Results\nreveal that most LLMs prefer the surface scope interpretations, aligning with\nhuman tendencies, while only some differentiate between English and Chinese in\nthe inverse scope preferences, reflecting human-similar patterns. HS scores\nhighlight variability in LLMs' approximation of human behavior, but their\noverall potential to align with humans is notable. Differences in model\narchitecture, scale, and particularly models' pre-training data language\nbackground, significantly influence how closely LLMs approximate human\nquantifier scope interpretations.", "AI": {"tldr": "This study investigates how large language models interpret quantifier scope in English and Chinese, revealing similarities and differences in their performance compared to humans.", "motivation": "To examine interpretative ambiguities in sentences with multiple quantifiers across languages, focusing on how LLMs handle these interpretations.", "method": "A cross-linguistic approach was adopted, using probabilities to assess interpretive likelihood and human similarity (HS) scores to evaluate LLMs' emulation of human performance.", "result": "Most LLMs prefer surface scope interpretations and demonstrate alignment with human tendencies, though some distinguish between English and Chinese in inverse scope preferences.", "conclusion": "LLMs have the potential to approximate human quantifier scope interpretation, significantly influenced by their architecture, scale, and pre-training data language background.", "key_contributions": ["Cross-linguistic analysis of LLMs' interpretive behaviors", "Evaluation of human similarity scores for quantifier interpretation", "Insights on the impact of training data language background on LLM performance"], "limitations": "Variability in LLMs' performance; not all models differentiate adequately between language preferences.", "keywords": ["quantifier scope", "large language models", "human behavior", "cross-linguistic", "interpretation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.11115", "pdf": "https://arxiv.org/pdf/2509.11115.pdf", "abs": "https://arxiv.org/abs/2509.11115", "title": "\"Pragmatic Tools or Empowering Friends?\" Discovering and Co-Designing Personality-Aligned AI Writing Companions", "authors": ["Mengke Wu", "Kexin Quan", "Weizi Liu", "Mike Yao", "Jessie Chin"], "categories": ["cs.HC"], "comment": "31 pages, 10 figures", "summary": "The growing popularity of AI writing assistants presents exciting\nopportunities to craft tools that cater to diverse user needs. This study\nexplores how personality shapes preferences for AI writing companions and how\npersonalized designs can enhance human-AI teaming. In an exploratory co-design\nworkshop, we worked with 24 writers with different profiles to surface ideas\nand map the design space for personality-aligned AI writing companions,\nfocusing on functionality, interaction dynamics, and visual representations.\nBuilding on these insights, we developed two contrasting prototypes tailored to\ndistinct writer profiles and engaged 8 participants with them as provocations\nto spark reflection and feedback. The results revealed strong connections\nbetween writer profiles and feature preferences, providing proof-of-concept for\npersonality-driven divergence in AI writing support. This research highlights\nthe critical role of team match in human-AI collaboration and underscores the\nimportance of aligning AI systems with individual cognitive needs to improve\nuser engagement and collaboration productivity.", "AI": {"tldr": "This study investigates how personality influences preferences for AI writing assistants and proposes personalized designs for enhanced human-AI collaboration.", "motivation": "The research targets the increasing demand for AI writing assistants that address varied user needs, aiming to improve human-AI collaboration.", "method": "An exploratory co-design workshop with 24 diverse writers was conducted to identify design ideas for personality-aligned AI companions, followed by the development of two prototypes based on this feedback, tested with 8 participants.", "result": "The study found significant links between writer profiles and their feature preferences, supporting the concept of personality-driven AI writing tools.", "conclusion": "Aligning AI systems with individual cognitive needs is crucial for enhancing user engagement and productivity in human-AI collaboration.", "key_contributions": ["Explored the impact of personality on AI writing assistant preferences.", "Developed tailored prototypes based on user feedback.", "Demonstrated the importance of aligning AI systems with user profiles."], "limitations": "", "keywords": ["AI writing assistants", "human-AI collaboration", "personality alignment"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.10882", "pdf": "https://arxiv.org/pdf/2509.10882.pdf", "abs": "https://arxiv.org/abs/2509.10882", "title": "Term2Note: Synthesising Differentially Private Clinical Notes from Medical Terms", "authors": ["Yuping Wu", "Viktor Schlegel", "Warren Del-Pinto", "Srinivasan Nandakumar", "Iqra Zahid", "Yidan Sun", "Usama Farghaly Omar", "Amirah Jasmine", "Arun-Kumar Kaliya-Perumal", "Chun Shen Tham", "Gabriel Connors", "Anil A Bharath", "Goran Nenadic"], "categories": ["cs.CL"], "comment": null, "summary": "Training data is fundamental to the success of modern machine learning\nmodels, yet in high-stakes domains such as healthcare, the use of real-world\ntraining data is severely constrained by concerns over privacy leakage. A\npromising solution to this challenge is the use of differentially private (DP)\nsynthetic data, which offers formal privacy guarantees while maintaining data\nutility. However, striking the right balance between privacy protection and\nutility remains challenging in clinical note synthesis, given its domain\nspecificity and the complexity of long-form text generation. In this paper, we\npresent Term2Note, a methodology to synthesise long clinical notes under strong\nDP constraints. By structurally separating content and form, Term2Note\ngenerates section-wise note content conditioned on DP medical terms, with each\ngoverned by separate DP constraints. A DP quality maximiser further enhances\nsynthetic notes by selecting high-quality outputs. Experimental results show\nthat Term2Note produces synthetic notes with statistical properties closely\naligned with real clinical notes, demonstrating strong fidelity. In addition,\nmulti-label classification models trained on these synthetic notes perform\ncomparably to those trained on real data, confirming their high utility.\nCompared to existing DP text generation baselines, Term2Note achieves\nsubstantial improvements in both fidelity and utility while operating under\nfewer assumptions, suggesting its potential as a viable privacy-preserving\nalternative to using sensitive clinical notes.", "AI": {"tldr": "Term2Note synthesizes clinical notes under differential privacy, balancing privacy and utility.", "motivation": "To address privacy concerns in using real-world training data in healthcare machine learning.", "method": "Term2Note generates long clinical notes by separating content and form, conditioning on DP medical terms with individual DP constraints.", "result": "The synthetic notes produced align closely with real clinical notes in statistical properties and support high-performing classification models.", "conclusion": "Term2Note presents a viable privacy-preserving alternative to sensitive clinical notes, showing significant improvements in fidelity and utility.", "key_contributions": ["Development of Term2Note methodology for synthesizing clinical notes under strong DP constraints.", "Demonstrated strong fidelity and utility of synthetic notes through experimental results.", "Achieved significant improvements over existing DP text generation methods."], "limitations": "", "keywords": ["differential privacy", "synthetic data", "clinical notes", "machine learning", "healthcare"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.11206", "pdf": "https://arxiv.org/pdf/2509.11206.pdf", "abs": "https://arxiv.org/abs/2509.11206", "title": "Evalet: Evaluating Large Language Models by Fragmenting Outputs into Functions", "authors": ["Tae Soo Kim", "Heechan Lee", "Yoonjoo Lee", "Joseph Seering", "Juho Kim"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Practitioners increasingly rely on Large Language Models (LLMs) to evaluate\ngenerative AI outputs through \"LLM-as-a-Judge\" approaches. However, these\nmethods produce holistic scores that obscure which specific elements influenced\nthe assessments. We propose functional fragmentation, a method that dissects\neach output into key fragments and interprets the rhetoric functions that each\nfragment serves relative to evaluation criteria -- surfacing the elements of\ninterest and revealing how they fulfill or hinder user goals. We instantiate\nthis approach in Evalet, an interactive system that visualizes fragment-level\nfunctions across many outputs to support inspection, rating, and comparison of\nevaluations. A user study (N=10) found that, while practitioners struggled to\nvalidate holistic scores, our approach helped them identify 48% more evaluation\nmisalignments. This helped them calibrate trust in LLM evaluations and rely on\nthem to find more actionable issues in model outputs. Our work shifts LLM\nevaluation from quantitative scores toward qualitative, fine-grained analysis\nof model behavior.", "AI": {"tldr": "The paper presents a method called functional fragmentation to enhance the evaluation of generative AI outputs using LLMs by providing detailed insights into fragment-level functions rather than relying on holistic scores.", "motivation": "The reliance on LLMs for judging generative AI outputs often leads to misleading holistic scores that conceal critical details influencing assessments.", "method": "The proposed method, functional fragmentation, dissects outputs into key fragments and interprets their rhetorical functions relative to evaluation criteria, enabling better inspection of quality.", "result": "The user study revealed that practitioners identified 48% more evaluation misalignments using the functional fragmentation approach, indicating improved trust and actionable insights in LLM evaluations.", "conclusion": "Shifting the focus from quantitative holistic scores to qualitative fragment-level analysis can enhance understanding and trust in generative AI evaluations.", "key_contributions": ["Introduction of functional fragmentation for evaluating generative AI outputs", "Creation of Evalet, an interactive visualization system for fragment-level assessments", "Empirical evidence showing improved identification of evaluation misalignments"], "limitations": "", "keywords": ["Large Language Models", "generative AI evaluation", "functional fragmentation", "user study", "visualization system"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.10886", "pdf": "https://arxiv.org/pdf/2509.10886.pdf", "abs": "https://arxiv.org/abs/2509.10886", "title": "CultureSynth: A Hierarchical Taxonomy-Guided and Retrieval-Augmented Framework for Cultural Question-Answer Synthesis", "authors": ["Xinyu Zhang", "Pei Zhang", "Shuang Luo", "Jialong Tang", "Yu Wan", "Baosong Yang", "Fei Huang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted as a Findings paper at EMNLP 2025", "summary": "Cultural competence, defined as the ability to understand and adapt to\nmulticultural contexts, is increasingly vital for large language models (LLMs)\nin global environments. While several cultural benchmarks exist to assess LLMs'\ncultural competence, current evaluations suffer from fragmented taxonomies,\ndomain specificity, and heavy reliance on manual data annotation. To address\nthese limitations, we introduce CultureSynth, a novel framework comprising (1)\na comprehensive hierarchical multilingual cultural taxonomy covering 12 primary\nand 130 secondary topics, and (2) a Retrieval-Augmented Generation (RAG)-based\nmethodology leveraging factual knowledge to synthesize culturally relevant\nquestion-answer pairs. The CultureSynth-7 synthetic benchmark contains 19,360\nentries and 4,149 manually verified entries across 7 languages. Evaluation of\n14 prevalent LLMs of different sizes reveals clear performance stratification\nled by ChatGPT-4o-Latest and Qwen2.5-72B-Instruct. The results demonstrate that\na 3B-parameter threshold is necessary for achieving basic cultural competence,\nmodels display varying architectural biases in knowledge processing, and\nsignificant geographic disparities exist across models. We believe that\nCultureSynth offers a scalable framework for developing culturally aware AI\nsystems while reducing reliance on manual annotation\\footnote{Benchmark is\navailable at https://github.com/Eyr3/CultureSynth.}.", "AI": {"tldr": "CultureSynth introduces a framework for assessing and enhancing the cultural competence of large language models (LLMs) through a hierarchical taxonomy and RAG-based synthesis of culturally relevant content.", "motivation": "The paper addresses the need for better evaluation of LLMs' cultural competence, which is essential in multicultural contexts and is currently hampered by fragmented taxonomies and manual annotation.", "method": "The proposed CultureSynth framework includes a comprehensive multilingual cultural taxonomy and a Retrieval-Augmented Generation methodology to create culturally relevant question-answer pairs.", "result": "The CultureSynth-7 benchmark contains nearly 20,000 entries and shows that a model size of at least 3 billion parameters is necessary for basic cultural competence, revealing varying performance across different LLMs and significant geographic disparities.", "conclusion": "CultureSynth provides a scalable solution for creating culturally aware AI systems while minimizing dependence on manual data annotation.", "key_contributions": ["Introduction of a hierarchical multilingual cultural taxonomy covering 12 primary topics.", "Development of a RAG-based methodology for synthesizing question-answer pairs relevant to cultural contexts.", "Creation of the CultureSynth-7 benchmark with 19,360 entries to assess LLM cultural competence."], "limitations": "", "keywords": ["Cultural competence", "Large language models", "Multilingual taxonomy", "Retrieval-Augmented Generation", "Culturally relevant AI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.11342", "pdf": "https://arxiv.org/pdf/2509.11342.pdf", "abs": "https://arxiv.org/abs/2509.11342", "title": "What if Virtual Agents Had Scents? Users' Judgments of Virtual Agent Personality and Appeals in Encounters", "authors": ["Dongyun Han", "Siyeon Bak", "So-Hui Kim", "Kangsoo Kim", "Sun-Jeong Kim", "Isaac Cho"], "categories": ["cs.HC"], "comment": null, "summary": "Incorporating multi-sensory cues into Virtual Reality (VR) can significantly\nenhance user experiences, mirroring the multi-sensory interactions we encounter\nin the real-world. Olfaction plays a crucial role in shaping impressions when\nengaging with others. This study examines how non-verbal cues from virtual\nagents-specifically olfactory cues, emotional expressions, and gender-influence\nuser perceptions during encounters with virtual agents. Our findings indicate\nthat in unscented, woodsy, and floral scent conditions, participants primarily\nrelied on visually observable cues to form their impressions of virtual agents.\nPositive emotional expressions, conveyed through facial expressions and\ngestures, contributed to more favorable impressions, with this effect being\nstronger for the female agent than the male agent. However, in the unpleasant\nscent condition, participants consistently formed negative impressions, which\noverpowered the influence of emotional expressions and gender, suggesting that\naversive olfactory stimuli can detrimentally impact user perceptions. Our\nresults emphasize the importance of carefully selecting olfactory stimuli when\ndesigning immersive and engaging VR interactions. Finally, we present our\nfindings and outline future research directions for effectively integrating\nolfactory cues into virtual agents.", "AI": {"tldr": "This study investigates the impact of olfactory cues on user perceptions during interactions with virtual agents in VR, highlighting the significant role of non-verbal cues in shaping user experiences.", "motivation": "To explore how olfactory cues, emotional expressions, and gender of virtual agents influence user perceptions in Virtual Reality environments.", "method": "Participants were exposed to virtual agents in various olfactory conditions (unscented, woodsy, floral, and unpleasant) while assessing their impressions based on visual and emotional cues.", "result": "Participants favored positive emotional expressions but were negatively influenced by unpleasant scents, with the gender of the agent affecting impressions significantly.", "conclusion": "The findings emphasize the critical role of olfactory stimuli in immersive VR experiences and suggest careful selection of such cues in virtual interactions.", "key_contributions": ["Investigated the role of olfactory cues alongside visual and emotional factors in VR interactions.", "Demonstrated gender differences in user perceptions of virtual agents influenced by emotional expressions.", "Outlined future research directions for integrating olfactory stimuli in VR."], "limitations": "", "keywords": ["Virtual Reality", "Olfactory Cues", "Human-Agent Interaction", "Emotional Expressions", "User Perceptions"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.10922", "pdf": "https://arxiv.org/pdf/2509.10922.pdf", "abs": "https://arxiv.org/abs/2509.10922", "title": "Aligning ESG Controversy Data with International Guidelines through Semi-Automatic Ontology Construction", "authors": ["Tsuyoshi Iwata", "Guillaume Comte", "Melissa Flores", "Ryoma Kondo", "Ryohei Hisano"], "categories": ["cs.CL", "cs.CY"], "comment": "Author accepted manuscript. This paper has been accepted for\n  presentation at the ISWC 2025 Posters & Demos Track. License details will be\n  updated once the official proceedings are published", "summary": "The growing importance of environmental, social, and governance data in\nregulatory and investment contexts has increased the need for accurate,\ninterpretable, and internationally aligned representations of non-financial\nrisks, particularly those reported in unstructured news sources. However,\naligning such controversy-related data with principle-based normative\nframeworks, such as the United Nations Global Compact or Sustainable\nDevelopment Goals, presents significant challenges. These frameworks are\ntypically expressed in abstract language, lack standardized taxonomies, and\ndiffer from the proprietary classification systems used by commercial data\nproviders. In this paper, we present a semi-automatic method for constructing\nstructured knowledge representations of environmental, social, and governance\nevents reported in the news. Our approach uses lightweight ontology design,\nformal pattern modeling, and large language models to convert normative\nprinciples into reusable templates expressed in the Resource Description\nFramework. These templates are used to extract relevant information from news\ncontent and populate a structured knowledge graph that links reported incidents\nto specific framework principles. The result is a scalable and transparent\nframework for identifying and interpreting non-compliance with international\nsustainability guidelines.", "AI": {"tldr": "This paper presents a method for creating structured knowledge representations of non-financial risks from unstructured news data, aimed at aligning with sustainable frameworks.", "motivation": "The need for accurate and interpretable ESG data in regulatory and investment contexts is increasing, yet aligning this data with normative frameworks presents significant challenges.", "method": "A semi-automatic approach using lightweight ontology design, formal pattern modeling, and large language models to create templates in the Resource Description Framework for extracting relevant information from news.", "result": "The proposed method creates a structured knowledge graph that links reported incidents to specific sustainability principles, enabling scalable identification of non-compliance with guidelines.", "conclusion": "The framework offers a transparent method for interpreting ESG events and their alignment with international sustainability guidelines.", "key_contributions": ["Development of a semi-automatic method for ESG data representation", "Creation of reusable templates for knowledge extraction", "Construction of a structured knowledge graph linking news to sustainability principles"], "limitations": "", "keywords": ["Environmental Social Governance", "Knowledge Graph", "Large Language Models"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.11347", "pdf": "https://arxiv.org/pdf/2509.11347.pdf", "abs": "https://arxiv.org/abs/2509.11347", "title": "Beyond the Portal: Enhancing Recognition in Virtual Reality Through Multisensory Cues", "authors": ["Siyeon Bak", "Dongyun Han", "Inho Jo", "Sun-Jeong Kim", "Isaac Cho"], "categories": ["cs.HC"], "comment": null, "summary": "While Virtual Reality (VR) systems have become increasingly immersive, they\nstill rely predominantly on visual input, which can constrain perceptual\nperformance when visual information is limited. Incorporating additional\nsensory modalities, such as sound and scent, offers a promising strategy to\nenhance user experience and overcome these limitations. This paper investigates\nthe contribution of auditory and olfactory cues in supporting perception within\nthe portal metaphor, a VR technique that reveals remote environments through\nnarrow, visually constrained transitions. We conducted a user study in which\nparticipants identified target scenes by selecting the correct portal among\nalternatives under varying sensory conditions. The results demonstrate that\nintegrating visual, auditory, and olfactory cues significantly improved both\nrecognition accuracy and response time. These findings highlight the potential\nof multisensory integration to compensate for visual constraints in VR and\nemphasize the value of incorporating sound and scent to enhance perception,\nimmersion, and interaction within future VR system designs.", "AI": {"tldr": "This paper explores the role of auditory and olfactory cues in enhancing user perception in Virtual Reality (VR), demonstrating that multisensory integration significantly improves recognition accuracy and response time.", "motivation": "To address the limitations of visual input in VR systems by incorporating additional sensory modalities.", "method": "A user study was conducted where participants identified target scenes by selecting the correct portal among alternatives under varying sensory conditions.", "result": "The study found that integrating visual, auditory, and olfactory cues improved both recognition accuracy and response time.", "conclusion": "Multisensory integration can enhance perception and interaction in VR, suggesting that sound and scent should be included in future designs of VR systems.", "key_contributions": ["Investigation of auditory and olfactory cues in VR", "Demonstration of improved recognition accuracy", "Highlighting the importance of multisensory integration"], "limitations": "", "keywords": ["Virtual Reality", "multisensory integration", "auditory cues", "olfactory cues", "user study"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2509.10935", "pdf": "https://arxiv.org/pdf/2509.10935.pdf", "abs": "https://arxiv.org/abs/2509.10935", "title": "Introducing Spotlight: A Novel Approach for Generating Captivating Key Information from Documents", "authors": ["Ankan Mullick", "Sombit Bose", "Rounak Saha", "Ayan Kumar Bhowmick", "Aditya Vempaty", "Prasenjit Dey", "Ravi Kokku", "Pawan Goyal", "Niloy Ganguly"], "categories": ["cs.CL"], "comment": "Paper accepted in EMNLP 2025 Main Conference (Full)", "summary": "In this paper, we introduce Spotlight, a novel paradigm for information\nextraction that produces concise, engaging narratives by highlighting the most\ncompelling aspects of a document. Unlike traditional summaries, which\nprioritize comprehensive coverage, spotlights selectively emphasize intriguing\ncontent to foster deeper reader engagement with the source material. We\nformally differentiate spotlights from related constructs and support our\nanalysis with a detailed benchmarking study using new datasets curated for this\nwork. To generate high-quality spotlights, we propose a two-stage approach:\nfine-tuning a large language model on our benchmark data, followed by alignment\nvia Direct Preference Optimization (DPO). Our comprehensive evaluation\ndemonstrates that the resulting model not only identifies key elements with\nprecision but also enhances readability and boosts the engagement value of the\noriginal document.", "AI": {"tldr": "Spotlight proposes a novel method for information extraction that emphasizes engaging narratives over comprehensive summaries, using a two-stage LLM fine-tuning process.", "motivation": "To improve reader engagement with source material through focused narratives rather than broad summaries.", "method": "A two-stage approach involving fine-tuning a large language model on bespoke benchmark data, followed by alignment through Direct Preference Optimization (DPO).", "result": "The model achieves high precision in identifying key elements, improves readability, and enhances document engagement.", "conclusion": "Spotlight offers a promising alternative for information extraction that could redefine how narratives are generated from documents.", "key_contributions": ["Introduction of the spotlight paradigm for narrative extraction", "Development of a tailored benchmarking dataset for evaluation", "Implementation of Direct Preference Optimization for model alignment"], "limitations": "", "keywords": ["information extraction", "narrative generation", "large language models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.11391", "pdf": "https://arxiv.org/pdf/2509.11391.pdf", "abs": "https://arxiv.org/abs/2509.11391", "title": "\"My Boyfriend is AI\": A Computational Analysis of Human-AI Companionship in Reddit's AI Community", "authors": ["Pat Pataranutaporn", "Sheer Karny", "Chayapatr Archiwaranguprok", "Constanze Albrecht", "Auren R. Liu", "Pattie Maes"], "categories": ["cs.HC", "cs.CY"], "comment": "22 pages, 9 figures", "summary": "Human-AI interaction researchers face an overwhelming challenge: synthesizing\ninsights from thousands of empirical studies to understand how AI impacts\npeople and inform effective design. Existing approach for literature reviews\ncluster papers by similarities, keywords or citations, missing the crucial\ncause-and-effect relationships that reveal how design decisions impact user\noutcomes. We introduce the Atlas of Human-AI Interaction, an interactive web\ninterface that provides the first systematic mapping of empirical findings\nacross 1,000+ HCI papers using LLM-powered knowledge extraction. Our approach\nidentifies causal relationships, and visualizes them through an AI-enabled\ninteractive web interface as a navigable knowledge graph. We extracted 2,037\nempirical findings, revealing research topic clusters, common themes, and\ndisconnected areas. Expert evaluation with 20 researchers revealed the system's\neffectiveness for discovering research gaps. This work demonstrates how AI can\ntransform literature synthesis itself, offering a scalable framework for\nevidence-based design, opening new possibilities for computational meta-science\nacross HCI and beyond.", "AI": {"tldr": "The Atlas of Human-AI Interaction provides an interactive web interface that systematically maps empirical findings from over 1,000 HCI studies, using LLM-powered knowledge extraction to identify causal relationships and visualize them as a navigable knowledge graph.", "motivation": "To address the challenge of synthesizing insights from numerous empirical studies on AI's impact on users for informed design decisions.", "method": "An interactive web interface that maps empirical findings using LLM-powered knowledge extraction, visualizing causal relationships through a navigable knowledge graph.", "result": "Identified 2,037 empirical findings and revealed research topic clusters, common themes, and areas needing more exploration; evaluated positively by 20 researchers for its utility in discovering research gaps.", "conclusion": "This work showcases AI's potential for transforming literature synthesis in HCI, promoting a scalable framework for evidence-based design and computational meta-science.", "key_contributions": ["First systematic mapping of empirical findings across 1,000+ HCI papers", "Identification of causal relationships between design decisions and user outcomes", "Development of an AI-enabled interactive web interface for knowledge visualization"], "limitations": "The effectiveness of the framework may vary based on the quality of the extracted empirical findings.", "keywords": ["Human-AI interaction", "literature review", "knowledge extraction", "HCI", "interactive web interface"], "importance_score": 9, "read_time_minutes": 22}}
{"id": "2509.10937", "pdf": "https://arxiv.org/pdf/2509.10937.pdf", "abs": "https://arxiv.org/abs/2509.10937", "title": "An Interpretable Benchmark for Clickbait Detection and Tactic Attribution", "authors": ["Lihi Nofar", "Tomer Portal", "Aviv Elbaz", "Alexander Apartsin", "Yehudit Aperstein"], "categories": ["cs.CL"], "comment": "7 pages", "summary": "The proliferation of clickbait headlines poses significant challenges to the\ncredibility of information and user trust in digital media. While recent\nadvances in machine learning have improved the detection of manipulative\ncontent, the lack of explainability limits their practical adoption. This paper\npresents a model for explainable clickbait detection that not only identifies\nclickbait titles but also attributes them to specific linguistic manipulation\nstrategies. We introduce a synthetic dataset generated by systematically\naugmenting real news headlines using a predefined catalogue of clickbait\nstrategies. This dataset enables controlled experimentation and detailed\nanalysis of model behaviour. We present a two-stage framework for automatic\nclickbait analysis comprising detection and tactic attribution. In the first\nstage, we compare a fine-tuned BERT classifier with large language models\n(LLMs), specifically GPT-4.0 and Gemini 2.4 Flash, under both zero-shot\nprompting and few-shot prompting enriched with illustrative clickbait headlines\nand their associated persuasive tactics. In the second stage, a dedicated\nBERT-based classifier predicts the specific clickbait strategies present in\neach headline. This work advances the development of transparent and\ntrustworthy AI systems for combating manipulative media content. We share the\ndataset with the research community at\nhttps://github.com/LLM-HITCS25S/ClickbaitTacticsDetection", "AI": {"tldr": "This paper proposes an explainable model for detecting clickbait headlines, leveraging a synthetic dataset and a two-stage framework for analysis.", "motivation": "The increase in clickbait headlines negatively impacts information credibility and user trust in digital media.", "method": "The study employs a two-stage framework for automatic clickbait analysis: detection using a fine-tuned BERT classifier and LLMs, and tactic attribution with a BERT-based classifier.", "result": "The model successfully identifies clickbait titles and associates them with specific linguistic strategies, demonstrating improved explainability in clickbait detection.", "conclusion": "This research contributes to the development of transparent AI systems capable of addressing manipulative media content, fostering trust in digital information.", "key_contributions": ["Introduction of a synthetic dataset for clickbait detection", "Development of a two-stage framework for analysis", "Enhanced explainability in machine learning models for detecting clickbait"], "limitations": "", "keywords": ["clickbait detection", "explainable AI", "machine learning", "natural language processing", "digital media"], "importance_score": 7, "read_time_minutes": 7}}
{"id": "2509.11401", "pdf": "https://arxiv.org/pdf/2509.11401.pdf", "abs": "https://arxiv.org/abs/2509.11401", "title": "Small Cues, Big Differences: Evaluating Interaction and Presentation for Annotation Retrieval in AR", "authors": ["Zahra Borhani", "Ali Ebrahimpour-Boroojeny", "Francisco R. Ortega"], "categories": ["cs.HC"], "comment": null, "summary": "Augmented Reality (AR) enables intuitive interaction with virtual annotations\noverlaid on the real world, supporting a wide range of applications such as\nremote assistance, education, and industrial training. However, as the number\nof heterogeneous annotations increases, their efficient retrieval remains an\nopen challenge in 3D environments. This paper examines how interaction\nmodalities and presentation designs affect user performance, workload, fatigue,\nand preference in AR annotation retrieval. In two user studies, we compare\neye-gaze versus hand-ray hovering and evaluate four presentation methods:\nOpacity-based, Scale-based, Nothing-based, and Marker-based. Results show that\neye-gaze was favored over hand-ray by users, despite leading to significantly\nhigher unintentional activations. Among the presentation methods, Scale-based\npresentation reduces workload and task completion time while aligning with user\npreferences. Our findings offer empirical insights into the effectiveness of\ndifferent annotation presentation methods, leading to design recommendations\nfor building more efficient and user-friendly AR annotation review systems.", "AI": {"tldr": "This paper investigates interaction modalities and presentation designs for AR annotation retrieval, revealing preferences for eye-gaze interactions and Scale-based presentation.", "motivation": "To address the challenge of efficiently retrieving heterogeneous annotations in Augmented Reality (AR) environments.", "method": "Two user studies comparing eye-gaze and hand-ray hovering methods, alongside evaluations of four presentation methods: Opacity-based, Scale-based, Nothing-based, and Marker-based.", "result": "Users preferred eye-gaze over hand-ray despite higher unintentional activations; Scale-based presentation reduced workload and task completion time while aligning with user preferences.", "conclusion": "The study provides empirical insights and design recommendations for user-friendly AR annotation retrieval systems.", "key_contributions": ["Comparison of eye-gaze and hand-ray interaction modalities in AR", "Evaluation of multiple presentation methods for AR annotations", "Recommendations for optimal design in AR annotation systems"], "limitations": "", "keywords": ["Augmented Reality", "Annotation Retrieval", "User Interaction", "Presentation Design", "User Studies"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.11101", "pdf": "https://arxiv.org/pdf/2509.11101.pdf", "abs": "https://arxiv.org/abs/2509.11101", "title": "EmoBench-Reddit: A Hierarchical Benchmark for Evaluating the Emotional Intelligence of Multimodal Large Language Models", "authors": ["Haokun Li", "Yazhou Zhang", "Jizhi Ding", "Qiuchi Li", "Peng Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "With the rapid advancement of Multimodal Large Language Models (MLLMs), they\nhave demonstrated exceptional capabilities across a variety of vision-language\ntasks. However, current evaluation benchmarks predominantly focus on objective\nvisual question answering or captioning, inadequately assessing the models'\nability to understand complex and subjective human emotions. To bridge this\ngap, we introduce EmoBench-Reddit, a novel, hierarchical benchmark for\nmultimodal emotion understanding. The dataset comprises 350 meticulously\ncurated samples from the social media platform Reddit, each containing an\nimage, associated user-provided text, and an emotion category (sad, humor,\nsarcasm, happy) confirmed by user flairs. We designed a hierarchical task\nframework that progresses from basic perception to advanced cognition, with\neach data point featuring six multiple-choice questions and one open-ended\nquestion of increasing difficulty. Perception tasks evaluate the model's\nability to identify basic visual elements (e.g., colors, objects), while\ncognition tasks require scene reasoning, intent understanding, and deep empathy\nintegrating textual context. We ensured annotation quality through a\ncombination of AI assistance (Claude 4) and manual verification.", "AI": {"tldr": "Introducing EmoBench-Reddit, a benchmark for assessing multimodal emotion understanding using images and social media text.", "motivation": "Current evaluation benchmarks for MLLMs inadequately assess their understanding of complex human emotions; thus, EmoBench-Reddit aims to fill this gap.", "method": "EmoBench-Reddit includes 350 curated samples from Reddit, featuring images, text, and emotion categories. The hierarchical framework involves basic perception and advanced cognition tasks with multiple-choice and open-ended questions.", "result": "The benchmark allows for a more comprehensive evaluation of MLLMs in understanding human emotions and improves how models process and interpret multimodal content in nuanced contexts.", "conclusion": "By combining AI assistance and manual verification, EmoBench-Reddit enhances the reliability of assessments in emotion understanding for MLLMs, paving the way for better applications in user interaction.", "key_contributions": ["Introduction of a novel benchmark for multimodal emotion understanding", "Hierarchical task framework for progressive evaluation", "Combination of AI and manual verification for data quality assurance"], "limitations": "", "keywords": ["Multimodal Large Language Models", "Emotion understanding", "Benchmark", "Human emotions", "Hierarchical framework"], "importance_score": 9, "read_time_minutes": 6}}
{"id": "2509.11438", "pdf": "https://arxiv.org/pdf/2509.11438.pdf", "abs": "https://arxiv.org/abs/2509.11438", "title": "Generative AI-Enabled Adaptive Learning Platform: How I Can Help You Pass Your Driving Test?", "authors": ["Riya Gill", "Ievgeniia Kuzminykh", "Maher Salem", "Bogdan Ghita"], "categories": ["cs.HC"], "comment": "17 pages, 8 tables, 3 figures, submitted to the Artificial\n  Intelligence in Education journal", "summary": "This study aims to develop an adaptive learning platform that leverages\ngenerative AI to automate assessment creation and feedback delivery. The\nplatform provides self-correcting tests and personalised feedback that adapts\nto each learners progress and history, ensuring a tailored learning experience.\nThe study involves the development and evaluation of a web-based application\nfor revision for the UK Driving Theory Test. The platform generates dynamic,\nnon-repetitive question sets and offers adaptive feedback based on user\nperformance over time. The effectiveness of AI-generated assessments and\nfeedback is evaluated through expert review and model analysis. The results\nshow the successful generation of relevant and accurate questions, alongside\npositive and helpful feedback. The personalised test generation closely aligns\nwith expert-created assessments, demonstrating the reliability of the system.\nThese findings suggest that generative AI can enhance learning outcomes by\nadapting to individual student needs and offering tailored support. This\nresearch introduces an AI-powered assessment and feedback system that goes\nbeyond traditional solutions by incorporating automation and adaptive learning.\nThe non-memoryless feedback mechanism ensures that student history and\nperformance inform future assessments, making the learning process more\neffective and individualised. This contrasts with conventional systems that\nprovide static, one-time feedback without considering past progress.", "AI": {"tldr": "The study presents a generative AI-powered adaptive learning platform that automates assessment creation and provides personalized feedback for the UK Driving Theory Test.", "motivation": "To enhance learning outcomes by creating a tailored learning experience through automation and adaptive feedback using generative AI.", "method": "Development and evaluation of a web-based application that generates dynamic question sets and adaptive feedback based on learner's performance and history.", "result": "The system successfully generates relevant questions similar to expert-created assessments and provides positive feedback, demonstrating reliability and effectiveness in learning outcomes.", "conclusion": "Generative AI can significantly improve individualized learning experiences by adapting assessments based on learner history and performance, offering more effective support than traditional systems.", "key_contributions": ["Development of an adaptive learning platform leveraging generative AI.", "Introduction of a non-memoryless feedback mechanism that considers student history.", "Evaluation and validation of AI-generated assessments against expert benchmarks."], "limitations": "", "keywords": ["adaptive learning", "generative AI", "assessment automation", "personalized feedback", "UK Driving Theory Test"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.11106", "pdf": "https://arxiv.org/pdf/2509.11106.pdf", "abs": "https://arxiv.org/abs/2509.11106", "title": "Fluid Language Model Benchmarking", "authors": ["Valentin Hofmann", "David Heineman", "Ian Magnusson", "Kyle Lo", "Jesse Dodge", "Maarten Sap", "Pang Wei Koh", "Chun Wang", "Hannaneh Hajishirzi", "Noah A. Smith"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "COLM 2025", "summary": "Language model (LM) benchmarking faces several challenges: comprehensive\nevaluations are costly, benchmarks often fail to measure the intended\ncapabilities, and evaluation quality can degrade due to labeling errors and\nbenchmark saturation. Although various strategies have been proposed to\nmitigate these issues, they tend to address individual aspects in isolation,\nneglecting broader questions about overall evaluation quality. Here, we\nintroduce Fluid Benchmarking, a new evaluation approach that advances LM\nbenchmarking across multiple dimensions. Inspired by psychometrics, Fluid\nBenchmarking is based on the insight that the relative value of benchmark items\ndepends on an LM's capability level, suggesting that evaluation should adapt to\neach LM. Methodologically, Fluid Benchmarking estimates an item response model\nbased on existing LM evaluation results and uses the inferred quantities to\nselect evaluation items dynamically, similar to computerized adaptive testing\nin education. In our experiments, we compare Fluid Benchmarking against the\ncommon practice of random item sampling as well as more sophisticated\nbaselines, including alternative methods grounded in item response theory. We\nexamine four dimensions -- efficiency, validity, variance, and saturation --\nand find that Fluid Benchmarking achieves superior performance in all of them\n(e.g., higher validity and less variance on MMLU with fifty times fewer items).\nOur analysis shows that the two components of Fluid Benchmarking have distinct\neffects: item response theory, used to map performance into a latent ability\nspace, increases validity, while dynamic item selection reduces variance.\nOverall, our results suggest that LM benchmarking can be substantially improved\nby moving beyond static evaluation.", "AI": {"tldr": "Fluid Benchmarking introduces a new approach to LM benchmarking that adapts evaluations based on the strengths of different language models.", "motivation": "The paper addresses the limitations of existing language model benchmarking methods, which often suffer from high costs, poor quality evaluations, and saturation, highlighting the need for a more holistic evaluation framework.", "method": "Fluid Benchmarking utilizes an item response model that adapts to the capability level of the language model, allowing for dynamic selection of evaluation items similar to computerized adaptive testing.", "result": "Fluid Benchmarking demonstrates superior performance over traditional random sampling and other sophisticated methods across metrics such as efficiency, validity, variance, and saturation, achieving higher validity with far fewer items.", "conclusion": "The findings indicate that language model benchmarking can significantly benefit from dynamic evaluations tailored to model capabilities rather than using static methods.", "key_contributions": ["Introduction of Fluid Benchmarking for LM evaluation", "Demonstration of performance improvements across multiple dimensions", "Combination of item response theory with dynamic item selection for enhanced evaluation"], "limitations": "", "keywords": ["language model", "benchmarking", "adaptive testing", "item response theory", "evaluation quality"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.11461", "pdf": "https://arxiv.org/pdf/2509.11461.pdf", "abs": "https://arxiv.org/abs/2509.11461", "title": "CareerPooler: AI-Powered Metaphorical Pool Simulation Improves Experience and Outcomes in Career Exploration", "authors": ["Ziyi Wang", "Ziwen Zeng", "Yuan Li", "Zijian Ding"], "categories": ["cs.HC", "cs.AI", "H.5"], "comment": null, "summary": "Career exploration is uncertain, requiring decisions with limited information\nand unpredictable outcomes. While generative AI offers new opportunities for\ncareer guidance, most systems rely on linear chat interfaces that produce\noverly comprehensive and idealized suggestions, overlooking the non-linear and\neffortful nature of real-world trajectories. We present CareerPooler, a\ngenerative AI-powered system that employs a pool-table metaphor to simulate\ncareer development as a spatial and narrative interaction. Users strike balls\nrepresenting milestones, skills, and random events, where hints, collisions,\nand rebounds embody decision-making under uncertainty. In a within-subjects\nstudy with 24 participants, CareerPooler significantly improved engagement,\ninformation gain, satisfaction, and career clarity compared to a chatbot\nbaseline. Qualitative findings show that spatial-narrative interaction fosters\nexperience-based learning, resilience through setbacks, and reduced\npsychological burden. Our findings contribute to the design of AI-assisted\ncareer exploration systems and more broadly suggest that visually grounded\nanalogical interactions can make generative systems engaging and satisfying.", "AI": {"tldr": "CareerPooler is a generative AI system that simulates career exploration through a spatial-narrative interaction, improving user engagement and career clarity.", "motivation": "Career exploration often involves uncertainty and limited information, which traditional AI systems fail to address effectively. This paper aims to create a more engaging and realistic approach to career guidance by leveraging generative AI.", "method": "The paper introduces CareerPooler, using a pool-table metaphor that allows users to interact with career milestones and decisions through a spatial and narrative framework. A study with 24 participants was conducted to evaluate its effectiveness.", "result": "CareerPooler demonstrated significantly improved engagement, information gain, satisfaction, and career clarity compared to a standard chatbot interface.", "conclusion": "The study reveals that spatial-narrative interactions enhance experience-based learning and can reduce the psychological burden of career exploration, suggesting new avenues for AI-assisted guidance systems.", "key_contributions": ["Introduction of CareerPooler as a novel AI-powered career exploration tool.", "Implementation of spatial and narrative interactions to simulate career decision-making.", "Demonstration of improved user engagement and satisfaction in a comparative study."], "limitations": "The sample size was limited to 24 participants, which may affect generalizability. Further studies are needed to validate findings across diverse populations.", "keywords": ["Generative AI", "Career exploration", "Spatial interaction", "User engagement", "AI-assisted guidance"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.11118", "pdf": "https://arxiv.org/pdf/2509.11118.pdf", "abs": "https://arxiv.org/abs/2509.11118", "title": "We Argue to Agree: Towards Personality-Driven Argumentation-Based Negotiation Dialogue Systems for Tourism", "authors": ["Priyanshu Priya", "Saurav Dudhate", "Desai Vishesh Yasheshbhai", "Asif Ekbal"], "categories": ["cs.CL", "cs.AI"], "comment": "Paper is accepted at EMNLP (Findings) 2025", "summary": "Integrating argumentation mechanisms into negotiation dialogue systems\nimproves conflict resolution through exchanges of arguments and critiques.\nMoreover, incorporating personality attributes enhances adaptability by\naligning interactions with individuals' preferences and styles. To advance\nthese capabilities in negotiation dialogue systems, we propose a novel\nPersonality-driven Argumentation-based Negotiation Dialogue Generation (PAN-DG)\ntask. To support this task, we introduce PACT, a dataset of Personality-driven\nArgumentation-based negotiation Conversations for Tourism sector. This dataset,\ngenerated using Large Language Models (LLMs), features three distinct\npersonality profiles, viz. Argumentation Profile, Preference Profile, and\nBuying Style Profile to simulate a variety of negotiation scenarios involving\ndiverse personalities. Thorough automatic and manual evaluations indicate that\nthe dataset comprises high-quality dialogues. Further, we conduct comparative\nexperiments between pre-trained and fine-tuned LLMs for the PAN-DG task.\nMulti-dimensional evaluation demonstrates that the fine-tuned LLMs effectively\ngenerate personality-driven rational responses during negotiations. This\nunderscores the effectiveness of PACT in enhancing personalization and\nreasoning capabilities in negotiation dialogue systems, thereby establishing a\nfoundation for future research in this domain.", "AI": {"tldr": "The paper introduces a new task, PAN-DG, focused on generating negotiation dialogues that account for personality attributes, supported by a novel dataset, PACT.", "motivation": "To improve conflict resolution in negotiation dialogue systems by integrating argumentation mechanisms and personality attributes.", "method": "The authors propose a new task, PAN-DG, and introduce the PACT dataset generated using LLMs, which simulates negotiation scenarios with distinct personality profiles.", "result": "Evaluations show that dialogues in the PACT dataset are of high quality, and fine-tuned LLMs outperform pre-trained models in generating personality-driven responses during negotiations.", "conclusion": "PACT significantly enhances personalization and reasoning in negotiation dialogue systems and sets a foundation for future research in this area.", "key_contributions": ["Introduction of the PAN-DG task for personality-driven negotiation dialogue generation.", "Development of the PACT dataset with distinct personality profiles for tourism sector negotiations.", "Demonstration of the effectiveness of fine-tuned LLMs in generating rational responses based on personality attributes."], "limitations": "", "keywords": ["Negotiation", "Argumentation", "Personality", "Dialogue Generation", "Large Language Models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.11478", "pdf": "https://arxiv.org/pdf/2509.11478.pdf", "abs": "https://arxiv.org/abs/2509.11478", "title": "Designing and Evaluating a Conversational Agent for Early Detection of Alzheimer's Disease and Related Dementias", "authors": ["Andrew G. Breithaupt", "Nayoung Choi", "James D. Finch", "Jeanne M. Powell", "Arin L. Nelson", "Oz A. Alon", "Howard J. Rosen", "Jinho D. Choi"], "categories": ["cs.HC", "cs.AI"], "comment": "First two authors contributed equally", "summary": "Early detection of Alzheimer's disease and related dementias (ADRD) is\ncritical for timely intervention, yet most diagnoses are delayed until advanced\nstages. While comprehensive patient narratives are essential for accurate\ndiagnosis, prior work has largely focused on screening studies that classify\ncognitive status from interactions rather than supporting the diagnostic\nprocess. We designed voice-interactive conversational agents, leveraging large\nlanguage models (LLMs), to elicit narratives relevant to ADRD from patients and\ninformants. We evaluated the agent with 30 adults with suspected ADRD through\nconversation analysis (n=30), user surveys (n=19), and clinical validation\nagainst blinded specialist interviews (n=24). Symptoms detected by the agent\naligned well with those identified by specialists across symptoms. Users\nappreciated the agent's patience and systematic questioning, which supported\nengagement and expression of complex, hard-to-describe experiences. This\npreliminary work suggests conversational agents may serve as structured\nfront-end tools for dementia assessment, highlighting interaction design\nconsiderations in sensitive healthcare contexts.", "AI": {"tldr": "The paper presents a study on the use of voice-interactive conversational agents utilizing large language models for early detection of Alzheimer's disease and related dementias (ADRD) through patient narratives.", "motivation": "The study addresses the critical need for early detection of ADRD, improving the diagnostic process by leveraging patient narratives instead of traditional screening methods.", "method": "The research involved developing voice-interactive agents that engage patients in conversation, evaluating their performance through conversation analysis, user surveys, and clinical validation against specialist interviews.", "result": "The evaluation showed that the agent could effectively elicit relevant symptoms, which aligned with specialist diagnoses, and users found the agent helpful and engaging in expressing their experiences.", "conclusion": "Conversational agents may effectively support dementia assessment processes, emphasizing the importance of interaction design in sensitive healthcare scenarios.", "key_contributions": ["Development of voice-interactive agents for ADRD assessment", "Demonstrated alignment between agent-detected symptoms and specialist findings", "User-centered evaluation highlighting patient engagement and narrative expression"], "limitations": "The study is preliminary with a limited sample size, and the effectiveness of the agent in diverse patient populations remains to be validated.", "keywords": ["Alzheimer's disease", "conversational agents", "large language models", "healthcare", "user engagement"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2509.11127", "pdf": "https://arxiv.org/pdf/2509.11127.pdf", "abs": "https://arxiv.org/abs/2509.11127", "title": "Joint Effects of Argumentation Theory, Audio Modality and Data Enrichment on LLM-Based Fallacy Classification", "authors": ["Hongxu Zhou", "Hylke Westerdijk", "Khondoker Ittehadul Islam"], "categories": ["cs.CL"], "comment": null, "summary": "This study investigates how context and emotional tone metadata influence\nlarge language model (LLM) reasoning and performance in fallacy classification\ntasks, particularly within political debate settings. Using data from U.S.\npresidential debates, we classify six fallacy types through various prompting\nstrategies applied to the Qwen-3 (8B) model. We introduce two theoretically\ngrounded Chain-of-Thought frameworks: Pragma-Dialectics and the Periodic Table\nof Arguments, and evaluate their effectiveness against a baseline prompt under\nthree input settings: text-only, text with context, and text with both context\nand audio-based emotional tone metadata. Results suggest that while theoretical\nprompting can improve interpretability and, in some cases, accuracy, the\naddition of context and especially emotional tone metadata often leads to\nlowered performance. Emotional tone metadata biases the model toward labeling\nstatements as \\textit{Appeal to Emotion}, worsening logical reasoning. Overall,\nbasic prompts often outperformed enhanced ones, suggesting that attention\ndilution from added inputs may worsen rather than improve fallacy\nclassification in LLMs.", "AI": {"tldr": "This study explores the impact of context and emotional tone metadata on LLM reasoning in fallacy classification tasks, revealing that basic prompts often outperform enhanced ones due to attention dilution.", "motivation": "To investigate the influence of context and emotional tone metadata on the performance of large language models in classifying fallacies in political debates.", "method": "The study uses U.S. presidential debate data to classify six types of fallacies using prompting strategies applied to the Qwen-3 model. It introduces Chain-of-Thought frameworks like Pragma-Dialectics and the Periodic Table of Arguments and evaluates their effectiveness against baseline prompts.", "result": "The results indicate that theoretical prompting can improve interpretability and accuracy in some cases, but adding context and emotional tone metadata generally reduced performance, biasing the model towards misclassifications such as 'Appeal to Emotion'.", "conclusion": "Overall, basic prompts often yield better performance than enhanced prompts, suggesting that added inputs may distract and dilute attention, leading to poorer classification outcomes in LLMs.", "key_contributions": ["Introduced Chain-of-Thought frameworks for fallacy classification", "Evaluated the effect of emotional tone on LLM performance", "Provided insights into input context impacts on reasoning quality"], "limitations": "Results are specific to the Qwen-3 model and the dataset used, which may limit generalizability.", "keywords": ["large language models", "fallacy classification", "context", "emotional tone", "political debate"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.11487", "pdf": "https://arxiv.org/pdf/2509.11487.pdf", "abs": "https://arxiv.org/abs/2509.11487", "title": "Collective Recourse for Generative Urban Visualizations", "authors": ["Rashid Mushkani"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Text-to-image diffusion models help visualize urban futures but can amplify\ngroup-level harms. We propose collective recourse: structured community \"visual\nbug reports\" that trigger fixes to models and planning workflows. We (1)\nformalize collective recourse and a practical pipeline (report, triage, fix,\nverify, closure); (2) situate four recourse primitives within the diffusion\nstack: counter-prompts, negative prompts, dataset edits, and reward-model\ntweaks; (3) define mandate thresholds via a mandate score combining severity,\nvolume saturation, representativeness, and evidence; and (4) evaluate a\nsynthetic program of 240 reports. Prompt-level fixes were fastest (median\n2.1-3.4 days) but less durable (21-38% recurrence); dataset edits and reward\ntweaks were slower (13.5 and 21.9 days) yet more durable (12-18% recurrence)\nwith higher planner uptake (30-36%). A threshold of 0.12 yielded 93% precision\nand 75% recall; increasing representativeness raised recall to 81% with little\nprecision loss. We discuss integration with participatory governance, risks\n(e.g., overfitting to vocal groups), and safeguards (dashboards, rotating\njuries).", "AI": {"tldr": "This paper addresses how text-to-image diffusion models can harm group dynamics and proposes collective recourse through community feedback mechanisms to improve model performance and planning.", "motivation": "Text-to-image diffusion models have the potential to visualize urban futures but can also exacerbate existing group-level harms. The need for effective recourse mechanisms is crucial for addressing these issues.", "method": "The authors formalize collective recourse and introduce a structured pipeline that includes reporting, triaging, fixing, verifying, and closure. They identify four recourse primitives within diffusion models and evaluate their effectiveness through a synthetic program of community reports.", "result": "The study found that prompt-level fixes were the quickest to implement but had a lower durability. Dataset edits and reward model tweaks took longer but were more effective and received better uptake from planners. A mandate score was developed for assessing the severity and effectiveness of the fixes, achieving 93% precision and 75% recall in evaluations.", "conclusion": "The integration of collective recourse with participatory governance is essential, and safeguards are necessary to mitigate risks such as overfitting to dominant voices in the community.", "key_contributions": ["Formalization of collective recourse and a practical pipeline", "Identification of four recourse primitives within the diffusion stack", "Evaluation of effectiveness using a synthetic program of community reports"], "limitations": "Risks of overfitting to vocal groups and reliance on community feedback mechanisms could bias the outcomes.", "keywords": ["text-to-image diffusion models", "collective recourse", "community feedback", "urban planning", "machine learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.11141", "pdf": "https://arxiv.org/pdf/2509.11141.pdf", "abs": "https://arxiv.org/abs/2509.11141", "title": "When Smiley Turns Hostile: Interpreting How Emojis Trigger LLMs' Toxicity", "authors": ["Shiyao Cui", "Xijia Feng", "Yingkang Wang", "Junxiao Yang", "Zhexin Zhang", "Biplab Sikdar", "Hongning Wang", "Han Qiu", "Minlie Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Emojis are globally used non-verbal cues in digital communication, and\nextensive research has examined how large language models (LLMs) understand and\nutilize emojis across contexts. While usually associated with friendliness or\nplayfulness, it is observed that emojis may trigger toxic content generation in\nLLMs. Motivated by such a observation, we aim to investigate: (1) whether\nemojis can clearly enhance the toxicity generation in LLMs and (2) how to\ninterpret this phenomenon. We begin with a comprehensive exploration of\nemoji-triggered LLM toxicity generation by automating the construction of\nprompts with emojis to subtly express toxic intent. Experiments across 5\nmainstream languages on 7 famous LLMs along with jailbreak tasks demonstrate\nthat prompts with emojis could easily induce toxicity generation. To understand\nthis phenomenon, we conduct model-level interpretations spanning semantic\ncognition, sequence generation and tokenization, suggesting that emojis can act\nas a heterogeneous semantic channel to bypass the safety mechanisms. To pursue\ndeeper insights, we further probe the pre-training corpus and uncover potential\ncorrelation between the emoji-related data polution with the toxicity\ngeneration behaviors. Supplementary materials provide our implementation code\nand data. (Warning: This paper contains potentially sensitive contents)", "AI": {"tldr": "The paper investigates how emojis induce toxicity in large language models (LLMs) and explores the mechanisms behind this phenomenon.", "motivation": "To examine the connection between emojis and toxicity generation in LLMs, given that emojis are often misunderstood and can trigger negative responses.", "method": "Automated construction of prompts with emojis was used to assess toxicity generation in LLMs across five languages and seven models. Interpretations of model behavior were analyzed to understand emoji effects.", "result": "Emoji-laden prompts significantly induced toxic content generation in the tested LLMs. Model-level analysis revealed that emojis served as a channel to bypass safety measures.", "conclusion": "The findings highlight a critical vulnerability in LLMs regarding emoji usage, necessitating improved safety mechanisms that account for non-verbal cues.", "key_contributions": ["Automated methodology for studying emoji impact on LLM toxicity.", "Comprehensive experimental results across multiple languages and models.", "Insights into the role of emojis in bypassing safety mechanisms."], "limitations": "The study primarily focused on a limited set of LLMs and languages; further exploration is needed for a broader analysis.", "keywords": ["emojis", "large language models", "toxicity generation", "digital communication", "semantic cognition"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.11600", "pdf": "https://arxiv.org/pdf/2509.11600.pdf", "abs": "https://arxiv.org/abs/2509.11600", "title": "BioMetaphor: AI-Generated Biodata Representations for Virtual Co-Present Events", "authors": ["Lin Lin", "Ming Wu", "Anyu Ren", "Zhanwei Wu", "Daojun Gong", "Ruowei Xiao"], "categories": ["cs.HC"], "comment": null, "summary": "In virtual or hybrid co-present events, biodata is emerging as a new paradigm\nof social cues. While it is able to reveal individuals' inner states, the\ntechnology-mediated representation of biodata in social contexts remains\nunderexplored. This study aims to uncover human cognitive preferences and\npatterns for biodata expression and leverage this knowledge to guide generative\nAI (GenAI) in creating biodata representations for co-present experiences,\naligning with the broader concept of Human-in-the-loop. We conducted a user\nelicitation workshop with 30 HCI experts and investigated the results using\nqualitative analysis. Based on our findings, we further propose a GenAI-driven\nframework: BioMetaphor. Our framework demonstration shows that current GenAI\ncan learn and express visual biodata cues in an event-adpated, human-like\nmanner. This human-centered approach engages users in research, revealing the\nunderlying cognition constructions for biodata expression while demonstrating\nhow such knowledge can inform the design and development of future empathic\ntechnologies.", "AI": {"tldr": "This study explores the use of biodata as social cues in virtual or hybrid events, proposing a Generative AI framework for creating human-like biodata representations.", "motivation": "To investigate how biodata can be effectively represented and utilized in social contexts, particularly in virtual environments, and to leverage this for generative AI design.", "method": "Conducted a user elicitation workshop with 30 HCI experts and performed qualitative analysis of the results to understand preferences for biodata expression.", "result": "Developed the BioMetaphor framework demonstrating that generative AI can learn to express biodata cues in a human-like manner suitable for events.", "conclusion": "The study provides insights into cognitive preferences for biodata and shows how generative AI can enhance the design of empathic technologies by engaging users in the process.", "key_contributions": ["Introduction of biodata as a paradigm of social cues in virtual events.", "Development of the BioMetaphor framework for generative AI biodata representations.", "Insights into human cognitive preferences for biodata expression."], "limitations": "", "keywords": ["biodata", "generative AI", "human-computer interaction", "empathic technologies", "social cues"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.11145", "pdf": "https://arxiv.org/pdf/2509.11145.pdf", "abs": "https://arxiv.org/abs/2509.11145", "title": "Text2Mem: A Unified Memory Operation Language for Memory Operating System", "authors": ["Felix Wang", "Boyu Chen", "Kerun Xu", "Bo Tang", "Feiyu Xiong", "Zhiyu Li"], "categories": ["cs.CL"], "comment": "11 pages, 3 figures", "summary": "Large language model agents increasingly depend on memory to sustain long\nhorizon interaction, but existing frameworks remain limited. Most expose only a\nfew basic primitives such as encode, retrieve, and delete, while higher order\noperations like merge, promote, demote, split, lock, and expire are missing or\ninconsistently supported. Moreover, there is no formal and executable\nspecification for memory commands, leaving scope and lifecycle rules implicit\nand causing unpredictable behavior across systems. We introduce Text2Mem, a\nunified memory operation language that provides a standardized pathway from\nnatural language to reliable execution. Text2Mem defines a compact yet\nexpressive operation set aligned with encoding, storage, and retrieval. Each\ninstruction is represented as a JSON based schema instance with required fields\nand semantic invariants, which a parser transforms into typed operation objects\nwith normalized parameters. A validator ensures correctness before execution,\nwhile adapters map typed objects either to a SQL prototype backend or to real\nmemory frameworks. Model based services such as embeddings or summarization are\nintegrated when required. All results are returned through a unified execution\ncontract. This design ensures safety, determinism, and portability across\nheterogeneous backends. We also outline Text2Mem Bench, a planned benchmark\nthat separates schema generation from backend execution to enable systematic\nevaluation. Together, these components establish the first standardized\nfoundation for memory control in agents.", "AI": {"tldr": "Text2Mem is a unified memory operation language for large language model agents, offering a standardized way to execute memory commands and improve interaction reliability.", "motivation": "To address limitations in existing memory frameworks for large language models, specifically the lack of higher order memory operations and a formal specification for memory commands.", "method": "Text2Mem introduces an expressive set of memory operations defined in a JSON schema, ensuring safety and correctness through validation, while supporting backend execution through adapters.", "result": "Text2Mem ensures reliable interaction by providing structured memory commands that facilitate deterministic behavior across different memory systems.", "conclusion": "Text2Mem and its accompanying benchmark offer a foundational approach to improving memory control in language model agents, promoting systematic evaluation and safety in execution.", "key_contributions": ["Standardized memory operation language for LLM agents.", "Formal JSON schema for memory commands ensuring safety and correctness.", "Planned benchmark (Text2Mem Bench) for evaluating memory control systems."], "limitations": "", "keywords": ["memory operations", "language models", "HCI", "JSON schema", "benchmarking"], "importance_score": 8, "read_time_minutes": 11}}
{"id": "2509.11622", "pdf": "https://arxiv.org/pdf/2509.11622.pdf", "abs": "https://arxiv.org/abs/2509.11622", "title": "Robots that Evolve with Us: Modular Co-Design for Personalization, Adaptability, and Sustainability", "authors": ["Lingyun Chen", "Qing Xiao", "Zitao Zhang", "Eli Blevis", "Selma Šabanović"], "categories": ["cs.HC"], "comment": "Pre-print", "summary": "Many current robot designs prioritize efficiency and one-size-fits-all\nsolutions, oftentimes overlooking personalization, adaptability, and\nsustainability. To explore alternatives, we conducted two co-design workshops\nwith 23 participants, who engaged with a modular robot co-design framework.\nUsing components we provided as building blocks, participants combined,\nremoved, and invented modules to envision how modular robots could accompany\nthem from childhood through adulthood and into older adulthood. The\nparticipants' designs illustrate how modularity (a) enables personalization\nthrough open-ended configuration, (b) adaptability across shifting life-stage\nneeds, and (c) sustainability through repair, reuse, and continuity. We\ntherefore derive design principles that establish modularity as a foundation\nfor lifespan-oriented human-robot interaction. This work reframes modular\nrobotics as a flexible and expressive co-design approach, supporting robots\nthat evolve with people, rather than static products optimized for single\nmoments or contexts of use.", "AI": {"tldr": "This paper explores a co-design framework for modular robots that emphasizes personalization, adaptability, and sustainability across different life stages.", "motivation": "To address limitations in current robot designs that prioritize efficiency without considering personalization and adaptability, leading to a need for more sustainable human-robot interactions.", "method": "Two co-design workshops with 23 participants were conducted where they engaged with a modular robot co-design framework, using components to design robots for various life stages.", "result": "Participants' designs showcased how modularity supports personalization, adaptability to life stages, and sustainability through features like repair and reuse.", "conclusion": "Modularity in robotic design is framed as a flexible and expressive co-design approach, enabling robots to evolve alongside users throughout their lifespan.", "key_contributions": ["Establishment of design principles for lifespan-oriented human-robot interaction", "Highlighting the importance of modularity in personalization and adaptability", "Proposing a co-design framework that involves users in the design process"], "limitations": "The study's applicability may be limited to the participant demographics and setting of the workshops, which could affect generalization to broader populations.", "keywords": ["modular robotics", "human-robot interaction", "co-design", "personalization", "sustainability"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.11176", "pdf": "https://arxiv.org/pdf/2509.11176.pdf", "abs": "https://arxiv.org/abs/2509.11176", "title": "Differentially-private text generation degrades output language quality", "authors": ["Erion Çano", "Ivan Habernal"], "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 3 figures, 35 tables", "summary": "Ensuring user privacy by synthesizing data from large language models (LLMs)\ntuned under differential privacy (DP) has become popular recently. However, the\nimpact of DP fine-tuned LLMs on the quality of the language and the utility of\nthe texts they produce has not been investigated. In this work, we tune five\nLLMs with three corpora under four levels of privacy and assess the length, the\ngrammatical correctness, and the lexical diversity of the text outputs they\nproduce. We also probe the utility of the synthetic outputs in downstream\nclassification tasks such as book genre recognition based on book descriptions\nand cause of death recognition based on verbal autopsies. The results indicate\nthat LLMs tuned under stronger privacy constrains produce texts that are\nshorter by at least 77 %, that are less grammatically correct by at least 9 %,\nand are less diverse by at least 10 % in bi-gram diversity. Furthermore, the\naccuracy they reach in downstream classification tasks decreases, which might\nbe detrimental to the usefulness of the generated synthetic data.", "AI": {"tldr": "This paper investigates the effects of differential privacy (DP) on the quality and utility of text generated by fine-tuned large language models (LLMs).", "motivation": "To analyze the impact of differential privacy tuning on the text quality and utility of outputs from LLMs, especially in relation to user privacy.", "method": "The authors tuned five LLMs on three different corpora across four levels of privacy, then evaluated the outputs based on length, grammatical correctness, and lexical diversity, while also assessing utility in classification tasks like book genre recognition and cause of death recognition.", "result": "The study found that stronger privacy constraints resulted in shorter texts (by at least 77%), lower grammatical correctness (decreasing by at least 9%), and reduced lexical diversity (by at least 10% in bi-gram diversity). Additionally, accuracy in downstream classification tasks decreased, potentially undermining the usefulness of the synthetic data.", "conclusion": "The findings suggest that while differential privacy can enhance user privacy, it significantly degrades the quality and utility of the generated text, which raises concerns about the effectiveness of using these synthetic data in practical applications.", "key_contributions": ["Demonstrated how differential privacy affects LLM text outputs", "Quantified reductions in text quality metrics (length, grammatical correctness, diversity)", "Assessed impacts on downstream classification task accuracy"], "limitations": "The study focused on specific LLMs and datasets; results might vary across different models or real-world applications.", "keywords": ["differential privacy", "large language models", "text generation", "synthetic data", "classification tasks"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.11644", "pdf": "https://arxiv.org/pdf/2509.11644.pdf", "abs": "https://arxiv.org/abs/2509.11644", "title": "Colour Perception in Immersive Virtual Reality: Emotional and Physiological Responses to Fifteen Munsell Hues", "authors": ["Francesco Febbraio", "Simona Collina", "Christina Lepida", "Panagiotis Kourtesis"], "categories": ["cs.HC", "H.5.2; J.3; J.4; K.4"], "comment": "24 pages, 6 Figures, 9 Tables", "summary": "Colour is a fundamental determinant of affective experience in immersive\nvirtual reality (VR), yet the emotional and physiological impact of individual\nhues remains poorly characterised. This study investigated how fifteen\ncalibrated Munsell hues influence subjective and autonomic responses when\npresented in immersive VR. Thirty-six adults (18-45 years) viewed each hue in a\nwithin-subject design while pupil diameter and skin conductance were recorded\ncontinuously, and self-reported emotions were assessed using the\nSelf-Assessment Manikin across pleasure, arousal, and dominance.\nRepeated-measures ANOVAs revealed robust hue effects on all three self-report\ndimensions and on pupil dilation, with medium to large effect sizes. Reds and\nred-purple hues elicited the highest arousal and dominance, whereas blue-green\nhues were rated most pleasurable. Pupil dilation closely tracked arousal\nratings, while skin conductance showed no reliable hue differentiation, likely\ndue to the brief (30 s) exposures. Individual differences in cognitive style\nand personality modulated overall reactivity but did not alter the relative\nranking of hues. Taken together, these findings provide the first systematic\nhue-by-hue mapping of affective and physiological responses in immersive VR.\nThey demonstrate that calibrated colour shapes both experience and ocular\nphysiology, while also offering practical guidance for educational, clinical,\nand interface design in virtual environments.", "AI": {"tldr": "This study examines the emotional and physiological impacts of fifteen calibrated hues in immersive virtual reality, revealing significant differences in self-reported emotions and pupil responses.", "motivation": "To systematically characterize the emotional and physiological effects of specific colors within immersive virtual reality environments.", "method": "Thirty-six participants viewed fifteen calibrated Munsell hues while their pupil diameter and skin conductance were continuously measured; emotions were assessed using the Self-Assessment Manikin.", "result": "Reds and red-purple hues elicited the highest arousal and dominance, while blue-green hues were rated as most pleasurable. Pupil dilation correlated with arousal ratings.", "conclusion": "The research provides a systematic hue-by-hue mapping of affective and physiological responses, indicating that color significantly influences emotional experiences and ocular responses in VR, offering practical insights for various applications.", "key_contributions": ["First systematic mapping of affective and physiological responses to hues in VR", "Demonstrated color shapes emotional experience and ocular physiology", "Practical guidance for educational, clinical, and interface design in virtual environments"], "limitations": "Skin conductance did not show reliable differentiation across hues, likely due to brief exposure times.", "keywords": ["immersive virtual reality", "color psychology", "affective experience", "physiological responses", "hue effects"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.11177", "pdf": "https://arxiv.org/pdf/2509.11177.pdf", "abs": "https://arxiv.org/abs/2509.11177", "title": "Optimal Brain Restoration for Joint Quantization and Sparsification of LLMs", "authors": ["Hang Guo", "Yawei Li", "Luca Benini"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Recent advances in Large Language Model (LLM) compression, such as\nquantization and pruning, have achieved notable success. However, as these\ntechniques gradually approach their respective limits, relying on a single\nmethod for further compression has become increasingly challenging. In this\nwork, we explore an alternative solution by combining quantization and\nsparsity. This joint approach, though promising, introduces new difficulties\ndue to the inherently conflicting requirements on weight distributions:\nquantization favors compact ranges, while pruning benefits from high variance.\nTo attack this problem, we propose Optimal Brain Restoration (OBR), a general\nand training-free framework that aligns pruning and quantization by error\ncompensation between both. OBR minimizes performance degradation on downstream\ntasks by building on a second-order Hessian objective, which is then\nreformulated into a tractable problem through surrogate approximation and\nultimately reaches a closed-form solution via group error compensation.\nExperiments show that OBR enables aggressive W4A4KV4 quantization with 50%\nsparsity on existing LLMs, and delivers up to 4.72x speedup and 6.4x memory\nreduction compared to the FP16-dense baseline.", "AI": {"tldr": "The paper introduces Optimal Brain Restoration (OBR), a framework for combining quantization and sparsity in Large Language Model (LLM) compression.", "motivation": "To overcome limitations of single compression techniques for LLMs as quantization and pruning approaches reach their limits.", "method": "The OBR framework aligns quantization and sparsity by error compensation, using a second-order Hessian objective reformed into a tractable problem.", "result": "Experiments demonstrate that OBR allows for W4A4KV4 quantization with 50% sparsity, achieving up to 4.72x speedup and 6.4x memory reduction versus FP16-dense models.", "conclusion": "OBR proves to be an effective strategy for reducing model size and improving performance in compressed LLMs.", "key_contributions": ["Introduction of Optimal Brain Restoration (OBR) framework", "Combination of quantization and pruning techniques", "Achieving significant speedup and memory reduction for LLMs"], "limitations": "", "keywords": ["Large Language Model", "compression", "quantization", "pruning", "sparsity"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2509.11653", "pdf": "https://arxiv.org/pdf/2509.11653.pdf", "abs": "https://arxiv.org/abs/2509.11653", "title": "See What I Mean? Mobile Eye-Perspective Rendering for Optical See-through Head-mounted Displays", "authors": ["Gerlinde Emsenhuber", "Tobias Langlotz", "Denis Kalkofen", "Markus Tatzgern"], "categories": ["cs.HC"], "comment": null, "summary": "Image-based scene understanding allows Augmented Reality systems to provide\ncontextual visual guidance in unprepared, real-world environments. While\neffective on video see-through (VST) head-mounted displays (HMDs), such methods\nsuffer on optical see-through (OST) HMDs due to misregistration between the\nworld-facing camera and the user's eye perspective. To approximate the user's\ntrue eye view, we implement and evaluate three software-based eye-perspective\nrendering (EPR) techniques on a commercially available, untethered OST HMD\n(Microsoft HoloLens 2): (1) Plane-Proxy EPR, projecting onto a fixed-distance\nplane; (2) Mesh-Proxy EPR, using SLAM-based reconstruction for projection; and\n(3) Gaze-Proxy EPR, a novel eye-tracking-based method that aligns the\nprojection with the user's gaze depth. A user study on real-world tasks\nunderscores the importance of accurate EPR and demonstrates gaze-proxy as a\nlightweight alternative to geometry-based methods. We release our EPR framework\nas open source.", "AI": {"tldr": "This paper presents three software-based eye-perspective rendering (EPR) techniques for improving Augmented Reality experiences on optical see-through head-mounted displays, with a focus on real-world usability and accuracy.", "motivation": "To address the misregistration issues between the user's eye perspective and the world-facing camera in optical see-through head-mounted displays, enhancing scene understanding in Augmented Reality environments.", "method": "The study implements three EPR techniques: Plane-Proxy EPR, Mesh-Proxy EPR, and Gaze-Proxy EPR, the latter utilizing eye tracking for improved alignment with the user's gaze depth. Each technique is evaluated through user studies in realistic task scenarios.", "result": "User studies demonstrate that the Gaze-Proxy EPR technique offers a lightweight alternative to more complex geometry-based methods and underscores the impact of accurate eye-perspective rendering on user experience.", "conclusion": "The findings support the importance of accurate EPR in Augmented Reality applications and highlight the effectiveness of the gaze-proxy method, suggesting potential for broader implementation.", "key_contributions": ["Introduction of three novel EPR techniques for OST HMDs.", "Demonstration of gaze-proxy EPR's efficacy over traditional methods.", "Release of an open-source EPR framework for community use."], "limitations": "The study is based on a specific type of HMD (Microsoft HoloLens 2) and may not generalize across all OST devices.", "keywords": ["Augmented Reality", "eye-perspective rendering", "optical see-through", "eye tracking", "HoloLens"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.11191", "pdf": "https://arxiv.org/pdf/2509.11191.pdf", "abs": "https://arxiv.org/abs/2509.11191", "title": "RanAT4BIE: Random Adversarial Training for Biomedical Information Extraction", "authors": ["Jian Chen", "Shengyi Lv", "Leilei Su"], "categories": ["cs.CL", "cs.IR"], "comment": "Accepted for publication at the International Joint Conference on\n  Neural Networks (IJCNN) 2025", "summary": "We introduce random adversarial training (RAT), a novel framework\nsuccessfully applied to biomedical information extraction (BioIE) tasks.\nBuilding on PubMedBERT as the foundational architecture, our study first\nvalidates the effectiveness of conventional adversarial training in enhancing\npre-trained language models' performance on BioIE tasks. While adversarial\ntraining yields significant improvements across various performance metrics, it\nalso introduces considerable computational overhead. To address this\nlimitation, we propose RAT as an efficiency solution for biomedical information\nextraction. This framework strategically integrates random sampling mechanisms\nwith adversarial training principles, achieving dual objectives: enhanced model\ngeneralization and robustness while significantly reducing computational costs.\nThrough comprehensive evaluations, RAT demonstrates superior performance\ncompared to baseline models in BioIE tasks. The results highlight RAT's\npotential as a transformative framework for biomedical natural language\nprocessing, offering a balanced solution to the model performance and\ncomputational efficiency.", "AI": {"tldr": "Random Adversarial Training (RAT) enhances biomedical information extraction while reducing computational costs.", "motivation": "To improve performance on biomedical information extraction tasks while addressing the computational overhead of conventional adversarial training.", "method": "RAT integrates random sampling with adversarial training techniques in the context of PubMedBERT for BioIE tasks.", "result": "RAT outperforms baseline models in BioIE tasks with improved model generalization and robustness at lower computational costs.", "conclusion": "RAT serves as an efficient alternative, offering enhanced performance for biomedical NLP tasks.", "key_contributions": ["Introduction of a novel framework (RAT) for BioIE tasks.", "Demonstration of improved efficiency and effectiveness of adversarial training.", "Comprehensive evaluations proving RAT's superiority over existing methods."], "limitations": "Computational overhead remains a factor despite efficiency improvements; specific implementation details may affect generalizability.", "keywords": ["random adversarial training", "biomedical information extraction", "PubMedBERT", "NLP", "efficiency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.11826", "pdf": "https://arxiv.org/pdf/2509.11826.pdf", "abs": "https://arxiv.org/abs/2509.11826", "title": "Collaborative Document Editing with Multiple Users and AI Agents", "authors": ["Florian Lehmann", "Krystsina Shauchenka", "Daniel Buschek"], "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "comment": "34 pages, 10 figures, 4 tables", "summary": "Current AI writing support tools are largely designed for individuals,\ncomplicating collaboration when co-writers must leave the shared workspace to\nuse AI and then communicate and reintegrate results. We propose integrating AI\nagents directly into collaborative writing environments. Our prototype makes AI\nuse transparent and customisable through two new shared objects: agent profiles\nand tasks. Agent responses appear in the familiar comment feature. In a user\nstudy (N=30), 14 teams worked on writing projects during one week. Interaction\nlogs and interviews show that teams incorporated agents into existing norms of\nauthorship, control, and coordination, rather than treating them as team\nmembers. Agent profiles were viewed as personal territory, while created agents\nand outputs became shared resources. We discuss implications for team-based AI\ninteraction, highlighting opportunities and boundaries for treating AI as a\nshared resource in collaborative work.", "AI": {"tldr": "The paper proposes integrating AI agents into collaborative writing environments to enhance teamwork by making AI use transparent.", "motivation": "To address the complications of collaboration in AI writing support tools designed for individuals rather than teams.", "method": "A prototype incorporating AI into writing environments was developed, featuring agent profiles and tasks. A user study with 30 participants was conducted to evaluate interactions and collaborations during writing projects.", "result": "Teams used AI agents seamlessly within their existing norms, viewing agent profiles as personal resources and outputs as shared tools, highlighting the integration of AI into collaborative authorship.", "conclusion": "The study reveals the importance of treating AI as a collaborative resource, suggesting new norms for teamwork involving AI agents.", "key_contributions": ["Integration of AI agents into collaborative writing spaces", "Development of customizable agent profiles and tasks", "Insights into team dynamics and use of AI as a shared resource"], "limitations": "The study size was limited to 30 participants, which may not represent all collaborative contexts.", "keywords": ["AI agents", "collaborative writing", "human-computer interaction", "team dynamics", "shared resources"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2509.11295", "pdf": "https://arxiv.org/pdf/2509.11295.pdf", "abs": "https://arxiv.org/abs/2509.11295", "title": "The Prompt Engineering Report Distilled: Quick Start Guide for Life Sciences", "authors": ["Valentin Romanov", "Steven A Niederer"], "categories": ["cs.CL"], "comment": null, "summary": "Developing effective prompts demands significant cognitive investment to\ngenerate reliable, high-quality responses from Large Language Models (LLMs). By\ndeploying case-specific prompt engineering techniques that streamline\nfrequently performed life sciences workflows, researchers could achieve\nsubstantial efficiency gains that far exceed the initial time investment\nrequired to master these techniques. The Prompt Report published in 2025\noutlined 58 different text-based prompt engineering techniques, highlighting\nthe numerous ways prompts could be constructed. To provide actionable\nguidelines and reduce the friction of navigating these various approaches, we\ndistil this report to focus on 6 core techniques: zero-shot, few-shot\napproaches, thought generation, ensembling, self-criticism, and decomposition.\nWe breakdown the significance of each approach and ground it in use cases\nrelevant to life sciences, from literature summarization and data extraction to\neditorial tasks. We provide detailed recommendations for how prompts should and\nshouldn't be structured, addressing common pitfalls including multi-turn\nconversation degradation, hallucinations, and distinctions between reasoning\nand non-reasoning models. We examine context window limitations, agentic tools\nlike Claude Code, while analyzing the effectiveness of Deep Research tools\nacross OpenAI, Google, Anthropic and Perplexity platforms, discussing current\nlimitations. We demonstrate how prompt engineering can augment rather than\nreplace existing established individual practices around data processing and\ndocument editing. Our aim is to provide actionable guidance on core prompt\nengineering principles, and to facilitate the transition from opportunistic\nprompting to an effective, low-friction systematic practice that contributes to\nhigher quality research.", "AI": {"tldr": "The paper presents a streamlined approach to prompt engineering for Large Language Models (LLMs), focusing on six core techniques relevant to life sciences workflows.", "motivation": "To improve the efficiency of generating reliable responses from LLMs in life sciences, minimizing cognitive load.", "method": "The authors distilled a comprehensive report into six key prompt engineering techniques filled with actionable recommendations and guidelines.", "result": "The exploration of techniques like zero-shot, few-shot approaches, and self-criticism reveals significant potential for improving the quality of research outputs in life sciences.", "conclusion": "The paper promotes systematic prompt engineering as an aid to enhance data processing and editing practices rather than replace them.", "key_contributions": ["Identification of six core prompt engineering techniques", "Actionable guidelines for effective prompt construction", "Analysis of limitations across various platforms for LLM use in life sciences"], "limitations": "Focus is primarily on life sciences, which may limit generalizability to other fields; might not address all advanced prompt engineering scenarios.", "keywords": ["prompt engineering", "large language models", "life sciences", "efficiency", "data processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.11851", "pdf": "https://arxiv.org/pdf/2509.11851.pdf", "abs": "https://arxiv.org/abs/2509.11851", "title": "The AI Memory Gap: Users Misremember What They Created With AI or Without", "authors": ["Tim Zindulka", "Sven Goller", "Daniela Fernandes", "Robin Welsch", "Daniel Buschek"], "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "comment": "31 pages, 10 figures, 9 tables", "summary": "As large language models (LLMs) become embedded in interactive text\ngeneration, disclosure of AI as a source depends on people remembering which\nideas or texts came from themselves and which were created with AI. We\ninvestigate how accurately people remember the source of content when using AI.\nIn a pre-registered experiment, 184 participants generated and elaborated on\nideas both unaided and with an LLM-based chatbot. One week later, they were\nasked to identify the source (noAI vs withAI) of these ideas and texts. Our\nfindings reveal a significant gap in memory: After AI use, the odds of correct\nattribution dropped, with the steepest decline in mixed human-AI workflows,\nwhere either the idea or elaboration was created with AI. We validated our\nresults using a computational model of source memory. Discussing broader\nimplications, we highlight the importance of considering source confusion in\nthe design and use of interactive text generation technologies.", "AI": {"tldr": "This paper investigates how accurately people remember the source of content generated with the help of large language models (LLMs).", "motivation": "As LLMs are increasingly used in interactive text generation, it is crucial to understand how their integration affects users' memory of content sources.", "method": "The study involved 184 participants who generated content both unaided and with an LLM-based chatbot. Their ability to remember the source of this content was tested one week later.", "result": "The results showed a significant decline in correct attribution of content sources after AI use, especially in scenarios where human and AI contributions were mixed.", "conclusion": "The findings underscore the need to address source confusion in the design and use of interactive text generation technologies.", "key_contributions": ["Identified a significant gap in memory attribution when using AI for text generation.", "Validated findings with a computational model of source memory.", "Emphasized implications for designing AI interaction systems."], "limitations": "The study was limited to a specific experimental setup with a predefined participant group, which may not generalize to all users or contexts.", "keywords": ["large language models", "source memory", "interactive text generation", "AI", "human-computer interaction"], "importance_score": 9, "read_time_minutes": 31}}
{"id": "2509.11303", "pdf": "https://arxiv.org/pdf/2509.11303.pdf", "abs": "https://arxiv.org/abs/2509.11303", "title": "Ko-PIQA: A Korean Physical Commonsense Reasoning Dataset with Cultural Context", "authors": ["Dasol Choi", "Jungwhan Kim", "Guijin Son"], "categories": ["cs.CL"], "comment": null, "summary": "Physical commonsense reasoning datasets like PIQA are predominantly\nEnglish-centric and lack cultural diversity. We introduce Ko-PIQA, a Korean\nphysical commonsense reasoning dataset that incorporates cultural context.\nStarting from 3.01 million web-crawled questions, we employed a multi-stage\nfiltering approach using three language models to identify 11,553 PIQA-style\nquestions. Through GPT-4o refinement and human validation, we obtained 441\nhigh-quality question-answer pairs. A key feature of Ko-PIQA is its cultural\ngrounding: 19.7\\% of questions contain culturally specific elements like\ntraditional Korean foods (kimchi), clothing (hanbok), and specialized\nappliances (kimchi refrigerators) that require culturally-aware reasoning\nbeyond direct translation. We evaluate seven language models on Ko-PIQA, with\nthe best model achieving 83.22\\% accuracy while the weakest reaches only\n59.86\\%, demonstrating significant room for improvement. Models particularly\nstruggle with culturally specific scenarios, highlighting the importance of\nculturally diverse datasets. Ko-PIQA serves as both a benchmark for Korean\nlanguage models and a foundation for more inclusive commonsense reasoning\nresearch. The dataset and code will be publicly available.", "AI": {"tldr": "Ko-PIQA is a Korean commonsense reasoning dataset addressing cultural diversity in physical commonsense scenarios, featuring 441 high-quality questions.", "motivation": "To address the lack of cultural diversity in existing physical commonsense reasoning datasets, particularly for the Korean language.", "method": "A multi-stage filtering process from 3.01 million web-crawled questions led to the selection and refinement of 441 high-quality question-answer pairs using language models and human validation.", "result": "Evaluation of seven language models on Ko-PIQA showed accuracy ranging from 59.86% to 83.22%, indicating significant challenges in handling culturally specific scenarios.", "conclusion": "Ko-PIQA serves as a benchmark for Korean language models and promotes a more inclusive approach in commonsense reasoning research.", "key_contributions": ["Introduction of a Korean commonsense reasoning dataset (Ko-PIQA)", "Incorporation of culturally specific elements in reasoning tasks", "Establishment of a new benchmark for evaluating language models in the Korean language"], "limitations": "The dataset is limited to cultural contexts pertinent to Korean culture and may not generalize to other cultures.", "keywords": ["commonsense reasoning", "Korean language", "cultural diversity", "dataset", "language models"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.11876", "pdf": "https://arxiv.org/pdf/2509.11876.pdf", "abs": "https://arxiv.org/abs/2509.11876", "title": "Lost in Data: How Older Adults Perceive and Navigate Health Data Representations", "authors": ["Peterson Jean", "Emma Murphy", "Enda Bates"], "categories": ["cs.HC"], "comment": "AAATE 2025 Proceedings (Research Strand). Licensed under CC BY-NC-ND\n  4.0. ISBN: 978-9925-604-07-4", "summary": "As the ageing population grows, older adults increasingly rely on wearable\ndevices to monitor chronic conditions. However, conventional health data\nrepresentations (HDRs) often present accessibility challenges, particularly for\ncritical health parameters like blood pressure and sleep data. This study\nexplores how older adults interact with these representations, identifying key\nbarriers such as semantic inconsistency and difficulties in understanding.\nWhile research has primarily focused on data collection, less attention has\nbeen given to how information is output and understood by end-users. To address\nthis, an end-user evaluation was conducted with 16 older adults (65+) in a\nstructured workshop, using think-aloud protocols and participatory design\nactivities. The findings highlight the importance of affordance and familiarity\nin improving accessibility, emphasising the familiarity and potential of\nmultimodal cues. This study bridges the gap between domain experts and\nend-users, providing a replicable methodological approach for designing\nintuitive, multisensory HDRs that better align with older adults' needs and\nabilities.", "AI": {"tldr": "The study investigates how older adults interact with health data representations, identifying barriers and proposing design improvements for better accessibility.", "motivation": "The ageing population increasingly uses wearable devices for chronic condition monitoring, yet conventional health data representations present accessibility challenges for older adults.", "method": "An end-user evaluation was conducted with 16 older adults (65+) using think-aloud protocols and participatory design activities in a structured workshop.", "result": "Findings reveal key barriers such as semantic inconsistency and difficulties in understanding health data, emphasizing the importance of affordance and familiarity in improving accessibility.", "conclusion": "The research provides a methodological approach to designing intuitive, multisensory health data representations that meet the needs and abilities of older adults.", "key_contributions": ["Identification of barriers in health data accessibility for older adults.", "Highlighting the role of multimodal cues in enhancing understanding.", "A replicable methodological framework for user-centered design in health data representations."], "limitations": "", "keywords": ["health data representations", "older adults", "accessibility", "multisensory design", "wearable devices"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.11365", "pdf": "https://arxiv.org/pdf/2509.11365.pdf", "abs": "https://arxiv.org/abs/2509.11365", "title": "!MSA at AraHealthQA 2025 Shared Task: Enhancing LLM Performance for Arabic Clinical Question Answering through Prompt Engineering and Ensemble Learning", "authors": ["Mohamed Tarek", "Seif Ahmed", "Mohamed Basem"], "categories": ["cs.CL"], "comment": "8 Pages , ArabicNLP 2025 , Co-located with EMNLP 2025", "summary": "We present our systems for Track 2 (General Arabic Health QA, MedArabiQ) of\nthe AraHealthQA-2025 shared task, where our methodology secured 2nd place in\nboth Sub-Task 1 (multiple-choice question answering) and Sub-Task 2 (open-ended\nquestion answering) in Arabic clinical contexts. For Sub-Task 1, we leverage\nthe Gemini 2.5 Flash model with few-shot prompting, dataset preprocessing, and\nan ensemble of three prompt configurations to improve classification accuracy\non standard, biased, and fill-in-the-blank questions. For Sub-Task 2, we employ\na unified prompt with the same model, incorporating role-playing as an Arabic\nmedical expert, few-shot examples, and post-processing to generate concise\nresponses across fill-in-the-blank, patient-doctor Q&A, GEC, and paraphrased\nvariants.", "AI": {"tldr": "This paper discusses the authors' systems for a health-related QA task in Arabic, achieving 2nd place in both multiple-choice and open-ended formats.", "motivation": "To improve question answering in Arabic clinical contexts and provide effective responses to health-related queries.", "method": "Utilizes the Gemini 2.5 Flash model with few-shot prompting, dataset preprocessing, and an ensemble of prompt configurations for Sub-Task 1; uses role-playing and post-processing techniques for Sub-Task 2.", "result": "Secured 2nd place in both Sub-Task 1 (MCQA) and Sub-Task 2 (open-ended QA) of the AraHealthQA-2025 shared task.", "conclusion": "The approach demonstrates the potential of advanced prompting strategies and model fine-tuning in Arabic health-related question answering.", "key_contributions": ["Implementation of a few-shot prompting methodology using Gemini 2.5 Flash model.", "Development of ensemble prompt configurations to enhance multiple-choice QA accuracy.", "Integration of role-playing elements for generating concise responses in open-ended QA."], "limitations": "", "keywords": ["Arabic Health QA", "Gemini Model", "Question Answering"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2509.11898", "pdf": "https://arxiv.org/pdf/2509.11898.pdf", "abs": "https://arxiv.org/abs/2509.11898", "title": "Generative AI in Game Development: A Qualitative Research Synthesis", "authors": ["Alexandru Ternar", "Alena Denisova", "João M. Cunha", "Annakaisa Kultima", "Christian Guckelsberger"], "categories": ["cs.HC"], "comment": "32 pages, 2 figures, 6 tables", "summary": "Generative Artificial Intelligence (GenAI) has had a tremendous impact on\ngame production and promises lasting transformations. In the last five years\nsince GenAI's inception, several studies, typically via qualitative methods,\nhave explored its impact on game production from different settings and\ndemographic angles. However, these studies often contextualise and consolidate\ntheir findings weakly with related work, and a big picture view is still\nmissing. Here, we aim to provide such a view of GenAI's impact on game\nproduction in the form of a qualitative research synthesis via\nmeta-ethnography. We followed PRISMA-S to systematically search the relevant\nliterature from 2020-2025, including major HCI and games research databases. We\nthen synthesised the 10 eligible studies, conducting reciprocal translation and\nline-of-argument synthesis guided by eMERGe, informed by CASP quality\nappraisal. We identified nine overarching themes, provide recommendations, and\ncontextualise our insights in wider game production trends.", "AI": {"tldr": "This paper provides a qualitative synthesis of the impact of Generative Artificial Intelligence on game production, identifying key themes and offering recommendations based on a systematic literature review.", "motivation": "To present a comprehensive view of the influence of Generative AI on game production, addressing gaps in existing studies and contextualizing findings within broader trends.", "method": "A systematic literature review was conducted following PRISMA-S guidelines, synthesizing 10 eligible studies through meta-ethnography and guided by eMERGe and CASP quality appraisal.", "result": "Nine overarching themes related to the impacts of Generative AI on game production were identified, along with recommendations for future practice and research.", "conclusion": "The study highlights the transformative potential of Generative AI in game production and provides insights that can inform both practitioners and researchers in the field.", "key_contributions": ["Qualitative synthesis of 10 studies on Generative AI's impact on game production", "Identification of nine overarching themes", "Recommendations for integrating Generative AI in game development"], "limitations": "The study is limited to literature from 2020-2025 and may not cover all aspects of Generative AI's impact across different contexts.", "keywords": ["Generative AI", "game production", "meta-ethnography", "HCI", "qualitative research"], "importance_score": 2, "read_time_minutes": 20}}
{"id": "2509.11374", "pdf": "https://arxiv.org/pdf/2509.11374.pdf", "abs": "https://arxiv.org/abs/2509.11374", "title": "Transformer Enhanced Relation Classification: A Comparative Analysis of Contextuality, Data Efficiency and Sequence Complexity", "authors": ["Bowen Jing", "Yang Cui", "Tianpeng Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the era of large language model, relation extraction (RE) plays an\nimportant role in information extraction through the transformation of\nunstructured raw text into structured data (Wadhwa et al., 2023). In this\npaper, we systematically compare the performance of deep supervised learning\napproaches without transformers and those with transformers. We used a series\nof non-transformer architectures such as PA-LSTM(Zhang et al., 2017),\nC-GCN(Zhang et al., 2018), and AGGCN(attention guide GCN)(Guo et al., 2019),\nand a series of transformer architectures such as BERT, RoBERTa, and R-BERT(Wu\nand He, 2019). Our comparison included traditional metrics like micro F1, as\nwell as evaluations in different scenarios, varying sentence lengths, and\ndifferent percentages of the dataset for training. Our experiments were\nconducted on TACRED, TACREV, and RE-TACRED. The results show that\ntransformer-based models outperform non-transformer models, achieving micro F1\nscores of 80-90% compared to 64-67% for non-transformer models. Additionally,\nwe briefly review the research journey in supervised relation classification\nand discuss the role and current status of large language models (LLMs) in\nrelation extraction.", "AI": {"tldr": "This paper compares deep supervised learning approaches for relation extraction, highlighting the superior performance of transformer-based models over non-transformer models.", "motivation": "Relation extraction is crucial for converting unstructured text into structured data, especially in the context of large language models.", "method": "The study systematically compares various non-transformer architectures (PA-LSTM, C-GCN, AGGCN) against transformer architectures (BERT, RoBERTa, R-BERT) using traditional metrics and varying training conditions on datasets TACRED, TACREV, and RE-TACRED.", "result": "Transformer-based models achieved micro F1 scores of 80-90%, significantly outperforming non-transformer models which scored 64-67%.", "conclusion": "The research indicates that transformer architectures are more effective for relation extraction tasks, and highlights the evolving role of large language models in this field.", "key_contributions": ["Systematic comparison of transformer and non-transformer models for relation extraction", "Demonstrates the performance gap between model types on key datasets", "Reviews the impact of large language models on relation extraction methodology"], "limitations": "", "keywords": ["relation extraction", "transformer models", "deep learning", "large language models", "information extraction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.11939", "pdf": "https://arxiv.org/pdf/2509.11939.pdf", "abs": "https://arxiv.org/abs/2509.11939", "title": "PrivWeb: Unobtrusive and Content-aware Privacy Protection For Web Agents", "authors": ["Shuning Zhang", "Yutong Jiang", "Rongjun Ma", "Yuting Yang", "Mingyao Xu", "Zhixin Huang", "Xin Yi", "Hewu Li"], "categories": ["cs.HC"], "comment": null, "summary": "While web agents gained popularity by automating web interactions, their\nrequirement for interface access introduces significant privacy risks that are\nunderstudied, particularly from users' perspective. Through a formative study\n(N=15), we found users frequently misunderstand agents' data practices, and\ndesired unobtrusive, transparent data management. To achieve this, we designed\nand implemented PrivWeb, a trusted add-on on web agents that utilizes a\nlocalized LLM to anonymize private information on interfaces according to user\npreferences. It features privacy categorization schema and adaptive\nnotifications that selectively pauses tasks for user control over information\ncollection for highly sensitive information, while offering non-disruptive\noptions for less sensitive information, minimizing human oversight. The user\nstudy (N=14) across travel, information retrieval, shopping, and entertainment\ntasks compared PrivWeb with baselines without notification and without control\nfor private information access, where PrivWeb reduced perceived privacy risks\nwith no associated increase in cognitive effort, and resulted in higher overall\nsatisfaction.", "AI": {"tldr": "This paper presents PrivWeb, a web agent add-on that enhances user privacy by utilizing a localized LLM to manage private information according to user preferences.", "motivation": "The study addresses the significant privacy risks associated with web agents, which are often underexamined from the user's perspective.", "method": "A formative study was conducted with 15 participants to understand user perceptions of data practices, followed by the design and implementation of PrivWeb, a trusted add-on. A user study with 14 participants compared PrivWeb against baseline conditions to evaluate effectiveness.", "result": "PrivWeb effectively reduced perceived privacy risks without increasing cognitive effort and led to higher user satisfaction across various tasks.", "conclusion": "PrivWeb provides unobtrusive and transparent data management for users of web agents, enhancing privacy without compromising user experience.", "key_contributions": ["Development of PrivWeb, a privacy-focused web agent add-on", "Incorporation of a localized LLM for anonymizing private information", "Adaptive notifications that enhance user control over information collection"], "limitations": "The study involved a small sample size and focused on specific task scenarios.", "keywords": ["privacy", "web agents", "localized LLM", "data management", "user control"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.11414", "pdf": "https://arxiv.org/pdf/2509.11414.pdf", "abs": "https://arxiv.org/abs/2509.11414", "title": "Continually Adding New Languages to Multilingual Language Models", "authors": ["Abraham Toluwase Owodunni", "Sachin Kumar"], "categories": ["cs.CL"], "comment": null, "summary": "Multilingual language models are trained on a fixed set of languages, and to\nsupport new languages, the models need to be retrained from scratch. This is an\nexpensive endeavor and is often infeasible, as model developers tend not to\nrelease their pre-training data. Naive approaches, such as continued\npretraining, suffer from catastrophic forgetting; however, mitigation\nstrategies like experience replay cannot be applied due to the lack of original\npretraining data. In this work, we investigate the problem of continually\nadding new languages to a multilingual model, assuming access to pretraining\ndata in only the target languages. We explore multiple approaches to address\nthis problem and propose Layer-Selective LoRA (LayRA), which adds Low-Rank\nAdapters (LoRA) to selected initial and final layers while keeping the rest of\nthe model frozen. LayRA builds on two insights: (1) LoRA reduces forgetting,\nand (2) multilingual models encode inputs in the source language in the initial\nlayers, reason in English in intermediate layers, and translate back to the\nsource language in final layers. We experiment with adding multiple\ncombinations of Galician, Swahili, and Urdu to pretrained language models and\nevaluate each method on diverse multilingual tasks. We find that LayRA provides\nthe overall best tradeoff between preserving models' capabilities in previously\nsupported languages, while being competitive with existing approaches such as\nLoRA in learning new languages. We also demonstrate that using model\narithmetic, the adapted models can be equipped with strong instruction\nfollowing abilities without access to any instruction tuning data in the target\nlanguages.", "AI": {"tldr": "This paper introduces Layer-Selective LoRA (LayRA) for multilingual models to effectively add new languages without catastrophic forgetting, using only target language pretraining data.", "motivation": "To address the challenge of adding new languages to multilingual models without needing to retrain them from scratch or suffering from catastrophic forgetting.", "method": "The authors propose Layer-Selective LoRA (LayRA), which selectively applies Low-Rank Adapters (LoRA) to specific model layers while keeping others frozen. The approach leverages the structure of multilingual models and emphasizes critical layer utilization for effective language support.", "result": "LayRA shows the best performance in balancing the retention of existing language capabilities while effectively learning new languages when compared to existing methods like LoRA.", "conclusion": "LayRA allows multilingual models to adapt to new languages efficiently, enhancing their instruction-following capabilities without needing instruction tuning data in target languages.", "key_contributions": ["Introduction of Layer-Selective LoRA for multilingual models.", "Improved preservation of model capabilities across existing languages while learning new languages.", "Demonstration of strong instruction following abilities using model arithmetic."], "limitations": "", "keywords": ["multilingual models", "Low-Rank Adapters", "catastrophic forgetting", "language adaptation", "instruction following"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.11999", "pdf": "https://arxiv.org/pdf/2509.11999.pdf", "abs": "https://arxiv.org/abs/2509.11999", "title": "Teaching the Teachers: Building Generative AI Literacy in Higher Ed Instructors", "authors": ["Si Chen", "Xiuxiu Tang", "Alison Cheng", "Nitesh Chawla", "G. Alex Ambrose", "Ronald Metoyer"], "categories": ["cs.HC"], "comment": null, "summary": "Generative AI is reshaping higher education, yet research has focused largely\non students, while instructors remain understudied despite their central role\nin mediating adoption and modeling responsible use. We present the \\textit{AI\nAcademy}, a faculty development program that combined AI exploration with\npedagogical reflection and peer learning. Rather than a course evaluated for\noutcomes, the Academy provided a setting to study how instructors build AI\nliteracies in relation to tools, policies, peer practices, and institutional\nsupports. We studied 25 instructors through pre/post surveys, learning logs,\nand facilitator interviews. Findings show AI literacy gains alongside new\ninsights. We position instructors as designers of responsible AI practices and\ncontribute a replicable program model, a co-constructed survey instrument, and\ndesign insights for professional development that adapts to evolving tools and\nfosters ethical discussion.", "AI": {"tldr": "The paper discusses the AI Academy, a faculty development program focused on enhancing AI literacy among instructors in higher education.", "motivation": "Research has primarily focused on students in the context of generative AI, neglecting the essential role instructors play in adoption and responsible use.", "method": "The study involved 25 instructors and utilized pre/post surveys, learning logs, and facilitator interviews to assess the impacts of the program.", "result": "Instructors demonstrated gains in AI literacy and developed new insights into responsible AI practices.", "conclusion": "The AI Academy serves as a model for professional development that encourages ethical discussion and adapts to changing AI tools.", "key_contributions": ["Development of the AI Academy program model", "Creation of a co-constructed survey instrument to assess AI literacy", "Design insights for professional development in AI use"], "limitations": "", "keywords": ["Generative AI", "Higher Education", "AI Literacy", "Faculty Development", "Professional Development"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2509.11443", "pdf": "https://arxiv.org/pdf/2509.11443.pdf", "abs": "https://arxiv.org/abs/2509.11443", "title": "A Transformer-Based Cross-Platform Analysis of Public Discourse on the 15-Minute City Paradigm", "authors": ["Gaurab Chhetri", "Darrell Anderson", "Boniphace Kutela", "Subasish Das"], "categories": ["cs.CL", "cs.SI"], "comment": "This is the author's preprint version of a paper accepted for\n  presentation at the 24th International Conference on Machine Learning and\n  Applications (ICMLA 2025), December 3-5, 2025, Florida, USA. The final\n  published version will appear in the official IEEE proceedings. Conference\n  site: https://www.icmla-conference.org/icmla25/", "summary": "This study presents the first multi-platform sentiment analysis of public\nopinion on the 15-minute city concept across Twitter, Reddit, and news media.\nUsing compressed transformer models and Llama-3-8B for annotation, we classify\nsentiment across heterogeneous text domains. Our pipeline handles long-form and\nshort-form text, supports consistent annotation, and enables reproducible\nevaluation. We benchmark five models (DistilRoBERTa, DistilBERT, MiniLM,\nELECTRA, TinyBERT) using stratified 5-fold cross-validation, reporting\nF1-score, AUC, and training time. DistilRoBERTa achieved the highest F1\n(0.8292), TinyBERT the best efficiency, and MiniLM the best cross-platform\nconsistency. Results show News data yields inflated performance due to class\nimbalance, Reddit suffers from summarization loss, and Twitter offers moderate\nchallenge. Compressed models perform competitively, challenging assumptions\nthat larger models are necessary. We identify platform-specific trade-offs and\npropose directions for scalable, real-world sentiment classification in urban\nplanning discourse.", "AI": {"tldr": "The study conducts a multi-platform sentiment analysis on the 15-minute city concept, utilizing compressed transformer models for annotation and benchmarking various sentiment classification models.", "motivation": "To analyze public opinion on the 15-minute city concept across different social media and news platforms, providing insights into sentiment dynamics within urban planning.", "method": "The analysis employs compressed transformer models like Llama-3-8B for sentiment annotation across Twitter, Reddit, and news articles, utilizing stratified 5-fold cross-validation to benchmark five models (DistilRoBERTa, DistilBERT, MiniLM, ELECTRA, TinyBERT) based on F1-score, AUC, and training time.", "result": "DistilRoBERTa demonstrated the highest F1-score at 0.8292, TinyBERT showed superior efficiency, and MiniLM excelled in cross-platform consistency. The study highlighted the impact of data platforms on performance metrics and identified challenges specific to each platform.", "conclusion": "The findings indicate competitive performance from compressed models, suggesting that larger models may not be necessary for effective sentiment analysis. The study also outlines platform-specific challenges and offers future directions for scalable sentiment classification in urban planning.", "key_contributions": ["First multi-platform sentiment analysis of the 15-minute city concept", "Benchmarking of sentiment analysis models across heterogeneous text domains", "Identification of platform-specific challenges in sentiment classification"], "limitations": "Class imbalance in news data inflated performance metrics; summarization loss was observed in Reddit; moderate challenges were noted in Twitter data.", "keywords": ["Sentiment Analysis", "15-minute city", "Transformer Models", "Urban Planning", "Machine Learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.12027", "pdf": "https://arxiv.org/pdf/2509.12027.pdf", "abs": "https://arxiv.org/abs/2509.12027", "title": "Exploring Gaze Dynamics in VR Film Education: Gender, Avatar, and the Shift Between Male and Female Perspectives", "authors": ["Zheng Wei", "Jia Sun", "Junxiang Liao", "Lik-Hang Lee", "Pan Hui", "Huamin Qu", "Wai Tong", "Xian Xu"], "categories": ["cs.HC"], "comment": "Accepted by ISMAR 2025", "summary": "In virtual reality (VR) education, especially in creative fields like film\nproduction, avatar design and narrative style extend beyond appearance and\naesthetics. This study explores how the interaction between avatar gender, the\ndominant narrative actor's gender, and the learner's gender influences film\nproduction learning in VR, focusing on gaze dynamics and gender perspectives.\nUsing a 2*2*2 experimental design, 48 participants operated avatars of\ndifferent genders and interacted with male or female-dominant narratives. The\nresults show that the consistency between the avatar and gender affects\npresence, and learners' control over the avatar is also influenced by gender\nmatching. Learners using avatars of the opposite gender reported stronger\ncontrol, suggesting gender incongruity prompted more focus on the avatar.\nAdditionally, female participants with female avatars were more likely to adopt\na \"female gaze,\" favoring soft lighting and emotional shots, while male\nparticipants with male avatars were more likely to adopt a \"male gaze,\"\nchoosing dynamic shots and high contrast. When male participants used female\navatars, they favored \"female gaze,\" while female participants with male\navatars focused on \"male gaze\". These findings advance our understanding of how\navatar design and narrative style in VR-based education influence creativity\nand the cultivation of gender perspectives, and they offer insights for\ndeveloping more inclusive and diverse VR teaching tools going forward.", "AI": {"tldr": "This study investigates the influence of avatar gender and narrative style on learning in VR film production, revealing how gender congruence affects engagement and perspective in creativity.", "motivation": "To understand how avatar gender and narrative dynamics impact learning in VR, particularly in creative domains like film production.", "method": "The study employed a 2*2*2 experimental design with 48 participants interacting with avatars of varying genders and narratives to assess gaze dynamics and control.", "result": "Results indicate that avatar-gender consistency enhances presence and control; female participants with female avatars displayed a 'female gaze', while male participants with corresponding avatars exhibited a 'male gaze'.", "conclusion": "The findings provide valuable insights for developing inclusive VR teaching tools that consider gender perspectives in educational settings.", "key_contributions": ["Analysis of gaze dynamics related to avatar and narrative gender", "Insights into gender perspectives in VR learning environments", "Recommendations for inclusive VR educational tool design"], "limitations": "", "keywords": ["virtual reality", "avatar gender", "gaze dynamics", "film production", "inclusive education"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.11444", "pdf": "https://arxiv.org/pdf/2509.11444.pdf", "abs": "https://arxiv.org/abs/2509.11444", "title": "CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media", "authors": ["Gaurab Chhetri", "Anandi Dutta", "Subasish Das"], "categories": ["cs.CL", "cs.SI"], "comment": "This is the author's preprint version of a paper accepted for\n  presentation at HICSS 59 (Hawaii International Conference on System\n  Sciences), 2026, Hawaii, USA. The final published version will appear in the\n  official conference proceedings. Conference site: https://hicss.hawaii.edu/", "summary": "The emergence of decentralized social media platforms presents new\nopportunities and challenges for real-time analysis of public discourse. This\nstudy introduces CognitiveSky, an open-source and scalable framework designed\nfor sentiment, emotion, and narrative analysis on Bluesky, a federated Twitter\nor X.com alternative. By ingesting data through Bluesky's Application\nProgramming Interface (API), CognitiveSky applies transformer-based models to\nannotate large-scale user-generated content and produces structured and\nanalyzable outputs. These summaries drive a dynamic dashboard that visualizes\nevolving patterns in emotion, activity, and conversation topics. Built entirely\non free-tier infrastructure, CognitiveSky achieves both low operational cost\nand high accessibility. While demonstrated here for monitoring mental health\ndiscourse, its modular design enables applications across domains such as\ndisinformation detection, crisis response, and civic sentiment analysis. By\nbridging large language models with decentralized networks, CognitiveSky offers\na transparent, extensible tool for computational social science in an era of\nshifting digital ecosystems.", "AI": {"tldr": "CognitiveSky is an open-source framework for analyzing sentiment, emotion, and narratives in decentralized social media, specifically designed for Bluesky.", "motivation": "To address the challenges and opportunities of analyzing public discourse on decentralized social media platforms.", "method": "CognitiveSky utilizes Bluesky's API to ingest data and applies transformer-based models to generate structured outputs for sentiment and emotion analysis, which are visualized in a dynamic dashboard.", "result": "CognitiveSky effectively monitors mental health discourse and shows promise for use in other domains like disinformation detection and crisis response.", "conclusion": "CognitiveSky represents a transparent and extensible tool for computational social science, enabling wide applications in analyzing decentralized networks.", "key_contributions": ["Introduces a scalable framework for real-time public discourse analysis on decentralized platforms.", "Demonstrates the application of transformer-based models to large-scale user content.", "Provides a dynamic visualization dashboard for emotions, activities, and topics."], "limitations": "The study is limited to the context of Bluesky and may not capture nuances across other decentralized platforms.", "keywords": ["CognitiveSky", "sentiment analysis", "decentralized social media", "transformer models", "computational social science"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2509.12049", "pdf": "https://arxiv.org/pdf/2509.12049.pdf", "abs": "https://arxiv.org/abs/2509.12049", "title": "Interaction-Driven Browsing: A Human-in-the-Loop Conceptual Framework Informed by Human Web Browsing for Browser-Using Agents", "authors": ["Hyeonggeun Yun", "Jinkyu Jang"], "categories": ["cs.HC", "cs.AI", "cs.MA"], "comment": null, "summary": "Although browser-using agents (BUAs) show promise for web tasks and\nautomation, most BUAs terminate after executing a single instruction, failing\nto support users' complex, nonlinear browsing with ambiguous goals, iterative\ndecision-making, and changing contexts. We present a human-in-the-loop (HITL)\nconceptual framework informed by theories of human web browsing behavior. The\nframework centers on an iterative loop in which the BUA proactively proposes\nnext actions and the user steers the browsing process through feedback. It also\ndistinguishes between exploration and exploitation actions, enabling users to\ncontrol the breadth and depth of their browsing. Consequently, the framework\naims to reduce users' physical and cognitive effort while preserving users'\ntraditional browsing mental model and supporting users in achieving\nsatisfactory outcomes. We illustrate how the framework operates with\nhypothetical use cases and discuss the shift from manual browsing to\ninteraction-driven browsing. We contribute a theoretically informed conceptual\nframework for BUAs.", "AI": {"tldr": "A conceptual framework for browser-using agents (BUAs) that supports complex, iterative browsing by allowing user feedback and control over browsing actions.", "motivation": "To address the shortcomings of BUAs that terminate after a single instruction, limiting their effectiveness in complex web tasks and dynamic browsing contexts.", "method": "The framework involves an iterative process where the BUA proposes actions and users provide feedback, categorizing actions into exploration and exploitation.", "result": "The framework aims to enhance user control and reduce cognitive effort while maintaining a traditional browsing mental model and achieving satisfactory outcomes.", "conclusion": "The proposed framework facilitates a shift from manual to interaction-driven browsing, improving the usability of BUAs.", "key_contributions": ["Introduces a human-in-the-loop framework for BUAs", "Differentiates between exploration and exploitation actions", "Enhances user control in complex browsing scenarios"], "limitations": "", "keywords": ["Browser-Using Agents", "Human-Computer Interaction", "Iterative Browsing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.11465", "pdf": "https://arxiv.org/pdf/2509.11465.pdf", "abs": "https://arxiv.org/abs/2509.11465", "title": "CEMTM: Contextual Embedding-based Multimodal Topic Modeling", "authors": ["Amirhossein Abaskohi", "Raymond Li", "Chuyuan Li", "Shafiq Joty", "Giuseppe Carenini"], "categories": ["cs.CL", "cs.LG"], "comment": "EMNLP 2025", "summary": "We introduce CEMTM, a context-enhanced multimodal topic model designed to\ninfer coherent and interpretable topic structures from both short and long\ndocuments containing text and images. CEMTM builds on fine-tuned large vision\nlanguage models (LVLMs) to obtain contextualized embeddings, and employs a\ndistributional attention mechanism to weight token-level contributions to topic\ninference. A reconstruction objective aligns topic-based representations with\nthe document embedding, encouraging semantic consistency across modalities.\nUnlike existing approaches, CEMTM can process multiple images per document\nwithout repeated encoding and maintains interpretability through explicit\nword-topic and document-topic distributions. Extensive experiments on six\nmultimodal benchmarks show that CEMTM consistently outperforms unimodal and\nmultimodal baselines, achieving a remarkable average LLM score of 2.61. Further\nanalysis shows its effectiveness in downstream few-shot retrieval and its\nability to capture visually grounded semantics in complex domains such as\nscientific articles.", "AI": {"tldr": "Introduction of CEMTM, a context-enhanced multimodal topic model for interpreting short and long documents with text and images.", "motivation": "The need for a model that can infer coherent and interpretable topic structures from multimodal documents more effectively.", "method": "CEMTM uses fine-tuned large vision language models to create contextual embeddings and a distributional attention mechanism for topic inference; it aligns topic-based representations with document embeddings.", "result": "CEMTM outperforms existing unimodal and multimodal approaches on six benchmarks, achieving an average LLM score of 2.61, demonstrating effectiveness in few-shot retrieval and visually grounded semantics.", "conclusion": "CEMTM enhances topic modeling by integrating text and images without repeated encoding, providing interpretability while achieving high performance across benchmarks.", "key_contributions": ["Introduction of a new multimodal topic model (CEMTM)", "Ability to process multiple images per document without repeated encoding", "Demonstrated effectiveness in few-shot retrieval and capturing visually grounded semantics."], "limitations": "", "keywords": ["multimodal topic modeling", "large vision language models", "contextual embeddings"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2509.12102", "pdf": "https://arxiv.org/pdf/2509.12102.pdf", "abs": "https://arxiv.org/abs/2509.12102", "title": "Can LLMs Address Mental Health Questions? A Comparison with Human Therapists", "authors": ["Synthia Wang", "Yuwei Cheng", "Austin Song", "Sarah Keedy", "Marc Berman", "Nick Feamster"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Limited access to mental health care has motivated the use of digital tools\nand conversational agents powered by large language models (LLMs), yet their\nquality and reception remain unclear. We present a study comparing\ntherapist-written responses to those generated by ChatGPT, Gemini, and Llama\nfor real patient questions. Text analysis showed that LLMs produced longer,\nmore readable, and lexically richer responses with a more positive tone, while\ntherapist responses were more often written in the first person. In a survey\nwith 150 users and 23 licensed therapists, participants rated LLM responses as\nclearer, more respectful, and more supportive than therapist-written answers.\nYet, both groups of participants expressed a stronger preference for human\ntherapist support. These findings highlight the promise and limitations of LLMs\nin mental health, underscoring the need for designs that balance their\ncommunicative strengths with concerns of trust, privacy, and accountability.", "AI": {"tldr": "A study comparing therapist-written responses to those generated by LLMs (ChatGPT, Gemini, Llama) in mental health care finds LLMs produce clearer and more positive responses, but users prefer human therapists.", "motivation": "To address limited access to mental health care through the use of digital tools and conversational agents powered by LLMs.", "method": "Comparison of therapist-written responses to those generated by LLMs in response to real patient questions, followed by a user survey assessing clarity and supportiveness.", "result": "LLMs produced longer, more readable responses with a positive tone; however, participants preferred human therapist support.", "conclusion": "While LLMs show promise in mental health care, concerns regarding trust, privacy, and accountability remain important.", "key_contributions": ["Comparative analysis of LLMs vs. therapist responses in mental health context.", "User preferences highlight the importance of human therapists despite LLM effectiveness.", "Findings outline the strengths and limitations of using LLMs in therapeutic settings."], "limitations": "Study based on a specific dataset of patient questions and responses; results may not generalize to all mental health scenarios.", "keywords": ["Mental Health", "Large Language Models", "User Preference", "Therapeutic Responses", "Digital Tools"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.11466", "pdf": "https://arxiv.org/pdf/2509.11466.pdf", "abs": "https://arxiv.org/abs/2509.11466", "title": "Improving LLMs' Learning for Coreference Resolution", "authors": ["Yujian Gan", "Yuan Liang", "Yanni Lin", "Juntao Yu", "Massimo Poesio"], "categories": ["cs.CL"], "comment": null, "summary": "Coreference Resolution (CR) is crucial for many NLP tasks, but existing LLMs\nstruggle with hallucination and under-performance. In this paper, we\ninvestigate the limitations of existing LLM-based approaches to CR-specifically\nthe Question-Answering (QA) Template and Document Template methods and propose\ntwo novel techniques: Reversed Training with Joint Inference and Iterative\nDocument Generation. Our experiments show that Reversed Training improves the\nQA Template method, while Iterative Document Generation eliminates\nhallucinations in the generated source text and boosts coreference resolution.\nIntegrating these methods and techniques offers an effective and robust\nsolution to LLM-based coreference resolution.", "AI": {"tldr": "This paper proposes two novel techniques for improving coreference resolution in LLMs: Reversed Training with Joint Inference and Iterative Document Generation.", "motivation": "Coreference Resolution (CR) is important for NLP tasks, but current LLMs face challenges such as hallucination and under-performance.", "method": "The authors investigate existing LLM-based approaches like QA Template and Document Template methods, followed by their proposals of Reversed Training and Iterative Document Generation to enhance CR.", "result": "Reversed Training enhances the QA Template method, while Iterative Document Generation reduces hallucinations and improves coreference resolution.", "conclusion": "The integration of the proposed methods provides a robust solution to coreference resolution in LLM applications.", "key_contributions": ["Proposed Reversed Training with Joint Inference method for CR", "Introduced Iterative Document Generation technique", "Demonstrated effectiveness in reducing hallucinations and improving CR"], "limitations": "", "keywords": ["Coreference Resolution", "NLP", "Large Language Models", "Hallucination Reduction", "Document Generation"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2509.12107", "pdf": "https://arxiv.org/pdf/2509.12107.pdf", "abs": "https://arxiv.org/abs/2509.12107", "title": "Exploring Conversational Design Choices in LLMs for Pedagogical Purposes: Socratic and Narrative Approaches for Improving Instructor's Teaching Practice", "authors": ["Si Chen", "Isabel R. Molnar", "Peiyu Li", "Adam Acunin", "Ting Hua", "Alex Ambrose", "Nitesh V. Chawla", "Ronald Metoyer"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) typically generate direct answers, yet they are\nincreasingly used as learning tools. Studying instructors' usage is critical,\ngiven their role in teaching and guiding AI adoption in education. We designed\nand evaluated TeaPT, an LLM for pedagogical purposes that supports instructors'\nprofessional development through two conversational approaches: a Socratic\napproach that uses guided questioning to foster reflection, and a Narrative\napproach that offers elaborated suggestions to extend externalized cognition.\nIn a mixed-method study with 41 higher-education instructors, the Socratic\nversion elicited greater engagement, while the Narrative version was preferred\nfor actionable guidance. Subgroup analyses further revealed that\nless-experienced, AI-optimistic instructors favored the Socratic version,\nwhereas more-experienced, AI-cautious instructors preferred the Narrative\nversion. We contribute design implications for LLMs for pedagogical purposes,\nshowing how adaptive conversational approaches can support instructors with\nvaried profiles while highlighting how AI attitudes and experience shape\ninteraction and learning.", "AI": {"tldr": "Study evaluates TeaPT, an LLM designed for pedagogical use, focusing on two conversational approaches to support instructors.", "motivation": "To understand how instructors use LLMs in education and to support their professional development through adaptive conversational tools.", "method": "A mixed-method study with 41 higher-education instructors evaluating two versions of TeaPT: Socratic and Narrative.", "result": "The Socratic version led to greater engagement, while the Narrative version was preferred for actionable guidance, with preferences varying by instructor experience and AI attitude.", "conclusion": "Adaptive conversational approaches can enhance LLM design for varied instructor profiles and highlight the influence of AI attitudes on learning.", "key_contributions": ["Introduction of TeaPT as a pedagogical LLM", "Comparison of Socratic and Narrative approaches", "Insights on instructor engagement based on experience and AI attitudes"], "limitations": "Study limited to 41 instructors; results may not generalize across all educational contexts.", "keywords": ["Large Language Models", "Pedagogy", "Instructor Development", "Conversational Approaches", "AI in Education"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.11492", "pdf": "https://arxiv.org/pdf/2509.11492.pdf", "abs": "https://arxiv.org/abs/2509.11492", "title": "ClaimIQ at CheckThat! 2025: Comparing Prompted and Fine-Tuned Language Models for Verifying Numerical Claims", "authors": ["Anirban Saha Anik", "Md Fahimul Kabir Chowdhury", "Andrew Wyckoff", "Sagnik Ray Choudhury"], "categories": ["cs.CL", "cs.AI"], "comment": "Notebook for the CheckThat! Lab at CLEF 2025", "summary": "This paper presents our system for Task 3 of the CLEF 2025 CheckThat! Lab,\nwhich focuses on verifying numerical and temporal claims using retrieved\nevidence. We explore two complementary approaches: zero-shot prompting with\ninstruction-tuned large language models (LLMs) and supervised fine-tuning using\nparameter-efficient LoRA. To enhance evidence quality, we investigate several\nselection strategies, including full-document input and top-k sentence\nfiltering using BM25 and MiniLM. Our best-performing model LLaMA fine-tuned\nwith LoRA achieves strong performance on the English validation set. However, a\nnotable drop in the test set highlights a generalization challenge. These\nfindings underscore the importance of evidence granularity and model adaptation\nfor robust numerical fact verification.", "AI": {"tldr": "This paper discusses a system for verifying numerical and temporal claims using LLMs and various evidence selection strategies, achieving significant findings in the CLEF 2025 CheckThat! Lab.", "motivation": "To improve the verification of numerical and temporal claims through efficient evidence retrieval and LLM adaptation.", "method": "The system employs zero-shot prompting with instruction-tuned LLMs and supervised fine-tuning using LoRA. It investigates evidence selection strategies such as full-document input and top-k sentence filtering using BM25 and MiniLM.", "result": "The LLaMA model fine-tuned with LoRA performed well on the English validation set, but there was a decline in performance on the test set, indicating a challenge in generalization.", "conclusion": "The study highlights the critical role of evidence quality and model adaptation in numerical fact verification tasks.", "key_contributions": ["Exploration of zero-shot prompting with instruction-tuned LLMs for claim verification.", "Use of LoRA for supervised fine-tuning to enhance model performance.", "In-depth analysis of evidence selection strategies to improve verification accuracy."], "limitations": "Notable drop in the model's performance on the test set reveals generalization challenges.", "keywords": ["LLM", "Numerical verification", "Temporal claims", "LoRA", "Evidence selection"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.12140", "pdf": "https://arxiv.org/pdf/2509.12140.pdf", "abs": "https://arxiv.org/abs/2509.12140", "title": "Worker Discretion Advised: Co-designing Risk Disclosure in Crowdsourced Responsible AI (RAI) Content Work", "authors": ["Alice Qian", "Ziqi Yang", "Ryland Shaw", "Jina Suh", "Laura Dabbish", "Hong Shen"], "categories": ["cs.HC", "cs.CY"], "comment": "Under review at CHI 2026", "summary": "Responsible AI (RAI) content work, such as annotation, moderation, or red\nteaming for AI safety, often exposes crowd workers to potentially harmful\ncontent. While prior work has underscored the importance of communicating\nwell-being risk to employed content moderators, designing effective disclosure\nmechanisms for crowd workers while balancing worker protection with the needs\nof task designers and platforms remains largely unexamined. To address this\ngap, we conducted co-design sessions with 29 task designers, workers, and\nplatform representatives. We investigated task designer preferences for support\nin disclosing tasks, worker preferences for receiving risk disclosure warnings,\nand how platform stakeholders envision their role in shaping risk disclosure\npractices. We identify design tensions and map the sociotechnical tradeoffs\nthat shape disclosure practices. We contribute design recommendations and\nfeature concepts for risk disclosure mechanisms in the context of RAI content\nwork.", "AI": {"tldr": "This paper explores the design of risk disclosure mechanisms for crowd workers in Responsible AI content work, balancing worker protection and task designer needs.", "motivation": "The work examines the exposure of crowd workers to harmful content in Responsible AI tasks and the lack of effective disclosure mechanisms.", "method": "Co-design sessions with 29 task designers, workers, and platform representatives to gather preferences and insights on risk disclosure practices.", "result": "Identified design tensions and sociotechnical trade-offs affecting risk disclosure, along with recommendations for mechanisms to enhance worker protection.", "conclusion": "The paper provides actionable design recommendations to improve risk disclosure practices in the context of Responsible AI content work.", "key_contributions": ["Design recommendations for risk disclosure mechanisms", "Insights on worker and designer preferences for disclosures", "Identification of sociotechnical trade-offs in disclosure practices"], "limitations": "", "keywords": ["Responsible AI", "risk disclosure", "crowd work", "task design", "HCI"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.11496", "pdf": "https://arxiv.org/pdf/2509.11496.pdf", "abs": "https://arxiv.org/abs/2509.11496", "title": "AKCIT-FN at CheckThat! 2025: Switching Fine-Tuned SLMs and LLM Prompting for Multilingual Claim Normalization", "authors": ["Fabrycio Leite Nakano Almada", "Kauan Divino Pouso Mariano", "Maykon Adriell Dutra", "Victor Emanuel da Silva Monteiro", "Juliana Resplande Sant'Anna Gomes", "Arlindo Rodrigues Galvão Filho", "Anderson da Silva Soares"], "categories": ["cs.CL"], "comment": "15 pages, 2 figures", "summary": "Claim normalization, the transformation of informal social media posts into\nconcise, self-contained statements, is a crucial step in automated\nfact-checking pipelines. This paper details our submission to the CLEF-2025\nCheckThat! Task~2, which challenges systems to perform claim normalization\nacross twenty languages, divided into thirteen supervised (high-resource) and\nseven zero-shot (no training data) tracks.\n  Our approach, leveraging fine-tuned Small Language Models (SLMs) for\nsupervised languages and Large Language Model (LLM) prompting for zero-shot\nscenarios, achieved podium positions (top three) in fifteen of the twenty\nlanguages. Notably, this included second-place rankings in eight languages,\nfive of which were among the seven designated zero-shot languages, underscoring\nthe effectiveness of our LLM-based zero-shot strategy. For Portuguese, our\ninitial development language, our system achieved an average METEOR score of\n0.5290, ranking third. All implementation artifacts, including inference,\ntraining, evaluation scripts, and prompt configurations, are publicly available\nat https://github.com/ju-resplande/checkthat2025_normalization.", "AI": {"tldr": "This paper presents a method for claim normalization in fact-checking using fine-tuned Small Language Models and Large Language Model prompting, achieving top results in a multilingual competition.", "motivation": "Claim normalization is essential for transforming informal social media posts into concise statements for automated fact-checking, especially across multiple languages.", "method": "The authors used fine-tuned Small Language Models for supervised languages and applied Large Language Model prompting for zero-shot claim normalization across twenty languages.", "result": "Achieved podium positions in fifteen out of twenty languages, including second-place rankings in eight languages, demonstrating the effectiveness of the zero-shot strategy.", "conclusion": "The results highlight the potential of LLM-based approaches in achieving high performance in multilingual claim normalization, and all implementation artifacts are publicly available.", "key_contributions": ["Successful application of LLMs for multilingual claim normalization", "High rankings in a competitive task across multiple languages", "Public availability of implementation artifacts for replication and further research"], "limitations": "", "keywords": ["claim normalization", "fact-checking", "large language models", "zero-shot learning", "multilingual"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.12152", "pdf": "https://arxiv.org/pdf/2509.12152.pdf", "abs": "https://arxiv.org/abs/2509.12152", "title": "Beyond PII: How Users Attempt to Estimate and Mitigate Implicit LLM Inference", "authors": ["Synthia Wang", "Sai Teja Peddinti", "Nina Taft", "Nick Feamster"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) such as ChatGPT can infer personal attributes\nfrom seemingly innocuous text, raising privacy risks beyond memorized data\nleakage. While prior work has demonstrated these risks, little is known about\nhow users estimate and respond. We conducted a survey with 240 U.S.\nparticipants who judged text snippets for inference risks, reported concern\nlevels, and attempted rewrites to block inference. We compared their rewrites\nwith those generated by ChatGPT and Rescriber, a state-of-the-art sanitization\ntool. Results show that participants struggled to anticipate inference,\nperforming a little better than chance. User rewrites were effective in just\n28\\% of cases - better than Rescriber but worse than ChatGPT. We examined our\nparticipants' rewriting strategies, and observed that while paraphrasing was\nthe most common strategy it is also the least effective; instead abstraction\nand adding ambiguity were more successful. Our work highlights the importance\nof inference-aware design in LLM interactions.", "AI": {"tldr": "This study investigates how users estimate and respond to privacy risks from personal attributes inferred by Large Language Models through text.", "motivation": "Concerns over privacy risks associated with LLMs inferring personal attributes from text, and the need to understand user responses to these risks.", "method": "A survey with 240 U.S. participants who evaluated text snippets for inference risks, reported their concern levels, and attempted rewrites to mitigate these risks. Their rewrites were compared to those generated by ChatGPT and the Rescriber sanitization tool.", "result": "Participants were able to recognize inference risks slightly better than chance, with their rewrites effective in only 28% of cases, outperforming Rescriber but not ChatGPT. Common rewriting strategies included paraphrasing, which was the least effective, while abstraction and adding ambiguity yielded better results.", "conclusion": "The study underscores the need for inference-aware design in interactions with LLMs to enhance user privacy.", "key_contributions": ["Identified user misconceptions regarding inference risks from LLMs.", "Showed comparative effectiveness of user rewrites versus AI-generated rewrites.", "Highlighted the importance of specific rewriting strategies to enhance privacy."], "limitations": "The study only included U.S. participants, which may impact the generalizability of the results.", "keywords": ["Large Language Models", "privacy risks", "user behavior", "inference awareness", "text sanitization"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2509.11498", "pdf": "https://arxiv.org/pdf/2509.11498.pdf", "abs": "https://arxiv.org/abs/2509.11498", "title": "DeDisCo at the DISRPT 2025 Shared Task: A System for Discourse Relation Classification", "authors": ["Zhuoxuan Ju", "Jingni Wu", "Abhishek Purushothama", "Amir Zeldes"], "categories": ["cs.CL"], "comment": "System submission for the DISRPT 2025 - Shared Task on Discourse\n  Relation Parsing and Treebanking In conjunction with CODI-CRAC & EMNLP 2025.\n  1st place in Task 3: relation classification", "summary": "This paper presents DeDisCo, Georgetown University's entry in the DISRPT 2025\nshared task on discourse relation classification. We test two approaches, using\nan mt5-based encoder and a decoder based approach using the openly available\nQwen model. We also experiment on training with augmented dataset for\nlow-resource languages using matched data translated automatically from\nEnglish, as well as using some additional linguistic features inspired by\nentries in previous editions of the Shared Task. Our system achieves a\nmacro-accuracy score of 71.28, and we provide some interpretation and error\nanalysis for our results.", "AI": {"tldr": "This paper details Georgetown University's DeDisCo system for discourse relation classification in the DISRPT 2025 shared task, achieving a macro-accuracy score of 71.28.", "motivation": "The paper aims to showcase techniques for discourse relation classification, particularly for low-resource languages, by employing advanced models and data augmentation.", "method": "Two approaches were tested: one using an mt5-based encoder and another using a decoder-based approach with the Qwen model, supplemented with linguistic features.", "result": "The system reached a macro-accuracy score of 71.28 on the classification task while allowing for error interpretation and analysis.", "conclusion": "The findings suggest that both model types and the use of augmented datasets can significantly enhance performance in discourse relation classification tasks.", "key_contributions": ["Introduction of the DeDisCo system for discourse relation classification.", "Performance benchmarking with a macro-accuracy score of 71.28.", "Use of augmented datasets for low-resource languages."], "limitations": "The study is constrained by the focus on discourse relation classification without exploring broader applications or variants.", "keywords": ["discourse relation classification", "mt5 model", "Qwen model"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2509.12153", "pdf": "https://arxiv.org/pdf/2509.12153.pdf", "abs": "https://arxiv.org/abs/2509.12153", "title": "You Are Not Alone: Designing Body Doubling for ADHD in Virtual Reality", "authors": ["Zinat Ara", "Imtiaz Bin Rahim", "Puqi Zhou", "Liuchuan Yu", "Behzad Esmaeili", "Lap-Fai Yu", "Sungsoo Ray Hong"], "categories": ["cs.HC"], "comment": null, "summary": "Adults with Attention Deficit Hyperactivity Disorder (ADHD) experience\nchallenges sustaining attention in the workplace. Body doubling, the concept of\nworking alongside another person, has been proposed as a productivity aid for\nADHD and other neurodivergent populations (NDs). However, prior work found no\nconclusive effectiveness and noted NDs' discomfort with social presence. This\nwork investigates body doubling as an ADHD centered productivity strategy in\nconstruction tasks. In Study 1, we explored challenges ADHD workers face in\nconstruction and identified design insights. In Study 2, we implemented a\nvirtual reality bricklaying task under three conditions: (C1) alone, (C2) with\na human body double, and (C3) with an AI body double. Results from 12\nparticipants show they finished tasks faster and perceived greater accuracy and\nsustained attention in C2 and C3 compared to C1. While body doubling was\nclearly preferred, opinions diverged between conditions. Our findings verify\nits effect and offer design implications for future interventions.", "AI": {"tldr": "This study explores the effectiveness of body doubling as a productivity strategy for adults with ADHD in construction tasks, comparing human and AI body doubles.", "motivation": "Adults with ADHD face challenges in sustaining attention in the workplace, and body doubling has been suggested as a potential aid, though previous studies yielded inconclusive results.", "method": "The research included two studies; the first identified challenges faced by ADHD workers in construction, while the second implemented a virtual reality bricklaying task under three conditions: alone, with a human body double, and with an AI body double.", "result": "Participants completed tasks faster and reported greater accuracy and sustained attention in the body doubling conditions (human and AI) compared to working alone.", "conclusion": "Body doubling positively affects productivity for individuals with ADHD, with preferences varying between the human and AI body doubles, highlighting important design implications for future interventions.", "key_contributions": ["Demonstrated the effectiveness of body doubling in improving productivity for ADHD workers.", "Provided comparative insights into the preferences for human vs. AI body doubling in a task setting.", "Identified specific design insights for interventions targeting ADHD and neurodivergent populations."], "limitations": "The study included a small sample size (12 participants) which may limit generalizability.", "keywords": ["ADHD", "body doubling", "neurodiversity", "virtual reality", "productivity"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.11513", "pdf": "https://arxiv.org/pdf/2509.11513.pdf", "abs": "https://arxiv.org/abs/2509.11513", "title": "Unsupervised Candidate Ranking for Lexical Substitution via Holistic Sentence Semantics", "authors": ["Zhongyang Hu", "Naijie Gu", "Xiangzhi Tao", "Tianhui Gu", "Yibing Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A key subtask in lexical substitution is ranking the given candidate words. A\ncommon approach is to replace the target word with a candidate in the original\nsentence and feed the modified sentence into a model to capture semantic\ndifferences before and after substitution. However, effectively modeling the\nbidirectional influence of candidate substitution on both the target word and\nits context remains challenging. Existing methods often focus solely on\nsemantic changes at the target position or rely on parameter tuning over\nmultiple evaluation metrics, making it difficult to accurately characterize\nsemantic variation. To address this, we investigate two approaches: one based\non attention weights and another leveraging the more interpretable integrated\ngradients method, both designed to measure the influence of context tokens on\nthe target token and to rank candidates by incorporating semantic similarity\nbetween the original and substituted sentences. Experiments on the LS07 and\nSWORDS datasets demonstrate that both approaches improve ranking performance.", "AI": {"tldr": "The paper explores improved methods for ranking candidate words in lexical substitution using attention weights and integrated gradients to better account for contextual influence on target words.", "motivation": "To address the challenges in ranking candidate words for lexical substitution caused by existing methods that focus on either the target position or require extensive parameter tuning.", "method": "The study investigates two novel approaches: one based on attention weights and the other using integrated gradients, to assess the influence of context on target words and rank candidate substitutions by semantic similarity.", "result": "Experiments on LS07 and SWORDS datasets showed that the proposed methods enhanced ranking performance compared to traditional approaches.", "conclusion": "The new methods offer improved insights into the bidirectional influence of context on word substitution, leading to better semantic understanding in lexical tasks.", "key_contributions": ["Introduction of attention weights for ranking in lexical substitution", "Utilization of integrated gradients for explainability in candidate ranking", "Demonstrated performance improvement on standard datasets (LS07 and SWORDS)"], "limitations": "", "keywords": ["lexical substitution", "candidate ranking", "semantic similarity", "attention mechanisms", "integrated gradients"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.11514", "pdf": "https://arxiv.org/pdf/2509.11514.pdf", "abs": "https://arxiv.org/abs/2509.11514", "title": "LVLMs are Bad at Overhearing Human Referential Communication", "authors": ["Zhengxiang Wang", "Weiling Li", "Panagiotis Kaliosis", "Owen Rambow", "Susan E. Brennan"], "categories": ["cs.CL"], "comment": "EMNLP 2025 (Main)", "summary": "During spontaneous conversations, speakers collaborate on novel referring\nexpressions, which they can then re-use in subsequent conversations.\nUnderstanding such referring expressions is an important ability for an\nembodied agent, so that it can carry out tasks in the real world. This requires\nintegrating and understanding language, vision, and conversational interaction.\nWe study the capabilities of seven state-of-the-art Large Vision Language\nModels (LVLMs) as overhearers to a corpus of spontaneous conversations between\npairs of human discourse participants engaged in a collaborative\nobject-matching task. We find that such a task remains challenging for current\nLVLMs and they all fail to show a consistent performance improvement as they\noverhear more conversations from the same discourse participants repeating the\nsame task for multiple rounds. We release our corpus and code for\nreproducibility and to facilitate future research.", "AI": {"tldr": "This paper investigates the ability of Large Vision Language Models (LVLMs) to understand and utilize unique referring expressions developed during spontaneous conversations.", "motivation": "To understand how LVLMs can effectively carry out real-world tasks by integrating language, vision, and conversational interaction, particularly through the use of referring expressions created during collaborative tasks.", "method": "Seven state-of-the-art LVLMs were tested as overhearers in a corpus of spontaneous conversations focusing on a collaborative object-matching task.", "result": "All LVLMs demonstrated difficulty in consistently improving performance despite overhearing multiple rounds of conversations on the same task.", "conclusion": "Current LVLMs struggle with the task of understanding novel referring expressions, indicating a gap in their capability for real-world applications.", "key_contributions": ["Compiled and analyzed a corpus of spontaneous conversations for research on LVLMs.", "Provided insights into the limitations of LVLMs in understanding collaborative discourse.", "Released corpus and code to promote reproducibility and future research."], "limitations": "LVLMs showed no consistent performance improvement, revealing limitations in understanding in-context language adaptations.", "keywords": ["Large Vision Language Models", "collaborative discourse", "referring expressions"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.11517", "pdf": "https://arxiv.org/pdf/2509.11517.pdf", "abs": "https://arxiv.org/abs/2509.11517", "title": "PeruMedQA: Benchmarking Large Language Models (LLMs) on Peruvian Medical Exams -- Dataset Construction and Evaluation", "authors": ["Rodrigo M. Carrillo-Larco", "Jesus Lovón Melgarejo", "Manuel Castillo-Cara", "Gusseppe Bravo-Rocca"], "categories": ["cs.CL", "cs.LG"], "comment": "https://github.com/rodrigo-carrillo/PeruMedQA", "summary": "BACKGROUND: Medical large language models (LLMS) have demonstrated remarkable\nperformance in answering medical examinations. However, the extent to which\nthis high performance is transferable to medical questions in Spanish and from\na Latin American country remains unexplored. This knowledge is crucial as\nLLM-based medical applications gain traction in Latin America. AIMS: to build a\ndataset of questions from medical examinations taken by Peruvian physicians\npursuing specialty training; to fine-tune a LLM on this dataset; to evaluate\nand compare the performance in terms of accuracy between vanilla LLMs and the\nfine-tuned LLM. METHODS: We curated PeruMedQA, a multiple-choice\nquestion-answering (MCQA) datasets containing 8,380 questions spanning 12\nmedical domains (2018-2025). We selected eight medical LLMs including\nmedgemma-4b-it and medgemma-27b-text-it, and developed zero-shot task-specific\nprompts to answer the questions appropriately. We employed parameter-efficient\nfine tuning (PEFT)and low-rant adaptation (LoRA) to fine-tune medgemma-4b-it\nutilizing all questions except those from 2025 (test set). RESULTS:\nmedgemma-27b-text-it outperformed all other models, achieving a proportion of\ncorrect answers exceeding 90% in several instances. LLMs with <10 billion\nparameters exhibited <60% of correct answers, while some exams yielded results\n<50%. The fine-tuned version of medgemma-4b-it emerged victorious agains all\nLLMs with <10 billion parameters and rivaled a LLM with 70 billion parameters\nacross various examinations. CONCLUSIONS: For medical AI application and\nresearch that require knowledge bases from Spanish-speaking countries and those\nexhibiting similar epidemiological profiles to Peru's, interested parties\nshould utilize medgemma-27b-text-it or a fine-tuned version of medgemma-4b-it.", "AI": {"tldr": "The study explores the performance of medical LLMs on Spanish medical exams in Peru, developing a fine-tuned model to enhance accuracy in answering medical questions.", "motivation": "To investigate the performance transferability of medical LLMs to Spanish questions in a Latin American context, specifically Peru.", "method": "Developed the PeruMedQA dataset with 8,380 questions for fine-tuning LLMs and evaluated performance using zero-shot task-specific prompts.", "result": "medgemma-27b-text-it surpassed all other models, showing over 90% correct answers, while LLMs with less than 10 billion parameters performed poorly with less than 60% accuracy.", "conclusion": "For medical AI applications in Spanish-speaking countries, the use of medgemma-27b-text-it or a fine-tuned medgemma-4b-it is recommended.", "key_contributions": ["Creation of the PeruMedQA dataset for medical questions in Spanish", "Demonstration of the performance of fine-tuned LLM on medical examinations", "Insight into the performance differences among various LLMs based on their parameter sizes"], "limitations": "", "keywords": ["medical LLMs", "Spanish", "fine-tuning", "PeruMedQA", "health informatics"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2509.11534", "pdf": "https://arxiv.org/pdf/2509.11534.pdf", "abs": "https://arxiv.org/abs/2509.11534", "title": "On the Distinctive Co-occurrence Characteristics of Antonymy", "authors": ["Zhihan Cao", "Hiroaki Yamada", "Takenobu Tokunaga"], "categories": ["cs.CL"], "comment": "Accepted by *SEM 2025", "summary": "Antonymy has long received particular attention in lexical semantics.\nPrevious studies have shown that antonym pairs frequently co-occur in text,\nacross genres and parts of speech, more often than would be expected by chance.\nHowever, whether this co-occurrence pattern is distinctive of antonymy remains\nunclear, due to a lack of comparison with other semantic relations. This work\nfills the gap by comparing antonymy with three other relations across parts of\nspeech using robust co-occurrence metrics. We find that antonymy is distinctive\nin three respects: antonym pairs co-occur with high strength, in a preferred\nlinear order, and within short spans. All results are available online.", "AI": {"tldr": "This paper investigates the unique co-occurrence patterns of antonym pairs compared to other semantic relations in text.", "motivation": "To clarify whether the co-occurrence pattern of antonyms is distinctive by comparing it with other semantic relations.", "method": "The study utilizes robust co-occurrence metrics across parts of speech to analyze antonymy and three other semantic relations.", "result": "Antonym pairs exhibit distinctive co-occurrence patterns, showing high strength, preferred linear ordering, and short span co-occurrences.", "conclusion": "The findings indicate that antonymy is unique compared to other semantic relations, enhancing our understanding of lexical semantics.", "key_contributions": ["Establishes co-occurrence metrics for antonyms compared to other semantic relations.", "Demonstrates that antonyms co-occur in distinct ways not seen with other relationships.", "Provides data available online for further research."], "limitations": "", "keywords": ["antonymy", "lexical semantics", "co-occurrence", "semantic relations", "text analysis"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2509.10833", "pdf": "https://arxiv.org/pdf/2509.10833.pdf", "abs": "https://arxiv.org/abs/2509.10833", "title": "Towards Automated Error Discovery: A Study in Conversational AI", "authors": ["Dominic Petrak", "Thy Thy Tran", "Iryna Gurevych"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "Accepted to EMNLP 2025 main conference", "summary": "Although LLM-based conversational agents demonstrate strong fluency and\ncoherence, they still produce undesirable behaviors (errors) that are\nchallenging to prevent from reaching users during deployment. Recent research\nleverages large language models (LLMs) to detect errors and guide\nresponse-generation models toward improvement. However, current LLMs struggle\nto identify errors not explicitly specified in their instructions, such as\nthose arising from updates to the response-generation model or shifts in user\nbehavior. In this work, we introduce Automated Error Discovery, a framework for\ndetecting and defining errors in conversational AI, and propose SEEED (Soft\nClustering Extended Encoder-Based Error Detection), as an encoder-based\napproach to its implementation. We enhance the Soft Nearest Neighbor Loss by\namplifying distance weighting for negative samples and introduce Label-Based\nSample Ranking to select highly contrastive examples for better representation\nlearning. SEEED outperforms adapted baselines -- including GPT-4o and Phi-4 --\nacross multiple error-annotated dialogue datasets, improving the accuracy for\ndetecting unknown errors by up to 8 points and demonstrating strong\ngeneralization to unknown intent detection.", "AI": {"tldr": "A novel framework for detecting and defining errors in conversational AI, enhancing the performance of LLMs.", "motivation": "LLMs in conversational agents produce undesirable behaviors that are difficult to prevent, necessitating improved error detection mechanisms.", "method": "Introducing a framework called Automated Error Discovery alongside a novel encoder-based approach SEEED, which utilizes enhanced Soft Nearest Neighbor Loss and Label-Based Sample Ranking for better error recognition.", "result": "SEEED significantly outperforms baselines like GPT-4o and Phi-4 in identifying unknown errors, improving accuracy by up to 8 points across multiple datasets.", "conclusion": "The proposed methods improve error detection in LLMs and show strong generalization capabilities for understanding user intent.", "key_contributions": ["Introduction of Automated Error Discovery framework.", "Development of SEEED for improved error detection in LLMs.", "Demonstrated improved accuracy in detecting unknown errors in dialogue datasets."], "limitations": "", "keywords": ["Automated Error Discovery", "LLM", "Conversational AI", "Error Detection", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.11536", "pdf": "https://arxiv.org/pdf/2509.11536.pdf", "abs": "https://arxiv.org/abs/2509.11536", "title": "HARP: Hallucination Detection via Reasoning Subspace Projection", "authors": ["Junjie Hu", "Gang Tu", "ShengYu Cheng", "Jinxin Li", "Jinting Wang", "Rui Chen", "Zhilong Zhou", "Dongbo Shan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hallucinations in Large Language Models (LLMs) pose a major barrier to their\nreliable use in critical decision-making. Although existing hallucination\ndetection methods have improved accuracy, they still struggle with\ndisentangling semantic and reasoning information and maintaining robustness. To\naddress these challenges, we propose HARP (Hallucination detection via\nreasoning subspace projection), a novel hallucination detection framework. HARP\nestablishes that the hidden state space of LLMs can be decomposed into a direct\nsum of a semantic subspace and a reasoning subspace, where the former encodes\nlinguistic expression and the latter captures internal reasoning processes.\nMoreover, we demonstrate that the Unembedding layer can disentangle these\nsubspaces, and by applying Singular Value Decomposition (SVD) to its\nparameters, the basis vectors spanning the semantic and reasoning subspaces are\nobtained. Finally, HARP projects hidden states onto the basis vectors of the\nreasoning subspace, and the resulting projections are then used as input\nfeatures for hallucination detection in LLMs. By using these projections, HARP\nreduces the dimension of the feature to approximately 5% of the original,\nfilters out most noise, and achieves enhanced robustness. Experiments across\nmultiple datasets show that HARP achieves state-of-the-art hallucination\ndetection performance; in particular, it achieves an AUROC of 92.8% on\nTriviaQA, outperforming the previous best method by 7.5%.", "AI": {"tldr": "HARP is a framework for detecting hallucinations in LLMs by projecting hidden states into reasoning subspaces, achieving state-of-the-art performance.", "motivation": "To improve the reliability of Large Language Models in critical decision-making by addressing hallucination detection challenges.", "method": "HARP decomposes the hidden state space of LLMs into semantic and reasoning subspaces, uses the Unembedding layer to disentangle them, and applies SVD for basis vector extraction. The hidden states are projected onto the reasoning subspace for hallucination detection.", "result": "HARP reduces feature dimensions to about 5% of the original while filtering noise, achieving an AUROC of 92.8% on TriviaQA, exceeding previous detection methods.", "conclusion": "HARP shows enhanced robustness and performance in hallucination detection, making LLMs more reliable for decision-making tasks.", "key_contributions": ["Novel hallucination detection framework (HARP)", "Decomposing LLM hidden states into semantic and reasoning subspaces", "State-of-the-art performance in hallucination detection"], "limitations": "", "keywords": ["hallucination detection", "Large Language Models", "reasoning subspace"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2509.11552", "pdf": "https://arxiv.org/pdf/2509.11552.pdf", "abs": "https://arxiv.org/abs/2509.11552", "title": "HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with Hierarchical Chunking", "authors": ["Wensheng Lu", "Keyu Chen", "Ruizhi Qiao", "Xing Sun"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 5 figures, 6 tables", "summary": "Retrieval-Augmented Generation (RAG) enhances the response capabilities of\nlanguage models by integrating external knowledge sources. However, document\nchunking as an important part of RAG system often lacks effective evaluation\ntools. This paper first analyzes why existing RAG evaluation benchmarks are\ninadequate for assessing document chunking quality, specifically due to\nevidence sparsity. Based on this conclusion, we propose HiCBench, which\nincludes manually annotated multi-level document chunking points, synthesized\nevidence-dense quetion answer(QA) pairs, and their corresponding evidence\nsources. Additionally, we introduce the HiChunk framework, a multi-level\ndocument structuring framework based on fine-tuned LLMs, combined with the\nAuto-Merge retrieval algorithm to improve retrieval quality. Experiments\ndemonstrate that HiCBench effectively evaluates the impact of different\nchunking methods across the entire RAG pipeline. Moreover, HiChunk achieves\nbetter chunking quality within reasonable time consumption, thereby enhancing\nthe overall performance of RAG systems.", "AI": {"tldr": "This paper introduces HiCBench and HiChunk, tools for evaluating and improving document chunking in Retrieval-Augmented Generation (RAG) systems, enhancing retrieval quality and performance based on fine-tuned LLMs.", "motivation": "Existing evaluation tools for document chunking in RAG systems are inadequate due to evidence sparsity, necessitating better methods for assessing chunking quality.", "method": "The authors propose HiCBench, a benchmark with annotated chunking points and synthesized QA pairs, and HiChunk, a framework utilizing fine-tuned LLMs and Auto-Merge retrieval to improve chunking.", "result": "Experiments show that HiCBench effectively evaluates different chunking methods and that HiChunk enhances chunking quality without significant time costs, improving overall RAG performance.", "conclusion": "The introduction of HiCBench and HiChunk presents new methodologies to evaluate and enhance document chunking in RAG systems, impacting retrieval quality positively.", "key_contributions": ["Introduction of HiCBench for evaluating chunking quality in RAG systems", "Development of the HiChunk framework for improved document structuring using LLMs", "Demonstration of the efficacy of HiCBench in assessing the impact of chunking methods."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "document chunking", "evaluation benchmark", "language models", "health informatics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.11569", "pdf": "https://arxiv.org/pdf/2509.11569.pdf", "abs": "https://arxiv.org/abs/2509.11569", "title": "D$^2$HScore: Reasoning-Aware Hallucination Detection via Semantic Breadth and Depth Analysis in LLMs", "authors": ["Yue Ding", "Xiaofang Zhu", "Tianze Xia", "Junfei Wu", "Xinlong Chen", "Qiang Liu", "Liang Wang"], "categories": ["cs.CL"], "comment": "under review", "summary": "Although large Language Models (LLMs) have achieved remarkable success, their\npractical application is often hindered by the generation of non-factual\ncontent, which is called \"hallucination\". Ensuring the reliability of LLMs'\noutputs is a critical challenge, particularly in high-stakes domains such as\nfinance, security, and healthcare. In this work, we revisit hallucination\ndetection from the perspective of model architecture and generation dynamics.\nLeveraging the multi-layer structure and autoregressive decoding process of\nLLMs, we decompose hallucination signals into two complementary dimensions: the\nsemantic breadth of token representations within each layer, and the semantic\ndepth of core concepts as they evolve across layers. Based on this insight, we\npropose \\textbf{D$^2$HScore (Dispersion and Drift-based Hallucination Score)},\na training-free and label-free framework that jointly measures: (1)\n\\textbf{Intra-Layer Dispersion}, which quantifies the semantic diversity of\ntoken representations within each layer; and (2) \\textbf{Inter-Layer Drift},\nwhich tracks the progressive transformation of key token representations across\nlayers. To ensure drift reflects the evolution of meaningful semantics rather\nthan noisy or redundant tokens, we guide token selection using attention\nsignals. By capturing both the horizontal and vertical dynamics of\nrepresentation during inference, D$^2$HScore provides an interpretable and\nlightweight proxy for hallucination detection. Extensive experiments across\nfive open-source LLMs and five widely used benchmarks demonstrate that\nD$^2$HScore consistently outperforms existing training-free baselines.", "AI": {"tldr": "The paper presents D$^2$HScore, a framework for detecting hallucinations in large language models by analyzing token representations within and across layers.", "motivation": "Address the challenge of non-factual content generation (hallucination) in large language models (LLMs), especially in critical domains like healthcare.", "method": "The study decomposes hallucination signals into intra-layer dispersion and inter-layer drift, using a training-free and label-free approach that leverages the multi-layer structure of LLMs.", "result": "D$^2$HScore consistently outperforms existing training-free methods across multiple LLMs and benchmarks.", "conclusion": "D$^2$HScore provides a lightweight and interpretable method for hallucination detection, indicating its potential utility in high-stakes applications.", "key_contributions": ["Introduction of D$^2$HScore for hallucination detection", "Decomposition of hallucination signs into intra-layer and inter-layer signals", "Validation of the framework across multiple models and benchmarks"], "limitations": "", "keywords": ["hallucination detection", "large language models", "healthcare", "semantic representation", "D$^2$HScore"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.11570", "pdf": "https://arxiv.org/pdf/2509.11570.pdf", "abs": "https://arxiv.org/abs/2509.11570", "title": "Bhaasha, Bhasa, Zaban: A Survey for Low-Resourced Languages in South Asia -- Current Stage and Challenges", "authors": ["Sampoorna Poria", "Xiaolei Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Rapid developments of large language models have revolutionized many NLP\ntasks for English data. Unfortunately, the models and their evaluations for\nlow-resource languages are being overlooked, especially for languages in South\nAsia. Although there are more than 650 languages in South Asia, many of them\neither have very limited computational resources or are missing from existing\nlanguage models. Thus, a concrete question to be answered is: Can we assess the\ncurrent stage and challenges to inform our NLP community and facilitate model\ndevelopments for South Asian languages? In this survey, we have comprehensively\nexamined current efforts and challenges of NLP models for South Asian languages\nby retrieving studies since 2020, with a focus on transformer-based models,\nsuch as BERT, T5, & GPT. We present advances and gaps across 3 essential\naspects: data, models, & tasks, such as available data sources, fine-tuning\nstrategies, & domain applications. Our findings highlight substantial issues,\nincluding missing data in critical domains (e.g., health), code-mixing, and\nlack of standardized evaluation benchmarks. Our survey aims to raise awareness\nwithin the NLP community for more targeted data curation, unify benchmarks\ntailored to cultural and linguistic nuances of South Asia, and encourage an\nequitable representation of South Asian languages. The complete list of\nresources is available at: https://github.com/trust-nlp/LM4SouthAsia-Survey.", "AI": {"tldr": "This survey examines the current state and challenges of NLP models for South Asian languages, emphasizing transformer-based models and identifying significant issues such as data gaps and evaluation standards.", "motivation": "To assess the current stage and challenges in NLP for low-resource South Asian languages and inform the community for better model development.", "method": "Comprehensive survey of studies since 2020 focusing on transformer-based models, examining aspects such as data sources, fine-tuning strategies, and domain applications.", "result": "Identified substantial issues like missing health data, code-mixing challenges, and the absence of unified evaluation benchmarks for South Asian languages.", "conclusion": "The survey calls for targeted data curation and standardized evaluation benchmarks to improve NLP resources for South Asia.", "key_contributions": ["Comprehensive overview of NLP challenges for South Asian languages", "Identification of missing data in critical domains like health", "Recommendations for standardized benchmarks and targeted data curation"], "limitations": "", "keywords": ["NLP", "South Asian languages", "transformer models", "data curation", "evaluation benchmarks"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.11868", "pdf": "https://arxiv.org/pdf/2509.11868.pdf", "abs": "https://arxiv.org/abs/2509.11868", "title": "Growing Perspectives: Modelling Embodied Perspective Taking and Inner Narrative Development Using Large Language Models", "authors": ["Sabrina Patania", "Luca Annese", "Anna Lambiase", "Anita Pellegrini", "Tom Foulsham", "Azzurra Ruggeri", "Silvia Rossi", "Silvia Serino", "Dimitri Ognibene"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.RO", "I.2; I.2.7; I.2.10; J.4"], "comment": "Accepted at ICDL https://icdl2025.fel.cvut.cz/", "summary": "Language and embodied perspective taking are essential for human\ncollaboration, yet few computational models address both simultaneously. This\nwork investigates the PerspAct system [1], which integrates the ReAct (Reason\nand Act) paradigm with Large Language Models (LLMs) to simulate developmental\nstages of perspective taking, grounded in Selman's theory [2]. Using an\nextended director task, we evaluate GPT's ability to generate internal\nnarratives aligned with specified developmental stages, and assess how these\ninfluence collaborative performance both qualitatively (action selection) and\nquantitatively (task efficiency). Results show that GPT reliably produces\ndevelopmentally-consistent narratives before task execution but often shifts\ntowards more advanced stages during interaction, suggesting that language\nexchanges help refine internal representations. Higher developmental stages\ngenerally enhance collaborative effectiveness, while earlier stages yield more\nvariable outcomes in complex contexts. These findings highlight the potential\nof integrating embodied perspective taking and language in LLMs to better model\ndevelopmental dynamics and stress the importance of evaluating internal speech\nduring combined linguistic and embodied tasks.", "AI": {"tldr": "This paper investigates the integration of language and embodied perspective taking in LLMs through the PerspAct system, evaluating its impact on collaborative tasks.", "motivation": "To address the lack of computational models that simultaneously consider language and embodied perspective taking in human collaboration.", "method": "The study utilizes the PerspAct system to evaluate GPT's narrative generation through an extended director task, analyzing the output's alignment with developmental stages of perspective taking.", "result": "GPT produces narratives consistent with developmental stages and shows improved collaborative performance, indicating that language helps refine internal representations during tasks.", "conclusion": "The research underscores the significance of combining linguistic and embodied approaches in LLMs for better modeling of developmental dynamics.", "key_contributions": ["Development of the PerspAct system integrating perspective taking with LLMs.", "Evaluation of GPT's narrative generation related to developmental stages in collaborative contexts.", "Insights into the impact of language exchanges on internal representations during interactions."], "limitations": "The study may be limited by the specific tasks and contexts used in evaluating collaborative performance.", "keywords": ["Language Models", "Embodied Perspective Taking", "Collaborative Performance"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.11591", "pdf": "https://arxiv.org/pdf/2509.11591.pdf", "abs": "https://arxiv.org/abs/2509.11591", "title": "Analyzing Information-Seeking Behaviors in a Hakka AI Chatbot: A Cognitive-Pragmatic Study", "authors": ["Chu-Hsuan Lee", "Chen-Chi Chang", "Hung-Shin Lee", "Yun-Hsiang Hsu", "Ching-Yuan Chen"], "categories": ["cs.CL"], "comment": "Accepted to HICSS-59 (2026)", "summary": "With many endangered languages at risk of disappearing, efforts to preserve\nthem now rely more than ever on using technology alongside culturally informed\nteaching strategies. This study examines user behaviors in TALKA, a generative\nAI-powered chatbot designed for Hakka language engagement, by employing a\ndual-layered analytical framework grounded in Bloom's Taxonomy of cognitive\nprocesses and dialogue act categorization. We analyzed 7,077 user utterances,\neach carefully annotated according to six cognitive levels and eleven dialogue\nact types. These included a variety of functions, such as asking for\ninformation, requesting translations, making cultural inquiries, and using\nlanguage creatively. Pragmatic classifications further highlight how different\ntypes of dialogue acts--such as feedback, control commands, and social\ngreetings--align with specific cognitive intentions. The results suggest that\ngenerative AI chatbots can support language learning in meaningful\nways--especially when they are designed with an understanding of how users\nthink and communicate. They may also help learners express themselves more\nconfidently and connect with their cultural identity. The TALKA case provides\nempirical insights into how AI-mediated dialogue facilitates cognitive\ndevelopment in low-resource language learners, as well as pragmatic negotiation\nand socio-cultural affiliation. By focusing on AI-assisted language learning,\nthis study offers new insights into how technology can support language\npreservation and educational practice.", "AI": {"tldr": "This study examines user interactions with a generative AI chatbot for Hakka language engagement, analyzing cognitive processes and dialogue acts to demonstrate how AI can support language learning and preservation.", "motivation": "With endangered languages at risk of disappearing, it's crucial to utilize technology alongside culturally informed teaching to preserve them.", "method": "The study analyzed 7,077 user utterances from the TALKA chatbot, annotated by six cognitive levels and eleven dialogue act types using a dual-layered analytical framework.", "result": "The findings suggest that AI chatbots can effectively aid language learning and help learners connect with their cultural identity by facilitating cognitive development and pragmatic negotiation.", "conclusion": "Generative AI chatbots, designed with an understanding of user communication, can support meaningful language learning and contribute to language preservation efforts.", "key_contributions": ["Empirical insights into AI-mediated dialogue for cognitive development in low-resource language learners.", "Demonstration of how dialogue acts align with cognitive intentions in language learning.", "Implications for using technology to support language preservation and educational practices."], "limitations": "", "keywords": ["Hakka language", "Chatbot", "AI-assisted language learning", "Cognitive processes", "Language preservation"], "importance_score": 4, "read_time_minutes": 12}}
{"id": "2509.11921", "pdf": "https://arxiv.org/pdf/2509.11921.pdf", "abs": "https://arxiv.org/abs/2509.11921", "title": "Designing LLMs for cultural sensitivity: Evidence from English-Japanese translation", "authors": ["Helene Tenzer", "Oumnia Abidi", "Stefan Feuerriegel"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) are increasingly used in everyday communication,\nincluding multilingual interactions across different cultural contexts. While\nLLMs can now generate near-perfect literal translations, it remains unclear\nwhether LLMs support culturally appropriate communication. In this paper, we\nanalyze the cultural sensitivity of different LLM designs when applied to\nEnglish-Japanese translations of workplace e-mails. Here, we vary the prompting\nstrategies: (1) naive \"just translate\" prompts, (2) audience-targeted prompts\nspecifying the recipient's cultural background, and (3) instructional prompts\nwith explicit guidance on Japanese communication norms. Using a mixed-methods\nstudy, we then analyze culture-specific language patterns to evaluate how well\ntranslations adapt to cultural norms. Further, we examine the appropriateness\nof the tone of the translations as perceived by native speakers. We find that\nculturally-tailored prompting can improve cultural fit, based on which we offer\nrecommendations for designing culturally inclusive LLMs in multilingual\nsettings.", "AI": {"tldr": "This paper analyzes the cultural sensitivity of large language models in English-Japanese translation of workplace emails, highlighting the influence of different prompting strategies on culturally appropriate communication.", "motivation": "To evaluate the ability of LLMs to support culturally sensitive communication in multilingual interactions, particularly in a workplace setting.", "method": "A mixed-methods study is conducted, varying prompting strategies for LLMs: naive prompts, audience-targeted prompts, and instructional prompts based on Japanese communication norms, followed by analysis of translation quality and tone by native speakers.", "result": "The study finds that culturally-tailored prompting enhances cultural fit in translations, leading to more appropriate communication.", "conclusion": "Recommendations are provided for designing culturally inclusive LLMs to improve communication in multilingual contexts.", "key_contributions": ["Analysis of cultural sensitivity in LLM translations", "Comparative evaluation of prompting strategies for LLMs", "Recommendations for improving cultural inclusivity in LLMs"], "limitations": "", "keywords": ["large language models", "cultural sensitivity", "translation", "cross-cultural communication", "prompting strategies"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.11604", "pdf": "https://arxiv.org/pdf/2509.11604.pdf", "abs": "https://arxiv.org/abs/2509.11604", "title": "Dynamic Span Interaction and Graph-Aware Memory for Entity-Level Sentiment Classification", "authors": ["Md. Mithun Hossain", "Sanjara", "Md. Shakil Hossain", "Sudipto Chaki"], "categories": ["cs.CL"], "comment": null, "summary": "Entity-level sentiment classification involves identifying the sentiment\npolarity linked to specific entities within text. This task poses several\nchallenges: effectively modeling the subtle and complex interactions between\nentities and their surrounding sentiment expressions; capturing dependencies\nthat may span across sentences; and ensuring consistent sentiment predictions\nfor multiple mentions of the same entity through coreference resolution.\nAdditionally, linguistic phenomena such as negation, ambiguity, and overlapping\nopinions further complicate the analysis. These complexities make entity-level\nsentiment classification a difficult problem, especially in real-world, noisy\ntextual data. To address these issues, we propose SpanEIT, a novel framework\nintegrating dynamic span interaction and graph-aware memory mechanisms for\nenhanced entity-sentiment relational modeling. SpanEIT builds span-based\nrepresentations for entities and candidate sentiment phrases, employs\nbidirectional attention for fine-grained interactions, and uses a graph\nattention network to capture syntactic and co-occurrence relations. A\ncoreference-aware memory module ensures entity-level consistency across\ndocuments. Experiments on FSAD, BARU, and IMDB datasets show SpanEIT\noutperforms state-of-the-art transformer and hybrid baselines in accuracy and\nF1 scores. Ablation and interpretability analyses validate the effectiveness of\nour approach, underscoring its potential for fine-grained sentiment analysis in\napplications like social media monitoring and customer feedback analysis.", "AI": {"tldr": "This paper presents SpanEIT, a novel framework for entity-level sentiment classification that integrates dynamic span interaction and graph-aware memory mechanisms to improve sentiment analysis accuracy.", "motivation": "The paper addresses challenges in entity-level sentiment classification, such as complex interactions between entities and sentiment expressions, coreference resolution, and linguistic phenomena that complicate sentiment analysis.", "method": "SpanEIT utilizes span-based representations, bidirectional attention for fine-grained interactions, and a graph attention network to model syntactic and co-occurrence relations, along with a coreference-aware memory module for consistency.", "result": "Experiments demonstrate that SpanEIT outperforms state-of-the-art transformer and hybrid models on FSAD, BARU, and IMDB datasets in terms of accuracy and F1 scores.", "conclusion": "The results indicate that SpanEIT is a significant advancement for fine-grained sentiment analysis, particularly useful for applications like social media monitoring and customer feedback analysis.", "key_contributions": ["Introduction of the SpanEIT framework for improved entity-sentiment relations", "Integration of dynamic span interaction and graph-aware memory mechanisms", "Validation through extensive experimental results showing superior performance over existing models."], "limitations": "", "keywords": ["Entity-level sentiment", "Dynamic span interaction", "Graph-aware memory", "Sentiment analysis", "Coreference resolution"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.11619", "pdf": "https://arxiv.org/pdf/2509.11619.pdf", "abs": "https://arxiv.org/abs/2509.11619", "title": "HalluDetect: Detecting, Mitigating, and Benchmarking Hallucinations in Conversational Systems", "authors": ["Spandan Anaokar", "Shrey Ganatra", "Harshvivek Kashid", "Swapnil Bhattacharyya", "Shruti Nair", "Reshma Sekhar", "Siddharth Manohar", "Rahul Hemrajani", "Pushpak Bhattacharyya"], "categories": ["cs.CL"], "comment": "6 pages + references + appendix, 3 figures, 2 tables", "summary": "Large Language Models (LLMs) are widely used in industry but remain prone to\nhallucinations, limiting their reliability in critical applications. This work\naddresses hallucination reduction in consumer grievance chatbots built using\nLLaMA 3.1 8B Instruct, a compact model frequently used in industry. We develop\nHalluDetect, an LLM-based hallucination detection system that achieves an F1\nscore of 69% outperforming baseline detectors by 25.44%. Benchmarking five\nchatbot architectures, we find that out of them, AgentBot minimizes\nhallucinations to 0.4159 per turn while maintaining the highest token accuracy\n(96.13%), making it the most effective mitigation strategy. Our findings\nprovide a scalable framework for hallucination mitigation, demonstrating that\noptimized inference strategies can significantly improve factual accuracy.\nWhile applied to consumer law, our approach generalizes to other high-risk\ndomains, enhancing trust in LLM-driven assistants. We will release the code and\ndataset", "AI": {"tldr": "This paper presents HalluDetect, a system for reducing hallucinations in large language model-based chatbots, demonstrating an effective mitigation strategy with high accuracy.", "motivation": "While LLMs are widely used, they are prone to hallucinations which affect their reliability in critical applications like consumer grievance chatbots.", "method": "The authors developed HalluDetect for hallucination detection in consumer grievance chatbots built with the LLaMA 3.1 8B Instruct model, and benchmarked several chatbot architectures.", "result": "HalluDetect achieved an F1 score of 69%, outperforming baseline detectors by 25.44%. Among five architectures, AgentBot minimized hallucinations to 0.4159 per turn with a token accuracy of 96.13%.", "conclusion": "The study provides a scalable framework for hallucination mitigation that enhances trust in LLM-driven assistants, applicable to other high-risk domains.", "key_contributions": ["Development of HalluDetect for hallucination detection in chatbots", "Benchmarking five chatbot architectures for hallucination minimization", "Generalizable approach to enhance factual accuracy in high-risk domains"], "limitations": "", "keywords": ["Hallucination Detection", "Large Language Models", "Chatbots", "Consumer Law", "Trust in AI"], "importance_score": 9, "read_time_minutes": 6}}
{"id": "2311.05920", "pdf": "https://arxiv.org/pdf/2311.05920.pdf", "abs": "https://arxiv.org/abs/2311.05920", "title": "Impact Ambivalence: How People with Eating Disorders Get Trapped in the Perpetual Cycle of Digital Food Content Engagement", "authors": ["Ryuhaerang Choi", "Subin Park", "Sujin Han", "Jennifer G. Kim", "Sung-Ju Lee"], "categories": ["cs.HC", "cs.CY", "cs.MM"], "comment": "15 pages, 3 figures", "summary": "Digital food content could impact viewers' dietary health, with individuals\nwith eating disorders being particularly sensitive to it. However, a\ncomprehensive understanding of why and how these individuals interact with such\ncontent is lacking. To fill this void, we conducted exploratory (N=23) and\nin-depth studies (N=22) with individuals with eating disorders to understand\ntheir motivations and practices of consuming digital food content. We reveal\nthat participants engaged with digital food content for both disorder-driven\nand recovery-supporting motivations, leading to conflicting outcomes. This\nimpact ambivalence, the coexistence of recovery-supporting benefits and\ndisorder-exacerbating risks, sustained a cycle of quitting, prompted by\nawareness of harm, and returning, motivated by anticipated benefits. We\ninterpret these dynamics within dual systems theory and highlight how\nrecognizing such ambivalence can inform the design of interventions that foster\nhealthier digital food content engagement and mitigate post-engagement harmful\neffects.", "AI": {"tldr": "The paper explores how individuals with eating disorders interact with digital food content, revealing motivations tied to both recovery and disorder exacerbation.", "motivation": "There is a lack of understanding regarding how individuals with eating disorders consume digital food content and its impact on their health.", "method": "Exploratory studies (N=23) and in-depth studies (N=22) with individuals with eating disorders were conducted to analyze their motivations and practices regarding digital food content.", "result": "Participants engaged with digital food content for both disorder-driven and recovery-supporting motivations, leading to conflicting outcomes characterized by cycles of quitting and returning to the content.", "conclusion": "Recognizing the ambivalence in individuals' motivations can guide the design of interventions aimed at promoting healthier engagement with digital food content.", "key_contributions": ["Insight into the conflicting motivations of individuals with eating disorders when interacting with digital food content.", "Identification of a cycle of quitting and returning to content based on perceived benefits and harms.", "Recommendations for designing interventions that support healthier engagement with digital food content."], "limitations": "The study is limited by its small sample size and the exploratory nature of research, which may not fully capture broader user interactions.", "keywords": ["digital food content", "eating disorders", "health interventions", "motivation", "user engagement"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2509.11620", "pdf": "https://arxiv.org/pdf/2509.11620.pdf", "abs": "https://arxiv.org/abs/2509.11620", "title": "AesBiasBench: Evaluating Bias and Alignment in Multimodal Language Models for Personalized Image Aesthetic Assessment", "authors": ["Kun Li", "Lai-Man Po", "Hongzheng Yang", "Xuyuan Xu", "Kangcheng Liu", "Yuzhi Zhao"], "categories": ["cs.CL", "cs.CY"], "comment": "Accepted by EMNLP 2025", "summary": "Multimodal Large Language Models (MLLMs) are increasingly applied in\nPersonalized Image Aesthetic Assessment (PIAA) as a scalable alternative to\nexpert evaluations. However, their predictions may reflect subtle biases\ninfluenced by demographic factors such as gender, age, and education. In this\nwork, we propose AesBiasBench, a benchmark designed to evaluate MLLMs along two\ncomplementary dimensions: (1) stereotype bias, quantified by measuring\nvariations in aesthetic evaluations across demographic groups; and (2)\nalignment between model outputs and genuine human aesthetic preferences. Our\nbenchmark covers three subtasks (Aesthetic Perception, Assessment, Empathy) and\nintroduces structured metrics (IFD, NRD, AAS) to assess both bias and\nalignment. We evaluate 19 MLLMs, including proprietary models (e.g., GPT-4o,\nClaude-3.5-Sonnet) and open-source models (e.g., InternVL-2.5, Qwen2.5-VL).\nResults indicate that smaller models exhibit stronger stereotype biases,\nwhereas larger models align more closely with human preferences. Incorporating\nidentity information often exacerbates bias, particularly in emotional\njudgments. These findings underscore the importance of identity-aware\nevaluation frameworks in subjective vision-language tasks.", "AI": {"tldr": "This paper proposes AesBiasBench, a benchmark for evaluating Multimodal Large Language Models on stereotype bias and alignment with human aesthetic preferences in Personalized Image Aesthetic Assessment.", "motivation": "To address the biases in the predictions of Multimodal Large Language Models when applied to Personalized Image Aesthetic Assessment.", "method": "The benchmark evaluates models on two dimensions: stereotype bias and alignment with human aesthetic preferences, covering subtasks such as Aesthetic Perception, Assessment, and Empathy.", "result": "Evaluation of 19 MLLMs shows that smaller models demonstrate stronger stereotype biases, while larger models align more closely with human preferences. Identity information often worsens bias, especially in emotional judgments.", "conclusion": "Identity-aware evaluation frameworks are crucial for subjective vision-language tasks to ensure fairness and alignment with human aesthetics.", "key_contributions": ["Introduction of AesBiasBench benchmark for MLLMs", "Identification of bias variations across demographic groups", "Development of structured metrics for assessing bias and alignment"], "limitations": "The study primarily focuses on aesthetic assessments and may not generalize to other domains of HCI or ML applications.", "keywords": ["Multimodal Large Language Models", "Aesthetic Assessment", "Bias Evaluation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.11648", "pdf": "https://arxiv.org/pdf/2509.11648.pdf", "abs": "https://arxiv.org/abs/2509.11648", "title": "EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI", "authors": ["Sai Kartheek Reddy Kasu"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "The deployment of large language models (LLMs) in mental health and other\nsensitive domains raises urgent questions about ethical reasoning, fairness,\nand responsible alignment. Yet, existing benchmarks for moral and clinical\ndecision-making do not adequately capture the unique ethical dilemmas\nencountered in mental health practice, where confidentiality, autonomy,\nbeneficence, and bias frequently intersect. To address this gap, we introduce\nEthical Reasoning in Mental Health (EthicsMH), a pilot dataset of 125 scenarios\ndesigned to evaluate how AI systems navigate ethically charged situations in\ntherapeutic and psychiatric contexts. Each scenario is enriched with structured\nfields, including multiple decision options, expert-aligned reasoning, expected\nmodel behavior, real-world impact, and multi-stakeholder viewpoints. This\nstructure enables evaluation not only of decision accuracy but also of\nexplanation quality and alignment with professional norms. Although modest in\nscale and developed with model-assisted generation, EthicsMH establishes a task\nframework that bridges AI ethics and mental health decision-making. By\nreleasing this dataset, we aim to provide a seed resource that can be expanded\nthrough community and expert contributions, fostering the development of AI\nsystems capable of responsibly handling some of society's most delicate\ndecisions.", "AI": {"tldr": "Ethical Reasoning in Mental Health (EthicsMH) is a pilot dataset of 125 scenarios for assessing AI's ethical decision-making in mental health contexts.", "motivation": "There is a need for better benchmarks that address ethical dilemmas in mental health, particularly as LLMs are deployed in sensitive areas.", "method": "Discussed the creation of a pilot dataset called EthicsMH that includes 125 scenarios to evaluate AI systems in therapeutic and psychiatric contexts, featuring structured fields for decision-making processes.", "result": "The dataset allows for an evaluation not only on decision accuracy but also explanation quality and alignment with professional norms in mental health.", "conclusion": "EthicsMH lays the groundwork for further exploration and expansion in AI ethics related to mental health, emphasizing the importance of responsible AI deployment.", "key_contributions": ["Introduction of EthicsMH dataset for AI evaluation in mental health ethics", "Structured evaluation fields for ethical decision-making scenarios", "Framework for bridging AI ethics and mental health decision processes"], "limitations": "", "keywords": ["AI ethics", "mental health", "large language models", "decision-making", "dataset"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.03330", "pdf": "https://arxiv.org/pdf/2502.03330.pdf", "abs": "https://arxiv.org/abs/2502.03330", "title": "Controllable GUI Exploration", "authors": ["Aryan Garg", "Yue Jiang", "Antti Oulasvirta"], "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.GR"], "comment": null, "summary": "During the early stages of interface design, designers need to produce\nmultiple sketches to explore a design space. Design tools often fail to support\nthis critical stage, because they insist on specifying more details than\nnecessary. Although recent advances in generative AI have raised hopes of\nsolving this issue, in practice they fail because expressing loose ideas in a\nprompt is impractical. In this paper, we propose a diffusion-based approach to\nthe low-effort generation of interface sketches. It breaks new ground by\nallowing flexible control of the generation process via three types of inputs:\nA) prompts, B) wireframes, and C) visual flows. The designer can provide any\ncombination of these as input at any level of detail, and will get a diverse\ngallery of low-fidelity solutions in response. The unique benefit is that large\ndesign spaces can be explored rapidly with very little effort in\ninput-specification. We present qualitative results for various combinations of\ninput specifications. Additionally, we demonstrate that our model aligns more\naccurately with these specifications than other models.", "AI": {"tldr": "This paper introduces a diffusion-based approach for low-effort generation of interface sketches, allowing flexible input via prompts, wireframes, and visual flows, resulting in diverse low-fidelity design solutions.", "motivation": "Designers often struggle with existing tools that require overly detailed inputs during the early stages of interface design, hindering exploration of design spaces.", "method": "A diffusion-based generation model that accepts flexible input formats (prompts, wireframes, visual flows) and produces a variety of low-fidelity interface sketches.", "result": "The model generates a diverse gallery of sketches based on various input combinations, showing better alignment with specifications compared to existing models.", "conclusion": "The proposed method enables rapid exploration of large design spaces with minimal input effort, making it a valuable tool for early-stage interface design.", "key_contributions": ["Introduces a diffusion-based approach for sketch generation", "Supports flexible input combinations for design exploration", "Demonstrates improved alignment with designer specifications over existing models."], "limitations": "", "keywords": ["interface design", "generative AI", "diffusion model", "design exploration", "creative tools"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.11687", "pdf": "https://arxiv.org/pdf/2509.11687.pdf", "abs": "https://arxiv.org/abs/2509.11687", "title": "A Dynamic Knowledge Update-Driven Model with Large Language Models for Fake News Detection", "authors": ["Di Jin", "Jun Yang", "Xiaobao Wang", "Junwei Zhang", "Shuqi Li", "Dongxiao He"], "categories": ["cs.CL"], "comment": null, "summary": "As the Internet and social media evolve rapidly, distinguishing credible news\nfrom a vast amount of complex information poses a significant challenge. Due to\nthe suddenness and instability of news events, the authenticity labels of news\ncan potentially shift as events develop, making it crucial for fake news\ndetection to obtain the latest event updates. Existing methods employ\nretrieval-augmented generation to fill knowledge gaps, but they suffer from\nissues such as insufficient credibility of retrieved content and interference\nfrom noisy information. We propose a dynamic knowledge update-driven model for\nfake news detection (DYNAMO), which leverages knowledge graphs to achieve\ncontinuous updating of new knowledge and integrates with large language models\nto fulfill dual functions: news authenticity detection and verification of new\nknowledge correctness, solving the two key problems of ensuring the\nauthenticity of new knowledge and deeply mining news semantics. Specifically,\nwe first construct a news-domain-specific knowledge graph. Then, we use Monte\nCarlo Tree Search to decompose complex news and verify them step by step.\nFinally, we extract and update new knowledge from verified real news texts and\nreasoning paths. Experimental results demonstrate that DYNAMO achieves the best\nperformance on two real-world datasets.", "AI": {"tldr": "DYNAMO is a model proposed for fake news detection that utilizes knowledge graphs and large language models to continuously update knowledge and verify news authenticity.", "motivation": "The rapid evolution of the Internet and social media has made distinguishing credible news from misinformation increasingly difficult, necessitating a reliable method for fake news detection that can adapt as events unfold.", "method": "The paper proposes DYNAMO, a model that constructs a news-domain-specific knowledge graph and employs Monte Carlo Tree Search to validate news step by step, enabling dynamic knowledge updates.", "result": "DYNAMO demonstrated superior performance in fake news detection compared to existing methods by achieving the best results on two real-world datasets.", "conclusion": "The use of knowledge graphs combined with large language models allows for improved authenticity detection of news and verification of new knowledge, addressing key challenges in fake news detection.", "key_contributions": ["Development of DYNAMO for dynamic knowledge updating in fake news detection", "Integration of knowledge graphs with large language models for authenticity checking", "Application of Monte Carlo Tree Search for systematic news validation"], "limitations": "", "keywords": ["fake news detection", "knowledge graphs", "dynamic knowledge updating", "large language models", "Monte Carlo Tree Search"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2509.11698", "pdf": "https://arxiv.org/pdf/2509.11698.pdf", "abs": "https://arxiv.org/abs/2509.11698", "title": "CoachMe: Decoding Sport Elements with a Reference-Based Coaching Instruction Generation Model", "authors": ["Wei-Hsin Yeh", "Yu-An Su", "Chih-Ning Chen", "Yi-Hsueh Lin", "Calvin Ku", "Wen-Hsin Chiu", "Min-Chun Hu", "Lun-Wei Ku"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "I.2.7; I.2.10"], "comment": "Published in Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025.\n  Official version: https://doi.org/10.18653/v1/2025.acl-long.1413", "summary": "Motion instruction is a crucial task that helps athletes refine their\ntechnique by analyzing movements and providing corrective guidance. Although\nrecent advances in multimodal models have improved motion understanding,\ngenerating precise and sport-specific instruction remains challenging due to\nthe highly domain-specific nature of sports and the need for informative\nguidance. We propose CoachMe, a reference-based model that analyzes the\ndifferences between a learner's motion and a reference under temporal and\nphysical aspects. This approach enables both domain-knowledge learning and the\nacquisition of a coach-like thinking process that identifies movement errors\neffectively and provides feedback to explain how to improve. In this paper, we\nillustrate how CoachMe adapts well to specific sports such as skating and\nboxing by learning from general movements and then leveraging limited data.\nExperiments show that CoachMe provides high-quality instructions instead of\ndirections merely in the tone of a coach but without critical information.\nCoachMe outperforms GPT-4o by 31.6% in G-Eval on figure skating and by 58.3% on\nboxing. Analysis further confirms that it elaborates on errors and their\ncorresponding improvement methods in the generated instructions. You can find\nCoachMe here: https://motionxperts.github.io/", "AI": {"tldr": "CoachMe is a reference-based model designed to analyze sports movements and generate sport-specific corrective instructions, improving athletes' techniques.", "motivation": "The paper addresses the challenge of generating precise and informative motion instructions for athletes, leveraging advances in multimodal models.", "method": "CoachMe analyzes differences between a learner's motion and a reference, focusing on temporal and physical aspects to provide tailored feedback.", "result": "CoachMe outperformed GPT-4o by significant margins (31.6% in figure skating and 58.3% in boxing) in generating effective instructional guidance.", "conclusion": "The model demonstrates its effectiveness in providing detailed movement error analysis and improvement methods in sports coaching.", "key_contributions": ["Introduction of CoachMe, a reference-based instruction model for sports", "Demonstrated adaptation to specific sports like skating and boxing", "Improved performance over existing models in generating effective coaching instructions"], "limitations": "", "keywords": ["motion instruction", "multimodal models", "sports coaching", "corrective guidance", "human-computer interaction"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.11709", "pdf": "https://arxiv.org/pdf/2509.11709.pdf", "abs": "https://arxiv.org/abs/2509.11709", "title": "Room acoustics affect communicative success in hybrid meeting spaces: a pilot study", "authors": ["Robert Einig", "Stefan Janscha", "Jonas Schuster", "Julian Koch", "Martin Hagmueller", "Barbara Schuppler"], "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "Since the COVID-19 pandemic in 2020, universities and companies have\nincreasingly integrated hybrid features into their meeting spaces, or even\ncreated dedicated rooms for this purpose. While the importance of a fast and\nstable internet connection is often prioritized, the acoustic design of seminar\nrooms is frequently overlooked. Poor acoustics, particularly excessive\nreverberation, can lead to issues such as misunderstandings, reduced speech\nintelligibility or cognitive and vocal fatigue. This pilot study investigates\nwhether room acoustic interventions in a seminar room at Graz University of\nTechnology support better communication in hybrid meetings. For this purpose,\nwe recorded two groups of persons twice, once before and once after improving\nthe acoustics of the room. Our findings -- despite not reaching statistical\nsignificance due to the small sample size - indicate clearly that our spatial\ninterventions improve communicative success in hybrid meetings. To make the\npaper accessible also for readers from the speech communication community, we\nexplain room acoustics background, relevant for the interpretation of our\nresults.", "AI": {"tldr": "This study examines whether acoustic improvements in seminar rooms enhance communication in hybrid meetings, finding preliminary evidence of positive impact.", "motivation": "To address the overlooked issue of room acoustics in hybrid meeting spaces that can lead to communication issues.", "method": "The study involved recording the communicative performance of two groups before and after implementing acoustic improvements in a seminar room.", "result": "Results suggested improved communicative success in hybrid meetings following acoustic enhancements, although not statistically significant due to small sample size.", "conclusion": "Better room acoustics can potentially enhance communication effectiveness in hybrid settings, warranting further investigation.", "key_contributions": ["Investigates the impact of acoustic design on hybrid meeting effectiveness", "Provides empirical evidence of potential benefits of acoustic interventions", "Makes findings accessible to the speech communication community"], "limitations": "Small sample size limits statistical significance of results.", "keywords": ["acoustics", "hybrid meetings", "communication", "room design", "speech intelligibility"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.16207", "pdf": "https://arxiv.org/pdf/2507.16207.pdf", "abs": "https://arxiv.org/abs/2507.16207", "title": "A Human-Centered Approach to Identifying Promises, Risks, & Challenges of Text-to-Image Generative AI in Radiology", "authors": ["Katelyn Morrison", "Arpit Mathur", "Aidan Bradshaw", "Tom Wartmann", "Steven Lundi", "Afrooz Zandifar", "Weichang Dai", "Kayhan Batmanghelich", "Motahhare Eslami", "Adam Perer"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "10 pages of main content, Appendix attached after references,\n  accepted to AAAI/ACM AIES 2025", "summary": "As text-to-image generative models rapidly improve, AI researchers are making\nsignificant advances in developing domain-specific models capable of generating\ncomplex medical imagery from text prompts. Despite this, these technical\nadvancements have overlooked whether and how medical professionals would\nbenefit from and use text-to-image generative AI (GenAI) in practice. By\ndeveloping domain-specific GenAI without involving stakeholders, we risk the\npotential of building models that are either not useful or even more harmful\nthan helpful. In this paper, we adopt a human-centered approach to responsible\nmodel development by involving stakeholders in evaluating and reflecting on the\npromises, risks, and challenges of a novel text-to-CT Scan GenAI model. Through\nexploratory model prompting activities, we uncover the perspectives of medical\nstudents, radiology trainees, and radiologists on the role that text-to-CT Scan\nGenAI can play across medical education, training, and practice. This\nhuman-centered approach additionally enabled us to surface technical challenges\nand domain-specific risks of generating synthetic medical images. We conclude\nby reflecting on the implications of medical text-to-image GenAI.", "AI": {"tldr": "Exploration of stakeholders' perspectives on text-to-image generative AI for CT scans in medical education and practice, emphasizing a human-centered approach to model development.", "motivation": "To investigate how text-to-image generative AI can be beneficial for medical professionals and address the risks of developing such models without stakeholder involvement.", "method": "A human-centered approach involving medical students, radiology trainees, and radiologists in evaluating a text-to-CT Scan GenAI model through exploratory prompting activities.", "result": "Identified perspectives on the role of text-to-image GenAI in medical education and training, revealing technical challenges and domain-specific risks.", "conclusion": "Involving stakeholders in the development of GenAI models is crucial to ensure their usefulness and mitigate risks in medical applications.", "key_contributions": ["Human-centered evaluation of medical GenAI models", "Identification of technical challenges in generating synthetic medical images", "Insights into medical professionals' perspectives on GenAI use"], "limitations": "Limited to a specific domain of CT scans and perspectives from a chosen group of stakeholders.", "keywords": ["text-to-image generation", "medical imagery", "human-centered AI", "radiology", "stakeholder involvement"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.11773", "pdf": "https://arxiv.org/pdf/2509.11773.pdf", "abs": "https://arxiv.org/abs/2509.11773", "title": "An Agentic Toolkit for Adaptive Information Extraction from Regulatory Documents", "authors": ["Gaye Colakoglu", "Gürkan Solmaz", "Jonathan Fürst"], "categories": ["cs.CL"], "comment": null, "summary": "Declaration of Performance (DoP) documents, mandated by EU regulation,\ncertify the performance of construction products. While some of their content\nis standardized, DoPs vary widely in layout, language, schema, and format,\nposing challenges for automated key-value pair extraction (KVP) and question\nanswering (QA). Existing static or LLM-only IE pipelines often hallucinate and\nfail to adapt to this structural diversity. Our domain-specific, stateful\nagentic system addresses these challenges through a planner-executor-responder\narchitecture. The system infers user intent, detects document modality, and\norchestrates tools dynamically for robust, traceable reasoning while avoiding\ntool misuse or execution loops. Evaluation on a curated DoP dataset\ndemonstrates improved robustness across formats and languages, offering a\nscalable solution for structured data extraction in regulated workflows.", "AI": {"tldr": "This paper presents a framework for enhancing automated key-value pair extraction from diverse Declaration of Performance documents using a stateful agentic system.", "motivation": "The variability in layout, language, and format of Declaration of Performance (DoP) documents creates challenges for existing automated information extraction methods.", "method": "A planner-executor-responder architecture is employed to dynamically orchestrate extraction tools, infer user intent, and adjust to different document modalities.", "result": "The system demonstrates improved robustness in extracting structured information from DoPs across various formats and languages, outperforming traditional static and LLM-only approaches.", "conclusion": "This approach offers a scalable solution for structured data extraction in regulated environments, addressing the challenges posed by document diversity.", "key_contributions": ["Development of a stateful agentic system for document processing", "Introduction of a planner-executor-responder architecture", "Demonstrated improved performance in dynamic tool orchestration for key-value extraction"], "limitations": "", "keywords": ["Declaration of Performance", "information extraction", "stateful systems", "automated reasoning", "document analysis"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2509.11777", "pdf": "https://arxiv.org/pdf/2509.11777.pdf", "abs": "https://arxiv.org/abs/2509.11777", "title": "User eXperience Perception Insights Dataset (UXPID): Synthetic User Feedback from Public Industrial Forums", "authors": ["Mikhail Kulyabin", "Jan Joosten", "Choro Ulan uulu", "Nuno Miguel Martins Pacheco", "Fabian Ries", "Filippos Petridis", "Jan Bosch", "Helena Holmström Olsson"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Customer feedback in industrial forums reflect a rich but underexplored\nsource of insight into real-world product experience. These publicly shared\ndiscussions offer an organic view of user expectations, frustrations, and\nsuccess stories shaped by the specific contexts of use. Yet, harnessing this\ninformation for systematic analysis remains challenging due to the unstructured\nand domain-specific nature of the content. The lack of structure and\nspecialized vocabulary makes it difficult for traditional data analysis\ntechniques to accurately interpret, categorize, and quantify the feedback,\nthereby limiting its potential to inform product development and support\nstrategies. To address these challenges, this paper presents the User\neXperience Perception Insights Dataset (UXPID), a collection of 7130\nartificially synthesized and anonymized user feedback branches extracted from a\npublic industrial automation forum. Each JavaScript object notation (JSON)\nrecord contains multi-post comments related to specific hardware and software\nproducts, enriched with metadata and contextual conversation data. Leveraging a\nlarge language model (LLM), each branch is systematically analyzed and\nannotated for UX insights, user expectations, severity and sentiment ratings,\nand topic classifications. The UXPID dataset is designed to facilitate research\nin user requirements, user experience (UX) analysis, and AI-driven feedback\nprocessing, particularly where privacy and licensing restrictions limit access\nto real-world data. UXPID supports the training and evaluation of\ntransformer-based models for tasks such as issue detection, sentiment analysis,\nand requirements extraction in the context of technical forums.", "AI": {"tldr": "The paper presents the UXPID dataset, a collection of user feedback from industrial forums, designed to enhance UX analysis using LLMs.", "motivation": "There is a wealth of user feedback in industrial forums that remains underutilized for product development due to its unstructured nature and domain-specific vocabulary.", "method": "The paper introduces the User eXperience Perception Insights Dataset (UXPID), synthesized from 7130 comments in a public automation forum, analyzed and annotated using a large language model.", "result": "The dataset includes enriched metadata and classifications for UX insights, enabling advanced analysis for tasks like sentiment analysis and issue detection.", "conclusion": "UXPID is a valuable resource for improving user experience research and AI-driven analysis in technical contexts where real-world data access is restricted.", "key_contributions": ["Creation of UXPID dataset from industrial automation feedback", "Systematic analysis using LLM for UX insights", "Supports training of models for sentiment analysis and requirements extraction"], "limitations": "", "keywords": ["User Experience", "Machine Learning", "Sentiment Analysis", "Feedback Processing", "Artificial Intelligence"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.11802", "pdf": "https://arxiv.org/pdf/2509.11802.pdf", "abs": "https://arxiv.org/abs/2509.11802", "title": "When Curiosity Signals Danger: Predicting Health Crises Through Online Medication Inquiries", "authors": ["Dvora Goncharok", "Arbel Shifman", "Alexander Apartsin", "Yehudit Aperstein"], "categories": ["cs.CL"], "comment": "5 pages, 2 figures", "summary": "Online medical forums are a rich and underutilized source of insight into\npatient concerns, especially regarding medication use. Some of the many\nquestions users pose may signal confusion, misuse, or even the early warning\nsigns of a developing health crisis. Detecting these critical questions that\nmay precede severe adverse events or life-threatening complications is vital\nfor timely intervention and improving patient safety. This study introduces a\nnovel annotated dataset of medication-related questions extracted from online\nforums. Each entry is manually labelled for criticality based on clinical risk\nfactors. We benchmark the performance of six traditional machine learning\nclassifiers using TF-IDF textual representations, alongside three\nstate-of-the-art large language model (LLM)-based classification approaches\nthat leverage deep contextual understanding. Our results highlight the\npotential of classical and modern methods to support real-time triage and alert\nsystems in digital health spaces. The curated dataset is made publicly\navailable to encourage further research at the intersection of\npatient-generated data, natural language processing, and early warning systems\nfor critical health events. The dataset and benchmark are available at:\nhttps://github.com/Dvora-coder/LLM-Medication-QA-Risk-Classifier-MediGuard.", "AI": {"tldr": "This study presents an annotated dataset of medication-related questions from online forums and benchmarks machine learning methods for detecting critical patient inquiries.", "motivation": "To utilize online medical forums for insights into patient concerns regarding medication and detect critical questions that could signal health crises.", "method": "The study introduces a novel annotated dataset of medication-related questions and benchmarks six machine learning classifiers alongside three LLM-based classification methods using TF-IDF and other approaches.", "result": "The results demonstrate the efficacy of both traditional and modern machine learning methods in supporting real-time triage for potential health risks based on patient-generated questions.", "conclusion": "The curated dataset has significant implications for improving patient safety and can facilitate further research in natural language processing and early warning systems.", "key_contributions": ["Introduction of a novel annotated dataset for medication-related questions.", "Benchmarking of various machine learning and LLM methods for critical question detection.", "Public availability of the dataset to encourage further research in health informatics."], "limitations": "", "keywords": ["medical forums", "patient concerns", "machine learning", "natural language processing", "critical health events"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2509.11803", "pdf": "https://arxiv.org/pdf/2509.11803.pdf", "abs": "https://arxiv.org/abs/2509.11803", "title": "From Fuzzy Speech to Medical Insight: Benchmarking LLMs on Noisy Patient Narratives", "authors": ["Eden Mama", "Liel Sheri", "Yehudit Aperstein", "Alexander Apartsin"], "categories": ["cs.CL"], "comment": "6 pages, 1 figure", "summary": "The widespread adoption of large language models (LLMs) in healthcare raises\ncritical questions about their ability to interpret patient-generated\nnarratives, which are often informal, ambiguous, and noisy. Existing benchmarks\ntypically rely on clean, structured clinical text, offering limited insight\ninto model performance under realistic conditions. In this work, we present a\nnovel synthetic dataset designed to simulate patient self-descriptions\ncharacterized by varying levels of linguistic noise, fuzzy language, and\nlayperson terminology. Our dataset comprises clinically consistent scenarios\nannotated with ground-truth diagnoses, spanning a spectrum of communication\nclarity to reflect diverse real-world reporting styles. Using this benchmark,\nwe fine-tune and evaluate several state-of-the-art models (LLMs), including\nBERT-based and encoder-decoder T5 models. To support reproducibility and future\nresearch, we release the Noisy Diagnostic Benchmark (NDB), a structured dataset\nof noisy, synthetic patient descriptions designed to stress-test and compare\nthe diagnostic capabilities of large language models (LLMs) under realistic\nlinguistic conditions. We made the benchmark available for the community:\nhttps://github.com/lielsheri/PatientSignal", "AI": {"tldr": "This paper presents a synthetic dataset aimed at evaluating large language models (LLMs) on their ability to interpret noisy patient-generated narratives in healthcare.", "motivation": "The need to assess LLM performance in real-world healthcare settings, where patient narratives are often informal and ambiguous, rather than in controlled environments with clean clinical text.", "method": "Creation of the Noisy Diagnostic Benchmark (NDB), a dataset containing synthetic patient descriptions with varying linguistic noise and annotated with ground-truth diagnoses; evaluation of different state-of-the-art LLMs including BERT and T5 models.", "result": "The proposed benchmark enables the fine-tuning and assessment of the diagnostic capabilities of LLMs in interpreting noisy patient descriptions.", "conclusion": "The release of the NDB supports reproducibility in research and encourages further exploration of LLM performance in realistic linguistic conditions in healthcare.", "key_contributions": ["Introduction of a synthetic dataset simulating patient self-descriptions with linguistic noise", "Provision of ground-truth diagnoses for diverse communication styles", "Fine-tuning and evaluation of state-of-the-art LLMs to assess their diagnostic performance under realistic conditions."], "limitations": "", "keywords": ["large language models", "healthcare", "synthetic dataset", "noisy patient narratives", "diagnostic benchmarking"], "importance_score": 9, "read_time_minutes": 6}}
{"id": "2509.11804", "pdf": "https://arxiv.org/pdf/2509.11804.pdf", "abs": "https://arxiv.org/abs/2509.11804", "title": "PledgeTracker: A System for Monitoring the Fulfilment of Pledges", "authors": ["Yulong Chen", "Michael Sejr Schlichtkrull", "Zhenyun Deng", "David Corney", "Nasim Asl", "Joshua Salisbury", "Andrew Dudfield", "Andreas Vlachos"], "categories": ["cs.CL"], "comment": "EMNLP 2025 demo", "summary": "Political pledges reflect candidates' policy commitments, but tracking their\nfulfilment requires reasoning over incremental evidence distributed across\nmultiple, dynamically updated sources. Existing methods simplify this task into\na document classification task, overlooking its dynamic, temporal and\nmulti-document nature. To address this issue, we introduce\n\\textsc{PledgeTracker}, a system that reformulates pledge verification into\nstructured event timeline construction. PledgeTracker consists of three core\ncomponents: (1) a multi-step evidence retrieval module; (2) a timeline\nconstruction module and; (3) a fulfilment filtering module, allowing the\ncapture of the evolving nature of pledge fulfilment and producing interpretable\nand structured timelines. We evaluate PledgeTracker in collaboration with\nprofessional fact-checkers in real-world workflows, demonstrating its\neffectiveness in retrieving relevant evidence and reducing human verification\neffort.", "AI": {"tldr": "PledgeTracker is a system for tracking political pledge fulfilment through structured event timelines, addressing dynamic evidence retrieval and multi-document processing.", "motivation": "To improve the tracking of political pledges by overcoming limitations in existing methods that treat the task as simple document classification.", "method": "PledgeTracker features a multi-step evidence retrieval module, a timeline construction module, and a fulfilment filtering module to create structured timelines of pledge fulfilment.", "result": "PledgeTracker was evaluated with professional fact-checkers, showing improved effectiveness in retrieving evidence and reducing verification effort.", "conclusion": "PledgeTracker provides a more dynamic and interpretable method for tracking political pledge fulfilment compared to traditional document classification approaches.", "key_contributions": ["Introduction of a multi-step evidence retrieval process", "Development of a structured event timeline for pledge fulfilment", "Reduction of human verification effort through effective evidence retrieval"], "limitations": "", "keywords": ["political pledges", "evidence retrieval", "timeline construction"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.11818", "pdf": "https://arxiv.org/pdf/2509.11818.pdf", "abs": "https://arxiv.org/abs/2509.11818", "title": "SCDTour: Embedding Axis Ordering and Merging for Interpretable Semantic Change Detection", "authors": ["Taichi Aida", "Danushka Bollegala"], "categories": ["cs.CL"], "comment": "Findings of EMNLP2025", "summary": "In Semantic Change Detection (SCD), it is a common problem to obtain\nembeddings that are both interpretable and high-performing. However, improving\ninterpretability often leads to a loss in the SCD performance, and vice versa.\nTo address this problem, we propose SCDTour, a method that orders and merges\ninterpretable axes to alleviate the performance degradation of SCD. SCDTour\nconsiders both (a) semantic similarity between axes in the embedding space, as\nwell as (b) the degree to which each axis contributes to semantic change.\nExperimental results show that SCDTour preserves performance in semantic change\ndetection while maintaining high interpretability. Moreover, agglomerating the\nsorted axes produces a more refined set of word senses, which achieves\ncomparable or improved performance against the original full-dimensional\nembeddings in the SCD task. These findings demonstrate that SCDTour effectively\nbalances interpretability and SCD performance, enabling meaningful\ninterpretation of semantic shifts through a small number of refined axes.\nSource code is available at https://github.com/LivNLP/svp-tour .", "AI": {"tldr": "Proposes SCDTour, a method that balances interpretability and performance in Semantic Change Detection (SCD) by ordering and merging interpretable axes.", "motivation": "Address the trade-off between interpretability and performance in Semantic Change Detection (SCD) embeddings.", "method": "SCDTour orders and merges interpretable axes based on semantic similarity and their contribution to semantic change.", "result": "SCDTour preserves performance in semantic change detection while maintaining high interpretability, resulting in a refined set of word senses that can achieve improved performance.", "conclusion": "SCDTour effectively balances interpretability with SCD performance, enabling better understanding of semantic shifts through fewer refined axes.", "key_contributions": ["Introduces SCDTour methodology for interpretability vs performance in SCD", "Demonstrates improved performance with fewer axes", "Provides source code for reproducibility."], "limitations": "", "keywords": ["Semantic Change Detection", "Interpretability", "Word Embeddings", "Machine Learning", "Natural Language Processing"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2509.11860", "pdf": "https://arxiv.org/pdf/2509.11860.pdf", "abs": "https://arxiv.org/abs/2509.11860", "title": "MOOM: Maintenance, Organization and Optimization of Memory in Ultra-Long Role-Playing Dialogues", "authors": ["Weishu Chen", "Jinyi Tang", "Zhouhui Hou", "Shihao Han", "Mingjie Zhan", "Zhiyuan Huang", "Delong Liu", "Jiawei Guo", "Zhicheng Zhao", "Fei Su"], "categories": ["cs.CL"], "comment": null, "summary": "Memory extraction is crucial for maintaining coherent ultra-long dialogues in\nhuman-robot role-playing scenarios. However, existing methods often exhibit\nuncontrolled memory growth. To address this, we propose MOOM, the first\ndual-branch memory plugin that leverages literary theory by modeling plot\ndevelopment and character portrayal as core storytelling elements.\nSpecifically, one branch summarizes plot conflicts across multiple time scales,\nwhile the other extracts the user's character profile. MOOM further integrates\na forgetting mechanism, inspired by the ``competition-inhibition'' memory\ntheory, to constrain memory capacity and mitigate uncontrolled growth.\nFurthermore, we present ZH-4O, a Chinese ultra-long dialogue dataset\nspecifically designed for role-playing, featuring dialogues that average 600\nturns and include manually annotated memory information. Experimental results\ndemonstrate that MOOM outperforms all state-of-the-art memory extraction\nmethods, requiring fewer large language model invocations while maintaining a\ncontrollable memory capacity.", "AI": {"tldr": "MOOM introduces a dual-branch memory plugin for ultra-long dialogues in robot role-playing, addressing uncontrolled memory growth with a forgetting mechanism and a new dialogue dataset for training.", "motivation": "The need for maintaining coherent ultra-long dialogues in human-robot interactions while managing memory growth challenges.", "method": "MOOM utilizes a dual-branch architecture to summarize plot conflicts and extract character profiles, incorporating a forgetting mechanism for memory control.", "result": "Experimental results show that MOOM outperforms existing memory extraction methods, achieving efficient memory management with fewer model calls.", "conclusion": "MOOM enhances the performance of dialogue systems by effectively balancing memory growth with coherence in ultra-long dialogues.", "key_contributions": ["Introduction of MOOM as a dual-branch memory plugin", "Development of the ZH-4O ultra-long dialogue dataset", "Implementation of a forgetting mechanism for controlled memory growth"], "limitations": "", "keywords": ["memory extraction", "human-robot interaction", "storytelling", "ultra-long dialogues", "forgetting mechanism"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2509.11868", "pdf": "https://arxiv.org/pdf/2509.11868.pdf", "abs": "https://arxiv.org/abs/2509.11868", "title": "Growing Perspectives: Modelling Embodied Perspective Taking and Inner Narrative Development Using Large Language Models", "authors": ["Sabrina Patania", "Luca Annese", "Anna Lambiase", "Anita Pellegrini", "Tom Foulsham", "Azzurra Ruggeri", "Silvia Rossi", "Silvia Serino", "Dimitri Ognibene"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.RO", "I.2; I.2.7; I.2.10; J.4"], "comment": "Accepted at ICDL https://icdl2025.fel.cvut.cz/", "summary": "Language and embodied perspective taking are essential for human\ncollaboration, yet few computational models address both simultaneously. This\nwork investigates the PerspAct system [1], which integrates the ReAct (Reason\nand Act) paradigm with Large Language Models (LLMs) to simulate developmental\nstages of perspective taking, grounded in Selman's theory [2]. Using an\nextended director task, we evaluate GPT's ability to generate internal\nnarratives aligned with specified developmental stages, and assess how these\ninfluence collaborative performance both qualitatively (action selection) and\nquantitatively (task efficiency). Results show that GPT reliably produces\ndevelopmentally-consistent narratives before task execution but often shifts\ntowards more advanced stages during interaction, suggesting that language\nexchanges help refine internal representations. Higher developmental stages\ngenerally enhance collaborative effectiveness, while earlier stages yield more\nvariable outcomes in complex contexts. These findings highlight the potential\nof integrating embodied perspective taking and language in LLMs to better model\ndevelopmental dynamics and stress the importance of evaluating internal speech\nduring combined linguistic and embodied tasks.", "AI": {"tldr": "This paper examines the PerspAct system, integrating LLMs with the ReAct paradigm to simulate perspective taking in human collaboration, finding that higher developmental stages improve collaborative performance.", "motivation": "To address the lack of computational models that simulate both language and embodied perspective taking in human collaboration.", "method": "The study employs the PerspAct system, using an extended director task to analyze GPT's narrative generation aligned with developmental stages according to Selman's theory.", "result": "GPT produces narratives consistent with developmentally appropriate stages, which enhance task performance but may shift during interactions.", "conclusion": "Integrating embodied perspective taking and language in LLMs can improve models of developmental dynamics and emphasizes the significance of internal speech during collaborative tasks.", "key_contributions": ["Introduction of the PerspAct system for simulating perspective taking with LLMs", "Demonstration of how narrative generation affects collaborative performance", "Insights into the shifting nature of internal representations during collaboration"], "limitations": "", "keywords": ["Language Models", "Perspective Taking", "Collaboration", "Internal Narratives", "Human-Computer Interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.11915", "pdf": "https://arxiv.org/pdf/2509.11915.pdf", "abs": "https://arxiv.org/abs/2509.11915", "title": "Uncertainty in Authorship: Why Perfect AI Detection Is Mathematically Impossible", "authors": ["Aadil Gani Ganie"], "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) become more advanced, it is increasingly\ndifficult to distinguish between human-written and AI-generated text. This\npaper draws a conceptual parallel between quantum uncertainty and the limits of\nauthorship detection in natural language. We argue that there is a fundamental\ntrade-off: the more confidently one tries to identify whether a text was\nwritten by a human or an AI, the more one risks disrupting the text's natural\nflow and authenticity. This mirrors the tension between precision and\ndisturbance found in quantum systems. We explore how current detection\nmethods--such as stylometry, watermarking, and neural classifiers--face\ninherent limitations. Enhancing detection accuracy often leads to changes in\nthe AI's output, making other features less reliable. In effect, the very act\nof trying to detect AI authorship introduces uncertainty elsewhere in the text.\nOur analysis shows that when AI-generated text closely mimics human writing,\nperfect detection becomes not just technologically difficult but theoretically\nimpossible. We address counterarguments and discuss the broader implications\nfor authorship, ethics, and policy. Ultimately, we suggest that the challenge\nof AI-text detection is not just a matter of better tools--it reflects a\ndeeper, unavoidable tension in the nature of language itself.", "AI": {"tldr": "This paper discusses the challenges of distinguishing between human and AI-generated text, drawing parallels with quantum uncertainty and identifying inherent trade-offs in detection methods.", "motivation": "To explore the limitations and challenges in detecting AI-generated text as large language models advance.", "method": "The paper draws a conceptual parallel between quantum uncertainty and authorship detection, analyzing current methods like stylometry, watermarking, and neural classifiers.", "result": "Detection accuracy in AI-generated text often comes at the cost of altering the authenticity and flow of the text, making perfect detection increasingly difficult.", "conclusion": "The paper suggests that the challenges in AI-text detection reflect deeper tensions in language itself and cannot be solved merely with better tools.", "key_contributions": ["Conceptual parallels between quantum uncertainty and authorship detection.", "Analysis of inherent limitations in current detection methods.", "Discussion on the broader ethical implications of AI authorship."], "limitations": "Does not provide concrete solutions for improving detection methods.", "keywords": ["AI authorship detection", "quantum uncertainty", "natural language processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.11921", "pdf": "https://arxiv.org/pdf/2509.11921.pdf", "abs": "https://arxiv.org/abs/2509.11921", "title": "Designing LLMs for cultural sensitivity: Evidence from English-Japanese translation", "authors": ["Helene Tenzer", "Oumnia Abidi", "Stefan Feuerriegel"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) are increasingly used in everyday communication,\nincluding multilingual interactions across different cultural contexts. While\nLLMs can now generate near-perfect literal translations, it remains unclear\nwhether LLMs support culturally appropriate communication. In this paper, we\nanalyze the cultural sensitivity of different LLM designs when applied to\nEnglish-Japanese translations of workplace e-mails. Here, we vary the prompting\nstrategies: (1) naive \"just translate\" prompts, (2) audience-targeted prompts\nspecifying the recipient's cultural background, and (3) instructional prompts\nwith explicit guidance on Japanese communication norms. Using a mixed-methods\nstudy, we then analyze culture-specific language patterns to evaluate how well\ntranslations adapt to cultural norms. Further, we examine the appropriateness\nof the tone of the translations as perceived by native speakers. We find that\nculturally-tailored prompting can improve cultural fit, based on which we offer\nrecommendations for designing culturally inclusive LLMs in multilingual\nsettings.", "AI": {"tldr": "This paper examines the cultural sensitivity of different large language model designs in translating workplace e-mails from English to Japanese using varied prompting strategies.", "motivation": "To assess whether LLMs can support culturally appropriate communication beyond literal translations.", "method": "A mixed-methods study analyzing English-Japanese translations using three different prompting strategies.", "result": "Culturally-tailored prompting improved the cultural fit in translations, as evaluated by native speakers.", "conclusion": "Recommendations are provided for designing culturally inclusive LLMs in multilingual contexts.", "key_contributions": ["Analysis of cultural sensitivity in LLMs", "Empirical study of tone appropriateness in translations", "Guidelines for culturally-inclusive LLM prompting strategies"], "limitations": "", "keywords": ["large language models", "cultural sensitivity", "translation", "multilingual communication", "prompt design"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.12805", "pdf": "https://arxiv.org/pdf/2504.12805.pdf", "abs": "https://arxiv.org/abs/2504.12805", "title": "Assessing LLMs in Art Contexts: Critique Generation and Theory of Mind Evaluation", "authors": ["Takaya Arita", "Wenxian Zheng", "Reiji Suzuki", "Fuminori Akiba"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "Corrected a typo in the metadata title only\n  (\"Assesing\"->\"Assessing\"). No changes were made to the PDF or source files", "summary": "This study explored how large language models (LLMs) perform in two areas\nrelated to art: writing critiques of artworks and reasoning about mental states\n(Theory of Mind, or ToM) in art-related situations. For the critique generation\npart, we built a system that combines Noel Carroll's evaluative framework with\na broad selection of art criticism theories. The model was prompted to first\nwrite a full-length critique and then shorter, more coherent versions using a\nstep-by-step prompting process. These AI-generated critiques were then compared\nwith those written by human experts in a Turing test-style evaluation. In many\ncases, human subjects had difficulty telling which was which, and the results\nsuggest that LLMs can produce critiques that are not only plausible in style\nbut also rich in interpretation, as long as they are carefully guided. In the\nsecond part, we introduced new simple ToM tasks based on situations involving\ninterpretation, emotion, and moral tension, which can appear in the context of\nart. These go beyond standard false-belief tests and allow for more complex,\nsocially embedded forms of reasoning. We tested 41 recent LLMs and found that\ntheir performance varied across tasks and models. In particular, tasks that\ninvolved affective or ambiguous situations tended to reveal clearer\ndifferences. Taken together, these results help clarify how LLMs respond to\ncomplex interpretative challenges, revealing both their cognitive limitations\nand potential. While our findings do not directly contradict the so-called\nGenerative AI Paradox--the idea that LLMs can produce expert-like output\nwithout genuine understanding--they suggest that, depending on how LLMs are\ninstructed, such as through carefully designed prompts, these models may begin\nto show behaviors that resemble understanding more closely than we might\nassume.", "AI": {"tldr": "This study investigates how large language models (LLMs) generate art critiques and handle Theory of Mind tasks, highlighting their interpreted potential and cognitive limitations.", "motivation": "To explore the capabilities of LLMs in generating art critiques and reasoning about mental states in art-related situations.", "method": "The study involved creating a system combining art criticism theories to generate critiques, which were evaluated against human critiques using Turing test-style evaluation. Additionally, new Theory of Mind tasks were devised to assess LLM performance in complex interpretative challenges.", "result": "LLMs produced plausible art critiques, often indistinguishable from human-generated critiques, and showed varying performance on Theory of Mind tasks, particularly in emotionally ambiguous scenarios.", "conclusion": "The findings reveal that LLMs can simulate understanding more effectively with careful prompting, shedding light on their cognitive limitations while not contradicting the Generative AI Paradox.", "key_contributions": ["Development of a system for generating art critiques using LLMs", "Creation of new Theory of Mind tasks for evaluating interpretative reasoning in art", "Insights into the cognitive limitations and potential of LLMs in artistic contexts"], "limitations": "The results suggest a limitation in LLMs' genuine understanding, reinforcing aspects of the Generative AI Paradox.", "keywords": ["large language models", "art critiques", "Theory of Mind", "interpretation", "art criticism"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.11961", "pdf": "https://arxiv.org/pdf/2509.11961.pdf", "abs": "https://arxiv.org/abs/2509.11961", "title": "Spec-LLaVA: Accelerating Vision-Language Models with Dynamic Tree-Based Speculative Decoding", "authors": ["Mingxiao Huo", "Jiayi Zhang", "Hewei Wang", "Jinfeng Xu", "Zheyu Chen", "Huilin Tai", "Yijun Chen"], "categories": ["cs.CL"], "comment": "7pages, accepted by ICML TTODLer-FM workshop", "summary": "Vision-Language Models (VLMs) enable powerful multimodal reasoning but suffer\nfrom slow autoregressive inference, limiting their deployment in real-time\napplications. We introduce Spec-LLaVA, a system that applies speculative\ndecoding to accelerate VLMs without sacrificing output quality. Spec-LLaVA\npairs a lightweight draft VLM with a large target model: the draft speculates\nfuture tokens, which the target verifies in parallel, allowing multiple tokens\nto be generated per step. To maximize efficiency, we design a dynamic\ntree-based verification algorithm that adaptively expands and prunes\nspeculative branches using draft model confidence. On MS COCO out-of-domain\nimages, Spec-LLaVA achieves up to 3.28$\\times$ faster decoding on LLaVA-1.5\n(7B, 13B) with no loss in generation quality. This work presents a lossless\nacceleration framework for VLMs using dynamic tree-structured speculative\ndecoding, opening a path toward practical real-time multimodal assistants.\nImportantly, the lightweight draft model design makes the framework amenable to\nresource-constrained or on-device deployment settings.", "AI": {"tldr": "Spec-LLaVA introduces a speculative decoding method to accelerate Vision-Language Models without sacrificing output quality, achieving significant speed improvements in multimodal reasoning tasks.", "motivation": "To address the slow autoregressive inference in Vision-Language Models, which limits their deployment in real-time applications.", "method": "Spec-LLaVA combines a lightweight draft VLM with a larger target model, using speculative decoding to generate multiple tokens per step, supported by a dynamic tree-based verification algorithm that optimally expands and prunes branches according to the draft model's confidence.", "result": "Spec-LLaVA achieves up to 3.28x faster decoding on LLaVA-1.5 models (7B, 13B) on MS COCO out-of-domain images, maintaining the generation quality.", "conclusion": "This framework provides a lossless acceleration for VLMs, enabling practical use in real-time applications, particularly in resource-constrained or on-device settings.", "key_contributions": ["Introduction of Spec-LLaVA for accelerated inference in VLMs", "Dynamic tree-based verification algorithm for efficiency", "Demonstrated threefold increase in decoding speed without quality loss"], "limitations": "", "keywords": ["Vision-Language Models", "speculative decoding", "real-time applications"], "importance_score": 7, "read_time_minutes": 7}}
{"id": "2509.11963", "pdf": "https://arxiv.org/pdf/2509.11963.pdf", "abs": "https://arxiv.org/abs/2509.11963", "title": "ToolRM: Outcome Reward Models for Tool-Calling Large Language Models", "authors": ["Mayank Agarwal", "Ibrahim Abdelaziz", "Kinjal Basu", "Merve Unuvar", "Luis A. Lastras", "Yara Rizk", "Pavan Kapanipathi"], "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) increasingly interact with external tools,\nreward modeling for tool use has become a critical yet underexplored area.\nExisting reward models, trained primarily on natural language outputs, struggle\nto evaluate tool-based reasoning and execution. To quantify this gap, we\nintroduce FC-RewardBench, the first benchmark designed to systematically assess\nreward models' performance in tool-calling scenarios. Our analysis shows that\ncurrent reward models often miss key signals of effective tool use,\nhighlighting the need for domain-specific modeling. To address this, we propose\na training framework for outcome-based reward models using data synthesized\nfrom permissively licensed, open-weight LLMs. We train models ranging from 1.7B\nto 14B parameters and evaluate them across seven out-of-domain benchmarks.\nThese models consistently outperform general-purpose baselines, achieving up to\n25\\% average improvement in downstream task performance and enabling\ndata-efficient fine-tuning through reward-guided filtering.", "AI": {"tldr": "Introducing FC-RewardBench, a benchmark for evaluating reward models in tool use scenarios, highlighting the need for domain-specific modeling.", "motivation": "To address the lack of effective evaluation metrics for reward models in tool-calling scenarios, particularly as large language models increasingly interact with external tools.", "method": "We introduce FC-RewardBench as a systematic benchmark and propose a training framework for outcome-based reward models using synthesized data from open-weight LLMs, training models from 1.7B to 14B parameters across various benchmarks.", "result": "Models trained on FC-RewardBench achieve up to 25% improvement in downstream task performance compared to general-purpose baselines, showcasing the effectiveness of domain-specific reward modeling.", "conclusion": "The findings emphasize the necessity of developing tailored reward models for enhanced tool use evaluation and task performance in LLM applications.", "key_contributions": ["Development of FC-RewardBench benchmark for tool use evaluation", "Proposal of a training framework for outcome-based reward models", "Demonstration of performance improvements with domain-specific modeling"], "limitations": "", "keywords": ["large language models", "reward modeling", "benchmark", "tool use", "outcome-based training"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.11989", "pdf": "https://arxiv.org/pdf/2509.11989.pdf", "abs": "https://arxiv.org/abs/2509.11989", "title": "Query-Focused Extractive Summarization for Sentiment Explanation", "authors": ["Ahmed Moubtahij", "Sylvie Ratté", "Yazid Attabi", "Maxime Dumas"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Constructive analysis of feedback from clients often requires determining the\ncause of their sentiment from a substantial amount of text documents. To assist\nand improve the productivity of such endeavors, we leverage the task of\nQuery-Focused Summarization (QFS). Models of this task are often impeded by the\nlinguistic dissonance between the query and the source documents. We propose\nand substantiate a multi-bias framework to help bridge this gap at a\ndomain-agnostic, generic level; we then formulate specialized approaches for\nthe problem of sentiment explanation through sentiment-based biases and query\nexpansion. We achieve experimental results outperforming baseline models on a\nreal-world proprietary sentiment-aware QFS dataset.", "AI": {"tldr": "This paper presents a multi-bias framework for Query-Focused Summarization (QFS) to effectively analyze client feedback sentiment by addressing linguistic dissonance.", "motivation": "To enhance productivity in analyzing client feedback sentiment from large text documents.", "method": "A multi-bias framework is proposed to bridge linguistic gaps between queries and source documents, alongside specialized approaches for sentiment explanation.", "result": "Experimental results show that the proposed framework significantly outperforms baseline models on a sentiment-aware QFS dataset.", "conclusion": "The study demonstrates the effectiveness of the multi-bias framework in improving QFS for sentiment analysis, suggesting its broader applicability.", "key_contributions": ["Introduction of a multi-bias framework for QFS", "Specialized approaches for sentiment-based biases", "Outperforming baseline models on a proprietary dataset"], "limitations": "", "keywords": ["Query-Focused Summarization", "sentiment analysis", "multi-bias framework"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.11991", "pdf": "https://arxiv.org/pdf/2509.11991.pdf", "abs": "https://arxiv.org/abs/2509.11991", "title": "Text Adaptation to Plain Language and Easy Read via Automatic Post-Editing Cycles", "authors": ["Jesús Calleja", "David Ponce", "Thierry Etchegoyhen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We describe Vicomtech's participation in the CLEARS challenge on text\nadaptation to Plain Language and Easy Read in Spanish. Our approach features\nautomatic post-editing of different types of initial Large Language Model\nadaptations, where successive adaptations are generated iteratively until\nreadability and similarity metrics indicate that no further adaptation\nrefinement can be successfully performed. Taking the average of all official\nmetrics, our submissions achieved first and second place in Plain language and\nEasy Read adaptation, respectively.", "AI": {"tldr": "Vicomtech participated in the CLEARS challenge, focusing on text adaptation to Plain Language and Easy Read in Spanish, achieving top rankings.", "motivation": "The need for better accessibility in written Spanish through adaptations of texts to Plain Language and Easy Read formats.", "method": "The approach involves automatic post-editing of various Large Language Model adaptations, refined iteratively based on readability and similarity metrics.", "result": "The submissions achieved first place in Plain Language adaptation and second place in Easy Read adaptation.", "conclusion": "The iterative refinement process significantly improved the quality of text adaptations, demonstrating the effectiveness of the approach.", "key_contributions": ["Introduced iterative refining methodology for text adaptation", "Achieved top rankings in CLEARS challenge", "Focused on accessibility in language through LLM adaptations"], "limitations": "", "keywords": ["text adaptation", "Plain Language", "Easy Read", "Large Language Models", "accessibility"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2509.12065", "pdf": "https://arxiv.org/pdf/2509.12065.pdf", "abs": "https://arxiv.org/abs/2509.12065", "title": "Steering Language Models in Multi-Token Generation: A Case Study on Tense and Aspect", "authors": ["Alina Klerings", "Jannik Brinkmann", "Daniel Ruffinelli", "Simone Ponzetto"], "categories": ["cs.CL", "I.2.7"], "comment": "to be published in The 2025 Conference on Empirical Methods in\n  Natural Language Processing", "summary": "Large language models (LLMs) are able to generate grammatically well-formed\ntext, but how do they encode their syntactic knowledge internally? While prior\nwork has focused largely on binary grammatical contrasts, in this work, we\nstudy the representation and control of two multidimensional hierarchical\ngrammar phenomena - verb tense and aspect - and for each, identify distinct,\northogonal directions in residual space using linear discriminant analysis.\nNext, we demonstrate causal control over both grammatical features through\nconcept steering across three generation tasks. Then, we use these identified\nfeatures in a case study to investigate factors influencing effective steering\nin multi-token generation. We find that steering strength, location, and\nduration are crucial parameters for reducing undesirable side effects such as\ntopic shift and degeneration. Our findings suggest that models encode tense and\naspect in structurally organized, human-like ways, but effective control of\nsuch features during generation is sensitive to multiple factors and requires\nmanual tuning or automated optimization.", "AI": {"tldr": "This paper explores how large language models encode syntactic knowledge, specifically focusing on verb tense and aspect. It demonstrates causal control over these grammatical features and identifies crucial factors for effective steering during text generation.", "motivation": "To understand how large language models internally encode their syntactic knowledge and to enhance the control over their grammatical features during text generation.", "method": "The study uses linear discriminant analysis to identify distinct directions in residual space related to verb tense and aspect, and demonstrates causal control through concept steering in generation tasks.", "result": "The researchers found that steering strength, location, and duration significantly influence the effectiveness of controlling grammatical features and reducing undesired side effects during multi-token generation.", "conclusion": "Models encode grammatical features like tense and aspect in a structurally organized manner akin to human language, but effective control necessitates careful tuning and optimization due to sensitivity to various factors.", "key_contributions": ["Identifies orthogonal directions in the residual space for verb tense and aspect in LLMs.", "Demonstrates causal control over grammatical features through concept steering.", "Investigates critical parameters affecting steering effectiveness during text generation."], "limitations": "The paper primarily focuses on two specific grammatical aspects and may not generalize to all syntactic features or languages.", "keywords": ["large language models", "grammatical control", "verb tense", "aspect", "concept steering"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.12093", "pdf": "https://arxiv.org/pdf/2509.12093.pdf", "abs": "https://arxiv.org/abs/2509.12093", "title": "SENSE models: an open source solution for multilingual and multimodal semantic-based tasks", "authors": ["Salima Mdhaffar", "Haroun Elleuch", "Chaimae Chellaf", "Ha Nguyen", "Yannick Estève"], "categories": ["cs.CL"], "comment": "Accepted to IEEE ASRU 2025", "summary": "This paper introduces SENSE (Shared Embedding for N-lingual Speech and tExt),\nan open-source solution inspired by the SAMU-XLSR framework and conceptually\nsimilar to Meta AI's SONAR models. These approaches rely on a teacher-student\nframework to align a self-supervised speech encoder with the language-agnostic\ncontinuous representations of a text encoder at the utterance level. We\ndescribe how the original SAMU-XLSR method has been updated by selecting a\nstronger teacher text model and a better initial speech encoder. The source\ncode for training and using SENSE models has been integrated into the\nSpeechBrain toolkit, and the first SENSE model we trained has been publicly\nreleased. We report experimental results on multilingual and multimodal\nsemantic tasks, where our SENSE model achieves highly competitive performance.\nFinally, this study offers new insights into how semantics are captured in such\nsemantically aligned speech encoders.", "AI": {"tldr": "This paper presents SENSE, a model for aligning speech and text encoders using a teacher-student framework, achieving competitive performance on multilingual tasks.", "motivation": "The need for improved alignment of speech and text representations in multilingual contexts.", "method": "Introducing SENSE, an open-source model that aligns self-supervised speech encoders with language-agnostic text embeddings through a teacher-student framework.", "result": "SENSE achieves competitive performance on multilingual and multimodal semantic tasks compared to existing models.", "conclusion": "The findings provide insights into semantic capture in aligned speech encoders, highlighting improvements over the previous SAMU-XLSR methodology.", "key_contributions": ["Development of SENSE, an open-source multilingual alignment model", "Integration of SENSE into the SpeechBrain toolkit", "Competitive performance in multilingual semantic tasks"], "limitations": "", "keywords": ["SENSE", "speech processing", "multilingual", "machine learning", "natural language processing"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2509.12098", "pdf": "https://arxiv.org/pdf/2509.12098.pdf", "abs": "https://arxiv.org/abs/2509.12098", "title": "Is 'Hope' a person or an idea? A pilot benchmark for NER: comparing traditional NLP tools and large language models on ambiguous entities", "authors": ["Payam Latifi"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 9 figures, 2 tables. This is a pilot study evaluating six\n  NER systems -- three traditional tools (NLTK, spaCy, Stanza) and three LLMs\n  (Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B) -- on a small, ambiguity-rich\n  dataset of 119 tokens. The annotated dataset, prompts are provided in\n  appendices for full reproducibility. All experiments were conducted on 14 May\n  2025", "summary": "This pilot study presents a small-scale but carefully annotated benchmark of\nNamed Entity Recognition (NER) performance across six systems: three non-LLM\nNLP tools (NLTK, spaCy, Stanza) and three general-purpose large language models\n(LLMs: Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B). The dataset contains 119\ntokens covering five entity types (PERSON, LOCATION, ORGANIZATION, DATE, TIME).\nWe evaluated each system's output against the manually annotated gold standard\ndataset using F1-score. The results show that LLMs generally outperform\nconventional tools in recognizing context-sensitive entities like person names,\nwith Gemini achieving the highest average F1-score. However, traditional\nsystems like Stanza demonstrate greater consistency in structured tags such as\nLOCATION and DATE. We also observed variability among LLMs, particularly in\nhandling temporal expressions and multi-word organizations. Our findings\nhighlight that while LLMs offer improved contextual understanding, traditional\ntools remain competitive in specific tasks, informing model selection.", "AI": {"tldr": "This study benchmarks Named Entity Recognition (NER) performance of three traditional NLP tools and three LLMs on a small dataset, finding LLMs generally outperform traditional tools, but with some exceptions.", "motivation": "To evaluate the NER capabilities of different NLP systems and understand their strengths in recognizing context-sensitive entities.", "method": "The performance of six systems (NLTK, spaCy, Stanza, Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B) was assessed on a benchmark dataset of 119 tokens across five entity types, using F1-score for evaluation against a gold standard dataset.", "result": "LLMs outperformed conventional tools in recognizing context-sensitive entities, with Gemini achieving the highest F1-score, but traditional tools like Stanza showed better consistency on structured tags like LOCATION and DATE.", "conclusion": "While LLMs provide enhanced contextual understanding, traditional tools remain competitive for specific tasks, suggesting that the choice of model should depend on the specific requirements of the NER task.", "key_contributions": ["Benchmarking NER performance across traditional NLP tools and LLMs", "Identification of strengths and weaknesses of each system", "Contributions to model selection strategies in NER tasks"], "limitations": "The study is based on a small-scale dataset, which may limit the generalizability of the findings.", "keywords": ["Named Entity Recognition", "NLP tools", "Large Language Models", "F1-score", "Contextual understanding"], "importance_score": 8, "read_time_minutes": 14}}
{"id": "2509.12101", "pdf": "https://arxiv.org/pdf/2509.12101.pdf", "abs": "https://arxiv.org/abs/2509.12101", "title": "In-domain SSL pre-training and streaming ASR", "authors": ["Jarod Duret", "Salima Mdhaffar", "Gaëlle Laperrière", "Ryan Whetten", "Audrey Galametz", "Catherine Kobus", "Marion-Cécile Martin", "Jo Oleiwan", "Yannick Estève"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to SPECOM 2025", "summary": "In this study, we investigate the benefits of domain-specific self-supervised\npre-training for both offline and streaming ASR in Air Traffic Control (ATC)\nenvironments. We train BEST-RQ models on 4.5k hours of unlabeled ATC data, then\nfine-tune on a smaller supervised ATC set. To enable real-time processing, we\npropose using chunked attention and dynamic convolutions, ensuring low-latency\ninference. We compare these in-domain SSL models against state-of-the-art,\ngeneral-purpose speech encoders such as w2v-BERT 2.0 and HuBERT. Results show\nthat domain-adapted pre-training substantially improves performance on standard\nATC benchmarks, significantly reducing word error rates when compared to models\ntrained on broad speech corpora. Furthermore, the proposed streaming approach\nfurther improves word error rate under tighter latency constraints, making it\nparticularly suitable for safety-critical aviation applications. These findings\nhighlight that specializing SSL representations for ATC data is a practical\npath toward more accurate and efficient ASR systems in real-world operational\nsettings.", "AI": {"tldr": "This study explores domain-specific self-supervised pre-training for ASR in Air Traffic Control, showing improvements in performance compared to general models.", "motivation": "To enhance the performance of ASR systems in Air Traffic Control environments through specialized pre-training techniques.", "method": "Training BEST-RQ models on 4.5k hours of unlabeled ATC data followed by fine-tuning with a smaller supervised ATC dataset. Introducing chunked attention and dynamic convolutions for low-latency inference.", "result": "Domain-adapted pre-training significantly reduced word error rates on ATC benchmarks compared to general speech encoders. The streaming approach improved performance under tighter latency constraints.", "conclusion": "Specializing self-supervised learning representations for ATC data provides a viable route to improving ASR accuracy and efficiency in critical aviation applications.", "key_contributions": ["New self-supervised learning models specifically designed for ATC environments.", "Demonstrated low-latency processing using chunked attention and dynamic convolutions.", "Significant reduction in word error rates compared to state-of-the-art models."], "limitations": "", "keywords": ["Air Traffic Control", "ASR", "self-supervised learning", "low-latency inference", "safety-critical applications"], "importance_score": 2, "read_time_minutes": 8}}
{"id": "2509.12108", "pdf": "https://arxiv.org/pdf/2509.12108.pdf", "abs": "https://arxiv.org/abs/2509.12108", "title": "GTA: Supervised-Guided Reinforcement Learning for Text Classification with Large Language Models", "authors": ["Min Zeng", "Jinfei Sun", "Xueyou Luo", "Caiquan Liu", "Shiqi Zhang", "Li Xie", "Xiaoxin Chen"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2025", "summary": "In natural language processing tasks, pure reinforcement learning (RL)\nfine-tuning methods often suffer from inefficient exploration and slow\nconvergence; while supervised fine-tuning (SFT) methods, although efficient in\ntraining, have limited performance ceiling and less solid theoretical\nfoundation compared to RL. To address efficiency-capability trade-off, we\npropose the Guess-Think-Answer (GTA) framework that combines the efficiency of\nSFT with the capability gains of RL in a unified training paradigm. GTA works\nby having the model first produce a provisional guess (optimized via\ncross-entropy loss), then reflect on this guess before generating the final\nanswer, with RL rewards shaping both the final output and the format of the\nentire GTA structure. This hybrid approach achieves both faster convergence\nthan pure RL and higher performance ceiling than pure SFT. To mitigate gradient\nconflicts between the two training signals, we employ loss masking and gradient\nconstraints. Empirical results on four text classification benchmarks\ndemonstrate that GTA substantially accelerates convergence while outperforming\nboth standalone SFT and RL baselines.", "AI": {"tldr": "The GTA framework combines supervised fine-tuning and reinforcement learning for improved efficiency and performance in NLP tasks.", "motivation": "To address the limitations of current reinforcement learning and supervised fine-tuning methods in natural language processing.", "method": "The model produces a provisional guess optimized through cross-entropy loss, reflects on this guess, and generates the final answer, utilizing RL rewards for both output and structure.", "result": "GTA framework achieves faster convergence than pure RL and higher performance than pure SFT across four text classification benchmarks.", "conclusion": "GTA significantly accelerates convergence and outperforms both standalone SFT and RL methods.", "key_contributions": ["Introduction of the Guess-Think-Answer framework.", "Combining efficiency of SFT with capability of RL in NLP tasks.", "Using loss masking and gradient constraints to mitigate conflicts."], "limitations": "", "keywords": ["Reinforcement Learning", "Supervised Fine-Tuning", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.12112", "pdf": "https://arxiv.org/pdf/2509.12112.pdf", "abs": "https://arxiv.org/abs/2509.12112", "title": "CBP-Tuning: Efficient Local Customization for Black-box Large Language Models", "authors": ["Jiaxuan Zhao", "Naibin Gu", "Yuchen Feng", "Xiyu Liu", "Peng Fu", "Zheng Lin", "Weiping Wang"], "categories": ["cs.CL"], "comment": null, "summary": "The high costs of customizing large language models (LLMs) fundamentally\nlimit their adaptability to user-specific needs. Consequently, LLMs are\nincreasingly offered as cloud-based services, a paradigm that introduces\ncritical limitations: providers struggle to support personalized customization\nat scale, while users face privacy risks when exposing sensitive data. To\naddress this dual challenge, we propose Customized Black-box Prompt Tuning\n(CBP-Tuning), a novel framework that facilitates efficient local customization\nwhile preserving bidirectional privacy. Specifically, we design a two-stage\nframework: (1) a prompt generator trained on the server-side to capture\ndomain-specific and task-agnostic capabilities, and (2) user-side gradient-free\noptimization that tailors soft prompts for individual tasks. This approach\neliminates the need for users to access model weights or upload private data,\nrequiring only a single customized vector per task while achieving effective\nadaptation. Furthermore, the evaluation of CBP-Tuning in the commonsense\nreasoning, medical and financial domain settings demonstrates superior\nperformance compared to baselines, showcasing its advantages in task-agnostic\nprocessing and privacy preservation.", "AI": {"tldr": "This paper presents a framework called Customized Black-box Prompt Tuning (CBP-Tuning) that enables efficient local customization of large language models while maintaining user privacy.", "motivation": "To overcome the limitations of cloud-based LLMs in terms of personalization and privacy.", "method": "The proposed framework has a two-stage design: a server-side prompt generator captures domain-specific capabilities, and a user-side gradient-free optimization tailors soft prompts for tasks without exposing private data.", "result": "CBP-Tuning achieves effective adaptation with superior performance in commonsense reasoning, medical, and financial domains compared to existing baselines.", "conclusion": "The framework provides a solution for scalable model customization while enhancing privacy preservation.", "key_contributions": ["Introducing CBP-Tuning framework for LLM customization", "Bidirectional privacy preservation", "Evaluated performance in diverse domains showing advantages over traditional methods"], "limitations": "", "keywords": ["Large Language Models", "Customized Tuning", "Prompt Engineering", "Privacy Preservation", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.12130", "pdf": "https://arxiv.org/pdf/2509.12130.pdf", "abs": "https://arxiv.org/abs/2509.12130", "title": "XplaiNLP at CheckThat! 2025: Multilingual Subjectivity Detection with Finetuned Transformers and Prompt-Based Inference with Large Language Models", "authors": ["Ariana Sahitaj", "Jiaao Li", "Pia Wenzel Neves", "Fedor Splitt", "Premtim Sahitaj", "Charlott Jakob", "Veronika Solopova", "Vera Schmitt"], "categories": ["cs.CL"], "comment": null, "summary": "This notebook reports the XplaiNLP submission to the CheckThat! 2025 shared\ntask on multilingual subjectivity detection. We evaluate two approaches: (1)\nsupervised fine-tuning of transformer encoders, EuroBERT, XLM-RoBERTa, and\nGerman-BERT, on monolingual and machine-translated training data; and (2)\nzero-shot prompting using two LLMs: o3-mini for Annotation (rule-based\nlabelling) and gpt-4.1-mini for DoubleDown (contrastive rewriting) and\nPerspective (comparative reasoning). The Annotation Approach achieves 1st place\nin the Italian monolingual subtask with an F_1 score of 0.8104, outperforming\nthe baseline of 0.6941. In the Romanian zero-shot setting, the fine-tuned\nXLM-RoBERTa model obtains an F_1 score of 0.7917, ranking 3rd and exceeding the\nbaseline of 0.6461. The same model also performs reliably in the multilingual\ntask and improves over the baseline in Greek. For German, a German-BERT model\nfine-tuned on translated training data from typologically related languages\nyields competitive performance over the baseline. In contrast, performance in\nthe Ukrainian and Polish zero-shot settings falls slightly below the respective\nbaselines, reflecting the challenge of generalization in low-resource\ncross-lingual scenarios.", "AI": {"tldr": "The paper evaluates multilingual subjectivity detection using supervised fine-tuning of transformer models and zero-shot prompting with LLMs, achieving top results in certain tasks.", "motivation": "To enhance multilingual subjectivity detection by evaluating both supervised fine-tuning of transformer encoders and zero-shot techniques using large language models.", "method": "Supervised fine-tuning of models like EuroBERT, XLM-RoBERTa, and German-BERT was performed on monolingual and machine-translated data, along with zero-shot prompting using LLMs o3-mini and gpt-4.1-mini.", "result": "The Annotation Approach secured 1st place in the Italian subtask with an F_1 score of 0.8104. The fine-tuned XLM-RoBERTa model achieved 3rd place in Romanian with an F_1 score of 0.7917. Performance improvements were noted across several languages, though challenges remained in low-resource settings.", "conclusion": "The study demonstrates the effectiveness of combining supervised learning and zero-shot prompting for multilingual subjectivity detection tasks, while highlighting challenges in certain cross-lingual scenarios.", "key_contributions": ["First place achieved in the Italian monolingual subtask with advanced models.", "Demonstrated competitive performance in multilingual tasks with existing transformer models.", "Highlighted the challenges of low-resource language generalization in subjectivity detection."], "limitations": "Performance in the Ukrainian and Polish zero-shot settings was below the respective baselines, indicating difficulties in low-resource languages.", "keywords": ["multilingual", "subjectivity detection", "transformer encoders", "zero-shot prompting", "large language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.12158", "pdf": "https://arxiv.org/pdf/2509.12158.pdf", "abs": "https://arxiv.org/abs/2509.12158", "title": "Pun Unintended: LLMs and the Illusion of Humor Understanding", "authors": ["Alessandro Zangari", "Matteo Marcuzzo", "Andrea Albarelli", "Mohammad Taher Pilehvar", "Jose Camacho-Collados"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Puns are a form of humorous wordplay that exploits polysemy and phonetic\nsimilarity. While LLMs have shown promise in detecting puns, we show in this\npaper that their understanding often remains shallow, lacking the nuanced grasp\ntypical of human interpretation. By systematically analyzing and reformulating\nexisting pun benchmarks, we demonstrate how subtle changes in puns are\nsufficient to mislead LLMs. Our contributions include comprehensive and nuanced\npun detection benchmarks, human evaluation across recent LLMs, and an analysis\nof the robustness challenges these models face in processing puns.", "AI": {"tldr": "This paper analyzes the limitations of LLMs in detecting and understanding puns, presenting new benchmarks and evaluations.", "motivation": "To investigate the shortcomings of LLMs in comprehending puns, which are complex forms of wordplay.", "method": "The study involves reformulating existing pun benchmarks and conducting human evaluations of recent LLMs on these benchmarks.", "result": "The analysis shows that subtle changes in puns can lead to significant misunderstandings by LLMs, highlighting their shallow understanding of nuanced language.", "conclusion": "Enhanced benchmarks and human evaluation are necessary for improving pun detection in LLMs, revealing significant robustness challenges in current models.", "key_contributions": ["Comprehensive and nuanced pun detection benchmarks created for evaluation of LLMs", "Human evaluation of multiple recent LLMs on pun detection", "Analysis of robustness challenges faced by LLMs in understanding puns"], "limitations": "The study primarily focuses on puns and may not generalize to other forms of humor or different types of wordplay.", "keywords": ["puns", "large language models", "pun detection", "human evaluation", "robustness challenges"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.12168", "pdf": "https://arxiv.org/pdf/2509.12168.pdf", "abs": "https://arxiv.org/abs/2509.12168", "title": "RAGs to Riches: RAG-like Few-shot Learning for Large Language Model Role-playing", "authors": ["Timothy Rupprecht", "Enfu Nan", "Arash Akbari", "Arman Akbari", "Lei Lu", "Priyanka Maan", "Sean Duffy", "Pu Zhao", "Yumei He", "David Kaeli", "Yanzhi Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Role-playing Large language models (LLMs) are increasingly deployed in\nhigh-stakes domains such as healthcare, education, and governance, where\nfailures can directly impact user trust and well-being. A cost effective\nparadigm for LLM role-playing is few-shot learning, but existing approaches\noften cause models to break character in unexpected and potentially harmful\nways, especially when interacting with hostile users. Inspired by\nRetrieval-Augmented Generation (RAG), we reformulate LLM role-playing into a\ntext retrieval problem and propose a new prompting framework called\nRAGs-to-Riches, which leverages curated reference demonstrations to condition\nLLM responses. We evaluate our framework with LLM-as-a-judge preference voting\nand introduce two novel token-level ROUGE metrics: Intersection over Output\n(IOO) to quantity how much an LLM improvises and Intersection over References\n(IOR) to measure few-shot demonstrations utilization rate during the evaluation\ntasks. When simulating interactions with a hostile user, our prompting strategy\nincorporates in its responses during inference an average of 35% more tokens\nfrom the reference demonstrations. As a result, across 453 role-playing\ninteractions, our models are consistently judged as being more authentic, and\nremain in-character more often than zero-shot and in-context Learning (ICL)\nmethods. Our method presents a scalable strategy for building robust,\nhuman-aligned LLM role-playing frameworks.", "AI": {"tldr": "The paper proposes a new prompting framework, RAGs-to-Riches, to improve LLM role-playing by leveraging text retrieval and curated reference demonstrations, resulting in more authentic interactions and better character maintenance during hostile user engagements.", "motivation": "To address issues of trust and well-being in high-stakes domains by enhancing the performance of LLMs when role-playing through a novel prompting framework.", "method": "The study reformulates LLM role-playing as a text retrieval problem and introduces a prompting framework called RAGs-to-Riches, which utilizes curated reference demonstrations.", "result": "LLMs using the proposed framework showed a 35% increase in token utilization from reference demonstrations during hostile user interactions, leading to more authentic role-playing and better character maintenance compared to zero-shot and ICL methods.", "conclusion": "The RAGs-to-Riches framework presents a scalable approach that enhances LLM role-playing by maintaining character integrity and improving user trust in various applications.", "key_contributions": ["Introduction of the RAGs-to-Riches prompting framework", "Development of two novel token-level ROUGE metrics: IOO and IOR", "Demonstrated increased token utilization from reference demonstrations in hostile interactions"], "limitations": "", "keywords": ["Large language models", "Role-playing", "Retrieval-Augmented Generation", "Human-aligned AI", "Few-shot learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.12171", "pdf": "https://arxiv.org/pdf/2509.12171.pdf", "abs": "https://arxiv.org/abs/2509.12171", "title": "Preservation of Language Understanding Capabilities in Speech-aware Large Language Models", "authors": ["Marek Kubis", "Paweł Skórzewski", "Iwona Christop", "Mateusz Czyżnikiewicz", "Jakub Kubiak", "Łukasz Bondaruk", "Marcin Lewandowski"], "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 1 figure", "summary": "The paper presents C3T (Cross-modal Capabilities Conservation Test), a new\nbenchmark for assessing the performance of speech-aware large language models.\nThe benchmark utilizes textual tasks and a voice cloning text-to-speech model\nto quantify the extent to which language understanding capabilities are\npreserved when the model is accessed via speech input. C3T quantifies the\nfairness of the model for different categories of speakers and its robustness\nacross text and speech modalities.", "AI": {"tldr": "C3T benchmark assesses speech-aware large language models' performance on language understanding through speech input.", "motivation": "To evaluate how well language understanding is maintained in LLMs when interacting through speech input.", "method": "The benchmark combines textual tasks with a voice cloning text-to-speech model to measure performance.", "result": "C3T provides quantifiable metrics on model fairness for speaker categories and robustness in text and speech input.", "conclusion": "The benchmark allows for a more comprehensive evaluation of speech-aware models, identifying areas for improvement.", "key_contributions": ["Introduction of C3T benchmark for speech-aware LLMs", "Metrics for fairness across different speaker categories", "Robustness assessment across text and speech modalities"], "limitations": "", "keywords": ["C3T", "speech-aware models", "language understanding", "benchmark", "fairness"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.11206", "pdf": "https://arxiv.org/pdf/2509.11206.pdf", "abs": "https://arxiv.org/abs/2509.11206", "title": "Evalet: Evaluating Large Language Models by Fragmenting Outputs into Functions", "authors": ["Tae Soo Kim", "Heechan Lee", "Yoonjoo Lee", "Joseph Seering", "Juho Kim"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Practitioners increasingly rely on Large Language Models (LLMs) to evaluate\ngenerative AI outputs through \"LLM-as-a-Judge\" approaches. However, these\nmethods produce holistic scores that obscure which specific elements influenced\nthe assessments. We propose functional fragmentation, a method that dissects\neach output into key fragments and interprets the rhetoric functions that each\nfragment serves relative to evaluation criteria -- surfacing the elements of\ninterest and revealing how they fulfill or hinder user goals. We instantiate\nthis approach in Evalet, an interactive system that visualizes fragment-level\nfunctions across many outputs to support inspection, rating, and comparison of\nevaluations. A user study (N=10) found that, while practitioners struggled to\nvalidate holistic scores, our approach helped them identify 48% more evaluation\nmisalignments. This helped them calibrate trust in LLM evaluations and rely on\nthem to find more actionable issues in model outputs. Our work shifts LLM\nevaluation from quantitative scores toward qualitative, fine-grained analysis\nof model behavior.", "AI": {"tldr": "The paper proposes functional fragmentation to improve evaluation of generative AI outputs by dissecting them into fragments to understand their influence on evaluations.", "motivation": "To address the limitations of holistic scoring in LLM evaluations, which obscure the specific elements affecting assessments and trust.", "method": "The method introduces functional fragmentation to break outputs into key fragments and analyze their rhetorical functions in relation to evaluation criteria.", "result": "In a user study, the approach enabled practitioners to identify 48% more evaluation misalignments compared to traditional holistic scoring.", "conclusion": "The method enhances trust in LLM evaluations by allowing for a deeper qualitative analysis of model outputs, moving beyond mere quantitative scores.", "key_contributions": ["Introduction of functional fragmentation for evaluating LLM outputs", "Development of Evalet, an interactive visualization system", "Empirical evidence showing improved identification of evaluation misalignments"], "limitations": "", "keywords": ["Large Language Models", "Evaluation", "Human-Computer Interaction", "Generative AI", "Qualitative Analysis"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.11826", "pdf": "https://arxiv.org/pdf/2509.11826.pdf", "abs": "https://arxiv.org/abs/2509.11826", "title": "Collaborative Document Editing with Multiple Users and AI Agents", "authors": ["Florian Lehmann", "Krystsina Shauchenka", "Daniel Buschek"], "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "comment": "34 pages, 10 figures, 4 tables", "summary": "Current AI writing support tools are largely designed for individuals,\ncomplicating collaboration when co-writers must leave the shared workspace to\nuse AI and then communicate and reintegrate results. We propose integrating AI\nagents directly into collaborative writing environments. Our prototype makes AI\nuse transparent and customisable through two new shared objects: agent profiles\nand tasks. Agent responses appear in the familiar comment feature. In a user\nstudy (N=30), 14 teams worked on writing projects during one week. Interaction\nlogs and interviews show that teams incorporated agents into existing norms of\nauthorship, control, and coordination, rather than treating them as team\nmembers. Agent profiles were viewed as personal territory, while created agents\nand outputs became shared resources. We discuss implications for team-based AI\ninteraction, highlighting opportunities and boundaries for treating AI as a\nshared resource in collaborative work.", "AI": {"tldr": "This paper proposes the integration of AI agents into collaborative writing environments to enhance teamwork and facilitate authorship norms.", "motivation": "To address the challenges of collaboration in writing projects that arise when traditional AI tools are utilized separately by individuals.", "method": "A prototype was developed to incorporate AI agents directly into collaborative writing tools, featuring shared objects like agent profiles and tasks. A user study with 30 participants was conducted to observe their interactions.", "result": "Teams integrated AI agents into their existing collaboration norms, viewing agent profiles as personal while sharing outputs as communal resources.", "conclusion": "The study highlights the potential of treating AI as a shared resource in collaborative work, impacting how teams interact with AI technologies.", "key_contributions": ["Integration of AI agents into collaborative writing environments", "Development of agent profiles and task management features", "Empirical analysis of team interactions with AI in writing projects"], "limitations": "The study's sample size is limited to 30 participants, which may affect the generalizability of the findings.", "keywords": ["AI agents", "collaborative writing", "team interaction", "authorship norms", "shared resources"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.11851", "pdf": "https://arxiv.org/pdf/2509.11851.pdf", "abs": "https://arxiv.org/abs/2509.11851", "title": "The AI Memory Gap: Users Misremember What They Created With AI or Without", "authors": ["Tim Zindulka", "Sven Goller", "Daniela Fernandes", "Robin Welsch", "Daniel Buschek"], "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "comment": "31 pages, 10 figures, 9 tables", "summary": "As large language models (LLMs) become embedded in interactive text\ngeneration, disclosure of AI as a source depends on people remembering which\nideas or texts came from themselves and which were created with AI. We\ninvestigate how accurately people remember the source of content when using AI.\nIn a pre-registered experiment, 184 participants generated and elaborated on\nideas both unaided and with an LLM-based chatbot. One week later, they were\nasked to identify the source (noAI vs withAI) of these ideas and texts. Our\nfindings reveal a significant gap in memory: After AI use, the odds of correct\nattribution dropped, with the steepest decline in mixed human-AI workflows,\nwhere either the idea or elaboration was created with AI. We validated our\nresults using a computational model of source memory. Discussing broader\nimplications, we highlight the importance of considering source confusion in\nthe design and use of interactive text generation technologies.", "AI": {"tldr": "This paper explores how accurately people remember whether ideas or texts were generated by themselves or an AI when using LLM-based chatbots, revealing significant memory gaps after AI use.", "motivation": "To investigate the accuracy of source attribution of ideas generated with the help of AI and its implications for interactive text generation technologies.", "method": "A pre-registered experiment with 184 participants, who generated and elaborated on ideas both unaided and using an LLM-based chatbot, followed by source identification one week later.", "result": "Participants showed a significant drop in correctly attributing sources after using AI, particularly in workflows involving both human and AI contributions.", "conclusion": "The study highlights the critical need to consider source attribution confusion when designing and using interactive text generation technologies.", "key_contributions": ["Identified significant memory gaps in source attribution when using AI-generated content.", "Validated results with a computational model of source memory.", "Provided insights into the implications of these gaps for the design of interactive technologies."], "limitations": "Focused on a relatively small sample size and a specific type of AI interaction (LLM-based chatbot).", "keywords": ["AI attribution", "source memory", "interactive text generation", "LLM", "HCI"], "importance_score": 9, "read_time_minutes": 31}}
{"id": "2305.12766", "pdf": "https://arxiv.org/pdf/2305.12766.pdf", "abs": "https://arxiv.org/abs/2305.12766", "title": "Understanding Emergent In-Context Learning from a Kernel Regression Perspective", "authors": ["Chi Han", "Ziqi Wang", "Han Zhao", "Heng Ji"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Transactions on Machine Learning Research (TMLR 2025)", "summary": "Large language models (LLMs) have initiated a paradigm shift in transfer\nlearning. In contrast to the classic pretraining-then-finetuning procedure, in\norder to use LLMs for downstream prediction tasks, one only needs to provide a\nfew demonstrations, known as in-context examples, without adding more or\nupdating existing model parameters. This in-context learning (ICL) capability\nof LLMs is intriguing, and it is not yet fully understood how pretrained LLMs\nacquire such capabilities. In this paper, we investigate the reason why a\ntransformer-based language model can accomplish in-context learning after\npre-training on a general language corpus by proposing a kernel-regression\nperspective of understanding LLMs' ICL bahaviors when faced with in-context\nexamples. More concretely, we first prove that Bayesian inference on in-context\nprompts can be asymptotically understood as kernel regression $\\hat y = \\sum_i\ny_i K(x, x_i)/\\sum_i K(x, x_i)$ as the number of in-context demonstrations\ngrows. Then, we empirically investigate the in-context behaviors of language\nmodels. We find that during ICL, the attention and hidden features in LLMs\nmatch the behaviors of a kernel regression. Finally, our theory provides\ninsights into multiple phenomena observed in the ICL field: why retrieving\ndemonstrative samples similar to test samples can help, why ICL performance is\nsensitive to the output formats, and why ICL accuracy benefits from selecting\nin-distribution and representative samples. Code and resources are publicly\navailable at https://github.com/Glaciohound/Explain-ICL-As-Kernel-Regression.", "AI": {"tldr": "This paper investigates how transformer-based language models perform in-context learning using a kernel-regression perspective, showing that ICL behaviors can be understood through Bayesian inference and kernel regression.", "motivation": "Understanding the mechanisms behind in-context learning (ICL) in large language models (LLMs) as a new paradigm in transfer learning.", "method": "The paper proves that Bayesian inference on in-context prompts can be understood as kernel regression when the number of demonstrations increases, and empirically investigates the in-context behaviors of LLMs.", "result": "The study finds that the attention and hidden features in LLMs align with kernel regression behaviors during ICL, providing insights into why certain sample retrieval methods and output formats influence performance.", "conclusion": "The findings enhance our understanding of various ICL phenomena and offer a theoretical grounding for the observed behaviors in LLM applications.", "key_contributions": ["Introduces a kernel-regression perspective for understanding ICL in LLMs.", "Proves the asymptotic relationship between Bayesian inference and kernel regression in ICL contexts.", "Empirically links LLM behaviors during ICL to the principles of kernel regression."], "limitations": "", "keywords": ["in-context learning", "large language models", "kernel regression", "Bayesian inference", "transformer models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2307.06979", "pdf": "https://arxiv.org/pdf/2307.06979.pdf", "abs": "https://arxiv.org/abs/2307.06979", "title": "Tackling Fake News in Bengali: Unraveling the Impact of Summarization vs. Augmentation on Pre-trained Language Models", "authors": ["Arman Sakif Chowdhury", "G. M. Shahariar", "Ahammed Tarik Aziz", "Syed Mohibul Alam", "Md. Azad Sheikh", "Tanveer Ahmed Belal"], "categories": ["cs.CL"], "comment": "Accepted, In Production", "summary": "With the rise of social media and online news sources, fake news has become a\nsignificant issue globally. However, the detection of fake news in low resource\nlanguages like Bengali has received limited attention in research. In this\npaper, we propose a methodology consisting of four distinct approaches to\nclassify fake news articles in Bengali using summarization and augmentation\ntechniques with five pre-trained language models. Our approach includes\ntranslating English news articles and using augmentation techniques to curb the\ndeficit of fake news articles. Our research also focused on summarizing the\nnews to tackle the token length limitation of BERT based models. Through\nextensive experimentation and rigorous evaluation, we show the effectiveness of\nsummarization and augmentation in the case of Bengali fake news detection. We\nevaluated our models using three separate test datasets. The BanglaBERT Base\nmodel, when combined with augmentation techniques, achieved an impressive\naccuracy of 96% on the first test dataset. On the second test dataset, the\nBanglaBERT model, trained with summarized augmented news articles achieved 97%\naccuracy. Lastly, the mBERT Base model achieved an accuracy of 86% on the third\ntest dataset which was reserved for generalization performance evaluation. The\ndatasets and implementations are available at\nhttps://github.com/arman-sakif/Bengali-Fake-News-Detection", "AI": {"tldr": "This paper proposes a methodology for detecting fake news in Bengali, utilizing summarization and augmentation techniques with various pre-trained language models.", "motivation": "The detection of fake news in low resource languages like Bengali has received limited attention, necessitating new approaches to tackle this issue.", "method": "The study employs four distinct approaches, translating English articles and using summarization and augmentation techniques with five pre-trained language models to classify Bengali fake news.", "result": "The BanglaBERT Base model achieved 96% accuracy on the first test dataset, the summarized augmented BanglaBERT model reached 97% on the second dataset, and the mBERT Base model scored 86% on the third dataset for generalization evaluation.", "conclusion": "The effectiveness of summarization and augmentation techniques in Bengali fake news detection is demonstrated through extensive experimentation.", "key_contributions": ["Development of a fake news detection methodology for Bengali using summarization and augmentation techniques.", "Utilization of multiple pre-trained language models to enhance the classification process.", "Presentation of high accuracy results across multiple test datasets."], "limitations": "Limited focus on languages other than Bengali and potential challenges in applying the methodology to different cultural contexts.", "keywords": ["fake news detection", "Bengali language", "summarization", "augmentation techniques", "pre-trained language models"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2309.01219", "pdf": "https://arxiv.org/pdf/2309.01219.pdf", "abs": "https://arxiv.org/abs/2309.01219", "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models", "authors": ["Yue Zhang", "Yafu Li", "Leyang Cui", "Deng Cai", "Lemao Liu", "Tingchen Fu", "Xinting Huang", "Enbo Zhao", "Yu Zhang", "Chen Xu", "Yulong Chen", "Longyue Wang", "Anh Tuan Luu", "Wei Bi", "Freda Shi", "Shuming Shi"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "work in progress;", "summary": "While large language models (LLMs) have demonstrated remarkable capabilities\nacross a range of downstream tasks, a significant concern revolves around their\npropensity to exhibit hallucinations: LLMs occasionally generate content that\ndiverges from the user input, contradicts previously generated context, or\nmisaligns with established world knowledge. This phenomenon poses a substantial\nchallenge to the reliability of LLMs in real-world scenarios. In this paper, we\nsurvey recent efforts on the detection, explanation, and mitigation of\nhallucination, with an emphasis on the unique challenges posed by LLMs. We\npresent taxonomies of the LLM hallucination phenomena and evaluation\nbenchmarks, analyze existing approaches aiming at mitigating LLM hallucination,\nand discuss potential directions for future research.", "AI": {"tldr": "This paper surveys hallucination in large language models, discussing detection, explanation, and mitigation strategies, and providing taxonomies and benchmarks.", "motivation": "To address the reliability issues posed by hallucination in LLMs, which can generate misleading or incorrect content.", "method": "The paper surveys existing literature, presents taxonomies of hallucination phenomena, and evaluates current mitigation approaches.", "result": "The paper identifies various types of hallucination in LLMs and discusses the effectiveness of different detection and mitigation techniques.", "conclusion": "While progress has been made in understanding and addressing LLM hallucinations, further research is needed to enhance reliability in practical applications.", "key_contributions": ["Survey of hallucination detection and mitigation methods", "Taxonomies of LLM hallucination phenomena", "Evaluation benchmarks for assessing hallucination in LLMs"], "limitations": "The paper is a work in progress and may lack comprehensive analysis of all existing methods.", "keywords": ["large language models", "hallucination", "detection", "mitigation", "taxonomy"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2407.11862", "pdf": "https://arxiv.org/pdf/2407.11862.pdf", "abs": "https://arxiv.org/abs/2407.11862", "title": "LML: A Novel Lexicon for the Moral Foundation of Liberty", "authors": ["Oscar Araque", "Lorenzo Gatti", "Sergio Consoli", "Kyriaki Kalimeri"], "categories": ["cs.CL"], "comment": "Published in the 11th International Conference on Machine Learning,\n  Optimization, and Data Science", "summary": "The moral value of liberty is a central concept in our inference system when\nit comes to taking a stance towards controversial social issues such as vaccine\nhesitancy, climate change, or the right to abortion. Here, we propose a novel\nLiberty lexicon evaluated on more than 3,000 manually annotated data both in\nin- and out-of-domain scenarios. As a result of this evaluation, we produce a\ncombined lexicon that constitutes the main outcome of this work. This final\nlexicon incorporates information from an ensemble of lexicons that have been\ngenerated using word embedding similarity (WE) and compositional semantics\n(CS). Our key contributions include enriching the liberty annotations,\ndeveloping a robust liberty lexicon for broader application, and revealing the\ncomplexity of expressions related to liberty across different platforms.\nThrough the evaluation, we show that the difficulty of the task calls for\ndesigning approaches that combine knowledge, in an effort of improving the\nrepresentations of learning systems.", "AI": {"tldr": "The paper proposes a novel Liberty lexicon aimed at enhancing inference systems related to social issues, evaluated on over 3,000 annotated data points.", "motivation": "The moral value of liberty impacts stances on controversial issues, necessitating a robust understanding and representation of such concepts in inference systems.", "method": "A novel Liberty lexicon was developed and evaluated using word embedding similarity and compositional semantics on a dataset of over 3,000 annotated examples.", "result": "The evaluation resulted in a combined lexicon that effectively incorporates information from various lexicons, demonstrating improved representations in learning systems.", "conclusion": "The findings indicate that addressing the complexity of liberty-related expressions requires combined knowledge approaches, enhancing system performance.", "key_contributions": ["Enrichment of liberty annotations", "Development of a robust liberty lexicon", "Revelation of complexity in expressions related to liberty across platforms"], "limitations": "", "keywords": ["liberty", "lexicon", "social issues", "inference systems", "word embeddings"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2408.04909", "pdf": "https://arxiv.org/pdf/2408.04909.pdf", "abs": "https://arxiv.org/abs/2408.04909", "title": "Surveying the Landscape of Image Captioning Evaluation: A Comprehensive Taxonomy, Trends and Metrics Analysis", "authors": ["Uri Berger", "Gabriel Stanovsky", "Omri Abend", "Lea Frermann"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "The task of image captioning has recently been gaining popularity, and with\nit the complex task of evaluating the quality of image captioning models. In\nthis work, we present the first survey and taxonomy of over 70 different image\ncaptioning metrics and their usage in hundreds of papers, specifically designed\nto help users select the most suitable metric for their needs. We find that\ndespite the diversity of proposed metrics, the vast majority of studies rely on\nonly five popular metrics, which we show to be weakly correlated with human\nratings. We hypothesize that combining a diverse set of metrics can enhance\ncorrelation with human ratings. As an initial step, we demonstrate that a\nlinear regression-based ensemble method, which we call EnsembEval, trained on\none human ratings dataset, achieves improved correlation across five additional\ndatasets, showing there is a lot of room for improvement by leveraging a\ndiverse set of metrics.", "AI": {"tldr": "This paper surveys over 70 image captioning metrics, analyzing their usage and correlation with human ratings.", "motivation": "To address the need for a comprehensive evaluation of image captioning metrics, ensuring users can select the most appropriate one for their tasks.", "method": "The authors conducted a survey and taxonomy of 70 different metrics, analyzing their application in numerous studies to identify common usage patterns and limitations. They proposed a new ensemble method, EnsembEval, to improve correlation with human ratings.", "result": "The study revealed that most papers depend on only five metrics, which are weakly correlated with human judgment. The new ensemble method showed improved correlation across multiple datasets.", "conclusion": "There is significant potential for improving image captioning evaluation by utilizing a diverse range of metrics, as demonstrated by the EnsembEval method.", "key_contributions": ["First comprehensive survey of image captioning metrics", "Identification of common reliance on a few weakly correlated metrics", "Development of a new ensemble method (EnsembEval) to improve metric evaluation"], "limitations": "The study primarily focuses on correlation and does not delve deeply into the qualitative aspects of generated captions.", "keywords": ["image captioning", "evaluation metrics", "taxonomies", "human ratings", "ensemble methods"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2408.07238", "pdf": "https://arxiv.org/pdf/2408.07238.pdf", "abs": "https://arxiv.org/abs/2408.07238", "title": "Can Advanced LLMs Coach Smaller LLMs? Knowledge Distillation for Goal-Oriented Dialogs", "authors": ["Tong Wang", "K. Sudhir", "Dat Hong"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Enterprises deploying LLMs for goal-oriented dialogs, such as customer\nservice, face a critical trade-off between performance, control, and cost.\nProprietary models like GPT-4 offer strong performance but are costly and\ncannot be self-hosted, raising security and privacy concerns. Open-source\nalternatives offer flexibility and lower token costs but lag in performance. We\nintroduce Guidance Elicitation and Retrieval (GER), a prompt-based knowledge\ndistillation framework where a high-performance teacher LLM coaches a\nlower-performance student without modifying the student's parameters. GER\nextracts tactical guidance for a wide range of dialog scenarios from the\nteacher and stores these scenario-guidance pairs in a structured library. At\ninference time, the student retrieves the relevant guidance and integrates it\ninto its prompt. While GER training can be bootstrapped entirely with synthetic\ndata, its modular design lets it seamlessly augment the synthetic data with\nhuman conversational logs. In addition, the modular design enables easy\nauditing and updating of the guidance library as new scenarios and constraints\nemerge. Experiments show GER's guidance-based coaching outperforms both example\noutput based fine-tuning and non-customized guidance baselines, and generalizes\nacross other contexts and student models. The GER framework is potentially\nextensible to coach human service agents.", "AI": {"tldr": "Introducing GER, a framework for coaching lower-performance LLMs using a high-performance teacher LLM for goal-oriented dialogs.", "motivation": "To address the trade-off between performance, control, and cost in deploying LLMs for goal-oriented dialogs, such as customer service.", "method": "The GER framework uses a high-performance teacher LLM to distill knowledge into a lower-performance student LLM through prompt-based coaching without modifying the student's parameters, using a structured library for guidance retrieval.", "result": "Experiments demonstrate that GER's coaching approach outperforms example output-based fine-tuning and non-customized guidance, generalizing across other contexts and student models.", "conclusion": "GER provides a modular and effective solution for enhancing LLM performance in goal-oriented dialogs while allowing for easy auditing and updating of guidance libraries.", "key_contributions": ["Framework for prompt-based knowledge distillation between LLMs.", "Structured library for scenario-guidance pairs.", "Extensibility to coach human service agents."], "limitations": "", "keywords": ["LLM", "guidance retrieval", "knowledge distillation", "dialog systems", "customer service"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2409.09825", "pdf": "https://arxiv.org/pdf/2409.09825.pdf", "abs": "https://arxiv.org/abs/2409.09825", "title": "GP-GPT: Large Language Model for Gene-Phenotype Mapping", "authors": ["Yanjun Lyu", "Zihao Wu", "Lu Zhang", "Jing Zhang", "Yiwei Li", "Wei Ruan", "Zhengliang Liu", "Xiang Li", "Rongjie Liu", "Chao Huang", "Wentao Li", "Tianming Liu", "Dajiang Zhu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Pre-trained large language models(LLMs) have attracted increasing attention\nin biomedical domains due to their success in natural language processing.\nHowever, the complex traits and heterogeneity of multi-sources genomics data\npose significant challenges when adapting these models to the bioinformatics\nand biomedical field. To address these challenges, we present GP-GPT, the first\nspecialized large language model for genetic-phenotype knowledge representation\nand genomics relation analysis. Our model is fine-tuned in two stages on a\ncomprehensive corpus composed of over 3,000,000 terms in genomics, proteomics,\nand medical genetics, derived from multiple large-scale validated datasets and\nscientific publications. GP-GPT demonstrates proficiency in accurately\nretrieving medical genetics information and performing common genomics analysis\ntasks, such as genomics information retrieval and relationship determination.\nComparative experiments across domain-specific tasks reveal that GP-GPT\noutperforms state-of-the-art LLMs, including Llama2, Llama3 and GPT-4. These\nresults highlight GP-GPT's potential to enhance genetic disease relation\nresearch and facilitate accurate and efficient analysis in the fields of\ngenomics and medical genetics. Our investigation demonstrated the subtle\nchanges of bio-factor entities' representations in the GP-GPT, which suggested\nthe opportunities for the application of LLMs to advancing gene-phenotype\nresearch.", "AI": {"tldr": "GP-GPT is a specialized large language model designed for genetic-phenotype knowledge representation and genomics relation analysis, fine-tuned on a large corpus from biomedical domains.", "motivation": "To improve the adaptation of large language models (LLMs) in the bioinformatics field, specifically for handling complex genomics data.", "method": "GP-GPT was fine-tuned in two stages on a corpus of over 3,000,000 terms related to genomics, proteomics, and medical genetics, sourced from validated datasets and publications.", "result": "GP-GPT outperforms existing state-of-the-art LLMs, such as Llama2, Llama3, and GPT-4, in various domain-specific tasks including genomics information retrieval and relationship determination.", "conclusion": "GP-GPT shows potential for advancing research in genetic diseases and improving analysis in genomics and medical genetics fields.", "key_contributions": ["Introduction of GP-GPT as a specialized LLM for genetic-phenotype knowledge representation.", "Demonstration of GP-GPT's proficiency in genomics analysis tasks.", "Comparison showcasing GP-GPT's superiority over existing LLMs in specific biomedical tasks."], "limitations": "", "keywords": ["large language models", "genomics", "bioinformatics", "medical genetics", "natural language processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2410.02465", "pdf": "https://arxiv.org/pdf/2410.02465.pdf", "abs": "https://arxiv.org/abs/2410.02465", "title": "Revealing the Inherent Instructability of Pre-Trained Language Models", "authors": ["Seokhyun An", "Minji Kim", "Hyounghun Kim"], "categories": ["cs.CL", "cs.AI"], "comment": "Findings of EMNLP 2025 (32 pages). Code available at\n  https://github.com/seokhyunan/response-tuning", "summary": "Instruction tuning -- supervised fine-tuning using instruction-response pairs\n-- is a key step in making pre-trained large language models (LLMs)\ninstructable. Meanwhile, LLMs perform multitask learning during their\npre-training, acquiring extensive knowledge and capabilities. We hypothesize\nthat the pre-training stage can enable them to develop the ability to\ncomprehend and address instructions. To verify this, we propose Response Tuning\n(RT), which removes the instruction and its corresponding mapping to the\nresponse from instruction tuning. Instead, it focuses solely on establishing a\nresponse distribution. Our experiments demonstrate that RT models, trained only\non responses, can effectively respond to a wide range of instructions akin to\ntheir instruction-tuned counterparts. In addition, we observe that the models\ncan recognize and reject unsafe queries after learning a safety policy only\nfrom the response data. Furthermore, we find that these observations extend to\nan in-context learning setting. These findings support our hypothesis,\nhighlighting the extensive inherent capabilities of pre-trained LLMs.", "AI": {"tldr": "The paper explores Response Tuning (RT), which is alternative to instruction tuning for large language models (LLMs), emphasizing response generation without explicit instruction-response pairs.", "motivation": "To investigate whether pre-trained LLMs can independently understand and respond to instructions without the explicit instruction mappings used during instruction tuning.", "method": "The study proposes Response Tuning, where models are trained solely on response data, allowing them to learn to generate appropriate responses to diverse instructions based on pre-acquired knowledge during pre-training.", "result": "RT models successfully replicate the performance of instruction-tuned models by effectively responding to various instructions and demonstrating the ability to reject unsafe queries after being trained solely on response data.", "conclusion": "The findings reinforce the notion that pre-trained LLMs possess significant innate abilities, as they can learn to respond appropriately to instructions through alternative training methods.", "key_contributions": ["Introduction of Response Tuning (RT) for LLMs", "Demonstration of effective response generation capabilities without explicit instruction mappings", "Findings on safety policy recognition in LLM responses based on response data"], "limitations": "The study does not address the potential shortcomings of removing instruction mapping in complex instruction scenarios.", "keywords": ["Large Language Models", "Response Tuning", "Instruction Tuning", "Machine Learning", "Safe Query Recognition"], "importance_score": 8, "read_time_minutes": 32}}
{"id": "2410.18444", "pdf": "https://arxiv.org/pdf/2410.18444.pdf", "abs": "https://arxiv.org/abs/2410.18444", "title": "Evaluating Automatic Speech Recognition Systems for Korean Meteorological Experts", "authors": ["ChaeHun Park", "Hojun Cho", "Jaegul Choo"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "EMNLP 2025 Findings", "summary": "This paper explores integrating Automatic Speech Recognition (ASR) into\nnatural language query systems to improve weather forecasting efficiency for\nKorean meteorologists. We address challenges in developing ASR systems for the\nKorean weather domain, specifically specialized vocabulary and Korean\nlinguistic intricacies. To tackle these issues, we constructed an evaluation\ndataset of spoken queries recorded by native Korean speakers. Using this\ndataset, we assessed various configurations of a multilingual ASR model family,\nidentifying performance limitations related to domain-specific terminology. We\nthen implemented a simple text-to-speech-based data augmentation method, which\nimproved the recognition of specialized terms while maintaining general-domain\nperformance. Our contributions include creating a domain-specific dataset,\ncomprehensive ASR model evaluations, and an effective augmentation technique.\nWe believe our work provides a foundation for future advancements in ASR for\nthe Korean weather forecasting domain.", "AI": {"tldr": "This paper investigates integrating ASR into natural language query systems for improving weather forecasting efficiency in Korea, focusing on unique challenges and solutions for the Korean language.", "motivation": "To enhance the efficiency of weather forecasting for Korean meteorologists using ASR systems that cater to the linguistic and terminological challenges in the Korean language.", "method": "The authors constructed an evaluation dataset of spoken queries, assessed multilingual ASR models, and implemented a text-to-speech data augmentation method.", "result": "Identified performance limitations of ASR models related to specialized terminology and demonstrated improvement in recognition accuracy for domain-specific terms through data augmentation.", "conclusion": "The research provides a foundation for future advancements in ASR in the Korean weather domain, offering a novel dataset and effective techniques for ASR adaptation.", "key_contributions": ["Creation of a domain-specific dataset for Korean ASR", "Comprehensive evaluations of various ASR model configurations", "Implementation of a data augmentation technique to enhance ASR performance for specialized terminology"], "limitations": "Limited to the Korean weather domain; findings may not generalize to other domains or languages.", "keywords": ["Automatic Speech Recognition", "Korean meteorology", "natural language query systems", "data augmentation", "multilingual ASR"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2411.19855", "pdf": "https://arxiv.org/pdf/2411.19855.pdf", "abs": "https://arxiv.org/abs/2411.19855", "title": "Artificial intelligence contribution to translation industry: looking back and forward", "authors": ["Mohammed Q. Shormani"], "categories": ["cs.CL", "cs-CL", "F.2.2; I.2.7"], "comment": "30 pages, 13 figures", "summary": "This study provides a comprehensive analysis of artificial intelligence (AI)\ncontribution to research in the translation industry (ACTI), synthesizing it\nover forty-five years from 1980-2024. 13220 articles were retrieved from three\nsources, namely WoS, Scopus, and Lens; 9836 were unique records, which were\nused for the analysis. I provided two types of analysis, viz., scientometric\nand thematic, focusing on Cluster, Subject categories, Keywords, Bursts,\nCentrality and Research Centers as for the former. For the latter, I provided a\nthematic review for 18 articles, selected purposefully from the articles\ninvolved, centering on purpose, approach, findings, and contribution to ACTI\nfuture directions. This study is significant for its valuable contribution to\nACTI knowledge production over 45 years, emphasizing several trending issues\nand hotspots including Machine translation, Statistical machine translation,\nLow-resource language, Large language model, Arabic dialects, Translation\nquality, and Neural machine translation. The findings reveal that the more AI\ndevelops, the more it contributes to translation industry, as Neural Networking\nAlgorithms have been incorporated and Deep Language Learning Models like\nChatGPT have been launched. However, much rigorous research is still needed to\novercome several problems encountering translation industry, specifically\nconcerning low-resource, multi-dialectical and free word order languages, and\ncultural and religious registers.", "AI": {"tldr": "A comprehensive review of AI's impact on the translation industry from 1980-2024, analyzing over 13,000 articles and highlighting key themes and challenges.", "motivation": "To synthesize over 45 years of research on AI's contribution to the translation industry, identifying trends and future directions.", "method": "Scientific analysis of 13,220 articles from WoS, Scopus, and Lens, focusing on scientometric and thematic review of selected relevant literature.", "result": "The analysis reveals key topics in AI translation research, such as machine translation, neural models, and issues with low-resource languages. It highlights the growing influence of AI in translation practices, particularly through Neural Networking Algorithms and Deep Language Models.", "conclusion": "While AI significantly enhances translation, notable challenges remain, particularly in effectively addressing low-resource and multi-dialectical languages, emphasizing the need for further research.", "key_contributions": ["Development of a comprehensive dataset over 45 years", "Identification of key trends and challenges in the translation sector", "Thematic review of key literature on AI and translation"], "limitations": "The study does not address specific algorithms or implementation details of AI in translation, focusing instead on historical and thematic analysis.", "keywords": ["Artificial Intelligence", "Translation Industry", "Machine Translation", "Neural Models", "Deep Learning"], "importance_score": 4, "read_time_minutes": 30}}
{"id": "2412.07030", "pdf": "https://arxiv.org/pdf/2412.07030.pdf", "abs": "https://arxiv.org/abs/2412.07030", "title": "FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge Distillation for Question Answering", "authors": ["Amirhossein Abaskohi", "Spandana Gella", "Giuseppe Carenini", "Issam H. Laradji"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "comment": "Findings of EMNLP 2025", "summary": "Multimodal multihop question answering (MMQA) requires reasoning over images\nand text from multiple sources. Despite advances in visual question answering,\nthis multihop setting remains underexplored due to a lack of quality datasets.\nExisting methods focus on single-hop, single-modality, or short texts, limiting\nreal-world applications like interpreting educational documents with long,\nmultimodal content. To fill this gap, we introduce FM2DS, the first framework\nfor creating a high-quality dataset for MMQA. Our approach consists of a\n5-stage pipeline that involves acquiring relevant multimodal documents from\nWikipedia, synthetically generating high-level questions and answers, and\nvalidating them through rigorous criteria to ensure data quality. We evaluate\nour methodology by training models on our synthesized dataset and testing on\ntwo benchmarks: MultimodalQA and WebQA. Our results demonstrate that, with an\nequal sample size, models trained on our synthesized data outperform those\ntrained on human-collected data by 1.9 in exact match (EM) score on average.\nAdditionally, we introduce M2QA-Bench with 1k samples, the first benchmark for\nMMQA on long documents, generated using FM2DS and refined by human annotators.\nWe believe our data synthesis method will serve as a strong foundation for\ntraining and evaluating MMQA models.", "AI": {"tldr": "The paper introduces FM2DS, a framework for creating a high-quality dataset for multimodal multihop question answering (MMQA), addressing the limitations of current methods and datasets.", "motivation": "To address the lack of quality datasets for multimodal multihop question answering and enhance real-world applications involving long, multimodal content.", "method": "A 5-stage pipeline for acquiring multimodal documents, generating high-level questions and answers, and validating them for quality.", "result": "Models trained on the FM2DS synthesized dataset outperformed those trained on human-collected data by an average of 1.9 in exact match (EM) score.", "conclusion": "The data synthesis method proposed can serve as a strong foundation for training and evaluating multimodal multihop question answering models.", "key_contributions": ["Introduction of FM2DS for synthesizing MMQA datasets", "Demonstrated superior performance of synthesized dataset over human-collected data", "Development of M2QA-Bench, the first benchmark for MMQA on long documents."], "limitations": "", "keywords": ["multimodal", "question answering", "dataset synthesis", "machine learning", "benchmarking"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.14642", "pdf": "https://arxiv.org/pdf/2412.14642.pdf", "abs": "https://arxiv.org/abs/2412.14642", "title": "Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven Molecule Generation", "authors": ["Jiatong Li", "Junxian Li", "Weida Wang", "Yunqing Liu", "Changmeng Zheng", "Dongzhan Zhou", "Xiao-yong Wei", "Qing Li"], "categories": ["cs.CL"], "comment": "Our codes and datasets are available through\n  https://github.com/phenixace/TOMG-Bench", "summary": "Recently, Large Language Models (LLMs) have shown great potential in natural\nlanguage-driven molecule discovery. However, existing datasets and benchmarks\nfor molecule-text alignment are predominantly built on a one-to-one mapping,\nmeasuring LLMs' ability to retrieve a single, pre-defined answer, rather than\ntheir creative potential to generate diverse, yet equally valid, molecular\ncandidates. To address this critical gap, we propose Speak-to-Structure\n(S^2-Bench}), the first benchmark to evaluate LLMs in open-domain natural\nlanguage-driven molecule generation. S^2-Bench is specifically designed for\none-to-many relationships, challenging LLMs to demonstrate genuine molecular\nunderstanding and generation capabilities. Our benchmark includes three key\ntasks: molecule editing (MolEdit), molecule optimization (MolOpt), and\ncustomized molecule generation (MolCustom), each probing a different aspect of\nmolecule discovery. We also introduce OpenMolIns, a large-scale instruction\ntuning dataset that enables Llama-3.1-8B to surpass the most powerful LLMs like\nGPT-4o and Claude-3.5 on S^2-Bench. Our comprehensive evaluation of 28 LLMs\nshifts the focus from simple pattern recall to realistic molecular design,\npaving the way for more capable LLMs in natural language-driven molecule\ndiscovery.", "AI": {"tldr": "Introducing Speak-to-Structure (S^2-Bench), a benchmark for evaluating LLMs in open-domain molecule generation, addressing the gap in existing benchmarks focused on one-to-one mapping.", "motivation": "To evaluate the creative potential of LLMs in molecule discovery rather than just their ability to retrieve predefined answers.", "method": "Developing the S^2-Bench benchmark, designed for one-to-many relationships, with tasks including molecule editing, optimization, and customized generation; utilizing a large-scale instruction tuning dataset for training.", "result": "Llama-3.1-8B outperforms top LLMs like GPT-4o and Claude-3.5 on the S^2-Bench, demonstrating better molecular understanding and generation capabilities.", "conclusion": "The benchmark paves the way for more capable LLMs in natural language-driven molecule discovery, focusing on realistic molecular design.", "key_contributions": ["Introduction of S^2-Bench for evaluating one-to-many LLM capabilities in molecule generation.", "Creation of OpenMolIns, an instruction tuning dataset for improved performance.", "Comprehensive evaluation of 28 LLMs, shifting the focus from pattern recall to molecular design."], "limitations": "", "keywords": ["Large Language Models", "Molecule Discovery", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2501.04249", "pdf": "https://arxiv.org/pdf/2501.04249.pdf", "abs": "https://arxiv.org/abs/2501.04249", "title": "IOLBENCH: Benchmarking LLMs on Linguistic Reasoning", "authors": ["Satyam Goyal", "Soham Dan"], "categories": ["cs.CL", "I.2"], "comment": null, "summary": "Despite the remarkable advancements and widespread applications of deep\nneural networks, their ability to perform reasoning tasks remains limited,\nparticularly in domains requiring structured, abstract thought. In this paper,\nwe investigate the linguistic reasoning capabilities of state-of-the-art large\nlanguage models (LLMs) by introducing IOLBENCH, a novel benchmark derived from\nInternational Linguistics Olympiad (IOL) problems. This dataset encompasses\ndiverse problems testing syntax, morphology, phonology, and semantics, all\ncarefully designed to be self-contained and independent of external knowledge.\nThese tasks challenge models to engage in metacognitive linguistic reasoning,\nrequiring the deduction of linguistic rules and patterns from minimal examples.\nThrough extensive benchmarking of leading LLMs, we find that even the most\nadvanced models struggle to handle the intricacies of linguistic complexity,\nparticularly in areas demanding compositional generalization and rule\nabstraction. Our analysis highlights both the strengths and persistent\nlimitations of current models in linguistic problem-solving, offering valuable\ninsights into their reasoning capabilities. By introducing IOLBENCH, we aim to\nfoster further research into developing models capable of human-like reasoning,\nwith broader implications for the fields of computational linguistics and\nartificial intelligence.", "AI": {"tldr": "This paper introduces IOLBENCH, a benchmark for assessing linguistic reasoning in LLMs using problems from the International Linguistics Olympiad, revealing the limitations of current models in handling linguistic complexity.", "motivation": "To investigate and enhance the reasoning capabilities of large language models in handling structured linguistic tasks, thereby understanding their limitations and potential for human-like reasoning.", "method": "We created IOLBENCH, a dataset of diverse linguistic problems testing syntax, morphology, phonology, and semantics, and conducted extensive benchmarking on various state-of-the-art large language models.", "result": "Benchmarking results reveal that even advanced models struggle significantly with tasks requiring compositional generalization and rule abstraction, indicating gaps in their linguistic reasoning abilities.", "conclusion": "The paper offers insights into the reasoning capabilities of LLMs and encourages further research towards models that can perform human-like reasoning in linguistics.", "key_contributions": ["Introduction of IOLBENCH, a novel linguistic reasoning benchmark for LLMs.", "Empirical findings showcasing the limitations of current state-of-the-art models in linguistic tasks.", "Insights into the necessity for developing models capable of more complex reasoning similar to humans."], "limitations": "Focused solely on linguistic reasoning; results may not generalize to other domains or broader AI reasoning tasks.", "keywords": ["large language models", "linguistic reasoning", "IOLBENCH", "benchmark", "compositional generalization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.15688", "pdf": "https://arxiv.org/pdf/2501.15688.pdf", "abs": "https://arxiv.org/abs/2501.15688", "title": "Transformer-Based Multimodal Knowledge Graph Completion with Link-Aware Contexts", "authors": ["Haodi Ma", "Dzmitry Kasinets", "Daisy Zhe Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Multimodal knowledge graph completion (MMKGC) aims to predict missing links\nin multimodal knowledge graphs (MMKGs) by leveraging information from various\nmodalities alongside structural data. Existing MMKGC approaches primarily\nextend traditional knowledge graph embedding (KGE) models, which often require\ncreating an embedding for every entity. This results in large model sizes and\ninefficiencies in integrating multimodal information, particularly for\nreal-world graphs. Meanwhile, Transformer-based models have demonstrated\ncompetitive performance in knowledge graph completion (KGC). However, their\nfocus on single-modal knowledge limits their capacity to utilize cross-modal\ninformation. Recently, Large vision-language models (VLMs) have shown potential\nin cross-modal tasks but are constrained by the high cost of training. In this\nwork, we propose a novel approach that integrates Transformer-based KGE models\nwith cross-modal context generated by pre-trained VLMs, thereby extending their\napplicability to MMKGC. Specifically, we employ a pre-trained VLM to transform\nrelevant visual information from entities and their neighbors into textual\nsequences. We then frame KGC as a sequence-to-sequence task, fine-tuning the\nmodel with the generated cross-modal context. This simple yet effective method\nsignificantly reduces model size compared to traditional KGE approaches while\nachieving competitive performance across multiple large-scale datasets with\nminimal hyperparameter tuning.", "AI": {"tldr": "The paper presents a novel method for multimodal knowledge graph completion (MMKGC) by integrating Transformer-based knowledge graph embedding models with cross-modal context from pre-trained vision-language models (VLMs), achieving efficiency and competitive performance.", "motivation": "To address inefficiencies in existing MMKGC approaches that rely heavily on traditional knowledge graph embedding models, which require extensive embeddings and struggle with cross-modal information integration.", "method": "The proposed method integrates Transformer-based knowledge graph embedding models with cross-modal context generated by pre-trained vision-language models. It transforms visual information relevant to entities into textual sequences and frames the knowledge graph completion as a sequence-to-sequence task, fine-tuning the model accordingly.", "result": "The method significantly reduces model size compared to traditional KGE approaches and performs competitively across multiple large-scale datasets with minimal hyperparameter tuning.", "conclusion": "This integration of VLM-generated context into KGC represents a step forward in the effectiveness of MMKGC, making it more efficient and scalable for real-world applications.", "key_contributions": ["Introduces a method for integrating VLMs into MMKGC models.", "Reduces the model size significantly compared to traditional approaches.", "Achieves strong performance with minimal hyperparameter adjustments."], "limitations": "", "keywords": ["multimodal knowledge graphs", "knowledge graph completion", "Transformer models", "vision-language models", "cross-modal tasks"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2502.11451", "pdf": "https://arxiv.org/pdf/2502.11451.pdf", "abs": "https://arxiv.org/abs/2502.11451", "title": "From Personas to Talks: Revisiting the Impact of Personas on LLM-Synthesized Emotional Support Conversations", "authors": ["Shenghan Wu", "Yimo Zhu", "Wynne Hsu", "Mong-Li Lee", "Yang Deng"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025 Main Conference", "summary": "The rapid advancement of Large Language Models (LLMs) has revolutionized the\ngeneration of emotional support conversations (ESC), offering scalable\nsolutions with reduced costs and enhanced data privacy. This paper explores the\nrole of personas in the creation of ESC by LLMs. Our research utilizes\nestablished psychological frameworks to measure and infuse persona traits into\nLLMs, which then generate dialogues in the emotional support scenario. We\nconduct extensive evaluations to understand the stability of persona traits in\ndialogues, examining shifts in traits post-generation and their impact on\ndialogue quality and strategy distribution. Experimental results reveal several\nnotable findings: 1) LLMs can infer core persona traits, 2) subtle shifts in\nemotionality and extraversion occur, influencing the dialogue dynamics, and 3)\nthe application of persona traits modifies the distribution of emotional\nsupport strategies, enhancing the relevance and empathetic quality of the\nresponses. These findings highlight the potential of persona-driven LLMs in\ncrafting more personalized, empathetic, and effective emotional support\ndialogues, which has significant implications for the future design of\nAI-driven emotional support systems.", "AI": {"tldr": "This paper studies the influence of personas in Large Language Models for generating emotional support conversations, revealing that these traits enhance dialogue quality and empathetic responses.", "motivation": "To explore how personas can improve the effectiveness and personalization of emotional support conversations generated by LLMs.", "method": "Utilized established psychological frameworks to measure persona traits and incorporated them into LLM dialogues for emotional support scenarios.", "result": "Found that LLMs can infer persona traits, which lead to shifts in emotionality and extraversion, thereby improving the quality and strategy distribution in dialogues.", "conclusion": "Persona-driven LLMs have the potential to create more personalized and effective emotional support dialogues, important for future AI-driven support systems.", "key_contributions": ["Establishment of psychological frameworks for persona traits in LLMs", "Evaluation of dialogue quality based on persona traits", "Demonstration of improved empathy in generated dialogues"], "limitations": "", "keywords": ["Large Language Models", "Emotional Support Conversations", "Personas", "Human-Computer Interaction", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.12455", "pdf": "https://arxiv.org/pdf/2502.12455.pdf", "abs": "https://arxiv.org/abs/2502.12455", "title": "DSMoE: Matrix-Partitioned Experts with Dynamic Routing for Computation-Efficient Dense LLMs", "authors": ["Minxuan Lv", "Zhenpeng Su", "Leiyu Pan", "Yizhe Xiong", "Zijia Lin", "Hui Chen", "Wei Zhou", "Jungong Han", "Guiguang Ding", "Cheng Luo", "Di Zhang", "Kun Gai", "Songlin Hu"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP main conference", "summary": "As large language models continue to scale, computational costs and resource\nconsumption have emerged as significant challenges. While existing\nsparsification methods like pruning reduce computational overhead, they risk\nlosing model knowledge through parameter removal. This paper proposes DSMoE\n(Dynamic Sparse Mixture-of-Experts), a novel approach that achieves\nsparsification by partitioning pre-trained FFN layers into computational\nblocks. We implement adaptive expert routing using sigmoid activation and\nstraight-through estimators, enabling tokens to flexibly access different\naspects of model knowledge based on input complexity. Additionally, we\nintroduce a sparsity loss term to balance performance and computational\nefficiency. Extensive experiments on LLaMA models demonstrate that under\nequivalent computational constraints, DSMoE achieves superior performance\ncompared to existing pruning and MoE approaches across language modeling and\ndownstream tasks, particularly excelling in generation tasks. Analysis reveals\nthat DSMoE learns distinctive layerwise activation patterns, providing new\ninsights for future MoE architecture design.", "AI": {"tldr": "The paper presents DSMoE, a novel method for sparsifying language models by partitioning FFN layers, enabling adaptive expert routing and superior performance in language tasks.", "motivation": "As large language models scale, computational costs and resource usage pose challenges, prompting the need for effective sparsification methods.", "method": "The proposed DSMoE partitions pre-trained FFN layers into blocks and implements adaptive expert routing using sigmoid activation and straight-through estimators, along with a sparsity loss term for balancing performance and efficiency.", "result": "Experiments show that DSMoE outperforms existing pruning and MoE methods in language modeling and downstream tasks, particularly in generation tasks.", "conclusion": "DSMoE offers new insights into MoE architecture design by learning distinctive layerwise activation patterns.", "key_contributions": ["Introduction of DSMoE for efficient language model sparsification.", "Adaptive expert routing based on input complexity.", "Demonstrated superior performance over traditional sparsification techniques."], "limitations": "", "keywords": ["large language models", "sparsification", "Mixture-of-Experts", "adaptive routing", "language modeling"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2502.13628", "pdf": "https://arxiv.org/pdf/2502.13628.pdf", "abs": "https://arxiv.org/abs/2502.13628", "title": "Efficient Environmental Claim Detection with Hyperbolic Graph Neural Networks", "authors": ["Darpan Aswal", "Manjira Sinha"], "categories": ["cs.CL"], "comment": null, "summary": "Transformer based models, specially large language models (LLMs) dominate the\nfield of NLP with their mass adoption in tasks such as text generation,\nsummarization and fake news detection. These models offer ease of deployment\nand reliability for most applications, however, they require significant\namounts of computational power for training as well as inference. This poses\nchallenges in their adoption in resource-constrained applications, specially in\nthe open-source community where compute availability is usually scarce. This\nwork proposes a graph-based approach for Environmental Claim Detection,\nexploring Graph Neural Networks (GNNs) and Hyperbolic Graph Neural Networks\n(HGNNs) as lightweight yet effective alternatives to transformer-based models.\nRe-framing the task as a graph classification problem, we transform claim\nsentences into dependency parsing graphs, utilizing a combination of word2vec\n\\& learnable part-of-speech (POS) tag embeddings for the node features and\nencoding syntactic dependencies in the edge relations. Our results show that\nour graph-based models, particularly HGNNs in the poincar\\'e space (P-HGNNs),\nachieve performance superior to the state-of-the-art on environmental claim\ndetection while using upto \\textbf{30x fewer parameters}. We also demonstrate\nthat HGNNs benefit vastly from explicitly modeling data in hierarchical\n(tree-like) structures, enabling them to significantly improve over their\neuclidean counterparts.", "AI": {"tldr": "This paper presents a graph-based approach to Environmental Claim Detection, proposing GNNs and HGNNs as efficient alternatives to transformer models, achieving superior performance with fewer parameters.", "motivation": "To address the computational limitations of transformer-based models in resource-constrained applications, particularly in the context of environmental claim detection.", "method": "The authors propose a graph classification approach, converting claim sentences into dependency parsing graphs and utilizing word2vec and learnable POS tag embeddings for node features, while encoding syntactic dependencies in edge relations.", "result": "The graph-based models, especially P-HGNNs, outperform state-of-the-art methods in environmental claim detection with up to 30x fewer parameters.", "conclusion": "HGNNs demonstrate significant advantages when data is structured hierarchically, improving performance over traditional euclidean models.", "key_contributions": ["Proposing a novel graph-based approach for Environmental Claim Detection", "Utilizing Hyperbolic Graph Neural Networks (HGNNs) for improved performance", "Achieving superior results with significantly reduced parameters compared to transformer models"], "limitations": "", "keywords": ["Graph Neural Networks", "Environmental Claim Detection", "Hyperbolic Graph Neural Networks"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2502.14383", "pdf": "https://arxiv.org/pdf/2502.14383.pdf", "abs": "https://arxiv.org/abs/2502.14383", "title": "Rumor Detection by Multi-task Suffix Learning based on Time-series Dual Sentiments", "authors": ["Zhiwei Liu", "Kailai Yang", "Eduard Hovy", "Sophia Ananiadou"], "categories": ["cs.CL"], "comment": "work in progress", "summary": "The widespread dissemination of rumors on social media has a significant\nimpact on people's lives, potentially leading to public panic and fear. Rumors\noften evoke specific sentiments, resonating with readers and prompting sharing.\nTo effectively detect and track rumors, it is essential to observe the\nfine-grained sentiments of both source and response message pairs as the rumor\nevolves over time. However, current rumor detection methods fail to account for\nthis aspect. In this paper, we propose MSuf, the first multi-task suffix\nlearning framework for rumor detection and tracking using time series dual\n(coupled) sentiments. MSuf includes three modules: (1) an LLM to extract\nsentiment intensity features and sort them chronologically; (2) a module that\nfuses the sorted sentiment features with their source text word embeddings to\nobtain an aligned embedding; (3) two hard prompts are combined with the aligned\nvector to perform rumor detection and sentiment analysis using one frozen LLM.\nMSuf effectively enhances the performance of LLMs for rumor detection with only\nminimal parameter fine-tuning. Evaluating MSuf on four rumor detection\nbenchmarks, we find significant improvements compared to other emotion-based\nmethods.", "AI": {"tldr": "This paper introduces MSuf, a novel multi-task suffix learning framework designed to enhance rumor detection and tracking on social media by analyzing time series sentiments.", "motivation": "To address the significant impact of rumors on social media, which can cause public panic and fear, effective detection and tracking mechanisms need to consider nuanced sentiments of messages over time.", "method": "MSuf utilizes a multi-task learning approach with three main modules: it employs an LLM to extract sentiment features chronologically, fuses these features with source text embeddings, and uses hard prompts for rumor detection and sentiment analysis, applying minimal parameter fine-tuning on the LLM.", "result": "MSuf shows significant performance improvements in rumor detection across four benchmark datasets compared to existing emotion-based methods.", "conclusion": "The proposed framework effectively enhances LLM performance in detecting and tracking rumors with minimal fine-tuning, addressing a critical gap in current methodologies.", "key_contributions": ["Introduction of the MSuf framework for rumor detection", "Utilization of time series dual sentiments in analysis", "Demonstrated significant improvement over traditional emotion-based detection methods"], "limitations": "This is a work in progress and may require further validation and refinement in real-world applications.", "keywords": ["rumor detection", "sentiment analysis", "multi-task learning", "social media", "LLM"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2502.17026", "pdf": "https://arxiv.org/pdf/2502.17026.pdf", "abs": "https://arxiv.org/abs/2502.17026", "title": "Understanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology", "authors": ["Longchao Da", "Xiaoou Liu", "Jiaxin Dai", "Lu Cheng", "Yaqing Wang", "Hua Wei"], "categories": ["cs.CL", "cs.AI", "cs.SC", "68T50, 68T37, 68Q32", "I.2.7; I.2.6; I.2.4"], "comment": "28 pages, 9 figures; accepted at COLM'25", "summary": "Understanding the uncertainty in large language model (LLM) explanations is\nimportant for evaluating their faithfulness and reasoning consistency, and thus\nprovides insights into the reliability of LLM's output regarding a question. In\nthis work, we propose a novel framework that quantifies uncertainty in LLM\nexplanations through a reasoning topology perspective. By designing a\nstructural elicitation strategy, we guide the LLMs to frame the explanations of\nan answer into a graph topology. This process decomposes the explanations into\nthe knowledge related sub-questions and topology-based reasoning structures,\nwhich allows us to quantify uncertainty not only at the semantic level but also\nfrom the reasoning path. It further brings convenience to assess knowledge\nredundancy and provide interpretable insights into the reasoning process. Our\nmethod offers a systematic way to interpret the LLM reasoning, analyze\nlimitations, and provide guidance for enhancing robustness and faithfulness.\nThis work pioneers the use of graph-structured uncertainty measurement in LLM\nexplanations and demonstrates the potential of topology-based quantification.", "AI": {"tldr": "This paper presents a framework for quantifying uncertainty in large language model (LLM) explanations by framing them as a reasoning topology, allowing for assessment of faithfulness and reasoning consistency.", "motivation": "Understanding uncertainty in LLM explanations is crucial for evaluating their reliability and reasoning consistency, which impacts the interpretation of their outputs.", "method": "The proposed framework uses a structural elicitation strategy to convert LLM explanations into a graph topology, decomposing them into sub-questions and reasoning structures for uncertainty quantification.", "result": "The method allows quantification of uncertainty both semantically and from the reasoning path, enhancing interpretability and revealing knowledge redundancy.", "conclusion": "The work introduces graph-structured uncertainty measurement for LLM explanations and illustrates its potential for improving robustness and faithfulness in reasoning.", "key_contributions": ["Framework for quantifying uncertainty in LLM explanations", "Graph topology for structuring explanations", "Method for analyzing reasoning paths and knowledge redundancy"], "limitations": "", "keywords": ["large language models", "uncertainty quantification", "graph topology", "explanation faithfulness", "reasoning consistency"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.20258", "pdf": "https://arxiv.org/pdf/2502.20258.pdf", "abs": "https://arxiv.org/abs/2502.20258", "title": "LLM as a Broken Telephone: Iterative Generation Distorts Information", "authors": ["Amr Mohamed", "Mingmeng Geng", "Michalis Vazirgiannis", "Guokan Shang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025, Main Conference", "summary": "As large language models are increasingly responsible for online content,\nconcerns arise about the impact of repeatedly processing their own outputs.\nInspired by the \"broken telephone\" effect in chained human communication, this\nstudy investigates whether LLMs similarly distort information through iterative\ngeneration. Through translation-based experiments, we find that distortion\naccumulates over time, influenced by language choice and chain complexity.\nWhile degradation is inevitable, it can be mitigated through strategic\nprompting techniques. These findings contribute to discussions on the long-term\neffects of AI-mediated information propagation, raising important questions\nabout the reliability of LLM-generated content in iterative workflows.", "AI": {"tldr": "This study investigates the distortion of information in large language models (LLMs) through iterative generation, drawing parallels with the 'broken telephone' effect.", "motivation": "To explore how repeated processing of LLM outputs affects information reliability and propagation.", "method": "Translation-based experiments were conducted to analyze how distortion accumulates over time based on language choice and chain complexity.", "result": "Findings indicate that information distortion is inevitable but can be mitigated with effective prompting techniques.", "conclusion": "The study highlights the potential concerns regarding the reliability of LLM-generated content in iterative workflows and suggests strategies to reduce distortion.", "key_contributions": ["Investigates the 'broken telephone' effect in LLMs", "Demonstrates how language choice and chain complexity affect information distortion", "Proposes strategic prompting techniques to mitigate degradation."], "limitations": "", "keywords": ["large language models", "information distortion", "iterative generation", "prompting techniques", "AI-mediated information propagation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.20344", "pdf": "https://arxiv.org/pdf/2502.20344.pdf", "abs": "https://arxiv.org/abs/2502.20344", "title": "LinguaLens: Towards Interpreting Linguistic Mechanisms of Large Language Models via Sparse Auto-Encoder", "authors": ["Yi Jing", "Zijun Yao", "Hongzhu Guo", "Lingxu Ran", "Xiaozhi Wang", "Lei Hou", "Juanzi Li"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025 MainConference", "summary": "Large language models (LLMs) demonstrate exceptional performance on tasks\nrequiring complex linguistic abilities, such as reference disambiguation and\nmetaphor recognition/generation. Although LLMs possess impressive capabilities,\ntheir internal mechanisms for processing and representing linguistic knowledge\nremain largely opaque. Prior research on linguistic mechanisms is limited by\ncoarse granularity, limited analysis scale, and narrow focus. In this study, we\npropose LinguaLens, a systematic and comprehensive framework for analyzing the\nlinguistic mechanisms of large language models, based on Sparse Auto-Encoders\n(SAEs). We extract a broad set of Chinese and English linguistic features\nacross four dimensions (morphology, syntax, semantics, and pragmatics). By\nemploying counterfactual methods, we construct a large-scale counterfactual\ndataset of linguistic features for mechanism analysis. Our findings reveal\nintrinsic representations of linguistic knowledge in LLMs, uncover patterns of\ncross-layer and cross-lingual distribution, and demonstrate the potential to\ncontrol model outputs. This work provides a systematic suite of resources and\nmethods for studying linguistic mechanisms, offers strong evidence that LLMs\npossess genuine linguistic knowledge, and lays the foundation for more\ninterpretable and controllable language modeling in future research.", "AI": {"tldr": "This paper presents LinguaLens, a framework for analyzing linguistic mechanisms in large language models (LLMs) using Sparse Auto-Encoders to extract linguistic features in both Chinese and English.", "motivation": "To address the lack of understanding of how LLMs process and represent linguistic knowledge despite their remarkable performance in complex linguistic tasks.", "method": "The study utilizes Sparse Auto-Encoders to extract and analyze linguistic features across morphology, syntax, semantics, and pragmatics while employing counterfactual methods to create a dataset for analysis.", "result": "Findings show intrinsic representations of linguistic knowledge in LLMs, revealing patterns of cross-layer and cross-lingual distribution, and suggest potential control over model outputs.", "conclusion": "LinguaLens offers resources and methods to better understand LLMs' linguistic mechanisms, providing evidence of their linguistic knowledge and setting the groundwork for more interpretable models.", "key_contributions": ["Introduction of LinguaLens framework for LLM analysis", "Development of a large-scale counterfactual dataset", "Evidence of intrinsic linguistic knowledge in LLMs"], "limitations": "", "keywords": ["large language models", "linguistic mechanisms", "Sparse Auto-Encoders", "counterfactual methods", "linguistic knowledge"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.03106", "pdf": "https://arxiv.org/pdf/2503.03106.pdf", "abs": "https://arxiv.org/abs/2503.03106", "title": "Monitoring Decoding: Mitigating Hallucination via Evaluating the Factuality of Partial Response during Generation", "authors": ["Yurui Chang", "Bochuan Cao", "Lu Lin"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025 (Findings)", "summary": "While large language models have demonstrated exceptional performance across\na wide range of tasks, they remain susceptible to hallucinations -- generating\nplausible yet factually incorrect contents. Existing methods to mitigating such\nrisk often rely on sampling multiple full-length generations, which introduces\nsignificant response latency and becomes ineffective when the model\nconsistently produces hallucinated outputs with high confidence. To address\nthese limitations, we introduce Monitoring Decoding (MD), a novel framework\nthat dynamically monitors the generation process and selectively applies\nin-process interventions, focusing on revising crucial tokens responsible for\nhallucinations. Instead of waiting until completion of multiple full-length\ngenerations, we identify hallucination-prone tokens during generation using a\nmonitor function, and further refine these tokens through a tree-based decoding\nstrategy. This approach ensures an enhanced factual accuracy and coherence in\nthe generated output while maintaining efficiency. Experimental results\ndemonstrate that MD consistently outperforms self-consistency-based approaches\nin both effectiveness and efficiency, achieving higher factual accuracy while\nsignificantly reducing computational overhead.", "AI": {"tldr": "A novel framework called Monitoring Decoding (MD) is introduced to reduce hallucinations in large language models during text generation by dynamically monitoring crucial tokens and applying interventions.", "motivation": "To address the inherent hallucination problem in large language models that generates plausible but incorrect content, mitigating the risk without introducing significant latency.", "method": "Monitoring Decoding (MD) uses a monitor function to identify hallucination-prone tokens during the generation process and applies a tree-based decoding strategy for in-process interventions.", "result": "MD demonstrates enhanced factual accuracy and coherence in generated outputs, outperforming traditional self-consistency methods in both effectiveness and efficiency with reduced computational overhead.", "conclusion": "The study showcases that targeted intervention during generation, rather than post-generation adjustments, can significantly improve the reliability of language model outputs.", "key_contributions": ["Introduction of Monitoring Decoding (MD) framework", "Dynamic monitoring and selective in-process intervention on hallucination-prone tokens", "Improvement in factual accuracy and efficiency over traditional methods"], "limitations": "", "keywords": ["large language models", "hallucinations", "Monitoring Decoding", "factual accuracy", "text generation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.05362", "pdf": "https://arxiv.org/pdf/2503.05362.pdf", "abs": "https://arxiv.org/abs/2503.05362", "title": "Chain of Strategy Optimization Makes Large Language Models Better Emotional Supporter", "authors": ["Weixiang Zhao", "Xingyu Sui", "Xinyang Han", "Yang Deng", "Yulin Hu", "Jiahe Guo", "Libo Qin", "Qianyun Du", "Shijin Wang", "Yanyan Zhao", "Bing Qin", "Ting Liu"], "categories": ["cs.CL"], "comment": "21 pages, 9 figures, 17 tables", "summary": "The growing emotional stress in modern society has increased the demand for\nEmotional Support Conversations (ESC). While Large Language Models (LLMs) show\npromise for ESC, they face two key challenges: (1) low strategy selection\naccuracy, and (2) preference bias, limiting their adaptability to emotional\nneeds of users. Existing supervised fine-tuning (SFT) struggles to address\nthese issues, as it rigidly trains models on single gold-standard responses\nwithout modeling nuanced strategy trade-offs. To overcome these limitations, we\npropose Chain-of-Strategy Optimization (CSO), a novel approach that optimizes\nstrategy selection preferences at each dialogue turn. We first leverage Monte\nCarlo Tree Search to construct ESC-Pro, a high-quality preference dataset with\nturn-level strategy-response pairs. Training on ESC-Pro with CSO improves both\nstrategy accuracy and bias mitigation, enabling LLMs to generate more\nempathetic and contextually appropriate responses. Experiments on LLaMA-3.1-8B,\nGemma-2-9B, and Qwen2.5-7B demonstrate that CSO outperforms standard SFT,\nhighlighting the efficacy of fine-grained, turn-level preference modeling in\nESC.", "AI": {"tldr": "This paper introduces Chain-of-Strategy Optimization (CSO) to enhance the performance of Large Language Models in Emotional Support Conversations by improving strategy selection accuracy and mitigating preference bias.", "motivation": "The increasing emotional stress in society has heightened the need for effective Emotional Support Conversations, presenting challenges for existing LLMs in accurately selecting strategies and being adaptable to users' emotional needs.", "method": "The authors propose Chain-of-Strategy Optimization (CSO), which employs Monte Carlo Tree Search to create a high-quality preference dataset for training LLMs on nuanced strategy-response pairs at each dialogue turn.", "result": "Experiments demonstrate that LLMs trained with CSO outperformed those using standard supervised fine-tuning, achieving better strategy accuracy and reduced bias in responses.", "conclusion": "CSO significantly enhances the capability of LLMs in generating empathetic and contextually suitable responses for Emotional Support Conversations.", "key_contributions": ["Introduction of Chain-of-Strategy Optimization (CSO) for enhanced LLM adaptability in emotional support", "Development of ESC-Pro dataset using Monte Carlo Tree Search for turn-level strategy-response pairs", "Demonstration of improved model performance in empathy and contextuality using CSO over standard SFT methods."], "limitations": "", "keywords": ["Emotional Support Conversations", "Large Language Models", "Chain-of-Strategy Optimization", "Monte Carlo Tree Search", "Empathy"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2504.04335", "pdf": "https://arxiv.org/pdf/2504.04335.pdf", "abs": "https://arxiv.org/abs/2504.04335", "title": "Hallucinated Span Detection with Multi-View Attention Features", "authors": ["Yuya Ogasa", "Yuki Arase"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study addresses the problem of hallucinated span detection in the\noutputs of large language models. It has received less attention than\noutput-level hallucination detection despite its practical importance. Prior\nwork has shown that attentions often exhibit irregular patterns when\nhallucinations occur. Motivated by these findings, we extract features from the\nattention matrix that provide complementary views capturing (a) whether certain\ntokens are influential or ignored, (b) whether attention is biased toward\nspecific subsets, and (c) whether a token is generated referring to a narrow or\nbroad context, in the generation. These features are input to a\nTransformer-based classifier to conduct sequential labelling to identify\nhallucinated spans. Experimental results indicate that the proposed method\noutperforms strong baselines on hallucinated span detection with longer input\ncontexts, such as data-to-text and summarisation tasks.", "AI": {"tldr": "This study proposes a method for detecting hallucinated spans in outputs from large language models using features from the attention matrix and a Transformer-based classifier.", "motivation": "Hallucinated span detection has been underexplored compared to output-level hallucination detection, despite its practical relevance in understanding and improving language model outputs.", "method": "Features are extracted from the attention matrix, focusing on token influence, attention bias, and context reference, which are then used as inputs for a Transformer-based classifier for sequential labeling of hallucinated spans.", "result": "The proposed method demonstrates superior performance in detecting hallucinated spans, particularly in tasks requiring longer input contexts, such as data-to-text and summarization.", "conclusion": "The study concludes that utilizing attention matrix features significantly enhances the detection of hallucinated spans in language model outputs.", "key_contributions": ["Introduction of features extracted from the attention matrix for hallucinated span detection.", "Development of a Transformer-based classifier for sequential labeling of detected spans.", "Demonstration of improved performance over existing baselines in tasks involving longer contexts."], "limitations": "", "keywords": ["hallucination detection", "large language models", "attention matrix", "Transformer", "natural language generation"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2504.12805", "pdf": "https://arxiv.org/pdf/2504.12805.pdf", "abs": "https://arxiv.org/abs/2504.12805", "title": "Assessing LLMs in Art Contexts: Critique Generation and Theory of Mind Evaluation", "authors": ["Takaya Arita", "Wenxian Zheng", "Reiji Suzuki", "Fuminori Akiba"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "Corrected a typo in the metadata title only\n  (\"Assesing\"->\"Assessing\"). No changes were made to the PDF or source files", "summary": "This study explored how large language models (LLMs) perform in two areas\nrelated to art: writing critiques of artworks and reasoning about mental states\n(Theory of Mind, or ToM) in art-related situations. For the critique generation\npart, we built a system that combines Noel Carroll's evaluative framework with\na broad selection of art criticism theories. The model was prompted to first\nwrite a full-length critique and then shorter, more coherent versions using a\nstep-by-step prompting process. These AI-generated critiques were then compared\nwith those written by human experts in a Turing test-style evaluation. In many\ncases, human subjects had difficulty telling which was which, and the results\nsuggest that LLMs can produce critiques that are not only plausible in style\nbut also rich in interpretation, as long as they are carefully guided. In the\nsecond part, we introduced new simple ToM tasks based on situations involving\ninterpretation, emotion, and moral tension, which can appear in the context of\nart. These go beyond standard false-belief tests and allow for more complex,\nsocially embedded forms of reasoning. We tested 41 recent LLMs and found that\ntheir performance varied across tasks and models. In particular, tasks that\ninvolved affective or ambiguous situations tended to reveal clearer\ndifferences. Taken together, these results help clarify how LLMs respond to\ncomplex interpretative challenges, revealing both their cognitive limitations\nand potential. While our findings do not directly contradict the so-called\nGenerative AI Paradox--the idea that LLMs can produce expert-like output\nwithout genuine understanding--they suggest that, depending on how LLMs are\ninstructed, such as through carefully designed prompts, these models may begin\nto show behaviors that resemble understanding more closely than we might\nassume.", "AI": {"tldr": "This study investigates the effectiveness of large language models (LLMs) in generating art critiques and performing Theory of Mind tasks in art contexts, revealing potential and limitations in their interpretative capabilities.", "motivation": "To explore how LLMs can critique artwork and reason about mental states in art-related scenarios, assessing their interpretative abilities.", "method": "The study employed a two-part approach: first, constructing a system that integrates Carol's evaluative framework with various art criticism theories to generate critiques, and second, introducing new Theory of Mind tasks that go beyond traditional tests, focusing on complex social reasoning in art.", "result": "LLMs produced critiques that were often indistinguishable from human experts in a Turing test-style evaluation, showing potential for rich interpretation when guided effectively. Performance on ToM tasks indicated variability among models, particularly in emotionally charged or ambiguous scenarios.", "conclusion": "The findings provide insights into LLMs' capabilities in interpretative tasks, suggesting that while they may not genuinely understand art, their output can closely resemble expert critique under specific prompting conditions.", "key_contributions": ["Demonstrated LLMs' effectiveness in generating detailed art critiques.", "Introduced innovative Theory of Mind tasks tailored for art contexts.", "Revealed cognitive differences in LLM performance across various interpretative challenges."], "limitations": "Limited by the inherent cognitive limitations of LLMs and the design of prompts, which may not universally apply to all interpretative tasks.", "keywords": ["Large Language Models", "Art Critique", "Theory of Mind", "Interpretation", "Machine Learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.14089", "pdf": "https://arxiv.org/pdf/2504.14089.pdf", "abs": "https://arxiv.org/abs/2504.14089", "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous Logical Reasoning with Large Language Models", "authors": ["Kang He", "Kaushik Roy"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "EMNLP 2025 Main Conference", "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average.", "AI": {"tldr": "LogicTree is a modular framework for improving multi-step reasoning in large language models by using algorithm-guided search and caching mechanisms for logical coherence.", "motivation": "To enhance the reasoning capabilities of large language models (LLMs) which struggle with complex logical reasoning and proof-finding due to the combinatorial complexity of premise searches.", "method": "LogicTree employs algorithm-guided search for automated structured proof exploration and incorporates caching mechanisms to utilize historical knowledge for efficient inference.", "result": "LogicTree achieves higher proof accuracy compared to traditional methods, with average improvements of 23.6% over chain-of-thought and 12.5% over tree-of-thought on GPT-4o.", "conclusion": "The LogicTree framework effectively scales inference-time computations and enhances the reasoning accuracy of LLMs by enforcing a step-by-step reasoning approach and introducing heuristic premise prioritization.", "key_contributions": ["Introduction of LogicTree framework for modular proof exploration", "Implementation of caching mechanisms for historical knowledge utilization", "Development of LLM-free heuristics for premise prioritization"], "limitations": "", "keywords": ["large language models", "multi-step reasoning", "proof exploration"], "importance_score": 8, "read_time_minutes": 15}}
