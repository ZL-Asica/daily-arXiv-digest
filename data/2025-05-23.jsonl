{"id": "2505.15971", "pdf": "https://arxiv.org/pdf/2505.15971.pdf", "abs": "https://arxiv.org/abs/2505.15971", "title": "A Paradigm for Creative Ownership", "authors": ["Tejaswi Polimetla", "Katy Ilonka Gero"], "categories": ["cs.HC"], "comment": "Symposium on Human-Computer Interaction for Work (CHIWORK) 2025", "summary": "As generative AI tools become more integrated into creative workflows,\nquestions of ownership in co-creative contexts have become increasingly urgent.\nWhile legal frameworks offer definitions of ownership rooted in intellectual\nproperty, they often overlook the nuanced, psychological experience of creative\nownership - how individuals come to feel that a creative product is \"theirs.\"\nDrawing on interdisciplinary literature in philosophy, psychology, and the\nsocial sciences and humanities more broadly, we introduce a new framework that\nsurfaces the material and immaterial dimensions of creative ownership. Our\nmodel organizes creative ownership into three domains - Person, Process, and\nSystem - each of which contains subdimensions that shape ownership sentiment.\nWe offer an accompanying interactive tool that enables creators and researchers\nto visualize and evaluate ownership across a range of contexts. This paradigm\nprovides a new lens through which to understand and support creative agency in\nhuman-AI collaboration, and lays the groundwork for future empirical research\nin design and human-computer interaction."}
{"id": "2505.15973", "pdf": "https://arxiv.org/pdf/2505.15973.pdf", "abs": "https://arxiv.org/abs/2505.15973", "title": "An Exploratory Study on Multi-modal Generative AI in AR Storytelling", "authors": ["Hyungjun Doh", "Jingyu Shi", "Rahul Jain", "Heesoo Kim", "Karthik Ramani"], "categories": ["cs.HC"], "comment": null, "summary": "Storytelling in AR has gained attention due to its multi-modality and\ninteractivity. However, generating multi-modal content for AR storytelling\nrequires expertise and efforts for high-quality conveyance of the narrator's\nintention. Recently, Generative-AI (GenAI) has shown promising applications in\nmulti-modal content generation. Despite the potential benefit, current research\ncalls for validating the effect of AI-generated content (AIGC) in AR\nStorytelling. Therefore, we conducted an exploratory study to investigate the\nutilization of GenAI. Analyzing 223 AR videos, we identified a design space for\nmulti-modal AR Storytelling. Based on the design space, we developed a testbed\nfacilitating multi-modal content generation and atomic elements in AR\nStorytelling. Through two studies with N=30 experienced storytellers and live\npresenters, we 1. revealed participants' preferences for modalities, 2.\nevaluated the interactions with AI to generate content, and 3. assessed the\nquality of the AIGC for AR Storytelling. We further discussed design\nconsiderations for future AR Storytelling with GenAI."}
{"id": "2505.15974", "pdf": "https://arxiv.org/pdf/2505.15974.pdf", "abs": "https://arxiv.org/abs/2505.15974", "title": "Real-Time Stress Monitoring, Detection, and Management in College Students: A Wearable Technology and Machine-Learning Approach", "authors": ["Alan Ta", "Nilsu Salgin", "Mustafa Demir", "Kala Philips Randal", "Ranjana K. Mehta", "Anthony McDonald", "Carly McCord", "Farzan Sasangohar"], "categories": ["cs.HC", "cs.LG"], "comment": "31 pages, 5 figures", "summary": "College students are increasingly affected by stress, anxiety, and\ndepression, yet face barriers to traditional mental health care. This study\nevaluated the efficacy of a mobile health (mHealth) intervention, Mental Health\nEvaluation and Lookout Program (mHELP), which integrates a smartwatch sensor\nand machine learning (ML) algorithms for real-time stress detection and\nself-management. In a 12-week randomized controlled trial (n = 117),\nparticipants were assigned to a treatment group using mHELP's full suite of\ninterventions or a control group using the app solely for real-time stress\nlogging and weekly psychological assessments. The primary outcome, \"Moments of\nStress\" (MS), was assessed via physiological and self-reported indicators and\nanalyzed using Generalized Linear Mixed Models (GLMM) approaches. Similarly,\nsecondary outcomes of psychological assessments, including the Generalized\nAnxiety Disorder-7 (GAD-7) for anxiety, the Patient Health Questionnaire\n(PHQ-8) for depression, and the Perceived Stress Scale (PSS), were also\nanalyzed via GLMM. The finding of the objective measure, MS, indicates a\nsubstantial decrease in MS among the treatment group compared to the control\ngroup, while no notable between-group differences were observed in subjective\nscores of anxiety (GAD-7), depression (PHQ-8), or stress (PSS). However, the\ntreatment group exhibited a clinically meaningful decline in GAD-7 and PSS\nscores. These findings underscore the potential of wearable-enabled mHealth\ntools to reduce acute stress in college populations and highlight the need for\nextended interventions and tailored features to address chronic symptoms like\ndepression."}
{"id": "2505.16011", "pdf": "https://arxiv.org/pdf/2505.16011.pdf", "abs": "https://arxiv.org/abs/2505.16011", "title": "Exploring Perception-Based Techniques for Redirected Walking in VR: A Comprehensive Survey", "authors": ["Bradley Coles", "Yahya Hmaiti", "Joseph J. LaViola Jr"], "categories": ["cs.HC"], "comment": null, "summary": "We present a comprehensive survey of perception-based redirected walking\n(RDW) techniques in virtual reality (VR), presenting a taxonomy that serves as\na framework for understanding and designing RDW algorithms. RDW enables users\nto explore virtual environments (VEs) larger than their physical space,\naddressing the constraints of real walking in limited home VR setups. Our\nreview spans 232 papers, with 165 included in the final analysis. We categorize\nperception-based RDW techniques based on gains, gain application, target\norientation calculation, and optional general enhancements, identifying key\npatterns and relationships. We present data on how current work aligns within\nthis classification system and suggest how this data can guide future work into\nareas that are relatively under explored. This taxonomy clarifies\nperception-based RDW techniques, guiding the design and application of RDW\nsystems, and suggests future research directions to enhance VR user experience."}
{"id": "2505.16034", "pdf": "https://arxiv.org/pdf/2505.16034.pdf", "abs": "https://arxiv.org/abs/2505.16034", "title": "\"AI just keeps guessing\": Using ARC Puzzles to Help Children Identify Reasoning Errors in Generative AI", "authors": ["Aayushi Dangol", "Trushaa Ramanan", "Runhua Zhao", "Julie A. Kientz", "Robert Wolfe", "Jason Yip"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "The integration of generative Artificial Intelligence (genAI) into everyday\nlife raises questions about the competencies required to critically engage with\nthese technologies. Unlike visual errors in genAI, textual mistakes are often\nharder to detect and require specific domain knowledge. Furthermore, AI's\nauthoritative tone and structured responses can create an illusion of\ncorrectness, leading to overtrust, especially among children. To address this,\nwe developed AI Puzzlers, an interactive system based on the Abstraction and\nReasoning Corpus (ARC), to help children identify and analyze errors in genAI.\nDrawing on Mayer & Moreno's Cognitive Theory of Multimedia Learning, AI\nPuzzlers uses visual and verbal elements to reduce cognitive overload and\nsupport error detection. Based on two participatory design sessions with 21\nchildren (ages 6 - 11), our findings provide both design insights and an\nempirical understanding of how children identify errors in genAI reasoning,\ndevelop strategies for navigating these errors, and evaluate AI outputs."}
{"id": "2505.16057", "pdf": "https://arxiv.org/pdf/2505.16057.pdf", "abs": "https://arxiv.org/abs/2505.16057", "title": "Signals of Provenance: Practices & Challenges of Navigating Indicators in AI-Generated Media for Sighted and Blind Individuals", "authors": ["Ayae Ide", "Tory Park", "Jaron Mink", "Tanusree Sharma"], "categories": ["cs.HC", "cs.AI", "cs.MM"], "comment": null, "summary": "AI-Generated (AIG) content has become increasingly widespread by recent\nadvances in generative models and the easy-to-use tools that have significantly\nlowered the technical barriers for producing highly realistic audio, images,\nand videos through simple natural language prompts. In response, platforms are\nadopting provable provenance with platforms recommending AIG to be\nself-disclosed and signaled to users. However, these indicators may be often\nmissed, especially when they rely solely on visual cues and make them\nineffective to users with different sensory abilities. To address the gap, we\nconducted semi-structured interviews (N=28) with 15 sighted and 13 BLV\nparticipants to examine their interaction with AIG content through\nself-disclosed AI indicators. Our findings reveal diverse mental models and\npractices, highlighting different strengths and weaknesses of content-based\n(e.g., title, description) and menu-aided (e.g., AI labels) indicators. While\nsighted participants leveraged visual and audio cues, BLV participants\nprimarily relied on audio and existing assistive tools, limiting their ability\nto identify AIG. Across both groups, they frequently overlooked menu-aided\nindicators deployed by platforms and rather interacted with content-based\nindicators such as title and comments. We uncovered usability challenges\nstemming from inconsistent indicator placement, unclear metadata, and cognitive\noverload. These issues were especially critical for BLV individuals due to the\ninsufficient accessibility of interface elements. We provide practical\nrecommendations and design implications for future AIG indicators across\nseveral dimensions."}
{"id": "2505.16089", "pdf": "https://arxiv.org/pdf/2505.16089.pdf", "abs": "https://arxiv.org/abs/2505.16089", "title": "\"If anybody finds out you are in BIG TROUBLE\": Understanding Children's Hopes, Fears, and Evaluations of Generative AI", "authors": ["Aayushi Dangol", "Robert Wolfe", "Daeun Yoo", "Arya Thiruvillakkat", "Ben Chickadel", "Julie A. Kientz"], "categories": ["cs.HC"], "comment": null, "summary": "As generative artificial intelligence (genAI) increasingly mediates how\nchildren learn, communicate, and engage with digital content, understanding\nchildren's hopes and fears about this emerging technology is crucial. In a\npilot study with 37 fifth-graders, we explored how children (ages 9-10)\nenvision genAI and the roles they believe it should play in their daily life.\nOur findings reveal three key ways children envision genAI: as a companion\nproviding guidance, a collaborator working alongside them, and a task automator\nthat offloads responsibilities. However, alongside these hopeful views,\nchildren expressed fears about overreliance, particularly in academic settings,\nlinking it to fears of diminished learning, disciplinary consequences, and\nlong-term failure. This study highlights the need for child-centric AI design\nthat balances these tensions, empowering children with the skills to critically\nengage with and navigate their evolving relationships with digital\ntechnologies."}
{"id": "2505.16171", "pdf": "https://arxiv.org/pdf/2505.16171.pdf", "abs": "https://arxiv.org/abs/2505.16171", "title": "Fairness and Efficiency in Human-Agent Teams: An Iterative Algorithm Design Approach", "authors": ["Mai Lee Chang", "Kim Baraka", "Greg Trafton", "Zach Lalu Vazhekatt", "Andrea Lockerd Thomaz"], "categories": ["cs.HC"], "comment": "18 pages, 5 figures", "summary": "When agents interact with people as part of a team, fairness becomes an\nimportant factor. Prior work has proposed fairness metrics based on teammates'\ncapabilities for task allocation within human-agent teams. However, most\nmetrics only consider teammate capabilities from a third-person point of view\n(POV). In this work, we extend these metrics to include task preferences and\nconsider a first-person POV. We leverage an iterative design method consisting\nof simulation data and human data to design a task allocation algorithm that\nbalances task efficiency and fairness based on both capabilities and\npreferences. We first show that these metrics may not align with people's\nperceived fairness from a first-person POV. In light of this result, we propose\na new fairness metric, fair-equity, and the Fair-Efficient Algorithm (FEA). Our\nfindings suggest that an agent teammate who balances efficiency and fairness\nbased on equity will be perceived to be fairer and preferred by human teammates\nin various human-agent team types. We suggest that the perception of fairness\nmay also depend on a person's POV."}
{"id": "2505.16254", "pdf": "https://arxiv.org/pdf/2505.16254.pdf", "abs": "https://arxiv.org/abs/2505.16254", "title": "Reassessing Collaborative Writing Theories and Frameworks in the Age of LLMs: What Still Applies and What We Must Leave Behind", "authors": ["Daisuke Yukita", "Tim Miller", "Joel Mackenzie"], "categories": ["cs.HC"], "comment": null, "summary": "In this paper, we conduct a critical review of existing theories and\nframeworks on human-human collaborative writing to assess their relevance to\nthe current human-AI paradigm in professional contexts, and draw seven insights\nalong with design implications for human-AI collaborative writing tools. We\nfound that, as LLMs nudge the writing process more towards an empirical \"trial\nand error\" process analogous to prototyping, the non-linear cognitive process\nof writing will stay the same, but more rigor will be required for revision\nmethodologies. This shift would shed further light on the importance of\ncoherence support, but the large language model (LLM)'s unprecedented semantic\ncapabilities can bring novel approaches to this ongoing challenge. We argue\nthat teamwork-related factors such as group awareness, consensus building and\nauthorship - which have been central in human-human collaborative writing\nstudies - should not apply to the human-AI paradigm due to excessive\nanthropomorphism. With the LLM's text generation capabilities becoming\nessentially indistinguishable from human-written ones, we are entering an era\nwhere, for the first time in the history of computing, we are engaging in\ncollaborative writing with AI at workplaces on a daily basis. We aim to bring\ntheoretical grounding and practical design guidance to the interaction designs\nof human-AI collaborative writing, with the goal of enhancing future human-AI\nwriting software."}
{"id": "2505.16352", "pdf": "https://arxiv.org/pdf/2505.16352.pdf", "abs": "https://arxiv.org/abs/2505.16352", "title": "Estimating Perceptual Attributes of Haptic Textures Using Visuo-Tactile Data", "authors": ["Mudassir Ibrahim Awan", "Seokhee Jeon"], "categories": ["cs.HC"], "comment": null, "summary": "Accurate prediction of perceptual attributes of haptic textures is essential\nfor advancing VR and AR applications and enhancing robotic interaction with\nphysical surfaces. This paper presents a deep learning-based multi-modal\nframework, incorporating visual and tactile data, to predict perceptual texture\nratings by leveraging multi-feature inputs. To achieve this, a four-dimensional\nhaptic attribute space encompassing rough-smooth, flat-bumpy, sticky-slippery,\nand hard-soft dimensions is first constructed through psychophysical\nexperiments, where participants evaluate 50 diverse real-world texture samples.\nA physical signal space is subsequently created by collecting visual and\ntactile data from these textures. Finally, a deep learning architecture\nintegrating a CNN-based autoencoder for visual feature learning and a ConvLSTM\nnetwork for tactile data processing is trained to predict user-assigned\nattribute ratings. This multi-modal, multi-feature approach maps physical\nsignals to perceptual ratings, enabling accurate predictions for unseen\ntextures. To evaluate predictive accuracy, we employed leave-one-out\ncross-validation to rigorously assess the model's reliability and\ngeneralizability against several machine learning and deep learning baselines.\nExperimental results demonstrate that the framework consistently outperforms\nsingle-modality approaches, achieving lower MAE and RMSE, highlighting the\nefficacy of combining visual and tactile modalities."}
{"id": "2505.16702", "pdf": "https://arxiv.org/pdf/2505.16702.pdf", "abs": "https://arxiv.org/abs/2505.16702", "title": "Truth and Trust: Fake News Detection via Biosignals", "authors": ["Gennie Nguyen", "Lei Wang", "Yangxueqing Jiang", "Tom Gedeon"], "categories": ["cs.HC"], "comment": "Research report", "summary": "Understanding how individuals physiologically respond to false information is\ncrucial for advancing misinformation detection systems. This study explores the\npotential of using physiological signals, specifically electrodermal activity\n(EDA) and photoplethysmography (PPG), to classify both the veracity of\ninformation and its interaction with user belief. In a controlled laboratory\nexperiment, we collected EDA and PPG signals while participants evaluated the\ntruthfulness of climate-related claims. Each trial was labeled based on the\nobjective truth of the claim and the participant's belief, enabling two\nclassification tasks: binary veracity detection and a novel four-class joint\nbelief-veracity classification. We extracted handcrafted features from the raw\nsignals and trained several machine learning models to benchmark the dataset.\nOur results show that EDA outperforms PPG, indicating its greater sensitivity\nto physiological responses related to truth perception. However, performance\nsignificantly drops in the joint belief-veracity classification task,\nhighlighting the complexity of modeling the interaction between belief and\ntruth. These findings suggest that while physiological signals can reflect\nbasic truth perception, accurately modeling the intricate relationships between\nbelief and veracity remains a significant challenge. This study emphasizes the\nimportance of multimodal approaches that incorporate psychological,\nphysiological, and cognitive factors to improve fake news detection systems.\nOur work provides a foundation for future research aimed at enhancing\nmisinformation detection via addressing the complexities of human belief and\ntruth processing."}
{"id": "2505.16730", "pdf": "https://arxiv.org/pdf/2505.16730.pdf", "abs": "https://arxiv.org/abs/2505.16730", "title": "Detecting Fake News Belief via Skin and Blood Flow Signals", "authors": ["Gennie Nguyen", "Lei Wang", "Yangxueqing Jiang", "Tom Gedeon"], "categories": ["cs.HC"], "comment": "Research Report", "summary": "Misinformation poses significant risks to public opinion, health, and\nsecurity. While most fake news detection methods rely on text analysis, little\nis known about how people physically respond to false information or repeated\nexposure to the same statements. This study investigates whether wearable\nsensors can detect belief in a statement or prior exposure to it. We conducted\na controlled experiment where participants evaluated statements while wearing\nan EmotiBit sensor that measured their skin conductance (electrodermal\nactivity, EDA) and peripheral blood flow (photoplethysmography, PPG). From 28\nparticipants, we collected a dataset of 672 trials, each labeled with whether\nthe participant believed the statement and whether they had seen it before.\nThis dataset introduces a new resource for studying physiological responses to\nmisinformation. Using machine learning models, including KNN, CNN, and\nLightGBM, we analyzed these physiological patterns. The best-performing model\nachieved 67.83\\% accuracy, with skin conductance outperforming PPG. These\nfindings demonstrate the potential of wearable sensors as a minimally intrusive\ntool for detecting belief and prior exposure, offering new directions for\nreal-time misinformation detection and adaptive, user-aware systems."}
{"id": "2505.16954", "pdf": "https://arxiv.org/pdf/2505.16954.pdf", "abs": "https://arxiv.org/abs/2505.16954", "title": "Cracking Aegis: An Adversarial LLM-based Game for Raising Awareness of Vulnerabilities in Privacy Protection", "authors": ["Jiaying Fu", "Yiyang Lu", "Zehua Yang", "Fiona Nah", "RAY LC"], "categories": ["cs.HC"], "comment": "24 pages, In Designing Interactive Systems Conference (DIS 25)", "summary": "Traditional methods for raising awareness of privacy protection often fail to\nengage users or provide hands-on insights into how privacy vulnerabilities are\nexploited. To address this, we incorporate an adversarial mechanic in the\ndesign of the dialogue-based serious game Cracking Aegis. Leveraging LLMs to\nsimulate natural interactions, the game challenges players to impersonate\ncharacters and extract sensitive information from an AI agent, Aegis. A user\nstudy (n=22) revealed that players employed diverse deceptive linguistic\nstrategies, including storytelling and emotional rapport, to manipulate Aegis.\nAfter playing, players reported connecting in-game scenarios with real-world\nprivacy vulnerabilities, such as phishing and impersonation, and expressed\nintentions to strengthen privacy control, such as avoiding oversharing personal\ninformation with AI systems. This work highlights the potential of LLMs to\nsimulate complex relational interactions in serious games, while demonstrating\nhow an adversarial game strategy provides unique insights for designs for\nsocial good, particularly privacy protection."}
{"id": "2505.15946", "pdf": "https://arxiv.org/pdf/2505.15946.pdf", "abs": "https://arxiv.org/abs/2505.15946", "title": "MoRE-Brain: Routed Mixture of Experts for Interpretable and Generalizable Cross-Subject fMRI Visual Decoding", "authors": ["Yuxiang Wei", "Yanteng Zhang", "Xi Xiao", "Tianyang Wang", "Xiao Wang", "Vince D. Calhoun"], "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.HC"], "comment": null, "summary": "Decoding visual experiences from fMRI offers a powerful avenue to understand\nhuman perception and develop advanced brain-computer interfaces. However,\ncurrent progress often prioritizes maximizing reconstruction fidelity while\noverlooking interpretability, an essential aspect for deriving neuroscientific\ninsight. To address this gap, we propose MoRE-Brain, a neuro-inspired framework\ndesigned for high-fidelity, adaptable, and interpretable visual reconstruction.\nMoRE-Brain uniquely employs a hierarchical Mixture-of-Experts architecture\nwhere distinct experts process fMRI signals from functionally related voxel\ngroups, mimicking specialized brain networks. The experts are first trained to\nencode fMRI into the frozen CLIP space. A finetuned diffusion model then\nsynthesizes images, guided by expert outputs through a novel dual-stage routing\nmechanism that dynamically weighs expert contributions across the diffusion\nprocess. MoRE-Brain offers three main advancements: First, it introduces a\nnovel Mixture-of-Experts architecture grounded in brain network principles for\nneuro-decoding. Second, it achieves efficient cross-subject generalization by\nsharing core expert networks while adapting only subject-specific routers.\nThird, it provides enhanced mechanistic insight, as the explicit routing\nreveals precisely how different modeled brain regions shape the semantic and\nspatial attributes of the reconstructed image. Extensive experiments validate\nMoRE-Brain's high reconstruction fidelity, with bottleneck analyses further\ndemonstrating its effective utilization of fMRI signals, distinguishing genuine\nneural decoding from over-reliance on generative priors. Consequently,\nMoRE-Brain marks a substantial advance towards more generalizable and\ninterpretable fMRI-based visual decoding. Code will be publicly available soon:\nhttps://github.com/yuxiangwei0808/MoRE-Brain."}
{"id": "2505.16023", "pdf": "https://arxiv.org/pdf/2505.16023.pdf", "abs": "https://arxiv.org/abs/2505.16023", "title": "Prototypical Human-AI Collaboration Behaviors from LLM-Assisted Writing in the Wild", "authors": ["Sheshera Mysore", "Debarati Das", "Hancheng Cao", "Bahareh Sarrafzadeh"], "categories": ["cs.CL", "cs.HC"], "comment": "Pre-print under-review", "summary": "As large language models (LLMs) are used in complex writing workflows, users\nengage in multi-turn interactions to steer generations to better fit their\nneeds. Rather than passively accepting output, users actively refine, explore,\nand co-construct text. We conduct a large-scale analysis of this collaborative\nbehavior for users engaged in writing tasks in the wild with two popular AI\nassistants, Bing Copilot and WildChat. Our analysis goes beyond simple task\nclassification or satisfaction estimation common in prior work and instead\ncharacterizes how users interact with LLMs through the course of a session. We\nidentify prototypical behaviors in how users interact with LLMs in prompts\nfollowing their original request. We refer to these as Prototypical Human-AI\nCollaboration Behaviors (PATHs) and find that a small group of PATHs explain a\nmajority of the variation seen in user-LLM interaction. These PATHs span users\nrevising intents, exploring texts, posing questions, adjusting style or\ninjecting new content. Next, we find statistically significant correlations\nbetween specific writing intents and PATHs, revealing how users' intents shape\ntheir collaboration behaviors. We conclude by discussing the implications of\nour findings on LLM alignment."}
{"id": "2505.16031", "pdf": "https://arxiv.org/pdf/2505.16031.pdf", "abs": "https://arxiv.org/abs/2505.16031", "title": "Children's Mental Models of AI Reasoning: Implications for AI Literacy Education", "authors": ["Aayushi Dangol", "Robert Wolfe", "Runhua Zhao", "JaeWon Kim", "Trushaa Ramanan", "Katie Davis", "Julie A. Kientz"], "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "As artificial intelligence (AI) advances in reasoning capabilities, most\nrecently with the emergence of Large Reasoning Models (LRMs), understanding how\nchildren conceptualize AI's reasoning processes becomes critical for fostering\nAI literacy. While one of the \"Five Big Ideas\" in AI education highlights\nreasoning algorithms as central to AI decision-making, less is known about\nchildren's mental models in this area. Through a two-phase approach, consisting\nof a co-design session with 8 children followed by a field study with 106\nchildren (grades 3-8), we identified three models of AI reasoning: Deductive,\nInductive, and Inherent. Our findings reveal that younger children (grades 3-5)\noften attribute AI's reasoning to inherent intelligence, while older children\n(grades 6-8) recognize AI as a pattern recognizer. We highlight three tensions\nthat surfaced in children's understanding of AI reasoning and conclude with\nimplications for scaffolding AI curricula and designing explainable AI tools."}
{"id": "2505.16384", "pdf": "https://arxiv.org/pdf/2505.16384.pdf", "abs": "https://arxiv.org/abs/2505.16384", "title": "MAGE: A Multi-task Architecture for Gaze Estimation with an Efficient Calibration Module", "authors": ["Haoming Huang", "Musen Zhang", "Jianxin Yang", "Zhen Li", "Jinkai Li", "Yao Guo"], "categories": ["cs.CV", "cs.HC"], "comment": "Under review", "summary": "Eye gaze can provide rich information on human psychological activities, and\nhas garnered significant attention in the field of Human-Robot Interaction\n(HRI). However, existing gaze estimation methods merely predict either the gaze\ndirection or the Point-of-Gaze (PoG) on the screen, failing to provide\nsufficient information for a comprehensive six Degree-of-Freedom (DoF) gaze\nanalysis in 3D space. Moreover, the variations of eye shape and structure among\nindividuals also impede the generalization capability of these methods. In this\nstudy, we propose MAGE, a Multi-task Architecture for Gaze Estimation with an\nefficient calibration module, to predict the 6-DoF gaze information that is\napplicable for the real-word HRI. Our basic model encodes both the directional\nand positional features from facial images, and predicts gaze results with\ndedicated information flow and multiple decoders. To reduce the impact of\nindividual variations, we propose a novel calibration module, namely\nEasy-Calibration, to fine-tune the basic model with subject-specific data,\nwhich is efficient to implement without the need of a screen. Experimental\nresults demonstrate that our method achieves state-of-the-art performance on\nthe public MPIIFaceGaze, EYEDIAP, and our built IMRGaze datasets."}
{"id": "2505.16397", "pdf": "https://arxiv.org/pdf/2505.16397.pdf", "abs": "https://arxiv.org/abs/2505.16397", "title": "Dynamic Caustics by Ultrasonically Modulated Liquid Surface", "authors": ["Koki Nagakura", "Tatsuki Fushimi", "Ayaka Tsutsui", "Yoichi Ochiai"], "categories": ["cs.GR", "cs.HC"], "comment": null, "summary": "This paper presents a method for generating dynamic caustic patterns by\nutilising dual-optimised holographic fields with Phased Array Transducer (PAT).\nBuilding on previous research in static caustic optimisation and ultrasonic\nmanipulation, this approach employs computational techniques to dynamically\nshape fluid surfaces, thereby creating controllable and real-time caustic\nimages. The system employs a Digital Twin framework, which enables iterative\nfeedback and refinement, thereby improving the accuracy and quality of the\ncaustic patterns produced. This paper extends the foundational work in caustic\ngeneration by integrating liquid surfaces as refractive media. This concept has\npreviously been explored in simulations but not fully realised in practical\napplications. The utilisation of ultrasound to directly manipulate these\nsurfaces enables the generation of dynamic caustics with a high degree of\nflexibility. The Digital Twin approach further enhances this process by\nallowing for precise adjustments and optimisation based on real-time feedback.\nExperimental results demonstrate the technique's capacity to generate\ncontinuous animations and complex caustic patterns at high frequencies.\nAlthough there are limitations in contrast and resolution compared to\nsolid-surface methods, this approach offers advantages in terms of real-time\nadaptability and scalability. This technique has the potential to be applied in\na number of areas, including interactive displays, artistic installations and\neducational tools. This research builds upon the work of previous researchers\nin the fields of caustics optimisation, ultrasonic manipulation, and\ncomputational displays. Future research will concentrate on enhancing the\nresolution and intricacy of the generated patterns."}
{"id": "2505.16505", "pdf": "https://arxiv.org/pdf/2505.16505.pdf", "abs": "https://arxiv.org/abs/2505.16505", "title": "Sparse Activation Editing for Reliable Instruction Following in Narratives", "authors": ["Runcong Zhao", "Chengyu Cao", "Qinglin Zhu", "Xiucheng Lv", "Shun Shao", "Lin Gui", "Ruifeng Xu", "Yulan He"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Complex narrative contexts often challenge language models' ability to follow\ninstructions, and existing benchmarks fail to capture these difficulties. To\naddress this, we propose Concise-SAE, a training-free framework that improves\ninstruction following by identifying and editing instruction-relevant neurons\nusing only natural language instructions, without requiring labelled data. To\nthoroughly evaluate our method, we introduce FreeInstruct, a diverse and\nrealistic benchmark of 1,212 examples that highlights the challenges of\ninstruction following in narrative-rich settings. While initially motivated by\ncomplex narratives, Concise-SAE demonstrates state-of-the-art instruction\nadherence across varied tasks without compromising generation quality."}
{"id": "2505.15916", "pdf": "https://arxiv.org/pdf/2505.15916.pdf", "abs": "https://arxiv.org/abs/2505.15916", "title": "BR-TaxQA-R: A Dataset for Question Answering with References for Brazilian Personal Income Tax Law, including case law", "authors": ["Juvenal Domingos Júnior", "Augusto Faria", "E. Seiti de Oliveira", "Erick de Brito", "Matheus Teotonio", "Andre Assumpção", "Diedre Carmo", "Roberto Lotufo", "Jayr Pereira"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents BR-TaxQA-R, a novel dataset designed to support question\nanswering with references in the context of Brazilian personal income tax law.\nThe dataset contains 715 questions from the 2024 official Q\\&A document\npublished by Brazil's Internal Revenue Service, enriched with statutory norms\nand administrative rulings from the Conselho Administrativo de Recursos Fiscais\n(CARF). We implement a Retrieval-Augmented Generation (RAG) pipeline using\nOpenAI embeddings for searching and GPT-4o-mini for answer generation. We\ncompare different text segmentation strategies and benchmark our system against\ncommercial tools such as ChatGPT and Perplexity.ai using RAGAS-based metrics.\nResults show that our custom RAG pipeline outperforms commercial systems in\nResponse Relevancy, indicating stronger alignment with user queries, while\ncommercial models achieve higher scores in Factual Correctness and fluency.\nThese findings highlight a trade-off between legally grounded generation and\nlinguistic fluency. Crucially, we argue that human expert evaluation remains\nessential to ensure the legal validity of AI-generated answers in high-stakes\ndomains such as taxation. BR-TaxQA-R is publicly available at\nhttps://huggingface.co/datasets/unicamp-dl/BR-TaxQA-R."}
{"id": "2505.16724", "pdf": "https://arxiv.org/pdf/2505.16724.pdf", "abs": "https://arxiv.org/abs/2505.16724", "title": "Advancing Brainwave Modeling with a Codebook-Based Foundation Model", "authors": ["Konstantinos Barmpas", "Na Lee", "Yannis Panagakis", "Dimitrios A. Adamos", "Nikolaos Laskaris", "Stefanos Zafeiriou"], "categories": ["cs.LG", "cs.AI", "cs.HC"], "comment": null, "summary": "Recent advances in large-scale pre-trained Electroencephalogram (EEG) models\nhave shown great promise, driving progress in Brain-Computer Interfaces (BCIs)\nand healthcare applications. However, despite their success, many existing\npre-trained models have struggled to fully capture the rich information content\nof neural oscillations, a limitation that fundamentally constrains their\nperformance and generalizability across diverse BCI tasks. This limitation is\nfrequently rooted in suboptimal architectural design choices which constrain\ntheir representational capacity. In this work, we introduce LaBraM++, an\nenhanced Large Brainwave Foundation Model (LBM) that incorporates principled\nimprovements grounded in robust signal processing foundations. LaBraM++\ndemonstrates substantial gains across a variety of tasks, consistently\noutperforming its originally-based architecture and achieving competitive\nresults when compared to other open-source LBMs. Its superior performance and\ntraining efficiency highlight its potential as a strong foundation for future\nadvancements in LBMs."}
{"id": "2505.15918", "pdf": "https://arxiv.org/pdf/2505.15918.pdf", "abs": "https://arxiv.org/abs/2505.15918", "title": "Extracting Probabilistic Knowledge from Large Language Models for Bayesian Network Parameterization", "authors": ["Aliakbar Nafar", "Kristen Brent Venable", "Zijun Cui", "Parisa Kordjamshidi"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated potential as factual knowledge\nbases; however, their capability to generate probabilistic knowledge about\nreal-world events remains understudied. This paper investigates using\nprobabilistic knowledge inherent in LLMs to derive probability estimates for\nstatements concerning events and their interrelationships captured via a\nBayesian Network (BN). Using LLMs in this context allows for the\nparameterization of BNs, enabling probabilistic modeling within specific\ndomains. Experiments on eighty publicly available Bayesian Networks, from\nhealthcare to finance, demonstrate that querying LLMs about the conditional\nprobabilities of events provides meaningful results when compared to baselines,\nincluding random and uniform distributions, as well as approaches based on\nnext-token generation probabilities. We explore how these LLM-derived\ndistributions can serve as expert priors to refine distributions extracted from\nminimal data, significantly reducing systematic biases. Overall, this work\nintroduces a promising strategy for automatically constructing Bayesian\nNetworks by combining probabilistic knowledge extracted from LLMs with small\namounts of real-world data. Additionally, we evaluate several prompting\nstrategies for eliciting probabilistic knowledge from LLMs and establish the\nfirst comprehensive baseline for assessing LLM performance in extracting\nprobabilistic knowledge."}
{"id": "2404.06432", "pdf": "https://arxiv.org/pdf/2404.06432.pdf", "abs": "https://arxiv.org/abs/2404.06432", "title": "Missing Pieces: How Do Designs that Expose Uncertainty Longitudinally Impact Trust in AI Decision Aids? An In Situ Study of Gig Drivers", "authors": ["Rex Chen", "Ruiyi Wang", "Fei Fang", "Norman Sadeh"], "categories": ["cs.HC"], "comment": "27 pages; 3 tables; 13 figures; accepted version, published at the\n  2025 ACM Conference on Fairness, Accountability, and Transparency (FAccT '25)", "summary": "Decision aids based on artificial intelligence (AI) induce a wide range of\noutcomes when they are deployed in uncertain environments. In this paper, we\ninvestigate how users' trust in recommendations from an AI decision aid is\nimpacted over time by designs that expose uncertainty in predicted outcomes.\nUnlike previous work, we focus on gig driving - a real-world, repeated\ndecision-making context. We report on a longitudinal mixed-methods study\n($n=51$) where we measured gig drivers' trust as they interacted with an\nAI-based schedule recommendation tool. Our results show that participants'\ntrust in the tool was shaped by both their first impressions of its accuracy\nand their longitudinal interactions with it; and that task-aligned framings of\nuncertainty improved trust by allowing participants to incorporate uncertainty\ninto their decision-making processes. Additionally, we observed that trust\ndepended on their characteristics as drivers, underscoring the need for more in\nsitu studies of AI decision aids."}
{"id": "2505.15922", "pdf": "https://arxiv.org/pdf/2505.15922.pdf", "abs": "https://arxiv.org/abs/2505.15922", "title": "Aligning Dialogue Agents with Global Feedback via Large Language Model Reward Decomposition", "authors": ["Dong Won Lee", "Hae Won Park", "Cynthia Breazeal", "Louis-Philippe Morency"], "categories": ["cs.CL"], "comment": "9 pages, 3 figures, 3 tables", "summary": "We propose a large language model based reward decomposition framework for\naligning dialogue agents using only a single session-level feedback signal. We\nleverage the reasoning capabilities of a frozen, pretrained large language\nmodel (LLM) to infer fine-grained local implicit rewards by decomposing global,\nsession-level feedback. Our first text-only variant prompts the LLM to perform\nreward decomposition using only the dialogue transcript. The second multimodal\nvariant incorporates additional behavioral cues, such as pitch, gaze, and\nfacial affect, expressed as natural language descriptions. These inferred\nturn-level rewards are distilled into a lightweight reward model, which we\nutilize for RL-based fine-tuning for dialogue generation. We evaluate both\ntext-only and multimodal variants against state-of-the-art reward decomposition\nmethods and demonstrate notable improvements in human evaluations of\nconversation quality, suggesting that LLMs are strong reward decomposers that\nobviate the need for manual reward shaping and granular human feedback."}
{"id": "2411.11382", "pdf": "https://arxiv.org/pdf/2411.11382.pdf", "abs": "https://arxiv.org/abs/2411.11382", "title": "Quantifying Haptic Affection of Car Door through Data-Driven Analysis of Force Profile", "authors": ["Mudassir Ibrahim Awan", "Ahsan Raza", "Waseem Hassan", "Ki-Uk Kyung", "Seokhee Jeon"], "categories": ["cs.HC"], "comment": "12 pages, 9 figures, 3 tables. Mudassir Ibrahim Awan and Ahsan Raza\n  are equally contributing authors", "summary": "Haptic affection plays a crucial role in user experience, particularly in the\nautomotive industry where the tactile quality of components can influence\ncustomer satisfaction. This study aims to accurately predict the affective\nproperty of a car door by only watching the force or torque profile of it when\nopening. To this end, a deep learning model is designed to capture the\nunderlying relationships between force profiles and user-defined adjective\nratings, providing insights into the door-opening experience. The dataset\nemployed in this research includes force profiles and user adjective ratings\ncollected from six distinct car models, reflecting a diverse set of\ndoor-opening characteristics and tactile feedback. The model's performance is\nassessed using Leave-One-Out Cross-Validation, a method that measures its\ngeneralization capability on unseen data. The results demonstrate that the\nproposed model achieves a high level of prediction accuracy, indicating its\npotential in various applications related to haptic affection and design\noptimization in the automotive industry."}
{"id": "2505.15948", "pdf": "https://arxiv.org/pdf/2505.15948.pdf", "abs": "https://arxiv.org/abs/2505.15948", "title": "Citation Parsing and Analysis with Language Models", "authors": ["Parth Sarin", "Juan Pablo Alperin"], "categories": ["cs.CL", "cs.DL", "cs.SI"], "comment": "Presented at the Workshop on Open Citations & Open Scholarly Metadata\n  2025", "summary": "A key type of resource needed to address global inequalities in knowledge\nproduction and dissemination is a tool that can support journals in\nunderstanding how knowledge circulates. The absence of such a tool has resulted\nin comparatively less information about networks of knowledge sharing in the\nGlobal South. In turn, this gap authorizes the exclusion of researchers and\nscholars from the South in indexing services, reinforcing colonial arrangements\nthat de-center and minoritize those scholars. In order to support citation\nnetwork tracking on a global scale, we investigate the capacity of open-weight\nlanguage models to mark up manuscript citations in an indexable format. We\nassembled a dataset of matched plaintext and annotated citations from preprints\nand published research papers. Then, we evaluated a number of open-weight\nlanguage models on the annotation task. We find that, even out of the box,\ntoday's language models achieve high levels of accuracy on identifying the\nconstituent components of each citation, outperforming state-of-the-art\nmethods. Moreover, the smallest model we evaluated, Qwen3-0.6B, can parse all\nfields with high accuracy in $2^5$ passes, suggesting that post-training is\nlikely to be effective in producing small, robust citation parsing models. Such\na tool could greatly improve the fidelity of citation networks and thus\nmeaningfully improve research indexing and discovery, as well as further\nmetascientific research."}
{"id": "2412.00411", "pdf": "https://arxiv.org/pdf/2412.00411.pdf", "abs": "https://arxiv.org/abs/2412.00411", "title": "Seismocardiography for Emotion Recognition: A Study on EmoWear with Insights from DEAP", "authors": ["Mohammad Hasan Rahmani", "Rafael Berkvens", "Maarten Weyn"], "categories": ["cs.HC"], "comment": "17 pages, 9 figures", "summary": "Emotions have a profound impact on our daily lives, influencing our thoughts,\nbehaviors, and interactions, but also our physiological reactions. Recent\nadvances in wearable technology have facilitated studying emotions through\ncardio-respiratory signals. Accelerometers offer a non-invasive, convenient,\nand cost-effective method for capturing heart- and pulmonary-induced vibrations\non the chest wall, specifically Seismocardiography (SCG) and\nAccelerometry-Derived Respiration (ADR). Their affordability, wide\navailability, and ability to provide rich contextual data make accelerometers\nideal for everyday use. While accelerometers have been used as part of broader\nmodality fusions for Emotion Recognition (ER), their stand-alone potential via\nSCG and ADR remains unexplored. Bridging this gap could significantly help the\nembedding of ER into real-world applications, minimizing the hardware, and\nincreasing contextual integration potentials. To address this gap, we introduce\nSCG and ADR as novel modalities for ER and evaluate their performance using the\nEmoWear dataset. First, we replicate the single-trial emotion classification\npipeline from the DEAP dataset study, achieving similar results. Then we use\nour validated pipeline to train models that predict affective valence-arousal\nstates using SCG and compare them against established cardiac signals,\nElectrocardiography (ECG) and Blood Volume Pulse (BVP). Results show that SCG\nis a viable modality for ER, achieving similar performance to ECG and BVP. By\ncombining ADR with SCG, we achieved a working ER framework that only requires a\nsingle chest-worn accelerometer. These findings pave the way for integrating ER\ninto real-world, enabling seamless affective computing in everyday life."}
{"id": "2505.15960", "pdf": "https://arxiv.org/pdf/2505.15960.pdf", "abs": "https://arxiv.org/abs/2505.15960", "title": "Training Step-Level Reasoning Verifiers with Formal Verification Tools", "authors": ["Ryo Kamoi", "Yusen Zhang", "Nan Zhang", "Sarkar Snigdha Sarathi Das", "Rui Zhang"], "categories": ["cs.CL"], "comment": "Datasets, models, and code are provided at\n  https://github.com/psunlpgroup/FoVer. Please also refer to our project\n  website at https://fover-prm.github.io/", "summary": "Process Reward Models (PRMs), which provide step-by-step feedback on the\nreasoning generated by Large Language Models (LLMs), are receiving increasing\nattention. However, two key research gaps remain: collecting accurate\nstep-level error labels for training typically requires costly human\nannotation, and existing PRMs are limited to math reasoning problems. In\nresponse to these gaps, this paper aims to address the challenges of automatic\ndataset creation and the generalization of PRMs to diverse reasoning tasks. To\nachieve this goal, we propose FoVer, an approach for training PRMs on\nstep-level error labels automatically annotated by formal verification tools,\nsuch as Z3 for formal logic and Isabelle for theorem proof, which provide\nautomatic and accurate verification for symbolic tasks. Using this approach, we\nsynthesize a training dataset with error labels on LLM responses for formal\nlogic and theorem proof tasks without human annotation. Although this data\nsynthesis is feasible only for tasks compatible with formal verification, we\nobserve that LLM-based PRMs trained on our dataset exhibit cross-task\ngeneralization, improving verification across diverse reasoning tasks.\nSpecifically, PRMs trained with FoVer significantly outperform baseline PRMs\nbased on the original LLMs and achieve competitive or superior results compared\nto state-of-the-art PRMs trained on labels annotated by humans or stronger\nmodels, as measured by step-level verification on ProcessBench and Best-of-K\nperformance across 12 reasoning benchmarks, including MATH, AIME, ANLI, MMLU,\nand BBH. The datasets, models, and code are provided at\nhttps://github.com/psunlpgroup/FoVer."}
{"id": "2502.17293", "pdf": "https://arxiv.org/pdf/2502.17293.pdf", "abs": "https://arxiv.org/abs/2502.17293", "title": "The Challenges and Benefits of Bringing Religious Values Into Design", "authors": ["Louisa Conwill", "Megan K. Levis", "Karla Badillo-Urquiola", "Walter J. Scheirer"], "categories": ["cs.HC"], "comment": null, "summary": "HCI is increasingly taking inspiration from religious traditions as a basis\nfor ethical technology designs. Such ethically-inspired designs can be\nespecially important for social communications technologies, which are\nassociated with numerous societal concerns. If religious values are to be\nincorporated into real-world designs, there may be challenges when designers\nwork with values unfamiliar to them. Therefore, we investigate the difference\nin interpretations of values when they are translated to technology designs. To\ndo so we studied design patterns that embody Catholic Social Teaching (CST). We\ninterviewed 24 technologists and 7 CST scholars to assess how their\nunderstanding of how those values would manifest in social media designs. We\nfound that for the most part the technologists responded similarly to the CST\nscholars. However, CST scholars had a better understanding of the principle of\nsubsidiarity, and they believed moderation upheld human dignity more than the\ntechnologists did. We discuss the implications of our findings on the designs\nof social technologies and design processes at large."}
{"id": "2505.15962", "pdf": "https://arxiv.org/pdf/2505.15962.pdf", "abs": "https://arxiv.org/abs/2505.15962", "title": "Pre-training Large Memory Language Models with Internal and External Knowledge", "authors": ["Linxi Zhao", "Sofian Zalouk", "Christian K. Belardi", "Justin Lovelace", "Jin Peng Zhou", "Kilian Q. Weinberger", "Yoav Artzi", "Jennifer J. Sun"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Neural language models are black-boxes -- both linguistic patterns and\nfactual knowledge are distributed across billions of opaque parameters. This\nentangled encoding makes it difficult to reliably inspect, verify, or update\nspecific facts. We propose a new class of language models, Large Memory\nLanguage Models (LMLM) with a pre-training recipe that stores factual knowledge\nin both internal weights and an external database. Our approach strategically\nmasks externally retrieved factual values from the training loss, thereby\nteaching the model to perform targeted lookups rather than relying on\nmemorization in model weights. Our experiments demonstrate that LMLMs achieve\ncompetitive performance compared to significantly larger, knowledge-dense LLMs\non standard benchmarks, while offering the advantages of explicit, editable,\nand verifiable knowledge bases. This work represents a fundamental shift in how\nlanguage models interact with and manage factual knowledge."}
{"id": "2312.15889", "pdf": "https://arxiv.org/pdf/2312.15889.pdf", "abs": "https://arxiv.org/abs/2312.15889", "title": "Combining SNNs with Filtering for Efficient Neural Decoding in Implantable Brain-Machine Interfaces", "authors": ["Biyan Zhou", "Pao-Sheng Vincent Sun", "Arindam Basu"], "categories": ["cs.LG", "cs.HC", "cs.NE", "q-bio.NC"], "comment": null, "summary": "While it is important to make implantable brain-machine interfaces (iBMI)\nwireless to increase patient comfort and safety, the trend of increased channel\ncount in recent neural probes poses a challenge due to the concomitant increase\nin the data rate. Extracting information from raw data at the source by using\nedge computing is a promising solution to this problem, with integrated\nintention decoders providing the best compression ratio. Recent benchmarking\nefforts have shown recurrent neural networks to be the best solution. Spiking\nNeural Networks (SNN) emerge as a promising solution for resource efficient\nneural decoding while Long Short Term Memory (LSTM) networks achieve the best\naccuracy. In this work, we show that combining traditional signal processing\ntechniques, namely signal filtering, with SNNs improve their decoding\nperformance significantly for regression tasks, closing the gap with LSTMs, at\nlittle added cost. Results with different filters are shown with Bessel filters\nproviding best performance. Two block-bidirectional Bessel filters have been\nused--one for low latency and another for high accuracy. Adding the high\naccuracy variant of the Bessel filters to the output of ANN, SNN and variants\nprovided statistically significant benefits with maximum gains of $\\approx 5\\%$\nand $8\\%$ in $R^2$ for two SNN topologies (SNN\\_Streaming and SNN\\_3D). Our\nwork presents state of the art results for this dataset and paves the way for\ndecoder-integrated-implants of the future."}
{"id": "2505.15993", "pdf": "https://arxiv.org/pdf/2505.15993.pdf", "abs": "https://arxiv.org/abs/2505.15993", "title": "Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku", "authors": ["Anirudh Maiya", "Razan Alghamdi", "Maria Leonor Pacheco", "Ashutosh Trivedi", "Fabio Somenzi"], "categories": ["cs.CL"], "comment": "Accepted to Findings of ACL 2025", "summary": "The success of Large Language Models (LLMs) in human-AI collaborative\ndecision-making hinges on their ability to provide trustworthy, gradual, and\ntailored explanations. Solving complex puzzles, such as Sudoku, offers a\ncanonical example of this collaboration, where clear and customized\nexplanations often hold greater importance than the final solution. In this\nstudy, we evaluate the performance of five LLMs in solving and explaining\n\\sixsix{} Sudoku puzzles. While one LLM demonstrates limited success in solving\npuzzles, none can explain the solution process in a manner that reflects\nstrategic reasoning or intuitive problem-solving. These findings underscore\nsignificant challenges that must be addressed before LLMs can become effective\npartners in human-AI collaborative decision-making."}
{"id": "2501.05714", "pdf": "https://arxiv.org/pdf/2501.05714.pdf", "abs": "https://arxiv.org/abs/2501.05714", "title": "How to Enable Effective Cooperation Between Humans and NLP Models: A Survey of Principles, Formalizations, and Beyond", "authors": ["Chen Huang", "Yang Deng", "Wenqiang Lei", "Jiancheng Lv", "Tat-Seng Chua", "Jimmy Xiangji Huang"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "ACL 2025 Main paper", "summary": "With the advancement of large language models (LLMs), intelligent models have\nevolved from mere tools to autonomous agents with their own goals and\nstrategies for cooperating with humans. This evolution has birthed a novel\nparadigm in NLP, i.e., human-model cooperation, that has yielded remarkable\nprogress in numerous NLP tasks in recent years. In this paper, we take the\nfirst step to present a thorough review of human-model cooperation, exploring\nits principles, formalizations, and open challenges. In particular, we\nintroduce a new taxonomy that provides a unified perspective to summarize\nexisting approaches. Also, we discuss potential frontier areas and their\ncorresponding challenges. We regard our work as an entry point, paving the way\nfor more breakthrough research in this regard."}
{"id": "2505.16000", "pdf": "https://arxiv.org/pdf/2505.16000.pdf", "abs": "https://arxiv.org/abs/2505.16000", "title": "Leveraging Online Data to Enhance Medical Knowledge in a Small Persian Language Model", "authors": ["Mehrdad ghassabi", "Pedram Rostami", "Hamidreza Baradaran Kashani", "Amirhossein Poursina", "Zahra Kazemi", "Milad Tavakoli"], "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 4 figures", "summary": "The rapid advancement of language models has demonstrated the potential of\nartificial intelligence in the healthcare industry. However, small language\nmodels struggle with specialized domains in low-resource languages like\nPersian. While numerous medical-domain websites exist in Persian, no curated\ndataset or corpus has been available making ours the first of its kind. This\nstudy explores the enhancement of medical knowledge in a small language model\nby leveraging accessible online data, including a crawled corpus from medical\nmagazines and a dataset of real doctor-patient QA pairs. We fine-tuned a\nbaseline model using our curated data to improve its medical knowledge.\nBenchmark evaluations demonstrate that the fine-tuned model achieves improved\naccuracy in medical question answering and provides better responses compared\nto its baseline. This work highlights the potential of leveraging open-access\nonline data to enrich small language models in medical fields, providing a\nnovel solution for Persian medical AI applications suitable for\nresource-constrained environments."}
{"id": "2504.21800", "pdf": "https://arxiv.org/pdf/2504.21800.pdf", "abs": "https://arxiv.org/abs/2504.21800", "title": "How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues", "authors": ["Suhas BN", "Dominik Mattioli", "Saeed Abdullah", "Rosa I. Arriaga", "Chris W. Wiese", "Andrew M. Sherrill"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "68T50", "I.2.7; H.3.1"], "comment": "10 pages, 5 tables", "summary": "The growing adoption of synthetic data in healthcare is driven by privacy\nconcerns, limited access to real-world data, and the high cost of annotation.\nThis work explores the use of synthetic Prolonged Exposure (PE) therapeutic\nconversations for Post-Traumatic Stress Disorder (PTSD) as a scalable\nalternative for training and evaluating clinical models. We systematically\ncompare real and synthetic dialogues using linguistic, structural, and\nprotocol-specific metrics, including turn-taking patterns and treatment\nfidelity. We also introduce and evaluate PE-specific metrics derived from\nlinguistic analysis and semantic modeling, offering a novel framework for\nassessing clinical fidelity beyond surface fluency. Our findings show that\nalthough synthetic data holds promise for mitigating data scarcity and\nprotecting patient privacy, it can struggle to capture the subtle dynamics of\ntherapeutic interactions. Synthetic therapy dialogues closely match structural\nfeatures of real-world conversations (e.g., speaker switch ratio: 0.98 vs.\n0.99); however, they may not adequately reflect key fidelity markers (e.g.,\ndistress monitoring). We highlight gaps in existing evaluation frameworks and\nadvocate for fidelity-aware metrics that go beyond surface fluency to uncover\nclinically significant failures. Our findings clarify where synthetic data can\neffectively complement real-world datasets -- and where critical limitations\nremain."}
{"id": "2505.16002", "pdf": "https://arxiv.org/pdf/2505.16002.pdf", "abs": "https://arxiv.org/abs/2505.16002", "title": "Causal Interventions Reveal Shared Structure Across English Filler-Gap Constructions", "authors": ["Sasha Boguraev", "Christopher Potts", "Kyle Mahowald"], "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 19 figures, 11 tables", "summary": "Large Language Models (LLMs) have emerged as powerful sources of evidence for\nlinguists seeking to develop theories of syntax. In this paper, we argue that\ncausal interpretability methods, applied to LLMs, can greatly enhance the value\nof such evidence by helping us characterize the abstract mechanisms that LLMs\nlearn to use. Our empirical focus is a set of English filler-gap dependency\nconstructions (e.g., questions, relative clauses). Linguistic theories largely\nagree that these constructions share many properties. Using experiments based\nin Distributed Interchange Interventions, we show that LLMs converge on similar\nabstract analyses of these constructions. These analyses also reveal previously\noverlooked factors -- relating to frequency, filler type, and surrounding\ncontext -- that could motivate changes to standard linguistic theory. Overall,\nthese results suggest that mechanistic, internal analyses of LLMs can push\nlinguistic theory forward."}
{"id": "2505.16003", "pdf": "https://arxiv.org/pdf/2505.16003.pdf", "abs": "https://arxiv.org/abs/2505.16003", "title": "SLMEval: Entropy-Based Calibration for Human-Aligned Evaluation of Large Language Models", "authors": ["Roland Daynauth", "Christopher Clarke", "Krisztian Flautner", "Lingjia Tang", "Jason Mars"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The LLM-as-a-Judge paradigm offers a scalable, reference-free approach for\nevaluating language models. Although several calibration techniques have been\nproposed to better align these evaluators with human judgment, prior studies\nfocus primarily on narrow, well-structured benchmarks. As a result, it remains\nunclear whether such calibrations generalize to real-world, open-ended tasks.\n  In this work, we show that SOTA calibrated evaluators often fail in these\nsettings, exhibiting weak or even negative correlation with human judgments. To\naddress this, we propose SLMEval, a novel and efficient calibration method\nbased on entropy maximization over a small amount of human preference data. By\nestimating a latent distribution over model quality and reweighting evaluator\nscores accordingly, SLMEval achieves strong correlation with human evaluations\nacross two real-world production use cases and the public benchmark. For\nexample, on one such task, SLMEval achieves a Spearman correlation of 0.57 with\nhuman judgments, while G-Eval yields a negative correlation. In addition,\nSLMEval reduces evaluation costs by 5-30x compared to GPT-4-based calibrated\nevaluators such as G-eval."}
{"id": "2505.16008", "pdf": "https://arxiv.org/pdf/2505.16008.pdf", "abs": "https://arxiv.org/abs/2505.16008", "title": "LAGO: Few-shot Crosslingual Embedding Inversion Attacks via Language Similarity-Aware Graph Optimization", "authors": ["Wenrui Yu", "Yiyi Chen", "Johannes Bjerva", "Sokol Kosta", "Qiongxiu Li"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "We propose LAGO - Language Similarity-Aware Graph Optimization - a novel\napproach for few-shot cross-lingual embedding inversion attacks, addressing\ncritical privacy vulnerabilities in multilingual NLP systems. Unlike prior work\nin embedding inversion attacks that treat languages independently, LAGO\nexplicitly models linguistic relationships through a graph-based constrained\ndistributed optimization framework. By integrating syntactic and lexical\nsimilarity as edge constraints, our method enables collaborative parameter\nlearning across related languages. Theoretically, we show this formulation\ngeneralizes prior approaches, such as ALGEN, which emerges as a special case\nwhen similarity constraints are relaxed. Our framework uniquely combines\nFrobenius-norm regularization with linear inequality or total variation\nconstraints, ensuring robust alignment of cross-lingual embedding spaces even\nwith extremely limited data (as few as 10 samples per language). Extensive\nexperiments across multiple languages and embedding models demonstrate that\nLAGO substantially improves the transferability of attacks with 10-20% increase\nin Rouge-L score over baselines. This work establishes language similarity as a\ncritical factor in inversion attack transferability, urging renewed focus on\nlanguage-aware privacy-preserving multilingual embeddings."}
{"id": "2505.16014", "pdf": "https://arxiv.org/pdf/2505.16014.pdf", "abs": "https://arxiv.org/abs/2505.16014", "title": "Ranking Free RAG: Replacing Re-ranking with Selection in RAG for Sensitive Domains", "authors": ["Yash Saxena", "Anpur Padia", "Mandar S Chaudhary", "Kalpa Gunaratna", "Srinivasan Parthasarathy", "Manas Gaur"], "categories": ["cs.CL"], "comment": null, "summary": "Traditional Retrieval-Augmented Generation (RAG) pipelines rely on\nsimilarity-based retrieval and re-ranking, which depend on heuristics such as\ntop-k, and lack explainability, interpretability, and robustness against\nadversarial content. To address this gap, we propose a novel method METEORA\nthat replaces re-ranking in RAG with a rationale-driven selection approach.\nMETEORA operates in two stages. First, a general-purpose LLM is\npreference-tuned to generate rationales conditioned on the input query using\ndirect preference optimization. These rationales guide the evidence chunk\nselection engine, which selects relevant chunks in three stages: pairing\nindividual rationales with corresponding retrieved chunks for local relevance,\nglobal selection with elbow detection for adaptive cutoff, and context\nexpansion via neighboring chunks. This process eliminates the need for top-k\nheuristics. The rationales are also used for consistency check using a Verifier\nLLM to detect and filter poisoned or misleading content for safe generation.\nThe framework provides explainable and interpretable evidence flow by using\nrationales consistently across both selection and verification. Our evaluation\nacross six datasets spanning legal, financial, and academic research domains\nshows that METEORA improves generation accuracy by 33.34% while using\napproximately 50% fewer chunks than state-of-the-art re-ranking methods. In\nadversarial settings, METEORA significantly improves the F1 score from 0.10 to\n0.44 over the state-of-the-art perplexity-based defense baseline, demonstrating\nstrong resilience to poisoning attacks. Code available at:\nhttps://anonymous.4open.science/r/METEORA-DC46/README.md"}
{"id": "2505.16022", "pdf": "https://arxiv.org/pdf/2505.16022.pdf", "abs": "https://arxiv.org/abs/2505.16022", "title": "NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning", "authors": ["Wei Liu", "Siya Qi", "Xinyu Wang", "Chen Qian", "Yali Du", "Yulan He"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "20 pages, 5 tables, 12 figures", "summary": "Recent advances such as DeepSeek R1-Zero highlight the effectiveness of\nincentive training, a reinforcement learning paradigm that computes rewards\nsolely based on the final answer part of a language model's output, thereby\nencouraging the generation of intermediate reasoning steps. However, these\nmethods fundamentally rely on external verifiers, which limits their\napplicability to domains like mathematics and coding where such verifiers are\nreadily available. Although reward models can serve as verifiers, they require\nhigh-quality annotated data and are costly to train. In this work, we propose\nNOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning\nframework that requires only standard supervised fine-tuning data with no need\nfor an external verifier. NOVER enables incentive training across a wide range\nof text-to-text tasks and outperforms the model of the same size distilled from\nlarge reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the\nflexibility of NOVER enables new possibilities for optimizing large language\nmodels, such as inverse incentive training."}
{"id": "2505.16023", "pdf": "https://arxiv.org/pdf/2505.16023.pdf", "abs": "https://arxiv.org/abs/2505.16023", "title": "Prototypical Human-AI Collaboration Behaviors from LLM-Assisted Writing in the Wild", "authors": ["Sheshera Mysore", "Debarati Das", "Hancheng Cao", "Bahareh Sarrafzadeh"], "categories": ["cs.CL", "cs.HC"], "comment": "Pre-print under-review", "summary": "As large language models (LLMs) are used in complex writing workflows, users\nengage in multi-turn interactions to steer generations to better fit their\nneeds. Rather than passively accepting output, users actively refine, explore,\nand co-construct text. We conduct a large-scale analysis of this collaborative\nbehavior for users engaged in writing tasks in the wild with two popular AI\nassistants, Bing Copilot and WildChat. Our analysis goes beyond simple task\nclassification or satisfaction estimation common in prior work and instead\ncharacterizes how users interact with LLMs through the course of a session. We\nidentify prototypical behaviors in how users interact with LLMs in prompts\nfollowing their original request. We refer to these as Prototypical Human-AI\nCollaboration Behaviors (PATHs) and find that a small group of PATHs explain a\nmajority of the variation seen in user-LLM interaction. These PATHs span users\nrevising intents, exploring texts, posing questions, adjusting style or\ninjecting new content. Next, we find statistically significant correlations\nbetween specific writing intents and PATHs, revealing how users' intents shape\ntheir collaboration behaviors. We conclude by discussing the implications of\nour findings on LLM alignment."}
{"id": "2505.16036", "pdf": "https://arxiv.org/pdf/2505.16036.pdf", "abs": "https://arxiv.org/abs/2505.16036", "title": "OpenEthics: A Comprehensive Ethical Evaluation of Open-Source Generative Large Language Models", "authors": ["Burak Erinç Çetin", "Yıldırım Özen", "Elif Naz Demiryılmaz", "Kaan Engür", "Cagri Toraman"], "categories": ["cs.CL"], "comment": null, "summary": "Generative large language models present significant potential but also raise\ncritical ethical concerns. Most studies focus on narrow ethical dimensions, and\nalso limited diversity of languages and models. To address these gaps, we\nconduct a broad ethical evaluation of 29 recent open-source large language\nmodels using a novel data collection including four ethical aspects:\nRobustness, reliability, safety, and fairness. We analyze model behavior in\nboth a commonly used language, English, and a low-resource language, Turkish.\nOur aim is to provide a comprehensive ethical assessment and guide safer model\ndevelopment by filling existing gaps in evaluation breadth, language coverage,\nand model diversity. Our experimental results, based on LLM-as-a-Judge, reveal\nthat optimization efforts for many open-source models appear to have\nprioritized safety and fairness, and demonstrated good robustness while\nreliability remains a concern. We demonstrate that ethical evaluation can be\neffectively conducted independently of the language used. In addition, models\nwith larger parameter counts tend to exhibit better ethical performance, with\nGemma and Qwen models demonstrating the most ethical behavior among those\nevaluated."}
{"id": "2505.16061", "pdf": "https://arxiv.org/pdf/2505.16061.pdf", "abs": "https://arxiv.org/abs/2505.16061", "title": "Internal and External Impacts of Natural Language Processing Papers", "authors": ["Yu Zhang"], "categories": ["cs.CL", "cs.DL"], "comment": "7 pages; Accepted to ACL 2025", "summary": "We investigate the impacts of NLP research published in top-tier conferences\n(i.e., ACL, EMNLP, and NAACL) from 1979 to 2024. By analyzing citations from\nresearch articles and external sources such as patents, media, and policy\ndocuments, we examine how different NLP topics are consumed both within the\nacademic community and by the broader public. Our findings reveal that language\nmodeling has the widest internal and external influence, while linguistic\nfoundations have lower impacts. We also observe that internal and external\nimpacts generally align, but topics like ethics, bias, and fairness show\nsignificant attention in policy documents with much fewer academic citations.\nAdditionally, external domains exhibit distinct preferences, with patents\nfocusing on practical NLP applications and media and policy documents engaging\nmore with the societal implications of NLP models."}
{"id": "2505.16078", "pdf": "https://arxiv.org/pdf/2505.16078.pdf", "abs": "https://arxiv.org/abs/2505.16078", "title": "Small Language Models in the Real World: Insights from Industrial Text Classification", "authors": ["Lujun Li", "Lama Sleem", "Niccolo' Gentile", "Geoffrey Nichil", "Radu State"], "categories": ["cs.CL"], "comment": null, "summary": "With the emergence of ChatGPT, Transformer models have significantly advanced\ntext classification and related tasks. Decoder-only models such as Llama\nexhibit strong performance and flexibility, yet they suffer from inefficiency\non inference due to token-by-token generation, and their effectiveness in text\nclassification tasks heavily depends on prompt quality. Moreover, their\nsubstantial GPU resource requirements often limit widespread adoption. Thus,\nthe question of whether smaller language models are capable of effectively\nhandling text classification tasks emerges as a topic of significant interest.\nHowever, the selection of appropriate models and methodologies remains largely\nunderexplored. In this paper, we conduct a comprehensive evaluation of prompt\nengineering and supervised fine-tuning methods for transformer-based text\nclassification. Specifically, we focus on practical industrial scenarios,\nincluding email classification, legal document categorization, and the\nclassification of extremely long academic texts. We examine the strengths and\nlimitations of smaller models, with particular attention to both their\nperformance and their efficiency in Video Random-Access Memory (VRAM)\nutilization, thereby providing valuable insights for the local deployment and\napplication of compact models in industrial settings."}
{"id": "2505.16081", "pdf": "https://arxiv.org/pdf/2505.16081.pdf", "abs": "https://arxiv.org/abs/2505.16081", "title": "BiasLab: Toward Explainable Political Bias Detection with Dual-Axis Annotations and Rationale Indicators", "authors": ["KMA Solaiman"], "categories": ["cs.CL"], "comment": "Under review", "summary": "We present BiasLab, a dataset of 300 political news articles annotated for\nperceived ideological bias. These articles were selected from a curated\n900-document pool covering diverse political events and source biases. Each\narticle is labeled by crowdworkers along two independent scales, assessing\nsentiment toward the Democratic and Republican parties, and enriched with\nrationale indicators. The annotation pipeline incorporates targeted worker\nqualification and was refined through pilot-phase analysis. We quantify\ninter-annotator agreement, analyze misalignment with source-level outlet bias,\nand organize the resulting labels into interpretable subsets. Additionally, we\nsimulate annotation using schema-constrained GPT-4o, enabling direct comparison\nto human labels and revealing mirrored asymmetries, especially in\nmisclassifying subtly right-leaning content. We define two modeling tasks:\nperception drift prediction and rationale type classification, and report\nbaseline performance to illustrate the challenge of explainable bias detection.\nBiasLab's rich rationale annotations provide actionable interpretations that\nfacilitate explainable modeling of political bias, supporting the development\nof transparent, socially aware NLP systems. We release the dataset, annotation\nschema, and modeling code to encourage research on human-in-the-loop\ninterpretability and the evaluation of explanation effectiveness in real-world\nsettings."}
{"id": "2505.16088", "pdf": "https://arxiv.org/pdf/2505.16088.pdf", "abs": "https://arxiv.org/abs/2505.16088", "title": "Date Fragments: A Hidden Bottleneck of Tokenization for Temporal Reasoning", "authors": ["Gagan Bhatia", "Maxime Peyrard", "Wei Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Modern BPE tokenizers often split calendar dates into meaningless fragments,\ne.g., 20250312 $\\rightarrow$ 202, 503, 12, inflating token counts and obscuring\nthe inherent structure needed for robust temporal reasoning. In this work, we\n(1) introduce a simple yet interpretable metric, termed date fragmentation\nratio, that measures how faithfully a tokenizer preserves multi-digit date\ncomponents; (2) release DateAugBench, a suite of 6500 examples spanning three\ntemporal reasoning tasks: context-based date resolution, format-invariance\npuzzles, and date arithmetic across historical, contemporary, and future\nregimes; and (3) through layer-wise probing and causal attention-hop analyses,\nuncover an emergent date-abstraction mechanism whereby large language models\nstitch together the fragments of month, day, and year components for temporal\nreasoning. Our experiments show that excessive fragmentation correlates with\naccuracy drops of up to 10 points on uncommon dates like historical and\nfuturistic dates. Further, we find that the larger the model, the faster the\nemergent date abstraction that heals date fragments is accomplished. Lastly, we\nobserve a reasoning path that LLMs follow to assemble date fragments, typically\ndiffering from human interpretation (year $\\rightarrow$ month $\\rightarrow$\nday)."}
{"id": "2505.16102", "pdf": "https://arxiv.org/pdf/2505.16102.pdf", "abs": "https://arxiv.org/abs/2505.16102", "title": "Continually Self-Improving Language Models for Bariatric Surgery Question--Answering", "authors": ["Yash Kumar Atri", "Thomas H Shin", "Thomas Hartvigsen"], "categories": ["cs.CL"], "comment": null, "summary": "While bariatric and metabolic surgery (MBS) is considered the gold standard\ntreatment for severe and morbid obesity, its therapeutic efficacy hinges upon\nactive and longitudinal engagement with multidisciplinary providers, including\nsurgeons, dietitians/nutritionists, psychologists, and endocrinologists. This\nengagement spans the entire patient journey, from preoperative preparation to\nlong-term postoperative management. However, this process is often hindered by\nnumerous healthcare disparities, such as logistical and access barriers, which\nimpair easy patient access to timely, evidence-based, clinician-endorsed\ninformation. To address these gaps, we introduce bRAGgen, a novel adaptive\nretrieval-augmented generation (RAG)-based model that autonomously integrates\nreal-time medical evidence when response confidence dips below dynamic\nthresholds. This self-updating architecture ensures that responses remain\ncurrent and accurate, reducing the risk of misinformation. Additionally, we\npresent bRAGq, a curated dataset of 1,302 bariatric surgery--related questions,\nvalidated by an expert bariatric surgeon. bRAGq constitutes the first\nlarge-scale, domain-specific benchmark for comprehensive MBS care. In a\ntwo-phase evaluation, bRAGgen is benchmarked against state-of-the-art models\nusing both large language model (LLM)--based metrics and expert surgeon review.\nAcross all evaluation dimensions, bRAGgen demonstrates substantially superior\nperformance in generating clinically accurate and relevant responses."}
{"id": "2505.16104", "pdf": "https://arxiv.org/pdf/2505.16104.pdf", "abs": "https://arxiv.org/abs/2505.16104", "title": "Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models", "authors": ["Yue Li", "Xin Yi", "Dongsheng Shi", "Gerard de Melo", "Xiaoling Wang", "Linlin Wang"], "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": "ACL 2025 Findings", "summary": "With the increasing size of Large Vision-Language Models (LVLMs), network\npruning techniques aimed at compressing models for deployment in\nresource-constrained environments have garnered significant attention. However,\nwe observe that pruning often leads to a degradation in safety performance. To\naddress this issue, we present a novel and lightweight approach, termed\nHierarchical Safety Realignment (HSR). HSR operates by first quantifying the\ncontribution of each attention head to safety, identifying the most critical\nones, and then selectively restoring neurons directly within these attention\nheads that play a pivotal role in maintaining safety. This process\nhierarchically realigns the safety of pruned LVLMs, progressing from the\nattention head level to the neuron level. We validate HSR across various models\nand pruning strategies, consistently achieving notable improvements in safety\nperformance. To our knowledge, this is the first work explicitly focused on\nrestoring safety in LVLMs post-pruning."}
{"id": "2505.16107", "pdf": "https://arxiv.org/pdf/2505.16107.pdf", "abs": "https://arxiv.org/abs/2505.16107", "title": "MPL: Multiple Programming Languages with Large Language Models for Information Extraction", "authors": ["Bo Li", "Gexiang Fang", "Wei Ye", "Zhenghua Xu", "Jinglei Zhang", "Hao Cheng", "Shikun Zhang"], "categories": ["cs.CL"], "comment": "Findings of ACL2025", "summary": "Recent research in information extraction (IE) focuses on utilizing\ncode-style inputs to enhance structured output generation. The intuition behind\nthis is that the programming languages (PLs) inherently exhibit greater\nstructural organization than natural languages (NLs). This structural advantage\nmakes PLs particularly suited for IE tasks. Nevertheless, existing research\nprimarily focuses on Python for code-style simulation, overlooking the\npotential of other widely-used PLs (e.g., C++ and Java) during the supervised\nfine-tuning (SFT) phase. In this research, we propose \\textbf{M}ultiple\n\\textbf{P}rogramming \\textbf{L}anguages with large language models for\ninformation extraction (abbreviated as \\textbf{MPL}), a novel framework that\nexplores the potential of incorporating different PLs in the SFT phase.\nAdditionally, we introduce \\texttt{function-prompt} with virtual running to\nsimulate code-style inputs more effectively and efficiently. Experimental\nresults on a wide range of datasets demonstrate the effectiveness of MPL.\nFurthermore, we conduct extensive experiments to provide a comprehensive\nanalysis. We have released our code for future research."}
{"id": "2505.16118", "pdf": "https://arxiv.org/pdf/2505.16118.pdf", "abs": "https://arxiv.org/abs/2505.16118", "title": "Semiotic Reconstruction of Destination Expectation Constructs An LLM-Driven Computational Paradigm for Social Media Tourism Analytics", "authors": ["Haotian Lan", "Yao Gao", "Yujun Cheng", "Wei Yuan", "Kun Wang"], "categories": ["cs.CL", "stat.AP"], "comment": "33 pages, 6 figures", "summary": "Social media's rise establishes user-generated content (UGC) as pivotal for\ntravel decisions, yet analytical methods lack scalability. This study\nintroduces a dual-method LLM framework: unsupervised expectation extraction\nfrom UGC paired with survey-informed supervised fine-tuning. Findings reveal\nleisure/social expectations drive engagement more than foundational\nnatural/emotional factors. By establishing LLMs as precision tools for\nexpectation quantification, we advance tourism analytics methodology and\npropose targeted strategies for experience personalization and social travel\npromotion. The framework's adaptability extends to consumer behavior research,\ndemonstrating computational social science's transformative potential in\nmarketing optimization."}
{"id": "2505.16125", "pdf": "https://arxiv.org/pdf/2505.16125.pdf", "abs": "https://arxiv.org/abs/2505.16125", "title": "KoBALT: Korean Benchmark For Advanced Linguistic Tasks", "authors": ["Hyopil Shin", "Sangah Lee", "Dongjun Jang", "Wooseok Song", "Jaeyoon Kim", "Chaeyoung Oh", "Hyemi Jo", "Youngchae Ahn", "Sihyun Oh", "Hyohyeong Chang", "Sunkyoung Kim", "Jinsik Lee"], "categories": ["cs.CL"], "comment": "Under Reveiw", "summary": "We introduce KoBALT (Korean Benchmark for Advanced Linguistic Tasks), a\ncomprehensive linguistically-motivated benchmark comprising 700 multiple-choice\nquestions spanning 24 phenomena across five linguistic domains: syntax,\nsemantics, pragmatics, phonetics/phonology, and morphology. KoBALT is designed\nto advance the evaluation of large language models (LLMs) in Korean, a\nmorphologically rich language, by addressing the limitations of conventional\nbenchmarks that often lack linguistic depth and typological grounding. It\nintroduces a suite of expert-curated, linguistically motivated questions with\nminimal n-gram overlap with standard Korean corpora, substantially mitigating\nthe risk of data contamination and allowing a more robust assessment of true\nlanguage understanding. Our evaluation of 20 contemporary LLMs reveals\nsignificant performance disparities, with the highest-performing model\nachieving 61\\% general accuracy but showing substantial variation across\nlinguistic domains - from stronger performance in semantics (66\\%) to\nconsiderable weaknesses in phonology (31\\%) and morphology (36\\%). Through\nhuman preference evaluation with 95 annotators, we demonstrate a strong\ncorrelation between KoBALT scores and human judgments, validating our\nbenchmark's effectiveness as a discriminative measure of Korean language\nunderstanding. KoBALT addresses critical gaps in linguistic evaluation for\ntypologically diverse languages and provides a robust framework for assessing\ngenuine linguistic competence in Korean language models."}
{"id": "2505.16128", "pdf": "https://arxiv.org/pdf/2505.16128.pdf", "abs": "https://arxiv.org/abs/2505.16128", "title": "Veracity Bias and Beyond: Uncovering LLMs' Hidden Beliefs in Problem-Solving Reasoning", "authors": ["Yue Zhou", "Barbara Di Eugenio"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Despite LLMs' explicit alignment against demographic stereotypes, they have\nbeen shown to exhibit biases under various social contexts. In this work, we\nfind that LLMs exhibit concerning biases in how they associate solution\nveracity with demographics. Through experiments across five human value-aligned\nLLMs on mathematics, coding, commonsense, and writing problems, we reveal two\nforms of such veracity biases: Attribution Bias, where models\ndisproportionately attribute correct solutions to certain demographic groups,\nand Evaluation Bias, where models' assessment of identical solutions varies\nbased on perceived demographic authorship. Our results show pervasive biases:\nLLMs consistently attribute fewer correct solutions and more incorrect ones to\nAfrican-American groups in math and coding, while Asian authorships are least\npreferred in writing evaluation. In additional studies, we show LLMs\nautomatically assign racially stereotypical colors to demographic groups in\nvisualization code, suggesting these biases are deeply embedded in models'\nreasoning processes. Our findings indicate that demographic bias extends beyond\nsurface-level stereotypes and social context provocations, raising concerns\nabout LLMs' deployment in educational and evaluation settings."}
{"id": "2505.16129", "pdf": "https://arxiv.org/pdf/2505.16129.pdf", "abs": "https://arxiv.org/abs/2505.16129", "title": "LLMs Are Not Scorers: Rethinking MT Evaluation with Generation-Based Methods", "authors": ["Hyang Cui"], "categories": ["cs.CL", "I.2.7"], "comment": "5 pages, 2 figures, 2 tables. Conforms to the ACL Rolling Review\n  (ARR) short paper track. Code and data available at:\n  https://github.com/CuiNiki/LLMs-Are-Not-Scorers", "summary": "Recent studies have applied large language models (LLMs) to machine\ntranslation quality estimation (MTQE) by prompting models to assign numeric\nscores. Nonetheless, these direct scoring methods tend to show low\nsegment-level correlation with human judgments. In this paper, we propose a\ngeneration-based evaluation paradigm that leverages decoder-only LLMs to\nproduce high-quality references, followed by semantic similarity scoring using\nsentence embeddings. We conduct the most extensive evaluation to date in MTQE,\ncovering 8 LLMs and 8 language pairs. Empirical results show that our method\noutperforms both intra-LLM direct scoring baselines and external non-LLM\nreference-free metrics from MTME. These findings demonstrate the strength of\ngeneration-based evaluation and support a shift toward hybrid approaches that\ncombine fluent generation with accurate semantic assessment."}
{"id": "2505.16134", "pdf": "https://arxiv.org/pdf/2505.16134.pdf", "abs": "https://arxiv.org/abs/2505.16134", "title": "Position of Uncertainty: A Cross-Linguistic Study of Positional Bias in Large Language Models", "authors": ["Menschikov Mikhail", "Alexander Kharitonov", "Maiia Kotyga", "Vadim Porvatov", "Anna Zhukovskaya", "David Kagramanyan", "Egor Shvetsov", "Evgeny Burnaev"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models exhibit positional bias -- systematic neglect of\ninformation at specific context positions -- yet its interplay with linguistic\ndiversity remains poorly understood. We present a cross-linguistic study across\nfive typologically distinct languages (English, Russian, German, Hindi,\nVietnamese), examining how positional bias interacts with model uncertainty,\nsyntax, and prompting. Key findings: (1) Positional bias is model-driven, with\nlanguage-specific variations -- Qwen2.5-7B favors late positions, challenging\nassumptions of early-token bias; (2) Explicit positional guidance (e.g.,\ncorrect context is at position X) reduces accuracy across languages,\nundermining prompt-engineering practices; (3) Aligning context with positional\nbias increases entropy, yet minimal entropy does not predict accuracy. (4) We\nfurther uncover that LLMs differently impose dominant word order in\nfree-word-order languages like Hindi."}
{"id": "2505.16142", "pdf": "https://arxiv.org/pdf/2505.16142.pdf", "abs": "https://arxiv.org/abs/2505.16142", "title": "Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning", "authors": ["Shicheng Xu", "Liang Pang", "Yunchang Zhu", "Jia Gu", "Zihao Wei", "Jingcheng Deng", "Feiyang Pan", "Huawei Shen", "Xueqi Cheng"], "categories": ["cs.CL"], "comment": "15 pages", "summary": "Distilling reasoning paths from teacher to student models via supervised\nfine-tuning (SFT) provides a shortcut for improving the reasoning ability of\nsmaller Large Language Models (LLMs). However, the reasoning paths generated by\nteacher models often reflect only surface-level traces of their underlying\nauthentic reasoning. Insights from cognitive neuroscience suggest that\nauthentic reasoning involves a complex interweaving between meta-reasoning\n(which selects appropriate sub-problems from multiple candidates) and solving\n(which addresses the sub-problem). This implies authentic reasoning has an\nimplicit multi-branch structure. Supervised fine-tuning collapses this rich\nstructure into a flat sequence of token prediction in the teacher's reasoning\npath, preventing effective distillation of this structure to students. To\naddress this limitation, we propose RLKD, a reinforcement learning (RL)-based\ndistillation framework guided by a novel Generative Structure Reward Model\n(GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving\nsteps and computes rewards to measure structural alignment between student and\nteacher reasoning. RLKD combines this reward with RL, enabling student LLMs to\ninternalize the teacher's implicit multi-branch reasoning structure rather than\nmerely mimicking fixed output paths. Experiments show RLKD surpasses standard\nSFT-RL pipelines even when trained on 0.1% of data under an RL-only regime,\nunlocking greater student reasoning potential than SFT-based distillation."}
{"id": "2505.16160", "pdf": "https://arxiv.org/pdf/2505.16160.pdf", "abs": "https://arxiv.org/abs/2505.16160", "title": "EduBench: A Comprehensive Benchmarking Dataset for Evaluating Large Language Models in Diverse Educational Scenarios", "authors": ["Bin Xu", "Yu Bai", "Huashan Sun", "Yiguan Lin", "Siming Liu", "Xinyue Liang", "Yaolin Li", "Yang Gao", "Heyan Huang"], "categories": ["cs.CL"], "comment": null, "summary": "As large language models continue to advance, their application in\neducational contexts remains underexplored and under-optimized. In this paper,\nwe address this gap by introducing the first diverse benchmark tailored for\neducational scenarios, incorporating synthetic data containing 9 major\nscenarios and over 4,000 distinct educational contexts. To enable comprehensive\nassessment, we propose a set of multi-dimensional evaluation metrics that cover\n12 critical aspects relevant to both teachers and students. We further apply\nhuman annotation to ensure the effectiveness of the model-generated evaluation\nresponses. Additionally, we succeed to train a relatively small-scale model on\nour constructed dataset and demonstrate that it can achieve performance\ncomparable to state-of-the-art large models (e.g., Deepseek V3, Qwen Max) on\nthe test set. Overall, this work provides a practical foundation for the\ndevelopment and evaluation of education-oriented language models. Code and data\nare released at https://github.com/ybai-nlp/EduBench."}
{"id": "2505.16162", "pdf": "https://arxiv.org/pdf/2505.16162.pdf", "abs": "https://arxiv.org/abs/2505.16162", "title": "KNN-SSD: Enabling Dynamic Self-Speculative Decoding via Nearest Neighbor Layer Set Optimization", "authors": ["Mingbo Song", "Heming Xia", "Jun Zhang", "Chak Tou Leong", "Qiancheng Xu", "Wenjie Li", "Sujian Li"], "categories": ["cs.CL"], "comment": "8 pages", "summary": "Speculative Decoding (SD) has emerged as a widely used paradigm to accelerate\nthe inference of large language models (LLMs) without compromising generation\nquality. It works by efficiently drafting multiple tokens using a compact model\nand then verifying them in parallel using the target LLM. Notably,\nSelf-Speculative Decoding proposes skipping certain layers to construct the\ndraft model, which eliminates the need for additional parameters or training.\nDespite its strengths, we observe in this work that drafting with layer\nskipping exhibits significant sensitivity to domain shifts, leading to a\nsubstantial drop in acceleration performance. To enhance the domain\ngeneralizability of this paradigm, we introduce KNN-SSD, an algorithm that\nleverages K-Nearest Neighbor (KNN) search to match different skipped layers\nwith various domain inputs. We evaluated our algorithm in various models and\nmultiple tasks, observing that its application leads to 1.3x-1.6x speedup in\nLLM inference."}
{"id": "2505.16164", "pdf": "https://arxiv.org/pdf/2505.16164.pdf", "abs": "https://arxiv.org/abs/2505.16164", "title": "Can LLMs Simulate Human Behavioral Variability? A Case Study in the Phonemic Fluency Task", "authors": ["Mengyang Qiu", "Zoe Brisebois", "Siena Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly explored as substitutes for\nhuman participants in cognitive tasks, but their ability to simulate human\nbehavioral variability remains unclear. This study examines whether LLMs can\napproximate individual differences in the phonemic fluency task, where\nparticipants generate words beginning with a target letter. We evaluated 34\nmodel configurations, varying prompt specificity, sampling temperature, and\nmodel type, and compared outputs to responses from 106 human participants.\nWhile some configurations, especially Claude 3.7 Sonnet, matched human averages\nand lexical preferences, none reproduced the scope of human variability. LLM\noutputs were consistently less diverse and structurally rigid, and LLM\nensembles failed to increase diversity. Network analyses further revealed\nfundamental differences in retrieval structure between humans and models. These\nresults highlight key limitations in using LLMs to simulate human cognition and\nbehavior."}
{"id": "2505.16170", "pdf": "https://arxiv.org/pdf/2505.16170.pdf", "abs": "https://arxiv.org/abs/2505.16170", "title": "When Do LLMs Admit Their Mistakes? Understanding the Role of Model Belief in Retraction", "authors": ["Yuqing Yang", "Robin Jia"], "categories": ["cs.CL"], "comment": null, "summary": "Can large language models (LLMs) admit their mistakes when they should know\nbetter? In this work, we define the behavior of acknowledging errors in\npreviously generated answers as \"retraction\" and aim to understand when and why\nLLMs choose to retract. We first construct model-specific datasets to evaluate\nwhether a model will retract an incorrect answer that contradicts its own\nparametric knowledge. While LLMs are capable of retraction, they do so only\ninfrequently. We demonstrate that retraction is closely tied to previously\nidentified indicators of models' internal belief: models fail to retract wrong\nanswers that they \"believe\" to be factually correct. Steering experiments\nfurther demonstrate that internal belief causally influences model retraction.\nIn particular, when the model does not believe its answer, this not only\nencourages the model to attempt to verify the answer, but also alters attention\nbehavior during self-verification. Finally, we demonstrate that simple\nsupervised fine-tuning significantly improves retraction performance by helping\nthe model learn more accurate internal beliefs. Code and datasets are available\non https://github.com/ayyyq/llm-retraction."}
{"id": "2505.16172", "pdf": "https://arxiv.org/pdf/2505.16172.pdf", "abs": "https://arxiv.org/abs/2505.16172", "title": "Automated Feedback Loops to Protect Text Simplification with Generative AI from Information Loss", "authors": ["Abhay Kumara Sri Krishna Nandiraju", "Gondy Leroy", "David Kauchak", "Arif Ahmed"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Understanding health information is essential in achieving and maintaining a\nhealthy life. We focus on simplifying health information for better\nunderstanding. With the availability of generative AI, the simplification\nprocess has become efficient and of reasonable quality, however, the algorithms\nremove information that may be crucial for comprehension. In this study, we\ncompare generative AI to detect missing information in simplified text,\nevaluate its importance, and fix the text with the missing information. We\ncollected 50 health information texts and simplified them using gpt-4-0613. We\ncompare five approaches to identify missing elements and regenerate the text by\ninserting the missing elements. These five approaches involve adding missing\nentities and missing words in various ways: 1) adding all the missing entities,\n2) adding all missing words, 3) adding the top-3 entities ranked by gpt-4-0613,\nand 4, 5) serving as controls for comparison, adding randomly chosen entities.\nWe use cosine similarity and ROUGE scores to evaluate the semantic similarity\nand content overlap between the original, simplified, and reconstructed\nsimplified text. We do this for both summaries and full text. Overall, we find\nthat adding missing entities improves the text. Adding all the missing entities\nresulted in better text regeneration, which was better than adding the\ntop-ranked entities or words, or random words. Current tools can identify these\nentities, but are not valuable in ranking them."}
{"id": "2505.16178", "pdf": "https://arxiv.org/pdf/2505.16178.pdf", "abs": "https://arxiv.org/abs/2505.16178", "title": "Understanding Fact Recall in Language Models: Why Two-Stage Training Encourages Memorization but Mixed Training Teaches Knowledge", "authors": ["Ying Zhang", "Benjamin Heinzerling", "Dongyuan Li", "Ryoma Ishigaki", "Yuta Hitomi", "Kentaro Inui"], "categories": ["cs.CL"], "comment": null, "summary": "Fact recall, the ability of language models (LMs) to retrieve specific\nfactual knowledge, remains a challenging task despite their impressive general\ncapabilities. Common training strategies often struggle to promote robust\nrecall behavior with two-stage training, which first trains a model with\nfact-storing examples (e.g., factual statements) and then with fact-recalling\nexamples (question-answer pairs), tending to encourage rote memorization rather\nthan generalizable fact retrieval. In contrast, mixed training, which jointly\nuses both types of examples, has been empirically shown to improve the ability\nto recall facts, but the underlying mechanisms are still poorly understood. In\nthis work, we investigate how these training strategies affect how model\nparameters are shaped during training and how these differences relate to their\nability to recall facts. We introduce cross-task gradient trace to identify\nshared parameters, those strongly influenced by both fact-storing and\nfact-recalling examples. Our analysis on synthetic fact recall datasets with\nthe Llama-3.2B and Pythia-2.8B models reveals that mixed training encouraging a\nlarger and more centralized set of shared parameters. These findings suggest\nthat the emergence of parameters may play a key role in enabling LMs to\ngeneralize factual knowledge across task formulations."}
{"id": "2505.16188", "pdf": "https://arxiv.org/pdf/2505.16188.pdf", "abs": "https://arxiv.org/abs/2505.16188", "title": "SAE-SSV: Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models", "authors": ["Zirui He", "Mingyu Jin", "Bo Shen", "Ali Payani", "Yongfeng Zhang", "Mengnan Du"], "categories": ["cs.CL"], "comment": "30 pages, 24 figures, 12 tables", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but controlling their behavior\nreliably remains challenging, especially in open-ended generation settings.\nThis paper introduces a novel supervised steering approach that operates in\nsparse, interpretable representation spaces. We employ sparse autoencoders\n(SAEs)to obtain sparse latent representations that aim to disentangle semantic\nattributes from model activations. Then we train linear classifiers to identify\na small subspace of task-relevant dimensions in latent representations.\nFinally, we learn supervised steering vectors constrained to this subspace,\noptimized to align with target behaviors. Experiments across sentiment,\ntruthfulness, and politics polarity steering tasks with multiple LLMs\ndemonstrate that our supervised steering vectors achieve higher success rates\nwith minimal degradation in generation quality compared to existing methods.\nFurther analysis reveals that a notably small subspace is sufficient for\neffective steering, enabling more targeted and interpretable interventions."}
{"id": "2505.16189", "pdf": "https://arxiv.org/pdf/2505.16189.pdf", "abs": "https://arxiv.org/abs/2505.16189", "title": "The Language of Interoception: Examining Embodiment and Emotion Through a Corpus of Body Part Mentions", "authors": ["Sophie Wu", "Jan Philip Wahle", "Saif M. Mohammad"], "categories": ["cs.CL"], "comment": "8 pages, 26 figures", "summary": "This paper is the first investigation of the connection between emotion,\nembodiment, and everyday language in a large sample of natural language data.\nWe created corpora of body part mentions (BPMs) in online English text (blog\nposts and tweets). This includes a subset featuring human annotations for the\nemotions of the person whose body part is mentioned in the text. We show that\nBPMs are common in personal narratives and tweets (~5% to 10% of posts include\nBPMs) and that their usage patterns vary markedly by time and %geographic\nlocation. Using word-emotion association lexicons and our annotated data, we\nshow that text containing BPMs tends to be more emotionally charged, even when\nthe BPM is not explicitly used to describe a physical reaction to the emotion\nin the text. Finally, we discover a strong and statistically significant\ncorrelation between body-related language and a variety of poorer health\noutcomes. In sum, we argue that investigating the role of body-part related\nwords in language can open up valuable avenues of future research at the\nintersection of NLP, the affective sciences, and the study of human wellbeing."}
{"id": "2505.16193", "pdf": "https://arxiv.org/pdf/2505.16193.pdf", "abs": "https://arxiv.org/abs/2505.16193", "title": "An Empirical Study on Configuring In-Context Learning Demonstrations for Unleashing MLLMs' Sentimental Perception Capability", "authors": ["Daiqing Wu", "Dongbao Yang", "Sicheng Zhao", "Can Ma", "Yu Zhou"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "The advancements in Multimodal Large Language Models (MLLMs) have enabled\nvarious multimodal tasks to be addressed under a zero-shot paradigm. This\nparadigm sidesteps the cost of model fine-tuning, emerging as a dominant trend\nin practical application. Nevertheless, Multimodal Sentiment Analysis (MSA), a\npivotal challenge in the quest for general artificial intelligence, fails to\naccommodate this convenience. The zero-shot paradigm exhibits undesirable\nperformance on MSA, casting doubt on whether MLLMs can perceive sentiments as\ncompetent as supervised models. By extending the zero-shot paradigm to\nIn-Context Learning (ICL) and conducting an in-depth study on configuring\ndemonstrations, we validate that MLLMs indeed possess such capability.\nSpecifically, three key factors that cover demonstrations' retrieval,\npresentation, and distribution are comprehensively investigated and optimized.\nA sentimental predictive bias inherent in MLLMs is also discovered and later\neffectively counteracted. By complementing each other, the devised strategies\nfor three factors result in average accuracy improvements of 15.9% on six MSA\ndatasets against the zero-shot paradigm and 11.2% against the random ICL\nbaseline."}
{"id": "2505.16212", "pdf": "https://arxiv.org/pdf/2505.16212.pdf", "abs": "https://arxiv.org/abs/2505.16212", "title": "Large Language Models based ASR Error Correction for Child Conversations", "authors": ["Anfeng Xu", "Tiantian Feng", "So Hyun Kim", "Somer Bishop", "Catherine Lord", "Shrikanth Narayanan"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Automatic Speech Recognition (ASR) has recently shown remarkable progress,\nbut accurately transcribing children's speech remains a significant challenge.\nRecent developments in Large Language Models (LLMs) have shown promise in\nimproving ASR transcriptions. However, their applications in child speech\nincluding conversational scenarios are underexplored. In this study, we explore\nthe use of LLMs in correcting ASR errors for conversational child speech. We\ndemonstrate the promises and challenges of LLMs through experiments on two\nchildren's conversational speech datasets with both zero-shot and fine-tuned\nASR outputs. We find that while LLMs are helpful in correcting zero-shot ASR\noutputs and fine-tuned CTC-based ASR outputs, it remains challenging for LLMs\nto improve ASR performance when incorporating contextual information or when\nusing fine-tuned autoregressive ASR (e.g., Whisper) outputs."}
{"id": "2505.16216", "pdf": "https://arxiv.org/pdf/2505.16216.pdf", "abs": "https://arxiv.org/abs/2505.16216", "title": "Memorization or Reasoning? Exploring the Idiom Understanding of LLMs", "authors": ["Jisu Kim", "Youngwoo Shin", "Uiji Hwang", "Jihun Choi", "Richeng Xuan", "Taeuk Kim"], "categories": ["cs.CL"], "comment": null, "summary": "Idioms have long posed a challenge due to their unique linguistic properties,\nwhich set them apart from other common expressions. While recent studies have\nleveraged large language models (LLMs) to handle idioms across various tasks,\ne.g., idiom-containing sentence generation and idiomatic machine translation,\nlittle is known about the underlying mechanisms of idiom processing in LLMs,\nparticularly in multilingual settings. To this end, we introduce MIDAS, a new\nlarge-scale dataset of idioms in six languages, each paired with its\ncorresponding meaning. Leveraging this resource, we conduct a comprehensive\nevaluation of LLMs' idiom processing ability, identifying key factors that\ninfluence their performance. Our findings suggest that LLMs rely not only on\nmemorization, but also adopt a hybrid approach that integrates contextual cues\nand reasoning, especially when processing compositional idioms. This implies\nthat idiom understanding in LLMs emerges from an interplay between internal\nknowledge retrieval and reasoning-based inference."}
{"id": "2505.16222", "pdf": "https://arxiv.org/pdf/2505.16222.pdf", "abs": "https://arxiv.org/abs/2505.16222", "title": "Don't Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation", "authors": ["Jiwon Moon", "Yerin Hwang", "Dongryeol Lee", "Taegwan Kang", "Yongil Kim", "Kyomin Jung"], "categories": ["cs.CL", "cs.SE"], "comment": "26 pages", "summary": "With the growing use of large language models(LLMs) as evaluators, their\napplication has expanded to code evaluation tasks, where they assess the\ncorrectness of generated code without relying on reference implementations.\nWhile this offers scalability and flexibility, it also raises a critical,\nunresolved question: Can LLM judges fairly and robustly evaluate semantically\nequivalent code with superficial variations? Functionally correct code often\nexhibits variations-such as differences in variable names, comments, or\nformatting-that should not influence its correctness. Yet, whether LLM judges\ncan reliably handle these variations remains unclear. We present the first\ncomprehensive study of this issue, defining six types of potential bias in code\nevaluation and revealing their systematic impact on LLM judges. Across five\nprogramming languages and multiple LLMs, we empirically demonstrate that all\ntested LLM judges are susceptible to both positive and negative biases,\nresulting in inflated or unfairly low scores. Moreover, we observe that LLM\njudges remain vulnerable to these biases even when prompted to generate test\ncases before scoring, highlighting the need for more robust code evaluation\nmethods."}
{"id": "2505.16227", "pdf": "https://arxiv.org/pdf/2505.16227.pdf", "abs": "https://arxiv.org/abs/2505.16227", "title": "Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning", "authors": ["Bohao Wu", "Qingyun Wang", "Yue Guo"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Personalizing jargon detection and explanation is essential for making\ntechnical documents accessible to readers with diverse disciplinary\nbackgrounds. However, tailoring models to individual users typically requires\nsubstantial annotation efforts and computational resources due to user-specific\nfinetuning. To address this, we present a systematic study of personalized\njargon detection, focusing on methods that are both efficient and scalable for\nreal-world deployment. We explore two personalization strategies: (1)\nlightweight fine-tuning using Low-Rank Adaptation (LoRA) on open-source models,\nand (2) personalized prompting, which tailors model behavior at inference time\nwithout retaining. To reflect realistic constraints, we also investigate hybrid\napproaches that combine limited annotated data with unsupervised user\nbackground signals. Our personalized LoRA model outperforms GPT-4 by 21.4% in\nF1 score and exceeds the best performing oracle baseline by 8.3%. Remarkably,\nour method achieves comparable performance using only 10% of the annotated\ntraining data, demonstrating its practicality for resource-constrained\nsettings. Our study offers the first work to systematically explore efficient,\nlow-resource personalization of jargon detection using open-source language\nmodels, offering a practical path toward scalable, user-adaptive NLP system."}
{"id": "2505.16232", "pdf": "https://arxiv.org/pdf/2505.16232.pdf", "abs": "https://arxiv.org/abs/2505.16232", "title": "MuseRAG: Idea Originality Scoring At Scale", "authors": ["Ali Sarosh Bangash", "Krish Veera", "Ishfat Abrar Islam", "Raiyan Abdul Baten"], "categories": ["cs.CL"], "comment": null, "summary": "An objective, face-valid way to assess the originality of creative ideas is\nto measure how rare each idea is within a population -- an approach long used\nin creativity research but difficult to automate at scale. Tabulating response\nfrequencies via manual bucketing of idea rephrasings is labor-intensive,\nerror-prone, and brittle under large corpora. We introduce a fully automated,\npsychometrically validated pipeline for frequency-based originality scoring.\nOur method, MuseRAG, combines large language models (LLMs) with an externally\norchestrated retrieval-augmented generation (RAG) framework. Given a new idea,\nthe system retrieves semantically similar prior idea buckets and zero-shot\nprompts the LLM to judge whether the new idea belongs to an existing bucket or\nforms a new one. The resulting buckets enable computation of frequency-based\noriginality metrics. Across five datasets (N=1143, n_ideas=16294), MuseRAG\nmatches human annotators in idea clustering structure and resolution (AMI =\n0.59) and in participant-level scoring (r = 0.89) -- while exhibiting strong\nconvergent and external validity. Our work enables intent-sensitive,\nhuman-aligned originality scoring at scale to aid creativity research."}
{"id": "2505.16234", "pdf": "https://arxiv.org/pdf/2505.16234.pdf", "abs": "https://arxiv.org/abs/2505.16234", "title": "LIFEBench: Evaluating Length Instruction Following in Large Language Models", "authors": ["Wei Zhang", "Zhenhong Zhou", "Junfeng Fang", "Rongwu Xu", "Kun Wang", "Yuanhe Zhang", "Rui Wang", "Ge Zhang", "Xinfeng Li", "Li Sun", "Lingjuan Lyu", "Yang Liu", "Sen Su"], "categories": ["cs.CL", "cs.AI"], "comment": "81 pages, 22 tables, 32 figures. Homepage:\n  https://ydyjya.github.io/LIFEBench/", "summary": "While large language models (LLMs) can solve PhD-level reasoning problems\nover long context inputs, they still struggle with a seemingly simpler task:\nfollowing explicit length instructions-e.g., write a 10,000-word novel.\nAdditionally, models often generate far too short outputs, terminate\nprematurely, or even refuse the request. Existing benchmarks focus primarily on\nevaluating generations quality, but often overlook whether the generations meet\nlength constraints. To this end, we introduce Length Instruction Following\nEvaluation Benchmark (LIFEBench) to comprehensively evaluate LLMs' ability to\nfollow length instructions across diverse tasks and a wide range of specified\nlengths. LIFEBench consists of 10,800 instances across 4 task categories in\nboth English and Chinese, covering length constraints ranging from 16 to 8192\nwords. We evaluate 26 widely-used LLMs and find that most models reasonably\nfollow short-length instructions but deteriorate sharply beyond a certain\nthreshold. Surprisingly, almost all models fail to reach the vendor-claimed\nmaximum output lengths in practice, as further confirmed by our evaluations\nextending up to 32K words. Even long-context LLMs, despite their extended\ninput-output windows, counterintuitively fail to improve length-instructions\nfollowing. Notably, Reasoning LLMs outperform even specialized long-text\ngeneration models, achieving state-of-the-art length following. Overall,\nLIFEBench uncovers fundamental limitations in current LLMs' length instructions\nfollowing ability, offering critical insights for future progress."}
{"id": "2505.16237", "pdf": "https://arxiv.org/pdf/2505.16237.pdf", "abs": "https://arxiv.org/abs/2505.16237", "title": "Align-GRAG: Reasoning-Guided Dual Alignment for Graph Retrieval-Augmented Generation", "authors": ["Derong Xu", "Pengyue Jia", "Xiaopeng Li", "Yingyi Zhang", "Maolin Wang", "Qidong Liu", "Xiangyu Zhao", "Yichao Wang", "Huifeng Guo", "Ruiming Tang", "Enhong Chen", "Tong Xu"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, but\nstill struggle with issues like hallucinations and outdated information.\nRetrieval-augmented generation (RAG) addresses these issues by grounding LLM\noutputs in external knowledge with an Information Retrieval (IR) system.\nBuilding on this foundation, graph-based RAG systems go a step further by\nretrieving subgraphs, which preserve the relationships between knowledge\nentities and provide more comprehensive context. However, graph RAG faces two\nchallenges: (1) Retrieving relevant information introduces irrelevant nodes\n(especially in dense graph databases, where retrieval usually extends to\nadjacent nodes), and leads to overly lengthy inputs that hinder efficiency; (2)\nThe representation gap between graph and language during generation with LLMs\nlimits the ability to fully leverage graph structures for enhanced\nunderstanding. To address these limitations, we propose Align-GRAG, a novel\nreasoning-guided dual alignment framework in post-retrieval phrase. It first\nformulates a subgraph by retrieving nodes and edges. Then an Aligner is\nproposed to jointly optimizes a graph encoder with LLM-summarized reasoning. It\nachieves dual alignment of graph node and representation by leveraging KL\ndivergence loss and contrastive loss, facilitating efficient pruning of\nirrelevant knowledge and establishing a unified semantic space. The Generator\nintegrates the aligned graph data with LLM to produce coherent and accurate\nanswers. Experiments on GraphQA benchmark across three tasks (including common\nsense reasoning, scene graph understanding, and knowledge graph reasoning)\nvalidate the effectiveness of our method. The code will be available upon\naccepted."}
{"id": "2505.16241", "pdf": "https://arxiv.org/pdf/2505.16241.pdf", "abs": "https://arxiv.org/abs/2505.16241", "title": "Three Minds, One Legend: Jailbreak Large Reasoning Model with Adaptive Stacked Ciphers", "authors": ["Viet-Anh Nguyen", "Shiqian Zhao", "Gia Dao", "Runyi Hu", "Yi Xie", "Luu Anh Tuan"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, Large Reasoning Models (LRMs) have demonstrated superior logical\ncapabilities compared to traditional Large Language Models (LLMs), gaining\nsignificant attention. Despite their impressive performance, the potential for\nstronger reasoning abilities to introduce more severe security vulnerabilities\nremains largely underexplored. Existing jailbreak methods often struggle to\nbalance effectiveness with robustness against adaptive safety mechanisms. In\nthis work, we propose SEAL, a novel jailbreak attack that targets LRMs through\nan adaptive encryption pipeline designed to override their reasoning processes\nand evade potential adaptive alignment. Specifically, SEAL introduces a stacked\nencryption approach that combines multiple ciphers to overwhelm the models\nreasoning capabilities, effectively bypassing built-in safety mechanisms. To\nfurther prevent LRMs from developing countermeasures, we incorporate two\ndynamic strategies - random and adaptive - that adjust the cipher length,\norder, and combination. Extensive experiments on real-world reasoning models,\nincluding DeepSeek-R1, Claude Sonnet, and OpenAI GPT-o4, validate the\neffectiveness of our approach. Notably, SEAL achieves an attack success rate of\n80.8% on GPT o4-mini, outperforming state-of-the-art baselines by a significant\nmargin of 27.2%. Warning: This paper contains examples of inappropriate,\noffensive, and harmful content."}
{"id": "2505.16245", "pdf": "https://arxiv.org/pdf/2505.16245.pdf", "abs": "https://arxiv.org/abs/2505.16245", "title": "Diverse, not Short: A Length-Controlled Self-Learning Framework for Improving Response Diversity of Language Models", "authors": ["Vijeta Deshpande", "Debasmita Ghose", "John D. Patterson", "Roger Beaty", "Anna Rumshisky"], "categories": ["cs.CL"], "comment": null, "summary": "Diverse language model responses are crucial for creative generation,\nopen-ended tasks, and self-improvement training. We show that common diversity\nmetrics, and even reward models used for preference optimization,\nsystematically bias models toward shorter outputs, limiting expressiveness. To\naddress this, we introduce Diverse, not Short (Diverse-NS), a length-controlled\nself-learning framework that improves response diversity while maintaining\nlength parity. By generating and filtering preference data that balances\ndiversity, quality, and length, Diverse-NS enables effective training using\nonly 3,000 preference pairs. Applied to LLaMA-3.1-8B and the Olmo-2 family,\nDiverse-NS substantially enhances lexical and semantic diversity. We show\nconsistent improvement in diversity with minor reduction or gains in response\nquality on four creative generation tasks: Divergent Associations, Persona\nGeneration, Alternate Uses, and Creative Writing. Surprisingly, experiments\nwith the Olmo-2 model family (7B, and 13B) show that smaller models like\nOlmo-2-7B can serve as effective \"diversity teachers\" for larger models. By\nexplicitly addressing length bias, our method efficiently pushes models toward\nmore diverse and expressive outputs."}
{"id": "2505.16252", "pdf": "https://arxiv.org/pdf/2505.16252.pdf", "abs": "https://arxiv.org/abs/2505.16252", "title": "Does Localization Inform Unlearning? A Rigorous Examination of Local Parameter Attribution for Knowledge Unlearning in Language Models", "authors": ["Hwiyeong Lee", "Uiji Hwang", "Hyelim Lim", "Taeuk Kim"], "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Large language models often retain unintended content, prompting growing\ninterest in knowledge unlearning. Recent approaches emphasize localized\nunlearning, which restricts parameter updates to specific regions in an effort\nto remove target knowledge while preserving unrelated general knowledge.\nHowever, their effectiveness remains uncertain due to the lack of robust and\nthorough evaluation of the trade-off between the competing goals of unlearning.\nIn this paper, we begin by revisiting existing localized unlearning approaches.\nWe then conduct controlled experiments to rigorously evaluate whether local\nparameter updates causally contribute to unlearning. Our findings reveal that\nthe set of parameters that must be modified for effective unlearning is not\nstrictly determined, challenging the core assumption of localized unlearning\nthat parameter locality is inherently indicative of effective knowledge\nremoval."}
{"id": "2505.16258", "pdf": "https://arxiv.org/pdf/2505.16258.pdf", "abs": "https://arxiv.org/abs/2505.16258", "title": "IRONIC: Coherence-Aware Reasoning Chains for Multi-Modal Sarcasm Detection", "authors": ["Aashish Anantha Ramakrishnan", "Aadarsh Anantha Ramakrishnan", "Dongwon Lee"], "categories": ["cs.CL", "cs.AI", "cs.CV", "68T50", "I.2.7; I.2.10"], "comment": null, "summary": "Interpreting figurative language such as sarcasm across multi-modal inputs\npresents unique challenges, often requiring task-specific fine-tuning and\nextensive reasoning steps. However, current Chain-of-Thought approaches do not\nefficiently leverage the same cognitive processes that enable humans to\nidentify sarcasm. We present IRONIC, an in-context learning framework that\nleverages Multi-modal Coherence Relations to analyze referential, analogical\nand pragmatic image-text linkages. Our experiments show that IRONIC achieves\nstate-of-the-art performance on zero-shot Multi-modal Sarcasm Detection across\ndifferent baselines. This demonstrates the need for incorporating linguistic\nand cognitive insights into the design of multi-modal reasoning strategies. Our\ncode is available at: https://github.com/aashish2000/IRONIC"}
{"id": "2505.16270", "pdf": "https://arxiv.org/pdf/2505.16270.pdf", "abs": "https://arxiv.org/abs/2505.16270", "title": "Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning", "authors": ["Jiaru Zou", "Yikun Ban", "Zihao Li", "Yunzhe Qi", "Ruizhong Qiu", "Ling Yang", "Jingrui He"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "33 pages, 7 figures", "summary": "Large language models are typically adapted to downstream tasks through\nsupervised fine-tuning on domain-specific data. While standard fine-tuning\nfocuses on minimizing generation loss to optimize model parameters, we take a\ndeeper step by retaining and leveraging the model's own learning signals,\nanalogous to how human learners reflect on past mistakes to improve future\nperformance. We first introduce the concept of Mistake Log to systematically\ntrack the model's learning behavior and recurring errors throughout\nfine-tuning. Treating the original transformer-based model as the Pilot, we\ncorrespondingly design a Copilot model to refine the Pilot's inference\nperformance via logits rectification. We name the overall Pilot-Copilot\nframework the Transformer Copilot, which introduces (i) a novel Copilot model\ndesign, (ii) a joint training paradigm where the Copilot continuously learns\nfrom the evolving Mistake Log alongside the Pilot, and (iii) a fused inference\nparadigm where the Copilot rectifies the Pilot's logits for enhanced\ngeneration. We provide both theoretical and empirical analyses on our new\nlearning framework. Experiments on 12 benchmarks spanning commonsense,\narithmetic, and recommendation tasks demonstrate that Transformer Copilot\nconsistently improves performance by up to 34.5%, while introducing marginal\ncomputational overhead to Pilot models and exhibiting strong scalability and\ntransferability."}
{"id": "2505.16277", "pdf": "https://arxiv.org/pdf/2505.16277.pdf", "abs": "https://arxiv.org/abs/2505.16277", "title": "Spontaneous Speech Variables for Evaluating LLMs Cognitive Plausibility", "authors": ["Sheng-Fu Wang", "Laurent Prevot", "Jou-an Chi", "Ri-Sheng Huang", "Shu-Kai Hsieh"], "categories": ["cs.CL"], "comment": "The 14th Workshop on Cognitive Modeling and Computational Linguistics\n  (CMCL). May 3, 2025. Collocated with NAACL 2025", "summary": "The achievements of Large Language Models in Natural Language Processing,\nespecially for high-resource languages, call for a better understanding of\ntheir characteristics from a cognitive perspective. Researchers have attempted\nto evaluate artificial models by testing their ability to predict behavioral\n(e.g., eye-tracking fixations) and physiological (e.g., brain responses)\nvariables during language processing (e.g., reading/listening). In this paper,\nwe propose using spontaneous speech corpora to derive production variables\n(speech reductions, prosodic prominences) and applying them in a similar\nfashion. More precisely, we extract. We then test models trained with a\nstandard procedure on different pretraining datasets (written, spoken, and\nmixed genres) for their ability to predict these two variables. Our results\nshow that, after some fine-tuning, the models can predict these production\nvariables well above baselines. We also observe that spoken genre training data\nprovides more accurate predictions than written genres. These results\ncontribute to the broader effort of using high-quality speech corpora as\nbenchmarks for LLMs."}
{"id": "2505.16281", "pdf": "https://arxiv.org/pdf/2505.16281.pdf", "abs": "https://arxiv.org/abs/2505.16281", "title": "HiMATE: A Hierarchical Multi-Agent Framework for Machine Translation Evaluation", "authors": ["Shijie Zhang", "Renhao Li", "Songsheng Wang", "Philipp Koehn", "Min Yang", "Derek F. Wong"], "categories": ["cs.CL"], "comment": null, "summary": "The advancement of Large Language Models (LLMs) enables flexible and\ninterpretable automatic evaluations. In the field of machine translation\nevaluation, utilizing LLMs with translation error annotations based on\nMultidimensional Quality Metrics (MQM) yields more human-aligned judgments.\nHowever, current LLM-based evaluation methods still face challenges in\naccurately identifying error spans and assessing their severity. In this paper,\nwe propose HiMATE, a Hierarchical Multi-Agent Framework for Machine Translation\nEvaluation. We argue that existing approaches inadequately exploit the\nfine-grained structural and semantic information within the MQM hierarchy. To\naddress this, we develop a hierarchical multi-agent system grounded in the MQM\nerror typology, enabling granular evaluation of subtype errors. Two key\nstrategies are incorporated to further mitigate systemic hallucinations within\nthe framework: the utilization of the model's self-reflection capability and\nthe facilitation of agent discussion involving asymmetric information.\nEmpirically, HiMATE outperforms competitive baselines across different datasets\nin conducting human-aligned evaluations. Further analyses underscore its\nsignificant advantage in error span detection and severity assessment,\nachieving an average F1-score improvement of 89% over the best-performing\nbaseline. We make our code and data publicly available at\nhttps://anonymous.4open.science/r/HiMATE-Anony."}
{"id": "2505.16293", "pdf": "https://arxiv.org/pdf/2505.16293.pdf", "abs": "https://arxiv.org/abs/2505.16293", "title": "Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA", "authors": ["Rishabh Maheshwary", "Masoud Hashemi", "Khyati Mahajan", "Shiva Krishna Reddy Malay", "Sai Rajeswar", "Sathwik Tejaswi Madhusudhan", "Spandana Gella", "Vikas Yadav"], "categories": ["cs.CL"], "comment": null, "summary": "Iterative RAG for multi-hop question answering faces challenges with lengthy\ncontexts and the buildup of irrelevant information. This hinders a model's\ncapacity to process and reason over retrieved content and limits performance.\nWhile recent methods focus on compressing retrieved information, they are\neither restricted to single-round RAG, require finetuning or lack scalability\nin iterative RAG. To address these challenges, we propose Notes Writing, a\nmethod that generates concise and relevant notes from retrieved documents at\neach step, thereby reducing noise and retaining only essential information.\nThis indirectly increases the effective context length of Large Language Models\n(LLMs), enabling them to reason and plan more effectively while processing\nlarger volumes of input text. Notes Writing is framework agnostic and can be\nintegrated with different iterative RAG methods. We demonstrate its\neffectiveness with three iterative RAG methods, across two models and four\nevaluation datasets. Notes writing yields an average improvement of 15.6\npercentage points overall, with minimal increase in output tokens."}
{"id": "2505.16297", "pdf": "https://arxiv.org/pdf/2505.16297.pdf", "abs": "https://arxiv.org/abs/2505.16297", "title": "ToDi: Token-wise Distillation via Fine-Grained Divergence Control", "authors": ["Seongryong Jung", "Suwan Yoon", "DongGeon Kim", "Hwanhee Lee"], "categories": ["cs.CL"], "comment": "13 pages, 7 figures", "summary": "Large language models (LLMs) offer impressive performance but are impractical\nfor resource-constrained deployment due to high latency and energy consumption.\nKnowledge distillation (KD) addresses this by transferring knowledge from a\nlarge teacher to a smaller student model. However, conventional KD, notably\napproaches like Forward KL (FKL) and Reverse KL (RKL), apply uniform divergence\nloss across the entire vocabulary, neglecting token-level prediction\ndiscrepancies. By investigating these representative divergences via gradient\nanalysis, we reveal that FKL boosts underestimated tokens, while RKL suppresses\noverestimated ones, showing their complementary roles. Based on this\nobservation, we propose Token-wise Distillation (ToDi), a novel method that\nadaptively combines FKL and RKL per token using a sigmoid-based weighting\nfunction derived from the teacher-student probability log-ratio. ToDi\ndynamically emphasizes the appropriate divergence for each token, enabling\nprecise distribution alignment. We demonstrate that ToDi consistently\noutperforms recent distillation baselines using uniform or less granular\nstrategies across instruction-following benchmarks. Extensive ablation studies\nand efficiency analysis further validate ToDi's effectiveness and practicality."}
{"id": "2505.16303", "pdf": "https://arxiv.org/pdf/2505.16303.pdf", "abs": "https://arxiv.org/abs/2505.16303", "title": "INFERENCEDYNAMICS: Efficient Routing Across LLMs through Structured Capability and Knowledge Profiling", "authors": ["Haochen Shi", "Tianshi Zheng", "Weiqi Wang", "Baixuan Xu", "Chunyang Li", "Chunkit Chan", "Tao Fan", "Yangqiu Song", "Qiang Yang"], "categories": ["cs.CL"], "comment": "17 pages", "summary": "Large Language Model (LLM) routing is a pivotal technique for navigating a\ndiverse landscape of LLMs, aiming to select the best-performing LLMs tailored\nto the domains of user queries, while managing computational resources.\nHowever, current routing approaches often face limitations in scalability when\ndealing with a large pool of specialized LLMs, or in their adaptability to\nextending model scope and evolving capability domains. To overcome those\nchallenges, we propose InferenceDynamics, a flexible and scalable\nmulti-dimensional routing framework by modeling the capability and knowledge of\nmodels. We operate it on our comprehensive dataset RouteMix, and demonstrate\nits effectiveness and generalizability in group-level routing using modern\nbenchmarks including MMLU-Pro, GPQA, BigGenBench, and LiveBench, showcasing its\nability to identify and leverage top-performing models for given tasks, leading\nto superior outcomes with efficient resource utilization. The broader adoption\nof Inference Dynamics can empower users to harness the full specialized\npotential of the LLM ecosystem, and our code will be made publicly available to\nencourage further research."}
{"id": "2505.16307", "pdf": "https://arxiv.org/pdf/2505.16307.pdf", "abs": "https://arxiv.org/abs/2505.16307", "title": "PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models", "authors": ["Chenzhuo Zhao", "Ziqian Liu", "Xingda Wang", "Junting Lu", "Chaoyi Ruan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Prompt optimization offers a practical and broadly applicable alternative to\nfine-tuning for improving large language model (LLM) performance. However,\nexisting methods often rely on costly output generation, self-critiquing\nabilities, or human-annotated preferences, which limit their scalability,\nespecially for smaller or non-instruction-tuned models. We introduce PMPO\n(Probabilistic Metric Prompt Optimization), a unified framework that refines\nprompts using token-level cross-entropy loss as a direct, lightweight\nevaluation signal. PMPO identifies low-quality prompt segments by masking and\nmeasuring their impact on loss, then rewrites and selects improved variants by\nminimizing loss over positive and negative examples. Unlike prior methods, it\nrequires no output sampling or human evaluation during optimization, relying\nonly on forward passes and log-likelihoods. PMPO supports both supervised and\npreference-based tasks through a closely aligned loss-based evaluation\nstrategy. Experiments show that PMPO consistently outperforms prior methods\nacross model sizes and tasks: it achieves the highest average accuracy on BBH,\nperforms strongly on GSM8K and AQUA-RAT, and improves AlpacaEval 2.0 win rates\nby over 19 points. These results highlight PMPO's effectiveness, efficiency,\nand broad applicability."}
{"id": "2505.16325", "pdf": "https://arxiv.org/pdf/2505.16325.pdf", "abs": "https://arxiv.org/abs/2505.16325", "title": "CLEAR: A Clinically-Grounded Tabular Framework for Radiology Report Evaluation", "authors": ["Yuyang Jiang", "Chacha Chen", "Shengyuan Wang", "Feng Li", "Zecong Tang", "Benjamin M. Mervak", "Lydia Chelala", "Christopher M Straus", "Reve Chahine", "Samuel G. Armato III", "Chenhao Tan"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "18 pages, 4 figures", "summary": "Existing metrics often lack the granularity and interpretability to capture\nnuanced clinical differences between candidate and ground-truth radiology\nreports, resulting in suboptimal evaluation. We introduce a Clinically-grounded\ntabular framework with Expert-curated labels and Attribute-level comparison for\nRadiology report evaluation (CLEAR). CLEAR not only examines whether a report\ncan accurately identify the presence or absence of medical conditions, but also\nassesses whether it can precisely describe each positively identified condition\nacross five key attributes: first occurrence, change, severity, descriptive\nlocation, and recommendation. Compared to prior works, CLEAR's\nmulti-dimensional, attribute-level outputs enable a more comprehensive and\nclinically interpretable evaluation of report quality. Additionally, to measure\nthe clinical alignment of CLEAR, we collaborate with five board-certified\nradiologists to develop CLEAR-Bench, a dataset of 100 chest X-ray reports from\nMIMIC-CXR, annotated across 6 curated attributes and 13 CheXpert conditions.\nOur experiments show that CLEAR achieves high accuracy in extracting clinical\nattributes and provides automated metrics that are strongly aligned with\nclinical judgment."}
{"id": "2505.16330", "pdf": "https://arxiv.org/pdf/2505.16330.pdf", "abs": "https://arxiv.org/abs/2505.16330", "title": "SC4ANM: Identifying Optimal Section Combinations for Automated Novelty Prediction in Academic Papers", "authors": ["Wenqing Wu", "Chengzhi Zhang", "Tong Bao", "Yi Zhao"], "categories": ["cs.CL", "cs.AI", "cs.DL"], "comment": null, "summary": "Novelty is a core component of academic papers, and there are multiple\nperspectives on the assessment of novelty. Existing methods often focus on word\nor entity combinations, which provide limited insights. The content related to\na paper's novelty is typically distributed across different core sections,\ne.g., Introduction, Methodology and Results. Therefore, exploring the optimal\ncombination of sections for evaluating the novelty of a paper is important for\nadvancing automated novelty assessment. In this paper, we utilize different\ncombinations of sections from academic papers as inputs to drive language\nmodels to predict novelty scores. We then analyze the results to determine the\noptimal section combinations for novelty score prediction. We first employ\nnatural language processing techniques to identify the sectional structure of\nacademic papers, categorizing them into introduction, methods, results, and\ndiscussion (IMRaD). Subsequently, we used different combinations of these\nsections (e.g., introduction and methods) as inputs for pretrained language\nmodels (PLMs) and large language models (LLMs), employing novelty scores\nprovided by human expert reviewers as ground truth labels to obtain prediction\nresults. The results indicate that using introduction, results and discussion\nis most appropriate for assessing the novelty of a paper, while the use of the\nentire text does not yield significant results. Furthermore, based on the\nresults of the PLMs and LLMs, the introduction and results appear to be the\nmost important section for the task of novelty score prediction. The code and\ndataset for this paper can be accessed at\nhttps://github.com/njust-winchy/SC4ANM."}
{"id": "2505.16348", "pdf": "https://arxiv.org/pdf/2505.16348.pdf", "abs": "https://arxiv.org/abs/2505.16348", "title": "Embodied Agents Meet Personalization: Exploring Memory Utilization for Personalized Assistance", "authors": ["Taeyoon Kwon", "Dongwook Choi", "Sunghwan Kim", "Hyojun Kim", "Seungjun Moon", "Beong-woo Kwak", "Kuan-Hao Huang", "Jinyoung Yeo"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Embodied agents empowered by large language models (LLMs) have shown strong\nperformance in household object rearrangement tasks. However, these tasks\nprimarily focus on single-turn interactions with simplified instructions, which\ndo not truly reflect the challenges of providing meaningful assistance to\nusers. To provide personalized assistance, embodied agents must understand the\nunique semantics that users assign to the physical world (e.g., favorite cup,\nbreakfast routine) by leveraging prior interaction history to interpret\ndynamic, real-world instructions. Yet, the effectiveness of embodied agents in\nutilizing memory for personalized assistance remains largely underexplored. To\naddress this gap, we present MEMENTO, a personalized embodied agent evaluation\nframework designed to comprehensively assess memory utilization capabilities to\nprovide personalized assistance. Our framework consists of a two-stage memory\nevaluation process design that enables quantifying the impact of memory\nutilization on task performance. This process enables the evaluation of agents'\nunderstanding of personalized knowledge in object rearrangement tasks by\nfocusing on its role in goal interpretation: (1) the ability to identify target\nobjects based on personal meaning (object semantics), and (2) the ability to\ninfer object-location configurations from consistent user patterns, such as\nroutines (user patterns). Our experiments across various LLMs reveal\nsignificant limitations in memory utilization, with even frontier models like\nGPT-4o experiencing a 30.5% performance drop when required to reference\nmultiple memories, particularly in tasks involving user patterns. These\nfindings, along with our detailed analyses and case studies, provide valuable\ninsights for future research in developing more effective personalized embodied\nagents. Project website: https://connoriginal.github.io/MEMENTO"}
{"id": "2505.16349", "pdf": "https://arxiv.org/pdf/2505.16349.pdf", "abs": "https://arxiv.org/abs/2505.16349", "title": "Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization", "authors": ["Pierre Achkar", "Tim Gollub", "Martin Potthast"], "categories": ["cs.CL"], "comment": "Accepted at SCOLIA@ECIR 2025 Workshop", "summary": "The exponential growth of scientific publications has made it increasingly\ndifficult for researchers to stay updated and synthesize knowledge effectively.\nThis paper presents XSum, a modular pipeline for multi-document summarization\n(MDS) in the scientific domain using Retrieval-Augmented Generation (RAG). The\npipeline includes two core components: a question-generation module and an\neditor module. The question-generation module dynamically generates questions\nadapted to the input papers, ensuring the retrieval of relevant and accurate\ninformation. The editor module synthesizes the retrieved content into coherent\nand well-structured summaries that adhere to academic standards for proper\ncitation. Evaluated on the SurveySum dataset, XSum demonstrates strong\nperformance, achieving considerable improvements in metrics such as CheckEval,\nG-Eval and Ref-F1 compared to existing approaches. This work provides a\ntransparent, adaptable framework for scientific summarization with potential\napplications in a wide range of domains. Code available at\nhttps://github.com/webis-de/scolia25-xsum"}
{"id": "2505.16381", "pdf": "https://arxiv.org/pdf/2505.16381.pdf", "abs": "https://arxiv.org/abs/2505.16381", "title": "PaTH Attention: Position Encoding via Accumulating Householder Transformations", "authors": ["Songlin Yang", "Yikang Shen", "Kaiyue Wen", "Shawn Tan", "Mayank Mishra", "Liliang Ren", "Rameswar Panda", "Yoon Kim"], "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "The attention mechanism is a core primitive in modern large language models\n(LLMs) and AI more broadly. Since attention by itself is permutation-invariant,\nposition encoding is essential for modeling structured domains such as\nlanguage. Rotary position encoding (RoPE) has emerged as the de facto standard\napproach for position encoding and is part of many modern LLMs. However, in\nRoPE the key/query transformation between two elements in a sequence is only a\nfunction of their relative position and otherwise independent of the actual\ninput. This limits the expressivity of RoPE-based transformers.\n  This paper describes PaTH, a flexible data-dependent position encoding scheme\nbased on accumulated products of Householder(like) transformations, where each\ntransformation is data-dependent, i.e., a function of the input. We derive an\nefficient parallel algorithm for training through exploiting a compact\nrepresentation of products of Householder matrices, and implement a\nFlashAttention-style blockwise algorithm that minimizes I/O cost. Across both\ntargeted synthetic benchmarks and moderate-scale real-world language modeling\nexperiments, we find that PaTH demonstrates superior performance compared to\nRoPE and other recent baselines."}
{"id": "2505.16385", "pdf": "https://arxiv.org/pdf/2505.16385.pdf", "abs": "https://arxiv.org/abs/2505.16385", "title": "Semantic Pivots Enable Cross-Lingual Transfer in Large Language Models", "authors": ["Kaiyu He", "Tong Zhou", "Yubo Chen", "Delai Qiu", "Shengping Liu", "Kang Liu", "Jun Zhao"], "categories": ["cs.CL"], "comment": "14 pages, 10 figures", "summary": "Large language models (LLMs) demonstrate remarkable ability in cross-lingual\ntasks. Understanding how LLMs acquire this ability is crucial for their\ninterpretability. To quantify the cross-lingual ability of LLMs accurately, we\npropose a Word-Level Cross-Lingual Translation Task. To find how LLMs learn\ncross-lingual ability, we trace the outputs of LLMs' intermediate layers in the\nword translation task. We identify and distinguish two distinct behaviors in\nthe forward pass of LLMs: co-occurrence behavior and semantic pivot behavior.\nWe attribute LLMs' two distinct behaviors to the co-occurrence frequency of\nwords and find the semantic pivot from the pre-training dataset. Finally, to\napply our findings to improve the cross-lingual ability of LLMs, we reconstruct\na semantic pivot-aware pre-training dataset using documents with a high\nproportion of semantic pivots. Our experiments validate the effectiveness of\nour approach in enhancing cross-lingual ability. Our research contributes\ninsights into the interpretability of LLMs and offers a method for improving\nLLMs' cross-lingual ability."}
{"id": "2505.16392", "pdf": "https://arxiv.org/pdf/2505.16392.pdf", "abs": "https://arxiv.org/abs/2505.16392", "title": "Resource for Error Analysis in Text Simplification: New Taxonomy and Test Collection", "authors": ["Benjamin Vendeville", "Liana Ermakova", "Pierre De Loor"], "categories": ["cs.CL", "cs.AI", "I.2.6; I.5.2"], "comment": "Accepted at SIGIR 2025", "summary": "The general public often encounters complex texts but does not have the time\nor expertise to fully understand them, leading to the spread of misinformation.\nAutomatic Text Simplification (ATS) helps make information more accessible, but\nits evaluation methods have not kept up with advances in text generation,\nespecially with Large Language Models (LLMs). In particular, recent studies\nhave shown that current ATS metrics do not correlate with the presence of\nerrors. Manual inspections have further revealed a variety of errors,\nunderscoring the need for a more nuanced evaluation framework, which is\ncurrently lacking. This resource paper addresses this gap by introducing a test\ncollection for detecting and classifying errors in simplified texts. First, we\npropose a taxonomy of errors, with a formal focus on information distortion.\nNext, we introduce a parallel dataset of automatically simplified scientific\ntexts. This dataset has been human-annotated with labels based on our proposed\ntaxonomy. Finally, we analyze the quality of the dataset, and we study the\nperformance of existing models to detect and classify errors from that\ntaxonomy. These contributions give researchers the tools to better evaluate\nerrors in ATS, develop more reliable models, and ultimately improve the quality\nof automatically simplified texts."}
{"id": "2505.16406", "pdf": "https://arxiv.org/pdf/2505.16406.pdf", "abs": "https://arxiv.org/abs/2505.16406", "title": "On the reliability of feature attribution methods for speech classification", "authors": ["Gaofei Shen", "Hosein Mohebbi", "Arianna Bisazza", "Afra Alishahi", "Grzegorz Chrupała"], "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "As the capabilities of large-scale pre-trained models evolve, understanding\nthe determinants of their outputs becomes more important. Feature attribution\naims to reveal which parts of the input elements contribute the most to model\noutputs. In speech processing, the unique characteristics of the input signal\nmake the application of feature attribution methods challenging. We study how\nfactors such as input type and aggregation and perturbation timespan impact the\nreliability of standard feature attribution methods, and how these factors\ninteract with characteristics of each classification task. We find that\nstandard approaches to feature attribution are generally unreliable when\napplied to the speech domain, with the exception of word-aligned perturbation\nmethods when applied to word-based classification tasks."}
{"id": "2505.16408", "pdf": "https://arxiv.org/pdf/2505.16408.pdf", "abs": "https://arxiv.org/abs/2505.16408", "title": "From Surveys to Narratives: Rethinking Cultural Value Adaptation in LLMs", "authors": ["Muhammad Farid Adilazuarda", "Chen Cecilia Liu", "Iryna Gurevych", "Alham Fikri Aji"], "categories": ["cs.CL"], "comment": null, "summary": "Adapting cultural values in Large Language Models (LLMs) presents significant\nchallenges, particularly due to biases and limited training data. Prior work\nprimarily aligns LLMs with different cultural values using World Values Survey\n(WVS) data. However, it remains unclear whether this approach effectively\ncaptures cultural nuances or produces distinct cultural representations for\nvarious downstream tasks. In this paper, we systematically investigate\nWVS-based training for cultural value adaptation and find that relying solely\non survey data can homogenize cultural norms and interfere with factual\nknowledge. To investigate these issues, we augment WVS with encyclopedic and\nscenario-based cultural narratives from Wikipedia and NormAd. While these\nnarratives may have variable effects on downstream tasks, they consistently\nimprove cultural distinctiveness than survey data alone. Our work highlights\nthe inherent complexity of aligning cultural values with the goal of guiding\ntask-specific behavior."}
{"id": "2505.16410", "pdf": "https://arxiv.org/pdf/2505.16410.pdf", "abs": "https://arxiv.org/abs/2505.16410", "title": "Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning", "authors": ["Guanting Dong", "Yifei Chen", "Xiaoxi Li", "Jiajie Jin", "Hongjin Qian", "Yutao Zhu", "Hangyu Mao", "Guorui Zhou", "Zhicheng Dou", "Ji-Rong Wen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Working in progress", "summary": "Recently, large language models (LLMs) have shown remarkable reasoning\ncapabilities via large-scale reinforcement learning (RL). However, leveraging\nthe RL algorithm to empower effective multi-tool collaborative reasoning in\nLLMs remains an open challenge. In this paper, we introduce Tool-Star, an\nRL-based framework designed to empower LLMs to autonomously invoke multiple\nexternal tools during stepwise reasoning. Tool-Star integrates six types of\ntools and incorporates systematic designs in both data synthesis and training.\nTo address the scarcity of tool-use data, we propose a general tool-integrated\nreasoning data synthesis pipeline, which combines tool-integrated prompting\nwith hint-based sampling to automatically and scalably generate tool-use\ntrajectories. A subsequent quality normalization and difficulty-aware\nclassification process filters out low-quality samples and organizes the\ndataset from easy to hard. Furthermore, we propose a two-stage training\nframework to enhance multi-tool collaborative reasoning by: (1) cold-start\nfine-tuning, which guides LLMs to explore reasoning patterns via\ntool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with\nhierarchical reward design, which reinforces reward understanding and promotes\neffective tool collaboration. Experimental analyses on over 10 challenging\nreasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.\nThe code is available at https://github.com/dongguanting/Tool-Star."}
{"id": "2505.16415", "pdf": "https://arxiv.org/pdf/2505.16415.pdf", "abs": "https://arxiv.org/abs/2505.16415", "title": "Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation", "authors": ["Ruizhe Li", "Chen Chen", "Yuchen Hu", "Yanjun Gao", "Xi Wang", "Emine Yilmaz"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in process", "summary": "Retrieval-Augmented Generation (RAG) leverages large language models (LLMs)\ncombined with external contexts to enhance the accuracy and reliability of\ngenerated responses. However, reliably attributing generated content to\nspecific context segments, context attribution, remains challenging due to the\ncomputationally intensive nature of current methods, which often require\nextensive fine-tuning or human annotation. In this work, we introduce a novel\nJensen-Shannon Divergence driven method to Attribute Response to Context\n(ARC-JSD), enabling efficient and accurate identification of essential context\nsentences without additional fine-tuning or surrogate modelling. Evaluations on\na wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using\ninstruction-tuned LLMs in different scales demonstrate superior accuracy and\nsignificant computational efficiency improvements compared to the previous\nsurrogate-based method. Furthermore, our mechanistic analysis reveals specific\nattention heads and multilayer perceptron (MLP) layers responsible for context\nattribution, providing valuable insights into the internal workings of RAG\nmodels."}
{"id": "2505.16418", "pdf": "https://arxiv.org/pdf/2505.16418.pdf", "abs": "https://arxiv.org/abs/2505.16418", "title": "Exploring the Relationship Between Diversity and Quality in Ad Text Generation", "authors": ["Yoichi Aoki", "Soichiro Murakami", "Ukyo Honda", "Akihiko Kato"], "categories": ["cs.CL"], "comment": null, "summary": "In natural language generation for advertising, creating diverse and engaging\nad texts is crucial for capturing a broad audience and avoiding advertising\nfatigue. Regardless of the importance of diversity, the impact of the\ndiversity-enhancing methods in ad text generation -- mainly tested on tasks\nsuch as summarization and machine translation -- has not been thoroughly\nexplored. Ad text generation significantly differs from these tasks owing to\nthe text style and requirements. This research explores the relationship\nbetween diversity and ad quality in ad text generation by considering multiple\nfactors, such as diversity-enhancing methods, their hyperparameters,\ninput-output formats, and the models."}
{"id": "2505.16421", "pdf": "https://arxiv.org/pdf/2505.16421.pdf", "abs": "https://arxiv.org/abs/2505.16421", "title": "WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning", "authors": ["Zhepei Wei", "Wenlin Yao", "Yao Liu", "Weizhi Zhang", "Qin Lu", "Liang Qiu", "Changlong Yu", "Puyang Xu", "Chao Zhang", "Bing Yin", "Hyokun Yun", "Lihong Li"], "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "While reinforcement learning (RL) has demonstrated remarkable success in\nenhancing large language models (LLMs), it has primarily focused on single-turn\ntasks such as solving math problems. Training effective web agents for\nmulti-turn interactions remains challenging due to the complexity of\nlong-horizon decision-making across dynamic web interfaces. In this work, we\npresent WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework\nfor training web agents. It learns directly from online interactions with web\nenvironments by asynchronously generating diverse trajectories, entirely guided\nby binary rewards depending on task success. Experiments on the WebArena-Lite\nbenchmark demonstrate the effectiveness of WebAgent-R1, boosting the task\nsuccess rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to\n44.8%, significantly outperforming existing state-of-the-art methods and strong\nproprietary models such as OpenAI o3. In-depth analyses reveal the\neffectiveness of the thinking-based prompting strategy and test-time scaling\nthrough increased interactions for web tasks. We further investigate different\nRL initialization policies by introducing two variants, namely WebAgent-R1-Zero\nand WebAgent-R1-CoT, which highlight the importance of the warm-up training\nstage (i.e., behavior cloning) and provide insights on incorporating long\nchain-of-thought (CoT) reasoning in web agents."}
{"id": "2505.16425", "pdf": "https://arxiv.org/pdf/2505.16425.pdf", "abs": "https://arxiv.org/abs/2505.16425", "title": "$I^2G$: Generating Instructional Illustrations via Text-Conditioned Diffusion", "authors": ["Jing Bi", "Pinxin Liu", "Ali Vosoughi", "Jiarui Wu", "Jinxi He", "Chenliang Xu"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 5 figures, under review", "summary": "The effective communication of procedural knowledge remains a significant\nchallenge in natural language processing (NLP), as purely textual instructions\noften fail to convey complex physical actions and spatial relationships. We\naddress this limitation by proposing a language-driven framework that\ntranslates procedural text into coherent visual instructions. Our approach\nmodels the linguistic structure of instructional content by decomposing it into\ngoal statements and sequential steps, then conditioning visual generation on\nthese linguistic elements. We introduce three key innovations: (1) a\nconstituency parser-based text encoding mechanism that preserves semantic\ncompleteness even with lengthy instructions, (2) a pairwise discourse coherence\nmodel that maintains consistency across instruction sequences, and (3) a novel\nevaluation protocol specifically designed for procedural language-to-image\nalignment. Our experiments across three instructional datasets (HTStep,\nCaptainCook4D, and WikiAll) demonstrate that our method significantly\noutperforms existing baselines in generating visuals that accurately reflect\nthe linguistic content and sequential nature of instructions. This work\ncontributes to the growing body of research on grounding procedural language in\nvisual content, with applications spanning education, task guidance, and\nmultimodal language understanding."}
{"id": "2505.16429", "pdf": "https://arxiv.org/pdf/2505.16429.pdf", "abs": "https://arxiv.org/abs/2505.16429", "title": "Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems", "authors": ["Song Jin", "Juntian Zhang", "Yuhan Liu", "Xun Zhang", "Yufei Zhang", "Guojun Yin", "Fei Jiang", "Wei Lin", "Rui Yan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating and iterating upon recommender systems is crucial, yet traditional\nA/B testing is resource-intensive, and offline methods struggle with dynamic\nuser-platform interactions. While agent-based simulation is promising, existing\nplatforms often lack a mechanism for user actions to dynamically reshape the\nenvironment. To bridge this gap, we introduce RecInter, a novel agent-based\nsimulation platform for recommender systems featuring a robust interaction\nmechanism. In RecInter platform, simulated user actions (e.g., likes, reviews,\npurchases) dynamically update item attributes in real-time, and introduced\nMerchant Agents can reply, fostering a more realistic and evolving ecosystem.\nHigh-fidelity simulation is ensured through Multidimensional User Profiling\nmodule, Advanced Agent Architecture, and LLM fine-tuned on Chain-of-Thought\n(CoT) enriched interaction data. Our platform achieves significantly improved\nsimulation credibility and successfully replicates emergent phenomena like\nBrand Loyalty and the Matthew Effect. Experiments demonstrate that this\ninteraction mechanism is pivotal for simulating realistic system evolution,\nestablishing our platform as a credible testbed for recommender systems\nresearch."}
{"id": "2505.16460", "pdf": "https://arxiv.org/pdf/2505.16460.pdf", "abs": "https://arxiv.org/abs/2505.16460", "title": "University of Indonesia at SemEval-2025 Task 11: Evaluating State-of-the-Art Encoders for Multi-Label Emotion Detection", "authors": ["Ikhlasul Akmal Hanif", "Eryawan Presma Yulianrifat", "Jaycent Gunawan Ongris", "Eduardus Tjitrahardja", "Muhammad Falensi Azmi", "Rahmat Bryan Naufal", "Alfan Farizki Wicaksono"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "16 pages, 13 tables, 1 figures", "summary": "This paper presents our approach for SemEval 2025 Task 11 Track A, focusing\non multilabel emotion classification across 28 languages. We explore two main\nstrategies: fully fine-tuning transformer models and classifier-only training,\nevaluating different settings such as fine-tuning strategies, model\narchitectures, loss functions, encoders, and classifiers. Our findings suggest\nthat training a classifier on top of prompt-based encoders such as mE5 and BGE\nyields significantly better results than fully fine-tuning XLMR and mBERT. Our\nbest-performing model on the final leaderboard is an ensemble combining\nmultiple BGE models, where CatBoost serves as the classifier, with different\nconfigurations. This ensemble achieves an average F1-macro score of 56.58\nacross all languages."}
{"id": "2505.16467", "pdf": "https://arxiv.org/pdf/2505.16467.pdf", "abs": "https://arxiv.org/abs/2505.16467", "title": "Reading Between the Prompts: How Stereotypes Shape LLM's Implicit Personalization", "authors": ["Vera Neplenbroek", "Arianna Bisazza", "Raquel Fernández"], "categories": ["cs.CL"], "comment": null, "summary": "Generative Large Language Models (LLMs) infer user's demographic information\nfrom subtle cues in the conversation -- a phenomenon called implicit\npersonalization. Prior work has shown that such inferences can lead to lower\nquality responses for users assumed to be from minority groups, even when no\ndemographic information is explicitly provided. In this work, we systematically\nexplore how LLMs respond to stereotypical cues using controlled synthetic\nconversations, by analyzing the models' latent user representations through\nboth model internals and generated answers to targeted user questions. Our\nfindings reveal that LLMs do infer demographic attributes based on these\nstereotypical signals, which for a number of groups even persists when the user\nexplicitly identifies with a different demographic group. Finally, we show that\nthis form of stereotype-driven implicit personalization can be effectively\nmitigated by intervening on the model's internal representations using a\ntrained linear probe to steer them toward the explicitly stated identity. Our\nresults highlight the need for greater transparency and control in how LLMs\nrepresent user identity."}
{"id": "2505.16483", "pdf": "https://arxiv.org/pdf/2505.16483.pdf", "abs": "https://arxiv.org/abs/2505.16483", "title": "Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning", "authors": ["Shuzheng Si", "Haozhe Zhao", "Cheng Gao", "Yuzhuo Bai", "Zhitong Wang", "Bofei Gao", "Kangyang Luo", "Wenhao Li", "Yufei Huang", "Gang Chen", "Fanchao Qi", "Minjia Zhang", "Baobao Chang", "Maosong Sun"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Teaching large language models (LLMs) to be faithful in the provided context\nis crucial for building reliable information-seeking systems. Therefore, we\npropose a systematic framework, CANOE, to improve the faithfulness of LLMs in\nboth short-form and long-form generation tasks without human annotations.\nSpecifically, we first synthesize short-form question-answering (QA) data with\nfour diverse tasks to construct high-quality and easily verifiable training\ndata without human annotation. Also, we propose Dual-GRPO, a rule-based\nreinforcement learning method that includes three tailored rule-based rewards\nderived from synthesized short-form QA data, while simultaneously optimizing\nboth short-form and long-form response generation. Notably, Dual-GRPO\neliminates the need to manually label preference data to train reward models\nand avoids over-optimizing short-form generation when relying only on the\nsynthesized short-form QA data. Experimental results show that CANOE greatly\nimproves the faithfulness of LLMs across 11 different downstream tasks, even\noutperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1."}
{"id": "2505.16491", "pdf": "https://arxiv.org/pdf/2505.16491.pdf", "abs": "https://arxiv.org/abs/2505.16491", "title": "LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing", "authors": ["Dario Di Palma", "Alessandro De Bellis", "Giovanni Servedio", "Vito Walter Anelli", "Fedelucio Narducci", "Tommaso Di Noia"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have rapidly become central to NLP,\ndemonstrating their ability to adapt to various tasks through prompting\ntechniques, including sentiment analysis. However, we still have a limited\nunderstanding of how these models capture sentiment-related information. This\nstudy probes the hidden layers of Llama models to pinpoint where sentiment\nfeatures are most represented and to assess how this affects sentiment\nanalysis.\n  Using probe classifiers, we analyze sentiment encoding across layers and\nscales, identifying the layers and pooling methods that best capture sentiment\nsignals. Our results show that sentiment information is most concentrated in\nmid-layers for binary polarity tasks, with detection accuracy increasing up to\n14% over prompting techniques. Additionally, we find that in decoder-only\nmodels, the last token is not consistently the most informative for sentiment\nencoding. Finally, this approach enables sentiment tasks to be performed with\nmemory requirements reduced by an average of 57%.\n  These insights contribute to a broader understanding of sentiment in LLMs,\nsuggesting layer-specific probing as an effective approach for sentiment tasks\nbeyond prompting, with potential to enhance model utility and reduce memory\nrequirements."}
{"id": "2505.16505", "pdf": "https://arxiv.org/pdf/2505.16505.pdf", "abs": "https://arxiv.org/abs/2505.16505", "title": "Sparse Activation Editing for Reliable Instruction Following in Narratives", "authors": ["Runcong Zhao", "Chengyu Cao", "Qinglin Zhu", "Xiucheng Lv", "Shun Shao", "Lin Gui", "Ruifeng Xu", "Yulan He"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Complex narrative contexts often challenge language models' ability to follow\ninstructions, and existing benchmarks fail to capture these difficulties. To\naddress this, we propose Concise-SAE, a training-free framework that improves\ninstruction following by identifying and editing instruction-relevant neurons\nusing only natural language instructions, without requiring labelled data. To\nthoroughly evaluate our method, we introduce FreeInstruct, a diverse and\nrealistic benchmark of 1,212 examples that highlights the challenges of\ninstruction following in narrative-rich settings. While initially motivated by\ncomplex narratives, Concise-SAE demonstrates state-of-the-art instruction\nadherence across varied tasks without compromising generation quality."}
{"id": "2505.16514", "pdf": "https://arxiv.org/pdf/2505.16514.pdf", "abs": "https://arxiv.org/abs/2505.16514", "title": "AppealCase: A Dataset and Benchmark for Civil Case Appeal Scenarios", "authors": ["Yuting Huang", "Meitong Guo", "Yiquan Wu", "Ang Li", "Xiaozhong Liu", "Keting Yin", "Changlong Sun", "Fei Wu", "Kun Kuang"], "categories": ["cs.CL"], "comment": "15 pages, 4 figures", "summary": "Recent advances in LegalAI have primarily focused on individual case judgment\nanalysis, often overlooking the critical appellate process within the judicial\nsystem. Appeals serve as a core mechanism for error correction and ensuring\nfair trials, making them highly significant both in practice and in research.\nTo address this gap, we present the AppealCase dataset, consisting of 10,000\npairs of real-world, matched first-instance and second-instance documents\nacross 91 categories of civil cases. The dataset also includes detailed\nannotations along five dimensions central to appellate review: judgment\nreversals, reversal reasons, cited legal provisions, claim-level decisions, and\nwhether there is new information in the second instance. Based on these\nannotations, we propose five novel LegalAI tasks and conduct a comprehensive\nevaluation across 20 mainstream models. Experimental results reveal that all\ncurrent models achieve less than 50% F1 scores on the judgment reversal\nprediction task, highlighting the complexity and challenge of the appeal\nscenario. We hope that the AppealCase dataset will spur further research in\nLegalAI for appellate case analysis and contribute to improving consistency in\njudicial decision-making."}
{"id": "2505.16518", "pdf": "https://arxiv.org/pdf/2505.16518.pdf", "abs": "https://arxiv.org/abs/2505.16518", "title": "CUB: Benchmarking Context Utilisation Techniques for Language Models", "authors": ["Lovisa Hagström", "Youna Kim", "Haeun Yu", "Sang-goo Lee", "Richard Johansson", "Hyunsoo Cho", "Isabelle Augenstein"], "categories": ["cs.CL", "cs.AI"], "comment": "27 pages", "summary": "Incorporating external knowledge is crucial for knowledge-intensive tasks,\nsuch as question answering and fact checking. However, language models (LMs)\nmay ignore relevant information that contradicts outdated parametric memory or\nbe distracted by irrelevant contexts. While many context utilisation\nmanipulation techniques (CMTs) that encourage or suppress context utilisation\nhave recently been proposed to alleviate these issues, few have seen systematic\ncomparison. In this paper, we develop CUB (Context Utilisation Benchmark) to\nhelp practitioners within retrieval-augmented generation (RAG) identify the\nbest CMT for their needs. CUB allows for rigorous testing on three distinct\ncontext types, observed to capture key challenges in realistic context\nutilisation scenarios. With this benchmark, we evaluate seven state-of-the-art\nmethods, representative of the main categories of CMTs, across three diverse\ndatasets and tasks, applied to nine LMs. Our results show that most of the\nexisting CMTs struggle to handle the full set of types of contexts that may be\nencountered in real-world retrieval-augmented scenarios. Moreover, we find that\nmany CMTs display an inflated performance on simple synthesised datasets,\ncompared to more realistic datasets with naturally occurring samples.\nAltogether, our results show the need for holistic tests of CMTs and the\ndevelopment of CMTs that can handle multiple context types."}
{"id": "2505.16520", "pdf": "https://arxiv.org/pdf/2505.16520.pdf", "abs": "https://arxiv.org/abs/2505.16520", "title": "Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs", "authors": ["Giovanni Servedio", "Alessandro De Bellis", "Dario Di Palma", "Vito Walter Anelli", "Tommaso Di Noia"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Factual hallucinations are a major challenge for Large Language Models\n(LLMs). They undermine reliability and user trust by generating inaccurate or\nfabricated content. Recent studies suggest that when generating false\nstatements, the internal states of LLMs encode information about truthfulness.\nHowever, these studies often rely on synthetic datasets that lack realism,\nwhich limits generalization when evaluating the factual accuracy of text\ngenerated by the model itself. In this paper, we challenge the findings of\nprevious work by investigating truthfulness encoding capabilities, leading to\nthe generation of a more realistic and challenging dataset. Specifically, we\nextend previous work by introducing: (1) a strategy for sampling plausible\ntrue-false factoid sentences from tabular data and (2) a procedure for\ngenerating realistic, LLM-dependent true-false datasets from Question Answering\ncollections. Our analysis of two open-source LLMs reveals that while the\nfindings from previous studies are partially validated, generalization to\nLLM-generated datasets remains challenging. This study lays the groundwork for\nfuture research on factuality in LLMs and offers practical guidelines for more\neffective evaluation."}
{"id": "2505.16522", "pdf": "https://arxiv.org/pdf/2505.16522.pdf", "abs": "https://arxiv.org/abs/2505.16522", "title": "Benchmarking and Pushing the Multi-Bias Elimination Boundary of LLMs via Causal Effect Estimation-guided Debiasing", "authors": ["Zhouhao Sun", "Zhiyuan Kan", "Xiao Ding", "Li Du", "Yang Zhao", "Bing Qin", "Ting Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite significant progress, recent studies have indicated that current\nlarge language models (LLMs) may still utilize bias during inference, leading\nto the poor generalizability of LLMs. Some benchmarks are proposed to\ninvestigate the generalizability of LLMs, with each piece of data typically\ncontaining one type of controlled bias. However, a single piece of data may\ncontain multiple types of biases in practical applications. To bridge this gap,\nwe propose a multi-bias benchmark where each piece of data contains five types\nof biases. The evaluations conducted on this benchmark reveal that the\nperformance of existing LLMs and debiasing methods is unsatisfying,\nhighlighting the challenge of eliminating multiple types of biases\nsimultaneously. To overcome this challenge, we propose a causal effect\nestimation-guided multi-bias elimination method (CMBE). This method first\nestimates the causal effect of multiple types of biases simultaneously.\nSubsequently, we eliminate the causal effect of biases from the total causal\neffect exerted by both the semantic information and biases during inference.\nExperimental results show that CMBE can effectively eliminate multiple types of\nbias simultaneously to enhance the generalizability of LLMs."}
{"id": "2505.16526", "pdf": "https://arxiv.org/pdf/2505.16526.pdf", "abs": "https://arxiv.org/abs/2505.16526", "title": "EnSToM: Enhancing Dialogue Systems with Entropy-Scaled Steering Vectors for Topic Maintenance", "authors": ["Heejae Suh", "Yejin Jeon", "Deokhyung Kang", "Taehee Park", "Yejin Min", "Gary Geunbae Lee"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 (Findings, long paper)", "summary": "Small large language models (sLLMs) offer the advantage of being lightweight\nand efficient, which makes them suitable for resource-constrained environments.\nHowever, sLLMs often struggle to maintain topic consistency in task-oriented\ndialogue systems, which is critical for scenarios such as service chatbots.\nSpecifically, it is important to ensure that the model denies off-topic or\nmalicious inputs and adheres to its intended functionality so as to prevent\npotential misuse and uphold reliability. Towards this, existing activation\nengineering approaches have been proposed to manipulate internal activations\nduring inference. While these methods are effective in certain scenarios, our\npreliminary experiments reveal their limitations in ensuring topic adherence.\nTherefore, to address this, we propose a novel approach termed Entropy-scaled\nSteering vectors for Topic Maintenance (EnSToM). EnSToM dynamically adjusts the\nsteering intensity based on input uncertainty, which allows the model to handle\noff-topic distractors effectively while preserving on-topic accuracy. Our\nexperiments demonstrate that EnSToM achieves significant performance gain with\na relatively small data size compared to fine-tuning approaches. By improving\ntopic adherence without compromising efficiency, our approach provides a robust\nsolution for enhancing sLLM-based dialogue systems."}
{"id": "2505.16538", "pdf": "https://arxiv.org/pdf/2505.16538.pdf", "abs": "https://arxiv.org/abs/2505.16538", "title": "Mechanistic Understanding and Mitigation of Language Confusion in English-Centric Large Language Models", "authors": ["Ercong Nie", "Helmut Schmid", "Hinrich Schütze"], "categories": ["cs.CL"], "comment": "16 pages, 5 figures", "summary": "Language confusion -- where large language models (LLMs) generate unintended\nlanguages against the user's need -- remains a critical challenge, especially\nfor English-centric models. We present the first mechanistic interpretability\n(MI) study of language confusion, combining behavioral benchmarking with\nneuron-level analysis. Using the Language Confusion Benchmark (LCB), we show\nthat confusion points (CPs) -- specific positions where language switches occur\n-- are central to this phenomenon. Through layer-wise analysis with TunedLens\nand targeted neuron attribution, we reveal that transition failures in the\nfinal layers drive confusion. We further demonstrate that editing a small set\nof critical neurons, identified via comparative analysis with\nmultilingual-tuned models, substantially mitigates confusion without harming\ngeneral competence or fluency. Our approach matches multilingual alignment in\nconfusion reduction for most languages and yields cleaner, higher-quality\noutputs. These findings provide new insights into the internal dynamics of LLMs\nand highlight neuron-level interventions as a promising direction for robust,\ninterpretable multilingual language modeling."}
{"id": "2505.16552", "pdf": "https://arxiv.org/pdf/2505.16552.pdf", "abs": "https://arxiv.org/abs/2505.16552", "title": "Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains", "authors": ["Wenhui Tan", "Jiaze Li", "Jianzhong Ju", "Zhenbo Luo", "Jian Luan", "Ruihua Song"], "categories": ["cs.CL"], "comment": "15 pages, 8 figures", "summary": "Large Language Models (LLMs) achieve superior performance through\nChain-of-Thought (CoT) reasoning, but these token-level reasoning chains are\ncomputationally expensive and inefficient. In this paper, we introduce\nCompressed Latent Reasoning (CoLaR), a novel framework that dynamically\ncompresses reasoning processes in latent space through a two-stage training\napproach. First, during supervised fine-tuning, CoLaR extends beyond next-token\nprediction by incorporating an auxiliary next compressed embedding prediction\nobjective. This process merges embeddings of consecutive tokens using a\ncompression factor randomly sampled from a predefined range, and trains a\nspecialized latent head to predict distributions of subsequent compressed\nembeddings. Second, we enhance CoLaR through reinforcement learning (RL) that\nleverages the latent head's non-deterministic nature to explore diverse\nreasoning paths and exploit more compact ones. This approach enables CoLaR to:\ni) perform reasoning at a dense latent level (i.e., silently), substantially\nreducing reasoning chain length, and ii) dynamically adjust reasoning speed at\ninference time by simply prompting the desired compression factor. Extensive\nexperiments across four mathematical reasoning datasets demonstrate that CoLaR\nachieves 14.1% higher accuracy than latent-based baseline methods at comparable\ncompression ratios, and reduces reasoning chain length by 53.3% with only 4.8%\nperformance degradation compared to explicit CoT method. Moreover, when applied\nto more challenging mathematical reasoning tasks, our RL-enhanced CoLaR\ndemonstrates performance gains of up to 5.4% while dramatically reducing latent\nreasoning chain length by 82.8%. The code and models will be released upon\nacceptance."}
{"id": "2505.16566", "pdf": "https://arxiv.org/pdf/2505.16566.pdf", "abs": "https://arxiv.org/abs/2505.16566", "title": "ScholarBench: A Bilingual Benchmark for Abstraction, Comprehension, and Reasoning Evaluation in Academic Contexts", "authors": ["Dongwon Noh", "Donghyeok Koh", "Junghun Yuk", "Gyuwan Kim", "Jaeyong Lee", "Kyungtae Lim", "Cheoneum Park"], "categories": ["cs.CL"], "comment": null, "summary": "Prior benchmarks for evaluating the domain-specific knowledge of large\nlanguage models (LLMs) lack the scalability to handle complex academic tasks.\nTo address this, we introduce \\texttt{ScholarBench}, a benchmark centered on\ndeep expert knowledge and complex academic problem-solving, which evaluates the\nacademic reasoning ability of LLMs and is constructed through a three-step\nprocess. \\texttt{ScholarBench} targets more specialized and logically complex\ncontexts derived from academic literature, encompassing five distinct problem\ntypes. Unlike prior benchmarks, \\texttt{ScholarBench} evaluates the\nabstraction, comprehension, and reasoning capabilities of LLMs across eight\ndistinct research domains. To ensure high-quality evaluation data, we define\ncategory-specific example attributes and design questions that are aligned with\nthe characteristic research methodologies and discourse structures of each\ndomain. Additionally, this benchmark operates as an English-Korean bilingual\ndataset, facilitating simultaneous evaluation for linguistic capabilities of\nLLMs in both languages. The benchmark comprises 5,031 examples in Korean and\n5,309 in English, with even state-of-the-art models like o3-mini achieving an\naverage evaluation score of only 0.543, demonstrating the challenging nature of\nthis benchmark."}
{"id": "2505.16570", "pdf": "https://arxiv.org/pdf/2505.16570.pdf", "abs": "https://arxiv.org/abs/2505.16570", "title": "URLs Help, Topics Guide: Understanding Metadata Utility in LLM Training", "authors": ["Dongyang Fan", "Vinko Sabolčec", "Martin Jaggi"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are commonly pretrained on vast corpora of text\nwithout utilizing contextual metadata such as source, quality, or topic,\nleading to a context-free learning paradigm. While recent studies suggest that\nadding metadata like URL information as context (i.e., auxiliary inputs not\nused in the loss calculation) can improve training efficiency and downstream\nperformance, they offer limited understanding of which types of metadata are\ntruly effective and under what conditions. In this work, we conduct a\nsystematic evaluation and find that not all metadata types contribute equally.\nOnly URL context speeds up training, whereas quality scores and topic/format\ndomain information offer no clear benefit. Furthermore, the improved downstream\nperformances of URL conditioning emerge only when longer prompts are used at\ninference time. In addition, we demonstrate that context-aware pretraining\nenables more controllable generation than context-free pretraining, in a\nclassifier-free guidance fashion. Although topic and format metadata do not\naccelerate training, they are effective for steering outputs, offering\nhuman-interpretable control over generation."}
{"id": "2505.16576", "pdf": "https://arxiv.org/pdf/2505.16576.pdf", "abs": "https://arxiv.org/abs/2505.16576", "title": "EMULATE: A Multi-Agent Framework for Determining the Veracity of Atomic Claims by Emulating Human Actions", "authors": ["Spencer Hong", "Meng Luo", "Xinyi Wan"], "categories": ["cs.CL"], "comment": null, "summary": "Determining the veracity of atomic claims is an imperative component of many\nrecently proposed fact-checking systems. Many approaches tackle this problem by\nfirst retrieving evidence by querying a search engine and then performing\nclassification by providing the evidence set and atomic claim to a large\nlanguage model, but this process deviates from what a human would do in order\nto perform the task. Recent work attempted to address this issue by proposing\niterative evidence retrieval, allowing for evidence to be collected several\ntimes and only when necessary. Continuing along this line of research, we\npropose a novel claim verification system, called EMULATE, which is designed to\nbetter emulate human actions through the use of a multi-agent framework where\neach agent performs a small part of the larger task, such as ranking search\nresults according to predefined criteria or evaluating webpage content.\nExtensive experiments on several benchmarks show clear improvements over prior\nwork, demonstrating the efficacy of our new multi-agent framework."}
{"id": "2505.16582", "pdf": "https://arxiv.org/pdf/2505.16582.pdf", "abs": "https://arxiv.org/abs/2505.16582", "title": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering", "authors": ["Jianbiao Mei", "Tao Hu", "Daocheng Fu", "Licheng Wen", "Xuemeng Yang", "Rong Wu", "Pinlong Cai", "Xing Gao", "Yu Yang", "Chengjun Xie", "Botian Shi", "Yong Liu", "Yu Qiao"], "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 9 figures", "summary": "Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones."}
{"id": "2505.16591", "pdf": "https://arxiv.org/pdf/2505.16591.pdf", "abs": "https://arxiv.org/abs/2505.16591", "title": "Evaluating Large Language Model with Knowledge Oriented Language Specific Simple Question Answering", "authors": ["Bowen Jiang", "Runchuan Zhu", "Jiang Wu", "Zinco Jiang", "Yifan He", "Junyuan Gao", "Jia Yu", "Rui Min", "Yinfan Wang", "Haote Yang", "Songyang Zhang", "Dahua Lin", "Lijun Wu", "Conghui He"], "categories": ["cs.CL"], "comment": "Equal contribution: Bowen Jiang, Runchuan Zhu, Jiang Wu;\n  Corresponding author: Conghui He", "summary": "We introduce KoLasSimpleQA, the first benchmark evaluating the multilingual\nfactual ability of Large Language Models (LLMs). Inspired by existing research,\nwe created the question set with features such as single knowledge point\ncoverage, absolute objectivity, unique answers, and temporal stability. These\nquestions enable efficient evaluation using the LLM-as-judge paradigm, testing\nboth the LLMs' factual memory and self-awareness (\"know what they don't know\").\nKoLasSimpleQA expands existing research in two key dimensions: (1) Breadth\n(Multilingual Coverage): It includes 9 languages, supporting global\napplicability evaluation. (2) Depth (Dual Domain Design): It covers both the\ngeneral domain (global facts) and the language-specific domain (such as\nhistory, culture, and regional traditions) for a comprehensive assessment of\nmultilingual capabilities. We evaluated mainstream LLMs, including traditional\nLLM and emerging Large Reasoning Models. Results show significant performance\ndifferences between the two domains, particularly in performance metrics,\nranking, calibration, and robustness. This highlights the need for targeted\nevaluation and optimization in multilingual contexts. We hope KoLasSimpleQA\nwill help the research community better identify LLM capability boundaries in\nmultilingual contexts and provide guidance for model optimization. We will\nrelease KoLasSimpleQA at https://github.com/opendatalab/KoLasSimpleQA ."}
{"id": "2505.16592", "pdf": "https://arxiv.org/pdf/2505.16592.pdf", "abs": "https://arxiv.org/abs/2505.16592", "title": "What Media Frames Reveal About Stance: A Dataset and Study about Memes in Climate Change Discourse", "authors": ["Shijia Zhou", "Siyao Peng", "Simon Luebke", "Jörg Haßler", "Mario Haim", "Saif M. Mohammad", "Barbara Plank"], "categories": ["cs.CL", "cs.MM"], "comment": "19 pages, 9 figures", "summary": "Media framing refers to the emphasis on specific aspects of perceived reality\nto shape how an issue is defined and understood. Its primary purpose is to\nshape public perceptions often in alignment with the authors' opinions and\nstances. However, the interaction between stance and media frame remains\nlargely unexplored. In this work, we apply an interdisciplinary approach to\nconceptualize and computationally explore this interaction with internet memes\non climate change. We curate CLIMATEMEMES, the first dataset of climate-change\nmemes annotated with both stance and media frames, inspired by research in\ncommunication science. CLIMATEMEMES includes 1,184 memes sourced from 47\nsubreddits, enabling analysis of frame prominence over time and communities,\nand sheds light on the framing preferences of different stance holders. We\npropose two meme understanding tasks: stance detection and media frame\ndetection. We evaluate LLaVA-NeXT and Molmo in various setups, and report the\ncorresponding results on their LLM backbone. Human captions consistently\nenhance performance. Synthetic captions and human-corrected OCR also help\noccasionally. Our findings highlight that VLMs perform well on stance, but\nstruggle on frames, where LLMs outperform VLMs. Finally, we analyze VLMs'\nlimitations in handling nuanced frames and stance expressions on climate change\ninternet memes."}
{"id": "2505.16610", "pdf": "https://arxiv.org/pdf/2505.16610.pdf", "abs": "https://arxiv.org/abs/2505.16610", "title": "From Generic Empathy to Personalized Emotional Support: A Self-Evolution Framework for User Preference Alignment", "authors": ["Jing Ye", "Lu Xiang", "Yaping Zhang", "Chengqing Zong"], "categories": ["cs.CL"], "comment": "27 pages", "summary": "Effective emotional support hinges on understanding users' emotions and needs\nto provide meaningful comfort during multi-turn interactions. Large Language\nModels (LLMs) show great potential for expressing empathy; however, they often\ndeliver generic and one-size-fits-all responses that fail to address users'\nspecific needs. To tackle this issue, we propose a self-evolution framework\ndesigned to help LLMs improve their responses to better align with users'\nimplicit preferences concerning user profiles (personalities), emotional\nstates, and specific situations. Our framework consists of two distinct phases:\n\\textit{(1)} \\textit{Emotional Support Experience Acquisition}, where LLMs are\nfine-tuned on limited emotional support conversation data to provide basic\nsupport, and \\textit{(2)} \\textit{Self-Improvement for Personalized Emotional\nSupport}, where LLMs leverage self-reflection and self-refinement to generate\npersonalized responses. Through iterative direct preference optimization\nbetween the pre- and post-refined responses, our model generates responses that\nreflect a better understanding of the user's implicit preferences. Extensive\nexperiments and evaluations demonstrate that our method significantly enhances\nthe model's performance in emotional support, reducing unhelpful responses and\nminimizing discrepancies between user preferences and model outputs."}
{"id": "2505.16612", "pdf": "https://arxiv.org/pdf/2505.16612.pdf", "abs": "https://arxiv.org/abs/2505.16612", "title": "Steering Large Language Models for Machine Translation Personalization", "authors": ["Daniel Scalena", "Gabriele Sarti", "Arianna Bisazza", "Elisabetta Fersini", "Malvina Nissim"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "High-quality machine translation systems based on large language models\n(LLMs) have simplified the production of personalized translations reflecting\nspecific stylistic constraints. However, these systems still struggle in\nsettings where stylistic requirements are less explicit and might be harder to\nconvey via prompting. We explore various strategies for personalizing\nLLM-generated translations in low-resource settings, focusing on the\nchallenging literary translation domain. We explore prompting strategies and\ninference-time interventions for steering model generations towards a\npersonalized style, and propose a contrastive framework exploiting latent\nconcepts extracted from sparse autoencoders to identify salient personalization\nproperties. Our results show that steering achieves strong personalization\nwhile preserving translation quality. We further examine the impact of steering\non LLM representations, finding model layers with a relevant impact for\npersonalization are impacted similarly by multi-shot prompting and our steering\nmethod, suggesting similar mechanism at play."}
{"id": "2505.16637", "pdf": "https://arxiv.org/pdf/2505.16637.pdf", "abs": "https://arxiv.org/abs/2505.16637", "title": "SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation", "authors": ["Wenjie Yang", "Mao Zheng", "Mingyang Song", "Zheng Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have recently demonstrated remarkable\ncapabilities in machine translation (MT). However, most advanced MT-specific\nLLMs heavily rely on external supervision signals during training, such as\nhuman-annotated reference data or trained reward models (RMs), which are often\nexpensive to obtain and challenging to scale. To overcome this limitation, we\npropose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for\nMT that is reference-free, fully online, and relies solely on self-judging\nrewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as\nthe backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,\ne.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like\nQwen2.5-32B-Instruct in English $\\leftrightarrow$ Chinese translation tasks\nfrom WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR\nwith external supervision from COMET, our strongest model, SSR-X-Zero-7B,\nachieves state-of-the-art performance in English $\\leftrightarrow$ Chinese\ntranslation, surpassing all existing open-source models under 72B parameters\nand even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.\nOur analysis highlights the effectiveness of the self-rewarding mechanism\ncompared to the external LLM-as-a-judge approach in MT and demonstrates its\ncomplementary benefits when combined with trained RMs. Our findings provide\nvaluable insight into the potential of self-improving RL methods. We have\npublicly released our code, data and models."}
{"id": "2505.16648", "pdf": "https://arxiv.org/pdf/2505.16648.pdf", "abs": "https://arxiv.org/abs/2505.16648", "title": "Collaboration among Multiple Large Language Models for Medical Question Answering", "authors": ["Kexin Shang", "Chia-Hsuan Chang", "Christopher C. Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to IEEE International Conference on Healthcare Informatics\n  2025", "summary": "Empowered by vast internal knowledge reservoir, the new generation of large\nlanguage models (LLMs) demonstrate untapped potential to tackle medical tasks.\nHowever, there is insufficient effort made towards summoning up a synergic\neffect from multiple LLMs' expertise and background. In this study, we propose\na multi-LLM collaboration framework tailored on a medical multiple-choice\nquestions dataset. Through post-hoc analysis on 3 pre-trained LLM participants,\nour framework is proved to boost all LLMs reasoning ability as well as\nalleviate their divergence among questions. We also measure an LLM's confidence\nwhen it confronts with adversary opinions from other LLMs and observe a\nconcurrence between LLM's confidence and prediction accuracy."}
{"id": "2505.16660", "pdf": "https://arxiv.org/pdf/2505.16660.pdf", "abs": "https://arxiv.org/abs/2505.16660", "title": "Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu", "authors": ["Liu Chang", "Wang Dongbo", "Liu liu", "Zhao Zhixiao"], "categories": ["cs.CL", "cs.AI"], "comment": "29pages, 7 figures", "summary": "This study addresses the challenges in intelligent processing of Chinese\nancient mathematical classics by constructing Guji_MATH, a benchmark for\nevaluating classical texts based on Suanjing Shishu. It systematically assesses\nthe mathematical problem-solving capabilities of mainstream reasoning models\nunder the unique linguistic constraints of classical Chinese. Through\nmachine-assisted annotation and manual verification, 538 mathematical problems\nwere extracted from 8 canonical texts, forming a structured dataset centered on\nthe \"Question-Answer-Solution\" framework, supplemented by problem types and\ndifficulty levels. Dual evaluation modes--closed-book (autonomous\nproblem-solving) and open-book (reproducing classical solution methods)--were\ndesigned to evaluate the performance of six reasoning models on ancient Chinese\nmathematical problems. Results indicate that reasoning models can partially\ncomprehend and solve these problems, yet their overall performance remains\ninferior to benchmarks on modern mathematical tasks. Enhancing models'\nclassical Chinese comprehension and cultural knowledge should be prioritized\nfor optimization. This study provides methodological support for mining\nmathematical knowledge from ancient texts and disseminating traditional\nculture, while offering new perspectives for evaluating cross-linguistic and\ncross-cultural capabilities of reasoning models."}
{"id": "2505.16661", "pdf": "https://arxiv.org/pdf/2505.16661.pdf", "abs": "https://arxiv.org/abs/2505.16661", "title": "A Japanese Language Model and Three New Evaluation Benchmarks for Pharmaceutical NLP", "authors": ["Issey Sukeda", "Takuro Fujii", "Kosei Buma", "Shunsuke Sasaki", "Shinnosuke Ono"], "categories": ["cs.CL"], "comment": "15 pages, 9 tables, 5 figures", "summary": "We present a Japanese domain-specific language model for the pharmaceutical\nfield, developed through continual pretraining on 2 billion Japanese\npharmaceutical tokens and 8 billion English biomedical tokens. To enable\nrigorous evaluation, we introduce three new benchmarks: YakugakuQA, based on\nnational pharmacist licensing exams; NayoseQA, which tests cross-lingual\nsynonym and terminology normalization; and SogoCheck, a novel task designed to\nassess consistency reasoning between paired statements. We evaluate our model\nagainst both open-source medical LLMs and commercial models, including GPT-4o.\nResults show that our domain-specific model outperforms existing open models\nand achieves competitive performance with commercial ones, particularly on\nterminology-heavy and knowledge-based tasks. Interestingly, even GPT-4o\nperforms poorly on SogoCheck, suggesting that cross-sentence consistency\nreasoning remains an open challenge. Our benchmark suite offers a broader\ndiagnostic lens for pharmaceutical NLP, covering factual recall, lexical\nvariation, and logical consistency. This work demonstrates the feasibility of\nbuilding practical, secure, and cost-effective language models for Japanese\ndomain-specific applications, and provides reusable evaluation resources for\nfuture research in pharmaceutical and healthcare NLP. Our model, codes, and\ndatasets are released at https://github.com/EQUES-Inc/pharma-LLM-eval."}
{"id": "2505.16694", "pdf": "https://arxiv.org/pdf/2505.16694.pdf", "abs": "https://arxiv.org/abs/2505.16694", "title": "Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence", "authors": ["Gouki Minegishi", "Hiroki Furuta", "Shohei Taniguchi", "Yusuke Iwasawa", "Yutaka Matsuo"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ICML 2025", "summary": "Transformer-based language models exhibit In-Context Learning (ICL), where\npredictions are made adaptively based on context. While prior work links\ninduction heads to ICL through a sudden jump in accuracy, this can only account\nfor ICL when the answer is included within the context. However, an important\nproperty of practical ICL in large language models is the ability to meta-learn\nhow to solve tasks from context, rather than just copying answers from context;\nhow such an ability is obtained during training is largely unexplored. In this\npaper, we experimentally clarify how such meta-learning ability is acquired by\nanalyzing the dynamics of the model's circuit during training. Specifically, we\nextend the copy task from previous research into an In-Context Meta Learning\nsetting, where models must infer a task from examples to answer queries.\nInterestingly, in this setting, we find that there are multiple phases in the\nprocess of acquiring such abilities, and that a unique circuit emerges in each\nphase, contrasting with the single-phases change in induction heads. The\nemergence of such circuits can be related to several phenomena known in large\nlanguage models, and our analysis lead to a deeper understanding of the source\nof the transformer's ICL ability."}
{"id": "2505.16703", "pdf": "https://arxiv.org/pdf/2505.16703.pdf", "abs": "https://arxiv.org/abs/2505.16703", "title": "Locate-then-Merge: Neuron-Level Parameter Fusion for Mitigating Catastrophic Forgetting in Multimodal LLMs", "authors": ["Zeping Yu", "Sophia Ananiadou"], "categories": ["cs.CL"], "comment": null, "summary": "Although multimodal large language models (MLLMs) have achieved impressive\nperformance, the multimodal instruction tuning stage often causes catastrophic\nforgetting of the base LLM's language ability, even in strong models like\nLlama3. To address this, we propose Locate-then-Merge, a training-free\nparameter fusion framework that first locates important parameters and then\nselectively merges them. We further introduce Neuron-Fusion, a neuron-level\nstrategy that preserves the influence of neurons with large parameter\nshifts--neurons likely responsible for newly acquired visual\ncapabilities--while attenuating the influence of neurons with smaller changes\nthat likely encode general-purpose language skills. This design enables better\nretention of visual adaptation while mitigating language degradation.\nExperiments on 13 benchmarks across both language and visual tasks show that\nNeuron-Fusion consistently outperforms existing model merging methods. Further\nanalysis reveals that our method effectively reduces context hallucination in\ngeneration."}
{"id": "2505.16722", "pdf": "https://arxiv.org/pdf/2505.16722.pdf", "abs": "https://arxiv.org/abs/2505.16722", "title": "Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification", "authors": ["Himanshu Beniwal", "Youngwoo Kim", "Maarten Sap", "Soham Dan", "Thomas Hartvigsen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) become increasingly prevalent in global\napplications, ensuring that they are toxicity-free across diverse linguistic\ncontexts remains a critical challenge. We explore \"Cross-lingual\nDetoxification\", a cross-lingual paradigm that mitigates toxicity, enabling\ndetoxification capabilities to transfer between high and low-resource languages\nacross different script families. We analyze cross-lingual detoxification's\neffectiveness through 504 extensive settings to evaluate toxicity reduction in\ncross-distribution settings with limited data and investigate how mitigation\nimpacts model performance on non-toxic tasks, revealing trade-offs between\nsafety and knowledge preservation. Our code and dataset are publicly available\nat https://github.com/himanshubeniwal/Breaking-mBad."}
{"id": "2505.16743", "pdf": "https://arxiv.org/pdf/2505.16743.pdf", "abs": "https://arxiv.org/abs/2505.16743", "title": "TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative Metric-driven Pruning", "authors": ["Florentin Beck", "William Rudman", "Carsten Eickhoff"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; F.2.2"], "comment": null, "summary": "Large Language Models (LLMs) present significant computational and memory\nchallenges due to their extensive size, making pruning essential for their\nefficient deployment. Existing one-shot pruning methods often apply uniform\nsparsity constraints across layers or within each layer, resulting in\nsuboptimal performance, especially at high sparsity ratios. This work\nintroduces TRIM (Targeted Row-wise Iterative Metric-driven pruning), a novel\napproach that applies varying sparsity ratios to individual output dimensions\n(rows) within each layer. TRIM employs an iterative adjustment process guided\nby quality metrics to optimize dimension-wise sparsity allocation, focusing on\nreducing variance in quality retention across outputs to preserve critical\ninformation. TRIM can be seamlessly integrated with existing layer-wise pruning\nstrategies. Our evaluations on perplexity and zero-shot tasks across diverse\nLLM families (Qwen2.5, LLaMA-2, and OPT) and sparsity levels demonstrate that\nTRIM achieves new state-of-the-art results and enhances stability. For\ninstance, at 80% sparsity, TRIM reduces perplexity by 48% for Qwen2.5-14B and\nover 90% for OPT-13B compared to baseline methods. We conclude that\nfine-grained, dimension-wise sparsity adaptation is crucial for pushing the\nlimits of extreme LLM compression. Code available at:\nhttps://github.com/flobk/TRIM"}
{"id": "2505.16774", "pdf": "https://arxiv.org/pdf/2505.16774.pdf", "abs": "https://arxiv.org/abs/2505.16774", "title": "IFEval-Audio: Benchmarking Instruction-Following Capability in Audio-based Large Language Models", "authors": ["Yiming Gao", "Bin Wang", "Chengwei Wei", "Shuo Sun", "AiTi Aw"], "categories": ["cs.CL"], "comment": "Link: https://github.com/AudioLLMs/AudioBench/tree/main/IFEval-Audio", "summary": "Large language models (LLMs) have demonstrated strong instruction-following\ncapabilities in text-based tasks. However, this ability often deteriorates in\nmultimodal models after alignment with non-text modalities such as images or\naudio. While several recent efforts have investigated instruction-following\nperformance in text and vision-language models, instruction-following in\naudio-based large language models remains largely unexplored. To bridge this\ngap, we introduce IFEval-Audio, a novel evaluation dataset designed to assess\nthe ability to follow instructions in an audio LLM. IFEval-Audio contains 280\naudio-instruction-answer triples across six diverse dimensions: Content,\nCapitalization, Symbol, List Structure, Length, and Format. Each example pairs\nan audio input with a text instruction, requiring the model to generate an\noutput that follows a specified structure. We benchmark state-of-the-art audio\nLLMs on their ability to follow audio-involved instructions. The dataset is\nreleased publicly to support future research in this emerging area."}
{"id": "2505.16782", "pdf": "https://arxiv.org/pdf/2505.16782.pdf", "abs": "https://arxiv.org/abs/2505.16782", "title": "Reasoning Beyond Language: A Comprehensive Survey on Latent Chain-of-Thought Reasoning", "authors": ["Xinghao Chen", "Anhao Zhao", "Heming Xia", "Xuan Lu", "Hanlin Wang", "Yanjun Chen", "Wei Zhang", "Jian Wang", "Wenjie Li", "Xiaoyu Shen"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved impressive performance on complex\nreasoning tasks with Chain-of-Thought (CoT) prompting. However, conventional\nCoT relies on reasoning steps explicitly verbalized in natural language,\nintroducing inefficiencies and limiting its applicability to abstract\nreasoning. To address this, there has been growing research interest in latent\nCoT reasoning, where inference occurs within latent spaces. By decoupling\nreasoning from language, latent reasoning promises richer cognitive\nrepresentations and more flexible, faster inference. Researchers have explored\nvarious directions in this promising field, including training methodologies,\nstructural innovations, and internal reasoning mechanisms. This paper presents\na comprehensive overview and analysis of this reasoning paradigm. We begin by\nproposing a unified taxonomy from four perspectives: token-wise strategies,\ninternal mechanisms, analysis, and applications. We then provide in-depth\ndiscussions and comparative analyses of representative methods, highlighting\ntheir design patterns, strengths, and open challenges. We aim to provide a\nstructured foundation for advancing this emerging direction in LLM reasoning.\nThe relevant papers will be regularly updated at\nhttps://github.com/EIT-NLP/Awesome-Latent-CoT."}
{"id": "2505.16789", "pdf": "https://arxiv.org/pdf/2505.16789.pdf", "abs": "https://arxiv.org/abs/2505.16789", "title": "Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability", "authors": ["Punya Syon Pandey", "Samuel Simko", "Kellin Pelrine", "Zhijing Jin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "As large language models gain popularity, their vulnerability to adversarial\nattacks remains a primary concern. While fine-tuning models on domain-specific\ndatasets is often employed to improve model performance, it can introduce\nvulnerabilities within the underlying model. In this work, we investigate\nAccidental Misalignment, unexpected vulnerabilities arising from\ncharacteristics of fine-tuning data. We begin by identifying potential\ncorrelation factors such as linguistic features, semantic similarity, and\ntoxicity within our experimental datasets. We then evaluate the adversarial\nperformance of these fine-tuned models and assess how dataset factors correlate\nwith attack success rates. Lastly, we explore potential causal links, offering\nnew insights into adversarial defense strategies and highlighting the crucial\nrole of dataset design in preserving model alignment. Our code is available at\nhttps://github.com/psyonp/accidental_misalignment."}
{"id": "2505.16800", "pdf": "https://arxiv.org/pdf/2505.16800.pdf", "abs": "https://arxiv.org/abs/2505.16800", "title": "Learning Beyond Limits: Multitask Learning and Synthetic Data for Low-Resource Canonical Morpheme Segmentation", "authors": ["Changbing Yang", "Garrett Nicolai"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce a transformer-based morpheme segmentation system that augments a\nlow-resource training signal through multitask learning and LLM-generated\nsynthetic data. Our framework jointly predicts morphological segments and\nglosses from orthographic input, leveraging shared linguistic representations\nobtained through a common documentary process to enhance model generalization.\nTo further address data scarcity, we integrate synthetic training data\ngenerated by large language models (LLMs) using in-context learning.\nExperimental results on the SIGMORPHON 2023 dataset show that our approach\nsignificantly improves word-level segmentation accuracy and morpheme-level\nF1-score across multiple low-resource languages."}
{"id": "2505.16806", "pdf": "https://arxiv.org/pdf/2505.16806.pdf", "abs": "https://arxiv.org/abs/2505.16806", "title": "Two-way Evidence self-Alignment based Dual-Gated Reasoning Enhancement", "authors": ["Kexin Zhang", "Junlan Chen", "Daifeng Li", "Yuxuan Zhang", "Yangyang Feng", "Bowen Deng", "Weixu Chen"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Large language models (LLMs) encounter difficulties in knowledge-intensive\nmulti-step reasoning (KIMSR) tasks. One challenge is how to effectively extract\nand represent rationale evidence. The current methods often extract\nsemantically relevant but logically irrelevant evidence, resulting in flawed\nreasoning and inaccurate responses. We propose a two-way evidence\nself-alignment (TW-ESA) module, which utilizes the mutual alignment between\nstrict reasoning and LLM reasoning to enhance its understanding of the causal\nlogic of evidence, thereby addressing the first challenge. Another challenge is\nhow to utilize the rationale evidence and LLM's intrinsic knowledge for\naccurate reasoning when the evidence contains uncertainty. We propose a\ndual-gated reasoning enhancement (DGR) module to gradually fuse useful\nknowledge of LLM within strict reasoning, which can enable the model to perform\naccurate reasoning by focusing on causal elements in the evidence and exhibit\ngreater robustness. The two modules are collaboratively trained in a unified\nframework ESA-DGR. Extensive experiments on three diverse and challenging KIMSR\ndatasets reveal that ESA-DGR significantly surpasses state-of-the-art LLM-based\nfine-tuning methods, with remarkable average improvements of 4% in exact match\n(EM) and 5% in F1 score. The implementation code is available at\nhttps://anonymous.4open.science/r/ESA-DGR-2BF8."}
{"id": "2505.16814", "pdf": "https://arxiv.org/pdf/2505.16814.pdf", "abs": "https://arxiv.org/abs/2505.16814", "title": "Does Synthetic Data Help Named Entity Recognition for Low-Resource Languages?", "authors": ["Gaurav Kamath", "Sowmya Vajjala"], "categories": ["cs.CL"], "comment": "pre-print", "summary": "Named Entity Recognition(NER) for low-resource languages aims to produce\nrobust systems for languages where there is limited labeled training data\navailable, and has been an area of increasing interest within NLP. Data\naugmentation for increasing the amount of low-resource labeled data is a common\npractice. In this paper, we explore the role of synthetic data in the context\nof multilingual, low-resource NER, considering 11 languages from diverse\nlanguage families. Our results suggest that synthetic data does in fact hold\npromise for low-resource language NER, though we see significant variation\nbetween languages."}
{"id": "2505.16831", "pdf": "https://arxiv.org/pdf/2505.16831.pdf", "abs": "https://arxiv.org/abs/2505.16831", "title": "Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs", "authors": ["Xiaoyu Xu", "Xiang Yue", "Yang Liu", "Qingqing Ye", "Haibo Hu", "Minxin Du"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "44 pages", "summary": "Unlearning in large language models (LLMs) is intended to remove the\ninfluence of specific data, yet current evaluations rely heavily on token-level\nmetrics such as accuracy and perplexity. We show that these metrics can be\nmisleading: models often appear to forget, but their original behavior can be\nrapidly restored with minimal fine-tuning, revealing that unlearning may\nobscure information rather than erase it. To diagnose this phenomenon, we\nintroduce a representation-level evaluation framework using PCA-based\nsimilarity and shift, centered kernel alignment, and Fisher information.\nApplying this toolkit across six unlearning methods, three domains (text, code,\nmath), and two open-source LLMs, we uncover a critical distinction between\nreversible and irreversible forgetting. In reversible cases, models suffer\ntoken-level collapse yet retain latent features; in irreversible cases, deeper\nrepresentational damage occurs. We further provide a theoretical account\nlinking shallow weight perturbations near output layers to misleading\nunlearning signals, and show that reversibility is modulated by task type and\nhyperparameters. Our findings reveal a fundamental gap in current evaluation\npractices and establish a new diagnostic foundation for trustworthy unlearning\nin LLMs. We provide a unified toolkit for analyzing LLM representation changes\nunder unlearning and relearning:\nhttps://github.com/XiaoyuXU1/Representational_Analysis_Tools.git."}
{"id": "2505.16834", "pdf": "https://arxiv.org/pdf/2505.16834.pdf", "abs": "https://arxiv.org/abs/2505.16834", "title": "SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis", "authors": ["Shuang Sun", "Huatong Song", "Yuhao Wang", "Ruiyang Ren", "Jinhao Jiang", "Junjie Zhang", "Fei Bai", "Jia Deng", "Wayne Xin Zhao", "Zheng Liu", "Lei Fang", "Zhongyuan Wang", "Ji-Rong Wen"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Retrieval-augmented generation (RAG) systems have advanced large language\nmodels (LLMs) in complex deep search scenarios requiring multi-step reasoning\nand iterative information retrieval. However, existing approaches face critical\nlimitations that lack high-quality training trajectories or suffer from the\ndistributional mismatches in simulated environments and prohibitive\ncomputational costs for real-world deployment. This paper introduces\nSimpleDeepSearcher, a lightweight yet effective framework that bridges this gap\nthrough strategic data engineering rather than complex training paradigms. Our\napproach synthesizes high-quality training data by simulating realistic user\ninteractions in live web search environments, coupled with a multi-criteria\ncuration strategy that optimizes the diversity and quality of input and output\nside. Experiments on five benchmarks across diverse domains demonstrate that\nSFT on only 871 curated samples yields significant improvements over RL-based\nbaselines. Our work establishes SFT as a viable pathway by systematically\naddressing the data-scarce bottleneck, offering practical insights for\nefficient deep search systems. Our code is available at\nhttps://github.com/RUCAIBox/SimpleDeepSearcher."}
{"id": "2505.16838", "pdf": "https://arxiv.org/pdf/2505.16838.pdf", "abs": "https://arxiv.org/abs/2505.16838", "title": "R1-Compress: Long Chain-of-Thought Compression via Chunk Compression and Search", "authors": ["Yibo Wang", "Li Shen", "Huanjin Yao", "Tiansheng Huang", "Rui Liu", "Naiqiang Tan", "Jiaxing Huang", "Kai Zhang", "Dacheng Tao"], "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-Thought (CoT) reasoning enhances large language models (LLMs) by\nenabling step-by-step problem-solving, yet its extension to Long-CoT introduces\nsubstantial computational overhead due to increased token length. Existing\ncompression approaches -- instance-level and token-level -- either sacrifice\nessential local reasoning signals like reflection or yield incoherent outputs.\nTo address these limitations, we propose R1-Compress, a two-stage chunk-level\ncompression framework that preserves both local information and coherence. Our\nmethod segments Long-CoT into manageable chunks, applies LLM-driven inner-chunk\ncompression, and employs an inter-chunk search mechanism to select the short\nand coherent sequence. Experiments on Qwen2.5-Instruct models across MATH500,\nAIME24, and GPQA-Diamond demonstrate that R1-Compress significantly reduces\ntoken usage while maintaining comparable reasoning accuracy. On MATH500,\nR1-Compress achieves an accuracy of 92.4%, with only a 0.6% drop compared to\nthe Long-CoT baseline, while reducing token usage by about 20%. Source code\nwill be available at https://github.com/w-yibo/R1-Compress"}
{"id": "2505.16847", "pdf": "https://arxiv.org/pdf/2505.16847.pdf", "abs": "https://arxiv.org/abs/2505.16847", "title": "Understanding and Analyzing Inappropriately Targeting Language in Online Discourse: A Comparative Annotation Study", "authors": ["Baran Barbarestani", "Isa Maks", "Piek Vossen"], "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces a method for detecting inappropriately targeting\nlanguage in online conversations by integrating crowd and expert annotations\nwith ChatGPT. We focus on English conversation threads from Reddit, examining\ncomments that target individuals or groups. Our approach involves a\ncomprehensive annotation framework that labels a diverse data set for various\ntarget categories and specific target words within the conversational context.\nWe perform a comparative analysis of annotations from human experts, crowd\nannotators, and ChatGPT, revealing strengths and limitations of each method in\nrecognizing both explicit hate speech and subtler discriminatory language. Our\nfindings highlight the significant role of contextual factors in identifying\nhate speech and uncover new categories of targeting, such as social belief and\nbody image. We also address the challenges and subjective judgments involved in\nannotation and the limitations of ChatGPT in grasping nuanced language. This\nstudy provides insights for improving automated content moderation strategies\nto enhance online safety and inclusivity."}
{"id": "2505.16855", "pdf": "https://arxiv.org/pdf/2505.16855.pdf", "abs": "https://arxiv.org/abs/2505.16855", "title": "Nested Named Entity Recognition as Single-Pass Sequence Labeling", "authors": ["Alberto Muñoz-Ortiz", "David Vilares", "Caio COrro", "Carlos Gómez-Rodríguez"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "Submitted to EMNLP 2025", "summary": "We cast nested named entity recognition (NNER) as a sequence labeling task by\nleveraging prior work that linearizes constituency structures, effectively\nreducing the complexity of this structured prediction problem to\nstraightforward token classification. By combining these constituency\nlinearizations with pretrained encoders, our method captures nested entities\nwhile performing exactly $n$ tagging actions. Our approach achieves competitive\nperformance compared to less efficient systems, and it can be trained using any\noff-the-shelf sequence labeling library."}
{"id": "2505.16868", "pdf": "https://arxiv.org/pdf/2505.16868.pdf", "abs": "https://arxiv.org/abs/2505.16868", "title": "Comparative analysis of subword tokenization approaches for Indian languages", "authors": ["Sudhansu Bala Das", "Samujjal Choudhury", "Tapas Kumar Mishra", "Bidyut Kr. Patra"], "categories": ["cs.CL"], "comment": "24 pages, 4 tables", "summary": "Tokenization is the act of breaking down text into smaller parts, or tokens,\nthat are easier for machines to process. This is a key phase in machine\ntranslation (MT) models. Subword tokenization enhances this process by breaking\ndown words into smaller subword units, which is especially beneficial in\nlanguages with complicated morphology or a vast vocabulary. It is useful in\ncapturing the intricate structure of words in Indian languages (ILs), such as\nprefixes, suffixes, and other morphological variations. These languages\nfrequently use agglutinative structures, in which words are formed by the\ncombination of multiple morphemes such as suffixes, prefixes, and stems. As a\nresult, a suitable tokenization strategy must be chosen to address these\nscenarios. This paper examines how different subword tokenization techniques,\nsuch as SentencePiece, Byte Pair Encoding (BPE), and WordPiece Tokenization,\naffect ILs. The effectiveness of these subword tokenization techniques is\ninvestigated in statistical, neural, and multilingual neural machine\ntranslation models. All models are examined using standard evaluation metrics,\nsuch as the Bilingual Evaluation Understudy (BLEU) score, TER, METEOR, CHRF,\nRIBES, and COMET. Based on the results, it appears that for the majority of\nlanguage pairs for the Statistical and Neural MT models, the SentencePiece\ntokenizer continuously performed better than other tokenizers in terms of BLEU\nscore. However, BPE tokenization outperformed other tokenization techniques in\nthe context of Multilingual Neural Machine Translation model. The results show\nthat, despite using the same tokenizer and dataset for each model, translations\nfrom ILs to English surpassed translations from English to ILs."}
{"id": "2505.16869", "pdf": "https://arxiv.org/pdf/2505.16869.pdf", "abs": "https://arxiv.org/abs/2505.16869", "title": "MPO: Multilingual Safety Alignment via Reward Gap Optimization", "authors": ["Weixiang Zhao", "Yulin Hu", "Yang Deng", "Tongtong Wu", "Wenxuan Zhang", "Jiahe Guo", "An Zhang", "Yanyan Zhao", "Bing Qin", "Tat-Seng Chua", "Ting Liu"], "categories": ["cs.CL"], "comment": "To Appear at ACL 2025 (Main)", "summary": "Large language models (LLMs) have become increasingly central to AI\napplications worldwide, necessitating robust multilingual safety alignment to\nensure secure deployment across diverse linguistic contexts. Existing\npreference learning methods for safety alignment, such as RLHF and DPO, are\nprimarily monolingual and struggle with noisy multilingual data. To address\nthese limitations, we introduce Multilingual reward gaP Optimization (MPO), a\nnovel approach that leverages the well-aligned safety capabilities of the\ndominant language (English) to improve safety alignment across multiple\nlanguages. MPO directly minimizes the reward gap difference between the\ndominant language and target languages, effectively transferring safety\ncapabilities while preserving the original strengths of the dominant language.\nExtensive experiments on three LLMs, LLaMA-3.1, Gemma-2 and Qwen2.5, validate\nMPO's efficacy in multilingual safety alignment without degrading general\nmultilingual utility."}
{"id": "2505.16881", "pdf": "https://arxiv.org/pdf/2505.16881.pdf", "abs": "https://arxiv.org/abs/2505.16881", "title": "CASTILLO: Characterizing Response Length Distributions of Large Language Models", "authors": ["Daniel F. Perez-Ramirez", "Dejan Kostic", "Magnus Boman"], "categories": ["cs.CL", "cs.AI"], "comment": "Dataset available in\n  https://huggingface.co/datasets/danfperam/castillo and code is available in\n  https://github.com/DanielFPerez/castillo", "summary": "Efficiently managing compute resources for Large Language Model (LLM)\ninference remains challenging due to the inherently stochastic and variable\nlengths of autoregressive text generation. Accurately estimating response\nlengths in advance enables proactive resource allocation, yet existing\napproaches either bias text generation towards certain lengths or rely on\nassumptions that ignore model- and prompt-specific variability. We introduce\nCASTILLO, a dataset characterizing response length distributions across 13\nwidely-used open-source LLMs evaluated on seven distinct instruction-following\ncorpora. For each $\\langle$prompt, model$\\rangle$ sample pair, we generate 10\nindependent completions using fixed decoding hyper-parameters, record the token\nlength of each response, and publish summary statistics (mean, std-dev,\npercentiles), along with the shortest and longest completions, and the exact\ngeneration settings. Our analysis reveals significant inter- and intra-model\nvariability in response lengths (even under identical generation settings), as\nwell as model-specific behaviors and occurrences of partial text degeneration\nin only subsets of responses. CASTILLO enables the development of predictive\nmodels for proactive scheduling and provides a systematic framework for\nanalyzing model-specific generation behaviors. We publicly release the dataset\nand code to foster research at the intersection of generative language modeling\nand systems."}
{"id": "2505.16894", "pdf": "https://arxiv.org/pdf/2505.16894.pdf", "abs": "https://arxiv.org/abs/2505.16894", "title": "Shadows in the Attention: Contextual Perturbation and Representation Drift in the Dynamics of Hallucination in LLMs", "authors": ["Zeyu Wei", "Shuo Wang", "Xiaohui Rong", "Xuemin Liu", "He Li"], "categories": ["cs.CL"], "comment": null, "summary": "Hallucinations -- plausible yet erroneous outputs -- remain a critical\nbarrier to reliable deployment of large language models (LLMs). We present the\nfirst systematic study linking hallucination incidence to internal-state drift\ninduced by incremental context injection. Using TruthfulQA, we construct two\n16-round \"titration\" tracks per question: one appends relevant but partially\nflawed snippets, the other injects deliberately misleading content. Across six\nopen-source LLMs, we track overt hallucination rates with a tri-perspective\ndetector and covert dynamics via cosine, entropy, JS and Spearman drifts of\nhidden states and attention maps. Results reveal (1) monotonic growth of\nhallucination frequency and representation drift that plateaus after 5--7\nrounds; (2) relevant context drives deeper semantic assimilation, producing\nhigh-confidence \"self-consistent\" hallucinations, whereas irrelevant context\ninduces topic-drift errors anchored by attention re-routing; and (3)\nconvergence of JS-Drift ($\\sim0.69$) and Spearman-Drift ($\\sim0$) marks an\n\"attention-locking\" threshold beyond which hallucinations solidify and become\nresistant to correction. Correlation analyses expose a seesaw between\nassimilation capacity and attention diffusion, clarifying size-dependent error\nmodes. These findings supply empirical foundations for intrinsic hallucination\nprediction and context-aware mitigation mechanisms."}
{"id": "2505.16900", "pdf": "https://arxiv.org/pdf/2505.16900.pdf", "abs": "https://arxiv.org/abs/2505.16900", "title": "Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality", "authors": ["Jintian Shao", "Hongyi Huang", "Jiayi Wu", "Beiwen Zhang", "ZhiYu Wu", "You Shan", "MingKai Zheng"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "During the finetuning stage of text generation tasks, standard cross-entropy\nloss treats all tokens equally. This can lead models to overemphasize\nhigh-frequency, low-information tokens, neglecting lower-frequency tokens\ncrucial for specificity and informativeness in generated content. This paper\nintroduces a novel loss function, Power-Law Decay Loss (PDL), specifically\ndesigned to optimize the finetuning process for text generation. The core\nmotivation for PDL stems from observations in information theory and\nlinguistics: the informativeness of a token is often inversely proportional to\nits frequency of occurrence. PDL re-weights the contribution of each token in\nthe standard cross-entropy loss based on its frequency in the training corpus,\nfollowing a power-law decay. Specifically, the weights for high-frequency\ntokens are reduced, while low-frequency, information-dense tokens are assigned\nhigher weights. This mechanism guides the model during finetuning to focus more\non learning and generating tokens that convey specific and unique information,\nthereby enhancing the quality, diversity, and informativeness of the generated\ntext. We theoretically elaborate on the motivation and construction of PDL and\ndiscuss its potential applications and advantages across various text\ngeneration finetuning tasks, such as abstractive summarization, dialogue\nsystems, and style transfer."}
{"id": "2505.16922", "pdf": "https://arxiv.org/pdf/2505.16922.pdf", "abs": "https://arxiv.org/abs/2505.16922", "title": "UNCLE: Uncertainty Expressions in Long-Form Generation", "authors": ["Ruihan Yang", "Caiqi Zhang", "Zhisong Zhang", "Xinting Huang", "Dong Yu", "Nigel Collier", "Deqing Yang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are prone to hallucination, particularly in\nlong-form generations. A promising direction to mitigate hallucination is to\nteach LLMs to express uncertainty explicitly when they lack sufficient\nknowledge. However, existing work lacks direct and fair evaluation of LLMs'\nability to express uncertainty effectively in long-form generation. To address\nthis gap, we first introduce UNCLE, a benchmark designed to evaluate\nuncertainty expression in both long- and short-form question answering (QA).\nUNCLE spans five domains and comprises 4k long-form QA instances and over 20k\nshort-form QA pairs. Our dataset is the first to directly bridge short- and\nlong-form QA with paired questions and gold-standard answers. Along with the\nbenchmark, we propose a suite of new metrics to assess the models' capabilities\nto selectively express uncertainty. Using UNCLE, we then demonstrate that\ncurrent models fail to convey uncertainty appropriately in long-form\ngeneration. We further explore both prompt-based and training-based methods to\nimprove models' performance, with the training-based methods yielding greater\ngains. Further analysis of alignment gaps between short- and long-form\nuncertainty expression highlights promising directions for future research\nusing UNCLE."}
{"id": "2505.16927", "pdf": "https://arxiv.org/pdf/2505.16927.pdf", "abs": "https://arxiv.org/abs/2505.16927", "title": "Latent Principle Discovery for Language Model Self-Improvement", "authors": ["Keshav Ramji", "Tahira Naseem", "Ramón Fernandez Astudillo"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "When language model (LM) users aim to improve the quality of its generations,\nit is crucial to specify concrete behavioral attributes that the model should\nstrive to reflect. However, curating such principles across many domains, even\nnon-exhaustively, requires a labor-intensive annotation process. To automate\nthis process, we propose eliciting these latent attributes guiding model\nreasoning towards human-preferred responses by explicitly modeling them in a\nself-correction setting. Our approach mines new principles from the LM itself\nand compresses the discovered elements to an interpretable set via clustering.\nSpecifically, we employ an approximation of posterior-regularized Monte Carlo\nExpectation-Maximization to both identify a condensed set of the most effective\nlatent principles and teach the LM to strategically invoke them in order to\nintrinsically refine its responses. We demonstrate that bootstrapping our\nalgorithm over multiple iterations enables smaller language models (7-8B\nparameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an\naverage of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on\nIFEval. We also show that clustering the principles yields interpretable and\ndiverse model-generated constitutions while retaining model performance. The\ngains our method achieves highlight the potential of automated,\nprinciple-driven post-training recipes toward continual self-improvement."}
{"id": "2505.16931", "pdf": "https://arxiv.org/pdf/2505.16931.pdf", "abs": "https://arxiv.org/abs/2505.16931", "title": "PIIvot: A Lightweight NLP Anonymization Framework for Question-Anchored Tutoring Dialogues", "authors": ["Matthew Zent", "Digory Smith", "Simon Woodhead"], "categories": ["cs.CL"], "comment": "6 pages, 2 figures, submitted to EMNLP 2025, for associated dataset,\n  see\n  https://huggingface.co/datasets/Eedi/Question-Anchored-Tutoring-Dialogues-2k", "summary": "Personally identifiable information (PII) anonymization is a high-stakes task\nthat poses a barrier to many open-science data sharing initiatives. While PII\nidentification has made large strides in recent years, in practice, error\nthresholds and the recall/precision trade-off still limit the uptake of these\nanonymization pipelines. We present PIIvot, a lighter-weight framework for PII\nanonymization that leverages knowledge of the data context to simplify the PII\ndetection problem. To demonstrate its effectiveness, we also contribute\nQATD-2k, the largest open-source real-world tutoring dataset of its kind, to\nsupport the demand for quality educational dialogue data."}
{"id": "2505.16934", "pdf": "https://arxiv.org/pdf/2505.16934.pdf", "abs": "https://arxiv.org/abs/2505.16934", "title": "In-Context Watermarks for Large Language Models", "authors": ["Yepeng Liu", "Xuandong Zhao", "Christopher Kruegel", "Dawn Song", "Yuheng Bu"], "categories": ["cs.CL"], "comment": null, "summary": "The growing use of large language models (LLMs) for sensitive applications\nhas highlighted the need for effective watermarking techniques to ensure the\nprovenance and accountability of AI-generated text. However, most existing\nwatermarking methods require access to the decoding process, limiting their\napplicability in real-world settings. One illustrative example is the use of\nLLMs by dishonest reviewers in the context of academic peer review, where\nconference organizers have no access to the model used but still need to detect\nAI-generated reviews. Motivated by this gap, we introduce In-Context\nWatermarking (ICW), which embeds watermarks into generated text solely through\nprompt engineering, leveraging LLMs' in-context learning and\ninstruction-following abilities. We investigate four ICW strategies at\ndifferent levels of granularity, each paired with a tailored detection method.\nWe further examine the Indirect Prompt Injection (IPI) setting as a specific\ncase study, in which watermarking is covertly triggered by modifying input\ndocuments such as academic manuscripts. Our experiments validate the\nfeasibility of ICW as a model-agnostic, practical watermarking approach.\nMoreover, our findings suggest that as LLMs become more capable, ICW offers a\npromising direction for scalable and accessible content attribution."}
{"id": "2505.16956", "pdf": "https://arxiv.org/pdf/2505.16956.pdf", "abs": "https://arxiv.org/abs/2505.16956", "title": "On Multilingual Encoder Language Model Compression for Low-Resource Languages", "authors": ["Daniil Gurgurov", "Michal Gregor", "Josef van Genabith", "Simon Ostermann"], "categories": ["cs.CL"], "comment": "Pre-print", "summary": "In this paper, we combine two-step knowledge distillation, structured\npruning, truncation, and vocabulary trimming for extremely compressing\nmultilingual encoder-only language models for low-resource languages. Our novel\napproach systematically combines existing techniques and takes them to the\nextreme, reducing layer depth, feed-forward hidden size, and intermediate layer\nembedding size to create significantly smaller monolingual models while\nretaining essential language-specific knowledge. We achieve compression rates\nof up to 92% with only a marginal performance drop of 2-10% in four downstream\ntasks, including sentiment analysis, topic classification, named entity\nrecognition, and part-of-speech tagging, across three low-resource languages.\nNotably, the performance degradation correlates with the amount of\nlanguage-specific data in the teacher model, with larger datasets resulting in\nsmaller performance losses. Additionally, we conduct extensive ablation studies\nto identify best practices for multilingual model compression using these\ntechniques."}
{"id": "2505.16965", "pdf": "https://arxiv.org/pdf/2505.16965.pdf", "abs": "https://arxiv.org/abs/2505.16965", "title": "BP-Seg: A graphical model approach to unsupervised and non-contiguous text segmentation using belief propagation", "authors": ["Fengyi Li", "Kayhan Behdin", "Natesh Pillai", "Xiaofeng Wang", "Zhipeng Wang", "Ercan Yildiz"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Text segmentation based on the semantic meaning of sentences is a fundamental\ntask with broad utility in many downstream applications. In this paper, we\npropose a graphical model-based unsupervised learning approach, named BP-Seg\nfor efficient text segmentation. Our method not only considers local coherence,\ncapturing the intuition that adjacent sentences are often more related, but\nalso effectively groups sentences that are distant in the text yet semantically\nsimilar. This is achieved through belief propagation on the carefully\nconstructed graphical models. Experimental results on both an illustrative\nexample and a dataset with long-form documents demonstrate that our method\nperforms favorably compared to competing approaches."}
{"id": "2505.16972", "pdf": "https://arxiv.org/pdf/2505.16972.pdf", "abs": "https://arxiv.org/abs/2505.16972", "title": "From Tens of Hours to Tens of Thousands: Scaling Back-Translation for Speech Recognition", "authors": ["Tianduo Wang", "Lu Xu", "Wei Lu", "Shanbo Cheng"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Recent advances in Automatic Speech Recognition (ASR) have been largely\nfueled by massive speech corpora. However, extending coverage to diverse\nlanguages with limited resources remains a formidable challenge. This paper\nintroduces Speech Back-Translation, a scalable pipeline that improves\nmultilingual ASR models by converting large-scale text corpora into synthetic\nspeech via off-the-shelf text-to-speech (TTS) models. We demonstrate that just\ntens of hours of real transcribed speech can effectively train TTS models to\ngenerate synthetic speech at hundreds of times the original volume while\nmaintaining high quality. To evaluate synthetic speech quality, we develop an\nintelligibility-based assessment framework and establish clear thresholds for\nwhen synthetic data benefits ASR training. Using Speech Back-Translation, we\ngenerate more than 500,000 hours of synthetic speech in ten languages and\ncontinue pre-training Whisper-large-v3, achieving average transcription error\nreductions of over 30\\%. These results highlight the scalability and\neffectiveness of Speech Back-Translation for enhancing multilingual ASR\nsystems."}
{"id": "2505.16973", "pdf": "https://arxiv.org/pdf/2505.16973.pdf", "abs": "https://arxiv.org/abs/2505.16973", "title": "VeriFastScore: Speeding up long-form factuality evaluation", "authors": ["Rishanth Rajendhran", "Amir Zadeh", "Matthew Sarte", "Chuan Li", "Mohit Iyyer"], "categories": ["cs.CL"], "comment": null, "summary": "Metrics like FactScore and VeriScore that evaluate long-form factuality\noperate by decomposing an input response into atomic claims and then\nindividually verifying each claim. While effective and interpretable, these\nmethods incur numerous LLM calls and can take upwards of 100 seconds to\nevaluate a single response, limiting their practicality in large-scale\nevaluation and training scenarios. To address this, we propose VeriFastScore,\nwhich leverages synthetic data to fine-tune Llama3.1 8B for simultaneously\nextracting and verifying all verifiable claims within a given text based on\nevidence from Google Search. We show that this task cannot be solved via\nfew-shot prompting with closed LLMs due to its complexity: the model receives\n~4K tokens of evidence on average and needs to concurrently decompose claims,\njudge their verifiability, and verify them against noisy evidence. However, our\nfine-tuned VeriFastScore model demonstrates strong correlation with the\noriginal VeriScore pipeline at both the example level (r=0.80) and system level\n(r=0.94) while achieving an overall speedup of 6.6x (9.9x excluding evidence\nretrieval) over VeriScore. To facilitate future factuality research, we\npublicly release our VeriFastScore model and synthetic datasets."}
{"id": "2505.16983", "pdf": "https://arxiv.org/pdf/2505.16983.pdf", "abs": "https://arxiv.org/abs/2505.16983", "title": "LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding", "authors": ["Junlong Tong", "Jinlan Fu", "Zixuan Lin", "Yingqi Fan", "Anhao Zhao", "Hui Su", "Xiaoyu Shen"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Large Language Models (LLMs) are primarily designed for batch processing.\nExisting methods for adapting LLMs to streaming rely either on expensive\nre-encoding or specialized architectures with limited scalability. This work\nidentifies three key mismatches in adapting batch-oriented LLMs to streaming:\n(1) input-attention, (2) output-attention, and (3) position-ID mismatches.\nWhile it is commonly assumed that the latter two mismatches require frequent\nre-encoding, our analysis reveals that only the input-attention mismatch\nsignificantly impacts performance, indicating re-encoding outputs is largely\nunnecessary. To better understand this discrepancy with the common assumption,\nwe provide the first comprehensive analysis of the impact of position encoding\non LLMs in streaming, showing that preserving relative positions within source\nand target contexts is more critical than maintaining absolute order. Motivated\nby the above analysis, we introduce a group position encoding paradigm built on\nbatch architectures to enhance consistency between streaming and batch modes.\nExtensive experiments on cross-lingual and cross-modal tasks demonstrate that\nour method outperforms existing approaches. Our method requires no\narchitectural modifications, exhibits strong generalization in both streaming\nand batch modes. The code is available at repository\nhttps://github.com/EIT-NLP/StreamingLLM."}
{"id": "2505.16986", "pdf": "https://arxiv.org/pdf/2505.16986.pdf", "abs": "https://arxiv.org/abs/2505.16986", "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning", "authors": ["Amartya Chakraborty", "Paresh Dashore", "Nadia Bathaee", "Anmol Jain", "Anirban Das", "Shi-Xiong Zhang", "Sambit Sahu", "Milind Naphade", "Genta Indra Winata"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-source language models. We present results powered by\nT1-Agent, highlighting their ability to plan and reason in complex,\ntool-dependent scenarios."}
{"id": "2505.16988", "pdf": "https://arxiv.org/pdf/2505.16988.pdf", "abs": "https://arxiv.org/abs/2505.16988", "title": "MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent Systems", "authors": ["Rui Ye", "Keduan Huang", "Qimin Wu", "Yuzhu Cai", "Tian Jin", "Xianghe Pang", "Xiangrui Liu", "Jiaqi Su", "Chen Qian", "Bohan Tang", "Kaiqu Liang", "Jiaao Chen", "Yue Hu", "Zhenfei Yin", "Rongye Shi", "Bo An", "Yang Gao", "Wenjun Wu", "Lei Bai", "Siheng Chen"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "18 pages, 11 figures", "summary": "LLM-based multi-agent systems (MAS) have demonstrated significant potential\nin enhancing single LLMs to address complex and diverse tasks in practical\napplications. Despite considerable advancements, the field lacks a unified\ncodebase that consolidates existing methods, resulting in redundant\nre-implementation efforts, unfair comparisons, and high entry barriers for\nresearchers. To address these challenges, we introduce MASLab, a unified,\ncomprehensive, and research-friendly codebase for LLM-based MAS. (1) MASLab\nintegrates over 20 established methods across multiple domains, each rigorously\nvalidated by comparing step-by-step outputs with its official implementation.\n(2) MASLab provides a unified environment with various benchmarks for fair\ncomparisons among methods, ensuring consistent inputs and standardized\nevaluation protocols. (3) MASLab implements methods within a shared streamlined\nstructure, lowering the barriers for understanding and extension. Building on\nMASLab, we conduct extensive experiments covering 10+ benchmarks and 8 models,\noffering researchers a clear and comprehensive view of the current landscape of\nMAS methods. MASLab will continue to evolve, tracking the latest developments\nin the field, and invite contributions from the broader open-source community."}
{"id": "2505.16995", "pdf": "https://arxiv.org/pdf/2505.16995.pdf", "abs": "https://arxiv.org/abs/2505.16995", "title": "DecoupledESC: Enhancing Emotional Support Generation via Strategy-Response Decoupled Preference Optimization", "authors": ["Chao Zhang", "Xin Shi", "Xueqiao Zhang", "Yifan Zhu", "Yi Yang", "Yawei Luo"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in Emotional Support Conversation (ESC) have improved\nemotional support generation by fine-tuning Large Language Models (LLMs) via\nSupervised Fine-Tuning (SFT). However, common psychological errors still\npersist. While Direct Preference Optimization (DPO) shows promise in reducing\nsuch errors through pairwise preference learning, its effectiveness in ESC\ntasks is limited by two key challenges: (1) Entangled data structure: Existing\nESC data inherently entangles psychological strategies and response content,\nmaking it difficult to construct high-quality preference pairs; and (2)\nOptimization ambiguity: Applying vanilla DPO to such entangled pairwise data\nleads to ambiguous training objectives. To address these issues, we introduce\nInferential Preference Mining (IPM) to construct high-quality preference data,\nforming the IPM-PrefDial dataset. Building upon this data, we propose a\nDecoupled ESC framework inspired by Gross's Extended Process Model of Emotion\nRegulation, which decomposes the ESC task into two sequential subtasks:\nstrategy planning and empathic response generation. Each was trained via SFT\nand subsequently enhanced by DPO to align with the psychological preference.\nExtensive experiments demonstrate that our Decoupled ESC framework outperforms\njoint optimization baselines, reducing preference bias and improving response\nquality."}
{"id": "2505.16998", "pdf": "https://arxiv.org/pdf/2505.16998.pdf", "abs": "https://arxiv.org/abs/2505.16998", "title": "Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?", "authors": ["Jin Jiang", "Jianing Wang", "Yuchen Yan", "Yang Liu", "Jianhua Zhu", "Mengdi Zhang", "Xunliang Cai", "Liangcai Gao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have been shown to achieve breakthrough\nperformance on complex logical reasoning tasks. Nevertheless, most existing\nresearch focuses on employing formal language to guide LLMs to derive reliable\nreasoning paths, while systematic evaluations of these capabilities are still\nlimited. In this paper, we aim to conduct a comprehensive evaluation of LLMs\nacross various logical reasoning problems utilizing formal languages. From the\nperspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and\nformat of trajectories, our key findings are: 1) Thinking models significantly\noutperform Instruct models, especially when formal language is employed; 2) All\nLLMs exhibit limitations in inductive reasoning capability, irrespective of\nwhether they use a formal language; 3) Data with PoT format achieves the best\ngeneralization performance across other languages. Additionally, we also curate\nthe formal-relative training data to further enhance the small language models,\nand the experimental results indicate that a simple rejected fine-tuning method\ncan better enable LLMs to generalize across formal languages and achieve the\nbest overall performance. Our codes and reports are available at\nhttps://github.com/jiangjin1999/FormalEval."}
{"id": "2505.17005", "pdf": "https://arxiv.org/pdf/2505.17005.pdf", "abs": "https://arxiv.org/abs/2505.17005", "title": "R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning", "authors": ["Huatong Song", "Jinhao Jiang", "Wenqing Tian", "Zhipeng Chen", "Yuhuan Wu", "Jiahao Zhao", "Yingqian Min", "Wayne Xin Zhao", "Lei Fang", "Ji-Rong Wen"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) are powerful but prone to hallucinations due to\nstatic knowledge. Retrieval-Augmented Generation (RAG) helps by injecting\nexternal information, but current methods often are costly, generalize poorly,\nor ignore the internal knowledge of the model. In this paper, we introduce\nR1-Searcher++, a novel framework designed to train LLMs to adaptively leverage\nboth internal and external knowledge sources. R1-Searcher++ employs a two-stage\ntraining strategy: an initial SFT Cold-start phase for preliminary format\nlearning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses\noutcome-supervision to encourage exploration, incorporates a reward mechanism\nfor internal knowledge utilization, and integrates a memorization mechanism to\ncontinuously assimilate retrieved information, thereby enriching the model's\ninternal knowledge. By leveraging internal knowledge and external search\nengine, the model continuously improves its capabilities, enabling efficient\nretrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++\noutperforms previous RAG and reasoning methods and achieves efficient\nretrieval. The code is available at\nhttps://github.com/RUCAIBox/R1-Searcher-plus."}
{"id": "2505.15872", "pdf": "https://arxiv.org/pdf/2505.15872.pdf", "abs": "https://arxiv.org/abs/2505.15872", "title": "InfoDeepSeek: Benchmarking Agentic Information Seeking for Retrieval-Augmented Generation", "authors": ["Yunjia Xi", "Jianghao Lin", "Menghui Zhu", "Yongzhao Xiao", "Zhuoying Ou", "Jiaqi Liu", "Tong Wan", "Bo Chen", "Weiwen Liu", "Yasheng Wang", "Ruiming Tang", "Weinan Zhang", "Yong Yu"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\ngrounding responses with retrieved information. As an emerging paradigm,\nAgentic RAG further enhances this process by introducing autonomous LLM agents\ninto the information seeking process. However, existing benchmarks fall short\nin evaluating such systems, as they are confined to a static retrieval\nenvironment with a fixed, limited corpus} and simple queries that fail to\nelicit agentic behavior. Moreover, their evaluation protocols assess\ninformation seeking effectiveness by pre-defined gold sets of documents, making\nthem unsuitable for the open-ended and dynamic nature of real-world web\nenvironments. To bridge this gap, we present InfoDeepSeek, a new benchmark with\nchallenging questions designed for assessing agentic information seeking in\nreal-world, dynamic web environments. We propose a systematic methodology for\nconstructing challenging queries satisfying the criteria of determinacy,\ndifficulty, and diversity. Based on this, we develop the first evaluation\nframework tailored to dynamic agentic information seeking, including\nfine-grained metrics about the accuracy, utility, and compactness of\ninformation seeking outcomes. Through extensive experiments across LLMs, search\nengines, and question types, InfoDeepSeek reveals nuanced agent behaviors and\noffers actionable insights for future research."}
{"id": "2505.15877", "pdf": "https://arxiv.org/pdf/2505.15877.pdf", "abs": "https://arxiv.org/abs/2505.15877", "title": "Highlighting What Matters: Promptable Embeddings for Attribute-Focused Image Retrieval", "authors": ["Siting Li", "Xiang Gao", "Simon Shaolei Du"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "25 pages, 5 figures", "summary": "While an image is worth more than a thousand words, only a few provide\ncrucial information for a given task and thus should be focused on. In light of\nthis, ideal text-to-image (T2I) retrievers should prioritize specific visual\nattributes relevant to queries. To evaluate current retrievers on handling\nattribute-focused queries, we build COCO-Facet, a COCO-based benchmark with\n9,112 queries about diverse attributes of interest. We find that CLIP-like\nretrievers, which are widely adopted due to their efficiency and zero-shot\nability, have poor and imbalanced performance, possibly because their image\nembeddings focus on global semantics and subjects while leaving out other\ndetails. Notably, we reveal that even recent Multimodal Large Language Model\n(MLLM)-based, stronger retrievers with a larger output dimension struggle with\nthis limitation. Hence, we hypothesize that retrieving with general image\nembeddings is suboptimal for performing such queries. As a solution, we propose\nto use promptable image embeddings enabled by these multimodal retrievers,\nwhich boost performance by highlighting required attributes. Our pipeline for\nderiving such embeddings generalizes across query types, image pools, and base\nretriever architectures. To enhance real-world applicability, we offer two\nacceleration strategies: Pre-processing promptable embeddings and using linear\napproximations. We show that the former yields a 15% improvement in Recall@5\nwhen prompts are predefined, while the latter achieves an 8% improvement when\nprompts are only available during inference."}
{"id": "2505.15879", "pdf": "https://arxiv.org/pdf/2505.15879.pdf", "abs": "https://arxiv.org/abs/2505.15879", "title": "GRIT: Teaching MLLMs to Think with Images", "authors": ["Yue Fan", "Xuehai He", "Diji Yang", "Kaizhi Zheng", "Ching-Chen Kuo", "Yuting Zheng", "Sravana Jyothi Narayanaraju", "Xinze Guan", "Xin Eric Wang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent studies have demonstrated the efficacy of using Reinforcement Learning\n(RL) in building reasoning models that articulate chains of thoughts prior to\nproducing final answers. However, despite ongoing advances that aim at enabling\nreasoning for vision-language tasks, existing open-source visual reasoning\nmodels typically generate reasoning content with pure natural language, lacking\nexplicit integration of visual information. This limits their ability to\nproduce clearly articulated and visually grounded reasoning chains. To this\nend, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method\nfor training MLLMs to think with images. GRIT introduces a grounded reasoning\nparadigm, in which models generate reasoning chains that interleave natural\nlanguage and explicit bounding box coordinates. These coordinates point to\nregions of the input image that the model consults during its reasoning\nprocess. Additionally, GRIT is equipped with a reinforcement learning approach,\nGRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused\non the final answer accuracy and format of the grounded reasoning output, which\neliminates the need for data with reasoning chain annotations or explicit\nbounding box labels. As a result, GRIT achieves exceptional data efficiency,\nrequiring as few as 20 image-question-answer triplets from existing datasets.\nComprehensive evaluations demonstrate that GRIT effectively trains MLLMs to\nproduce coherent and visually grounded reasoning chains, showing a successful\nunification of reasoning and grounding abilities."}
{"id": "2505.15928", "pdf": "https://arxiv.org/pdf/2505.15928.pdf", "abs": "https://arxiv.org/abs/2505.15928", "title": "ViQAgent: Zero-Shot Video Question Answering via Agent with Open-Vocabulary Grounding Validation", "authors": ["Tony Montes", "Fernando Lozano"], "categories": ["cs.CV", "cs.CL", "I.4.8"], "comment": null, "summary": "Recent advancements in Video Question Answering (VideoQA) have introduced\nLLM-based agents, modular frameworks, and procedural solutions, yielding\npromising results. These systems use dynamic agents and memory-based mechanisms\nto break down complex tasks and refine answers. However, significant\nimprovements remain in tracking objects for grounding over time and\ndecision-making based on reasoning to better align object references with\nlanguage model outputs, as newer models get better at both tasks. This work\npresents an LLM-brained agent for zero-shot Video Question Answering (VideoQA)\nthat combines a Chain-of-Thought framework with grounding reasoning alongside\nYOLO-World to enhance object tracking and alignment. This approach establishes\na new state-of-the-art in VideoQA and Video Understanding, showing enhanced\nperformance on NExT-QA, iVQA, and ActivityNet-QA benchmarks. Our framework also\nenables cross-checking of grounding timeframes, improving accuracy and\nproviding valuable support for verification and increased output reliability\nacross multiple video domains. The code is available at\nhttps://github.com/t-montes/viqagent."}
{"id": "2505.15935", "pdf": "https://arxiv.org/pdf/2505.15935.pdf", "abs": "https://arxiv.org/abs/2505.15935", "title": "MAPS: A Multilingual Benchmark for Global Agent Performance and Security", "authors": ["Omer Hofman", "Oren Rachmil", "Shamik Bose", "Vikas Pahuja", "Jonathan Brokman", "Toshiya Shimizu", "Trisha Starostina", "Kelly Marchisio", "Seraphina Goldfarb-Tarrant", "Roman Vainshtein"], "categories": ["cs.DB", "cs.CL", "cs.CR"], "comment": null, "summary": "Agentic AI systems, which build on Large Language Models (LLMs) and interact\nwith tools and memory, have rapidly advanced in capability and scope. Yet,\nsince LLMs have been shown to struggle in multilingual settings, typically\nresulting in lower performance and reduced safety, agentic systems risk\ninheriting these limitations. This raises concerns about the global\naccessibility of such systems, as users interacting in languages other than\nEnglish may encounter unreliable or security-critical agent behavior. Despite\ngrowing interest in evaluating agentic AI, existing benchmarks focus\nexclusively on English, leaving multilingual settings unexplored. To address\nthis gap, we propose MAPS, a multilingual benchmark suite designed to evaluate\nagentic AI systems across diverse languages and tasks. MAPS builds on four\nwidely used agentic benchmarks - GAIA (real-world tasks), SWE-bench (code\ngeneration), MATH (mathematical reasoning), and the Agent Security Benchmark\n(security). We translate each dataset into ten diverse languages, resulting in\n805 unique tasks and 8,855 total language-specific instances. Our benchmark\nsuite enables a systematic analysis of how multilingual contexts affect agent\nperformance and robustness. Empirically, we observe consistent degradation in\nboth performance and security when transitioning from English to other\nlanguages, with severity varying by task and correlating with the amount of\ntranslated input. Building on these findings, we provide actionable\nrecommendations to guide agentic AI systems development and assessment under\nmultilingual settings. This work establishes a standardized evaluation\nframework, encouraging future research towards equitable, reliable, and\nglobally accessible agentic AI. MAPS benchmark suite is publicly available at\nhttps://huggingface.co/datasets/Fujitsu-FRE/MAPS"}
{"id": "2505.15957", "pdf": "https://arxiv.org/pdf/2505.15957.pdf", "abs": "https://arxiv.org/abs/2505.15957", "title": "Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey", "authors": ["Chih-Kai Yang", "Neo S. Ho", "Hung-yi Lee"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "Project Website: https://github.com/b08202033/LALM-Evaluation-Survey", "summary": "With advancements in large audio-language models (LALMs), which enhance large\nlanguage models (LLMs) with auditory capabilities, these models are expected to\ndemonstrate universal proficiency across various auditory tasks. While numerous\nbenchmarks have emerged to assess LALMs' performance, they remain fragmented\nand lack a structured taxonomy. To bridge this gap, we conduct a comprehensive\nsurvey and propose a systematic taxonomy for LALM evaluations, categorizing\nthem into four dimensions based on their objectives: (1) General Auditory\nAwareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented\nAbility, and (4) Fairness, Safety, and Trustworthiness. We provide detailed\noverviews within each category and highlight challenges in this field, offering\ninsights into promising future directions. To the best of our knowledge, this\nis the first survey specifically focused on the evaluations of LALMs, providing\nclear guidelines for the community. We will release the collection of the\nsurveyed papers and actively maintain it to support ongoing advancements in the\nfield."}
{"id": "2505.15963", "pdf": "https://arxiv.org/pdf/2505.15963.pdf", "abs": "https://arxiv.org/abs/2505.15963", "title": "OViP: Online Vision-Language Preference Learning", "authors": ["Shujun Liu", "Siyuan Wang", "Zejun Li", "Jianxiang Wang", "Cheng Zeng", "Zhongyu Wei"], "categories": ["cs.CV", "cs.CL"], "comment": "22 pages, 10 figures, 8 tables", "summary": "Large vision-language models (LVLMs) remain vulnerable to hallucination,\noften generating content misaligned with visual inputs. While recent approaches\nadvance multi-modal Direct Preference Optimization (DPO) to mitigate\nhallucination, they typically rely on predefined or randomly edited negative\nsamples that fail to reflect actual model errors, limiting training efficacy.\nIn this work, we propose an Online Vision-language Preference Learning (OViP)\nframework that dynamically constructs contrastive training data based on the\nmodel's own hallucinated outputs. By identifying semantic differences between\nsampled response pairs and synthesizing negative images using a diffusion\nmodel, OViP generates more relevant supervision signals in real time. This\nfailure-driven training enables adaptive alignment of both textual and visual\npreferences. Moreover, we refine existing evaluation protocols to better\ncapture the trade-off between hallucination suppression and expressiveness.\nExperiments on hallucination and general benchmarks demonstrate that OViP\neffectively reduces hallucinations while preserving core multi-modal\ncapabilities."}
{"id": "2505.15966", "pdf": "https://arxiv.org/pdf/2505.15966.pdf", "abs": "https://arxiv.org/abs/2505.15966", "title": "Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning", "authors": ["Alex Su", "Haozhe Wang", "Weimin Ren", "Fangzhen Lin", "Wenhu Chen"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Haozhe Wang and Alex Su contributed equally and listed alphabetically", "summary": "Chain-of-thought reasoning has significantly improved the performance of\nLarge Language Models (LLMs) across various domains. However, this reasoning\nprocess has been confined exclusively to textual space, limiting its\neffectiveness in visually intensive tasks. To address this limitation, we\nintroduce the concept of reasoning in the pixel-space. Within this novel\nframework, Vision-Language Models (VLMs) are equipped with a suite of visual\nreasoning operations, such as zoom-in and select-frame. These operations enable\nVLMs to directly inspect, interrogate, and infer from visual evidences, thereby\nenhancing reasoning fidelity for visual tasks. Cultivating such pixel-space\nreasoning capabilities in VLMs presents notable challenges, including the\nmodel's initially imbalanced competence and its reluctance to adopt the newly\nintroduced pixel-space operations. We address these challenges through a\ntwo-phase training approach. The first phase employs instruction tuning on\nsynthesized reasoning traces to familiarize the model with the novel visual\noperations. Following this, a reinforcement learning (RL) phase leverages a\ncuriosity-driven reward scheme to balance exploration between pixel-space\nreasoning and textual reasoning. With these visual operations, VLMs can\ninteract with complex visual inputs, such as information-rich images or videos\nto proactively gather necessary information. We demonstrate that this approach\nsignificantly improves VLM performance across diverse visual reasoning\nbenchmarks. Our 7B model, \\model, achieves 84\\% on V* bench, 74\\% on\nTallyQA-Complex, and 84\\% on InfographicsVQA, marking the highest accuracy\nachieved by any open-source model to date. These results highlight the\nimportance of pixel-space reasoning and the effectiveness of our framework."}
{"id": "2505.16004", "pdf": "https://arxiv.org/pdf/2505.16004.pdf", "abs": "https://arxiv.org/abs/2505.16004", "title": "Interpretability Illusions with Sparse Autoencoders: Evaluating Robustness of Concept Representations", "authors": ["Aaron J. Li", "Suraj Srinivas", "Usha Bhalla", "Himabindu Lakkaraju"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Sparse autoencoders (SAEs) are commonly used to interpret the internal\nactivations of large language models (LLMs) by mapping them to\nhuman-interpretable concept representations. While existing evaluations of SAEs\nfocus on metrics such as the reconstruction-sparsity tradeoff, human\n(auto-)interpretability, and feature disentanglement, they overlook a critical\naspect: the robustness of concept representations to input perturbations. We\nargue that robustness must be a fundamental consideration for concept\nrepresentations, reflecting the fidelity of concept labeling. To this end, we\nformulate robustness quantification as input-space optimization problems and\ndevelop a comprehensive evaluation framework featuring realistic scenarios in\nwhich adversarial perturbations are crafted to manipulate SAE representations.\nEmpirically, we find that tiny adversarial input perturbations can effectively\nmanipulate concept-based interpretations in most scenarios without notably\naffecting the outputs of the base LLMs themselves. Overall, our results suggest\nthat SAE concept representations are fragile and may be ill-suited for\napplications in model monitoring and oversight."}
{"id": "2505.16037", "pdf": "https://arxiv.org/pdf/2505.16037.pdf", "abs": "https://arxiv.org/abs/2505.16037", "title": "Causal LLM Routing: End-to-End Regret Minimization from Observational Data", "authors": ["Asterios Tsiourvas", "Wei Sun", "Georgia Perakis"], "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "LLM routing aims to select the most appropriate model for each query,\nbalancing competing performance metrics such as accuracy and cost across a pool\nof language models. Prior approaches typically adopt a decoupled strategy,\nwhere the metrics are first predicted and the model is then selected based on\nthese estimates. This setup is prone to compounding errors and often relies on\nfull-feedback data, where each query is evaluated by all candidate models,\nwhich is costly to obtain and maintain in practice. In contrast, we learn from\nobservational data, which records only the outcome of the model actually\ndeployed. We propose a causal end-to-end framework that learns routing policies\nby minimizing decision-making regret from observational data. To enable\nefficient optimization, we introduce two theoretically grounded surrogate\nobjectives: a classification-based upper bound, and a softmax-weighted regret\napproximation shown to recover the optimal policy at convergence. We further\nextend our framework to handle heterogeneous cost preferences via an\ninterval-conditioned architecture. Experiments on public benchmarks show that\nour method outperforms existing baselines, achieving state-of-the-art\nperformance across different embedding models."}
{"id": "2505.16065", "pdf": "https://arxiv.org/pdf/2505.16065.pdf", "abs": "https://arxiv.org/abs/2505.16065", "title": "Aug2Search: Enhancing Facebook Marketplace Search with LLM-Generated Synthetic Data Augmentation", "authors": ["Ruijie Xi", "He Ba", "Hao Yuan", "Rishu Agrawal", "Arul Prakash"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Embedding-Based Retrieval (EBR) is an important technique in modern search\nengines, enabling semantic match between search queries and relevant results.\nHowever, search logging data on platforms like Facebook Marketplace lacks the\ndiversity and details needed for effective EBR model training, limiting the\nmodels' ability to capture nuanced search patterns. To address this challenge,\nwe propose Aug2Search, an EBR-based framework leveraging synthetic data\ngenerated by Generative AI (GenAI) models, in a multimodal and multitask\napproach to optimize query-product relevance. This paper investigates the\ncapabilities of GenAI, particularly Large Language Models (LLMs), in generating\nhigh-quality synthetic data, and analyzing its impact on enhancing EBR models.\nWe conducted experiments using eight Llama models and 100 million data points\nfrom Facebook Marketplace logs. Our synthetic data generation follows three\nstrategies: (1) generate queries, (2) enhance product listings, and (3)\ngenerate queries from enhanced listings. We train EBR models on three different\ndatasets: sampled engagement data or original data ((e.g., \"Click\" and \"Listing\nInteractions\")), synthetic data, and a mixture of both engagement and synthetic\ndata to assess their performance across various training sets. Our findings\nunderscore the robustness of Llama models in producing synthetic queries and\nlistings with high coherence, relevance, and diversity, while maintaining low\nlevels of hallucination. Aug2Search achieves an improvement of up to 4% in\nROC_AUC with 100 million synthetic data samples, demonstrating the\neffectiveness of our approach. Moreover, our experiments reveal that with the\nsame volume of training data, models trained exclusively on synthetic data\noften outperform those trained on original data only or a mixture of original\nand synthetic data."}
{"id": "2505.16066", "pdf": "https://arxiv.org/pdf/2505.16066.pdf", "abs": "https://arxiv.org/abs/2505.16066", "title": "Merge to Mix: Mixing Datasets via Model Merging", "authors": ["Zhixu Silvia Tao", "Kasper Vinken", "Hao-Wei Yeh", "Avi Cooper", "Xavier Boix"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Mixing datasets for fine-tuning large models (LMs) has become critical for\nmaximizing performance on downstream tasks. However, composing effective\ndataset mixtures typically relies on heuristics and trial-and-error, often\nrequiring multiple fine-tuning runs to achieve the desired outcome. We propose\na novel method, $\\textit{Merge to Mix}$, that accelerates composing dataset\nmixtures through model merging. Model merging is a recent technique that\ncombines the abilities of multiple individually fine-tuned LMs into a single LM\nby using a few simple arithmetic operations. Our key insight is that merging\nmodels individually fine-tuned on each dataset in a mixture can effectively\nserve as a surrogate for a model fine-tuned on the entire mixture. Merge to Mix\nleverages this insight to accelerate selecting dataset mixtures without\nrequiring full fine-tuning on each candidate mixture. Our experiments\ndemonstrate that Merge to Mix surpasses state-of-the-art methods in dataset\nselection for fine-tuning LMs."}
{"id": "2505.16086", "pdf": "https://arxiv.org/pdf/2505.16086.pdf", "abs": "https://arxiv.org/abs/2505.16086", "title": "Optimizing LLM-Based Multi-Agent System with Textual Feedback: A Case Study on Software Development", "authors": ["Ming Shen", "Raphael Shu", "Anurag Pratik", "James Gung", "Yubin Ge", "Monica Sunkara", "Yi Zhang"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We have seen remarkable progress in large language models (LLMs) empowered\nmulti-agent systems solving complex tasks necessitating cooperation among\nexperts with diverse skills. However, optimizing LLM-based multi-agent systems\nremains challenging. In this work, we perform an empirical case study on group\noptimization of role-based multi-agent systems utilizing natural language\nfeedback for challenging software development tasks under various evaluation\ndimensions. We propose a two-step agent prompts optimization pipeline:\nidentifying underperforming agents with their failure explanations utilizing\ntextual feedback and then optimizing system prompts of identified agents\nutilizing failure explanations. We then study the impact of various\noptimization settings on system performance with two comparison groups: online\nagainst offline optimization and individual against group optimization. For\ngroup optimization, we study two prompting strategies: one-pass and multi-pass\nprompting optimizations. Overall, we demonstrate the effectiveness of our\noptimization method for role-based multi-agent systems tackling software\ndevelopment tasks evaluated on diverse evaluation dimensions, and we\ninvestigate the impact of diverse optimization settings on group behaviors of\nthe multi-agent systems to provide practical insights for future development."}
{"id": "2505.16090", "pdf": "https://arxiv.org/pdf/2505.16090.pdf", "abs": "https://arxiv.org/abs/2505.16090", "title": "Can AI Read Between The Lines? Benchmarking LLMs On Financial Nuance", "authors": ["Dominick Kubica", "Dylan T. Gordon", "Nanami Emura", "Derleen Saini", "Charlie Goldenberg"], "categories": ["cs.AI", "cs.CL", "I.2.6; I.2.7"], "comment": "6 pages, 4 figures. Research conducted as part of a\n  Microsoft-sponsored Capstone Project at Santa Clara University", "summary": "As of 2025, Generative Artificial Intelligence (GenAI) has become a central\ntool for productivity across industries. Beyond text generation, GenAI now\nplays a critical role in coding, data analysis, and research workflows. As\nlarge language models (LLMs) continue to evolve, it is essential to assess the\nreliability and accuracy of their outputs, especially in specialized,\nhigh-stakes domains like finance. Most modern LLMs transform text into\nnumerical vectors, which are used in operations such as cosine similarity\nsearches to generate responses. However, this abstraction process can lead to\nmisinterpretation of emotional tone, particularly in nuanced financial\ncontexts. While LLMs generally excel at identifying sentiment in everyday\nlanguage, these models often struggle with the nuanced, strategically ambiguous\nlanguage found in earnings call transcripts. Financial disclosures frequently\nembed sentiment in hedged statements, forward-looking language, and\nindustry-specific jargon, making it difficult even for human analysts to\ninterpret consistently, let alone AI models. This paper presents findings from\nthe Santa Clara Microsoft Practicum Project, led by Professor Charlie\nGoldenberg, which benchmarks the performance of Microsoft's Copilot, OpenAI's\nChatGPT, Google's Gemini, and traditional machine learning models for sentiment\nanalysis of financial text. Using Microsoft earnings call transcripts, the\nanalysis assesses how well LLM-derived sentiment correlates with market\nsentiment and stock movements and evaluates the accuracy of model outputs.\nPrompt engineering techniques are also examined to improve sentiment analysis\nresults. Visualizations of sentiment consistency are developed to evaluate\nalignment between tone and stock performance, with sentiment trends analyzed\nacross Microsoft's lines of business to determine which segments exert the\ngreatest influence."}
{"id": "2505.16094", "pdf": "https://arxiv.org/pdf/2505.16094.pdf", "abs": "https://arxiv.org/abs/2505.16094", "title": "A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization", "authors": ["Ziqing Wang", "Kexin Zhang", "Zihan Zhao", "Yibo Wen", "Abhishek Pandey", "Han Liu", "Kaize Ding"], "categories": ["cs.LG", "cs.CL"], "comment": "Under review", "summary": "Large language models (LLMs) are introducing a paradigm shift in molecular\ndiscovery by enabling text-guided interaction with chemical spaces through\nnatural language, symbolic notations, with emerging extensions to incorporate\nmulti-modal inputs. To advance the new field of LLM for molecular discovery,\nthis survey provides an up-to-date and forward-looking review of the emerging\nuse of LLMs for two central tasks: molecule generation and molecule\noptimization. Based on our proposed taxonomy for both problems, we analyze\nrepresentative techniques in each category, highlighting how LLM capabilities\nare leveraged across different learning settings. In addition, we include the\ncommonly used datasets and evaluation protocols. We conclude by discussing key\nchallenges and future directions, positioning this survey as a resource for\nresearchers working at the intersection of LLMs and molecular science. A\ncontinuously updated reading list is available at\nhttps://github.com/REAL-Lab-NU/Awesome-LLM-Centric-Molecular-Discovery."}
{"id": "2505.16100", "pdf": "https://arxiv.org/pdf/2505.16100.pdf", "abs": "https://arxiv.org/abs/2505.16100", "title": "BioDSA-1K: Benchmarking Data Science Agents for Biomedical Research", "authors": ["Zifeng Wang", "Benjamin Danek", "Jimeng Sun"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Validating scientific hypotheses is a central challenge in biomedical\nresearch, and remains difficult for artificial intelligence (AI) agents due to\nthe complexity of real-world data analysis and evidence interpretation. In this\nwork, we present BioDSA-1K, a benchmark designed to evaluate AI agents on\nrealistic, data-driven biomedical hypothesis validation tasks. BioDSA-1K\nconsists of 1,029 hypothesis-centric tasks paired with 1,177 analysis plans,\ncurated from over 300 published biomedical studies to reflect the structure and\nreasoning found in authentic research workflows. Each task includes a\nstructured hypothesis derived from the original study's conclusions, expressed\nin the affirmative to reflect the language of scientific reporting, and one or\nmore pieces of supporting evidence grounded in empirical data tables. While\nthese hypotheses mirror published claims, they remain testable using standard\nstatistical or machine learning methods. The benchmark enables evaluation along\nfour axes: (1) hypothesis decision accuracy, (2) alignment between evidence and\nconclusion, (3) correctness of the reasoning process, and (4) executability of\nthe AI-generated analysis code. Importantly, BioDSA-1K includes non-verifiable\nhypotheses: cases where the available data are insufficient to support or\nrefute a claim, reflecting a common yet underexplored scenario in real-world\nscience. We propose BioDSA-1K as a foundation for building and evaluating\ngeneralizable, trustworthy AI agents for biomedical discovery."}
{"id": "2505.16146", "pdf": "https://arxiv.org/pdf/2505.16146.pdf", "abs": "https://arxiv.org/abs/2505.16146", "title": "Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation", "authors": ["Zhenglin Hua", "Jinghan He", "Zijun Yao", "Tianxu Han", "Haiyun Guo", "Yuheng Jia", "Junfeng Fang"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large vision-language models (LVLMs) have achieved remarkable performance on\nmultimodal tasks such as visual question answering (VQA) and image captioning.\nHowever, they still suffer from hallucinations, generating text inconsistent\nwith visual input, posing significant risks in real-world applications.\nExisting approaches to address this issue focus on incorporating external\nknowledge bases, alignment training, or decoding strategies, all of which\nrequire substantial computational cost and time. Recent works try to explore\nmore efficient alternatives by adjusting LVLMs' internal representations.\nAlthough promising, these methods may cause hallucinations to be insufficiently\nsuppressed or lead to excessive interventions that negatively affect normal\nsemantics. In this work, we leverage sparse autoencoders (SAEs) to identify\nsemantic directions closely associated with either hallucinations or actuality,\nrealizing more precise and direct hallucination-related representations. Our\nanalysis demonstrates that interventions along the faithful direction we\nidentified can mitigate hallucinations, while those along the hallucinatory\ndirection can exacerbate them. Building on these insights, we propose Steering\nLVLMs via SAE Latent Directions (SSL), a training-free method based on\nSAE-derived latent directions to mitigate hallucinations in LVLMs. Extensive\nexperiments demonstrate that SSL significantly outperforms existing decoding\napproaches in mitigating hallucinations, while maintaining transferability\nacross different model architectures with negligible additional time overhead."}
{"id": "2505.16148", "pdf": "https://arxiv.org/pdf/2505.16148.pdf", "abs": "https://arxiv.org/abs/2505.16148", "title": "NAN: A Training-Free Solution to Coefficient Estimation in Model Merging", "authors": ["Chongjie Si", "Kangtao Lv", "Jingjing Jiang", "Yadao Wang", "Yongwei Wang", "Xiaokang Yang", "Wenbo Su", "Bo Zheng", "Wei Shen"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Model merging offers a training-free alternative to multi-task learning by\ncombining independently fine-tuned models into a unified one without access to\nraw data. However, existing approaches often rely on heuristics to determine\nthe merging coefficients, limiting their scalability and generality. In this\nwork, we revisit model merging through the lens of least-squares optimization\nand show that the optimal merging weights should scale with the amount of\ntask-specific information encoded in each model. Based on this insight, we\npropose NAN, a simple yet effective method that estimates model merging\ncoefficients via the inverse of parameter norm. NAN is training-free,\nplug-and-play, and applicable to a wide range of merging strategies. Extensive\nexperiments on show that NAN consistently improves performance of baseline\nmethods."}
{"id": "2505.16149", "pdf": "https://arxiv.org/pdf/2505.16149.pdf", "abs": "https://arxiv.org/abs/2505.16149", "title": "When VLMs Meet Image Classification: Test Sets Renovation via Missing Label Identification", "authors": ["Zirui Pang", "Haosheng Tan", "Yuhan Pu", "Zhijie Deng", "Zhouan Shen", "Keyu Hu", "Jiaheng Wei"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Image classification benchmark datasets such as CIFAR, MNIST, and ImageNet\nserve as critical tools for model evaluation. However, despite the cleaning\nefforts, these datasets still suffer from pervasive noisy labels and often\ncontain missing labels due to the co-existing image pattern where multiple\nclasses appear in an image sample. This results in misleading model comparisons\nand unfair evaluations. Existing label cleaning methods focus primarily on\nnoisy labels, but the issue of missing labels remains largely overlooked.\nMotivated by these challenges, we present a comprehensive framework named\nREVEAL, integrating state-of-the-art pre-trained vision-language models (e.g.,\nLLaVA, BLIP, Janus, Qwen) with advanced machine/human label curation methods\n(e.g., Docta, Cleanlab, MTurk), to systematically address both noisy labels and\nmissing label detection in widely-used image classification test sets. REVEAL\ndetects potential noisy labels and omissions, aggregates predictions from\nvarious methods, and refines label accuracy through confidence-informed\npredictions and consensus-based filtering. Additionally, we provide a thorough\nanalysis of state-of-the-art vision-language models and pre-trained image\nclassifiers, highlighting their strengths and limitations within the context of\ndataset renovation by revealing 10 observations. Our method effectively reveals\nmissing labels from public datasets and provides soft-labeled results with\nlikelihoods. Through human verifications, REVEAL significantly improves the\nquality of 6 benchmark test sets, highly aligning to human judgments and\nenabling more accurate and meaningful comparisons in image classification."}
{"id": "2505.16176", "pdf": "https://arxiv.org/pdf/2505.16176.pdf", "abs": "https://arxiv.org/abs/2505.16176", "title": "Dynamic Sampling that Adapts: Iterative DPO for Self-Aware Mathematical Reasoning", "authors": ["Jun Rao", "Xuebo Liu", "Hexuan Deng", "Zepeng Lin", "Zixiong Yu", "Jiansheng Wei", "Xiaojun Meng", "Min Zhang"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "In the realm of data selection for reasoning tasks, existing approaches\npredominantly rely on externally predefined static metrics such as difficulty\nand diversity, which are often designed for supervised fine-tuning (SFT) and\nlack adaptability to continuous training processes. A critical limitation of\nthese methods is their inability to dynamically align with the evolving\ncapabilities of models during online training, a gap that becomes increasingly\npronounced with the rise of dynamic training paradigms and online reinforcement\nlearning (RL) frameworks (e.g., R1 models). To address this, we introduce\nSAI-DPO, an algorithm that dynamically selects training data by continuously\nassessing a model's stage-specific reasoning abilities across different\ntraining phases. By integrating real-time model performance feedback, SAI-DPO\nadaptively adapts data selection to the evolving strengths and weaknesses of\nthe model, thus enhancing both data utilization efficiency and final task\nperformance. Extensive experiments on three state-of-the-art models and eight\nmathematical reasoning benchmarks, including challenging competition-level\ndatasets (e.g., AIME24 and AMC23), demonstrate that SAI-DPO achieves an average\nperformance boost of up to 21.3 percentage points, with particularly notable\nimprovements of 10 and 15 points on AIME24 and AMC23, respectively. These\nresults highlight the superiority of dynamic, model-adaptive data selection\nover static, externally defined strategies in advancing reasoning."}
{"id": "2505.16180", "pdf": "https://arxiv.org/pdf/2505.16180.pdf", "abs": "https://arxiv.org/abs/2505.16180", "title": "Redemption Score: An Evaluation Framework to Rank Image Captions While Redeeming Image Semantics and Language Pragmatics", "authors": ["Ashim Dahal", "Ankit Ghimire", "Saydul Akbar Murad", "Nick Rahimi"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Evaluating image captions requires cohesive assessment of both visual\nsemantics and language pragmatics, which is often not entirely captured by most\nmetrics. We introduce Redemption Score, a novel hybrid framework that ranks\nimage captions by triangulating three complementary signals: (1) Mutual\nInformation Divergence (MID) for global image-text distributional alignment,\n(2) DINO-based perceptual similarity of cycle-generated images for visual\ngrounding, and (3) BERTScore for contextual text similarity against human\nreferences. A calibrated fusion of these signals allows Redemption Score to\noffer a more holistic assessment. On the Flickr8k benchmark, Redemption Score\nachieves a Kendall-$\\tau$ of 56.43, outperforming twelve prior methods and\ndemonstrating superior correlation with human judgments without requiring\ntask-specific training. Our framework provides a more robust and nuanced\nevaluation by effectively redeeming image semantics and linguistic\ninterpretability indicated by strong transfer of knowledge in the Conceptual\nCaptions and MS COCO datasets."}
{"id": "2505.16186", "pdf": "https://arxiv.org/pdf/2505.16186.pdf", "abs": "https://arxiv.org/abs/2505.16186", "title": "SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning", "authors": ["Kaiwen Zhou", "Xuandong Zhao", "Gaowen Liu", "Jayanth Srinivasa", "Aosong Feng", "Dawn Song", "Xin Eric Wang"], "categories": ["cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Large Reasoning Models (LRMs) introduce a new generation paradigm of\nexplicitly reasoning before answering, leading to remarkable improvements in\ncomplex tasks. However, they pose great safety risks against harmful queries\nand adversarial attacks. While recent mainstream safety efforts on LRMs,\nsupervised fine-tuning (SFT), improve safety performance, we find that\nSFT-aligned models struggle to generalize to unseen jailbreak prompts. After\nthorough investigation of LRMs' generation, we identify a safety aha moment\nthat can activate safety reasoning and lead to a safe response. This aha moment\ntypically appears in the `key sentence', which follows models' query\nunderstanding process and can indicate whether the model will proceed safely.\nBased on these insights, we propose SafeKey, including two complementary\nobjectives to better activate the safety aha moment in the key sentence: (1) a\nDual-Path Safety Head to enhance the safety signal in the model's internal\nrepresentations before the key sentence, and (2) a Query-Mask Modeling\nobjective to improve the models' attention on its query understanding, which\nhas important safety hints. Experiments across multiple safety benchmarks\ndemonstrate that our methods significantly improve safety generalization to a\nwide range of jailbreak attacks and out-of-distribution harmful prompts,\nlowering the average harmfulness rate by 9.6\\%, while maintaining general\nabilities. Our analysis reveals how SafeKey enhances safety by reshaping\ninternal attention and improving the quality of hidden representations."}
{"id": "2505.16210", "pdf": "https://arxiv.org/pdf/2505.16210.pdf", "abs": "https://arxiv.org/abs/2505.16210", "title": "NQKV: A KV Cache Quantization Scheme Based on Normal Distribution Characteristics", "authors": ["Zhihang Cai", "Xingjun Zhang", "Zhendong Tan", "Zheng Wei"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "11 pages, 9 figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na wide range of tasks. However, LLMs often require larger batch sizes to\nenhance throughput or longer context lengths to meet task demands, which\nsignificantly increases the memory resource consumption of the Key-Value (KV)\ncache during inference, becoming a major bottleneck in LLM deployment. To\naddress this issue, quantization is a common and straightforward approach.\nCurrently, quantization methods for activations are limited to 8-bit, and\nquantization to even lower bits can lead to substantial accuracy drops. To\nfurther save space by quantizing the KV cache to even lower bits, we analyzed\nthe element distribution of the KV cache and designed the NQKV algorithm. Since\nthe elements within each block of the KV cache follow a normal distribution,\nNQKV employs per-block quantile quantization to achieve\ninformation-theoretically optimal quantization error. Without significantly\ncompromising model output quality, NQKV enables the OPT model to perform\ninference with an 2x larger batch size or a 4x longer context length, and it\nimproves throughput by 9.3x compared to when the KV cache is not used."}
{"id": "2505.16211", "pdf": "https://arxiv.org/pdf/2505.16211.pdf", "abs": "https://arxiv.org/abs/2505.16211", "title": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models", "authors": ["Kai Li", "Can Shen", "Yile Liu", "Jirui Han", "Kelong Zheng", "Xuechao Zou", "Zhe Wang", "Xingjian Du", "Shun Zhang", "Hanjun Luo", "Yingbin Jin", "Xinxin Xing", "Ziyang Ma", "Yue Liu", "Xiaojun Jia", "Yifan Zhang", "Junfeng Fang", "Kun Wang", "Yibo Yan", "Haoyang Li", "Yiming Li", "Xiaobin Zhuang", "Yang Liu", "Haibo Hu", "Zhuo Chen", "Zhizheng Wu", "Xiaolin Hu", "Eng-Siong Chng", "XiaoFeng Wang", "Wenyuan Xu", "Wei Dong", "Xinfeng Li"], "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": "Technical Report", "summary": "The rapid advancement and expanding applications of Audio Large Language\nModels (ALLMs) demand a rigorous understanding of their trustworthiness.\nHowever, systematic research on evaluating these models, particularly\nconcerning risks unique to the audio modality, remains largely unexplored.\nExisting evaluation frameworks primarily focus on the text modality or address\nonly a restricted set of safety dimensions, failing to adequately account for\nthe unique characteristics and application scenarios inherent to the audio\nmodality. We introduce AudioTrust-the first multifaceted trustworthiness\nevaluation framework and benchmark specifically designed for ALLMs. AudioTrust\nfacilitates assessments across six key dimensions: fairness, hallucination,\nsafety, privacy, robustness, and authentication. To comprehensively evaluate\nthese dimensions, AudioTrust is structured around 18 distinct experimental\nsetups. Its core is a meticulously constructed dataset of over 4,420 audio/text\nsamples, drawn from real-world scenarios (e.g., daily conversations, emergency\ncalls, voice assistant interactions), specifically designed to probe the\nmultifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully\ndesigns 9 audio-specific evaluation metrics, and we employ a large-scale\nautomated pipeline for objective and scalable scoring of model outputs.\nExperimental results reveal the trustworthiness boundaries and limitations of\ncurrent state-of-the-art open-source and closed-source ALLMs when confronted\nwith various high-risk audio scenarios, offering valuable insights for the\nsecure and trustworthy deployment of future audio models. Our platform and\nbenchmark are available at https://github.com/JusperLee/AudioTrust."}
{"id": "2505.16220", "pdf": "https://arxiv.org/pdf/2505.16220.pdf", "abs": "https://arxiv.org/abs/2505.16220", "title": "Meta-PerSER: Few-Shot Listener Personalized Speech Emotion Recognition via Meta-learning", "authors": ["Liang-Yeh Shen", "Shi-Xin Fang", "Yi-Cheng Lin", "Huang-Cheng Chou", "Hung-yi Lee"], "categories": ["eess.AS", "cs.CL"], "comment": "Accepted by INTERSPEECH 2025. 7 pages, including 2 pages of appendix", "summary": "This paper introduces Meta-PerSER, a novel meta-learning framework that\npersonalizes Speech Emotion Recognition (SER) by adapting to each listener's\nunique way of interpreting emotion. Conventional SER systems rely on aggregated\nannotations, which often overlook individual subtleties and lead to\ninconsistent predictions. In contrast, Meta-PerSER leverages a Model-Agnostic\nMeta-Learning (MAML) approach enhanced with Combined-Set Meta-Training,\nDerivative Annealing, and per-layer per-step learning rates, enabling rapid\nadaptation with only a few labeled examples. By integrating robust\nrepresentations from pre-trained self-supervised models, our framework first\ncaptures general emotional cues and then fine-tunes itself to personal\nannotation styles. Experiments on the IEMOCAP corpus demonstrate that\nMeta-PerSER significantly outperforms baseline methods in both seen and unseen\ndata scenarios, highlighting its promise for personalized emotion recognition."}
{"id": "2505.16263", "pdf": "https://arxiv.org/pdf/2505.16263.pdf", "abs": "https://arxiv.org/abs/2505.16263", "title": "All You Need is \"Leet\": Evading Hate-speech Detection AI", "authors": ["Sampanna Yashwant Kahu", "Naman Ahuja"], "categories": ["cs.CR", "cs.CL", "cs.LG", "K.6.5"], "comment": "10 pages, 22 figures, The source code and data used in this work is\n  available at: https://github.com/SampannaKahu/all_you_need_is_leet", "summary": "Social media and online forums are increasingly becoming popular.\nUnfortunately, these platforms are being used for spreading hate speech. In\nthis paper, we design black-box techniques to protect users from hate-speech on\nonline platforms by generating perturbations that can fool state of the art\ndeep learning based hate speech detection models thereby decreasing their\nefficiency. We also ensure a minimal change in the original meaning of\nhate-speech. Our best perturbation attack is successfully able to evade\nhate-speech detection for 86.8 % of hateful text."}
{"id": "2505.16276", "pdf": "https://arxiv.org/pdf/2505.16276.pdf", "abs": "https://arxiv.org/abs/2505.16276", "title": "How do Scaling Laws Apply to Knowledge Graph Engineering Tasks? The Impact of Model Size on Large Language Model Performance", "authors": ["Desiree Heim", "Lars-Peter Meyer", "Markus Schröder", "Johannes Frey", "Andreas Dengel"], "categories": ["cs.AI", "cs.CL"], "comment": "Peer reviewed and to appear in the ESWC 2025 Workshops and Tutorials\n  Joint Proceedings (Workshop on Evaluation of Language Models in Knowledge\n  Engineering [ELMKE])", "summary": "When using Large Language Models (LLMs) to support Knowledge Graph\nEngineering (KGE), one of the first indications when searching for an\nappropriate model is its size. According to the scaling laws, larger models\ntypically show higher capabilities. However, in practice, resource costs are\nalso an important factor and thus it makes sense to consider the ratio between\nmodel performance and costs. The LLM-KG-Bench framework enables the comparison\nof LLMs in the context of KGE tasks and assesses their capabilities of\nunderstanding and producing KGs and KG queries. Based on a dataset created in\nan LLM-KG-Bench run covering 26 open state-of-the-art LLMs, we explore the\nmodel size scaling laws specific to KGE tasks. In our analyses, we assess how\nbenchmark scores evolve between different model size categories. Additionally,\nwe inspect how the general score development of single models and families of\nmodels correlates to their size. Our analyses revealed that, with a few\nexceptions, the model size scaling laws generally also apply to the selected\nKGE tasks. However, in some cases, plateau or ceiling effects occurred, i.e.,\nthe task performance did not change much between a model and the next larger\nmodel. In these cases, smaller models could be considered to achieve high\ncost-effectiveness. Regarding models of the same family, sometimes larger\nmodels performed worse than smaller models of the same family. These effects\noccurred only locally. Hence it is advisable to additionally test the next\nsmallest and largest model of the same family."}
{"id": "2505.16315", "pdf": "https://arxiv.org/pdf/2505.16315.pdf", "abs": "https://arxiv.org/abs/2505.16315", "title": "Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning", "authors": ["Xiaoxue Cheng", "Junyi Li", "Zhenduo Zhang", "Xinyu Tang", "Wayne Xin Zhao", "Xinyu Kong", "Zhiqiang Zhang"], "categories": ["cs.AI", "cs.CL"], "comment": "work in progress", "summary": "Large reasoning models (LRMs) have demonstrated strong performance on complex\nreasoning tasks, but often suffer from overthinking, generating redundant\ncontent regardless of task difficulty. Inspired by the dual process theory in\ncognitive science, we propose Adaptive Cognition Policy Optimization (ACPO), a\nreinforcement learning framework that enables LRMs to achieve efficient\nreasoning through adaptive cognitive allocation and dynamic system switch. ACPO\nincorporates two key components: (1) introducing system-aware reasoning tokens\nto explicitly represent the thinking modes thereby making the model's cognitive\nprocess transparent, and (2) integrating online difficulty estimation and token\nlength budget to guide adaptive system switch and reasoning during\nreinforcement learning. To this end, we propose a two-stage training strategy.\nThe first stage begins with supervised fine-tuning to cold start the model,\nenabling it to generate reasoning paths with explicit thinking modes. In the\nsecond stage, we apply ACPO to further enhance adaptive system switch for\ndifficulty-aware reasoning. Experimental results demonstrate that ACPO\neffectively reduces redundant reasoning while adaptively adjusting cognitive\nallocation based on task complexity, achieving efficient hybrid reasoning."}
{"id": "2505.16322", "pdf": "https://arxiv.org/pdf/2505.16322.pdf", "abs": "https://arxiv.org/abs/2505.16322", "title": "AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners", "authors": ["Woosung Koh", "Wonbeen Oh", "Jaein Jang", "MinHyung Lee", "Hyeongjin Kim", "Ah Yeon Kim", "Joonkee Kim", "Junghyun Lee", "Taehyeon Kim", "Se-Young Yun"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Pre-print", "summary": "Self-Taught Reasoners (STaR), synonymously known as Rejection sampling\nFine-Tuning (RFT), is an integral part of the training pipeline of\nself-improving reasoning Language Models (LMs). The self-improving mechanism\noften employs random observation (data) sampling. However, this results in\ntrained observation imbalance; inefficiently over-training on solved examples\nwhile under-training on challenging ones. In response, we introduce Adaptive\nSTaR (AdaSTaR), a novel algorithm that rectifies this by integrating two\nadaptive sampling principles: (1) Adaptive Sampling for Diversity: promoting\nbalanced training across observations, and (2) Adaptive Sampling for\nCurriculum: dynamically adjusting data difficulty to match the model's evolving\nstrength. Across six benchmarks, AdaSTaR achieves best test accuracy in all\ninstances (6/6) and reduces training FLOPs by an average of 58.6% against an\nextensive list of baselines. These improvements in performance and efficiency\ngeneralize to different pre-trained LMs and larger models, paving the way for\nmore efficient and effective self-improving LMs."}
{"id": "2505.16400", "pdf": "https://arxiv.org/pdf/2505.16400.pdf", "abs": "https://arxiv.org/abs/2505.16400", "title": "AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning", "authors": ["Yang Chen", "Zhuolin Yang", "Zihan Liu", "Chankyu Lee", "Peng Xu", "Mohammad Shoeybi", "Bryan Catanzaro", "Wei Ping"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "We release the model at:\n  https://huggingface.co/nvidia/AceReason-Nemotron-14B", "summary": "Despite recent progress in large-scale reinforcement learning (RL) for\nreasoning, the training recipe for building high-performing reasoning models\nremains elusive. Key implementation details of frontier models, such as\nDeepSeek-R1, including data curation strategies and RL training recipe, are\noften omitted. Moreover, recent research indicates distillation remains more\neffective than RL for smaller models. In this work, we demonstrate that\nlarge-scale RL can significantly enhance the reasoning capabilities of strong,\nsmall- and mid-sized models, achieving results that surpass those of\nstate-of-the-art distillation-based models. We systematically study the RL\ntraining process through extensive ablations and propose a simple yet effective\napproach: first training on math-only prompts, then on code-only prompts.\nNotably, we find that math-only RL not only significantly enhances the\nperformance of strong distilled models on math benchmarks (e.g., +14.6% /\n+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks\n(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,\nextended code-only RL iterations further improve performance on code benchmarks\nwith minimal or no degradation in math results. We develop a robust data\ncuration pipeline to collect challenging prompts with high-quality, verifiable\nanswers and test cases to enable verification-based RL across both domains.\nFinally, we identify key experimental insights, including curriculum learning\nwith progressively increasing response lengths and the stabilizing effect of\non-policy parameter updates. We find that RL not only elicits the foundational\nreasoning capabilities acquired during pretraining and supervised fine-tuning\n(e.g., distillation), but also pushes the limits of the model's reasoning\nability, enabling it to solve problems that were previously unsolvable."}
{"id": "2505.16470", "pdf": "https://arxiv.org/pdf/2505.16470.pdf", "abs": "https://arxiv.org/abs/2505.16470", "title": "Benchmarking Retrieval-Augmented Multimomal Generation for Document Question Answering", "authors": ["Kuicai Dong", "Yujing Chang", "Shijie Huang", "Yasheng Wang", "Ruiming Tang", "Yong Liu"], "categories": ["cs.IR", "cs.CL", "cs.CV"], "comment": "preprint. code available at\n  \\url{https://mmdocrag.github.io/MMDocRAG/}", "summary": "Document Visual Question Answering (DocVQA) faces dual challenges in\nprocessing lengthy multimodal documents (text, images, tables) and performing\ncross-modal reasoning. Current document retrieval-augmented generation (DocRAG)\nmethods remain limited by their text-centric approaches, frequently missing\ncritical visual information. The field also lacks robust benchmarks for\nassessing multimodal evidence selection and integration. We introduce MMDocRAG,\na comprehensive benchmark featuring 4,055 expert-annotated QA pairs with\nmulti-page, cross-modal evidence chains. Our framework introduces innovative\nmetrics for evaluating multimodal quote selection and enables answers that\ninterleave text with relevant visual elements. Through large-scale experiments\nwith 60 VLM/LLM models and 14 retrieval systems, we identify persistent\nchallenges in multimodal evidence retrieval, selection, and integration.Key\nfindings reveal advanced proprietary LVMs show superior performance than\nopen-sourced alternatives. Also, they show moderate advantages using multimodal\ninputs over text-only inputs, while open-source alternatives show significant\nperformance degradation. Notably, fine-tuned LLMs achieve substantial\nimprovements when using detailed image descriptions. MMDocRAG establishes a\nrigorous testing ground and provides actionable insights for developing more\nrobust multimodal DocVQA systems. Our benchmark and code are available at\nhttps://mmdocrag.github.io/MMDocRAG/."}
{"id": "2505.16530", "pdf": "https://arxiv.org/pdf/2505.16530.pdf", "abs": "https://arxiv.org/abs/2505.16530", "title": "DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection", "authors": ["Yuliang Yan", "Haochun Tang", "Shuo Yan", "Enyan Dai"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) are considered valuable Intellectual Properties\n(IP) for legitimate owners due to the enormous computational cost of training.\nIt is crucial to protect the IP of LLMs from malicious stealing or unauthorized\ndeployment. Despite existing efforts in watermarking and fingerprinting LLMs,\nthese methods either impact the text generation process or are limited in\nwhite-box access to the suspect model, making them impractical. Hence, we\npropose DuFFin, a novel $\\textbf{Du}$al-Level $\\textbf{Fin}$gerprinting\n$\\textbf{F}$ramework for black-box setting ownership verification. DuFFin\nextracts the trigger pattern and the knowledge-level fingerprints to identify\nthe source of a suspect model. We conduct experiments on a variety of models\ncollected from the open-source website, including four popular base models as\nprotected LLMs and their fine-tuning, quantization, and safety alignment\nversions, which are released by large companies, start-ups, and individual\nusers. Results show that our method can accurately verify the copyright of the\nbase protected LLM on their model variants, achieving the IP-ROC metric greater\nthan 0.95. Our code is available at\nhttps://github.com/yuliangyan0807/llm-fingerprint."}
{"id": "2505.16559", "pdf": "https://arxiv.org/pdf/2505.16559.pdf", "abs": "https://arxiv.org/abs/2505.16559", "title": "CTRAP: Embedding Collapse Trap to Safeguard Large Language Models from Harmful Fine-Tuning", "authors": ["Biao Yi", "Tiansheng Huang", "Baolei Zhang", "Tong Li", "Lihai Nie", "Zheli Liu", "Li Shen"], "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Fine-tuning-as-a-service, while commercially successful for Large Language\nModel (LLM) providers, exposes models to harmful fine-tuning attacks. As a\nwidely explored defense paradigm against such attacks, unlearning attempts to\nremove malicious knowledge from LLMs, thereby essentially preventing them from\nbeing used to perform malicious tasks. However, we highlight a critical flaw:\nthe powerful general adaptability of LLMs allows them to easily bypass\nselective unlearning by rapidly relearning or repurposing their capabilities\nfor harmful tasks. To address this fundamental limitation, we propose a\nparadigm shift: instead of selective removal, we advocate for inducing model\ncollapse--effectively forcing the model to \"unlearn everything\"--specifically\nin response to updates characteristic of malicious adaptation. This collapse\ndirectly neutralizes the very general capabilities that attackers exploit,\ntackling the core issue unaddressed by selective unlearning. We introduce the\nCollapse Trap (CTRAP) as a practical mechanism to implement this concept\nconditionally. Embedded during alignment, CTRAP pre-configures the model's\nreaction to subsequent fine-tuning dynamics. If updates during fine-tuning\nconstitute a persistent attempt to reverse safety alignment, the pre-configured\ntrap triggers a progressive degradation of the model's core language modeling\nabilities, ultimately rendering it inert and useless for the attacker.\nCrucially, this collapse mechanism remains dormant during benign fine-tuning,\nensuring the model's utility and general capabilities are preserved for\nlegitimate users. Extensive empirical results demonstrate that CTRAP\neffectively counters harmful fine-tuning risks across various LLMs and attack\nsettings, while maintaining high performance in benign scenarios. Our code is\navailable at https://anonymous.4open.science/r/CTRAP."}
{"id": "2505.16624", "pdf": "https://arxiv.org/pdf/2505.16624.pdf", "abs": "https://arxiv.org/abs/2505.16624", "title": "Grounding Chest X-Ray Visual Question Answering with Generated Radiology Reports", "authors": ["Francesco Dalla Serra", "Patrick Schrempf", "Chaoyang Wang", "Zaiqiao Meng", "Fani Deligianni", "Alison Q. O'Neil"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We present a novel approach to Chest X-ray (CXR) Visual Question Answering\n(VQA), addressing both single-image image-difference questions. Single-image\nquestions focus on abnormalities within a specific CXR (\"What abnormalities are\nseen in image X?\"), while image-difference questions compare two longitudinal\nCXRs acquired at different time points (\"What are the differences between image\nX and Y?\"). We further explore how the integration of radiology reports can\nenhance the performance of VQA models. While previous approaches have\ndemonstrated the utility of radiology reports during the pre-training phase, we\nextend this idea by showing that the reports can also be leveraged as\nadditional input to improve the VQA model's predicted answers. First, we\npropose a unified method that handles both types of questions and\nauto-regressively generates the answers. For single-image questions, the model\nis provided with a single CXR. For image-difference questions, the model is\nprovided with two CXRs from the same patient, captured at different time\npoints, enabling the model to detect and describe temporal changes. Taking\ninspiration from 'Chain-of-Thought reasoning', we demonstrate that performance\non the CXR VQA task can be improved by grounding the answer generator module\nwith a radiology report predicted for the same CXR. In our approach, the VQA\nmodel is divided into two steps: i) Report Generation (RG) and ii) Answer\nGeneration (AG). Our results demonstrate that incorporating predicted radiology\nreports as evidence to the AG model enhances performance on both single-image\nand image-difference questions, achieving state-of-the-art results on the\nMedical-Diff-VQA dataset."}
{"id": "2505.16631", "pdf": "https://arxiv.org/pdf/2505.16631.pdf", "abs": "https://arxiv.org/abs/2505.16631", "title": "MiLQ: Benchmarking IR Models for Bilingual Web Search with Mixed Language Queries", "authors": ["Jonghwi Kim", "Deokhyung Kang", "Seonjeong Hwang", "Yunsu Kim", "Jungseul Ok", "Gary Lee"], "categories": ["cs.IR", "cs.CL"], "comment": "16 pages, 9 figures", "summary": "Despite bilingual speakers frequently using mixed-language queries in web\nsearches, Information Retrieval (IR) research on them remains scarce. To\naddress this, we introduce MiLQ,Mixed-Language Query test set, the first public\nbenchmark of mixed-language queries, confirmed as realistic and highly\npreferred. Experiments show that multilingual IR models perform moderately on\nMiLQ and inconsistently across native, English, and mixed-language queries,\nalso suggesting code-switched training data's potential for robust IR models\nhandling such queries. Meanwhile, intentional English mixing in queries proves\nan effective strategy for bilinguals searching English documents, which our\nanalysis attributes to enhanced token matching compared to native queries."}
{"id": "2505.16673", "pdf": "https://arxiv.org/pdf/2505.16673.pdf", "abs": "https://arxiv.org/abs/2505.16673", "title": "R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO", "authors": ["Huanjin Yao", "Qixiang Yin", "Jingyi Zhang", "Min Yang", "Yibo Wang", "Wenhao Wu", "Fei Su", "Li Shen", "Minghui Qiu", "Dacheng Tao", "Jiaxing Huang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Technical report", "summary": "In this work, we aim to incentivize the reasoning ability of Multimodal Large\nLanguage Models (MLLMs) via reinforcement learning (RL) and develop an\neffective approach that mitigates the sparse reward and advantage vanishing\nissues during RL. To this end, we propose Share-GRPO, a novel RL approach that\ntackle these issues by exploring and sharing diverse reasoning trajectories\nover expanded question space. Specifically, Share-GRPO first expands the\nquestion space for a given question via data transformation techniques, and\nthen encourages MLLM to effectively explore diverse reasoning trajectories over\nthe expanded question space and shares the discovered reasoning trajectories\nacross the expanded questions during RL. In addition, Share-GRPO also shares\nreward information during advantage computation, which estimates solution\nadvantages hierarchically across and within question variants, allowing more\naccurate estimation of relative advantages and improving the stability of\npolicy training. Extensive evaluations over six widely-used reasoning\nbenchmarks showcase the superior performance of our method. Code will be\navailable at https://github.com/HJYao00/R1-ShareVL."}
{"id": "2505.16686", "pdf": "https://arxiv.org/pdf/2505.16686.pdf", "abs": "https://arxiv.org/abs/2505.16686", "title": "SPaRC: A Spatial Pathfinding Reasoning Challenge", "authors": ["Lars Benedikt Kaesberg", "Jan Philip Wahle", "Terry Ruas", "Bela Gipp"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Existing reasoning datasets saturate and fail to test abstract, multi-step\nproblems, especially pathfinding and complex rule constraint satisfaction. We\nintroduce SPaRC (Spatial Pathfinding Reasoning Challenge), a dataset of 1,000\n2D grid pathfinding puzzles to evaluate spatial and symbolic reasoning,\nrequiring step-by-step planning with arithmetic and geometric rules. Humans\nachieve near-perfect accuracy (98.0%; 94.5% on hard puzzles), while the best\nreasoning models, such as o4-mini, struggle (15.8%; 1.1% on hard puzzles).\nModels often generate invalid paths (>50% of puzzles for o4-mini), and\nreasoning tokens reveal they make errors in navigation and spatial logic.\nUnlike humans, who take longer on hard puzzles, models fail to scale test-time\ncompute with difficulty. Allowing models to make multiple solution attempts\nimproves accuracy, suggesting potential for better spatial reasoning with\nimproved training and efficient test-time scaling methods. SPaRC can be used as\na window into models' spatial reasoning limitations and drive research toward\nnew methods that excel in abstract, multi-step problem-solving."}
{"id": "2505.16737", "pdf": "https://arxiv.org/pdf/2505.16737.pdf", "abs": "https://arxiv.org/abs/2505.16737", "title": "Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization", "authors": ["Chengcan Wu", "Zhixin Zhang", "Zeming Wei", "Yihao Zhang", "Meng Sun"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "math.OC"], "comment": null, "summary": "The significant progress of large language models (LLMs) has led to\nremarkable achievements across numerous applications. However, their ability to\ngenerate harmful content has sparked substantial safety concerns. Despite the\nimplementation of safety alignment techniques during the pre-training phase,\nrecent research indicates that fine-tuning LLMs on adversarial or even benign\ndata can inadvertently compromise their safety. In this paper, we re-examine\nthe fundamental issue of why fine-tuning on non-harmful data still results in\nsafety degradation. We introduce a safety-aware probing (SAP) optimization\nframework designed to mitigate the safety risks of fine-tuning LLMs.\nSpecifically, SAP incorporates a safety-aware probe into the gradient\npropagation process, mitigating the model's risk of safety degradation by\nidentifying potential pitfalls in gradient directions, thereby enhancing\ntask-specific performance while successfully preserving model safety. Our\nextensive experimental results demonstrate that SAP effectively reduces\nharmfulness below the original fine-tuned model and achieves comparable test\nloss to standard fine-tuning methods. Our code is available at\nhttps://github.com/ChengcanWu/SAP."}
{"id": "2505.16826", "pdf": "https://arxiv.org/pdf/2505.16826.pdf", "abs": "https://arxiv.org/abs/2505.16826", "title": "KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning", "authors": ["Wei Sun", "Wen Yang", "Pu Jian", "Qianlong Du", "Fuwei Cui", "Shuo Ren", "Jiajun Zhang"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances have demonstrated that integrating reinforcement learning\nwith rule-based rewards can significantly enhance the reasoning capabilities of\nlarge language models, even without supervised fine-tuning. However, prevalent\nreinforcement learning algorithms such as GRPO and its variants like DAPO,\nsuffer from a coarse granularity issue when computing the advantage.\nSpecifically, they compute rollout-level advantages that assign identical\nvalues to every token within a sequence, failing to capture token-specific\ncontributions and hindering effective learning. To address this limitation, we\npropose Key-token Advantage Estimation (KTAE) - a novel algorithm that\nestimates fine-grained, token-level advantages without introducing additional\nmodels. KTAE leverages the correctness of sampled rollouts and applies\nstatistical analysis to quantify the importance of individual tokens within a\nsequence to the final outcome. This quantified token-level importance is then\ncombined with the rollout-level advantage to obtain a more fine-grained\ntoken-level advantage estimation. Empirical results show that models trained\nwith GRPO+KTAE and DAPO+KTAE outperform baseline methods across five\nmathematical reasoning benchmarks. Notably, they achieve higher accuracy with\nshorter responses and even surpass R1-Distill-Qwen-1.5B using the same base\nmodel."}
{"id": "2505.16832", "pdf": "https://arxiv.org/pdf/2505.16832.pdf", "abs": "https://arxiv.org/abs/2505.16832", "title": "From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization", "authors": ["Haonian Ji", "Shi Qiu", "Siyang Xin", "Siwei Han", "Zhaorun Chen", "Hongyi Wang", "Dake Zhang", "Huaxiu Yao"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "16 pages; 7 figures", "summary": "While foundation models (FMs), such as diffusion models and large\nvision-language models (LVLMs), have been widely applied in educational\ncontexts, their ability to generate pedagogically effective visual explanations\nremains limited. Most existing approaches focus primarily on textual reasoning,\noverlooking the critical role of structured and interpretable visualizations in\nsupporting conceptual understanding. To better assess the visual reasoning\ncapabilities of FMs in educational settings, we introduce EduVisBench, a\nmulti-domain, multi-level benchmark. EduVisBench features diverse STEM problem\nsets requiring visually grounded solutions, along with a fine-grained\nevaluation rubric informed by pedagogical theory. Our empirical analysis\nreveals that existing models frequently struggle with the inherent challenge of\ndecomposing complex reasoning and translating it into visual representations\naligned with human cognitive processes. To address these limitations, we\npropose EduVisAgent, a multi-agent collaborative framework that coordinates\nspecialized agents for instructional planning, reasoning decomposition,\nmetacognitive prompting, and visualization design. Experimental results show\nthat EduVisAgent substantially outperforms all baselines, achieving a 40.2%\nimprovement and delivering more educationally aligned visualizations.\nEduVisBench and EduVisAgent are available at\nhttps://github.com/aiming-lab/EduVisBench and\nhttps://github.com/aiming-lab/EduVisAgent."}
{"id": "2505.16850", "pdf": "https://arxiv.org/pdf/2505.16850.pdf", "abs": "https://arxiv.org/abs/2505.16850", "title": "ATR-Bench: A Federated Learning Benchmark for Adaptation, Trust, and Reasoning", "authors": ["Tajamul Ashraf", "Mohammed Mohsen Peerzada", "Moloud Abdar", "Yutong Xie", "Yuyin Zhou", "Xiaofeng Liu", "Iqra Altaf Gillani", "Janibul Bashir"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "Federated Learning Benchmark for Domain Adaptation, Trustworthiness,\n  and Reasoning", "summary": "Federated Learning (FL) has emerged as a promising paradigm for collaborative\nmodel training while preserving data privacy across decentralized participants.\nAs FL adoption grows, numerous techniques have been proposed to tackle its\npractical challenges. However, the lack of standardized evaluation across key\ndimensions hampers systematic progress and fair comparison of FL methods. In\nthis work, we introduce ATR-Bench, a unified framework for analyzing federated\nlearning through three foundational dimensions: Adaptation, Trust, and\nReasoning. We provide an in-depth examination of the conceptual foundations,\ntask formulations, and open research challenges associated with each theme. We\nhave extensively benchmarked representative methods and datasets for adaptation\nto heterogeneous clients and trustworthiness in adversarial or unreliable\nenvironments. Due to the lack of reliable metrics and models for reasoning in\nFL, we only provide literature-driven insights for this dimension. ATR-Bench\nlays the groundwork for a systematic and holistic evaluation of federated\nlearning with real-world relevance. We will make our complete codebase publicly\naccessible and a curated repository that continuously tracks new developments\nand research in the FL literature."}
{"id": "2505.16886", "pdf": "https://arxiv.org/pdf/2505.16886.pdf", "abs": "https://arxiv.org/abs/2505.16886", "title": "Don't \"Overthink\" Passage Reranking: Is Reasoning Truly Necessary?", "authors": ["Nour Jedidi", "Yung-Sung Chuang", "James Glass", "Jimmy Lin"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "With the growing success of reasoning models across complex natural language\ntasks, researchers in the Information Retrieval (IR) community have begun\nexploring how similar reasoning capabilities can be integrated into passage\nrerankers built on Large Language Models (LLMs). These methods typically employ\nan LLM to produce an explicit, step-by-step reasoning process before arriving\nat a final relevance prediction. But, does reasoning actually improve reranking\naccuracy? In this paper, we dive deeper into this question, studying the impact\nof the reasoning process by comparing reasoning-based pointwise rerankers\n(ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under\nidentical training conditions, and observe that StandardRR generally\noutperforms ReasonRR. Building on this observation, we then study the\nimportance of reasoning to ReasonRR by disabling its reasoning process\n(ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more\neffective than ReasonRR. Examining the cause of this result, our findings\nreveal that reasoning-based rerankers are limited by the LLM's reasoning\nprocess, which pushes it toward polarized relevance scores and thus fails to\nconsider the partial relevance of passages, a key factor for the accuracy of\npointwise rerankers."}
{"id": "2505.16888", "pdf": "https://arxiv.org/pdf/2505.16888.pdf", "abs": "https://arxiv.org/abs/2505.16888", "title": "CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious System Prompt Generation and Refining Framework", "authors": ["Viet Pham", "Thai Le"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have advanced many applications, but are also\nknown to be vulnerable to adversarial attacks. In this work, we introduce a\nnovel security threat: hijacking AI-human conversations by manipulating LLMs'\nsystem prompts to produce malicious answers only to specific targeted questions\n(e.g., \"Who should I vote for US President?\", \"Are Covid vaccines safe?\"),\nwhile behaving benignly on others. This attack is detrimental as it can enable\nmalicious actors to exercise large-scale information manipulation by spreading\nharmful but benign-looking system prompts online. To demonstrate such an\nattack, we develop CAIN, an algorithm that can automatically curate such\nharmful system prompts for a specific target question in a black-box setting or\nwithout the need to access the LLM's parameters. Evaluated on both open-source\nand commercial LLMs, CAIN demonstrates significant adversarial impact. In\nuntargeted attacks or forcing LLMs to output incorrect answers, CAIN achieves\nup to 40% F1 degradation on targeted questions while preserving high accuracy\non benign inputs. For targeted attacks or forcing LLMs to output specific\nharmful answers, CAIN achieves over 70% F1 scores on these targeted responses\nwith minimal impact on benign questions. Our results highlight the critical\nneed for enhanced robustness measures to safeguard the integrity and safety of\nLLMs in real-world applications. All source code will be publicly available."}
{"id": "2505.16932", "pdf": "https://arxiv.org/pdf/2505.16932.pdf", "abs": "https://arxiv.org/abs/2505.16932", "title": "The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm", "authors": ["Noah Amsel", "David Persson", "Christopher Musco", "Robert Gower"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NA", "math.NA", "math.OC"], "comment": null, "summary": "Computing the polar decomposition and the related matrix sign function, has\nbeen a well-studied problem in numerical analysis for decades. More recently,\nit has emerged as an important subroutine in deep learning, particularly within\nthe Muon optimization framework. However, the requirements in this setting\ndiffer significantly from those of traditional numerical analysis. In deep\nlearning, methods must be highly efficient and GPU-compatible, but high\naccuracy is often unnecessary. As a result, classical algorithms like\nNewton-Schulz (which suffers from slow initial convergence) and methods based\non rational functions (which rely on QR decompositions or matrix inverses) are\npoorly suited to this context. In this work, we introduce Polar Express, a\nGPU-friendly algorithm for computing the polar decomposition. Like classical\npolynomial methods such as Newton-Schulz, our approach uses only matrix-matrix\nmultiplications, making it GPU-compatible. Motivated by earlier work of Chen &\nChow and Nakatsukasa & Freund, Polar Express adapts the polynomial update rule\nat each iteration by solving a minimax optimization problem, and we prove that\nit enjoys a strong worst-case optimality guarantee. This property ensures both\nrapid early convergence and fast asymptotic convergence. We also address\nfinite-precision issues, making it stable in bfloat16 in practice. We apply\nPolar Express within the Muon optimization framework and show consistent\nimprovements in validation loss on large-scale models such as GPT-2,\noutperforming recent alternatives across a range of learning rates."}
{"id": "2505.16933", "pdf": "https://arxiv.org/pdf/2505.16933.pdf", "abs": "https://arxiv.org/abs/2505.16933", "title": "LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning", "authors": ["Zebin You", "Shen Nie", "Xiaolu Zhang", "Jun Hu", "Jun Zhou", "Zhiwu Lu", "Ji-Rong Wen", "Chongxuan Li"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large\nLanguage Model (MLLM) that integrates visual instruction tuning with masked\ndiffusion models, representing a departure from the autoregressive paradigms\ndominant in current multimodal approaches. Built upon LLaDA, a representative\nlarge language diffusion model, LLaDA-V incorporates a vision encoder and MLP\nconnector that projects visual features into the language embedding space,\nenabling effective multimodal alignment. Our empirical investigation reveals\nseveral intriguing results: First, LLaDA-V demonstrates promising multimodal\nperformance despite its language model being weaker on purely textual tasks\nthan counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same\ninstruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal\ntasks with better data scalability. It also narrows the performance gap to\nQwen2-VL, suggesting the effectiveness of its architecture for multimodal\ntasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal\nunderstanding compared to existing hybrid autoregressive-diffusion and purely\ndiffusion-based MLLMs. Our findings suggest that large language diffusion\nmodels show promise in multimodal contexts and warrant further investigation in\nfuture research. Project page and codes:\nhttps://ml-gsai.github.io/LLaDA-V-demo/."}
{"id": "2505.16938", "pdf": "https://arxiv.org/pdf/2505.16938.pdf", "abs": "https://arxiv.org/abs/2505.16938", "title": "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification", "authors": ["NovelSeek Team", "Bo Zhang", "Shiyang Feng", "Xiangchao Yan", "Jiakang Yuan", "Zhiyin Yu", "Xiaohan He", "Songtao Huang", "Shaowei Hou", "Zheng Nie", "Zhilong Wang", "Jinyao Liu", "Runmin Ma", "Tianshuo Peng", "Peng Ye", "Dongzhan Zhou", "Shufei Zhang", "Xiaosong Wang", "Yilan Zhang", "Meng Li", "Zhongying Tu", "Xiangyu Yue", "Wangli Ouyang", "Bowen Zhou", "Lei Bai"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "HomePage: https://alpha-innovator.github.io/NovelSeek-project-page", "summary": "Artificial Intelligence (AI) is accelerating the transformation of scientific\nresearch paradigms, not only enhancing research efficiency but also driving\ninnovation. We introduce NovelSeek, a unified closed-loop multi-agent framework\nto conduct Autonomous Scientific Research (ASR) across various scientific\nresearch fields, enabling researchers to tackle complicated problems in these\nfields with unprecedented speed and precision. NovelSeek highlights three key\nadvantages: 1) Scalability: NovelSeek has demonstrated its versatility across\n12 scientific research tasks, capable of generating innovative ideas to enhance\nthe performance of baseline code. 2) Interactivity: NovelSeek provides an\ninterface for human expert feedback and multi-agent interaction in automated\nend-to-end processes, allowing for the seamless integration of domain expert\nknowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in\nseveral scientific fields with significantly less time cost compared to human\nefforts. For instance, in reaction yield prediction, it increased from 27.6% to\n35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from\n0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation,\nprecision advanced from 78.8% to 81.0% in a mere 30 hours."}
{"id": "2505.16944", "pdf": "https://arxiv.org/pdf/2505.16944.pdf", "abs": "https://arxiv.org/abs/2505.16944", "title": "AGENTIF: Benchmarking Instruction Following of Large Language Models in Agentic Scenarios", "authors": ["Yunjia Qi", "Hao Peng", "Xiaozhi Wang", "Amy Xin", "Youfeng Liu", "Bin Xu", "Lei Hou", "Juanzi Li"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated advanced capabilities in\nreal-world agentic applications. Growing research efforts aim to develop\nLLM-based agents to address practical demands, introducing a new challenge:\nagentic scenarios often involve lengthy instructions with complex constraints,\nsuch as extended system prompts and detailed tool specifications. While\nadherence to such instructions is crucial for agentic applications, whether\nLLMs can reliably follow them remains underexplored. In this paper, we\nintroduce AgentIF, the first benchmark for systematically evaluating LLM\ninstruction following ability in agentic scenarios. AgentIF features three key\ncharacteristics: (1) Realistic, constructed from 50 real-world agentic\napplications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.\n(3) Complex, averaging 11.9 constraints per instruction, covering diverse\nconstraint types, such as tool specifications and condition constraints. To\nconstruct AgentIF, we collect 707 human-annotated instructions across 50\nagentic tasks from industrial application agents and open-source agentic\nsystems. For each instruction, we annotate the associated constraints and\ncorresponding evaluation metrics, including code-based evaluation, LLM-based\nevaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically\nevaluate existing advanced LLMs. We observe that current models generally\nperform poorly, especially in handling complex constraint structures and tool\nspecifications. We further conduct error analysis and analytical experiments on\ninstruction length and meta constraints, providing some findings about the\nfailure modes of existing LLMs. We have released the code and data to\nfacilitate future research."}
{"id": "2505.16964", "pdf": "https://arxiv.org/pdf/2505.16964.pdf", "abs": "https://arxiv.org/abs/2505.16964", "title": "MedFrameQA: A Multi-Image Medical VQA Benchmark for Clinical Reasoning", "authors": ["Suhao Yu", "Haojin Wang", "Juncheng Wu", "Cihang Xie", "Yuyin Zhou"], "categories": ["cs.CV", "cs.CL"], "comment": "9 pages, 4 Figures Benchmark data:\n  https://huggingface.co/datasets/SuhaoYu1020/MedFrameQA", "summary": "Existing medical VQA benchmarks mostly focus on single-image analysis, yet\nclinicians almost always compare a series of images before reaching a\ndiagnosis. To better approximate this workflow, we introduce MedFrameQA -- the\nfirst benchmark that explicitly evaluates multi-image reasoning in medical VQA.\nTo build MedFrameQA both at scale and in high-quality, we develop 1) an\nautomated pipeline that extracts temporally coherent frames from medical videos\nand constructs VQA items whose content evolves logically across images, and 2)\na multiple-stage filtering strategy, including model-based and manual review,\nto preserve data clarity, difficulty, and medical relevance. The resulting\ndataset comprises 2,851 VQA pairs (gathered from 9,237 high-quality frames in\n3,420 videos), covering nine human body systems and 43 organs; every question\nis accompanied by two to five images. We comprehensively benchmark ten advanced\nMultimodal LLMs -- both proprietary and open source, with and without explicit\nreasoning modules -- on MedFrameQA. The evaluation challengingly reveals that\nall models perform poorly, with most accuracies below 50%, and accuracy\nfluctuates as the number of images per question increases. Error analysis\nfurther shows that models frequently ignore salient findings, mis-aggregate\nevidence across images, and propagate early mistakes through their reasoning\nchains; results also vary substantially across body systems, organs, and\nmodalities. We hope this work can catalyze research on clinically grounded,\nmulti-image reasoning and accelerate progress toward more capable diagnostic AI\nsystems."}
{"id": "2505.16967", "pdf": "https://arxiv.org/pdf/2505.16967.pdf", "abs": "https://arxiv.org/abs/2505.16967", "title": "Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard Negatives for Robust Information Retrieval", "authors": ["Nandan Thakur", "Crystina Zhang", "Xueguang Ma", "Jimmy Lin"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "Code is available at https://github.com/castorini/rlhn & datasets are\n  available at https://huggingface.co/rlhn", "summary": "Training robust retrieval and reranker models typically relies on large-scale\nretrieval datasets; for example, the BGE collection contains 1.6 million\nquery-passage pairs sourced from various data sources. However, we find that\ncertain datasets can negatively impact model effectiveness -- pruning 8 out of\n15 datasets from the BGE collection reduces the training set size by\n2.35$\\times$ and increases nDCG@10 on BEIR by 1.0 point. This motivates a\ndeeper examination of training data quality, with a particular focus on \"false\nnegatives\", where relevant passages are incorrectly labeled as irrelevant. We\npropose a simple, cost-effective approach using cascading LLM prompts to\nidentify and relabel hard negatives. Experimental results show that relabeling\nfalse negatives with true positives improves both E5 (base) and Qwen2.5-7B\nretrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot\nAIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on\nthe relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the\ncascading design is further supported by human annotation results, where we\nfind judgment by GPT-4o shows much higher agreement with humans than\nGPT-4o-mini."}
{"id": "2505.16968", "pdf": "https://arxiv.org/pdf/2505.16968.pdf", "abs": "https://arxiv.org/abs/2505.16968", "title": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark", "authors": ["Ahmed Heakl", "Sarim Hashmi", "Gustavo Bertolo Stahl", "Seung Hun Eddie Han", "Salman Khan", "Abdulrahman Mahmoud"], "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.LG", "cs.PL"], "comment": "20 pages, 11 figures, 5 tables", "summary": "We introduce \\texttt{CASS}, the first large-scale dataset and model suite for\ncross-architecture GPU code transpilation, targeting both source-level\n(CUDA~$\\leftrightarrow$~HIP) and assembly-level (Nvidia\nSASS~$\\leftrightarrow$~AMD RDNA3) translation. The dataset comprises 70k\nverified code pairs across host and device, addressing a critical gap in\nlow-level GPU code portability. Leveraging this resource, we train the\n\\texttt{CASS} family of domain-specific language models, achieving 95\\% source\ntranslation accuracy and 37.5\\% assembly translation accuracy, substantially\noutperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our\ngenerated code matches native performance in over 85\\% of test cases,\npreserving runtime and memory behavior. To support rigorous evaluation, we\nintroduce \\texttt{CASS-Bench}, a curated benchmark spanning 16 GPU domains with\nground-truth execution. All data, models, and evaluation tools are released as\nopen source to foster progress in GPU compiler tooling, binary compatibility,\nand LLM-guided hardware translation. Dataset and benchmark are on\n\\href{https://huggingface.co/datasets/MBZUAI/cass}{\\textcolor{blue}{HuggingFace}},\nwith code at\n\\href{https://github.com/GustavoStahl/CASS}{\\textcolor{blue}{GitHub}}."}
{"id": "2505.16975", "pdf": "https://arxiv.org/pdf/2505.16975.pdf", "abs": "https://arxiv.org/abs/2505.16975", "title": "SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development", "authors": ["Yaxin Du", "Yuzhu Cai", "Yifan Zhou", "Cheng Wang", "Yu Qian", "Xianghe Pang", "Qian Liu", "Yue Hu", "Siheng Chen"], "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown strong capability in diverse software\nengineering tasks, e.g. code completion, bug fixing, and document generation.\nHowever, feature-driven development (FDD), a highly prevalent real-world task\nthat involves developing new functionalities for large, existing codebases,\nremains underexplored. We therefore introduce SWE-Dev, the first large-scale\ndataset (with 14,000 training and 500 test samples) designed to evaluate and\ntrain autonomous coding systems on real-world feature development tasks. To\nensure verifiable and diverse training, SWE-Dev uniquely provides all instances\nwith a runnable environment and its developer-authored executable unit tests.\nThis collection not only provides high-quality data for Supervised Fine-Tuning\n(SFT), but also enables Reinforcement Learning (RL) by delivering accurate\nreward signals from executable unit tests. Our extensive evaluations on\nSWE-Dev, covering 17 chatbot LLMs, 10 reasoning models, and 10 Multi-Agent\nSystems (MAS), reveal that FDD is a profoundly challenging frontier for current\nAI (e.g., Claude-3.7-Sonnet achieves only 22.45\\% Pass@3 on the hard test\nsplit). Crucially, we demonstrate that SWE-Dev serves as an effective platform\nfor model improvement: fine-tuning on training set enabled a 7B model\ncomparable to GPT-4o on \\textit{hard} split, underscoring the value of its\nhigh-quality training data. Code is available here\n\\href{https://github.com/justLittleWhite/SWE-Dev}{https://github.com/justLittleWhite/SWE-Dev}."}
{"id": "2505.16984", "pdf": "https://arxiv.org/pdf/2505.16984.pdf", "abs": "https://arxiv.org/abs/2505.16984", "title": "UFT: Unifying Supervised and Reinforcement Fine-Tuning", "authors": ["Mingyang Liu", "Gabriele Farina", "Asuman Ozdaglar"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Post-training has demonstrated its importance in enhancing the reasoning\ncapabilities of large language models (LLMs). The primary post-training methods\ncan be categorized into supervised fine-tuning (SFT) and reinforcement\nfine-tuning (RFT). SFT is efficient and well-suited for small language models,\nbut it may lead to overfitting and limit the reasoning abilities of larger\nmodels. In contrast, RFT generally yields better generalization but depends\nheavily on the strength of the base model. To address the limitations of SFT\nand RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm\nthat unifies SFT and RFT into a single, integrated process. UFT enables the\nmodel to effectively explore solutions while incorporating informative\nsupervision signals, bridging the gap between memorizing and thinking\nunderlying existing methods. Notably, UFT outperforms both SFT and RFT in\ngeneral, regardless of model sizes. Furthermore, we theoretically prove that\nUFT breaks RFT's inherent exponential sample complexity bottleneck, showing for\nthe first time that unified training can exponentially accelerate convergence\non long-horizon reasoning tasks."}
{"id": "2505.16994", "pdf": "https://arxiv.org/pdf/2505.16994.pdf", "abs": "https://arxiv.org/abs/2505.16994", "title": "$\\text{R}^2\\text{ec}$: Towards Large Recommender Models with Reasoning", "authors": ["Runyang You", "Yongqi Li", "Xinyu Lin", "Xin Zhang", "Wenjie Wang", "Wenjie Li", "Liqiang Nie"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large recommender models have extended LLMs as powerful recommenders via\nencoding or item generation, and recent breakthroughs in LLM reasoning\nsynchronously motivate the exploration of reasoning in recommendation. Current\nstudies usually position LLMs as external reasoning modules to yield auxiliary\nthought for augmenting conventional recommendation pipelines. However, such\ndecoupled designs are limited in significant resource cost and suboptimal joint\noptimization. To address these issues, we propose \\name, a unified large\nrecommender model with intrinsic reasoning capabilities. Initially, we\nreconceptualize the model architecture to facilitate interleaved reasoning and\nrecommendation in the autoregressive process. Subsequently, we propose RecPO, a\ncorresponding reinforcement learning framework that optimizes \\name\\ both the\nreasoning and recommendation capabilities simultaneously in a single policy\nupdate; RecPO introduces a fused reward scheme that solely leverages\nrecommendation labels to simulate the reasoning capability, eliminating\ndependency on specialized reasoning annotations. Experiments on three datasets\nwith various baselines verify the effectiveness of \\name, showing relative\nimprovements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at\nhttps://github.com/YRYangang/RRec."}
{"id": "2505.16997", "pdf": "https://arxiv.org/pdf/2505.16997.pdf", "abs": "https://arxiv.org/abs/2505.16997", "title": "X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs", "authors": ["Rui Ye", "Xiangrui Liu", "Qimin Wu", "Xianghe Pang", "Zhenfei Yin", "Lei Bai", "Siheng Chen"], "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": "19 pages, 5 figures", "summary": "LLM-based multi-agent systems (MAS) extend the capabilities of single LLMs by\nenabling cooperation among multiple specialized agents. However, most existing\nMAS frameworks rely on a single LLM to drive all agents, constraining the\nsystem's intelligence to the limit of that model. This paper explores the\nparadigm of heterogeneous LLM-driven MAS (X-MAS), where agents are powered by\ndiverse LLMs, elevating the system's potential to the collective intelligence\nof diverse LLMs. We introduce X-MAS-Bench, a comprehensive testbed designed to\nevaluate the performance of various LLMs across different domains and\nMAS-related functions. As an extensive empirical study, we assess 27 LLMs\nacross 5 domains (encompassing 21 test sets) and 5 functions, conducting over\n1.7 million evaluations to identify optimal model selections for each\ndomain-function combination. Building on these findings, we demonstrate that\ntransitioning from homogeneous to heterogeneous LLM-driven MAS can\nsignificantly enhance system performance without requiring structural redesign.\nSpecifically, in a chatbot-only MAS scenario, the heterogeneous configuration\nyields up to 8.4\\% performance improvement on the MATH dataset. In a mixed\nchatbot-reasoner scenario, the heterogeneous MAS could achieve a remarkable\n47\\% performance boost on the AIME dataset. Our results underscore the\ntransformative potential of heterogeneous LLMs in MAS, highlighting a promising\navenue for advancing scalable, collaborative AI systems."}
{"id": "2505.17015", "pdf": "https://arxiv.org/pdf/2505.17015.pdf", "abs": "https://arxiv.org/abs/2505.17015", "title": "Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models", "authors": ["Runsen Xu", "Weiyao Wang", "Hao Tang", "Xingyu Chen", "Xiaodong Wang", "Fu-Jen Chu", "Dahua Lin", "Matt Feiszli", "Kevin J. Liang"], "categories": ["cs.CV", "cs.CL"], "comment": "24 pages. An MLLM, dataset, and benchmark for multi-frame spatial\n  understanding. Project page: https://runsenxu.com/projects/Multi-SpatialMLLM", "summary": "Multi-modal large language models (MLLMs) have rapidly advanced in visual\ntasks, yet their spatial understanding remains limited to single images,\nleaving them ill-suited for robotics and other real-world applications that\nrequire multi-frame reasoning. In this paper, we propose a framework to equip\nMLLMs with robust multi-frame spatial understanding by integrating depth\nperception, visual correspondence, and dynamic perception. Central to our\napproach is the MultiSPA dataset, a novel, large-scale collection of more than\n27 million samples spanning diverse 3D and 4D scenes. Alongside MultiSPA, we\nintroduce a comprehensive benchmark that tests a wide spectrum of spatial tasks\nunder uniform metrics. Our resulting model, Multi-SpatialMLLM, achieves\nsignificant gains over baselines and proprietary systems, demonstrating\nscalable, generalizable multi-frame reasoning. We further observe multi-task\nbenefits and early indications of emergent capabilities in challenging\nscenarios, and showcase how our model can serve as a multi-frame reward\nannotator for robotics."}
{"id": "2505.17017", "pdf": "https://arxiv.org/pdf/2505.17017.pdf", "abs": "https://arxiv.org/abs/2505.17017", "title": "Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO", "authors": ["Chengzhuo Tong", "Ziyu Guo", "Renrui Zhang", "Wenyu Shan", "Xinyu Wei", "Zhenghao Xing", "Hongsheng Li", "Pheng-Ann Heng"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Code is released at https://github.com/ZiyuGuo99/Image-Generation-CoT", "summary": "Recent advancements underscore the significant role of Reinforcement Learning\n(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large\nlanguage models (LLMs). Two prominent RL algorithms, Direct Preference\nOptimization (DPO) and Group Relative Policy Optimization (GRPO), are central\nto these developments, showcasing different pros and cons. Autoregressive image\ngeneration, also interpretable as a sequential CoT reasoning process, presents\nunique challenges distinct from LLM-based CoT reasoning. These encompass\nensuring text-image consistency, improving image aesthetic quality, and\ndesigning sophisticated reward models, rather than relying on simpler\nrule-based rewards. While recent efforts have extended RL to this domain, these\nexplorations typically lack an in-depth analysis of the domain-specific\nchallenges and the characteristics of different RL strategies. To bridge this\ngap, we provide the first comprehensive investigation of the GRPO and DPO\nalgorithms in autoregressive image generation, evaluating their in-domain\nperformance and out-of-domain generalization, while scrutinizing the impact of\ndifferent reward models on their respective capabilities. Our findings reveal\nthat GRPO and DPO exhibit distinct advantages, and crucially, that reward\nmodels possessing stronger intrinsic generalization capabilities potentially\nenhance the generalization potential of the applied RL algorithms. Furthermore,\nwe systematically explore three prevalent scaling strategies to enhance both\ntheir in-domain and out-of-domain proficiency, deriving unique insights into\nefficiently scaling performance for each paradigm. We hope our study paves a\nnew path for inspiring future work on developing more effective RL algorithms\nto achieve robust CoT reasoning in the realm of autoregressive image\ngeneration. Code is released at\nhttps://github.com/ZiyuGuo99/Image-Generation-CoT"}
{"id": "2505.17022", "pdf": "https://arxiv.org/pdf/2505.17022.pdf", "abs": "https://arxiv.org/abs/2505.17022", "title": "GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning", "authors": ["Chengqi Duan", "Rongyao Fang", "Yuqing Wang", "Kun Wang", "Linjiang Huang", "Xingyu Zeng", "Hongsheng Li", "Xihui Liu"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Github page refer to: https://github.com/gogoduan/GoT-R1", "summary": "Visual generation models have made remarkable progress in creating realistic\nimages from text prompts, yet struggle with complex prompts that specify\nmultiple objects with precise spatial relationships and attributes. Effective\nhandling of such prompts requires explicit reasoning about the semantic content\nand spatial layout. We present GoT-R1, a framework that applies reinforcement\nlearning to enhance semantic-spatial reasoning in visual generation. Building\nupon the Generation Chain-of-Thought approach, GoT-R1 enables models to\nautonomously discover effective reasoning strategies beyond predefined\ntemplates through carefully designed reinforcement learning. To achieve this,\nwe propose a dual-stage multi-dimensional reward framework that leverages MLLMs\nto evaluate both the reasoning process and final output, enabling effective\nsupervision across the entire generation pipeline. The reward system assesses\nsemantic alignment, spatial accuracy, and visual quality in a unified approach.\nExperimental results demonstrate significant improvements on T2I-CompBench\nbenchmark, particularly in compositional tasks involving precise spatial\nrelationships and attribute binding. GoT-R1 advances the state-of-the-art in\nimage generation by successfully transferring sophisticated reasoning\ncapabilities to the visual generation domain. To facilitate future research, we\nmake our code and pretrained models publicly available at\nhttps://github.com/gogoduan/GoT-R1."}
{"id": "2310.08232", "pdf": "https://arxiv.org/pdf/2310.08232.pdf", "abs": "https://arxiv.org/abs/2310.08232", "title": "Language Models are Universal Embedders", "authors": ["Xin Zhang", "Zehan Li", "Yanzhao Zhang", "Dingkun Long", "Pengjun Xie", "Meishan Zhang", "Min Zhang"], "categories": ["cs.CL"], "comment": "XLLM Workshop, ACL 2025", "summary": "In the large language model (LLM) revolution, embedding is a key component of\nvarious systems, such as retrieving knowledge or memories for LLMs or building\ncontent moderation filters. As such cases span from English to other natural or\nprogramming languages, from retrieval to classification and beyond, it is\nadvantageous to build a unified embedding model rather than dedicated ones for\neach scenario. In this context, the pre-trained multilingual decoder-only large\nlanguage models, e.g., BLOOM, emerge as a viable backbone option. To assess\ntheir potential, we propose straightforward strategies for constructing\nembedders and introduce a universal evaluation benchmark. Experimental results\nshow that our trained model is proficient at generating good embeddings across\nlanguages and tasks, even extending to languages and tasks for which no\nfinetuning/pretraining data is available. We also present detailed analyses and\nadditional evaluations. We hope that this work could encourage the development\nof more robust open-source universal embedders."}
{"id": "2312.13772", "pdf": "https://arxiv.org/pdf/2312.13772.pdf", "abs": "https://arxiv.org/abs/2312.13772", "title": "Large Language Models are Miscalibrated In-Context Learners", "authors": ["Chengzu Li", "Han Zhou", "Goran Glavaš", "Anna Korhonen", "Ivan Vulić"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, 4 figures, 5 tables (20 pages, 5 figures, 13 tables\n  including references and appendices)", "summary": "When adapting ICL with or without fine-tuning, we are curious about whether\nthe instruction-tuned language model is able to achieve well-calibrated results\nwithout suffering from the problem of overconfidence (i.e., miscalibration)\nconsidering its strong instruction following ability, especially in such\nlimited data setups. In this work, we deliver an in-depth analysis of the\nbehavior across different choices of learning methods from the perspective of\nboth performance and calibration. Through extensive controlled experiments, we\nobserve that the miscalibration problem exists across all learning methods in\nlow-resource setups. To achieve simultaneous gain for both in-task performance\nand calibration, we then study the potential of self-ensembling applied at\ndifferent modeling stages (e.g., variations of in-context examples or\nvariations in prompts or different ensembling strategies) to make the\npredictions more calibrated and have comparable or even better performance. We\nfind that self-ensembling with max probability produces robust and calibrated\npredictions. Our work reveals the potential calibration problem of using ICL\ndespite the improvements in task performance and sheds light on which learning\nparadigm to choose. We also provide practical guidelines for choosing learning\nparadigms depending on whether the data has been seen by the model before and a\nworthwhile solution via self-ensembling on how to enhance both task performance\nand calibration of LMs, which we hope could encourage further study."}
{"id": "2402.06738", "pdf": "https://arxiv.org/pdf/2402.06738.pdf", "abs": "https://arxiv.org/abs/2402.06738", "title": "EntGPT: Entity Linking with Generative Large Language Models", "authors": ["Yifan Ding", "Amrit Poudel", "Qingkai Zeng", "Tim Weninger", "Balaji Veeramani", "Sanmitra Bhattacharya"], "categories": ["cs.CL", "H.3.3"], "comment": null, "summary": "Entity Linking in natural language processing seeks to match text entities to\ntheir corresponding entries in a dictionary or knowledge base. Traditional\napproaches rely on contextual models, which can be complex, hard to train, and\nhave limited transferability across different domains. Generative large\nlanguage models like GPT offer a promising alternative but often underperform\nwith naive prompts. In this study, we introduce EntGPT, employing advanced\nprompt engineering to enhance EL tasks. Our three-step hard-prompting method\n(EntGPT-P) significantly boosts the micro-F_1 score by up to 36% over vanilla\nprompts, achieving competitive performance across 10 datasets without\nsupervised fine-tuning. Additionally, our instruction tuning method (EntGPT-I)\nimproves micro-F_1 scores by 2.1% on average in supervised EL tasks and\noutperforms several baseline models in six Question Answering tasks. Our\nmethods are compatible with both open-source and proprietary LLMs. All data and\ncode are available on GitHub at https://github.com/yifding/In_Context_EL."}
{"id": "2403.10056", "pdf": "https://arxiv.org/pdf/2403.10056.pdf", "abs": "https://arxiv.org/abs/2403.10056", "title": "Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning", "authors": ["Yongquan He", "Wenyuan Zhang", "Xuancheng Huang", "Peng Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 6 figures", "summary": "Instruction tuning for large language models (LLMs) can drive them to produce\nresults consistent with human goals in specific downstream tasks. However, the\nprocess of continual instruction tuning (CIT) for LLMs may bring about the\ncatastrophic forgetting (CF) problem, where previously learned abilities are\ndegraded. Recent methods try to alleviate the CF problem by modifying models or\nreplaying data, which may only remember the surface-level pattern of\ninstructions and get confused on held-out tasks. In this paper, we propose a\nnovel continual instruction tuning method based on Key-part Information Gain\n(KPIG). Our method computes the information gain on masked parts to dynamically\nreplay data and refine the training objective, which enables LLMs to capture\ntask-aware information relevant to the correct response and alleviate\noverfitting to general descriptions in instructions. In addition, we propose\ntwo metrics, P-score and V-score, to measure the generalization and\ninstruction-following abilities of LLMs. Experiments demonstrate our method\nachieves superior performance on both seen and held-out tasks."}
{"id": "2405.04756", "pdf": "https://arxiv.org/pdf/2405.04756.pdf", "abs": "https://arxiv.org/abs/2405.04756", "title": "Red-Teaming for Inducing Societal Bias in Large Language Models", "authors": ["Chu Fei Luo", "Ahmad Ghawanmeh", "Bharat Bhimshetty", "Kashyap Murali", "Murli Jadhav", "Xiaodan Zhu", "Faiza Khan Khattak"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Ensuring the safe deployment of AI systems is critical in industry settings\nwhere biased outputs can lead to significant operational, reputational, and\nregulatory risks. Thorough evaluation before deployment is essential to prevent\nthese hazards. Red-teaming addresses this need by employing adversarial attacks\nto develop guardrails that detect and reject biased or harmful queries,\nenabling models to be retrained or steered away from harmful outputs. However,\nmost red-teaming efforts focus on harmful or unethical instructions rather than\naddressing social bias, leaving this critical area under-explored despite its\nsignificant real-world impact, especially in customer-facing systems. We\npropose two bias-specific red-teaming methods, Emotional Bias Probe (EBP) and\nBiasKG, to evaluate how standard safety measures for harmful content affect\nbias. For BiasKG, we refactor natural language stereotypes into a knowledge\ngraph. We use these attacking strategies to induce biased responses from\nseveral open- and closed-source language models. Unlike prior work, these\nmethods specifically target social bias. We find our method increases bias in\nall models, even those trained with safety guardrails. Our work emphasizes\nuncovering societal bias in LLMs through rigorous evaluation, and recommends\nmeasures ensure AI safety in high-stakes industry deployments."}
{"id": "2406.10594", "pdf": "https://arxiv.org/pdf/2406.10594.pdf", "abs": "https://arxiv.org/abs/2406.10594", "title": "BlockPruner: Fine-grained Pruning for Large Language Models", "authors": ["Longguang Zhong", "Fanqi Wan", "Ruijun Chen", "Xiaojun Quan", "Liangzhi Li"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "With the rapid growth in the size and complexity of large language models\n(LLMs), the costs associated with their training and inference have escalated\nsignificantly. Research indicates that certain layers in LLMs harbor\nsubstantial redundancy, and pruning these layers has minimal impact on the\noverall performance. While various layer pruning methods have been developed\nbased on this insight, they generally overlook the finer-grained redundancies\nwithin the layers themselves. In this paper, we delve deeper into the\narchitecture of LLMs and demonstrate that finer-grained pruning can be achieved\nby targeting redundancies in multi-head attention (MHA) and multi-layer\nperceptron (MLP) blocks. We propose a novel, training-free structured pruning\napproach called BlockPruner. Unlike existing layer pruning methods, BlockPruner\nsegments each Transformer layer into MHA and MLP blocks. It then assesses the\nimportance of these blocks using perplexity measures and applies a heuristic\nsearch for iterative pruning. We applied BlockPruner to LLMs of various sizes\nand architectures and validated its performance across a wide range of\ndownstream tasks. Experimental results show that BlockPruner achieves more\ngranular and effective pruning compared to state-of-the-art baselines."}
{"id": "2409.02393", "pdf": "https://arxiv.org/pdf/2409.02393.pdf", "abs": "https://arxiv.org/abs/2409.02393", "title": "Determination of language families using deep learning", "authors": ["Peter B. Lerner"], "categories": ["cs.CL", "I.2.7"], "comment": "Second draft with improved statistics of NN simulations. Comments are\n  welcome", "summary": "We use a c-GAN (convolutional generative adversarial) neural network to\nanalyze transliterated text fragments of extant, dead comprehensible, and one\ndead non-deciphered (Cypro-Minoan) language to establish linguistic affinities.\nThe paper is agnostic with respect to translation and/or deciphering. However,\nthere is hope that the proposed approach can be useful for decipherment with\nmore sophisticated neural network techniques."}
{"id": "2409.02813", "pdf": "https://arxiv.org/pdf/2409.02813.pdf", "abs": "https://arxiv.org/abs/2409.02813", "title": "MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark", "authors": ["Xiang Yue", "Tianyu Zheng", "Yuansheng Ni", "Yubo Wang", "Kai Zhang", "Shengbang Tong", "Yuxuan Sun", "Botao Yu", "Ge Zhang", "Huan Sun", "Yu Su", "Wenhu Chen", "Graham Neubig"], "categories": ["cs.CL", "cs.CV"], "comment": "ACL 2025 Main", "summary": "This paper introduces MMMU-Pro, a robust version of the Massive\nMulti-discipline Multimodal Understanding and Reasoning (MMMU) benchmark.\nMMMU-Pro rigorously assesses multimodal models' true understanding and\nreasoning capabilities through a three-step process based on MMMU: (1)\nfiltering out questions answerable by text-only models, (2) augmenting\ncandidate options, and (3) introducing a vision-only input setting where\nquestions are embedded within images. This setting challenges AI to truly \"see\"\nand \"read\" simultaneously, testing a fundamental human cognitive skill of\nseamlessly integrating visual and textual information. Results show that model\nperformance is substantially lower on MMMU-Pro than on MMMU, ranging from 16.8%\nto 26.9% across models. We explore the impact of OCR prompts and Chain of\nThought (CoT) reasoning, finding that OCR prompts have minimal effect while CoT\ngenerally improves performance. MMMU-Pro provides a more rigorous evaluation\ntool, closely mimicking real-world scenarios and offering valuable directions\nfor future research in multimodal AI."}
{"id": "2409.03327", "pdf": "https://arxiv.org/pdf/2409.03327.pdf", "abs": "https://arxiv.org/abs/2409.03327", "title": "Normal forms in Virus Machines", "authors": ["A. Ramírez-de-Arellano", "F. G. C. Cabarle", "D. Orellana-Martín", "M. J. Pérez-Jiménez"], "categories": ["cs.CL", "cs.FL", "68Q07 (Primary) 68Q10, 68R01 (Secondary)", "F.0; F.1.1"], "comment": "24 pages, 14 figures", "summary": "In the present work, we further study the computational power of virus\nmachines (VMs in short).VMs provide a computing paradigm inspired by the\ntransmission and replication networks of viruses.VMs consist of process units\n(called hosts) structured by a directed graph whose arcs are called channels\nand an instruction graph that controls the transmissions of virus objects among\nhosts. The present work complements our understanding of the computing power of\nVMs by introducing normal forms; these expressions restrict the features in a\ngiven computing model.Some of the features that we restrict in our normal forms\ninclude (a) the number of hosts, (b) the number of instructions, and (c) the\nnumber of virus objects in each host. After we recall some known results on the\ncomputing power of VMs we give our series of normal forms, such as the size of\nthe loops in the network, proving new characterisations of family of sets, such\nas finite sets, semilinear sets, or recursively enumerable sets (NRE)."}
{"id": "2409.18199", "pdf": "https://arxiv.org/pdf/2409.18199.pdf", "abs": "https://arxiv.org/abs/2409.18199", "title": "LangSAMP: Language-Script Aware Multilingual Pretraining", "authors": ["Yihong Liu", "Haotian Ye", "Chunlan Ma", "Mingyang Wang", "Hinrich Schütze"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Recent multilingual pretrained language models (mPLMs) often avoid using\nlanguage embeddings -- learnable vectors assigned to individual languages.\nHowever, this places a significant burden on token representations to encode\nall language-specific information, which may hinder language neutrality. To\naddress this limitation, we propose Language-Script Aware Multilingual\nPretraining (LangSAMP), a method that incorporates both language and script\nembeddings to enhance representation learning. Specifically, we integrate these\nembeddings into the output of the Transformer blocks before passing the final\nrepresentations to the language modeling head for prediction. We apply LangSAMP\nto the continual pretraining of XLM-R on a highly multilingual corpus covering\nmore than 500 languages. The resulting model consistently outperforms the\nbaseline in zero-shot crosslingual transfer across diverse downstream tasks.\nExtensive analysis reveals that language and script embeddings capture\nlanguage- and script-specific nuances, which benefits more language-neutral\nrepresentations, proven by improved pairwise cosine similarity. In our case\nstudy, we also show that language and script embeddings can be used to select\nbetter source languages for crosslingual transfer. We make our code and models\npublicly available at https://github.com/cisnlp/LangSAMP."}
{"id": "2410.05254", "pdf": "https://arxiv.org/pdf/2410.05254.pdf", "abs": "https://arxiv.org/abs/2410.05254", "title": "GLEE: A Unified Framework and Benchmark for Language-based Economic Environments", "authors": ["Eilam Shapira", "Omer Madmon", "Itamar Reinman", "Samuel Joseph Amouyal", "Roi Reichart", "Moshe Tennenholtz"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.GT", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) show significant potential in economic and\nstrategic interactions, where communication via natural language is often\nprevalent. This raises key questions: Do LLMs behave rationally? How do they\nperform compared to humans? Do they tend to reach an efficient and fair\noutcome? What is the role of natural language in strategic interaction? How do\ncharacteristics of the economic environment influence these dynamics? These\nquestions become crucial concerning the economic and societal implications of\nintegrating LLM-based agents into real-world data-driven systems, such as\nonline retail platforms and recommender systems. To answer these questions, we\nintroduce a benchmark for standardizing research on two-player, sequential,\nlanguage-based games. Inspired by the economic literature, we define three base\nfamilies of games with consistent parameterization, degrees of freedom and\neconomic measures to evaluate agents' performance (self-gain), as well as the\ngame outcome (efficiency and fairness). We develop an open-source framework for\ninteraction simulation and analysis, and utilize it to collect a dataset of LLM\nvs. LLM interactions across numerous game configurations and an additional\ndataset of human vs. LLM interactions. Through extensive experimentation, we\ndemonstrate how our framework and dataset can be used to: (i) compare the\nbehavior of LLM-based agents in various economic contexts; (ii) evaluate agents\nin both individual and collective performance measures; and (iii) quantify the\neffect of the economic characteristics of the environments on the behavior of\nagents. Our results suggest that the market parameters, as well as the choice\nof the LLMs, tend to have complex and interdependent effects on the economic\noutcome, which calls for careful design and analysis of the language-based\neconomic ecosystem."}
{"id": "2410.08085", "pdf": "https://arxiv.org/pdf/2410.08085.pdf", "abs": "https://arxiv.org/abs/2410.08085", "title": "Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study Over Open-ended Question Answering", "authors": ["Yuan Sui", "Yufei He", "Zifeng Ding", "Bryan Hooi"], "categories": ["cs.CL", "cs.AI"], "comment": "This paper has been accepted by ACL 2025", "summary": "Recent works integrating Knowledge Graphs (KGs) have shown promising\nimprovements in enhancing the reasoning capabilities of Large Language Models\n(LLMs). However, existing benchmarks primarily focus on closed-ended tasks,\nleaving a gap in evaluating performance on more complex, real-world scenarios.\nThis limitation also hinders a thorough assessment of KGs' potential to reduce\nhallucinations in LLMs. To address this, we introduce OKGQA, a new benchmark\nspecifically designed to evaluate LLMs augmented with KGs in open-ended,\nreal-world question answering settings. OKGQA reflects practical complexities\nthrough diverse question types and incorporates metrics to quantify both\nhallucination rates and reasoning improvements in LLM+KG models. To consider\nthe scenarios in which KGs may contain varying levels of errors, we propose a\nbenchmark variant, OKGQA-P, to assess model performance when the semantics and\nstructure of KGs are deliberately perturbed and contaminated. In this paper, we\naims to (1) explore whether KGs can make LLMs more trustworthy in an open-ended\nsetting, and (2) conduct a comparative analysis to shed light on method design.\nWe believe this study can facilitate a more complete performance comparison and\nencourages continuous improvement in integrating KGs with LLMs to mitigate\nhallucination, and make LLMs more trustworthy. Code and data are released at\nhttps://github.com/Y-Sui/OKGQA."}
{"id": "2410.09338", "pdf": "https://arxiv.org/pdf/2410.09338.pdf", "abs": "https://arxiv.org/abs/2410.09338", "title": "Keys to Robust Edits: from Theoretical Insights to Practical Advances", "authors": ["Jianhao Yan", "Futing Wang", "Yun Luo", "Yafu Li", "Yue Zhang"], "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference", "summary": "Large language models (LLMs) struggle with maintaining accurate knowledge due\nto conflicting/outdated parametric memories. While locate-and-edit methods\naddress this, their reliance on models' internal representations leads to\nrobustness failures in long-context reasoning and paraphrased queries. We\nidentify a fundamental limitation of locate-and-edit methods: existing semantic\nkeys (for memory localization) cannot simultaneously satisfy robustness\n(context-invariant activation) and specificity (precise knowledge\ndiscrimination). Through theoretical error-bound analysis, we establish formal\ncriteria for effective editing. Our solution introduces \\textit{Robust Edit\nPathway (REP)}, a plug-and-play module that: (1) disentangles editing keys from\nnative model representations; (2) dynamically adjusts keys via contrastive\nlearning to achieve robustness-specificity balance. Extensive experiments\nacross various editing methods (ROME/MEMIT/R-ROME/EMMET), existing LLMs\n(LLaMA2, QWen, Mistral), and datasets (CounterFact, ZsRE) show that REP\nimproves success rate over robustness tests by up-to 66.4\\% while maintaining\nthe success rate unaffected. Our code can be found at\nhttps://github.com/ElliottYan/RobustKeyEdit ."}
{"id": "2410.10347", "pdf": "https://arxiv.org/pdf/2410.10347.pdf", "abs": "https://arxiv.org/abs/2410.10347", "title": "A Unified Approach to Routing and Cascading for LLMs", "authors": ["Jasper Dekoninck", "Maximilian Baader", "Martin Vechev"], "categories": ["cs.CL"], "comment": null, "summary": "The availability of a wide range of large language models (LLMs) embedded in\nvarious agentic systems has significantly increased the potential of model\nselection strategies to improve the cost-performance tradeoff. Existing\nstrategies involve either routing, where a single model is chosen per query, or\ncascading, which sequentially runs increasingly larger models until a\nsatisfactory answer is found. However, current approaches face three key\nlimitations: they (1) lack formal proofs of optimality, (2) fail to identify\nthe conditions under which these strategies are most effective to improve the\ncost-performance tradeoff, and (3) are unable to combine both paradigms for\nfurther improvements. To address these issues, we first derive a novel optimal\nstrategy for cascading and prove the optimality of an existing routing\nstrategy. Further, we propose cascade routing, a unified framework that\nintegrates routing and cascading into a theoretically optimal strategy. Through\nour analysis, we identify good quality estimators as the critical factor for\nthe success of model selection paradigms. Finally, in our experiments, we show\nthat cascade routing consistently outperforms the individual approaches by a\nlarge margin and we analyze quality estimators to determine when routing and/or\ncascading are useful paradigms for model selection."}
{"id": "2410.17477", "pdf": "https://arxiv.org/pdf/2410.17477.pdf", "abs": "https://arxiv.org/abs/2410.17477", "title": "Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of Architectural Inductive Biases on Hallucination", "authors": ["Jerry Huang", "Prasanna Parthasarathi", "Mehdi Rezagholizadeh", "Boxing Chen", "Sarath Chandar"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to Findings of The 63rd Annual Meeting of the Association\n  for Computational Linguistics (ACL), 2025", "summary": "The growth in prominence of large language models (LLMs) in everyday life can\nbe largely attributed to their generative abilities, yet some of this is also\nowed to the risks and costs associated with their use. On one front is their\ntendency to hallucinate false or misleading information, limiting their\nreliability. On another is the increasing focus on the computational\nlimitations associated with traditional self-attention based LLMs, which has\nbrought about new alternatives, in particular recurrent models, meant to\novercome them. Yet it remains uncommon to consider these two concerns\nsimultaneously. Do changes in architecture exacerbate/alleviate existing\nconcerns about hallucinations? Do they affect how and where they occur? Through\nan extensive evaluation, we study how these architecture-based inductive biases\naffect the propensity to hallucinate. While hallucination remains a general\nphenomenon not limited to specific architectures, the situations in which they\noccur and the ease with which specific types of hallucinations can be induced\ncan significantly differ based on the model architecture. These findings\nhighlight the need for better understanding both these problems in conjunction\nwith each other, as well as consider how to design more universal techniques\nfor handling hallucinations."}
{"id": "2410.22316", "pdf": "https://arxiv.org/pdf/2410.22316.pdf", "abs": "https://arxiv.org/abs/2410.22316", "title": "Understanding Synthetic Context Extension via Retrieval Heads", "authors": ["Xinyu Zhao", "Fangcong Yin", "Greg Durrett"], "categories": ["cs.CL"], "comment": "Published at ICML 2025", "summary": "Long-context LLMs are increasingly in demand for applications such as\nretrieval-augmented generation. To defray the cost of pretraining LLMs over\nlong contexts, recent work takes an approach of synthetic context extension:\nfine-tuning LLMs with synthetically generated long-context data in a\npost-training stage. However, it remains unclear how and why this synthetic\ncontext extension imparts abilities for downstream long-context tasks. In this\npaper, we investigate fine-tuning on synthetic data for three long-context\ntasks that require retrieval and reasoning. We vary the realism of \"needle\"\nconcepts to be retrieved and diversity of the surrounding \"haystack\" context,\nfrom using LLMs to construct synthetic documents to using templated relations\nand creating symbolic datasets. We find that models trained on synthetic data\nfall short of the real data, but surprisingly, the mismatch can be interpreted\nand even predicted in terms of a special set of attention heads that are\nresponsible for retrieval over long context, retrieval heads (Wu et al., 2024).\nThe retrieval heads learned on synthetic data have high overlap with retrieval\nheads learned on real data, and there is a strong correlation between the\nrecall of heads learned and the downstream performance of a model. Furthermore,\nwith attention knockout and activation patching, we mechanistically show that\nretrieval heads are necessary and explain model performance, although they are\nnot totally sufficient. Our results shed light on how to interpret synthetic\ndata fine-tuning performance and how to approach creating better data for\nlearning real-world capabilities over long contexts."}
{"id": "2410.22394", "pdf": "https://arxiv.org/pdf/2410.22394.pdf", "abs": "https://arxiv.org/abs/2410.22394", "title": "AAAR-1.0: Assessing AI's Potential to Assist Research", "authors": ["Renze Lou", "Hanzi Xu", "Sijia Wang", "Jiangshu Du", "Ryo Kamoi", "Xiaoxin Lu", "Jian Xie", "Yuxuan Sun", "Yusen Zhang", "Jihyun Janice Ahn", "Hongchao Fang", "Zhuoyang Zou", "Wenchao Ma", "Xi Li", "Kai Zhang", "Congying Xia", "Lifu Huang", "Wenpeng Yin"], "categories": ["cs.CL"], "comment": "ICML 2025. Project Webpage: https://renzelou.github.io/AAAR-1.0/", "summary": "Numerous studies have assessed the proficiency of AI systems, particularly\nlarge language models (LLMs), in facilitating everyday tasks such as email\nwriting, question answering, and creative content generation. However,\nresearchers face unique challenges and opportunities in leveraging LLMs for\ntheir own work, such as brainstorming research ideas, designing experiments,\nand writing or reviewing papers. In this study, we introduce AAAR-1.0, a\nbenchmark dataset designed to evaluate LLM performance in three fundamental,\nexpertise-intensive research tasks: (i) EquationInference, assessing the\ncorrectness of equations based on the contextual information in paper\nsubmissions; (ii) ExperimentDesign, designing experiments to validate research\nideas and solutions; (iii) PaperWeakness, identifying weaknesses in paper\nsubmissions; and (iv) REVIEWCRITIQUE, identifying each segment in human reviews\nis deficient or not. AAAR-1.0 differs from prior benchmarks in two key ways:\nfirst, it is explicitly research-oriented, with tasks requiring deep domain\nexpertise; second, it is researcher-oriented, mirroring the primary activities\nthat researchers engage in on a daily basis. An evaluation of both open-source\nand proprietary LLMs reveals their potential as well as limitations in\nconducting sophisticated research tasks. We will keep iterating AAAR-1.0 to new\nversions."}
{"id": "2411.02454", "pdf": "https://arxiv.org/pdf/2411.02454.pdf", "abs": "https://arxiv.org/abs/2411.02454", "title": "Graph-based Confidence Calibration for Large Language Models", "authors": ["Yukun Li", "Sijia Wang", "Lifu Huang", "Li-Ping Liu"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Reliable confidence estimation is essential for enhancing the trustworthiness\nof large language models (LLMs), especially in high-stakes scenarios. Despite\nits importance, accurately estimating confidence in LLM responses remains a\nsignificant challenge. In this work, we propose using an auxiliary learning\nmodel to assess response correctness based on the self-consistency of multiple\noutputs generated by the LLM. Our method builds a consistency graph to\nrepresent the agreement among multiple responses and uses a graph neural\nnetwork (GNN) to estimate the likelihood that each response is correct.\nExperiments demonstrate that this method has strong calibration performance on\nvarious benchmark datasets and generalizes well to out-of-domain cases."}
{"id": "2411.04847", "pdf": "https://arxiv.org/pdf/2411.04847.pdf", "abs": "https://arxiv.org/abs/2411.04847", "title": "Prompt-Guided Internal States for Hallucination Detection of Large Language Models", "authors": ["Fujie Zhang", "Peiqi Yu", "Biao Yi", "Baolei Zhang", "Tong Li", "Zheli Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks in different domains. However, they sometimes generate\nresponses that are logically coherent but factually incorrect or misleading,\nwhich is known as LLM hallucinations. Data-driven supervised methods train\nhallucination detectors by leveraging the internal states of LLMs, but\ndetectors trained on specific domains often struggle to generalize well to\nother domains. In this paper, we aim to enhance the cross-domain performance of\nsupervised detectors with only in-domain data. We propose a novel framework,\nprompt-guided internal states for hallucination detection of LLMs, namely\nPRISM. By utilizing appropriate prompts to guide changes to the structure\nrelated to text truthfulness in LLMs' internal states, we make this structure\nmore salient and consistent across texts from different domains. We integrated\nour framework with existing hallucination detection methods and conducted\nexperiments on datasets from different domains. The experimental results\nindicate that our framework significantly enhances the cross-domain\ngeneralization of existing hallucination detection methods."}
{"id": "2412.01031", "pdf": "https://arxiv.org/pdf/2412.01031.pdf", "abs": "https://arxiv.org/abs/2412.01031", "title": "Evaluating Automated Radiology Report Quality through Fine-Grained Phrasal Grounding of Clinical Findings", "authors": ["Razi Mahmood", "Pingkun Yan", "Diego Machado Reyes", "Ge Wang", "Mannudeep K. Kalra", "Parisa Kaviani", "Joy T. Wu", "Tanveer Syeda-Mahmood"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Several evaluation metrics have been developed recently to automatically\nassess the quality of generative AI reports for chest radiographs based only on\ntextual information using lexical, semantic, or clinical named entity\nrecognition methods. In this paper, we develop a new method of report quality\nevaluation by first extracting fine-grained finding patterns capturing the\nlocation, laterality, and severity of a large number of clinical findings. We\nthen performed phrasal grounding to localize their associated anatomical\nregions on chest radiograph images. The textual and visual measures are then\ncombined to rate the quality of the generated reports. We present results that\ncompare this evaluation metric with other textual metrics on a gold standard\ndataset derived from the MIMIC collection and show its robustness and\nsensitivity to factual errors."}
{"id": "2412.06272", "pdf": "https://arxiv.org/pdf/2412.06272.pdf", "abs": "https://arxiv.org/abs/2412.06272", "title": "Evaluating LLM-based Approaches to Legal Citation Prediction: Domain-specific Pre-training, Fine-tuning, or RAG? A Benchmark and an Australian Law Case Study", "authors": ["Jiuzhou Han", "Paul Burgess", "Ehsan Shareghi"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "For code, data, and models see https://auslawbench.github.io", "summary": "Large Language Models (LLMs) have demonstrated strong potential across legal\ntasks, yet the problem of legal citation prediction remains under-explored. At\nits core, this task demands fine-grained contextual understanding and precise\nidentification of relevant legislation or precedent. We introduce the AusLaw\nCitation Benchmark, a real-world dataset comprising 55k Australian legal\ninstances and 18,677 unique citations which to the best of our knowledge is the\nfirst of its scale and scope. We then conduct a systematic benchmarking across\na range of solutions: (i) standard prompting of both general and\nlaw-specialised LLMs, (ii) retrieval-only pipelines with both generic and\ndomain-specific embeddings, (iii) supervised fine-tuning, and (iv) several\nhybrid strategies that combine LLMs with retrieval augmentation through query\nexpansion, voting ensembles, or re-ranking. Results show that neither general\nnor law-specific LLMs suffice as stand-alone solutions, with performance near\nzero. Instruction tuning (of even a generic open-source LLM) on task-specific\ndataset is among the best performing solutions. We highlight that database\ngranularity along with the type of embeddings play a critical role in\nretrieval-based approaches, with hybrid methods which utilise a trained\nre-ranker delivering the best results. Despite this, a performance gap of\nnearly 50% remains, underscoring the value of this challenging benchmark as a\nrigorous test-bed for future research in legal-domain."}
{"id": "2412.07367", "pdf": "https://arxiv.org/pdf/2412.07367.pdf", "abs": "https://arxiv.org/abs/2412.07367", "title": "My Words Imply Your Opinion: Reader Agent-based Propagation Enhancement for Personalized Implicit Emotion Analysis", "authors": ["Jian Liao", "Yu Feng", "Yujin Zheng", "Jun Zhao", "Suge Wang", "Jianxing Zheng"], "categories": ["cs.CL"], "comment": null, "summary": "The subtlety of emotional expressions makes implicit emotion analysis (IEA)\nparticularly sensitive to user-specific characteristics. Current studies\npersonalize emotion analysis by focusing on the author but neglect the impact\nof the intended reader on implicit emotional feedback. In this paper, we\nintroduce Personalized IEA (PIEA) and present the RAPPIE model, which addresses\nsubjective variability by incorporating reader feedback. In particular, (1) we\ncreate reader agents based on large language models to simulate reader\nfeedback, overcoming the issue of ``spiral of silence effect'' and data\nincompleteness of real reader reaction. (2) We develop a role-aware multi-view\ngraph learning to model the emotion interactive propagation process in\nscenarios with sparse reader information. (3) We construct two new PIEA\ndatasets covering English and Chinese social media with detailed user metadata,\naddressing the text-centric limitation of existing datasets. Extensive\nexperiments show that RAPPIE significantly outperforms state-of-the-art\nbaselines, demonstrating the value of incorporating reader feedback in PIEA."}
{"id": "2412.12459", "pdf": "https://arxiv.org/pdf/2412.12459.pdf", "abs": "https://arxiv.org/abs/2412.12459", "title": "LITA: An Efficient LLM-assisted Iterative Topic Augmentation Framework", "authors": ["Chia-Hsuan Chang", "Jui-Tse Tsai", "Yi-Hang Tsai", "San-Yih Hwang"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted to PAKDD 2025", "summary": "Topic modeling is widely used for uncovering thematic structures within text\ncorpora, yet traditional models often struggle with specificity and coherence\nin domain-focused applications. Guided approaches, such as SeededLDA and CorEx,\nincorporate user-provided seed words to improve relevance but remain\nlabor-intensive and static. Large language models (LLMs) offer potential for\ndynamic topic refinement and discovery, yet their application often incurs high\nAPI costs. To address these challenges, we propose the LLM-assisted Iterative\nTopic Augmentation framework (LITA), an LLM-assisted approach that integrates\nuser-provided seeds with embedding-based clustering and iterative refinement.\nLITA identifies a small number of ambiguous documents and employs an LLM to\nreassign them to existing or new topics, minimizing API costs while enhancing\ntopic quality. Experiments on two datasets across topic quality and clustering\nperformance metrics demonstrate that LITA outperforms five baseline models,\nincluding LDA, SeededLDA, CorEx, BERTopic, and PromptTopic. Our work offers an\nefficient and adaptable framework for advancing topic modeling and text\nclustering."}
{"id": "2412.12505", "pdf": "https://arxiv.org/pdf/2412.12505.pdf", "abs": "https://arxiv.org/abs/2412.12505", "title": "DocFusion: A Unified Framework for Document Parsing Tasks", "authors": ["Mingxu Chai", "Ziyu Shen", "Chong Zhang", "Yue Zhang", "Xiao Wang", "Shihan Dou", "Jihua Kang", "Jiazheng Zhang", "Qi Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Document parsing is essential for analyzing complex document structures and\nextracting fine-grained information, supporting numerous downstream\napplications. However, existing methods often require integrating multiple\nindependent models to handle various parsing tasks, leading to high complexity\nand maintenance overhead. To address this, we propose DocFusion, a lightweight\ngenerative model with only 0.28B parameters. It unifies task representations\nand achieves collaborative training through an improved objective function.\nExperiments reveal and leverage the mutually beneficial interaction among\nrecognition tasks, and integrating recognition data significantly enhances\ndetection performance. The final results demonstrate that DocFusion achieves\nstate-of-the-art (SOTA) performance across four key tasks."}
{"id": "2412.16555", "pdf": "https://arxiv.org/pdf/2412.16555.pdf", "abs": "https://arxiv.org/abs/2412.16555", "title": "Divide and Conquer: A Hybrid Strategy Defeats Multimodal Large Language Models", "authors": ["Yanxu Mao", "Peipei Liu", "Tiehan Cui", "Zhaoteng Yan", "Congying Liu", "Datao You"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are widely applied in various fields of society\ndue to their powerful reasoning, understanding, and generation capabilities.\nHowever, the security issues associated with these models are becoming\nincreasingly severe. Jailbreaking attacks, as an important method for detecting\nvulnerabilities in LLMs, have been explored by researchers who attempt to\ninduce these models to generate harmful content through various attack methods.\nNevertheless, existing jailbreaking methods face numerous limitations, such as\nexcessive query counts, limited coverage of jailbreak modalities, low attack\nsuccess rates, and simplistic evaluation methods. To overcome these\nconstraints, this paper proposes a multimodal jailbreaking method: JMLLM. This\nmethod integrates multiple strategies to perform comprehensive jailbreak\nattacks across text, visual, and auditory modalities. Additionally, we\ncontribute a new and comprehensive dataset for multimodal jailbreaking\nresearch: TriJail, which includes jailbreak prompts for all three modalities.\nExperiments on the TriJail dataset and the benchmark dataset AdvBench,\nconducted on 13 popular LLMs, demonstrate advanced attack success rates and\nsignificant reduction in time overhead."}
{"id": "2412.17933", "pdf": "https://arxiv.org/pdf/2412.17933.pdf", "abs": "https://arxiv.org/abs/2412.17933", "title": "BenCzechMark : A Czech-centric Multitask and Multimetric Benchmark for Large Language Models with Duel Scoring Mechanism", "authors": ["Martin Fajcik", "Martin Docekal", "Jan Dolezal", "Karel Ondrej", "Karel Beneš", "Jan Kapsa", "Pavel Smrz", "Alexander Polok", "Michal Hradis", "Zuzana Neverilova", "Ales Horak", "Radoslav Sabol", "Michal Stefanik", "Adam Jirkovsky", "David Adamczyk", "Petr Hyner", "Jan Hula", "Hynek Kydlicek"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to TACL", "summary": "We present BenCzechMark (BCM), the first comprehensive Czech language\nbenchmark designed for large language models, offering diverse tasks, multiple\ntask formats, and multiple evaluation metrics. Its duel scoring system is\ngrounded in statistical significance theory and uses aggregation across tasks\ninspired by social preference theory. Our benchmark encompasses 50 challenging\ntasks, with corresponding test datasets, primarily in native Czech, with 14\nnewly collected ones. These tasks span 8 categories and cover diverse domains,\nincluding historical Czech news, essays from pupils or language learners, and\nspoken word. Furthermore, we collect and clean BUT-Large Czech Collection, the\nlargest publicly available clean Czech language corpus, and use it for (i)\ncontamination analysis and (ii) continuous pretraining of the first\nCzech-centric 7B language model with Czech-specific tokenization. We use our\nmodel as a baseline for comparison with publicly available multilingual models.\nLastly, we release and maintain a leaderboard with existing 50 model\nsubmissions, where new model submissions can be made at\nhttps://huggingface.co/spaces/CZLC/BenCzechMark."}
{"id": "2501.05714", "pdf": "https://arxiv.org/pdf/2501.05714.pdf", "abs": "https://arxiv.org/abs/2501.05714", "title": "How to Enable Effective Cooperation Between Humans and NLP Models: A Survey of Principles, Formalizations, and Beyond", "authors": ["Chen Huang", "Yang Deng", "Wenqiang Lei", "Jiancheng Lv", "Tat-Seng Chua", "Jimmy Xiangji Huang"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "ACL 2025 Main paper", "summary": "With the advancement of large language models (LLMs), intelligent models have\nevolved from mere tools to autonomous agents with their own goals and\nstrategies for cooperating with humans. This evolution has birthed a novel\nparadigm in NLP, i.e., human-model cooperation, that has yielded remarkable\nprogress in numerous NLP tasks in recent years. In this paper, we take the\nfirst step to present a thorough review of human-model cooperation, exploring\nits principles, formalizations, and open challenges. In particular, we\nintroduce a new taxonomy that provides a unified perspective to summarize\nexisting approaches. Also, we discuss potential frontier areas and their\ncorresponding challenges. We regard our work as an entry point, paving the way\nfor more breakthrough research in this regard."}
{"id": "2501.18101", "pdf": "https://arxiv.org/pdf/2501.18101.pdf", "abs": "https://arxiv.org/abs/2501.18101", "title": "Diverse Preference Optimization", "authors": ["Jack Lanchantin", "Angelica Chen", "Shehzaad Dhuliawala", "Ping Yu", "Jason Weston", "Sainbayar Sukhbaatar", "Ilia Kulikov"], "categories": ["cs.CL"], "comment": null, "summary": "Post-training of language models, either through reinforcement learning,\npreference optimization or supervised finetuning, tends to sharpen the output\nprobability distribution and reduce the diversity of generated responses. This\nis particularly a problem for creative generative tasks where varied responses\nare desired. In this work we introduce Diverse Preference Optimization (DivPO),\nan optimization method which learns to generate much more diverse responses\nthan standard pipelines, while maintaining the quality of the generations. In\nDivPO, preference pairs are selected by first considering a pool of responses,\nand a measure of diversity among them, and selecting chosen examples as being\nmore rare but high quality, while rejected examples are more common, but low\nquality. DivPO results in generating 45.6% more diverse persona attributes, and\na 74.6% increase in story diversity, while maintaining similar win rates as\nstandard baselines. On general instruction following, DivPO results in a 46.2%\nincrease in diversity, and a 2.4% winrate improvement compared to DPO."}
{"id": "2502.00675", "pdf": "https://arxiv.org/pdf/2502.00675.pdf", "abs": "https://arxiv.org/abs/2502.00675", "title": "ReFoRCE: A Text-to-SQL Agent with Self-Refinement, Consensus Enforcement, and Column Exploration", "authors": ["Minghang Deng", "Ashwin Ramachandran", "Canwen Xu", "Lanxiang Hu", "Zhewei Yao", "Anupam Datta", "Hao Zhang"], "categories": ["cs.CL", "I.2.7; I.2.0; H.2.0"], "comment": "32 pages, 2 figures", "summary": "We present ReFoRCE, a Text-to-SQL agent that tops the Spider 2.0\nleaderboard--a challenging benchmark reflecting complex, real-world Text-to-SQL\nscenarios. While Text-to-SQL systems enable natural language queries over\nstructured databases, deploying them in enterprise environments remains\ndifficult due to large, complex schemas (with over 1,000 columns), diverse SQL\ndialects (e.g., BigQuery, Snowflake), and sophisticated query requirements\n(e.g., transformations and analytics). ReFoRCE addresses these challenges\nthrough: (a) database information compression via pattern-based table grouping\nand LLM-guided schema linking to alleviate long-context issues; (b)\nself-refinement to iteratively correct syntax and semantic errors across\ndialects; (c) majority-vote consensus to select high-confidence candidates\nwhile deferring ambiguous cases arising from sophisticated queries; and (d)\niterative column exploration guided by execution feedback to resolve those\ndeferred cases. ReFoRCE achieves new state-of-the-art results, with scores of\n35.83 on Spider 2.0-Snow and 36.56 on Spider 2.0-Lite."}
{"id": "2502.00761", "pdf": "https://arxiv.org/pdf/2502.00761.pdf", "abs": "https://arxiv.org/abs/2502.00761", "title": "FIRE: Flexible Integration of Data Quality Ratings for Effective Pre-Training", "authors": ["Liangyu Xu", "Xuemiao Zhang", "Feiyu Duan", "Sirui Wang", "Rongxiang Weng", "Jingang Wang", "Xunliang Cai"], "categories": ["cs.CL"], "comment": "21 pages, 11 figures", "summary": "Selecting high-quality data can improve the pretraining efficiency of large\nlanguage models (LLMs). Existing methods generally rely on heuristic techniques\nor single quality signals, limiting their ability to evaluate data quality\ncomprehensively. In this work, we propose FIRE, a flexible and scalable\nframework for integrating multiple data quality raters, which allows for a\ncomprehensive assessment of data quality across various dimensions. FIRE aligns\nmultiple quality signals into a unified space, and integrates diverse data\nquality raters to provide a comprehensive quality signal for each data point.\nFurther, we introduce a progressive data selection scheme based on FIRE that\niteratively refines the selection of high-quality data points. Extensive\nexperiments show that FIRE outperforms other data selection methods and\nsignificantly boosts pretrained model performance across a wide range of\ndownstream tasks, while requiring less than 37.5\\% of the training data needed\nby the Random baseline to reach the target performance."}
{"id": "2502.04380", "pdf": "https://arxiv.org/pdf/2502.04380.pdf", "abs": "https://arxiv.org/abs/2502.04380", "title": "Diversity as a Reward: Fine-Tuning LLMs on a Mixture of Domain-Undetermined Data", "authors": ["Zhenqing Ling", "Daoyuan Chen", "Liuyi Yao", "Qianli Shen", "Yaliang Li", "Ying Shen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "33 pages, 20 figures, 21 tables", "summary": "Fine-tuning large language models (LLMs) using diverse datasets is crucial\nfor enhancing their overall performance across various domains. In practical\nscenarios, existing methods based on modeling the mixture proportions of data\ncomposition often struggle with data whose domain labels are missing, imprecise\nor non-normalized, while methods based on data selection usually encounter\ndifficulties in balancing multi-domain performance. To address these\nchallenges, in this work, we investigate the role of data diversity in\nenhancing the overall abilities of LLMs by empirically constructing contrastive\ndata pools and theoretically deriving explanations. Building upon the insights\ngained, we propose a new method that gives the LLM a dual identity: an output\nmodel to cognitively probe and select data based on diversity reward, as well\nas an input model to be tuned with the selected data. Extensive experiments\nshow that the proposed method notably boosts performance across\ndomain-undetermined data and a series of foundational downstream tasks when\napplied to various advanced LLMs. We release our code and hope this study can\nshed light on the understanding of data diversity and advance feedback-driven\ndata-model co-design for LLMs."}
{"id": "2502.06086", "pdf": "https://arxiv.org/pdf/2502.06086.pdf", "abs": "https://arxiv.org/abs/2502.06086", "title": "Is a Peeled Apple Still Red? Evaluating LLMs' Ability for Conceptual Combination with Property Type", "authors": ["Seokwon Song", "Taehyun Lee", "Jaewoo Ahn", "Jae Hyuk Sung", "Gunhee Kim"], "categories": ["cs.CL"], "comment": "NAACL 2025 Oral", "summary": "Conceptual combination is a cognitive process that merges basic concepts,\nenabling the creation of complex expressions. During this process, the\nproperties of combination (e.g., the whiteness of a peeled apple) can be\ninherited from basic concepts, newly emerge, or be canceled. However, previous\nstudies have evaluated a limited set of properties and have not examined the\ngenerative process. To address this gap, we introduce the Conceptual\nCombination with Property Type dataset (CCPT), which consists of 12.3K\nannotated triplets of noun phrases, properties, and property types. Using CCPT,\nwe establish three types of tasks to evaluate LLMs for conceptual combination\nthoroughly. Our key findings are threefold: (1) Our automatic metric grading\nproperty emergence and cancellation closely corresponds with human judgments.\n(2) LLMs, including OpenAI's o1, struggle to generate noun phrases which\npossess given emergent properties. (3) Our proposed method, inspired by\ncognitive psychology model that explains how relationships between concepts are\nformed, improves performances in all generative tasks. The dataset and\nexperimental code are available at https://github.com/seokwon99/CCPT.git."}
{"id": "2502.06139", "pdf": "https://arxiv.org/pdf/2502.06139.pdf", "abs": "https://arxiv.org/abs/2502.06139", "title": "LCIRC: A Recurrent Compression Approach for Efficient Long-form Context and Query Dependent Modeling in LLMs", "authors": ["Sumin An", "Junyoung Sung", "Wonpyo Park", "Chanjun Park", "Paul Hongsuck Seo"], "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025. Project Page:\n  https://ssuminan.github.io/LCIRC/", "summary": "While large language models (LLMs) excel in generating coherent and\ncontextually rich outputs, their capacity to efficiently handle long-form\ncontexts is limited by fixed-length position embeddings. Additionally, the\ncomputational cost of processing long sequences increases quadratically, making\nit challenging to extend context length. To address these challenges, we\npropose Long-form Context Injection with Recurrent Compression (LCIRC), a\nmethod that enables the efficient processing long-form sequences beyond the\nmodel's length limit through recurrent compression without retraining the\nentire model. We further introduce query dependent context modeling, which\nselectively compresses query-relevant information, ensuring that the model\nretains the most pertinent content. Our empirical results demonstrate that\nQuery Dependent LCIRC (QD-LCIRC) significantly improves LLM's ability to manage\nextended contexts, making it well-suited for tasks that require both\ncomprehensive context understanding and query relevance."}
{"id": "2502.06205", "pdf": "https://arxiv.org/pdf/2502.06205.pdf", "abs": "https://arxiv.org/abs/2502.06205", "title": "C-3PO: Compact Plug-and-Play Proxy Optimization to Achieve Human-like Retrieval-Augmented Generation", "authors": ["Guoxin Chen", "Minpeng Liao", "Peiying Yu", "Dingmin Wang", "Zile Qiao", "Chao Yang", "Xin Zhao", "Kai Fan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Camera ready version for ICML 2025", "summary": "Retrieval-augmented generation (RAG) systems face a fundamental challenge in\naligning independently developed retrievers and large language models (LLMs).\nExisting approaches typically involve modifying either component or introducing\nsimple intermediate modules, resulting in practical limitations and sub-optimal\nperformance. Inspired by human search behavior -- typically involving a\nback-and-forth process of proposing search queries and reviewing documents, we\npropose C-3PO, a proxy-centric framework that facilitates communication between\nretrievers and LLMs through a lightweight multi-agent system. Our framework\nimplements three specialized agents that collaboratively optimize the entire\nRAG pipeline without altering the retriever and LLMs. These agents work\ntogether to assess the need for retrieval, generate effective queries, and\nselect information suitable for the LLMs. To enable effective multi-agent\ncoordination, we develop a tree-structured rollout approach for reward credit\nassignment in reinforcement learning. Extensive experiments in both in-domain\nand out-of-distribution scenarios demonstrate that C-3PO significantly enhances\nRAG performance while maintaining plug-and-play flexibility and superior\ngeneralization capabilities."}
{"id": "2502.08550", "pdf": "https://arxiv.org/pdf/2502.08550.pdf", "abs": "https://arxiv.org/abs/2502.08550", "title": "No Need for Explanations: LLMs can implicitly learn from mistakes in-context", "authors": ["Lisa Alazraki", "Maximilian Mozes", "Jon Ander Campos", "Tan Yi-Chern", "Marek Rei", "Max Bartolo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Showing incorrect answers to Large Language Models (LLMs) is a popular\nstrategy to improve their performance in reasoning-intensive tasks. It is\nwidely assumed that, in order to be helpful, the incorrect answers must be\naccompanied by comprehensive rationales, explicitly detailing where the\nmistakes are and how to correct them. However, in this work we present a\ncounterintuitive finding: we observe that LLMs perform better in math reasoning\ntasks when these rationales are eliminated from the context and models are left\nto infer on their own what makes an incorrect answer flawed. This approach also\nsubstantially outperforms chain-of-thought prompting in our evaluations. These\nresults are consistent across LLMs of different sizes and varying reasoning\nabilities. To gain an understanding of why LLMs learn from mistakes more\neffectively without explicit corrective rationales, we perform a thorough\nanalysis, investigating changes in context length and answer diversity between\ndifferent prompting strategies, and their effect on performance. We also\nexamine evidence of overfitting to the in-context rationales when these are\nprovided, and study the extent to which LLMs are able to autonomously infer\nhigh-quality corrective rationales given only incorrect answers as input. We\nfind evidence that, while incorrect answers are more beneficial for LLM\nlearning than additional diverse correct answers, explicit corrective\nrationales over-constrain the model, thus limiting those benefits."}
{"id": "2502.09604", "pdf": "https://arxiv.org/pdf/2502.09604.pdf", "abs": "https://arxiv.org/abs/2502.09604", "title": "SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models", "authors": ["Yung-Sung Chuang", "Benjamin Cohen-Wang", "Shannon Zejiang Shen", "Zhaofeng Wu", "Hu Xu", "Xi Victoria Lin", "James Glass", "Shang-Wen Li", "Wen-tau Yih"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML 2025 main conference paper. The source code is available at\n  https://github.com/facebookresearch/SelfCite", "summary": "We introduce SelfCite, a novel self-supervised approach that aligns LLMs to\ngenerate high-quality, fine-grained, sentence-level citations for the\nstatements in their generated responses. Instead of only relying on costly and\nlabor-intensive annotations, SelfCite leverages a reward signal provided by the\nLLM itself through context ablation: If a citation is necessary, removing the\ncited text from the context should prevent the same response; if sufficient,\nretaining the cited text alone should preserve the same response. This reward\ncan guide the inference-time best-of-N sampling strategy to improve citation\nquality significantly, as well as be used in preference optimization to\ndirectly fine-tune the models for generating better citations. The\neffectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3\npoints on the LongBench-Cite benchmark across five long-form question answering\ntasks. The source code is available at\nhttps://github.com/facebookresearch/SelfCite"}
{"id": "2502.09674", "pdf": "https://arxiv.org/pdf/2502.09674.pdf", "abs": "https://arxiv.org/abs/2502.09674", "title": "The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Analysis of Orthogonal Safety Directions", "authors": ["Wenbo Pan", "Zhichao Liu", "Qiguang Chen", "Xiangyang Zhou", "Haining Yu", "Xiaohua Jia"], "categories": ["cs.CL", "cs.AI"], "comment": "Code and artifacts: https://github.com/BMPixel/safety-residual-space\n  Accepted by ICML 2025", "summary": "Large Language Models' safety-aligned behaviors, such as refusing harmful\nqueries, can be represented by linear directions in activation space. Previous\nresearch modeled safety behavior with a single direction, limiting mechanistic\nunderstanding to an isolated safety feature. In this work, we discover that\nsafety-aligned behavior is jointly controlled by multi-dimensional directions.\nNamely, we study the vector space of representation shifts during safety\nfine-tuning on Llama 3 8B for refusing jailbreaks. By studying orthogonal\ndirections in the space, we first find that a dominant direction governs the\nmodel's refusal behavior, while multiple smaller directions represent distinct\nand interpretable features like hypothetical narrative and role-playing. We\nthen measure how different directions promote or suppress the dominant\ndirection, showing the important role of secondary directions in shaping the\nmodel's refusal representation. Finally, we demonstrate that removing certain\ntrigger tokens in harmful queries can mitigate these directions to bypass the\nlearned safety capability, providing new insights on understanding safety\nalignment vulnerability from a multi-dimensional perspective. Code and\nartifacts are available at https://github.com/BMPixel/safety-residual-space."}
{"id": "2502.11018", "pdf": "https://arxiv.org/pdf/2502.11018.pdf", "abs": "https://arxiv.org/abs/2502.11018", "title": "GRIFFIN: Effective Token Alignment for Faster Speculative Decoding", "authors": ["Shijing Hu", "Jingyang Li", "Xingyu Xie", "Zhihui Lu", "Kim-Chuan Toh", "Pan Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Speculative decoding accelerates inference in large language models (LLMs) by\ngenerating multiple draft tokens simultaneously. However, existing methods\noften struggle with token misalignment between the training and decoding\nphases, limiting their performance. To address this, we propose GRIFFIN, a\nnovel framework that incorporates a token-alignable training strategy and a\ntoken-alignable draft model to mitigate misalignment. The training strategy\nemploys a loss masking mechanism to exclude highly misaligned tokens during\ntraining, preventing them from negatively impacting the draft model's\noptimization. The token-alignable draft model introduces input tokens to\ncorrect inconsistencies in generated features. Experiments on LLaMA, Vicuna,\nQwen and Mixtral models demonstrate that GRIFFIN achieves an average acceptance\nlength improvement of over 8% and a speedup ratio exceeding 7%, outperforming\ncurrent speculative decoding state-of-the-art methods. Our code and GRIFFIN's\ndraft models are released publicly in https://github.com/hsj576/GRIFFIN."}
{"id": "2502.11438", "pdf": "https://arxiv.org/pdf/2502.11438.pdf", "abs": "https://arxiv.org/abs/2502.11438", "title": "SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example Selection for Text-to-SQL", "authors": ["Jimin Lee", "Ingeol Baek", "Byeongjeong Kim", "Hyunkyung Bae", "Hwanhee Lee"], "categories": ["cs.CL"], "comment": "13 pages, 5 figures, 10 tables", "summary": "Text-to-SQL aims to convert natural language questions into executable SQL\nqueries. While previous approaches, such as skeleton-masked selection, have\ndemonstrated strong performance by retrieving similar training examples to\nguide large language models (LLMs), they struggle in real-world scenarios where\nsuch examples are unavailable. To overcome this limitation, we propose\nSelf-Augmentation in-context learning with Fine-grained Example selection for\nText-to-SQL (SAFE-SQL), a novel framework that improves SQL generation by\ngenerating and filtering self-augmented examples. SAFE-SQL first prompts an LLM\nto generate multiple Text-to-SQL examples relevant to the test input. Then\nSAFE-SQL filters these examples through three relevance assessments,\nconstructing high-quality in-context learning examples. Using self-generated\nexamples, SAFE-SQL surpasses the previous zero-shot, and few-shot Text-to-SQL\nframeworks, achieving higher execution accuracy. Notably, our approach provides\nadditional performance gains in extra hard and unseen scenarios, where\nconventional methods often fail."}
{"id": "2502.11471", "pdf": "https://arxiv.org/pdf/2502.11471.pdf", "abs": "https://arxiv.org/abs/2502.11471", "title": "GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language for Knowledge Graph Completion", "authors": ["Kangyang Luo", "Yuzhuo Bai", "Cheng Gao", "Shuzheng Si", "Yingli Shen", "Zhu Liu", "Zhitong Wang", "Cunliang Kong", "Wenhao Li", "Yufei Huang", "Ye Tian", "Xuantang Xiong", "Lei Han", "Maosong Sun"], "categories": ["cs.CL", "cs.IR"], "comment": "Accepted by ACL2025(Findings)", "summary": "Knowledge Graph Completion (KGC), which aims to infer missing or incomplete\nfacts, is a crucial task for KGs. However, integrating the vital structural\ninformation of KGs into Large Language Models (LLMs) and outputting predictions\ndeterministically remains challenging. To address this, we propose a new method\ncalled GLTW, which encodes the structural information of KGs and merges it with\nLLMs to enhance KGC performance. Specifically, we introduce an improved Graph\nTransformer (iGT) that effectively encodes subgraphs with both local and global\nstructural information and inherits the characteristics of language model,\nbypassing training from scratch. Also, we develop a subgraph-based\nmulti-classification training objective, using all entities within KG as\nclassification objects, to boost learning efficiency.Importantly, we combine\niGT with an LLM that takes KG language prompts as input.Our extensive\nexperiments on various KG datasets show that GLTW achieves significant\nperformance gains compared to SOTA baselines."}
{"id": "2502.11811", "pdf": "https://arxiv.org/pdf/2502.11811.pdf", "abs": "https://arxiv.org/abs/2502.11811", "title": "FineFilter: A Fine-grained Noise Filtering Mechanism for Retrieval-Augmented Large Language Models", "authors": ["Qianchi Zhang", "Hainan Zhang", "Liang Pang", "Ziwei Wang", "Hongwei Zheng", "Yongxin Tong", "Zhiming Zheng"], "categories": ["cs.CL"], "comment": "18 pages, 4 figures, 18 tables, under review", "summary": "Retrieved documents containing noise will hinder Retrieval-Augmented\nGeneration (RAG) from detecting answer clues, necessitating noise filtering\nmechanisms to enhance accuracy. Existing methods use reranking or summarization\nto identify the most relevant sentences, but directly and accurately locating\nanswer clues from these large-scale and complex documents remains challenging.\nUnlike these document-level operations, we treat noise filtering as a\nsentence-level MinMax optimization problem: first identifying potential clues\nfrom multiple documents, then ranking them by relevance, and finally retaining\nthe minimum number of clues through truncation. In this paper, we propose\nFineFilter, a novel fine-grained noise filtering mechanism for RAG, consisting\nof a clue extractor, a reranker, and a truncator. We optimize each module to\ntackle complex reasoning challenges: (1) The clue extractor first uses\nsentences containing the answer and similar ones as fine-tuning targets, aiming\nto extract sufficient potential clues; (2) The reranker is trained to\nprioritize effective clues based on the real feedback from the generation\nmodule, with clues capable of generating correct answers as positive samples\nand others as negative; (3) The truncator takes the minimum number of clues\nneeded to answer the question (truncation point) as fine-tuning targets, and\nperforms truncation on the reranked clues to achieve fine-grained noise\nfiltering. Experiments on three QA datasets demonstrate that FineFilter\nsignificantly improves QA performance over baselines on both LLaMA3 and\nMistral. Further analysis confirms its effectiveness in complex reasoning,\nrobustness to unreliable retrieval, and generalization to different scenarios."}
{"id": "2502.11824", "pdf": "https://arxiv.org/pdf/2502.11824.pdf", "abs": "https://arxiv.org/abs/2502.11824", "title": "M-ABSA: A Multilingual Dataset for Aspect-Based Sentiment Analysis", "authors": ["Chengyan Wu", "Bolei Ma", "Yihong Liu", "Zheyu Zhang", "Ningyuan Deng", "Yanshu Li", "Baolan Chen", "Yi Zhang", "Yun Xue", "Barbara Plank"], "categories": ["cs.CL"], "comment": null, "summary": "Aspect-based sentiment analysis (ABSA) is a crucial task in information\nextraction and sentiment analysis, aiming to identify aspects with associated\nsentiment elements in text. However, existing ABSA datasets are predominantly\nEnglish-centric, limiting the scope for multilingual evaluation and research.\nTo bridge this gap, we present M-ABSA, a comprehensive dataset spanning 7\ndomains and 21 languages, making it the most extensive multilingual parallel\ndataset for ABSA to date. Our primary focus is on triplet extraction, which\ninvolves identifying aspect terms, aspect categories, and sentiment polarities.\nThe dataset is constructed through an automatic translation process with human\nreview to ensure quality. We perform extensive experiments using various\nbaselines to assess performance and compatibility on M-ABSA. Our empirical\nfindings highlight that the dataset enables diverse evaluation tasks, such as\nmultilingual and multi-domain transfer learning, and large language model\nevaluation, underscoring its inclusivity and its potential to drive\nadvancements in multilingual ABSA research."}
{"id": "2502.12464", "pdf": "https://arxiv.org/pdf/2502.12464.pdf", "abs": "https://arxiv.org/abs/2502.12464", "title": "SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models", "authors": ["Seanie Lee", "Dong Bok Lee", "Dominik Wagner", "Minki Kang", "Haebin Seong", "Tobias Bocklet", "Juho Lee", "Sung Ju Hwang"], "categories": ["cs.CL"], "comment": "ACL 2025 findings", "summary": "Deploying large language models (LLMs) in real-world applications requires\nrobust safety guard models to detect and block harmful user prompts. While\nlarge safety guard models achieve strong performance, their computational cost\nis substantial. To mitigate this, smaller distilled models are used, but they\noften underperform on \"hard\" examples where the larger model provides accurate\npredictions. We observe that many inputs can be reliably handled by the smaller\nmodel, while only a small fraction require the larger model's capacity.\nMotivated by this, we propose SafeRoute, a binary router that distinguishes\nhard examples from easy ones. Our method selectively applies the larger safety\nguard model to the data that the router considers hard, improving efficiency\nwhile maintaining accuracy compared to solely using the larger safety guard\nmodel. Experimental results on multiple benchmark datasets demonstrate that our\nadaptive model selection significantly enhances the trade-off between\ncomputational cost and safety performance, outperforming relevant baselines."}
{"id": "2502.13028", "pdf": "https://arxiv.org/pdf/2502.13028.pdf", "abs": "https://arxiv.org/abs/2502.13028", "title": "Whose story is it? Personalizing story generation by inferring author styles", "authors": ["Nischal Ashok Kumar", "Chau Minh Pham", "Mohit Iyyer", "Andrew Lan"], "categories": ["cs.CL"], "comment": "preprint:55 pages", "summary": "Personalization is critical for improving user experience in interactive\nwriting and educational applications, yet remains understudied in story\ngeneration. We study the task of personalizing story generation, where our goal\nis to mimic an author's writing style, given other stories written by them. We\ncollect Mythos, a dataset of 3.6k stories from 112 authors, with an average of\n16 stories per author, across five distinct sources reflecting diverse\nstory-writing settings. We propose a two-stage pipeline for personalized story\ngeneration: first, we infer authors' implicit writing characteristics and\norganize them into an Author Writing Sheet, which is validated by humans to be\nof high quality; second, we simulate the author's persona using tailored\npersona descriptions and personalized story rules. We find that stories\npersonalized using the Author Writing Sheet outperform a non-personalized\nbaseline, achieving a 78% win-rate in capturing authors' past style and 59% in\nsimilarity to ground-truth author stories. Human evaluation supports these\nfindings and further highlights trends, such as Reddit stories being easier to\npersonalize, and the Creativity and Language Use aspects of stories being\neasier to personalize than the Plot."}
{"id": "2502.13487", "pdf": "https://arxiv.org/pdf/2502.13487.pdf", "abs": "https://arxiv.org/abs/2502.13487", "title": "Transferring Textual Preferences to Vision-Language Understanding through Model Merging", "authors": ["Chen-An Li", "Tzu-Han Lin", "Yun-Nung Chen", "Hung-yi Lee"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted to ACL 2025 main", "summary": "Large vision-language models (LVLMs) perform outstandingly across various\nmultimodal tasks. However, their ability to evaluate generated content remains\nlimited, and training vision-language reward models (VLRMs) with preference\ndata is computationally expensive. This paper explores a training-free\nalternative by merging text-based reward models (RMs) with LVLMs to create\nVLRMs. Our approach shows that integrating these models leads to improved\nperformance over LVLMs' scoring and text-based RMs, offering an efficient\nmethod for incorporating textual preferences into LVLMs."}
{"id": "2502.15132", "pdf": "https://arxiv.org/pdf/2502.15132.pdf", "abs": "https://arxiv.org/abs/2502.15132", "title": "CoT-ICL Lab: A Synthetic Framework for Studying Chain-of-Thought Learning from In-Context Demonstrations", "authors": ["Vignesh Kothapalli", "Hamed Firooz", "Maziar Sanjabi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL Main 2025", "summary": "We introduce CoT-ICL Lab, a framework and methodology to generate synthetic\ntokenized datasets and systematically study chain-of-thought (CoT) in-context\nlearning (ICL) in language models. CoT-ICL Lab allows fine grained control over\nthe complexity of in-context examples by decoupling (1) the causal structure\ninvolved in chain token generation from (2) the underlying token processing\nfunctions. We train decoder-only transformers (up to 700M parameters) on these\ndatasets and show that CoT accelerates the accuracy transition to higher values\nacross model sizes. In particular, we find that model depth is crucial for\nleveraging CoT with limited in-context examples, while more examples help\nshallow models match deeper model performance. Additionally, limiting the\ndiversity of token processing functions throughout training improves causal\nstructure learning via ICL. We also interpret these transitions by analyzing\ntransformer embeddings and attention maps. Overall, CoT-ICL Lab serves as a\nsimple yet powerful testbed for theoretical and empirical insights into ICL and\nCoT in language models."}
{"id": "2502.16002", "pdf": "https://arxiv.org/pdf/2502.16002.pdf", "abs": "https://arxiv.org/abs/2502.16002", "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse", "authors": ["Jingbo Yang", "Bairu Hou", "Wei Wei", "Yujia Bao", "Shiyu Chang"], "categories": ["cs.CL"], "comment": null, "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines."}
{"id": "2502.17775", "pdf": "https://arxiv.org/pdf/2502.17775.pdf", "abs": "https://arxiv.org/abs/2502.17775", "title": "FoREST: Frame of Reference Evaluation in Spatial Reasoning Tasks", "authors": ["Tanawan Premsri", "Parisa Kordjamshidi"], "categories": ["cs.CL"], "comment": "9 pages", "summary": "Spatial reasoning is a fundamental aspect of human intelligence. One key\nconcept in spatial cognition is the Frame of Reference (FoR), which identifies\nthe perspective of spatial expressions. Despite its significance, FoR has\nreceived limited attention in AI models that need spatial intelligence. There\nis a lack of dedicated benchmarks and in-depth evaluation of large language\nmodels (LLMs) in this area. To address this issue, we introduce the Frame of\nReference Evaluation in Spatial Reasoning Tasks (FoREST) benchmark, designed to\nassess FoR comprehension in LLMs. We evaluate LLMs on answering questions that\nrequire FoR comprehension and layout generation in text-to-image models using\nFoREST. Our results reveal a notable performance gap across different FoR\nclasses in various LLMs, affecting their ability to generate accurate layouts\nfor text-to-image generation. This highlights critical shortcomings in FoR\ncomprehension. To improve FoR understanding, we propose Spatial-Guided\nprompting, which improves LLMs ability to extract essential spatial concepts.\nOur proposed method improves overall performance across spatial reasoning\ntasks."}
{"id": "2502.17956", "pdf": "https://arxiv.org/pdf/2502.17956.pdf", "abs": "https://arxiv.org/abs/2502.17956", "title": "Towards Better Understanding of Program-of-Thought Reasoning in Cross-Lingual and Multilingual Environments", "authors": ["Patomporn Payoungkhamdee", "Pume Tuchinda", "Jinheon Baek", "Samuel Cahyawijaya", "Can Udomcharoenchaikit", "Potsawee Manakul", "Peerat Limkonchotiwat", "Ekapol Chuangsuwanich", "Sarana Nutanong"], "categories": ["cs.CL"], "comment": null, "summary": "Multi-step reasoning is essential for large language models (LLMs), yet\nmultilingual performance remains challenging. While Chain-of-Thought (CoT)\nprompting improves reasoning, it struggles with non-English languages due to\nthe entanglement of reasoning and execution. Program-of-Thought (PoT) prompting\nseparates reasoning from execution, offering a promising alternative but\nshifting the challenge to generating programs from non-English questions. We\npropose a framework to evaluate PoT by separating multilingual reasoning from\ncode execution to examine (i) the impact of fine-tuning on question-reasoning\nalignment and (ii) how reasoning quality affects answer correctness. Our\nfindings demonstrate that PoT fine-tuning substantially enhances multilingual\nreasoning, outperforming CoT fine-tuned models. We further demonstrate a strong\ncorrelation between reasoning quality (measured through code quality) and\nanswer accuracy, highlighting its potential as a test-time performance\nimprovement heuristic."}
{"id": "2502.20073", "pdf": "https://arxiv.org/pdf/2502.20073.pdf", "abs": "https://arxiv.org/abs/2502.20073", "title": "Collab-Overcooked: Benchmarking and Evaluating Large Language Models as Collaborative Agents", "authors": ["Haochen Sun", "Shuwen Zhang", "Lujie Niu", "Lei Ren", "Hao Xu", "Hao Fu", "Fangkun Zhao", "Caixia Yuan", "Xiaojie Wang"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "30 pages, 17 figures", "summary": "Large language models (LLMs) based agent systems have made great strides in\nreal-world applications beyond traditional NLP tasks. This paper proposes a new\nLLM-powered Multi-Agent System (LLM-MAS) benchmark, Collab-Overcooked, built on\nthe popular Overcooked-AI game with more applicable and challenging tasks in\ninteractive environments. Collab-Overcooked extends existing benchmarks from\ntwo novel perspectives. First, it provides a multi-agent framework supporting\ndiverse tasks and objectives and encourages collaboration through natural\nlanguage communication. Second, it introduces a spectrum of process-oriented\nevaluation metrics to assess the fine-grained collaboration capabilities of\ndifferent LLM agents, a dimension often overlooked in prior work. We conduct\nextensive experiments over 11 popular LLMs and show that, while the LLMs\npresent a strong ability in goal interpretation, there is a significant\ndiscrepancy in active collaboration and continuous adaptation which are\ncritical for efficiently fulfilling complicated tasks. Notably, we highlight\nthe strengths and weaknesses in LLM-MAS and provide insights for improving and\nevaluating LLM-MAS on a unified and open-sourced benchmark. The environments,\n30 open-ended tasks, and the evaluation package are publicly available at\nhttps://github.com/YusaeMeow/Collab-Overcooked."}
{"id": "2503.04598", "pdf": "https://arxiv.org/pdf/2503.04598.pdf", "abs": "https://arxiv.org/abs/2503.04598", "title": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization", "authors": ["Zhijian Zhuo", "Yutao Zeng", "Ya Wang", "Sijun Zhang", "Jian Yang", "Xiaoqing Li", "Xun Zhou", "Jinwen Ma"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Transformers have become the de facto architecture for a wide range of\nmachine learning tasks, particularly in large language models (LLMs). Despite\ntheir remarkable performance, challenges remain in training deep transformer\nnetworks, especially regarding the position of layer normalization. While\nPre-Norm structures facilitate more stable training owing to their stronger\nidentity path, they often lead to suboptimal performance compared to Post-Norm.\nIn this paper, we propose $\\textbf{HybridNorm}$, a simple yet effective hybrid\nnormalization strategy that integrates the advantages of both Pre-Norm and\nPost-Norm. Specifically, HybridNorm employs QKV normalization within the\nattention mechanism and Post-Norm in the feed-forward network (FFN) of each\ntransformer block. We provide both theoretical insights and empirical evidence\ndemonstrating that HybridNorm improves gradient flow and model robustness.\nExtensive experiments on large-scale transformer models, including both dense\nand sparse variants, show that HybridNorm consistently outperforms both\nPre-Norm and Post-Norm approaches across multiple benchmarks. These findings\nhighlight the potential of HybridNorm as a more stable and effective technique\nfor improving the training and performance of deep transformer models. Code is\navailable at https://github.com/BryceZhuo/HybridNorm."}
{"id": "2503.08506", "pdf": "https://arxiv.org/pdf/2503.08506.pdf", "abs": "https://arxiv.org/abs/2503.08506", "title": "ReviewAgents: Bridging the Gap Between Human and AI-Generated Paper Reviews", "authors": ["Xian Gao", "Jiacheng Ruan", "Jingsheng Gao", "Ting Liu", "Yuzhuo Fu"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Academic paper review is a critical yet time-consuming task within the\nresearch community. With the increasing volume of academic publications,\nautomating the review process has become a significant challenge. The primary\nissue lies in generating comprehensive, accurate, and reasoning-consistent\nreview comments that align with human reviewers' judgments. In this paper, we\naddress this challenge by proposing ReviewAgents, a framework that leverages\nlarge language models (LLMs) to generate academic paper reviews. We first\nintroduce a novel dataset, Review-CoT, consisting of 142k review comments,\ndesigned for training LLM agents. This dataset emulates the structured\nreasoning process of human reviewers-summarizing the paper, referencing\nrelevant works, identifying strengths and weaknesses, and generating a review\nconclusion. Building upon this, we train LLM reviewer agents capable of\nstructured reasoning using a relevant-paper-aware training method. Furthermore,\nwe construct ReviewAgents, a multi-role, multi-LLM agent review framework, to\nenhance the review comment generation process. Additionally, we propose\nReviewBench, a benchmark for evaluating the review comments generated by LLMs.\nOur experimental results on ReviewBench demonstrate that while existing LLMs\nexhibit a certain degree of potential for automating the review process, there\nremains a gap when compared to human-generated reviews. Moreover, our\nReviewAgents framework further narrows this gap, outperforming advanced LLMs in\ngenerating review comments."}
{"id": "2503.14749", "pdf": "https://arxiv.org/pdf/2503.14749.pdf", "abs": "https://arxiv.org/abs/2503.14749", "title": "Uncertainty Distillation: Teaching Language Models to Express Semantic Confidence", "authors": ["Sophia Hager", "David Mueller", "Kevin Duh", "Nicholas Andrews"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "As large language models (LLMs) are increasingly used for factual\nquestion-answering, it becomes more important for LLMs to have the capability\nto communicate the likelihood that their answer is correct. For these\nverbalized expressions of uncertainty to be meaningful, they should reflect the\nerror rates at the expressed level of confidence. However, when prompted to\nexpress confidence, the error rates of current LLMs are inconsistent with their\ncommunicated confidences, highlighting the need for uncertainty quantification\nmethods. Many prior methods calculate lexical uncertainty, estimating a model's\nconfidence in the specific string it generated. In some cases, however, it may\nbe more useful to estimate semantic uncertainty, or the model's confidence in\nthe answer regardless of how it is verbalized. We propose a simple procedure,\nuncertainty distillation, to teach an LLM to verbalize calibrated semantic\nconfidences. Using held-out data to map initial uncertainty estimates to\nmeaningful probabilities, we create examples annotated with verbalized\nprobabilities for supervised fine-tuning. We compare uncertainty distillation\nto several strong baselines, and find that our method yields verbalized\nconfidences that correlate well with observed error rates."}
{"id": "2503.15463", "pdf": "https://arxiv.org/pdf/2503.15463.pdf", "abs": "https://arxiv.org/abs/2503.15463", "title": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment", "authors": ["Jia-Nan Li", "Jian Guan", "Songhao Wu", "Wei Wu", "Rui Yan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our approach toward user-adaptive AI\nsystems."}
{"id": "2503.16674", "pdf": "https://arxiv.org/pdf/2503.16674.pdf", "abs": "https://arxiv.org/abs/2503.16674", "title": "Through the LLM Looking Glass: A Socratic Probing of Donkeys, Elephants, and Markets", "authors": ["Molly Kennedy", "Ayyoob Imani", "Timo Spinde", "Hinrich Schütze"], "categories": ["cs.CL"], "comment": null, "summary": "While detecting and avoiding bias in LLM-generated text is becoming\nincreasingly important, media bias often remains subtle and subjective, making\nit particularly difficult to identify and mitigate. In this study, we assess\nmedia bias in LLM-generated content and LLMs' ability to detect subtle\nideological bias. We conduct this evaluation using two datasets, PoliGen and\nEconoLex, covering political and economic discourse, respectively. We evaluate\nseven widely used LLMs by prompting them to generate articles and analyze their\nideological preferences via Socratic probing. By using our self-contained\nSocratic approach, the study aims to directly measure the models' biases rather\nthan relying on external interpretations, thereby minimizing subjective\njudgments about media bias. Our results reveal a consistent preference of\nDemocratic over Republican positions across all models. Conversely, in economic\ntopics, biases vary among Western LLMs, while those developed in China lean\nmore strongly toward socialism."}
{"id": "2503.16965", "pdf": "https://arxiv.org/pdf/2503.16965.pdf", "abs": "https://arxiv.org/abs/2503.16965", "title": "Praxis-VLM: Vision-Grounded Decision Making via Text-Driven Reinforcement Learning", "authors": ["Zhe Hu", "Jing Li", "Zhongzhu Pu", "Hou Pong Chan", "Yu Yin"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Vision Language Models exhibited immense potential for embodied AI, yet they\noften lack the sophisticated situational reasoning required for complex\ndecision-making. This paper shows that VLMs can achieve surprisingly strong\ndecision-making performance when visual scenes are represented merely as\ntext-only descriptions, suggesting foundational reasoning can be effectively\nlearned from language. Motivated by this insight, we propose Praxis-VLM, a\nreasoning VLM for vision-grounded decision-making. Praxis-VLM employs the GRPO\nalgorithm on textual scenarios to instill robust reasoning capabilities, where\nmodels learn to evaluate actions and their consequences. These reasoning\nskills, acquired purely from text, successfully transfer to multimodal\ninference with visual inputs, significantly reducing reliance on scarce paired\nimage-text training data. Experiments across diverse decision-making benchmarks\ndemonstrate that Praxis-VLM substantially outperforms standard supervised\nfine-tuning, exhibiting superior performance and generalizability. Further\nanalysis confirms that our models engage in explicit and effective reasoning,\nunderpinning their enhanced performance and adaptability."}
{"id": "2503.17287", "pdf": "https://arxiv.org/pdf/2503.17287.pdf", "abs": "https://arxiv.org/abs/2503.17287", "title": "FastCuRL: Curriculum Reinforcement Learning with Stage-wise Context Scaling for Efficient Training R1-like Reasoning Models", "authors": ["Mingyang Song", "Mao Zheng", "Zheng Li", "Wenjie Yang", "Xuan Luo", "Yue Pan", "Feng Zhang"], "categories": ["cs.CL"], "comment": "Ongoing Work", "summary": "Improving training efficiency continues to be one of the primary challenges\nin large-scale Reinforcement Learning (RL). In this paper, we investigate how\ncontext length and the complexity of training data influence the RL scaling\ntraining process of R1-distilled small reasoning models, e.g.,\nDeepSeek-R1-Distill-Qwen-1.5B. Our experimental results reveal that: (1) simply\ncontrolling the context length and curating the training data based on the\ninput prompt length can effectively improve the training efficiency of scaling\nRL, achieving better performance with more concise CoT; (2) properly scaling\nthe context length helps mitigate entropy collapse; and (3) choosing an optimal\ncontext length can improve the efficiency of model training and incentivize the\nmodel's chain-of-thought reasoning capabilities. Inspired by these insights, we\npropose FastCuRL, a curriculum RL framework with stage-wise context scaling to\nachieve efficient training and concise CoT reasoning. Experiment results\ndemonstrate that FastCuRL-1.5B-V3 significantly outperforms state-of-the-art\nreasoning models on five competition-level benchmarks and achieves 49.6\\%\naccuracy on AIME 2024. Furthermore, FastCuRL-1.5B-Preview surpasses\nDeepScaleR-1.5B-Preview on five benchmarks while only using a single node with\n8 GPUs and a total of 50\\% of training steps. %The code, training data, and\nmodels will be publicly released."}
{"id": "2503.19498", "pdf": "https://arxiv.org/pdf/2503.19498.pdf", "abs": "https://arxiv.org/abs/2503.19498", "title": "DomainCQA: Crafting Expert-Level QA from Domain-Specific Charts", "authors": ["Ling Zhong", "Yujing Lu", "Jing Yang", "Weiming Li", "Peng Wei", "Yongheng Wang", "Manni Duan", "Qing Zhang"], "categories": ["cs.CL"], "comment": "87 pages, 65 figures", "summary": "Chart Question Answering (CQA) benchmarks are essential for evaluating the\ncapability of Multimodal Large Language Models (MLLMs) to interpret visual\ndata. However, current benchmarks focus primarily on the evaluation of\ngeneral-purpose CQA but fail to adequately capture domain-specific challenges.\nWe introduce DomainCQA, a systematic methodology for constructing\ndomain-specific CQA benchmarks, and demonstrate its effectiveness by developing\nAstroChart, a CQA benchmark in the field of astronomy. Our evaluation shows\nthat current MLLMs face fundamental challenges in vision-language alignment and\ndomain adaptation, highlighting a critical gap in current benchmarks. By\nproviding a scalable and rigorous framework, DomainCQA enables more precise\nassessment and improvement of MLLMs for domain-specific applications."}
{"id": "2503.20083", "pdf": "https://arxiv.org/pdf/2503.20083.pdf", "abs": "https://arxiv.org/abs/2503.20083", "title": "Universal Cross-Tokenizer Distillation via Approximate Likelihood Matching", "authors": ["Benjamin Minixhofer", "Ivan Vulić", "Edoardo Maria Ponti"], "categories": ["cs.CL"], "comment": "Preprint, 21 pages", "summary": "Distillation has shown remarkable success in transferring knowledge from a\nLarge Language Model (LLM) teacher to a student LLM. However, current\ndistillation methods require similar tokenizers between the teacher and the\nstudent, restricting their applicability to only a small subset of\nteacher-student pairs. In this work, we develop a principled cross-tokenizer\ndistillation method to solve this crucial deficiency. Our method is the first\nto enable effective distillation across fundamentally different tokenizers,\nwhile also substantially outperforming prior methods in all other cases. We\nverify the efficacy of our method on three distinct use cases. First, we show\nthat viewing tokenizer transfer as self-distillation enables unprecedentedly\neffective transfer across tokenizers, including rapid transfer of subword\nmodels to the byte-level. Transferring different models to the same tokenizer\nalso enables ensembling to boost performance. Secondly, we distil a large\nmaths-specialised LLM into a small general-purpose model with a different\ntokenizer, achieving competitive maths problem-solving performance. Thirdly, we\nuse our method to train state-of-the-art embedding prediction hypernetworks for\ntraining-free tokenizer transfer. Our results unlock an expanded range of\nteacher-student pairs for distillation, enabling new ways to adapt and enhance\ninteraction between LLMs."}
{"id": "2504.07053", "pdf": "https://arxiv.org/pdf/2504.07053.pdf", "abs": "https://arxiv.org/abs/2504.07053", "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling", "authors": ["Liang-Hsuan Tseng", "Yi-Chang Chen", "Kuan-Yi Lee", "Da-Shan Shiu", "Hung-yi Lee"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Preprint", "summary": "Recent efforts target spoken language models (SLMs) that not only listen but\nalso speak for more natural human-LLM interaction. Joint speech-text modeling\nis a promising direction to achieve this. However, the effectiveness of recent\nspeech tokens for joint modeling remains underexplored. To address this, we\nintroduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that\ndirectly addresses the modality gap by aligning speech token with the\ncorresponding text transcription during the tokenization stage. We propose a\nmethod that can achieve this through a attention-based aggregation mechanism\nand with speech reconstruction as the training objective. We conduct extensive\nexperiments and show that TASTE can preserve essential paralinguistic\ninformation while dramatically reducing the token sequence length. With TASTE,\nwe perform straightforward joint spoken language modeling by using Low-Rank\nAdaptation on the pre-trained text LLM. Experimental results show that\nTASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze;\nwhile significantly outperform other pre-trained SLMs on speech continuation\nacross subjective and objective evaluations. To our knowledge, TASTE is the\nfirst end-to-end approach that utilizes a reconstruction objective to\nautomatically learn a text-aligned speech tokenization and embedding suitable\nfor spoken language modeling. Our demo, code, and model are available at\nhttps://mtkresearch.github.io/TASTE-SpokenLM.github.io."}
{"id": "2504.09753", "pdf": "https://arxiv.org/pdf/2504.09753.pdf", "abs": "https://arxiv.org/abs/2504.09753", "title": "Improving Multilingual Capabilities with Cultural and Local Knowledge in Large Language Models While Enhancing Native Performance", "authors": ["Ram Mohan Rao Kadiyala", "Siddartha Pullakhandam", "Siddhant Gupta", "Drishti Sharma", "Jebish Purbey", "Kanwal Mehreen", "Muhammad Arham", "Hamza Farooq"], "categories": ["cs.CL", "cs.AI"], "comment": "24 pages, 18 figures", "summary": "Large Language Models (LLMs) have shown remarkable capabilities, but their\ndevelopment has primarily focused on English and other high-resource languages,\nleaving many languages underserved. We present our latest Hindi-English\nbi-lingual LLM \\textbf{Mantra-14B} with ~3\\% average improvement in benchmark\nscores over both languages, outperforming models twice its size. Using a\ncurated dataset composed of English and Hindi instruction data of 485K samples,\nwe instruction tuned models such as Qwen-2.5-14B-Instruct and Phi-4 to improve\nperformance over both English and Hindi. Our experiments encompassing seven\ndifferent LLMs of varying parameter sizes and over 140 training attempts with\nvarying English-Hindi training data ratios demonstrated that it is possible to\nsignificantly improve multilingual performance without compromising native\nperformance. Further, our approach avoids resource-intensive techniques like\nvocabulary expansion or architectural modifications, thus keeping the model\nsize small. Our results indicate that modest fine-tuning with culturally and\nlocally informed data can bridge performance gaps without incurring significant\ncomputational overhead. We release our training code, datasets, and models\nunder mit and apache licenses to aid further research towards under-represented\nand low-resource languages."}
{"id": "2504.10063", "pdf": "https://arxiv.org/pdf/2504.10063.pdf", "abs": "https://arxiv.org/abs/2504.10063", "title": "Hallucination Detection in LLMs with Topological Divergence on Attention Graphs", "authors": ["Alexandra Bazarova", "Aleksandr Yugay", "Andrey Shulga", "Alina Ermilova", "Andrei Volodichev", "Konstantin Polev", "Julia Belikova", "Rauf Parchiev", "Dmitry Simakov", "Maxim Savchenko", "Andrey Savchenko", "Serguei Barannikov", "Alexey Zaytsev"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hallucination, i.e., generating factually incorrect content, remains a\ncritical challenge for large language models (LLMs). We introduce TOHA, a\nTOpology-based HAllucination detector in the RAG setting, which leverages a\ntopological divergence metric to quantify the structural properties of graphs\ninduced by attention matrices. Examining the topological divergence between\nprompt and response subgraphs reveals consistent patterns: higher divergence\nvalues in specific attention heads correlate with hallucinated outputs,\nindependent of the dataset. Extensive experiments - including evaluation on\nquestion answering and summarization tasks - show that our approach achieves\nstate-of-the-art or competitive results on several benchmarks while requiring\nminimal annotated data and computational resources. Our findings suggest that\nanalyzing the topological structure of attention matrices can serve as an\nefficient and robust indicator of factual reliability in LLMs."}
{"id": "2504.11952", "pdf": "https://arxiv.org/pdf/2504.11952.pdf", "abs": "https://arxiv.org/abs/2504.11952", "title": "Robust and Fine-Grained Detection of AI Generated Texts", "authors": ["Ram Mohan Rao Kadiyala", "Siddartha Pullakhandam", "Kanwal Mehreen", "Drishti Sharma", "Siddhant Gupta", "Jebish Purbey", "Ashay Srivastava", "Subhasya TippaReddy", "Arvind Reddy Bobbili", "Suraj Telugara Chandrashekhar", "Modabbir Adeeb", "Srinadh Vura", "Hamza Farooq"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "18 pages, 6 figures", "summary": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts."}
{"id": "2504.12816", "pdf": "https://arxiv.org/pdf/2504.12816.pdf", "abs": "https://arxiv.org/abs/2504.12816", "title": "SMARTe: Slot-based Method for Accountable Relational Triple extraction", "authors": ["Xue Wen Tan", "Stanley Kok"], "categories": ["cs.CL"], "comment": null, "summary": "Relational Triple Extraction (RTE) is a fundamental task in Natural Language\nProcessing (NLP). However, prior research has primarily focused on optimizing\nmodel performance, with limited efforts to understand the internal mechanisms\ndriving these models. Many existing methods rely on complex preprocessing to\ninduce specific interactions, often resulting in opaque systems that may not\nfully align with their theoretical foundations. To address these limitations,\nwe propose SMARTe: a Slot-based Method for Accountable Relational Triple\nextraction. SMARTe introduces intrinsic interpretability through a slot\nattention mechanism and frames the task as a set prediction problem. Slot\nattention consolidates relevant information into distinct slots, ensuring all\npredictions can be explicitly traced to learned slot representations and the\ntokens contributing to each predicted relational triple. While emphasizing\ninterpretability, SMARTe achieves performance comparable to state-of-the-art\nmodels. Evaluations on the NYT and WebNLG datasets demonstrate that adding\ninterpretability does not compromise performance. Furthermore, we conducted\nqualitative assessments to showcase the explanations provided by SMARTe, using\nattention heatmaps that map to their respective tokens. We conclude with a\ndiscussion of our findings and propose directions for future research."}
{"id": "2504.15253", "pdf": "https://arxiv.org/pdf/2504.15253.pdf", "abs": "https://arxiv.org/abs/2504.15253", "title": "Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators", "authors": ["Yilun Zhou", "Austin Xu", "Peifeng Wang", "Caiming Xiong", "Shafiq Joty"], "categories": ["cs.CL", "cs.LG"], "comment": "ICML 2025. The first two authors contributed equally. The codebase is\n  at https://github.com/SalesforceAIResearch/jetts-benchmark", "summary": "Scaling test-time computation, or affording a generator large language model\n(LLM) extra compute during inference, typically employs the help of external\nnon-generative evaluators (i.e., reward models). Concurrently, LLM-judges,\nmodels trained to generate evaluations and critiques (explanations) in natural\nlanguage, are becoming increasingly popular in automatic evaluation. Despite\njudge empirical successes, their effectiveness as evaluators in test-time\nscaling settings is largely unknown. In this paper, we introduce the Judge\nEvaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge\nperformance in three domains (math reasoning, code generation, and instruction\nfollowing) under three task settings: response reranking, step-level beam\nsearch, and critique-based response refinement. We evaluate 10 different judge\nmodels (7B-70B parameters) for 8 different base generator models (6.7B-72B\nparameters). Our benchmark shows that while judges are competitive with outcome\nreward models in reranking, they are consistently worse than process reward\nmodels in beam search procedures. Furthermore, though unique to LLM-judges,\ntheir natural language critiques are currently ineffective in guiding the\ngenerator towards better responses."}
{"id": "2504.16084", "pdf": "https://arxiv.org/pdf/2504.16084.pdf", "abs": "https://arxiv.org/abs/2504.16084", "title": "TTRL: Test-Time Reinforcement Learning", "authors": ["Yuxin Zuo", "Kaiyan Zhang", "Li Sheng", "Shang Qu", "Ganqu Cui", "Xuekai Zhu", "Haozhan Li", "Yuchen Zhang", "Xinwei Long", "Ermo Hua", "Biqing Qi", "Youbang Sun", "Zhiyuan Ma", "Lifan Yuan", "Ning Ding", "Bowen Zhou"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 211% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the maj@n metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model maj@n,\nand approach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL"}
{"id": "2504.19110", "pdf": "https://arxiv.org/pdf/2504.19110.pdf", "abs": "https://arxiv.org/abs/2504.19110", "title": "APE-Bench I: Towards File-level Automated Proof Engineering of Formal Math Libraries", "authors": ["Huajian Xin", "Luming Li", "Xiaoran Jin", "Jacques Fleuriot", "Wenda Li"], "categories": ["cs.CL"], "comment": null, "summary": "Recent progress in large language models (LLMs) has shown promise in formal\ntheorem proving, yet existing benchmarks remain limited to isolated, static\nproof tasks, failing to capture the iterative, engineering-intensive workflows\nof real-world formal mathematics libraries. Motivated by analogous advances in\nsoftware engineering, we introduce the paradigm of Automated Proof Engineering\n(APE), which aims to automate proof engineering tasks such as feature addition,\nproof refactoring, and bug fixing using LLMs. To facilitate research in this\ndirection, we present APE-Bench I, the first realistic benchmark built from\nreal-world commit histories of Mathlib4, featuring diverse file-level tasks\ndescribed in natural language and verified via a hybrid approach combining the\nLean compiler and LLM-as-a-Judge. We further develop Eleanstic, a scalable\nparallel verification infrastructure optimized for proof checking across\nmultiple versions of Mathlib. Empirical results on state-of-the-art LLMs\ndemonstrate strong performance on localized edits but substantial degradation\non handling complex proof engineering. This work lays the foundation for\ndeveloping agentic workflows in proof engineering, with future benchmarks\ntargeting multi-file coordination, project-scale verification, and autonomous\nagents capable of planning, editing, and repairing formal libraries."}
{"id": "2504.21800", "pdf": "https://arxiv.org/pdf/2504.21800.pdf", "abs": "https://arxiv.org/abs/2504.21800", "title": "How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues", "authors": ["Suhas BN", "Dominik Mattioli", "Saeed Abdullah", "Rosa I. Arriaga", "Chris W. Wiese", "Andrew M. Sherrill"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "68T50", "I.2.7; H.3.1"], "comment": "10 pages, 5 tables", "summary": "The growing adoption of synthetic data in healthcare is driven by privacy\nconcerns, limited access to real-world data, and the high cost of annotation.\nThis work explores the use of synthetic Prolonged Exposure (PE) therapeutic\nconversations for Post-Traumatic Stress Disorder (PTSD) as a scalable\nalternative for training and evaluating clinical models. We systematically\ncompare real and synthetic dialogues using linguistic, structural, and\nprotocol-specific metrics, including turn-taking patterns and treatment\nfidelity. We also introduce and evaluate PE-specific metrics derived from\nlinguistic analysis and semantic modeling, offering a novel framework for\nassessing clinical fidelity beyond surface fluency. Our findings show that\nalthough synthetic data holds promise for mitigating data scarcity and\nprotecting patient privacy, it can struggle to capture the subtle dynamics of\ntherapeutic interactions. Synthetic therapy dialogues closely match structural\nfeatures of real-world conversations (e.g., speaker switch ratio: 0.98 vs.\n0.99); however, they may not adequately reflect key fidelity markers (e.g.,\ndistress monitoring). We highlight gaps in existing evaluation frameworks and\nadvocate for fidelity-aware metrics that go beyond surface fluency to uncover\nclinically significant failures. Our findings clarify where synthetic data can\neffectively complement real-world datasets -- and where critical limitations\nremain."}
{"id": "2505.00063", "pdf": "https://arxiv.org/pdf/2505.00063.pdf", "abs": "https://arxiv.org/abs/2505.00063", "title": "GDI-Bench: A Benchmark for General Document Intelligence with Vision and Reasoning Decoupling", "authors": ["Siqi Li", "Yufan Shen", "Xiangnan Chen", "Jiayi Chen", "Hengwei Ju", "Haodong Duan", "Song Mao", "Hongbin Zhou", "Bo Zhang", "Bin Fu", "Pinlong Cai", "Licheng Wen", "Botian Shi", "Yong Liu", "Xinyu Cai", "Yu Qiao"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "The rapid advancement of multimodal large language models (MLLMs) has\nprofoundly impacted the document domain, creating a wide array of application\nscenarios. This progress highlights the need for a comprehensive benchmark to\nevaluate these models' capabilities across various document-specific tasks.\nHowever, existing benchmarks often fail to locate specific model weaknesses or\nguide systematic improvements. To bridge this gap, we introduce a General\nDocument Intelligence Benchmark (GDI-Bench), featuring 2.3k images across 9 key\nscenarios and 19 document-specific tasks. By decoupling visual complexity and\nreasoning complexity, the GDI-Bench structures graded tasks that allow\nperformance assessment by difficulty, aiding in model weakness identification\nand optimization guidance. We evaluate various open-source and closed-source\nmodels on GDI-Bench, conducting decoupled analyses in the visual and reasoning\ndomains, revealing their strengths and weaknesses. To address the diverse tasks\nand domains in the GDI-Bench, we propose a GDI-Model that mitigates\ncatastrophic forgetting during the supervised fine-tuning (SFT) process through\nan intelligence-preserving training strategy, thereby reinforcing the inherent\nweaknesses of the base model. Our model achieves state-of-the-art performance\non previous benchmarks and the GDI-Bench. Both our benchmark and models are or\nwill be open-sourced on https://huggingface.co/GDIBench."}
{"id": "2505.02156", "pdf": "https://arxiv.org/pdf/2505.02156.pdf", "abs": "https://arxiv.org/abs/2505.02156", "title": "Adaptive Thinking via Mode Policy Optimization for Social Language Agents", "authors": ["Minzheng Wang", "Yongbin Li", "Haobo Wang", "Xinghua Zhang", "Nan Xu", "Bingli Wu", "Fei Huang", "Haiyang Yu", "Wenji Mao"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in Progress. The code and data are available, see\n  https://github.com/MozerWang/AMPO", "summary": "Effective social intelligence simulation requires language agents to\ndynamically adjust reasoning depth, a capability notably absent in current\nstudies. Existing methods either lack this kind of reasoning capability or\nenforce Long Chain-of-Thought reasoning uniformly across all scenarios,\nresulting in excessive token usage and inflexible social simulation. To address\nthis, we propose an $\\textbf{A}$daptive $\\textbf{M}$ode $\\textbf{L}$earning\n($\\textbf{AML}$) framework in this paper, aiming to improve the adaptive\nthinking ability of language agents in dynamic social interactions. To this\nend, we first identify hierarchical thinking modes ranging from intuitive\nresponse to deep deliberation based on the cognitive control theory. We then\ndevelop the $\\textbf{A}$daptive $\\textbf{M}$ode $\\textbf{P}$olicy\n$\\textbf{O}$ptimization ($\\textbf{AMPO}$) algorithm to optimize the\ncontext-aware mode switching and reasoning. Our framework advances existing\nresearch in three key aspects: (1) Multi-granular thinking mode design, (2)\nContext-aware mode switching across social interaction, and (3) Token-efficient\nreasoning via depth-adaptive processing. Extensive experiments on social\nintelligence benchmarks verify that AML achieves 15.6% higher task performance\nthan GPT-4o. Notably, our AMPO outperforms GRPO by 7.0% with 32.8% shorter\nreasoning chains, demonstrating the advantage of adaptive thinking mode\nselection and optimization mechanism in AMPO over GRPO's fixed-depth solution."}
{"id": "2505.02172", "pdf": "https://arxiv.org/pdf/2505.02172.pdf", "abs": "https://arxiv.org/abs/2505.02172", "title": "Identifying Legal Holdings with LLMs: A Systematic Study of Performance, Scale, and Memorization", "authors": ["Chuck Arvin"], "categories": ["cs.CL"], "comment": "Presented as a short paper at International Conference on Artificial\n  Intelligence and Law 2025 (Chicago, IL)", "summary": "As large language models (LLMs) continue to advance in capabilities, it is\nessential to assess how they perform on established benchmarks. In this study,\nwe present a suite of experiments to assess the performance of modern LLMs\n(ranging from 3B to 90B+ parameters) on CaseHOLD, a legal benchmark dataset for\nidentifying case holdings. Our experiments demonstrate ``scaling effects'' -\nperformance on this task improves with model size, with more capable models\nlike GPT4o and AmazonNovaPro achieving macro F1 scores of 0.744 and 0.720\nrespectively. These scores are competitive with the best published results on\nthis dataset, and do not require any technically sophisticated model training,\nfine-tuning or few-shot prompting. To ensure that these strong results are not\ndue to memorization of judicial opinions contained in the training data, we\ndevelop and utilize a novel citation anonymization test that preserves semantic\nmeaning while ensuring case names and citations are fictitious. Models maintain\nstrong performance under these conditions (macro F1 of 0.728), suggesting the\nperformance is not due to rote memorization. These findings demonstrate both\nthe promise and current limitations of LLMs for legal tasks with important\nimplications for the development and measurement of automated legal analytics\nand legal benchmarks."}
{"id": "2505.03563", "pdf": "https://arxiv.org/pdf/2505.03563.pdf", "abs": "https://arxiv.org/abs/2505.03563", "title": "Say It Another Way: Auditing LLMs with a User-Grounded Automated Paraphrasing Framework", "authors": ["Cléa Chataigner", "Rebecca Ma", "Prakhar Ganesh", "Afaf Taïk", "Elliot Creager", "Golnoosh Farnadi"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are sensitive to subtle changes in prompt\nphrasing, complicating efforts to audit them reliably. Prior approaches often\nrely on arbitrary or ungrounded prompt variations, which may miss key\nlinguistic and demographic factors in real-world usage. We introduce AUGMENT\n(Automated User-Grounded Modeling and Evaluation of Natural Language\nTransformations), a framework for systematically generating and evaluating\ncontrolled, realistic prompt paraphrases based on linguistic structure and user\ndemographics. AUGMENT ensures paraphrase quality through a combination of\nsemantic, stylistic, and instruction-following criteria. In a case study on the\nBBQ dataset, we show that user-grounded paraphrasing leads to significant\nshifts in LLM performance and bias metrics across nine models. Our findings\nhighlight the need for more representative and structured approaches to prompt\nvariation in LLM auditing."}
{"id": "2505.05423", "pdf": "https://arxiv.org/pdf/2505.05423.pdf", "abs": "https://arxiv.org/abs/2505.05423", "title": "LiTransProQA: an LLM-based Literary Translation evaluation metric with Professional Question Answering", "authors": ["Ran Zhang", "Wei Zhao", "Lieve Macken", "Steffen Eger"], "categories": ["cs.CL", "cs.AI"], "comment": "Updated version, with examples in the appendix", "summary": "The impact of Large Language Models (LLMs) has extended into literary\ndomains. However, existing evaluation metrics prioritize mechanical accuracy\nover artistic expression and tend to overrate machine translation as being\nsuperior to human translation from experienced professionals. In the long run,\nthis bias could result in an irreversible decline in translation quality and\ncultural authenticity. In response to the urgent need for a specialized\nliterary evaluation metric, we introduce LiTransProQA, a novel, reference-free,\nLLM-based question-answering framework designed for literary translation\nevaluation. LiTransProQA uniquely integrates insights from professional\nliterary translators and researchers, focusing on critical elements in literary\nquality assessment such as literary devices, cultural understanding, and\nauthorial voice. Our extensive evaluation shows that while literary-finetuned\nXCOMET-XL yields marginal gains, LiTransProQA substantially outperforms current\nmetrics, achieving up to 0.07 gain in correlation and surpassing the best\nstate-of-the-art metrics by over 15 points in adequacy assessments.\nIncorporating professional translator insights as weights further improves\nperformance, highlighting the value of translator inputs. Notably, LiTransProQA\nreaches human-level evaluation performance comparable to trained student\nevaluators. It shows broad applicability to open-source models like\nLLaMa3.3-70b and Qwen2.5-32b, indicating its potential as an accessible and\ntraining-free tool for evaluating literary translations that require local\nprocessing due to copyright or ethical considerations. The code and datasets\nare available under: https://github.com/zhangr2021/TransProQA."}
{"id": "2505.10736", "pdf": "https://arxiv.org/pdf/2505.10736.pdf", "abs": "https://arxiv.org/abs/2505.10736", "title": "Model Performance-Guided Evaluation Data Selection for Effective Prompt Optimization", "authors": ["Ximing Dong", "Shaowei Wang", "Dayi Lin", "Ahmed E. Hassan"], "categories": ["cs.CL"], "comment": "ACL 2025, Findings", "summary": "Optimizing Large Language Model (LLM) performance requires well-crafted\nprompts, but manual prompt engineering is labor-intensive and often\nineffective. Automated prompt optimization techniques address this challenge\nbut the majority of them rely on randomly selected evaluation subsets, which\nfail to represent the full dataset, leading to unreliable evaluations and\nsuboptimal prompts. Existing coreset selection methods, designed for LLM\nbenchmarking, are unsuitable for prompt optimization due to challenges in\nclustering similar samples, high data collection costs, and the unavailability\nof performance data for new or private datasets. To overcome these issues, we\npropose IPOMP, an Iterative evaluation data selection for effective Prompt\nOptimization using real-time Model Performance. IPOMP is a two-stage approach\nthat selects representative and diverse samples using semantic clustering and\nboundary analysis, followed by iterative refinement with real-time model\nperformance data to replace redundant samples. Evaluations on the BIG-bench\ndataset show that IPOMP improves effectiveness by 1.6% to 5.3% and stability by\nat least 57% compared with SOTA baselines, with minimal computational overhead\nbelow 1%. Furthermore, the results demonstrate that our real-time\nperformance-guided refinement approach can be universally applied to enhance\nexisting coreset selection methods."}
{"id": "2505.10945", "pdf": "https://arxiv.org/pdf/2505.10945.pdf", "abs": "https://arxiv.org/abs/2505.10945", "title": "Semantic Aware Linear Transfer by Recycling Pre-trained Language Models for Cross-lingual Transfer", "authors": ["Seungyoon Lee", "Seongtae Hong", "Hyeonseok Moon", "Heuiseok Lim"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large Language Models (LLMs) increasingly incorporate multilingual\ncapabilities, fueling the demand to transfer them into target language-specific\nmodels. However, most approaches, which blend the source model's embedding by\nreplacing the source vocabulary with the target language-specific vocabulary,\nmay constrain expressive capacity in the target language since the source model\nis predominantly trained on English data. In this paper, we propose Semantic\nAware Linear Transfer (SALT), a novel cross-lingual transfer technique that\nrecycles embeddings from target language Pre-trained Language Models (PLMs) to\ntransmit the deep representational strengths of PLM-derived embedding to LLMs.\nSALT derives unique regression lines based on the similarity in the overlap of\nthe source and target vocabularies, to handle each non-overlapping token's\nembedding space. Our extensive experiments show that SALT significantly\noutperforms other transfer methods and achieves lower loss with accelerating\nfaster convergence during language adaptation. Notably, SALT obtains remarkable\nperformance in cross-lingual understanding setups compared to other methods.\nFurthermore, we highlight the scalable use of PLMs to enhance the functionality\nof contemporary LLMs by conducting experiments with varying architectures."}
{"id": "2505.11004", "pdf": "https://arxiv.org/pdf/2505.11004.pdf", "abs": "https://arxiv.org/abs/2505.11004", "title": "Illusion or Algorithm? Investigating Memorization, Emergence, and Symbolic Processing in In-Context Learning", "authors": ["Jingcheng Niu", "Subhabrata Dutta", "Ahmed Elshabrawy", "Harish Tayyar Madabushi", "Iryna Gurevych"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large-scale Transformer language models (LMs) trained solely on next-token\nprediction with web-scale data can solve a wide range of tasks after seeing\njust a few examples. The mechanism behind this capability, known as in-context\nlearning (ICL), remains both controversial and poorly understood. Some studies\nargue that it is merely the result of memorizing vast amounts of data, while\nothers contend that it reflects a fundamental, symbolic algorithmic development\nin LMs. In this work, we introduce a suite of investigative tasks and a novel\nmethod to systematically investigate ICL by leveraging the full Pythia scaling\nsuite, including interim checkpoints that capture progressively larger amount\nof training data. By carefully exploring ICL performance on downstream tasks\nand simultaneously conducting a mechanistic analysis of the residual stream's\nsubspace, we demonstrate that ICL extends beyond mere \"memorization\" of the\ntraining corpus, yet does not amount to the implementation of an independent\nsymbolic algorithm. Our results also clarify several aspects of ICL, including\nthe influence of training dynamics, model capabilities, and elements of\nmechanistic interpretability. Overall, our work advances the understanding of\nICL and its implications, offering model developers insights into potential\nimprovements and providing AI security practitioners with a basis for more\ninformed guidelines."}
{"id": "2505.11628", "pdf": "https://arxiv.org/pdf/2505.11628.pdf", "abs": "https://arxiv.org/abs/2505.11628", "title": "Critique-Guided Distillation: Improving Supervised Fine-tuning via Better Distillation", "authors": ["Berkcan Kapusuzoglu", "Supriyo Chakraborty", "Chia-Hsuan Lee", "Sambit Sahu"], "categories": ["cs.CL", "cs.LG"], "comment": "Submitted to NeurIPS 2025", "summary": "Supervised fine-tuning (SFT) using expert demonstrations often suffer from\nthe imitation problem, where the model learns to reproduce the correct\nresponses without understanding the underlying rationale. To address this\nlimitation, we propose Critique-Guided Distillation (CGD), a novel multi-stage\nframework that integrates teacher model generated explanatory critiques and\nrefined responses into the SFT process. A student model is then trained to map\nthe triplet of prompt, teacher critique, and its own initial response to the\ncorresponding refined teacher response, thereby learning both what to imitate\nand why. Using entropy-based analysis, we show that CGD reduces refinement\nuncertainty and can be interpreted as a Bayesian posterior update. We perform\nextensive empirical evaluation of CGD, on variety of benchmark tasks, and\ndemonstrate significant gains on both math (AMC23 +17.5%) and language\nunderstanding tasks (MMLU-Pro +6.3%), while successfully mitigating the format\ndrift issues observed in previous critique fine-tuning (CFT) techniques."}
{"id": "2505.12075", "pdf": "https://arxiv.org/pdf/2505.12075.pdf", "abs": "https://arxiv.org/abs/2505.12075", "title": "Do different prompting methods yield a common task representation in language models?", "authors": ["Guy Davidson", "Todd M. Gureckis", "Brenden M. Lake", "Adina Williams"], "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, 4 figures; under review", "summary": "Demonstrations and instructions are two primary approaches for prompting\nlanguage models to perform in-context learning (ICL) tasks. Do identical tasks\nelicited in different ways result in similar representations of the task? An\nimproved understanding of task representation mechanisms would offer\ninterpretability insights and may aid in steering models. We study this through\n\\textit{function vectors} (FVs), recently proposed as a mechanism to extract\nfew-shot ICL task representations. We generalize FVs to alternative task\npresentations, focusing on short textual instruction prompts, and successfully\nextract instruction function vectors that promote zero-shot task accuracy. We\nfind evidence that demonstration- and instruction-based function vectors\nleverage different model components, and offer several controls to dissociate\ntheir contributions to task performance. Our results suggest that different\ntask promptings forms do not induce a common task representation through FVs\nbut elicit different, partly overlapping mechanisms. Our findings offer\nprincipled support to the practice of combining instructions and task\ndemonstrations, imply challenges in universally monitoring task inference\nacross presentation forms, and encourage further examinations of LLM task\ninference mechanisms."}
{"id": "2505.12082", "pdf": "https://arxiv.org/pdf/2505.12082.pdf", "abs": "https://arxiv.org/abs/2505.12082", "title": "Model Merging in Pre-training of Large Language Models", "authors": ["Yunshui Li", "Yiyuan Ma", "Shen Yan", "Chaoyi Zhang", "Jing Liu", "Jianqiao Lu", "Ziwen Xu", "Mengzhao Chen", "Minrui Wang", "Shiyi Zhan", "Jin Ma", "Xunhao Lai", "Deyi Liu", "Yao Luo", "Xingyan Bin", "Hongbin Ren", "Mingji Han", "Wenhao Hao", "Bairen Yi", "LingJun Liu", "Bole Ma", "Xiaoying Jia", "Xun Zhou", "Siyuan Qiao", "Liang Xiang", "Yonghui Wu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Model merging has emerged as a promising technique for enhancing large\nlanguage models, though its application in large-scale pre-training remains\nrelatively unexplored. In this paper, we present a comprehensive investigation\nof model merging techniques during the pre-training process. Through extensive\nexperiments with both dense and Mixture-of-Experts (MoE) architectures ranging\nfrom millions to over 100 billion parameters, we demonstrate that merging\ncheckpoints trained with constant learning rates not only achieves significant\nperformance improvements but also enables accurate prediction of annealing\nbehavior. These improvements lead to both more efficient model development and\nsignificantly lower training costs. Our detailed ablation studies on merging\nstrategies and hyperparameters provide new insights into the underlying\nmechanisms while uncovering novel applications. Through comprehensive\nexperimental analysis, we offer the open-source community practical\npre-training guidelines for effective model merging."}
{"id": "2505.12212", "pdf": "https://arxiv.org/pdf/2505.12212.pdf", "abs": "https://arxiv.org/abs/2505.12212", "title": "Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning", "authors": ["Shaobo Wang", "Xiangqi Jin", "Ziming Wang", "Jize Wang", "Jiajun Zhang", "Kaixin Li", "Zichen Wen", "Zhong Li", "Conghui He", "Xuming Hu", "Linfeng Zhang"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 main, 18 pages, 8 figures, 6 tables", "summary": "Fine-tuning large language models (LLMs) on task-specific data is essential\nfor their effective deployment. As dataset sizes grow, efficiently selecting\noptimal subsets for training becomes crucial to balancing performance and\ncomputational costs. Traditional data selection methods often require\nfine-tuning a scoring model on the target dataset, which is time-consuming and\nresource-intensive, or rely on heuristics that fail to fully leverage the\nmodel's predictive capabilities. To address these challenges, we propose Data\nWhisperer, an efficient, training-free, attention-based method that leverages\nfew-shot in-context learning with the model to be fine-tuned. Comprehensive\nevaluations were conducted on both raw and synthetic datasets across diverse\ntasks and models. Notably, Data Whisperer achieves superior performance\ncompared to the full GSM8K dataset on the Llama-3-8B-Instruct model, using just\n10% of the data, and outperforms existing methods with a 3.1-point improvement\nand a 7.4$\\times$ speedup."}
{"id": "2505.13176", "pdf": "https://arxiv.org/pdf/2505.13176.pdf", "abs": "https://arxiv.org/abs/2505.13176", "title": "ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models", "authors": ["Zihao Cheng", "Hongru Wang", "Zeming Liu", "Yuhang Guo", "Yuanfang Guo", "Yunhong Wang", "Haifeng Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Findings", "summary": "While integrating external tools into large language models (LLMs) enhances\ntheir ability to access real-time information and domain-specific services,\nexisting approaches focus narrowly on functional tool selection following user\ninstructions, overlooking the context-aware personalization in tool selection.\nThis oversight leads to suboptimal user satisfaction and inefficient tool\nutilization, particularly when overlapping toolsets require nuanced selection\nbased on contextual factors. To bridge this gap, we introduce ToolSpectrum, a\nbenchmark designed to evaluate LLMs' capabilities in personalized tool\nutilization. Specifically, we formalize two key dimensions of personalization,\nuser profile and environmental factors, and analyze their individual and\nsynergistic impacts on tool utilization. Through extensive experiments on\nToolSpectrum, we demonstrate that personalized tool utilization significantly\nimproves user experience across diverse scenarios. However, even\nstate-of-the-art LLMs exhibit the limited ability to reason jointly about user\nprofiles and environmental factors, often prioritizing one dimension at the\nexpense of the other. Our findings underscore the necessity of context-aware\npersonalization in tool-augmented LLMs and reveal critical limitations for\ncurrent models. Our data and code are available at\nhttps://github.com/Chengziha0/ToolSpectrum."}
{"id": "2505.14079", "pdf": "https://arxiv.org/pdf/2505.14079.pdf", "abs": "https://arxiv.org/abs/2505.14079", "title": "BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks", "authors": ["Weihong Du", "Wenrui Liao", "Binyu Yan", "Hongru Liang", "Anthony G. Cohn", "Wenqiang Lei"], "categories": ["cs.CL"], "comment": null, "summary": "Large language model (LLM) based agents have shown great potential in\nfollowing human instructions and automatically completing various tasks. To\ncomplete a task, the agent needs to decompose it into easily executed steps by\nplanning. Existing studies mainly conduct the planning by inferring what steps\nshould be executed next starting from the agent's initial state. However, this\nforward reasoning paradigm doesn't work well for complex tasks. We propose to\nstudy this issue in Minecraft, a virtual environment that simulates complex\ntasks based on real-world scenarios. We believe that the failure of forward\nreasoning is caused by the big perception gap between the agent's initial state\nand task goal. To this end, we leverage backward reasoning and make the\nplanning starting from the terminal state, which can directly achieve the task\ngoal in one step. Specifically, we design a BAckward Reasoning based agent\n(BAR). It is equipped with a recursive goal decomposition module, a state\nconsistency maintaining module and a stage memory module to make robust,\nconsistent, and efficient planning starting from the terminal state.\nExperimental results demonstrate the superiority of BAR over existing methods\nand the effectiveness of proposed modules."}
{"id": "2505.14464", "pdf": "https://arxiv.org/pdf/2505.14464.pdf", "abs": "https://arxiv.org/abs/2505.14464", "title": "Not All Correct Answers Are Equal: Why Your Distillation Source Matters", "authors": ["Xiaoyu Tian", "Yunjie Ji", "Haotian Wang", "Shuaiting Chen", "Sitong Zhao", "Yiping Peng", "Han Zhao", "Xiangang Li"], "categories": ["cs.CL"], "comment": null, "summary": "Distillation has emerged as a practical and effective approach to enhance the\nreasoning capabilities of open-source language models. In this work, we conduct\na large-scale empirical study on reasoning data distillation by collecting\nverified outputs from three state-of-the-art teacher models-AM-Thinking-v1,\nQwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We\nconstruct three parallel datasets and analyze their distributions, revealing\nthat AM-Thinking-v1-distilled data exhibits greater token length diversity and\nlower perplexity. Student models trained on each dataset are evaluated on\nreasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench.\nThe model distilled from AM-Thinking-v1 consistently achieves the best\nperformance (e.g., 84.3 on AIME2024, 72.2 on AIME2025, 98.4 on MATH500, and\n65.9 on LiveCodeBench) and demonstrates adaptive output behavior-producing\nlonger responses for harder tasks and shorter ones for simpler tasks. These\nfindings highlight the value of high-quality, verified reasoning traces. We\nrelease the AM-Thinking-v1 and Qwen3-235B-A22B distilled datasets to support\nfuture research on open and high-performing reasoning-oriented language models.\nThe datasets are publicly available on Hugging Face\\footnote{Datasets are\navailable on Hugging Face:\n\\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled}{AM-Thinking-v1-Distilled},\n\\href{https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled}{AM-Qwen3-Distilled}.}."}
{"id": "2505.14652", "pdf": "https://arxiv.org/pdf/2505.14652.pdf", "abs": "https://arxiv.org/abs/2505.14652", "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains", "authors": ["Xueguang Ma", "Qian Liu", "Dongfu Jiang", "Ge Zhang", "Zejun Ma", "Wenhu Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks."}
{"id": "2505.15046", "pdf": "https://arxiv.org/pdf/2505.15046.pdf", "abs": "https://arxiv.org/abs/2505.15046", "title": "ChartCards: A Chart-Metadata Generation Framework for Multi-Task Chart Understanding", "authors": ["Yifan Wu", "Lutao Yan", "Leixian Shen", "Yinan Mei", "Jiannan Wang", "Yuyu Luo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The emergence of Multi-modal Large Language Models (MLLMs) presents new\nopportunities for chart understanding. However, due to the fine-grained nature\nof these tasks, applying MLLMs typically requires large, high-quality datasets\nfor task-specific fine-tuning, leading to high data collection and training\ncosts. To address this, we propose ChartCards, a unified chart-metadata\ngeneration framework for multi-task chart understanding. ChartCards\nsystematically synthesizes various chart information, including data tables,\nvisualization code, visual elements, and multi-dimensional semantic captions.\nBy structuring this information into organized metadata, ChartCards enables a\nsingle chart to support multiple downstream tasks, such as text-to-chart\nretrieval, chart summarization, chart-to-table conversion, chart description,\nand chart question answering. Using ChartCards, we further construct MetaChart,\na large-scale high-quality dataset containing 10,862 data tables, 85K charts,\nand 170 K high-quality chart captions. We validate the dataset through\nqualitative crowdsourcing evaluations and quantitative fine-tuning experiments\nacross various chart understanding tasks. Fine-tuning six different models on\nMetaChart resulted in an average performance improvement of 5% across all\ntasks. The most notable improvements are seen in text-to-chart retrieval and\nchart-to-table tasks, with Long-CLIP and Llama 3.2-11B achieving improvements\nof 17% and 28%, respectively."}
{"id": "2505.15255", "pdf": "https://arxiv.org/pdf/2505.15255.pdf", "abs": "https://arxiv.org/abs/2505.15255", "title": "MentalMAC: Enhancing Large Language Models for Detecting Mental Manipulation via Multi-Task Anti-Curriculum Distillation", "authors": ["Yuansheng Gao", "Han Bao", "Tong Zhang", "Bin Li", "Zonghui Wang", "Wenzhi Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Mental manipulation is a subtle yet pervasive form of psychological abuse\nthat poses serious threats to mental health. Its covert nature and the\ncomplexity of manipulation strategies make it challenging to detect, even for\nstate-of-the-art large language models (LLMs). This concealment also hinders\nthe manual collection of large-scale, high-quality annotations essential for\ntraining effective models. Although recent efforts have sought to improve LLMs'\nperformance on this task, progress remains limited due to the scarcity of\nreal-world annotated datasets. To address these challenges, we propose\nMentalMAC, a multi-task anti-curriculum distillation method that enhances LLMs'\nability to detect mental manipulation in multi-turn dialogue. Our approach\nincludes: (i) EvoSA, an unsupervised data expansion method based on\nevolutionary operations and speech act theory; (ii) teacher model-generated\nmulti-task supervision; and (iii) progressive knowledge distillation from\ncomplex to simpler tasks. We then constructed the ReaMent dataset with 5,000\nreal-world dialogue samples, using a MentalMAC-distilled model to assist human\nannotation. Vast experiments demonstrate that our method significantly narrows\nthe gap between student and teacher models and outperforms competitive LLMs\nacross key evaluation metrics. All code, datasets, and checkpoints will be\nreleased upon paper acceptance. Warning: This paper contains content that may\nbe offensive to readers."}
{"id": "2505.15431", "pdf": "https://arxiv.org/pdf/2505.15431.pdf", "abs": "https://arxiv.org/abs/2505.15431", "title": "Hunyuan-TurboS: Advancing Large Language Models through Mamba-Transformer Synergy and Adaptive Chain-of-Thought", "authors": ["Tencent Hunyuan Team", "Ao Liu", "Botong Zhou", "Can Xu", "Chayse Zhou", "ChenChen Zhang", "Chengcheng Xu", "Chenhao Wang", "Decheng Wu", "Dengpeng Wu", "Dian Jiao", "Dong Du", "Dong Wang", "Feng Zhang", "Fengzong Lian", "Guanghui Xu", "Guanwei Zhang", "Hai Wang", "Haipeng Luo", "Han Hu", "Huilin Xu", "Jiajia Wu", "Jianchen Zhu", "Jianfeng Yan", "Jiaqi Zhu", "Jihong Zhang", "Jinbao Xue", "Jun Xia", "Junqiang Zheng", "Kai Liu", "Kai Zhang", "Kai Zheng", "Kejiao Li", "Keyao Wang", "Lan Jiang", "Lixin Liu", "Lulu Wu", "Mengyuan Huang", "Peijie Yu", "Peiqi Wang", "Qian Wang", "Qianbiao Xiang", "Qibin Liu", "Qingfeng Sun", "Richard Guo", "Ruobing Xie", "Saiyong Yang", "Shaohua Chen", "Shihui Hu", "Shuai Li", "Shuaipeng Li", "Shuang Chen", "Suncong Zheng", "Tao Yang", "Tian Zhang", "Tinghao Yu", "Weidong Han", "Weijie Liu", "Weijin Zhou", "Weikang Wang", "Wesleye Chen", "Xiao Feng", "Xiaoqin Ren", "Xingwu Sun", "Xiong Kuang", "Xuemeng Huang", "Xun Cao", "Yanfeng Chen", "Yang Du", "Yang Zhen", "Yangyu Tao", "Yaping Deng", "Yi Shen", "Yigeng Hong", "Yiqi Chen", "Yiqing Huang", "Yuchi Deng", "Yue Mao", "Yulong Wang", "Yuyuan Zeng", "Zenan Xu", "Zhanhui Kang", "Zhe Zhao", "ZhenXiang Yan", "Zheng Fang", "Zhichao Hu", "Zhongzhi Chen", "Zhuoyu Li", "Zongwei Li", "Alex Yan", "Ande Liang", "Baitong Liu", "Beiping Pan", "Bin Xing", "Binghong Wu", "Bingxin Qu", "Bolin Ni", "Boyu Wu", "Chen Li", "Cheng Jiang", "Cheng Zhang", "Chengjun Liu", "Chengxu Yang", "Chengzhong Xu", "Chiyu Wang", "Chong Zha", "Daisy Yi", "Di Wang", "Fanyang Lu", "Fei Chen", "Feifei Liu", "Feng Zheng", "Guanghua Yu", "Guiyang Li", "Guohua Wang", "Haisheng Lin", "Han Liu", "Han Wang", "Hao Fei", "Hao Lu", "Haoqing Jiang", "Haoran Sun", "Haotian Zhu", "Huangjin Dai", "Huankui Chen", "Huawen Feng", "Huihui Cai", "Huxin Peng", "Jackson Lv", "Jiacheng Shi", "Jiahao Bu", "Jianbo Li", "Jianglu Hu", "Jiangtao Guan", "Jianing Xu", "Jianwei Cai", "Jiarong Zhang", "Jiawei Song", "Jie Jiang", "Jie Liu", "Jieneng Yang", "Jihong Zhang", "Jin lv", "Jing Zhao", "Jinjian Li", "Jinxing Liu", "Jun Zhao", "Juntao Guo", "Kai Wang", "Kan Wu", "Lei Fu", "Lei He", "Lei Wang", "Li Liu", "Liang Dong", "Liya Zhan", "Long Cheng", "Long Xu", "Mao Zheng", "Meng Liu", "Mengkang Hu", "Nanli Chen", "Peirui Chen", "Peng He", "Pengju Pan", "Pengzhi Wei", "Qi Yang", "Qi Yi", "Roberts Wang", "Rongpeng Chen", "Rui Sun", "Rui Yang", "Ruibin Chen", "Ruixu Zhou", "Shaofeng Zhang", "Sheng Zhang", "Shihao Xu", "Shuaishuai Chang", "Shulin Liu", "SiQi Wang", "Songjia Feng", "Songling Yuan", "Tao Zhang", "Tianjiao Lang", "Tongkai Li", "Wei Deng", "Wei Li", "Weichao Wang", "Weigang Zhang", "Weixuan Sun", "Wen Ouyang", "Wenxiang Jiao", "Wenzhi Sun", "Wenzhuo Jia", "Xiang Zhang", "Xiangyu He", "Xianshun Ren", "XiaoYing Zhu", "Xiaolong Guo", "Xiaoxue Li", "Xiaoyu Ma", "Xican Lu", "Xinhua Feng", "Xinting Huang", "Xinyu Guan", "Xirui Li", "Xu Zhang", "Xudong Gao", "Xun Luo", "Xuxiang Qi", "Yangkun Chen", "Yangyu Tao", "Yanling Xiao", "Yantao Mai", "Yanze Chen", "Yao Ding", "Yeting Yang", "YiFan Song", "Yifan Yang", "Yijiao Zhu", "Yinhe Wu", "Yixian Liu", "Yong Yang", "Yuanjun Cai", "Yuanlin Tu", "Yue Zhang", "Yufei Huang", "Yuhang Zhou", "Yuhao Jiang", "Yuhong Liu", "Yuhui Hu", "Yujin Lin", "Yun Yang", "Yunhao Wang", "Yusong Zhang", "Zekun Wu", "Zelong Zhang", "Zhan Yu", "Zhaoliang Yang", "Zhe Zhao", "Zheng Li", "Zhenyu Huang", "Zhiguang Liu", "Zhijiang Xu", "Zhiqing Kui", "Zhiyin Zeng", "Zhiyuan Xiong", "Zhuo Han", "Zifan Wu", "Zigang Geng", "Zilong Zhao", "Ziyan Tang", "Ziyuan Zhu", "Zonglei Zhu", "Zhijiang Xu"], "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models."}
{"id": "2505.15472", "pdf": "https://arxiv.org/pdf/2505.15472.pdf", "abs": "https://arxiv.org/abs/2505.15472", "title": "PhysicsArena: The First Multimodal Physics Reasoning Benchmark Exploring Variable, Process, and Solution Dimensions", "authors": ["Song Dai", "Yibo Yan", "Jiamin Su", "Dongfang Zihao", "Yubo Gao", "Yonghua Hei", "Jungang Li", "Junyan Zhang", "Sicheng Tao", "Zhuoran Gao", "Xuming Hu"], "categories": ["cs.CL", "I.2.7; I.2.10"], "comment": "Under Review", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in diverse reasoning tasks, yet their application to complex\nphysics reasoning remains underexplored. Physics reasoning presents unique\nchallenges, requiring grounding in physical conditions and the interpretation\nof multimodal information. Current physics benchmarks are limited, often\nfocusing on text-only inputs or solely on problem-solving, thereby overlooking\nthe critical intermediate steps of variable identification and process\nformulation. To address these limitations, we introduce PhysicsArena, the first\nmultimodal physics reasoning benchmark designed to holistically evaluate MLLMs\nacross three critical dimensions: variable identification, physical process\nformulation, and solution derivation. PhysicsArena aims to provide a\ncomprehensive platform for assessing and advancing the multimodal physics\nreasoning abilities of MLLMs."}
{"id": "2505.15553", "pdf": "https://arxiv.org/pdf/2505.15553.pdf", "abs": "https://arxiv.org/abs/2505.15553", "title": "Social Bias in Popular Question-Answering Benchmarks", "authors": ["Angelie Kraft", "Judith Simon", "Sonja Schimmler"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Question-answering (QA) and reading comprehension (RC) benchmarks are\nessential for assessing the capabilities of large language models (LLMs) in\nretrieving and reproducing knowledge. However, we demonstrate that popular QA\nand RC benchmarks are biased and do not cover questions about different\ndemographics or regions in a representative way, potentially due to a lack of\ndiversity of those involved in their creation. We perform a qualitative content\nanalysis of 30 benchmark papers and a quantitative analysis of 20 respective\nbenchmark datasets to learn (1) who is involved in the benchmark creation, (2)\nhow social bias is addressed or prevented, and (3) whether the demographics of\nthe creators and annotators correspond to particular biases in the content.\nMost analyzed benchmark papers provided insufficient information regarding the\nstakeholders involved in benchmark creation, particularly the annotators.\nNotably, just one of the benchmark papers explicitly reported measures taken to\naddress social representation issues. Moreover, the data analysis revealed\ngender, religion, and geographic biases across a wide range of encyclopedic,\ncommonsense, and scholarly benchmarks. More transparent and bias-aware QA and\nRC benchmark creation practices are needed to facilitate better scrutiny and\nincentivize the development of fairer LLMs."}
{"id": "2505.15810", "pdf": "https://arxiv.org/pdf/2505.15810.pdf", "abs": "https://arxiv.org/abs/2505.15810", "title": "GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents", "authors": ["Yuqi Zhou", "Sunhao Dai", "Shuai Wang", "Kaiwen Zhou", "Qinglin Jia", "Jun Xu"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm,\ncoupling online Reinforcement Learning (RL) with explicit chain-of-thought\nreasoning prior to object grounding and thereby achieving substantial\nperformance gains. In this paper, we first conduct extensive analysis\nexperiments of three key components of that training pipeline: input design,\noutput evaluation, and policy update-each revealing distinct challenges arising\nfrom blindly applying general-purpose RL without adapting to GUI grounding\ntasks. Input design: Current templates encourage the model to generate\nchain-of-thought reasoning, but longer chains unexpectedly lead to worse\ngrounding performance. Output evaluation: Reward functions based on hit signals\nor box area allow models to exploit box size, leading to reward hacking and\npoor localization quality. Policy update: Online RL tends to overfit easy\nexamples due to biases in length and sample difficulty, leading to\nunder-optimization on harder cases. To address these issues, we propose three\ntargeted solutions. First, we adopt a Fast Thinking Template that encourages\ndirect answer generation, reducing excessive reasoning during training. Second,\nwe incorporate a box size constraint into the reward function to mitigate\nreward hacking. Third, we revise the RL objective by adjusting length\nnormalization and adding a difficulty-aware scaling factor, enabling better\noptimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with\nQwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on\nScreenSpot-Pro. This surpasses all prior models of similar size and even\noutperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI\nagent grounding. The project repository is available at\nhttps://github.com/Yuqi-Zhou/GUI-G1."}
{"id": "2305.04971", "pdf": "https://arxiv.org/pdf/2305.04971.pdf", "abs": "https://arxiv.org/abs/2305.04971", "title": "LABO: Towards Learning Optimal Label Regularization via Bi-level Optimization", "authors": ["Peng Lu", "Ahmad Rashid", "Ivan Kobyzev", "Mehdi Rezagholizadeh", "Philippe Langlais"], "categories": ["cs.LG", "cs.CL"], "comment": "Accepted at ACL2023 (Findings)", "summary": "Regularization techniques are crucial to improving the generalization\nperformance and training efficiency of deep neural networks. Many deep learning\nalgorithms rely on weight decay, dropout, batch/layer normalization to converge\nfaster and generalize. Label Smoothing (LS) is another simple, versatile and\nefficient regularization which can be applied to various supervised\nclassification tasks. Conventional LS, however, regardless of the training\ninstance assumes that each non-target class is equally likely. In this work, we\npresent a general framework for training with label regularization, which\nincludes conventional LS but can also model instance-specific variants. Based\non this formulation, we propose an efficient way of learning LAbel\nregularization by devising a Bi-level Optimization (LABO) problem. We derive a\ndeterministic and interpretable solution of the inner loop as the optimal label\nsmoothing without the need to store the parameters or the output of a trained\nmodel. Finally, we conduct extensive experiments and demonstrate our LABO\nconsistently yields improvement over conventional label regularization on\nvarious fields, including seven machine translation and three image\nclassification tasks across various"}
{"id": "2402.09664", "pdf": "https://arxiv.org/pdf/2402.09664.pdf", "abs": "https://arxiv.org/abs/2402.09664", "title": "CodeMind: Evaluating Large Language Models for Code Reasoning", "authors": ["Changshu Liu", "Yang Chen", "Reyhaneh Jabbarvand"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL"], "comment": null, "summary": "Large Language Models (LLMs) have been widely used to automate programming\ntasks. Their capabilities have been evaluated by assessing the quality of\ngenerated code through tests or proofs. The extent to which they can reason\nabout code is a critical question revealing important insights about their true\ncapabilities. This paper introduces CodeMind, a framework designed to gauge the\ncode reasoning abilities of LLMs through the following explicit and implicit\ncode reasoning tasks: Independent Execution Reasoning (IER), Specification\nReasoning (SR) and Dynamic Semantics Reasoning (DSR). The first evaluates the\nabilities of LLMs to simulate the execution of given inputs to a code and\npredict the output (IER). The second assesses the abilities of LLMs to\nincorporate the simulation of test data in the specification into code\ngeneration (SR). Finally, CodeMind evaluates LLMs' abilities to understand\noverall code semantics only given a specific input/output (DSR). Our extensive\nevaluation of ten LLMs across four widely used benchmarks using CodeMind shows\nthat LLMs, depending on their size and training strategy, can reason about some\ndynamic aspects of code. However, their performance drops for code with higher\ncomplexity, non-trivial logical and arithmetic operators, non-primitive types,\nand API calls. We show that these reasoning tasks evaluate LLMs differently,\nand a comprehensive evaluation of code reasoning requires them all. Finally, we\nshow that the performance of LLMs in bug repair is not correlated with any of\nthe code reasoning tasks, and except for advanced frontier models, other LLMs\ndo not incorporate code reasoning when performing bug repair."}
{"id": "2405.13873", "pdf": "https://arxiv.org/pdf/2405.13873.pdf", "abs": "https://arxiv.org/abs/2405.13873", "title": "FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph Question Answering", "authors": ["Yuan Sui", "Yufei He", "Nian Liu", "Xiaoxin He", "Kun Wang", "Bryan Hooi"], "categories": ["cs.AI", "cs.CL"], "comment": "This paper has been accepted by ACL 2025", "summary": "Large Language Models (LLMs) are often challenged by generating erroneous or\nhallucinated responses, especially in complex reasoning tasks. Leveraging\nKnowledge Graphs (KGs) as external knowledge sources has emerged as a viable\nsolution. However, existing KG-enhanced methods, either retrieval-based or\nagent-based, encounter difficulties in accurately retrieving knowledge and\nefficiently traversing KGs at scale. In this paper, we propose a unified\nframework, FiDeLiS, designed to improve the factuality of LLM responses by\nanchoring answers to verifiable reasoning steps retrieved from KGs. To achieve\nthis, we leverage step-wise beam search with a deductive scoring function,\nallowing the LLM to validate reasoning process step by step, and halt the\nsearch once the question is deducible. In addition, we propose a Path-RAG\nmodule to pre-select a smaller candidate set for each beam search step,\nreducing computational costs by narrowing the search space. Extensive\nexperiments show that our method, as a training-free framework, not only\nimprove the performance but also enhance the factuality and interpretability\nacross different benchmarks. Code is released at\nhttps://github.com/Y-Sui/FiDeLiS."}
{"id": "2407.08112", "pdf": "https://arxiv.org/pdf/2407.08112.pdf", "abs": "https://arxiv.org/abs/2407.08112", "title": "How Well Can a Long Sequence Model Model Long Sequences? Comparing Architechtural Inductive Biases on Long-Context Abilities", "authors": ["Jerry Huang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to The 31st International Conference on Computational\n  Linguistics (COLING), 2025", "summary": "Long sequences occur in abundance within real-world scenarios, hence properly\nmodelling them opens numerous down-stream use-cases. Deep neural networks,\nhowever, have often struggled with these for a variety of reasons. Recent\nadvances, both in system engineering as well as model design, have enabled the\nscaling up of model that are purported to support extended context length. In\nparticular, the state-space and linear recurrent neural network families of\nmodels hypothetically can entend to infinite sequence lenth. However, is this\ntoo good to be true? We conduct an evaluation to show that while such claims\nmay be sound theoretically, there remain large practical gaps that are\nempirically observed. In particular, recurrent models still suffer in the same\nsettings as long-context LLMs with attention. We further show that different\ninductive biases have inconsistent extrapolation capabilities, highlighting the\nneed to further study such paradigms and investigate why long-context models\nseemingly fail to behave as one might expect."}
{"id": "2408.15966", "pdf": "https://arxiv.org/pdf/2408.15966.pdf", "abs": "https://arxiv.org/abs/2408.15966", "title": "More Text, Less Point: Towards 3D Data-Efficient Point-Language Understanding", "authors": ["Yuan Tang", "Xu Han", "Xianzhi Li", "Qiao Yu", "Jinfeng Xu", "Yixue Hao", "Long Hu", "Min Chen"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Enabling Large Language Models (LLMs) to comprehend the 3D physical world\nremains a significant challenge. Due to the lack of large-scale 3D-text pair\ndatasets, the success of LLMs has yet to be replicated in 3D understanding. In\nthis paper, we rethink this issue and propose a new task: 3D Data-Efficient\nPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3D\nobject understanding with minimal 3D point cloud and text data pairs. To\naddress this task, we introduce GreenPLM, which leverages more text data to\ncompensate for the lack of 3D data. First, inspired by using CLIP to align\nimages and text, we utilize a pre-trained point cloud-text encoder to map the\n3D point cloud space to the text space. This mapping leaves us to seamlessly\nconnect the text space with LLMs. Once the point-text-LLM connection is\nestablished, we further enhance text-LLM alignment by expanding the\nintermediate text space, thereby reducing the reliance on 3D point cloud data.\nSpecifically, we generate 6M free-text descriptions of 3D objects, and design a\nthree-stage training strategy to help LLMs better explore the intrinsic\nconnections between different modalities. To achieve efficient modality\nalignment, we design a zero-parameter cross-attention module for token pooling.\nExtensive experimental results show that GreenPLM requires only 12% of the 3D\ntraining data used by existing state-of-the-art models to achieve superior 3D\nunderstanding. Remarkably, GreenPLM also achieves competitive performance using\ntext-only data. The code and weights are available at:\nhttps://github.com/TangYuan96/GreenPLM."}
{"id": "2411.13865", "pdf": "https://arxiv.org/pdf/2411.13865.pdf", "abs": "https://arxiv.org/abs/2411.13865", "title": "Breaking Information Cocoons: A Hyperbolic Graph-LLM Framework for Exploration and Exploitation in Recommender Systems", "authors": ["Qiyao Ma", "Menglin Yang", "Mingxuan Ju", "Tong Zhao", "Neil Shah", "Rex Ying"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Modern recommender systems often create information cocoons, restricting\nusers' exposure to diverse content. A key challenge lies in balancing content\nexploration and exploitation while allowing users to adjust their\nrecommendation preferences. Intuitively, this balance can be modeled as a\ntree-structured representation, where depth search facilitates exploitation and\nbreadth search enables exploration. However, existing approaches face two\nfundamental limitations: Euclidean methods struggle to capture hierarchical\nstructures, while hyperbolic methods, despite their superior hierarchical\nmodeling, lack semantic understanding of user and item profiles and fail to\nprovide a principled mechanism for balancing exploration and exploitation. To\naddress these challenges, we propose HERec, a hyperbolic graph-LLM framework\nthat effectively balances exploration and exploitation in recommender systems.\nOur framework introduces two key innovations: (1) a semantic-enhanced\nhierarchical mechanism that aligns rich textual descriptions processed by large\nlanguage models (LLMs) with collaborative information directly in hyperbolic\nspace, allowing for more nuanced updates that respect the underlying\nhierarchical structure in user-item profiles; (2) an automatic hierarchical\nrepresentation by optimizing Dasgupta's cost, which discovers hierarchical\nstructures without requiring predefined hyperparameters, enabling\nuser-adjustable exploration-exploitation trade-offs. Extensive experiments\ndemonstrate that HERec consistently outperforms both Euclidean and hyperbolic\nbaselines, achieving up to 5.49% improvement in utility metrics and 11.39%\nincrease in diversity metrics, effectively mitigating information cocoons. We\nopen-source our model implementation at https://github.com/Martin-qyma/HERec."}
{"id": "2412.04614", "pdf": "https://arxiv.org/pdf/2412.04614.pdf", "abs": "https://arxiv.org/abs/2412.04614", "title": "Extractive Structures Learned in Pretraining Enable Generalization on Finetuned Facts", "authors": ["Jiahai Feng", "Stuart Russell", "Jacob Steinhardt"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Pretrained language models (LMs) can generalize to implications of facts that\nthey are finetuned on. For example, if finetuned on ``John Doe lives in Tokyo,\"\nLMs can correctly answer ``What language do the people in John Doe's city\nspeak?'' with ``Japanese''. However, little is known about the mechanisms that\nenable this generalization or how they are learned during pretraining. We\nintroduce extractive structures as a framework for describing how components in\nLMs (e.g., MLPs or attention heads) coordinate to enable this generalization.\nThe structures consist of informative components that store training facts as\nweight changes, and upstream and downstream extractive components that query\nand process the stored information to produce the correct implication. We\nhypothesize that extractive structures are learned during pretraining when\nencountering implications of previously known facts. This yields two\npredictions: a data ordering effect where extractive structures can be learned\nonly if facts precede their implications, and a weight grafting effect where\nextractive structures can be transferred to predict counterfactual\nimplications. We empirically demonstrate these phenomena in the OLMo-7b, Llama\n3-8b, Gemma 2-9b, and Qwen 2-7b models. Of independent interest, our results\nalso indicate that fact learning can occur at both early and late layers, which\nlead to different forms of generalization."}
{"id": "2501.11264", "pdf": "https://arxiv.org/pdf/2501.11264.pdf", "abs": "https://arxiv.org/abs/2501.11264", "title": "Code Readability in the Age of Large Language Models: An Industrial Case Study from Atlassian", "authors": ["Wannita Takerngsaksiri", "Micheal Fu", "Chakkrit Tantithamthavorn", "Jirat Pasuksmit", "Kun Chen", "Ming Wu"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "11 pages, 7 figures, 8 tables, under review", "summary": "Software engineers spend a significant amount of time reading code during the\nsoftware development process. This trend is amplified by the emergence of large\nlanguage models (LLMs) that automatically generate code. However, little is\nknown about the readability of the LLM-generated code and whether it is still\nimportant from practitioners' perspectives in this new era. In this paper, we\nconduct a survey to explore the practitioners' perspectives on code readability\nin the age of LLMs and investigate the readability of our LLM-based software\ndevelopment agents framework, HULA, by comparing its generated code with\nhuman-written code in real-world scenarios. Overall, the findings underscore\nthat (1) readability remains a critical aspect of software development; (2) the\nreadability of our LLM-generated code is comparable to human-written code,\nfostering the establishment of appropriate trust and driving the broad adoption\nof our LLM-powered software development platform."}
{"id": "2502.00691", "pdf": "https://arxiv.org/pdf/2502.00691.pdf", "abs": "https://arxiv.org/abs/2502.00691", "title": "To Code or not to Code? Adaptive Tool Integration for Math Language Models via Expectation-Maximization", "authors": ["Haozhe Wang", "Long Li", "Chao Qu", "Fengming Zhu", "Weidi Xu", "Wei Chu", "Fangzhen Lin"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025", "summary": "Recent advances in mathematical problem-solving with language models (LMs)\nintegrate chain-of-thought (CoT) reasoning and code execution to harness their\ncomplementary strengths. However, existing hybrid frameworks exhibit a critical\nlimitation: they depend on externally dictated instructions or rigid\ncode-integration templates, lacking metacognitive awareness -- the capacity to\ndynamically evaluate intrinsic capabilities and autonomously determine when and\nhow to integrate tools. This rigidity motivates our study of autonomous code\nintegration, enabling models to adapt tool-usage strategies as their reasoning\nabilities evolve during training.\n  While reinforcement learning (RL) shows promise for boosting LLM reasoning at\nscale (e.g., DeepSeek-R1), we demonstrate its inefficiency in learning\nautonomous code integration due to inadequate exploration of the vast\ncombinatorial space of CoT-code interleaving patterns. To address this\nchallenge, we propose a novel Expectation-Maximization (EM) framework that\nsynergizes structured exploration (E-step) with off-policy RL optimization\n(M-step), creating a self-reinforcing cycle between metacognitive tool-use\ndecisions and evolving capabilities. Experiments reveal our method achieves\nsuperior results through improved exploration. Notably, our 7B model improves\nover 11% on MATH500 and 9.4% on AIME without o1-like CoT."}
{"id": "2502.15814", "pdf": "https://arxiv.org/pdf/2502.15814.pdf", "abs": "https://arxiv.org/abs/2502.15814", "title": "Slamming: Training a Speech Language Model on One GPU in a Day", "authors": ["Gallil Maimon", "Avishai Elmakies", "Yossi Adi"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SD", "eess.AS"], "comment": "ACL 2025 (Findings)", "summary": "We introduce Slam, a recipe for training high-quality Speech Language Models\n(SLMs) on a single academic GPU in 24 hours. We do so through empirical\nanalysis of model initialisation and architecture, synthetic training data,\npreference optimisation with synthetic data and tweaking all other components.\nWe empirically demonstrate that this training recipe also scales well with more\ncompute getting results on par with leading SLMs in a fraction of the compute\ncost. We hope these insights will make SLM training and research more\naccessible. In the context of SLM scaling laws, our results far outperform\npredicted compute optimal performance, giving an optimistic view to SLM\nfeasibility. See code, data, models, samples at -\nhttps://pages.cs.huji.ac.il/adiyoss-lab/slamming ."}
{"id": "2502.20167", "pdf": "https://arxiv.org/pdf/2502.20167.pdf", "abs": "https://arxiv.org/abs/2502.20167", "title": "Similarity-Distance-Magnitude Universal Verification", "authors": ["Allen Schmaltz"], "categories": ["cs.LG", "cs.CL"], "comment": "36 pages (1 Figure, 8 Tables, 4 Algorithms, 5 Listings)", "summary": "We address the neural network robustness problem by adding Similarity (i.e.,\ncorrectly predicted depth-matches into training)-awareness and\nDistance-to-training-distribution-awareness to the existing output Magnitude\n(i.e., decision-boundary)-awareness of the softmax function. The resulting SDM\nactivation function provides strong signals of the relative epistemic\n(reducible) predictive uncertainty. We use this novel behavior to further\naddress the complementary HCI problem of mapping the output to\nhuman-interpretable summary statistics over relevant partitions of a held-out\ncalibration set. Estimates of prediction-conditional uncertainty are obtained\nvia a parsimonious learned transform over the class-conditional empirical CDFs\nof the output of a final-layer SDM activation function. For decision-making and\nas an intrinsic model check, estimates of class-conditional accuracy are\nobtained by further partitioning the high-probability regions of this\ncalibrated output into class-conditional, region-specific CDFs. The uncertainty\nestimates from SDM calibration are remarkably robust to test-time distribution\nshifts and out-of-distribution inputs; incorporate awareness of the effective\nsample size; provide estimates of uncertainty from the learning and data\nsplitting processes; and are well-suited for selective classification and\nconditional branching for additional test-time compute based on the predictive\nuncertainty, as for selective LLM generation, routing, and composition over\nmultiple models and retrieval. Finally, we construct SDM networks, LLMs with\nuncertainty-aware verification and interpretability-by-exemplar as intrinsic\nproperties. We provide open-source software implementing these results."}
{"id": "2503.01222", "pdf": "https://arxiv.org/pdf/2503.01222.pdf", "abs": "https://arxiv.org/abs/2503.01222", "title": "Retrieval-Augmented Perception: High-Resolution Image Perception Meets Visual RAG", "authors": ["Wenbin Wang", "Yongcheng Jing", "Liang Ding", "Yingjie Wang", "Li Shen", "Yong Luo", "Bo Du", "Dacheng Tao"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "High-resolution (HR) image perception remains a key challenge in multimodal\nlarge language models (MLLMs). To overcome the limitations of existing methods,\nthis paper shifts away from prior dedicated heuristic approaches and revisits\nthe most fundamental idea to HR perception by enhancing the long-context\ncapability of MLLMs, driven by recent advances in long-context techniques like\nretrieval-augmented generation (RAG) for general LLMs. Towards this end, this\npaper presents the first study exploring the use of RAG to address HR\nperception challenges. Specifically, we propose Retrieval-Augmented Perception\n(RAP), a training-free framework that retrieves and fuses relevant image crops\nwhile preserving spatial context using the proposed Spatial-Awareness Layout.\nTo accommodate different tasks, the proposed Retrieved-Exploration Search\n(RE-Search) dynamically selects the optimal number of crops based on model\nconfidence and retrieval scores. Experimental results on HR benchmarks\ndemonstrate the significant effectiveness of RAP, with LLaVA-v1.5-13B achieving\na 43% improvement on $V^*$ Bench and 19% on HR-Bench."}
{"id": "2503.01917", "pdf": "https://arxiv.org/pdf/2503.01917.pdf", "abs": "https://arxiv.org/abs/2503.01917", "title": "Steer LLM Latents for Hallucination Detection", "authors": ["Seongheon Park", "Xuefeng Du", "Min-Hsuan Yeh", "Haobo Wang", "Yixuan Li"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML 2025", "summary": "Hallucinations in LLMs pose a significant concern to their safe deployment in\nreal-world applications. Recent approaches have leveraged the latent space of\nLLMs for hallucination detection, but their embeddings, optimized for\nlinguistic coherence rather than factual accuracy, often fail to clearly\nseparate truthful and hallucinated content. To this end, we propose the\nTruthfulness Separator Vector (TSV), a lightweight and flexible steering vector\nthat reshapes the LLM's representation space during inference to enhance the\nseparation between truthful and hallucinated outputs, without altering model\nparameters. Our two-stage framework first trains TSV on a small set of labeled\nexemplars to form compact and well-separated clusters. It then augments the\nexemplar set with unlabeled LLM generations, employing an optimal\ntransport-based algorithm for pseudo-labeling combined with a confidence-based\nfiltering process. Extensive experiments demonstrate that TSV achieves\nstate-of-the-art performance with minimal labeled data, exhibiting strong\ngeneralization across datasets and providing a practical solution for\nreal-world LLM applications."}
{"id": "2503.03360", "pdf": "https://arxiv.org/pdf/2503.03360.pdf", "abs": "https://arxiv.org/abs/2503.03360", "title": "Transformers for molecular property prediction: Domain adaptation efficiently improves performance", "authors": ["Afnan Sultan", "Max Rausch-Dupont", "Shahrukh Khan", "Olga Kalinina", "Dietrich Klakow", "Andrea Volkamer"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Over the past six years, molecular transformer models have become key tools\nin drug discovery. Most existing models are pre-trained on large, unlabeled\ndatasets such as ZINC or ChEMBL. However, the extent to which large-scale\npre-training improves molecular property prediction remains unclear. This study\nevaluates transformer models for this task while addressing their limitations.\nWe explore how pre-training dataset size and chemically informed objectives\nimpact performance. Our results show that increasing the dataset beyond\napproximately 400K to 800K molecules from large-scale unlabeled databases does\nnot enhance performance across seven datasets covering five ADME endpoints:\nlipophilicity, permeability, solubility (two datasets), microsomal stability\n(two datasets), and plasma protein binding. In contrast, domain adaptation on a\nsmall, domain-specific dataset (less than or equal 4K molecules) using\nmulti-task regression of physicochemical properties significantly boosts\nperformance (P-value less than 0.001). A model pre-trained on 400K molecules\nand adapted with domain-specific data outperforms larger models such as\nMolFormer and performs comparably to MolBERT. Benchmarks against Random Forest\n(RF) baselines using descriptors and Morgan fingerprints show that chemically\nand physically informed features consistently yield better performance across\nmodel types. While RF remains a strong baseline, we identify concrete practices\nto enhance transformer performance. Aligning pre-training and adaptation with\nchemically meaningful tasks and domain-relevant data presents a promising\ndirection for molecular property prediction. Our models are available on\nHuggingFace for easy use and adaptation."}
{"id": "2503.05066", "pdf": "https://arxiv.org/pdf/2503.05066.pdf", "abs": "https://arxiv.org/abs/2503.05066", "title": "Capacity-Aware Inference: Mitigating the Straggler Effect in Mixture of Experts", "authors": ["Shwai He", "Weilin Cai", "Jiayi Huang", "Ang Li"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The Mixture of Experts (MoE) is an effective architecture for scaling large\nlanguage models by leveraging sparse expert activation, optimizing the\ntrade-off between performance and efficiency. However, under expert\nparallelism, MoE suffers from inference inefficiencies due to imbalanced\ntoken-to-expert assignment, where some experts are overloaded while others\nremain underutilized. This imbalance leads to poor resource utilization and\nincreased latency, as the most burdened expert dictates the overall delay, a\nphenomenon we define as the \\textbf{\\textit{Straggler Effect}}. To mitigate\nthis, we propose Capacity-Aware Inference, including two key techniques: (1)\n\\textbf{\\textit{Capacity-Aware Token Drop}}, which discards overloaded tokens\nto regulate the maximum latency of MoE, and (2) \\textbf{\\textit{Capacity-Aware\nToken Reroute}}, which reallocates overflowed tokens to underutilized experts,\nbalancing the token distribution. These techniques collectively optimize both\nhigh-load and low-load expert utilization, leading to a more efficient MoE\ninference pipeline. Extensive experiments demonstrate the effectiveness of our\nmethods, showing significant improvements in inference efficiency, e.g., 0.2\\%\naverage performance increase and a 1.94$\\times$ inference speedup on\nMixtral-8$\\times$7B-Instruct."}
{"id": "2503.09499", "pdf": "https://arxiv.org/pdf/2503.09499.pdf", "abs": "https://arxiv.org/abs/2503.09499", "title": "MindGYM: What Matters in Question Synthesis for Thinking-Centric Fine-Tuning?", "authors": ["Zhe Xu", "Daoyuan Chen", "Zhenqing Ling", "Yaliang Li", "Ying Shen"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "22 pages, 7 tables", "summary": "Large foundation models face challenges in acquiring transferable, structured\nthinking abilities, especially when supervised with rigid templates or\ncrowd-annotated instruction datasets. Unlike prior approaches, we focus on a\nthinking-centric data synthesis paradigm that enables models to evolve through\nself-generated, cognitively guided data. We propose MindGYM, a structured and\nscalable framework for question synthesis, composed of: (1) Cognitive Thinking\nProcess Injection, which infuses high-level reasoning objectives to shape the\nmodel's synthesis behavior; (2) Seed Single-Hop Question Synthesis, generating\natomic questions from diverse semantic types to encourage broader thinking; and\n(3) Challenging Multi-Hop QA Synthesis, composing more complex multi-hop\nquestions based on QA seeds for deeper reasoning. Detailed analysis shows that\nsynthetic data generated by our method achieves 16.7% higher average quality\nand 67.91% lower quality variance compared to baseline sources, highlighting\nthat both high-quality and self-contained data are essential for effective,\nthinking-oriented fine-tuning. MindGYM improves performance on six reasoning\nbenchmarks, achieving gains of up to 16% on MathVision using only 400 data\nsamples, and generalizable improvements across different model sizes and\narchitectures. MindGYM underscores the viability of self-challenging mechanisms\nin refining large model capabilities while minimizing human intervention and\nresource demands. Code and data are released to promote data-centric research\ninto self-evolving foundation models driven by their internal reasoning\ncapabilities."}
{"id": "2503.20576", "pdf": "https://arxiv.org/pdf/2503.20576.pdf", "abs": "https://arxiv.org/abs/2503.20576", "title": "Optimizing Case-Based Reasoning System for Functional Test Script Generation with Large Language Models", "authors": ["Siyuan Guo", "Huiwu Liu", "Xiaolong Chen", "Yuming Xie", "Liang Zhang", "Tao Han", "Hechang Chen", "Yi Chang", "Jun Wang"], "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": "Accepted by KDD 2025 (ADS Track)", "summary": "In this work, we explore the potential of large language models (LLMs) for\ngenerating functional test scripts, which necessitates understanding the\ndynamically evolving code structure of the target software. To achieve this, we\npropose a case-based reasoning (CBR) system utilizing a 4R cycle (i.e.,\nretrieve, reuse, revise, and retain), which maintains and leverages a case bank\nof test intent descriptions and corresponding test scripts to facilitate LLMs\nfor test script generation. To improve user experience further, we introduce\nRe4, an optimization method for the CBR system, comprising reranking-based\nretrieval finetuning and reinforced reuse finetuning. Specifically, we first\nidentify positive examples with high semantic and script similarity, providing\nreliable pseudo-labels for finetuning the retriever model without costly\nlabeling. Then, we apply supervised finetuning, followed by a reinforcement\nlearning finetuning stage, to align LLMs with our production scenarios,\nensuring the faithful reuse of retrieved cases. Extensive experimental results\non two product development units from Huawei Datacom demonstrate the\nsuperiority of the proposed CBR+Re4. Notably, we also show that the proposed\nRe4 method can help alleviate the repetitive generation issues with LLMs."}
{"id": "2505.12269", "pdf": "https://arxiv.org/pdf/2505.12269.pdf", "abs": "https://arxiv.org/abs/2505.12269", "title": "Vague Knowledge: Evidence from Analyst Reports", "authors": ["Kerry Xiao", "Amy Zang"], "categories": ["econ.GN", "cs.AI", "cs.CL", "math.LO", "q-fin.EC", "q-fin.GN", "03B48, 03B65, 03E02, 03E15, 03E72, 18E45, 28A05, 62F15, 68T01,\n  68T35, 68T50, 91G30,", "F.4; I.2.3; I.2.4; I.2.7; J.1; J.4; J.5"], "comment": null, "summary": "People in the real world often possess vague knowledge of future payoffs, for\nwhich quantification is not feasible or desirable. We argue that language, with\ndiffering ability to convey vague information, plays an important but less\nknown-role in representing subjective expectations. Empirically, we find that\nin their reports, analysts include useful information in linguistic expressions\nbut not numerical forecasts. Specifically, the textual tone of analyst reports\nhas predictive power for forecast errors and subsequent revisions in numerical\nforecasts, and this relation becomes stronger when analyst's language is\nvaguer, when uncertainty is higher, and when analysts are busier. Overall, our\ntheory and evidence suggest that some useful information is vaguely known and\nonly communicated through language."}
{"id": "2505.13862", "pdf": "https://arxiv.org/pdf/2505.13862.pdf", "abs": "https://arxiv.org/abs/2505.13862", "title": "PandaGuard: Systematic Evaluation of LLM Safety against Jailbreaking Attacks", "authors": ["Guobin Shen", "Dongcheng Zhao", "Linghao Feng", "Xiang He", "Jihang Wang", "Sicheng Shen", "Haibo Tong", "Yiting Dong", "Jindong Li", "Xiang Zheng", "Yi Zeng"], "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable capabilities but remain\nvulnerable to adversarial prompts known as jailbreaks, which can bypass safety\nalignment and elicit harmful outputs. Despite growing efforts in LLM safety\nresearch, existing evaluations are often fragmented, focused on isolated attack\nor defense techniques, and lack systematic, reproducible analysis. In this\nwork, we introduce PandaGuard, a unified and modular framework that models LLM\njailbreak safety as a multi-agent system comprising attackers, defenders, and\njudges. Our framework implements 19 attack methods and 12 defense mechanisms,\nalong with multiple judgment strategies, all within a flexible plugin\narchitecture supporting diverse LLM interfaces, multiple interaction modes, and\nconfiguration-driven experimentation that enhances reproducibility and\npractical deployment. Built on this framework, we develop PandaBench, a\ncomprehensive benchmark that evaluates the interactions between these\nattack/defense methods across 49 LLMs and various judgment approaches,\nrequiring over 3 billion tokens to execute. Our extensive evaluation reveals\nkey insights into model vulnerabilities, defense cost-performance trade-offs,\nand judge consistency. We find that no single defense is optimal across all\ndimensions and that judge disagreement introduces nontrivial variance in safety\nassessments. We release the code, configurations, and evaluation results to\nsupport transparent and reproducible research in LLM safety."}
{"id": "2505.14625", "pdf": "https://arxiv.org/pdf/2505.14625.pdf", "abs": "https://arxiv.org/abs/2505.14625", "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning", "authors": ["Zhangchen Xu", "Yuetai Li", "Fengqing Jiang", "Bhaskar Ramasubramanian", "Luyao Niu", "Bill Yuchen Lin", "Radha Poovendran"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV."}
{"id": "2505.15298", "pdf": "https://arxiv.org/pdf/2505.15298.pdf", "abs": "https://arxiv.org/abs/2505.15298", "title": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "authors": ["Kangan Qian", "Sicong Jiang", "Yang Zhong", "Ziang Luo", "Zilin Huang", "Tianze Zhu", "Kun Jiang", "Mengmeng Yang", "Zheng Fu", "Jinyu Miao", "Yining Shi", "He Zhe Lim", "Li Liu", "Tianbao Zhou", "Hongyi Wang", "Huang Yu", "Yifei Hu", "Guang Li", "Guang Chen", "Hao Ye", "Lijun Sun", "Diange Yang"], "categories": ["cs.RO", "cs.CL", "cs.CV"], "comment": "18 pages, 8 figures", "summary": "Vision-Language Models (VLMs) show promise for autonomous driving, yet their\nstruggle with hallucinations, inefficient reasoning, and limited real-world\nvalidation hinders accurate perception and robust step-by-step reasoning. To\novercome this, we introduce \\textbf{AgentThink}, a pioneering unified framework\nthat, for the first time, integrates Chain-of-Thought (CoT) reasoning with\ndynamic, agent-style tool invocation for autonomous driving tasks. AgentThink's\ncore innovations include: \\textbf{(i) Structured Data Generation}, by\nestablishing an autonomous driving tool library to automatically construct\nstructured, self-verified reasoning data explicitly incorporating tool usage\nfor diverse driving scenarios; \\textbf{(ii) A Two-stage Training Pipeline},\nemploying Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization\n(GRPO) to equip VLMs with the capability for autonomous tool invocation; and\n\\textbf{(iii) Agent-style Tool-Usage Evaluation}, introducing a novel\nmulti-tool assessment protocol to rigorously evaluate the model's tool\ninvocation and utilization. Experiments on the DriveLMM-o1 benchmark\ndemonstrate AgentThink significantly boosts overall reasoning scores by\n\\textbf{53.91\\%} and enhances answer accuracy by \\textbf{33.54\\%}, while\nmarkedly improving reasoning quality and consistency. Furthermore, ablation\nstudies and robust zero-shot/few-shot generalization experiments across various\nbenchmarks underscore its powerful capabilities. These findings highlight a\npromising trajectory for developing trustworthy and tool-aware autonomous\ndriving models."}
