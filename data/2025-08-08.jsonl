{"id": "2508.04713", "pdf": "https://arxiv.org/pdf/2508.04713.pdf", "abs": "https://arxiv.org/abs/2508.04713", "title": "AI Should Be More Human, Not More Complex", "authors": ["Carlo Esposito"], "categories": ["cs.HC", "cs.AI"], "comment": "2025 - Knowledge Commons - Eyed Research Collection", "summary": "Large Language Models (LLMs) in search applications increasingly prioritize\nverbose, lexically complex responses that paradoxically reduce user\nsatisfaction and engagement. Through a comprehensive study of 10.000 (est.)\nparticipants comparing responses from five major AI-powered search systems, we\ndemonstrate that users overwhelmingly prefer concise, source-attributed\nresponses over elaborate explanations. Our analysis reveals that current AI\ndevelopment trends toward \"artificial sophistication\" create an uncanny valley\neffect where systems sound knowledgeable but lack genuine critical thinking,\nleading to reduced trust and increased cognitive load. We present evidence that\noptimal AI communication mirrors effective human discourse: direct, properly\nsourced, and honest about limitations. Our findings challenge the prevailing\nassumption that more complex AI responses indicate better performance, instead\nsuggesting that human-like brevity and transparency are key to user engagement\nand system reliability."}
{"id": "2508.04787", "pdf": "https://arxiv.org/pdf/2508.04787.pdf", "abs": "https://arxiv.org/abs/2508.04787", "title": "Evaluating the Impact of LLM-guided Reflection on Learning Outcomes with Interactive AI-Generated Educational Podcasts", "authors": ["Vishnu Menon", "Andy Cherney", "Elizabeth B. Cloude", "Li Zhang", "Tiffany D. Do"], "categories": ["cs.HC", "cs.AI"], "comment": "Accepted to NCME Special Interest Group on AI in Measurement:\n  AIME-CON 2025 conference", "summary": "This study examined whether embedding LLM-guided reflection prompts in an\ninteractive AI-generated podcast improved learning and user experience compared\nto a version without prompts. Thirty-six undergraduates participated, and while\nlearning outcomes were similar across conditions, reflection prompts reduced\nperceived attractiveness, highlighting a call for more research on reflective\ninteractivity design."}
{"id": "2508.04821", "pdf": "https://arxiv.org/pdf/2508.04821.pdf", "abs": "https://arxiv.org/abs/2508.04821", "title": "At a Glance to Your Fingertips: Enabling Direct Manipulation of Distant Objects Through SightWarp", "authors": ["Yang Liu", "Thorbj√∏rn Mikkelsen", "Zehai Liu", "Gengchen Tian", "Diako Mardanbegi", "Qiushi Zhou", "Hans Gellersen", "Ken Pfeuffer"], "categories": ["cs.HC"], "comment": "12 pages, 11 figures, The 38th Annual ACM Symposium on User Interface\n  Software and Technology (UIST '25), September 28-October 01, 2025, Busan,\n  Republic of Korea", "summary": "In 3D user interfaces, reaching out to grab and manipulate something works\ngreat until it is out of reach. Indirect techniques like gaze and pinch offer\nan alternative for distant interaction, but do not provide the same immediacy\nor proprioceptive feedback as direct gestures. To support direct gestures for\nfaraway objects, we introduce SightWarp, an interaction technique that exploits\neye-hand coordination to seamlessly summon object proxies to the user's\nfingertips. The idea is that after looking at a distant object, users either\nshift their gaze to the hand or move their hand into view-triggering the\ncreation of a scaled near-space proxy of the object and its surrounding\ncontext. The proxy remains active until the eye-hand pattern is released. The\nkey benefit is that users always have an option to immediately operate on the\ndistant object through a natural, direct hand gesture. Through a user study of\na 3D object docking task, we show that users can easily employ SightWarp, and\nthat subsequent direct manipulation improves performance over gaze and pinch.\nApplication examples illustrate its utility for 6DOF manipulation,\noverview-and-detail navigation, and world-in-miniature interaction. Our work\ncontributes to expressive and flexible object interactions across near and far\nspaces."}
{"id": "2508.04842", "pdf": "https://arxiv.org/pdf/2508.04842.pdf", "abs": "https://arxiv.org/abs/2508.04842", "title": "Charts-of-Thought: Enhancing LLM Visualization Literacy Through Structured Data Extraction", "authors": ["Amit Kumar Das", "Mohammad Tarun", "Klaus Mueller"], "categories": ["cs.HC"], "comment": "11 pages, 8 figures. Accepted at IEEE VIS: Visualization & Visual\n  Analytics 2025 conference, November 2-7, 2025, Vienna, Austria", "summary": "This paper evaluates the visualization literacy of modern Large Language\nModels (LLMs) and introduces a novel prompting technique called\nCharts-of-Thought. We tested three state-of-the-art LLMs (Claude-3.7-sonnet,\nGPT-4.5 preview, and Gemini-2.0-pro) on the Visualization Literacy Assessment\nTest (VLAT) using standard prompts and our structured approach. The\nCharts-of-Thought method guides LLMs through a systematic data extraction,\nverification, and analysis process before answering visualization questions.\nOur results show Claude-3.7-sonnet achieved a score of 50.17 using this method,\nfar exceeding the human baseline of 28.82. This approach improved performance\nacross all models, with score increases of 21.8% for GPT-4.5, 9.4% for\nGemini-2.0, and 13.5% for Claude-3.7 compared to standard prompting. The\nperformance gains were consistent across original and modified VLAT charts,\nwith Claude correctly answering 100% of questions for several chart types that\npreviously challenged LLMs. Our study reveals that modern multimodal LLMs can\nsurpass human performance on visualization literacy tasks when given the proper\nanalytical framework. These findings establish a new benchmark for LLM\nvisualization literacy and demonstrate the importance of structured prompting\nstrategies for complex visual interpretation tasks. Beyond improving LLM\nvisualization literacy, Charts-of-Thought could also enhance the accessibility\nof visualizations, potentially benefiting individuals with visual impairments\nor lower visualization literacy."}
{"id": "2508.04795", "pdf": "https://arxiv.org/pdf/2508.04795.pdf", "abs": "https://arxiv.org/abs/2508.04795", "title": "Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a Frozen LLM", "authors": ["Thomas Thebaud", "Yen-Ju Lu", "Matthew Wiesner", "Peter Viechnicki", "Najim Dehak"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted in the 2025 IEEE Automatic Speech Recognition and\n  Understanding Workshop", "summary": "In dialogue transcription pipelines, Large Language Models (LLMs) are\nfrequently employed in post-processing to improve grammar, punctuation, and\nreadability. We explore a complementary post-processing step: enriching\ntranscribed dialogues by adding metadata tags for speaker characteristics such\nas age, gender, and emotion. Some of the tags are global to the entire\ndialogue, while some are time-variant. Our approach couples frozen audio\nfoundation models, such as Whisper or WavLM, with a frozen LLAMA language model\nto infer these speaker attributes, without requiring task-specific fine-tuning\nof either model. Using lightweight, efficient connectors to bridge audio and\nlanguage representations, we achieve competitive performance on speaker\nprofiling tasks while preserving modularity and speed. Additionally, we\ndemonstrate that a frozen LLAMA model can compare x-vectors directly, achieving\nan Equal Error Rate of 8.8% in some scenarios."}
{"id": "2508.04859", "pdf": "https://arxiv.org/pdf/2508.04859.pdf", "abs": "https://arxiv.org/abs/2508.04859", "title": "An Implementation of a Visual Stepper in the GRASP Programming System", "authors": ["Panicz Maciej Godek"], "categories": ["cs.HC", "68", "H.5.2"], "comment": "Scheme Workshop 2024 (ICFP), 23 pages", "summary": "The direct purpose of this paper - as its title suggests - is to present how\nthe visual evaluator extension is implemented in the GRASP programming system.\nThe indirect purpose is to provide a tutorial around the design of GRASP, and\nin particular - around the architecture of its extension mechanism. Neither\nGRASP nor its extension mechanisms are, at the moment of writing this paper,\nfinal or complete, and we are certain that some details of the solutions\ndescribed in here will change even before the first release. What will not\nchange, though, is the set of problems that need to be solved in order to build\na system with capabilities similar to those of GRASP. We believe that these\nproblems might be of interest to the Scheme community."}
{"id": "2508.04796", "pdf": "https://arxiv.org/pdf/2508.04796.pdf", "abs": "https://arxiv.org/abs/2508.04796", "title": "Parity-Aware Byte-Pair Encoding: Improving Cross-lingual Fairness in Tokenization", "authors": ["Negar Foroutan", "Clara Meister", "Debjit Paul", "Joel Niklaus", "Sina Ahmadi", "Antoine Bosselut", "Rico Sennrich"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Tokenization is the first -- and often least scrutinized -- step of most NLP\npipelines. Standard algorithms for learning tokenizers rely on frequency-based\nobjectives, which favor languages dominant in the training data and\nconsequently leave lower-resource languages with tokenizations that are\ndisproportionately longer, morphologically implausible, or even riddled with\n<UNK> placeholders. This phenomenon ultimately amplifies computational and\nfinancial inequalities between users from different language backgrounds. To\nremedy this, we introduce Parity-aware Byte Pair Encoding (BPE), a variant of\nthe widely-used BPE algorithm. At every merge step, Parity-aware BPE maximizes\nthe compression gain of the currently worst-compressed language, trading a\nsmall amount of global compression for cross-lingual parity. We find\nempirically that Parity-aware BPE leads to more equitable token counts across\nlanguages, with negligible impact on global compression rate and no substantial\neffect on language-model performance in downstream tasks."}
{"id": "2508.04902", "pdf": "https://arxiv.org/pdf/2508.04902.pdf", "abs": "https://arxiv.org/abs/2508.04902", "title": "Learning AI Auditing: A Case Study of Teenagers Auditing a Generative AI Model", "authors": ["Luis Morales-Navarro", "Michelle Gan", "Evelyn Yu", "Lauren Vogelstein", "Yasmin B. Kafai", "Dana√© Metaxa"], "categories": ["cs.HC", "cs.CY", "H.5.0; K.3.2"], "comment": null, "summary": "This study investigates how high school-aged youth engage in algorithm\nauditing to identify and understand biases in artificial intelligence and\nmachine learning (AI/ML) tools they encounter daily. With AI/ML technologies\nbeing increasingly integrated into young people's lives, there is an urgent\nneed to equip teenagers with AI literacies that build both technical knowledge\nand awareness of social impacts. Algorithm audits (also called AI audits) have\ntraditionally been employed by experts to assess potential harmful biases, but\nrecent research suggests that non-expert users can also participate\nproductively in auditing. We conducted a two-week participatory design workshop\nwith 14 teenagers (ages 14-15), where they audited the generative AI model\nbehind TikTok's Effect House, a tool for creating interactive TikTok filters.\nWe present a case study describing how teenagers approached the audit, from\ndeciding what to audit to analyzing data using diverse strategies and\ncommunicating their results. Our findings show that participants were engaged\nand creative throughout the activities, independently raising and exploring new\nconsiderations, such as age-related biases, that are uncommon in professional\naudits. We drew on our expertise in algorithm auditing to triangulate their\nfindings as a way to examine if the workshop supported participants to reach\ncoherent conclusions in their audit. Although the resulting number of changes\nin race, gender, and age representation uncovered by the teens were slightly\ndifferent from ours, we reached similar conclusions. This study highlights the\npotential for auditing to inspire learning activities to foster AI literacies,\nempower teenagers to critically examine AI systems, and contribute fresh\nperspectives to the study of algorithmic harms."}
{"id": "2508.04814", "pdf": "https://arxiv.org/pdf/2508.04814.pdf", "abs": "https://arxiv.org/abs/2508.04814", "title": "Pitch Accent Detection improves Pretrained Automatic Speech Recognition", "authors": ["David Sasu", "Natalie Schluter"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "We show the performance of Automatic Speech Recognition (ASR) systems that\nuse semi-supervised speech representations can be boosted by a complimentary\npitch accent detection module, by introducing a joint ASR and pitch accent\ndetection model. The pitch accent detection component of our model achieves a\nsignificant improvement on the state-of-the-art for the task, closing the gap\nin F1-score by 41%. Additionally, the ASR performance in joint training\ndecreases WER by 28.3% on LibriSpeech, under limited resource fine-tuning. With\nthese results, we show the importance of extending pretrained speech models to\nretain or re-learn important prosodic cues such as pitch accent."}
{"id": "2508.04904", "pdf": "https://arxiv.org/pdf/2508.04904.pdf", "abs": "https://arxiv.org/abs/2508.04904", "title": "Root Cause Analysis Training for Healthcare Professionals With AI-Powered Virtual Simulation: A Proof-of-Concept", "authors": ["Yuqi Hu", "Qiwen Xiong", "Zhenzhen Qin", "Brandon Watanabe", "Yujing Wang", "Mirjana Prpa", "Ilmi Yoon"], "categories": ["cs.HC"], "comment": null, "summary": "Root Cause Analysis (RCA) is a critical tool for investigating adverse events\nin healthcare and improving patient safety. However, existing RCA training\nprograms are often limited by high resource demands, leading to insufficient\ntraining and inconsistent implementation. To address this challenge, we present\nan AI-powered 3D simulation game that helps healthcare professionals develop\nRCA skills through interactive, immersive simulations. This approach offers a\ncost-effective, scalable, and accessible alternative to traditional training.\nThe prototype simulates an RCA investigation following a death in the ICU,\nwhere learners interview five virtual avatars representing ICU team members to\ninvestigate the incident and complete a written report. The system enables\nnatural, life-like interactions with avatars via large language models (LLMs),\nemotional text-to-speech, and AI-powered animations. An additional LLM\ncomponent provides formative and summative feedback to support continual\nimprovement. We conclude by outlining plans to empirically evaluate the\nsystem's efficacy."}
{"id": "2508.04826", "pdf": "https://arxiv.org/pdf/2508.04826.pdf", "abs": "https://arxiv.org/abs/2508.04826", "title": "Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History", "authors": ["Tommaso Tosato", "Saskia Helbling", "Yorguin-Jose Mantilla-Ramos", "Mahmood Hegazy", "Alberto Tosato", "David John Lemay", "Irina Rish", "Guillaume Dumas"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models require consistent behavioral patterns for safe\ndeployment, yet their personality-like traits remain poorly understood. We\npresent PERSIST (PERsonality Stability in Synthetic Text), a comprehensive\nevaluation framework testing 25+ open-source models (1B-671B parameters) across\n500,000+ responses. Using traditional (BFI-44, SD3) and novel LLM-adapted\npersonality instruments, we systematically vary question order, paraphrasing,\npersonas, and reasoning modes. Our findings challenge fundamental deployment\nassumptions: (1) Even 400B+ models exhibit substantial response variability (SD\n> 0.4); (2) Minor prompt reordering alone shifts personality measurements by up\nto 20%; (3) Interventions expected to stabilize behavior, such as\nchain-of-thought reasoning, detailed personas instruction, inclusion of\nconversation history, can paradoxically increase variability; (4) LLM-adapted\ninstruments show equal instability to human-centric versions, confirming\narchitectural rather than translational limitations. This persistent\ninstability across scales and mitigation strategies suggests current LLMs lack\nthe foundations for genuine behavioral consistency. For safety-critical\napplications requiring predictable behavior, these findings indicate that\npersonality-based alignment strategies may be fundamentally inadequate."}
{"id": "2508.04920", "pdf": "https://arxiv.org/pdf/2508.04920.pdf", "abs": "https://arxiv.org/abs/2508.04920", "title": "Toward Supporting Narrative-Driven Data Exploration: Barriers and Design Opportunities", "authors": ["Oliver Huang", "Carolina Nobre"], "categories": ["cs.HC"], "comment": "VIS 2025 Poster Summary", "summary": "Analysts increasingly explore data through evolving, narrative-driven\ninquiries, moving beyond static dashboards and predefined metrics as their\nquestions deepen and shift. As these explorations progress, insights often\nbecome dispersed across views, making it challenging to maintain context or\nclarify how conclusions arise. Through a formative study with 48 participants,\nwe identify key barriers that hinder narrative-driven exploration, including\ndifficulty maintaining context across views, tracing reasoning paths, and\nexternalizing evolving interpretations. Our findings surface design\nopportunities to support narrative-driven analysis better."}
{"id": "2508.04903", "pdf": "https://arxiv.org/pdf/2508.04903.pdf", "abs": "https://arxiv.org/abs/2508.04903", "title": "RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM Systems with Structured Memory", "authors": ["Jun Liu", "Zhenglun Kong", "Changdi Yang", "Fan Yang", "Tianqi Li", "Peiyan Dong", "Joannah Nanjekye", "Hao Tang", "Geng Yuan", "Wei Niu", "Wenbin Zhang", "Pu Zhao", "Xue Lin", "Dong Huang", "Yanzhi Wang"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "Multi-agent large language model (LLM) systems have shown strong potential in\ncomplex reasoning and collaborative decision-making tasks. However, most\nexisting coordination schemes rely on static or full-context routing\nstrategies, which lead to excessive token consumption, redundant memory\nexposure, and limited adaptability across interaction rounds. We introduce\nRCR-Router, a modular and role-aware context routing framework designed to\nenable efficient, adaptive collaboration in multi-agent LLMs. To our knowledge,\nthis is the first routing approach that dynamically selects semantically\nrelevant memory subsets for each agent based on its role and task stage, while\nadhering to a strict token budget. A lightweight scoring policy guides memory\nselection, and agent outputs are iteratively integrated into a shared memory\nstore to facilitate progressive context refinement. To better evaluate model\nbehavior, we further propose an Answer Quality Score metric that captures\nLLM-generated explanations beyond standard QA accuracy. Experiments on three\nmulti-hop QA benchmarks -- HotPotQA, MuSiQue, and 2WikiMultihop -- demonstrate\nthat RCR-Router reduces token usage (up to 30%) while improving or maintaining\nanswer quality. These results highlight the importance of structured memory\nrouting and output-aware evaluation in advancing scalable multi-agent LLM\nsystems."}
{"id": "2508.04995", "pdf": "https://arxiv.org/pdf/2508.04995.pdf", "abs": "https://arxiv.org/abs/2508.04995", "title": "Situated Epistemic Infrastructures: A Diagnostic Framework for Post-Coherence Knowledge", "authors": ["Matthew Kelly"], "categories": ["cs.HC", "cs.AI", "cs.DL", "K.4.1; K.3; K.2"], "comment": "27 pages including references. Draft prepared for submission to\n  Science, Technology & Human Values", "summary": "Large Language Models (LLMs) such as ChatGPT have rendered visible the\nfragility of contemporary knowledge infrastructures by simulating coherence\nwhile bypassing traditional modes of citation, authority, and validation. This\npaper introduces the Situated Epistemic Infrastructures (SEI) framework as a\ndiagnostic tool for analyzing how knowledge becomes authoritative across hybrid\nhuman-machine systems under post-coherence conditions. Rather than relying on\nstable scholarly domains or bounded communities of practice, SEI traces how\ncredibility is mediated across institutional, computational, and temporal\narrangements. Integrating insights from infrastructure studies, platform\ntheory, and epistemology, the framework foregrounds coordination over\nclassification, emphasizing the need for anticipatory and adaptive models of\nepistemic stewardship. The paper contributes to debates on AI governance,\nknowledge production, and the ethical design of information systems by offering\na robust alternative to representationalist models of scholarly communication."}
{"id": "2508.04939", "pdf": "https://arxiv.org/pdf/2508.04939.pdf", "abs": "https://arxiv.org/abs/2508.04939", "title": "I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating Linguistic Shibboleth Detection in LLM Hiring Evaluations", "authors": ["Julia Kharchenko", "Tanya Roosta", "Aman Chadha", "Chirag Shah"], "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces a comprehensive benchmark for evaluating how Large\nLanguage Models (LLMs) respond to linguistic shibboleths: subtle linguistic\nmarkers that can inadvertently reveal demographic attributes such as gender,\nsocial class, or regional background. Through carefully constructed interview\nsimulations using 100 validated question-response pairs, we demonstrate how\nLLMs systematically penalize certain linguistic patterns, particularly hedging\nlanguage, despite equivalent content quality. Our benchmark generates\ncontrolled linguistic variations that isolate specific phenomena while\nmaintaining semantic equivalence, which enables the precise measurement of\ndemographic bias in automated evaluation systems. We validate our approach\nalong multiple linguistic dimensions, showing that hedged responses receive\n25.6% lower ratings on average, and demonstrate the benchmark's effectiveness\nin identifying model-specific biases. This work establishes a foundational\nframework for detecting and measuring linguistic discrimination in AI systems,\nwith broad applications to fairness in automated decision-making contexts."}
{"id": "2508.05045", "pdf": "https://arxiv.org/pdf/2508.05045.pdf", "abs": "https://arxiv.org/abs/2508.05045", "title": "Human-AI Schema Discovery and Application for Creative Problem Solving", "authors": ["Sitong Wang"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Humans often rely on underlying structural patterns-schemas-to create,\nwhether by writing stories, designing software, or composing music. Schemas\nhelp organize ideas and guide exploration, but they are often difficult to\ndiscover and apply, especially in complex or unfamiliar domains. My Ph.D.\nresearch develops a framework for human-AI schema discovery and application to\nsupport creative problem solving. I design systems that support users in\nsensemaking over examples to abstract schemas, and in operationalizing schemas\ninto human-AI co-creative workflows for application. This research offers\ninsights into how schema-guided interaction can make implicit knowledge more\naccessible and actionable, advancing more transparent and collaborative\nhuman-AI systems."}
{"id": "2508.04945", "pdf": "https://arxiv.org/pdf/2508.04945.pdf", "abs": "https://arxiv.org/abs/2508.04945", "title": "Towards Robust Evaluation of Visual Activity Recognition: Resolving Verb Ambiguity with Sense Clustering", "authors": ["Louie Hong Yao", "Nicholas Jarvis", "Tianyu Jiang"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "18 pages, 5 figures", "summary": "Evaluating visual activity recognition systems is challenging due to inherent\nambiguities in verb semantics and image interpretation. When describing actions\nin images, synonymous verbs can refer to the same event (e.g., brushing vs.\ngrooming), while different perspectives can lead to equally valid but distinct\nverb choices (e.g., piloting vs. operating). Standard exact-match evaluation,\nwhich relies on a single gold answer, fails to capture these ambiguities,\nresulting in an incomplete assessment of model performance. To address this, we\npropose a vision-language clustering framework that constructs verb sense\nclusters, providing a more robust evaluation. Our analysis of the imSitu\ndataset shows that each image maps to an average of 2.8 sense clusters, with\neach cluster representing a distinct perspective of the image. We evaluate\nmultiple activity recognition models and compare our cluster-based evaluation\nwith standard evaluation methods. Additionally, our human alignment analysis\nsuggests that the cluster-based evaluation better aligns with human judgements,\noffering a more nuanced assessment of model performance."}
{"id": "2508.05056", "pdf": "https://arxiv.org/pdf/2508.05056.pdf", "abs": "https://arxiv.org/abs/2508.05056", "title": "Accessibility Beyond Accommodations: A Systematic Redesign of Introduction to Computer Science for Students with Visual Impairments", "authors": ["Vaanee Tripathi", "Aalok Thakkar"], "categories": ["cs.HC"], "comment": null, "summary": "Computer science education has evolved extensively; however, systemic\nbarriers still prevent students with visual impairments from fully\nparticipating. While existing research has developed specialized programming\ntools and assistive technologies, these solutions remain fragmented and often\nrequire complex technical infrastructure, which limits their classroom\nimplementation. Current approaches treat accessibility as individual\naccommodations rather than integral curriculum design, creating gaps in\nholistic educational support. This paper presents a comprehensive framework for\nredesigning introductory computer science curricula to provide equitable\nlearning experiences for students with visual impairments without requiring\nspecialized technical infrastructure. The framework outlines five key\ncomponents that together contribute a systematic approach to curriculum\naccessibility: accessible learning resources with pre-distributed materials and\ntactile diagrams, in-class learning kits with hands-on demonstrations,\nstructured support systems with dedicated teaching assistance, an online tool\nrepository, and psychosocial support for classroom participation. Unlike\nexisting tool-focused solutions, this framework addresses both technical and\npedagogical dimensions of inclusive education while emphasizing practical\nimplementation in standard university settings. The design is grounded in\nuniversal design principles and validated through expert consultation with\naccessibility specialists and disability services professionals, establishing\nfoundations for future empirical evaluation of learning outcomes and student\nengagement while serving as a template for broader institutional adoption."}
{"id": "2508.05003", "pdf": "https://arxiv.org/pdf/2508.05003.pdf", "abs": "https://arxiv.org/abs/2508.05003", "title": "A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health", "authors": ["Song Wang", "Yishu Wei", "Haotian Ma", "Max Lovitt", "Kelly Deng", "Yuan Meng", "Zihan Xu", "Jingze Zhang", "Yunyu Xiao", "Ying Ding", "Xuhai Xu", "Joydeep Ghosh", "Yifan Peng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Background: Understanding social determinants of health (SDoH) factors\ncontributing to suicide incidents is crucial for early intervention and\nprevention. However, data-driven approaches to this goal face challenges such\nas long-tailed factor distributions, analyzing pivotal stressors preceding\nsuicide incidents, and limited model explainability. Methods: We present a\nmulti-stage large language model framework to enhance SDoH factor extraction\nfrom unstructured text. Our approach was compared to other state-of-the-art\nlanguage models (i.e., pre-trained BioBERT and GPT-3.5-turbo) and reasoning\nmodels (i.e., DeepSeek-R1). We also evaluated how the model's explanations help\npeople annotate SDoH factors more quickly and accurately. The analysis included\nboth automated comparisons and a pilot user study. Results: We show that our\nproposed framework demonstrated performance boosts in the overarching task of\nextracting SDoH factors and in the finer-grained tasks of retrieving relevant\ncontext. Additionally, we show that fine-tuning a smaller, task-specific model\nachieves comparable or better performance with reduced inference costs. The\nmulti-stage design not only enhances extraction but also provides intermediate\nexplanations, improving model explainability. Conclusions: Our approach\nimproves both the accuracy and transparency of extracting suicide-related SDoH\nfrom unstructured texts. These advancements have the potential to support early\nidentification of individuals at risk and inform more effective prevention\nstrategies."}
{"id": "2508.05088", "pdf": "https://arxiv.org/pdf/2508.05088.pdf", "abs": "https://arxiv.org/abs/2508.05088", "title": "A Desktop-Centric Design Space for Direct Object Examination and Visualization in Mixed-Reality Environments", "authors": ["Sam Johnson-Lacoss", "Santiago V. Lombeyda", "S. George Djorgovski"], "categories": ["cs.HC"], "comment": null, "summary": "Mixed reality (MR) environments are bound to become ubiquitous as MR\ntechnology becomes lighter, higher resolution, more affordable, and overall\nbecomes a seamless extension of our current work and living spaces. For\nresearch scientists and clinicians focused on understanding 3D phenomena or\npatient pathologies within the context of the larger human anatomy, that means\na necessary evolution of their workstations currently only utilizing 2D\ninterfaces for everyday communication, logistics and data analysis. MR\ntechnologies bring forth immersive 3D representations coexisting in our natural\nspaces, while allowing for richer interconnected information displays, where 3D\nrepresentations greatly aid in the detailed understanding of physical\nstructures, spatial relationships, and 3D contextualization of 2D measurements,\nprojections, abstractions, and other data details. We present a breakdown of\nthe different interaction zones and modalities into a design space that best\naccommodates the creation of applications for users engaged through MR\ntechnologies in precise object-centric data analysis within the ergonomic\nconfines of their desktop physical spaces."}
{"id": "2508.05023", "pdf": "https://arxiv.org/pdf/2508.05023.pdf", "abs": "https://arxiv.org/abs/2508.05023", "title": "Dialogues Aspect-based Sentiment Quadruple Extraction via Structural Entropy Minimization Partitioning", "authors": ["Kun Peng", "Cong Cao", "Hao Peng", "Zhifeng Hao", "Lei Jiang", "Kongjing Gu", "Yanbing Liu", "Philip S. Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by CIKM2025", "summary": "Dialogues Aspect-based Sentiment Quadruple Extraction (DiaASQ) aims to\nextract all target-aspect-opinion-sentiment quadruples from a given\nmulti-round, multi-participant dialogue. Existing methods typically learn word\nrelations across entire dialogues, assuming a uniform distribution of sentiment\nelements. However, we find that dialogues often contain multiple semantically\nindependent sub-dialogues without clear dependencies between them. Therefore,\nlearning word relationships across the entire dialogue inevitably introduces\nadditional noise into the extraction process. To address this, our method\nfocuses on partitioning dialogues into semantically independent sub-dialogues.\nAchieving completeness while minimizing these sub-dialogues presents a\nsignificant challenge. Simply partitioning based on reply relationships is\nineffective. Instead, we propose utilizing a structural entropy minimization\nalgorithm to partition the dialogues. This approach aims to preserve relevant\nutterances while distinguishing irrelevant ones as much as possible.\nFurthermore, we introduce a two-step framework for quadruple extraction: first\nextracting individual sentiment elements at the utterance level, then matching\nquadruples at the sub-dialogue level. Extensive experiments demonstrate that\nour approach achieves state-of-the-art performance in DiaASQ with much lower\ncomputational costs."}
{"id": "2508.05098", "pdf": "https://arxiv.org/pdf/2508.05098.pdf", "abs": "https://arxiv.org/abs/2508.05098", "title": "SparseEMG: Computational Design of Sparse EMG Layouts for Sensing Gestures", "authors": ["Anand Kumar", "Antony Albert Raj Irudayaraj", "Ishita Chandra", "Adwait Sharma", "Aditya Shekhar Nittala"], "categories": ["cs.HC"], "comment": "UIST'25: Proceedings of the 38th Annual ACM Symposium on User\n  Interface Software and Technology", "summary": "Gesture recognition with electromyography (EMG) is a complex problem\ninfluenced by gesture sets, electrode count and placement, and machine learning\nparameters (e.g., features, classifiers). Most existing toolkits focus on\nstreamlining model development but overlook the impact of electrode selection\non classification accuracy. In this work, we present the first data-driven\nanalysis of how electrode selection and classifier choice affect both accuracy\nand sparsity. Through a systematic evaluation of 28 combinations (4 selection\nschemes, 7 classifiers), across six datasets, we identify an approach that\nminimizes electrode count without compromising accuracy. The results show that\nPermutation Importance (selection scheme) with Random Forest (classifier)\nreduces the number of electrodes by 53.5\\%. Based on these findings, we\nintroduce SparseEMG, a design tool that generates sparse electrode layouts\nbased on user-selected gesture sets, electrode constraints, and ML parameters\nwhile also predicting classification performance. SparseEMG supports 50+ unique\ngestures and is validated in three real-world applications using different\nhardware setups. Results from our multi-dataset evaluation show that the\nlayouts generated from the SparseEMG design tool are transferable across users\nwith only minimal variation in gesture recognition performance."}
{"id": "2508.05028", "pdf": "https://arxiv.org/pdf/2508.05028.pdf", "abs": "https://arxiv.org/abs/2508.05028", "title": "Evaluation of LLMs in AMR Parsing", "authors": ["Shu Han Ho"], "categories": ["cs.CL", "cs.AI"], "comment": "27 pages, 32 figures", "summary": "Meaning Representation (AMR) is a semantic formalism that encodes sentence\nmeaning as rooted, directed, acyclic graphs, where nodes represent concepts and\nedges denote semantic relations. Finetuning decoder only Large Language Models\n(LLMs) represent a promising novel straightfoward direction for AMR parsing.\nThis paper presents a comprehensive evaluation of finetuning four distinct LLM\narchitectures, Phi 3.5, Gemma 2, LLaMA 3.2, and DeepSeek R1 LLaMA Distilled\nusing the LDC2020T02 Gold AMR3.0 test set. Our results have shown that\nstraightfoward finetuning of decoder only LLMs can achieve comparable\nperformance to complex State of the Art (SOTA) AMR parsers. Notably, LLaMA 3.2\ndemonstrates competitive performance against SOTA AMR parsers given a\nstraightforward finetuning approach. We achieved SMATCH F1: 0.804 on the full\nLDC2020T02 test split, on par with APT + Silver (IBM) at 0.804 and approaching\nGraphene Smatch (MBSE) at 0.854. Across our analysis, we also observed a\nconsistent pattern where LLaMA 3.2 leads in semantic performance while Phi 3.5\nexcels in structural validity."}
{"id": "2508.05112", "pdf": "https://arxiv.org/pdf/2508.05112.pdf", "abs": "https://arxiv.org/abs/2508.05112", "title": "Metacognition and self-regulated learning in manipulative robotic problem-solving task", "authors": ["Margarida Romero", "George Kalmpourtzis"], "categories": ["cs.HC"], "comment": null, "summary": "Metacognition is an important aspect in creative problem solving (CPS) and\nthrough this chapter we analyse the meta-reasoning aspects applied in the\ndifferent processes of monitoring the progress of learners' reasoning and CPS\nactivities. Meta-reasoning monitors the way that problem-solving processes\nadvance and regulate time and efforts towards a solution. In the context of an\nill-defined problem, exploration is required to develop a better-defined\nproblem space and advance towards the solution space. The way learners engage\nin exploration and exploitations is regulated by the meta-reasoning within the\nCPS activity. The objective of this chapter is to examine and identify the CPS\nprocess with educational robots through a metacognitive and interactionist\napproach. This chapter presents a case study, where, to solve a problem, a\nparticipant had to explore a set of robot cubes to develop the technological\nknowledge associated with each single component of the system, but also\nconceptualize a system-level behaviour of the cubes when they are assembled.\nThe chapter presents the emergence of knowledge through the metacognitive\nregulation of the process of exploration and exploitation of prior knowledge\nand emergent knowledge until finding a solution"}
{"id": "2508.05078", "pdf": "https://arxiv.org/pdf/2508.05078.pdf", "abs": "https://arxiv.org/abs/2508.05078", "title": "Align, Don't Divide: Revisiting the LoRA Architecture in Multi-Task Learning", "authors": ["Jinda Liu", "Bo Cheng", "Yi Chang", "Yuan Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Parameter-Efficient Fine-Tuning (PEFT) is essential for adapting Large\nLanguage Models (LLMs). In practice, LLMs are often required to handle a\ndiverse set of tasks from multiple domains, a scenario naturally addressed by\nmulti-task learning (MTL). Within this MTL context, a prevailing trend involves\nLoRA variants with multiple adapters or heads, which advocate for structural\ndiversity to capture task-specific knowledge. Our findings present a direct\nchallenge to this paradigm. We first show that a simplified multi-head\narchitecture with high inter-head similarity substantially outperforms complex\nmulti-adapter and multi-head systems. This leads us to question the\nmulti-component paradigm itself, and we further demonstrate that a standard\nsingle-adapter LoRA, with a sufficiently increased rank, also achieves highly\ncompetitive performance. These results lead us to a new hypothesis: effective\nMTL generalization hinges on learning robust shared representations, not\nisolating task-specific features. To validate this, we propose Align-LoRA,\nwhich incorporates an explicit loss to align task representations within the\nshared adapter space. Experiments confirm that Align-LoRA significantly\nsurpasses all baselines, establishing a simpler yet more effective paradigm for\nadapting LLMs to multiple tasks. The code is available at\nhttps://github.com/jinda-liu/Align-LoRA."}
{"id": "2508.05156", "pdf": "https://arxiv.org/pdf/2508.05156.pdf", "abs": "https://arxiv.org/abs/2508.05156", "title": "AI Conversational Tutors in Foreign Language Learning: A Mixed-Methods Evaluation Study", "authors": ["Nikolaos Avouris"], "categories": ["cs.HC"], "comment": "To be cited as: Avouris N., (2025). AI Conversational Tutors in\n  Foreign Language Learning: A Mixed-Methods Evaluation Study, in Proceedings\n  14th Panhellenic Conference ICT in Education HCICTE 2025, Rhodes, October\n  2025", "summary": "This paper focuses on AI tutors in foreign language learning, a field of\napplication of AI tutors with great development, especially during the last\nyears, when great advances in natural language understanding and processing in\nreal time, have been achieved. These tutors attempt to address needs for\nimproving language skills (speaking, or communicative competence,\nunderstanding). In this paper, a mixed-methos empirical study on the use of\ndifferent kinds of state-of-the-art AI tutors for language learning is\nreported. This study involves a user experience evaluation of typical such\ntools, with special focus in their conversation functionality and an evaluation\nof their quality, based on chat transcripts. This study can help establish\ncriteria for assessing the quality of such systems and inform the design of\nfuture tools, including concerns about data privacy and secure handling of\nlearner information."}
{"id": "2508.05097", "pdf": "https://arxiv.org/pdf/2508.05097.pdf", "abs": "https://arxiv.org/abs/2508.05097", "title": "Multimodal Fact Checking with Unified Visual, Textual, and Contextual Representations", "authors": ["Aditya Kishore", "Gaurav Kumar", "Jasabanta Patro"], "categories": ["cs.CL"], "comment": null, "summary": "The growing rate of multimodal misinformation, where claims are supported by\nboth text and images, poses significant challenges to fact-checking systems\nthat rely primarily on textual evidence. In this work, we have proposed a\nunified framework for fine-grained multimodal fact verification called\n\"MultiCheck\", designed to reason over structured textual and visual signals.\nOur architecture combines dedicated encoders for text and images with a fusion\nmodule that captures cross-modal relationships using element-wise interactions.\nA classification head then predicts the veracity of a claim, supported by a\ncontrastive learning objective that encourages semantic alignment between\nclaim-evidence pairs in a shared latent space. We evaluate our approach on the\nFactify 2 dataset, achieving a weighted F1 score of 0.84, substantially\noutperforming the baseline. These results highlight the effectiveness of\nexplicit multimodal reasoning and demonstrate the potential of our approach for\nscalable and interpretable fact-checking in complex, real-world scenarios."}
{"id": "2508.05228", "pdf": "https://arxiv.org/pdf/2508.05228.pdf", "abs": "https://arxiv.org/abs/2508.05228", "title": "CWEFS: Brain volume conduction effects inspired channel-wise EEG feature selection for multi-dimensional emotion recognition", "authors": ["Xueyuan Xu", "Wenjia Dong", "Fulin Wei", "Li Zhuo"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Due to the intracranial volume conduction effects, high-dimensional\nmulti-channel electroencephalography (EEG) features often contain substantial\nredundant and irrelevant information. This issue not only hinders the\nextraction of discriminative emotional representations but also compromises the\nreal-time performance. Feature selection has been established as an effective\napproach to address the challenges while enhancing the transparency and\ninterpretability of emotion recognition models. However, existing EEG feature\nselection research overlooks the influence of latent EEG feature structures on\nemotional label correlations and assumes uniform importance across various\nchannels, directly limiting the precise construction of EEG feature selection\nmodels for multi-dimensional affective computing. To address these limitations,\na novel channel-wise EEG feature selection (CWEFS) method is proposed for\nmulti-dimensional emotion recognition. Specifically, inspired by brain volume\nconduction effects, CWEFS integrates EEG emotional feature selection into a\nshared latent structure model designed to construct a consensus latent space\nacross diverse EEG channels. To preserve the local geometric structure, this\nconsensus space is further integrated with the latent semantic analysis of\nmulti-dimensional emotional labels. Additionally, CWEFS incorporates adaptive\nchannel-weight learning to automatically determine the significance of\ndifferent EEG channels in the emotional feature selection task. The\neffectiveness of CWEFS was validated using three popular EEG datasets with\nmulti-dimensional emotional labels. Comprehensive experimental results,\ncompared against nineteen feature selection methods, demonstrate that the EEG\nfeature subsets chosen by CWEFS achieve optimal emotion recognition performance\nacross six evaluation metrics."}
{"id": "2508.05100", "pdf": "https://arxiv.org/pdf/2508.05100.pdf", "abs": "https://arxiv.org/abs/2508.05100", "title": "BEE-RAG: Balanced Entropy Engineering for Retrieval-Augmented Generation", "authors": ["Yuhao Wang", "Ruiyang Ren", "Yucheng Wang", "Jing Liu", "Wayne Xin Zhao", "Hua Wu", "Haifeng Wang"], "categories": ["cs.CL"], "comment": null, "summary": "With the rapid advancement of large language models (LLMs),\nretrieval-augmented generation (RAG) has emerged as a critical approach to\nsupplement the inherent knowledge limitations of LLMs. However, due to the\ntypically large volume of retrieved information, RAG tends to operate with long\ncontext lengths. From the perspective of entropy engineering, we identify\nunconstrained entropy growth and attention dilution due to long retrieval\ncontext as significant factors affecting RAG performance. In this paper, we\npropose the balanced entropy-engineered RAG (BEE-RAG) framework, which improves\nthe adaptability of RAG systems to varying context lengths through the\nprinciple of entropy invariance. By leveraging balanced context entropy to\nreformulate attention dynamics, BEE-RAG separates attention sensitivity from\ncontext length, ensuring a stable entropy level. Building upon this, we\nintroduce a zero-shot inference strategy for multi-importance estimation and a\nparameter-efficient adaptive fine-tuning mechanism to obtain the optimal\nbalancing factor for different settings. Extensive experiments across multiple\nRAG tasks demonstrate the effectiveness of BEE-RAG."}
{"id": "2508.05229", "pdf": "https://arxiv.org/pdf/2508.05229.pdf", "abs": "https://arxiv.org/abs/2508.05229", "title": "ADSEL: Adaptive dual self-expression learning for EEG feature selection via incomplete multi-dimensional emotional tagging", "authors": ["Tianze Yu", "Junming Zhang", "Wenjia Dong", "Xueyuan Xu", "Li Zhuo"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "EEG based multi-dimension emotion recognition has attracted substantial\nresearch interest in human computer interfaces. However, the high\ndimensionality of EEG features, coupled with limited sample sizes, frequently\nleads to classifier overfitting and high computational complexity. Feature\nselection constitutes a critical strategy for mitigating these challenges. Most\nexisting EEG feature selection methods assume complete multi-dimensional\nemotion labels. In practice, open acquisition environment, and the inherent\nsubjectivity of emotion perception often result in incomplete label data, which\ncan compromise model generalization. Additionally, existing feature selection\nmethods for handling incomplete multi-dimensional labels primarily focus on\ncorrelations among various dimensions during label recovery, neglecting the\ncorrelation between samples in the label space and their interaction with\nvarious dimensions. To address these issues, we propose a novel incomplete\nmulti-dimensional feature selection algorithm for EEG-based emotion\nrecognition. The proposed method integrates an adaptive dual self-expression\nlearning (ADSEL) with least squares regression. ADSEL establishes a\nbidirectional pathway between sample-level and dimension-level self-expression\nlearning processes within the label space. It could facilitate the\ncross-sharing of learned information between these processes, enabling the\nsimultaneous exploitation of effective information across both samples and\ndimensions for label reconstruction. Consequently, ADSEL could enhances label\nrecovery accuracy and effectively identifies the optimal EEG feature subset for\nmulti-dimensional emotion recognition."}
{"id": "2508.05128", "pdf": "https://arxiv.org/pdf/2508.05128.pdf", "abs": "https://arxiv.org/abs/2508.05128", "title": "Attention Basin: Why Contextual Position Matters in Large Language Models", "authors": ["Zihao Yi", "Delong Zeng", "Zhenqing Ling", "Haohao Luo", "Zhe Xu", "Wei Liu", "Jian Luan", "Wanxia Cao", "Ying Shen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The performance of Large Language Models (LLMs) is significantly sensitive to\nthe contextual position of information in the input. To investigate the\nmechanism behind this positional bias, our extensive experiments reveal a\nconsistent phenomenon we term the attention basin: when presented with a\nsequence of structured items (e.g., retrieved documents or few-shot examples),\nmodels systematically assign higher attention to the items at the beginning and\nend of the sequence, while neglecting those in the middle. Crucially, our\nanalysis further reveals that allocating higher attention to critical\ninformation is key to enhancing model performance. Based on these insights, we\nintroduce Attention-Driven Reranking (AttnRank), a two-stage framework that (i)\nestimates a model's intrinsic positional attention preferences using a small\ncalibration set, and (ii) reorders retrieved documents or few-shot examples to\nalign the most salient content with these high-attention positions. AttnRank is\na model-agnostic, training-free, and plug-and-play method with minimal\ncomputational overhead. Experiments on multi-hop QA and few-shot in-context\nlearning tasks demonstrate that AttnRank achieves substantial improvements\nacross 10 large language models of varying architectures and scales, without\nmodifying model parameters or training procedures."}
{"id": "2508.05231", "pdf": "https://arxiv.org/pdf/2508.05231.pdf", "abs": "https://arxiv.org/abs/2508.05231", "title": "FDC-Net: Rethinking the association between EEG artifact removal and multi-dimensional affective computing", "authors": ["Wenjia Dong", "Xueyuan Xu", "Tianze Yu", "Junming Zhang", "Li Zhuo"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Electroencephalogram (EEG)-based emotion recognition holds significant value\nin affective computing and brain-computer interfaces. However, in practical\napplications, EEG recordings are susceptible to the effects of various\nphysiological artifacts. Current approaches typically treat denoising and\nemotion recognition as independent tasks using cascaded architectures, which\nnot only leads to error accumulation, but also fails to exploit potential\nsynergies between these tasks. Moreover, conventional EEG-based emotion\nrecognition models often rely on the idealized assumption of \"perfectly\ndenoised data\", lacking a systematic design for noise robustness. To address\nthese challenges, a novel framework that deeply couples denoising and emotion\nrecognition tasks is proposed for end-to-end noise-robust emotion recognition,\ntermed as Feedback-Driven Collaborative Network for Denoising-Classification\nNexus (FDC-Net). Our primary innovation lies in establishing a dynamic\ncollaborative mechanism between artifact removal and emotion recognition\nthrough: (1) bidirectional gradient propagation with joint optimization\nstrategies; (2) a gated attention mechanism integrated with frequency-adaptive\nTransformer using learnable band-position encoding. Two most popular EEG-based\nemotion datasets (DEAP and DREAMER) with multi-dimensional emotional labels\nwere employed to compare the artifact removal and emotion recognition\nperformance between ASLSL and nine state-of-the-art methods. In terms of the\ndenoising task, FDC-Net obtains a maximum correlation coefficient (CC) value of\n96.30% on DEAP and a maximum CC value of 90.31% on DREAMER. In terms of the\nemotion recognition task under physiological artifact interference, FDC-Net\nachieves emotion recognition accuracies of 82.3+7.1% on DEAP and 88.1+0.8% on\nDREAMER."}
{"id": "2508.05132", "pdf": "https://arxiv.org/pdf/2508.05132.pdf", "abs": "https://arxiv.org/abs/2508.05132", "title": "Towards Assessing Medical Ethics from Knowledge to Practice", "authors": ["Chang Hong", "Minghao Wu", "Qingying Xiao", "Yuchi Wang", "Xiang Wan", "Guangjun Yu", "Benyou Wang", "Yan Hu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The integration of large language models into healthcare necessitates a\nrigorous evaluation of their ethical reasoning, an area current benchmarks\noften overlook. We introduce PrinciplismQA, a comprehensive benchmark with\n3,648 questions designed to systematically assess LLMs' alignment with core\nmedical ethics. Grounded in Principlism, our benchmark features a high-quality\ndataset. This includes multiple-choice questions curated from authoritative\ntextbooks and open-ended questions sourced from authoritative medical ethics\ncase study literature, all validated by medical experts. Our experiments reveal\na significant gap between models' ethical knowledge and their practical\napplication, especially in dynamically applying ethical principles to\nreal-world scenarios. Most LLMs struggle with dilemmas concerning Beneficence,\noften over-emphasizing other principles. Frontier closed-source models, driven\nby strong general capabilities, currently lead the benchmark. Notably, medical\ndomain fine-tuning can enhance models' overall ethical competence, but further\nprogress requires better alignment with medical ethical knowledge.\nPrinciplismQA offers a scalable framework to diagnose these specific ethical\nweaknesses, paving the way for more balanced and responsible medical AI."}
{"id": "2508.05238", "pdf": "https://arxiv.org/pdf/2508.05238.pdf", "abs": "https://arxiv.org/abs/2508.05238", "title": "Driver Assistant: Persuading Drivers to Adjust Secondary Tasks Using Large Language Models", "authors": ["Wei Xiang", "Muchen Li", "Jie Yan", "Manling Zheng", "Hanfei Zhu", "Mengyun Jiang", "Lingyun Sun"], "categories": ["cs.HC", "cs.AI"], "comment": "6 pages, 4 figures, 2025 IEEE International Conference on Systems,\n  Man, and Cybernetics (SMC)", "summary": "Level 3 automated driving systems allows drivers to engage in secondary tasks\nwhile diminishing their perception of risk. In the event of an emergency\nnecessitating driver intervention, the system will alert the driver with a\nlimited window for reaction and imposing a substantial cognitive burden. To\naddress this challenge, this study employs a Large Language Model (LLM) to\nassist drivers in maintaining an appropriate attention on road conditions\nthrough a \"humanized\" persuasive advice. Our tool leverages the road conditions\nencountered by Level 3 systems as triggers, proactively steering driver\nbehavior via both visual and auditory routes. Empirical study indicates that\nour tool is effective in sustaining driver attention with reduced cognitive\nload and coordinating secondary tasks with takeover behavior. Our work provides\ninsights into the potential of using LLMs to support drivers during multi-task\nautomated driving."}
{"id": "2508.05179", "pdf": "https://arxiv.org/pdf/2508.05179.pdf", "abs": "https://arxiv.org/abs/2508.05179", "title": "ATLANTIS at SemEval-2025 Task 3: Detecting Hallucinated Text Spans in Question Answering", "authors": ["Catherine Kobus", "Fran√ßois Lancelot", "Marion-C√©cile Martin", "Nawal Ould Amer"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents the contributions of the ATLANTIS team to SemEval-2025\nTask 3, focusing on detecting hallucinated text spans in question answering\nsystems. Large Language Models (LLMs) have significantly advanced Natural\nLanguage Generation (NLG) but remain susceptible to hallucinations, generating\nincorrect or misleading content. To address this, we explored methods both with\nand without external context, utilizing few-shot prompting with a LLM,\ntoken-level classification or LLM fine-tuned on synthetic data. Notably, our\napproaches achieved top rankings in Spanish and competitive placements in\nEnglish and German. This work highlights the importance of integrating relevant\ncontext to mitigate hallucinations and demonstrate the potential of fine-tuned\nmodels and prompt engineering."}
{"id": "2508.05281", "pdf": "https://arxiv.org/pdf/2508.05281.pdf", "abs": "https://arxiv.org/abs/2508.05281", "title": "A Methodological Framework and Questionnaire for Investigating Perceived Algorithmic Fairness", "authors": ["Ahmed Abdal Shafi Rasel", "Ahmed Mustafa Amlan", "Tasmim Shajahan Mim", "Tanvir Hasan"], "categories": ["cs.HC"], "comment": "34 pages, Submitted for review", "summary": "This study explores perceptions of fairness in algorithmic decision-making\namong users in Bangladesh through a comprehensive mixed-methods approach. By\nintegrating quantitative survey data with qualitative interview insights, we\nexamine how cultural, social, and contextual factors influence users'\nunderstanding of fairness, transparency, and accountability in AI systems. Our\nfindings reveal nuanced attitudes toward human oversight, explanation\nmechanisms, and contestability, highlighting the importance of culturally aware\ndesign principles for equitable and trustworthy algorithmic systems. These\ninsights contribute to ongoing discussions on algorithmic fairness by\nforegrounding perspectives from a non-Western context, thus broadening the\nglobal dialogue on ethical AI deployment."}
{"id": "2508.05234", "pdf": "https://arxiv.org/pdf/2508.05234.pdf", "abs": "https://arxiv.org/abs/2508.05234", "title": "Resource-Limited Joint Multimodal Sentiment Reasoning and Classification via Chain-of-Thought Enhancement and Distillation", "authors": ["Haonan Shangguan", "Xiaocui Yang", "Shi Feng", "Daling Wang", "Yifei Zhang", "Ge Yu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The surge in rich multimodal content on social media platforms has greatly\nadvanced Multimodal Sentiment Analysis (MSA), with Large Language Models (LLMs)\nfurther accelerating progress in this field. Current approaches primarily\nleverage the knowledge and reasoning capabilities of parameter-heavy\n(Multimodal) LLMs for sentiment classification, overlooking autonomous\nmultimodal sentiment reasoning generation in resource-constrained environments.\nTherefore, we focus on the Resource-Limited Joint Multimodal Sentiment\nReasoning and Classification task, JMSRC, which simultaneously performs\nmultimodal sentiment reasoning chain generation and sentiment classification\nonly with a lightweight model. We propose a Multimodal Chain-of-Thought\nReasoning Distillation model, MulCoT-RD, designed for JMSRC that employs a\n\"Teacher-Assistant-Student\" distillation paradigm to address deployment\nconstraints in resource-limited environments. We first leverage a\nhigh-performance Multimodal Large Language Model (MLLM) to generate the initial\nreasoning dataset and train a medium-sized assistant model with a multi-task\nlearning mechanism. A lightweight student model is jointly trained to perform\nefficient multimodal sentiment reasoning generation and classification.\nExtensive experiments on four datasets demonstrate that MulCoT-RD with only 3B\nparameters achieves strong performance on JMSRC, while exhibiting robust\ngeneralization and enhanced interpretability."}
{"id": "2508.05325", "pdf": "https://arxiv.org/pdf/2508.05325.pdf", "abs": "https://arxiv.org/abs/2508.05325", "title": "Critical Design Strategy: a Method for Heuristically Evaluating Visualisation Designs", "authors": ["Jonathan C. Roberts", "Hanan Alnjar", "Aron E. Owen", "Panagiotis D. Ritsos"], "categories": ["cs.HC", "H.5.2; K.3.0; D.0; I.3.8"], "comment": "11 pages, 6 pages supplemental material, 2 CDS versions", "summary": "We present the Critical Design Strategy (CDS) - a structured method designed\nto facilitate the examination of visualisation designs through reflection and\ncritical thought. The CDS helps designers think critically and make informed\nimprovements using heuristic evaluation. When developing a visual tool or\npioneering a novel visualisation approach, identifying areas for enhancement\ncan be challenging. Critical thinking is particularly crucial for visualisation\ndesigners and tool developers, especially those new to the field, such as\nstudying visualisation in higher education. The CDS consists of three stages\nacross six perspectives: Stage 1 captures the essence of the idea by assigning\nan indicative title and selecting five adjectives (from twenty options) to form\ninitial impressions of the design. Stage 2 involves an in-depth critique using\n30 heuristic questions spanning six key perspectives - user, environment,\ninterface, components, design, and visual marks. Stage 3 focuses on\nsynthesising insights, reflecting on design decisions, and determining the next\nsteps forward. We introduce the CDS and explore its use across three\nvisualisation modules in both undergraduate and postgraduate courses. Our\nlongstanding experience with the CDS has allowed us to refine and develop it\nover time: from its initial creation through workshops in 2017/18 to\nimprovements in wording and the development of two applications by 2020,\nfollowed by the expansion of support notes and refinement of heuristics through\n2023; while using it in our teaching each year. This sustained use allows us to\nreflect on its practical application and offer guidance on how others can\nincorporate it into their own work."}
{"id": "2508.05239", "pdf": "https://arxiv.org/pdf/2508.05239.pdf", "abs": "https://arxiv.org/abs/2508.05239", "title": "Pruning Large Language Models by Identifying and Preserving Functional Networks", "authors": ["Yiheng Liu", "Junhao Ning", "Sichen Xia", "Xiaohui Gao", "Ning Qiang", "Bao Ge", "Junwei Han", "Xintao Hu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, 5 figures", "summary": "Structured pruning is one of the representative techniques for compressing\nlarge language models (LLMs) to reduce GPU memory consumption and accelerate\ninference speed. It offers significant practical value in improving the\nefficiency of LLMs in real-world applications. Current structured pruning\nmethods typically rely on assessment of the importance of the structure units\nand pruning the units with less importance. Most of them overlooks the\ninteraction and collaboration among artificial neurons that are crucial for the\nfunctionalities of LLMs, leading to a disruption in the macro functional\narchitecture of LLMs and consequently a pruning performance degradation.\nInspired by the inherent similarities between artificial neural networks and\nfunctional neural networks in the human brain, we alleviate this challenge and\npropose to prune LLMs by identifying and preserving functional networks within\nLLMs in this study. To achieve this, we treat an LLM as a digital brain and\ndecompose the LLM into functional networks, analogous to identifying functional\nbrain networks in neuroimaging data. Afterwards, an LLM is pruned by preserving\nthe key neurons within these functional networks. Experimental results\ndemonstrate that the proposed method can successfully identify and locate\nfunctional networks and key neurons in LLMs, enabling efficient model pruning.\nOur code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION."}
{"id": "2508.05332", "pdf": "https://arxiv.org/pdf/2508.05332.pdf", "abs": "https://arxiv.org/abs/2508.05332", "title": "Implementation and Application of Multi-Format 3D Data Integration in a Cross-Device Commercial Metaverse Platform", "authors": ["Masanori Ibara", "Yuichi Hiroi", "Takushi Kamegai", "Takefumi Hiraki"], "categories": ["cs.HC"], "comment": "10 pages, to appear in IEEE International Symposium on Emerging\n  Metaverse (ISEMV)", "summary": "Traditionally, specialized 3D design data, such as BIM and CAD, have been\naccessible only to a select group of experts, creating significant barriers\nthat prevent general users from participating in decision-making processes.\nThis paper provides a systematic overview of practical insights for utilizing\n3D data in industrial and architectural domains by presenting implementation\ncases of the industrial metaverse on Cluster, a commercial cross-device\nmetaverse platform. This paper analyzes the characteristics and constraints of\nmajor data formats in the industrial and architectural fields and organizes\nintegration workflows for the metaverse. Through application cases utilizing 3D\ndata across multiple domains, we present practical examples of collaborative\ndecision-making support enabled by the fusion of metaverse and digital twin\ntechnologies. Specifically, we demonstrate that multi-device access and\nsimultaneous multi-user participation capabilities foster democratic\nenvironments in the industrial metaverse, which are challenging to achieve with\nconventional, expert-dependent systems."}
{"id": "2508.05242", "pdf": "https://arxiv.org/pdf/2508.05242.pdf", "abs": "https://arxiv.org/abs/2508.05242", "title": "CodeBoost: Boosting Code LLMs by Squeezing Knowledge from Code Snippets with RL", "authors": ["Sijie Wang", "Quanjiang Guo", "Kai Zhao", "Yawei Zhang", "Xin Li", "Xiang Li", "Siqi Li", "Rui She", "Shangshu Yu", "Wee Peng Tay"], "categories": ["cs.CL"], "comment": "Technical report. Project page: https://github.com/sijieaaa/CodeBoost", "summary": "Code large language models (LLMs) have become indispensable tools for\nbuilding efficient and automated coding pipelines. Existing models are\ntypically post-trained using reinforcement learning (RL) from general-purpose\nLLMs using \"human instruction-final answer\" pairs, where the instructions are\nusually from manual annotations. However, collecting high-quality coding\ninstructions is both labor-intensive and difficult to scale. On the other hand,\ncode snippets are abundantly available from various sources. This imbalance\npresents a major bottleneck in instruction-based post-training. We propose\nCodeBoost, a post-training framework that enhances code LLMs purely from code\nsnippets, without relying on human-annotated instructions. CodeBoost introduces\nthe following key components: (1) maximum-clique curation, which selects a\nrepresentative and diverse training corpus from code; (2) bi-directional\nprediction, which enables the model to learn from both forward and backward\nprediction objectives; (3) error-aware prediction, which incorporates learning\nsignals from both correct and incorrect outputs; (4) heterogeneous\naugmentation, which diversifies the training distribution to enrich code\nsemantics; and (5) heterogeneous rewarding, which guides model learning through\nmultiple reward types including format correctness and execution feedback from\nboth successes and failures. Extensive experiments across several code LLMs and\nbenchmarks verify that CodeBoost consistently improves performance,\ndemonstrating its effectiveness as a scalable and effective training pipeline."}
{"id": "2508.05497", "pdf": "https://arxiv.org/pdf/2508.05497.pdf", "abs": "https://arxiv.org/abs/2508.05497", "title": "Towards Human-Centric Evaluation of Interaction-Aware Automated Vehicle Controllers: A Framework and Case Study", "authors": ["Federico Scar√¨", "Olger Siebinga", "Arkady Zgonnikov"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "As automated vehicles (AVs) increasingly integrate into mixed-traffic\nenvironments, evaluating their interaction with human-driven vehicles (HDVs)\nbecomes critical. In most research focused on developing new AV control\nalgorithms (controllers), the performance of these algorithms is assessed\nsolely based on performance metrics such as collision avoidance or lane-keeping\nefficiency, while largely overlooking the human-centred dimensions of\ninteraction with HDVs. This paper proposes a structured evaluation framework\nthat addresses this gap by incorporating metrics grounded in the human-robot\ninteraction literature. The framework spans four key domains: a) interaction\neffect, b) interaction perception, c) interaction effort, and d) interaction\nability. These domains capture both the performance of the AV and its impact on\nhuman drivers around it. To demonstrate the utility of the framework, we apply\nit to a case study evaluating how a state-of-the-art AV controller interacts\nwith human drivers in a merging scenario in a driving simulator. Measuring\nHDV-HDV interactions as a baseline, this study included one representative\nmetric per domain: a) perceived safety, b) subjective ratings, specifically how\nparticipants perceived the other vehicle's driving behaviour (e.g.,\naggressiveness or predictability) , c) driver workload, and d) merging success.\nThe results showed that incorporating metrics covering all four domains in the\nevaluation of AV controllers can illuminate critical differences in driver\nexperience when interacting with AVs. This highlights the need for a more\ncomprehensive evaluation approach. Our framework offers researchers,\ndevelopers, and policymakers a systematic method for assessing AV behaviour\nbeyond technical performance, fostering the development of AVs that are not\nonly functionally capable but also understandable, acceptable, and safe from a\nhuman perspective."}
{"id": "2508.05282", "pdf": "https://arxiv.org/pdf/2508.05282.pdf", "abs": "https://arxiv.org/abs/2508.05282", "title": "ASCoT: An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs", "authors": ["Dongxu Zhang", "Ning Yang", "Jihua Zhu", "Jinnan Yang", "Miao Xin", "Baoliang Tian"], "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-Thought (CoT) prompting has significantly advanced the reasoning\ncapabilities of Large Language Models (LLMs), yet the reliability of these\nreasoning chains remains a critical challenge. A widely held \"cascading\nfailure\" hypothesis suggests that errors are most detrimental when they occur\nearly in the reasoning process. This paper challenges that assumption through\nsystematic error-injection experiments, revealing a counter-intuitive\nphenomenon we term \"Late-Stage Fragility\": errors introduced in the later\nstages of a CoT chain are significantly more likely to corrupt the final answer\nthan identical errors made at the beginning. To address this specific\nvulnerability, we introduce the Adaptive Self-Correction Chain-of-Thought\n(ASCoT) method. ASCoT employs a modular pipeline in which an Adaptive\nVerification Manager (AVM) operates first, followed by the Multi-Perspective\nSelf-Correction Engine (MSCE). The AVM leverages a Positional Impact Score\nfunction I(k) that assigns different weights based on the position within the\nreasoning chains, addressing the Late-Stage Fragility issue by identifying and\nprioritizing high-risk, late-stage steps. Once these critical steps are\nidentified, the MSCE applies robust, dual-path correction specifically to the\nfailure parts. Extensive experiments on benchmarks such as GSM8K and MATH\ndemonstrate that ASCoT achieves outstanding accuracy, outperforming strong\nbaselines, including standard CoT. Our work underscores the importance of\ndiagnosing specific failure modes in LLM reasoning and advocates for a shift\nfrom uniform verification strategies to adaptive, vulnerability-aware\ncorrection mechanisms."}
{"id": "2508.05572", "pdf": "https://arxiv.org/pdf/2508.05572.pdf", "abs": "https://arxiv.org/abs/2508.05572", "title": "Discrepancy-Aware Contrastive Adaptation in Medical Time Series Analysis", "authors": ["Yifan Wang", "Hongfeng Ai", "Ruiqi Li", "Maowei Jiang", "Ruiyuan Kang", "Jiahua Dong", "Cheng Jiang", "Chenzhong Li"], "categories": ["cs.HC"], "comment": "10 pages", "summary": "In medical time series disease diagnosis, two key challenges are identified.\nFirst, the high annotation cost of medical data leads to overfitting in models\ntrained on label-limited, single-center datasets. To address this, we propose\nincorporating external data from related tasks and leveraging AE-GAN to extract\nprior knowledge, providing valuable references for downstream tasks. Second,\nmany existing studies employ contrastive learning to derive more generalized\nmedical sequence representations for diagnostic tasks, usually relying on\nmanually designed diverse positive and negative sample pairs. However, these\napproaches are complex, lack generalizability, and fail to adaptively capture\ndisease-specific features across different conditions. To overcome this, we\nintroduce LMCF (Learnable Multi-views Contrastive Framework), a framework that\nintegrates a multi-head attention mechanism and adaptively learns\nrepresentations from different views through inter-view and intra-view\ncontrastive learning strategies. Additionally, the pre-trained AE-GAN is used\nto reconstruct discrepancies in the target data as disease probabilities, which\nare then integrated into the contrastive learning process. Experiments on three\ntarget datasets demonstrate that our method consistently outperforms other\nseven baselines, highlighting its significant impact on healthcare applications\nsuch as the diagnosis of myocardial infarction, Alzheimer's disease, and\nParkinson's disease. We release the source code at xxxxx."}
{"id": "2508.05283", "pdf": "https://arxiv.org/pdf/2508.05283.pdf", "abs": "https://arxiv.org/abs/2508.05283", "title": "Decision-Making with Deliberation: Meta-reviewing as a Document-grounded Dialogue", "authors": ["Sukannya Purkayastha", "Nils Dycke", "Anne Lauscher", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": "36 pages, 16 tables, 13 figures", "summary": "Meta-reviewing is a pivotal stage in the peer-review process, serving as the\nfinal step in determining whether a paper is recommended for acceptance. Prior\nresearch on meta-reviewing has treated this as a summarization problem over\nreview reports. However, complementary to this perspective, meta-reviewing is a\ndecision-making process that requires weighing reviewer arguments and placing\nthem within a broader context. Prior research has demonstrated that\ndecision-makers can be effectively assisted in such scenarios via dialogue\nagents. In line with this framing, we explore the practical challenges for\nrealizing dialog agents that can effectively assist meta-reviewers. Concretely,\nwe first address the issue of data scarcity for training dialogue agents by\ngenerating synthetic data using Large Language Models (LLMs) based on a\nself-refinement strategy to improve the relevance of these dialogues to expert\ndomains. Our experiments demonstrate that this method produces higher-quality\nsynthetic data and can serve as a valuable resource towards training\nmeta-reviewing assistants. Subsequently, we utilize this data to train dialogue\nagents tailored for meta-reviewing and find that these agents outperform\n\\emph{off-the-shelf} LLM-based assistants for this task. Finally, we apply our\nagents in real-world meta-reviewing scenarios and confirm their effectiveness\nin enhancing the efficiency of meta-reviewing.\\footnote{Code and Data:\nhttps://github.com/UKPLab/arxiv2025-meta-review-as-dialog"}
{"id": "2508.04889", "pdf": "https://arxiv.org/pdf/2508.04889.pdf", "abs": "https://arxiv.org/abs/2508.04889", "title": "Graffiti: Enabling an Ecosystem of Personalized and Interoperable Social Applications", "authors": ["Theia Henderson", "David R. Karger", "David D. Clark"], "categories": ["cs.SI", "cs.HC", "cs.SE"], "comment": "Accepted to The 38th Annual ACM Symposium on User Interface Software\n  and Technology (UIST '25), September 28-October 1, 2025, Busan, Republic of\n  Korea. 21 pages", "summary": "Most social applications, from Twitter to Wikipedia, have rigid\none-size-fits-all designs, but building new social applications is both\ntechnically challenging and results in applications that are siloed away from\nexisting communities. We present Graffiti, a system that can be used to build a\nwide variety of personalized social applications with relative ease that also\ninteroperate with each other. People can freely move between a plurality of\ndesigns -- each with its own aesthetic, feature set, and moderation -- all\nwithout losing their friends or data.\n  Our concept of total reification makes it possible for seemingly\ncontradictory designs, including conflicting moderation rules, to interoperate.\nConversely, our concept of channels prevents interoperation from occurring by\naccident, avoiding context collapse.\n  Graffiti applications interact through a minimal client-side API, which we\nshow admits at least two decentralized implementations. Above the API, we built\na Vue.js plugin, which we use to develop applications similar to Twitter,\nMessenger, and Wikipedia using only client-side code. Our case studies explore\nhow these and other novel applications interoperate, as well as the broader\necosystem that Graffiti enables."}
{"id": "2508.05305", "pdf": "https://arxiv.org/pdf/2508.05305.pdf", "abs": "https://arxiv.org/abs/2508.05305", "title": "SONAR-LLM: Autoregressive Transformer that Thinks in Sentence Embeddings and Speaks in Tokens", "authors": ["Nikita Dragunov", "Temurbek Rahmatullaev", "Elizaveta Goncharova", "Andrey Kuznetsov", "Anton Razzhigaev"], "categories": ["cs.CL"], "comment": null, "summary": "The recently proposed Large Concept Model (LCM) generates text by predicting\na sequence of sentence-level embeddings and training with either mean-squared\nerror or diffusion objectives. We present SONAR-LLM, a decoder-only transformer\nthat \"thinks\" in the same continuous SONAR embedding space, yet is supervised\nthrough token-level cross-entropy propagated via the frozen SONAR decoder. This\nhybrid objective retains the semantic abstraction of LCM while eliminating its\ndiffusion sampler and restoring a likelihood-based training signal. Across\nmodel sizes from 39M to 1.3B parameters, SONAR-LLM attains competitive\ngeneration quality. We report scaling trends, ablations, benchmark results, and\nrelease the complete training code and all pretrained checkpoints to foster\nreproducibility and future research."}
{"id": "2508.05025", "pdf": "https://arxiv.org/pdf/2508.05025.pdf", "abs": "https://arxiv.org/abs/2508.05025", "title": "Will You Be Aware? Eye Tracking-Based Modeling of Situational Awareness in Augmented Reality", "authors": ["Zhehan Qu", "Tianyi Hu", "Christian Fronk", "Maria Gorlatova"], "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "Augmented Reality (AR) systems, while enhancing task performance through\nreal-time guidance, pose risks of inducing cognitive tunneling-a hyperfocus on\nvirtual content that compromises situational awareness (SA) in safety-critical\nscenarios. This paper investigates SA in AR-guided cardiopulmonary\nresuscitation (CPR), where responders must balance effective compressions with\nvigilance to unpredictable hazards (e.g., patient vomiting). We developed an AR\napp on a Magic Leap 2 that overlays real-time CPR feedback (compression depth\nand rate) and conducted a user study with simulated unexpected incidents (e.g.,\nbleeding) to evaluate SA, in which SA metrics were collected via observation\nand questionnaires administered during freeze-probe events. Eye tracking\nanalysis revealed that higher SA levels were associated with greater saccadic\namplitude and velocity, and with reduced proportion and frequency of fixations\non virtual content. To predict SA, we propose FixGraphPool, a graph neural\nnetwork that structures gaze events (fixations, saccades) into spatiotemporal\ngraphs, effectively capturing dynamic attentional patterns. Our model achieved\n83.0% accuracy (F1=81.0%), outperforming feature-based machine learning and\nstate-of-the-art time-series models by leveraging domain knowledge and\nspatial-temporal information encoded in ET data. These findings demonstrate the\npotential of eye tracking for SA modeling in AR and highlight its utility in\ndesigning AR systems that ensure user safety and situational awareness."}
{"id": "2508.05337", "pdf": "https://arxiv.org/pdf/2508.05337.pdf", "abs": "https://arxiv.org/abs/2508.05337", "title": "Efficient Reasoning for Large Reasoning Language Models via Certainty-Guided Reflection Suppression", "authors": ["Jiameng Huang", "Baijiong Lin", "Guhao Feng", "Jierun Chen", "Di He", "Lu Hou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Technical Report", "summary": "Recent Large Reasoning Language Models (LRLMs) employ long chain-of-thought\nreasoning with complex reflection behaviors, typically signaled by specific\ntrigger words (e.g., \"Wait\" and \"Alternatively\") to enhance performance.\nHowever, these reflection behaviors can lead to the overthinking problem where\nthe generation of redundant reasoning steps that unnecessarily increase token\nusage, raise inference costs, and reduce practical utility. In this paper, we\npropose Certainty-Guided Reflection Suppression (CGRS), a novel method that\nmitigates overthinking in LRLMs while maintaining reasoning accuracy. CGRS\noperates by dynamically suppressing the model's generation of reflection\ntriggers when it exhibits high confidence in its current response, thereby\npreventing redundant reflection cycles without compromising output quality. Our\napproach is model-agnostic, requires no retraining or architectural\nmodifications, and can be integrated seamlessly with existing autoregressive\ngeneration pipelines. Extensive experiments across four reasoning benchmarks\n(i.e., AIME24, AMC23, MATH500, and GPQA-D) demonstrate CGRS's effectiveness: it\nreduces token usage by an average of 18.5% to 41.9% while preserving accuracy.\nIt also achieves the optimal balance between length reduction and performance\ncompared to state-of-the-art baselines. These results hold consistently across\nmodel architectures (e.g., DeepSeek-R1-Distill series, QwQ-32B, and Qwen3\nfamily) and scales (4B to 32B parameters), highlighting CGRS's practical value\nfor efficient reasoning."}
{"id": "2508.05286", "pdf": "https://arxiv.org/pdf/2508.05286.pdf", "abs": "https://arxiv.org/abs/2508.05286", "title": "Everything You Need to Know About CS Education: Open Results from a Survey of More Than 18,000 Participants", "authors": ["Katsiaryna Dzialets", "Aleksandra Makeeva", "Ilya Vlasov", "Anna Potriasaeva", "Aleksei Rostovskii", "Yaroslav Golubev", "Anastasiia Birillo"], "categories": ["cs.CY", "cs.HC", "cs.SE"], "comment": "Accepted to CompEd'25, 7 pages, 1 figure", "summary": "Computer science education is a dynamic field with many aspects that\ninfluence the learner's path. While these aspects are usually studied in depth\nseparately, it is also important to carry out broader large-scale studies that\ntouch on many topics, because they allow us to put different results into each\nother's perspective. Past large-scale surveys have provided valuable insights,\nhowever, the emergence of new trends (e.g., AI), new learning formats (e.g.,\nin-IDE learning), and the increasing learner diversity highlight the need for\nan updated comprehensive study. To address this, we conducted a survey with\n18,032 learners from 173 countries, ensuring diverse representation and\nexploring a wide range of topics - formal education, learning formats, AI\nusage, challenges, motivation, and more. This paper introduces the results of\nthis survey as an open dataset, describes our methodology and the survey\nquestions, and highlights, as a motivating example, three possible research\ndirections within this data: challenges in learning, emerging formats, and\ninsights into the in-IDE format. The dataset aims to support further research\nand foster advancements in computer education."}
{"id": "2508.05358", "pdf": "https://arxiv.org/pdf/2508.05358.pdf", "abs": "https://arxiv.org/abs/2508.05358", "title": "Evaluation of a Sign Language Avatar on Comprehensibility, User Experience \\& Acceptability", "authors": ["Fenya Wasserroth", "Eleftherios Avramidis", "Vera Czehmann", "Tanja Kojic", "Fabrizio Nunnari", "Sebastian M√∂ller"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "This paper presents an investigation into the impact of adding adjustment\nfeatures to an existing sign language (SL) avatar on a Microsoft Hololens 2\ndevice. Through a detailed analysis of interactions of expert German Sign\nLanguage (DGS) users with both adjustable and non-adjustable avatars in a\nspecific use case, this study identifies the key factors influencing the\ncomprehensibility, the user experience (UX), and the acceptability of such a\nsystem. Despite user preference for adjustable settings, no significant\nimprovements in UX or comprehensibility were observed, which remained at low\nlevels, amid missing SL elements (mouthings and facial expressions) and\nimplementation issues (indistinct hand shapes, lack of feedback and menu\npositioning). Hedonic quality was rated higher than pragmatic quality,\nindicating that users found the system more emotionally or aesthetically\npleasing than functionally useful. Stress levels were higher for the adjustable\navatar, reflecting lower performance, greater effort and more frustration.\nAdditionally, concerns were raised about whether the Hololens adjustment\ngestures are intuitive and easy to familiarise oneself with. While\nacceptability of the concept of adjustability was generally positive, it was\nstrongly dependent on usability and animation quality. This study highlights\nthat personalisation alone is insufficient, and that SL avatars must be\ncomprehensible by default. Key recommendations include enhancing mouthing and\nfacial animation, improving interaction interfaces, and applying participatory\ndesign."}
{"id": "2508.05310", "pdf": "https://arxiv.org/pdf/2508.05310.pdf", "abs": "https://arxiv.org/abs/2508.05310", "title": "ASkDAgger: Active Skill-level Data Aggregation for Interactive Imitation Learning", "authors": ["Jelle Luijkx", "Zlatan Ajanoviƒá", "Laura Ferranti", "Jens Kober"], "categories": ["cs.LG", "cs.AI", "cs.HC", "cs.RO", "68T05", "I.2.6; I.2.8; I.2.9"], "comment": "Accepted for publication in Transactions on Machine Learning Research\n  (TMLR, 2025)", "summary": "Human teaching effort is a significant bottleneck for the broader\napplicability of interactive imitation learning. To reduce the number of\nrequired queries, existing methods employ active learning to query the human\nteacher only in uncertain, risky, or novel situations. However, during these\nqueries, the novice's planned actions are not utilized despite containing\nvaluable information, such as the novice's capabilities, as well as\ncorresponding uncertainty levels. To this end, we allow the novice to say: \"I\nplan to do this, but I am uncertain.\" We introduce the Active Skill-level Data\nAggregation (ASkDAgger) framework, which leverages teacher feedback on the\nnovice plan in three key ways: (1) S-Aware Gating (SAG): Adjusts the gating\nthreshold to track sensitivity, specificity, or a minimum success rate; (2)\nForesight Interactive Experience Replay (FIER), which recasts valid and\nrelabeled novice action plans into demonstrations; and (3) Prioritized\nInteractive Experience Replay (PIER), which prioritizes replay based on\nuncertainty, novice success, and demonstration age. Together, these components\nbalance query frequency with failure incidence, reduce the number of required\ndemonstration annotations, improve generalization, and speed up adaptation to\nchanging domains. We validate the effectiveness of ASkDAgger through\nlanguage-conditioned manipulation tasks in both simulation and real-world\nenvironments. Code, data, and videos are available at\nhttps://askdagger.github.io."}
{"id": "2508.05366", "pdf": "https://arxiv.org/pdf/2508.05366.pdf", "abs": "https://arxiv.org/abs/2508.05366", "title": "Can Language Models Critique Themselves? Investigating Self-Feedback for Retrieval Augmented Generation at BioASQ 2025", "authors": ["Samy Ateia", "Udo Kruschwitz"], "categories": ["cs.CL"], "comment": "Version as accepted at the BioASQ Lab at CLEF 2025", "summary": "Agentic Retrieval Augmented Generation (RAG) and 'deep research' systems aim\nto enable autonomous search processes where Large Language Models (LLMs)\niteratively refine outputs. However, applying these systems to domain-specific\nprofessional search, such as biomedical research, presents challenges, as\nautomated systems may reduce user involvement and misalign with expert\ninformation needs. Professional search tasks often demand high levels of user\nexpertise and transparency. The BioASQ CLEF 2025 challenge, using\nexpert-formulated questions, can serve as a platform to study these issues. We\nexplored the performance of current reasoning and nonreasoning LLMs like\nGemini-Flash 2.0, o3-mini, o4-mini and DeepSeek-R1. A key aspect of our\nmethodology was a self-feedback mechanism where LLMs generated, evaluated, and\nthen refined their outputs for query expansion and for multiple answer types\n(yes/no, factoid, list, ideal). We investigated whether this iterative\nself-correction improves performance and if reasoning models are more capable\nof generating useful feedback. Preliminary results indicate varied performance\nfor the self-feedback strategy across models and tasks. This work offers\ninsights into LLM self-correction and informs future work on comparing the\neffectiveness of LLM-generated feedback with direct human expert input in these\nsearch systems."}
{"id": "2508.05358", "pdf": "https://arxiv.org/pdf/2508.05358.pdf", "abs": "https://arxiv.org/abs/2508.05358", "title": "Evaluation of a Sign Language Avatar on Comprehensibility, User Experience \\& Acceptability", "authors": ["Fenya Wasserroth", "Eleftherios Avramidis", "Vera Czehmann", "Tanja Kojic", "Fabrizio Nunnari", "Sebastian M√∂ller"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "This paper presents an investigation into the impact of adding adjustment\nfeatures to an existing sign language (SL) avatar on a Microsoft Hololens 2\ndevice. Through a detailed analysis of interactions of expert German Sign\nLanguage (DGS) users with both adjustable and non-adjustable avatars in a\nspecific use case, this study identifies the key factors influencing the\ncomprehensibility, the user experience (UX), and the acceptability of such a\nsystem. Despite user preference for adjustable settings, no significant\nimprovements in UX or comprehensibility were observed, which remained at low\nlevels, amid missing SL elements (mouthings and facial expressions) and\nimplementation issues (indistinct hand shapes, lack of feedback and menu\npositioning). Hedonic quality was rated higher than pragmatic quality,\nindicating that users found the system more emotionally or aesthetically\npleasing than functionally useful. Stress levels were higher for the adjustable\navatar, reflecting lower performance, greater effort and more frustration.\nAdditionally, concerns were raised about whether the Hololens adjustment\ngestures are intuitive and easy to familiarise oneself with. While\nacceptability of the concept of adjustability was generally positive, it was\nstrongly dependent on usability and animation quality. This study highlights\nthat personalisation alone is insufficient, and that SL avatars must be\ncomprehensible by default. Key recommendations include enhancing mouthing and\nfacial animation, improving interaction interfaces, and applying participatory\ndesign."}
{"id": "2508.05374", "pdf": "https://arxiv.org/pdf/2508.05374.pdf", "abs": "https://arxiv.org/abs/2508.05374", "title": "The TUB Sign Language Corpus Collection", "authors": ["Eleftherios Avramidis", "Vera Czehmann", "Fabian Deckert", "Lorenz Hufe", "Aljoscha Lipski", "Yuni Amaloa Quintero Villalobos", "Tae Kwon Rhee", "Mengqian Shi", "Lennart St√∂lting", "Fabrizio Nunnari", "Sebastian M√∂ller"], "categories": ["cs.CL"], "comment": null, "summary": "We present a collection of parallel corpora of 12 sign languages in video\nformat, together with subtitles in the dominant spoken languages of the\ncorresponding countries. The entire collection includes more than 1,300 hours\nin 4,381 video files, accompanied by 1,3~M subtitles containing 14~M tokens.\nMost notably, it includes the first consistent parallel corpora for 8 Latin\nAmerican sign languages, whereas the size of the German Sign Language corpora\nis ten times the size of the previously available corpora. The collection was\ncreated by collecting and processing videos of multiple sign languages from\nvarious online sources, mainly broadcast material of news shows, governmental\nbodies and educational channels. The preparation involved several stages,\nincluding data collection, informing the content creators and seeking usage\napprovals, scraping, and cropping. The paper provides statistics on the\ncollection and an overview of the methods used to collect the data."}
{"id": "2508.05524", "pdf": "https://arxiv.org/pdf/2508.05524.pdf", "abs": "https://arxiv.org/abs/2508.05524", "title": "GASP: A Gradient-Aware Shortest Path Algorithm for Boundary-Confined Visualization of 2-Manifold Reeb Graphs", "authors": ["Sefat Rahman", "Tushar M. Athawale", "Paul Rosen"], "categories": ["cs.GR", "cs.CG", "cs.HC"], "comment": null, "summary": "Reeb graphs are an important tool for abstracting and representing the\ntopological structure of a function defined on a manifold. We have identified\nthree properties for faithfully representing Reeb graphs in a visualization.\nNamely, they should be constrained to the boundary, compact, and aligned with\nthe function gradient. Existing algorithms for drawing Reeb graphs are agnostic\nto or violate these properties. In this paper, we introduce an algorithm to\ngenerate Reeb graph visualizations, called \\textit{GASP}, that is cognizant of\nthese properties, thereby producing visualizations that are more representative\nof the underlying data. To demonstrate the improvements, the resulting Reeb\ngraphs are evaluated both qualitatively and quantitatively against the\ngeometric barycenter algorithm, using its implementation available in the\nTopology ToolKit (TTK), a widely adopted tool for calculating and visualizing\nReeb graphs."}
{"id": "2508.05429", "pdf": "https://arxiv.org/pdf/2508.05429.pdf", "abs": "https://arxiv.org/abs/2508.05429", "title": "MyCulture: Exploring Malaysia's Diverse Culture under Low-Resource Language Constraints", "authors": ["Zhong Ken Hew", "Jia Xin Low", "Sze Jue Yang", "Chee Seng chan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often exhibit cultural biases due to training\ndata dominated by high-resource languages like English and Chinese. This poses\nchallenges for accurately representing and evaluating diverse cultural\ncontexts, particularly in low-resource language settings. To address this, we\nintroduce MyCulture, a benchmark designed to comprehensively evaluate LLMs on\nMalaysian culture across six pillars: arts, attire, customs, entertainment,\nfood, and religion presented in Bahasa Melayu. Unlike conventional benchmarks,\nMyCulture employs a novel open-ended multiple-choice question format without\npredefined options, thereby reducing guessing and mitigating format bias. We\nprovide a theoretical justification for the effectiveness of this open-ended\nstructure in improving both fairness and discriminative power. Furthermore, we\nanalyze structural bias by comparing model performance on structured versus\nfree-form outputs, and assess language bias through multilingual prompt\nvariations. Our evaluation across a range of regional and international LLMs\nreveals significant disparities in cultural comprehension, highlighting the\nurgent need for culturally grounded and linguistically inclusive benchmarks in\nthe development and assessment of LLMs."}
{"id": "2508.05535", "pdf": "https://arxiv.org/pdf/2508.05535.pdf", "abs": "https://arxiv.org/abs/2508.05535", "title": "Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation", "authors": ["Albert Yu", "Chengshu Li", "Luca Macesanu", "Arnav Balaji", "Ruchira Ray", "Raymond Mooney", "Roberto Mart√≠n-Mart√≠n"], "categories": ["cs.RO", "cs.CL", "cs.HC", "cs.LG", "cs.MA", "I.2.9; I.2.7; I.2.6"], "comment": "Project website at https://robin-lab.cs.utexas.edu/MicoBot/", "summary": "Effective robotic systems for long-horizon human-robot collaboration must\nadapt to a wide range of human partners, whose physical behavior, willingness\nto assist, and understanding of the robot's capabilities may change over time.\nThis demands a tightly coupled communication loop that grants both agents the\nflexibility to propose, accept, or decline requests as they coordinate toward\ncompleting the task effectively. We apply a Mixed-Initiative dialog paradigm to\nCollaborative human-roBot teaming and propose MICoBot, a system that handles\nthe common scenario where both agents, using natural language, take initiative\nin formulating, accepting, or rejecting proposals on who can best complete\ndifferent steps of a task. To handle diverse, task-directed dialog, and find\nsuccessful collaborative strategies that minimize human effort, MICoBot makes\ndecisions at three levels: (1) a meta-planner considers human dialog to\nformulate and code a high-level collaboration strategy, (2) a planner optimally\nallocates the remaining steps to either agent based on the robot's capabilities\n(measured by a simulation-pretrained affordance model) and the human's\nestimated availability to help, and (3) an action executor decides the\nlow-level actions to perform or words to say to the human. Our extensive\nevaluations in simulation and real-world -- on a physical robot with 18 unique\nhuman participants over 27 hours -- demonstrate the ability of our method to\neffectively collaborate with diverse human users, yielding significantly\nimproved task success and user experience than a pure LLM baseline and other\nagent allocation models. See additional videos and materials at\nhttps://robin-lab.cs.utexas.edu/MicoBot/."}
{"id": "2508.05452", "pdf": "https://arxiv.org/pdf/2508.05452.pdf", "abs": "https://arxiv.org/abs/2508.05452", "title": "LLMEval-3: A Large-Scale Longitudinal Study on Robust and Fair Evaluation of Large Language Models", "authors": ["Ming Zhang", "Yujiong Shen", "Jingyi Deng", "Yuhui Wang", "Yue Zhang", "Junzhe Wang", "Shichun Liu", "Shihan Dou", "Huayu Sha", "Qiyuan Peng", "Changhao Jiang", "Jingqi Tong", "Yilong Wu", "Zhihao Zhang", "Mingqi Wu", "Zhiheng Xi", "Mingxu Chai", "Tao Liang", "Zhihui Fei", "Zhen Wang", "Mingyang Wan", "Guojun Ma", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Existing evaluation of Large Language Models (LLMs) on static benchmarks is\nvulnerable to data contamination and leaderboard overfitting, critical issues\nthat obscure true model capabilities. To address this, we introduce LLMEval-3,\na framework for dynamic evaluation of LLMs. LLMEval-3 is built on a proprietary\nbank of 220k graduate-level questions, from which it dynamically samples unseen\ntest sets for each evaluation run. Its automated pipeline ensures integrity via\ncontamination-resistant data curation, a novel anti-cheating architecture, and\na calibrated LLM-as-a-judge process achieving 90% agreement with human experts,\ncomplemented by a relative ranking system for fair comparison. An 20-month\nlongitudinal study of nearly 50 leading models reveals a performance ceiling on\nknowledge memorization and exposes data contamination vulnerabilities\nundetectable by static benchmarks. The framework demonstrates exceptional\nrobustness in ranking stability and consistency, providing strong empirical\nvalidation for the dynamic evaluation paradigm. LLMEval-3 offers a robust and\ncredible methodology for assessing the true capabilities of LLMs beyond\nleaderboard scores, promoting the development of more trustworthy evaluation\nstandards."}
{"id": "2310.05853", "pdf": "https://arxiv.org/pdf/2310.05853.pdf", "abs": "https://arxiv.org/abs/2310.05853", "title": "\"Mango Mango, How to Let The Lettuce Dry Without A Spinner?\": Exploring User Perceptions of Using An LLM-Based Conversational Assistant Toward Cooking Partner", "authors": ["Szeyi Chan", "Jiachen Li", "Bingsheng Yao", "Amama Mahmood", "Chien-Ming Huang", "Holly Jimison", "Elizabeth D Mynatt", "Dakuo Wang"], "categories": ["cs.HC"], "comment": "To appear at CSCW 2025", "summary": "The rapid advancement of Large Language Models (LLMs) has created numerous\npotentials for integration with conversational assistants (CAs) assisting\npeople in their daily tasks, particularly due to their extensive flexibility.\nHowever, users' real-world experiences interacting with these assistants remain\nunexplored. In this research, we chose cooking, a complex daily task, as a\nscenario to explore people's successful and unsatisfactory experiences while\nreceiving assistance from an LLM-based CA, Mango Mango. We discovered that\nparticipants value the system's ability to offer customized instructions based\non context, provide extensive information beyond the recipe, and assist them in\ndynamic task planning. However, users expect the system to be more adaptive to\noral conversation and provide more suggestive responses to keep them actively\ninvolved. Recognizing that users began treating our LLM-CA as a personal\nassistant or even a partner rather than just a recipe-reading tool, we propose\nfive design considerations for future development."}
{"id": "2508.05468", "pdf": "https://arxiv.org/pdf/2508.05468.pdf", "abs": "https://arxiv.org/abs/2508.05468", "title": "TASE: Token Awareness and Structured Evaluation for Multilingual Language Models", "authors": ["Chenzhuo Zhao", "Xinda Wang", "Yue Huang", "Junting Lu", "Ziqian Liu"], "categories": ["cs.CL"], "comment": null, "summary": "While large language models (LLMs) have demonstrated remarkable performance\non high-level semantic tasks, they often struggle with fine-grained,\ntoken-level understanding and structural reasoning--capabilities that are\nessential for applications requiring precision and control. We introduce TASE,\na comprehensive benchmark designed to evaluate LLMs' ability to perceive and\nreason about token-level information across languages. TASE covers 10 tasks\nunder two core categories: token awareness and structural understanding,\nspanning Chinese, English, and Korean, with a 35,927-instance evaluation set\nand a scalable synthetic data generation pipeline for training. Tasks include\ncharacter counting, token alignment, syntactic structure parsing, and length\nconstraint satisfaction. We evaluate over 30 leading commercial and open-source\nLLMs, including O3, Claude 4, Gemini 2.5 Pro, and DeepSeek-R1, and train a\ncustom Qwen2.5-14B model using the GRPO training method. Results show that\nhuman performance significantly outpaces current LLMs, revealing persistent\nweaknesses in token-level reasoning. TASE sheds light on these limitations and\nprovides a new diagnostic lens for future improvements in low-level language\nunderstanding and cross-lingual generalization. Our code and dataset are\npublicly available at https://github.com/cyzcz/Tase ."}
{"id": "2502.01325", "pdf": "https://arxiv.org/pdf/2502.01325.pdf", "abs": "https://arxiv.org/abs/2502.01325", "title": "The Homework Wars: Exploring Emotions, Behaviours, and Conflicts in Parent-Child Homework Interactions", "authors": ["Nan Gao", "Yibin Liu", "Xin Tang", "Yanyan Liu", "Chun Yu", "Yun Huang", "Yuntao Wang", "Flora D. Salim", "Xuhai Orson Xu", "Jun Wei", "Yuanchun Shi"], "categories": ["cs.HC"], "comment": "This paper was accepted by Proceedings of the ACM on Interactive,\n  Mobile, Wearable and Ubiquitous Technologies (IMWUT)", "summary": "Parental involvement in homework is a crucial aspect of family education, but\nit often triggers emotional strain and conflicts. Despite growing concern over\nits impact on family well-being, prior research has lacked access to\nfine-grained, real-time dynamics of these interactions. To bridge this gap, we\npresent a framework that leverages naturalistic parent-child interaction data\nand large language models (LLMs) to analyse homework conversations at scale. In\na four-week in situ study with 78 Chinese families, we collected 475 hours of\naudio recordings and accompanying daily surveys, capturing 602 homework\nsessions in everyday home settings. Our LLM-based pipeline reliably extracted\nand categorised parental behaviours and conflict patterns from transcribed\nconversations, achieving high agreement with expert annotations. The analysis\nrevealed significant emotional shifts in parents before and after homework, 18\nrecurring parental behaviours and seven common conflict types, with Knowledge\nConflict being the most frequent. Notably, even well-intentioned behaviours\nwere significantly positively correlated with specific conflicts. This work\nadvances ubiquitous computing methods for studying complex family dynamics and\noffers empirical insights to enrich family education theory and inform more\neffective parenting strategies and interventions in the future."}
{"id": "2508.05470", "pdf": "https://arxiv.org/pdf/2508.05470.pdf", "abs": "https://arxiv.org/abs/2508.05470", "title": "Rethinking Creativity Evaluation: A Critical Analysis of Existing Creativity Evaluations", "authors": ["Li-Chun Lu", "Miri Liu", "Pin-Chun Lu", "Yufei Tian", "Shao-Hua Sun", "Nanyun Peng"], "categories": ["cs.CL"], "comment": "15 pages, 6 figures", "summary": "We systematically examine, analyze, and compare representative creativity\nmeasures--creativity index, perplexity, syntactic templates, and\nLLM-as-a-Judge--across diverse creative domains, including creative writing,\nunconventional problem-solving, and research ideation. Our analyses reveal that\nthese metrics exhibit limited consistency, capturing different dimensions of\ncreativity. We highlight key limitations, including the creativity index's\nfocus on lexical diversity, perplexity's sensitivity to model confidence, and\nsyntactic templates' inability to capture conceptual creativity. Additionally,\nLLM-as-a-Judge shows instability and bias. Our findings underscore the need for\nmore robust, generalizable evaluation frameworks that better align with human\njudgments of creativity."}
{"id": "2505.00956", "pdf": "https://arxiv.org/pdf/2505.00956.pdf", "abs": "https://arxiv.org/abs/2505.00956", "title": "Audio Personas: Augmenting Social Perception via Body-Anchored Audio Cues", "authors": ["Yujie Tao", "Libby Ye", "Jeremy N. Bailenson", "Sean Follmer"], "categories": ["cs.HC"], "comment": "Accepted to Transactions on Computer-Human Interaction", "summary": "We introduce Audio Personas, enabling users to \"decorate\" themselves with\nbody-anchored sounds in audio augmented reality. Like outfits, makeup, and\nfragrances, audio personas offer an alternative yet dynamic channel to augment\nface-to-face interactions. For instance, one can set their audio persona as\nrain sounds to reflect a bad mood, bee sounds to establish personal boundaries,\nor a playful \"woosh\" sound to mimic passing by someone like a breeze. To\ninstantiate the concept, we implemented a headphone-based prototype with\nmulti-user tracking and audio streaming. Our preregistered in-lab study with 64\nparticipants showed that audio personas influenced how participants formed\nimpressions. Individuals with positive audio personas were rated as more\nsocially attractive, more likable, and less threatening than those with\nnegative audio personas. Our study with audio designers revealed that audio\npersonas were preferred in public and semi-public-private spaces for managing\nsocial impressions (e.g., personality) and signaling current states (e.g.,\nemotions)."}
{"id": "2508.05509", "pdf": "https://arxiv.org/pdf/2508.05509.pdf", "abs": "https://arxiv.org/abs/2508.05509", "title": "LAG: Logic-Augmented Generation from a Cartesian Perspective", "authors": ["Yilin Xiao", "Chuang Zhou", "Qinggang Zhang", "Su Dong", "Shengyuan Chen", "Xiao Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet exhibit critical limitations in knowledge-intensive\ntasks, often generating hallucinations when faced with questions requiring\nspecialized expertise. While retrieval-augmented generation (RAG) mitigates\nthis by integrating external knowledge, it struggles with complex reasoning\nscenarios due to its reliance on direct semantic retrieval and lack of\nstructured logical organization. Inspired by Cartesian principles from\n\\textit{Discours de la m\\'ethode}, this paper introduces Logic-Augmented\nGeneration (LAG), a novel paradigm that reframes knowledge augmentation through\nsystematic question decomposition and dependency-aware reasoning. Specifically,\nLAG first decomposes complex questions into atomic sub-questions ordered by\nlogical dependencies. It then resolves these sequentially, using prior answers\nto guide context retrieval for subsequent sub-questions, ensuring stepwise\ngrounding in logical chain. To prevent error propagation, LAG incorporates a\nlogical termination mechanism that halts inference upon encountering\nunanswerable sub-questions and reduces wasted computation on excessive\nreasoning. Finally, it synthesizes all sub-resolutions to generate verified\nresponses. Experiments on four benchmark datasets demonstrate that LAG\nsignificantly enhances reasoning robustness, reduces hallucination, and aligns\nLLM problem-solving with human cognition, offering a principled alternative to\nexisting RAG systems."}
{"id": "2505.03185", "pdf": "https://arxiv.org/pdf/2505.03185.pdf", "abs": "https://arxiv.org/abs/2505.03185", "title": "A Review of Behavioral Closed-Loop Paradigm from Sensing to Intervention for Ingestion Health", "authors": ["Jun Fang", "Yanuo Zhou", "Ka I Chan", "Jiajin Li", "Zeyi Sun", "Zhengnan Li", "Zicong Fu", "Hongjing Piao", "Haodong Xu", "Yuanchun Shi", "Yuntao Wang"], "categories": ["cs.HC"], "comment": null, "summary": "Ingestive behavior plays a critical role in health, yet many existing\ninterventions remain limited to static guidance or manual self-tracking. With\nthe increasing integration of sensors, context-aware computing, and perceptual\ncomputing, recent systems have begun to support closed-loop interventions that\ndynamically sense user behavior and provide feedback during or around ingestion\nepisodes. In this survey, we review 136 studies that leverage sensor-enabled or\ninteraction-mediated approaches to influence ingestive behavior. We propose a\nbehavioral closed-loop paradigm rooted in context-aware computing and inspired\nby HCI behavior change frameworks, comprising four components: target\nbehaviors, sensing modalities, reasoning and intervention strategies. A\ntaxonomy of sensing and intervention modalities is presented, organized along\nhuman- and environment-based dimensions. Our analysis also examines evaluation\nmethods and design trends across different modality-behavior pairings. This\nreview reveals prevailing patterns and critical gaps, offering design insights\nfor future adaptive and context-aware ingestion health interventions."}
{"id": "2508.05525", "pdf": "https://arxiv.org/pdf/2508.05525.pdf", "abs": "https://arxiv.org/abs/2508.05525", "title": "The World According to LLMs: How Geographic Origin Influences LLMs' Entity Deduction Capabilities", "authors": ["Harsh Nishant Lalai", "Raj Sanjay Shah", "Jiaxin Pei", "Sashank Varma", "Yi-Chia Wang", "Ali Emami"], "categories": ["cs.CL", "cs.AI"], "comment": "Conference on Language Modeling 2025", "summary": "Large Language Models (LLMs) have been extensively tuned to mitigate explicit\nbiases, yet they often exhibit subtle implicit biases rooted in their\npre-training data. Rather than directly probing LLMs with human-crafted\nquestions that may trigger guardrails, we propose studying how models behave\nwhen they proactively ask questions themselves. The 20 Questions game, a\nmulti-turn deduction task, serves as an ideal testbed for this purpose. We\nsystematically evaluate geographic performance disparities in entity deduction\nusing a new dataset, Geo20Q+, consisting of both notable people and culturally\nsignificant objects (e.g., foods, landmarks, animals) from diverse regions. We\ntest popular LLMs across two gameplay configurations (canonical 20-question and\nunlimited turns) and in seven languages (English, Hindi, Mandarin, Japanese,\nFrench, Spanish, and Turkish). Our results reveal geographic disparities: LLMs\nare substantially more successful at deducing entities from the Global North\nthan the Global South, and the Global West than the Global East. While\nWikipedia pageviews and pre-training corpus frequency correlate mildly with\nperformance, they fail to fully explain these disparities. Notably, the\nlanguage in which the game is played has minimal impact on performance gaps.\nThese findings demonstrate the value of creative, free-form evaluation\nframeworks for uncovering subtle biases in LLMs that remain hidden in standard\nprompting setups. By analyzing how models initiate and pursue reasoning goals\nover multiple turns, we find geographic and cultural disparities embedded in\ntheir reasoning processes. We release the dataset (Geo20Q+) and code at\nhttps://sites.google.com/view/llmbias20q/home."}
{"id": "2507.21411", "pdf": "https://arxiv.org/pdf/2507.21411.pdf", "abs": "https://arxiv.org/abs/2507.21411", "title": "InSituTale: Enhancing Augmented Data Storytelling with Physical Objects", "authors": ["Kentaro Takahira", "Yue Yu", "Takanori Fujiwara", "Ryo Suzuki", "Huamin Qu"], "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "Augmented data storytelling enhances narrative delivery by integrating\nvisualizations with physical environments and presenter actions. Existing\nsystems predominantly rely on body gestures or speech to control\nvisualizations, leaving interactions with physical objects largely\nunderexplored. We introduce augmented physical data storytelling, an approach\nenabling presenters to manipulate visualizations through physical object\ninteractions. To inform this approach, we first conducted a survey of\ndata-driven presentations to identify common visualization commands. We then\nconducted workshops with nine HCI/VIS researchers to collect mappings between\nphysical manipulations and these commands. Guided by these insights, we\ndeveloped InSituTale, a prototype that combines object tracking via a depth\ncamera with Vision-LLM for detecting real-world events. Through physical\nmanipulations, presenters can dynamically execute various visualization\ncommands, delivering cohesive data storytelling experiences that blend physical\nand digital elements. A user study with 12 participants demonstrated that\nInSituTale enables intuitive interactions, offers high utility, and facilitates\nan engaging presentation experience."}
{"id": "2508.05534", "pdf": "https://arxiv.org/pdf/2508.05534.pdf", "abs": "https://arxiv.org/abs/2508.05534", "title": "CoCoLex: Confidence-guided Copy-based Decoding for Grounded Legal Text Generation", "authors": ["Santosh T. Y. S. S", "Youssef Tarek Elkhayat", "Oana Ichim", "Pranav Shetty", "Dongsheng Wang", "Zhiqiang Ma", "Armineh Nourbakhsh", "Xiaomo Liu"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025-Main Conference", "summary": "Due to their ability to process long and complex contexts, LLMs can offer key\nbenefits to the Legal domain, but their adoption has been hindered by their\ntendency to generate unfaithful, ungrounded, or hallucinatory outputs. While\nRetrieval-Augmented Generation offers a promising solution by grounding\ngenerations in external knowledge, it offers no guarantee that the provided\ncontext will be effectively integrated. To address this, context-aware decoding\nstrategies have been proposed to amplify the influence of relevant context, but\nthey usually do not explicitly enforce faithfulness to the context. In this\nwork, we introduce Confidence-guided Copy-based Decoding for Legal Text\nGeneration (CoCoLex)-a decoding strategy that dynamically interpolates the\nmodel produced vocabulary distribution with a distribution derived based on\ncopying from the context. CoCoLex encourages direct copying based on the\nmodel's confidence, ensuring greater fidelity to the source. Experimental\nresults on five legal benchmarks demonstrate that CoCoLex outperforms existing\ncontext-aware decoding methods, particularly in long-form generation tasks."}
{"id": "2508.03061", "pdf": "https://arxiv.org/pdf/2508.03061.pdf", "abs": "https://arxiv.org/abs/2508.03061", "title": "Facilitating Visual Media Exploration for Blind and Low Vision Users through AI-Powered Interactive Storytelling", "authors": ["Shuchang Xu"], "categories": ["cs.HC"], "comment": null, "summary": "Empowering blind and low vision (BLV) users to explore visual media improves\ncontent comprehension, strengthens user agency, and fulfills diverse\ninformation needs. However, most existing tools separate exploration from the\nmain narration, which disrupts the narrative flow, increases cognitive load,\nand limits deep engagement with visual media. To address these challenges, my\nPhD research introduces the paradigm of AI-powered interactive storytelling,\nwhich leverages AI to generate interactive narratives, enabling BLV users to\nexplore visual media within a coherent storytelling experience. I have\noperationalized this paradigm through three techniques: (1) Hierarchical\nNarrative, which supports photo-collection exploration at different levels of\ndetail; (2) Parallel Narrative, which provides seamless access to time-synced\nvideo comments; and (3) Branching Narrative, which enables immersive navigation\nof 360{\\deg} videos. Together, these techniques demonstrate that AI-powered\ninteractive storytelling can effectively balance user agency with narrative\ncoherence across diverse media formats. My future work will advance this\nparadigm by enabling more personalized and expressive storytelling experiences\nfor BLV audiences."}
{"id": "2508.05544", "pdf": "https://arxiv.org/pdf/2508.05544.pdf", "abs": "https://arxiv.org/abs/2508.05544", "title": "Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees", "authors": ["Guang Yang", "Xinyang Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "under review", "summary": "Large Language Models (LLMs) have shown remarkable progress in\nmultiple-choice question answering (MCQA), but their inherent unreliability,\nsuch as hallucination and overconfidence, limits their application in high-risk\ndomains. To address this, we propose a frequency-based uncertainty\nquantification method under black-box settings, leveraging conformal prediction\n(CP) to ensure provable coverage guarantees. Our approach involves multiple\nindependent samplings of the model's output distribution for each input, with\nthe most frequent sample serving as a reference to calculate predictive entropy\n(PE). Experimental evaluations across six LLMs and four datasets (MedMCQA,\nMedQA, MMLU, MMLU-Pro) demonstrate that frequency-based PE outperforms\nlogit-based PE in distinguishing between correct and incorrect predictions, as\nmeasured by AUROC. Furthermore, the method effectively controls the empirical\nmiscoverage rate under user-specified risk levels, validating that sampling\nfrequency can serve as a viable substitute for logit-based probabilities in\nblack-box scenarios. This work provides a distribution-free model-agnostic\nframework for reliable uncertainty quantification in MCQA with guaranteed\ncoverage, enhancing the trustworthiness of LLMs in practical applications."}
{"id": "2508.03852", "pdf": "https://arxiv.org/pdf/2508.03852.pdf", "abs": "https://arxiv.org/abs/2508.03852", "title": "A11yShape: AI-Assisted 3-D Modeling for Blind and Low-Vision Programmers", "authors": ["Zhuohao Jerry Zhang", "Haichang Li", "Chun Meng Yu", "Faraz Faruqi", "Junan Xie", "Gene S-H Kim", "Mingming Fan", "Angus G. Forbes", "Jacob O. Wobbrock", "Anhong Guo", "Liang He"], "categories": ["cs.HC"], "comment": "ASSETS 2025", "summary": "Building 3-D models is challenging for blind and low-vision (BLV) users due\nto the inherent complexity of 3-D models and the lack of support for non-visual\ninteraction in existing tools. To address this issue, we introduce A11yShape, a\nnovel system designed to help BLV users who possess basic programming skills\nunderstand, modify, and iterate on 3-D models. A11yShape leverages LLMs and\nintegrates with OpenSCAD, a popular open-source editor that generates 3-D\nmodels from code. Key functionalities of A11yShape include accessible\ndescriptions of 3-D models, version control to track changes in models and\ncode, and a hierarchical representation of model components. Most importantly,\nA11yShape employs a cross-representation highlighting mechanism to synchronize\nsemantic selections across all model representations -- code, semantic\nhierarchy, AI description, and 3-D rendering. We conducted a multi-session user\nstudy with four BLV programmers, where, after an initial tutorial session,\nparticipants independently completed 12 distinct models across two testing\nsessions, achieving results that aligned with their own satisfaction. The\nresult demonstrates that participants were able to comprehend provided 3-D\nmodels, as well as independently create and modify 3-D models -- tasks that\nwere previously impossible without assistance from sighted individuals."}
{"id": "2508.05553", "pdf": "https://arxiv.org/pdf/2508.05553.pdf", "abs": "https://arxiv.org/abs/2508.05553", "title": "Do Political Opinions Transfer Between Western Languages? An Analysis of Unaligned and Aligned Multilingual LLMs", "authors": ["Franziska Weeber", "Tanise Ceron", "Sebastian Pad√≥"], "categories": ["cs.CL", "cs.CY", "I.2.7; J.4"], "comment": null, "summary": "Public opinion surveys show cross-cultural differences in political opinions\nbetween socio-cultural contexts. However, there is no clear evidence whether\nthese differences translate to cross-lingual differences in multilingual large\nlanguage models (MLLMs). We analyze whether opinions transfer between languages\nor whether there are separate opinions for each language in MLLMs of various\nsizes across five Western languages. We evaluate MLLMs' opinions by prompting\nthem to report their (dis)agreement with political statements from voting\nadvice applications. To better understand the interaction between languages in\nthe models, we evaluate them both before and after aligning them with more left\nor right views using direct preference optimization and English alignment data\nonly. Our findings reveal that unaligned models show only very few significant\ncross-lingual differences in the political opinions they reflect. The political\nalignment shifts opinions almost uniformly across all five languages. We\nconclude that in Western language contexts, political opinions transfer between\nlanguages, demonstrating the challenges in achieving explicit socio-linguistic,\ncultural, and political alignment of MLLMs."}
{"id": "2508.03969", "pdf": "https://arxiv.org/pdf/2508.03969.pdf", "abs": "https://arxiv.org/abs/2508.03969", "title": "Human-Centered Human-AI Interaction (HC-HAII): A Human-Centered AI Perspective", "authors": ["Wei Xu"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This chapter systematically promotes an emerging interdisciplinary field of\nhuman-artificial intelligence interaction (human-AI interaction, HAII) from a\nhuman-centered AI (HCAI) perspective. It introduces a framework of\nhuman-centered HAII (HC-HAII). HC-HAII places humans at the core of HAII\nresearch and applications, emphasizing the importance of adopting a\nhuman-centered approach over a technology-centered one. The chapter presents\nthe HC-HAII methodology, including human-centered methods, process,\ninterdisciplinary teams, and multi-level design paradigms. It also highlights\nkey research challenges and future directions. As the first chapter, this\nchapter also provides a structural overview of this book, which brings together\ncontributions from an interdisciplinary community of researchers and\npractitioners to advance the theory, methodology, and applications of HCAI in\ndiverse domains of HAII. The purpose of this chapter is to provide a\nfundamental framework for this book, centered on HAII research and applications\nbased on the HCAI approach, which will pave the way for the content of\nsubsequent chapters."}
{"id": "2508.05592", "pdf": "https://arxiv.org/pdf/2508.05592.pdf", "abs": "https://arxiv.org/abs/2508.05592", "title": "MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging Synthetic Problems with a Reinforced Policy", "authors": ["Shaoxiong Zhan", "Yanlin Lai", "Ziyu Lu", "Dahua Lin", "Ziqing Yang", "Fei Tang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models have achieved substantial progress in mathematical\nreasoning, yet their advancement is limited by the scarcity of high-quality,\nhigh-difficulty training data. Existing synthesis methods largely rely on\ntransforming human-written templates, limiting both diversity and scalability.\nWe propose MathSmith, a novel framework for synthesizing challenging\nmathematical problems to enhance LLM reasoning. Rather than modifying existing\nproblems, MathSmith constructs new ones from scratch by randomly sampling\nconcept-explanation pairs from PlanetMath, ensuring data independence and\navoiding contamination. To increase difficulty, we design nine predefined\nstrategies as soft constraints during rationales. We further adopts\nreinforcement learning to jointly optimize structural validity, reasoning\ncomplexity, and answer consistency. The length of the reasoning trace generated\nunder autoregressive prompting is used to reflect cognitive complexity,\nencouraging the creation of more demanding problems aligned with\nlong-chain-of-thought reasoning. Experiments across five benchmarks,\ncategorized as easy & medium (GSM8K, MATH-500) and hard (AIME2024, AIME2025,\nOlympiadBench), show that MathSmith consistently outperforms existing baselines\nunder both short and long CoT settings. Additionally, a weakness-focused\nvariant generation module enables targeted improvement on specific concepts.\nOverall, MathSmith exhibits strong scalability, generalization, and\ntransferability, highlighting the promise of high-difficulty synthetic data in\nadvancing LLM reasoning capabilities."}
{"id": "2401.13408", "pdf": "https://arxiv.org/pdf/2401.13408.pdf", "abs": "https://arxiv.org/abs/2401.13408", "title": "Toward A Causal Framework for Modeling Perception", "authors": ["Jose M. Alvarez", "Salvatore Ruggieri"], "categories": ["cs.AI", "cs.CY", "cs.HC"], "comment": "arXiv admin note: text overlap with arXiv:2305.09535 by other authors", "summary": "Perception occurs when individuals interpret the same information\ndifferently. It is a known cognitive phenomenon with implications for bias in\nhuman decision-making. Perception, however, remains understudied in machine\nlearning (ML). This is problematic as modern decision flows, whether partially\nor fully automated by ML applications, always involve human experts. For\ninstance, how might we account for cases in which two experts interpret\ndifferently the same deferred instance or explanation from a ML model?\nAddressing this and similar questions requires first a formulation of\nperception, particularly, in a manner that integrates with ML-enabled decision\nflows. In this work, we present a first approach to modeling perception\ncausally. We define perception under causal reasoning using structural causal\nmodels (SCMs). Our approach formalizes individual experience as additional\ncausal knowledge that comes with and is used by the expert decision-maker in\nthe form of a SCM. We define two kinds of probabilistic causal perception:\nstructural and parametrical. We showcase our framework through a series of\nexamples of modern decision flows. We also emphasize the importance of\naddressing perception in fair ML, discussing relevant fairness implications and\npossible applications."}
{"id": "2508.05613", "pdf": "https://arxiv.org/pdf/2508.05613.pdf", "abs": "https://arxiv.org/abs/2508.05613", "title": "Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models", "authors": ["Haitao Hong", "Yuchen Yan", "Xingyu Wu", "Guiyang Hou", "Wenqi Zhang", "Weiming Lu", "Yongliang Shen", "Jun Xiao"], "categories": ["cs.CL", "cs.AI"], "comment": "Project Page: https://zju-real.github.io/cooper Code:\n  https://github.com/zju-real/cooper", "summary": "Large language models (LLMs) have demonstrated remarkable performance in\nreasoning tasks, where reinforcement learning (RL) serves as a key algorithm\nfor enhancing their reasoning capabilities. Currently, there are two mainstream\nreward paradigms: model-based rewards and rule-based rewards. However, both\napproaches suffer from limitations: rule-based rewards lack robustness, while\nmodel-based rewards are vulnerable to reward hacking. To address these issues,\nwe propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework\nthat jointly optimizes both the policy model and the reward model. Cooper\nleverages the high precision of rule-based rewards when identifying correct\nresponses, and dynamically constructs and selects positive-negative sample\npairs for continued training the reward model. This design enhances robustness\nand mitigates the risk of reward hacking. To further support Cooper, we\nintroduce a hybrid annotation strategy that efficiently and accurately\ngenerates training data for the reward model. We also propose a reference-based\nreward modeling paradigm, where the reward model takes a reference answer as\ninput. Based on this design, we train a reward model named VerifyRM, which\nachieves higher accuracy on VerifyBench compared to other models of the same\nsize. We conduct reinforcement learning using both VerifyRM and Cooper. Our\nexperiments show that Cooper not only alleviates reward hacking but also\nimproves end-to-end RL performance, for instance, achieving a 0.54% gain in\naverage accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that\ndynamically updating reward model is an effective way to combat reward hacking,\nproviding a reference for better integrating reward models into RL."}
{"id": "2405.00708", "pdf": "https://arxiv.org/pdf/2405.00708.pdf", "abs": "https://arxiv.org/abs/2405.00708", "title": "Understanding Large Language Model Behaviors through Interactive Counterfactual Generation and Analysis", "authors": ["Furui Cheng", "Vil√©m Zouhar", "Robin Shing Moon Chan", "Daniel F√ºrst", "Hendrik Strobelt", "Mennatallah El-Assady"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG", "I.2.7; H.5.2"], "comment": null, "summary": "Understanding the behavior of large language models (LLMs) is crucial for\nensuring their safe and reliable use. However, existing explainable AI (XAI)\nmethods for LLMs primarily rely on word-level explanations, which are often\ncomputationally inefficient and misaligned with human reasoning processes.\nMoreover, these methods often treat explanation as a one-time output,\noverlooking its inherently interactive and iterative nature. In this paper, we\npresent LLM Analyzer, an interactive visualization system that addresses these\nlimitations by enabling intuitive and efficient exploration of LLM behaviors\nthrough counterfactual analysis. Our system features a novel algorithm that\ngenerates fluent and semantically meaningful counterfactuals via targeted\nremoval and replacement operations at user-defined levels of granularity. These\ncounterfactuals are used to compute feature attribution scores, which are then\nintegrated with concrete examples in a table-based visualization, supporting\ndynamic analysis of model behavior. A user study with LLM practitioners and\ninterviews with experts demonstrate the system's usability and effectiveness,\nemphasizing the importance of involving humans in the explanation process as\nactive participants rather than passive recipients."}
{"id": "2508.05614", "pdf": "https://arxiv.org/pdf/2508.05614.pdf", "abs": "https://arxiv.org/abs/2508.05614", "title": "OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks", "authors": ["Zixuan Wang", "Dingming Li", "Hongxing Li", "Shuo Chen", "Yuchen Yan", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "categories": ["cs.CL", "cs.AI"], "comment": "Project Page: https://zju-real.github.io/OmniEmbodied Code:\n  https://github.com/ZJU-REAL/OmniEmbodied", "summary": "Large language models excel at abstract reasoning but their capacity for\nembodied agent reasoning remains largely unexplored. We present OmniEAR, a\ncomprehensive framework for evaluating how language models reason about\nphysical interactions, tool usage, and multi-agent coordination in embodied\ntasks. Unlike existing benchmarks that provide predefined tool sets or explicit\ncollaboration directives, OmniEAR requires agents to dynamically acquire\ncapabilities and autonomously determine coordination strategies based on task\ndemands. Through text-based environment representation, we model continuous\nphysical properties and complex spatial relationships across 1,500 scenarios\nspanning household and industrial domains. Our systematic evaluation reveals\nsevere performance degradation when models must reason from constraints: while\nachieving 85-96% success with explicit instructions, performance drops to\n56-85% for tool reasoning and 63-85% for implicit collaboration, with compound\ntasks showing over 50% failure rates. Surprisingly, complete environmental\ninformation degrades coordination performance, indicating models cannot filter\ntask-relevant constraints. Fine-tuning improves single-agent tasks dramatically\n(0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing\nfundamental architectural limitations. These findings demonstrate that embodied\nreasoning poses fundamentally different challenges than current models can\naddress, establishing OmniEAR as a rigorous benchmark for evaluating and\nadvancing embodied AI systems. Our code and data are included in the\nsupplementary materials and will be open-sourced upon acceptance."}
{"id": "2412.00082", "pdf": "https://arxiv.org/pdf/2412.00082.pdf", "abs": "https://arxiv.org/abs/2412.00082", "title": "PL-DCP: A Pairwise Learning framework with Domain and Class Prototypes for EEG emotion recognition under unseen target conditions", "authors": ["Guangli Li", "Canbiao Wu", "Zhehao Zhou", "Tuo Sun", "Ping Tan", "Li Zhang", "Zhen Liang"], "categories": ["cs.LG", "cs.AI", "cs.HC", "eess.SP"], "comment": null, "summary": "Electroencephalogram (EEG) signals serve as a powerful tool in affective\nBrain-Computer Interfaces (aBCIs) and play a crucial role in affective\ncomputing. In recent years, the introduction of deep learning techniques has\nsignificantly advanced the development of aBCIs. However, the current emotion\nrecognition methods based on deep transfer learning face the challenge of the\ndual dependence of the model on source domain and target domain, As well as\nbeing affected by label noise, which seriously affects the performance and\ngeneralization ability of the model. To overcome this limitation, we proposes a\nPairwise Learning framework with Domain and Category Prototypes for EEG emotion\nrecognition under unseen target conditions (PL-DCP), and integrating concepts\nof feature disentanglement and prototype inference. Here, the feature\ndisentanglement module extracts and decouples the emotional EEG features to\nform domain features and class features, and further calculates the dual\nprototype representation. The Domain-pprototype captures the individual\nvariations across subjects, while the class-prototype captures the\ncross-individual commonality of emotion categories. In addition, the pairwise\nlearning strategy effectively reduces the noise effect caused by wrong labels.\nThe PL-DCP framework conducts a systematic experimental evaluation on the\npublished datasets SEED, SEED-IV and SEED-V, and the accuracy are 82.88\\%,\n65.15\\% and 61.29\\%, respectively. The results show that compared with other\nState-of-the-Art(SOTA) Methods, the PL-DCP model still achieves slightly better\nperformance than the deep transfer learning method that requires both source\nand target data, although the target domain is completely unseen during the\ntraining. This work provides an effective and robust potential solution for\nemotion recognition. The source code is available at\nhttps://github.com/WuCB-BCI/PL_DCP."}
{"id": "2508.05618", "pdf": "https://arxiv.org/pdf/2508.05618.pdf", "abs": "https://arxiv.org/abs/2508.05618", "title": "Learning to Reason for Factuality", "authors": ["Xilun Chen", "Ilia Kulikov", "Vincent-Pierre Berges", "Barlas Oƒüuz", "Rulin Shao", "Gargi Ghosh", "Jason Weston", "Wen-tau Yih"], "categories": ["cs.CL"], "comment": null, "summary": "Reasoning Large Language Models (R-LLMs) have significantly advanced complex\nreasoning tasks but often struggle with factuality, generating substantially\nmore hallucinations than their non-reasoning counterparts on long-form\nfactuality benchmarks. However, extending online Reinforcement Learning (RL), a\nkey component in recent R-LLM advancements, to the long-form factuality setting\nposes several unique challenges due to the lack of reliable verification\nmethods. Previous work has utilized automatic factuality evaluation frameworks\nsuch as FActScore to curate preference data in the offline RL setting, yet we\nfind that directly leveraging such methods as the reward in online RL leads to\nreward hacking in multiple ways, such as producing less detailed or relevant\nresponses. We propose a novel reward function that simultaneously considers the\nfactual precision, response detail level, and answer relevance, and applies\nonline RL to learn high quality factual reasoning. Evaluated on six long-form\nfactuality benchmarks, our factual reasoning model achieves an average\nreduction of 23.1 percentage points in hallucination rate, a 23% increase in\nanswer detail level, and no degradation in the overall response helpfulness."}
{"id": "2505.20924", "pdf": "https://arxiv.org/pdf/2505.20924.pdf", "abs": "https://arxiv.org/abs/2505.20924", "title": "Label Leakage in Federated Inertial-based Human Activity Recognition", "authors": ["Marius Bock", "Maximilian Hopp", "Kristof Van Laerhoven", "Michael Moeller"], "categories": ["cs.LG", "cs.HC"], "comment": "7 pages, 4 figures", "summary": "While prior work has shown that Federated Learning updates can leak sensitive\ninformation, label reconstruction attacks, which aim to recover input labels\nfrom shared gradients, have not yet been examined in the context of Human\nActivity Recognition (HAR). Given the sensitive nature of activity labels, this\nstudy evaluates the effectiveness of state-of-the-art gradient-based label\nleakage attacks on HAR benchmark datasets. Our findings show that the number of\nactivity classes, sampling strategy, and class imbalance are critical factors\ninfluencing the extent of label leakage, with reconstruction accuracies\nreaching well-above 90% on two benchmark datasets, even for trained models.\nMoreover, we find that Local Differential Privacy techniques such as gradient\nnoise and clipping offer only limited protection, as certain attacks still\nreliably infer both majority and minority class labels. We conclude by offering\npractical recommendations for the privacy-aware deployment of federated HAR\nsystems and identify open challenges for future research. Code to reproduce our\nexperiments is publicly available via github.com/mariusbock/leakage_har."}
{"id": "2508.05625", "pdf": "https://arxiv.org/pdf/2508.05625.pdf", "abs": "https://arxiv.org/abs/2508.05625", "title": "How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations", "authors": ["Brandon Jaipersaud", "David Krueger", "Ekdeep Singh Lubana"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have started to demonstrate the ability to\npersuade humans, yet our understanding of how this dynamic transpires is\nlimited. Recent work has used linear probes, lightweight tools for analyzing\nmodel representations, to study various LLM skills such as the ability to model\nuser sentiment and political perspective. Motivated by this, we apply probes to\nstudy persuasion dynamics in natural, multi-turn conversations. We leverage\ninsights from cognitive science to train probes on distinct aspects of\npersuasion: persuasion success, persuadee personality, and persuasion strategy.\nDespite their simplicity, we show that they capture various aspects of\npersuasion at both the sample and dataset levels. For instance, probes can\nidentify the point in a conversation where the persuadee was persuaded or where\npersuasive success generally occurs across the entire dataset. We also show\nthat in addition to being faster than expensive prompting-based approaches,\nprobes can do just as well and even outperform prompting in some settings, such\nas when uncovering persuasion strategy. This suggests probes as a plausible\navenue for studying other complex behaviours such as deception and\nmanipulation, especially in multi-turn settings and large-scale dataset\nanalysis where prompting-based methods would be computationally inefficient."}
{"id": "2506.12496", "pdf": "https://arxiv.org/pdf/2506.12496.pdf", "abs": "https://arxiv.org/abs/2506.12496", "title": "Improving Factuality for Dialogue Response Generation via Graph-Based Knowledge Augmentation", "authors": ["Xiangyan Chen", "Yujian Gan", "Yimeng Gu", "Matthew Purver"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Large Language Models (LLMs) succeed in many natural language processing\ntasks. However, their tendency to hallucinate - generate plausible but\ninconsistent or factually incorrect text - can cause significant problems in\ncertain tasks, including response generation in dialogue. To mitigate this\nissue, we propose two novel graph knowledge-augmented frameworks, Dialogue\nResponse Generation via Textualised Graphs (TG-DRG) and Graph-Aware Dialogue\nResponse Generation (GA-DRG), which combine reasoning-guided dialogue\nreformulation, dialogue sense knowledge selection, and graph-enhanced response\ngeneration to improve the factuality of dialogue responses. To evaluate the\nfactuality of generated responses, we propose a dialogue fact score that\naddresses the limitations of existing fact-score methods in dialogue settings,\nproviding a more reliable assessment of factual consistency. We evaluate our\nmethods using different baselines on the OpendialKG and HybriDialogue datasets.\nOur methods noticeably improve factuality compared to other graph\nknowledge-augmentation baselines, including the state-of-the-art G-retriever,\nachieving improvements of 3.47% on OpendialKG and 3.12% on HybriDialogue in\nterms of dialogue fact score. The code will be released on GitHub."}
{"id": "2508.05628", "pdf": "https://arxiv.org/pdf/2508.05628.pdf", "abs": "https://arxiv.org/abs/2508.05628", "title": "H-Net++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling in Morphologically-Rich Languages", "authors": ["Mehrdad Zakershahrak", "Samira Ghodratnama"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Byte-level language models eliminate fragile tokenizers but face\ncomputational challenges in morphologically-rich languages (MRLs), where words\nspan many bytes. We propose H-NET++, a hierarchical dynamic-chunking model that\nlearns linguistically-informed segmentation through end-to-end training. Key\ninnovations include: (1) a lightweight Transformer context-mixer (1.9M\nparameters) for cross-chunk attention, (2) a two-level latent hyper-prior for\ndocument-level consistency, (3) specialized handling of orthographic artifacts\n(e.g. Persian ZWNJ), and (4) curriculum-based training with staged sequence\nlengths. On a 1.4B-token Persian corpus, H-NET++ achieves state-of-the-art\nresults: 0.159 BPB reduction versus BPE-based GPT-2-fa (12% better\ncompression), 5.4pp gain on ParsGLUE, 53% improved robustness to ZWNJ\ncorruption, and 73.8% F1 on gold morphological boundaries. Our learned chunks\nalign with Persian morphology without explicit supervision, demonstrating that\nhierarchical dynamic chunking provides an effective tokenizer-free solution for\nMRLs while maintaining computational efficiency."}
{"id": "2508.01545", "pdf": "https://arxiv.org/pdf/2508.01545.pdf", "abs": "https://arxiv.org/abs/2508.01545", "title": "Getting out of the Big-Muddy: Escalation of Commitment in LLMs", "authors": ["Emilio Barkett", "Olivia Long", "Paul Kr√∂ger"], "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in autonomous\ndecision-making roles across high-stakes domains. However, since models are\ntrained on human-generated data, they may inherit cognitive biases that\nsystematically distort human judgment, including escalation of commitment,\nwhere decision-makers continue investing in failing courses of action due to\nprior investment. Understanding when LLMs exhibit such biases presents a unique\nchallenge. While these biases are well-documented in humans, it remains unclear\nwhether they manifest consistently in LLMs or require specific triggering\nconditions. This paper investigates this question using a two-stage investment\ntask across four experimental conditions: model as investor, model as advisor,\nmulti-agent deliberation, and compound pressure scenario. Across N = 6,500\ntrials, we find that bias manifestation in LLMs is highly context-dependent. In\nindividual decision-making contexts (Studies 1-2, N = 4,000), LLMs demonstrate\nstrong rational cost-benefit logic with minimal escalation of commitment.\nHowever, multi-agent deliberation reveals a striking hierarchy effect (Study 3,\nN = 500): while asymmetrical hierarchies show moderate escalation rates\n(46.2%), symmetrical peer-based decision-making produces near-universal\nescalation (99.2%). Similarly, when subjected to compound organizational and\npersonal pressures (Study 4, N = 2,000), models exhibit high degrees of\nescalation of commitment (68.95% average allocation to failing divisions).\nThese findings reveal that LLM bias manifestation depends critically on social\nand organizational context rather than being inherent, with significant\nimplications for the deployment of multi-agent systems and unsupervised\noperations where such conditions may emerge naturally."}
{"id": "2508.04714", "pdf": "https://arxiv.org/pdf/2508.04714.pdf", "abs": "https://arxiv.org/abs/2508.04714", "title": "Prescriptive Agents based on Rag for Automated Maintenance (PARAM)", "authors": ["Chitranshu Harbola", "Anupam Purwar"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "eess.SP"], "comment": null, "summary": "Industrial machinery maintenance requires timely intervention to prevent\ncatastrophic failures and optimize operational efficiency. This paper presents\nan integrated Large Language Model (LLM)-based intelligent system for\nprescriptive maintenance that extends beyond traditional anomaly detection to\nprovide actionable maintenance recommendations. Building upon our prior LAMP\nframework for numerical data analysis, we develop a comprehensive solution that\ncombines bearing vibration frequency analysis with multi agentic generation for\nintelligent maintenance planning. Our approach serializes bearing vibration\ndata (BPFO, BPFI, BSF, FTF frequencies) into natural language for LLM\nprocessing, enabling few-shot anomaly detection with high accuracy. The system\nclassifies fault types (inner race, outer race, ball/roller, cage faults) and\nassesses severity levels. A multi-agentic component processes maintenance\nmanuals using vector embeddings and semantic search, while also conducting web\nsearches to retrieve comprehensive procedural knowledge and access up-to-date\nmaintenance practices for more accurate and in-depth recommendations. The\nGemini model then generates structured maintenance recommendations includes\nimmediate actions, inspection checklists, corrective measures, parts\nrequirements, and timeline specifications. Experimental validation in bearing\nvibration datasets demonstrates effective anomaly detection and contextually\nrelevant maintenance guidance. The system successfully bridges the gap between\ncondition monitoring and actionable maintenance planning, providing industrial\npractitioners with intelligent decision support. This work advances the\napplication of LLMs in industrial maintenance, offering a scalable framework\nfor prescriptive maintenance across machinery components and industrial\nsectors."}
{"id": "2508.01674", "pdf": "https://arxiv.org/pdf/2508.01674.pdf", "abs": "https://arxiv.org/abs/2508.01674", "title": "CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions", "authors": ["Tae Soo Kim", "Yoonjoo Lee", "Yoonah Park", "Jiho Kim", "Young-Ho Kim", "Juho Kim"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Accepted to COLM 2025. Project Website: https://cupid.kixlab.org/", "summary": "Personalization of Large Language Models (LLMs) often assumes users hold\nstatic preferences that reflect globally in all tasks. In reality, humans hold\ndynamic preferences that change depending on the context. As users interact\nwith an LLM in various contexts, they naturally reveal their contextual\npreferences, which a model must infer and apply in future contexts to ensure\nalignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated\ninteraction session histories between users and LLM-based chat assistants. In\neach interaction session, the user provides a request in a specific context and\nexpresses their preference through multi-turn feedback. Given a new user\nrequest and prior interaction sessions, our benchmark assesses whether LLMs can\ninfer the preference relevant to this request and generate a response that\nsatisfies this preference. With CUPID, we evaluated 10 open and proprietary\nLLMs, revealing that state-of-the-art LLMs struggle to infer preferences from\nmulti-turn interactions and fail to discern what previous context is relevant\nto a new request -- under 50% precision and 65% recall. Our work highlights the\nneed to advance LLM capabilities for more contextually personalized\ninteractions and proposes CUPID as a resource to drive these improvements."}
{"id": "2508.04830", "pdf": "https://arxiv.org/pdf/2508.04830.pdf", "abs": "https://arxiv.org/abs/2508.04830", "title": "Federal Reserve Communication and the COVID-19 Pandemic", "authors": ["Jonathan Benchimol", "Sophia Kazinnik", "Yossi Saadon"], "categories": ["econ.GN", "cs.CL", "cs.IT", "math.IT", "q-fin.EC", "stat.AP", "stat.ML"], "comment": null, "summary": "In this study, we examine the Federal Reserve's communication strategies\nduring the COVID-19 pandemic, comparing them with communication during previous\nperiods of economic stress. Using specialized dictionaries tailored to\nCOVID-19, unconventional monetary policy (UMP), and financial stability,\ncombined with sentiment analysis and topic modeling techniques, we identify a\ndistinct focus in Fed communication during the pandemic on financial stability,\nmarket volatility, social welfare, and UMP, characterized by notable contextual\nuncertainty. Through comparative analysis, we juxtapose the Fed's communication\nduring the COVID-19 crisis with its responses during the dot-com and global\nfinancial crises, examining content, sentiment, and timing dimensions. Our\nfindings reveal that Fed communication and policy actions were more reactive to\nthe COVID-19 crisis than to previous crises. Additionally, declining sentiment\nrelated to financial stability in interest rate announcements and minutes\nanticipated subsequent accommodative monetary policy decisions. We further\ndocument that communicating about UMP has become the \"new normal\" for the Fed's\nFederal Open Market Committee meeting minutes and Chairman's speeches since the\nGlobal Financial Crisis, reflecting an institutional adaptation in\ncommunication strategy following periods of economic distress. These findings\ncontribute to our understanding of how central bank communication evolves\nduring crises and how communication strategies adapt to exceptional economic\ncircumstances."}
{"id": "2508.02926", "pdf": "https://arxiv.org/pdf/2508.02926.pdf", "abs": "https://arxiv.org/abs/2508.02926", "title": "GrandJury: A Collaborative Machine Learning Model Evaluation Protocol for Dynamic Quality Rubrics", "authors": ["Arthur Cho"], "categories": ["cs.LG", "cs.AI", "cs.HC", "I.2.6; I.2.7"], "comment": "14 pages (incl. arXiv cover), 1 table, code & dataset links inside.\n  Open-source implementation available on PyPI (grandjury package) and GitHub.\n  Dataset available on Hugging Face under CC-BY-4.0 license", "summary": "Generative Machine Learning models have become central to modern systems,\npowering applications in creative writing, summarization, multi-hop reasoning,\nand context-aware dialogue. These models underpin large-scale AI assistants,\nworkflow automation, and autonomous decision-making. In such domains,\nacceptable response is rarely absolute or static, but plural and highly\ncontext-dependent. Yet standard evaluation regimes still rely on static,\nbenchmark-style tests, incentivizing optimization toward leaderboard scores\nrather than alignment with dynamic user needs or evolving realities. GrandJury\nintroduces a formal evaluation protocol combining time-decayed aggregation,\ncomplete traceability, with the support of dynamic, transparent task rubric\nattribution, and multi-rater human judgment. Together, these elements enable\npluralistic, accountable evaluation that captures evolving consensus and\nsurfaces disagreement. We provide an open-source implementation (grandjury PyPI\npackage) and a public collection of Large Language Model (LLM) inference\noutputs to illustrate the need and method. GrandJury provides a new paradigm\nfor AI practitioners when evaluating machine learning outputs without absolute\nground truth."}
{"id": "2508.04846", "pdf": "https://arxiv.org/pdf/2508.04846.pdf", "abs": "https://arxiv.org/abs/2508.04846", "title": "Fine-Tuning Small Language Models (SLMs) for Autonomous Web-based Geographical Information Systems (AWebGIS)", "authors": ["Mahdi Nazari Ashani", "Ali Asghar Alesheikh", "Saba Kazemi", "Kimya Kheirkhah", "Yasin Mohammadi", "Fatemeh Rezaie", "Amir Mahdi Manafi", "Hedieh Zarkesh"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Autonomous web-based geographical information systems (AWebGIS) aim to\nperform geospatial operations from natural language input, providing intuitive,\nintelligent, and hands-free interaction. However, most current solutions rely\non cloud-based large language models (LLMs), which require continuous internet\naccess and raise users' privacy and scalability issues due to centralized\nserver processing. This study compares three approaches to enabling AWebGIS:\n(1) a fully-automated online method using cloud-based LLMs (e.g., Cohere); (2)\na semi-automated offline method using classical machine learning classifiers\nsuch as support vector machine and random forest; and (3) a fully autonomous\noffline (client-side) method based on a fine-tuned small language model (SLM),\nspecifically T5-small model, executed in the client's web browser. The third\napproach, which leverages SLMs, achieved the highest accuracy among all\nmethods, with an exact matching accuracy of 0.93, Levenshtein similarity of\n0.99, and recall-oriented understudy for gisting evaluation ROUGE-1 and ROUGE-L\nscores of 0.98. Crucially, this client-side computation strategy reduces the\nload on backend servers by offloading processing to the user's device,\neliminating the need for server-based inference. These results highlight the\nfeasibility of browser-executable models for AWebGIS solutions."}
{"id": "2508.04913", "pdf": "https://arxiv.org/pdf/2508.04913.pdf", "abs": "https://arxiv.org/abs/2508.04913", "title": "Advancing Hate Speech Detection with Transformers: Insights from the MetaHate", "authors": ["Santosh Chapagain", "Shah Muhammad Hamdi", "Soukaina Filali Boubrahimi"], "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to the Deviant Dynamics in Digital Spaces workshop at ASONAM\n  2025", "summary": "Hate speech is a widespread and harmful form of online discourse,\nencompassing slurs and defamatory posts that can have serious social,\npsychological, and sometimes physical impacts on targeted individuals and\ncommunities. As social media platforms such as X (formerly Twitter), Facebook,\nInstagram, Reddit, and others continue to facilitate widespread communication,\nthey also become breeding grounds for hate speech, which has increasingly been\nlinked to real-world hate crimes. Addressing this issue requires the\ndevelopment of robust automated methods to detect hate speech in diverse social\nmedia environments. Deep learning approaches, such as vanilla recurrent neural\nnetworks (RNNs), long short-term memory (LSTM), and convolutional neural\nnetworks (CNNs), have achieved good results, but are often limited by issues\nsuch as long-term dependencies and inefficient parallelization. This study\nrepresents the comprehensive exploration of transformer-based models for hate\nspeech detection using the MetaHate dataset--a meta-collection of 36 datasets\nwith 1.2 million social media samples. We evaluate multiple state-of-the-art\ntransformer models, including BERT, RoBERTa, GPT-2, and ELECTRA, with\nfine-tuned ELECTRA achieving the highest performance (F1 score: 0.8980). We\nalso analyze classification errors, revealing challenges with sarcasm, coded\nlanguage, and label noise."}
{"id": "2508.04915", "pdf": "https://arxiv.org/pdf/2508.04915.pdf", "abs": "https://arxiv.org/abs/2508.04915", "title": "ConfAgents: A Conformal-Guided Multi-Agent Framework for Cost-Efficient Medical Diagnosis", "authors": ["Huiya Zhao", "Yinghao Zhu", "Zixiang Wang", "Yasha Wang", "Junyi Gao", "Liantao Ma"], "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": "Code: https://github.com/PKU-AICare/ConfAgents", "summary": "The efficacy of AI agents in healthcare research is hindered by their\nreliance on static, predefined strategies. This creates a critical limitation:\nagents can become better tool-users but cannot learn to become better strategic\nplanners, a crucial skill for complex domains like healthcare. We introduce\nHealthFlow, a self-evolving AI agent that overcomes this limitation through a\nnovel meta-level evolution mechanism. HealthFlow autonomously refines its own\nhigh-level problem-solving policies by distilling procedural successes and\nfailures into a durable, strategic knowledge base. To anchor our research and\nfacilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark\nfeaturing complex, realistic health data analysis tasks derived from\npeer-reviewed clinical research. Our comprehensive experiments demonstrate that\nHealthFlow's self-evolving approach significantly outperforms state-of-the-art\nagent frameworks. This work marks a necessary shift from building better\ntool-users to designing smarter, self-evolving task-managers, paving the way\nfor more autonomous and effective AI for scientific discovery."}
{"id": "2508.04946", "pdf": "https://arxiv.org/pdf/2508.04946.pdf", "abs": "https://arxiv.org/abs/2508.04946", "title": "REINA: Regularized Entropy Information-Based Loss for Efficient Simultaneous Speech Translation", "authors": ["Nameer Hirschkind", "Joseph Liu", "Mahesh Kumar Nandwana", "Xiao Yu"], "categories": ["cs.LG", "cs.CL", "eess.AS"], "comment": null, "summary": "Simultaneous Speech Translation (SimulST) systems stream in audio while\nsimultaneously emitting translated text or speech. Such systems face the\nsignificant challenge of balancing translation quality and latency. We\nintroduce a strategy to optimize this tradeoff: wait for more input only if you\ngain information by doing so. Based on this strategy, we present Regularized\nEntropy INformation Adaptation (REINA), a novel loss to train an adaptive\npolicy using an existing non-streaming translation model. We derive REINA from\ninformation theory principles and show that REINA helps push the reported\nPareto frontier of the latency/quality tradeoff over prior works. Utilizing\nREINA, we train a SimulST model on French, Spanish and German, both from and\ninto English. Training on only open source or synthetically generated data, we\nachieve state-of-the-art (SOTA) streaming results for models of comparable\nsize. We also introduce a metric for streaming efficiency, quantitatively\nshowing REINA improves the latency/quality trade-off by as much as 21% compared\nto prior approaches, normalized against non-streaming baseline BLEU scores."}
{"id": "2508.05004", "pdf": "https://arxiv.org/pdf/2508.05004.pdf", "abs": "https://arxiv.org/abs/2508.05004", "title": "R-Zero: Self-Evolving Reasoning LLM from Zero Data", "authors": ["Chengsong Huang", "Wenhao Yu", "Xiaoyang Wang", "Hongming Zhang", "Zongxia Li", "Ruosen Li", "Jiaxin Huang", "Haitao Mi", "Dong Yu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Self-evolving Large Language Models (LLMs) offer a scalable path toward\nsuper-intelligence by autonomously generating, refining, and learning from\ntheir own experiences. However, existing methods for training such models still\nrely heavily on vast human-curated tasks and labels, typically via fine-tuning\nor reinforcement learning, which poses a fundamental bottleneck to advancing AI\nsystems toward capabilities beyond human intelligence. To overcome this\nlimitation, we introduce R-Zero, a fully autonomous framework that generates\nits own training data from scratch. Starting from a single base LLM, R-Zero\ninitializes two independent models with distinct roles, a Challenger and a\nSolver. These models are optimized separately and co-evolve through\ninteraction: the Challenger is rewarded for proposing tasks near the edge of\nthe Solver capability, and the Solver is rewarded for solving increasingly\nchallenging tasks posed by the Challenger. This process yields a targeted,\nself-improving curriculum without any pre-existing tasks and labels.\nEmpirically, R-Zero substantially improves reasoning capability across\ndifferent backbone LLMs, e.g., boosting the Qwen3-4B-Base by +6.49 on\nmath-reasoning benchmarks and +7.54 on general-domain reasoning benchmarks."}
{"id": "2508.05009", "pdf": "https://arxiv.org/pdf/2508.05009.pdf", "abs": "https://arxiv.org/abs/2508.05009", "title": "Can Large Language Models Integrate Spatial Data? Empirical Insights into Reasoning Strengths and Computational Weaknesses", "authors": ["Bin Han", "Robert Wolfe", "Anat Caspi", "Bill Howe"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We explore the application of large language models (LLMs) to empower domain\nexperts in integrating large, heterogeneous, and noisy urban spatial datasets.\nTraditional rule-based integration methods are unable to cover all edge cases,\nrequiring manual verification and repair. Machine learning approaches require\ncollecting and labeling of large numbers of task-specific samples. In this\nstudy, we investigate the potential of LLMs for spatial data integration. Our\nanalysis first considers how LLMs reason about environmental spatial\nrelationships mediated by human experience, such as between roads and\nsidewalks. We show that while LLMs exhibit spatial reasoning capabilities, they\nstruggle to connect the macro-scale environment with the relevant computational\ngeometry tasks, often producing logically incoherent responses. But when\nprovided relevant features, thereby reducing dependence on spatial reasoning,\nLLMs are able to generate high-performing results. We then adapt a\nreview-and-refine method, which proves remarkably effective in correcting\nerroneous initial responses while preserving accurate responses. We discuss\npractical implications of employing LLMs for spatial data integration in\nreal-world contexts and outline future research directions, including\npost-training, multi-modal integration methods, and support for diverse data\nformats. Our findings position LLMs as a promising and flexible alternative to\ntraditional rule-based heuristics, advancing the capabilities of adaptive\nspatial data integration."}
{"id": "2508.05012", "pdf": "https://arxiv.org/pdf/2508.05012.pdf", "abs": "https://arxiv.org/abs/2508.05012", "title": "Making Prompts First-Class Citizens for Adaptive LLM Pipelines", "authors": ["Ugur Cetintemel", "Shu Chen", "Alexander W. Lee", "Deepti Raghavan"], "categories": ["cs.DB", "cs.AI", "cs.CL"], "comment": null, "summary": "Modern LLM pipelines increasingly resemble data-centric systems: they\nretrieve external context, compose intermediate outputs, validate results, and\nadapt based on runtime feedback. Yet, the central element guiding this process\n-- the prompt -- remains a brittle, opaque string, disconnected from the\nsurrounding dataflow. This disconnect limits reuse, optimization, and runtime\ncontrol.\n  In this paper, we describe our vision and an initial design for SPEAR, a\nlanguage and runtime that fills this prompt management gap by making prompts\nstructured, adaptive, and first-class components of the execution model. SPEAR\nenables (1) runtime prompt refinement -- modifying prompts dynamically in\nresponse to execution-time signals such as confidence, latency, or missing\ncontext; and (2) structured prompt management -- organizing prompt fragments\ninto versioned views with support for introspection and logging.\n  SPEAR defines a prompt algebra that governs how prompts are constructed and\nadapted within a pipeline. It supports multiple refinement modes (manual,\nassisted, and automatic), giving developers a balance between control and\nautomation. By treating prompt logic as structured data, SPEAR enables\noptimizations such as operator fusion, prefix caching, and view reuse.\nPreliminary experiments quantify the behavior of different refinement modes\ncompared to static prompts and agentic retries, as well as the impact of\nprompt-level optimizations such as operator fusion."}
{"id": "2508.05064", "pdf": "https://arxiv.org/pdf/2508.05064.pdf", "abs": "https://arxiv.org/abs/2508.05064", "title": "A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding", "authors": ["Mahmoud Chick Zaouali", "Todd Charter", "Yehor Karpichev", "Brandon Haworth", "Homayoun Najjjaran"], "categories": ["cs.GR", "cs.CL", "cs.CV"], "comment": null, "summary": "Gaussian Splatting has rapidly emerged as a transformative technique for\nreal-time 3D scene representation, offering a highly efficient and expressive\nalternative to Neural Radiance Fields (NeRF). Its ability to render complex\nscenes with high fidelity has enabled progress across domains such as scene\nreconstruction, robotics, and interactive content creation. More recently, the\nintegration of Large Language Models (LLMs) and language embeddings into\nGaussian Splatting pipelines has opened new possibilities for text-conditioned\ngeneration, editing, and semantic scene understanding. Despite these advances,\na comprehensive overview of this emerging intersection has been lacking. This\nsurvey presents a structured review of current research efforts that combine\nlanguage guidance with 3D Gaussian Splatting, detailing theoretical\nfoundations, integration strategies, and real-world use cases. We highlight key\nlimitations such as computational bottlenecks, generalizability, and the\nscarcity of semantically annotated 3D Gaussian data and outline open challenges\nand future directions for advancing language-guided 3D scene understanding\nusing Gaussian Splatting."}
{"id": "2508.05081", "pdf": "https://arxiv.org/pdf/2508.05081.pdf", "abs": "https://arxiv.org/abs/2508.05081", "title": "Cognitive Duality for Adaptive Web Agents", "authors": ["Jiarun Liu", "Chunhong Zhang", "Zheng Hu"], "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Web navigation represents a critical and challenging domain for evaluating\nartificial general intelligence (AGI), demanding complex decision-making within\nhigh-entropy, dynamic environments with combinatorially explosive action\nspaces. Current approaches to building autonomous web agents either focus on\noffline imitation learning or online exploration, but rarely integrate both\nparadigms effectively. Inspired by the dual-process theory of human cognition,\nwe derive a principled decomposition into fast System 1 and slow System 2\ncognitive processes. This decomposition provides a unifying perspective on\nexisting web agent methodologies, bridging the gap between offline learning of\nintuitive reactive behaviors and online acquisition of deliberative planning\ncapabilities. We implement this framework in CogniWeb, a modular agent\narchitecture that adaptively toggles between fast intuitive processing and\ndeliberate reasoning based on task complexity. Our evaluation on WebArena\ndemonstrates that CogniWeb achieves competitive performance (43.96% success\nrate) while maintaining significantly higher efficiency (75% reduction in token\nusage)."}
{"id": "2508.05087", "pdf": "https://arxiv.org/pdf/2508.05087.pdf", "abs": "https://arxiv.org/abs/2508.05087", "title": "JPS: Jailbreak Multimodal Large Language Models with Collaborative Visual Perturbation and Textual Steering", "authors": ["Renmiao Chen", "Shiyao Cui", "Xuancheng Huang", "Chengwei Pan", "Victor Shea-Jay Huang", "QingLin Zhang", "Xuan Ouyang", "Zhexin Zhang", "Hongning Wang", "Minlie Huang"], "categories": ["cs.MM", "cs.AI", "cs.CL", "cs.CR", "I.2.7; K.4.1; K.6.5"], "comment": "10 pages, 3 tables, 2 figures, to appear in the Proceedings of the\n  33rd ACM International Conference on Multimedia (MM '25)", "summary": "Jailbreak attacks against multimodal large language Models (MLLMs) are a\nsignificant research focus. Current research predominantly focuses on\nmaximizing attack success rate (ASR), often overlooking whether the generated\nresponses actually fulfill the attacker's malicious intent. This oversight\nfrequently leads to low-quality outputs that bypass safety filters but lack\nsubstantial harmful content. To address this gap, we propose JPS,\n\\underline{J}ailbreak MLLMs with collaborative visual \\underline{P}erturbation\nand textual \\underline{S}teering, which achieves jailbreaks via corporation of\nvisual image and textually steering prompt. Specifically, JPS utilizes\ntarget-guided adversarial image perturbations for effective safety bypass,\ncomplemented by \"steering prompt\" optimized via a multi-agent system to\nspecifically guide LLM responses fulfilling the attackers' intent. These visual\nand textual components undergo iterative co-optimization for enhanced\nperformance. To evaluate the quality of attack outcomes, we propose the\nMalicious Intent Fulfillment Rate (MIFR) metric, assessed using a\nReasoning-LLM-based evaluator. Our experiments show JPS sets a new\nstate-of-the-art in both ASR and MIFR across various MLLMs and benchmarks, with\nanalyses confirming its efficacy. Codes are available at\n\\href{https://github.com/thu-coai/JPS}{https://github.com/thu-coai/JPS}.\n\\color{warningcolor}{Warning: This paper contains potentially sensitive\ncontents.}"}
{"id": "2508.05118", "pdf": "https://arxiv.org/pdf/2508.05118.pdf", "abs": "https://arxiv.org/abs/2508.05118", "title": "Exploring Superior Function Calls via Reinforcement Learning", "authors": ["Bingguang Hao", "Maolin Wang", "Zengzhuang Xu", "Yicheng Chen", "Cunyin Peng", "Jinjie GU", "Chenyi Zhuang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Function calling capabilities are crucial for deploying Large Language Models\nin real-world applications, yet current training approaches fail to develop\nrobust reasoning strategies. Supervised fine-tuning produces models that rely\non superficial pattern matching, while standard reinforcement learning methods\nstruggle with the complex action space of structured function calls. We present\na novel reinforcement learning framework designed to enhance group relative\npolicy optimization through strategic entropy based exploration specifically\ntailored for function calling tasks. Our approach addresses three critical\nchallenges in function calling: insufficient exploration during policy\nlearning, lack of structured reasoning in chain-of-thought generation, and\ninadequate verification of parameter extraction. Our two-stage data preparation\npipeline ensures high-quality training samples through iterative LLM evaluation\nand abstract syntax tree validation. Extensive experiments on the Berkeley\nFunction Calling Leaderboard demonstrate that this framework achieves\nstate-of-the-art performance among open-source models with 86.02\\% overall\naccuracy, outperforming standard GRPO by up to 6\\% on complex multi-function\nscenarios. Notably, our method shows particularly strong improvements on\ncode-pretrained models, suggesting that structured language generation\ncapabilities provide an advantageous starting point for reinforcement learning\nin function calling tasks. We will release all the code, models and dataset to\nbenefit the community."}
{"id": "2508.05129", "pdf": "https://arxiv.org/pdf/2508.05129.pdf", "abs": "https://arxiv.org/abs/2508.05129", "title": "Navigating Through Paper Flood: Advancing LLM-based Paper Evaluation through Domain-Aware Retrieval and Latent Reasoning", "authors": ["Wuqiang Zheng", "Yiyan Xu", "Xinyu Lin", "Chongming Gao", "Wenjie Wang", "Fuli Feng"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "With the rapid and continuous increase in academic publications, identifying\nhigh-quality research has become an increasingly pressing challenge. While\nrecent methods leveraging Large Language Models (LLMs) for automated paper\nevaluation have shown great promise, they are often constrained by outdated\ndomain knowledge and limited reasoning capabilities. In this work, we present\nPaperEval, a novel LLM-based framework for automated paper evaluation that\naddresses these limitations through two key components: 1) a domain-aware paper\nretrieval module that retrieves relevant concurrent work to support\ncontextualized assessments of novelty and contributions, and 2) a latent\nreasoning mechanism that enables deep understanding of complex motivations and\nmethodologies, along with comprehensive comparison against concurrently related\nwork, to support more accurate and reliable evaluation. To guide the reasoning\nprocess, we introduce a progressive ranking optimization strategy that\nencourages the LLM to iteratively refine its predictions with an emphasis on\nrelative comparison. Experiments on two datasets demonstrate that PaperEval\nconsistently outperforms existing methods in both academic impact and paper\nquality evaluation. In addition, we deploy PaperEval in a real-world paper\nrecommendation system for filtering high-quality papers, which has gained\nstrong engagement on social media -- amassing over 8,000 subscribers and\nattracting over 10,000 views for many filtered high-quality papers --\ndemonstrating the practical effectiveness of PaperEval."}
{"id": "2508.05149", "pdf": "https://arxiv.org/pdf/2508.05149.pdf", "abs": "https://arxiv.org/abs/2508.05149", "title": "Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the Impact of Pretraining on High-Resource Languages", "authors": ["Seraphina Fong", "Marco Matassoni", "Alessio Brutti"], "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": "Accepted at Interspeech 2025. 5 pages, 2 figures, 3 tables", "summary": "Large language models (LLMs) have demonstrated potential in handling spoken\ninputs for high-resource languages, reaching state-of-the-art performance in\nvarious tasks. However, their applicability is still less explored in\nlow-resource settings. This work investigates the use of Speech LLMs for\nlow-resource Automatic Speech Recognition using the SLAM-ASR framework, where a\ntrainable lightweight projector connects a speech encoder and a LLM. Firstly,\nwe assess training data volume requirements to match Whisper-only performance,\nre-emphasizing the challenges of limited data. Secondly, we show that\nleveraging mono- or multilingual projectors pretrained on high-resource\nlanguages reduces the impact of data scarcity, especially with small training\nsets. Using multilingual LLMs (EuroLLM, Salamandra) with\nwhisper-large-v3-turbo, we evaluate performance on several public benchmarks,\nproviding insights for future research on optimizing Speech LLMs for\nlow-resource languages and multilinguality."}
{"id": "2508.05165", "pdf": "https://arxiv.org/pdf/2508.05165.pdf", "abs": "https://arxiv.org/abs/2508.05165", "title": "Aligning LLMs on a Budget: Inference-Time Alignment with Heuristic Reward Models", "authors": ["Mason Nakamura", "Saaduddin Mahmud", "Kyle H. Wray", "Hamed Zamani", "Shlomo Zilberstein"], "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7; I.2.6; I.2.8"], "comment": null, "summary": "Aligning LLMs with user preferences is crucial for real-world use but often\nrequires costly fine-tuning or expensive inference, forcing trade-offs between\nalignment quality and computational cost. Existing inference-time methods\ntypically ignore this balance, focusing solely on the optimized policy's\nperformance. We propose HIA (Heuristic-Guided Inference-time Alignment), a\ntuning-free, black-box-compatible approach that uses a lightweight prompt\noptimizer, heuristic reward models, and two-stage filtering to reduce inference\ncalls while preserving alignment quality. On real-world prompt datasets,\nHelpSteer and ComPRed, HIA outperforms best-of-N sampling, beam search, and\ngreedy search baselines in multi-objective, goal-conditioned tasks under the\nsame inference budget. We also find that HIA is effective under low-inference\nbudgets with as little as one or two response queries, offering a practical\nsolution for scalable, personalized LLM deployment."}
{"id": "2508.05170", "pdf": "https://arxiv.org/pdf/2508.05170.pdf", "abs": "https://arxiv.org/abs/2508.05170", "title": "Posterior-GRPO: Rewarding Reasoning Processes in Code Generation", "authors": ["Lishui Fan", "Yu Zhang", "Mouxiang Chen", "Zhongxin Liu"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) has significantly advanced code generation for\nlarge language models (LLMs). However, current paradigms rely on outcome-based\nrewards from test cases, neglecting the quality of the intermediate reasoning\nprocess. While supervising the reasoning process directly is a promising\ndirection, it is highly susceptible to reward hacking, where the policy model\nlearns to exploit the reasoning reward signal without improving final outcomes.\nTo address this, we introduce a unified framework that can effectively\nincorporate the quality of the reasoning process during RL. First, to enable\nreasoning evaluation, we develop LCB-RB, a benchmark comprising preference\npairs of superior and inferior reasoning processes. Second, to accurately score\nreasoning quality, we introduce an Optimized-Degraded based (OD-based) method\nfor reward model training. This method generates high-quality preference pairs\nby systematically optimizing and degrading initial reasoning paths along\ncurated dimensions of reasoning quality, such as factual accuracy, logical\nrigor, and coherence. A 7B parameter reward model with this method achieves\nstate-of-the-art (SOTA) performance on LCB-RB and generalizes well to other\nbenchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method\nthat conditions process-based rewards on task success. By selectively applying\nrewards to the reasoning processes of only successful outcomes, P-GRPO\neffectively mitigates reward hacking and aligns the model's internal reasoning\nwith final code correctness. A 7B parameter model with P-GRPO achieves superior\nperformance across diverse code generation tasks, outperforming outcome-only\nbaselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further\ndemonstrate the generalizability of our approach by extending it to\nmathematical tasks. Our models, dataset, and code are publicly available."}
{"id": "2508.05197", "pdf": "https://arxiv.org/pdf/2508.05197.pdf", "abs": "https://arxiv.org/abs/2508.05197", "title": "QA-Dragon: Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question Answering", "authors": ["Zhuohang Jiang", "Pangjing Wu", "Xu Yuan", "Wenqi Fan", "Qing Li"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "The source code for our system is released in\n  https://github.com/jzzzzh/QA-Dragon", "summary": "Retrieval-Augmented Generation (RAG) has been introduced to mitigate\nhallucinations in Multimodal Large Language Models (MLLMs) by incorporating\nexternal knowledge into the generation process, and it has become a widely\nadopted approach for knowledge-intensive Visual Question Answering (VQA).\nHowever, existing RAG methods typically retrieve from either text or images in\nisolation, limiting their ability to address complex queries that require\nmulti-hop reasoning or up-to-date factual knowledge. To address this\nlimitation, we propose QA-Dragon, a Query-Aware Dynamic RAG System for\nKnowledge-Intensive VQA. Specifically, QA-Dragon introduces a domain router to\nidentify the query's subject domain for domain-specific reasoning, along with a\nsearch router that dynamically selects optimal retrieval strategies. By\norchestrating both text and image search agents in a hybrid setup, our system\nsupports multimodal, multi-turn, and multi-hop reasoning, enabling it to tackle\ncomplex VQA tasks effectively. We evaluate our QA-Dragon on the Meta CRAG-MM\nChallenge at KDD Cup 2025, where it significantly enhances the reasoning\nperformance of base models under challenging scenarios. Our framework achieves\nsubstantial improvements in both answer accuracy and knowledge overlap scores,\noutperforming baselines by 5.06% on the single-source task, 6.35% on the\nmulti-source task, and 5.03% on the multi-turn task."}
{"id": "2508.05201", "pdf": "https://arxiv.org/pdf/2508.05201.pdf", "abs": "https://arxiv.org/abs/2508.05201", "title": "FAITH: A Framework for Assessing Intrinsic Tabular Hallucinations in finance", "authors": ["Mengao Zhang", "Jiayu Fu", "Tanya Warrier", "Yuwen Wang", "Tianhui Tan", "Ke-wei Huang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "9 pages", "summary": "Hallucination remains a critical challenge for deploying Large Language\nModels (LLMs) in finance. Accurate extraction and precise calculation from\ntabular data are essential for reliable financial analysis, since even minor\nnumerical errors can undermine decision-making and regulatory compliance.\nFinancial applications have unique requirements, often relying on\ncontext-dependent, numerical, and proprietary tabular data that existing\nhallucination benchmarks rarely capture. In this study, we develop a rigorous\nand scalable framework for evaluating intrinsic hallucinations in financial\nLLMs, conceptualized as a context-aware masked span prediction task over\nreal-world financial documents. Our main contributions are: (1) a novel,\nautomated dataset creation paradigm using a masking strategy; (2) a new\nhallucination evaluation dataset derived from S&P 500 annual reports; and (3) a\ncomprehensive evaluation of intrinsic hallucination patterns in\nstate-of-the-art LLMs on financial tabular data. Our work provides a robust\nmethodology for in-house LLM evaluation and serves as a critical step toward\nbuilding more trustworthy and reliable financial Generative AI systems."}
{"id": "2508.05266", "pdf": "https://arxiv.org/pdf/2508.05266.pdf", "abs": "https://arxiv.org/abs/2508.05266", "title": "Understanding and Mitigating Errors of LLM-Generated RTL Code", "authors": ["Jiazheng Zhang", "Cheng Liu", "Huawei Li"], "categories": ["cs.AR", "cs.CL", "cs.LG"], "comment": "14 pages, 26 figures", "summary": "Despite the promising potential of large language model (LLM) based\nregister-transfer-level (RTL) code generation, the overall success rate remains\nunsatisfactory. Errors arise from various factors, with limited understanding\nof specific failure causes hindering improvement. To address this, we conduct a\ncomprehensive error analysis and manual categorization. Our findings reveal\nthat most errors stem not from LLM reasoning limitations, but from insufficient\nRTL programming knowledge, poor understanding of circuit concepts, ambiguous\ndesign descriptions, or misinterpretation of complex multimodal inputs.\nLeveraging in-context learning, we propose targeted error correction\ntechniques. Specifically, we construct a domain-specific knowledge base and\nemploy retrieval-augmented generation (RAG) to supply necessary RTL knowledge.\nTo mitigate ambiguity errors, we introduce design description rules and\nimplement a rule-checking mechanism. For multimodal misinterpretation, we\nintegrate external tools to convert inputs into LLM-compatible meta-formats.\nFor remaining errors, we adopt an iterative debugging loop (simulation-error\nlocalization-correction). Integrating these techniques into an LLM-based\nframework significantly improves performance. We incorporate these error\ncorrection techniques into a foundational LLM-based RTL code generation\nframework, resulting in significantly improved performance. Experimental\nresults show that our enhanced framework achieves 91.0\\% accuracy on the\nVerilogEval benchmark, surpassing the baseline code generation approach by\n32.7\\%, demonstrating the effectiveness of our methods."}
{"id": "2508.05311", "pdf": "https://arxiv.org/pdf/2508.05311.pdf", "abs": "https://arxiv.org/abs/2508.05311", "title": "A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents", "authors": ["Andrew Kiruluta"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We propose a hybrid architecture that integrates decision tree-based symbolic\nreasoning with the generative capabilities of large language models (LLMs)\nwithin a coordinated multi-agent framework. Unlike prior approaches that\nloosely couple symbolic and neural modules, our design embeds decision trees\nand random forests as callable oracles within a unified reasoning system.\nTree-based modules enable interpretable rule inference and causal logic, while\nLLM agents handle abductive reasoning, generalization, and interactive\nplanning. A central orchestrator maintains belief state consistency and\nmediates communication across agents and external tools, enabling reasoning\nover both structured and unstructured inputs.\n  The system achieves strong performance on reasoning benchmarks. On\n\\textit{ProofWriter}, it improves entailment consistency by +7.2\\% through\nlogic-grounded tree validation. On GSM8k, it achieves +5.3\\% accuracy gains in\nmultistep mathematical problems via symbolic augmentation. On \\textit{ARC}, it\nboosts abstraction accuracy by +6.0\\% through integration of symbolic oracles.\nApplications in clinical decision support and scientific discovery show how the\nsystem encodes domain rules symbolically while leveraging LLMs for contextual\ninference and hypothesis generation. This architecture offers a robust,\ninterpretable, and extensible solution for general-purpose neuro-symbolic\nreasoning."}
{"id": "2508.05464", "pdf": "https://arxiv.org/pdf/2508.05464.pdf", "abs": "https://arxiv.org/abs/2508.05464", "title": "Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?", "authors": ["Matteo Prandi", "Vincenzo Suriani", "Federico Pierucci", "Marcello Galisai", "Daniele Nardi", "Piercosma Bisconti"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The rapid advancement of General Purpose AI (GPAI) models necessitates robust\nevaluation frameworks, especially with emerging regulations like the EU AI Act\nand its associated Code of Practice (CoP). Current AI evaluation practices\ndepend heavily on established benchmarks, but these tools were not designed to\nmeasure the systemic risks that are the focus of the new regulatory landscape.\nThis research addresses the urgent need to quantify this \"benchmark-regulation\ngap.\" We introduce Bench-2-CoP, a novel, systematic framework that uses\nvalidated LLM-as-judge analysis to map the coverage of 194,955 questions from\nwidely-used benchmarks against the EU AI Act's taxonomy of model capabilities\nand propensities. Our findings reveal a profound misalignment: the evaluation\necosystem is overwhelmingly focused on a narrow set of behavioral propensities,\nsuch as \"Tendency to hallucinate\" (53.7% of the corpus) and \"Discriminatory\nbias\" (28.9%), while critical functional capabilities are dangerously\nneglected. Crucially, capabilities central to loss-of-control scenarios,\nincluding evading human oversight, self-replication, and autonomous AI\ndevelopment, receive zero coverage in the entire benchmark corpus. This\ntranslates to a near-total evaluation gap for systemic risks like \"Loss of\nControl\" (0.4% coverage) and \"Cyber Offence\" (0.8% coverage). This study\nprovides the first comprehensive, quantitative analysis of this gap, offering\ncritical insights for policymakers to refine the CoP and for developers to\nbuild the next generation of evaluation tools, ultimately fostering safer and\nmore compliant AI."}
{"id": "2508.05474", "pdf": "https://arxiv.org/pdf/2508.05474.pdf", "abs": "https://arxiv.org/abs/2508.05474", "title": "Can Large Language Models Generate Effective Datasets for Emotion Recognition in Conversations?", "authors": ["Burak Can Kaplan", "Hugo Cesar De Castro Carneiro", "Stefan Wermter"], "categories": ["cs.AI", "cs.CL"], "comment": "8 pages, 4 figures", "summary": "Emotion recognition in conversations (ERC) focuses on identifying emotion\nshifts within interactions, representing a significant step toward advancing\nmachine intelligence. However, ERC data remains scarce, and existing datasets\nface numerous challenges due to their highly biased sources and the inherent\nsubjectivity of soft labels. Even though Large Language Models (LLMs) have\ndemonstrated their quality in many affective tasks, they are typically\nexpensive to train, and their application to ERC tasks--particularly in data\ngeneration--remains limited. To address these challenges, we employ a small,\nresource-efficient, and general-purpose LLM to synthesize ERC datasets with\ndiverse properties, supplementing the three most widely used ERC benchmarks. We\ngenerate six novel datasets, with two tailored to enhance each benchmark. We\nevaluate the utility of these datasets to (1) supplement existing datasets for\nERC classification, and (2) analyze the effects of label imbalance in ERC. Our\nexperimental results indicate that ERC classifier models trained on the\ngenerated datasets exhibit strong robustness and consistently achieve\nstatistically significant performance improvements on existing ERC benchmarks."}
{"id": "2508.05502", "pdf": "https://arxiv.org/pdf/2508.05502.pdf", "abs": "https://arxiv.org/abs/2508.05502", "title": "MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs", "authors": ["Yufei Gao", "Jiaying Fei", "Nuo Chen", "Ruirui Chen", "Guohang Yan", "Yunshi Lan", "Botian Shi"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable performance in\nhigh-resource languages. However, their effectiveness diminishes significantly\nin the contexts of low-resource languages. Current multilingual enhancement\nmethods are often limited to text modality or rely solely on machine\ntranslation. While such approaches help models acquire basic linguistic\ncapabilities and produce \"thin descriptions\", they neglect the importance of\nmultimodal informativeness and cultural groundedness, both of which are crucial\nfor serving low-resource language users effectively. To bridge this gap, in\nthis study, we identify two significant objectives for a truly effective MLLM\nin low-resource language settings, namely 1) linguistic capability and 2)\ncultural groundedness, placing special emphasis on cultural awareness. To\nachieve these dual objectives, we propose a dual-source strategy that guides\nthe collection of data tailored to each goal, sourcing native web alt-text for\nculture and MLLM-generated captions for linguistics. As a concrete\nimplementation, we introduce MELLA, a multimodal, multilingual dataset.\nExperiment results show that after fine-tuning on MELLA, there is a general\nperformance improvement for the eight languages on various MLLM backbones, with\nmodels producing \"thick descriptions\". We verify that the performance gains are\nfrom both cultural knowledge enhancement and linguistic capability enhancement.\nOur dataset can be found at https://opendatalab.com/applyMultilingualCorpus."}
{"id": "2508.05535", "pdf": "https://arxiv.org/pdf/2508.05535.pdf", "abs": "https://arxiv.org/abs/2508.05535", "title": "Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation", "authors": ["Albert Yu", "Chengshu Li", "Luca Macesanu", "Arnav Balaji", "Ruchira Ray", "Raymond Mooney", "Roberto Mart√≠n-Mart√≠n"], "categories": ["cs.RO", "cs.CL", "cs.HC", "cs.LG", "cs.MA", "I.2.9; I.2.7; I.2.6"], "comment": "Project website at https://robin-lab.cs.utexas.edu/MicoBot/", "summary": "Effective robotic systems for long-horizon human-robot collaboration must\nadapt to a wide range of human partners, whose physical behavior, willingness\nto assist, and understanding of the robot's capabilities may change over time.\nThis demands a tightly coupled communication loop that grants both agents the\nflexibility to propose, accept, or decline requests as they coordinate toward\ncompleting the task effectively. We apply a Mixed-Initiative dialog paradigm to\nCollaborative human-roBot teaming and propose MICoBot, a system that handles\nthe common scenario where both agents, using natural language, take initiative\nin formulating, accepting, or rejecting proposals on who can best complete\ndifferent steps of a task. To handle diverse, task-directed dialog, and find\nsuccessful collaborative strategies that minimize human effort, MICoBot makes\ndecisions at three levels: (1) a meta-planner considers human dialog to\nformulate and code a high-level collaboration strategy, (2) a planner optimally\nallocates the remaining steps to either agent based on the robot's capabilities\n(measured by a simulation-pretrained affordance model) and the human's\nestimated availability to help, and (3) an action executor decides the\nlow-level actions to perform or words to say to the human. Our extensive\nevaluations in simulation and real-world -- on a physical robot with 18 unique\nhuman participants over 27 hours -- demonstrate the ability of our method to\neffectively collaborate with diverse human users, yielding significantly\nimproved task success and user experience than a pure LLM baseline and other\nagent allocation models. See additional videos and materials at\nhttps://robin-lab.cs.utexas.edu/MicoBot/."}
{"id": "2508.05554", "pdf": "https://arxiv.org/pdf/2508.05554.pdf", "abs": "https://arxiv.org/abs/2508.05554", "title": "SPGISpeech 2.0: Transcribed multi-speaker financial audio for speaker-tagged transcription", "authors": ["Raymond Grossman", "Taejin Park", "Kunal Dhawan", "Andrew Titus", "Sophia Zhi", "Yulia Shchadilova", "Weiqing Wang", "Jagadeesh Balam", "Boris Ginsburg"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "To be presented at Interspeech 2025", "summary": "We introduce SPGISpeech 2.0, a dataset suitable for speaker-tagged\ntranscription in the financial domain. SPGISpeech 2.0 improves the diversity of\napplicable modeling tasks while maintaining the core characteristic of the\noriginal SPGISpeech dataset: audio snippets and their corresponding fully\nformatted text transcriptions, usable for end-to-end automatic speech\nrecognition (ASR). SPGISpeech 2.0 consists of 3,780 additional hours of\nprofessionally transcribed earnings calls. Furthermore, the dataset contains\ncall and speaker information for each audio snippet facilitating multi-talker\nASR. We validate the utility of SPGISpeech 2.0 through improvements in\nspeaker-tagged ASR performance of popular speech recognition models after\nfine-tuning on SPGISpeech 2.0. Released free for non-commercial use, we expect\nSPGISpeech 2.0 to foster advancements in speech recognition technologies and\ninspire a wide range of research applications."}
{"id": "2508.05571", "pdf": "https://arxiv.org/pdf/2508.05571.pdf", "abs": "https://arxiv.org/abs/2508.05571", "title": "Fairy$\\pm i$: the First 2-bit Complex LLM with All Parameters in $\\{\\pm1, \\pm i\\}$", "authors": ["Feiyu Wang", "Guoan Wang", "Yihao Zhang", "Shengfan Wang", "Weitao Li", "Bokai Huang", "Shimao Chen", "Zihan Jiang", "Rui Xu", "Tong Yang"], "categories": ["cs.LG", "cs.CL"], "comment": "13 pages, 14 figures", "summary": "Quantization-Aware Training (QAT) integrates quantization into the training\nloop, enabling LLMs to learn robust low-bit representations, and is widely\nrecognized as one of the most promising research directions. All current QAT\nresearch focuses on minimizing quantization error on full-precision models,\nwhere the full-precision accuracy acts as an upper bound (accuracy ceiling). No\nexisting method has even attempted to surpass this ceiling. To break this\nceiling, we propose a new paradigm: raising the ceiling (full-precision model),\nand then still quantizing it efficiently into 2 bits. We propose Fairy$\\pm i$,\nthe first 2-bit quantization framework for complex-valued LLMs. Specifically,\nour method leverages the representational advantages of the complex domain to\nboost full-precision accuracy. We map weights to the fourth roots of unity\n$\\{\\pm1, \\pm i\\}$, forming a perfectly symmetric and information-theoretically\noptimal 2-bit representation. Importantly, each quantized weight has either a\nzero real or imaginary part, enabling multiplication-free inference using only\nadditions and element swaps. Experimental results show that Fairy$\\pm i$\noutperforms the ceiling of existing 2-bit quantization approaches in terms of\nboth PPL and downstream tasks, while maintaining strict storage and compute\nefficiency. This work opens a new direction for building highly accurate and\npractical LLMs under extremely low-bit constraints."}
{"id": "2508.05581", "pdf": "https://arxiv.org/pdf/2508.05581.pdf", "abs": "https://arxiv.org/abs/2508.05581", "title": "Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension using Large Language Models", "authors": ["Guilherme Seidyo Imai Aldeia", "Daniel S. Herman", "William G. La Cava"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "To appear in PMLR, Volume 298, Machine Learning for Healthcare, 2025", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities for\nmedical question answering and programming, but their potential for generating\ninterpretable computable phenotypes (CPs) is under-explored. In this work, we\ninvestigate whether LLMs can generate accurate and concise CPs for six clinical\nphenotypes of varying complexity, which could be leveraged to enable scalable\nclinical decision support to improve care for patients with hypertension. In\naddition to evaluating zero-short performance, we propose and test a\nsynthesize, execute, debug, instruct strategy that uses LLMs to generate and\niteratively refine CPs using data-driven feedback. Our results show that LLMs,\ncoupled with iterative learning, can generate interpretable and reasonably\naccurate programs that approach the performance of state-of-the-art ML methods\nwhile requiring significantly fewer training examples."}
{"id": "2508.05606", "pdf": "https://arxiv.org/pdf/2508.05606.pdf", "abs": "https://arxiv.org/abs/2508.05606", "title": "Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and Vision", "authors": ["Luozheng Qin", "Jia Gong", "Yuqing Sun", "Tianjiao Li", "Mengping Yang", "Xiaomeng Yang", "Chao Qu", "Zhiyu Tan", "Hao Li"], "categories": ["cs.CV", "cs.CL"], "comment": "https://sais-fuxi.github.io/projects/uni-cot/", "summary": "Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large\nLanguage Models (LLMs) by decomposing complex tasks into simpler, sequential\nsubtasks. However, extending CoT to vision-language reasoning tasks remains\nchallenging, as it often requires interpreting transitions of visual states to\nsupport reasoning. Existing methods often struggle with this due to limited\ncapacity of modeling visual state transitions or incoherent visual trajectories\ncaused by fragmented architectures.\n  To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought\nframework that enables coherent and grounded multimodal reasoning within a\nsingle unified model. The key idea is to leverage a model capable of both image\nunderstanding and generation to reason over visual content and model evolving\nvisual states. However, empowering a unified model to achieve that is\nnon-trivial, given the high computational cost and the burden of training. To\naddress this, Uni-CoT introduces a novel two-level reasoning paradigm: A\nMacro-Level CoT for high-level task planning and A Micro-Level CoT for subtask\nexecution. This design significantly reduces the computational overhead.\nFurthermore, we introduce a structured training paradigm that combines\ninterleaved image-text supervision for macro-level CoT with multi-task\nobjectives for micro-level CoT. Together, these innovations allow Uni-CoT to\nperform scalable and coherent multi-modal reasoning. Furthermore, thanks to our\ndesign, all experiments can be efficiently completed using only 8 A100 GPUs\nwith 80GB VRAM each. Experimental results on reasoning-driven image generation\nbenchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT\ndemonstrates SOTA performance and strong generalization, establishing Uni-CoT\nas a promising solution for multi-modal reasoning. Project Page and Code:\nhttps://sais-fuxi.github.io/projects/uni-cot/"}
{"id": "2508.05615", "pdf": "https://arxiv.org/pdf/2508.05615.pdf", "abs": "https://arxiv.org/abs/2508.05615", "title": "Test-Time Reinforcement Learning for GUI Grounding via Region Consistency", "authors": ["Yong Du", "Yuchen Yan", "Fei Tang", "Zhengxi Lu", "Chang Zong", "Weiming Lu", "Shengpei Jiang", "Yongliang Shen"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project Page: https://zju-real.github.io/gui-rcpo Code:\n  https://github.com/zju-real/gui-rcpo", "summary": "Graphical User Interface (GUI) grounding, the task of mapping natural\nlanguage instructions to precise screen coordinates, is fundamental to\nautonomous GUI agents. While existing methods achieve strong performance\nthrough extensive supervised training or reinforcement learning with labeled\nrewards, they remain constrained by the cost and availability of pixel-level\nannotations. We observe that when models generate multiple predictions for the\nsame GUI element, the spatial overlap patterns reveal implicit confidence\nsignals that can guide more accurate localization. Leveraging this insight, we\npropose GUI-RC (Region Consistency), a test-time scaling method that constructs\nspatial voting grids from multiple sampled predictions to identify consensus\nregions where models show highest agreement. Without any training, GUI-RC\nimproves accuracy by 2-3% across various architectures on ScreenSpot\nbenchmarks. We further introduce GUI-RCPO (Region Consistency Policy\nOptimization), which transforms these consistency patterns into rewards for\ntest-time reinforcement learning. By computing how well each prediction aligns\nwith the collective consensus, GUI-RCPO enables models to iteratively refine\ntheir outputs on unlabeled data during inference. Extensive experiments\ndemonstrate the generality of our approach: GUI-RC boosts\nQwen2.5-VL-3B-Instruct from 80.11% to 83.57% on ScreenSpot-v2, while GUI-RCPO\nfurther improves it to 85.14% through self-supervised optimization. Our\napproach reveals the untapped potential of test-time scaling and test-time\nreinforcement learning for GUI grounding, offering a promising path toward more\nrobust and data-efficient GUI agents."}
{"id": "2201.08214", "pdf": "https://arxiv.org/pdf/2201.08214.pdf", "abs": "https://arxiv.org/abs/2201.08214", "title": "A Latent-Variable Model for Intrinsic Probing", "authors": ["Karolina Sta≈Ñczak", "Lucas Torroba Hennigen", "Adina Williams", "Ryan Cotterell", "Isabelle Augenstein"], "categories": ["cs.CL"], "comment": null, "summary": "The success of pre-trained contextualized representations has prompted\nresearchers to analyze them for the presence of linguistic information. Indeed,\nit is natural to assume that these pre-trained representations do encode some\nlevel of linguistic knowledge as they have brought about large empirical\nimprovements on a wide variety of NLP tasks, which suggests they are learning\ntrue linguistic generalization. In this work, we focus on intrinsic probing, an\nanalysis technique where the goal is not only to identify whether a\nrepresentation encodes a linguistic attribute but also to pinpoint where this\nattribute is encoded. We propose a novel latent-variable formulation for\nconstructing intrinsic probes and derive a tractable variational approximation\nto the log-likelihood. Our results show that our model is versatile and yields\ntighter mutual information estimates than two intrinsic probes previously\nproposed in the literature. Finally, we find empirical evidence that\npre-trained representations develop a cross-lingually entangled notion of\nmorphosyntax."}
{"id": "2402.13213", "pdf": "https://arxiv.org/pdf/2402.13213.pdf", "abs": "https://arxiv.org/abs/2402.13213", "title": "Probabilities of Chat LLMs Are Miscalibrated but Still Predict Correctness on Multiple-Choice Q&A", "authors": ["Benjamin Plaut", "Nguyen X. Khanh", "Tu Trinh"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published in Transactions on Machine Learning Research (TMLR)", "summary": "We study 15 large language models (LLMs) fine-tuned for chat and find that\ntheir maximum softmax probabilities (MSPs) are consistently miscalibrated on\nmultiple-choice Q&A. However, those MSPs might still encode useful uncertainty\ninformation. Specifically, we hypothesized that wrong answers would be\nassociated with smaller MSPs compared to correct answers. Via rigorous\nstatistical testing, we show that this hypothesis holds for models which\nperform well on the underlying Q&A task. We also find a strong direction\ncorrelation between Q&A accuracy and MSP correctness prediction, while finding\nno correlation between Q&A accuracy and calibration error. This suggests that\nwithin the current fine-tuning paradigm, we can expect correctness prediction\nbut not calibration to improve as LLM capabilities progress. To demonstrate the\nutility of correctness prediction, we show that when models have the option to\nabstain, performance can be improved by selectively abstaining based on the MSP\nof the initial model response, using only a small amount of labeled data to\nchoose the MSP threshold."}
{"id": "2405.00708", "pdf": "https://arxiv.org/pdf/2405.00708.pdf", "abs": "https://arxiv.org/abs/2405.00708", "title": "Understanding Large Language Model Behaviors through Interactive Counterfactual Generation and Analysis", "authors": ["Furui Cheng", "Vil√©m Zouhar", "Robin Shing Moon Chan", "Daniel F√ºrst", "Hendrik Strobelt", "Mennatallah El-Assady"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG", "I.2.7; H.5.2"], "comment": null, "summary": "Understanding the behavior of large language models (LLMs) is crucial for\nensuring their safe and reliable use. However, existing explainable AI (XAI)\nmethods for LLMs primarily rely on word-level explanations, which are often\ncomputationally inefficient and misaligned with human reasoning processes.\nMoreover, these methods often treat explanation as a one-time output,\noverlooking its inherently interactive and iterative nature. In this paper, we\npresent LLM Analyzer, an interactive visualization system that addresses these\nlimitations by enabling intuitive and efficient exploration of LLM behaviors\nthrough counterfactual analysis. Our system features a novel algorithm that\ngenerates fluent and semantically meaningful counterfactuals via targeted\nremoval and replacement operations at user-defined levels of granularity. These\ncounterfactuals are used to compute feature attribution scores, which are then\nintegrated with concrete examples in a table-based visualization, supporting\ndynamic analysis of model behavior. A user study with LLM practitioners and\ninterviews with experts demonstrate the system's usability and effectiveness,\nemphasizing the importance of involving humans in the explanation process as\nactive participants rather than passive recipients."}
{"id": "2406.15477", "pdf": "https://arxiv.org/pdf/2406.15477.pdf", "abs": "https://arxiv.org/abs/2406.15477", "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics", "authors": ["Kai Yin", "Bo Li", "Chengkai Liu", "Ali Mostafavi", "Xia Hu"], "categories": ["cs.CL", "cs.AI"], "comment": "Relevant source code and data is available:\n  https://github.com/KaiYin97/CrsisLLM", "summary": "In the field of crisis/disaster informatics, social media is increasingly\nbeing used for improving situational awareness to inform response and relief\nefforts. Efficient and accurate text classification tools have been a focal\narea of investigation in crisis informatics. However, current methods mostly\nrely on single-label text classification models, which fails to capture\ndifferent insights embedded in dynamic and multifaceted disaster-related social\nmedia data. This study introduces a novel approach to disaster text\nclassification by enhancing a pre-trained Large Language Model (LLM) through\ninstruction fine-tuning targeted for multi-label classification of\ndisaster-related tweets. Our methodology involves creating a comprehensive\ninstruction dataset from disaster-related tweets, which is then used to\nfine-tune an open-source LLM, thereby embedding it with disaster-specific\nknowledge. This fine-tuned model can classify multiple aspects of\ndisaster-related information simultaneously, such as the type of event,\ninformativeness, and involvement of human aid, significantly improving the\nutility of social media data for situational awareness in disasters. The\nresults demonstrate that this approach enhances the categorization of critical\ninformation from social media posts, thereby facilitating a more effective\ndeployment for situational awareness during emergencies. This research paves\nthe way for more advanced, adaptable, and robust disaster management tools,\nleveraging the capabilities of LLMs to improve real-time situational awareness\nand response strategies in disaster scenarios."}
{"id": "2407.06323", "pdf": "https://arxiv.org/pdf/2407.06323.pdf", "abs": "https://arxiv.org/abs/2407.06323", "title": "When in Doubt, Cascade: Towards Building Efficient and Capable Guardrails", "authors": ["Manish Nagireddy", "Inkit Padhi", "Soumya Ghosh", "Prasanna Sattigeri"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have convincing performance in a variety of\ndownstream tasks. However, these systems are prone to generating undesirable\noutputs such as harmful and biased text. In order to remedy such generations,\nthe development of guardrail (or detector) models has gained traction.\nMotivated by findings from developing a detector for social bias, we adopt the\nnotion of a use-mention distinction - which we identified as the primary source\nof under-performance in the preliminary versions of our social bias detector.\nArmed with this information, we describe a fully extensible and reproducible\nsynthetic data generation pipeline which leverages taxonomy-driven instructions\nto create targeted and labeled data. Using this pipeline, we generate over 300K\nunique contrastive samples and provide extensive experiments to systematically\nevaluate performance on a suite of open source datasets. We show that our\nmethod achieves competitive performance with a fraction of the cost in compute\nand offers insight into iteratively developing efficient and capable guardrail\nmodels.\n  Warning: This paper contains examples of text which are toxic, biased, and\npotentially harmful."}
{"id": "2409.02098", "pdf": "https://arxiv.org/pdf/2409.02098.pdf", "abs": "https://arxiv.org/abs/2409.02098", "title": "CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation", "authors": ["Ingo Ziegler", "Abdullatif K√∂ksal", "Desmond Elliott", "Hinrich Sch√ºtze"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at TACL; Pre-MIT Press publication version. Code and dataset\n  available at: https://github.com/ziegler-ingo/CRAFT", "summary": "Building high-quality datasets for specialized tasks is a time-consuming and\nresource-intensive process that often requires specialized domain knowledge. We\npropose Corpus Retrieval and Augmentation for Fine-Tuning (CRAFT), a method for\ngenerating synthetic datasets, given a small number of user-written few-shots\nthat demonstrate the task to be performed. Given these examples, CRAFT uses\nlarge-scale public web-crawled corpora and similarity-based document retrieval\nto find other relevant human-written documents. Lastly, instruction-tuned large\nlanguage models (LLMs) augment the retrieved documents into custom-formatted\ntask samples, which then can be used for fine-tuning. We demonstrate that CRAFT\ncan efficiently generate large-scale task-specific training datasets for four\ndiverse tasks: biology, medicine, and commonsense question-answering (QA), as\nwell as summarization. Our experiments show that CRAFT-based models outperform\nor match general LLMs on QA tasks, while exceeding models trained on\nhuman-curated summarization data by 46 preference points. CRAFT outperforms\nother synthetic dataset generation methods such as Self- and Evol-Instruct, and\nremains robust even when the quality of the initial few-shots varies."}
{"id": "2409.06518", "pdf": "https://arxiv.org/pdf/2409.06518.pdf", "abs": "https://arxiv.org/abs/2409.06518", "title": "Medal Matters: Probing LLMs' Failure Cases Through Olympic Rankings", "authors": ["Juhwan Choi", "Seunguk Yu", "JungMin Yun", "YoungBin Kim"], "categories": ["cs.CL", "cs.AI"], "comment": "COLM 2025 ORIGen Workshop", "summary": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage processing tasks, yet their internal knowledge structures remain\npoorly understood. This study examines these structures through the lens of\nhistorical Olympic medal tallies, evaluating LLMs on two tasks: (1) retrieving\nmedal counts for specific teams and (2) identifying rankings of each team.\nWhile state-of-the-art LLMs excel in recalling medal counts, they struggle with\nproviding rankings, highlighting a key difference between their knowledge\norganization and human reasoning. These findings shed light on the limitations\nof LLMs' internal knowledge integration and suggest directions for improvement.\nTo facilitate further research, we release our code, dataset, and model\noutputs."}
{"id": "2409.08107", "pdf": "https://arxiv.org/pdf/2409.08107.pdf", "abs": "https://arxiv.org/abs/2409.08107", "title": "WhisperNER: Unified Open Named Entity and Speech Recognition", "authors": ["Gil Ayache", "Menachem Pirchi", "Aviv Navon", "Aviv Shamsian", "Gill Hetz", "Joseph Keshet"], "categories": ["cs.CL", "cs.LG"], "comment": "ASRU 2025, IEEE", "summary": "Integrating named entity recognition (NER) with automatic speech recognition\n(ASR) can significantly enhance transcription accuracy and informativeness. In\nthis paper, we introduce WhisperNER, a novel model that allows joint speech\ntranscription and entity recognition. WhisperNER supports open-type NER,\nenabling recognition of diverse and evolving entities at inference. Building on\nrecent advancements in open NER research, we augment a large synthetic dataset\nwith synthetic speech samples. This allows us to train WhisperNER on a large\nnumber of examples with diverse NER tags. During training, the model is\nprompted with NER labels and optimized to output the transcribed utterance\nalong with the corresponding tagged entities. To evaluate WhisperNER, we\ngenerate synthetic speech for commonly used NER benchmarks and annotate\nexisting ASR datasets with open NER tags. Our experiments demonstrate that\nWhisperNER outperforms natural baselines on both out-of-domain open type NER\nand supervised finetuning."}
{"id": "2409.19492", "pdf": "https://arxiv.org/pdf/2409.19492.pdf", "abs": "https://arxiv.org/abs/2409.19492", "title": "MedHalu: Hallucinations in Responses to Healthcare Queries by Large Language Models", "authors": ["Vibhor Agarwal", "Yiqiao Jin", "Mohit Chandra", "Munmun De Choudhury", "Srijan Kumar", "Nishanth Sastry"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ICWSM2026", "summary": "Large language models (LLMs) are starting to complement traditional\ninformation seeking mechanisms such as web search. LLM-powered chatbots like\nChatGPT are gaining prominence among the general public. AI chatbots are also\nincreasingly producing content on social media platforms. However, LLMs are\nalso prone to hallucinations, generating plausible yet factually incorrect or\nfabricated information. This becomes a critical problem when laypeople start\nseeking information about sensitive issues such as healthcare. Existing works\nin LLM hallucinations in the medical domain mainly focus on testing the medical\nknowledge of LLMs through standardized medical exam questions which are often\nwell-defined and clear-cut with definitive answers. However, these approaches\nmay not fully capture how these LLMs perform during real-world interactions\nwith patients. This work conducts a pioneering study on hallucinations in\nLLM-generated responses to real-world healthcare queries from patients.We\nintroduce MedHalu, a novel medical hallucination benchmark featuring diverse\nhealth-related topics and hallucinated responses from LLMs, with detailed\nannotation of the hallucination types and text spans. We also propose\nMedHaluDetect, a comprehensive framework for evaluating LLMs' abilities to\ndetect hallucinations. Furthermore, we study the vulnerability to medical\nhallucinations among three groups -- medical experts, LLMs, and laypeople.\nNotably, LLMs significantly underperform human experts and, in some cases, even\nlaypeople in detecting medical hallucinations. To improve hallucination\ndetection, we propose an expert-in-the-loop approach that integrates expert\nreasoning into LLM inputs, significantly improving hallucination detection for\nall LLMs, including a 6.3% macro-F1 improvement for GPT-4."}
{"id": "2410.01215", "pdf": "https://arxiv.org/pdf/2410.01215.pdf", "abs": "https://arxiv.org/abs/2410.01215", "title": "From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging", "authors": ["Yuling Shi", "Songsong Wang", "Chengcheng Wan", "Min Wang", "Xiaodong Gu"], "categories": ["cs.CL", "cs.AI", "cs.PL", "cs.SE"], "comment": "Code and data available at https://github.com/YerbaPage/MGDebugger", "summary": "While large language models have made significant strides in code generation,\nthe pass rate of the generated code is bottlenecked on subtle errors, often\nrequiring human intervention to pass tests, especially for complex problems.\nExisting LLM-based debugging systems treat generated programs as monolithic\nunits, failing to address bugs at multiple levels of granularity, from\nlow-level syntax errors to high-level algorithmic flaws. In this paper, we\nintroduce Multi-Granularity Debugger (MGDebugger), a hierarchical code debugger\nby isolating, identifying, and resolving bugs at various levels of granularity.\nMGDebugger decomposes problematic code into a hierarchical tree structure of\nsubfunctions, with each level representing a particular granularity of error.\nDuring debugging, it analyzes each subfunction and iteratively resolves bugs in\na bottom-up manner. To effectively test each subfunction, we propose an\nLLM-simulated Python executor, which traces code execution and tracks important\nvariable states to pinpoint errors accurately. Extensive experiments\ndemonstrate that MGDebugger outperforms existing debugging systems, achieving\nan 18.9% improvement in accuracy over seed generations in HumanEval and a 97.6%\nrepair success rate in HumanEvalFix. Furthermore, MGDebugger effectively fixes\nbugs across different categories and difficulty levels, demonstrating its\nrobustness and effectiveness."}
{"id": "2410.03751", "pdf": "https://arxiv.org/pdf/2410.03751.pdf", "abs": "https://arxiv.org/abs/2410.03751", "title": "Recent Advances in Speech Language Models: A Survey", "authors": ["Wenqian Cui", "Dianzhi Yu", "Xiaoqi Jiao", "Ziqiao Meng", "Guangyan Zhang", "Qichao Wang", "Yiwen Guo", "Irwin King"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "The reduced version of this paper has been accepted at ACL 2025", "summary": "Large Language Models (LLMs) have recently garnered significant attention,\nprimarily for their capabilities in text-based interactions. However, natural\nhuman interaction often relies on speech, necessitating a shift towards\nvoice-based models. A straightforward approach to achieve this involves a\npipeline of ``Automatic Speech Recognition (ASR) + LLM + Text-to-Speech (TTS)\",\nwhere input speech is transcribed to text, processed by an LLM, and then\nconverted back to speech. Despite being straightforward, this method suffers\nfrom inherent limitations, such as information loss during modality conversion,\nsignificant latency due to the complex pipeline, and error accumulation across\nthe three stages. To address these issues, Speech Language Models (SpeechLMs)\n-- end-to-end models that generate speech without converting from text -- have\nemerged as a promising alternative. This survey paper provides the first\ncomprehensive overview of recent methodologies for constructing SpeechLMs,\ndetailing the key components of their architecture and the various training\nrecipes integral to their development. Additionally, we systematically survey\nthe various capabilities of SpeechLMs, categorize their evaluation metrics, and\ndiscuss the challenges and future research directions in this rapidly evolving\nfield. The GitHub repository is available at\nhttps://github.com/dreamtheater123/Awesome-SpeechLM-Survey"}
{"id": "2410.04094", "pdf": "https://arxiv.org/pdf/2410.04094.pdf", "abs": "https://arxiv.org/abs/2410.04094", "title": "BloomWise: Enhancing Problem-Solving capabilities of Large Language Models using Bloom's-Taxonomy-Inspired Prompts", "authors": ["Maria-Eleni Zoumpoulidi", "Georgios Paraskevopoulos", "Alexandros Potamianos"], "categories": ["cs.CL"], "comment": "16 pages, 2 figures", "summary": "Despite the remarkable capabilities of large language models (LLMs) across a\nrange of tasks, mathematical reasoning remains a challenging frontier.\nMotivated by the observation that humans learn more effectively when prompted\nnot what to think but how to think, we introduce BloomWise, a\ncognitively-inspired prompting technique designed to enhance LLMs' performance\non mathematical problem solving while making their solutions more explainable.\nBloomWise encourages LLMs to generate solutions - in the form of explanations -\nby progressing through a sequence of cognitive operations-from basic (e.g.,\nremembering) to more advanced reasoning skills (e.g., evaluating) - mirroring\nhow humans build understanding. The process iterates through these levels,\nhalting early if a convergence criterion is met: specifically, if two or more\nconsecutive levels yield the same answer, the solution from the earliest such\nlevel is output; otherwise, the process continues until all levels are\ncompleted. Through extensive experiments across five popular math reasoning\ndatasets, we demonstrate the effectiveness of BloomWise. We also present\ncomprehensive ablation studies to analyze the strengths of each component\nwithin our system."}
{"id": "2410.06722", "pdf": "https://arxiv.org/pdf/2410.06722.pdf", "abs": "https://arxiv.org/abs/2410.06722", "title": "Scaling Laws For Mixed Quantization", "authors": ["Zeyu Cao", "Boyang Gu", "Cheng Zhang", "Pedro Gimenes", "Jianqiao Lu", "Jianyi Cheng", "Xitong Gao", "Yiren Zhao"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Post-training quantization of Large Language Models (LLMs) has proven\neffective in reducing the memory and computational requirements for inference.\nIn this study, we focus on a straightforward question: When aiming for a target\naccuracy or perplexity with low-precision quantization, how much high-precision\ncomputation needs to be preserved, and how fine-grained this quantization would\nneed to be as we scale LLMs to larger sizes? We first introduce two critical\nmetrics, named the quantization ratio ($Q_r$) and quantization block size\n($Q_b$). The former measures the number of parameters quantized to\nlow-precision arithmetic normalized by the total parameter count, whereas the\nlatter defines the number of values within a block that share a scaling factor,\nakin to the block size concept introduced in the FP4 format in NVIDIA's\nBlackwell architecture. Through extensive and carefully controlled experiments\nacross different models and quantization methods, we propose a unified scaling\nlaw on post-training quantization (PTQ) that can predict loss degeneration for\nvarying $Q_r$ and $Q_b$. For $Q_r$, our scaling law implies that parameter\nscaling and ratio scaling have a multiplicative relationship. Consequently,\nlarger models are more amenable to a higher quantization ratio $Q_r$, thus\nsupporting an increase in the adoption of mixed quantization for inference.\nRegarding $Q_b$, our findings indicate that a small block size, similar to that\nused in Blackwell, is not essential for large models. Employing a small $Q_b$\ncan instead unnecessarily complicate the design of the hardware circuit."}
{"id": "2410.08800", "pdf": "https://arxiv.org/pdf/2410.08800.pdf", "abs": "https://arxiv.org/abs/2410.08800", "title": "Data Processing for the OpenGPT-X Model Family", "authors": ["Nicolo' Brandizzi", "Hammam Abdelwahab", "Anirban Bhowmick", "Lennard Helmer", "Benny J√∂rg Stein", "Pavel Denisov", "Qasid Saleem", "Michael Fromm", "Mehdi Ali", "Richard Rutmann", "Farzad Naderi", "Mohamad Saif Agy", "Alexander Schwirjow", "Fabian K√ºch", "Luzian Hahn", "Malte Ostendorff", "Pedro Ortiz Suarez", "Georg Rehm", "Dennis Wegener", "Nicolas Flores-Herr", "Joachim K√∂hler", "Johannes Leveling"], "categories": ["cs.CL", "H.3.1; I.2.7"], "comment": null, "summary": "This paper presents a comprehensive overview of the data preparation pipeline\ndeveloped for the OpenGPT-X project, a large-scale initiative aimed at creating\nopen and high-performance multilingual large language models (LLMs). The\nproject goal is to deliver models that cover all major European languages, with\na particular focus on real-world applications within the European Union. We\nexplain all data processing steps, starting with the data selection and\nrequirement definition to the preparation of the final filtered data. We\ndistinguish between curated data and web data, as each of these categories is\nhandled by distinct pipelines, with curated data undergoing minimal filtering\nand web data requiring extensive filtering and deduplication. This distinction\nguided the development of specialized algorithmic solutions for both pipelines.\nIn addition to describing the processing methodologies, we provide an in-depth\nanalysis of the datasets, increasing transparency and alignment with European\ndata regulations. Finally, we share key insights and challenges faced during\nthe project, offering recommendations for future endeavors in large-scale\nmultilingual data preparation for LLMs."}
{"id": "2410.17519", "pdf": "https://arxiv.org/pdf/2410.17519.pdf", "abs": "https://arxiv.org/abs/2410.17519", "title": "Large Language Models Still Exhibit Bias in Long Text", "authors": ["Wonje Jeung", "Dongjae Jeon", "Ashkan Yousefpour", "Jonghyun Choi"], "categories": ["cs.CL"], "comment": "Accepted by ACL, code and models are available at\n  https://github.com/WonjeJeung/LTF-TEST", "summary": "Existing fairness benchmarks for large language models (LLMs) primarily focus\non simple tasks, such as multiple-choice questions, overlooking biases that may\narise in more complex scenarios like long-text generation. To address this gap,\nwe introduce the Long Text Fairness Test (LTF-TEST), a framework that evaluates\nbiases in LLMs through essay-style prompts. LTF-TEST covers 14 topics and 10\ndemographic axes, including gender and race, resulting in 11,948 samples. By\nassessing both model responses and the reasoning behind them, LTF-TEST uncovers\nsubtle biases that are difficult to detect in simple responses. In our\nevaluation of five recent LLMs, including GPT-4o and LLaMa3, we identify two\nkey patterns of bias. First, these models frequently favor certain demographic\ngroups in their responses. Second, they show excessive sensitivity toward\ntraditionally disadvantaged groups, often providing overly protective responses\nwhile neglecting others. To mitigate these biases, we propose FT-REGARD, a\nfinetuning approach that pairs biased prompts with neutral responses. FT-REGARD\nreduces gender bias by 34.6% and improves performance by 1.4 percentage points\non the BBQ benchmark, offering a promising approach to addressing biases in\nlong-text generation tasks."}
{"id": "2412.03930", "pdf": "https://arxiv.org/pdf/2412.03930.pdf", "abs": "https://arxiv.org/abs/2412.03930", "title": "GuARD: Effective Anomaly Detection through a Text-Rich and Graph-Informed Language Model", "authors": ["Yunhe Pang", "Bo Chen", "Fanjin Zhang", "Yanghui Rao", "Evgeny Kharlamov", "Jie Tang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at KDD 2025", "summary": "Anomaly detection on text-rich graphs is widely prevalent in real life, such\nas detecting incorrectly assigned academic papers to authors and detecting bots\nin social networks. The remarkable capabilities of large language models (LLMs)\npave a new revenue by utilizing rich-text information for effective anomaly\ndetection. However, simply introducing rich texts into LLMs can obscure\nessential detection cues and introduce high fine-tuning costs. Moreover, LLMs\noften overlook the intrinsic structural bias of graphs which is vital for\ndistinguishing normal from abnormal node patterns. To this end, this paper\nintroduces GuARD, a text-rich and graph-informed language model that combines\nkey structural features from graph-based methods with fine-grained semantic\nattributes extracted via small language models for effective anomaly detection\non text-rich graphs. GuARD is optimized with the progressive multi-modal\nmulti-turn instruction tuning framework in the task-guided instruction tuning\nregime tailed to incorporate both rich-text and structural modalities.\nExtensive experiments on four datasets reveal that GuARD outperforms\ngraph-based and LLM-based anomaly detection methods, while offering up to\n5$\\times$ times speedup in training and 5$\\times$ times speedup in inference\nover vanilla long-context LLMs on the large-scale WhoIsWho dataset."}
{"id": "2412.14964", "pdf": "https://arxiv.org/pdf/2412.14964.pdf", "abs": "https://arxiv.org/abs/2412.14964", "title": "Efficient Knowledge Injection in LLMs via Self-Distillation", "authors": ["Kalle Kujanp√§√§", "Pekka Marttinen", "Harri Valpola", "Alexander Ilin"], "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "In many practical applications, large language models (LLMs) need to acquire\nnew knowledge not present in their pre-training data. Efficiently leveraging\nthis knowledge usually relies on supervised fine-tuning or retrieval-augmented\ngeneration (RAG). Although RAG has emerged as the industry standard for\nknowledge injection, fine-tuning has not yet achieved comparable success. This\npaper proposes utilizing prompt distillation, a self-distillation-based method\npreviously explored primarily for style alignment and instruction tuning, to\ninternalize new factual knowledge from free-form documents. Unlike prior\nmethods, our approach requires neither larger teacher models nor structured\nknowledge formats. Across multiple LLM sizes and model families, we show that\nprompt distillation outperforms standard supervised fine-tuning and can even\nsurpass RAG. We analyze the key factors contributing to prompt distillation's\neffectiveness and examine how it scales."}
{"id": "2412.16936", "pdf": "https://arxiv.org/pdf/2412.16936.pdf", "abs": "https://arxiv.org/abs/2412.16936", "title": "Rationale-guided Prompting for Knowledge-based Visual Question Answering", "authors": ["Zhongjian Hu", "Peng Yang", "Bing Li", "Fengyuan Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "We would like to withdraw this submission due to ongoing internal\n  review and coordination among the author team. Upon the supervisor's\n  recommendation, we have decided to delay public dissemination until the\n  manuscript undergoes further refinement and aligns with our intended academic\n  trajectory", "summary": "Recently, Large Language Models (LLMs) have been used for knowledge-based\nVisual Question Answering (VQA). Despite the encouraging results of previous\nstudies, prior methods prompt LLMs to predict answers directly, neglecting\nintermediate thought processes. We argue that prior methods do not sufficiently\nactivate the capacities of LLMs. We propose a framework called PLRH that\nPrompts LLMs with Rationale Heuristics for knowledge-based VQA. The PLRH\nprompts LLMs with Chain of Thought (CoT) to generate rationale heuristics,\ni.e., intermediate thought processes, and then leverages the rationale\nheuristics to inspire LLMs to predict answers. Experiments show that our\napproach outperforms the existing baselines by more than 2.2 and 2.1 on OK-VQA\nand A-OKVQA, respectively."}
{"id": "2412.18351", "pdf": "https://arxiv.org/pdf/2412.18351.pdf", "abs": "https://arxiv.org/abs/2412.18351", "title": "Multi-Agents Based on Large Language Models for Knowledge-based Visual Question Answering", "authors": ["Zhongjian Hu", "Peng Yang", "Bing Li", "Zhenqi Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "We would like to withdraw this submission due to ongoing internal\n  review and coordination among the author team. Upon the supervisor's\n  recommendation, we have decided to delay public dissemination until the\n  manuscript undergoes further refinement and aligns with our intended academic\n  trajectory", "summary": "Large Language Models (LLMs) have achieved impressive results in\nknowledge-based Visual Question Answering (VQA). However existing methods still\nhave challenges: the inability to use external tools autonomously, and the\ninability to work in teams. Humans tend to know whether they need to use\nexternal tools when they encounter a new question, e.g., they tend to be able\nto give a direct answer to a familiar question, whereas they tend to use tools\nsuch as search engines when they encounter an unfamiliar question. In addition,\nhumans also tend to collaborate and discuss with others to get better answers.\nInspired by this, we propose the multi-agent voting framework. We design three\nLLM-based agents that simulate different levels of staff in a team, and assign\nthe available tools according to the levels. Each agent provides the\ncorresponding answer, and finally all the answers provided by the agents are\nvoted to get the final answer. Experiments on OK-VQA and A-OKVQA show that our\napproach outperforms other baselines by 2.2 and 1.0, respectively."}
{"id": "2501.12106", "pdf": "https://arxiv.org/pdf/2501.12106.pdf", "abs": "https://arxiv.org/abs/2501.12106", "title": "Can open source large language models be used for tumor documentation in Germany? -- An evaluation on urological doctors' notes", "authors": ["Stefan Lenz", "Arsenij Ustjanzew", "Marco Jeray", "Meike Ressing", "Torsten Panholzer"], "categories": ["cs.CL", "cs.AI"], "comment": "53 pages, 5 figures", "summary": "Tumor documentation in Germany is largely done manually, requiring reading\npatient records and entering data into structured databases. Large language\nmodels (LLMs) could potentially enhance this process by improving efficiency\nand reliability. This evaluation tests eleven different open source LLMs with\nsizes ranging from 1-70 billion model parameters on three basic tasks of the\ntumor documentation process: identifying tumor diagnoses, assigning ICD-10\ncodes, and extracting the date of first diagnosis. For evaluating the LLMs on\nthese tasks, a dataset of annotated text snippets based on anonymized doctors'\nnotes from urology was prepared. Different prompting strategies were used to\ninvestigate the effect of the number of examples in few-shot prompting and to\nexplore the capabilities of the LLMs in general. The models Llama 3.1 8B,\nMistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks.\nModels with less extensive training data or having fewer than 7 billion\nparameters showed notably lower performance, while larger models did not\ndisplay performance gains. Examples from a different medical domain than\nurology could also improve the outcome in few-shot prompting, which\ndemonstrates the ability of LLMs to handle tasks needed for tumor\ndocumentation. Open source LLMs show a strong potential for automating tumor\ndocumentation. Models from 7-12 billion parameters could offer an optimal\nbalance between performance and resource efficiency. With tailored fine-tuning\nand well-designed prompting, these models might become important tools for\nclinical documentation in the future. The code for the evaluation is available\nfrom https://github.com/stefan-m-lenz/UroLlmEval. We also release the dataset\nas a new valuable resource that addresses the shortage of authentic and easily\naccessible benchmarks in German-language medical NLP."}
{"id": "2502.13417", "pdf": "https://arxiv.org/pdf/2502.13417.pdf", "abs": "https://arxiv.org/abs/2502.13417", "title": "RLTHF: Targeted Human Feedback for LLM Alignment", "authors": ["Yifei Xu", "Tusher Chakraborty", "Emre Kƒ±cƒ±man", "Bibek Aryal", "Eduardo Rodrigues", "Srinagesh Sharma", "Roberto Estevao", "Maria Angels de Luis Balaguer", "Jessica Wolk", "Rafael Padilha", "Leonardo Nunes", "Shobana Balakrishnan", "Songwu Lu", "Ranveer Chandra"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Presented at ICML 2025", "summary": "Fine-tuning large language models (LLMs) to align with user preferences is\nchallenging due to the high cost of quality human annotations in Reinforcement\nLearning from Human Feedback (RLHF) and the generalizability limitations of AI\nFeedback. To address these challenges, we propose RLTHF, a human-AI hybrid\nframework that combines LLM-based initial alignment with selective human\nannotations to achieve full-human annotation alignment with minimal effort.\nRLTHF identifies hard-to-annotate samples mislabeled by LLMs using a reward\nmodel's reward distribution and iteratively enhances alignment by integrating\nstrategic human corrections while leveraging LLM's correctly labeled samples.\nEvaluations on HH-RLHF and TL;DR datasets show that RLTHF reaches full-human\nannotation-level alignment with only 6-7% of the human annotation effort.\nFurthermore, models trained on RLTHF's curated datasets for downstream tasks\noutperform those trained on fully human-annotated datasets, underscoring the\neffectiveness of RLTHF."}
{"id": "2502.17383", "pdf": "https://arxiv.org/pdf/2502.17383.pdf", "abs": "https://arxiv.org/abs/2502.17383", "title": "Which Questions Improve Learning the Most? Utility Estimation of Questions with LM-based Simulations", "authors": ["Dong-Ho Lee", "Hyundong Cho", "Jonathan May", "Jay Pujara"], "categories": ["cs.CL"], "comment": "17 pages, 5 figures, 6 tables", "summary": "Asking good questions is critical for comprehension and learning, yet\nevaluating and generating such questions remains a challenging problem. Prior\nwork on inquisitive questions focuses on learner-generated, curiosity-driven\nqueries and evaluates them using indirect metrics, such as salience or\ninformation gain, that do not directly capture a question's impact on actual\nlearning outcomes. We introduce QUEST (Question Utility Estimation with\nSimulated Tests), a framework that uses language models to simulate learners\nand directly quantify the utility of a question - its contribution to exam\nperformance. QUEST simulates a learner who asks questions and receives answers\nwhile studying a textbook chapter, then uses them to take an end-of-chapter\nexam. Through this simulation, the utility of each question is estimated by its\ndirect effect on exam performance, rather than inferred indirectly based on the\nunderlying content. To support this evaluation, we curate TEXTBOOK-EXAM, a\nbenchmark that aligns textbook sections with end-of-section exam questions\nacross five academic disciplines. Using QUEST, we filter for high-utility\nquestions and fine-tune question generators via rejection sampling. Experiments\nshow that questions generated by QUEST-trained models improve simulated test\nscores by over 20% compared to strong baselines that are fine-tuned using\nindirect metrics or leverage prompting methods. Furthermore, utility is only\nweakly correlated with salience and similarity to exam questions, suggesting\nthat it captures unique signal that benefits downstream performance. QUEST\noffers a new outcome-driven paradigm for question evaluation and generation -\none that moves beyond question-answer content toward measurable improvements in\nlearning outcomes."}
{"id": "2503.10533", "pdf": "https://arxiv.org/pdf/2503.10533.pdf", "abs": "https://arxiv.org/abs/2503.10533", "title": "The Impact of Item-Writing Flaws on Difficulty and Discrimination in Item Response Theory", "authors": ["Robin Schmucker", "Steven Moore"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Added Acknowledgments", "summary": "High-quality test items are essential for educational assessments,\nparticularly within Item Response Theory (IRT). Traditional validation methods\nrely on resource-intensive pilot testing to estimate item difficulty and\ndiscrimination. More recently, Item-Writing Flaw (IWF) rubrics emerged as a\ndomain-general approach for evaluating test items based on textual features.\nThis method offers a scalable, pre-deployment evaluation without requiring\nstudent data, but its predictive validity concerning empirical IRT parameters\nis underexplored. To address this gap, we conducted a study involving 7,126\nmultiple-choice questions across various STEM subjects (physical science,\nmathematics, and life/earth sciences). Using an automated approach, we\nannotated each question with a 19-criteria IWF rubric and studied relationships\nto data-driven IRT parameters. Our analysis revealed statistically significant\nlinks between the number of IWFs and IRT difficulty and discrimination\nparameters, particularly in life/earth and physical science domains. We further\nobserved how specific IWF criteria can impact item quality more and less\nseverely (e.g., negative wording vs. implausible distractors) and how they\nmight make a question more or less challenging. Overall, our findings establish\nautomated IWF analysis as a valuable supplement to traditional validation,\nproviding an efficient method for initial item screening, particularly for\nflagging low-difficulty MCQs. Our findings show the need for further research\non domain-general evaluation rubrics and algorithms that understand\ndomain-specific content for robust item validation."}
{"id": "2503.19168", "pdf": "https://arxiv.org/pdf/2503.19168.pdf", "abs": "https://arxiv.org/abs/2503.19168", "title": "Language Model Uncertainty Quantification with Attention Chain", "authors": ["Yinghao Li", "Rushi Qiang", "Lama Moukheiber", "Chao Zhang"], "categories": ["cs.CL"], "comment": "36 pages, 7 figures, 36 tables", "summary": "Accurately quantifying a large language model's (LLM) predictive uncertainty\nis crucial for judging the reliability of its answers. While most existing\nresearch focuses on short, directly answerable questions with closed-form\noutputs (e.g., multiple-choice), involving intermediate reasoning steps in LLM\nresponses is increasingly important. This added complexity complicates\nuncertainty quantification (UQ) because the probabilities assigned to answer\ntokens are conditioned on a vast space of preceding reasoning tokens. Direct\nmarginalization is infeasible, and the dependency inflates probability\nestimates, causing overconfidence in UQ. To address this, we propose UQAC, an\nefficient method that narrows the reasoning space to a tractable size for\nmarginalization. UQAC iteratively constructs an \"attention chain\" of tokens\ndeemed \"semantically crucial\" to the final answer via a backtracking procedure.\nStarting from the answer tokens, it uses attention weights to identify the most\ninfluential predecessors, then iterates this process until reaching the input\ntokens. The resulting chain is further refined with similarity filtering and\nprobability thresholding, which reduce the reasoning space, facilitating the\napproximation of the marginal answer token probabilities. We validate UQAC on\nmultiple reasoning benchmarks with advanced open-source LLMs, demonstrating\nthat it consistently delivers reliable UQ estimates with high computational\nefficiency."}
{"id": "2503.24013", "pdf": "https://arxiv.org/pdf/2503.24013.pdf", "abs": "https://arxiv.org/abs/2503.24013", "title": "You Cannot Feed Two Birds with One Score: the Accuracy-Naturalness Tradeoff in Translation", "authors": ["Gergely Flamich", "David Vilar", "Jan-Thorsten Peter", "Markus Freitag"], "categories": ["cs.CL"], "comment": "Accepted to COLM 2025. Camera-ready version", "summary": "The goal of translation, be it by human or by machine, is, given some text in\na source language, to produce text in a target language that simultaneously 1)\npreserves the meaning of the source text and 2) achieves natural expression in\nthe target language. However, researchers in the machine translation community\nusually assess translations using a single score intended to capture semantic\naccuracy and the naturalness of the output simultaneously. In this paper, we\nbuild on recent advances in information theory to mathematically prove and\nempirically demonstrate that such single-score summaries do not and cannot give\nthe complete picture of a system's true performance. Concretely, we prove that\na tradeoff exists between accuracy and naturalness and demonstrate it by\nevaluating the submissions to the WMT24 shared task. Our findings help explain\nwell-known empirical phenomena, such as the observation that optimizing\ntranslation systems for a specific accuracy metric (like BLEU) initially\nimproves the system's naturalness, while ``overfitting'' the system to the\nmetric can significantly degrade its naturalness. Thus, we advocate for a\nchange in how translations are evaluated: rather than comparing systems using a\nsingle number, they should be compared on an accuracy-naturalness plane."}
{"id": "2504.00255", "pdf": "https://arxiv.org/pdf/2504.00255.pdf", "abs": "https://arxiv.org/abs/2504.00255", "title": "SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers", "authors": ["Yanzheng Xiang", "Hanqi Yan", "Shuyin Ouyang", "Lin Gui", "Yulan He"], "categories": ["cs.CL", "cs.AI", "cs.MA", "cs.SE"], "comment": null, "summary": "This study evaluates large language models (LLMs) in generating code from\nalgorithm descriptions in recent NLP papers. The task requires two key\ncompetencies: (1) algorithm comprehension: synthesizing information from papers\nand academic literature to understand implementation logic, and (2) coding\nexpertise: identifying dependencies and correctly implementing necessary APIs.\nTo facilitate rigorous evaluation, we introduce SciReplicate-Bench, a benchmark\nof 100 tasks from 36 NLP papers published in 2024, featuring detailed\nannotations and comprehensive test cases. Building on SciReplicate-Bench, we\npropose Sci-Reproducer, a dual-agent framework consisting of a Paper Agent that\ninterprets algorithmic concepts from literature and a Code Agent that retrieves\ndependencies from repositories and implements solutions. To assess algorithm\nunderstanding, we introduce reasoning graph accuracy, which quantifies\nsimilarity between generated and reference reasoning graphs derived from code\ncomments and structure. For evaluating implementation quality, we employ\nexecution accuracy, CodeBLEU, and repository dependency/API recall metrics. In\nour experiments, we evaluate various powerful non-reasoning and reasoning LLMs\nas foundational models. The best-performing LLM using \\ModelName~achieves only\n39% execution accuracy, highlighting the benchmark's difficulty. Our analysis\nidentifies missing or inconsistent algorithm descriptions as key barriers to\nsuccessful reproduction. We make available our benchmark and code at\nhttps://github.com/xyzCS/SciReplicate-Bench and project homepage at\nhttps://xyzcs.github.io/scireplicate.github.io/."}
{"id": "2504.04377", "pdf": "https://arxiv.org/pdf/2504.04377.pdf", "abs": "https://arxiv.org/abs/2504.04377", "title": "PolyGuard: A Multilingual Safety Moderation Tool for 17 Languages", "authors": ["Priyanshu Kumar", "Devansh Jain", "Akhila Yerukola", "Liwei Jiang", "Himanshu Beniwal", "Thomas Hartvigsen", "Maarten Sap"], "categories": ["cs.CL"], "comment": "Accepted to COLM 2025 Main Conference", "summary": "Truly multilingual safety moderation efforts for Large Language Models (LLMs)\nhave been hindered by a narrow focus on a small set of languages (e.g.,\nEnglish, Chinese) as well as a limited scope of safety definition, resulting in\nsignificant gaps in moderation capabilities. To bridge these gaps, we release\nPOLYGUARD, a new state-of-the-art multilingual safety model for safeguarding\nLLM generations, and the corresponding training and evaluation datasets.\nPOLYGUARD is trained on POLYGUARDMIX, the largest multilingual safety training\ncorpus to date containing 1.91M samples across 17 languages (e.g., Chinese,\nCzech, English, Hindi). We also introduce POLYGUARDPROMPTS, a high quality\nmultilingual benchmark with 29K samples for the evaluation of safety\nguardrails. Created by combining naturally occurring multilingual human-LLM\ninteractions and human-verified machine translations of an English-only safety\ndataset (WildGuardMix; Han et al., 2024), our datasets contain prompt-output\npairs with labels of prompt harmfulness, response harmfulness, and response\nrefusal. Through extensive evaluations across multiple safety and toxicity\nbenchmarks, we demonstrate that POLYGUARD outperforms existing state-of-the-art\nopen-weight and commercial safety classifiers by 5.5%. Our contributions\nadvance efforts toward safer multilingual LLMs for all global users."}
{"id": "2504.05598", "pdf": "https://arxiv.org/pdf/2504.05598.pdf", "abs": "https://arxiv.org/abs/2504.05598", "title": "DEL: Context-Aware Dynamic Exit Layer for Efficient Self-Speculative Decoding", "authors": ["Hossein Entezari Zarch", "Lei Gao", "Chaoyi Jiang", "Murali Annavaram"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Speculative Decoding (SD) is a widely used approach to accelerate the\ninference of large language models (LLMs) without reducing generation quality.\nIt operates by first using a compact model to draft multiple tokens\nefficiently, followed by parallel verification using the target LLM. This\napproach leads to faster inference compared to auto-regressive decoding. While\nthere are multiple approaches to create a draft model, one promising approach\nis to use early-exit methods. These methods draft candidate tokens by using a\nsubset of layers of the primary model and applying the remaining layers for\nverification, allowing a single model to handle both drafting and verification.\nWhile this technique reduces memory usage and computational cost, its\nperformance relies on the choice of the exit layer for drafting and the number\nof tokens drafted (speculation length) in each SD round. Prior works use\nhyperparameter exploration to statically select these values. However, our\nevaluations show that these hyperparameter values are task-specific, and even\nwithin a task they are dependent on the current sequence context. We introduce\nDEL (Dynamic Exit Layer), a plug-and-play method that adaptively selects the\nexit layer and speculation length during inference. DEL dynamically tracks the\ntoken acceptance rate if the tokens are drafted at each layer of an LLM and\nuses that knowledge to heuristically select the optimal exit layer and\nspeculation length. Our experiments across a broad range of models and\ndownstream tasks show that DEL achieves overall speedups of\n$2.16\\times$$\\sim$$2.62\\times$ over vanilla auto-regressive decoding and\nimproves upon state-of-the-art SD methods, which peak at $2.43\\times$, by up to\n$0.19\\times$. The code is available at https://github.com/hoenza/DEL."}
{"id": "2505.05815", "pdf": "https://arxiv.org/pdf/2505.05815.pdf", "abs": "https://arxiv.org/abs/2505.05815", "title": "Tell Me Who Your Students Are: GPT Can Generate Valid Multiple-Choice Questions When Students' (Mis)Understanding Is Hinted", "authors": ["Machi Shimmei", "Masaki Uto", "Yuichiroh Matsubayashi", "Kentaro Inui", "Aditi Mallavarapu", "Noboru Matsuda"], "categories": ["cs.CL"], "comment": "This is a pre-print version of a paper to appear in AIED2025. The\n  camera-ready version is available at\n  https://link.springer.com/chapter/10.1007/978-3-031-99264-3_16", "summary": "The primary goal of this study is to develop and evaluate an innovative\nprompting technique, AnaQuest, for generating multiple-choice questions (MCQs)\nusing a pre-trained large language model. In AnaQuest, the choice items are\nsentence-level assertions about complex concepts. The technique integrates\nformative and summative assessments. In the formative phase, students answer\nopen-ended questions for target concepts in free text. For summative\nassessment, AnaQuest analyzes these responses to generate both correct and\nincorrect assertions. To evaluate the validity of the generated MCQs, Item\nResponse Theory (IRT) was applied to compare item characteristics between MCQs\ngenerated by AnaQuest, a baseline ChatGPT prompt, and human-crafted items. An\nempirical study found that expert instructors rated MCQs generated by both AI\nmodels to be as valid as those created by human instructors. However, IRT-based\nanalysis revealed that AnaQuest-generated questions - particularly those with\nincorrect assertions (foils) - more closely resembled human-crafted items in\nterms of difficulty and discrimination than those produced by ChatGPT."}
{"id": "2505.18601", "pdf": "https://arxiv.org/pdf/2505.18601.pdf", "abs": "https://arxiv.org/abs/2505.18601", "title": "Flex-Judge: Text-Only Reasoning Unleashes Zero-Shot Multimodal Evaluators", "authors": ["Jongwoo Ko", "Sungnyun Kim", "Sungwoo Cho", "Se-Young Yun"], "categories": ["cs.CL", "cs.AI"], "comment": "The code is available at https://github.com/jongwooko/flex-judge", "summary": "Human-generated reward signals are critical for aligning generative models\nwith human preferences, guiding both training and inference-time evaluations.\nWhile large language models (LLMs) employed as proxy evaluators, i.e.,\nLLM-as-a-Judge, significantly reduce the costs associated with manual\nannotations, they typically require extensive modality-specific training data\nand fail to generalize well across diverse multimodal tasks. In this paper, we\npropose Flex-Judge, a reasoning-guided multimodal judge model that leverages\nminimal textual reasoning data to robustly generalize across multiple\nmodalities and evaluation formats. Our core intuition is that structured\ntextual reasoning explanations inherently encode generalizable decision-making\npatterns, enabling an effective transfer to multimodal judgments, e.g., with\nimages or videos. Empirical results demonstrate that Flex-Judge, despite being\ntrained on significantly fewer text data, achieves competitive or superior\nperformance compared to state-of-the-art commercial APIs and extensively\ntrained multimodal evaluators. Notably, Flex-Judge presents broad impact in\nmodalities like molecule, where comprehensive evaluation benchmarks are scarce,\nunderscoring its practical value in resource-constrained domains. Our framework\nhighlights reasoning-based text supervision as a powerful, cost-effective\nalternative to traditional annotation-intensive approaches, substantially\nadvancing scalable multimodal model-as-a-judge."}
{"id": "2506.11105", "pdf": "https://arxiv.org/pdf/2506.11105.pdf", "abs": "https://arxiv.org/abs/2506.11105", "title": "Enabling On-Device Medical AI Assistants via Input-Driven Saliency Adaptation", "authors": ["Uttej Kallakurik", "Edward Humes", "Rithvik Jonna", "Xiaomin Lin", "Tinoosh Mohsenin"], "categories": ["cs.CL", "cs.AI", "cs.AR", "cs.SY", "eess.SY"], "comment": "Accepted for publication in the Proceedings of IEEE BioCAS 2025", "summary": "Large Language Models (LLMs) have significant impact on the healthcare\nscenarios but remain prohibitively large for deployment in real-time,\nresource-constrained environments such as edge devices. In this work, we\nintroduce a novel medical assistant system, optimized through our\ngeneral-purpose compression framework, which tailors Large Language Models\n(LLMs) for deployment in specialized domains. By measuring neuron saliency on\ndomain-specific data, our method can aggressively prune irrelevant neurons,\nreducing model size while preserving performance. Following pruning, we apply\npost-training quantization to further reduce the memory footprint, and evaluate\nthe compressed model across medical benchmarks including MedMCQA, MedQA, and\nPubMedQA. We also deploy the 50\\% compressed Gemma and the 67\\% compressed\nLLaMA3 models on Jetson Orin Nano (18.7W peak) and Raspberry Pi 5 (6.3W peak),\nachieving real-time, energy-efficient inference under hardware constraints."}
{"id": "2506.12496", "pdf": "https://arxiv.org/pdf/2506.12496.pdf", "abs": "https://arxiv.org/abs/2506.12496", "title": "Improving Factuality for Dialogue Response Generation via Graph-Based Knowledge Augmentation", "authors": ["Xiangyan Chen", "Yujian Gan", "Yimeng Gu", "Matthew Purver"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Large Language Models (LLMs) succeed in many natural language processing\ntasks. However, their tendency to hallucinate - generate plausible but\ninconsistent or factually incorrect text - can cause significant problems in\ncertain tasks, including response generation in dialogue. To mitigate this\nissue, we propose two novel graph knowledge-augmented frameworks, Dialogue\nResponse Generation via Textualised Graphs (TG-DRG) and Graph-Aware Dialogue\nResponse Generation (GA-DRG), which combine reasoning-guided dialogue\nreformulation, dialogue sense knowledge selection, and graph-enhanced response\ngeneration to improve the factuality of dialogue responses. To evaluate the\nfactuality of generated responses, we propose a dialogue fact score that\naddresses the limitations of existing fact-score methods in dialogue settings,\nproviding a more reliable assessment of factual consistency. We evaluate our\nmethods using different baselines on the OpendialKG and HybriDialogue datasets.\nOur methods noticeably improve factuality compared to other graph\nknowledge-augmentation baselines, including the state-of-the-art G-retriever,\nachieving improvements of 3.47% on OpendialKG and 3.12% on HybriDialogue in\nterms of dialogue fact score. The code will be released on GitHub."}
{"id": "2506.16123", "pdf": "https://arxiv.org/pdf/2506.16123.pdf", "abs": "https://arxiv.org/abs/2506.16123", "title": "FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning", "authors": ["Natapong Nitarach", "Warit Sirichotedumrong", "Panop Pitchayarthorn", "Pittawat Taveekitworachai", "Potsawee Manakul", "Kunat Pipatanakul"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents FinCoT, a structured chain-of-thought (CoT) prompting\nframework that embeds domain-specific expert financial reasoning blueprints to\nguide large language models' behaviors. We identify three main prompting styles\nin financial NLP (FinNLP): (1) standard prompting (zero-shot), (2) unstructured\nCoT (free-form reasoning), and (3) structured CoT (with explicitly structured\nreasoning steps). Prior work has mainly focused on the first two, while\nstructured CoT remains underexplored and lacks domain expertise incorporation.\nTherefore, we evaluate all three prompting approaches across ten CFA-style\nfinancial domains and introduce FinCoT as the first structured finance-specific\nprompting approach incorporating blueprints from domain experts. FinCoT\nimproves the accuracy of a general-purpose model, Qwen3-8B-Base, from 63.2% to\n80.5%, and boosts Fin-R1 (7B), a finance-specific model, from 65.7% to 75.7%,\nwhile reducing output length by up to 8.9x and 1.16x compared to structured CoT\nmethods, respectively. We find that FinCoT proves most effective for models\nlacking financial post-training. Our findings show that FinCoT does not only\nimprove performance and reduce inference costs but also yields more\ninterpretable and expert-aligned reasoning traces."}
{"id": "2506.21586", "pdf": "https://arxiv.org/pdf/2506.21586.pdf", "abs": "https://arxiv.org/abs/2506.21586", "title": "Can Vision Language Models Understand Mimed Actions?", "authors": ["Hyundong Cho", "Spencer Lin", "Tejas Srinivasan", "Michael Saxon", "Deuksin Kwon", "Natali T. Chavez", "Jonathan May"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "ACL 2025 Findings", "summary": "Nonverbal communication (NVC) plays an integral role in human language, but\nstudying NVC in general is challenging because of its broad scope and high\nvariance in interpretation among individuals and cultures. However, mime -- the\ntheatrical technique of suggesting intent using only gesture, expression, and\nmovement -- is a subset of NVC that consists of explicit and embodied actions\nwith much lower human interpretation variance. We argue that a solid\nunderstanding of mimed actions is a crucial prerequisite for vision-language\nmodels capable of interpreting and commanding more subtle aspects of NVC.\nHence, we propose Mime Identification Multimodal Evaluation (MIME), a novel\nvideo-based question answering benchmark comprising of 86 mimed actions.\nConstructed with motion capture data, MIME consists of variations of each\naction with perturbations applied to the character, background, and viewpoint\nfor evaluating recognition robustness. We find that both open-weight and\nAPI-based vision-language models perform significantly worse than humans on\nMIME, motivating the need for increased research for instilling more robust\nunderstanding of human gestures."}
{"id": "2507.02088", "pdf": "https://arxiv.org/pdf/2507.02088.pdf", "abs": "https://arxiv.org/abs/2507.02088", "title": "McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models", "authors": ["Tian Lan", "Xiangdong Su", "Xu Liu", "Ruirui Wang", "Ke Chang", "Jiang Li", "Guanglai Gao"], "categories": ["cs.CL"], "comment": "Accepted by ACL2025 Findings", "summary": "As large language models (LLMs) are increasingly applied to various NLP\ntasks, their inherent biases are gradually disclosed. Therefore, measuring\nbiases in LLMs is crucial to mitigate its ethical risks. However, most existing\nbias evaluation datasets focus on English and North American culture, and their\nbias categories are not fully applicable to other cultures. The datasets\ngrounded in the Chinese language and culture are scarce. More importantly,\nthese datasets usually only support single evaluation tasks and cannot evaluate\nthe bias from multiple aspects in LLMs. To address these issues, we present a\nMulti-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias\nevaluation instances, covering 12 single bias categories, 82 subcategories and\nintroducing 5 evaluation tasks, providing extensive category coverage, content\ndiversity, and measuring comprehensiveness. Additionally, we evaluate several\npopular LLMs from different series and with parameter sizes. In general, all\nthese LLMs demonstrated varying degrees of bias. We conduct an in-depth\nanalysis of results, offering novel insights into bias in LLMs."}
{"id": "2507.06448", "pdf": "https://arxiv.org/pdf/2507.06448.pdf", "abs": "https://arxiv.org/abs/2507.06448", "title": "Perception-Aware Policy Optimization for Multimodal Reasoning", "authors": ["Zhenhailong Wang", "Xuehang Guo", "Sofia Stoica", "Haiyang Xu", "Hongru Wang", "Hyeonjeong Ha", "Xiusi Chen", "Yangyi Chen", "Ming Yan", "Fei Huang", "Heng Ji"], "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a\nhighly effective strategy for endowing Large Language Models (LLMs) with robust\nmulti-step reasoning abilities. However, its design and optimizations remain\ntailored to purely textual domains, resulting in suboptimal performance when\napplied to multimodal reasoning tasks. In particular, we observe that a major\nsource of error in current multimodal reasoning lies in the perception of\nvisual inputs. To address this bottleneck, we propose PAPO, a novel policy\ngradient algorithm that encourages the model to learn to perceive while\nlearning to reason. Specifically, we introduce the Implicit Perception Loss in\nthe form of a KL divergence term, which can be seamlessly plugged into\nmainstream RLVR algorithms such as GRPO and DAPO. Notably, PAPO does not rely\non additional data curation, reward models, or stronger teacher models. To\nfurther enhance the training stability of PAPO, we introduce the Double Entropy\nLoss, which effectively regularizes the new KL objective without compromising\nperformance. Despite its simplicity, PAPO yields significant overall\nimprovements of 4.4%-17.5% on diverse multimodal benchmarks. The improvements\nare more pronounced, approaching 8.0%-19.1%, on tasks with high vision\ndependency. We also observe a substantial reduction of 30.5% in perception\nerrors, indicating improved perceptual capabilities with PAPO. Overall, our\nwork introduces a deeper integration of perception-aware supervision into core\nlearning objectives and lays the groundwork for a new RL framework that\nencourages visually grounded reasoning. Code and data will be made publicly\navailable for research purposes. Project page:\nhttps://mikewangwzhl.github.io/PAPO."}
{"id": "2507.19595", "pdf": "https://arxiv.org/pdf/2507.19595.pdf", "abs": "https://arxiv.org/abs/2507.19595", "title": "Efficient Attention Mechanisms for Large Language Models: A Survey", "authors": ["Yutao Sun", "Zhenyu Li", "Yike Zhang", "Tengyu Pan", "Bowen Dong", "Yuyi Guo", "Jianyong Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "work in progress", "summary": "Transformer-based architectures have become the prevailing backbone of large\nlanguage models. However, the quadratic time and memory complexity of\nself-attention remains a fundamental obstacle to efficient long-context\nmodeling. To address this limitation, recent research has introduced two\nprincipal categories of efficient attention mechanisms. Linear attention\nmethods achieve linear complexity through kernel approximations, recurrent\nformulations, or fastweight dynamics, thereby enabling scalable inference with\nreduced computational overhead. Sparse attention techniques, in contrast, limit\nattention computation to selected subsets of tokens based on fixed patterns,\nblock-wise routing, or clustering strategies, enhancing efficiency while\npreserving contextual coverage. This survey provides a systematic and\ncomprehensive overview of these developments, integrating both algorithmic\ninnovations and hardware-level considerations. In addition, we analyze the\nincorporation of efficient attention into largescale pre-trained language\nmodels, including both architectures built entirely on efficient attention and\nhybrid designs that combine local and global components. By aligning\ntheoretical foundations with practical deployment strategies, this work aims to\nserve as a foundational reference for advancing the design of scalable and\nefficient language models."}
{"id": "2507.22924", "pdf": "https://arxiv.org/pdf/2507.22924.pdf", "abs": "https://arxiv.org/abs/2507.22924", "title": "Using Sentiment Analysis to Investigate Peer Feedback by Native and Non-Native English Speakers", "authors": ["Brittney Exline", "Melanie Duffin", "Brittany Harbison", "Chrissa da Gomez", "David Joyner"], "categories": ["cs.CL", "I.2.7; K.3.1"], "comment": null, "summary": "Graduate-level CS programs in the U.S. increasingly enroll international\nstudents, with 60.2 percent of master's degrees in 2023 awarded to non-U.S.\nstudents. Many of these students take online courses, where peer feedback is\nused to engage students and improve pedagogy in a scalable manner. Since these\ncourses are conducted in English, many students study in a language other than\ntheir first. This paper examines how native versus non-native English speaker\nstatus affects three metrics of peer feedback experience in online U.S.-based\ncomputing courses. Using the Twitter-roBERTa-based model, we analyze the\nsentiment of peer reviews written by and to a random sample of 500 students. We\nthen relate sentiment scores and peer feedback ratings to students' language\nbackground. Results show that native English speakers rate feedback less\nfavorably, while non-native speakers write more positively but receive less\npositive sentiment in return. When controlling for sex and age, significant\ninteractions emerge, suggesting that language background plays a modest but\ncomplex role in shaping peer feedback experiences."}
{"id": "2508.01473", "pdf": "https://arxiv.org/pdf/2508.01473.pdf", "abs": "https://arxiv.org/abs/2508.01473", "title": "TreeDiff: AST-Guided Code Generation with Diffusion LLMs", "authors": ["Yiming Zeng", "Jinghan Cao", "Zexin Li", "Yiming Chen", "Tao Ren", "Dawei Xiang", "Xidong Wu", "Shangqian Gao", "Tingting Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in diffusion-based language models have opened new\npossibilities for controllable and bidirectional sequence generation. These\nmodels provide an alternative to traditional autoregressive approaches by\nframing text generation as an iterative denoising process. However, applying\ndiffusion models to structured domains such as source code remains a\nsignificant challenge. Programming languages differ from natural language in\nthat they follow strict syntactic and semantic rules, with hierarchical\norganization that must be preserved for correctness. Standard token-level\ncorruption techniques used during training often ignore this structure, which\nmay hinder the model's ability to learn meaningful representations of code. To\naddress this limitation, we propose a syntax-aware diffusion framework that\nincorporates structural priors from Abstract Syntax Trees (ASTs) into the\ndenoising process. Instead of masking individual tokens at random, we\nselectively corrupt syntactically meaningful code spans derived from AST\nsubtrees. This enables the model to reconstruct programs in a way that respects\ngrammatical boundaries and captures long-range dependencies. Experimental\nresults demonstrate that syntax-aware corruption significantly improves\nsyntactic correctness, reconstruction accuracy, and generalization to unseen\ncode patterns. These findings highlight the potential of incorporating\nstructural information into diffusion-based training and suggest that\nsyntax-guided denoising is a promising direction for advancing diffusion-based\nlanguage models in code generation tasks."}
{"id": "2508.01674", "pdf": "https://arxiv.org/pdf/2508.01674.pdf", "abs": "https://arxiv.org/abs/2508.01674", "title": "CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions", "authors": ["Tae Soo Kim", "Yoonjoo Lee", "Yoonah Park", "Jiho Kim", "Young-Ho Kim", "Juho Kim"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Accepted to COLM 2025. Project Website: https://cupid.kixlab.org/", "summary": "Personalization of Large Language Models (LLMs) often assumes users hold\nstatic preferences that reflect globally in all tasks. In reality, humans hold\ndynamic preferences that change depending on the context. As users interact\nwith an LLM in various contexts, they naturally reveal their contextual\npreferences, which a model must infer and apply in future contexts to ensure\nalignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated\ninteraction session histories between users and LLM-based chat assistants. In\neach interaction session, the user provides a request in a specific context and\nexpresses their preference through multi-turn feedback. Given a new user\nrequest and prior interaction sessions, our benchmark assesses whether LLMs can\ninfer the preference relevant to this request and generate a response that\nsatisfies this preference. With CUPID, we evaluated 10 open and proprietary\nLLMs, revealing that state-of-the-art LLMs struggle to infer preferences from\nmulti-turn interactions and fail to discern what previous context is relevant\nto a new request -- under 50% precision and 65% recall. Our work highlights the\nneed to advance LLM capabilities for more contextually personalized\ninteractions and proposes CUPID as a resource to drive these improvements."}
{"id": "2508.02074", "pdf": "https://arxiv.org/pdf/2508.02074.pdf", "abs": "https://arxiv.org/abs/2508.02074", "title": "The SMeL Test: A simple benchmark for media literacy in language models", "authors": ["Gustaf Ahdritz", "Anat Kleiman"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The internet is rife with unattributed, deliberately misleading, or otherwise\nuntrustworthy content. Though large language models (LLMs) are often tasked\nwith autonomous web browsing, the extent to which they have learned the simple\nheuristics human researchers use to navigate this noisy environment is not\ncurrently known. In this paper, we introduce the Synthetic Media Literacy Test\n(SMeL Test), a minimal benchmark that tests the ability of language models to\nactively filter out untrustworthy information in context. We benchmark a\nvariety of commonly used instruction-tuned LLMs, including reasoning models,\nand find that no model consistently succeeds; while reasoning in particular is\nassociated with higher scores, even the best API model we test hallucinates up\nto 70% of the time. Remarkably, larger and more capable models do not\nnecessarily outperform their smaller counterparts. We hope our work sheds more\nlight on this important form of hallucination and guides the development of new\nmethods to combat it."}
{"id": "2508.02317", "pdf": "https://arxiv.org/pdf/2508.02317.pdf", "abs": "https://arxiv.org/abs/2508.02317", "title": "VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo", "authors": ["Qianli Ma", "Yaowei Zheng", "Zhelun Shi", "Zhongkai Zhao", "Bin Jia", "Ziyue Huang", "Zhiqi Lin", "Youjie Li", "Jiacheng Yang", "Yanghua Peng", "Zhi Zhang", "Xin Liu"], "categories": ["cs.CL", "cs.AI", "cs.DC"], "comment": null, "summary": "Recent advances in large language models (LLMs) have driven impressive\nprogress in omni-modal understanding and generation. However, training\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\narchitectures required to process diverse modalities, necessitating\nsophisticated system design for efficient large-scale training. Existing\nframeworks typically entangle model definition with parallel logic, incurring\nlimited scalability and substantial engineering overhead for end-to-end\nomni-modal training. We present VeOmni, a modular and efficient training\nframework to accelerate the development of omni-modal LLMs. VeOmni introduces\nmodel-centric distributed recipes that decouples communication from\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. VeOmni also\nfeatures a flexible configuration interface supporting seamless integration of\nnew modalities with minimal code change. Using VeOmni, a omni-modal\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\ntraining large omni-modal LLMs."}
{"id": "2508.03440", "pdf": "https://arxiv.org/pdf/2508.03440.pdf", "abs": "https://arxiv.org/abs/2508.03440", "title": "LLMs are Single-threaded Reasoners: Demystifying the Working Mechanism of Soft Thinking", "authors": ["Ch√ºnhung Wu", "Jinliang Lu", "Zixuan Ren", "Gangqiang Hu", "Zhi Wu", "Dai Dai", "Hua Wu"], "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 7 figures, working in progress", "summary": "Human cognition naturally engages with abstract and fluid concepts, whereas\nexisting reasoning models often rely on generating discrete tokens, potentially\nconstraining their expressive capabilities. Recent advancements aim to address\nthis limitation by enabling large language models (LLMs) to generate soft,\nabstract tokens, thus facilitating reasoning within a continuous concept space.\nThis paper explores the `Soft Thinking' capabilities of various LLMs by\nexamining the models' internal behavior using a suite of probing techniques.\nContrary to the common belief that Soft Thinking enables the simultaneous\nexploration of diverse reasoning paths, our findings reveal that LLMs\npredominantly rely on the most influential component of the soft inputs during\nsubsequent decoding steps. This reliance hinders the exploration of different\nreasoning paths and reduces vanilla Soft Thinking to a form of greedy decoding,\nobscuring the advantage of transmitting more information through Soft Tokens.\nTo tackle this issue, we explore sampling strategies to introduce\n\\emph{randomness}, employing methods such as Dirichlet resampling and the\nGumbel-Softmax trick. Our experiments demonstrate that incorporating randomness\ncan alleviate the limitations of vanilla approaches and unleash the potential\nof Soft Thinking. Notably, the Gumbel-Softmax trick provides adequate\nrandomness with controlled smoothness, resulting in superior performance across\neight reasoning benchmarks."}
{"id": "2508.03865", "pdf": "https://arxiv.org/pdf/2508.03865.pdf", "abs": "https://arxiv.org/abs/2508.03865", "title": "An Entity Linking Agent for Question Answering", "authors": ["Yajie Luo", "Yihong Wu", "Muzhi Li", "Fengran Mo", "Jia Ao Sun", "Xinyu Wang", "Liheng Ma", "Yingxue Zhang", "Jian-Yun Nie"], "categories": ["cs.CL"], "comment": "12 pages, 2 figures", "summary": "Some Question Answering (QA) systems rely on knowledge bases (KBs) to provide\naccurate answers. Entity Linking (EL) plays a critical role in linking natural\nlanguage mentions to KB entries. However, most existing EL methods are designed\nfor long contexts and do not perform well on short, ambiguous user questions in\nQA tasks. We propose an entity linking agent for QA, based on a Large Language\nModel that simulates human cognitive workflows. The agent actively identifies\nentity mentions, retrieves candidate entities, and makes decision. To verify\nthe effectiveness of our agent, we conduct two experiments: tool-based entity\nlinking and QA task evaluation. The results confirm the robustness and\neffectiveness of our agent."}
{"id": "2508.04088", "pdf": "https://arxiv.org/pdf/2508.04088.pdf", "abs": "https://arxiv.org/abs/2508.04088", "title": "GM-PRM: A Generative Multimodal Process Reward Model for Multimodal Mathematical Reasoning", "authors": ["Jianghangfan Zhang", "Yibo Yan", "Kening Zheng", "Xin Zou", "Song Dai", "Xuming Hu"], "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities\nbut often struggle with complex, multi-step mathematical reasoning, where minor\nerrors in visual perception or logical deduction can lead to complete failure.\nWhile Process Reward Models (PRMs) offer step-by-step supervision, existing\nmultimodal PRMs are limited to being binary verifiers that can identify but not\ncorrect errors, offering little explanatory power. To address these\ndeficiencies, we introduce the Generative Multimodal Process Reward Model\n(GM-PRM), a novel paradigm that transforms the PRM from a passive judge into an\nactive reasoning collaborator. Instead of a simple scalar score, GM-PRM\nprovides a fine-grained, interpretable analysis of each reasoning step,\nevaluating its step intent, visual alignment, and logical soundness. More\ncritically, GM-PRM is trained to generate a corrected version of the first\nerroneous step it identifies. This unique corrective capability enables our new\ntest-time inference strategy, Refined Best-of-N (Refined-BoN). This framework\nactively enhances solution quality by using the PRM's generated correction to\nguide the policy model toward a more promising reasoning trajectory, thereby\nimproving the diversity and correctness of the solution pool. We demonstrate\nthat GM-PRM achieves state-of-the-art results on multiple multimodal math\nbenchmarks, significantly boosting policy model performance with remarkable\ndata efficiency, requiring only a 20K-sample training dataset. Our code will be\nreleased upon acceptance."}
{"id": "2508.04530", "pdf": "https://arxiv.org/pdf/2508.04530.pdf", "abs": "https://arxiv.org/abs/2508.04530", "title": "Balancing Stylization and Truth via Disentangled Representation Steering", "authors": ["Chenglei Shen", "Zhongxiang Sun", "Teng Shi", "Xiao Zhang", "Jun Xu"], "categories": ["cs.CL"], "comment": null, "summary": "Generating stylized large language model (LLM) responses via representation\nediting is a promising way for fine-grained output control. However, there\nexists an inherent trade-off: imposing a distinctive style often degrades\ntruthfulness. Existing representation editing methods, by naively injecting\nstyle signals, overlook this collateral impact and frequently contaminate the\nmodel's core truthfulness representations, resulting in reduced answer\ncorrectness. We term this phenomenon stylization-induced truthfulness collapse.\nWe attribute this issue to latent coupling between style and truth directions\nin certain key attention heads, and propose StyliTruth, a mechanism that\npreserves stylization while keeping truthfulness intact. StyliTruth separates\nthe style-relevant and truth-relevant subspaces in the model's representation\nspace via an orthogonal deflation process. This decomposition enables\nindependent control of style and truth in their own subspaces, minimizing\ninterference. By designing adaptive, token-level steering vectors within each\nsubspace, we dynamically and precisely control the generation process to\nmaintain both stylistic fidelity and truthfulness. We validate our method on\nmultiple styles and languages. Extensive experiments and analyses show that\nStyliTruth significantly reduces stylization-induced truthfulness collapse and\noutperforms existing inference-time intervention methods in balancing style\nadherence with truthfulness."}
{"id": "2508.04632", "pdf": "https://arxiv.org/pdf/2508.04632.pdf", "abs": "https://arxiv.org/abs/2508.04632", "title": "IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards", "authors": ["Xu Guo", "Tianyi Liang", "Tong Jian", "Xiaogui Yang", "Ling-I Wu", "Chenhui Li", "Zhihui Lu", "Qipeng Guo", "Kai Chen"], "categories": ["cs.CL"], "comment": "7 pages, 4 figures", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction\nfollowing capabilities of large language models (LLMs), but suffers from\ntraining inefficiency due to inadequate difficulty assessment. Moreover, RLVR\nis prone to over-optimization, where LLMs exploit verification shortcuts\nwithout aligning to the actual intent of user instructions. We introduce\nInstruction Following Decorator (IFDecorator}, a framework that wraps RLVR\ntraining into a robust and sample-efficient pipeline. It consists of three\ncomponents: (1) a cooperative-adversarial data flywheel that co-evolves\ninstructions and hybrid verifications, generating progressively more\nchallenging instruction-verification pairs; (2) IntentCheck, a bypass module\nenforcing intent alignment; and (3) trip wires, a diagnostic mechanism that\ndetects reward hacking via trap instructions, which trigger and capture\nshortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves\n87.43% accuracy on IFEval, outperforming larger proprietary models such as\nGPT-4o. Additionally, we demonstrate substantial improvements on FollowBench\nwhile preserving general capabilities. Our trip wires show significant\nreductions in reward hacking rates. We will release models, code, and data for\nfuture research."}
{"id": "2405.15877", "pdf": "https://arxiv.org/pdf/2405.15877.pdf", "abs": "https://arxiv.org/abs/2405.15877", "title": "Basis Selection: Low-Rank Decomposition of Pretrained Large Language Models for Target Applications", "authors": ["Yang Li", "Daniel Agyei Asante", "Changsheng Zhao", "Ernie Chang", "Yangyang Shi", "Vikas Chandra"], "categories": ["cs.LG", "cs.AR", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) significantly enhance the performance of various\napplications, but they are computationally intensive and energy-demanding. This\nmakes it challenging to deploy them on devices with limited resources, such as\npersonal computers and mobile/wearable devices, and results in substantial\ninference costs in resource-rich environments like cloud servers. To extend the\nuse of LLMs, we introduce a low-rank decomposition approach to effectively\ncompress these models, tailored to the requirements of specific applications.\nWe observe that LLMs pretrained on general datasets contain many redundant\ncomponents not needed for particular applications. Our method focuses on\nidentifying and removing these redundant parts, retaining only the necessary\nelements for the target applications. Specifically, we represent the weight\nmatrices of LLMs as a linear combination of base components. We then prune the\nirrelevant bases and enhance the model with new bases beneficial for specific\napplications. Deep compression results on the Llama 2-7b and -13B models,\nconducted on target applications including mathematical reasoning and code\ngeneration, show that our method significantly reduces model size while\nmaintaining comparable accuracy to state-of-the-art low-rank compression\ntechniques."}
{"id": "2410.03864", "pdf": "https://arxiv.org/pdf/2410.03864.pdf", "abs": "https://arxiv.org/abs/2410.03864", "title": "DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning Trajectories Search", "authors": ["Murong Yue", "Wenlin Yao", "Haitao Mi", "Dian Yu", "Ziyu Yao", "Dong Yu"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to ICLR 2025", "summary": "Enhancing the capability of large language models (LLMs) in reasoning has\ngained significant attention in recent years. Previous studies have\ndemonstrated the effectiveness of various prompting strategies in aiding LLMs\nin reasoning (called \"reasoning actions\"), such as step-by-step thinking,\nreflecting before answering, solving with programs, and their combinations.\nHowever, these approaches often applied static, predefined reasoning actions\nuniformly to all questions, without considering the specific characteristics of\neach question or the capability of the task-solving LLM. In this paper, we\npropose DOTS, an approach enabling LLMs to reason dynamically via optimal\nreasoning trajectory search, tailored to the specific characteristics of each\nquestion and the inherent capability of the task-solving LLM. Our approach\ninvolves three key steps: i) defining atomic reasoning action modules that can\nbe composed into various reasoning action trajectories; ii) searching for the\noptimal action trajectory for each training question through iterative\nexploration and evaluation for the specific task-solving LLM; and iii) using\nthe collected optimal trajectories to train an LLM to plan for the reasoning\ntrajectories of unseen questions. In particular, we propose two learning\nparadigms, i.e., fine-tuning an external LLM as a planner to guide the\ntask-solving LLM, or directly fine-tuning the task-solving LLM with an\ninternalized capability for reasoning actions planning. Our experiments across\neight reasoning tasks show that our method consistently outperforms static\nreasoning techniques and the vanilla instruction tuning approach. Further\nanalysis reveals that our method enables LLMs to adjust their computation based\non problem complexity, allocating deeper thinking and reasoning to harder\nproblems."}
{"id": "2411.18651", "pdf": "https://arxiv.org/pdf/2411.18651.pdf", "abs": "https://arxiv.org/abs/2411.18651", "title": "Verbalized Representation Learning for Interpretable Few-Shot Generalization", "authors": ["Cheng-Fu Yang", "Da Yin", "Wenbo Hu", "Heng Ji", "Nanyun Peng", "Bolei Zhou", "Kai-Wei Chang"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Accepted to ICCV 2025", "summary": "Humans recognize objects after observing only a few examples, a remarkable\ncapability enabled by their inherent language understanding of the real-world\nenvironment. Developing verbalized and interpretable representation can\nsignificantly improve model generalization in low-data settings. In this work,\nwe propose Verbalized Representation Learning (VRL), a novel approach for\nautomatically extracting human-interpretable features for object recognition\nusing few-shot data. Our method uniquely captures inter-class differences and\nintra-class commonalities in the form of natural language by employing a\nVision-Language Model (VLM) to identify key discriminative features between\ndifferent classes and shared characteristics within the same class. These\nverbalized features are then mapped to numeric vectors through the VLM. The\nresulting feature vectors can be further utilized to train and infer with\ndownstream classifiers. Experimental results show that, at the same model\nscale, VRL achieves a 24% absolute improvement over prior state-of-the-art\nmethods while using 95% less data and a smaller mode. Furthermore, compared to\nhuman-labeled attributes, the features learned by VRL exhibit a 20% absolute\ngain when used for downstream classification tasks. Code is available at:\nhttps://github.com/joeyy5588/VRL/tree/main."}
{"id": "2412.20367", "pdf": "https://arxiv.org/pdf/2412.20367.pdf", "abs": "https://arxiv.org/abs/2412.20367", "title": "Enhancing Code LLMs with Reinforcement Learning in Code Generation: A Survey", "authors": ["Junqiao Wang", "Zeng Zhang", "Yangfan He", "Zihao Zhang", "Xinyuan Song", "Yuyang Song", "Tianyu Shi", "Yuchen Li", "Hengyuan Xu", "Kunyu Wu", "Xin Yi", "Zhongwei Wan", "Xinhang Yuan", "Zijun Wang", "Kuan Lu", "Menghao Huo", "Tang Jingqun", "Guangwu Qian", "Keqin Li", "Qiuwu Chen", "Lewei He"], "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "With the rapid evolution of large language models (LLM), reinforcement\nlearning (RL) has emerged as a pivotal technique for code generation and\noptimization in various domains. This paper presents a systematic survey of the\napplication of RL in code optimization and generation, highlighting its role in\nenhancing compiler optimization, resource allocation, and the development of\nframeworks and tools. Subsequent sections first delve into the intricate\nprocesses of compiler optimization, where RL algorithms are leveraged to\nimprove efficiency and resource utilization. The discussion then progresses to\nthe function of RL in resource allocation, emphasizing register allocation and\nsystem optimization. We also explore the burgeoning role of frameworks and\ntools in code generation, examining how RL can be integrated to bolster their\ncapabilities. This survey aims to serve as a comprehensive resource for\nresearchers and practitioners interested in harnessing the power of RL to\nadvance code generation and optimization techniques."}
{"id": "2502.16435", "pdf": "https://arxiv.org/pdf/2502.16435.pdf", "abs": "https://arxiv.org/abs/2502.16435", "title": "Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs", "authors": ["Jen-Tse Huang", "Dasen Dai", "Jen-Yuan Huang", "Youliang Yuan", "Xiaoyuan Liu", "Wenxuan Wang", "Wenxiang Jiao", "Pinjia He", "Zhaopeng Tu", "Haodong Duan"], "categories": ["cs.CV", "cs.CL"], "comment": "Update: Evaluated 20 MLLMs; Added generated test cases", "summary": "Despite significant progress on popular multimodal benchmarks,\nstate-of-the-art Multimodal Large Language Models (MLLMs) continue to struggle\nwith basic visual reasoning tasks that are trivially solved by humans, such as\nrecognizing spatial relationships. To systematically investigate this gap, we\nintroduce VisFactor, a benchmark that digitizes 20 vision-centric subtests from\na well-established cognitive psychology assessment. These subtests span four\ncore domains of human visual cognition: (1) Visualization and Spatial\nProcessing, (2) Perceptual and Closure, (3) Memory, and (4) Reasoning. We\nevaluate 20 frontier MLLMs from GPT, Gemini, Claude, LLaMA, Qwen, and SEED\nfamilies. The best-performing model achieves a score of only 25.19 out of 100,\nwith consistent failures on tasks such as mental rotation, spatial relation\ninference, and figure-ground discrimination, regardless of model size or\nprompting strategy. These findings suggest that current MLLM performance gains\non high-level benchmarks do not reflect human-like low-level visual cognition,\nchallenging the assumption that large-scale pretraining naturally induces\ngestalt-like perceptual capabilities. The dataset and evaluation toolkit are\npublicly available at: https://github.com/CUHK-ARISE/VisFactor."}
{"id": "2503.00600", "pdf": "https://arxiv.org/pdf/2503.00600.pdf", "abs": "https://arxiv.org/abs/2503.00600", "title": "Semantic Integrity Constraints: Declarative Guardrails for AI-Augmented Data Processing Systems", "authors": ["Alexander W. Lee", "Justin Chan", "Michael Fu", "Nicolas Kim", "Akshay Mehta", "Deepti Raghavan", "Ugur Cetintemel"], "categories": ["cs.DB", "cs.AI", "cs.CL"], "comment": null, "summary": "AI-augmented data processing systems (DPSs) integrate large language models\n(LLMs) into query pipelines, allowing powerful semantic operations on\nstructured and unstructured data. However, the reliability (a.k.a. trust) of\nthese systems is fundamentally challenged by the potential for LLMs to produce\nerrors, limiting their adoption in critical domains. To help address this\nreliability bottleneck, we introduce semantic integrity constraints (SICs) -- a\ndeclarative abstraction for specifying and enforcing correctness conditions\nover LLM outputs in semantic queries. SICs generalize traditional database\nintegrity constraints to semantic settings, supporting common types of\nconstraints, such as grounding, soundness, and exclusion, with both reactive\nand proactive enforcement strategies.\n  We argue that SICs provide a foundation for building reliable and auditable\nAI-augmented data systems. Specifically, we present a system design for\nintegrating SICs into query planning and runtime execution and discuss its\nrealization in AI-augmented DPSs. To guide and evaluate our vision, we outline\nseveral design goals -- covering criteria around expressiveness, runtime\nsemantics, integration, performance, and enterprise-scale applicability -- and\ndiscuss how our framework addresses each, along with open research challenges."}
{"id": "2503.09032", "pdf": "https://arxiv.org/pdf/2503.09032.pdf", "abs": "https://arxiv.org/abs/2503.09032", "title": "Teaching LLMs How to Learn with Contextual Fine-Tuning", "authors": ["Younwoo Choi", "Muhammad Adil Asif", "Ziwen Han", "John Willes", "Rahul G. Krishnan"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICLR 2025", "summary": "Prompting Large Language Models (LLMs), or providing context on the expected\nmodel of operation, is an effective way to steer the outputs of such models to\nsatisfy human desiderata after they have been trained. But in rapidly evolving\ndomains, there is often need to fine-tune LLMs to improve either the kind of\nknowledge in their memory or their abilities to perform open ended reasoning in\nnew domains. When human's learn new concepts, we often do so by linking the new\nmaterial that we are studying to concepts we have already learned before. To\nthat end, we ask, \"can prompting help us teach LLMs how to learn\". In this\nwork, we study a novel generalization of instruction tuning, called contextual\nfine-tuning, to fine-tune LLMs. Our method leverages instructional prompts\ndesigned to mimic human cognitive strategies in learning and problem-solving to\nguide the learning process during training, aiming to improve the model's\ninterpretation and understanding of domain-specific knowledge. We empirically\ndemonstrate that this simple yet effective modification improves the ability of\nLLMs to be fine-tuned rapidly on new datasets both within the medical and\nfinancial domains."}
{"id": "2504.04699", "pdf": "https://arxiv.org/pdf/2504.04699.pdf", "abs": "https://arxiv.org/abs/2504.04699", "title": "R2Vul: Learning to Reason about Software Vulnerabilities with Reinforcement Learning and Structured Reasoning Distillation", "authors": ["Martin Weyssow", "Chengran Yang", "Junkai Chen", "Ratnadira Widyasari", "Ting Zhang", "Huihui Huang", "Huu Hung Nguyen", "Yan Naing Tun", "Tan Bui", "Yikun Li", "Ang Han Wei", "Frank Liauw", "Eng Lieh Ouh", "Lwin Khin Shar", "David Lo"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown promising performance in software\nvulnerability detection, yet their reasoning capabilities remain unreliable. We\npropose R2Vul, a method that combines reinforcement learning from AI feedback\n(RLAIF) and structured reasoning distillation to teach small code LLMs to\ndetect vulnerabilities while generating security-aware explanations. Unlike\nprior chain-of-thought and instruction tuning approaches, R2Vul rewards\nwell-founded over deceptively plausible vulnerability explanations through\nRLAIF, which results in more precise detection and high-quality reasoning\ngeneration. To support RLAIF, we construct the first multilingual preference\ndataset for vulnerability detection, comprising 18,000 high-quality samples in\nC\\#, JavaScript, Java, Python, and C. We evaluate R2Vul across five programming\nlanguages and against four static analysis tools, eight state-of-the-art\nLLM-based baselines, and various fine-tuning approaches. Our results\ndemonstrate that a 1.5B R2Vul model exceeds the performance of its 32B teacher\nmodel and leading commercial LLMs such as Claude-4-Opus. Furthermore, we\nintroduce a lightweight calibration step that reduces false positive rates\nunder varying imbalanced data distributions. Finally, through qualitative\nanalysis, we show that both LLM and human evaluators consistently rank R2Vul\nmodel's reasoning higher than other reasoning-based baselines."}
{"id": "2504.10496", "pdf": "https://arxiv.org/pdf/2504.10496.pdf", "abs": "https://arxiv.org/abs/2504.10496", "title": "ArXivBench: When You Should Avoid Using ChatGPT for Academic Writing", "authors": ["Ning Li", "Jingran Zhang", "Justin Cui"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) demonstrate strong capabilities in reasoning and\nquestion answering, yet their tendency to generate factually incorrect content\nremains a critical challenge. This study evaluates proprietary and open-source\nLLMs on generating relevant research papers with accurate arXiv links. Our\nevaluation reveals critical academic risks: LLMs frequently generate incorrect\narXiv links or references to non-existent papers, fundamentally undermining\ntheir ability to properly attribute research contributions to the actual\nauthors. We introduce arXivBench, a benchmark specifically designed to assess\nLLM performance across eight major subject categories on arXiv and five\nsubfields within computer science, one of the most popular categories among\nthem. Our findings show concerning accuracy variations across subjects, with\nClaude-3.5-Sonnet exhibiting a substantial advantage in generating both\nrelevant and accurate responses. Notably, most LLMs perform significantly\nbetter in Artificial Intelligence than other subfields. This benchmark provides\na standardized tool for evaluating LLM reliability in scientific contexts,\npromoting more dependable academic use in research environments. Our code and\ndataset are available at https://github.com/liningresearch/arXivBench and\nhttps://huggingface.co/datasets/arXivBenchLLM/arXivBench."}
{"id": "2504.10512", "pdf": "https://arxiv.org/pdf/2504.10512.pdf", "abs": "https://arxiv.org/abs/2504.10512", "title": "JEPA4Rec: Learning Effective Language Representations for Sequential Recommendation via Joint Embedding Predictive Architecture", "authors": ["Minh-Anh Nguyen", "Dung D. Le"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Language representation learning has emerged as a promising approach for\nsequential recommendation, thanks to its ability to learn generalizable\nrepresentations. However, despite its advantages, this approach still struggles\nwith data sparsity and a limited understanding of common-sense user\npreferences. To address these limitations, we propose $\\textbf{JEPA4Rec}$, a\nframework that combines $\\textbf{J}$oint $\\textbf{E}$mbedding\n$\\textbf{P}$redictive $\\textbf{A}$rchitecture with language modeling of item\ntextual descriptions. JEPA4Rec captures semantically rich and transferable\nrepresentations, improving recommendation performance and reducing reliance on\nlarge-scale pre-training data. Specifically, JEPA4Rec represents items as text\nsentences by flattening descriptive information such as $\\textit{title,\ncategory}$, and other attributes. To encode these sentences, we employ a\nbidirectional Transformer encoder with modified embedding layers tailored for\ncapturing item information in recommendation datasets. We apply masking to text\nsentences and use them to predict the representations of the unmasked\nsentences, helping the model learn generalizable item embeddings. To further\nimprove recommendation performance and language understanding, we employ a\ntwo-stage training strategy incorporating self-supervised learning losses.\nExperiments on six real-world datasets demonstrate that JEPA4Rec consistently\noutperforms state-of-the-art methods, particularly in cross-domain,\ncross-platform, and low-resource scenarios."}
{"id": "2504.14147", "pdf": "https://arxiv.org/pdf/2504.14147.pdf", "abs": "https://arxiv.org/abs/2504.14147", "title": "Explainable Recommendation with Simulated Human Feedback", "authors": ["Jiakai Tang", "Jingsen Zhang", "Zihang Tian", "Xueyang Feng", "Lei Wang", "Xu Chen"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advancements in explainable recommendation have greatly bolstered user\nexperience by elucidating the decision-making rationale. However, the existing\nmethods actually fail to provide effective feedback signals for potentially\nbetter or worse generated explanations due to their reliance on traditional\nsupervised learning paradigms in sparse interaction data. To address these\nissues, we propose a novel human-like feedback-driven optimization framework.\nThis framework employs a dynamic interactive optimization mechanism for\nachieving human-centered explainable requirements without incurring high labor\ncosts. Specifically, we propose to utilize large language models (LLMs) as\nhuman simulators to predict human-like feedback for guiding the learning\nprocess. To enable the LLMs to deeply understand the task essence and meet\nuser's diverse personalized requirements, we introduce a human-induced\ncustomized reward scoring method, which helps stimulate the language\nunderstanding and logical reasoning capabilities of LLMs. Furthermore,\nconsidering the potential conflicts between different perspectives of\nexplanation quality, we introduce a principled Pareto optimization that\ntransforms the multi-perspective quality enhancement task into a\nmulti-objective optimization problem for improving explanation performance. At\nlast, to achieve efficient model training, we design an off-policy optimization\npipeline. By incorporating a replay buffer and addressing the data distribution\nbiases, we can effectively improve data utilization and enhance model\ngenerality. Extensive experiments on four datasets demonstrate the superiority\nof our approach."}
{"id": "2505.16086", "pdf": "https://arxiv.org/pdf/2505.16086.pdf", "abs": "https://arxiv.org/abs/2505.16086", "title": "Optimizing LLM-Based Multi-Agent System with Textual Feedback: A Case Study on Software Development", "authors": ["Ming Shen", "Raphael Shu", "Anurag Pratik", "James Gung", "Yubin Ge", "Monica Sunkara", "Yi Zhang"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We have seen remarkable progress in large language models (LLMs) empowered\nmulti-agent systems solving complex tasks necessitating cooperation among\nexperts with diverse skills. However, optimizing LLM-based multi-agent systems\nremains challenging. In this work, we perform an empirical case study on group\noptimization of role-based multi-agent systems utilizing natural language\nfeedback for challenging software development tasks under various evaluation\ndimensions. We propose a two-step agent prompts optimization pipeline:\nidentifying underperforming agents with their failure explanations utilizing\ntextual feedback and then optimizing system prompts of identified agents\nutilizing failure explanations. We then study the impact of various\noptimization settings on system performance with two comparison groups: online\nagainst offline optimization and individual against group optimization. For\ngroup optimization, we study two prompting strategies: one-pass and multi-pass\nprompting optimizations. Overall, we demonstrate the effectiveness of our\noptimization method for role-based multi-agent systems tackling software\ndevelopment tasks evaluated on diverse evaluation dimensions, and we\ninvestigate the impact of diverse optimization settings on group behaviors of\nthe multi-agent systems to provide practical insights for future development."}
{"id": "2506.04450", "pdf": "https://arxiv.org/pdf/2506.04450.pdf", "abs": "https://arxiv.org/abs/2506.04450", "title": "Learning to Diagnose Privately: DP-Powered LLMs for Radiology Report Classification", "authors": ["Payel Bhattacharjee", "Fengwei Tian", "Geoffrey D. Rubin", "Joseph Y. Lo", "Nirav Merchant", "Heidi Hanson", "John Gounley", "Ravi Tandon"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": "18 pages, 5 figures, 2 tables", "summary": "Purpose: This study proposes a framework for fine-tuning large language\nmodels (LLMs) with differential privacy (DP) to perform multi-abnormality\nclassification on radiology report text. By injecting calibrated noise during\nfine-tuning, the framework seeks to mitigate the privacy risks associated with\nsensitive patient data and protect against data leakage while maintaining\nclassification performance. Materials and Methods: We used 50,232 radiology\nreports from the publicly available MIMIC-CXR chest radiography and CT-RATE\ncomputed tomography datasets, collected between 2011 and 2019. Fine-tuning of\nLLMs was conducted to classify 14 labels from MIMIC-CXR dataset, and 18 labels\nfrom CT-RATE dataset using Differentially Private Low-Rank Adaptation (DP-LoRA)\nin high and moderate privacy regimes (across a range of privacy budgets =\n{0.01, 0.1, 1.0, 10.0}). Model performance was evaluated using weighted F1\nscore across three model architectures: BERT-medium, BERT-small, and\nALBERT-base. Statistical analyses compared model performance across different\nprivacy levels to quantify the privacy-utility trade-off. Results: We observe a\nclear privacy-utility trade-off through our experiments on 2 different datasets\nand 3 different models. Under moderate privacy guarantees the DP fine-tuned\nmodels achieved comparable weighted F1 scores of 0.88 on MIMIC-CXR and 0.59 on\nCT-RATE, compared to non-private LoRA baselines of 0.90 and 0.78, respectively.\nConclusion: Differentially private fine-tuning using LoRA enables effective and\nprivacy-preserving multi-abnormality classification from radiology reports,\naddressing a key challenge in fine-tuning LLMs on sensitive medical data."}
{"id": "2507.15844", "pdf": "https://arxiv.org/pdf/2507.15844.pdf", "abs": "https://arxiv.org/abs/2507.15844", "title": "Hierarchical Budget Policy Optimization for Adaptive Reasoning", "authors": ["Shangke Lyu", "Linjuan Wu", "Yuchen Yan", "Xingyu Wu", "Hao Li", "Yongliang Shen", "Peisheng Jiang", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "categories": ["cs.AI", "cs.CL"], "comment": "Code: https://github.com/zju-real/hbpo Project\n  Page:https://zju-real.github.io/hbpo/", "summary": "Large reasoning models achieve remarkable performance through extensive\nchain-of-thought generation, yet they suffer from a critical inefficiency:\napplying uniformly extensive reasoning regardless of problem complexity. We\npresent Hierarchical Budget Policy Optimization (HBPO), a reinforcement\nlearning framework that enables models to learn problem-specific reasoning\ndepths without sacrificing capability. Unlike existing approaches that impose\nrigid constraints or rely on discrete mode selection, HBPO partitions the\nexploration space into budget-constrained hierarchies (512-2560 tokens), each\nwith differentiated reward structures that preserve both efficiency incentives\nand reasoning capabilities. This design addresses a fundamental challenge in\nefficient reasoning training: traditional length penalties systematically bias\nmodels away from necessary long reasoning paths, causing exploration space\ncollapse. Through hierarchical sampling and budget-aware rewards, HBPO\nmaintains exploration diversity while teaching models to recognize when\nextended deliberation is warranted. Extensive experiments demonstrate that HBPO\nreduces average token usage by up to 60.6% while improving accuracy by 3.14%\nacross four reasoning benchmarks. Most notably, HBPO exhibits emergent adaptive\nbehavior where models automatically adjust reasoning depth based on problem\ncomplexity. Our results suggest that reasoning efficiency and capability are\nnot inherently conflicting, and can be simultaneously optimized through\nappropriately structured hierarchical training that preserves exploration\ndiversity."}
{"id": "2507.18576", "pdf": "https://arxiv.org/pdf/2507.18576.pdf", "abs": "https://arxiv.org/abs/2507.18576", "title": "SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\\circ}$ Law", "authors": ["Shanghai AI Lab", ":", "Yicheng Bao", "Guanxu Chen", "Mingkang Chen", "Yunhao Chen", "Chiyu Chen", "Lingjie Chen", "Sirui Chen", "Xinquan Chen", "Jie Cheng", "Yu Cheng", "Dengke Deng", "Yizhuo Ding", "Dan Ding", "Xiaoshan Ding", "Yi Ding", "Zhichen Dong", "Lingxiao Du", "Yuyu Fan", "Xinshun Feng", "Yanwei Fu", "Yuxuan Gao", "Ruijun Ge", "Tianle Gu", "Lujun Gui", "Jiaxuan Guo", "Qianxi He", "Yuenan Hou", "Xuhao Hu", "Hong Huang", "Kaichen Huang", "Shiyang Huang", "Yuxian Jiang", "Shanzhe Lei", "Jie Li", "Lijun Li", "Hao Li", "Juncheng Li", "Xiangtian Li", "Yafu Li", "Lingyu Li", "Xueyan Li", "Haotian Liang", "Dongrui Liu", "Qihua Liu", "Zhixuan Liu", "Bangwei Liu", "Huacan Liu", "Yuexiao Liu", "Zongkai Liu", "Chaochao Lu", "Yudong Lu", "Xiaoya Lu", "Zhenghao Lu", "Qitan Lv", "Caoyuan Ma", "Jiachen Ma", "Xiaoya Ma", "Zhongtian Ma", "Lingyu Meng", "Ziqi Miao", "Yazhe Niu", "Yuezhang Peng", "Yuan Pu", "Han Qi", "Chen Qian", "Xingge Qiao", "Jingjing Qu", "Jiashu Qu", "Wanying Qu", "Wenwen Qu", "Xiaoye Qu", "Qihan Ren", "Qingnan Ren", "Qingyu Ren", "Jing Shao", "Wenqi Shao", "Shuai Shao", "Dongxing Shi", "Xin Song", "Xinhao Song", "Yan Teng", "Xuan Tong", "Yingchun Wang", "Xuhong Wang", "Shujie Wang", "Xin Wang", "Yige Wang", "Yixu Wang", "Yuanfu Wang", "Futing Wang", "Ruofan Wang", "Wenjie Wang", "Yajie Wang", "Muhao Wei", "Xiaoyu Wen", "Fenghua Weng", "Yuqi Wu", "Yingtong Xiong", "Xingcheng Xu", "Chao Yang", "Yue Yang", "Yang Yao", "Yulei Ye", "Zhenyun Yin", "Yi Yu", "Bo Zhang", "Qiaosheng Zhang", "Jinxuan Zhang", "Yexin Zhang", "Yinqiang Zheng", "Hefeng Zhou", "Zhanhui Zhou", "Pengyu Zhu", "Qingzi Zhu", "Yubo Zhu", "Bowen Zhou"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "47 pages, 18 figures, authors are listed in alphabetical order by\n  their last names; v3 modifies minor issues", "summary": "We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that\ndemonstrates the coevolution of capabilities and safety. It is developed by our\nproposed SafeLadder framework, which incorporates large-scale, progressive,\nsafety-oriented reinforcement learning post-training, supported by a suite of\nmulti-principled verifiers. Unlike previous alignment methods such as RLHF that\nsimply learn human preferences, SafeLadder enables SafeWork-R1 to develop\nintrinsic safety reasoning and self-reflection abilities, giving rise to safety\n`aha' moments. Notably, SafeWork-R1 achieves an average improvement of\n$46.54\\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks\nwithout compromising general capabilities, and delivers state-of-the-art safety\nperformance compared to leading proprietary models such as GPT-4.1 and Claude\nOpus 4. To further bolster its reliability, we implement two distinct\ninference-time intervention methods and a deliberative search mechanism,\nenforcing step-level verification. Finally, we further develop\nSafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and\nSafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and\ncapability can co-evolve synergistically, highlighting the generalizability of\nour framework in building robust, reliable, and trustworthy general-purpose AI."}
{"id": "2507.23336", "pdf": "https://arxiv.org/pdf/2507.23336.pdf", "abs": "https://arxiv.org/abs/2507.23336", "title": "DSBC : Data Science task Benchmarking with Context engineering", "authors": ["Ram Mohan Rao Kadiyala", "Siddhant Gupta", "Jebish Purbey", "Giulio Martini", "Ali Shafique", "Suman Debnath", "Hamza Farooq"], "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": "32 pages", "summary": "Recent advances in large language models (LLMs) have significantly impacted\ndata science workflows, giving rise to specialized data science agents designed\nto automate analytical tasks. Despite rapid adoption, systematic benchmarks\nevaluating the efficacy and limitations of these agents remain scarce. In this\npaper, we introduce a comprehensive benchmark specifically crafted to reflect\nreal-world user interactions with data science agents by observing usage of our\ncommercial applications. We evaluate three LLMs: Claude-4.0-Sonnet,\nGemini-2.5-Flash, and OpenAI-o4-Mini across three approaches: zero-shot with\ncontext engineering, multi-step with context engineering, and with SmolAgent.\nOur benchmark assesses performance across a diverse set of eight data science\ntask categories, additionally exploring the sensitivity of models to common\nprompting issues, such as data leakage and slightly ambiguous instructions. We\nfurther investigate the influence of temperature parameters on overall and\ntask-specific outcomes for each model and approach. Our findings reveal\ndistinct performance disparities among the evaluated models and methodologies,\nhighlighting critical factors that affect practical deployment. The benchmark\ndataset and evaluation framework introduced herein aim to provide a foundation\nfor future research of more robust and effective data science agents."}
