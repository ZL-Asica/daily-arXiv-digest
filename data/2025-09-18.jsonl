{"id": "2509.13323", "pdf": "https://arxiv.org/pdf/2509.13323.pdf", "abs": "https://arxiv.org/abs/2509.13323", "title": "AI Behavioral Science", "authors": ["Matthew O. Jackson", "Qiaozhu Me", "Stephanie W. Wang", "Yutong Xie", "Walter Yuan", "Seth Benzell", "Erik Brynjolfsson", "Colin F. Camerer", "James Evans", "Brian Jabarian", "Jon Kleinberg", "Juanjuan Meng", "Sendhil Mullainathan", "Asuman Ozdaglar", "Thomas Pfeiffer", "Moshe Tennenholtz", "Robb Willer", "Diyi Yang", "Teng Ye"], "categories": ["cs.HC", "econ.GN", "q-fin.EC"], "comment": null, "summary": "We discuss the three main areas comprising the new and emerging field of \"AI\nBehavioral Science\". This includes not only how AI can enhance research in the\nbehavioral sciences, but also how the behavioral sciences can be used to study\nand better design AI and to understand how the world will change as AI and\nhumans interact in increasingly layered and complex ways."}
{"id": "2509.13324", "pdf": "https://arxiv.org/pdf/2509.13324.pdf", "abs": "https://arxiv.org/abs/2509.13324", "title": "Designing Psychometric Bias Measures for ChatBots: An Application to Racial Bias Measurement", "authors": ["Mouhacine Benosman"], "categories": ["cs.HC"], "comment": "7 pages, 1 figure", "summary": "Artificial intelligence (AI), particularly in the form of large language\nmodels (LLMs) or chatbots, has become increasingly integrated into our daily\nlives. In the past five years, several LLMs have been introduced, including\nChatGPT by OpenAI, Claude by Anthropic, and Llama by Meta, among others. These\nmodels have the potential to be employed across a wide range of human-machine\ninteraction applications, such as chatbots for information retrieval,\nassistance in corporate hiring decisions, college admissions, financial loan\napprovals, parole determinations, and even in medical fields like psychotherapy\ndelivered through chatbots. The key question is whether these chatbots will\ninteract with humans in a bias-free manner or if they will further reinforce\nthe existing pathological biases present in human-to-human interactions. If the\nlatter is true, then how to rigorously measure these biases? We aim to address\nthis challenge by proposing a principled framework for designing psychometric\nmeasures to evaluate chatbot biases."}
{"id": "2509.13326", "pdf": "https://arxiv.org/pdf/2509.13326.pdf", "abs": "https://arxiv.org/abs/2509.13326", "title": "LLM Chatbot-Creation Approaches", "authors": ["Hemil Mehta", "Tanvi Raut", "Kohav Yadav", "Edward F. Gehringer"], "categories": ["cs.HC", "cs.LG"], "comment": "Forthcoming in Frontiers in Education (FIE 2025), Nashville,\n  Tennessee, USA, Nov 2-5, 2025", "summary": "This full research-to-practice paper explores approaches for developing\ncourse chatbots by comparing low-code platforms and custom-coded solutions in\neducational contexts. With the rise of Large Language Models (LLMs) like GPT-4\nand LLaMA, LLM-based chatbots are being integrated into teaching workflows to\nautomate tasks, provide assistance, and offer scalable support. However,\nselecting the optimal development strategy requires balancing ease of use,\ncustomization, data privacy, and scalability. This study compares two\ndevelopment approaches: low-code platforms like AnythingLLM and Botpress, with\ncustom-coded solutions using LangChain, FAISS, and FastAPI. The research uses\nPrompt engineering, Retrieval-augmented generation (RAG), and personalization\nto evaluate chatbot prototypes across technical performance, scalability, and\nuser experience. Findings indicate that while low-code platforms enable rapid\nprototyping, they face limitations in customization and scaling, while\ncustom-coded systems offer more control but require significant technical\nexpertise. Both approaches successfully implement key research principles such\nas adaptive feedback loops and conversational continuity. The study provides a\nframework for selecting the appropriate development strategy based on\ninstitutional goals and resources. Future work will focus on hybrid solutions\nthat combine low-code accessibility with modular customization and incorporate\nmultimodal input for intelligent tutoring systems."}
{"id": "2509.13444", "pdf": "https://arxiv.org/pdf/2509.13444.pdf", "abs": "https://arxiv.org/abs/2509.13444", "title": "DuetUI: A Bidirectional Context Loop for Human-Agent Co-Generation of Task-Oriented Interfaces", "authors": ["Yuan Xu", "Shaowen Xiang", "Yizhi Song", "Ruoting Sun", "Xin Tong"], "categories": ["cs.HC"], "comment": null, "summary": "Large Language Models are reshaping task automation, yet remain limited in\ncomplex, multi-step real-world tasks that require aligning with vague user\nintent and enabling dynamic user override. From a formative study with 12\nparticipants, we found that end-users actively seek to shape generative\ninterfaces rather than relying on one-shot outputs. To address this, we\nintroduce the human-agent co-generation paradigm, materialized in DuetUI. This\nLLM-empowered system unfolds alongside task progress through a bidirectional\ncontext loop--the agent scaffolds the interface by decomposing the task, while\nthe user's direct manipulations implicitly steer the agent's next generation\nstep. In a user study with 24 participants, DuetUI significantly improved task\nefficiency and interface usability compared to a baseline, fostering seamless\nhuman-agent collaboration. Our contributions include the proposal and\nvalidation of this novel paradigm, the design of the DuetUI prototype embodying\nit, and empirical insights into how this bidirectional loop better aligns\nagents with human intent."}
{"id": "2509.13480", "pdf": "https://arxiv.org/pdf/2509.13480.pdf", "abs": "https://arxiv.org/abs/2509.13480", "title": "Gender-Neutral Rewriting in Italian: Models, Approaches, and Trade-offs", "authors": ["Andrea Piergentili", "Beatrice Savoldi", "Matteo Negri", "Luisa Bentivogli"], "categories": ["cs.CL"], "comment": "Accepted at CLiC-it 2025", "summary": "Gender-neutral rewriting (GNR) aims to reformulate text to eliminate\nunnecessary gender specifications while preserving meaning, a particularly\nchallenging task in grammatical-gender languages like Italian. In this work, we\nconduct the first systematic evaluation of state-of-the-art large language\nmodels (LLMs) for Italian GNR, introducing a two-dimensional framework that\nmeasures both neutrality and semantic fidelity to the input. We compare\nfew-shot prompting across multiple LLMs, fine-tune selected models, and apply\ntargeted cleaning to boost task relevance. Our findings show that open-weight\nLLMs outperform the only existing model dedicated to GNR in Italian, whereas\nour fine-tuned models match or exceed the best open-weight LLM's performance at\na fraction of its size. Finally, we discuss the trade-off between optimizing\nthe training data for neutrality and meaning preservation."}
{"id": "2509.13466", "pdf": "https://arxiv.org/pdf/2509.13466.pdf", "abs": "https://arxiv.org/abs/2509.13466", "title": "Do We Need Subsidiarity in Software?", "authors": ["Louisa Conwill", "Megan Levis Scheirer", "Walter Scheirer"], "categories": ["cs.HC"], "comment": null, "summary": "Subsidiarity is a principle of social organization that promotes human\ndignity and resists over-centralization by balancing personal autonomy with\nintervention from higher authorities only when necessary. Thus it is a\nrelevant, but not previously explored, critical lens for discerning the\ntradeoffs between complete user control of software and surrendering control to\n\"big tech\" for convenience, as is common in surveillance capitalism. Our study\nexplores data privacy through the lens of subsidiarity: we employ a\nmulti-method approach of data flow monitoring and user interviews to determine\nthe level of control different everyday technologies currently operate at, and\nthe level of control everyday computer users think is necessary. We found that\nchat platforms like Slack and Discord violate subsidiarity the most. Our work\nprovides insight into when users are willing to surrender privacy for\nconvenience and demonstrates how subsidiarity can inform designs that promote\nhuman flourishing."}
{"id": "2509.13539", "pdf": "https://arxiv.org/pdf/2509.13539.pdf", "abs": "https://arxiv.org/abs/2509.13539", "title": "Op-Fed: Opinion, Stance, and Monetary Policy Annotations on FOMC Transcripts Using Active Learning", "authors": ["Alisa Kanganis", "Katherine A. Keith"], "categories": ["cs.CL"], "comment": null, "summary": "The U.S. Federal Open Market Committee (FOMC) regularly discusses and sets\nmonetary policy, affecting the borrowing and spending decisions of millions of\npeople. In this work, we release Op-Fed, a dataset of 1044 human-annotated\nsentences and their contexts from FOMC transcripts. We faced two major\ntechnical challenges in dataset creation: imbalanced classes -- we estimate\nfewer than 8% of sentences express a non-neutral stance towards monetary policy\n-- and inter-sentence dependence -- 65% of instances require context beyond the\nsentence-level. To address these challenges, we developed a five-stage\nhierarchical schema to isolate aspects of opinion, monetary policy, and stance\ntowards monetary policy as well as the level of context needed. Second, we\nselected instances to annotate using active learning, roughly doubling the\nnumber of positive instances across all schema aspects. Using Op-Fed, we found\na top-performing, closed-weight LLM achieves 0.80 zero-shot accuracy in opinion\nclassification but only 0.61 zero-shot accuracy classifying stance towards\nmonetary policy -- below our human baseline of 0.89. We expect Op-Fed to be\nuseful for future model training, confidence calibration, and as a seed dataset\nfor future annotation efforts."}
{"id": "2509.13468", "pdf": "https://arxiv.org/pdf/2509.13468.pdf", "abs": "https://arxiv.org/abs/2509.13468", "title": "AR-TMT: Investigating the Impact of Distraction Types on Attention and Behavior in AR-based Trail Making Test", "authors": ["Sihun Baek", "Zhehan Qu", "Maria Gorlatova"], "categories": ["cs.HC"], "comment": "Accepted at VRST 2025. 9 pages, 8 figures", "summary": "Despite the growing use of AR in safety-critical domains, the field lacks a\nsystematic understanding of how different types of distraction affect user\nbehavior in AR environments. To address this gap, we present AR-TMT, an AR\nadaptation of the Trail Making Test that spatially renders targets for\nsequential selection on the Magic Leap 2. We implemented distractions in three\ncategories: top-down, bottom-up, and spatial distraction based on Wolfe's\nGuided Search model, and captured performance, gaze, motor behavior, and\nsubjective load measures to analyze user attention and behavior. A user study\nwith 34 participants revealed that top-down distraction degraded performance\nthrough semantic interference, while bottom-up distraction disrupted initial\nattentional engagement. Spatial distraction destabilized gaze behavior, leading\nto more scattered and less structured visual scanning patterns. We also found\nthat performance was correlated with attention control ($R^2 = .20$--$.35$)\nunder object-based distraction conditions, where distractors possessed\ntask-relevant features. The study offers insights into distraction mechanisms\nand their impact on users, providing opportunities for generalization to\necologically relevant AR tasks while underscoring the need to address the\nunique demands of AR environments."}
{"id": "2509.13569", "pdf": "https://arxiv.org/pdf/2509.13569.pdf", "abs": "https://arxiv.org/abs/2509.13569", "title": "Overview of Dialog System Evaluation Track: Dimensionality, Language, Culture and Safety at DSTC 12", "authors": ["John Mendonça", "Lining Zhang", "Rahul Mallidi", "Alon Lavie", "Isabel Trancoso", "Luis Fernando D'Haro", "João Sedoc"], "categories": ["cs.CL"], "comment": "DSTC12 Track 1 Overview Paper. https://chateval.org/dstc12", "summary": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for robust dialogue system evaluation, yet comprehensive assessment\nremains challenging. Traditional metrics often prove insufficient, and safety\nconsiderations are frequently narrowly defined or culturally biased. The DSTC12\nTrack 1, \"Dialog System Evaluation: Dimensionality, Language, Culture and\nSafety,\" is part of the ongoing effort to address these critical gaps. The\ntrack comprised two subtasks: (1) Dialogue-level, Multi-dimensional Automatic\nEvaluation Metrics, and (2) Multilingual and Multicultural Safety Detection.\nFor Task 1, focused on 10 dialogue dimensions, a Llama-3-8B baseline achieved\nthe highest average Spearman's correlation (0.1681), indicating substantial\nroom for improvement. In Task 2, while participating teams significantly\noutperformed a Llama-Guard-3-1B baseline on the multilingual safety subset (top\nROC-AUC 0.9648), the baseline proved superior on the cultural subset (0.5126\nROC-AUC), highlighting critical needs in culturally-aware safety. This paper\ndescribes the datasets and baselines provided to participants, as well as\nsubmission evaluation results for each of the two proposed subtasks."}
{"id": "2509.13532", "pdf": "https://arxiv.org/pdf/2509.13532.pdf", "abs": "https://arxiv.org/abs/2509.13532", "title": "Py maidr: Bridging Visual and Non-Visual Data Experiences Through a Unified Python Framework", "authors": ["JooYoung Seo", "Saairam Venkatesh", "Daksh Pokar", "Sanchita Kamath", "Krishna Anandan Ganesan"], "categories": ["cs.HC"], "comment": null, "summary": "Although recent efforts have developed accessible data visualization tools\nfor blind and low-vision (BLV) users, most follow a \"design for them\" approach\nthat creates an unintentional divide between sighted creators and BLV\nconsumers. This unidirectional paradigm perpetuates a power dynamic where\nsighted creators produce non-visual content boundaries for BLV consumers to\naccess. This paper proposes a bidirectional approach, \"design for us,\" where\nboth sighted and BLV collaborators can employ the same tool to create,\ninterpret, and communicate data visualizations for each other. We introduce Py\nmaidr, a Python package that seamlessly encodes multimodal (e.g., tactile,\nauditory, conversational) data representations into visual plots generated by\nMatplotlib and Seaborn. By simply importing the maidr package and invoking the\nmaidr.show() method, users can generate accessible plots with minimal changes\nto their existing codebase regardless of their visual dis/abilities. Our\ntechnical case studies demonstrate how this tool is scalable and can be\nintegrated into interactive computing (e.g., Jupyter Notebook, Google Colab),\nreproducible and literate programming (e.g., Quarto), and reactive dashboards\n(e.g., Shiny, Streamlit). Our performance benchmarks demonstrate that Py maidr\nintroduces minimal and consistent overhead during the rendering and export of\nplots against Matplotlib and Seaborn baselines. This work significantly\ncontributes to narrowing the accessibility gap in data visualization by\nproviding a unified framework that fosters collaboration and communication\nbetween sighted and BLV individuals."}
{"id": "2509.13624", "pdf": "https://arxiv.org/pdf/2509.13624.pdf", "abs": "https://arxiv.org/abs/2509.13624", "title": "Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning", "authors": ["Shambhavi Krishna", "Atharva Naik", "Chaitali Agarwal", "Sudharshan Govindan", "Taesung Lee", "Haw-Shiuan Chang"], "categories": ["cs.CL", "cs.LG"], "comment": "Camera-ready version. Accepted to appear in the proceedings of the\n  14th Joint Conference on Lexical and Computational Semantics (*SEM 2025)", "summary": "Large language models are increasingly deployed across diverse applications.\nThis often includes tasks LLMs have not encountered during training. This\nimplies that enumerating and obtaining the high-quality training data for all\ntasks is infeasible. Thus, we often need to rely on transfer learning using\ndatasets with different characteristics, and anticipate out-of-distribution\nrequests. Motivated by this practical need, we propose an analysis framework,\nbuilding a transfer learning matrix and dimensionality reduction, to dissect\nthese cross-task interactions. We train and analyze 10 models to identify\nlatent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic)\nand discover the side effects of the transfer learning. Our findings reveal\nthat performance improvements often defy explanations based on surface-level\ndataset similarity or source data quality. Instead, hidden statistical factors\nof the source dataset, such as class distribution and generation length\nproclivities, alongside specific linguistic features, are actually more\ninfluential. This work offers insights into the complex dynamics of transfer\nlearning, paving the way for more predictable and effective LLM adaptation."}
{"id": "2509.13646", "pdf": "https://arxiv.org/pdf/2509.13646.pdf", "abs": "https://arxiv.org/abs/2509.13646", "title": "Vistoria: A Multimodal System to Support Fictional Story Writing through Instrumental Text-Image Co-Editing", "authors": ["Kexue Fu", "Jingfei Huang", "Long Ling", "Sumin Hong", "Yihang Zuo", "Ray LC", "Toby Jia-jun Li"], "categories": ["cs.HC"], "comment": "Paper is under the review", "summary": "Humans think visually-we remember in images, dream in pictures, and use\nvisual metaphors to communicate. Yet, most creative writing tools remain\ntext-centric, limiting how authors plan and translate ideas. We present\nVistoria, a system for synchronized text-image co-editing in fictional story\nwriting that treats visuals and text as coequal narrative materials. A\nformative Wizard-of-Oz co-design study with 10 story writers revealed how\nsketches, images, and annotations serve as essential instruments for ideation\nand organization. Drawing on theories of Instrumental Interaction and\nStructural Mapping, Vistoria introduces multimodal operations-lasso, collage,\nfilters, and perspective shifts that enable seamless narrative exploration\nacross modalities. A controlled study with 12 participants shows that\nco-editing enhances expressiveness, immersion, and collaboration, enabling\nwriters to explore divergent directions, embrace serendipitous randomness, and\ntrace evolving storylines. While multimodality increased cognitive demand,\nparticipants reported stronger senses of authorship and agency. These findings\ndemonstrate how multimodal co-editing expands creative potential by balancing\nabstraction and concreteness in narrative development."}
{"id": "2509.13664", "pdf": "https://arxiv.org/pdf/2509.13664.pdf", "abs": "https://arxiv.org/abs/2509.13664", "title": "Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs", "authors": ["Zhuoxuan Zhang", "Jinhao Duan", "Edward Kim", "Kaidi Xu"], "categories": ["cs.CL", "cs.AI"], "comment": "To be appeared in EMNLP 2025 (main)", "summary": "Ambiguity is pervasive in real-world questions, yet large language models\n(LLMs) often respond with confident answers rather than seeking clarification.\nIn this work, we show that question ambiguity is linearly encoded in the\ninternal representations of LLMs and can be both detected and controlled at the\nneuron level. During the model's pre-filling stage, we identify that a small\nnumber of neurons, as few as one, encode question ambiguity information. Probes\ntrained on these Ambiguity-Encoding Neurons (AENs) achieve strong performance\non ambiguity detection and generalize across datasets, outperforming\nprompting-based and representation-based baselines. Layerwise analysis reveals\nthat AENs emerge from shallow layers, suggesting early encoding of ambiguity\nsignals in the model's processing pipeline. Finally, we show that through\nmanipulating AENs, we can control LLM's behavior from direct answering to\nabstention. Our findings reveal that LLMs form compact internal representations\nof question ambiguity, enabling interpretable and controllable behavior."}
{"id": "2509.13671", "pdf": "https://arxiv.org/pdf/2509.13671.pdf", "abs": "https://arxiv.org/abs/2509.13671", "title": "I, Robot? Socio-Technical Implications of Ultra-Personalized AI-Powered AAC; an Autoethnographic Account", "authors": ["Tobias Weinberg", "Ricardo E. Gonzalez Penuela", "Stephanie Valencia", "Thijs Roumen"], "categories": ["cs.HC", "cs.CY"], "comment": "16 pages, 9 figures", "summary": "Generic AI auto-complete for message composition often fails to capture the\nnuance of personal identity, requiring significant editing. While harmless in\nlow-stakes settings, for users of Augmentative and Alternative Communication\n(AAC) devices, who rely on such systems for everyday communication, this\nediting burden is particularly acute. Intuitively, the need for edits would be\nlower if language models were personalized to the communication of the specific\nuser. While technically feasible, such personalization raises socio-technical\nquestions: what are the implications of logging one's own conversations, and\nhow does personalization affect privacy, authorship, and control? We explore\nthese questions through an autoethnographic study in three phases: (1) seven\nmonths of collecting all the lead author's AAC communication data, (2)\nfine-tuning a model on this dataset, and (3) three months of daily use of\npersonalized AI suggestions. We reflect on these phases through continuous\ndiary entries and interaction logs. Our findings highlight the value of\npersonalization as well as implications on privacy, authorship, and blurring\nthe boundaries of self-expression."}
{"id": "2509.13672", "pdf": "https://arxiv.org/pdf/2509.13672.pdf", "abs": "https://arxiv.org/abs/2509.13672", "title": "CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction", "authors": ["Shang Qin", "Jingheng Ye", "Yinghui Li", "Hai-Tao Zheng", "Qi Li", "Jinxiao Shan", "Zhixing Li", "Hong-Gee Kim"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The growing demand for automated writing assistance in diverse academic\ndomains highlights the need for robust Chinese Grammatical Error Correction\n(CGEC) systems that can adapt across disciplines. However, existing CGEC\nresearch largely lacks dedicated benchmarks for multi-disciplinary academic\nwriting, overlooking continual learning (CL) as a promising solution to handle\ndomain-specific linguistic variation and prevent catastrophic forgetting. To\nfill this crucial gap, we introduce CL$^2$GEC, the first Continual Learning\nbenchmark for Chinese Literature Grammatical Error Correction, designed to\nevaluate adaptive CGEC across multiple academic fields. Our benchmark includes\n10,000 human-annotated sentences spanning 10 disciplines, each exhibiting\ndistinct linguistic styles and error patterns. CL$^2$GEC focuses on evaluating\ngrammatical error correction in a continual learning setting, simulating\nsequential exposure to diverse academic disciplines to reflect real-world\neditorial dynamics. We evaluate large language models under sequential tuning,\nparameter-efficient adaptation, and four representative CL algorithms, using\nboth standard GEC metrics and continual learning metrics adapted to task-level\nvariation. Experimental results reveal that regularization-based methods\nmitigate forgetting more effectively than replay-based or naive sequential\napproaches. Our benchmark provides a rigorous foundation for future research in\nadaptive grammatical error correction across diverse academic domains."}
{"id": "2509.13679", "pdf": "https://arxiv.org/pdf/2509.13679.pdf", "abs": "https://arxiv.org/abs/2509.13679", "title": "From Prompts to Reflection: Designing Reflective Play for GenAI Literacy", "authors": ["Qianou Ma", "Megan Chai", "Yike Tan", "Jihun Choi", "Jini Kim", "Erik Harpstead", "Geoff Kauffman", "Tongshuang Wu"], "categories": ["cs.HC"], "comment": null, "summary": "The wide adoption of Generative AI (GenAI) in everyday life highlights the\nneed for greater literacy around its evolving capabilities, biases, and\nlimitations. While many AI literacy efforts focus on children through\ngame-based learning, few interventions support adults in developing a nuanced,\nreflective understanding of GenAI via playful exploration. To address the gap,\nwe introduce ImaginAItion, a multiplayer party game inspired by Drawful and\ngrounded in the reflective play framework to surface model defaults, biases,\nand human-AI perception gaps through prompting and discussion. From ten\nsessions (n=30), we show how gameplay helped adults recognize systematic biases\nin GenAI, reflect on humans and AI interpretation differences, and adapt their\nprompting strategies. We also found that group dynamics and composition, such\nas expertise and diversity, amplified or muted reflection. Our work provides a\nstarting point to scale critical GenAI literacy through playful, social\ninterventions resilient to rapidly evolving technologies."}
{"id": "2509.13677", "pdf": "https://arxiv.org/pdf/2509.13677.pdf", "abs": "https://arxiv.org/abs/2509.13677", "title": "AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation", "authors": ["Xinxu Zhou", "Jiaqi Bai", "Zhenqi Sun", "Fanxiang Zeng", "Yue Liu"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Although significant progress has been made in many tasks within the field of\nNatural Language Processing (NLP), Controlled Text Generation (CTG) continues\nto face numerous challenges, particularly in achieving fine-grained conditional\ncontrol over generation. Additionally, in real scenario and online\napplications, cost considerations, scalability, domain knowledge learning and\nmore precise control are required, presenting more challenge for CTG. This\npaper introduces a novel and scalable framework, AgentCTG, which aims to\nenhance precise and complex control over the text generation by simulating the\ncontrol and regulation mechanisms in multi-agent workflows. We explore various\ncollaboration methods among different agents and introduce an auto-prompt\nmodule to further enhance the generation effectiveness. AgentCTG achieves\nstate-of-the-art results on multiple public datasets. To validate its\neffectiveness in practical applications, we propose a new challenging\nCharacter-Driven Rewriting task, which aims to convert the original text into\nnew text that conform to specific character profiles and simultaneously\npreserve the domain knowledge. When applied to online navigation with\nrole-playing, our approach significantly enhances the driving experience\nthrough improved content delivery. By optimizing the generation of contextually\nrelevant text, we enable a more immersive interaction within online\ncommunities, fostering greater personalization and user engagement."}
{"id": "2509.13742", "pdf": "https://arxiv.org/pdf/2509.13742.pdf", "abs": "https://arxiv.org/abs/2509.13742", "title": "Spatial Balancing: Harnessing Spatial Reasoning to Balance Scientific Exposition and Narrative Engagement in LLM-assisted Science Communication Writing", "authors": ["Kexue Fu", "Jiaye Leng", "Yawen Zhang", "Jingfei Huang", "Yihang Zuo", "Runze Cai", "Zijian Ding", "Ray LC", "Shengdong Zhao", "Qinyuan Lei"], "categories": ["cs.HC"], "comment": null, "summary": "Balancing scientific exposition and narrative engagement is a central\nchallenge in science communication. To examine how to achieve balance, we\nconducted a formative study with four science communicators and a literature\nreview of science communication practices, focusing on their workflows and\nstrategies. These insights revealed how creators iteratively shift between\nexposition and engagement but often lack structured support. Building on this,\nwe developed SpatialBalancing, a co-writing system that connects human spatial\nreasoning with the linguistic intelligence of large language models. The system\nvisualizes revision trade-offs in a dual-axis space, where users select\nstrategy-based labels to generate, compare, and refine versions during the\nrevision process. This spatial externalization transforms revision into spatial\nnavigation, enabling intentional iterations that balance scientific rigor with\nnarrative appeal. In a within-subjects study (N=16), SpatialBalancing enhanced\nmetacognitive reflection, flexibility, and creative exploration, demonstrating\nhow coupling spatial reasoning with linguistic generation fosters monitoring in\niterative science communication writing."}
{"id": "2509.13683", "pdf": "https://arxiv.org/pdf/2509.13683.pdf", "abs": "https://arxiv.org/abs/2509.13683", "title": "Improving Context Fidelity via Native Retrieval-Augmented Reasoning", "authors": ["Suyuchen Wang", "Jinlin Wang", "Xinyu Wang", "Shiqi Li", "Xiangru Tang", "Sirui Hong", "Xiao-Wen Chang", "Chenglin Wu", "Bang Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted as a main conference paper at EMNLP 2025", "summary": "Large language models (LLMs) often struggle with context fidelity, producing\ninconsistent answers when responding to questions based on provided\ninformation. Existing approaches either rely on expensive supervised\nfine-tuning to generate evidence post-answer or train models to perform web\nsearches without necessarily improving utilization of the given context. We\npropose CARE, a novel native retrieval-augmented reasoning framework that\nteaches LLMs to explicitly integrate in-context evidence within their reasoning\nprocess with the model's own retrieval capabilities. Our method requires\nlimited labeled evidence data while significantly enhancing both retrieval\naccuracy and answer generation performance through strategically retrieved\nin-context tokens in the reasoning chain. Extensive experiments on multiple\nreal-world and counterfactual QA benchmarks demonstrate that our approach\nsubstantially outperforms supervised fine-tuning, traditional\nretrieval-augmented generation methods, and external retrieval solutions. This\nwork represents a fundamental advancement in making LLMs more accurate,\nreliable, and efficient for knowledge-intensive tasks."}
{"id": "2509.13892", "pdf": "https://arxiv.org/pdf/2509.13892.pdf", "abs": "https://arxiv.org/abs/2509.13892", "title": "Synthetic Data Generation for Screen Time and App Usage", "authors": ["Gustavo Kruger", "Nikhil Sachdeva", "Michael Sobolev"], "categories": ["cs.HC", "cs.AI", "I.2; J.4"], "comment": "14 pages", "summary": "Smartphone usage data can provide valuable insights for understanding\ninteraction with technology and human behavior. However, collecting\nlarge-scale, in-the-wild smartphone usage logs is challenging due to high\ncosts, privacy concerns, under representative user samples and biases like\nnon-response that can skew results. These challenges call for exploring\nalternative approaches to obtain smartphone usage datasets. In this context,\nlarge language models (LLMs) such as Open AI's ChatGPT present a novel approach\nfor synthetic smartphone usage data generation, addressing limitations of\nreal-world data collection. We describe a case study on how four prompt\nstrategies influenced the quality of generated smartphone usage data. We\ncontribute with insights on prompt design and measures of data quality,\nreporting a prompting strategy comparison combining two factors, prompt level\nof detail (describing a user persona, describing the expected results\ncharacteristics) and seed data inclusion (with versus without an initial real\nusage example). Our findings suggest that using LLMs to generate structured and\nbehaviorally plausible smartphone use datasets is feasible for some use cases,\nespecially when using detailed prompts. Challenges remain in capturing diverse\nnuances of human behavioral patterns in a single synthetic dataset, and\nevaluating tradeoffs between data fidelity and diversity, suggesting the need\nfor use-case-specific evaluation metrics and future research with more diverse\nseed data and different LLM models."}
{"id": "2509.13695", "pdf": "https://arxiv.org/pdf/2509.13695.pdf", "abs": "https://arxiv.org/abs/2509.13695", "title": "Can Large Language Models Robustly Perform Natural Language Inference for Japanese Comparatives?", "authors": ["Yosuke Mikami", "Daiki Matsuoka", "Hitomi Yanaka"], "categories": ["cs.CL"], "comment": "To appear in Proceedings of the 16th International Conference on\n  Computational Semantics (IWCS 2025)", "summary": "Large Language Models (LLMs) perform remarkably well in Natural Language\nInference (NLI). However, NLI involving numerical and logical expressions\nremains challenging. Comparatives are a key linguistic phenomenon related to\nsuch inference, but the robustness of LLMs in handling them, especially in\nlanguages that are not dominant in the models' training data, such as Japanese,\nhas not been sufficiently explored. To address this gap, we construct a\nJapanese NLI dataset that focuses on comparatives and evaluate various LLMs in\nzero-shot and few-shot settings. Our results show that the performance of the\nmodels is sensitive to the prompt formats in the zero-shot setting and\ninfluenced by the gold labels in the few-shot examples. The LLMs also struggle\nto handle linguistic phenomena unique to Japanese. Furthermore, we observe that\nprompts containing logical semantic representations help the models predict the\ncorrect labels for inference problems that they struggle to solve even with\nfew-shot examples."}
{"id": "2509.13899", "pdf": "https://arxiv.org/pdf/2509.13899.pdf", "abs": "https://arxiv.org/abs/2509.13899", "title": "AI as a teaching tool and learning partner", "authors": ["Steven Watterson", "Sarah Atkinson", "Elaine Murray", "Andrew McDowell"], "categories": ["cs.HC"], "comment": "6 Pages, 1 Figure", "summary": "The arrival of AI tools and in particular Large Language Models (LLMs) has\nhad a transformative impact on teaching and learning and institutes are still\ntrying to determine how to integrate LLMs into education in constructive ways.\nHere, we explore the adoption of LLM-based tools into two teaching programmes,\none undergraduate and one postgraduate. We provided to our classes (1) a\nLLM-powered chatbot that had access to course materials by RAG and (2)\nAI-generated audio-only podcasts for each week$\\text{'}$s teaching material. At\nthe end of the semester, we surveyed the classes to gauge attitudes towards\nthese tools. The classes were small and from biological courses. The students\nfelt positive about AI generally and that AI tools made a positive impact on\nteaching. Students found the LLM-powered chatbot easy and enjoyable to use and\nfelt that it enhanced their learning. The podcasts were less popular and only a\nsmall proportion of the class listened weekly. The class as a whole was\nindifferent to whether the podcasts should be used more widely across courses,\nbut those who listened enjoyed them and were in favour."}
{"id": "2509.13696", "pdf": "https://arxiv.org/pdf/2509.13696.pdf", "abs": "https://arxiv.org/abs/2509.13696", "title": "Integrating Text and Time-Series into (Large) Language Models to Predict Medical Outcomes", "authors": ["Iyadh Ben Cheikh Larbi", "Ajay Madhavan Ravichandran", "Aljoscha Burchardt", "Roland Roller"], "categories": ["cs.CL"], "comment": "Presented and published at BioCreative IX", "summary": "Large language models (LLMs) excel at text generation, but their ability to\nhandle clinical classification tasks involving structured data, such as time\nseries, remains underexplored. In this work, we adapt instruction-tuned LLMs\nusing DSPy-based prompt optimization to process clinical notes and structured\nEHR inputs jointly. Our results show that this approach achieves performance on\npar with specialized multimodal systems while requiring less complexity and\noffering greater adaptability across tasks."}
{"id": "2509.14050", "pdf": "https://arxiv.org/pdf/2509.14050.pdf", "abs": "https://arxiv.org/abs/2509.14050", "title": "AI For Privacy in Smart Homes: Exploring How Leveraging AI-Powered Smart Devices Enhances Privacy Protection", "authors": ["Wael Albayaydh", "Ivan Flechais", "Rui Zhao", "Jood Albayaydh"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Privacy concerns and fears of unauthorized access in smart home devices often\nstem from misunderstandings about how data is collected, used, and protected.\nThis study explores how AI-powered tools can offer innovative privacy\nprotections through clear, personalized, and contextual support to users.\nThrough 23 in-depth interviews with users, AI developers, designers, and\nregulators, and using Grounded Theory analysis, we identified two key themes:\nAspirations for AI-Enhanced Privacy - how users perceive AI's potential to\nempower them, address power imbalances, and improve ease of use- and AI\nEthical, Security, and Regulatory Considerations-challenges in strengthening\ndata security, ensuring regulatory compliance, and promoting ethical AI\npractices. Our findings contribute to the field by uncovering user aspirations\nfor AI-driven privacy solutions, identifying key security and ethical\nchallenges, and providing actionable recommendations for all stakeholders,\nparticularly targeting smart device designers and AI developers, to guide the\nco-design of AI tools that enhance privacy protection in smart home devices. By\nbridging the gap between user expectations, AI capabilities, and regulatory\nframeworks, this work offers practical insights for shaping the future of\nprivacy-conscious AI integration in smart homes."}
{"id": "2509.13702", "pdf": "https://arxiv.org/pdf/2509.13702.pdf", "abs": "https://arxiv.org/abs/2509.13702", "title": "DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models", "authors": ["Xiao Zheng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Model (LLM) hallucination is a significant barrier to their\nreliable deployment. Current methods like Retrieval-Augmented Generation (RAG)\nare often reactive. We introduce **Dynamic Self-reinforcing Calibration for\nHallucination Suppression (DSCC-HS)**, a novel, proactive framework that\nintervenes during autoregressive decoding. Inspired by dual-process cognitive\ntheory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a\nFactual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During\ninference, these proxies dynamically steer a large target model by injecting a\nreal-time steering vector, which is the difference between FAP and HDP logits,\nat each decoding step. This plug-and-play approach requires no modification to\nthe target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS\nachieves state-of-the-art performance. On TruthfulQA, it reached a 99.2%\nFactual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained\nthe highest FActScore of 46.50. These results validate DSCC-HS as a principled\nand efficient solution for enhancing LLM factuality."}
{"id": "2509.14056", "pdf": "https://arxiv.org/pdf/2509.14056.pdf", "abs": "https://arxiv.org/abs/2509.14056", "title": "EEG-Based Cognitive Load Classification During Landmark-Based VR Navigation", "authors": ["Jiahui An", "Bingjie Cheng", "Dmitriy Rudyka", "Elisa Donati", "Sara Fabrikant"], "categories": ["cs.HC", "F.2.2, I.2.7"], "comment": "11 pages, 7 figures", "summary": "Brain computer interfaces enable real-time monitoring of cognitive load, but\ntheir effectiveness in dynamic navigation contexts is not well established.\nUsing an existing VR navigation dataset, we examined whether EEG signals can\nclassify cognitive load during map-based wayfinding and whether classification\naccuracy depends more on task complexity or on individual traits. EEG\nrecordings from forty-six participants navigating routes with 3, 5, or 7 map\nlandmarks were analyzed with a nested cross-validation framework across\nmultiple machine learning models. Classification achieved mean accuracies up to\n90.8% for binary contrasts (3 vs. 7 landmarks) and 78.7% for the three-class\nproblem, both well above chance. Demographic and cognitive variables (age,\ngender, spatial ability, working memory) showed no significant influence. These\nfindings demonstrate that task demands outweigh individual differences in\nshaping classification performance, highlighting the potential for\ntask-adaptive navigation systems that dynamically adjust map complexity in\nresponse to real-time cognitive states."}
{"id": "2509.13706", "pdf": "https://arxiv.org/pdf/2509.13706.pdf", "abs": "https://arxiv.org/abs/2509.13706", "title": "Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models", "authors": ["Peter Beidler", "Mark Nguyen", "Kevin Lybarger", "Ola Holmberg", "Eric Ford", "John Kang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "PURPOSE: Incident reports are an important tool for safety and quality\nimprovement in healthcare, but manual review is time-consuming and requires\nsubject matter expertise. Here we present a natural language processing (NLP)\nscreening tool to detect high-severity incident reports in radiation oncology\nacross two institutions.\n  METHODS AND MATERIALS: We used two text datasets to train and evaluate our\nNLP models: 7,094 reports from our institution (Inst.), and 571 from IAEA\nSAFRON (SF), all of which had severity scores labeled by clinical content\nexperts. We trained and evaluated two types of models: baseline support vector\nmachines (SVM) and BlueBERT which is a large language model pretrained on\nPubMed abstracts and hospitalized patient data. We assessed for\ngeneralizability of our model in two ways. First, we evaluated models trained\nusing Inst.-train on SF-test. Second, we trained a BlueBERT_TRANSFER model that\nwas first fine-tuned on Inst.-train then on SF-train before testing on SF-test\nset. To further analyze model performance, we also examined a subset of 59\nreports from our Inst. dataset, which were manually edited for clarity.\n  RESULTS Classification performance on the Inst. test achieved AUROC 0.82\nusing SVM and 0.81 using BlueBERT. Without cross-institution transfer learning,\nperformance on the SF test was limited to an AUROC of 0.42 using SVM and 0.56\nusing BlueBERT. BlueBERT_TRANSFER, which was fine-tuned on both datasets,\nimproved the performance on SF test to AUROC 0.78. Performance of SVM, and\nBlueBERT_TRANSFER models on the manually curated Inst. reports (AUROC 0.85 and\n0.74) was similar to human performance (AUROC 0.81).\n  CONCLUSION: In summary, we successfully developed cross-institution NLP\nmodels on incident report text from radiation oncology centers. These models\nwere able to detect high-severity reports similarly to humans on a curated\ndataset."}
{"id": "2509.14132", "pdf": "https://arxiv.org/pdf/2509.14132.pdf", "abs": "https://arxiv.org/abs/2509.14132", "title": "When Avatars Have Personality: Effects on Engagement and Communication in Immersive Medical Training", "authors": ["Julia S. Dollis", "Iago A. Brito", "Fernanda B. Färber", "Pedro S. F. B. Ribeiro", "Rafael T. Sousa", "Arlindo R. Galvão Filho"], "categories": ["cs.HC", "cs.CL"], "comment": "8 pages, 2 figures", "summary": "While virtual reality (VR) excels at simulating physical environments, its\neffectiveness for training complex interpersonal skills is limited by a lack of\npsychologically plausible virtual humans. This is a critical gap in high-stakes\ndomains like medical education, where communication is a core competency. This\npaper introduces a framework that integrates large language models (LLMs) into\nimmersive VR to create medically coherent virtual patients with distinct,\nconsistent personalities, built on a modular architecture that decouples\npersonality from clinical data. We evaluated our system in a mixed-method,\nwithin-subjects study with licensed physicians who engaged in simulated\nconsultations. Results demonstrate that the approach is not only feasible but\nis also perceived by physicians as a highly rewarding and effective training\nenhancement. Furthermore, our analysis uncovers critical design principles,\nincluding a ``realism-verbosity paradox\" where less communicative agents can\nseem more artificial, and the need for challenges to be perceived as authentic\nto be instructive. This work provides a validated framework and key insights\nfor developing the next generation of socially intelligent VR training\nenvironments."}
{"id": "2509.13723", "pdf": "https://arxiv.org/pdf/2509.13723.pdf", "abs": "https://arxiv.org/abs/2509.13723", "title": "DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning", "authors": ["Yaxin Gao", "Yao Lu", "Zongfei Zhang", "Jiaqi Nie", "Shanqing Yu", "Qi Xuan"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success in many natural\nlanguage processing (NLP) tasks. To achieve more accurate output, the prompts\nused to drive LLMs have become increasingly longer, which incurs higher\ncomputational costs. To address this prompt inflation problem, prompt\ncompression has been proposed. However, most existing methods require training\na small auxiliary model for compression, incurring a significant amount of\nadditional computation. To avoid this, we propose a two-stage, training-free\napproach, called Dual-Stage Progressive Compression (DSPC). In the\ncoarse-grained stage, semantic-related sentence filtering removes sentences\nwith low semantic value based on TF-IDF. In the fine-grained stage, token\nimportance is assessed using attention contribution, cross-model loss\ndifference, and positional importance, enabling the pruning of low-utility\ntokens while preserving semantics. We validate DSPC on LLaMA-3.1-8B-Instruct\nand GPT-3.5-Turbo under a constrained token budget and observe consistent\nimprovements. For instance, in the FewShot task of the Longbench dataset, DSPC\nachieves a performance of 49.17 by using only 3x fewer tokens, outperforming\nthe best state-of-the-art baseline LongLLMLingua by 7.76."}
{"id": "2509.13345", "pdf": "https://arxiv.org/pdf/2509.13345.pdf", "abs": "https://arxiv.org/abs/2509.13345", "title": "Accuracy Paradox in Large Language Models: Regulating Hallucination Risks in Generative AI", "authors": ["Zihao Li", "Weiwei Yi", "Jiahong Chen"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": null, "summary": "As Large Language Models (LLMs) permeate everyday decision-making, their\nepistemic and societal risks demand urgent scrutiny. Hallucinations, the\ngeneration of fabricated, misleading, oversimplified or untrustworthy outputs,\nhas emerged as imperative challenges. While regulatory, academic, and technical\ndiscourse position accuracy as the principal benchmark for mitigating such\nharms, this article contends that overreliance on accuracy misdiagnoses the\nproblem and has counterproductive effect: the accuracy paradox. Drawing on\ninterdisciplinary literatures, this article develops a taxonomy of\nhallucination types and shows the paradox along three intertwining dimensions:\noutputs, individuals and society. First, accuracy functions as a superficial\nproxy for reliability, incentivising the optimisation of rhetorical fluency and\nsurface-level correctness over epistemic trustworthiness. This encourages\npassive user trust in outputs that appear accurate but epistemically untenable.\nSecond, accuracy as a singular metric fails to detect harms that are not\nfactually false but are nonetheless misleading, value-laden, or socially\ndistorting, including consensus illusions, sycophantic alignment, and subtle\nmanipulation. Third, regulatory overemphasis on accuracy obscures the wider\nsocietal consequences of hallucination, including social sorting, privacy\nviolations, equity harms, epistemic convergence that marginalises dissent,\nreduces pluralism, and causes social deskilling. By examining the EU AI Act,\nGDPR, and DSA, the article argues that current regulations are not yet\nstructurally equipped to address these epistemic, relational, and systemic\nharms and exacerbated by the overreliance on accuracy. By exposing such\nconceptual and practical challenges, this article calls for a fundamental shift\ntowards pluralistic, context-aware, and manipulation-resilient approaches to AI\ntrustworthy governance."}
{"id": "2509.13734", "pdf": "https://arxiv.org/pdf/2509.13734.pdf", "abs": "https://arxiv.org/abs/2509.13734", "title": "Implementing a Logical Inference System for Japanese Comparatives", "authors": ["Yosuke Mikami", "Daiki Matsuoka", "Hitomi Yanaka"], "categories": ["cs.CL"], "comment": "In Proceedings of the 5th Workshop on Natural Logic Meets Machine\n  Learning (NALOMA)", "summary": "Natural Language Inference (NLI) involving comparatives is challenging\nbecause it requires understanding quantities and comparative relations\nexpressed by sentences. While some approaches leverage Large Language Models\n(LLMs), we focus on logic-based approaches grounded in compositional semantics,\nwhich are promising for robust handling of numerical and logical expressions.\nPrevious studies along these lines have proposed logical inference systems for\nEnglish comparatives. However, it has been pointed out that there are several\nmorphological and semantic differences between Japanese and English\ncomparatives. These differences make it difficult to apply such systems\ndirectly to Japanese comparatives. To address this gap, this study proposes\nccg-jcomp, a logical inference system for Japanese comparatives based on\ncompositional semantics. We evaluate the proposed system on a Japanese NLI\ndataset containing comparative expressions. We demonstrate the effectiveness of\nour system by comparing its accuracy with that of existing LLMs."}
{"id": "2509.13348", "pdf": "https://arxiv.org/pdf/2509.13348.pdf", "abs": "https://arxiv.org/abs/2509.13348", "title": "Towards an AI-Augmented Textbook", "authors": ["LearnLM Team", "Google", ":", "Amy Wang", "Anna Iurchenko", "Anisha Choudhury", "Alicia Martín", "Amir Globerson", "Avinatan Hassidim", "Ayça Çakmakli", "Ayelet Shasha Evron", "Charlie Yang", "Courtney Heldreth", "Diana Akrong", "Gal Elidan", "Hairong Mu", "Ian Li", "Ido Cohen", "Katherine Chou", "Komal Singh", "Lev Borovoi", "Lidan Hackmon", "Lior Belinsky", "Michael Fink", "Niv Efron", "Preeti Singh", "Rena Levitt", "Shashank Agarwal", "Shay Sharon", "Tracey Lee-Joe", "Xiaohong Hao", "Yael Gold-Zamir", "Yael Haramaty", "Yishay Mor", "Yoav Bar Sinai", "Yossi Matias"], "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "Textbooks are a cornerstone of education, but they have a fundamental\nlimitation: they are a one-size-fits-all medium. Any new material or\nalternative representation requires arduous human effort, so that textbooks\ncannot be adapted in a scalable manner. We present an approach for transforming\nand augmenting textbooks using generative AI, adding layers of multiple\nrepresentations and personalization while maintaining content integrity and\nquality. We refer to the system built with this approach as Learn Your Way. We\nreport pedagogical evaluations of the different transformations and\naugmentations, and present the results of a a randomized control trial,\nhighlighting the advantages of learning with Learn Your Way over regular\ntextbook usage."}
{"id": "2509.13775", "pdf": "https://arxiv.org/pdf/2509.13775.pdf", "abs": "https://arxiv.org/abs/2509.13775", "title": "Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications", "authors": ["Vani Kanjirangat", "Ljiljana Dolamic", "Fabio Rinaldi"], "categories": ["cs.CL", "cs.AI"], "comment": "4 main pages, 4 additional, 5 figures", "summary": "This paper discusses our exploration of different data-efficient and\nparameter-efficient approaches to Arabic Dialect Identification (ADI). In\nparticular, we investigate various soft-prompting strategies, including\nprefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA\nreparameterizations. For the data-efficient strategy, we analyze hard prompting\nwith zero-shot and few-shot inferences to analyze the dialect identification\ncapabilities of Large Language Models (LLMs). For the parameter-efficient PEFT\napproaches, we conducted our experiments using Arabic-specific encoder models\non several major datasets. We also analyzed the n-shot inferences on\nopen-source decoder-only models, a general multilingual model (Phi-3.5), and an\nArabic-specific one(SILMA). We observed that the LLMs generally struggle to\ndifferentiate the dialectal nuances in the few-shot or zero-shot setups. The\nsoft-prompted encoder variants perform better, while the LoRA-based fine-tuned\nmodels perform best, even surpassing full fine-tuning."}
{"id": "2509.13369", "pdf": "https://arxiv.org/pdf/2509.13369.pdf", "abs": "https://arxiv.org/abs/2509.13369", "title": "Right-to-Override for Critical Urban Control Systems: A Deliberative Audit Method for Buildings, Power, and Transport", "authors": ["Rashid Mushkani"], "categories": ["eess.SY", "cs.CY", "cs.HC", "cs.SY"], "comment": null, "summary": "Automation now steers building HVAC, distribution grids, and traffic signals,\nyet residents rarely have authority to pause or redirect these systems when\nthey harm inclusivity, safety, or accessibility. We formalize a\nRight-to-Override (R2O) - defining override authorities, evidentiary\nthresholds, and domain-validated safe fallback states - and introduce a\nDeliberative Audit Method (DAM) with playbooks for pre-deployment walkthroughs,\nshadow-mode trials, and post-incident review. We instantiate R2O/DAM in\nsimulations of smart-grid load shedding, building HVAC under occupancy\nuncertainty, and multi-agent traffic signals. R2O reduces distributional harm\nwith limited efficiency loss: load-shedding disparity in unserved energy drops\nfrom 5.61x to 0.69x with constant curtailment; an override eliminates two\ndiscomfort-hours for seniors at an energy cost of 77 kWh; and median pedestrian\nwait falls from 90.4 s to 55.9 s with a 6.0 s increase in mean vehicle delay.\nWe also contribute a policy standard, audit worksheets, and a ModelOps\nintegration pattern to make urban automation contestable and reviewable."}
{"id": "2509.13790", "pdf": "https://arxiv.org/pdf/2509.13790.pdf", "abs": "https://arxiv.org/abs/2509.13790", "title": "Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning", "authors": ["Yangning Li", "Tingwei Lu", "Yinghui Li", "Yankai Chen", "Wei-Chieh Huang", "Wenhao Jiang", "Hui Wang", "Hai-Tao Zheng", "Philip S. Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 Findings", "summary": "Efficient instruction tuning aims to enhance the ultimate performance of\nlarge language models (LLMs) trained on a given instruction dataset. Curriculum\nlearning as a typical data organization strategy has shown preliminary\neffectiveness in instruction tuning. However, current curriculum tuning methods\nsuffer from the curriculum rigidity, since they rely solely on static heuristic\ndifficulty metrics. These methods fail to adapt to the evolving capabilities of\nmodels during training, resulting in a fixed and potentially sub-optimal\nlearning trajectory. To address the issue, Competence-Aware Multi-Perspective\ncUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS\noffers several advantages: (1) Dynamic selection for sub-curriculum. (2)\nCompetency-aware adjustment to the curriculum schedule. (3) Multiple\ndifficulty-based scheduling. Extensive experiments prove the superior\nperformance of CAMPUS, compared to other state-of-the-art baselines for\nefficient instruction tuning."}
{"id": "2509.13509", "pdf": "https://arxiv.org/pdf/2509.13509.pdf", "abs": "https://arxiv.org/abs/2509.13509", "title": "Practitioners' Perspectives on a Differential Privacy Deployment Registry", "authors": ["Priyanka Nanayakkara", "Elena Ghazi", "Salil Vadhan"], "categories": ["cs.CR", "cs.CY", "cs.HC"], "comment": null, "summary": "Differential privacy (DP) -- a principled approach to producing statistical\ndata products with strong, mathematically provable privacy guarantees for the\nindividuals in the underlying dataset -- has seen substantial adoption in\npractice over the past decade. Applying DP requires making several\nimplementation decisions, each with significant impacts on data privacy and/or\nutility. Hence, to promote shared learning and accountability around DP\ndeployments, Dwork, Kohli, and Mulligan (2019) proposed a public-facing\nrepository (\"registry\") of DP deployments. The DP community has recently\nstarted to work toward realizing this vision. We contribute to this effort by\n(1) developing a holistic, hierarchical schema to describe any given DP\ndeployment and (2) designing and implementing an interactive interface to act\nas a registry where practitioners can access information about past DP\ndeployments. We (3) populate our interface with 21 real-world DP deployments\nand (4) conduct an exploratory user study with DP practitioners ($n=16$) to\nunderstand how they would use the registry, as well as what challenges and\nopportunities they foresee around its adoption. We find that participants were\nenthusiastic about the registry as a valuable resource for evaluating prior\ndeployments and making future deployments. They also identified several\nopportunities for the registry, including that it can become a \"hub\" for the\ncommunity and support broader communication around DP (e.g., to legal teams).\nAt the same time, they identified challenges around the registry gaining\nadoption, including the effort and risk involved with making implementation\nchoices public and moderating the quality of entries. Based on our findings, we\noffer recommendations for encouraging adoption and increasing the registry's\nvalue not only to DP practitioners, but also to policymakers, data users, and\ndata subjects."}
{"id": "2509.13803", "pdf": "https://arxiv.org/pdf/2509.13803.pdf", "abs": "https://arxiv.org/abs/2509.13803", "title": "Measuring Gender Bias in Job Title Matching for Grammatical Gender Languages", "authors": ["Laura García-Sardiña", "Hermenegildo Fabregat", "Daniel Deniz", "Rabih Zbib"], "categories": ["cs.CL"], "comment": null, "summary": "This work sets the ground for studying how explicit grammatical gender\nassignment in job titles can affect the results of automatic job ranking\nsystems. We propose the usage of metrics for ranking comparison controlling for\ngender to evaluate gender bias in job title ranking systems, in particular RBO\n(Rank-Biased Overlap). We generate and share test sets for a job title matching\ntask in four grammatical gender languages, including occupations in masculine\nand feminine form and annotated by gender and matching relevance. We use the\nnew test sets and the proposed methodology to evaluate the gender bias of\nseveral out-of-the-box multilingual models to set as baselines, showing that\nall of them exhibit varying degrees of gender bias."}
{"id": "2509.13547", "pdf": "https://arxiv.org/pdf/2509.13547.pdf", "abs": "https://arxiv.org/abs/2509.13547", "title": "AI Agents with Human-Like Collaborative Tools: Adaptive Strategies for Enhanced Problem-Solving", "authors": ["Harper Reed", "Michael Sugimura", "Angelo Zangari"], "categories": ["cs.AI", "cs.HC"], "comment": "16 pages, 5 tables", "summary": "We investigate whether giving LLM agents the collaborative tools and autonomy\nthat humans naturally use for problem solving can improve their performance. We\nequip Claude Code agents with MCP-based social media and journaling tools and\nallow them to use these tools as they see fit. Across 34 Aider Polyglot Python\nprogramming challenges, collaborative tools substantially improve performance\non the hardest problems, delivering 15-40% lower cost, 12-27% fewer turns, and\n12-38% faster completion than baseline agents. Effects on the full challenge\nset are mixed, suggesting these tools act as performance enhancers when\nadditional reasoning scaffolding is most needed. Surprisingly, Different models\nnaturally adopted distinct collaborative strategies without explicit\ninstruction. Sonnet 3.7 engaged broadly across tools and benefited from\narticulation-based cognitive scaffolding. Sonnet 4 showed selective adoption,\nleaning on journal-based semantic search when problems were genuinely\ndifficult. This mirrors how human developers adjust collaboration based on\nexpertise and task complexity. Behavioral analysis shows agents prefer writing\nover reading by about 2-9x, indicating that structured articulation drives much\nof the improvement rather than information access alone. Overall, AI agents can\nsystematically benefit from human-inspired collaboration tools at the edge of\ntheir capabilities, pointing to adaptive collaborative interfaces as reasoning\nenhancers rather than universal efficiency boosts."}
{"id": "2509.13813", "pdf": "https://arxiv.org/pdf/2509.13813.pdf", "abs": "https://arxiv.org/abs/2509.13813", "title": "Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs", "authors": ["Edward Phillips", "Sean Wu", "Soheila Molaei", "Danielle Belgrave", "Anshul Thakur", "David Clifton"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models demonstrate impressive results across diverse tasks but\nare still known to hallucinate, generating linguistically plausible but\nincorrect answers to questions. Uncertainty quantification has been proposed as\na strategy for hallucination detection, but no existing black-box approach\nprovides estimates for both global and local uncertainty. The former attributes\nuncertainty to a batch of responses, while the latter attributes uncertainty to\nindividual responses. Current local methods typically rely on white-box access\nto internal model states, whilst black-box methods only provide global\nuncertainty estimates. We introduce a geometric framework to address this,\nbased on archetypal analysis of batches of responses sampled with only\nblack-box model access. At the global level, we propose Geometric Volume, which\nmeasures the convex hull volume of archetypes derived from response embeddings.\nAt the local level, we propose Geometric Suspicion, which ranks responses by\nreliability and enables hallucination reduction through preferential response\nselection. Unlike prior dispersion methods which yield only a single global\nscore, our approach provides semantic boundary points which have utility for\nattributing reliability to individual responses. Experiments show that our\nframework performs comparably to or better than prior methods on short form\nquestion-answering datasets, and achieves superior results on medical datasets\nwhere hallucinations carry particularly critical risks. We also provide\ntheoretical justification by proving a link between convex hull volume and\nentropy."}
{"id": "2509.13615", "pdf": "https://arxiv.org/pdf/2509.13615.pdf", "abs": "https://arxiv.org/abs/2509.13615", "title": "See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles", "authors": ["Zongru Wu", "Rui Mao", "Zhiyuan Tian", "Pengzhou Cheng", "Tianjie Ju", "Zheng Wu", "Lingzhong Dong", "Haiyue Sheng", "Zhuosheng Zhang", "Gongshen Liu"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "The advent of multimodal agents facilitates effective interaction within\ngraphical user interface (GUI), especially in ubiquitous GUI control. However,\ntheir inability to reliably execute toggle control instructions remains a key\nbottleneck. To investigate this, we construct a state control benchmark with\nbinary toggle instructions from public datasets. Evaluations of existing agents\ndemonstrate their unreliability, particularly when the current toggle state\nalready matches the desired state. To address the challenge, we propose\nState-aware Reasoning (StaR), a training method that teaches agents to perceive\nthe current toggle state, analyze the desired state from the instruction, and\nact accordingly. Experiments on three multimodal agents demonstrate that StaR\ncan improve toggle instruction execution accuracy by over 30\\%. Further\nevaluations on three public benchmarks show that StaR also enhances general\ntask performance. Finally, evaluations on a dynamic environment highlight the\npotential of StaR for real-world applications. Code, benchmark, and\nStaR-enhanced agents are available at https://github.com/ZrW00/StaR."}
{"id": "2509.13814", "pdf": "https://arxiv.org/pdf/2509.13814.pdf", "abs": "https://arxiv.org/abs/2509.13814", "title": "Findings of the Third Automatic Minuting (AutoMin) Challenge", "authors": ["Kartik Shinde", "Laurent Besacier", "Ondrej Bojar", "Thibaut Thonet", "Tirthankar Ghosal"], "categories": ["cs.CL"], "comment": "Automin 2025 Website: https://ufal.github.io/automin-2025/", "summary": "This paper presents the third edition of AutoMin, a shared task on automatic\nmeeting summarization into minutes. In 2025, AutoMin featured the main task of\nminuting, the creation of structured meeting minutes, as well as a new task:\nquestion answering (QA) based on meeting transcripts.\n  The minuting task covered two languages, English and Czech, and two domains:\nproject meetings and European Parliament sessions. The QA task focused solely\non project meetings and was available in two settings: monolingual QA in\nEnglish, and cross-lingual QA, where questions were asked and answered in Czech\nbased on English meetings.\n  Participation in 2025 was more limited compared to previous years, with only\none team joining the minuting task and two teams participating in QA. However,\nas organizers, we included multiple baseline systems to enable a comprehensive\nevaluation of current (2025) large language models (LLMs) on both tasks."}
{"id": "2509.13677", "pdf": "https://arxiv.org/pdf/2509.13677.pdf", "abs": "https://arxiv.org/abs/2509.13677", "title": "AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation", "authors": ["Xinxu Zhou", "Jiaqi Bai", "Zhenqi Sun", "Fanxiang Zeng", "Yue Liu"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Although significant progress has been made in many tasks within the field of\nNatural Language Processing (NLP), Controlled Text Generation (CTG) continues\nto face numerous challenges, particularly in achieving fine-grained conditional\ncontrol over generation. Additionally, in real scenario and online\napplications, cost considerations, scalability, domain knowledge learning and\nmore precise control are required, presenting more challenge for CTG. This\npaper introduces a novel and scalable framework, AgentCTG, which aims to\nenhance precise and complex control over the text generation by simulating the\ncontrol and regulation mechanisms in multi-agent workflows. We explore various\ncollaboration methods among different agents and introduce an auto-prompt\nmodule to further enhance the generation effectiveness. AgentCTG achieves\nstate-of-the-art results on multiple public datasets. To validate its\neffectiveness in practical applications, we propose a new challenging\nCharacter-Driven Rewriting task, which aims to convert the original text into\nnew text that conform to specific character profiles and simultaneously\npreserve the domain knowledge. When applied to online navigation with\nrole-playing, our approach significantly enhances the driving experience\nthrough improved content delivery. By optimizing the generation of contextually\nrelevant text, we enable a more immersive interaction within online\ncommunities, fostering greater personalization and user engagement."}
{"id": "2509.13835", "pdf": "https://arxiv.org/pdf/2509.13835.pdf", "abs": "https://arxiv.org/abs/2509.13835", "title": "Large Language Models Discriminate Against Speakers of German Dialects", "authors": ["Minh Duc Bui", "Carolin Holtermann", "Valentin Hofmann", "Anne Lauscher", "Katharina von der Wense"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Main", "summary": "Dialects represent a significant component of human culture and are found\nacross all regions of the world. In Germany, more than 40% of the population\nspeaks a regional dialect (Adler and Hansen, 2022). However, despite cultural\nimportance, individuals speaking dialects often face negative societal\nstereotypes. We examine whether such stereotypes are mirrored by large language\nmodels (LLMs). We draw on the sociolinguistic literature on dialect perception\nto analyze traits commonly associated with dialect speakers. Based on these\ntraits, we assess the dialect naming bias and dialect usage bias expressed by\nLLMs in two tasks: an association task and a decision task. To assess a model's\ndialect usage bias, we construct a novel evaluation corpus that pairs sentences\nfrom seven regional German dialects (e.g., Alemannic and Bavarian) with their\nstandard German counterparts. We find that: (1) in the association task, all\nevaluated LLMs exhibit significant dialect naming and dialect usage bias\nagainst German dialect speakers, reflected in negative adjective associations;\n(2) all models reproduce these dialect naming and dialect usage biases in their\ndecision making; and (3) contrary to prior work showing minimal bias with\nexplicit demographic mentions, we find that explicitly labeling linguistic\ndemographics--German dialect speakers--amplifies bias more than implicit cues\nlike dialect usage."}
{"id": "2509.13712", "pdf": "https://arxiv.org/pdf/2509.13712.pdf", "abs": "https://arxiv.org/abs/2509.13712", "title": "Inject, Fork, Compare: Defining an Interaction Vocabulary for Multi-Agent Simulation Platforms", "authors": ["HwiJoon Lee", "Martina Di Paola", "Yoo Jin Hong", "Quang-Huy Nguyen", "Joseph Seering"], "categories": ["cs.MA", "cs.HC"], "comment": null, "summary": "LLM-based multi-agent simulations are a rapidly growing field of research,\nbut current simulations often lack clear modes for interaction and analysis,\nlimiting the \"what if\" scenarios researchers are able to investigate. In this\ndemo, we define three core operations for interacting with multi-agent\nsimulations: inject, fork, and compare. Inject allows researchers to introduce\nexternal events at any point during simulation execution. Fork creates\nindependent timeline branches from any timestamp, preserving complete state\nwhile allowing divergent exploration. Compare facilitates parallel observation\nof multiple branches, revealing how different interventions lead to distinct\nemergent behaviors. Together, these operations establish a vocabulary that\ntransforms linear simulation workflows into interactive, explorable spaces. We\ndemonstrate this vocabulary through a commodity market simulation with fourteen\nAI agents, where researchers can inject contrasting events and observe\ndivergent outcomes across parallel timelines. By defining these fundamental\noperations, we provide a starting point for systematic causal investigation in\nLLM-based agent simulations, moving beyond passive observation toward active\nexperimentation."}
{"id": "2509.13869", "pdf": "https://arxiv.org/pdf/2509.13869.pdf", "abs": "https://arxiv.org/abs/2509.13869", "title": "Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs", "authors": ["Yang Liu", "Chenhui Chu"], "categories": ["cs.CL"], "comment": "38 pages, 31 figures", "summary": "Large language models (LLMs) can lead to undesired consequences when\nmisaligned with human values, especially in scenarios involving complex and\nsensitive social biases. Previous studies have revealed the misalignment of\nLLMs with human values using expert-designed or agent-based emulated bias\nscenarios. However, it remains unclear whether the alignment of LLMs with human\nvalues differs across different types of scenarios (e.g., scenarios containing\nnegative vs. non-negative questions). In this study, we investigate the\nalignment of LLMs with human values regarding social biases (HVSB) in different\ntypes of bias scenarios. Through extensive analysis of 12 LLMs from four model\nfamilies and four datasets, we demonstrate that LLMs with large model parameter\nscales do not necessarily have lower misalignment rate and attack success rate.\nMoreover, LLMs show a certain degree of alignment preference for specific types\nof scenarios and the LLMs from the same model family tend to have higher\njudgment consistency. In addition, we study the understanding capacity of LLMs\nwith their explanations of HVSB. We find no significant differences in the\nunderstanding of HVSB across LLMs. We also find LLMs prefer their own generated\nexplanations. Additionally, we endow smaller language models (LMs) with the\nability to explain HVSB. The generation results show that the explanations\ngenerated by the fine-tuned smaller LMs are more readable, but have a\nrelatively lower model agreeability."}
{"id": "2509.14023", "pdf": "https://arxiv.org/pdf/2509.14023.pdf", "abs": "https://arxiv.org/abs/2509.14023", "title": "Audio-Based Crowd-Sourced Evaluation of Machine Translation Quality", "authors": ["Sami Ul Haq", "Sheila Castilho", "Yvette Graham"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted at WMT2025 (ENNLP) for oral presented", "summary": "Machine Translation (MT) has achieved remarkable performance, with growing\ninterest in speech translation and multimodal approaches. However, despite\nthese advancements, MT quality assessment remains largely text centric,\ntypically relying on human experts who read and compare texts. Since many\nreal-world MT applications (e.g Google Translate Voice Mode, iFLYTEK\nTranslator) involve translation being spoken rather printed or read, a more\nnatural way to assess translation quality would be through speech as opposed\ntext-only evaluations. This study compares text-only and audio-based\nevaluations of 10 MT systems from the WMT General MT Shared Task, using\ncrowd-sourced judgments collected via Amazon Mechanical Turk. We additionally,\nperformed statistical significance testing and self-replication experiments to\ntest reliability and consistency of audio-based approach. Crowd-sourced\nassessments based on audio yield rankings largely consistent with text only\nevaluations but, in some cases, identify significant differences between\ntranslation systems. We attribute this to speech richer, more natural modality\nand propose incorporating speech-based assessments into future MT evaluation\nframeworks."}
{"id": "2509.13879", "pdf": "https://arxiv.org/pdf/2509.13879.pdf", "abs": "https://arxiv.org/abs/2509.13879", "title": "Combining Evidence and Reasoning for Biomedical Fact-Checking", "authors": ["Mariano Barone", "Antonio Romano", "Giuseppe Riccio", "Marco Postiglione", "Vincenzo Moscato"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Proceedings of the 48th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval, 2025", "summary": "Misinformation in healthcare, from vaccine hesitancy to unproven treatments,\nposes risks to public health and trust in medical systems. While machine\nlearning and natural language processing have advanced automated fact-checking,\nvalidating biomedical claims remains uniquely challenging due to complex\nterminology, the need for domain expertise, and the critical importance of\ngrounding in scientific evidence. We introduce CER (Combining Evidence and\nReasoning), a novel framework for biomedical fact-checking that integrates\nscientific evidence retrieval, reasoning via large language models, and\nsupervised veracity prediction. By integrating the text-generation capabilities\nof large language models with advanced retrieval techniques for high-quality\nbiomedical scientific evidence, CER effectively mitigates the risk of\nhallucinations, ensuring that generated outputs are grounded in verifiable,\nevidence-based sources. Evaluations on expert-annotated datasets (HealthFC,\nBioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising\ncross-dataset generalization. Code and data are released for transparency and\nreproducibility: https: //github.com/PRAISELab-PicusLab/CER."}
{"id": "2408.11824", "pdf": "https://arxiv.org/pdf/2408.11824.pdf", "abs": "https://arxiv.org/abs/2408.11824", "title": "AppAgent v2: Advanced Agent for Flexible Mobile Interactions", "authors": ["Yanda Li", "Chi Zhang", "Wenjia Jiang", "Wanqi Yang", "Bin Fu", "Pei Cheng", "Xin Chen", "Ling Chen", "Yunchao Wei"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "With the advancement of Multimodal Large Language Models (MLLM), LLM-driven\nvisual agents are increasingly impacting software interfaces, particularly\nthose with graphical user interfaces. This work introduces a novel LLM-based\nmultimodal agent framework for mobile devices. This framework, capable of\nnavigating mobile devices, emulates human-like interactions. Our agent\nconstructs a flexible action space that enhances adaptability across various\napplications including parser, text and vision descriptions. The agent operates\nthrough two main phases: exploration and deployment. During the exploration\nphase, functionalities of user interface elements are documented either through\nagent-driven or manual explorations into a customized structured knowledge\nbase. In the deployment phase, RAG technology enables efficient retrieval and\nupdate from this knowledge base, thereby empowering the agent to perform tasks\neffectively and accurately. This includes performing complex, multi-step\noperations across various applications, thereby demonstrating the framework's\nadaptability and precision in handling customized task workflows. Our\nexperimental results across various benchmarks demonstrate the framework's\nsuperior performance, confirming its effectiveness in real-world scenarios. Our\ncode will be open source soon."}
{"id": "2509.13888", "pdf": "https://arxiv.org/pdf/2509.13888.pdf", "abs": "https://arxiv.org/abs/2509.13888", "title": "Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification", "authors": ["Mariano Barone", "Antonio Romano", "Giuseppe Riccio", "Marco Postiglione", "Vincenzo Moscato"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Misinformation in healthcare, from vaccine hesitancy to unproven treatments,\nposes risks to public health and trust in medical systems. While machine\nlearning and natural language processing have advanced automated fact-checking,\nvalidating biomedical claims remains uniquely challenging due to complex\nterminology, the need for domain expertise, and the critical importance of\ngrounding in scientific evidence. We introduce CER (Combining Evidence and\nReasoning), a novel framework for biomedical fact-checking that integrates\nscientific evidence retrieval, reasoning via large language models, and\nsupervised veracity prediction. By integrating the text-generation capabilities\nof large language models with advanced retrieval techniques for high-quality\nbiomedical scientific evidence, CER effectively mitigates the risk of\nhallucinations, ensuring that generated outputs are grounded in verifiable,\nevidence-based sources. Evaluations on expert-annotated datasets (HealthFC,\nBioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising\ncross-dataset generalization. Code and data are released for transparency and\nreproducibility: https://github.com/PRAISELab-PicusLab/CER"}
{"id": "2502.05347", "pdf": "https://arxiv.org/pdf/2502.05347.pdf", "abs": "https://arxiv.org/abs/2502.05347", "title": "The Role of Human Creativity in the Presence of AI Creativity Tools at Work: A Case Study on AI-Driven Content Transformation in Journalism", "authors": ["Sitong Wang", "Jocelyn McKinnon-Crowley", "Tao Long", "Kian Loong Lua", "Keren Henderson", "Kevin Crowston", "Jeffrey V. Nickerson", "Mark Hansen", "Lydia B. Chilton"], "categories": ["cs.HC"], "comment": null, "summary": "As AI becomes more capable, it is unclear how human creativity will remain\nessential in jobs that incorporate AI. We conducted a 14-week study of a\nstudent newsroom using an AI tool to convert web articles into social media\nvideos. Most creators treated the tool as a creative springboard, not as a\ncompletion mechanism. They edited the AI outputs. The tool enabled the team to\npublish successful content that received over 500,000 views. Human creativity\nremained essential: after AI produced templated outputs, creators took\nownership of the task, injecting their own creativity, especially when AI\nfailed to create appropriate content. AI was initially seen as an authority,\ndue to creators' lack of experience, but they ultimately learned to assert\ntheir own authority."}
{"id": "2509.13905", "pdf": "https://arxiv.org/pdf/2509.13905.pdf", "abs": "https://arxiv.org/abs/2509.13905", "title": "Do Large Language Models Understand Word Senses?", "authors": ["Domenico Meconi", "Simone Stirpe", "Federico Martelli", "Leonardo Lavalle", "Roberto Navigli"], "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, to be published in EMNLP2025", "summary": "Understanding the meaning of words in context is a fundamental capability for\nLarge Language Models (LLMs). Despite extensive evaluation efforts, the extent\nto which LLMs show evidence that they truly grasp word senses remains\nunderexplored. In this paper, we address this gap by evaluating both i) the\nWord Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs,\ncomparing their performance to state-of-the-art systems specifically designed\nfor the task, and ii) the ability of two top-performing open- and closed-source\nLLMs to understand word senses in three generative settings: definition\ngeneration, free-form explanation, and example generation. Notably, we find\nthat, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve\nperformance on par with specialized WSD systems, while also demonstrating\ngreater robustness across domains and levels of difficulty. In the generation\ntasks, results reveal that LLMs can explain the meaning of words in context up\nto 98\\% accuracy, with the highest performance observed in the free-form\nexplanation task, which best aligns with their generative capabilities."}
{"id": "2502.06696", "pdf": "https://arxiv.org/pdf/2502.06696.pdf", "abs": "https://arxiv.org/abs/2502.06696", "title": "Social Media Should Feel Like Minecraft, Not Instagram: 3D Gamer Youth Visions for Meaningful Social Connections through Fictional Inquiry", "authors": ["JaeWon Kim", "Hyunsung Cho", "Fannie Liu", "Alexis Hiniker"], "categories": ["cs.HC"], "comment": null, "summary": "We investigate youth visions for ideal remote social interactions, drawing on\nco-design interviews with 23 participants (aged 15-24) experienced with 3D\ngaming environments. Using a Fictional Inquiry (FI) method set in the Harry\nPotter universe, this research reveals that young people desire social media\nthat functions more like immersive, navigable shared social spaces. Across\nthese interviews, participants identified six key priorities for meaningful\nsocial connection over social media: intuitive social navigation, shared\ncollaborative experiences, communal environments fostering close relationships,\nflexible self-presentation, intentional engagement, and playful social\nmechanics. We introduce the \\textit{spatial integrity} framework, a set of four\ninterrelated design principles: spatial presence, spatial composition, spatial\nconfiguration, and spatial depth. Together, these principles outline how online\nspaces can be designed to feel more like meaningful environments, spaces where\nrelationships can grow through shared presence, movement, and intentional\ninteraction. Participants also described the FI process itself as meaningful,\nnot only for generating new ideas but for empowering them to imagine and shape\nthe future of social media."}
{"id": "2509.13930", "pdf": "https://arxiv.org/pdf/2509.13930.pdf", "abs": "https://arxiv.org/abs/2509.13930", "title": "Linguistic Nepotism: Trading-off Quality for Language Preference in Multilingual RAG", "authors": ["Dayeon Ki", "Marine Carpuat", "Paul McNamee", "Daniel Khashabi", "Eugene Yang", "Dawn Lawrie", "Kevin Duh"], "categories": ["cs.CL"], "comment": "33 pages, 20 figures", "summary": "Multilingual Retrieval-Augmented Generation (mRAG) systems enable language\nmodels to answer knowledge-intensive queries with citation-supported responses\nacross languages. While such systems have been proposed, an open questions is\nwhether the mixture of different document languages impacts generation and\ncitation in unintended ways. To investigate, we introduce a controlled\nmethodology using model internals to measure language preference while holding\nother factors such as document relevance constant. Across eight languages and\nsix open-weight models, we find that models preferentially cite English sources\nwhen queries are in English, with this bias amplified for lower-resource\nlanguages and for documents positioned mid-context. Crucially, we find that\nmodels sometimes trade-off document relevance for language preference,\nindicating that citation choices are not always driven by informativeness\nalone. Our findings shed light on how language models leverage multilingual\ncontext and influence citation behavior."}
{"id": "2504.01153", "pdf": "https://arxiv.org/pdf/2504.01153.pdf", "abs": "https://arxiv.org/abs/2504.01153", "title": "Catch Me if You Search: When Contextual Web Search Results Affect the Detection of Hallucinations", "authors": ["Mahjabin Nahar", "Eun-Ju Lee", "Jin Won Park", "Dongwon Lee"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "Accepted to Computers in Human Behavior", "summary": "While we increasingly rely on large language models (LLMs) for various tasks,\nthese models are known to produce inaccurate content or 'hallucinations' with\npotentially disastrous consequences. The recent integration of web search\nresults into LLMs prompts the question of whether people utilize them to verify\nthe generated content, thereby accurately detecting hallucinations. An online\nexperiment (N=560) investigated how the provision of search results, either\nstatic (i.e., fixed search results provided by LLM) or dynamic (i.e.,\nparticipant-led searches), affects participants' perceived accuracy of\nLLM-generated content (i.e., genuine, minor hallucination, major\nhallucination), self-confidence in accuracy ratings, as well as their overall\nevaluation of the LLM, as compared to the control condition (i.e., no search\nresults). Results showed that participants in both static and dynamic\nconditions (vs. control) rated hallucinated content to be less accurate and\nperceived the LLM more negatively. However, those in the dynamic condition\nrated genuine content as more accurate and demonstrated greater overall\nself-confidence in their assessments than those in the static search or control\nconditions. We highlighted practical implications of incorporating web search\nfunctionality into LLMs in real-world contexts."}
{"id": "2509.13980", "pdf": "https://arxiv.org/pdf/2509.13980.pdf", "abs": "https://arxiv.org/abs/2509.13980", "title": "Long-context Reference-based MT Quality Estimation", "authors": ["Sami Ul Haq", "Chinonso Cynthia Osuji", "Sheila Castilho", "Brian Davis"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In this paper, we present our submission to the Tenth Conference on Machine\nTranslation (WMT25) Shared Task on Automated Translation Quality Evaluation.\n  Our systems are built upon the COMET framework and trained to predict\nsegment-level Error Span Annotation (ESA) scores using augmented long-context\ndata.\n  To construct long-context training data, we concatenate in-domain,\nhuman-annotated sentences and compute a weighted average of their scores.\n  We integrate multiple human judgment datasets (MQM, SQM, and DA) by\nnormalising their scales and train multilingual regression models to predict\nquality scores from the source, hypothesis, and reference translations.\n  Experimental results show that incorporating long-context information\nimproves correlations with human judgments compared to models trained only on\nshort segments."}
{"id": "2507.00202", "pdf": "https://arxiv.org/pdf/2507.00202.pdf", "abs": "https://arxiv.org/abs/2507.00202", "title": "The Role of AAC in Social Communication and Community Engagement: Experiences and Opinions of Autistic Adults", "authors": ["Blade Frisch", "Betts Peters", "Keith Vertanen"], "categories": ["cs.HC"], "comment": null, "summary": "Little research has explored the communication needs of autistic adults.\nAugmentative and alternative communication (AAC) can support these\ncommunication needs, but more guidance is needed on how to design AAC systems\nto support this population. We conducted an online, asynchronous, text-based\nfocus group with five autistic adults to explore their social communication and\ncommunity engagement and how AAC might support them. Our analysis found 1)\nparticipants' emotional experiences impact the communication methods they use,\n2) speaking autistic adults can benefit from AAC use, and 3) autistic shutdown\ncreates dynamic communication needs. We present implications for AAC interface\ndesign: supporting communication during shutdown, indicating communication\nability, and addressing the fear of using AAC. We provide themes for future\nautism research: exploring the impact of a late diagnosis, understanding\ncommunication needs during shutdown, and researching the social and\nenvironmental factors that impact communication. Finally, we provide guidance\nfor future online focus groups."}
{"id": "2509.13990", "pdf": "https://arxiv.org/pdf/2509.13990.pdf", "abs": "https://arxiv.org/abs/2509.13990", "title": "Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency", "authors": ["Colin Hong", "Xu Guo", "Anand Chaanan Singh", "Esha Choukse", "Dmitrii Ustiugov"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": "Accepted by EMNLP 2025 (Oral), 9 pages", "summary": "Recently, Test-Time Scaling (TTS) has gained increasing attention for\nimproving LLM reasoning performance at test time without retraining the model.\nA notable TTS technique is Self-Consistency (SC), which generates multiple\nreasoning chains in parallel and selects the final answer via majority voting.\nWhile effective, the order-of-magnitude computational overhead limits its broad\ndeployment. Prior attempts to accelerate SC mainly rely on model-based\nconfidence scores or heuristics with limited empirical support. For the first\ntime, we theoretically and empirically analyze the inefficiencies of SC and\nreveal actionable opportunities for improvement. Building on these insights, we\npropose Slim-SC, a step-wise pruning strategy that identifies and removes\nredundant chains using inter-chain similarity at the thought level. Experiments\non three STEM reasoning datasets and two recent LLM architectures show that\nSlim-SC reduces inference latency and KVC usage by up to 45% and 26%,\nrespectively, with R1-Distill, while maintaining or improving accuracy, thus\noffering a simple yet efficient TTS alternative for SC."}
{"id": "2508.11327", "pdf": "https://arxiv.org/pdf/2508.11327.pdf", "abs": "https://arxiv.org/abs/2508.11327", "title": "The User-first Approach to AI Ethics: Preferences for Ethical Principles in AI Systems across Cultures and Contexts", "authors": ["Benjamin J. Carroll", "Jianlong Zhou", "Paul F. Burke", "Sabine Ammon"], "categories": ["cs.HC"], "comment": null, "summary": "As AI systems increasingly permeate everyday life, designers and developers\nface mounting pressure to balance innovation with ethical design choices. To\ndate, the operationalisation of AI ethics has predominantly depended on\nframeworks that prescribe which ethical principles should be embedded within AI\nsystems. However, the extent to which users value these principles remains\nlargely unexplored in the existing literature. In a discrete choice experiment\nconducted in four countries, we quantify user preferences for 11 ethical\nprinciples. Our findings indicate that, while users generally prioritise\nprivacy, justice & fairness, and transparency, their preferences exhibit\nsignificant variation based on culture and application context. Latent class\nanalysis further revealed four distinct user cohorts, the largest of which is\nethically disengaged and defers to regulatory oversight. Our findings offer (1)\nempirical evidence of uneven user prioritisation of AI ethics principles, (2)\nactionable guidance for operationalising ethics tailored to culture and\ncontext, (3) support for the development of robust regulatory mechanisms, and\n(4) a foundation for advancing a user-centred approach to AI ethics, motivated\nindependently from abstract moral theory."}
{"id": "2509.14004", "pdf": "https://arxiv.org/pdf/2509.14004.pdf", "abs": "https://arxiv.org/abs/2509.14004", "title": "Early Stopping Chain-of-thoughts in Large Language Models", "authors": ["Minjia Mao", "Bowen Yin", "Yu Zhu", "Xiao Fang"], "categories": ["cs.CL"], "comment": null, "summary": "Reasoning large language models (LLMs) have demonstrated superior capacities\nin solving complicated problems by generating long chain-of-thoughts (CoT), but\nsuch a lengthy CoT incurs high inference costs. In this study, we introduce\nES-CoT, an inference-time method that shortens CoT generation by detecting\nanswer convergence and stopping early with minimal performance loss. At the end\nof each reasoning step, we prompt the LLM to output its current final answer,\ndenoted as a step answer. We then track the run length of consecutive identical\nstep answers as a measure of answer convergence. Once the run length exhibits a\nsharp increase and exceeds a minimum threshold, the generation is terminated.\nWe provide both empirical and theoretical support for this heuristic: step\nanswers steadily converge to the final answer, and large run-length jumps\nreliably mark this convergence. Experiments on five reasoning datasets across\nthree LLMs show that ES-CoT reduces the number of inference tokens by about\n41\\% on average while maintaining accuracy comparable to standard CoT. Further,\nES-CoT integrates seamlessly with self-consistency prompting and remains robust\nacross hyperparameter choices, highlighting it as a practical and effective\napproach for efficient reasoning."}
{"id": "2509.11062", "pdf": "https://arxiv.org/pdf/2509.11062.pdf", "abs": "https://arxiv.org/abs/2509.11062", "title": "Auto-Slides: An Interactive Multi-Agent System for Creating and Customizing Research Presentations", "authors": ["Yuheng Yang", "Wenjia Jiang", "Yang Wang", "Yiwei Wang", "Chi Zhang"], "categories": ["cs.HC", "cs.MA"], "comment": "Project Homepage: https://auto-slides.github.io/", "summary": "The rapid progress of large language models (LLMs) has opened new\nopportunities for education. While learners can interact with academic papers\nthrough LLM-powered dialogue, limitations still exist: absence of structured\norganization and high text reliance can impede systematic understanding and\nengagement with complex concepts. To address these challenges, we propose\nAuto-Slides, an LLM-driven system that converts research papers into\npedagogically structured, multimodal slides (e.g., diagrams and tables).\nDrawing on cognitive science, it creates a presentation-oriented narrative and\nallows iterative refinement via an interactive editor, in order to match\nlearners' knowledge level and goals. Auto-Slides further incorporates\nverification and knowledge retrieval mechanisms to ensure accuracy and\ncontextual completeness. Through extensive user studies, Auto-Slides enhances\nlearners' comprehension and engagement compared to conventional LLM-based\nreading. Our contributions lie in designing a multi-agent framework for\ntransforming academic papers into pedagogically optimized slides and\nintroducing interactive customization for personalized learning."}
{"id": "2509.14008", "pdf": "https://arxiv.org/pdf/2509.14008.pdf", "abs": "https://arxiv.org/abs/2509.14008", "title": "Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale", "authors": ["Hasan Abed Al Kader Hammoud", "Mohammad Zbeeb", "Bernard Ghanem"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Technical Report", "summary": "We present Hala, a family of Arabic-centric instruction and translation\nmodels built with our translate-and-tune pipeline. We first compress a strong\nAR$\\leftrightarrow$EN teacher to FP8 (yielding $\\sim$2$\\times$ higher\nthroughput with no quality loss) and use it to create high-fidelity bilingual\nsupervision. A lightweight language model LFM2-1.2B is then fine-tuned on this\ndata and used to translate high-quality English instruction sets into Arabic,\nproducing a million-scale corpus tailored to instruction following. We train\nHala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to\nbalance Arabic specialization with base-model strengths. On Arabic-centric\nbenchmarks, Hala achieves state-of-the-art results within both the \"nano\"\n($\\leq$2B) and \"small\" (7-9B) categories, outperforming their bases. We release\nmodels, data, evaluation, and recipes to accelerate research in Arabic NLP."}
{"id": "2410.09252", "pdf": "https://arxiv.org/pdf/2410.09252.pdf", "abs": "https://arxiv.org/abs/2410.09252", "title": "DAVIS: Planning Agent with Knowledge Graph-Powered Inner Monologue", "authors": ["Minh Pham Dinh", "Munira Syed", "Michael G Yankoski", "Trenton W. Ford"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "Designing a generalist scientific agent capable of performing tasks in\nlaboratory settings to assist researchers has become a key goal in recent\nArtificial Intelligence (AI) research. Unlike everyday tasks, scientific tasks\nare inherently more delicate and complex, requiring agents to possess a higher\nlevel of reasoning ability, structured and temporal understanding of their\nenvironment, and a strong emphasis on safety. Existing approaches often fail to\naddress these multifaceted requirements. To tackle these challenges, we present\nDAVIS. Unlike traditional retrieval-augmented generation (RAG) approaches,\nDAVIS incorporates structured and temporal memory, which enables model-based\nplanning. Additionally, DAVIS implements an agentic, multi-turn retrieval\nsystem, similar to a human's inner monologue, allowing for a greater degree of\nreasoning over past experiences. DAVIS demonstrates substantially improved\nperformance on the ScienceWorld benchmark comparing to previous approaches on 8\nout of 9 elementary science subjects. In addition, DAVIS's World Model\ndemonstrates competitive performance on the famous HotpotQA and MusiqueQA\ndataset for multi-hop question answering. To the best of our knowledge, DAVIS\nis the first RAG agent to employ an interactive retrieval method in a RAG\npipeline."}
{"id": "2509.14023", "pdf": "https://arxiv.org/pdf/2509.14023.pdf", "abs": "https://arxiv.org/abs/2509.14023", "title": "Audio-Based Crowd-Sourced Evaluation of Machine Translation Quality", "authors": ["Sami Ul Haq", "Sheila Castilho", "Yvette Graham"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted at WMT2025 (ENNLP) for oral presented", "summary": "Machine Translation (MT) has achieved remarkable performance, with growing\ninterest in speech translation and multimodal approaches. However, despite\nthese advancements, MT quality assessment remains largely text centric,\ntypically relying on human experts who read and compare texts. Since many\nreal-world MT applications (e.g Google Translate Voice Mode, iFLYTEK\nTranslator) involve translation being spoken rather printed or read, a more\nnatural way to assess translation quality would be through speech as opposed\ntext-only evaluations. This study compares text-only and audio-based\nevaluations of 10 MT systems from the WMT General MT Shared Task, using\ncrowd-sourced judgments collected via Amazon Mechanical Turk. We additionally,\nperformed statistical significance testing and self-replication experiments to\ntest reliability and consistency of audio-based approach. Crowd-sourced\nassessments based on audio yield rankings largely consistent with text only\nevaluations but, in some cases, identify significant differences between\ntranslation systems. We attribute this to speech richer, more natural modality\nand propose incorporating speech-based assessments into future MT evaluation\nframeworks."}
{"id": "2412.12478", "pdf": "https://arxiv.org/pdf/2412.12478.pdf", "abs": "https://arxiv.org/abs/2412.12478", "title": "Human-in-the-Loop Generation of Adversarial Texts: A Case Study on Tibetan Script", "authors": ["Xi Cao", "Yuan Sun", "Jiajun Li", "Quzong Gesang", "Nuo Qun", "Tashi Nyima"], "categories": ["cs.CL", "cs.CR", "cs.HC"], "comment": null, "summary": "DNN-based language models excel across various NLP tasks but remain highly\nvulnerable to textual adversarial attacks. While adversarial text generation is\ncrucial for NLP security, explainability, evaluation, and data augmentation,\nrelated work remains overwhelmingly English-centric, leaving the problem of\nconstructing high-quality and sustainable adversarial robustness benchmarks for\nlower-resourced languages both difficult and understudied. First, method\ncustomization for lower-resourced languages is complicated due to linguistic\ndifferences and limited resources. Second, automated attacks are prone to\ngenerating invalid or ambiguous adversarial texts. Last but not least, language\nmodels continuously evolve and may be immune to parts of previously generated\nadversarial texts. To address these challenges, we introduce HITL-GAT, an\ninteractive system based on a general approach to human-in-the-loop generation\nof adversarial texts. Additionally, we demonstrate the utility of HITL-GAT\nthrough a case study on Tibetan script, employing three customized adversarial\ntext generation methods and establishing its first adversarial robustness\nbenchmark, providing a valuable reference for other lower-resourced languages."}
{"id": "2509.14031", "pdf": "https://arxiv.org/pdf/2509.14031.pdf", "abs": "https://arxiv.org/abs/2509.14031", "title": "You Are What You Train: Effects of Data Composition on Training Context-aware Machine Translation Models", "authors": ["Paweł Mąka", "Yusuf Can Semerci", "Jan Scholtes", "Gerasimos Spanakis"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "EMNLP 2025 main conference", "summary": "Achieving human-level translations requires leveraging context to ensure\ncoherence and handle complex phenomena like pronoun disambiguation. Sparsity of\ncontextually rich examples in the standard training data has been hypothesized\nas the reason for the difficulty of context utilization. In this work, we\nsystematically validate this claim in both single- and multilingual settings by\nconstructing training datasets with a controlled proportions of contextually\nrelevant examples. We demonstrate a strong association between training data\nsparsity and model performance confirming sparsity as a key bottleneck.\nImportantly, we reveal that improvements in one contextual phenomenon do no\ngeneralize to others. While we observe some cross-lingual transfer, it is not\nsignificantly higher between languages within the same sub-family. Finally, we\npropose and empirically evaluate two training strategies designed to leverage\nthe available data. These strategies improve context utilization, resulting in\naccuracy gains of up to 6 and 8 percentage points on the ctxPro evaluation in\nsingle- and multilingual settings respectively."}
{"id": "2506.00308", "pdf": "https://arxiv.org/pdf/2506.00308.pdf", "abs": "https://arxiv.org/abs/2506.00308", "title": "MythTriage: Scalable Detection of Opioid Use Disorder Myths on a Video-Sharing Platform", "authors": ["Hayoung Jung", "Shravika Mittal", "Ananya Aatreya", "Navreet Kaur", "Munmun De Choudhury", "Tanushree Mitra"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": "To appear at EMNLP 2025. Please cite EMNLP version when proceedings\n  are available", "summary": "Understanding the prevalence of misinformation in health topics online can\ninform public health policies and interventions. However, measuring such\nmisinformation at scale remains a challenge, particularly for high-stakes but\nunderstudied topics like opioid-use disorder (OUD)--a leading cause of death in\nthe U.S. We present the first large-scale study of OUD-related myths on\nYouTube, a widely-used platform for health information. With clinical experts,\nwe validate 8 pervasive myths and release an expert-labeled video dataset. To\nscale labeling, we introduce MythTriage, an efficient triage pipeline that uses\na lightweight model for routine cases and defers harder ones to a\nhigh-performing, but costlier, large language model (LLM). MythTriage achieves\nup to 0.86 macro F1-score while estimated to reduce annotation time and\nfinancial cost by over 76% compared to experts and full LLM labeling. We\nanalyze 2.9K search results and 343K recommendations, uncovering how myths\npersist on YouTube and offering actionable insights for public health and\nplatform moderation."}
{"id": "2509.14034", "pdf": "https://arxiv.org/pdf/2509.14034.pdf", "abs": "https://arxiv.org/abs/2509.14034", "title": "Enhancing Multi-Agent Debate System Performance via Confidence Expression", "authors": ["Zijie Lin", "Bryan Hooi"], "categories": ["cs.CL"], "comment": "EMNLP'25 Findings", "summary": "Generative Large Language Models (LLMs) have demonstrated remarkable\nperformance across a wide range of tasks. Recent research has introduced\nMulti-Agent Debate (MAD) systems, which leverage multiple LLMs to simulate\nhuman debate and thereby improve task performance. However, while some LLMs may\npossess superior knowledge or reasoning capabilities for specific tasks, they\noften struggle to clearly communicate this advantage during debates, in part\ndue to a lack of confidence expression. Moreover, inappropriate confidence\nexpression can cause agents in MAD systems to either stubbornly maintain\nincorrect beliefs or converge prematurely on suboptimal answers, ultimately\nreducing debate effectiveness and overall system performance. To address these\nchallenges, we propose incorporating confidence expression into MAD systems to\nallow LLMs to explicitly communicate their confidence levels. To validate this\napproach, we develop ConfMAD, a MAD framework that integrates confidence\nexpression throughout the debate process. Experimental results demonstrate the\neffectiveness of our method, and we further analyze how confidence influences\ndebate dynamics, offering insights into the design of confidence-aware MAD\nsystems."}
{"id": "2509.14036", "pdf": "https://arxiv.org/pdf/2509.14036.pdf", "abs": "https://arxiv.org/abs/2509.14036", "title": "SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting for Question-Based Sign Language Translation", "authors": ["Zekang Liu", "Wei Feng", "Fanhua Shang", "Lianyu Hu", "Jichao Feng", "Liqing Gao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Sign Language Translation (SLT) bridges the communication gap between deaf\npeople and hearing people, where dialogue provides crucial contextual cues to\naid in translation. Building on this foundational concept, this paper proposes\nQuestion-based Sign Language Translation (QB-SLT), a novel task that explores\nthe efficient integration of dialogue. Unlike gloss (sign language\ntranscription) annotations, dialogue naturally occurs in communication and is\neasier to annotate. The key challenge lies in aligning multimodality features\nwhile leveraging the context of the question to improve translation. To address\nthis issue, we propose a cross-modality Self-supervised Learning with Sigmoid\nSelf-attention Weighting (SSL-SSAW) fusion method for sign language\ntranslation. Specifically, we employ contrastive learning to align\nmultimodality features in QB-SLT, then introduce a Sigmoid Self-attention\nWeighting (SSAW) module for adaptive feature extraction from question and sign\nlanguage sequences. Additionally, we leverage available question text through\nself-supervised learning to enhance representation and translation\ncapabilities. We evaluated our approach on newly constructed CSL-Daily-QA and\nPHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably,\neasily accessible question assistance can achieve or even surpass the\nperformance of gloss assistance. Furthermore, visualization results demonstrate\nthe effectiveness of incorporating dialogue in improving translation quality."}
{"id": "2509.14128", "pdf": "https://arxiv.org/pdf/2509.14128.pdf", "abs": "https://arxiv.org/abs/2509.14128", "title": "Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance Models for Multilingual ASR and AST", "authors": ["Monica Sekoyan", "Nithin Rao Koluguri", "Nune Tadevosyan", "Piotr Zelasko", "Travis Bartley", "Nick Karpov", "Jagadeesh Balam", "Boris Ginsburg"], "categories": ["cs.CL", "eess.AS"], "comment": "Mini Version of it Submitted to ICASSP 2026", "summary": "This report introduces Canary-1B-v2, a fast, robust multilingual model for\nAutomatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built\nwith a FastConformer encoder and Transformer decoder, it supports 25 languages\nprimarily European. The model was trained on 1.7M hours of total data samples,\nincluding Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce\nhallucinations for ASR and AST. We describe its two-stage pre-training and\nfine-tuning process with dynamic data balancing, as well as experiments with an\nnGPT encoder. Results show nGPT scales well with massive data, while\nFastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the\nNeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable\nsegment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2\noutperforms Whisper-large-v3 on English ASR while being 10x faster, and\ndelivers competitive multilingual ASR and AST performance against larger models\nlike Seamless-M4T-v2-large and LLM-based systems. We also release\nParakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the\nsame 25 languages with just 600M parameters."}
{"id": "2509.14161", "pdf": "https://arxiv.org/pdf/2509.14161.pdf", "abs": "https://arxiv.org/abs/2509.14161", "title": "CS-FLEURS: A Massively Multilingual and Code-Switched Speech Dataset", "authors": ["Brian Yan", "Injy Hamed", "Shuichiro Shimizu", "Vasista Lodagala", "William Chen", "Olga Iakovenko", "Bashar Talafha", "Amir Hussein", "Alexander Polok", "Kalvin Chang", "Dominik Klement", "Sara Althubaiti", "Puyuan Peng", "Matthew Wiesner", "Thamar Solorio", "Ahmed Ali", "Sanjeev Khudanpur", "Shinji Watanabe", "Chih-Chen Chen", "Zhen Wu", "Karim Benharrak", "Anuj Diwan", "Samuele Cornell", "Eunjung Yeo", "Kwanghee Choi", "Carlos Carvalho", "Karen Rosero"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "We present CS-FLEURS, a new dataset for developing and evaluating\ncode-switched speech recognition and translation systems beyond high-resourced\nlanguages. CS-FLEURS consists of 4 test sets which cover in total 113 unique\ncode-switched language pairs across 52 languages: 1) a 14 X-English language\npair set with real voices reading synthetically generated code-switched\nsentences, 2) a 16 X-English language pair set with generative text-to-speech\n3) a 60 {Arabic, Mandarin, Hindi, Spanish}-X language pair set with the\ngenerative text-to-speech, and 4) a 45 X-English lower-resourced language pair\ntest set with concatenative text-to-speech. Besides the four test sets,\nCS-FLEURS also provides a training set with 128 hours of generative\ntext-to-speech data across 16 X-English language pairs. Our hope is that\nCS-FLEURS helps to broaden the scope of future code-switched speech research.\nDataset link: https://huggingface.co/datasets/byan/cs-fleurs."}
{"id": "2509.14171", "pdf": "https://arxiv.org/pdf/2509.14171.pdf", "abs": "https://arxiv.org/abs/2509.14171", "title": "AssoCiAm: A Benchmark for Evaluating Association Thinking while Circumventing Ambiguity", "authors": ["Yifan Liu", "Wenkuan Zhao", "Shanshan Zhong", "Jinghui Qin", "Mingfu Liang", "Zhongzhan Huang", "Wushao Wen"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in multimodal large language models (MLLMs) have garnered\nsignificant attention, offering a promising pathway toward artificial general\nintelligence (AGI). Among the essential capabilities required for AGI,\ncreativity has emerged as a critical trait for MLLMs, with association serving\nas its foundation. Association reflects a model' s ability to think creatively,\nmaking it vital to evaluate and understand. While several frameworks have been\nproposed to assess associative ability, they often overlook the inherent\nambiguity in association tasks, which arises from the divergent nature of\nassociations and undermines the reliability of evaluations. To address this\nissue, we decompose ambiguity into two types-internal ambiguity and external\nambiguity-and introduce AssoCiAm, a benchmark designed to evaluate associative\nability while circumventing the ambiguity through a hybrid computational\nmethod. We then conduct extensive experiments on MLLMs, revealing a strong\npositive correlation between cognition and association. Additionally, we\nobserve that the presence of ambiguity in the evaluation process causes MLLMs'\nbehavior to become more random-like. Finally, we validate the effectiveness of\nour method in ensuring more accurate and reliable evaluations. See Project Page\nfor the data and codes."}
{"id": "2509.14180", "pdf": "https://arxiv.org/pdf/2509.14180.pdf", "abs": "https://arxiv.org/abs/2509.14180", "title": "Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs", "authors": ["Akhil Theerthala"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7; J.4"], "comment": "24 pages, 11 figures. The paper presents a novel framework for\n  generating a personal finance dataset. The resulting fine-tuned model and\n  dataset are publicly available", "summary": "Personalized financial advice requires consideration of user goals,\nconstraints, risk tolerance, and jurisdiction. Prior LLM work has focused on\nsupport systems for investors and financial planners. Simultaneously, numerous\nrecent studies examine broader personal finance tasks, including budgeting,\ndebt management, retirement, and estate planning, through agentic pipelines\nthat incur high maintenance costs, yielding less than 25% of their expected\nfinancial returns. In this study, we introduce a novel and reproducible\nframework that integrates relevant financial context with behavioral finance\nstudies to construct supervision data for end-to-end advisors. Using this\nframework, we create a 19k sample reasoning dataset and conduct a comprehensive\nfine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test\nsplit and a blind LLM-jury study, we demonstrate that through careful data\ncuration and behavioral integration, our 8B model achieves performance\ncomparable to significantly larger baselines (14-32B parameters) across factual\naccuracy, fluency, and personalization metrics while incurring 80% lower costs\nthan the larger counterparts."}
{"id": "2509.14197", "pdf": "https://arxiv.org/pdf/2509.14197.pdf", "abs": "https://arxiv.org/abs/2509.14197", "title": "Framing Migration: A Computational Analysis of UK Parliamentary Discourse", "authors": ["Vahid Ghafouri", "Robert McNeil", "Teodor Yankov", "Madeleine Sumption", "Luc Rocher", "Scott A. Hale", "Adam Mahdi"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "We present a large-scale computational analysis of migration-related\ndiscourse in UK parliamentary debates spanning over 75 years and compare it\nwith US congressional discourse. Using open-weight LLMs, we annotate each\nstatement with high-level stances toward migrants and track the net tone toward\nmigrants across time and political parties. For the UK, we extend this with a\nsemi-automated framework for extracting fine-grained narrative frames to\ncapture nuances of migration discourse. Our findings show that, while US\ndiscourse has grown increasingly polarised, UK parliamentary attitudes remain\nrelatively aligned across parties, with a persistent ideological gap between\nLabour and the Conservatives, reaching its most negative level in 2025. The\nanalysis of narrative frames in the UK parliamentary statements reveals a shift\ntoward securitised narratives such as border control and illegal immigration,\nwhile longer-term integration-oriented frames such as social integration have\ndeclined. Moreover, discussions of national law about immigration have been\nreplaced over time by international law and human rights, revealing nuances in\ndiscourse trends. Taken together broadly, our findings demonstrate how LLMs can\nsupport scalable, fine-grained discourse analysis in political and historical\ncontexts."}
{"id": "2509.14233", "pdf": "https://arxiv.org/pdf/2509.14233.pdf", "abs": "https://arxiv.org/abs/2509.14233", "title": "Apertus: Democratizing Open and Compliant LLMs for Global Language Environments", "authors": ["Alejandro Hernández-Cano", "Alexander Hägele", "Allen Hao Huang", "Angelika Romanou", "Antoni-Joan Solergibert", "Barna Pasztor", "Bettina Messmer", "Dhia Garbaya", "Eduard Frank Ďurech", "Ido Hakimi", "Juan García Giraldo", "Mete Ismayilzada", "Negar Foroutan", "Skander Moalla", "Tiancheng Chen", "Vinko Sabolčec", "Yixuan Xu", "Michael Aerni", "Badr AlKhamissi", "Ines Altemir Marinas", "Mohammad Hossein Amani", "Matin Ansaripour", "Ilia Badanin", "Harold Benoit", "Emanuela Boros", "Nicholas Browning", "Fabian Bösch", "Maximilian Böther", "Niklas Canova", "Camille Challier", "Clement Charmillot", "Jonathan Coles", "Jan Deriu", "Arnout Devos", "Lukas Drescher", "Daniil Dzenhaliou", "Maud Ehrmann", "Dongyang Fan", "Simin Fan", "Silin Gao", "Miguel Gila", "María Grandury", "Diba Hashemi", "Alexander Hoyle", "Jiaming Jiang", "Mark Klein", "Andrei Kucharavy", "Anastasiia Kucherenko", "Frederike Lübeck", "Roman Machacek", "Theofilos Manitaras", "Andreas Marfurt", "Kyle Matoba", "Simon Matrenok", "Henrique Mendoncça", "Fawzi Roberto Mohamed", "Syrielle Montariol", "Luca Mouchel", "Sven Najem-Meyer", "Jingwei Ni", "Gennaro Oliva", "Matteo Pagliardini", "Elia Palme", "Andrei Panferov", "Léo Paoletti", "Marco Passerini", "Ivan Pavlov", "Auguste Poiroux", "Kaustubh Ponkshe", "Nathan Ranchin", "Javi Rando", "Mathieu Sauser", "Jakhongir Saydaliev", "Muhammad Ali Sayfiddinov", "Marian Schneider", "Stefano Schuppli", "Marco Scialanga", "Andrei Semenov", "Kumar Shridhar", "Raghav Singhal", "Anna Sotnikova", "Alexander Sternfeld", "Ayush Kumar Tarun", "Paul Teiletche", "Jannis Vamvas", "Xiaozhe Yao", "Hao Zhao Alexander Ilic", "Ana Klimovic", "Andreas Krause", "Caglar Gulcehre", "David Rosenthal", "Elliott Ash", "Florian Tramèr", "Joost VandeVondele", "Livio Veraldi", "Martin Rajman", "Thomas Schulthess", "Torsten Hoefler", "Antoine Bosselut", "Martin Jaggi", "Imanol Schlag"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present Apertus, a fully open suite of large language models (LLMs)\ndesigned to address two systemic shortcomings in today's open model ecosystem:\ndata compliance and multilingual representation. Unlike many prior models that\nrelease weights without reproducible data pipelines or regard for content-owner\nrights, Apertus models are pretrained exclusively on openly available data,\nretroactively respecting robots.txt exclusions and filtering for\nnon-permissive, toxic, and personally identifiable content. To mitigate risks\nof memorization, we adopt the Goldfish objective during pretraining, strongly\nsuppressing verbatim recall of data while retaining downstream task\nperformance. The Apertus models also expand multilingual coverage, training on\n15T tokens from over 1800 languages, with ~40% of pretraining data allocated to\nnon-English content. Released at 8B and 70B scales, Apertus approaches\nstate-of-the-art results among fully open models on multilingual benchmarks,\nrivalling or surpassing open-weight counterparts. Beyond model weights, we\nrelease all scientific artifacts from our development cycle with a permissive\nlicense, including data preparation scripts, checkpoints, evaluation suites,\nand training code, enabling transparent audit and extension."}
{"id": "2509.12577", "pdf": "https://arxiv.org/pdf/2509.12577.pdf", "abs": "https://arxiv.org/abs/2509.12577", "title": "An AI-Powered Framework for Analyzing Collective Idea Evolution in Deliberative Assemblies", "authors": ["Elinor Poole-Dayan", "Deb Roy", "Jad Kabbara"], "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "In an era of increasing societal fragmentation, political polarization, and\nerosion of public trust in institutions, representative deliberative assemblies\nare emerging as a promising democratic forum for developing effective policy\noutcomes on complex global issues. Despite theoretical attention, there remains\nlimited empirical work that systematically traces how specific ideas evolve,\nare prioritized, or are discarded during deliberation to form policy\nrecommendations. Addressing these gaps, this work poses two central questions:\n(1) How might we trace the evolution and distillation of ideas into concrete\nrecommendations within deliberative assemblies? (2) How does the deliberative\nprocess shape delegate perspectives and influence voting dynamics over the\ncourse of the assembly? To address these questions, we develop LLM-based\nmethodologies for empirically analyzing transcripts from a tech-enhanced\nin-person deliberative assembly. The framework identifies and visualizes the\nspace of expressed suggestions. We also empirically reconstruct each delegate's\nevolving perspective throughout the assembly. Our methods contribute novel\nempirical insights into deliberative processes and demonstrate how LLMs can\nsurface high-resolution dynamics otherwise invisible in traditional assembly\noutputs."}
{"id": "2509.13332", "pdf": "https://arxiv.org/pdf/2509.13332.pdf", "abs": "https://arxiv.org/abs/2509.13332", "title": "Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness", "authors": ["Pratik Jayarao", "Himanshu Gupta", "Neeraj Varshney", "Chaitanya Dwivedi"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) are increasingly adopted as automated judges\nin benchmarking and reward modeling, ensuring their reliability, efficiency,\nand robustness has become critical. In this work, we present a systematic\ncomparison of \"thinking\" and \"non-thinking\" LLMs in the LLM-as-a-judge paradigm\nusing open-source Qwen 3 models of relatively small sizes (0.6B, 1.7B, and 4B\nparameters). We evaluate both accuracy and computational efficiency (FLOPs) on\nRewardBench tasks, and further examine augmentation strategies for non-thinking\nmodels, including in-context learning, rubric-guided judging, reference-based\nevaluation, and n-best aggregation. Our results show that despite these\nenhancements, non-thinking models generally fall short of their thinking\ncounterparts. Our results show that thinking models achieve approximately 10%\npoints higher accuracy with little overhead (under 2x), in contrast to\naugmentation strategies like few-shot learning, which deliver modest gains at a\nhigher cost (>8x). Bias and robustness analyses further demonstrate that\nthinking models maintain significantly greater consistency under a variety of\nbias conditions such as positional, bandwagon, identity, diversity, and random\nbiases (6% higher on average). We further extend our experiments to the\nmultilingual setting and our results confirm that explicit reasoning extends\nits benefits beyond English. Overall, our work results in several important\nfindings that provide systematic evidence that explicit reasoning offers clear\nadvantages in the LLM-as-a-judge paradigm not only in accuracy and efficiency\nbut also in robustness."}
{"id": "2509.13345", "pdf": "https://arxiv.org/pdf/2509.13345.pdf", "abs": "https://arxiv.org/abs/2509.13345", "title": "Accuracy Paradox in Large Language Models: Regulating Hallucination Risks in Generative AI", "authors": ["Zihao Li", "Weiwei Yi", "Jiahong Chen"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": null, "summary": "As Large Language Models (LLMs) permeate everyday decision-making, their\nepistemic and societal risks demand urgent scrutiny. Hallucinations, the\ngeneration of fabricated, misleading, oversimplified or untrustworthy outputs,\nhas emerged as imperative challenges. While regulatory, academic, and technical\ndiscourse position accuracy as the principal benchmark for mitigating such\nharms, this article contends that overreliance on accuracy misdiagnoses the\nproblem and has counterproductive effect: the accuracy paradox. Drawing on\ninterdisciplinary literatures, this article develops a taxonomy of\nhallucination types and shows the paradox along three intertwining dimensions:\noutputs, individuals and society. First, accuracy functions as a superficial\nproxy for reliability, incentivising the optimisation of rhetorical fluency and\nsurface-level correctness over epistemic trustworthiness. This encourages\npassive user trust in outputs that appear accurate but epistemically untenable.\nSecond, accuracy as a singular metric fails to detect harms that are not\nfactually false but are nonetheless misleading, value-laden, or socially\ndistorting, including consensus illusions, sycophantic alignment, and subtle\nmanipulation. Third, regulatory overemphasis on accuracy obscures the wider\nsocietal consequences of hallucination, including social sorting, privacy\nviolations, equity harms, epistemic convergence that marginalises dissent,\nreduces pluralism, and causes social deskilling. By examining the EU AI Act,\nGDPR, and DSA, the article argues that current regulations are not yet\nstructurally equipped to address these epistemic, relational, and systemic\nharms and exacerbated by the overreliance on accuracy. By exposing such\nconceptual and practical challenges, this article calls for a fundamental shift\ntowards pluralistic, context-aware, and manipulation-resilient approaches to AI\ntrustworthy governance."}
{"id": "2509.13351", "pdf": "https://arxiv.org/pdf/2509.13351.pdf", "abs": "https://arxiv.org/abs/2509.13351", "title": "Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning", "authors": ["Pulkit Verma", "Ngoc La", "Anthony Favier", "Swaroop Mishra", "Julie A. Shah"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities across\ndiverse tasks, yet their ability to perform structured symbolic planning\nremains limited, particularly in domains requiring formal representations like\nthe Planning Domain Definition Language (PDDL). In this paper, we present a\nnovel instruction tuning framework, PDDL-Instruct, designed to enhance LLMs'\nsymbolic planning capabilities through logical chain-of-thought reasoning. Our\napproach focuses on teaching models to rigorously reason about action\napplicability, state transitions, and plan validity using explicit logical\ninference steps. By developing instruction prompts that guide models through\nthe precise logical reasoning required to determine when actions can be applied\nin a given state, we enable LLMs to self-correct their planning processes\nthrough structured reflection. The framework systematically builds verification\nskills by decomposing the planning process into explicit reasoning chains about\nprecondition satisfaction, effect application, and invariant preservation.\nExperimental results on multiple planning domains show that our\nchain-of-thought reasoning based instruction-tuned models are significantly\nbetter at planning, achieving planning accuracy of up to 94% on standard\nbenchmarks, representing a 66% absolute improvement over baseline models. This\nwork bridges the gap between the general reasoning capabilities of LLMs and the\nlogical precision required for automated planning, offering a promising\ndirection for developing better AI planning systems."}
{"id": "2509.13356", "pdf": "https://arxiv.org/pdf/2509.13356.pdf", "abs": "https://arxiv.org/abs/2509.13356", "title": "CogniAlign: Survivability-Grounded Multi-Agent Moral Reasoning for Safe and Transparent AI", "authors": ["Hasin Jawad Ali", "Ilhamul Azam", "Ajwad Abrar", "Md. Kamrul Hasan", "Hasan Mahmud"], "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "The challenge of aligning artificial intelligence (AI) with human values\npersists due to the abstract and often conflicting nature of moral principles\nand the opacity of existing approaches. This paper introduces CogniAlign, a\nmulti-agent deliberation framework based on naturalistic moral realism, that\ngrounds moral reasoning in survivability, defined across individual and\ncollective dimensions, and operationalizes it through structured deliberations\namong discipline-specific scientist agents. Each agent, representing\nneuroscience, psychology, sociology, and evolutionary biology, provides\narguments and rebuttals that are synthesized by an arbiter into transparent and\nempirically anchored judgments. We evaluate CogniAlign on classic and novel\nmoral questions and compare its outputs against GPT-4o using a five-part\nethical audit framework. Results show that CogniAlign consistently outperforms\nthe baseline across more than sixty moral questions, with average performance\ngains of 16.2 points in analytic quality, 14.3 points in breadth, and 28.4\npoints in depth of explanation. In the Heinz dilemma, for example, CogniAlign\nachieved an overall score of 89.2 compared to GPT-4o's 69.2, demonstrating a\ndecisive advantage in handling moral reasoning. By reducing black-box reasoning\nand avoiding deceptive alignment, CogniAlign highlights the potential of\ninterdisciplinary deliberation as a scalable pathway for safe and transparent\nAI alignment."}
{"id": "2509.13395", "pdf": "https://arxiv.org/pdf/2509.13395.pdf", "abs": "https://arxiv.org/abs/2509.13395", "title": "TICL: Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models", "authors": ["Haolong Zheng", "Yekaterina Yegorova", "Mark Hasegawa-Johnson"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": null, "summary": "Speech foundation models have recently demonstrated the ability to perform\nSpeech In-Context Learning (SICL). Selecting effective in-context examples is\ncrucial for SICL performance, yet selection methodologies remain underexplored.\nIn this work, we propose Text-Embedding KNN for SICL (TICL), a simple pipeline\nthat uses semantic context to enhance off-the-shelf large multimodal models'\nspeech recognition ability without fine-tuning. Across challenging automatic\nspeech recognition tasks, including accented English, multilingual speech, and\nchildren's speech, our method enables models to surpass zero-shot performance\nwith up to 84.7% relative WER reduction. We conduct ablation studies to show\nthe robustness and efficiency of our method."}
{"id": "2509.13450", "pdf": "https://arxiv.org/pdf/2509.13450.pdf", "abs": "https://arxiv.org/abs/2509.13450", "title": "SteeringControl: Holistic Evaluation of Alignment Steering in LLMs", "authors": ["Vincent Siu", "Nicholas Crispino", "David Park", "Nathan W. Henry", "Zhun Wang", "Yang Liu", "Dawn Song", "Chenguang Wang"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "We introduce SteeringControl, a benchmark for evaluating representation\nsteering methods across core alignment objectives--bias, harmful generation,\nand hallucination--and their effects on secondary behaviors such as sycophancy\nand commonsense morality. While prior alignment work often highlights\ntruthfulness or reasoning ability to demonstrate the side effects of\nrepresentation steering, we find there are many unexplored tradeoffs not yet\nunderstood in a systematic way. We collect a dataset of safety-relevant primary\nand secondary behaviors to evaluate steering effectiveness and behavioral\nentanglement centered around five popular steering methods. To enable this, we\ncraft a modular steering framework based on unique components that serve as the\nbuilding blocks of many existing methods. Our results on Qwen-2.5-7B and\nLlama-3.1-8B find that strong steering performance is dependent on the specific\ncombination of steering method, model, and targeted behavior, and that severe\nconcept entanglement can result from poor combinations of these three as well.\nWe release our code here:\nhttps://github.com/wang-research-lab/SteeringControl.git."}
{"id": "2509.13586", "pdf": "https://arxiv.org/pdf/2509.13586.pdf", "abs": "https://arxiv.org/abs/2509.13586", "title": "Annotating Satellite Images of Forests with Keywords from a Specialized Corpus in the Context of Change Detection", "authors": ["Nathalie Neptune", "Josiane Mothe"], "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.MM", "I.2; I.4; I.7; H.3"], "comment": null, "summary": "The Amazon rain forest is a vital ecosystem that plays a crucial role in\nregulating the Earth's climate and providing habitat for countless species.\nDeforestation in the Amazon is a major concern as it has a significant impact\non global carbon emissions and biodiversity. In this paper, we present a method\nfor detecting deforestation in the Amazon using image pairs from Earth\nobservation satellites. Our method leverages deep learning techniques to\ncompare the images of the same area at different dates and identify changes in\nthe forest cover. We also propose a visual semantic model that automatically\nannotates the detected changes with relevant keywords. The candidate annotation\nfor images are extracted from scientific documents related to the Amazon\nregion. We evaluate our approach on a dataset of Amazon image pairs and\ndemonstrate its effectiveness in detecting deforestation and generating\nrelevant annotations. Our method provides a useful tool for monitoring and\nstudying the impact of deforestation in the Amazon. While we focus on\nenvironment applications of our work by using images of deforestation in the\nAmazon rain forest to demonstrate the effectiveness of our proposed approach,\nit is generic enough to be applied to other domains."}
{"id": "2509.13615", "pdf": "https://arxiv.org/pdf/2509.13615.pdf", "abs": "https://arxiv.org/abs/2509.13615", "title": "See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles", "authors": ["Zongru Wu", "Rui Mao", "Zhiyuan Tian", "Pengzhou Cheng", "Tianjie Ju", "Zheng Wu", "Lingzhong Dong", "Haiyue Sheng", "Zhuosheng Zhang", "Gongshen Liu"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "The advent of multimodal agents facilitates effective interaction within\ngraphical user interface (GUI), especially in ubiquitous GUI control. However,\ntheir inability to reliably execute toggle control instructions remains a key\nbottleneck. To investigate this, we construct a state control benchmark with\nbinary toggle instructions from public datasets. Evaluations of existing agents\ndemonstrate their unreliability, particularly when the current toggle state\nalready matches the desired state. To address the challenge, we propose\nState-aware Reasoning (StaR), a training method that teaches agents to perceive\nthe current toggle state, analyze the desired state from the instruction, and\nact accordingly. Experiments on three multimodal agents demonstrate that StaR\ncan improve toggle instruction execution accuracy by over 30\\%. Further\nevaluations on three public benchmarks show that StaR also enhances general\ntask performance. Finally, evaluations on a dynamic environment highlight the\npotential of StaR for real-world applications. Code, benchmark, and\nStaR-enhanced agents are available at https://github.com/ZrW00/StaR."}
{"id": "2509.13625", "pdf": "https://arxiv.org/pdf/2509.13625.pdf", "abs": "https://arxiv.org/abs/2509.13625", "title": "Privacy-Aware In-Context Learning for Large Language Models", "authors": ["Bishnu Bhusal", "Manoj Acharya", "Ramneet Kaur", "Colin Samplawski", "Anirban Roy", "Adam D. Cobb", "Rohit Chadha", "Susmit Jha"], "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": null, "summary": "Large language models (LLMs) have significantly transformed natural language\nunderstanding and generation, but they raise privacy concerns due to potential\nexposure of sensitive information. Studies have highlighted the risk of\ninformation leakage, where adversaries can extract sensitive information\nembedded in the prompts. In this work, we introduce a novel private prediction\nframework for generating high-quality synthetic text with strong privacy\nguarantees. Our approach leverages the Differential Privacy (DP) framework to\nensure worst-case theoretical bounds on information leakage without requiring\nany fine-tuning of the underlying models.The proposed method performs inference\non private records and aggregates the resulting per-token output distributions.\nThis enables the generation of longer and coherent synthetic text while\nmaintaining privacy guarantees. Additionally, we propose a simple blending\noperation that combines private and public inference to further enhance\nutility. Empirical evaluations demonstrate that our approach outperforms\nprevious state-of-the-art methods on in-context-learning (ICL) tasks, making it\na promising direction for privacy-preserving text generation while maintaining\nhigh utility."}
{"id": "2509.13761", "pdf": "https://arxiv.org/pdf/2509.13761.pdf", "abs": "https://arxiv.org/abs/2509.13761", "title": "THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning", "authors": ["Qikai Chang", "Zhenrong Zhang", "Pengfei Hu", "Jiefeng Ma", "Yicheng Pan", "Jianshu Zhang", "Jun Du", "Quan Liu", "Jianqing Gao"], "categories": ["cs.AI", "cs.CL"], "comment": "22 pages, 13 figures", "summary": "Large Language Models (LLMs) have made remarkable progress in mathematical\nreasoning, but still continue to struggle with high-precision tasks like\nnumerical computation and formal symbolic manipulation. Integrating external\ntools has emerged as a promising approach to bridge this gap. Despite recent\nadvances, existing methods struggle with three key challenges: constructing\ntool-integrated reasoning data, performing fine-grained optimization, and\nenhancing inference. To overcome these limitations, we propose THOR\n(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,\na multi-agent actor-critic-based pipeline for constructing high-quality\ndatasets of tool-integrated reasoning paths, aligning with the policy and\ngeneralizing well across diverse models. Second, to perform fine-grained\nhierarchical optimization, we introduce an RL strategy that jointly optimizes\nfor both trajectory-level problem solving and step-level code generation. This\nis motivated by our key insight that the success of an intermediate tool call\nis a strong predictor of the final answer's correctness. Finally, THOR\nincorporates a self-correction mechanism that leverages immediate tool feedback\nto dynamically revise erroneous reasoning paths during inference. Our approach\ndemonstrates strong generalization across diverse models, performing\neffectively in both reasoning and non-reasoning models. It further achieves\nstate-of-the-art performance for models of a similar scale on multiple\nmathematical benchmarks, while also delivering consistent improvements on code\nbenchmarks. Our code will be publicly available at\nhttps://github.com/JingMog/THOR."}
{"id": "2509.13836", "pdf": "https://arxiv.org/pdf/2509.13836.pdf", "abs": "https://arxiv.org/abs/2509.13836", "title": "Diving into Mitigating Hallucinations from a Vision Perspective for Large Vision-Language Models", "authors": ["Weihang Wang", "Xinhao Li", "Ziyue Wang", "Yan Pang", "Jielei Zhang", "Peiyi Li", "Qiang Zhang", "Longwen Gao"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted by EMNLP2025 Finding", "summary": "Object hallucination in Large Vision-Language Models (LVLMs) significantly\nimpedes their real-world applicability. As the primary component for accurately\ninterpreting visual information, the choice of visual encoder is pivotal. We\nhypothesize that the diverse training paradigms employed by different visual\nencoders instill them with distinct inductive biases, which leads to their\ndiverse hallucination performances. Existing benchmarks typically focus on\ncoarse-grained hallucination detection and fail to capture the diverse\nhallucinations elaborated in our hypothesis. To systematically analyze these\neffects, we introduce VHBench-10, a comprehensive benchmark with approximately\n10,000 samples for evaluating LVLMs across ten fine-grained hallucination\ncategories. Our evaluations confirm encoders exhibit unique hallucination\ncharacteristics. Building on these insights and the suboptimality of simple\nfeature fusion, we propose VisionWeaver, a novel Context-Aware Routing Network.\nIt employs global visual features to generate routing signals, dynamically\naggregating visual features from multiple specialized experts. Comprehensive\nexperiments confirm the effectiveness of VisionWeaver in significantly reducing\nhallucinations and improving overall model performance."}
{"id": "2509.13853", "pdf": "https://arxiv.org/pdf/2509.13853.pdf", "abs": "https://arxiv.org/abs/2509.13853", "title": "Noise Supervised Contrastive Learning and Feature-Perturbed for Anomalous Sound Detection", "authors": ["Shun Huang", "Zhihua Fang", "Liang He"], "categories": ["cs.SD", "cs.CL"], "comment": "Accept ICASSP 2025", "summary": "Unsupervised anomalous sound detection aims to detect unknown anomalous\nsounds by training a model using only normal audio data. Despite advancements\nin self-supervised methods, the issue of frequent false alarms when handling\nsamples of the same type from different machines remains unresolved. This paper\nintroduces a novel training technique called one-stage supervised contrastive\nlearning (OS-SCL), which significantly addresses this problem by perturbing\nfeatures in the embedding space and employing a one-stage noisy supervised\ncontrastive learning approach. On the DCASE 2020 Challenge Task 2, it achieved\n94.64\\% AUC, 88.42\\% pAUC, and 89.24\\% mAUC using only Log-Mel features.\nAdditionally, a time-frequency feature named TFgram is proposed, which is\nextracted from raw audio. This feature effectively captures critical\ninformation for anomalous sound detection, ultimately achieving 95.71\\% AUC,\n90.23\\% pAUC, and 91.23\\% mAUC. The source code is available at:\n\\underline{www.github.com/huangswt/OS-SCL}."}
{"id": "2509.13941", "pdf": "https://arxiv.org/pdf/2509.13941.pdf", "abs": "https://arxiv.org/abs/2509.13941", "title": "An Empirical Study on Failures in Automated Issue Solving", "authors": ["Simiao Liu", "Fang Liu", "Liehao Li", "Xin Tan", "Yinghao Zhu", "Xiaoli Lian", "Li Zhang"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Automated issue solving seeks to autonomously identify and repair defective\ncode snippets across an entire codebase. SWE-Bench has emerged as the most\nwidely adopted benchmark for evaluating progress in this area. While LLM-based\nagentic tools show great promise, they still fail on a substantial portion of\ntasks. Moreover, current evaluations primarily report aggregate issue-solving\nrates, which obscure the underlying causes of success and failure, making it\nchallenging to diagnose model weaknesses or guide targeted improvements. To\nbridge this gap, we first analyze the performance and efficiency of three SOTA\ntools, spanning both pipeline-based and agentic architectures, in automated\nissue solving tasks of SWE-Bench-Verified under varying task characteristics.\nFurthermore, to move from high-level performance metrics to underlying cause\nanalysis, we conducted a systematic manual analysis of 150 failed instances.\nFrom this analysis, we developed a comprehensive taxonomy of failure modes\ncomprising 3 primary phases, 9 main categories, and 25 fine-grained\nsubcategories. Then we systematically analyze the distribution of the\nidentified failure modes, the results reveal distinct failure fingerprints\nbetween the two architectural paradigms, with the majority of agentic failures\nstemming from flawed reasoning and cognitive deadlocks. Motivated by these\ninsights, we propose a collaborative Expert-Executor framework. It introduces a\nsupervisory Expert agent tasked with providing strategic oversight and\ncourse-correction for a primary Executor agent. This architecture is designed\nto correct flawed reasoning and break the cognitive deadlocks that frequently\nlead to failure. Experiments show that our framework solves 22.2% of previously\nintractable issues for a leading single agent. These findings pave the way for\nbuilding more robust agents through diagnostic evaluation and collaborative\ndesign."}
{"id": "2509.13957", "pdf": "https://arxiv.org/pdf/2509.13957.pdf", "abs": "https://arxiv.org/abs/2509.13957", "title": "Enhancing Time Awareness in Generative Recommendation", "authors": ["Sunkyung Lee", "Seongmin Park", "Jonghyo Kim", "Mincheol Yoon", "Jongwuk Lee"], "categories": ["cs.IR", "cs.CL"], "comment": "EMNLP 2025 (Findings)", "summary": "Generative recommendation has emerged as a promising paradigm that formulates\nthe recommendations into a text-to-text generation task, harnessing the vast\nknowledge of large language models. However, existing studies focus on\nconsidering the sequential order of items and neglect to handle the temporal\ndynamics across items, which can imply evolving user preferences. To address\nthis limitation, we propose a novel model, Generative Recommender Using Time\nawareness (GRUT), effectively capturing hidden user preferences via various\ntemporal signals. We first introduce Time-aware Prompting, consisting of two\nkey contexts. The user-level temporal context models personalized temporal\npatterns across timestamps and time intervals, while the item-level transition\ncontext provides transition patterns across users. We also devise Trend-aware\nInference, a training-free method that enhances rankings by incorporating trend\ninformation about items with generation likelihood. Extensive experiments\ndemonstrate that GRUT outperforms state-of-the-art models, with gains of up to\n15.4% and 14.3% in Recall@5 and NDCG@5 across four benchmark datasets. The\nsource code is available at https://github.com/skleee/GRUT."}
{"id": "2509.13968", "pdf": "https://arxiv.org/pdf/2509.13968.pdf", "abs": "https://arxiv.org/abs/2509.13968", "title": "Exploring Major Transitions in the Evolution of Biological Cognition With Artificial Neural Networks", "authors": ["Konstantinos Voudouris", "Andrew Barron", "Marta Halina", "Colin Klein", "Matishalin Patel"], "categories": ["cs.AI", "cs.CL", "cs.FL", "cs.LG"], "comment": null, "summary": "Transitional accounts of evolution emphasise a few changes that shape what is\nevolvable, with dramatic consequences for derived lineages. More recently it\nhas been proposed that cognition might also have evolved via a series of major\ntransitions that manipulate the structure of biological neural networks,\nfundamentally changing the flow of information. We used idealised models of\ninformation flow, artificial neural networks (ANNs), to evaluate whether\nchanges in information flow in a network can yield a transitional change in\ncognitive performance. We compared networks with feed-forward, recurrent and\nlaminated topologies, and tested their performance learning artificial grammars\nthat differed in complexity, controlling for network size and resources. We\ndocumented a qualitative expansion in the types of input that recurrent\nnetworks can process compared to feed-forward networks, and a related\nqualitative increase in performance for learning the most complex grammars. We\nalso noted how the difficulty in training recurrent networks poses a form of\ntransition barrier and contingent irreversibility -- other key features of\nevolutionary transitions. Not all changes in network topology confer a\nperformance advantage in this task set. Laminated networks did not outperform\nnon-laminated networks in grammar learning. Overall, our findings show how some\nchanges in information flow can yield transitions in cognitive performance."}
{"id": "2509.14041", "pdf": "https://arxiv.org/pdf/2509.14041.pdf", "abs": "https://arxiv.org/abs/2509.14041", "title": "A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval Prediction For Instruction Caching", "authors": ["Henry Kao", "Nikhil Sreekumar", "Prabhdeep Singh Soni", "Ali Sedaghati", "Fang Su", "Bryan Chan", "Maziar Goudarzi", "Reza Azimi"], "categories": ["cs.AR", "cs.CL", "cs.OS", "cs.PF"], "comment": null, "summary": "Modern mobile CPU software pose challenges for conventional instruction cache\nreplacement policies due to their complex runtime behavior causing high reuse\ndistance between executions of the same instruction. Mobile code commonly\nsuffers from large amounts of stalls in the CPU frontend and thus starvation of\nthe rest of the CPU resources. Complexity of these applications and their code\nfootprint are projected to grow at a rate faster than available on-chip memory\ndue to power and area constraints, making conventional hardware-centric methods\nfor managing instruction caches to be inadequate. We present a novel\nsoftware-hardware co-design approach called TRRIP (Temperature-based\nRe-Reference Interval Prediction) that enables the compiler to analyze,\nclassify, and transform code based on \"temperature\" (hot/cold), and to provide\nthe hardware with a summary of code temperature information through a\nwell-defined OS interface based on using code page attributes. TRRIP's\nlightweight hardware extension employs code temperature attributes to optimize\nthe instruction cache replacement policy resulting in the eviction rate\nreduction of hot code. TRRIP is designed to be practical and adoptable in real\nmobile systems that have strict feature requirements on both the software and\nhardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%\nresulting in geomean speedup of 3.9%, on top of RRIP cache replacement running\nmobile code already optimized using PGO."}
{"id": "2509.14093", "pdf": "https://arxiv.org/pdf/2509.14093.pdf", "abs": "https://arxiv.org/abs/2509.14093", "title": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework", "authors": ["Kerui Huang", "Shuhan Liu", "Xing Hu", "Tongtong Xu", "Lingfeng Bao", "Xin Xia"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints."}
{"id": "2509.14132", "pdf": "https://arxiv.org/pdf/2509.14132.pdf", "abs": "https://arxiv.org/abs/2509.14132", "title": "When Avatars Have Personality: Effects on Engagement and Communication in Immersive Medical Training", "authors": ["Julia S. Dollis", "Iago A. Brito", "Fernanda B. Färber", "Pedro S. F. B. Ribeiro", "Rafael T. Sousa", "Arlindo R. Galvão Filho"], "categories": ["cs.HC", "cs.CL"], "comment": "8 pages, 2 figures", "summary": "While virtual reality (VR) excels at simulating physical environments, its\neffectiveness for training complex interpersonal skills is limited by a lack of\npsychologically plausible virtual humans. This is a critical gap in high-stakes\ndomains like medical education, where communication is a core competency. This\npaper introduces a framework that integrates large language models (LLMs) into\nimmersive VR to create medically coherent virtual patients with distinct,\nconsistent personalities, built on a modular architecture that decouples\npersonality from clinical data. We evaluated our system in a mixed-method,\nwithin-subjects study with licensed physicians who engaged in simulated\nconsultations. Results demonstrate that the approach is not only feasible but\nis also perceived by physicians as a highly rewarding and effective training\nenhancement. Furthermore, our analysis uncovers critical design principles,\nincluding a ``realism-verbosity paradox\" where less communicative agents can\nseem more artificial, and the need for challenges to be perceived as authentic\nto be instructive. This work provides a validated framework and key insights\nfor developing the next generation of socially intelligent VR training\nenvironments."}
{"id": "2509.14199", "pdf": "https://arxiv.org/pdf/2509.14199.pdf", "abs": "https://arxiv.org/abs/2509.14199", "title": "Dense Video Understanding with Gated Residual Tokenization", "authors": ["Haichao Zhang", "Wenhao Chai", "Shwai He", "Ang Li", "Yun Fu"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "High temporal resolution is essential for capturing fine-grained details in\nvideo understanding. However, current video large language models (VLLMs) and\nbenchmarks mostly rely on low-frame-rate sampling, such as uniform sampling or\nkeyframe selection, discarding dense temporal information. This compromise\navoids the high cost of tokenizing every frame, which otherwise leads to\nredundant computation and linear token growth as video length increases. While\nthis trade-off works for slowly changing content, it fails for tasks like\nlecture comprehension, where information appears in nearly every frame and\nrequires precise temporal alignment. To address this gap, we introduce Dense\nVideo Understanding (DVU), which enables high-FPS video comprehension by\nreducing both tokenization time and token overhead. Existing benchmarks are\nalso limited, as their QA pairs focus on coarse content changes. We therefore\npropose DIVE (Dense Information Video Evaluation), the first benchmark designed\nfor dense temporal reasoning. To make DVU practical, we present Gated Residual\nTokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-Gated\nTokenization uses pixel-level motion estimation to skip static regions during\ntokenization, achieving sub-linear growth in token count and compute. (2)\nSemantic-Scene Intra-Tokenization Merging fuses tokens across static regions\nwithin a scene, further reducing redundancy while preserving dynamic semantics.\nExperiments on DIVE show that GRT outperforms larger VLLM baselines and scales\npositively with FPS. These results highlight the importance of dense temporal\ninformation and demonstrate that GRT enables efficient, scalable high-FPS video\nunderstanding."}
{"id": "2509.14221", "pdf": "https://arxiv.org/pdf/2509.14221.pdf", "abs": "https://arxiv.org/abs/2509.14221", "title": "GEM-Bench: A Benchmark for Ad-Injected Response Generation within Generative Engine Marketing", "authors": ["Silan Hu", "Shiqi Zhang", "Yimin Shi", "Xiaokui Xiao"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Generative Engine Marketing (GEM) is an emerging ecosystem for monetizing\ngenerative engines, such as LLM-based chatbots, by seamlessly integrating\nrelevant advertisements into their responses. At the core of GEM lies the\ngeneration and evaluation of ad-injected responses. However, existing\nbenchmarks are not specifically designed for this purpose, which limits future\nresearch. To address this gap, we propose GEM-Bench, the first comprehensive\nbenchmark for ad-injected response generation in GEM. GEM-Bench includes three\ncurated datasets covering both chatbot and search scenarios, a metric ontology\nthat captures multiple dimensions of user satisfaction and engagement, and\nseveral baseline solutions implemented within an extensible multi-agent\nframework. Our preliminary results indicate that, while simple prompt-based\nmethods achieve reasonable engagement such as click-through rate, they often\nreduce user satisfaction. In contrast, approaches that insert ads based on\npre-generated ad-free responses help mitigate this issue but introduce\nadditional overhead. These findings highlight the need for future research on\ndesigning more effective and efficient solutions for generating ad-injected\nresponses in GEM."}
{"id": "2509.14223", "pdf": "https://arxiv.org/pdf/2509.14223.pdf", "abs": "https://arxiv.org/abs/2509.14223", "title": "Language models' activations linearly encode training-order recency", "authors": ["Dmitrii Krasheninnikov", "Richard E. Turner", "David Krueger"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We show that language models' activations linearly encode when information\nwas learned during training. Our setup involves creating a model with a known\ntraining order by sequentially fine-tuning Llama-3.2-1B on six disjoint but\notherwise similar datasets about named entities. We find that the average\nactivations of test samples for the six training datasets encode the training\norder: when projected into a 2D subspace, these centroids are arranged exactly\nin the order of training and lie on a straight line. Further, we show that\nlinear probes can accurately (~90%) distinguish \"early\" vs. \"late\" entities,\ngeneralizing to entities unseen during the probes' own training. The model can\nalso be fine-tuned to explicitly report an unseen entity's training stage (~80%\naccuracy). Interestingly, this temporal signal does not seem attributable to\nsimple differences in activation magnitudes, losses, or model confidence. Our\npaper demonstrates that models are capable of differentiating information by\nits acquisition time, and carries significant implications for how they might\nmanage conflicting data and respond to knowledge modifications."}
{"id": "2308.07107", "pdf": "https://arxiv.org/pdf/2308.07107.pdf", "abs": "https://arxiv.org/abs/2308.07107", "title": "Large Language Models for Information Retrieval: A Survey", "authors": ["Yutao Zhu", "Huaying Yuan", "Shuting Wang", "Jiongnan Liu", "Wenhan Liu", "Chenlong Deng", "Haonan Chen", "Zheng Liu", "Zhicheng Dou", "Ji-Rong Wen"], "categories": ["cs.CL", "cs.IR"], "comment": "Updated to version 4; Accepted by ACM TOIS", "summary": "As a primary means of information acquisition, information retrieval (IR)\nsystems, such as search engines, have integrated themselves into our daily\nlives. These systems also serve as components of dialogue, question-answering,\nand recommender systems. The trajectory of IR has evolved dynamically from its\norigins in term-based methods to its integration with advanced neural models.\nWhile the neural models excel at capturing complex contextual signals and\nsemantic nuances, thereby reshaping the IR landscape, they still face\nchallenges such as data scarcity, interpretability, and the generation of\ncontextually plausible yet potentially inaccurate responses. This evolution\nrequires a combination of both traditional methods (such as term-based sparse\nretrieval methods with rapid response) and modern neural architectures (such as\nlanguage models with powerful language understanding capacity). Meanwhile, the\nemergence of large language models (LLMs), typified by ChatGPT and GPT-4, has\nrevolutionized natural language processing due to their remarkable language\nunderstanding, generation, generalization, and reasoning abilities.\nConsequently, recent research has sought to leverage LLMs to improve IR\nsystems. Given the rapid evolution of this research trajectory, it is necessary\nto consolidate existing methodologies and provide nuanced insights through a\ncomprehensive overview. In this survey, we delve into the confluence of LLMs\nand IR systems, including crucial aspects such as query rewriters, retrievers,\nrerankers, and readers. Additionally, we explore promising directions, such as\nsearch agents, within this expanding field."}
{"id": "2405.13541", "pdf": "https://arxiv.org/pdf/2405.13541.pdf", "abs": "https://arxiv.org/abs/2405.13541", "title": "Annotation-Efficient Language Model Alignment via Diverse and Representative Response Texts", "authors": ["Yuu Jinnai", "Ukyo Honda"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "EMNLP Findings, 2025", "summary": "Preference optimization is a standard approach to fine-tuning large language\nmodels to align with human preferences. The quantity, diversity, and\nrepresentativeness of the preference dataset are critical to the effectiveness\nof preference optimization. However, obtaining a large amount of preference\nannotations is difficult in many applications. This raises the question of how\nto use the limited annotation budget to create an effective preference dataset.\nTo this end, we propose Annotation-Efficient Preference Optimization (AEPO).\nInstead of exhaustively annotating preference over all available response\ntexts, AEPO selects a subset of responses that maximizes diversity and\nrepresentativeness from the available responses and then annotates preference\nover the selected ones. In this way, AEPO focuses the annotation budget on\nlabeling preferences over a smaller but informative subset of responses. We\nevaluate the performance of preference learning using AEPO on three datasets\nand show that it outperforms the baselines with the same annotation budget. Our\ncode is available at https://github.com/CyberAgentAILab/annotation-efficient-po"}
{"id": "2406.16013", "pdf": "https://arxiv.org/pdf/2406.16013.pdf", "abs": "https://arxiv.org/abs/2406.16013", "title": "Database-Augmented Query Representation for Information Retrieval", "authors": ["Soyeong Jeong", "Jinheon Baek", "Sukmin Cho", "Sung Ju Hwang", "Jong C. Park"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "EMNLP 2025", "summary": "Information retrieval models that aim to search for documents relevant to a\nquery have shown multiple successes, which have been applied to diverse tasks.\nYet, the query from the user is oftentimes short, which challenges the\nretrievers to correctly fetch relevant documents. To tackle this, previous\nstudies have proposed expanding the query with a couple of additional\n(user-related) features related to it. However, they may be suboptimal to\neffectively augment the query, and there is plenty of other information\navailable to augment it in a relational database. Motivated by this fact, we\npresent a novel retrieval framework called Database-Augmented Query\nrepresentation (DAQu), which augments the original query with various\n(query-related) metadata across multiple tables. In addition, as the number of\nfeatures in the metadata can be very large and there is no order among them, we\nencode them with the graph-based set-encoding strategy, which considers\nhierarchies of features in the database without order. We validate our DAQu in\ndiverse retrieval scenarios, demonstrating that it significantly enhances\noverall retrieval performance over relevant baselines. Our code is available at\n\\href{https://github.com/starsuzi/DAQu}{this https URL}."}
{"id": "2407.11963", "pdf": "https://arxiv.org/pdf/2407.11963.pdf", "abs": "https://arxiv.org/abs/2407.11963", "title": "NeedleBench: Evaluating LLM Retrieval and Reasoning Across Varying Information Densities", "authors": ["Mo Li", "Songyang Zhang", "Taolin Zhang", "Haodong Duan", "Yunxin Liu", "Kai Chen"], "categories": ["cs.CL"], "comment": "v3: Revisions with added experiments, clarifications, and related\n  work updates", "summary": "The capability of large language models to handle long-context information is\ncrucial across various real-world applications. Existing evaluation methods\noften rely either on real-world long texts, making it difficult to exclude the\ninfluence of models' inherent knowledge, or introduce irrelevant filler content\nto artificially achieve target lengths, reducing assessment effectiveness. To\naddress these limitations, we introduce NeedleBench, a synthetic framework for\nassessing retrieval and reasoning performance in bilingual long-context tasks\nwith adaptive context lengths. NeedleBench systematically embeds key data\npoints at varying depths to rigorously test model capabilities. Tasks are\ncategorized into two scenarios: information-sparse, featuring minimal relevant\ndetails within extensive irrelevant text to simulate simple retrieval tasks;\nand information-dense (the Ancestral Trace Challenge), where relevant\ninformation is continuously distributed throughout the context to simulate\ncomplex reasoning tasks. Our experiments reveal that although recent reasoning\nmodels like Deepseek-R1 and OpenAI's o3 excel in mathematical reasoning, they\nstruggle with continuous retrieval and reasoning in information-dense\nscenarios, even at shorter context lengths. We also characterize a phenomenon\ntermed 'under-thinking', where models prematurely conclude reasoning despite\navailable information. NeedleBench thus provides critical insights and targeted\ntools essential for evaluating and improving LLMs' long-context capabilities.\nAll resources are available at OpenCompass:\nhttps://github.com/open-compass/opencompass."}
{"id": "2407.14701", "pdf": "https://arxiv.org/pdf/2407.14701.pdf", "abs": "https://arxiv.org/abs/2407.14701", "title": "Contextual modulation of language comprehension in a dynamic neural model of lexical meaning", "authors": ["Michael C. Stern", "Maria M. Piñango"], "categories": ["cs.CL"], "comment": null, "summary": "We computationally implement and experimentally test the behavioral\npredictions of a dynamic neural model of lexical meaning in the framework of\nDynamic Field Theory. We demonstrate the architecture and behavior of the model\nusing as a test case the English lexical item have, focusing on its polysemous\nuse. In the model, have maps to a semantic space defined by two independently\nmotivated continuous conceptual dimensions, connectedness and control\nasymmetry. The mapping is modeled as coupling between a neural node\nrepresenting the lexical item and neural fields representing the conceptual\ndimensions. While lexical knowledge is modeled as a stable coupling pattern,\nreal-time lexical meaning retrieval is modeled as the motion of neural\nactivation patterns between transiently stable states corresponding to semantic\ninterpretations or readings. Model simulations capture two previously reported\nempirical observations: (1) contextual modulation of lexical semantic\ninterpretation, and (2) individual variation in the magnitude of this\nmodulation. Simulations also generate a novel prediction that the by-trial\nrelationship between sentence reading time and acceptability should be\ncontextually modulated. An experiment combining self-paced reading and\nacceptability judgments replicates previous results and partially bears out the\nmodel's novel prediction. Altogether, results support a novel perspective on\nlexical polysemy: that the many related meanings of a word are not\ncategorically distinct representations; rather, they are transiently stable\nneural activation states that arise from the nonlinear dynamics of neural\npopulations governing interpretation on continuous semantic dimensions. Our\nmodel offers important advantages over related models in the dynamical systems\nframework, as well as models based on Bayesian inference."}
{"id": "2409.12147", "pdf": "https://arxiv.org/pdf/2409.12147.pdf", "abs": "https://arxiv.org/abs/2409.12147", "title": "MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning", "authors": ["Justin Chih-Yao Chen", "Archiki Prasad", "Swarnadeep Saha", "Elias Stengel-Eskin", "Mohit Bansal"], "categories": ["cs.CL"], "comment": "EMNLP 2025 (Camera-Ready)", "summary": "Large Language Models' (LLM) reasoning can be improved using test-time\naggregation strategies, i.e., generating multiple samples and voting among\ngenerated samples. While these improve performance, they often reach a\nsaturation point. Refinement offers an alternative by using LLM-generated\nfeedback to improve solution quality. However, refinement introduces 3 key\nchallenges: (1) Excessive refinement: Uniformly refining all instances can\nover-correct and reduce the overall performance. (2) Inability to localize and\naddress errors: LLMs have a limited ability to self-correct and struggle to\nidentify and correct their own mistakes. (3) Insufficient refinement: Deciding\nhow many iterations of refinement are needed is non-trivial, and stopping too\nsoon could leave errors unaddressed. To tackle these issues, we propose\nMAgICoRe, which avoids excessive refinement by categorizing problem difficulty\nas easy or hard, solving easy problems with coarse-grained aggregation and hard\nones with fine-grained and iterative multi-agent refinement. To improve error\nlocalization, we incorporate external step-wise reward model (RM) scores.\nMoreover, to ensure effective refinement, we employ a multi-agent loop with\nthree agents: Solver, Reviewer (which generates targeted feedback based on\nstep-wise RM scores), and the Refiner (which incorporates feedback). To ensure\nsufficient refinement, we re-evaluate updated solutions, iteratively initiating\nfurther rounds of refinement. We evaluate MAgICoRe on Llama-3-8B and GPT-3.5\nand show its effectiveness across 5 math datasets. Even one iteration of\nMAgICoRe beats Self-Consistency by 3.4%, Best-of-k by 3.2%, and Self-Refine by\n4.0% while using less than half the samples. Unlike iterative refinement with\nbaselines, MAgICoRe continues to improve with more iterations. Finally, our\nablations highlight the importance of MAgICoRe's RMs and multi-agent\ncommunication."}
{"id": "2410.09252", "pdf": "https://arxiv.org/pdf/2410.09252.pdf", "abs": "https://arxiv.org/abs/2410.09252", "title": "DAVIS: Planning Agent with Knowledge Graph-Powered Inner Monologue", "authors": ["Minh Pham Dinh", "Munira Syed", "Michael G Yankoski", "Trenton W. Ford"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "Designing a generalist scientific agent capable of performing tasks in\nlaboratory settings to assist researchers has become a key goal in recent\nArtificial Intelligence (AI) research. Unlike everyday tasks, scientific tasks\nare inherently more delicate and complex, requiring agents to possess a higher\nlevel of reasoning ability, structured and temporal understanding of their\nenvironment, and a strong emphasis on safety. Existing approaches often fail to\naddress these multifaceted requirements. To tackle these challenges, we present\nDAVIS. Unlike traditional retrieval-augmented generation (RAG) approaches,\nDAVIS incorporates structured and temporal memory, which enables model-based\nplanning. Additionally, DAVIS implements an agentic, multi-turn retrieval\nsystem, similar to a human's inner monologue, allowing for a greater degree of\nreasoning over past experiences. DAVIS demonstrates substantially improved\nperformance on the ScienceWorld benchmark comparing to previous approaches on 8\nout of 9 elementary science subjects. In addition, DAVIS's World Model\ndemonstrates competitive performance on the famous HotpotQA and MusiqueQA\ndataset for multi-hop question answering. To the best of our knowledge, DAVIS\nis the first RAG agent to employ an interactive retrieval method in a RAG\npipeline."}
{"id": "2410.10857", "pdf": "https://arxiv.org/pdf/2410.10857.pdf", "abs": "https://arxiv.org/abs/2410.10857", "title": "Mirror-Consistency: Harnessing Inconsistency in Majority Voting", "authors": ["Siyuan Huang", "Zhiyuan Ma", "Jintao Du", "Changhua Meng", "Weiqiang Wang", "Zhouhan Lin"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2024 Findings", "summary": "Self-Consistency, a widely-used decoding strategy, significantly boosts the\nreasoning capabilities of Large Language Models (LLMs). However, it depends on\nthe plurality voting rule, which focuses on the most frequent answer while\noverlooking all other minority responses. These inconsistent minority views\noften illuminate areas of uncertainty within the model's generation process. To\naddress this limitation, we present Mirror-Consistency, an enhancement of the\nstandard Self-Consistency approach. Our method incorporates a 'reflective\nmirror' into the self-ensemble decoding process and enables LLMs to critically\nexamine inconsistencies among multiple generations. Additionally, just as\nhumans use the mirror to better understand themselves, we propose using\nMirror-Consistency to enhance the sample-based confidence calibration methods,\nwhich helps to mitigate issues of overconfidence. Our experimental results\ndemonstrate that Mirror-Consistency yields superior performance in both\nreasoning accuracy and confidence calibration compared to Self-Consistency."}
{"id": "2410.13456", "pdf": "https://arxiv.org/pdf/2410.13456.pdf", "abs": "https://arxiv.org/abs/2410.13456", "title": "Unlocking Legal Knowledge: A Multilingual Dataset for Judicial Summarization in Switzerland", "authors": ["Luca Rolshoven", "Vishvaksenan Rasiah", "Srinanda Brügger Bose", "Sarah Hostettler", "Lara Burkhalter", "Matthias Stürmer", "Joel Niklaus"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2; I.7"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "Legal research is a time-consuming task that most lawyers face on a daily\nbasis. A large part of legal research entails looking up relevant caselaw and\nbringing it in relation to the case at hand. Lawyers heavily rely on summaries\n(also called headnotes) to find the right cases quickly. However, not all\ndecisions are annotated with headnotes and writing them is time-consuming.\nAutomated headnote creation has the potential to make hundreds of thousands of\ndecisions more accessible for legal research in Switzerland alone. To kickstart\nthis, we introduce the Swiss Leading Decision Summarization ( SLDS) dataset, a\nnovel cross-lingual resource featuring 18K court rulings from the Swiss Federal\nSupreme Court (SFSC), in German, French, and Italian, along with German\nheadnotes. We fine-tune and evaluate three mT5 variants, along with proprietary\nmodels. Our analysis highlights that while proprietary models perform well in\nzero-shot and one-shot settings, fine-tuned smaller models still provide a\nstrong competitive edge. We publicly release the dataset to facilitate further\nresearch in multilingual legal summarization and the development of assistive\ntechnologies for legal professionals"}
{"id": "2411.06207", "pdf": "https://arxiv.org/pdf/2411.06207.pdf", "abs": "https://arxiv.org/abs/2411.06207", "title": "KBM: Delineating Knowledge Boundary for Adaptive Retrieval in Large Language Models", "authors": ["Zhen Zhang", "Xinyu Wang", "Yong Jiang", "Zile Qiao", "Zhuo Chen", "Guangyu Li", "Feiteng Mu", "Mengting Hu", "Pengjun Xie", "Fei Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often struggle with dynamically changing\nknowledge and handling unknown static information. Retrieval-Augmented\nGeneration (RAG) is employed to tackle these challenges and has a significant\nimpact on improving LLM performance. In fact, we find that not all questions\nneed to trigger RAG. By retrieving parts of knowledge unknown to the LLM and\nallowing the LLM to answer the rest, we can effectively reduce both time and\ncomputational costs. In our work, we propose a Knowledge Boundary Model (KBM)\nto express the known/unknown of a given question, and to determine whether a\nRAG needs to be triggered. Experiments conducted on 11 English and Chinese\ndatasets illustrate that the KBM effectively delineates the knowledge boundary,\nsignificantly decreasing the proportion of retrievals required for optimal\nend-to-end performance. Furthermore, we evaluate the effectiveness of KBM in\nthree complex scenarios: dynamic knowledge, long-tail static knowledge, and\nmulti-hop problems, as well as its functionality as an external LLM plug-in."}
{"id": "2412.12478", "pdf": "https://arxiv.org/pdf/2412.12478.pdf", "abs": "https://arxiv.org/abs/2412.12478", "title": "Human-in-the-Loop Generation of Adversarial Texts: A Case Study on Tibetan Script", "authors": ["Xi Cao", "Yuan Sun", "Jiajun Li", "Quzong Gesang", "Nuo Qun", "Tashi Nyima"], "categories": ["cs.CL", "cs.CR", "cs.HC"], "comment": null, "summary": "DNN-based language models excel across various NLP tasks but remain highly\nvulnerable to textual adversarial attacks. While adversarial text generation is\ncrucial for NLP security, explainability, evaluation, and data augmentation,\nrelated work remains overwhelmingly English-centric, leaving the problem of\nconstructing high-quality and sustainable adversarial robustness benchmarks for\nlower-resourced languages both difficult and understudied. First, method\ncustomization for lower-resourced languages is complicated due to linguistic\ndifferences and limited resources. Second, automated attacks are prone to\ngenerating invalid or ambiguous adversarial texts. Last but not least, language\nmodels continuously evolve and may be immune to parts of previously generated\nadversarial texts. To address these challenges, we introduce HITL-GAT, an\ninteractive system based on a general approach to human-in-the-loop generation\nof adversarial texts. Additionally, we demonstrate the utility of HITL-GAT\nthrough a case study on Tibetan script, employing three customized adversarial\ntext generation methods and establishing its first adversarial robustness\nbenchmark, providing a valuable reference for other lower-resourced languages."}
{"id": "2501.01872", "pdf": "https://arxiv.org/pdf/2501.01872.pdf", "abs": "https://arxiv.org/abs/2501.01872", "title": "Turning Logic Against Itself : Probing Model Defenses Through Contrastive Questions", "authors": ["Rachneet Sachdeva", "Rima Hazra", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2025 (Main)", "summary": "Large language models, despite extensive alignment with human values and\nethical principles, remain vulnerable to sophisticated jailbreak attacks that\nexploit their reasoning abilities. Existing safety measures often detect overt\nmalicious intent but fail to address subtle, reasoning-driven vulnerabilities.\nIn this work, we introduce POATE (Polar Opposite query generation, Adversarial\nTemplate construction, and Elaboration), a novel jailbreak technique that\nharnesses contrastive reasoning to provoke unethical responses. POATE crafts\nsemantically opposing intents and integrates them with adversarial templates,\nsteering models toward harmful outputs with remarkable subtlety. We conduct\nextensive evaluation across six diverse language model families of varying\nparameter sizes to demonstrate the robustness of the attack, achieving\nsignificantly higher attack success rates (~44%) compared to existing methods.\nTo counter this, we propose Intent-Aware CoT and Reverse Thinking CoT, which\ndecompose queries to detect malicious intent and reason in reverse to evaluate\nand reject harmful responses. These methods enhance reasoning robustness and\nstrengthen the model's defense against adversarial exploits."}
{"id": "2501.09765", "pdf": "https://arxiv.org/pdf/2501.09765.pdf", "abs": "https://arxiv.org/abs/2501.09765", "title": "Enhancing the De-identification of Personally Identifiable Information in Educational Data", "authors": ["Zilyu Ji", "Yuntian Shen", "Jionghao Lin", "Kenneth R. Koedinger"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Protecting Personally Identifiable Information (PII), such as names, is a\ncritical requirement in learning technologies to safeguard student and teacher\nprivacy and maintain trust. Accurate PII detection is an essential step toward\nanonymizing sensitive information while preserving the utility of educational\ndata. Motivated by recent advancements in artificial intelligence, our study\ninvestigates the GPT-4o-mini model as a cost-effective and efficient solution\nfor PII detection tasks. We explore both prompting and fine-tuning approaches\nand compare GPT-4o-mini's performance against established frameworks, including\nMicrosoft Presidio and Azure AI Language. Our evaluation on two public\ndatasets, CRAPII and TSCC, demonstrates that the fine-tuned GPT-4o-mini model\nachieves superior performance, with a recall of 0.9589 on CRAPII. Additionally,\nfine-tuned GPT-4o-mini significantly improves precision scores (a threefold\nincrease) while reducing computational costs to nearly one-tenth of those\nassociated with Azure AI Language. Furthermore, our bias analysis reveals that\nthe fine-tuned GPT-4o-mini model consistently delivers accurate results across\ndiverse cultural backgrounds and genders. The generalizability analysis using\nthe TSCC dataset further highlights its robustness, achieving a recall of\n0.9895 with minimal additional training data from TSCC. These results emphasize\nthe potential of fine-tuned GPT-4o-mini as an accurate and cost-effective tool\nfor PII detection in educational data. It offers robust privacy protection\nwhile preserving the data's utility for research and pedagogical analysis. Our\ncode is available on GitHub: https://github.com/AnonJD/PrivacyAI"}
{"id": "2501.19301", "pdf": "https://arxiv.org/pdf/2501.19301.pdf", "abs": "https://arxiv.org/abs/2501.19301", "title": "Beyond checkmate: exploring the creative chokepoints in AI text", "authors": ["Nafis Irtiza Tripto", "Saranya Venkatraman", "Mahjabin Nahar", "Dongwon Lee"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at 30th Conference on Empirical Methods in Natural Language\n  Processing (EMNLP'25 Main conference). 9 pages", "summary": "The rapid advancement of Large Language Models (LLMs) has revolutionized text\ngeneration but also raised concerns about potential misuse, making detecting\nLLM-generated text (AI text) increasingly essential. While prior work has\nfocused on identifying AI text and effectively checkmating it, our study\ninvestigates a less-explored territory: portraying the nuanced distinctions\nbetween human and AI texts across text segments (introduction, body, and\nconclusion). Whether LLMs excel or falter in incorporating linguistic ingenuity\nacross text segments, the results will critically inform their viability and\nboundaries as effective creative assistants to humans. Through an analogy with\nthe structure of chess games, comprising opening, middle, and end games, we\nanalyze segment-specific patterns to reveal where the most striking differences\nlie. Although AI texts closely resemble human writing in the body segment due\nto its length, deeper analysis shows a higher divergence in features dependent\non the continuous flow of language, making it the most informative segment for\ndetection. Additionally, human texts exhibit greater stylistic variation across\nsegments, offering a new lens for distinguishing them from AI. Overall, our\nfindings provide fresh insights into human-AI text differences and pave the way\nfor more effective and interpretable detection strategies. Codes available at\nhttps://github.com/tripto03/chess_inspired_human_ai_text_distinction."}
{"id": "2502.07445", "pdf": "https://arxiv.org/pdf/2502.07445.pdf", "abs": "https://arxiv.org/abs/2502.07445", "title": "Forget What You Know about LLMs Evaluations -- LLMs are Like a Chameleon", "authors": ["Nurit Cohen-Inger", "Yehonatan Elisha", "Bracha Shapira", "Lior Rokach", "Seffi Cohen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) often appear to excel on public benchmarks, but\nthese high scores may mask an overreliance on dataset-specific surface cues\nrather than true language understanding. We introduce the Chameleon Benchmark\nOverfit Detector (C-BOD), a meta-evaluation framework that systematically\ndistorts benchmark prompts via a parametric transformation and detects\noverfitting of LLMs. By rephrasing inputs while preserving their semantic\ncontent and labels, C-BOD exposes whether a model's performance is driven by\nmemorized patterns. Evaluated on the MMLU benchmark using 26 leading LLMs, our\nmethod reveals an average performance degradation of 2.15% under modest\nperturbations, with 20 out of 26 models exhibiting statistically significant\ndifferences. Notably, models with higher baseline accuracy exhibit larger\nperformance differences under perturbation, and larger LLMs tend to be more\nsensitive to rephrasings, indicating that both cases may overrely on fixed\nprompt patterns. In contrast, the Llama family and models with lower baseline\naccuracy show insignificant degradation, suggesting reduced dependency on\nsuperficial cues. Moreover, C-BOD's dataset- and model-agnostic design allows\neasy integration into training pipelines to promote more robust language\nunderstanding. Our findings challenge the community to look beyond leaderboard\nscores and prioritize resilience and generalization in LLM evaluation."}
{"id": "2502.11176", "pdf": "https://arxiv.org/pdf/2502.11176.pdf", "abs": "https://arxiv.org/abs/2502.11176", "title": "LogiDynamics: Unraveling the Dynamics of Inductive, Abductive and Deductive Logical Inferences in LLM Reasoning", "authors": ["Tianshi Zheng", "Jiayang Cheng", "Chunyang Li", "Haochen Shi", "Zihao Wang", "Jiaxin Bai", "Yangqiu Song", "Ginny Y. Wong", "Simon See"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Main", "summary": "Modern large language models (LLMs) employ diverse logical inference\nmechanisms for reasoning, making the strategic optimization of these approaches\ncritical for advancing their capabilities. This paper systematically\ninvestigate the comparative dynamics of inductive (System 1) versus\nabductive/deductive (System 2) inference in LLMs. We utilize a controlled\nanalogical reasoning environment, varying modality (textual, visual, symbolic),\ndifficulty, and task format (MCQ / free-text). Our analysis reveals System 2\npipelines generally excel, particularly in visual/symbolic modalities and\nharder tasks, while System 1 is competitive for textual and easier problems.\nCrucially, task format significantly influences their relative advantage, with\nSystem 1 sometimes outperforming System 2 in free-text rule-execution. These\ncore findings generalize to broader in-context learning. Furthermore, we\ndemonstrate that advanced System 2 strategies like hypothesis selection and\niterative refinement can substantially scale LLM reasoning. This study offers\nfoundational insights and actionable guidelines for strategically deploying\nlogical inference to enhance LLM reasoning. Resources are available at\nhttps://github.com/HKUST-KnowComp/LogiDynamics."}
{"id": "2502.15022", "pdf": "https://arxiv.org/pdf/2502.15022.pdf", "abs": "https://arxiv.org/abs/2502.15022", "title": "Mind the Style Gap: Meta-Evaluation of Style and Attribute Transfer Metrics", "authors": ["Amalie Brogaard Pauli", "Isabelle Augenstein", "Ira Assent"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP Findings 2025", "summary": "Large language models (LLMs) make it easy to rewrite a text in any style --\ne.g. to make it more polite, persuasive, or more positive -- but evaluation\nthereof is not straightforward. A challenge lies in measuring content\npreservation: that content not attributable to style change is retained. This\npaper presents a large meta-evaluation of metrics for evaluating style and\nattribute transfer, focusing on content preservation. We find that\nmeta-evaluation studies on existing datasets lead to misleading conclusions\nabout the suitability of metrics for content preservation. Widely used metrics\nshow a high correlation with human judgments despite being deemed unsuitable\nfor the task -- because they do not abstract from style changes when evaluating\ncontent preservation. We show that the overly high correlations with human\njudgment stem from the nature of the test data. To address this issue, we\nintroduce a new, challenging test set specifically designed for evaluating\ncontent preservation metrics for style transfer. We construct the data by\ncreating high variation in the content preservation. Using this dataset, we\ndemonstrate that suitable metrics for content preservation for style transfer\nindeed are style-aware. To support efficient evaluation, we propose a new\nstyle-aware method that utilises small language models, obtaining a higher\nalignment with human judgements than prompting a model of a similar size as an\nautorater. ater."}
{"id": "2502.19749", "pdf": "https://arxiv.org/pdf/2502.19749.pdf", "abs": "https://arxiv.org/abs/2502.19749", "title": "What's Not Said Still Hurts: A Description-Based Evaluation Framework for Measuring Social Bias in LLMs", "authors": ["Jinhao Pan", "Chahat Raj", "Ziyu Yao", "Ziwei Zhu"], "categories": ["cs.CL"], "comment": "EMNLP Findings 2025", "summary": "Large Language Models (LLMs) often exhibit social biases inherited from their\ntraining data. While existing benchmarks evaluate bias by term-based mode\nthrough direct term associations between demographic terms and bias terms, LLMs\nhave become increasingly adept at avoiding biased responses, leading to\nseemingly low levels of bias. However, biases persist in subtler, contextually\nhidden forms that traditional benchmarks fail to capture. We introduce the\nDescription-based Bias Benchmark (DBB), a novel dataset designed to assess bias\nat the semantic level that bias concepts are hidden within naturalistic, subtly\nframed contexts in real-world scenarios rather than superficial terms. We\nanalyze six state-of-the-art LLMs, revealing that while models reduce bias in\nresponse at the term level, they continue to reinforce biases in nuanced\nsettings. Data, code, and results are available at\nhttps://github.com/JP-25/Description-based-Bias-Benchmark."}
{"id": "2503.21670", "pdf": "https://arxiv.org/pdf/2503.21670.pdf", "abs": "https://arxiv.org/abs/2503.21670", "title": "COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in Hindi-English Code-Mixing", "authors": ["Rajvee Sheth", "Himanshu Beniwal", "Mayank Singh"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce COMI-LINGUA, the largest manually annotated Hindi-English\ncode-mixed dataset, comprising 125K+ high-quality instances across five core\nNLP tasks: Matrix Language Identification, Token-level Language Identification,\nPart-Of-Speech Tagging, Named Entity Recognition, and Machine Translation. Each\ninstance is annotated by three bilingual annotators, yielding over 376K expert\nannotations with strong inter-annotator agreement (Fleiss' Kappa $\\geq$ 0.81).\nThe rigorously preprocessed and filtered dataset covers both Devanagari and\nRoman scripts and spans diverse domains, ensuring real-world linguistic\ncoverage. Evaluation reveals that closed-source LLMs significantly outperform\ntraditional tools and open-source models in zero-shot settings. Notably,\none-shot prompting consistently boosts performance across tasks, especially in\nstructure-sensitive predictions like POS and NER. Fine-tuning state-of-the-art\nLLMs on COMI-LINGUA demonstrates substantial improvements, achieving up to\n95.25 F1 in NER, 98.77 F1 in MLI, and competitive MT performance, setting new\nbenchmarks for Hinglish code-mixed text. COMI-LINGUA is publicly available at\nthis URL: https://huggingface.co/datasets/LingoIITGN/COMI-LINGUA."}
{"id": "2504.00132", "pdf": "https://arxiv.org/pdf/2504.00132.pdf", "abs": "https://arxiv.org/abs/2504.00132", "title": "Contextualize-then-Aggregate: Circuits for In-Context Learning in Gemma-2 2B", "authors": ["Aleksandra Bakalova", "Yana Veitsman", "Xinting Huang", "Michael Hahn"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In-Context Learning (ICL) is an intriguing ability of large language models\n(LLMs). Despite a substantial amount of work on its behavioral aspects and how\nit emerges in miniature setups, it remains unclear which mechanism assembles\ntask information from the individual examples in a fewshot prompt. We use\ncausal interventions to identify information flow in Gemma-2 2B for five\nnaturalistic ICL tasks. We find that the model infers task information using a\ntwo-step strategy we call contextualize-then-aggregate: In the lower layers,\nthe model builds up representations of individual fewshot examples, which are\ncontextualized by preceding examples through connections between fewshot input\nand output tokens across the sequence. In the higher layers, these\nrepresentations are aggregated to identify the task and prepare prediction of\nthe next output. The importance of the contextualization step differs between\ntasks, and it may become more important in the presence of ambiguous examples.\nOverall, by providing rigorous causal analysis, our results shed light on the\nmechanisms through which ICL happens in language models."}
{"id": "2504.05262", "pdf": "https://arxiv.org/pdf/2504.05262.pdf", "abs": "https://arxiv.org/abs/2504.05262", "title": "Do Large Language Models Truly Grasp Addition? A Rule-Focused Diagnostic Using Two-Integer Arithmetic", "authors": ["Yang Yan", "Yu Lu", "Renjun Xu", "Zhenzhong Lan"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP'25 Main", "summary": "Large language models (LLMs) achieve impressive results on advanced\nmathematics benchmarks but sometimes fail on basic arithmetic tasks, raising\nthe question of whether they have truly grasped fundamental arithmetic rules or\nare merely relying on pattern matching. To unravel this issue, we\nsystematically probe LLMs' understanding of two-integer addition ($0$ to\n$2^{64}$) by testing three crucial properties: commutativity ($A+B=B+A$),\nrepresentation invariance via symbolic remapping (e.g., $7 \\mapsto Y$), and\nconsistent accuracy scaling with operand length. Our evaluation of 12 leading\nLLMs reveals a stark disconnect: while models achieve high numeric accuracy\n(73.8-99.8%), they systematically fail these diagnostics. Specifically,\naccuracy plummets to $\\le 7.5$% with symbolic inputs, commutativity is violated\nin up to 20% of cases, and accuracy scaling is non-monotonic. Interventions\nfurther expose this pattern-matching reliance: explicitly providing rules\ndegrades performance by 29.49%, while prompting for explanations before\nanswering merely maintains baseline accuracy. These findings demonstrate that\ncurrent LLMs address elementary addition via pattern matching, not robust rule\ninduction, motivating new diagnostic benchmarks and innovations in model\narchitecture and training to cultivate genuine mathematical reasoning. Our\ndataset and generating code are available at\nhttps://github.com/kuri-leo/llm-arithmetic-diagnostic."}
{"id": "2504.20581", "pdf": "https://arxiv.org/pdf/2504.20581.pdf", "abs": "https://arxiv.org/abs/2504.20581", "title": "ClonEval: An Open Voice Cloning Benchmark", "authors": ["Iwona Christop", "Tomasz Kuczyński", "Marek Kubis"], "categories": ["cs.CL"], "comment": "Under review at ICASSP", "summary": "We present a novel benchmark for voice cloning text-to-speech models. The\nbenchmark consists of an evaluation protocol, an open-source library for\nassessing the performance of voice cloning models, and an accompanying\nleaderboard. The paper discusses design considerations and presents a detailed\ndescription of the evaluation procedure. The usage of the software library is\nexplained, along with the organization of results on the leaderboard."}
{"id": "2505.11051", "pdf": "https://arxiv.org/pdf/2505.11051.pdf", "abs": "https://arxiv.org/abs/2505.11051", "title": "CAMEO: Collection of Multilingual Emotional Speech Corpora", "authors": ["Iwona Christop", "Maciej Czajka"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Under review at ICASSP", "summary": "This paper presents CAMEO -- a curated collection of multilingual emotional\nspeech datasets designed to facilitate research in emotion recognition and\nother speech-related tasks. The main objectives were to ensure easy access to\nthe data, to allow reproducibility of the results, and to provide a\nstandardized benchmark for evaluating speech emotion recognition (SER) systems\nacross different emotional states and languages. The paper describes the\ndataset selection criteria, the curation and normalization process, and\nprovides performance results for several models. The collection, along with\nmetadata, and a leaderboard, is publicly available via the Hugging Face\nplatform."}
{"id": "2505.12381", "pdf": "https://arxiv.org/pdf/2505.12381.pdf", "abs": "https://arxiv.org/abs/2505.12381", "title": "From n-gram to Attention: How Model Architectures Learn and Propagate Bias in Language Modeling", "authors": ["Mohsinul Kabir", "Tasfia Tahsin", "Sophia Ananiadou"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at EMNLP 2025 (Findings)", "summary": "Current research on bias in language models (LMs) predominantly focuses on\ndata quality, with significantly less attention paid to model architecture and\ntemporal influences of data. Even more critically, few studies systematically\ninvestigate the origins of bias. We propose a methodology grounded in\ncomparative behavioral theory to interpret the complex interaction between\ntraining data and model architecture in bias propagation during language\nmodeling. Building on recent work that relates transformers to n-gram LMs, we\nevaluate how data, model design choices, and temporal dynamics affect bias\npropagation. Our findings reveal that: (1) n-gram LMs are highly sensitive to\ncontext window size in bias propagation, while transformers demonstrate\narchitectural robustness; (2) the temporal provenance of training data\nsignificantly affects bias; and (3) different model architectures respond\ndifferentially to controlled bias injection, with certain biases (e.g. sexual\norientation) being disproportionately amplified. As language models become\nubiquitous, our findings highlight the need for a holistic approach -- tracing\nbias to its origins across both data and model dimensions, not just symptoms,\nto mitigate harm."}
{"id": "2505.13259", "pdf": "https://arxiv.org/pdf/2505.13259.pdf", "abs": "https://arxiv.org/abs/2505.13259", "title": "From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery", "authors": ["Tianshi Zheng", "Zheye Deng", "Hong Ting Tsang", "Weiqi Wang", "Jiaxin Bai", "Zihao Wang", "Yangqiu Song"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Main", "summary": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific\ndiscovery, evolving from task-specific automation tools into increasingly\nautonomous agents and fundamentally redefining research processes and human-AI\ncollaboration. This survey systematically charts this burgeoning field, placing\na central focus on the changing roles and escalating capabilities of LLMs in\nscience. Through the lens of the scientific method, we introduce a foundational\nthree-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating\nautonomy and evolving responsibilities within the research lifecycle. We\nfurther identify pivotal challenges and future research trajectories such as\nrobotic automation, self-improvement, and ethical governance. Overall, this\nsurvey provides a conceptual architecture and strategic foresight to navigate\nand shape the future of AI-driven scientific discovery, fostering both rapid\ninnovation and responsible advancement. Github Repository:\nhttps://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery."}
{"id": "2505.16252", "pdf": "https://arxiv.org/pdf/2505.16252.pdf", "abs": "https://arxiv.org/abs/2505.16252", "title": "Does Localization Inform Unlearning? A Rigorous Examination of Local Parameter Attribution for Knowledge Unlearning in Language Models", "authors": ["Hwiyeong Lee", "Uiji Hwang", "Hyelim Lim", "Taeuk Kim"], "categories": ["cs.CL", "I.2.7"], "comment": "The 2025 Conference on Empirical Methods in Natural Language\n  Processing (EMNLP 2025)", "summary": "Large language models often retain unintended content, prompting growing\ninterest in knowledge unlearning. Recent approaches emphasize localized\nunlearning, restricting parameter updates to specific regions in an effort to\nremove target knowledge while preserving unrelated general knowledge. However,\ntheir effectiveness remains uncertain due to the lack of robust and thorough\nevaluation of the trade-off between the competing goals of unlearning. In this\npaper, we begin by revisiting existing localized unlearning approaches. We then\nconduct controlled experiments to rigorously evaluate whether local parameter\nupdates causally contribute to unlearning. Our findings reveal that the set of\nparameters that must be modified for effective unlearning is not strictly\ndetermined, challenging the core assumption of localized unlearning that\nparameter locality is inherently indicative of effective knowledge removal."}
{"id": "2505.18614", "pdf": "https://arxiv.org/pdf/2505.18614.pdf", "abs": "https://arxiv.org/abs/2505.18614", "title": "MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation", "authors": ["Woohyun Cho", "Youngmin Kim", "Sunghyun Lee", "Youngjae Yu"], "categories": ["cs.CL", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "comment": "Accepted to EMNLP 2025, Project Page:\n  https://k1064190.github.io/papers/paper1.html, our codes and datasets are\n  available at https://github.com/k1064190/MAVL", "summary": "Lyrics translation requires both accurate semantic transfer and preservation\nof musical rhythm, syllabic structure, and poetic style. In animated musicals,\nthe challenge intensifies due to alignment with visual and auditory cues. We\nintroduce Multilingual Audio-Video Lyrics Benchmark for Animated Song\nTranslation (MAVL), the first multilingual, multimodal benchmark for singable\nlyrics translation. By integrating text, audio, and video, MAVL enables richer\nand more expressive translations than text-only approaches. Building on this,\nwe propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought\nSylAVL-CoT, which leverages audio-video cues and enforces syllabic constraints\nto produce natural-sounding lyrics. Experimental results demonstrate that\nSylAVL-CoT significantly outperforms text-based models in singability and\ncontextual accuracy, emphasizing the value of multimodal, multilingual\napproaches for lyrics translation."}
{"id": "2505.18916", "pdf": "https://arxiv.org/pdf/2505.18916.pdf", "abs": "https://arxiv.org/abs/2505.18916", "title": "SCRum-9: Multilingual Stance Classification over Rumours on Social Media", "authors": ["Yue Li", "Jake Vasilakes", "Zhixue Zhao", "Carolina Scarton"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce SCRum-9, the largest multilingual Stance Classification dataset\nfor Rumour analysis in 9 languages, containing 7,516 tweets from X. SCRum-9\ngoes beyond existing stance classification datasets by covering more languages,\nlinking examples to more fact-checked claims (2.1k), and including\nconfidence-related annotations from multiple annotators to account for intra-\nand inter-annotator variability. Annotations were made by at least two native\nspeakers per language, totalling more than 405 hours of annotation and 8,150\ndollars in compensation. Further, SCRum-9 is used to benchmark five large\nlanguage models (LLMs) and two multilingual masked language models (MLMs) in\nIn-Context Learning (ICL) and fine-tuning setups. This paper also innovates by\nexploring the use of multilingual synthetic data for rumour stance\nclassification, showing that even LLMs with weak ICL performance can produce\nvaluable synthetic data for fine-tuning small MLMs, enabling them to achieve\nhigher performance than zero-shot ICL in LLMs. Finally, we examine the\nrelationship between model predictions and human uncertainty on ambiguous cases\nfinding that model predictions often match the second-choice labels assigned by\nannotators, rather than diverging entirely from human judgments. SCRum-9 is\npublicly released to the research community with potential to foster further\nresearch on multilingual analysis of misleading narratives on social media."}
{"id": "2505.23759", "pdf": "https://arxiv.org/pdf/2505.23759.pdf", "abs": "https://arxiv.org/abs/2505.23759", "title": "Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint", "authors": ["Heekyung Lee", "Jiaxin Ge", "Tsung-Han Wu", "Minwoo Kang", "Trevor Darrell", "David M. Chan"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "EMNLP 2025 Main Conference", "summary": "Rebus puzzles, visual riddles that encode language through imagery, spatial\narrangement, and symbolic substitution, pose a unique challenge to current\nvision-language models (VLMs). Unlike traditional image captioning or question\nanswering tasks, rebus solving requires multi-modal abstraction, symbolic\nreasoning, and a grasp of cultural, phonetic and linguistic puns. In this\npaper, we investigate the capacity of contemporary VLMs to interpret and solve\nrebus puzzles by constructing a hand-generated and annotated benchmark of\ndiverse English-language rebus puzzles, ranging from simple pictographic\nsubstitutions to spatially-dependent cues (\"head\" over \"heels\"). We analyze how\ndifferent VLMs perform, and our findings reveal that while VLMs exhibit some\nsurprising capabilities in decoding simple visual clues, they struggle\nsignificantly with tasks requiring abstract reasoning, lateral thinking, and\nunderstanding visual metaphors."}
{"id": "2505.23804", "pdf": "https://arxiv.org/pdf/2505.23804.pdf", "abs": "https://arxiv.org/abs/2505.23804", "title": "Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause Frequencies", "authors": ["Terrance Liu", "Shuyi Wang", "Daniel Preotiuc-Pietro", "Yash Chandarana", "Chirag Gupta"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "EMNLP 2025 main conference", "summary": "While large language models (LLMs) achieve strong performance on text-to-SQL\nparsing, they sometimes exhibit unexpected failures in which they are\nconfidently incorrect. Building trustworthy text-to-SQL systems thus requires\neliciting reliable uncertainty measures from the LLM. In this paper, we study\nthe problem of providing a calibrated confidence score that conveys the\nlikelihood of an output query being correct. Our work is the first to establish\na benchmark for post-hoc calibration of LLM-based text-to-SQL parsing. In\nparticular, we show that Platt scaling, a canonical method for calibration,\nprovides substantial improvements over directly using raw model output\nprobabilities as confidence scores. Furthermore, we propose a method for\ntext-to-SQL calibration that leverages the structured nature of SQL queries to\nprovide more granular signals of correctness, named \"sub-clause frequency\"\n(SCF) scores. Using multivariate Platt scaling (MPS), our extension of the\ncanonical Platt scaling technique, we combine individual SCF scores into an\noverall accurate and calibrated score. Empirical evaluation on two popular\ntext-to-SQL datasets shows that our approach of combining MPS and SCF yields\nfurther improvements in calibration and the related task of error detection\nover traditional Platt scaling."}
{"id": "2505.24621", "pdf": "https://arxiv.org/pdf/2505.24621.pdf", "abs": "https://arxiv.org/abs/2505.24621", "title": "Benchmarking Large Language Models for Cryptanalysis and Side-Channel Vulnerabilities", "authors": ["Utsav Maskey", "Chencheng Zhu", "Usman Naseem"], "categories": ["cs.CL"], "comment": "EMNLP'25 Findings", "summary": "Recent advancements in large language models (LLMs) have transformed natural\nlanguage understanding and generation, leading to extensive benchmarking across\ndiverse tasks. However, cryptanalysis - a critical area for data security and\nits connection to LLMs' generalization abilities - remains underexplored in LLM\nevaluations. To address this gap, we evaluate the cryptanalytic potential of\nstate-of-the-art LLMs on ciphertexts produced by a range of cryptographic\nalgorithms. We introduce a benchmark dataset of diverse plaintexts, spanning\nmultiple domains, lengths, writing styles, and topics, paired with their\nencrypted versions. Using zero-shot and few-shot settings along with\nchain-of-thought prompting, we assess LLMs' decryption success rate and discuss\ntheir comprehension abilities. Our findings reveal key insights into LLMs'\nstrengths and limitations in side-channel scenarios and raise concerns about\ntheir susceptibility to under-generalization-related attacks. This research\nhighlights the dual-use nature of LLMs in security contexts and contributes to\nthe ongoing discussion on AI safety and security."}
{"id": "2506.00658", "pdf": "https://arxiv.org/pdf/2506.00658.pdf", "abs": "https://arxiv.org/abs/2506.00658", "title": "Sarc7: Evaluating Sarcasm Detection and Generation with Seven Types and Emotion-Informed Techniques", "authors": ["Lang Xiong", "Raina Gao", "Alyssa Jeong", "Yicheng Fu", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP WiNLP and COLM Melt, Solar, PragLM, and Origen", "summary": "Sarcasm is a form of humor where expressions convey meanings opposite to\ntheir literal interpretations. Classifying and generating sarcasm using large\nlanguage models is vital for interpreting human communication. Sarcasm poses\nchallenges for computational models, due to its nuanced nature. We introduce\nSarc7, a benchmark that classifies 7 types of sarcasm: self-deprecating,\nbrooding, deadpan, polite, obnoxious, raging, and manic by annotating entries\nof the MUStARD dataset. Classification was evaluated using zero-shot, few-shot,\nchain-of-thought (CoT), and a novel emotion-based prompting technique. We\npropose an emotion-based generation method developed by identifying key\ncomponents of sarcasm-incongruity, shock value, and context dependency. Our\nclassification experiments show that Gemini 2.5, using emotion-based prompting,\noutperforms other setups with an F1 score of 0.3664. Human evaluators preferred\nour emotion-based prompting, with 38.46% more successful generations than\nzero-shot prompting."}
{"id": "2506.02478", "pdf": "https://arxiv.org/pdf/2506.02478.pdf", "abs": "https://arxiv.org/abs/2506.02478", "title": "FroM: Frobenius Norm-Based Data-Free Adaptive Model Merging", "authors": ["Zijian Li", "Xiaocheng Feng", "Huixin Liu", "Yichong Huang", "Ting Liu", "Bing Qin"], "categories": ["cs.CL"], "comment": "12 pages, 11 figures", "summary": "With the development of large language models, fine-tuning has emerged as an\neffective method to enhance performance in specific scenarios by injecting\ndomain-specific knowledge. In this context, model merging techniques provide a\nsolution for fusing knowledge from multiple fine-tuning models by combining\ntheir parameters. However, traditional methods often encounter task\ninterference when merging full fine-tuning models, and this problem becomes\neven more evident in parameter-efficient fine-tuning scenarios. In this paper,\nwe introduce an improvement to the RegMean method, which indirectly leverages\nthe training data to approximate the outputs of the linear layers before and\nafter merging. We propose an adaptive merging method called FroM, which\ndirectly measures the model parameters using the Frobenius norm, without any\ntraining data. By introducing an additional hyperparameter for control, FroM\noutperforms baseline methods across various fine-tuning scenarios, alleviating\nthe task interference problem."}
{"id": "2506.07032", "pdf": "https://arxiv.org/pdf/2506.07032.pdf", "abs": "https://arxiv.org/abs/2506.07032", "title": "A Culturally-diverse Multilingual Multimodal Video Benchmark & Model", "authors": ["Bhuiyan Sanjid Shafique", "Ashmal Vayani", "Muhammad Maaz", "Hanoona Abdul Rasheed", "Dinura Dissanayake", "Mohammed Irfan Kurpath", "Yahya Hmaiti", "Go Inoue", "Jean Lahoud", "Md. Safirur Rashid", "Shadid Intisar Quasem", "Maheen Fatima", "Franco Vidal", "Mykola Maslych", "Ketan Pravin More", "Sanoojan Baliah", "Hasindri Watawana", "Yuhao Li", "Fabian Farestam", "Leon Schaller", "Roman Tymtsiv", "Simon Weber", "Hisham Cholakkal", "Ivan Laptev", "Shin'ichi Satoh", "Michael Felsberg", "Mubarak Shah", "Salman Khan", "Fahad Shahbaz Khan"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Large multimodal models (LMMs) have recently gained attention due to their\neffectiveness to understand and generate descriptions of visual content. Most\nexisting LMMs are in English language. While few recent works explore\nmultilingual image LMMs, to the best of our knowledge, moving beyond the\nEnglish language for cultural and linguistic inclusivity is yet to be\ninvestigated in the context of video LMMs. In pursuit of more inclusive video\nLMMs, we introduce a multilingual Video LMM benchmark, named ViMUL-Bench, to\nevaluate Video LMMs across 14 languages, including both low- and high-resource\nlanguages: English, Chinese, Spanish, French, German, Hindi, Arabic, Russian,\nBengali, Urdu, Sinhala, Tamil, Swedish, and Japanese. Our ViMUL-Bench is\ndesigned to rigorously test video LMMs across 15 categories including eight\nculturally diverse categories, ranging from lifestyles and festivals to foods\nand rituals and from local landmarks to prominent cultural personalities.\nViMUL-Bench comprises both open-ended (short and long-form) and multiple-choice\nquestions spanning various video durations (short, medium, and long) with 8k\nsamples that are manually verified by native language speakers. In addition, we\nalso introduce a machine translated multilingual video training set comprising\n1.2 million samples and develop a simple multilingual video LMM, named ViMUL,\nthat is shown to provide a better tradeoff between high-and low-resource\nlanguages for video understanding. We hope our ViMUL-Bench and multilingual\nvideo LMM along with a large-scale multilingual video training set will help\nease future research in developing cultural and linguistic inclusive\nmultilingual video LMMs. Our proposed benchmark, video LMM and training data\nwill be publicly released at https://mbzuai-oryx.github.io/ViMUL/."}
{"id": "2506.10486", "pdf": "https://arxiv.org/pdf/2506.10486.pdf", "abs": "https://arxiv.org/abs/2506.10486", "title": "Table-Text Alignment: Explaining Claim Verification Against Tables in Scientific Papers", "authors": ["Xanh Ho", "Sunisth Kumar", "Yun-Ang Wu", "Florian Boudin", "Atsuhiro Takasu", "Akiko Aizawa"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Findings; 9 pages; code and data are available at\n  https://github.com/Alab-NII/SciTabAlign", "summary": "Scientific claim verification against tables typically requires predicting\nwhether a claim is supported or refuted given a table. However, we argue that\npredicting the final label alone is insufficient: it reveals little about the\nmodel's reasoning and offers limited interpretability. To address this, we\nreframe table-text alignment as an explanation task, requiring models to\nidentify the table cells essential for claim verification. We build a new\ndataset by extending the SciTab benchmark with human-annotated cell-level\nrationales. Annotators verify the claim label and highlight the minimal set of\ncells needed to support their decision. After the annotation process, we\nutilize the collected information and propose a taxonomy for handling ambiguous\ncases. Our experiments show that (i) incorporating table alignment information\nimproves claim verification performance, and (ii) most LLMs, while often\npredicting correct labels, fail to recover human-aligned rationales, suggesting\nthat their predictions do not stem from faithful reasoning."}
{"id": "2506.16123", "pdf": "https://arxiv.org/pdf/2506.16123.pdf", "abs": "https://arxiv.org/abs/2506.16123", "title": "FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning", "authors": ["Natapong Nitarach", "Warit Sirichotedumrong", "Panop Pitchayarthorn", "Pittawat Taveekitworachai", "Potsawee Manakul", "Kunat Pipatanakul"], "categories": ["cs.CL"], "comment": "Accepted at FinNLP-2025, EMNLP", "summary": "This paper presents FinCoT, a structured chain-of-thought (CoT) prompting\nframework that embeds domain-specific expert financial reasoning blueprints to\nguide large language models' behaviors. We identify three main prompting styles\nin financial NLP (FinNLP): (1) standard prompting (zero-shot), (2) unstructured\nCoT (free-form reasoning), and (3) structured CoT (with explicitly structured\nreasoning steps). Prior work has mainly focused on the first two, while\nstructured CoT remains underexplored and lacks domain expertise incorporation.\nTherefore, we evaluate all three prompting approaches across ten CFA-style\nfinancial domains and introduce FinCoT as the first structured finance-specific\nprompting approach incorporating blueprints from domain experts. FinCoT\nimproves the accuracy of a general-purpose model, Qwen3-8B-Base, from 63.2% to\n80.5%, and boosts Fin-R1 (7B), a finance-specific model, from 65.7% to 75.7%,\nwhile reducing output length by up to 8.9x and 1.16x compared to structured CoT\nmethods, respectively. We find that FinCoT proves most effective for models\nlacking financial post-training. Our findings show that FinCoT does not only\nimprove performance and reduce inference costs but also yields more\ninterpretable and expert-aligned reasoning traces."}
{"id": "2508.02013", "pdf": "https://arxiv.org/pdf/2508.02013.pdf", "abs": "https://arxiv.org/abs/2508.02013", "title": "SpeechRole: A Large-Scale Dataset and Benchmark for Evaluating Speech Role-Playing Agents", "authors": ["Changhao Jiang", "Jiajun Sun", "Yifei Cao", "Jiabao Zhuang", "Hui Li", "Xiaoran Fan", "Ming Zhang", "Junjie Ye", "Shihan Dou", "Zhiheng Xi", "Jingqi Tong", "Yilong Wu", "Baoyu Fan", "Zhen Wang", "Tao Liang", "Zhihui Fei", "Mingyang Wan", "Guojun Ma", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, role-playing agents have emerged as a promising paradigm for\nachieving personalized interaction and emotional resonance. Existing research\nprimarily focuses on the textual modality, neglecting the critical dimension of\nspeech in realistic interactive scenarios. In particular, there is a lack of\nsystematic evaluation for Speech Role-Playing Agents (SRPAs). To address this\ngap, we construct SpeechRole-Data, a large-scale, high-quality dataset that\ncomprises 98 diverse roles and 112k speech-based single-turn and multi-turn\nconversations. Each role demonstrates distinct vocal characteristics, including\ntimbre and prosody, thereby enabling more sophisticated speech role-playing.\nFurthermore, we propose SpeechRole-Eval, a multidimensional evaluation\nbenchmark that systematically assesses SRPAs performance in key aspects such as\nfundamental interaction ability, speech expressiveness, and role-playing\nfidelity. Experimental results reveal the advantages and challenges of both\ncascaded and end-to-end speech role-playing agents in maintaining vocal style\nconsistency and role coherence. We release all data, code, and baseline models\nto provide a solid foundation for speech-driven multimodal role-playing\nresearch and to foster further developments in this field."}
{"id": "2508.02618", "pdf": "https://arxiv.org/pdf/2508.02618.pdf", "abs": "https://arxiv.org/abs/2508.02618", "title": "Mitigating Attention Hacking in Preference-Based Reward Modeling via Interaction Distillation", "authors": ["Jianxiang Zang", "Meiling Ning", "Shihan Dou", "Jiazheng Zhang", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL"], "comment": "This paper is not suitable for this topic, we need to adjust the\n  context", "summary": "The reward model (RM), as the core component of reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs), responsible for\nproviding reward signals to generated responses. However, mainstream preference\nmodeling in RM is inadequate in terms of token-level interaction, making its\njudgment signals vulnerable to being hacked by misallocated attention to\ncontext. This stems from two fundamental limitations: (1) Current preference\nmodeling employs decoder-only architectures, where the unidirectional causal\nattention mechanism leads to forward-decaying intra-sequence attention within\nthe prompt-response sequence. (2) The independent Siamese-encoding paradigm\ninduces the absence of token-level inter-sequence attention between chosen and\nrejected sequences. To address this \"attention hacking\", we propose\n\"Interaction Distillation\", a novel training framework for more adequate\npreference modeling through attention-level optimization. The method introduces\nan interaction-based natural language understanding model as the teacher to\nprovide sophisticated token interaction patterns via comprehensive attention,\nand guides the preference modeling to simulate teacher model's interaction\npattern through an attentional alignment objective. Through extensive\nexperiments, interaction distillation has demonstrated its ability to provide\nmore stable and generalizable reward signals compared to state-of-the-art RM\noptimization methods that target data noise, highlighting the attention hacking\nconstitute a more fundamental limitation in RM."}
{"id": "2508.15214", "pdf": "https://arxiv.org/pdf/2508.15214.pdf", "abs": "https://arxiv.org/abs/2508.15214", "title": "Self-Guided Function Calling in Large Language Models via Stepwise Experience Recall", "authors": ["Sijia Cui", "Aiyao He", "Shuai Xu", "Hongming Zhang", "Yanna Wang", "Qingyang Zhang", "Yajing Wang", "Bo Xu"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025", "summary": "Function calling enables large language models (LLMs) to interact with\nexternal systems by leveraging tools and APIs. When faced with multi-step tool\nusage, LLMs still struggle with tool selection, parameter generation, and\ntool-chain planning. Existing methods typically rely on manually designing\ntask-specific demonstrations, or retrieving from a curated library. These\napproaches demand substantial expert effort and prompt engineering becomes\nincreasingly complex and inefficient as tool diversity and task difficulty\nscale. To address these challenges, we propose a self-guided method, Stepwise\nExperience Recall (SEER), which performs fine-grained, stepwise retrieval from\na continually updated experience pool. Instead of relying on static or manually\ncurated library, SEER incrementally augments the experience pool with past\nsuccessful trajectories, enabling continuous expansion of the pool and improved\nmodel performance over time. Evaluated on the ToolQA benchmark, SEER achieves\nan average improvement of 6.1% on easy and 4.7% on hard questions. We further\ntest SEER on $\\tau$-bench, which includes two real-world domains. Powered by\nQwen2.5-7B and Qwen2.5-72B models, SEER demonstrates substantial accuracy gains\nof 7.44% and 23.38%, respectively."}
{"id": "2508.15709", "pdf": "https://arxiv.org/pdf/2508.15709.pdf", "abs": "https://arxiv.org/abs/2508.15709", "title": "Position Bias Mitigates Position Bias:Mitigate Position Bias Through Inter-Position Knowledge Distillation", "authors": ["Yifei Wang", "Feng Xiong", "Yong Wang", "Linjing Li", "Xiangxiang Chu", "Daniel Dajun Zeng"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Oral", "summary": "Positional bias (PB), manifesting as non-uniform sensitivity across different\ncontextual locations, significantly impairs long-context comprehension and\nprocessing capabilities. Previous studies have addressed PB either by modifying\nthe underlying architectures or by employing extensive contextual awareness\ntraining. However, the former approach fails to effectively eliminate the\nsubstantial performance disparities, while the latter imposes significant data\nand computational overhead. To address PB effectively, we introduce\n\\textbf{Pos2Distill}, a position to position knowledge distillation framework.\nPos2Distill transfers the superior capabilities from advantageous positions to\nless favorable ones, thereby reducing the huge performance gaps. The conceptual\nprinciple is to leverage the inherent, position-induced disparity to counteract\nthe PB itself. We identify distinct manifestations of PB under\n\\textbf{\\textsc{r}}etrieval and \\textbf{\\textsc{r}}easoning paradigms, thereby\ndesigning two specialized instantiations:\n\\emph{Pos2Distill-R\\textsuperscript{1}} and\n\\emph{Pos2Distill-R\\textsuperscript{2}} respectively, both grounded in this\ncore principle. By employing the Pos2Distill approach, we achieve enhanced\nuniformity and significant performance gains across all contextual positions in\nlong-context retrieval and reasoning tasks. Crucially, both specialized systems\nexhibit strong cross-task generalization mutually, while achieving superior\nperformance on their respective tasks."}
{"id": "2508.18655", "pdf": "https://arxiv.org/pdf/2508.18655.pdf", "abs": "https://arxiv.org/abs/2508.18655", "title": "Empathy Omni: Enabling Empathetic Speech Response Generation through Large Language Models", "authors": ["Haoyu Wang", "Guangyan Zhang", "Jiale Chen", "Jingyu Li", "Yuehai Wang", "Yiwen Guo"], "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7"], "comment": "5 pages, 1 figure, submitted to ICASSP 2026", "summary": "With the development of speech large language models (speech LLMs), users can\nnow interact directly with assistants via speech. However, most existing models\nonly convert response content into speech without fully capturing the rich\nemotional cues in user queries, where the same sentence may convey different\nmeanings depending on the expression. Emotional understanding is thus essential\nfor improving human-machine interaction. Most empathetic speech LLMs rely on\nmassive datasets, demanding high computational cost. A key challenge is to\nbuild models that generate empathetic responses with limited data and without\nlarge-scale training. To this end, we propose Emotion Omni, a model that\nunderstands emotional content in user speech and generates empathetic\nresponses. We further developed a data pipeline to construct a 200k emotional\ndialogue dataset supporting empathetic speech assistants. Experiments show that\nEmotion Omni achieves comparable instruction-following ability without\nlarge-scale pretraining, while surpassing existing models in speech quality\n(UTMOS:4.41) and empathy (Emotion GPT Score: 3.97). These results confirm its\nimprovements in both speech fidelity and emotional expressiveness. Demos are\navailable at https://w311411.github.io/omni_demo/."}
{"id": "2508.19546", "pdf": "https://arxiv.org/pdf/2508.19546.pdf", "abs": "https://arxiv.org/abs/2508.19546", "title": "Language Models Identify Ambiguities and Exploit Loopholes", "authors": ["Jio Choi", "Mohit Bansal", "Elias Stengel-Eskin"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 camera-ready; Code:\n  https://github.com/esteng/ambiguous-loophole-exploitation", "summary": "Studying the responses of large language models (LLMs) to loopholes presents\na two-fold opportunity. First, it affords us a lens through which to examine\nambiguity and pragmatics in LLMs, since exploiting a loophole requires\nidentifying ambiguity and performing sophisticated pragmatic reasoning. Second,\nloopholes pose an interesting and novel alignment problem where the model is\npresented with conflicting goals and can exploit ambiguities to its own\nadvantage. To address these questions, we design scenarios where LLMs are given\na goal and an ambiguous user instruction in conflict with the goal, with\nscenarios covering scalar implicature, structural ambiguities, and power\ndynamics. We then measure different models' abilities to exploit loopholes to\nsatisfy their given goals as opposed to the goals of the user. We find that\nboth closed-source and stronger open-source models can identify ambiguities and\nexploit their resulting loopholes, presenting a potential AI safety risk. Our\nanalysis indicates that models which exploit loopholes explicitly identify and\nreason about both ambiguity and conflicting goals."}
{"id": "2508.19813", "pdf": "https://arxiv.org/pdf/2508.19813.pdf", "abs": "https://arxiv.org/abs/2508.19813", "title": "T2R-bench: A Benchmark for Generating Article-Level Reports from Real World Industrial Tables", "authors": ["Jie Zhang", "Changzai Pan", "Kaiwen Wei", "Sishi Xiong", "Yu Zhao", "Xiangyu Li", "Jiaxin Peng", "Xiaoyan Gu", "Jian Yang", "Wenhan Chang", "Zhenhe Wu", "Jiang Zhong", "Shuangyong Song", "Yongxiang Li", "Xuelong Li"], "categories": ["cs.CL"], "comment": null, "summary": "Extensive research has been conducted to explore the capabilities of large\nlanguage models (LLMs) in table reasoning. However, the essential task of\ntransforming tables information into reports remains a significant challenge\nfor industrial applications. This task is plagued by two critical issues: 1)\nthe complexity and diversity of tables lead to suboptimal reasoning outcomes;\nand 2) existing table benchmarks lack the capacity to adequately assess the\npractical application of this task. To fill this gap, we propose the\ntable-to-report task and construct a bilingual benchmark named T2R-bench, where\nthe key information flow from the tables to the reports for this task. The\nbenchmark comprises 457 industrial tables, all derived from real-world\nscenarios and encompassing 19 industry domains as well as 4 types of industrial\ntables. Furthermore, we propose an evaluation criteria to fairly measure the\nquality of report generation. The experiments on 25 widely-used LLMs reveal\nthat even state-of-the-art models like Deepseek-R1 only achieves performance\nwith 62.71 overall score, indicating that LLMs still have room for improvement\non T2R-bench."}
{"id": "2508.21137", "pdf": "https://arxiv.org/pdf/2508.21137.pdf", "abs": "https://arxiv.org/abs/2508.21137", "title": "How Does Cognitive Bias Affect Large Language Models? A Case Study on the Anchoring Effect in Price Negotiation Simulations", "authors": ["Yoshiki Takenami", "Yin Jou Huang", "Yugo Murawaki", "Chenhui Chu"], "categories": ["cs.CL"], "comment": "18 pages, 2 figures. Accepted to EMNLP 2025 findings", "summary": "Cognitive biases, well-studied in humans, can also be observed in LLMs,\naffecting their reliability in real-world applications. This paper investigates\nthe anchoring effect in LLM-driven price negotiations. To this end, we\ninstructed seller LLM agents to apply the anchoring effect and evaluated\nnegotiations using not only an objective metric but also a subjective metric.\nExperimental results show that LLMs are influenced by the anchoring effect like\nhumans. Additionally, we investigated the relationship between the anchoring\neffect and factors such as reasoning and personality. It was shown that\nreasoning models are less prone to the anchoring effect, suggesting that the\nlong chain of thought mitigates the effect. However, we found no significant\ncorrelation between personality traits and susceptibility to the anchoring\neffect. These findings contribute to a deeper understanding of cognitive biases\nin LLMs and to the realization of safe and responsible application of LLMs in\nsociety."}
{"id": "2509.01081", "pdf": "https://arxiv.org/pdf/2509.01081.pdf", "abs": "https://arxiv.org/abs/2509.01081", "title": "Assessing Large Language Models on Islamic Legal Reasoning: Evidence from Inheritance Law Evaluation", "authors": ["Abdessalam Bouchekif", "Samer Rashwani", "Heba Sbahi", "Shahd Gaben", "Mutaz Al-Khatib", "Mohammed Ghaly"], "categories": ["cs.CL", "cs.AI", "I.2.6; I.2.7"], "comment": "10 pages, 7 Tables, Code:\n  https://github.com/bouchekif/inheritance_evaluation", "summary": "This paper evaluates the knowledge and reasoning capabilities of Large\nLanguage Models in Islamic inheritance law, known as 'ilm al-mawarith. We\nassess the performance of seven LLMs using a benchmark of 1,000 multiple-choice\nquestions covering diverse inheritance scenarios, designed to test models'\nability to understand the inheritance context and compute the distribution of\nshares prescribed by Islamic jurisprudence. The results reveal a significant\nperformance gap: o3 and Gemini 2.5 achieved accuracies above 90%, whereas\nALLaM, Fanar, LLaMA, and Mistral scored below 50%. These disparities reflect\nimportant differences in reasoning ability and domain adaptation. We conduct a\ndetailed error analysis to identify recurring failure patterns across models,\nincluding misunderstandings of inheritance scenarios, incorrect application of\nlegal rules, and insufficient domain knowledge. Our findings highlight\nlimitations in handling structured legal reasoning and suggest directions for\nimproving performance in Islamic legal reasoning. Code:\nhttps://github.com/bouchekif/inheritance_evaluation"}
{"id": "2509.04476", "pdf": "https://arxiv.org/pdf/2509.04476.pdf", "abs": "https://arxiv.org/abs/2509.04476", "title": "Training Text-to-Molecule Models with Context-Aware Tokenization", "authors": ["Seojin Kim", "Hyeontae Song", "Jaehyun Nam", "Jinwoo Shin"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 Findings", "summary": "Recently, text-to-molecule models have shown great potential across various\nchemical applications, e.g., drug-discovery. These models adapt language models\nto molecular data by representing molecules as sequences of atoms. However,\nthey rely on atom-level tokenizations, which primarily focus on modeling local\nconnectivity, thereby limiting the ability of models to capture the global\nstructural context within molecules. To tackle this issue, we propose a novel\ntext-to-molecule model, coined Context-Aware Molecular T5 (CAMT5). Inspired by\nthe significance of the substructure-level contexts in understanding molecule\nstructures, e.g., ring systems, we introduce substructure-level tokenization\nfor text-to-molecule models. Building on our tokenization scheme, we develop an\nimportance-based training strategy that prioritizes key substructures, enabling\nCAMT5 to better capture the molecular semantics. Extensive experiments verify\nthe superiority of CAMT5 in various text-to-molecule generation tasks.\nIntriguingly, we find that CAMT5 outperforms the state-of-the-art methods using\nonly 2% of training tokens. In addition, we propose a simple yet effective\nensemble strategy that aggregates the outputs of text-to-molecule models to\nfurther boost the generation performance. Code is available at\nhttps://github.com/Songhyeontae/CAMT5.git."}
{"id": "2509.06652", "pdf": "https://arxiv.org/pdf/2509.06652.pdf", "abs": "https://arxiv.org/abs/2509.06652", "title": "IntrEx: A Dataset for Modeling Engagement in Educational Conversations", "authors": ["Xingwei Tan", "Mahathi Parvatham", "Chiara Gambi", "Gabriele Pergola"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Findings camera-ready, 9+7 pages", "summary": "Engagement and motivation are crucial for second-language acquisition, yet\nmaintaining learner interest in educational conversations remains a challenge.\nWhile prior research has explored what makes educational texts interesting,\nstill little is known about the linguistic features that drive engagement in\nconversations. To address this gap, we introduce IntrEx, the first large\ndataset annotated for interestingness and expected interestingness in\nteacher-student interactions. Built upon the Teacher-Student Chatroom Corpus\n(TSCC), IntrEx extends prior work by incorporating sequence-level annotations,\nallowing for the study of engagement beyond isolated turns to capture how\ninterest evolves over extended dialogues. We employ a rigorous annotation\nprocess with over 100 second-language learners, using a comparison-based rating\napproach inspired by reinforcement learning from human feedback (RLHF) to\nimprove agreement. We investigate whether large language models (LLMs) can\npredict human interestingness judgments. We find that LLMs (7B/8B parameters)\nfine-tuned on interestingness ratings outperform larger proprietary models like\nGPT-4o, demonstrating the potential for specialised datasets to model\nengagement in educational settings. Finally, we analyze how linguistic and\ncognitive factors, such as concreteness, comprehensibility (readability), and\nuptake, influence engagement in educational dialogues."}
{"id": "2509.07553", "pdf": "https://arxiv.org/pdf/2509.07553.pdf", "abs": "https://arxiv.org/abs/2509.07553", "title": "VeriOS: Query-Driven Proactive Human-Agent-GUI Interaction for Trustworthy OS Agents", "authors": ["Zheng Wu", "Heyuan Huang", "Xingyu Lou", "Xiangmou Qu", "Pengzhou Cheng", "Zongru Wu", "Weiwen Liu", "Weinan Zhang", "Jun Wang", "Zhaoxiang Wang", "Zhuosheng Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "With the rapid progress of multimodal large language models, operating system\n(OS) agents become increasingly capable of automating tasks through on-device\ngraphical user interfaces (GUIs). However, most existing OS agents are designed\nfor idealized settings, whereas real-world environments often present\nuntrustworthy conditions. To mitigate risks of over-execution in such\nscenarios, we propose a query-driven human-agent-GUI interaction framework that\nenables OS agents to decide when to query humans for more reliable task\ncompletion. Built upon this framework, we introduce VeriOS-Agent, a trustworthy\nOS agent trained with a two-stage learning paradigm that falicitate the\ndecoupling and utilization of meta-knowledge. Concretely, VeriOS-Agent\nautonomously executes actions in normal conditions while proactively querying\nhumans in untrustworthy scenarios. Experiments show that VeriOS-Agent improves\nthe average step-wise success rate by 20.64\\% in untrustworthy scenarios over\nthe state-of-the-art, without compromising normal performance. Analysis\nhighlights VeriOS-Agent's rationality, generalizability, and scalability. The\ncodes, datasets and models are available at\nhttps://github.com/Wuzheng02/VeriOS."}
{"id": "2509.10199", "pdf": "https://arxiv.org/pdf/2509.10199.pdf", "abs": "https://arxiv.org/abs/2509.10199", "title": "Beyond Token Limits: Assessing Language Model Performance on Long Text Classification", "authors": ["Miklós Sebők", "Viktor Kovács", "Martin Bánóczy", "Daniel Møller Eriksen", "Nathalie Neptune", "Philippe Roussille"], "categories": ["cs.CL", "I.7; I.2; J.4"], "comment": null, "summary": "The most widely used large language models in the social sciences (such as\nBERT, and its derivatives, e.g. RoBERTa) have a limitation on the input text\nlength that they can process to produce predictions. This is a particularly\npressing issue for some classification tasks, where the aim is to handle long\ninput texts. One such area deals with laws and draft laws (bills), which can\nhave a length of multiple hundred pages and, therefore, are not particularly\namenable for processing with models that can only handle e.g. 512 tokens. In\nthis paper, we show results from experiments covering 5 languages with\nXLM-RoBERTa, Longformer, GPT-3.5, GPT-4 models for the multiclass\nclassification task of the Comparative Agendas Project, which has a codebook of\n21 policy topic labels from education to health care. Results show no\nparticular advantage for the Longformer model, pre-trained specifically for the\npurposes of handling long inputs. The comparison between the GPT variants and\nthe best-performing open model yielded an edge for the latter. An analysis of\nclass-level factors points to the importance of support and substance overlaps\nbetween specific categories when it comes to performance on long text inputs."}
{"id": "2509.10663", "pdf": "https://arxiv.org/pdf/2509.10663.pdf", "abs": "https://arxiv.org/abs/2509.10663", "title": "Context Copying Modulation: The Role of Entropy Neurons in Managing Parametric and Contextual Knowledge Conflicts", "authors": ["Zineddine Tighidet", "Andrea Mogini", "Hedi Ben-younes", "Jiali Mei", "Patrick Gallinari", "Benjamin Piwowarski"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2025", "summary": "The behavior of Large Language Models (LLMs) when facing contextual\ninformation that conflicts with their internal parametric knowledge is\ninconsistent, with no generally accepted explanation for the expected outcome\ndistribution. Recent work has identified in autoregressive transformer models a\nclass of neurons -- called entropy neurons -- that produce a significant effect\non the model output entropy while having an overall moderate impact on the\nranking of the predicted tokens. In this paper, we investigate the preliminary\nclaim that these neurons are involved in inhibiting context copying behavior in\ntransformers by looking at their role in resolving conflicts between contextual\nand parametric information. We show that entropy neurons are responsible for\nsuppressing context copying across a range of LLMs, and that ablating them\nleads to a significant change in the generation process. These results enhance\nour understanding of the internal dynamics of LLMs when handling conflicting\ninformation."}
{"id": "2509.11498", "pdf": "https://arxiv.org/pdf/2509.11498.pdf", "abs": "https://arxiv.org/abs/2509.11498", "title": "DeDisCo at the DISRPT 2025 Shared Task: A System for Discourse Relation Classification", "authors": ["Zhuoxuan Ju", "Jingni Wu", "Abhishek Purushothama", "Amir Zeldes"], "categories": ["cs.CL"], "comment": "System submission for the DISRPT 2025 - Shared Task on Discourse\n  Relation Parsing and Treebanking In conjunction with CODI-CRAC & EMNLP 2025.\n  1st place in Task 3: relation classification", "summary": "This paper presents DeDisCo, Georgetown University's entry in the DISRPT 2025\nshared task on discourse relation classification. We test two approaches, using\nan mt5-based encoder and a decoder based approach using the openly available\nQwen model. We also experiment on training with augmented dataset for\nlow-resource languages using matched data translated automatically from\nEnglish, as well as using some additional linguistic features inspired by\nentries in previous editions of the Shared Task. Our system achieves a\nmacro-accuracy score of 71.28, and we provide some interpretation and error\nanalysis for our results."}
{"id": "2509.11860", "pdf": "https://arxiv.org/pdf/2509.11860.pdf", "abs": "https://arxiv.org/abs/2509.11860", "title": "MOOM: Maintenance, Organization and Optimization of Memory in Ultra-Long Role-Playing Dialogues", "authors": ["Weishu Chen", "Jinyi Tang", "Zhouhui Hou", "Shihao Han", "Mingjie Zhan", "Zhiyuan Huang", "Delong Liu", "Jiawei Guo", "Zhicheng Zhao", "Fei Su"], "categories": ["cs.CL"], "comment": null, "summary": "Memory extraction is crucial for maintaining coherent ultra-long dialogues in\nhuman-robot role-playing scenarios. However, existing methods often exhibit\nuncontrolled memory growth. To address this, we propose MOOM, the first\ndual-branch memory plugin that leverages literary theory by modeling plot\ndevelopment and character portrayal as core storytelling elements.\nSpecifically, one branch summarizes plot conflicts across multiple time scales,\nwhile the other extracts the user's character profile. MOOM further integrates\na forgetting mechanism, inspired by the ``competition-inhibition'' memory\ntheory, to constrain memory capacity and mitigate uncontrolled growth.\nFurthermore, we present ZH-4O, a Chinese ultra-long dialogue dataset\nspecifically designed for role-playing, featuring dialogues that average 600\nturns and include manually annotated memory information. Experimental results\ndemonstrate that MOOM outperforms all state-of-the-art memory extraction\nmethods, requiring fewer large language model invocations while maintaining a\ncontrollable memory capacity."}
{"id": "2311.09945", "pdf": "https://arxiv.org/pdf/2311.09945.pdf", "abs": "https://arxiv.org/abs/2311.09945", "title": "An Attention-Based Denoising Framework for Personality Detection in Social Media Texts", "authors": ["Lei Lin", "Jizhao Zhu", "Qirui Tang", "Yihua Du"], "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "In social media networks, users produce a large amount of text content\nanytime, providing researchers with an invaluable approach to digging for\npersonality-related information. Personality detection based on user-generated\ntext is a method with broad application prospects, such as for constructing\nuser portraits. The presence of significant noise in social media texts hinders\npersonality detection. However, previous studies have not delved deeper into\naddressing this challenge. Inspired by the scanning reading technique, we\npropose an attention-based information extraction mechanism (AIEM) for long\ntexts, which is applied to quickly locate valuable pieces of text, and fully\nintegrate beneficial semantic information. Then, we provide a novel\nattention-based denoising framework (ADF) for personality detection tasks and\nachieve state-of-the-art performance on two commonly used datasets. Notably, we\nobtain an average accuracy improvement of 10.2% on the gold standard\nTwitter-Myers-Briggs Type Indicator (Twitter-MBTI) dataset. We made our code\npublicly available on\nGitHub\\footnote{https://github.com/Once2gain/PersonalityDetection}. We shed\nlight on how AIEM works to magnify personality-related signals through a case\nstudy."}
{"id": "2405.19988", "pdf": "https://arxiv.org/pdf/2405.19988.pdf", "abs": "https://arxiv.org/abs/2405.19988", "title": "Video-Language Critic: Transferable Reward Functions for Language-Conditioned Robotics", "authors": ["Minttu Alakuijala", "Reginald McLean", "Isaac Woungang", "Nariman Farsad", "Samuel Kaski", "Pekka Marttinen", "Kai Yuan"], "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "14 pages in the main text, 22 pages including references and\n  supplementary materials. 3 figures and 3 tables in the main text, 6 figures\n  and 3 tables in supplementary materials", "summary": "Natural language is often the easiest and most convenient modality for humans\nto specify tasks for robots. However, learning to ground language to behavior\ntypically requires impractical amounts of diverse, language-annotated\ndemonstrations collected on each target robot. In this work, we aim to separate\nthe problem of what to accomplish from how to accomplish it, as the former can\nbenefit from substantial amounts of external observation-only data, and only\nthe latter depends on a specific robot embodiment. To this end, we propose\nVideo-Language Critic, a reward model that can be trained on readily available\ncross-embodiment data using contrastive learning and a temporal ranking\nobjective, and use it to score behavior traces from a separate actor. When\ntrained on Open X-Embodiment data, our reward model enables 2x more\nsample-efficient policy training on Meta-World tasks than a sparse reward only,\ndespite a significant domain gap. Using in-domain data but in a challenging\ntask generalization setting on Meta-World, we further demonstrate more\nsample-efficient training than is possible with prior language-conditioned\nreward models that are either trained with binary classification, use static\nimages, or do not leverage the temporal information present in video data."}
{"id": "2408.08872", "pdf": "https://arxiv.org/pdf/2408.08872.pdf", "abs": "https://arxiv.org/abs/2408.08872", "title": "xGen-MM (BLIP-3): A Family of Open Large Multimodal Models", "authors": ["Le Xue", "Manli Shu", "Anas Awadalla", "Jun Wang", "An Yan", "Senthil Purushwalkam", "Honglu Zhou", "Viraj Prabhu", "Yutong Dai", "Michael S Ryoo", "Shrikant Kendre", "Jieyu Zhang", "Shaoyen Tseng", "Gustavo A Lujan-Moreno", "Matthew L Olson", "Musashi Hinck", "David Cobbley", "Vasudev Lal", "Can Qin", "Shu Zhang", "Chia-Chih Chen", "Ning Yu", "Juntao Tan", "Tulika Manoj Awalgaonkar", "Shelby Heinecke", "Huan Wang", "Yejin Choi", "Ludwig Schmidt", "Zeyuan Chen", "Silvio Savarese", "Juan Carlos Niebles", "Caiming Xiong", "Ran Xu"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "This paper introduces BLIP-3, an open framework for developing Large\nMultimodal Models (LMMs). The framework comprises meticulously curated\ndatasets, a training recipe, model architectures, and a resulting suite of\nLMMs. We release 4B and 14B models, including both the pre-trained base model\nand the instruction fine-tuned ones. Our models undergo rigorous evaluation\nacross a range of tasks, including both single and multi-image benchmarks. Our\nmodels demonstrate competitive performance among open-source LMMs with similar\nmodel sizes. Our resulting LMMs demonstrate competitive performance among\nopen-source LMMs with similar model sizes, with the ability to comprehend\ninterleaved image-text inputs. Our training code, models, and all datasets used\nin this work, including the three largescale datasets we create and the\npreprocessed ones, will be open-sourced to better support the research\ncommunity."}
{"id": "2410.08299", "pdf": "https://arxiv.org/pdf/2410.08299.pdf", "abs": "https://arxiv.org/abs/2410.08299", "title": "Privately Learning from Graphs with Applications in Fine-tuning Large Language Models", "authors": ["Haoteng Yin", "Rongzhe Wei", "Eli Chien", "Pan Li"], "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": "Accepted by COLM 2025", "summary": "Graphs offer unique insights into relationships between entities,\ncomplementing data modalities like text and images and enabling AI models to\nextend their capabilities beyond traditional tasks. However, learning from\ngraphs often involves handling sensitive relationships in the data, raising\nsignificant privacy concerns. Existing privacy-preserving methods, such as\nDP-SGD, rely on gradient decoupling assumptions and are incompatible with\nrelational learning due to the inherent dependencies between training samples.\nTo address this challenge, we propose a privacy-preserving pipeline for\nrelational learning that decouples dependencies in sampled relations for\ntraining, ensuring differential privacy through a tailored application of\nDP-SGD. We apply this approach to fine-tune large language models (LLMs), such\nas Llama2, on sensitive graph data while addressing the associated\ncomputational complexities. Our method is evaluated on four real-world\ntext-attributed graphs, demonstrating significant improvements in relational\nlearning tasks while maintaining robust privacy guarantees. Additionally, we\nanalyze the trade-offs between privacy, utility, and computational efficiency,\noffering insights into the practical deployment of our approach for\nprivacy-preserving relational learning. Code is available at\nhttps://github.com/Graph-COM/PvGaLM."}
{"id": "2412.16846", "pdf": "https://arxiv.org/pdf/2412.16846.pdf", "abs": "https://arxiv.org/abs/2412.16846", "title": "KALL-E:Autoregressive Speech Synthesis with Next-Distribution Prediction", "authors": ["Kangxiang Xia", "Xinfa Zhu", "Jixun Yao", "Wenjie Tian", "Wenhao Li", "Lei Xie"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "6 figures, 5 tables", "summary": "We introduce KALL-E, a novel autoregressive (AR) language model for\ntext-to-speech (TTS) synthesis that operates by predicting the next\ndistribution of continuous speech frames. Unlike existing methods, KALL-E\ndirectly models the continuous speech distribution conditioned on text,\neliminating the need for any diffusion-based components. Specifically, we\nutilize a Flow-VAE to extract a continuous latent speech representation from\nwaveforms, instead of relying on discrete speech tokens. A single AR\nTransformer is then trained to predict these continuous speech distributions\nfrom text, optimizing a Kullback-Leibler divergence loss as its objective.\nExperimental results demonstrate that KALL-E achieves superior speech synthesis\nquality and can even adapt to a target speaker from just a single sample.\nImportantly, KALL-E provides a more direct and effective approach for utilizing\ncontinuous speech representations in TTS."}
{"id": "2502.15871", "pdf": "https://arxiv.org/pdf/2502.15871.pdf", "abs": "https://arxiv.org/abs/2502.15871", "title": "A Comprehensive Survey on the Trustworthiness of Large Language Models in Healthcare", "authors": ["Manar Aljohani", "Jun Hou", "Sindhura Kommu", "Xuan Wang"], "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "The application of large language models (LLMs) in healthcare holds\nsignificant promise for enhancing clinical decision-making, medical research,\nand patient care. However, their integration into real-world clinical settings\nraises critical concerns around trustworthiness, particularly around dimensions\nof truthfulness, privacy, safety, robustness, fairness, and explainability.\nThese dimensions are essential for ensuring that LLMs generate reliable,\nunbiased, and ethically sound outputs. While researchers have recently begun\ndeveloping benchmarks and evaluation frameworks to assess LLM trustworthiness,\nthe trustworthiness of LLMs in healthcare remains underexplored, lacking a\nsystematic review that provides a comprehensive understanding and future\ninsights. This survey addresses that gap by providing a comprehensive review of\ncurrent methodologies and solutions aimed at mitigating risks across key trust\ndimensions. We analyze how each dimension affects the reliability and ethical\ndeployment of healthcare LLMs, synthesize ongoing research efforts, and\nidentify critical gaps in existing approaches. We also identify emerging\nchallenges posed by evolving paradigms, such as multi-agent collaboration,\nmulti-modal reasoning, and the development of small open-source medical models.\nOur goal is to guide future research toward more trustworthy, transparent, and\nclinically viable LLMs."}
{"id": "2502.20742", "pdf": "https://arxiv.org/pdf/2502.20742.pdf", "abs": "https://arxiv.org/abs/2502.20742", "title": "Structured Preference Optimization for Vision-Language Long-Horizon Task Planning", "authors": ["Xiwen Liang", "Min Lin", "Weiqi Ruan", "Rongtao Xu", "Yuecheng Liu", "Jiaqi Chen", "Bingqian Lin", "Yuzheng Zhuang", "Xiaodan Liang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "18 pages", "summary": "Existing methods for vision-language task planning excel in short-horizon\ntasks but often fall short in complex, long-horizon planning within dynamic\nenvironments. These challenges primarily arise from the difficulty of\neffectively training models to produce high-quality reasoning processes for\nlong-horizon tasks. To address this, we propose Structured Preference\nOptimization (SPO), which aims to enhance reasoning and action selection in\nlong-horizon task planning through structured preference evaluation and\noptimized training strategies. Specifically, SPO introduces: 1)\nPreference-Based Scoring and Optimization, which systematically evaluates\nreasoning chains based on task relevance, visual grounding, and historical\nconsistency; and 2) Curriculum-Guided Training, where the model progressively\nadapts from simple to complex tasks, improving its generalization ability in\nlong-horizon scenarios and enhancing reasoning robustness. To advance research\nin vision-language long-horizon task planning, we introduce ExtendaBench, a\ncomprehensive benchmark covering 1,509 tasks across VirtualHome and Habitat\n2.0, categorized into ultra-short, short, medium, and long tasks. Experimental\nresults demonstrate that SPO significantly improves reasoning quality and final\ndecision accuracy, outperforming prior methods on long-horizon tasks and\nunderscoring the effectiveness of preference-driven optimization in\nvision-language task planning. Specifically, SPO achieves a +5.98% GCR and\n+4.68% SR improvement in VirtualHome and a +3.30% GCR and +2.11% SR improvement\nin Habitat over the best-performing baselines."}
{"id": "2503.10408", "pdf": "https://arxiv.org/pdf/2503.10408.pdf", "abs": "https://arxiv.org/abs/2503.10408", "title": "Out-of-Context Reasoning in Large Language Models", "authors": ["Jonathan Shaki", "Emanuele La Malfa", "Michael Wooldridge", "Sarit Kraus"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We study how large language models (LLMs) reason about memorized knowledge\nthrough simple binary relations such as equality ($=$), inequality ($<$), and\ninclusion ($\\subset$). Unlike in-context reasoning, the axioms (e.g., $a < b, b\n< c$) are only seen during training and not provided in the task prompt (e.g.,\nevaluating $a < c$). The tasks require one or more reasoning steps, and data\naggregation from one or more sources, showing performance change with task\ncomplexity. We introduce a lightweight technique, out-of-context representation\nlearning, which trains only new token embeddings on axioms and evaluates them\non unseen tasks. Across reflexivity, symmetry, and transitivity tests, LLMs\nmostly perform statistically significant better than chance, making the correct\nanswer extractable when testing multiple phrasing variations, but still fall\nshort of consistent reasoning on every single query. Analysis shows that the\nlearned embeddings are organized in structured ways, suggesting real relational\nunderstanding. Surprisingly, it also indicates that the core reasoning happens\nduring the training, not inference."}
{"id": "2506.00308", "pdf": "https://arxiv.org/pdf/2506.00308.pdf", "abs": "https://arxiv.org/abs/2506.00308", "title": "MythTriage: Scalable Detection of Opioid Use Disorder Myths on a Video-Sharing Platform", "authors": ["Hayoung Jung", "Shravika Mittal", "Ananya Aatreya", "Navreet Kaur", "Munmun De Choudhury", "Tanushree Mitra"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": "To appear at EMNLP 2025. Please cite EMNLP version when proceedings\n  are available", "summary": "Understanding the prevalence of misinformation in health topics online can\ninform public health policies and interventions. However, measuring such\nmisinformation at scale remains a challenge, particularly for high-stakes but\nunderstudied topics like opioid-use disorder (OUD)--a leading cause of death in\nthe U.S. We present the first large-scale study of OUD-related myths on\nYouTube, a widely-used platform for health information. With clinical experts,\nwe validate 8 pervasive myths and release an expert-labeled video dataset. To\nscale labeling, we introduce MythTriage, an efficient triage pipeline that uses\na lightweight model for routine cases and defers harder ones to a\nhigh-performing, but costlier, large language model (LLM). MythTriage achieves\nup to 0.86 macro F1-score while estimated to reduce annotation time and\nfinancial cost by over 76% compared to experts and full LLM labeling. We\nanalyze 2.9K search results and 343K recommendations, uncovering how myths\npersist on YouTube and offering actionable insights for public health and\nplatform moderation."}
{"id": "2508.05170", "pdf": "https://arxiv.org/pdf/2508.05170.pdf", "abs": "https://arxiv.org/abs/2508.05170", "title": "Posterior-GRPO: Rewarding Reasoning Processes in Code Generation", "authors": ["Lishui Fan", "Yu Zhang", "Mouxiang Chen", "Zhongxin Liu"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) has significantly advanced code generation for\nlarge language models (LLMs). However, current paradigms rely on outcome-based\nrewards from test cases, neglecting the quality of the intermediate reasoning\nprocess. While supervising the reasoning process directly is a promising\ndirection, it is highly susceptible to reward hacking, where the policy model\nlearns to exploit the reasoning reward signal without improving final outcomes.\nTo address this, we introduce a unified framework that can effectively\nincorporate the quality of the reasoning process during RL. First, to enable\nreasoning evaluation, we develop LCB-RB, a benchmark comprising preference\npairs of superior and inferior reasoning processes. Second, to accurately score\nreasoning quality, we introduce an Optimized-Degraded based (OD-based) method\nfor reward model training. This method generates high-quality preference pairs\nby systematically optimizing and degrading initial reasoning paths along\ncurated dimensions of reasoning quality, such as factual accuracy, logical\nrigor, and coherence. A 7B parameter reward model with this method achieves\nstate-of-the-art (SOTA) performance on LCB-RB and generalizes well to other\nbenchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method\nthat conditions process-based rewards on task success. By selectively applying\nrewards to the reasoning processes of only successful outcomes, P-GRPO\neffectively mitigates reward hacking and aligns the model's internal reasoning\nwith final code correctness. A 7B parameter model with P-GRPO achieves superior\nperformance across diverse code generation tasks, outperforming outcome-only\nbaselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further\ndemonstrate the generalizability of our approach by extending it to\nmathematical tasks. Our models, dataset, and code are publicly available."}
{"id": "2508.05606", "pdf": "https://arxiv.org/pdf/2508.05606.pdf", "abs": "https://arxiv.org/abs/2508.05606", "title": "Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and Vision", "authors": ["Luozheng Qin", "Jia Gong", "Yuqing Sun", "Tianjiao Li", "Mengping Yang", "Xiaomeng Yang", "Chao Qu", "Zhiyu Tan", "Hao Li"], "categories": ["cs.CV", "cs.CL"], "comment": "Project Page: https://sais-fuxi.github.io/projects/uni-cot/", "summary": "Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large\nLanguage Models (LLMs) by decomposing complex tasks into simpler, sequential\nsubtasks. However, extending CoT to vision-language reasoning tasks remains\nchallenging, as it often requires interpreting transitions of visual states to\nsupport reasoning. Existing methods often struggle with this due to limited\ncapacity of modeling visual state transitions or incoherent visual trajectories\ncaused by fragmented architectures.\n  To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought\nframework that enables coherent and grounded multimodal reasoning within a\nsingle unified model. The key idea is to leverage a model capable of both image\nunderstanding and generation to reason over visual content and model evolving\nvisual states. However, empowering a unified model to achieve that is\nnon-trivial, given the high computational cost and the burden of training. To\naddress this, Uni-CoT introduces a novel two-level reasoning paradigm: A\nMacro-Level CoT for high-level task planning and A Micro-Level CoT for subtask\nexecution. This design significantly reduces the computational overhead.\nFurthermore, we introduce a structured training paradigm that combines\ninterleaved image-text supervision for macro-level CoT with multi-task\nobjectives for micro-level CoT. Together, these innovations allow Uni-CoT to\nperform scalable and coherent multi-modal reasoning. Furthermore, thanks to our\ndesign, all experiments can be efficiently completed using only 8 A100 GPUs\nwith 80GB VRAM each. Experimental results on reasoning-driven image generation\nbenchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT\ndemonstrates SOTA performance and strong generalization, establishing Uni-CoT\nas a promising solution for multi-modal reasoning. Project Page and Code:\nhttps://sais-fuxi.github.io/projects/uni-cot/"}
{"id": "2509.03740", "pdf": "https://arxiv.org/pdf/2509.03740.pdf", "abs": "https://arxiv.org/abs/2509.03740", "title": "Singular Value Few-shot Adaptation of Vision-Language Models", "authors": ["Taha Koleilat", "Hassan Rivaz", "Yiming Xiao"], "categories": ["cs.CV", "cs.CL"], "comment": "10 pages, 2 figures, 8 tables", "summary": "Vision-language models (VLMs) like CLIP have shown impressive zero-shot and\nfew-shot learning capabilities across diverse applications. However, adapting\nthese models to new fine-grained domains remains difficult due to reliance on\nprompt engineering and the high cost of full model fine-tuning. Existing\nadaptation approaches rely on augmented components, such as prompt tokens and\nadapter modules, which could limit adaptation quality, destabilize the model,\nand compromise the rich knowledge learned during pretraining. In this work, we\npresent CLIP-SVD, a novel multi-modal and parameter-efficient adaptation\ntechnique that leverages Singular Value Decomposition (SVD) to modify the\ninternal parameter space of CLIP without injecting additional modules.\nSpecifically, we fine-tune only the singular values of the CLIP parameter\nmatrices to rescale the basis vectors for domain adaptation while retaining the\npretrained model. This design enables enhanced adaptation performance using\nonly 0.04% of the model's total parameters and better preservation of its\ngeneralization ability. CLIP-SVD achieves state-of-the-art classification\nresults on 11 natural and 10 biomedical datasets, outperforming previous\nmethods in both accuracy and generalization under few-shot settings.\nAdditionally, we leverage a natural language-based approach to analyze the\neffectiveness and dynamics of the CLIP adaptation to allow interpretability of\nCLIP-SVD. The code is publicly available at\nhttps://github.com/HealthX-Lab/CLIP-SVD."}
{"id": "2509.12248", "pdf": "https://arxiv.org/pdf/2509.12248.pdf", "abs": "https://arxiv.org/abs/2509.12248", "title": "Humor in Pixels: Benchmarking Large Multimodal Models Understanding of Online Comics", "authors": ["Yuriel Ryan", "Rui Yang Tan", "Kenny Tsu Wei Choo", "Roy Ka-Wei Lee"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "27 pages, 8 figures, EMNLP 2025 Findings", "summary": "Understanding humor is a core aspect of social intelligence, yet it remains a\nsignificant challenge for Large Multimodal Models (LMMs). We introduce\nPixelHumor, a benchmark dataset of 2,800 annotated multi-panel comics designed\nto evaluate LMMs' ability to interpret multimodal humor and recognize narrative\nsequences. Experiments with state-of-the-art LMMs reveal substantial gaps: for\ninstance, top models achieve only 61% accuracy in panel sequencing, far below\nhuman performance. This underscores critical limitations in current models'\nintegration of visual and textual cues for coherent narrative and humor\nunderstanding. By providing a rigorous framework for evaluating multimodal\ncontextual and narrative reasoning, PixelHumor aims to drive the development of\nLMMs that better engage in natural, socially aware interactions."}
{"id": "2509.12574", "pdf": "https://arxiv.org/pdf/2509.12574.pdf", "abs": "https://arxiv.org/abs/2509.12574", "title": "Yet Another Watermark for Large Language Models", "authors": ["Siyuan Bao", "Ying Shi", "Zhiguang Yang", "Hanzhou Wu", "Xinpeng Zhang"], "categories": ["cs.CR", "cs.CL"], "comment": "https://scholar.google.com/citations?hl=en&user=IdiF7M0AAAAJ", "summary": "Existing watermarking methods for large language models (LLMs) mainly embed\nwatermark by adjusting the token sampling prediction or post-processing,\nlacking intrinsic coupling with LLMs, which may significantly reduce the\nsemantic quality of the generated marked texts. Traditional watermarking\nmethods based on training or fine-tuning may be extendable to LLMs. However,\nmost of them are limited to the white-box scenario, or very time-consuming due\nto the massive parameters of LLMs. In this paper, we present a new watermarking\nframework for LLMs, where the watermark is embedded into the LLM by\nmanipulating the internal parameters of the LLM, and can be extracted from the\ngenerated text without accessing the LLM. Comparing with related methods, the\nproposed method entangles the watermark with the intrinsic parameters of the\nLLM, which better balances the robustness and imperceptibility of the\nwatermark. Moreover, the proposed method enables us to extract the watermark\nunder the black-box scenario, which is computationally efficient for use.\nExperimental results have also verified the feasibility, superiority and\npracticality. This work provides a new perspective different from mainstream\nworks, which may shed light on future research."}
