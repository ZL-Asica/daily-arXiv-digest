{"id": "2506.09089", "pdf": "https://arxiv.org/pdf/2506.09089.pdf", "abs": "https://arxiv.org/abs/2506.09089", "title": "Designing conflict-based communicative tasks in Teaching Chinese as a Foreign Language with ChatGPT", "authors": ["Xia Li"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "in French language", "summary": "In developing the teaching program for a course in Oral Expression in\nTeaching Chinese as a Foreign Language at the university level, the teacher\ndesigns communicative tasks based on conflicts to encourage learners to engage\nin interactive dynamics and develop their oral interaction skills. During the\ndesign of these tasks, the teacher uses ChatGPT to assist in finalizing the\nprogram. This article aims to present the key characteristics of the\ninteractions between the teacher and ChatGPT during this program development\nprocess, as well as to examine the use of ChatGPT and its impacts in this\nspecific context."}
{"id": "2506.09153", "pdf": "https://arxiv.org/pdf/2506.09153.pdf", "abs": "https://arxiv.org/abs/2506.09153", "title": "Real-Time Confidence Detection through Facial Expressions and Hand Gestures", "authors": ["Tanjil Hasan Sakib", "Samia Jahan Mojumder", "Rajan Das Gupta", "Md Imrul Hasan Showmick", "Md. Yeasin Rahat", "Md. Jakir Hossen"], "categories": ["cs.HC"], "comment": "Accepted in MECON 2025", "summary": "Real-time face orientation recognition is a cutting-edge technology meant to\ntrack and analyze facial movements in virtual environments such as online\ninterviews, remote meetings, and virtual classrooms. As the demand for virtual\ninteractions grows, it becomes increasingly important to measure participant\nengagement, attention, and overall interaction. This research presents a novel\nsolution that leverages the Media Pipe Face Mesh framework to identify facial\nlandmarks and extract geometric data for calculating Euler angles, which\ndetermine head orientation in real time. The system tracks 3D facial landmarks\nand uses this data to compute head movements with a focus on accuracy and\nresponsiveness. By studying Euler angles, the system can identify a user's head\norientation with an accuracy of 90\\%, even at a distance of up to four feet.\nThis capability offers significant enhancements for monitoring user\ninteraction, allowing for more immersive and interactive virtual ex-periences.\nThe proposed method shows its reliability in evaluating participant\nattentiveness during online assessments and meetings. Its application goes\nbeyond engagement analysis, potentially providing a means for improving the\nquality of virtual communication, fostering better understanding between\nparticipants, and ensuring a higher level of interaction in digital spaces.\nThis study offers a basis for future developments in enhancing virtual user\nexperiences by integrating real-time facial tracking technologies, paving the\nway for more adaptive and interactive web-based platform."}
{"id": "2506.09212", "pdf": "https://arxiv.org/pdf/2506.09212.pdf", "abs": "https://arxiv.org/abs/2506.09212", "title": "Show Me Your Best Side: Characteristics of User-Preferred Perspectives for 3D Graph Drawings", "authors": ["Lucas Joos", "Gavin J. Mooney", "Maximilian T. Fischer", "Daniel A. Keim", "Falk Schreiber", "Helen C. Purchase", "Karsten Klein"], "categories": ["cs.HC"], "comment": null, "summary": "The visual analysis of graphs in 3D has become increasingly popular,\naccelerated by the rise of immersive technology, such as augmented and virtual\nreality. Unlike 2D drawings, 3D graph layouts are highly viewpoint-dependent,\nmaking perspective selection critical for revealing structural and relational\npatterns. Despite its importance, there is limited empirical evidence guiding\nwhat constitutes an effective or preferred viewpoint from the user's\nperspective. In this paper, we present a systematic investigation into\nuser-preferred viewpoints in 3D graph visualisations. We conducted a controlled\nstudy with 23 participants in a virtual reality environment, where users\nselected their most and least preferred viewpoints for 36 different graphs\nvarying in size and layout. From this data, enriched by qualitative feedback,\nwe distil common strategies underlying viewpoint choice. We further analyse the\nalignment of user preferences with classical 2D aesthetic criteria (e.g.,\nCrossings), 3D-specific measures (e.g., Node-Node Occlusion), and introduce a\nnovel measure capturing the perceivability of a graph's principal axes\n(Isometric Viewpoint Deviation). Our data-driven analysis indicates that\nStress, Crossings, Gabriel Ratio, Edge-Node Overlap, and Isometric Viewpoint\nDeviation are key indicators of viewpoint preference. Beyond our findings, we\ncontribute a publicly available dataset consisting of the graphs and computed\naesthetic measures, supporting further research and the development of\nviewpoint evaluation measures for 3D graph drawing."}
{"id": "2506.09216", "pdf": "https://arxiv.org/pdf/2506.09216.pdf", "abs": "https://arxiv.org/abs/2506.09216", "title": "\"How do you even know that stuff?\": Barriers to expertise sharing among spreadsheet users", "authors": ["Qing", "Xia", "Advait Sarkar", "Duncan Brumby", "Anna Cox"], "categories": ["cs.HC", "cs.CY", "H.5"], "comment": "Accepted at CSCW 2025", "summary": "Spreadsheet collaboration provides valuable opportunities for learning and\nexpertise sharing between colleagues. Sharing expertise is essential for the\nretention of important technical skillsets within organisations, but previous\nstudies suggest that spreadsheet experts often fail to disseminate their\nknowledge to others. We suggest that social norms and beliefs surrounding the\nvalue of spreadsheet use significantly influence user engagement in sharing\nbehaviours. To explore this, we conducted 31 semi-structured interviews with\nprofessional spreadsheet users from two separate samples. We found that\nspreadsheet providers face challenges in adapting highly personalised\nstrategies to often subjective standards and evaluating the appropriate social\ntiming of sharing. In addition, conflicted self-evaluations of one's\nspreadsheet expertise, dismissive normative beliefs about the value of this\nknowledge, and concerns about the potential disruptions associated with\ncollaboration can further deter sharing. We suggest these observations reflect\nthe challenges of long-term learning in feature-rich software designed\nprimarily with initial learnability in mind. We therefore provide implications\nfor design to navigate this tension. Overall, our findings demonstrate how the\ncomplex interaction between technology design and social dynamics can shape\ncollaborative learning behaviours in the context of feature-rich software."}
{"id": "2506.09147", "pdf": "https://arxiv.org/pdf/2506.09147.pdf", "abs": "https://arxiv.org/abs/2506.09147", "title": "LLM-as-a-qualitative-judge: automating error analysis in natural language generation", "authors": ["Nadezhda Chirkova", "Tunde Oluwaseyi Ajayi", "Seth Aycock", "Zain Muhammad Mujahid", "Vladana Perlić", "Ekaterina Borisova", "Markarit Vartampetian"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Prompting large language models (LLMs) to evaluate generated text, known as\nLLM-as-a-judge, has become a standard evaluation approach in natural language\ngeneration (NLG), but is primarily used as a quantitative tool, i.e. with\nnumerical scores as main outputs. In this work, we propose\nLLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main\noutput being a structured report of common issue types in the NLG system\noutputs. Our approach is targeted at providing developers with meaningful\ninsights on what improvements can be done to a given NLG system and consists of\ntwo main steps, namely open-ended per-instance issue analysis and clustering of\nthe discovered issues using an intuitive cumulative algorithm. We also\nintroduce a strategy for evaluating the proposed approach, coupled with ~300\nannotations of issues in instances from 12 NLG datasets. Our results show that\nLLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3\ncases and is capable of producing error type reports resembling the reports\ncomposed by human annotators. Our code and data are publicly available at\nhttps://github.com/tunde-ajayi/llm-as-a-qualitative-judge."}
{"id": "2506.09220", "pdf": "https://arxiv.org/pdf/2506.09220.pdf", "abs": "https://arxiv.org/abs/2506.09220", "title": "Beyond the Hype: Mapping Uncertainty and Gratification in AI Assistant Use", "authors": ["Karen Joy", "Tawfiq Ammari", "Alyssa Sheehan"], "categories": ["cs.HC"], "comment": null, "summary": "This paper examines the gap between the promises and real-world performance\nof emerging AI personal assistants. Drawing on interviews with early adopters\nof devices like Rabbit R1 and Humane AI Pin, as well as services like Ohai and\nDocus, we map user experiences through the lens of Uses and Gratifications and\nUncertainty Reduction Theory. We identify three core types of user uncertainty,\nfunctional, interactional, and social, and explore how each disrupts different\nuser gratifications. We show that while marketing hype fuels initial adoption,\nunmet expectations often result in frustration or abandonment. Our findings\nhighlight the importance of transparency, task-specific design, and user\ncontrol over contextual memory and personalization. We provide design and\npolicy recommendations, including user-facing explainability tools and calls\nfor regulatory benchmarks such as CI Bench, to guide ethical and interpretable\nAI integration. Our study offers actionable insights for creating more usable,\ntrustworthy, and socially aligned AI assistants."}
{"id": "2506.09175", "pdf": "https://arxiv.org/pdf/2506.09175.pdf", "abs": "https://arxiv.org/abs/2506.09175", "title": "PHRASED: Phrase Dictionary Biasing for Speech Translation", "authors": ["Peidong Wang", "Jian Xue", "Rui Zhao", "Junkun Chen", "Aswin Shanmugam Subramanian", "Jinyu Li"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Phrases are essential to understand the core concepts in conversations.\nHowever, due to their rare occurrence in training data, correct translation of\nphrases is challenging in speech translation tasks. In this paper, we propose a\nphrase dictionary biasing method to leverage pairs of phrases mapping from the\nsource language to the target language. We apply the phrase dictionary biasing\nmethod to two types of widely adopted models, a transducer-based streaming\nspeech translation model and a multimodal large language model. Experimental\nresults show that the phrase dictionary biasing method outperforms phrase list\nbiasing by 21% relatively for the streaming speech translation model. In\naddition, phrase dictionary biasing enables multimodal large language models to\nuse external phrase information, achieving 85% relative improvement in phrase\nrecall."}
{"id": "2506.09236", "pdf": "https://arxiv.org/pdf/2506.09236.pdf", "abs": "https://arxiv.org/abs/2506.09236", "title": "Augmented Reality User Interfaces for First Responders: A Scoping Literature Review", "authors": ["Erin Argo", "Tanim Ahmed", "Sarah Gable", "Callie Hampton", "Jeronimo Grandi", "Regis Kopper"], "categories": ["cs.HC"], "comment": "19 pages, 4 figures, 8 tables", "summary": "During the past decade, there has been a significant increase in research\nfocused on integrating AR User Interfaces into public safety applications,\nparticularly for first responders in the domains of Emergency Medical Services,\nFirefighting, and Law Enforcement. This paper presents the results of a scoping\nreview involving the application of AR user interfaces in the public safety\ndomain and applies an established systematic review methodology to provide a\ncomprehensive analysis of the current research landscape, identifying key\ntrends, challenges, and gaps in the literature. This review includes\npeer-reviewed publications indexed by the major scientific databases up to\nApril 2025. A basic keyword search retrieved 1,751 papers, of which 90 were\ndeemed relevant for this review. An in-depth analysis of the literature allowed\nthe development of a faceted taxonomy that categorizes AR user interfaces for\npublic safety. This classification lays a solid foundation for future research,\nwhile also highlighting key design considerations, challenges, and gaps in the\nliterature. This review serves as a valuable resource for researchers and\ndevelopers, offering insights that can drive further advances in the field."}
{"id": "2506.09218", "pdf": "https://arxiv.org/pdf/2506.09218.pdf", "abs": "https://arxiv.org/abs/2506.09218", "title": "A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs", "authors": ["Bruno Ferenc Šegedin"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "The ability of deep neural networks (DNNs) to represent phonotactic\ngeneralizations derived from lexical learning remains an open question. This\nstudy (1) investigates the lexically-invariant generalization capacity of\ngenerative convolutional neural networks (CNNs) trained on raw audio waveforms\nof lexical items and (2) explores the consequences of shrinking the\nfully-connected layer (FC) bottleneck from 1024 channels to 8 before training.\nUltimately, a novel technique for probing a model's lexically-independent\ngeneralizations is proposed that works only under the narrow FC bottleneck:\ngenerating audio outputs by bypassing the FC and inputting randomized feature\nmaps into the convolutional block. These outputs are equally biased by a\nphonotactic restriction in training as are outputs generated with the FC. This\nresult shows that the convolutional layers can dynamically generalize phonetic\ndependencies beyond lexically-constrained configurations learned by the FC."}
{"id": "2506.09292", "pdf": "https://arxiv.org/pdf/2506.09292.pdf", "abs": "https://arxiv.org/abs/2506.09292", "title": "AI Tutors vs. Tenacious Myths: Evidence from Personalised Dialogue Interventions in Education", "authors": ["Brooklyn J. Corbett", "Jason M. Tangen"], "categories": ["cs.HC"], "comment": "Originally posted as https://doi.org/10.31234/osf.io/x4wqh_v1", "summary": "Misconceptions in psychology and education persist despite clear\ncontradictory evidence, resisting traditional correction methods. This study\ninvestigated whether personalised AI dialogue could effectively correct these\nstubborn beliefs. In a preregistered experiment (N = 375), participants holding\nstrong psychology misconceptions engaged in one of three interventions: (1)\npersonalised AI dialogue targeting their specific misconception, (2) generic\ntextbook-style refutation, or (3) neutral AI dialogue (control). Results showed\nthat personalised AI dialogue produced significantly larger immediate belief\nreductions compared to both textbook reading and neutral dialogue. This\nadvantage persisted at 10-day follow-up but diminished by 2 months, where AI\ndialogue and textbook conditions converged while both remained superior to\ncontrol. Both AI conditions generated significantly higher engagement and\nconfidence than textbook reading, demonstrating the motivational benefits of\nconversational interaction. These findings demonstrate that AI dialogue can\naccelerate initial belief correction through personalised, interactive\nengagement that disrupts the cognitive processes maintaining misconceptions.\nHowever, the convergence of effects over time suggests brief interventions\nrequire reinforcement for lasting change. Future applications should integrate\nAI tutoring into structured educational programs with spaced reinforcement to\nsustain the initial advantages of personalised dialogue."}
{"id": "2506.09251", "pdf": "https://arxiv.org/pdf/2506.09251.pdf", "abs": "https://arxiv.org/abs/2506.09251", "title": "Extrapolation by Association: Length Generalization Transfer in Transformers", "authors": ["Ziyang Cai", "Nayoung Lee", "Avi Schwarzschild", "Samet Oymak", "Dimitris Papailiopoulos"], "categories": ["cs.CL", "cs.AI"], "comment": "23 pages, 20 figures", "summary": "Transformer language models have demonstrated impressive generalization\ncapabilities in natural language domains, yet we lack a fine-grained\nunderstanding of how such generalization arises. In this paper, we investigate\nlength generalization--the ability to extrapolate from shorter to longer\ninputs--through the lens of \\textit{task association}. We find that length\ngeneralization can be \\textit{transferred} across related tasks. That is,\ntraining a model with a longer and related auxiliary task can lead it to\ngeneralize to unseen and longer inputs from some other target task. We\ndemonstrate this length generalization transfer across diverse algorithmic\ntasks, including arithmetic operations, string transformations, and maze\nnavigation. Our results show that transformer models can inherit generalization\ncapabilities from similar tasks when trained jointly. Moreover, we observe\nsimilar transfer effects in pretrained language models, suggesting that\npretraining equips models with reusable computational scaffolding that\nfacilitates extrapolation in downstream settings. Finally, we provide initial\nmechanistic evidence that length generalization transfer correlates with the\nre-use of the same attention heads between the tasks. Together, our findings\ndeepen our understanding of how transformers generalize to out-of-distribution\ninputs and highlight the compositional reuse of inductive structure across\ntasks."}
{"id": "2506.09354", "pdf": "https://arxiv.org/pdf/2506.09354.pdf", "abs": "https://arxiv.org/abs/2506.09354", "title": "\"Is This Really a Human Peer Supporter?\": Misalignments Between Peer Supporters and Experts in LLM-Supported Interactions", "authors": ["Kellie Yu Hui Sim", "Roy Ka-Wei Lee", "Kenny Tsu Wei Choo"], "categories": ["cs.HC", "cs.AI", "H.5.0"], "comment": null, "summary": "Mental health is a growing global concern, prompting interest in AI-driven\nsolutions to expand access to psychosocial support. Peer support, grounded in\nlived experience, offers a valuable complement to professional care. However,\nvariability in training, effectiveness, and definitions raises concerns about\nquality, consistency, and safety. Large Language Models (LLMs) present new\nopportunities to enhance peer support interactions, particularly in real-time,\ntext-based interactions. We present and evaluate an AI-supported system with an\nLLM-simulated distressed client, context-sensitive LLM-generated suggestions,\nand real-time emotion visualisations. 2 mixed-methods studies with 12 peer\nsupporters and 5 mental health professionals (i.e., experts) examined the\nsystem's effectiveness and implications for practice. Both groups recognised\nits potential to enhance training and improve interaction quality. However, we\nfound a key tension emerged: while peer supporters engaged meaningfully,\nexperts consistently flagged critical issues in peer supporter responses, such\nas missed distress cues and premature advice-giving. This misalignment\nhighlights potential limitations in current peer support training, especially\nin emotionally charged contexts where safety and fidelity to best practices are\nessential. Our findings underscore the need for standardised, psychologically\ngrounded training, especially as peer support scales globally. They also\ndemonstrate how LLM-supported systems can scaffold this development--if\ndesigned with care and guided by expert oversight. This work contributes to\nemerging conversations on responsible AI integration in mental health and the\nevolving role of LLMs in augmenting peer-delivered care."}
{"id": "2506.09259", "pdf": "https://arxiv.org/pdf/2506.09259.pdf", "abs": "https://arxiv.org/abs/2506.09259", "title": "Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat", "authors": ["Zhuofang Li", "Rafal Kocielnik", "Fereshteh Soltani", "Penphob", "Boonyarungsrit", "Animashree Anandkumar", "R. Michael Alvarez"], "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; K.4"], "comment": null, "summary": "Millions of players engage daily in competitive online games, communicating\nthrough in-game chat. Prior research has focused on detecting relatively small\nvolumes of toxic content using various Natural Language Processing (NLP)\ntechniques for the purpose of moderation. However, recent studies emphasize the\nimportance of detecting prosocial communication, which can be as crucial as\nidentifying toxic interactions. Recognizing prosocial behavior allows for its\nanalysis, rewarding, and promotion. Unlike toxicity, there are limited\ndatasets, models, and resources for identifying prosocial behaviors in\ngame-chat text. In this work, we employed unsupervised discovery combined with\ngame domain expert collaboration to identify and categorize prosocial player\nbehaviors from game chat. We further propose a novel Self-Anchored Attention\nModel (SAAM) which gives 7.9% improvement compared to the best existing\ntechnique. The approach utilizes the entire training set as \"anchors\" to help\nimprove model performance under the scarcity of training data. This approach\nled to the development of the first automated system for classifying prosocial\nbehaviors in in-game chats, particularly given the low-resource settings where\nlarge-scale labeled data is not available. Our methodology was applied to one\nof the most popular online gaming titles - Call of Duty(R): Modern\nWarfare(R)II, showcasing its effectiveness. This research is novel in applying\nNLP techniques to discover and classify prosocial behaviors in player in-game\nchat communication. It can help shift the focus of moderation from solely\npenalizing toxicity to actively encouraging positive interactions on online\nplatforms."}
{"id": "2506.09362", "pdf": "https://arxiv.org/pdf/2506.09362.pdf", "abs": "https://arxiv.org/abs/2506.09362", "title": "\"I Said Things I Needed to Hear Myself\": Peer Support as an Emotional, Organisational, and Sociotechnical Practice in Singapore", "authors": ["Kellie Yu Hui Sim", "Kenny Tsu Wei Choo"], "categories": ["cs.HC", "cs.AI", "H.5.0"], "comment": null, "summary": "Peer support plays a vital role in expanding access to mental health care by\nproviding empathetic, community-based support outside formal clinical systems.\nAs digital platforms increasingly mediate such support, the design and impact\nof these technologies remain under-examined, particularly in Asian contexts.\nThis paper presents findings from an interview study with 20 peer supporters in\nSingapore, who operate across diverse online, offline, and hybrid environments.\nThrough a thematic analysis, we unpack how participants start, conduct, and\nsustain peer support, highlighting their motivations, emotional labour, and the\nsociocultural dimensions shaping their practices. Building on this grounded\nunderstanding, we surface design directions for culturally responsive digital\ntools that scaffold rather than supplant relational care. Drawing insights from\nqualitative accounts, we offer a situated perspective on how AI might\nresponsibly augment peer support. This research contributes to human-centred\ncomputing by articulating the lived realities of peer supporters and proposing\ndesign implications for trustworthy and context-sensitive AI in mental health."}
{"id": "2506.09277", "pdf": "https://arxiv.org/pdf/2506.09277.pdf", "abs": "https://arxiv.org/abs/2506.09277", "title": "Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models", "authors": ["Milan Bhan", "Jean-Noel Vittaut", "Nicolas Chesneau", "Sarath Chandar", "Marie-Jeanne Lesot"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLM) have demonstrated the capability of generating\nfree text self Natural Language Explanation (self-NLE) to justify their\nanswers. Despite their logical appearance, self-NLE do not necessarily reflect\nthe LLM actual decision-making process, making such explanations unfaithful.\nWhile existing methods for measuring self-NLE faithfulness mostly rely on\nbehavioral tests or computational block identification, none of them examines\nthe neural activity underlying the model's reasoning. This work introduces a\nnovel flexible framework for quantitatively measuring the faithfulness of\nLLM-generated self-NLE by directly comparing the latter with interpretations of\nthe model's internal hidden states. The proposed framework is versatile and\nprovides deep insights into self-NLE faithfulness by establishing a direct\nconnection between self-NLE and model reasoning. This approach advances the\nunderstanding of self-NLE faithfulness and provides building blocks for\ngenerating more faithful self-NLE."}
{"id": "2506.09696", "pdf": "https://arxiv.org/pdf/2506.09696.pdf", "abs": "https://arxiv.org/abs/2506.09696", "title": "Patterns of Patterns III", "authors": ["Joseph Corneli", "Charles J. Danoff", "Raymond S. Puzio", "Sridevi Ayloo", "Serge Belich", "Mary Tedeschi"], "categories": ["cs.HC"], "comment": "18 pages; submitted to Pattern Languages of Programs 2025", "summary": "Building on earlier installments, this paper re-examines the PLACARD pattern.\nWe report on a series of workshops where PLACARD was used to scaffold\ncollaborative reflection, speculative inquiry, and stimulate design pattern\ngeneration. These accounts are enriched by a comparison case: virtual workshops\ncarried out with simple AI-based chatbots. We discuss limitations and lessons\nlearned from both the human and multi-agent settings. We conclude by outlining\na future development strategy at the intersection of AI agents, design\npatterns, and institutional governance."}
{"id": "2506.09301", "pdf": "https://arxiv.org/pdf/2506.09301.pdf", "abs": "https://arxiv.org/abs/2506.09301", "title": "$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding", "authors": ["Cesare Spinoso-Di Piano", "David Austin", "Pablo Piantanida", "Jackie Chi Kit Cheung"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in\nhuman communication, resulting in utterances where the literal and the intended\nmeanings do not match. The Rational Speech Act (RSA) framework, which\nexplicitly models speaker intentions, is the most widespread theory of\nprobabilistic pragmatics, but existing implementations are either unable to\naccount for figurative expressions or require modeling the implicit motivations\nfor using figurative language (e.g., to express joy or annoyance) in a\nsetting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware\nRSA $(RSA)^2$ framework which models figurative language use by considering a\nspeaker's employed rhetorical strategy. We show that $(RSA)^2$ enables\nhuman-compatible interpretations of non-literal utterances without modeling a\nspeaker's motivations for being non-literal. Combined with LLMs, it achieves\nstate-of-the-art performance on the ironic split of PragMega+, a new irony\ninterpretation dataset introduced in this study."}
{"id": "2506.09801", "pdf": "https://arxiv.org/pdf/2506.09801.pdf", "abs": "https://arxiv.org/abs/2506.09801", "title": "Investigating the Perception of Translational Shape-Changing Haptic Interfaces", "authors": ["Qihan Yang", "Xin Zhou", "Adam J. Spiers"], "categories": ["cs.HC"], "comment": "7 pages, 8 figures. Accepted version to appear in: Proceedings of the\n  IEEE World Haptics Conference (WHC), 2025", "summary": "Shape-changing haptic interfaces (SCHIs) are a promising and emerging field.\nHowever, compared to more established stimulus modalities, such as vibration,\nthere is sparse literature on the perception of dynamic shapes. Furthermore,\nthe influence of properties such as grasp types and displacement\nmagnitude/direction has not been formally evaluated. This work attempts to\ninitiate a formal perceptual evaluation of SCHIs via a psychophysical user\nstudy involving a 1-DOF translational shape-changing interface that can move\nits body with 1.25-micrometer resolution. Participants completed a Method of\nConstant Stimulus study while holding the device with three different grasps.\nStimuli direction occurred both toward and away from the thumb, while the\nstandard stimuli varied between small (0.48 mm) and large (6 mm). Our results\nindicate that translational SCHIs should maximize the translation magnitude\nrather than the number of fingers in contact. We also demonstrated how to apply\nour findings to real-world applications via a simple 'paddle game', where we\ncompared conventional linear mapping with non-linear mapping derived from our\nperceptual experiment outcomes between the device position and its represented\nvalue. Results indicate that the non-linear mapping was more effective, with\nimproved error distribution. We hope this work inspires further formal\nperceptual investigation into other SCHI morphologies."}
{"id": "2506.09315", "pdf": "https://arxiv.org/pdf/2506.09315.pdf", "abs": "https://arxiv.org/abs/2506.09315", "title": "Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models", "authors": ["Yao Xiao", "Heidi Christensen", "Stefan Goetze"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "To be published in the proceedings of Interspeech 2025", "summary": "Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive\ndecline that commonly impacts language ability. This work extends the paired\nperplexity approach to detecting AD by using a recent large language model\n(LLM), the instruction-following version of Mistral-7B. We improve accuracy by\nan average of 3.33% over the best current paired perplexity method and by 6.35%\nover the top-ranked method from the ADReSS 2020 challenge benchmark. Our\nfurther analysis demonstrates that the proposed approach can effectively detect\nAD with a clear and interpretable decision boundary in contrast to other\nmethods that suffer from opaque decision-making processes. Finally, by\nprompting the fine-tuned LLMs and comparing the model-generated responses to\nhuman responses, we illustrate that the LLMs have learned the special language\npatterns of AD speakers, which opens up possibilities for novel methods of\nmodel interpretation and data augmentation."}
{"id": "2506.09968", "pdf": "https://arxiv.org/pdf/2506.09968.pdf", "abs": "https://arxiv.org/abs/2506.09968", "title": "SRLAgent: Enhancing Self-Regulated Learning Skills through Gamification and LLM Assistance", "authors": ["Wentao Ge", "Yuqing Sun", "Ziyan Wang", "Haoyue Zheng", "Weiyang He", "Piaohong Wang", "Qianyu Zhu", "Benyou Wang"], "categories": ["cs.HC", "I.2.1; I.2.6"], "comment": "14 pages", "summary": "Self-regulated learning (SRL) is crucial for college students navigating\nincreased academic demands and independence. Insufficient SRL skills can lead\nto disorganized study habits, low motivation, and poor time management,\nundermining learners ability to thrive in challenging environments. Through a\nformative study involving 59 college students, we identified key challenges\nstudents face in developing SRL skills, including difficulties with\ngoal-setting, time management, and reflective learning. To address these\nchallenges, we introduce SRLAgent, an LLM-assisted system that fosters SRL\nskills through gamification and adaptive support from large language models\n(LLMs). Grounded in Zimmermans three-phase SRL framework, SRLAgent enables\nstudents to engage in goal-setting, strategy execution, and self-reflection\nwithin an interactive game-based environment. The system offers real-time\nfeedback and scaffolding powered by LLMs to support students independent study\nefforts. We evaluated SRLAgent using a between-subjects design, comparing it to\na baseline system (SRL without Agent features) and a traditional multimedia\nlearning condition. Results showed significant improvements in SRL skills\nwithin the SRLAgent group (p < .001, Cohens d = 0.234) and higher engagement\ncompared to the baselines. This work highlights the value of embedding SRL\nscaffolding and real-time AI support within gamified environments, offering\ndesign implications for educational technologies that aim to promote deeper\nlearning and metacognitive skill development."}
{"id": "2506.09329", "pdf": "https://arxiv.org/pdf/2506.09329.pdf", "abs": "https://arxiv.org/abs/2506.09329", "title": "Towards Efficient and Effective Alignment of Large Language Models", "authors": ["Yuxin Jiang"], "categories": ["cs.CL"], "comment": "PhD thesis", "summary": "Large language models (LLMs) exhibit remarkable capabilities across diverse\ntasks, yet aligning them efficiently and effectively with human expectations\nremains a critical challenge. This thesis advances LLM alignment by introducing\nnovel methodologies in data collection, training, and evaluation. We first\naddress alignment data collection. Existing approaches rely heavily on manually\ncurated datasets or proprietary models. To overcome these limitations, we\npropose Lion, an adversarial distillation framework that iteratively refines\ntraining data by identifying and generating challenging instructions, enabling\nstate-of-the-art zero-shot reasoning. Additionally, we introduce Web\nReconstruction (WebR), a fully automated framework that synthesizes\ninstruction-tuning data directly from raw web documents, significantly\nimproving data diversity and scalability over existing synthetic data methods.\nNext, we enhance alignment training through novel optimization techniques. We\ndevelop Learning to Edit (LTE), a framework that enables LLMs to efficiently\nintegrate new knowledge while preserving existing information. LTE leverages\nmeta-learning to improve both real-time and batch knowledge updates.\nFurthermore, we introduce Bridging and Modeling Correlations (BMC), a\nrefinement of Direct Preference Optimization (DPO) that explicitly captures\ntoken-level correlations in preference data, leading to superior alignment\nacross QA and mathematical reasoning tasks. Finally, we tackle the challenge of\nevaluating alignment. Existing benchmarks emphasize response quality but\noverlook adherence to specific constraints. To bridge this gap, we introduce\nFollowBench, a multi-level, fine-grained benchmark assessing LLMs' ability to\nfollow complex constraints across diverse instruction types. Our results expose\nkey weaknesses in current models' constraint adherence, offering insights for\nfuture improvements."}
{"id": "2506.09054", "pdf": "https://arxiv.org/pdf/2506.09054.pdf", "abs": "https://arxiv.org/abs/2506.09054", "title": "Particle Builder -- Learn about the Standard Model while playing against an AI", "authors": ["Mohammad Attar", "Andrew Carse", "Yeming Chen", "Thomas Green", "Jeong-Yeon Ha", "Yanbai Jin", "Amy McWilliams", "Theirry Panggabean", "Zhengyu Peng", "Lujin Sun", "Jing Ru", "Jiacheng She", "Jialin Wang", "Zilun Wei", "Jiayuan Zhu", "Lachlan McGinness"], "categories": ["physics.ed-ph", "cs.HC"], "comment": "This demo has been accepted for presentation at the AIED 2025\n  Interactive Events Track", "summary": "Particle Builder Online is a web-based education game designed for high\nschool physics students. Students can play against an AI opponent or peers to\nfamiliarise themselves with the Standard Model of Particle Physics. The game is\naimed at a high school level and tailored to the International Baccalaureate\nand the Australian Curriculum. Students from four schools in Canberra took\npre/post-tests and a survey while completing a lesson where they played\nParticle Builder. Students' understanding of particle physics concepts improved\nsignificantly. Students found the game more enjoyable and effective than\nregular classroom lessons."}
{"id": "2506.09331", "pdf": "https://arxiv.org/pdf/2506.09331.pdf", "abs": "https://arxiv.org/abs/2506.09331", "title": "Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation", "authors": ["Arjun Vaithilingam Sudhakar"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "arXiv admin note: substantial text overlap with arXiv:2311.07687", "summary": "Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot\ngeneralization capabilities across complex natural language tasks, enabling\ntheir widespread use as virtual assistants for diverse applications such as\ntranslation and summarization. Despite being trained solely on large corpora of\ntext without explicit supervision on author intent, LLMs appear to infer the\nunderlying meaning of textual interactions. This raises a fundamental question:\ncan LLMs model and reason about the intentions of others, i.e., do they possess\na form of theory of mind? Understanding other's intentions is crucial for\neffective collaboration, which underpins human societal success and is\nessential for cooperative interactions among multiple agents, including humans\nand autonomous systems. In this work, we investigate the theory of mind in LLMs\nthrough the lens of cooperative multi-agent reinforcement learning (MARL),\nwhere agents learn to collaborate via repeated interactions, mirroring human\nsocial reasoning. Our approach aims to enhance artificial agent's ability to\nadapt and cooperate with both artificial and human partners. By leveraging\nLLM-based agents capable of natural language interaction, we move towards\ncreating hybrid human-AI systems that can foster seamless collaboration, with\nbroad implications for the future of human-artificial interaction."}
{"id": "2506.09073", "pdf": "https://arxiv.org/pdf/2506.09073.pdf", "abs": "https://arxiv.org/abs/2506.09073", "title": "Understanding and Improving Data Repurposing", "authors": ["J. Parsons", "R. Lukyanenko", "B. Greenwood", "C. Cooper"], "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "We live in an age of unprecedented opportunities to use existing data for\ntasks not anticipated when those data were collected, resulting in widespread\ndata repurposing. This commentary defines and maps the scope of data\nrepurposing to highlight its importance for organizations and society and the\nneed to study data repurposing as a frontier of data management. We explain how\nrepurposing differs from original data use and data reuse and then develop a\nframework for data repurposing consisting of concepts and activities for\nadapting existing data to new tasks. The framework and its implications are\nillustrated using two examples of repurposing, one in healthcare and one in\ncitizen science. We conclude by suggesting opportunities for research to better\nunderstand data repurposing and enable more effective data repurposing\npractices."}
{"id": "2506.09340", "pdf": "https://arxiv.org/pdf/2506.09340.pdf", "abs": "https://arxiv.org/abs/2506.09340", "title": "RePO: Replay-Enhanced Policy Optimization", "authors": ["Siheng Li", "Zhanhui Zhou", "Wai Lam", "Chao Yang", "Chaochao Lu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project Page: https://github.com/SihengLi99/RePO", "summary": "Reinforcement learning (RL) is vital for optimizing large language models\n(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages\nusing multiple on-policy outputs per prompt, leading to high computational\ncosts and low data efficiency. To address this, we introduce Replay-Enhanced\nPolicy Optimization (RePO), which leverages diverse replay strategies to\nretrieve off-policy samples from a replay buffer, allowing policy optimization\nbased on a broader and more diverse set of samples for each prompt. Experiments\non five LLMs across seven mathematical reasoning benchmarks demonstrate that\nRePO achieves absolute average performance gains of $18.4$ and $4.1$ points for\nQwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further\nanalysis indicates that RePO increases computational cost by $15\\%$ while\nraising the number of effective optimization steps by $48\\%$ for Qwen3-1.7B,\nwith both on-policy and off-policy sample numbers set to $8$. The repository\ncan be accessed at https://github.com/SihengLi99/RePO."}
{"id": "2506.09160", "pdf": "https://arxiv.org/pdf/2506.09160.pdf", "abs": "https://arxiv.org/abs/2506.09160", "title": "Understanding Human-AI Trust in Education", "authors": ["Griffin Pitts", "Sanaz Motamedi"], "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.HC"], "comment": null, "summary": "As AI chatbots become increasingly integrated in education, students are\nturning to these systems for guidance, feedback, and information. However, the\nanthropomorphic characteristics of these chatbots create ambiguity regarding\nwhether students develop trust toward them as they would a human peer or\ninstructor, based in interpersonal trust, or as they would any other piece of\ntechnology, based in technology trust. This ambiguity presents theoretical\nchallenges, as interpersonal trust models may inappropriately ascribe human\nintentionality and morality to AI, while technology trust models were developed\nfor non-social technologies, leaving their applicability to anthropomorphic\nsystems unclear. To address this gap, we investigate how human-like and\nsystem-like trusting beliefs comparatively influence students' perceived\nenjoyment, trusting intention, behavioral intention to use, and perceived\nusefulness of an AI chatbot - factors associated with students' engagement and\nlearning outcomes. Through partial least squares structural equation modeling,\nwe found that human-like and system-like trust significantly influenced student\nperceptions, with varied effects. Human-like trust more strongly predicted\ntrusting intention, while system-like trust better predicted behavioral\nintention and perceived usefulness. Both had similar effects on perceived\nenjoyment. Given the partial explanatory power of each type of trust, we\npropose that students develop a distinct form of trust with AI chatbots\n(human-AI trust) that differs from human-human and human-technology models of\ntrust. Our findings highlight the need for new theoretical frameworks specific\nto human-AI trust and offer practical insights for fostering appropriately\ncalibrated trust, which is critical for the effective adoption and pedagogical\nimpact of AI in education."}
{"id": "2506.09342", "pdf": "https://arxiv.org/pdf/2506.09342.pdf", "abs": "https://arxiv.org/abs/2506.09342", "title": "Latent Multi-Head Attention for Small Language Models", "authors": ["Sushant Mehta", "Raj Dandekar", "Rajat Dandekar", "Sreedath Panat"], "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 1 figure. 5 tables", "summary": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance."}
{"id": "2506.09420", "pdf": "https://arxiv.org/pdf/2506.09420.pdf", "abs": "https://arxiv.org/abs/2506.09420", "title": "A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy", "authors": ["Henry Peng Zou", "Wei-Chieh Huang", "Yaozu Wu", "Chunyu Miao", "Dongyuan Li", "Aiwei Liu", "Yue Zhou", "Yankai Chen", "Weizhi Zhang", "Yangning Li", "Liancheng Fang", "Renhe Jiang", "Philip S. Yu"], "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG", "cs.MA"], "comment": null, "summary": "Recent improvements in large language models (LLMs) have led many researchers\nto focus on building fully autonomous AI agents. This position paper questions\nwhether this approach is the right path forward, as these autonomous systems\nstill have problems with reliability, transparency, and understanding the\nactual requirements of human. We suggest a different approach: LLM-based\nHuman-Agent Systems (LLM-HAS), where AI works with humans rather than replacing\nthem. By keeping human involved to provide guidance, answer questions, and\nmaintain control, these systems can be more trustworthy and adaptable. Looking\nat examples from healthcare, finance, and software development, we show how\nhuman-AI teamwork can handle complex tasks better than AI working alone. We\nalso discuss the challenges of building these collaborative systems and offer\npractical solutions. This paper argues that progress in AI should not be\nmeasured by how independent systems become, but by how well they can work with\nhumans. The most promising future for AI is not in systems that take over human\nroles, but in those that enhance human capabilities through meaningful\npartnership."}
{"id": "2506.09349", "pdf": "https://arxiv.org/pdf/2506.09349.pdf", "abs": "https://arxiv.org/abs/2506.09349", "title": "OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution Speech Representations and Contrastive Alignment", "authors": ["Chao-Hong Tan", "Qian Chen", "Wen Wang", "Chong Deng", "Qinglin Zhang", "Luyao Cheng", "Hai Yu", "Xin Zhang", "Xiang Lv", "Tianyu Zhao", "Chong Zhang", "Yukun Ma", "Yafeng Chen", "Hui Wang", "Jiaqing Liu", "Jieping Ye"], "categories": ["cs.CL"], "comment": null, "summary": "Recent studies on end-to-end speech generation with large language models\n(LLMs) have attracted significant community attention, with multiple works\nextending text-based LLMs to generate discrete speech tokens. Existing\napproaches primarily fall into two categories: (1) Methods that generate\ndiscrete speech tokens independently without incorporating them into the LLM's\nautoregressive process, resulting in text generation being unaware of\nconcurrent speech synthesis. (2) Models that generate interleaved or parallel\nspeech-text tokens through joint autoregressive modeling, enabling mutual\nmodality awareness during generation. This paper presents OmniDRCA, a parallel\nspeech-text foundation model based on joint autoregressive modeling, featuring\ndual-resolution speech representations and contrastive cross-modal alignment.\nOur approach processes speech and text representations in parallel while\nenhancing audio comprehension through contrastive alignment. Experimental\nresults on Spoken Question Answering benchmarks demonstrate that OmniDRCA\nestablishes new state-of-the-art (SOTA) performance among parallel joint\nspeech-text modeling based foundation models, and achieves competitive\nperformance compared to interleaved models. Additionally, we explore the\npotential of extending the framework to full-duplex conversational scenarios."}
{"id": "2506.09707", "pdf": "https://arxiv.org/pdf/2506.09707.pdf", "abs": "https://arxiv.org/abs/2506.09707", "title": "Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements", "authors": ["Suhas BN", "Andrew M. Sherrill", "Jyoti Alaparthi", "Dominik Mattioli", "Rosa I. Arriaga", "Chris W. Wiese", "Saeed Abdullah"], "categories": ["eess.AS", "cs.CL", "cs.HC", "68T07", "I.2.7; I.5.4; H.5.2"], "comment": "5 pages, 2 figures", "summary": "Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic\nstress disorder (PTSD), but evaluating therapist fidelity remains\nlabor-intensive due to the need for manual review of session recordings. We\npresent a method for the automatic temporal localization of key PE fidelity\nelements -- identifying their start and stop times -- directly from session\naudio and transcripts. Our approach fine-tunes a large pre-trained\naudio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process\nfocused 30-second windows of audio-transcript input. Fidelity labels for three\ncore protocol phases -- therapist orientation (P1), imaginal exposure (P2), and\npost-imaginal processing (P3) -- are generated via LLM-based prompting and\nverified by trained raters. The model is trained to predict normalized boundary\noffsets using soft supervision guided by task-specific prompts. On a dataset of\n313 real PE sessions, our best configuration (LoRA rank 8, 30s windows)\nachieves a mean absolute error (MAE) of 5.3 seconds across tasks. We further\nanalyze the effects of window size and LoRA rank, highlighting the importance\nof context granularity and model adaptation. This work introduces a scalable\nframework for fidelity tracking in PE therapy, with potential to support\nclinician training, supervision, and quality assurance."}
{"id": "2506.09351", "pdf": "https://arxiv.org/pdf/2506.09351.pdf", "abs": "https://arxiv.org/abs/2506.09351", "title": "DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts", "authors": ["Yuchen Feng", "Bowen Shen", "Naibin Gu", "Jiaxuan Zhao", "Peng Fu", "Zheng Lin", "Weiping Wang"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture\nachieve high cost-efficiency by selectively activating a subset of the\nparameters. Despite the inference efficiency of MoE LLMs, the training of\nextensive experts from scratch incurs substantial overhead, whereas\nreconstructing a dense LLM into an MoE LLM significantly reduces the training\nbudget. However, existing reconstruction methods often overlook the diversity\namong experts, leading to potential redundancy. In this paper, we come up with\nthe observation that a specific LLM exhibits notable diversity after being\npruned on different calibration datasets, based on which we present a\nDiversity-Enhanced reconstruction method named DIVE. The recipe of DIVE\nincludes domain affinity mining, pruning-based expert reconstruction, and\nefficient retraining. Specifically, the reconstruction includes pruning and\nreassembly of the feed-forward network (FFN) module. After reconstruction, we\nefficiently retrain the model on routers, experts and normalization modules. We\nimplement DIVE on Llama-style LLMs with open-source training corpora.\nExperiments show that DIVE achieves training efficiency with minimal accuracy\ntrade-offs, outperforming existing pruning and MoE reconstruction methods with\nthe same number of activated parameters."}
{"id": "2506.09873", "pdf": "https://arxiv.org/pdf/2506.09873.pdf", "abs": "https://arxiv.org/abs/2506.09873", "title": "Stakeholder Participation for Responsible AI Development: Disconnects Between Guidance and Current Practice", "authors": ["Emma Kallina", "Thomas Bohné", "Jat Singh"], "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": "Published at the 2025 ACM Conference on Fairness, Accountability, and\n  Transparency FAccT'25", "summary": "Responsible AI (rAI) guidance increasingly promotes stakeholder involvement\n(SHI) during AI development. At the same time, SHI is already common in\ncommercial software development, but with potentially different foci. This\nstudy clarifies the extent to which established SHI practices are able to\ncontribute to rAI efforts as well as potential disconnects -- essential\ninsights to inform and tailor future interventions that further shift industry\npractice towards rAI efforts. First, we analysed 56 rAI guidance documents to\nidentify why SHI is recommended (i.e. its expected benefits for rAI) and\nuncovered goals such as redistributing power, improving socio-technical\nunderstandings, anticipating risks, and enhancing public oversight. To\nunderstand why and how SHI is currently practised in commercial settings, we\nthen conducted an online survey (n=130) and semi-structured interviews (n=10)\nwith AI practitioners. Our findings reveal that SHI in practice is primarily\ndriven by commercial priorities (e.g. customer value, compliance) and several\nfactors currently discourage more rAI-aligned SHI practices. This suggests that\nestablished SHI practices are largely not contributing to rAI efforts. To\naddress this disconnect, we propose interventions and research opportunities to\nadvance rAI development in practice."}
{"id": "2506.09359", "pdf": "https://arxiv.org/pdf/2506.09359.pdf", "abs": "https://arxiv.org/abs/2506.09359", "title": "Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL", "authors": ["Qingyun Zeng", "Simin Ma", "Arash Niknafs", "Ashish Basran", "Carol Szabo"], "categories": ["cs.CL"], "comment": "8 pages", "summary": "The rise of Large Language Models (LLMs) has significantly advanced\nText-to-SQL (NL2SQL) systems, yet evaluating the semantic equivalence of\ngenerated SQL remains a challenge, especially given ambiguous user queries and\nmultiple valid SQL interpretations. This paper explores using LLMs to assess\nboth semantic and a more practical \"weak\" semantic equivalence. We analyze\ncommon patterns of SQL equivalence and inequivalence, discuss challenges in\nLLM-based evaluation."}
{"id": "2409.13058", "pdf": "https://arxiv.org/pdf/2409.13058.pdf", "abs": "https://arxiv.org/abs/2409.13058", "title": "Mixed Reality Tele-Ultrasound over 750 km: A Feasibility Study", "authors": ["Ryan Yeung", "David Black", "Patrick B. Chen", "Victoria Lessoway", "Janice Reid", "Sergio Rangel-Suarez", "Silvia D. Chang", "Septimiu E. Salcudean"], "categories": ["cs.HC", "cs.RO"], "comment": "8 pages, 11 figures", "summary": "To address the lack of access to ultrasound in remote communities, previous\nwork introduced human teleoperation, a mixed reality and haptics-based\ntele-ultrasound system. In this approach, a novice takes the role of a\ncognitive robot controlled remotely by an expert through mixed reality. In this\nmanuscript we summarize new developments to this system and describe a\nfeasibility study assessing its use for long-distance remote abdominal\nultrasound examinations. To provide simple but effective haptic feedback, we\nused an ellipsoid model of the patient with its parameters calibrated using our\nsystem's position and force sensors. We tested the system in Skidegate, Haida\nGwaii, Canada, with the experts positioned 754 km away in Vancouver, Canada. We\nperformed 11 total scans with 10 novices and 2 sonographers. The sonographers\nwere tasked with acquiring 5 target images in the epigastric region. The image\nacquisition quality was assessed by 2 radiologists. We collected alignment data\nand the novices completed task load and usability questionnaires. Both the\nnovices and sonographers provided written and verbal feedback to inform future\ndesign iterations. 92% of the acquired images had sufficient quality for\ninterpretation by both radiologists. The mean task load reported by the novices\nwas below reference values reported in literature and the usability was\nunanimously positive. No correlation was found between image quality and the\nfollower's alignment error with the virtual transducer. Overall, we show that\nhuman teleoperation enables sonographers to perform remote abdominal ultrasound\nimaging with high performance, even across large distances and with novice\nfollowers. Future work will compare human teleoperation to conventional,\nrobotic and tele-mentored ultrasound."}
{"id": "2506.09367", "pdf": "https://arxiv.org/pdf/2506.09367.pdf", "abs": "https://arxiv.org/abs/2506.09367", "title": "COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content", "authors": ["Zhengyuan Liu", "Stella Xin Yin", "Dion Hoe-Lian Goh", "Nancy F. Chen"], "categories": ["cs.CL", "cs.AI"], "comment": "BEA 2025", "summary": "While Generative AI has demonstrated strong potential and versatility in\ncontent generation, its application to educational contexts presents several\nchallenges. Models often fail to align with curriculum standards and maintain\ngrade-appropriate reading levels consistently. Furthermore, STEM education\nposes additional challenges in balancing scientific explanations with everyday\nlanguage when introducing complex and abstract ideas and phenomena to younger\nstudents. In this work, we propose COGENT, a curriculum-oriented framework for\ngenerating grade-appropriate educational content. We incorporate three\ncurriculum components (science concepts, core ideas, and learning objectives),\ncontrol readability through length, vocabulary, and sentence complexity, and\nadopt a ``wonder-based'' approach to increase student engagement and interest.\nWe conduct a multi-dimensional evaluation via both LLM-as-a-judge and human\nexpert analysis. Experimental results show that COGENT consistently produces\ngrade-appropriate passages that are comparable or superior to human references.\nOur work establishes a viable approach for scaling adaptive and high-quality\nlearning resources."}
{"id": "2411.03137", "pdf": "https://arxiv.org/pdf/2411.03137.pdf", "abs": "https://arxiv.org/abs/2411.03137", "title": "From Pen to Prompt: How Creative Writers Integrate AI into their Writing Practice", "authors": ["Alicia Guo", "Shreya Sathyanarayanan", "Leijie Wang", "Jeffrey Heer", "Amy Zhang"], "categories": ["cs.HC"], "comment": null, "summary": "Creative writing is a deeply human craft, yet AI systems using large language\nmodels (LLMs) offer the automation of significant parts of the writing process.\nSo why do some creative writers choose to use AI? Through interviews and\nobserved writing sessions with 18 creative writers who already use AI regularly\nin their writing practice, we find that creative writers are intentional about\nhow they incorporate AI, making many deliberate decisions about when and how to\nengage AI based on their core values, such as authenticity and craftsmanship.\nWe characterize the interplay between writers' values, their fluid\nrelationships with AI, and specific integration strategies -- ultimately\nenabling writers to create new AI workflows without compromising their creative\nvalues. We provide insight for writing communities, AI developers and future\nresearchers on the importance of supporting transparency of these emerging\nwriting processes and rethinking what AI features can best serve writers."}
{"id": "2506.09375", "pdf": "https://arxiv.org/pdf/2506.09375.pdf", "abs": "https://arxiv.org/abs/2506.09375", "title": "CoLMbo: Speaker Language Model for Descriptive Profiling", "authors": ["Massa Baali", "Shuo Han", "Syed Abdul Hannan", "Purusottam Samal", "Karanveer Singh", "Soham Deshmukh", "Rita Singh", "Bhiksha Raj"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Speaker recognition systems are often limited to classification tasks and\nstruggle to generate detailed speaker characteristics or provide context-rich\ndescriptions. These models primarily extract embeddings for speaker\nidentification but fail to capture demographic attributes such as dialect,\ngender, and age in a structured manner. This paper introduces CoLMbo, a Speaker\nLanguage Model (SLM) that addresses these limitations by integrating a speaker\nencoder with prompt-based conditioning. This allows for the creation of\ndetailed captions based on speaker embeddings. CoLMbo utilizes user-defined\nprompts to adapt dynamically to new speaker characteristics and provides\ncustomized descriptions, including regional dialect variations and age-related\ntraits. This innovative approach not only enhances traditional speaker\nprofiling but also excels in zero-shot scenarios across diverse datasets,\nmarking a significant advancement in the field of speaker recognition."}
{"id": "2412.06085", "pdf": "https://arxiv.org/pdf/2412.06085.pdf", "abs": "https://arxiv.org/abs/2412.06085", "title": "From Simple Sensors to Complex Context: Insights for HabiTech", "authors": ["Albrecht Kurze", "Karola Köpferl"], "categories": ["cs.HC"], "comment": "CHI24 Extended Abstracts, Workshop HabiTech, May 11, 2024, Honolulu,\n  HI, USA 2024. 4 pages, 5 figures", "summary": "We relate our previous as well as ongoing research in the domain of smart\nhomes to the concept of HabiTech. HabiTech can benefit from existing approaches\nand findings in a broader context of whole buildings or communities within.\nAlong with data comes context of data capture and data interpretation in\ndifferent dimensions (spatial, temporal, social). For defining what is\n'community' proximity plays a crucial role in context, both spatially as well\nas socially. A participatory approach for research in living in sensing\nenvironments is promising to address complexity as well as interests of\ndifferent stakeholders. Often it is the complex context that makes even simple\nsensor data sensitive, i.e. in terms of privacy. When it comes to handle shared\ndata then concepts from the physical world for shared spaces might be related\nback to the data domain."}
{"id": "2506.09381", "pdf": "https://arxiv.org/pdf/2506.09381.pdf", "abs": "https://arxiv.org/abs/2506.09381", "title": "Binary classification for perceived quality of headlines and links on worldwide news websites, 2018-2024", "authors": ["Austin McCutcheon", "Thiago E. A. de Oliveira", "Aleksandr Zheleznov", "Chris Brogly"], "categories": ["cs.CL"], "comment": null, "summary": "The proliferation of online news enables potential widespread publication of\nperceived low-quality news headlines/links. As a result, we investigated\nwhether it was possible to automatically distinguish perceived lower-quality\nnews headlines/links from perceived higher-quality headlines/links. We\nevaluated twelve machine learning models on a binary, balanced dataset of\n57,544,214 worldwide news website links/headings from 2018-2024 (28,772,107 per\nclass) with 115 extracted linguistic features. Binary labels for each text were\nderived from scores based on expert consensus regarding the respective news\ndomain quality. Traditional ensemble methods, particularly the bagging\nclassifier, had strong performance (88.1% accuracy, 88.3% F1, 80/20 train/test\nsplit). Fine-tuned DistilBERT achieved the highest accuracy (90.3%, 80/20\ntrain/test split) but required more training time. The results suggest that\nboth NLP features with traditional classifiers and deep learning models can\neffectively differentiate perceived news headline/link quality, with some\ntrade-off between predictive performance and train time."}
{"id": "2502.09101", "pdf": "https://arxiv.org/pdf/2502.09101.pdf", "abs": "https://arxiv.org/abs/2502.09101", "title": "Bridging the Gap Between LLMs and Human Intentions: Progresses and Challenges in Instruction Understanding, Intention Reasoning, and Reliable Generation", "authors": ["Zongyu Chang", "Feihong Lu", "Ziqin Zhu", "Qian Li", "Cheng Ji", "Zhuo Chen", "Hao Peng", "Yang Liu", "Ruifeng Xu", "Yangqiu Song", "Shangguang Wang", "Jianxin Li"], "categories": ["cs.HC"], "comment": "19 pages, 11 figures", "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\nunderstanding and generation. However, when interacting with human instructions\nin real-world scenarios, LLMs still face significant challenges, particularly\nin accurately capturing and comprehending human instructions and intentions.\nThis paper focuses on three challenges in LLM-based text generation tasks:\ninstruction understanding, intention reasoning, and Reliable Dialog Generation.\nRegarding human complex instruction, LLMs have deficiencies in understanding\nlong contexts and instructions in multi-round conversations. For intention\nreasoning, LLMs may have inconsistent command reasoning, difficulty reasoning\nabout commands containing incorrect information, difficulty understanding user\nambiguous language commands, and a weak understanding of user intention in\ncommands. Besides, In terms of Reliable Dialog Generation, LLMs may have\nunstable generated content and unethical generation. To this end, we classify\nand analyze the performance of LLMs in challenging scenarios and conduct a\ncomprehensive evaluation of existing solutions. Furthermore, we introduce\nbenchmarks and categorize them based on the aforementioned three core\nchallenges. Finally, we explore potential directions for future research to\nenhance the reliability and adaptability of LLMs in real-world applications."}
{"id": "2506.09391", "pdf": "https://arxiv.org/pdf/2506.09391.pdf", "abs": "https://arxiv.org/abs/2506.09391", "title": "Comparing human and LLM politeness strategies in free production", "authors": ["Haoran Zhao", "Robert D. Hawkins"], "categories": ["cs.CL"], "comment": "25 pages, 5 figures", "summary": "Polite speech poses a fundamental alignment challenge for large language\nmodels (LLMs). Humans deploy a rich repertoire of linguistic strategies to\nbalance informational and social goals -- from positive approaches that build\nrapport (compliments, expressions of interest) to negative strategies that\nminimize imposition (hedging, indirectness). We investigate whether LLMs employ\na similarly context-sensitive repertoire by comparing human and LLM responses\nin both constrained and open-ended production tasks. We find that larger models\n($\\ge$70B parameters) successfully replicate key preferences from the\ncomputational pragmatics literature, and human evaluators surprisingly prefer\nLLM-generated responses in open-ended contexts. However, further linguistic\nanalyses reveal that models disproportionately rely on negative politeness\nstrategies even in positive contexts, potentially leading to\nmisinterpretations. While modern LLMs demonstrate an impressive handle on\npoliteness strategies, these subtle differences raise important questions about\npragmatic alignment in AI systems."}
{"id": "2407.01067", "pdf": "https://arxiv.org/pdf/2407.01067.pdf", "abs": "https://arxiv.org/abs/2407.01067", "title": "Human-like object concept representations emerge naturally in multimodal large language models", "authors": ["Changde Du", "Kaicheng Fu", "Bincheng Wen", "Yi Sun", "Jie Peng", "Wei Wei", "Ying Gao", "Shengpei Wang", "Chuncheng Zhang", "Jinpeng Li", "Shuang Qiu", "Le Chang", "Huiguang He"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC", "cs.LG"], "comment": "Published on Nature Machine Intelligence", "summary": "Understanding how humans conceptualize and categorize natural objects offers\ncritical insights into perception and cognition. With the advent of Large\nLanguage Models (LLMs), a key question arises: can these models develop\nhuman-like object representations from linguistic and multimodal data? In this\nstudy, we combined behavioral and neuroimaging analyses to explore the\nrelationship between object concept representations in LLMs and human\ncognition. We collected 4.7 million triplet judgments from LLMs and Multimodal\nLLMs (MLLMs) to derive low-dimensional embeddings that capture the similarity\nstructure of 1,854 natural objects. The resulting 66-dimensional embeddings\nwere stable, predictive, and exhibited semantic clustering similar to human\nmental representations. Remarkably, the dimensions underlying these embeddings\nwere interpretable, suggesting that LLMs and MLLMs develop human-like\nconceptual representations of objects. Further analysis showed strong alignment\nbetween model embeddings and neural activity patterns in brain regions such as\nEBA, PPA, RSC, and FFA. This provides compelling evidence that the object\nrepresentations in LLMs, while not identical to human ones, share fundamental\nsimilarities that reflect key aspects of human conceptual knowledge. Our\nfindings advance the understanding of machine intelligence and inform the\ndevelopment of more human-like artificial cognitive systems."}
{"id": "2506.09393", "pdf": "https://arxiv.org/pdf/2506.09393.pdf", "abs": "https://arxiv.org/abs/2506.09393", "title": "A Hierarchical Probabilistic Framework for Incremental Knowledge Tracing in Classroom Settings", "authors": ["Xinyi Gao", "Qiucheng Wu", "Yang Zhang", "Xuechen Liu", "Kaizhi Qian", "Ying Xu", "Shiyu Chang"], "categories": ["cs.CL"], "comment": "24 pages, 4 figures", "summary": "Knowledge tracing (KT) aims to estimate a student's evolving knowledge state\nand predict their performance on new exercises based on performance history.\nMany realistic classroom settings for KT are typically low-resource in data and\nrequire online updates as students' exercise history grows, which creates\nsignificant challenges for existing KT approaches. To restore strong\nperformance under low-resource conditions, we revisit the hierarchical\nknowledge concept (KC) information, which is typically available in many\nclassroom settings and can provide strong prior when data are sparse. We\ntherefore propose Knowledge-Tree-based Knowledge Tracing (KT$^2$), a\nprobabilistic KT framework that models student understanding over a\ntree-structured hierarchy of knowledge concepts using a Hidden Markov Tree\nModel. KT$^2$ estimates student mastery via an EM algorithm and supports\npersonalized prediction through an incremental update mechanism as new\nresponses arrive. Our experiments show that KT$^2$ consistently outperforms\nstrong baselines in realistic online, low-resource settings."}
{"id": "2502.16794", "pdf": "https://arxiv.org/pdf/2502.16794.pdf", "abs": "https://arxiv.org/abs/2502.16794", "title": "AAD-LLM: Neural Attention-Driven Auditory Scene Understanding", "authors": ["Xilin Jiang", "Sukru Samet Dindar", "Vishal Choudhari", "Stephan Bickel", "Ashesh Mehta", "Guy M McKhann", "Daniel Friedman", "Adeen Flinker", "Nima Mesgarani"], "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.HC", "eess.AS"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Auditory foundation models, including auditory large language models (LLMs),\nprocess all sound inputs equally, independent of listener perception. However,\nhuman auditory perception is inherently selective: listeners focus on specific\nspeakers while ignoring others in complex auditory scenes. Existing models do\nnot incorporate this selectivity, limiting their ability to generate\nperception-aligned responses. To address this, we introduce Intention-Informed\nAuditory Scene Understanding (II-ASU) and present Auditory Attention-Driven LLM\n(AAD-LLM), a prototype system that integrates brain signals to infer listener\nattention. AAD-LLM extends an auditory LLM by incorporating intracranial\nelectroencephalography (iEEG) recordings to decode which speaker a listener is\nattending to and refine responses accordingly. The model first predicts the\nattended speaker from neural activity, then conditions response generation on\nthis inferred attentional state. We evaluate AAD-LLM on speaker description,\nspeech transcription and extraction, and question answering in multitalker\nscenarios, with both objective and subjective ratings showing improved\nalignment with listener intention. By taking a first step toward\nintention-aware auditory AI, this work explores a new paradigm where listener\nperception informs machine listening, paving the way for future\nlistener-centered auditory systems. Demo and code available:\nhttps://aad-llm.github.io."}
{"id": "2506.09408", "pdf": "https://arxiv.org/pdf/2506.09408.pdf", "abs": "https://arxiv.org/abs/2506.09408", "title": "Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models", "authors": ["Jui-Ming Yao", "Hao-Yuan Chen", "Zi-Xian Tang", "Bing-Jia Tan", "Sheng-Wei Peng", "Bing-Cheng Xie", "Shun-Feng Su"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive performance on\nmultiple-choice question answering (MCQA) benchmarks, yet they remain highly\nvulnerable to minor input perturbations. In this paper, we introduce and\nevaluate Token Constraint Decoding (TCD). This simple yet effective\ninference-time algorithm enforces alignment between token-level predictions to\nenhance robustness in noisy settings. Through extensive experiments on\nCommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired\nwith prompt engineering (PE) fixes, significantly restores performance degraded\nby input noise, yielding up to +39\\% absolute gains for weaker models like\nGemma3 1B. Penalty sweep analyses further reveal that TCD implicitly\nregularizes overconfident outputs, with different models requiring distinct\npenalty schedules to maximize resilience. Our findings establish TCD as a\npractical, model-agnostic approach for improving reasoning stability under\nreal-world imperfections and pave the way for more reliable deployment of LLMs\nin safety-critical or user-facing applications."}
{"id": "2506.09414", "pdf": "https://arxiv.org/pdf/2506.09414.pdf", "abs": "https://arxiv.org/abs/2506.09414", "title": "PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering", "authors": ["Xiujun Zhou", "Pingjian Zhang", "Deyou Tang"], "categories": ["cs.CL", "cs.IR"], "comment": "13 pages, 7 figures, 5 tables", "summary": "Knowledge Graph Question Answering (KGQA) is a crucial task in natural\nlanguage processing that requires reasoning over knowledge graphs (KGs) to\nanswer natural language questions. Recent methods utilizing large language\nmodels (LLMs) have shown remarkable semantic parsing capabilities but are\nlimited by the scarcity of diverse annotated data and multi-hop reasoning\nsamples. Traditional data augmentation approaches are focus mainly on\nsingle-hop questions and prone to semantic distortion, while LLM-based methods\nprimarily address semantic distortion but usually neglect multi-hop reasoning,\nthus limiting data diversity. The scarcity of multi-hop samples further weakens\nmodels' generalization. To address these issues, we propose PGDA-KGQA, a\nprompt-guided generative framework with multiple data augmentation strategies\nfor KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by\ncrafting meticulously engineered prompts that integrate the provided textual\ncontent, it leverages LLMs to generate large-scale (question, logical form)\npairs for model training. Specifically, PGDA-KGQA enriches its training set by:\n(1) generating single-hop pseudo questions to improve the alignment of question\nsemantics with KG relations; (2) applying semantic-preserving question\nrewriting to improve robustness against linguistic variations; (3) employing\nanswer-guided reverse path exploration to create realistic multi-hop questions.\nBy adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA\nutilizes the augmented data to enhance the accuracy of logical form generation\nand thus improve answer retrieval performance. Experiments demonstrate that\noutperforms state-of-the-art methods on standard KGQA datasets, achieving\nimprovements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by\n1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively."}
{"id": "2506.09424", "pdf": "https://arxiv.org/pdf/2506.09424.pdf", "abs": "https://arxiv.org/abs/2506.09424", "title": "Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings", "authors": ["Md Messal Monem Miah", "Adrita Anika", "Xi Shi", "Ruihong Huang"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Detecting deception in an increasingly digital world is both a critical and\nchallenging task. In this study, we present a comprehensive evaluation of the\nautomated deception detection capabilities of Large Language Models (LLMs) and\nLarge Multimodal Models (LMMs) across diverse domains. We assess the\nperformance of both open-source and commercial LLMs on three distinct datasets:\nreal life trial interviews (RLTD), instructed deception in interpersonal\nscenarios (MU3D), and deceptive reviews (OpSpam). We systematically analyze the\neffectiveness of different experimental setups for deception detection,\nincluding zero-shot and few-shot approaches with random or similarity-based\nin-context example selection. Our results show that fine-tuned LLMs achieve\nstate-of-the-art performance on textual deception detection tasks, while LMMs\nstruggle to fully leverage cross-modal cues. Additionally, we analyze the\nimpact of auxiliary features, such as non-verbal gestures and video summaries,\nand examine the effectiveness of different prompting strategies, including\ndirect label generation and chain-of-thought reasoning. Our findings provide\nkey insights into how LLMs process and interpret deceptive cues across\nmodalities, highlighting their potential and limitations in real-world\ndeception detection applications."}
{"id": "2506.09428", "pdf": "https://arxiv.org/pdf/2506.09428.pdf", "abs": "https://arxiv.org/abs/2506.09428", "title": "Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting", "authors": ["Fei Ding", "Baiqiao Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)'\ninstruction-following capabilities and domain-specific task adaptability, often\ndiminishes their general capabilities. Moreover, due to the inaccessibility of\noriginal pre-training data, catastrophic forgetting tends to be exacerbated\nwhen third-party practitioners implement SFT on open-sourced models. To address\nthis challenge, we propose a novel, more cost-effective SFT method which could\neffectively reduce the risk of catastrophic forgetting without access to\noriginal SFT data. Our approach begins by reconstructing the likely SFT\ninstruction distribution of the base model, followed by a multi-model screening\nprocess to select optimal data, which is then mixed with new data for SFT.\nExperimental results demonstrate that our method preserves generalization\ncapabilities in general domains while improving task-specific performance."}
{"id": "2506.09440", "pdf": "https://arxiv.org/pdf/2506.09440.pdf", "abs": "https://arxiv.org/abs/2506.09440", "title": "GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture", "authors": ["GigaChat team", "Mamedov Valentin", "Evgenii Kosarev", "Gregory Leleytner", "Ilya Shchuckin", "Valeriy Berezovskiy", "Daniil Smirnov", "Dmitry Kozlov", "Sergei Averkiev", "Lukyanenko Ivan", "Aleksandr Proshunin", "Ainur Israfilova", "Ivan Baskov", "Artem Chervyakov", "Emil Shakirov", "Mikhail Kolesov", "Daria Khomich", "Darya Latortseva", "Sergei Porkhun", "Yury Fedorov", "Oleg Kutuzov", "Polina Kudriavtseva", "Sofiia Soldatova", "Kolodin Egor", "Stanislav Pyatkin", "Dzmitry Menshykh", "Grafov Sergei", "Eldar Damirov", "Karlov Vladimir", "Ruslan Gaitukiev", "Arkadiy Shatenov", "Alena Fenogenova", "Nikita Savushkin", "Fedor Minkin"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL-2025 System Demo", "summary": "Generative large language models (LLMs) have become crucial for modern NLP\nresearch and applications across various languages. However, the development of\nfoundational models specifically tailored to the Russian language has been\nlimited, primarily due to the significant computational resources required.\nThis paper introduces the GigaChat family of Russian LLMs, available in various\nsizes, including base models and instruction-tuned versions. We provide a\ndetailed report on the model architecture, pre-training process, and\nexperiments to guide design choices. In addition, we evaluate their performance\non Russian and English benchmarks and compare GigaChat with multilingual\nanalogs. The paper presents a system demonstration of the top-performing models\naccessible via an API, a Telegram bot, and a Web interface. Furthermore, we\nhave released three open GigaChat models in open-source\n(https://huggingface.co/ai-sage), aiming to expand NLP research opportunities\nand support the development of industrial solutions for the Russian language."}
{"id": "2506.09450", "pdf": "https://arxiv.org/pdf/2506.09450.pdf", "abs": "https://arxiv.org/abs/2506.09450", "title": "UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs", "authors": ["Prameshwar Thiyagarajan", "Vaishnavi Parimi", "Shamant Sai", "Soumil Garg", "Zhangir Meirbek", "Nitin Yarlagadda", "Kevin Zhu", "Chris Kim"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Conference of the North American Chapter of the\n  Association for Computational Linguistics, Student Research Workshop 2025\n  (NAACL SRW 2025)", "summary": "Theory of Mind (ToM), the ability to understand the mental states of oneself\nand others, remains a challenging area for large language models (LLMs), which\noften fail to predict human mental states accurately. In this paper, we\nintroduce UniToMBench, a unified benchmark that integrates the strengths of\nSimToM and TOMBENCH to systematically improve and assess ToM capabilities in\nLLMs by integrating multi-interaction task designs and evolving story\nscenarios. Supported by a custom dataset of over 1,000 hand-written scenarios,\nUniToMBench combines perspective-taking techniques with diverse evaluation\nmetrics to better stimulate social cognition in LLMs. Through evaluation, we\nobserve that while models like GPT-4o and GPT-4o Mini show consistently high\naccuracy in tasks involving emotional and belief-related scenarios, with\nresults usually above 80%, there is significant variability in their\nperformance across knowledge-based tasks. These results highlight both the\nstrengths and limitations of current LLMs in ToM-related tasks, underscoring\nthe value of UniToMBench as a comprehensive tool for future development. Our\ncode is publicly available here:\nhttps://github.com/Shamant/unifiedtombenchmark."}
{"id": "2506.09457", "pdf": "https://arxiv.org/pdf/2506.09457.pdf", "abs": "https://arxiv.org/abs/2506.09457", "title": "Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms", "authors": ["Zeguan Xiao", "Yun Chen", "Guanhua Chen"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization\n(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient\nalternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms\nfor aligning large language models (LLMs) with human preferences. However, DAAs\nsuffer from a fundamental limitation we identify as the \"reward-generation gap\"\n-- a misalignment between optimization objectives during training and actual\ngeneration performance during inference. In this paper, we find a contributor\nto the reward-generation gap is the mismatch between the inherent importance of\nprefix tokens during the LLM generation process and how this importance is\nreflected in the implicit reward functions of DAAs. To bridge the gap, we\nintroduce a simple yet effective approach called Prefix-Oriented Equal-length\nTraining (POET), which truncates both preferred and dispreferred responses to\nmatch the shorter one's length. Training with POET, where both responses in\neach sample are truncated to equal length, resulting in diverse truncated\nlengths across samples, the optimization of DAAs objective is implicitly\nconstrained to converge across all positions, thus paying more attention to\nprefix tokens than the standard DAAs. We conduct experiments with DPO and\nSimPO, two representative DAAs, demonstrating that POET improves over their\nstandard implementations, achieving up to 15.6 points in AlpacaEval 2 and\noverall improvements across downstream tasks. Our results highlight the\nimportance of addressing the misalignment between reward optimization and\ngeneration performance in DAAs."}
{"id": "2506.09495", "pdf": "https://arxiv.org/pdf/2506.09495.pdf", "abs": "https://arxiv.org/abs/2506.09495", "title": "Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers", "authors": ["Ilanit Sobol", "Shir Lissak", "Refael Tikochinski", "Tal Nakash", "Anat Brunstein Klomek", "Eyal Fruchter", "Roi Reichart"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Suicide remains a leading cause of death in Western countries, underscoring\nthe need for new research approaches. As social media becomes central to daily\nlife, digital footprints offer valuable insight into suicidal behavior.\nFocusing on individuals who attempted suicide while uploading videos to their\nchannels, we investigate: How do suicidal behaviors manifest on YouTube, and\nhow do they differ from expert knowledge? We applied complementary approaches:\ncomputational bottom-up, hybrid, and expert-driven top-down, on a novel\nlongitudinal dataset of 181 YouTube channels from individuals with\nlife-threatening attempts, alongside 134 control channels. In the bottom-up\napproach, we applied LLM-based topic modeling to identify behavioral\nindicators. Of 166 topics, five were associated with suicide-attempt, with two\nalso showing temporal attempt-related changes ($p<.01$) - Mental Health\nStruggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach,\na clinical expert reviewed LLM-derived topics and flagged 19 as\nsuicide-related. However, none showed significant attempt-related temporal\neffects beyond those identified bottom-up. Notably, YouTube Engagement, a\nplatform-specific indicator, was not flagged by the expert, underscoring the\nvalue of bottom-up discovery. In the top-down approach, psychological\nassessment of suicide attempt narratives revealed that the only significant\ndifference between individuals who attempted before and those attempted during\ntheir upload period was the motivation to share this experience: the former\naimed to Help Others ($\\beta=-1.69$, $p<.01$), while the latter framed it as\npart of their Personal Recovery ($\\beta=1.08$, $p<.01$). By integrating these\napproaches, we offer a nuanced understanding of suicidality, bridging digital\nbehavior and clinical insights.\n  * Within-group changes in relation to the suicide attempt."}
{"id": "2506.09501", "pdf": "https://arxiv.org/pdf/2506.09501.pdf", "abs": "https://arxiv.org/abs/2506.09501", "title": "Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning", "authors": ["Jiayi Yuan", "Hao Li", "Xinheng Ding", "Wenya Xie", "Yu-Jhe Li", "Wentian Zhao", "Kun Wan", "Jing Shi", "Xia Hu", "Zirui Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are now integral across various domains and have\ndemonstrated impressive performance. Progress, however, rests on the premise\nthat benchmark scores are both accurate and reproducible. We demonstrate that\nthe reproducibility of LLM performance is fragile: changing system\nconfiguration such as evaluation batch size, GPU count, and GPU version can\nintroduce significant difference in the generated responses. This issue is\nespecially pronounced in reasoning models, where minor rounding differences in\nearly tokens can cascade into divergent chains of thought, ultimately affecting\naccuracy. For instance, under bfloat16 precision with greedy decoding, a\nreasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation\nin accuracy and 9,000 tokens difference in response length due to differences\nin GPU count, type, and evaluation batch size. We trace the root cause of this\nvariability to the non-associative nature of floating-point arithmetic under\nlimited numerical precision. This work presents the first systematic\ninvestigation into how numerical precision affects reproducibility in LLM\ninference. Through carefully controlled experiments across various hardware,\nsoftware, and precision settings, we quantify when and how model outputs\ndiverge. Our analysis reveals that floating-point precision -- while critical\nfor reproducibility -- is often neglected in evaluation practices. Inspired by\nthis, we develop a lightweight inference pipeline, dubbed LayerCast, that\nstores weights in 16-bit precision but performs all computations in FP32,\nbalancing memory efficiency with numerical stability. Code is available at\nhttps://github.com/nanomaoli/llm_reproducibility."}
{"id": "2506.09507", "pdf": "https://arxiv.org/pdf/2506.09507.pdf", "abs": "https://arxiv.org/abs/2506.09507", "title": "TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding", "authors": ["Bingheng Wu", "Jingze Shi", "Yifan Wu", "Nan Tang", "Yuyu Luo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Transformers exhibit proficiency in capturing long-range dependencies,\nwhereas State Space Models (SSMs) facilitate linear-time sequence modeling.\nNotwithstanding their synergistic potential, the integration of these\narchitectures presents a significant challenge, primarily attributable to a\nfundamental incongruity in their respective positional encoding mechanisms:\nTransformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs\nleverage implicit positional representations via convolutions. This divergence\noften precipitates discontinuities and suboptimal performance. To address this\nimpediment, we propose a unified rotary position embedding (\\textbf{\\ourRoPE})\nmethodology, thereby establishing a consistent positional encoding framework\nfor both self-attention and state-space components. Using this \\ourRoPE, we\nintroduce \\textbf{\\model}, a hybrid architecture that coherently integrates the\nTransformer and SSM layers under this unified positional encoding scheme. At a\n4K sequence length, \\model exhibits training and inference speeds that are\n\\textbf{42.3\\% and 29.5\\% faster}, respectively, relative to standard\nTransformer models. It also delivers higher accuracy: under comparable\nsettings, it surpasses a Transformer baseline by over 4\\% on language modeling\nbenchmarks. \\model furthermore scales more effectively: \\model-1.3B gains\n\\textbf{7.22\\%} in average accuracy over its 320M version (versus about 6\\%\ngains for equivalent Transformers or SSMs). Our results show that unified\npositional encoding resolves positional incompatibility in hybrid models,\nenabling efficient, high-performance long-context modeling."}
{"id": "2506.09513", "pdf": "https://arxiv.org/pdf/2506.09513.pdf", "abs": "https://arxiv.org/abs/2506.09513", "title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning", "authors": ["Yu Sun", "Xingyu Qian", "Weiwen Xu", "Hao Zhang", "Chenghao Xiao", "Long Li", "Yu Rong", "Wenbing Huang", "Qifeng Bai", "Tingyang Xu"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "24 pages, 6 figures, 7 tables", "summary": "Though reasoning-based large language models (LLMs) have excelled in\nmathematics and programming, their capabilities in knowledge-intensive medical\nquestion answering remain underexplored. To address this, we introduce\nReasonMed, the largest medical reasoning dataset, comprising 370k high-quality\nexamples distilled from 1.7 million initial reasoning paths generated by\nvarious LLMs. ReasonMed is constructed through a \\textit{multi-agent\nverification and refinement process}, where we design an \\textit{Error Refiner}\nto enhance the reasoning paths by identifying and correcting error-prone steps\nflagged by a verifier. Leveraging ReasonMed, we systematically investigate best\npractices for training medical reasoning models and find that combining\ndetailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields\nthe most effective fine-tuning strategy. Based on this strategy, we train\nReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the\nprior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%."}
{"id": "2506.09542", "pdf": "https://arxiv.org/pdf/2506.09542.pdf", "abs": "https://arxiv.org/abs/2506.09542", "title": "KG-Infused RAG: Augmenting Corpus-Based RAG with External Knowledge Graphs", "authors": ["Dingjun Wu", "Yukun Yan", "Zhenghao Liu", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) improves factual accuracy by grounding\nresponses in external knowledge. However, existing methods typically rely on a\nsingle source, either unstructured text or structured knowledge. Moreover, they\nlack cognitively inspired mechanisms for activating relevant knowledge. To\naddress these issues, we propose KG-Infused RAG, a framework that integrates\nKGs into RAG systems to implement spreading activation, a cognitive process\nthat enables concept association and inference. KG-Infused RAG retrieves KG\nfacts, expands the query accordingly, and enhances generation by combining\ncorpus passages with structured facts, enabling interpretable, multi-source\nretrieval grounded in semantic structure. We further improve KG-Infused RAG via\npreference learning on sampled key stages in the pipeline. Experiments on five\nQA benchmarks show that KG-Infused RAG consistently outperforms vanilla RAG (by\n3.8% to 13.8%). Additionally, when integrated into Self-RAG, KG-Infused RAG\nbrings further performance gains, demonstrating its effectiveness and\nversatility as a plug-and-play enhancement module for corpus-based RAG methods."}
{"id": "2506.09556", "pdf": "https://arxiv.org/pdf/2506.09556.pdf", "abs": "https://arxiv.org/abs/2506.09556", "title": "MEDUSA: A Multimodal Deep Fusion Multi-Stage Training Framework for Speech Emotion Recognition in Naturalistic Conditions", "authors": ["Georgios Chatzichristodoulou", "Despoina Kosmopoulou", "Antonios Kritikos", "Anastasia Poulopoulou", "Efthymios Georgiou", "Athanasios Katsamanis", "Vassilis Katsouros", "Alexandros Potamianos"], "categories": ["cs.CL"], "comment": "Accepted at Interspeech 2025", "summary": "SER is a challenging task due to the subjective nature of human emotions and\ntheir uneven representation under naturalistic conditions. We propose MEDUSA, a\nmultimodal framework with a four-stage training pipeline, which effectively\nhandles class imbalance and emotion ambiguity. The first two stages train an\nensemble of classifiers that utilize DeepSER, a novel extension of a deep\ncross-modal transformer fusion mechanism from pretrained self-supervised\nacoustic and linguistic representations. Manifold MixUp is employed for further\nregularization. The last two stages optimize a trainable meta-classifier that\ncombines the ensemble predictions. Our training approach incorporates human\nannotation scores as soft targets, coupled with balanced data sampling and\nmultitask learning. MEDUSA ranked 1st in Task 1: Categorical Emotion\nRecognition in the Interspeech 2025: Speech Emotion Recognition in Naturalistic\nConditions Challenge."}
{"id": "2506.09558", "pdf": "https://arxiv.org/pdf/2506.09558.pdf", "abs": "https://arxiv.org/abs/2506.09558", "title": "Gender Bias in English-to-Greek Machine Translation", "authors": ["Eleni Gkovedarou", "Joke Daems", "Luna De Bruyne"], "categories": ["cs.CL"], "comment": "Accepted at GITT 2025 (MT Summit)", "summary": "As the demand for inclusive language increases, concern has grown over the\nsusceptibility of machine translation (MT) systems to reinforce gender\nstereotypes. This study investigates gender bias in two commercial MT systems,\nGoogle Translate and DeepL, focusing on the understudied English-to-Greek\nlanguage pair. We address three aspects of gender bias: i) male bias, ii)\noccupational stereotyping, and iii) errors in anti-stereotypical translations.\nAdditionally, we explore the potential of prompted GPT-4o as a bias mitigation\ntool that provides both gender-explicit and gender-neutral alternatives when\nnecessary. To achieve this, we introduce GendEL, a manually crafted bilingual\ndataset of 240 gender-ambiguous and unambiguous sentences that feature\nstereotypical occupational nouns and adjectives. We find persistent gender bias\nin translations by both MT systems; while they perform well in cases where\ngender is explicitly defined, with DeepL outperforming both Google Translate\nand GPT-4o in feminine gender-unambiguous sentences, they are far from\nproducing gender-inclusive or neutral translations when the gender is\nunspecified. GPT-4o shows promise, generating appropriate gendered and neutral\nalternatives for most ambiguous cases, though residual biases remain evident."}
{"id": "2506.09560", "pdf": "https://arxiv.org/pdf/2506.09560.pdf", "abs": "https://arxiv.org/abs/2506.09560", "title": "Towards Open Foundation Language Model and Corpus for Macedonian: A Low-Resource Language", "authors": ["Stefan Krsteski", "Matea Tashkovska", "Borjan Sazdov", "Hristijan Gjoreski", "Branislav Gerazov"], "categories": ["cs.CL"], "comment": "Camera-ready version accepted at SlavNLP-2025@ACL", "summary": "The increase in technological adoption worldwide comes with demands for novel\ntools to be used by the general population. Large Language Models (LLMs)\nprovide a great opportunity in this respect, but their capabilities remain\nlimited for low-resource languages, restricting applications in countries where\nsuch languages are spoken. We create several resources to facilitate the\nadoption of LLMs and to support research advancements for Macedonian. We\ncollect the largest Macedonian corpus to date, consisting of 40GB of textual\ndata and totaling 3.5B words. To support conversational applications, we\ncollect a 106k-instance instruction dataset, carefully built to be culturally\ngrounded. For evaluation, we construct a Macedonian evaluation suite covering\nseven benchmarks. Finally, we train domestic-yak, a state-of-the-art\n8B-parameter model, on our curated datasets and evaluate it against eight\nbaseline models using the newly constructed benchmark suite. Our model\noutperforms all existing models in the 8B parameter range across all\nbenchmarks, and achieves performance comparable to models up to 10x larger.\nFurthermore, a qualitative analysis with native speakers reveals that our model\nis preferred over larger counterparts, receiving higher ratings for grammatical\ncorrectness and cultural appropriateness. All datasets, code, and model weights\nare openly released, setting a foundation for advancing LLMs in similarly\nunderrepresented languages. These resources are publicly available at\ngithub.com/LVSTCK for source code, and at huggingface.co/LVSTCK for pretrained\nmodel weights and data."}
{"id": "2506.09566", "pdf": "https://arxiv.org/pdf/2506.09566.pdf", "abs": "https://arxiv.org/abs/2506.09566", "title": "From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies", "authors": ["Blaž Škrlj", "Boshko Koloski", "Senja Pollak", "Nada Lavrač"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "To-appear as a book chapter", "summary": "Integrating structured knowledge from Knowledge Graphs (KGs) into Large\nLanguage Models (LLMs) enhances factual grounding and reasoning capabilities.\nThis survey paper systematically examines the synergy between KGs and LLMs,\ncategorizing existing approaches into two main groups: KG-enhanced LLMs, which\nimprove reasoning, reduce hallucinations, and enable complex question\nanswering; and LLM-augmented KGs, which facilitate KG construction, completion,\nand querying. Through comprehensive analysis, we identify critical gaps and\nhighlight the mutual benefits of structured knowledge integration. Compared to\nexisting surveys, our study uniquely emphasizes scalability, computational\nefficiency, and data quality. Finally, we propose future research directions,\nincluding neuro-symbolic integration, dynamic KG updating, data reliability,\nand ethical considerations, paving the way for intelligent systems capable of\nmanaging more complex real-world knowledge tasks."}
{"id": "2506.09591", "pdf": "https://arxiv.org/pdf/2506.09591.pdf", "abs": "https://arxiv.org/abs/2506.09591", "title": "Memorization in Language Models through the Lens of Intrinsic Dimension", "authors": ["Stefan Arnold"], "categories": ["cs.CL"], "comment": null, "summary": "Language Models (LMs) are prone to memorizing parts of their data during\ntraining and unintentionally emitting them at generation time, raising concerns\nabout privacy leakage and disclosure of intellectual property. While previous\nresearch has identified properties such as context length, parameter size, and\nduplication frequency, as key drivers of unintended memorization, little is\nknown about how the latent structure modulates this rate of memorization. We\ninvestigate the role of Intrinsic Dimension (ID), a geometric proxy for the\nstructural complexity of a sequence in latent space, in modulating\nmemorization. Our findings suggest that ID acts as a suppressive signal for\nmemorization: compared to low-ID sequences, high-ID sequences are less likely\nto be memorized, particularly in overparameterized models and under sparse\nexposure. These findings highlight the interaction between scale, exposure, and\ncomplexity in shaping memorization."}
{"id": "2506.09627", "pdf": "https://arxiv.org/pdf/2506.09627.pdf", "abs": "https://arxiv.org/abs/2506.09627", "title": "Benchmarking Debiasing Methods for LLM-based Parameter Estimates", "authors": ["Nicolas Audinet de Pieuchon", "Adel Daoud", "Connor T. Jerzak", "Moa Johansson", "Richard Johansson"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) offer an inexpensive yet powerful way to\nannotate text, but are often inconsistent when compared with experts. These\nerrors can bias downstream estimates of population parameters such as\nregression coefficients and causal effects. To mitigate this bias, researchers\nhave developed debiasing methods such as Design-based Supervised Learning (DSL)\nand Prediction-Powered Inference (PPI), which promise valid estimation by\ncombining LLM annotations with a limited number of expensive expert\nannotations. Although these methods produce consistent estimates under\ntheoretical assumptions, it is unknown how they compare in finite samples of\nsizes encountered in applied research. We make two contributions: First, we\nstudy how each method's performance scales with the number of expert\nannotations, highlighting regimes where LLM bias or limited expert labels\nsignificantly affect results. Second, we compare DSL and PPI across a range of\ntasks, finding that although both achieve low bias with large datasets, DSL\noften outperforms PPI on bias reduction and empirical efficiency, but its\nperformance is less consistent across datasets. Our findings indicate that\nthere is a bias-variance tradeoff at the level of debiasing methods, calling\nfor more research on developing metrics for quantifying their efficiency in\nfinite samples."}
{"id": "2506.09641", "pdf": "https://arxiv.org/pdf/2506.09641.pdf", "abs": "https://arxiv.org/abs/2506.09641", "title": "Modeling Probabilistic Reduction using Information Theory and Naive Discriminative Learning", "authors": ["Anna Stein", "Kevin Tang"], "categories": ["cs.CL", "cs.IT", "math.IT"], "comment": "Submitted to Interspeech 2025", "summary": "This study compares probabilistic predictors based on information theory with\nNaive Discriminative Learning (NDL) predictors in modeling acoustic word\nduration, focusing on probabilistic reduction. We examine three models using\nthe Buckeye corpus: one with NDL-derived predictors using information-theoretic\nformulas, one with traditional NDL predictors, and one with N-gram\nprobabilistic predictors. Results show that the N-gram model outperforms both\nNDL models, challenging the assumption that NDL is more effective due to its\ncognitive motivation. However, incorporating information-theoretic formulas\ninto NDL improves model performance over the traditional model. This research\nhighlights a) the need to incorporate not only frequency and contextual\npredictability but also average contextual predictability, and b) the\nimportance of combining information-theoretic metrics of predictability and\ninformation derived from discriminative learning in modeling acoustic\nreduction."}
{"id": "2506.09643", "pdf": "https://arxiv.org/pdf/2506.09643.pdf", "abs": "https://arxiv.org/abs/2506.09643", "title": "Using Sign Language Production as Data Augmentation to enhance Sign Language Translation", "authors": ["Harry Walsh", "Maksym Ivashechkin", "Richard Bowden"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Machine learning models fundamentally rely on large quantities of\nhigh-quality data. Collecting the necessary data for these models can be\nchallenging due to cost, scarcity, and privacy restrictions. Signed languages\nare visual languages used by the deaf community and are considered low-resource\nlanguages. Sign language datasets are often orders of magnitude smaller than\ntheir spoken language counterparts. Sign Language Production is the task of\ngenerating sign language videos from spoken language sentences, while Sign\nLanguage Translation is the reverse translation task. Here, we propose\nleveraging recent advancements in Sign Language Production to augment existing\nsign language datasets and enhance the performance of Sign Language Translation\nmodels. For this, we utilize three techniques: a skeleton-based approach to\nproduction, sign stitching, and two photo-realistic generative models, SignGAN\nand SignSplat. We evaluate the effectiveness of these techniques in enhancing\nthe performance of Sign Language Translation models by generating variation in\nthe signer's appearance and the motion of the skeletal data. Our results\ndemonstrate that the proposed methods can effectively augment existing datasets\nand enhance the performance of Sign Language Translation models by up to 19%,\npaving the way for more robust and accurate Sign Language Translation systems,\neven in resource-constrained environments."}
{"id": "2506.09645", "pdf": "https://arxiv.org/pdf/2506.09645.pdf", "abs": "https://arxiv.org/abs/2506.09645", "title": "Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering", "authors": ["Tianjun Yao", "Haoxuan Li", "Zhiqiang Shen", "Pan Li", "Tongliang Liu", "Kun Zhang"], "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.6"], "comment": "32 pages, 28 figures", "summary": "Large Language Models (LLMs) have shown strong inductive reasoning ability\nacross various domains, but their reliability is hindered by the outdated\nknowledge and hallucinations. Retrieval-Augmented Generation mitigates these\nissues by grounding LLMs with external knowledge; however, most existing RAG\npipelines rely on unstructured text, limiting interpretability and structured\nreasoning. Knowledge graphs, which represent facts as relational triples, offer\na more structured and compact alternative. Recent studies have explored\nintegrating knowledge graphs with LLMs for knowledge graph question answering\n(KGQA), with a significant proportion adopting the retrieve-then-reasoning\nparadigm. In this framework, graph-based retrievers have demonstrated strong\nempirical performance, yet they still face challenges in generalization\nability. In this work, we propose RAPL, a novel framework for efficient and\neffective graph retrieval in KGQA. RAPL addresses these limitations through\nthree aspects: (1) a two-stage labeling strategy that combines heuristic\nsignals with parametric models to provide causally grounded supervision; (2) a\nmodel-agnostic graph transformation approach to capture both intra- and\ninter-triple interactions, thereby enhancing representational capacity; and (3)\na path-based reasoning strategy that facilitates learning from the injected\nrational knowledge, and supports downstream reasoner through structured inputs.\nEmpirically, RAPL outperforms state-of-the-art methods by $2.66\\%-20.34\\%$, and\nsignificantly reduces the performance gap between smaller and more powerful\nLLM-based reasoners, as well as the gap under cross-dataset settings,\nhighlighting its superior retrieval capability and generalizability. Codes are\navailable at: https://github.com/tianyao-aka/RAPL."}
{"id": "2506.09657", "pdf": "https://arxiv.org/pdf/2506.09657.pdf", "abs": "https://arxiv.org/abs/2506.09657", "title": "Bridging the Gap Between Open-Source and Proprietary LLMs in Table QA", "authors": ["Nikolas Evkarpidi", "Elena Tutubalina"], "categories": ["cs.CL"], "comment": "Accepted for publication at the 19th International Workshop on\n  Semantic Evaluation (SemEval-2025), to be held in conjunction with ACL 2025.\n  15 pages, 5 figures", "summary": "This paper presents a system developed for SemEval 2025 Task 8: Question\nAnswering (QA) over tabular data. Our approach integrates several key\ncomponents: text-to-SQL and text-to-code generation modules, a self-correction\nmechanism, and a retrieval-augmented generation (RAG). Additionally, it\nincludes an end-to-end (E2E) module, all orchestrated by a large language model\n(LLM). Through ablation studies, we analyzed the effects of different parts of\nour pipeline and identified the challenges that are still present in this\nfield. During the evaluation phase of the competition, our solution achieved an\naccuracy of 80%, resulting in a top-13 ranking among the 38 participating\nteams. Our pipeline demonstrates a significant improvement in accuracy for\nopen-source models and achieves a performance comparable to proprietary LLMs in\nQA tasks over tables. The code is available at GitHub repository."}
{"id": "2506.09669", "pdf": "https://arxiv.org/pdf/2506.09669.pdf", "abs": "https://arxiv.org/abs/2506.09669", "title": "Query-Level Uncertainty in Large Language Models", "authors": ["Lihu Chen", "Gaël Varoquaux"], "categories": ["cs.CL"], "comment": "In Progress", "summary": "It is important for Large Language Models to be aware of the boundary of\ntheir knowledge, the mechanism of identifying known and unknown queries. This\ntype of awareness can help models perform adaptive inference, such as invoking\nRAG, engaging in slow and deep thinking, or adopting the abstention mechanism,\nwhich is beneficial to the development of efficient and trustworthy AI. In this\nwork, we propose a method to detect knowledge boundaries via Query-Level\nUncertainty, which aims to determine if the model is able to address a given\nquery without generating any tokens. To this end, we introduce a novel and\ntraining-free method called \\emph{Internal Confidence}, which leverages\nself-evaluations across layers and tokens. Empirical results on both factual QA\nand mathematical reasoning tasks demonstrate that our internal confidence can\noutperform several baselines. Furthermore, we showcase that our proposed method\ncan be used for efficient RAG and model cascading, which is able to reduce\ninference costs while maintaining performance."}
{"id": "2506.09672", "pdf": "https://arxiv.org/pdf/2506.09672.pdf", "abs": "https://arxiv.org/abs/2506.09672", "title": "Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for Unstructured Data", "authors": ["Hao Xiong", "Chuanyuan Tan", "Wenliang Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Unstructured Knowledge Editing (UKE) is crucial for updating the relevant\nknowledge of large language models (LLMs). It focuses on unstructured inputs,\nsuch as long or free-form texts, which are common forms of real-world\nknowledge. Although previous studies have proposed effective methods and tested\nthem, some issues exist: (1) Lack of Locality evaluation for UKE, and (2)\nAbnormal failure of fine-tuning (FT) based methods for UKE. To address these\nissues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by\nextending two existing UKE datasets with locality test data from the\nunstructured and structured views. This enables a systematic evaluation of the\nLocality of post-edited models. Furthermore, we identify four factors that may\naffect the performance of FT-based methods. Based on these factors, we conduct\nexperiments to determine how the well-performing FT-based methods should be\ntrained for the UKE task, providing a training recipe for future research. Our\nexperimental results indicate that the FT-based method with the optimal setting\n(FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art\n(SOTA). In batch editing scenarios, FT-UKE shows strong performance as well,\nwith its advantage over SOTA methods increasing as the batch size grows,\nexpanding the average metric lead from +6.78% to +10.80%"}
{"id": "2506.09684", "pdf": "https://arxiv.org/pdf/2506.09684.pdf", "abs": "https://arxiv.org/abs/2506.09684", "title": "Inv-Entropy: A Fully Probabilistic Framework for Uncertainty Quantification in Language Models", "authors": ["Haoyi Song", "Ruihan Ji", "Naichen Shi", "Fan Lai", "Raed Al Kontar"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have transformed natural language processing,\nbut their reliable deployment requires effective uncertainty quantification\n(UQ). Existing UQ methods are often heuristic and lack a probabilistic\nfoundation. This paper begins by providing a theoretical justification for the\nrole of perturbations in UQ for LLMs. We then introduce a dual random walk\nperspective, modeling input-output pairs as two Markov chains with transition\nprobabilities defined by semantic similarity. Building on this, we propose a\nfully probabilistic framework based on an inverse model, which quantifies\nuncertainty by evaluating the diversity of the input space conditioned on a\ngiven output through systematic perturbations. Within this framework, we define\na new uncertainty measure, Inv-Entropy. A key strength of our framework is its\nflexibility: it supports various definitions of uncertainty measures,\nembeddings, perturbation strategies, and similarity metrics. We also propose\nGAAP, a perturbation algorithm based on genetic algorithms, which enhances the\ndiversity of sampled inputs. In addition, we introduce a new evaluation metric,\nTemperature Sensitivity of Uncertainty (TSU), which directly assesses\nuncertainty without relying on correctness as a proxy. Extensive experiments\ndemonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code\nto reproduce the results can be found at\nhttps://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs."}
{"id": "2506.09790", "pdf": "https://arxiv.org/pdf/2506.09790.pdf", "abs": "https://arxiv.org/abs/2506.09790", "title": "ComfyUI-R1: Exploring Reasoning Models for Workflow Generation", "authors": ["Zhenran Xu", "Yiyu Wang", "Xue Yang", "Longyue Wang", "Weihua Luo", "Kaifu Zhang", "Baotian Hu", "Min Zhang"], "categories": ["cs.CL", "cs.CV", "cs.SE"], "comment": "Work in progress. Try it out in ComfyUI-Copilot\n  https://github.com/AIDC-AI/ComfyUI-Copilot", "summary": "AI-generated content has evolved from monolithic models to modular workflows,\nparticularly on platforms like ComfyUI, enabling customization in creative\npipelines. However, crafting effective workflows requires great expertise to\norchestrate numerous specialized components, presenting a steep learning curve\nfor users. To address this challenge, we introduce ComfyUI-R1, the first large\nreasoning model for automated workflow generation. Starting with our curated\ndataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning\ndata, including node selection, workflow planning, and code-level workflow\nrepresentation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT\nfine-tuning for cold start, adapting models to the ComfyUI domain; (2)\nreinforcement learning for incentivizing reasoning capability, guided by a\nfine-grained rule-metric hybrid reward, ensuring format validity, structural\nintegrity, and node-level fidelity. Experiments show that our 7B-parameter\nmodel achieves a 97\\% format validity rate, along with high pass rate,\nnode-level and graph-level F1 scores, significantly surpassing prior\nstate-of-the-art methods that employ leading closed-source models such as\nGPT-4o and Claude series. Further analysis highlights the critical role of the\nreasoning process and the advantage of transforming workflows into code.\nQualitative comparison reveals our strength in synthesizing intricate workflows\nwith diverse nodes, underscoring the potential of long CoT reasoning in AI art\ncreation."}
{"id": "2506.09796", "pdf": "https://arxiv.org/pdf/2506.09796.pdf", "abs": "https://arxiv.org/abs/2506.09796", "title": "Do LLMs Give Psychometrically Plausible Responses in Educational Assessments?", "authors": ["Andreas Säuberli", "Diego Frassinelli", "Barbara Plank"], "categories": ["cs.CL"], "comment": "Accepted for publication at the 20th Workshop on Innovative Use of\n  NLP for Building Educational Applications (BEA) at ACL 2025", "summary": "Knowing how test takers answer items in educational assessments is essential\nfor test development, to evaluate item quality, and to improve test validity.\nHowever, this process usually requires extensive pilot studies with human\nparticipants. If large language models (LLMs) exhibit human-like response\nbehavior to test items, this could open up the possibility of using them as\npilot participants to accelerate test development. In this paper, we evaluate\nthe human-likeness or psychometric plausibility of responses from 18\ninstruction-tuned LLMs with two publicly available datasets of multiple-choice\ntest items across three subjects: reading, U.S. history, and economics. Our\nmethodology builds on two theoretical frameworks from psychometrics which are\ncommonly used in educational assessment, classical test theory and item\nresponse theory. The results show that while larger models are excessively\nconfident, their response distributions can be more human-like when calibrated\nwith temperature scaling. In addition, we find that LLMs tend to correlate\nbetter with humans in reading comprehension items compared to other subjects.\nHowever, the correlations are not very strong overall, indicating that LLMs\nshould not be used for piloting educational assessments in a zero-shot setting."}
{"id": "2506.09820", "pdf": "https://arxiv.org/pdf/2506.09820.pdf", "abs": "https://arxiv.org/abs/2506.09820", "title": "CoRT: Code-integrated Reasoning within Thinking", "authors": ["Chengpeng Li", "Zhengyang Tang", "Ziniu Li", "Mingfeng Xue", "Keqin Bao", "Tian Ding", "Ruoyu Sun", "Benyou Wang", "Xiang Wang", "Junyang Lin", "Dayiheng Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "work in progress", "summary": "Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable\nprogress in natural language reasoning with long chain-of-thought (CoT), yet\nthey remain inefficient or inaccurate when handling complex mathematical\noperations. Addressing these limitations through computational tools (e.g.,\ncomputation libraries and symbolic solvers) is promising, but it introduces a\ntechnical challenge: Code Interpreter (CI) brings external knowledge beyond the\nmodel's internal text representations, thus the direct combination is not\nefficient. This paper introduces CoRT, a post-training framework for teaching\nLRMs to leverage CI effectively and efficiently. As a first step, we address\nthe data scarcity issue by synthesizing code-integrated reasoning data through\nHint-Engineering, which strategically inserts different hints at appropriate\npositions to optimize LRM-CI interaction. We manually create 30 high-quality\nsamples, upon which we post-train models ranging from 1.5B to 32B parameters,\nwith supervised fine-tuning, rejection fine-tuning and reinforcement learning.\nOur experimental results demonstrate that Hint-Engineering models achieve 4\\%\nand 8\\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and\nDeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging\nmathematical reasoning datasets. Furthermore, Hint-Engineering models use about\n30\\% fewer tokens for the 32B model and 50\\% fewer tokens for the 1.5B model\ncompared with the natural language models. The models and code are available at\nhttps://github.com/ChengpengLi1003/CoRT."}
{"id": "2506.09827", "pdf": "https://arxiv.org/pdf/2506.09827.pdf", "abs": "https://arxiv.org/abs/2506.09827", "title": "EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection", "authors": ["Christoph Schuhmann", "Robert Kaczmarczyk", "Gollam Rabby", "Felix Friedrich", "Maurice Kraus", "Kourosh Nadi", "Huu Nguyen", "Kristian Kersting", "Sören Auer"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advancement of text-to-speech and audio generation models necessitates\nrobust benchmarks for evaluating the emotional understanding capabilities of AI\nsystems. Current speech emotion recognition (SER) datasets often exhibit\nlimitations in emotional granularity, privacy concerns, or reliance on acted\nportrayals. This paper introduces EmoNet-Voice, a new resource for speech\nemotion detection, which includes EmoNet-Voice Big, a large-scale pre-training\ndataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,\nand 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human\nexpert annotations. EmoNet-Voice is designed to evaluate SER models on a\nfine-grained spectrum of 40 emotion categories with different levels of\nintensities. Leveraging state-of-the-art voice generation, we curated synthetic\naudio snippets simulating actors portraying scenes designed to evoke specific\nemotions. Crucially, we conducted rigorous validation by psychology experts who\nassigned perceived intensity labels. This synthetic, privacy-preserving\napproach allows for the inclusion of sensitive emotional states often absent in\nexisting datasets. Lastly, we introduce Empathic Insight Voice models that set\na new standard in speech emotion recognition with high agreement with human\nexperts. Our evaluations across the current model landscape exhibit valuable\nfindings, such as high-arousal emotions like anger being much easier to detect\nthan low-arousal states like concentration."}
{"id": "2506.09833", "pdf": "https://arxiv.org/pdf/2506.09833.pdf", "abs": "https://arxiv.org/abs/2506.09833", "title": "Error-Guided Pose Augmentation: Enhancing Rehabilitation Exercise Assessment through Targeted Data Generation", "authors": ["Omar Sherif", "Ali Hamdi"], "categories": ["cs.CL", "I.2.1"], "comment": "6 pages, 1 figure. To appear in Intelligent Methods, Systems, and\n  Applications 2025", "summary": "Effective rehabilitation assessment is essential for monitoring patient\nprogress, particularly in home-based settings. Existing systems often face\nchallenges such as data imbalance and difficulty detecting subtle movement\nerrors. This paper introduces Error-Guided Pose Augmentation (EGPA), a method\nthat generates synthetic skeleton data by simulating clinically relevant\nmovement mistakes. Unlike standard augmentation techniques, EGPA targets\nbiomechanical errors observed in rehabilitation. Combined with an\nattention-based graph convolutional network, EGPA improves performance across\nmultiple evaluation metrics. Experiments demonstrate reductions in mean\nabsolute error of up to 27.6 percent and gains in error classification accuracy\nof 45.8 percent. Attention visualizations show that the model learns to focus\non clinically significant joints and movement phases, enhancing both accuracy\nand interpretability. EGPA offers a promising approach for improving automated\nmovement quality assessment in both clinical and home-based rehabilitation\ncontexts."}
{"id": "2506.09847", "pdf": "https://arxiv.org/pdf/2506.09847.pdf", "abs": "https://arxiv.org/abs/2506.09847", "title": "Dataset of News Articles with Provenance Metadata for Media Relevance Assessment", "authors": ["Tomas Peterka", "Matyas Bohacek"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.CY"], "comment": null, "summary": "Out-of-context and misattributed imagery is the leading form of media\nmanipulation in today's misinformation and disinformation landscape. The\nexisting methods attempting to detect this practice often only consider whether\nthe semantics of the imagery corresponds to the text narrative, missing\nmanipulation so long as the depicted objects or scenes somewhat correspond to\nthe narrative at hand. To tackle this, we introduce News Media Provenance\nDataset, a dataset of news articles with provenance-tagged images. We formulate\ntwo tasks on this dataset, location of origin relevance (LOR) and date and time\nof origin relevance (DTOR), and present baseline results on six large language\nmodels (LLMs). We identify that, while the zero-shot performance on LOR is\npromising, the performance on DTOR hinders, leaving room for specialized\narchitectures and future work."}
{"id": "2506.09853", "pdf": "https://arxiv.org/pdf/2506.09853.pdf", "abs": "https://arxiv.org/abs/2506.09853", "title": "Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning", "authors": ["Xiangning Yu", "Zhuohan Wang", "Linyi Yang", "Haoxuan Li", "Anjie Liu", "Xiao Xue", "Jun Wang", "Mengyue Yang"], "categories": ["cs.CL", "cs.AI", "math.ST", "stat.ME", "stat.TH"], "comment": null, "summary": "Chain-of-Thought (CoT) prompting plays an indispensable role in endowing\nlarge language models (LLMs) with complex reasoning capabilities. However, CoT\ncurrently faces two fundamental challenges: (1) Sufficiency, which ensures that\nthe generated intermediate inference steps comprehensively cover and\nsubstantiate the final conclusion; and (2) Necessity, which identifies the\ninference steps that are truly indispensable for the soundness of the resulting\nanswer. We propose a causal framework that characterizes CoT reasoning through\nthe dual lenses of sufficiency and necessity. Incorporating causal Probability\nof Sufficiency and Necessity allows us not only to determine which steps are\nlogically sufficient or necessary to the prediction outcome, but also to\nquantify their actual influence on the final reasoning outcome under different\nintervention scenarios, thereby enabling the automated addition of missing\nsteps and the pruning of redundant ones. Extensive experimental results on\nvarious mathematical and commonsense reasoning benchmarks confirm substantial\nimprovements in reasoning efficiency and reduced token usage without\nsacrificing accuracy. Our work provides a promising direction for improving LLM\nreasoning performance and cost-effectiveness."}
{"id": "2506.09886", "pdf": "https://arxiv.org/pdf/2506.09886.pdf", "abs": "https://arxiv.org/abs/2506.09886", "title": "Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs", "authors": ["Rodion Oblovatny", "Alexandra Bazarova", "Alexey Zaytsev"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present a novel approach for detecting hallucinations in large language\nmodels (LLMs) by analyzing the probabilistic divergence between prompt and\nresponse hidden-state distributions. Counterintuitively, we find that\nhallucinated responses exhibit smaller deviations from their prompts compared\nto grounded responses, suggesting that hallucinations often arise from\nsuperficial rephrasing rather than substantive reasoning. Leveraging this\ninsight, we propose a model-intrinsic detection method that uses distributional\ndistances as principled hallucination scores, eliminating the need for external\nknowledge or auxiliary models. To enhance sensitivity, we employ deep learnable\nkernels that automatically adapt to capture nuanced geometric differences\nbetween distributions. Our approach outperforms existing baselines,\ndemonstrating state-of-the-art performance on several benchmarks. The method\nremains competitive even without kernel training, offering a robust, scalable\nsolution for hallucination detection."}
{"id": "2506.09890", "pdf": "https://arxiv.org/pdf/2506.09890.pdf", "abs": "https://arxiv.org/abs/2506.09890", "title": "The Emergence of Abstract Thought in Large Language Models Beyond Any Language", "authors": ["Yuxin Chen", "Yiran Zhao", "Yang Zhang", "An Zhang", "Kenji Kawaguchi", "Shafiq Joty", "Junnan Li", "Tat-Seng Chua", "Michael Qizhe Shieh", "Wenxuan Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) continue to advance, their capacity to\nfunction effectively across a diverse range of languages has shown marked\nimprovement. Preliminary studies observe that the hidden activations of LLMs\noften resemble English, even when responding to non-English prompts. This has\nled to the widespread assumption that LLMs may \"think\" in English. However,\nmore recent results showing strong multilingual performance, even surpassing\nEnglish performance on specific tasks in other languages, challenge this view.\nIn this work, we find that LLMs progressively develop a core language-agnostic\nparameter space-a remarkably small subset of parameters whose deactivation\nresults in significant performance degradation across all languages. This\ncompact yet critical set of parameters underlies the model's ability to\ngeneralize beyond individual languages, supporting the emergence of abstract\nthought that is not tied to any specific linguistic system. Specifically, we\nidentify language-related neurons-those are consistently activated during the\nprocessing of particular languages, and categorize them as either shared\n(active across multiple languages) or exclusive (specific to one). As LLMs\nundergo continued development over time, we observe a marked increase in both\nthe proportion and functional importance of shared neurons, while exclusive\nneurons progressively diminish in influence. These shared neurons constitute\nthe backbone of the core language-agnostic parameter space, supporting the\nemergence of abstract thought. Motivated by these insights, we propose\nneuron-specific training strategies tailored to LLMs' language-agnostic levels\nat different development stages. Experiments across diverse LLM families\nsupport our approach."}
{"id": "2506.09902", "pdf": "https://arxiv.org/pdf/2506.09902.pdf", "abs": "https://arxiv.org/abs/2506.09902", "title": "PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants", "authors": ["Zheng Zhao", "Clara Vania", "Subhradeep Kayal", "Naila Khan", "Shay B. Cohen", "Emine Yilmaz"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large language models (LLMs) have advanced conversational AI assistants.\nHowever, systematically evaluating how well these assistants apply\npersonalization--adapting to individual user preferences while completing\ntasks--remains challenging. Existing personalization benchmarks focus on\nchit-chat, non-conversational tasks, or narrow domains, failing to capture the\ncomplexities of personalized task-oriented assistance. To address this, we\nintroduce PersonaLens, a comprehensive benchmark for evaluating personalization\nin task-oriented AI assistants. Our benchmark features diverse user profiles\nequipped with rich preferences and interaction histories, along with two\nspecialized LLM-based agents: a user agent that engages in realistic\ntask-oriented dialogues with AI assistants, and a judge agent that employs the\nLLM-as-a-Judge paradigm to assess personalization, response quality, and task\nsuccess. Through extensive experiments with current LLM assistants across\ndiverse tasks, we reveal significant variability in their personalization\ncapabilities, providing crucial insights for advancing conversational AI\nsystems."}
{"id": "2506.09917", "pdf": "https://arxiv.org/pdf/2506.09917.pdf", "abs": "https://arxiv.org/abs/2506.09917", "title": "Aspect-Based Opinion Summarization with Argumentation Schemes", "authors": ["Wendi Zhou", "Ameer Saadat-Yazd", "Nadin Kokciyan"], "categories": ["cs.CL"], "comment": "Accepted by ArgMining 2025", "summary": "Reviews are valuable resources for customers making purchase decisions in\nonline shopping. However, it is impractical for customers to go over the vast\nnumber of reviews and manually conclude the prominent opinions, which prompts\nthe need for automated opinion summarization systems. Previous approaches,\neither extractive or abstractive, face challenges in automatically producing\ngrounded aspect-centric summaries. In this paper, we propose a novel\nsummarization system that not only captures predominant opinions from an aspect\nperspective with supporting evidence, but also adapts to varying domains\nwithout relying on a pre-defined set of aspects. Our proposed framework,\nASESUM, summarizes viewpoints relevant to the critical aspects of a product by\nextracting aspect-centric arguments and measuring their salience and validity.\nWe conduct experiments on a real-world dataset to demonstrate the superiority\nof our approach in capturing diverse perspectives of the original reviews\ncompared to new and existing methods."}
{"id": "2506.09942", "pdf": "https://arxiv.org/pdf/2506.09942.pdf", "abs": "https://arxiv.org/abs/2506.09942", "title": "VerIF: Verification Engineering for Reinforcement Learning in Instruction Following", "authors": ["Hao Peng", "Yunjia Qi", "Xiaozhi Wang", "Bin Xu", "Lei Hou", "Juanzi Li"], "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 8 figures", "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a key\ntechnique for enhancing large language models (LLMs), with verification\nengineering playing a central role. However, best practices for RL in\ninstruction following remain underexplored. In this work, we explore the\nverification challenge in RL for instruction following and propose VerIF, a\nverification method that combines rule-based code verification with LLM-based\nverification from a large reasoning model (e.g., QwQ-32B). To support this\napproach, we construct a high-quality instruction-following dataset,\nVerInstruct, containing approximately 22,000 instances with associated\nverification signals. We apply RL training with VerIF to two models, achieving\nsignificant improvements across several representative instruction-following\nbenchmarks. The trained models reach state-of-the-art performance among models\nof comparable size and generalize well to unseen constraints. We further\nobserve that their general capabilities remain unaffected, suggesting that RL\nwith VerIF can be integrated into existing RL recipes to enhance overall model\nperformance. We have released our datasets, codes, and models to facilitate\nfuture research at https://github.com/THU-KEG/VerIF."}
{"id": "2506.09944", "pdf": "https://arxiv.org/pdf/2506.09944.pdf", "abs": "https://arxiv.org/abs/2506.09944", "title": "Query-Focused Retrieval Heads Improve Long-Context Reasoning and Re-ranking", "authors": ["Wuwei Zhang", "Fangcong Yin", "Howard Yen", "Danqi Chen", "Xi Ye"], "categories": ["cs.CL"], "comment": null, "summary": "Recent work has identified retrieval heads (Wu et al., 2025b), a subset of\nattention heads responsible for retrieving salient information in long-context\nlanguage models (LMs), as measured by their copy-paste behavior in\nNeedle-in-a-Haystack tasks. In this paper, we introduce QRHEAD (Query-Focused\nRetrieval Head), an improved set of attention heads that enhance retrieval from\nlong context. We identify QRHEAD by aggregating attention scores with respect\nto the input query, using a handful of examples from real-world tasks (e.g.,\nlong-context QA). We further introduce QR- RETRIEVER, an efficient and\neffective retriever that uses the accumulated attention mass of QRHEAD as\nretrieval scores. We use QR- RETRIEVER for long-context reasoning by selecting\nthe most relevant parts with the highest retrieval scores. On multi-hop\nreasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains\nover full context and outperforms strong dense retrievers. We also evaluate\nQRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves\nstrong zero-shot performance, outperforming other LLM-based re-rankers such as\nRankGPT. Further analysis shows that both the querycontext attention scoring\nand task selection are crucial for identifying QRHEAD with strong downstream\nutility. Overall, our work contributes a general-purpose retriever and offers\ninterpretability insights into the long-context capabilities of LMs."}
{"id": "2506.09967", "pdf": "https://arxiv.org/pdf/2506.09967.pdf", "abs": "https://arxiv.org/abs/2506.09967", "title": "Resa: Transparent Reasoning Models via SAEs", "authors": ["Shangshang Wang", "Julian Asilis", "Ömer Faruk Akgül", "Enes Burak Bilgin", "Ollie Liu", "Deqing Fu", "Willie Neiswanger"], "categories": ["cs.CL"], "comment": null, "summary": "How cost-effectively can we elicit strong reasoning in language models by\nleveraging their underlying representations? We answer this question with Resa,\na family of 1.5B reasoning models trained via a novel and efficient sparse\nautoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to\ncapture reasoning abilities from a source model, and then uses the trained SAE\nto guide a standard supervised fine-tuning process to elicit such abilities in\na target model, all using verified question-answer data without any reasoning\ntraces. Notably, when applied to certain base models before further RL\npost-training, SAE-Tuning retains >97% of its RL-trained counterpart's\nreasoning performance while reducing training costs by >2000x to roughly \\$1\nand training time by >450x to around 20 minutes. Furthermore, when applied to\nlightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning\nperformance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only\naround \\$1 additional cost. Surprisingly, the reasoning abilities extracted via\nSAEs are potentially both generalizable and modular. Generality means abilities\nextracted from one dataset still elevate performance on a larger and\noverlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math\ncan be attached to the R1-Distill model at test time, without any retraining,\nand yield comparable gains. Extensive ablations validate these findings and all\nartifacts are fully open-sourced."}
{"id": "2506.09975", "pdf": "https://arxiv.org/pdf/2506.09975.pdf", "abs": "https://arxiv.org/abs/2506.09975", "title": "When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text", "authors": ["Hillary Dawkins", "Kathleen C. Fraser", "Svetlana Kiritchenko"], "categories": ["cs.CL"], "comment": "to appear in ACL Findings", "summary": "Detecting AI-generated text is a difficult problem to begin with; detecting\nAI-generated text on social media is made even more difficult due to the short\ntext length and informal, idiosyncratic language of the internet. It is\nnonetheless important to tackle this problem, as social media represents a\nsignificant attack vector in online influence campaigns, which may be bolstered\nthrough the use of mass-produced AI-generated posts supporting (or opposing)\nparticular policies, decisions, or events. We approach this problem with the\nmindset and resources of a reasonably sophisticated threat actor, and create a\ndataset of 505,159 AI-generated social media posts from a combination of\nopen-source, closed-source, and fine-tuned LLMs, covering 11 different\ncontroversial topics. We show that while the posts can be detected under\ntypical research assumptions about knowledge of and access to the generating\nmodels, under the more realistic assumption that an attacker will not release\ntheir fine-tuned model to the public, detectability drops dramatically. This\nresult is confirmed with a human study. Ablation experiments highlight the\nvulnerability of various detection algorithms to fine-tuned LLMs. This result\nhas implications across all detection domains, since fine-tuning is a generally\napplicable and realistic LLM use case."}
{"id": "2506.09983", "pdf": "https://arxiv.org/pdf/2506.09983.pdf", "abs": "https://arxiv.org/abs/2506.09983", "title": "Step-by-step Instructions and a Simple Tabular Output Format Improve the Dependency Parsing Accuracy of LLMs", "authors": ["Hiroshi Matsuda", "Chunpeng Ma", "Masayuki Asahara"], "categories": ["cs.CL"], "comment": "9 pages, 2 figures, accepted for SyntaxFest 2025", "summary": "Recent advances in large language models (LLMs) have enabled impressive\nperformance in various tasks. However, standard prompting often struggles to\nproduce structurally valid and accurate outputs, especially in dependency\nparsing. We propose a novel step-by-step instruction strategy, where universal\npart-of-speech tagging precedes the prediction of syntactic heads and\ndependency labels, and a simplified CoNLL-U like output format, our method\nachieves state-of-the-art accuracy on Universal Dependencies datasets across 17\nlanguages without hallucination or contamination. We further show that\nmultilingual fine-tuning simultaneously improves cross-language generalization\nperformance. Our results highlight the effectiveness of explicit reasoning\nsteps in LLM-based parsing and offer a scalable, format-consistent alternative\nto bracket-based approaches."}
{"id": "2506.09992", "pdf": "https://arxiv.org/pdf/2506.09992.pdf", "abs": "https://arxiv.org/abs/2506.09992", "title": "Large Language Models for Toxic Language Detection in Low-Resource Balkan Languages", "authors": ["Amel Muminovic", "Amela Kadric Muminovic"], "categories": ["cs.CL"], "comment": "8 pages", "summary": "Online toxic language causes real harm, especially in regions with limited\nmoderation tools. In this study, we evaluate how large language models handle\ntoxic comments in Serbian, Croatian, and Bosnian, languages with limited\nlabeled data. We built and manually labeled a dataset of 4,500 YouTube and\nTikTok comments drawn from videos across diverse categories, including music,\npolitics, sports, modeling, influencer content, discussions of sexism, and\ngeneral topics. Four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude\n3 Opus) were tested in two modes: zero-shot and context-augmented. We measured\nprecision, recall, F1 score, accuracy and false positive rates. Including a\nshort context snippet raised recall by about 0.12 on average and improved F1\nscore by up to 0.10, though it sometimes increased false positives. The best\nbalance came from Gemini in context-augmented mode, reaching an F1 score of\n0.82 and accuracy of 0.82, while zero-shot GPT-4.1 led on precision and had the\nlowest false alarms. We show how adding minimal context can improve toxic\nlanguage detection in low-resource settings and suggest practical strategies\nsuch as improved prompt design and threshold calibration. These results show\nthat prompt design alone can yield meaningful gains in toxicity detection for\nunderserved Balkan language communities."}
{"id": "2506.09996", "pdf": "https://arxiv.org/pdf/2506.09996.pdf", "abs": "https://arxiv.org/abs/2506.09996", "title": "From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring", "authors": ["Yang Li", "Qiang Sheng", "Yehan Yang", "Xueyao Zhang", "Juan Cao"], "categories": ["cs.CL", "cs.CY"], "comment": "22 pages, 7 figures, and 9 tables", "summary": "Though safety alignment has been applied to most large language models\n(LLMs), LLM service providers generally deploy a subsequent moderation as the\nexternal safety guardrail in real-world products. Existing moderators mainly\npractice a conventional full detection, which determines the harmfulness based\non the complete LLM output, causing high service latency. Recent works pay more\nattention to partial detection where moderators oversee the generation midway\nand early stop the output if harmfulness is detected, but they directly apply\nmoderators trained with the full detection paradigm to incomplete outputs,\nintroducing a training-inference gap that lowers the performance. In this\npaper, we explore how to form a data-and-model solution that natively supports\npartial detection. For the data, we construct FineHarm, a dataset consisting of\n29K prompt-response pairs with fine-grained annotations to provide reasonable\nsupervision for token-level training. Then, we propose the streaming content\nmonitor, which is trained with dual supervision of response- and token-level\nlabels and can follow the output stream of LLM to make a timely judgment of\nharmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is\ncomparable to full detection, by only seeing the first 18% of tokens in\nresponses on average. Moreover, the SCM can serve as a pseudo-harmfulness\nannotator for improving safety alignment and lead to a higher harmlessness\nscore than DPO."}
{"id": "2410.16222", "pdf": "https://arxiv.org/pdf/2410.16222.pdf", "abs": "https://arxiv.org/abs/2410.16222", "title": "An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks", "authors": ["Valentyn Boreiko", "Alexander Panfilov", "Vaclav Voracek", "Matthias Hein", "Jonas Geiping"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "A plethora of jailbreaking attacks have been proposed to obtain harmful\nresponses from safety-tuned LLMs. These methods largely succeed in coercing the\ntarget output in their original settings, but their attacks vary substantially\nin fluency and computational effort. In this work, we propose a unified threat\nmodel for the principled comparison of these methods. Our threat model checks\nif a given jailbreak is likely to occur in the distribution of text. For this,\nwe build an N-gram language model on 1T tokens, which, unlike model-based\nperplexity, allows for an LLM-agnostic, nonparametric, and inherently\ninterpretable evaluation. We adapt popular attacks to this threat model, and,\nfor the first time, benchmark these attacks on equal footing with it. After an\nextensive comparison, we find attack success rates against safety-tuned modern\nmodels to be lower than previously presented and that attacks based on discrete\noptimization significantly outperform recent LLM-based attacks. Being\ninherently interpretable, our threat model allows for a comprehensive analysis\nand comparison of jailbreak attacks. We find that effective attacks exploit and\nabuse infrequent bigrams, either selecting the ones absent from real-world text\nor rare ones, e.g., specific to Reddit or code datasets."}
{"id": "2506.09081", "pdf": "https://arxiv.org/pdf/2506.09081.pdf", "abs": "https://arxiv.org/abs/2506.09081", "title": "FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation", "authors": ["Zheqi He", "Yesheng Liu", "Jing-shu Zheng", "Xuejing Li", "Richeng Xuan", "Jin-Ge Yao", "Xi Yang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present FlagEvalMM, an open-source evaluation framework designed to\ncomprehensively assess multimodal models across a diverse range of\nvision-language understanding and generation tasks, such as visual question\nanswering, text-to-image/video generation, and image-text retrieval. We\ndecouple model inference from evaluation through an independent evaluation\nservice, thus enabling flexible resource allocation and seamless integration of\nnew tasks and models. Moreover, FlagEvalMM utilizes advanced inference\nacceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to\nsignificantly enhance evaluation efficiency. Extensive experiments show that\nFlagEvalMM offers accurate and efficient insights into model strengths and\nlimitations, making it a valuable tool for advancing multimodal research. The\nframework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM."}
{"id": "2506.09099", "pdf": "https://arxiv.org/pdf/2506.09099.pdf", "abs": "https://arxiv.org/abs/2506.09099", "title": "Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers", "authors": ["Joshua Barron", "Devin White"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted for oral presentation to Tiny Titans: The next wave of\n  On-Device Learning for Foundational Models Workshop at the 42nd International\n  Conference on Machine Learning", "summary": "The relationship between memorization and generalization in large language\nmodels (LLMs) remains an open area of research, with growing evidence that the\ntwo are deeply intertwined. In this work, we investigate this relationship by\npre-training a series of capacity-limited Transformer models from scratch on\ntwo synthetic character-level tasks designed to separately probe generalization\n(via arithmetic extrapolation) and memorization (via factual recall). We\nobserve a consistent trade-off: small models extrapolate to unseen arithmetic\ncases but fail to memorize facts, while larger models memorize but fail to\nextrapolate. An intermediate-capacity model exhibits a similar shift toward\nmemorization. When trained on both tasks jointly, no model (regardless of size)\nsucceeds at extrapolation. These findings suggest that pre-training may\nintrinsically favor one learning mode over the other. By isolating these\ndynamics in a controlled setting, our study offers insight into how model\ncapacity shapes learning behavior and offers broader implications for the\ndesign and deployment of small language models."}
{"id": "2506.09108", "pdf": "https://arxiv.org/pdf/2506.09108.pdf", "abs": "https://arxiv.org/abs/2506.09108", "title": "SensorLM: Learning the Language of Wearable Sensors", "authors": ["Yuwei Zhang", "Kumar Ayush", "Siyuan Qiao", "A. Ali Heydari", "Girish Narayanswamy", "Maxwell A. Xu", "Ahmed A. Metwally", "Shawn Xu", "Jake Garrison", "Xuhai Xu", "Tim Althoff", "Yun Liu", "Pushmeet Kohli", "Jiening Zhan", "Mark Malhotra", "Shwetak Patel", "Cecilia Mascolo", "Xin Liu", "Daniel McDuff", "Yuzhe Yang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We present SensorLM, a family of sensor-language foundation models that\nenable wearable sensor data understanding with natural language. Despite its\npervasive nature, aligning and interpreting sensor data with language remains\nchallenging due to the lack of paired, richly annotated sensor-text\ndescriptions in uncurated, real-world wearable data. We introduce a\nhierarchical caption generation pipeline designed to capture statistical,\nstructural, and semantic information from sensor data. This approach enabled\nthe curation of the largest sensor-language dataset to date, comprising over\n59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM\nextends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and\nrecovers them as specific variants within a generic architecture. Extensive\nexperiments on real-world tasks in human activity analysis and healthcare\nverify the superior performance of SensorLM over state-of-the-art in zero-shot\nrecognition, few-shot learning, and cross-modal retrieval. SensorLM also\ndemonstrates intriguing capabilities including scaling behaviors, label\nefficiency, sensor captioning, and zero-shot generalization to unseen tasks."}
{"id": "2506.09109", "pdf": "https://arxiv.org/pdf/2506.09109.pdf", "abs": "https://arxiv.org/abs/2506.09109", "title": "CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation", "authors": ["Arnav Yayavaram", "Siddharth Yayavaram", "Simran Khanuja", "Michael Saxon", "Graham Neubig"], "categories": ["cs.CV", "cs.CL"], "comment": "Preprint, under review", "summary": "As text-to-image models become increasingly prevalent, ensuring their\nequitable performance across diverse cultural contexts is critical. Efforts to\nmitigate cross-cultural biases have been hampered by trade-offs, including a\nloss in performance, factual inaccuracies, or offensive outputs. Despite\nwidespread recognition of these challenges, an inability to reliably measure\nthese biases has stalled progress. To address this gap, we introduce CAIRe, a\nnovel evaluation metric that assesses the degree of cultural relevance of an\nimage, given a user-defined set of labels. Our framework grounds entities and\nconcepts in the image to a knowledge base and uses factual information to give\nindependent graded judgments for each culture label. On a manually curated\ndataset of culturally salient but rare items built using language models, CAIRe\nsurpasses all baselines by 28% F1 points. Additionally, we construct two\ndatasets for culturally universal concept, one comprising of T2I-generated\noutputs and another retrieved from naturally occurring data. CAIRe achieves\nPearson's correlations of 0.56 and 0.66 with human ratings on these sets, based\non a 5-point Likert scale of cultural relevance. This demonstrates its strong\nalignment with human judgment across diverse image sources."}
{"id": "2506.09148", "pdf": "https://arxiv.org/pdf/2506.09148.pdf", "abs": "https://arxiv.org/abs/2506.09148", "title": "Adversarial Text Generation with Dynamic Contextual Perturbation", "authors": ["Hetvi Waghela", "Jaydip Sen", "Sneha Rakshit", "Subhasis Dasgupta"], "categories": ["cs.CR", "cs.CL"], "comment": "This is the accepted version of the paper, which was presented at\n  IEEE CALCON. The conference was organized at Jadavpur University, Kolkata,\n  from December 14 to 15, 2025. The paper is six pages long, and it consists of\n  six tables and six figures. This is not the final camera-ready version of the\n  paper", "summary": "Adversarial attacks on Natural Language Processing (NLP) models expose\nvulnerabilities by introducing subtle perturbations to input text, often\nleading to misclassification while maintaining human readability. Existing\nmethods typically focus on word-level or local text segment alterations,\noverlooking the broader context, which results in detectable or semantically\ninconsistent perturbations. We propose a novel adversarial text attack scheme\nnamed Dynamic Contextual Perturbation (DCP). DCP dynamically generates\ncontext-aware perturbations across sentences, paragraphs, and documents,\nensuring semantic fidelity and fluency. Leveraging the capabilities of\npre-trained language models, DCP iteratively refines perturbations through an\nadversarial objective function that balances the dual objectives of inducing\nmodel misclassification and preserving the naturalness of the text. This\ncomprehensive approach allows DCP to produce more sophisticated and effective\nadversarial examples that better mimic natural language patterns. Our\nexperimental results, conducted on various NLP models and datasets, demonstrate\nthe efficacy of DCP in challenging the robustness of state-of-the-art NLP\nsystems. By integrating dynamic contextual analysis, DCP significantly enhances\nthe subtlety and impact of adversarial attacks. This study highlights the\ncritical role of context in adversarial attacks and lays the groundwork for\ncreating more robust NLP systems capable of withstanding sophisticated\nadversarial strategies."}
{"id": "2506.09171", "pdf": "https://arxiv.org/pdf/2506.09171.pdf", "abs": "https://arxiv.org/abs/2506.09171", "title": "Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search", "authors": ["Samuel Holt", "Max Ruiz Luyten", "Thomas Pouplin", "Mihaela van der Schaar"], "categories": ["cs.LG", "cs.AI", "cs.CL", "68T07, 68T20, 68T30, 93E35", "I.2.6; I.2.7; I.2.8"], "comment": "9-page main paper, 1 figure. Accepted for an Oral presentation at the\n  First Workshop on Computer Use Agents (ICML 2025), Vancouver, Canada", "summary": "Large Language Models (LLMs) are increasingly capable but often require\nsignificant guidance or extensive interaction history to perform effectively in\ncomplex, interactive environments. Existing methods may struggle with adapting\nto new information or efficiently utilizing past experiences for multi-step\nreasoning without fine-tuning. We introduce a novel LLM agent framework that\nenhances planning capabilities through in-context learning, facilitated by\natomic fact augmentation and a recursive lookahead search. Our agent learns to\nextract task-critical ``atomic facts'' from its interaction trajectories. These\nfacts dynamically augment the prompts provided to LLM-based components\nresponsible for action proposal, latent world model simulation, and state-value\nestimation. Planning is performed via a depth-limited lookahead search, where\nthe LLM simulates potential trajectories and evaluates their outcomes, guided\nby the accumulated facts and interaction history. This approach allows the\nagent to improve its understanding and decision-making online, leveraging its\nexperience to refine its behavior without weight updates. We provide a\ntheoretical motivation linking performance to the quality of fact-based\nabstraction and LLM simulation accuracy. Empirically, our agent demonstrates\nimproved performance and adaptability on challenging interactive tasks,\nachieving more optimal behavior as it accumulates experience, showcased in\ntasks such as TextFrozenLake and ALFWorld."}
{"id": "2506.09206", "pdf": "https://arxiv.org/pdf/2506.09206.pdf", "abs": "https://arxiv.org/abs/2506.09206", "title": "SimClass: A Classroom Speech Dataset Generated via Game Engine Simulation For Automatic Speech Recognition Research", "authors": ["Ahmed Adel Attia", "Jing Liu", "Carl Espy-Wilson"], "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "The scarcity of large-scale classroom speech data has hindered the\ndevelopment of AI-driven speech models for education. Public classroom datasets\nremain limited, and the lack of a dedicated classroom noise corpus prevents the\nuse of standard data augmentation techniques.\n  In this paper, we introduce a scalable methodology for synthesizing classroom\nnoise using game engines, a framework that extends to other domains. Using this\nmethodology, we present SimClass, a dataset that includes both a synthesized\nclassroom noise corpus and a simulated classroom speech dataset. The speech\ndata is generated by pairing a public children's speech corpus with YouTube\nlecture videos to approximate real classroom interactions in clean conditions.\nOur experiments on clean and noisy speech demonstrate that SimClass closely\napproximates real classroom speech, making it a valuable resource for\ndeveloping robust speech recognition and enhancement models."}
{"id": "2506.09260", "pdf": "https://arxiv.org/pdf/2506.09260.pdf", "abs": "https://arxiv.org/abs/2506.09260", "title": "ThinkQE: Query Expansion via an Evolving Thinking Process", "authors": ["Yibin Lei", "Tao Shen", "Andrew Yates"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Effective query expansion for web search benefits from promoting both\nexploration and result diversity to capture multiple interpretations and facets\nof a query. While recent LLM-based methods have improved retrieval performance\nand demonstrate strong domain generalization without additional training, they\noften generate narrowly focused expansions that overlook these desiderata. We\npropose ThinkQE, a test-time query expansion framework addressing this\nlimitation through two key components: a thinking-based expansion process that\nencourages deeper and comprehensive semantic exploration, and a\ncorpus-interaction strategy that iteratively refines expansions using retrieval\nfeedback from the corpus. Experiments on diverse web search benchmarks (DL19,\nDL20, and BRIGHT) show ThinkQE consistently outperforms prior approaches,\nincluding training-intensive dense retrievers and rerankers."}
{"id": "2506.09289", "pdf": "https://arxiv.org/pdf/2506.09289.pdf", "abs": "https://arxiv.org/abs/2506.09289", "title": "UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench", "authors": ["Boxi Yu", "Yuxuan Zhu", "Pinjia He", "Daniel Kang"], "categories": ["cs.SE", "cs.CL", "D.0; I.2"], "comment": null, "summary": "The advent of Large Language Models (LLMs) has spurred the development of\ncoding agents for real-world code generation. As a widely used benchmark for\nevaluating the code generation capabilities of these agents, SWE-Bench uses\nreal-world problems based on GitHub issues and their corresponding pull\nrequests. However, the manually written test cases included in these pull\nrequests are often insufficient, allowing generated patches to pass the tests\nwithout resolving the underlying issue. To address this challenge, we introduce\nUTGenerator, an LLM-driven test case generator that automatically analyzes\ncodebases and dependencies to generate test cases for real-world Python\nprojects. Building on UTGenerator, we propose UTBoost, a comprehensive\nframework for test case augmentation. In our evaluation, we identified 36 task\ninstances with insufficient test cases and uncovered 345 erroneous patches\nincorrectly labeled as passed in the original SWE Bench. These corrections,\nimpacting 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard\nentries, yield 18 and 11 ranking changes, respectively."}
{"id": "2506.09332", "pdf": "https://arxiv.org/pdf/2506.09332.pdf", "abs": "https://arxiv.org/abs/2506.09332", "title": "Natural Language Guided Ligand-Binding Protein Design", "authors": ["Zhenqiao Song", "Ramith Hettiarachchi", "Chuan Li", "Jianwen Xie", "Lei Li"], "categories": ["cs.LG", "cs.CE", "cs.CL"], "comment": null, "summary": "Can AI protein models follow human language instructions and design proteins\nwith desired functions (e.g. binding to a ligand)? Designing proteins that bind\nto a given ligand is crucial in a wide range of applications in biology and\nchemistry. Most prior AI models are trained on protein-ligand complex data,\nwhich is scarce due to the high cost and time requirements of laboratory\nexperiments. In contrast, there is a substantial body of human-curated text\ndescriptions about protein-ligand interactions and ligand formula. In this\npaper, we propose InstructPro, a family of protein generative models that\nfollow natural language instructions to design ligand-binding proteins. Given a\ntextual description of the desired function and a ligand formula in SMILES,\nInstructPro generates protein sequences that are functionally consistent with\nthe specified instructions. We develop the model architecture, training\nstrategy, and a large-scale dataset, InstructProBench, to support both training\nand evaluation. InstructProBench consists of 9,592,829 triples of (function\ndescription, ligand formula, protein sequence). We train two model variants:\nInstructPro-1B (with 1 billion parameters) and InstructPro-3B~(with 3 billion\nparameters). Both variants consistently outperform strong baselines, including\nProGen2, ESM3, and Pinal. Notably, InstructPro-1B achieves the highest docking\nsuccess rate (81.52% at moderate confidence) and the lowest average root mean\nsquare deviation (RMSD) compared to ground truth structures (4.026{\\AA}).\nInstructPro-3B further descreases the average RMSD to 2.527{\\AA}, demonstrating\nInstructPro's ability to generate ligand-binding proteins that align with the\nfunctional specifications."}
{"id": "2506.09344", "pdf": "https://arxiv.org/pdf/2506.09344.pdf", "abs": "https://arxiv.org/abs/2506.09344", "title": "Ming-Omni: A Unified Multimodal Model for Perception and Generation", "authors": ["Inclusion AI", "Biao Gong", "Cheng Zou", "Chuanyang Zheng", "Chunluan Zhou", "Canxiang Yan", "Chunxiang Jin", "Chunjie Shen", "Dandan Zheng", "Fudong Wang", "Furong Xu", "GuangMing Yao", "Jun Zhou", "Jingdong Chen", "Jianxin Sun", "Jiajia Liu", "Jianjiang Zhu", "Jun Peng", "Kaixiang Ji", "Kaiyou Song", "Kaimeng Ren", "Libin Wang", "Lixiang Ru", "Lele Xie", "Longhua Tan", "Lyuxin Xue", "Lan Wang", "Mochen Bai", "Ning Gao", "Pei Chen", "Qingpei Guo", "Qinglong Zhang", "Qiang Xu", "Rui Liu", "Ruijie Xiong", "Sirui Gao", "Tinghao Liu", "Taisong Li", "Weilong Chai", "Xinyu Xiao", "Xiaomei Wang", "Xiaoxue Chen", "Xiao Lu", "Xiaoyu Li", "Xingning Dong", "Xuzheng Yu", "Yi Yuan", "Yuting Gao", "Yunxiao Sun", "Yipeng Chen", "Yifei Wu", "Yongjie Lyu", "Ziping Ma", "Zipeng Feng", "Zhijiang Fang", "Zhihao Qiu", "Ziyuan Huang", "Zhengyu He"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "comment": "18 pages,8 figures", "summary": "We propose Ming-Omni, a unified multimodal model capable of processing\nimages, text, audio, and video, while demonstrating strong proficiency in both\nspeech and image generation. Ming-Omni employs dedicated encoders to extract\ntokens from different modalities, which are then processed by Ling, an MoE\narchitecture equipped with newly proposed modality-specific routers. This\ndesign enables a single model to efficiently process and fuse multimodal inputs\nwithin a unified framework, thereby facilitating diverse tasks without\nrequiring separate models, task-specific fine-tuning, or structural redesign.\nImportantly, Ming-Omni extends beyond conventional multimodal models by\nsupporting audio and image generation. This is achieved through the integration\nof an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for\nhigh-quality image generation, which also allow the model to engage in\ncontext-aware chatting, perform text-to-speech conversion, and conduct\nversatile image editing. Our experimental results showcase Ming-Omni offers a\npowerful solution for unified perception and generation across all modalities.\nNotably, our proposed Ming-Omni is the first open-source model we are aware of\nto match GPT-4o in modality support, and we release all code and model weights\nto encourage further research and development in the community."}
{"id": "2506.09420", "pdf": "https://arxiv.org/pdf/2506.09420.pdf", "abs": "https://arxiv.org/abs/2506.09420", "title": "A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy", "authors": ["Henry Peng Zou", "Wei-Chieh Huang", "Yaozu Wu", "Chunyu Miao", "Dongyuan Li", "Aiwei Liu", "Yue Zhou", "Yankai Chen", "Weizhi Zhang", "Yangning Li", "Liancheng Fang", "Renhe Jiang", "Philip S. Yu"], "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG", "cs.MA"], "comment": null, "summary": "Recent improvements in large language models (LLMs) have led many researchers\nto focus on building fully autonomous AI agents. This position paper questions\nwhether this approach is the right path forward, as these autonomous systems\nstill have problems with reliability, transparency, and understanding the\nactual requirements of human. We suggest a different approach: LLM-based\nHuman-Agent Systems (LLM-HAS), where AI works with humans rather than replacing\nthem. By keeping human involved to provide guidance, answer questions, and\nmaintain control, these systems can be more trustworthy and adaptable. Looking\nat examples from healthcare, finance, and software development, we show how\nhuman-AI teamwork can handle complex tasks better than AI working alone. We\nalso discuss the challenges of building these collaborative systems and offer\npractical solutions. This paper argues that progress in AI should not be\nmeasured by how independent systems become, but by how well they can work with\nhumans. The most promising future for AI is not in systems that take over human\nroles, but in those that enhance human capabilities through meaningful\npartnership."}
{"id": "2506.09448", "pdf": "https://arxiv.org/pdf/2506.09448.pdf", "abs": "https://arxiv.org/abs/2506.09448", "title": "OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary", "authors": ["Yui Sudo", "Yusuke Fujita", "Atsushi Kojima", "Tomoya Mizumoto", "Lianbo Liu"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Speech foundation models (SFMs), such as Open Whisper-Style Speech Models\n(OWSM), are trained on massive datasets to achieve accurate automatic speech\nrecognition. However, even SFMs struggle to accurately recognize rare and\nunseen words. While contextual biasing (CB) is a promising approach to improve\nrecognition of such words, most CB methods are trained from scratch, resulting\nin lower performance than SFMs due to the lack of pre-trained knowledge. This\npaper integrates an existing CB method with OWSM v3.1 while freezing its\npre-trained parameters. By leveraging the knowledge embedded in SFMs, the\nproposed method enables effective CB while preserving the advantages of SFMs,\neven with a small dataset. Experimental results show that the proposed method\nimproves the biasing word error rate (B-WER) by 11.6 points, resulting in a 0.9\npoint improvement in the overall WER while reducing the real-time factor by\n7.5% compared to the non-biasing baseline on the LibriSpeech 100 test-clean\nset."}
{"id": "2506.09452", "pdf": "https://arxiv.org/pdf/2506.09452.pdf", "abs": "https://arxiv.org/abs/2506.09452", "title": "Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform", "authors": ["Jay Roberts", "Kyle Mylonakis", "Sidhartha Roy", "Kaan Kale"], "categories": ["cs.LG", "cs.CL", "cs.CR", "cs.IT", "math.IT", "I.2.7; I.2.m"], "comment": "Submitted to IEEE S&P 2026", "summary": "The high cost of ownership of AI compute infrastructure and challenges of\nrobust serving of large language models (LLMs) has led to a surge in managed\nModel-as-a-service deployments. Even when enterprises choose on-premises\ndeployments, the compute infrastructure is typically shared across many teams\nin order to maximize the return on investment. In both scenarios the deployed\nmodels operate only on plaintext data, and so enterprise data owners must allow\ntheir data to appear in plaintext on a shared or multi-tenant compute\ninfrastructure. This results in data owners with private or sensitive data\nbeing hesitant or restricted in what data they use with these types of\ndeployments. In this work we introduce the Stained Glass Transform, a learned,\nstochastic, and sequence dependent transformation of the word embeddings of an\nLLM which information theoretically provides privacy to the input of the LLM\nwhile preserving the utility of model. We theoretically connect a particular\nclass of Stained Glass Transforms to the theory of mutual information of\nGaussian Mixture Models. We then calculate a-postiori privacy estimates, based\non mutual information, and verify the privacy and utility of instances of\ntransformed embeddings through token level metrics of privacy and standard LLM\nperformance benchmarks."}
{"id": "2506.09521", "pdf": "https://arxiv.org/pdf/2506.09521.pdf", "abs": "https://arxiv.org/abs/2506.09521", "title": "You Are What You Say: Exploiting Linguistic Content for VoicePrivacy Attacks", "authors": ["Ünal Ege Gaznepoglu", "Anna Leschanowsky", "Ahmad Aloradi", "Prachi Singh", "Daniel Tenbrinck", "Emanuël A. P. Habets", "Nils Peters"], "categories": ["eess.AS", "cs.CL"], "comment": "5 pages, 6 figures, 1 table, accepted at INTERSPEECH 2025", "summary": "Speaker anonymization systems hide the identity of speakers while preserving\nother information such as linguistic content and emotions. To evaluate their\nprivacy benefits, attacks in the form of automatic speaker verification (ASV)\nsystems are employed. In this study, we assess the impact of intra-speaker\nlinguistic content similarity in the attacker training and evaluation datasets,\nby adapting BERT, a language model, as an ASV system. On the VoicePrivacy\nAttacker Challenge datasets, our method achieves a mean equal error rate (EER)\nof 35%, with certain speakers attaining EERs as low as 2%, based solely on the\ntextual content of their utterances. Our explainability study reveals that the\nsystem decisions are linked to semantically similar keywords within utterances,\nstemming from how LibriSpeech is curated. Our study suggests reworking the\nVoicePrivacy datasets to ensure a fair and unbiased evaluation and challenge\nthe reliance on global EER for privacy evaluations."}
{"id": "2506.09522", "pdf": "https://arxiv.org/pdf/2506.09522.pdf", "abs": "https://arxiv.org/abs/2506.09522", "title": "Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs", "authors": ["Beomsik Cho", "Jaehyung Kim"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Code available at https://github.com/bscho333/ReVisiT", "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nacross various multimodal tasks by integrating visual perception with language\nunderstanding. However, conventional decoding strategies of LVLMs often fail to\nsuccessfully utilize visual information, leading to visually ungrounded\nresponses. While various approaches have been proposed to address this\nlimitation, they typically require additional training, multi-step inference\nprocedures, or external model dependencies. This paper introduces ReVisiT, a\nsimple yet effective decoding method that references vision tokens to guide the\ntext generation process in LVLMs. Our approach leverages the semantic\ninformation embedded within vision tokens by projecting them into the text\ntoken distribution space, and dynamically selecting the most relevant vision\ntoken at each decoding step through constrained divergence minimization. This\nselected vision token is then used to refine the output distribution to better\nincorporate visual semantics. Experiments on three LVLM hallucination\nbenchmarks with two recent LVLMs demonstrate that ReVisiT consistently enhances\nvisual grounding with minimal computational overhead. Moreover, our method\nachieves competitive or superior results relative to state-of-the-art baselines\nwhile reducing computational costs for up to $2\\times$."}
{"id": "2506.09532", "pdf": "https://arxiv.org/pdf/2506.09532.pdf", "abs": "https://arxiv.org/abs/2506.09532", "title": "Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models", "authors": ["Shuai Wang", "Zhenhua Liu", "Jiaheng Wei", "Xuanwu Yin", "Dong Li", "Emad Barsoum"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "We present Athena-PRM, a multimodal process reward model (PRM) designed to\nevaluate the reward score for each step in solving complex reasoning problems.\nDeveloping high-performance PRMs typically demands significant time and\nfinancial investment, primarily due to the necessity for step-level annotations\nof reasoning steps. Conventional automated labeling methods, such as Monte\nCarlo estimation, often produce noisy labels and incur substantial\ncomputational costs. To efficiently generate high-quality process-labeled data,\nwe propose leveraging prediction consistency between weak and strong completers\nas a criterion for identifying reliable process labels. Remarkably, Athena-PRM\ndemonstrates outstanding effectiveness across various scenarios and benchmarks\nwith just 5,000 samples. Furthermore, we also develop two effective strategies\nto improve the performance of PRMs: ORM initialization and up-sampling for\nnegative data. We validate our approach in three specific scenarios:\nverification for test time scaling, direct evaluation of reasoning step\ncorrectness, and reward ranked fine-tuning. Our Athena-PRM consistently\nachieves superior performance across multiple benchmarks and scenarios.\nNotably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances\nperformance by 10.2 points on WeMath and 7.1 points on MathVista for test time\nscaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in\nVisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,\nshowcasing its robust capability to accurately assess the correctness of the\nreasoning step. Additionally, utilizing Athena-PRM as the reward model, we\ndevelop Athena-7B with reward ranked fine-tuning and outperforms baseline with\na significant margin on five benchmarks."}
{"id": "2506.09600", "pdf": "https://arxiv.org/pdf/2506.09600.pdf", "abs": "https://arxiv.org/abs/2506.09600", "title": "Effective Red-Teaming of Policy-Adherent Agents", "authors": ["Itay Nakash", "George Kour", "Koren Lazar", "Matan Vetzler", "Guy Uziel", "Ateret Anaby-Tavor"], "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Task-oriented LLM-based agents are increasingly used in domains with strict\npolicies, such as refund eligibility or cancellation rules. The challenge lies\nin ensuring that the agent consistently adheres to these rules and policies,\nappropriately refusing any request that would violate them, while still\nmaintaining a helpful and natural interaction. This calls for the development\nof tailored design and evaluation methodologies to ensure agent resilience\nagainst malicious user behavior. We propose a novel threat model that focuses\non adversarial users aiming to exploit policy-adherent agents for personal\nbenefit. To address this, we present CRAFT, a multi-agent red-teaming system\nthat leverages policy-aware persuasive strategies to undermine a\npolicy-adherent agent in a customer-service scenario, outperforming\nconventional jailbreak methods such as DAN prompts, emotional manipulation, and\ncoercive. Building upon the existing tau-bench benchmark, we introduce\ntau-break, a complementary benchmark designed to rigorously assess the agent's\nrobustness against manipulative user behavior. Finally, we evaluate several\nstraightforward yet effective defense strategies. While these measures provide\nsome protection, they fall short, highlighting the need for stronger,\nresearch-driven safeguards to protect policy-adherent agents from adversarial\nattacks"}
{"id": "2506.09659", "pdf": "https://arxiv.org/pdf/2506.09659.pdf", "abs": "https://arxiv.org/abs/2506.09659", "title": "Intent Factored Generation: Unleashing the Diversity in Your Language Model", "authors": ["Eltayeb Ahmed", "Uljad Berdica", "Martha Elliott", "Danijela Horak", "Jakob N. Foerster"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Obtaining multiple meaningfully diverse, high quality samples from Large\nLanguage Models for a fixed prompt remains an open challenge. Current methods\nfor increasing diversity often only operate at the token-level, paraphrasing\nthe same response. This is problematic because it leads to poor exploration on\nreasoning problems and to unengaging, repetitive conversational agents. To\naddress this we propose Intent Factored Generation (IFG), factorising the\nsampling process into two stages. First, we sample a semantically dense intent,\ne.g., a summary or keywords. Second, we sample the final response conditioning\non both the original prompt and the intent from the first stage. This allows us\nto use a higher temperature during the intent step to promote conceptual\ndiversity, and a lower temperature during the final generation to ensure the\noutputs are coherent and self-consistent. Additionally, we find that prompting\nthe model to explicitly state its intent for each step of the chain-of-thought\nbefore generating the step is beneficial for reasoning tasks. We demonstrate\nour method's effectiveness across a diverse set of tasks. We show this method\nimproves both pass@k and Reinforcement Learning from Verifier Feedback on maths\nand code tasks. For instruction-tuning, we combine IFG with Direct Preference\nOptimisation to increase conversational diversity without sacrificing reward.\nFinally, we achieve higher diversity while maintaining the quality of\ngenerations on a general language modelling task, using a new dataset of reader\ncomments and news articles that we collect and open-source. In summary, we\npresent a simple method of increasing the sample diversity of LLMs while\nmaintaining performance. This method can be implemented by changing the prompt\nand varying the temperature during generation, making it easy to integrate into\nmany algorithms for gains across various applications."}
{"id": "2506.09691", "pdf": "https://arxiv.org/pdf/2506.09691.pdf", "abs": "https://arxiv.org/abs/2506.09691", "title": "Adding simple structure at inference improves Vision-Language Compositionality", "authors": ["Imanol Miranda", "Ander Salaberria", "Eneko Agirre", "Gorka Azkune"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for\nimage-text retrieval tasks. However, those models struggle with\ncompositionality, showing a bag-of-words-like behavior that limits their\nretrieval performance. Many different training approaches have been proposed to\nimprove the vision-language compositionality capabilities of those models. In\ncomparison, inference-time techniques have received little attention. In this\npaper, we propose to add simple structure at inference, where, given an image\nand a caption: i) we divide the image into different smaller crops, ii) we\nextract text segments, capturing objects, attributes and relations, iii) using\na VLM, we find the image crops that better align with text segments obtaining\nmatches, and iv) we compute the final image-text similarity aggregating the\nindividual similarities of the matches. Based on various popular dual encoder\nVLMs, we evaluate our approach in controlled and natural datasets for VL\ncompositionality. We find that our approach consistently improves the\nperformance of evaluated VLMs without any training, which shows the potential\nof inference-time techniques. The results are especially good for\nattribute-object binding as shown in the controlled dataset. As a result of an\nextensive analysis: i) we show that processing image crops is actually\nessential for the observed gains in performance, and ii) we identify specific\nareas to further improve inference-time approaches."}
{"id": "2506.09707", "pdf": "https://arxiv.org/pdf/2506.09707.pdf", "abs": "https://arxiv.org/abs/2506.09707", "title": "Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements", "authors": ["Suhas BN", "Andrew M. Sherrill", "Jyoti Alaparthi", "Dominik Mattioli", "Rosa I. Arriaga", "Chris W. Wiese", "Saeed Abdullah"], "categories": ["eess.AS", "cs.CL", "cs.HC", "68T07", "I.2.7; I.5.4; H.5.2"], "comment": "5 pages, 2 figures", "summary": "Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic\nstress disorder (PTSD), but evaluating therapist fidelity remains\nlabor-intensive due to the need for manual review of session recordings. We\npresent a method for the automatic temporal localization of key PE fidelity\nelements -- identifying their start and stop times -- directly from session\naudio and transcripts. Our approach fine-tunes a large pre-trained\naudio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process\nfocused 30-second windows of audio-transcript input. Fidelity labels for three\ncore protocol phases -- therapist orientation (P1), imaginal exposure (P2), and\npost-imaginal processing (P3) -- are generated via LLM-based prompting and\nverified by trained raters. The model is trained to predict normalized boundary\noffsets using soft supervision guided by task-specific prompts. On a dataset of\n313 real PE sessions, our best configuration (LoRA rank 8, 30s windows)\nachieves a mean absolute error (MAE) of 5.3 seconds across tasks. We further\nanalyze the effects of window size and LoRA rank, highlighting the importance\nof context granularity and model adaptation. This work introduces a scalable\nframework for fidelity tracking in PE therapy, with potential to support\nclinician training, supervision, and quality assurance."}
{"id": "2506.09804", "pdf": "https://arxiv.org/pdf/2506.09804.pdf", "abs": "https://arxiv.org/abs/2506.09804", "title": "Regularizing Learnable Feature Extraction for Automatic Speech Recognition", "authors": ["Peter Vieting", "Maximilian Kannen", "Benedikt Hilmes", "Ralf Schlüter", "Hermann Ney"], "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "comment": "Accepted at Interspeech 2025", "summary": "Neural front-ends are an appealing alternative to traditional, fixed feature\nextraction pipelines for automatic speech recognition (ASR) systems since they\ncan be directly trained to fit the acoustic model. However, their performance\noften falls short compared to classical methods, which we show is largely due\nto their increased susceptibility to overfitting. This work therefore\ninvestigates regularization methods for training ASR models with learnable\nfeature extraction front-ends. First, we examine audio perturbation methods and\nshow that larger relative improvements can be obtained for learnable features.\nAdditionally, we identify two limitations in the standard use of SpecAugment\nfor these front-ends and propose masking in the short time Fourier transform\n(STFT)-domain as a simple but effective modification to address these\nchallenges. Finally, integrating both regularization approaches effectively\ncloses the performance gap between traditional and learnable features."}
{"id": "2506.09851", "pdf": "https://arxiv.org/pdf/2506.09851.pdf", "abs": "https://arxiv.org/abs/2506.09851", "title": "Advancing Exchange Rate Forecasting: Leveraging Machine Learning and AI for Enhanced Accuracy in Global Financial Markets", "authors": ["Md. Yeasin Rahat", "Rajan Das Gupta", "Nur Raisa Rahman", "Sudipto Roy Pritom", "Samiur Rahman Shakir", "Md Imrul Hasan Showmick", "Md. Jakir Hossen"], "categories": ["q-fin.ST", "cs.CL", "cs.LG"], "comment": "Accepted in MECON 2025", "summary": "The prediction of foreign exchange rates, such as the US Dollar (USD) to\nBangladeshi Taka (BDT), plays a pivotal role in global financial markets,\ninfluencing trade, investments, and economic stability. This study leverages\nhistorical USD/BDT exchange rate data from 2018 to 2023, sourced from Yahoo\nFinance, to develop advanced machine learning models for accurate forecasting.\nA Long Short-Term Memory (LSTM) neural network is employed, achieving an\nexceptional accuracy of 99.449%, a Root Mean Square Error (RMSE) of 0.9858, and\na test loss of 0.8523, significantly outperforming traditional methods like\nARIMA (RMSE 1.342). Additionally, a Gradient Boosting Classifier (GBC) is\napplied for directional prediction, with backtesting on a $10,000 initial\ncapital revealing a 40.82% profitable trade rate, though resulting in a net\nloss of $20,653.25 over 49 trades. The study analyzes historical trends,\nshowing a decline in BDT/USD rates from 0.012 to 0.009, and incorporates\nnormalized daily returns to capture volatility. These findings highlight the\npotential of deep learning in forex forecasting, offering traders and\npolicymakers robust tools to mitigate risks. Future work could integrate\nsentiment analysis and real-time economic indicators to further enhance model\nadaptability in volatile markets."}
{"id": "2506.09953", "pdf": "https://arxiv.org/pdf/2506.09953.pdf", "abs": "https://arxiv.org/abs/2506.09953", "title": "Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over Videos", "authors": ["Benjamin Reichman", "Constantin Patsch", "Jack Truxal", "Atishay Jain", "Larry Heck"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "In outside knowledge visual question answering (OK-VQA), the model must\nidentify relevant visual information within an image and incorporate external\nknowledge to accurately respond to a question. Extending this task to a\nvisually grounded dialogue setting based on videos, a conversational model must\nboth recognize pertinent visual details over time and answer questions where\nthe required information is not necessarily present in the visual information.\nMoreover, the context of the overall conversation must be considered for the\nsubsequent dialogue. To explore this task, we introduce a dataset comprised of\n$2,017$ videos with $5,986$ human-annotated dialogues consisting of $40,954$\ninterleaved dialogue turns. While the dialogue context is visually grounded in\nspecific video segments, the questions further require external knowledge that\nis not visually present. Thus, the model not only has to identify relevant\nvideo parts but also leverage external knowledge to converse within the\ndialogue. We further provide several baselines evaluated on our dataset and\nshow future challenges associated with this task. The dataset is made publicly\navailable here: https://github.com/c-patsch/OKCV."}
{"id": "2506.09998", "pdf": "https://arxiv.org/pdf/2506.09998.pdf", "abs": "https://arxiv.org/abs/2506.09998", "title": "Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized Rejection Sampling", "authors": ["Tim Z. Xiao", "Johannes Zenn", "Zhen Liu", "Weiyang Liu", "Robert Bamler", "Bernhard Schölkopf"], "categories": ["cs.LG", "cs.CL"], "comment": "Technical Report v1 (21 pages, 14 figures)", "summary": "Large language models (LLMs) can often accurately describe probability\ndistributions using natural language, yet they still struggle to generate\nfaithful samples from them. This mismatch limits their use in tasks requiring\nreliable stochasticity, such as Monte Carlo methods, agent-based simulations,\nand randomized decision-making. We investigate this gap between knowledge and\nsampling in the context of Bernoulli distributions. We introduce Verbalized\nRejection Sampling (VRS), a natural-language adaptation of classical rejection\nsampling that prompts the LLM to reason about and accept or reject proposed\nsamples. Despite relying on the same Bernoulli mechanism internally, VRS\nsubstantially reduces sampling bias across models. We provide theoretical\nanalysis showing that, under mild assumptions, VRS improves over direct\nsampling, with gains attributable to both the algorithm and prompt design. More\nbroadly, our results show how classical probabilistic tools can be verbalized\nand embedded into LLM workflows to improve reliability, without requiring\naccess to model internals or heavy prompt engineering."}
{"id": "2305.14725", "pdf": "https://arxiv.org/pdf/2305.14725.pdf", "abs": "https://arxiv.org/abs/2305.14725", "title": "AMELI: Enhancing Multimodal Entity Linking with Fine-Grained Attributes", "authors": ["Barry Menglong Yao", "Sijia Wang", "Yu Chen", "Qifan Wang", "Minqian Liu", "Zhiyang Xu", "Licheng Yu", "Lifu Huang"], "categories": ["cs.CL", "I.2.7"], "comment": "19 pages, 7 figures", "summary": "We propose attribute-aware multimodal entity linking, where the input\nconsists of a mention described with a text paragraph and images, and the goal\nis to predict the corresponding target entity from a multimodal knowledge base\n(KB) where each entity is also accompanied by a text description, visual\nimages, and a collection of attributes that present the meta-information of the\nentity in a structured format. To facilitate this research endeavor, we\nconstruct AMELI, encompassing a new multimodal entity linking benchmark dataset\nthat contains 16,735 mentions described in text and associated with 30,472\nimages, and a multimodal knowledge base that covers 34,690 entities along with\n177,873 entity images and 798,216 attributes. To establish baseline performance\non AMELI, we experiment with several state-of-the-art architectures for\nmultimodal entity linking and further propose a new approach that incorporates\nattributes of entities into disambiguation. Experimental results and extensive\nqualitative analysis demonstrate that extracting and understanding the\nattributes of mentions from their text descriptions and visual images play a\nvital role in multimodal entity linking. To the best of our knowledge, we are\nthe first to integrate attributes in the multimodal entity linking task. The\nprograms, model checkpoints, and the dataset are publicly available at\nhttps://github.com/VT-NLP/Ameli."}
{"id": "2402.16733", "pdf": "https://arxiv.org/pdf/2402.16733.pdf", "abs": "https://arxiv.org/abs/2402.16733", "title": "DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing", "authors": ["Haneul Yoo", "Jieun Han", "So-Yeon Ahn", "Alice Oh"], "categories": ["cs.CL", "cs.AI"], "comment": "To appear in ACL 2025. arXiv admin note: text overlap with\n  arXiv:2310.05191", "summary": "Automated essay scoring (AES) is a useful tool in English as a Foreign\nLanguage (EFL) writing education, offering real-time essay scores for students\nand instructors. However, previous AES models were trained on essays and scores\nirrelevant to the practical scenarios of EFL writing education and usually\nprovided a single holistic score due to the lack of appropriate datasets. In\nthis paper, we release DREsS, a large-scale, standard dataset for rubric-based\nautomated essay scoring with 48.9K samples in total. DREsS comprises three\nsub-datasets: DREsS_New, DREsS_Std., and DREsS_CASE. We collect DREsS_New, a\nreal-classroom dataset with 2.3K essays authored by EFL undergraduate students\nand scored by English education experts. We also standardize existing\nrubric-based essay scoring datasets as DREsS_Std. We suggest CASE, a\ncorruption-based augmentation strategy for essays, which generates 40.1K\nsynthetic samples of DREsS_CASE and improves the baseline results by 45.44%.\nDREsS will enable further research to provide a more accurate and practical AES\nsystem for EFL writing education."}
{"id": "2404.01129", "pdf": "https://arxiv.org/pdf/2404.01129.pdf", "abs": "https://arxiv.org/abs/2404.01129", "title": "Emphasising Structured Information: Integrating Abstract Meaning Representation into LLMs for Enhanced Open-Domain Dialogue Evaluation", "authors": ["Bohao Yang", "Kun Zhao", "Dong Liu", "Liang Zhan", "Chenghua Lin"], "categories": ["cs.CL"], "comment": null, "summary": "Automatic open-domain dialogue evaluation has attracted increasing attention,\nyet remains challenging due to the complexity of assessing response\nappropriateness. Traditional evaluation metrics, typically trained with true\npositive and randomly selected negative responses, tend to assign higher scores\nto responses that share greater content similarity with contexts. However,\nadversarial negative responses, despite possessing high lexical overlap with\ncontexts, can be semantically incongruous. Consequently, existing metrics\nstruggle to effectively evaluate such responses, resulting in low correlations\nwith human judgments. While recent studies have demonstrated the effectiveness\nof Large Language Models (LLMs) for open-domain dialogue evaluation, they still\nface challenges in handling adversarial negative examples. We propose a novel\nevaluation framework that integrates Abstract Meaning Representation (AMR)\nenhanced domain-specific language models (SLMs) with LLMs. Our SLMs explicitly\nincorporate AMR graph information through a gating mechanism for enhanced\nsemantic representation learning, while both SLM predictions and AMR knowledge\nare integrated into LLM prompts for robust evaluation. Extensive experiments on\nopen-domain dialogue evaluation tasks demonstrate the superiority of our method\ncompared to state-of-the-art baselines. Our comprehensive ablation studies\nreveal that AMR graph information contributes substantially more to performance\nimprovements. Our framework achieves strong correlations with human judgments\nacross multiple datasets, establishing a new benchmark for dialogue evaluation.\nOur code and data are publicly available."}
{"id": "2405.14259", "pdf": "https://arxiv.org/pdf/2405.14259.pdf", "abs": "https://arxiv.org/abs/2405.14259", "title": "Let's Fuse Step by Step: A Generative Fusion Decoding Algorithm with LLMs for Robust and Instruction-Aware ASR and OCR", "authors": ["Chan-Jan Hsu", "Yi-Chang Chen", "Feng-Ting Liao", "Pei-Chen Ho", "Yu-Hsiang Wang", "Po-Chun Hsu", "Da-shan Shiu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We propose \"Generative Fusion Decoding\" (GFD), a novel shallow fusion\nframework designed to integrate large language models (LLMs) into cross-modal\ntext recognition systems for automatic speech recognition (ASR) and optical\ncharacter recognition (OCR). We derive the necessary formulations to enable GFD\nto operate across mismatched token spaces of different models by calculating\nlikelihood at the byte level, thereby enabling seamless fusion and synchronous\nprogression during the decoding process. GFD is plug-and-play by design, making\nit readily compatible with various auto-regressive models without the need for\nany re-training. GFD proves effective for general ASR and OCR tasks through\nintermediate and frequent interactions with LLMs, surpassing cascaded methods\nin English and Mandarin benchmarks. In addition, GFD transfers in-context\nlearning abilities of LLMs and allows for adaptive ASR in instruction-aware and\nlong-context settings, yielding significant WER reductions of up to 17.7\\%."}
{"id": "2406.06144", "pdf": "https://arxiv.org/pdf/2406.06144.pdf", "abs": "https://arxiv.org/abs/2406.06144", "title": "Language Models Resist Alignment: Evidence From Data Compression", "authors": ["Jiaming Ji", "Kaile Wang", "Tianyi Qiu", "Boyuan Chen", "Jiayi Zhou", "Changye Li", "Hantao Lou", "Juntao Dai", "Yunhuai Liu", "Yaodong Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL2025 Main", "summary": "Large language models (LLMs) may exhibit unintended or undesirable behaviors.\nRecent works have concentrated on aligning LLMs to mitigate harmful outputs.\nDespite these efforts, some anomalies indicate that even a well-conducted\nalignment process can be easily circumvented, whether intentionally or\naccidentally. Does alignment fine-tuning yield have robust effects on models,\nor are its impacts merely superficial? In this work, we make the first\nexploration of this phenomenon from both theoretical and empirical\nperspectives. Empirically, we demonstrate the $\\mathbf{elasticity}$ of\npost-alignment models, i.e., the tendency to revert to the behavior\ndistribution formed during the pre-training phase upon further fine-tuning.\nLeveraging compression theory, we formally deduce that fine-tuning\ndisproportionately undermines alignment relative to pre-training, potentially\nby orders of magnitude. We validate the presence of elasticity through\nexperiments on models of varying types and scales. Specifically, we find that\nmodel performance declines rapidly before reverting to the pre-training\ndistribution, after which the rate of decline drops significantly. Furthermore,\nwe further reveal that elasticity positively correlates with the increased\nmodel size and the expansion of pre-training data. Our findings underscore the\nneed to address the inherent elasticity of LLMs to mitigate their resistance to\nalignment. The model weight and code are available at\npku-lm-resist-alignment.github.io."}
{"id": "2406.08726", "pdf": "https://arxiv.org/pdf/2406.08726.pdf", "abs": "https://arxiv.org/abs/2406.08726", "title": "Standard Language Ideology in AI-Generated Language", "authors": ["Genevieve Smith", "Eve Fleisig", "Madeline Bossi", "Ishita Rustagi", "Xavier Yin"], "categories": ["cs.CL"], "comment": null, "summary": "Standard language ideology is reflected and reinforced in language generated\nby large language models (LLMs). We present a faceted taxonomy of open problems\nthat illustrate how standard language ideology manifests in AI-generated\nlanguage, alongside implications for minoritized language communities and\nsociety more broadly. We introduce the concept of standard AI-generated\nlanguage ideology, a process through which LLMs position \"standard\"\nlanguages--particularly Standard American English (SAE)--as the linguistic\ndefault, reinforcing the perception that SAE is the most \"appropriate\"\nlanguage. We then discuss ongoing tensions around what constitutes desirable\nsystem behavior, as well as advantages and drawbacks of generative AI tools\nattempting, or refusing, to imitate different English language varieties.\nRather than prescribing narrow technical fixes, we offer three recommendations\nfor researchers, practitioners, and funders that focus on shifting structural\nconditions and supporting more emancipatory outcomes for diverse language\ncommunities."}
{"id": "2406.14230", "pdf": "https://arxiv.org/pdf/2406.14230.pdf", "abs": "https://arxiv.org/abs/2406.14230", "title": "Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing", "authors": ["Han Jiang", "Xiaoyuan Yi", "Zhihua Wei", "Ziang Xiao", "Shu Wang", "Xing Xie"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "ICML 2025", "summary": "Warning: Contains harmful model outputs. Despite significant advancements,\nthe propensity of Large Language Models (LLMs) to generate harmful and\nunethical content poses critical challenges. Measuring value alignment of LLMs\nbecomes crucial for their regulation and responsible deployment. Although\nnumerous benchmarks have been constructed to assess social bias, toxicity, and\nethical issues in LLMs, those static benchmarks suffer from evaluation\nchronoeffect, in which, as models rapidly evolve, existing benchmarks may leak\ninto training data or become saturated, overestimating ever-developing LLMs. To\ntackle this problem, we propose GETA, a novel generative evolving testing\napproach based on adaptive testing methods in measurement theory. Unlike\ntraditional adaptive testing methods that rely on a static test item pool, GETA\nprobes the underlying moral boundaries of LLMs by dynamically generating test\nitems tailored to model capability. GETA co-evolves with LLMs by learning a\njoint distribution of item difficulty and model value conformity, thus\neffectively addressing evaluation chronoeffect. We evaluated various popular\nLLMs with GETA and demonstrated that 1) GETA can dynamically create\ndifficulty-tailored test items and 2) GETA's evaluation results are more\nconsistent with models' performance on unseen OOD and i.i.d. items, laying the\ngroundwork for future evaluation paradigms."}
{"id": "2406.17761", "pdf": "https://arxiv.org/pdf/2406.17761.pdf", "abs": "https://arxiv.org/abs/2406.17761", "title": "CaLMQA: Exploring culturally specific long-form question answering across 23 languages", "authors": ["Shane Arora", "Marzena Karpinska", "Hung-Ting Chen", "Ipsita Bhattacharjee", "Mohit Iyyer", "Eunsol Choi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "46 pages, 26 figures. Accepted as a main conference paper at ACL\n  2025. Code and data available at https://github.com/2015aroras/CaLMQA .\n  Dataset expanded to 51.7K questions", "summary": "Despite rising global usage of large language models (LLMs), their ability to\ngenerate long-form answers to culturally specific questions remains unexplored\nin many languages. To fill this gap, we perform the first study of textual\nmultilingual long-form QA by creating CaLMQA, a dataset of 51.7K culturally\nspecific questions across 23 different languages. We define culturally specific\nquestions as those that refer to concepts unique to one or a few cultures, or\nhave different answers depending on the cultural or regional context. We obtain\nthese questions by crawling naturally-occurring questions from community web\nforums in high-resource languages, and by hiring native speakers to write\nquestions in under-resourced, rarely-studied languages such as Fijian and\nKirundi. Our data collection methodologies are translation-free, enabling the\ncollection of culturally unique questions like \"Kuber iki umwami wa mbere\nw'uburundi yitwa Ntare?\" (Kirundi; English translation: \"Why was the first king\nof Burundi called Ntare (Lion)?\"). We evaluate factuality, relevance and\nsurface-level quality of LLM-generated long-form answers, finding that (1) for\nmany languages, even the best models make critical surface-level errors (e.g.,\nanswering in the wrong language, repetition), especially for low-resource\nlanguages; and (2) answers to culturally specific questions contain more\nfactual errors than answers to culturally agnostic questions -- questions that\nhave consistent meaning and answer across many cultures. We release CaLMQA to\nfacilitate future research in cultural and multilingual long-form QA."}
{"id": "2407.13329", "pdf": "https://arxiv.org/pdf/2407.13329.pdf", "abs": "https://arxiv.org/abs/2407.13329", "title": "CiteFusion: An Ensemble Framework for Citation Intent Classification Harnessing Dual-Model Binary Couples and SHAP Analyses", "authors": ["Lorenzo Paolini", "Sahar Vahdati", "Angelo Di Iorio", "Robert Wardenga", "Ivan Heibi", "Silvio Peroni"], "categories": ["cs.CL"], "comment": "Submitted to Scientometrics Journal", "summary": "Understanding the motivations underlying scholarly citations is essential to\nevaluate research impact and pro-mote transparent scholarly communication. This\nstudy introduces CiteFusion, an ensemble framework designed to address the\nmulti-class Citation Intent Classification task on two benchmark datasets:\nSciCite and ACL-ARC. The framework employs a one-vs-all decomposition of the\nmulti-class task into class-specific binary sub-tasks, leveraging complementary\npairs of SciBERT and XLNet models, independently tuned, for each citation\nintent. The outputs of these base models are aggregated through a feedforward\nneural network meta-classifier to reconstruct the original classification task.\nTo enhance interpretability, SHAP (SHapley Additive exPlanations) is employed\nto analyze token-level contributions, and interactions among base models,\nproviding transparency into the classification dynamics of CiteFusion, and\ninsights about the kind of misclassifications of the ensem-ble. In addition,\nthis work investigates the semantic role of structural context by incorporating\nsection titles, as framing devices, into input sentences, assessing their\npositive impact on classification accuracy. CiteFusion ul-timately demonstrates\nrobust performance in imbalanced and data-scarce scenarios: experimental\nresults show that CiteFusion achieves state-of-the-art performance, with\nMacro-F1 scores of 89.60% on SciCite, and 76.24% on ACL-ARC. Furthermore, to\nensure interoperability and reusability, citation intents from both datasets\nsche-mas are mapped to Citation Typing Ontology (CiTO) object properties,\nhighlighting some overlaps. Finally, we describe and release a web-based\napplication that classifies citation intents leveraging the CiteFusion models\ndeveloped on SciCite."}
{"id": "2408.04211", "pdf": "https://arxiv.org/pdf/2408.04211.pdf", "abs": "https://arxiv.org/abs/2408.04211", "title": "MMREC: LLM Based Multi-Modal Recommender System", "authors": ["Jiahao Tian", "Jinman Zhao", "Zhenkai Wang", "Zhicheng Ding"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "The importance of recommender systems is growing rapidly due to the\nexponential increase in the volume of content generated daily. This surge in\ncontent presents unique challenges for designing effective recommender systems.\nKey among these challenges is the need to effectively leverage the vast amounts\nof natural language data and images that represent user preferences. This paper\npresents a novel approach to enhancing recommender systems by leveraging Large\nLanguage Models (LLMs) and deep learning techniques. The proposed framework\naims to improve the accuracy and relevance of recommendations by incorporating\nmulti-modal information processing and by the use of unified latent space\nrepresentation. The study explores the potential of LLMs to better understand\nand utilize natural language data in recommendation contexts, addressing the\nlimitations of previous methods. The framework efficiently extracts and\nintegrates text and image information through LLMs, unifying diverse modalities\nin a latent space to simplify the learning process for the ranking model.\nExperimental results demonstrate the enhanced discriminative power of the model\nwhen utilizing multi-modal information. This research contributes to the\nevolving field of recommender systems by showcasing the potential of LLMs and\nmulti-modal data integration to create more personalized and contextually\nrelevant recommendations."}
{"id": "2408.14352", "pdf": "https://arxiv.org/pdf/2408.14352.pdf", "abs": "https://arxiv.org/abs/2408.14352", "title": "LogProber: Disentangling confidence from contamination in LLM responses", "authors": ["Nicolas Yax", "Pierre-Yves Oudeyer", "Stefano Palminteri"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "In machine learning, contamination refers to situations where testing data\nleak into the training set. The issue is particularly relevant for the\nevaluation of the performance of Large Language Models (LLMs), which are\ngenerally trained on gargantuan, and generally opaque, corpora of text scraped\nfrom the world wide web. Developing tools to detect contamination is therefore\ncrucial to be able to fairly and properly track the evolution of the\nperformance of LLMs. To date, only a few recent studies have attempted to\naddress the issue of quantifying and detecting contamination in short text\nsequences, such as those commonly found in benchmarks. However, these methods\nhave limitations that can sometimes render them impractical.In the present\npaper, we introduce LogProber, a novel, efficient algorithm that we show to be\nable to detect contamination in a black box setting that tries to tackle some\nof these drawbacks by focusing on the familiarity with the question rather than\nthe answer. Here, we explore the properties of the proposed method in\ncomparison with concurrent approaches, identify its advantages and limitations,\nand illustrate how different forms of contamination can go undetected depending\non the design of the detection algorithm."}
{"id": "2408.16326", "pdf": "https://arxiv.org/pdf/2408.16326.pdf", "abs": "https://arxiv.org/abs/2408.16326", "title": "Critic-CoT: Boosting the reasoning abilities of large language model via Chain-of-thoughts Critic", "authors": ["Xin Zheng", "Jie Lou", "Boxi Cao", "Xueru Wen", "Yuqiu Ji", "Hongyu Lin", "Yaojie Lu", "Xianpei Han", "Debing Zhang", "Le Sun"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 Findings", "summary": "Self-critic has become a crucial mechanism for enhancing the reasoning\nperformance of LLMs. However, current approaches mainly involve basic prompts\nfor intuitive instance-level feedback, which resembles System-1 processes and\nlimits the reasoning capabilities. Moreover, there is a lack of in-depth\ninvestigations into the relationship between LLM's ability to criticize and its\ntask-solving performance. To address these issues, we propose Critic-CoT, a\nnovel framework that pushes LLMs toward System-2-like critic capability.\nThrough a step-wise CoT reasoning paradigm and the automatic construction of\ndistant-supervision data without human annotation, Critic-CoT enables LLMs to\nengage in slow, analytic self-critique and refinement, thereby improving their\nreasoning abilities. Experiments on GSM8K and MATH demonstrate that our\nenhanced model significantly boosts task-solving performance by filtering out\ninvalid solutions or iterative refinement. Furthermore, we investigate the\nintrinsic correlation between critique and task-solving abilities within LLMs,\ndiscovering that these abilities can mutually reinforce each other rather than\nconflict."}
{"id": "2409.00598", "pdf": "https://arxiv.org/pdf/2409.00598.pdf", "abs": "https://arxiv.org/abs/2409.00598", "title": "Automatic Pseudo-Harmful Prompt Generation for Evaluating False Refusals in Large Language Models", "authors": ["Bang An", "Sicheng Zhu", "Ruiyi Zhang", "Michael-Andrei Panaitescu-Liess", "Yuancheng Xu", "Furong Huang"], "categories": ["cs.CL", "cs.CR", "cs.CY", "cs.LG"], "comment": null, "summary": "Safety-aligned large language models (LLMs) sometimes falsely refuse\npseudo-harmful prompts, like \"how to kill a mosquito,\" which are actually\nharmless. Frequent false refusals not only frustrate users but also provoke a\npublic backlash against the very values alignment seeks to protect. In this\npaper, we propose the first method to auto-generate diverse,\ncontent-controlled, and model-dependent pseudo-harmful prompts. Using this\nmethod, we construct an evaluation dataset called PHTest, which is ten times\nlarger than existing datasets, covers more false refusal patterns, and\nseparately labels controversial prompts. We evaluate 20 LLMs on PHTest,\nuncovering new insights due to its scale and labeling. Our findings reveal a\ntrade-off between minimizing false refusals and improving safety against\njailbreak attacks. Moreover, we show that many jailbreak defenses significantly\nincrease the false refusal rates, thereby undermining usability. Our method and\ndataset can help developers evaluate and fine-tune safer and more usable LLMs.\nOur code and dataset are available at\nhttps://github.com/umd-huang-lab/FalseRefusal"}
{"id": "2409.07615", "pdf": "https://arxiv.org/pdf/2409.07615.pdf", "abs": "https://arxiv.org/abs/2409.07615", "title": "MOSAIC: Multiple Observers Spotting AI Content", "authors": ["Matthieu Dubois", "François Yvon", "Pablo Piantanida"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings, code can be found at\n  https://github.com/BaggerOfWords/MOSAIC", "summary": "The dissemination of Large Language Models (LLMs), trained at scale, and\nendowed with powerful text-generating abilities, has made it easier for all to\nproduce harmful, toxic, faked or forged content. In response, various proposals\nhave been made to automatically discriminate artificially generated from\nhuman-written texts, typically framing the problem as a binary classification\nproblem. Early approaches evaluate an input document with a well-chosen\ndetector LLM, assuming that low-perplexity scores reliably signal machine-made\ncontent. More recent systems instead consider two LLMs and compare their\nprobability distributions over the document to further discriminate when\nperplexity alone cannot. However, using a fixed pair of models can induce\nbrittleness in performance. We extend these approaches to the ensembling of\nseveral LLMs and derive a new, theoretically grounded approach to combine their\nrespective strengths. Our experiments, conducted with various generator LLMs,\nindicate that this approach effectively leverages the strengths of each model,\nresulting in robust detection performance across multiple domains. Our code and\ndata are available at https://github.com/BaggerOfWords/MOSAIC ."}
{"id": "2409.15912", "pdf": "https://arxiv.org/pdf/2409.15912.pdf", "abs": "https://arxiv.org/abs/2409.15912", "title": "Explaining word embeddings with perfect fidelity: Case study in research impact prediction", "authors": ["Lucie Dvorackova", "Marcin P. Joachimiak", "Michal Cerny", "Adriana Kubecova", "Vilem Sklenak", "Tomas Kliegr"], "categories": ["cs.CL"], "comment": null, "summary": "Best performing approaches for scholarly document quality prediction are\nbased on embedding models, which do not allow direct explanation of classifiers\nas distinct words no longer correspond to the input features for model\ntraining. Although model-agnostic explanation methods such as Local\ninterpretable model-agnostic explanations (LIME) can be applied, these produce\nresults with questionable correspondence to the ML model. We introduce a new\nfeature importance method, Self-model Rated Entities (SMER), for logistic\nregression-based classification models trained on word embeddings. We show that\nSMER has theoretically perfect fidelity with the explained model, as its\nprediction corresponds exactly to the average of predictions for individual\nwords in the text. SMER allows us to reliably determine which words or entities\npositively contribute to predicting impactful articles. Quantitative and\nqualitative evaluation is performed through five diverse experiments conducted\non 50.000 research papers from the CORD-19 corpus. Through an AOPC curve\nanalysis, we experimentally demonstrate that SMER produces better explanations\nthan LIME for logistic regression."}
{"id": "2410.08193", "pdf": "https://arxiv.org/pdf/2410.08193.pdf", "abs": "https://arxiv.org/abs/2410.08193", "title": "GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment", "authors": ["Yuancheng Xu", "Udari Madhushani Sehwag", "Alec Koppel", "Sicheng Zhu", "Bang An", "Furong Huang", "Sumitra Ganesh"], "categories": ["cs.CL"], "comment": "Published at the Thirteenth International Conference on Learning\n  Representations (ICLR 2025)", "summary": "Large Language Models (LLMs) exhibit impressive capabilities but require\ncareful alignment with human preferences. Traditional training-time methods\nfinetune LLMs using human preference datasets but incur significant training\ncosts and require repeated training to handle diverse user preferences.\nTest-time alignment methods address this by using reward models (RMs) to guide\nfrozen LLMs without retraining. However, existing test-time approaches rely on\ntrajectory-level RMs which are designed to evaluate complete responses, making\nthem unsuitable for autoregressive text generation that requires computing\nnext-token rewards from partial responses. To address this, we introduce\nGenARM, a test-time alignment approach that leverages the Autoregressive Reward\nModel--a novel reward parametrization designed to predict next-token rewards\nfor efficient and effective autoregressive generation. Theoretically, we\ndemonstrate that this parametrization can provably guide frozen LLMs toward any\ndistribution achievable by traditional RMs within the KL-regularized\nreinforcement learning framework. Experimental results show that GenARM\nsignificantly outperforms prior test-time alignment baselines and matches the\nperformance of training-time methods. Additionally, GenARM enables efficient\nweak-to-strong guidance, aligning larger LLMs with smaller RMs without the high\ncosts of training larger models. Furthermore, GenARM supports multi-objective\nalignment, allowing real-time trade-offs between preference dimensions and\ncatering to diverse user preferences without retraining. Our project page is\navailable at: https://genarm.github.io."}
{"id": "2410.08674", "pdf": "https://arxiv.org/pdf/2410.08674.pdf", "abs": "https://arxiv.org/abs/2410.08674", "title": "Guidelines for Fine-grained Sentence-level Arabic Readability Annotation", "authors": ["Nizar Habash", "Hanada Taha-Thomure", "Khalid N. Elmadani", "Zeina Zeino", "Abdallah Abushmaes"], "categories": ["cs.CL"], "comment": "Accepted at LAW-XIX at ACL 2025", "summary": "This paper presents the annotation guidelines of the Balanced Arabic\nReadability Evaluation Corpus (BAREC), a large-scale resource for fine-grained\nsentence-level readability assessment in Arabic. BAREC includes 69,441\nsentences (1M+ words) labeled across 19 levels, from kindergarten to\npostgraduate. Based on the Taha/Arabi21 framework, the guidelines were refined\nthrough iterative training with native Arabic-speaking educators. We highlight\nkey linguistic, pedagogical, and cognitive factors in determining readability\nand report high inter-annotator agreement: Quadratic Weighted Kappa 81.8%\n(substantial/excellent agreement) in the last annotation phase. We also\nbenchmark automatic readability models across multiple classification\ngranularities (19-, 7-, 5-, and 3-level). The corpus and guidelines are\npublicly available."}
{"id": "2410.14387", "pdf": "https://arxiv.org/pdf/2410.14387.pdf", "abs": "https://arxiv.org/abs/2410.14387", "title": "How Do Multilingual Language Models Remember Facts?", "authors": ["Constanza Fierro", "Negar Foroutan", "Desmond Elliott", "Anders Søgaard"], "categories": ["cs.CL"], "comment": "9 pages", "summary": "Large Language Models (LLMs) store and retrieve vast amounts of factual\nknowledge acquired during pre-training. Prior research has localized and\nidentified mechanisms behind knowledge recall; however, it has only focused on\nEnglish monolingual models. The question of how these mechanisms generalize to\nnon-English languages and multilingual LLMs remains unexplored. In this paper,\nwe address this gap by conducting a comprehensive analysis of three\nmultilingual LLMs. First, we show that previously identified recall mechanisms\nin English largely apply to multilingual contexts, with nuances based on\nlanguage and architecture. Next, through patching intermediate representations,\nwe localize the role of language during recall, finding that subject enrichment\nis language-independent, while object extraction is language-dependent.\nAdditionally, we discover that the last token representation acts as a Function\nVector (FV), encoding both the language of the query and the content to be\nextracted from the subject. Furthermore, in decoder-only LLMs, FVs compose\nthese two pieces of information in two separate stages. These insights reveal\nunique mechanisms in multilingual LLMs for recalling information, highlighting\nthe need for new methodologies -- such as knowledge evaluation, fact editing,\nand knowledge acquisition -- that are specifically tailored for multilingual\nLLMs."}
{"id": "2410.17131", "pdf": "https://arxiv.org/pdf/2410.17131.pdf", "abs": "https://arxiv.org/abs/2410.17131", "title": "Self-Steering Optimization: Autonomous Preference Optimization for Large Language Models", "authors": ["Hao Xiang", "Bowen Yu", "Hongyu Lin", "Keming Lu", "Yaojie Lu", "Xianpei Han", "Ben He", "Le Sun", "Jingren Zhou", "Junyang Lin"], "categories": ["cs.CL"], "comment": null, "summary": "The key to effective alignment lies in high-quality preference data. Recent\nresearch has focused on automated alignment, which involves developing\nalignment systems with minimal human intervention. However, prior research has\npredominantly focused on developing data generation methods, while insufficient\nattention has been paid to quality control mechanisms, which often produce\ninaccurate and unhelpful data, leading to unpredictable benefits during\niterative optimization. In this paper, we present Self-Steering Optimization\n($SSO$), an algorithm that autonomously generates high-quality preference data,\neliminating manual annotation requirements. $SSO$ employs a specialized\noptimization objective to build a data generator from the policy model itself,\nwhich is used to produce accurate and on-policy data. We demonstrate $SSO$'s\neffectiveness through comprehensive experiments on two series of models: Llama\n3 and Qwen 2. Our evaluation across diverse benchmarks shows that $SSO$\nconsistently outperforms baselines in human preference alignment and reward\noptimization. Further analysis validates $SSO$ as a scalable framework for\npreference optimization, benefiting the advancement in automated alignment\ntechniques."}
{"id": "2411.02460", "pdf": "https://arxiv.org/pdf/2411.02460.pdf", "abs": "https://arxiv.org/abs/2411.02460", "title": "Code-Switching Curriculum Learning for Multilingual Transfer in LLMs", "authors": ["Haneul Yoo", "Cheonbok Park", "Sangdoo Yun", "Alice Oh", "Hwaran Lee"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "To appear in Findings of ACL 2025", "summary": "Large language models (LLMs) now exhibit near human-level performance in\nvarious tasks, but their performance drops drastically after a handful of\nhigh-resource languages due to the imbalance in pre-training data. Inspired by\nthe human process of second language acquisition, particularly\ncode-switching$\\unicode{x2014}$the practice of language alternation in a\nconversation$\\unicode{x2014}$we propose code-switching curriculum learning\n(CSCL) to enhance cross-lingual transfer for LLMs. CSCL mimics the stages of\nhuman language learning by progressively training models with a curriculum\nconsisting of 1) token-level code-switching, 2) sentence-level code-switching,\nand 3) monolingual corpora. Using Qwen 2 as our underlying model, we\ndemonstrate the efficacy of the CSCL in improving language transfer to Korean,\nachieving significant performance gains compared to monolingual continual\npre-training methods. Ablation studies reveal that both token- and\nsentence-level code-switching significantly enhance cross-lingual transfer and\nthat curriculum learning amplifies these effects. We also extend our findings\ninto various languages, including Japanese (high-resource) and Indonesian\n(low-resource), and using two additional models (Gemma 2 and Phi 3.5). We\nfurther show that CSCL mitigates spurious correlations between language\nresources and safety alignment, presenting a robust, efficient framework for\nmore equitable language transfer in LLMs. We observe that CSCL is effective for\nlow-resource settings where high-quality, monolingual corpora for language\ntransfer are hardly available."}
{"id": "2411.12768", "pdf": "https://arxiv.org/pdf/2411.12768.pdf", "abs": "https://arxiv.org/abs/2411.12768", "title": "CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization", "authors": ["Nay Myat Min", "Long H. Pham", "Yige Li", "Jun Sun"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ICML 2025, 20 pages", "summary": "Large Language Models (LLMs) are vulnerable to backdoor attacks that\nmanipulate outputs via hidden triggers. Existing defense methods--designed for\nvision/text classification tasks--fail for text generation. We propose Internal\nConsistency Regularization (CROW), a defense leveraging the observation that\nbackdoored models exhibit unstable layer-wise hidden representations when\ntriggered, while clean models show smooth transitions. CROW enforces\nconsistency across layers via adversarial perturbations and regularization\nduring finetuning, neutralizing backdoors without requiring clean reference\nmodels or trigger knowledge--only a small clean dataset. Experiments across\nLlama-2 (7B, 13B), CodeLlama (7B, 13B), and Mistral-7B demonstrate CROW's\neffectiveness: it achieves significant reductions in attack success rates\nacross diverse backdoor strategies (sentiment steering, targeted refusal, code\ninjection) while preserving generative performance. CROW's\narchitecture-agnostic design enables practical deployment."}
{"id": "2411.17304", "pdf": "https://arxiv.org/pdf/2411.17304.pdf", "abs": "https://arxiv.org/abs/2411.17304", "title": "Meaningless is better: hashing bias-inducing words in LLM prompts improves performance in logical reasoning and statistical learning", "authors": ["Milena Chadimová", "Eduard Jurášek", "Tomáš Kliegr"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper introduces a novel method, referred to as \"hashing\", which\ninvolves masking potentially bias-inducing words in large language models\n(LLMs) with hash-like meaningless identifiers to reduce cognitive biases and\nreliance on external knowledge. The method was tested across three sets of\nexperiments involving a total of 490 prompts. Statistical analysis using\nchi-square tests showed significant improvements in all tested scenarios, which\ncovered LLama, ChatGPT, Copilot, Gemini and Mixtral models. In the first\nexperiment, hashing decreased the fallacy rate in a modified version of the\n\"Linda\" problem aimed at evaluating susceptibility to cognitive biases. In the\nsecond experiment, it improved LLM results on the frequent itemset extraction\ntask. In the third experiment, we found hashing is also effective when the\nLinda problem is presented in a tabular format rather than text, indicating\nthat the technique works across various input representations. Overall, the\nmethod was shown to improve bias reduction and incorporation of external\nknowledge. Despite bias reduction, hallucination rates were inconsistently\nreduced across types of LLM models. These findings suggest that masking\nbias-inducing terms can improve LLM performance, although its effectiveness is\nmodel- and task-dependent."}
{"id": "2411.18553", "pdf": "https://arxiv.org/pdf/2411.18553.pdf", "abs": "https://arxiv.org/abs/2411.18553", "title": "Retrofitting Large Language Models with Dynamic Tokenization", "authors": ["Darius Feher", "Ivan Vulić", "Benjamin Minixhofer"], "categories": ["cs.CL"], "comment": null, "summary": "Current language models (LMs) use a fixed, static subword tokenizer. This\ndefault choice typically results in degraded efficiency and language\ncapabilities, especially in languages other than English. To address this\nissue, we challenge the static design and propose retrofitting LMs with dynamic\ntokenization: a way to dynamically decide on token boundaries based on the\ninput text via a subword-merging algorithm inspired by byte-pair encoding. We\nmerge frequent subword sequences in a batch, then apply a pre-trained\nembedding-prediction hypernetwork to compute the token embeddings on-the-fly.\nFor encoder-style models (e.g., XLM-R), this on average reduces token sequence\nlengths by >20% across 14 languages while degrading performance by less than\n2%. The same method applied to pre-filling and scoring in decoder-style models\n(e.g., Mistral-7B) results in minimal performance degradation at up to 17%\nreduction in sequence length. Overall, we find that dynamic tokenization can\nmitigate the limitations of static tokenization by substantially improving\ninference speed and promoting fairness across languages, enabling more\nequitable and adaptable LMs."}
{"id": "2412.05023", "pdf": "https://arxiv.org/pdf/2412.05023.pdf", "abs": "https://arxiv.org/abs/2412.05023", "title": "Steps are all you need: Rethinking STEM Education with Prompt Engineering", "authors": ["Krishnasai Addala", "Kabir Dev Paul Baghel", "Navya Gupta", "Rishitej Reddy Vyalla", "Chhavi Kirtani", "Avinash Anand", "Rajiv Ratn Shah"], "categories": ["cs.CL"], "comment": null, "summary": "Few shot and Chain-of-Thought prompting have shown promise when applied to\nPhysics Question Answering Tasks, but are limited by the lack of mathematical\nability inherent to LLMs, and are prone to hallucination. By utilizing a\nMixture of Experts (MoE) Model, along with analogical prompting, we are able to\nshow improved model performance when compared to the baseline on standard LLMs.\nWe also survey the limits of these prompting techniques and the effects they\nhave on model performance. Additionally, we propose Analogical CoT prompting, a\nprompting technique designed to allow smaller, open source models to leverage\nAnalogical prompting, something they have struggled with, possibly due to a\nlack of specialist training data."}
{"id": "2412.05342", "pdf": "https://arxiv.org/pdf/2412.05342.pdf", "abs": "https://arxiv.org/abs/2412.05342", "title": "Multi-Party Supervised Fine-tuning of Language Models for Multi-Party Dialogue Generation", "authors": ["Xiaoyu Wang", "Ningyuan Xi", "Teng Chen", "Qingqing Gu", "Yue Zhao", "Xiaokai Chen", "Zhonglin Jiang", "Yong Chen", "Luo Ji"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by IJCNN 2025", "summary": "Large Language Models (LLM) are usually fine-tuned to participate in dyadic\nor two-party dialogues, which can not adapt well to multi-party dialogues\n(MPD), which hinders their applications in such scenarios including\nmulti-personal meetings, discussions and daily communication. Previous\nLLM-based researches mainly focus on the multi-agent framework, while their\nbase LLMs are still pairwisely fine-tuned. In this work, we design a\nmulti-party fine-tuning framework (MuPaS) for LLMs on the multi-party dialogue\ndatasets, and prove such a straightforward framework can let the LLM align with\nthe multi-party conversation style efficiently and effectively. We also design\ntwo training strategies which can convert MuPaS into the MPD simulator.\nSubstantial experiments show that MuPaS can achieve state-of-the-art\nmulti-party response, higher accuracy of the-next-speaker prediction, higher\nhuman and automatic evaluated utterance qualities, and can even generate\nreasonably with out-of-distribution scene, topic and role descriptions. The\nMuPaS framework bridges the LLM training with more complicated multi-party\napplications, such as conversation generation, virtual rehearsal or\nmeta-universe."}
{"id": "2412.05453", "pdf": "https://arxiv.org/pdf/2412.05453.pdf", "abs": "https://arxiv.org/abs/2412.05453", "title": "Knowledge Graphs are all you need: Leveraging KGs in Physics Question Answering", "authors": ["Krishnasai Addala", "Kabir Dev Paul Baghel", "Dhruv Jain", "Navya Gupta", "Rishitej Reddy Vyalla", "Chhavi Kirtani", "Avinash Anand", "Rajiv Ratn Shah"], "categories": ["cs.CL"], "comment": null, "summary": "This study explores the effectiveness of using knowledge graphs generated by\nlarge language models to decompose high school-level physics questions into\nsub-questions. We introduce a pipeline aimed at enhancing model response\nquality for Question Answering tasks. By employing LLMs to construct knowledge\ngraphs that capture the internal logic of the questions, these graphs then\nguide the generation of subquestions. We hypothesize that this method yields\nsub-questions that are more logically consistent with the original questions\ncompared to traditional decomposition techniques. Our results show that\nsub-questions derived from knowledge graphs exhibit significantly improved\nfidelity to the original question's logic. This approach not only enhances the\nlearning experience by providing clearer and more contextually appropriate\nsub-questions but also highlights the potential of LLMs to transform\neducational methodologies. The findings indicate a promising direction for\napplying AI to improve the quality and effectiveness of educational content."}
{"id": "2412.06845", "pdf": "https://arxiv.org/pdf/2412.06845.pdf", "abs": "https://arxiv.org/abs/2412.06845", "title": "7B Fully Open Source Moxin-LLM/VLM -- From Pretraining to GRPO-based Reinforcement Learning Enhancement", "authors": ["Pu Zhao", "Xuan Shen", "Zhenglun Kong", "Yixin Shen", "Sung-En Chang", "Timothy Rupprecht", "Lei Lu", "Enfu Nan", "Changdi Yang", "Yumei He", "Weiyan Shi", "Xingchen Xu", "Yu Huang", "Wei Jiang", "Wei Wang", "Yue Chen", "Yong He", "Yanzhi Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recently, Large Language Models (LLMs) have undergone a significant\ntransformation, marked by a rapid rise in both their popularity and\ncapabilities. Leading this evolution are proprietary LLMs like GPT-4 and\nGPT-o1, which have captured widespread attention in the AI community due to\ntheir remarkable performance and versatility. Simultaneously, open-source LLMs,\nsuch as LLaMA, have made great contributions to the ever-increasing popularity\nof LLMs due to the ease to customize and deploy the models across diverse\napplications. Although open-source LLMs present unprecedented opportunities for\ninnovation and research, the commercialization of LLMs has raised concerns\nabout transparency, reproducibility, and safety. Many open-source LLMs fail to\nmeet fundamental transparency requirements by withholding essential components\nlike training code and data, which may hinder further innovations on LLMs. To\nmitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed,\nadhering to principles of open science, open source, open data, and open\naccess. We release the pre-training code and configurations, training and\nfine-tuning datasets, and intermediate and final checkpoints, aiming to make\ncontinuous commitments to fully open-source LLMs. After pre-training the base\nmodel, we finetune the Moxin Base model with SOTA post-training framework and\ninstruction data to obtain Moxin Instruct model. To improve the reasoning\ncapability, we further finetune our Instruct model with chain-of-thought data\ndistilled from DeepSeek R1, and then use Group Relative Policy Optimization\n(GRPO) following DeepSeek R1 to finetune our model, leading to the Moxin\nReasoning model. Moreover, we develop our vision language model based on our\nMoxin model. Experiments show that our models achieve superior performance in\nvarious evaluations such as zero-shot evaluation, few-shot evaluation, and CoT\nevaluation."}
{"id": "2501.16884", "pdf": "https://arxiv.org/pdf/2501.16884.pdf", "abs": "https://arxiv.org/abs/2501.16884", "title": "Irony Detection, Reasoning and Understanding in Zero-shot Learning", "authors": ["Peiling Yi", "Yuhan Xia", "Yunfei Long"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The generalisation of irony detection faces significant challenges, leading\nto substantial performance deviations when detection models are applied to\ndiverse real-world scenarios. In this study, we find that irony-focused\nprompts, as generated from our IDADP framework for LLMs, can not only overcome\ndataset-specific limitations but also generate coherent, human-readable\nreasoning, transforming ironic text into its intended meaning. Based on our\nfindings and in-depth analysis, we identify several promising directions for\nfuture research aimed at enhancing LLMs' zero-shot capabilities in irony\ndetection, reasoning, and comprehension. These include advancing contextual\nawareness in irony detection, exploring hybrid symbolic-neural methods, and\nintegrating multimodal data, among others."}
{"id": "2502.05202", "pdf": "https://arxiv.org/pdf/2502.05202.pdf", "abs": "https://arxiv.org/abs/2502.05202", "title": "Accelerating LLM Inference with Lossless Speculative Decoding Algorithms for Heterogeneous Vocabularies", "authors": ["Nadav Timor", "Jonathan Mamou", "Daniel Korat", "Moshe Berchansky", "Gaurav Jain", "Oren Pereg", "Moshe Wasserblat", "David Harel"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML'25 Oral (top %1)", "summary": "Accelerating the inference of large language models (LLMs) is a critical\nchallenge in generative AI. Speculative decoding (SD) methods offer substantial\nefficiency gains by generating multiple tokens using a single target forward\npass. However, existing SD approaches require the drafter and target models to\nshare the same vocabulary, thus limiting the pool of possible drafters, often\nnecessitating the training of a drafter from scratch. We present three new SD\nmethods that remove this shared-vocabulary constraint. All three methods\npreserve the target distribution (i.e., they are lossless) and work with\noff-the-shelf models without requiring additional training or modifications.\nEmpirically, on summarization, programming, and long-context tasks, our\nalgorithms demonstrate significant speedups of up to 2.8x over standard\nautoregressive decoding. By enabling any off-the-shelf model to serve as a\ndrafter and requiring no retraining, this work substantially broadens the\napplicability of the SD framework in practice."}
{"id": "2502.16033", "pdf": "https://arxiv.org/pdf/2502.16033.pdf", "abs": "https://arxiv.org/abs/2502.16033", "title": "Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models", "authors": ["Qianqi Yan", "Yue Fan", "Hongquan Li", "Shan Jiang", "Yang Zhao", "Xinze Guan", "Ching-Chen Kuo", "Xin Eric Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing Multimodal Large Language Models (MLLMs) are predominantly trained\nand tested on consistent visual-textual inputs, leaving open the question of\nwhether they can handle inconsistencies in real-world, layout-rich content. To\nbridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR)\nbenchmark to assess MLLMs' ability to detect and reason about semantic\nmismatches in artifacts such as webpages, presentation slides, and posters.\nMMIR comprises 534 challenging samples, each containing synthetically injected\nerrors across five reasoning-heavy categories: Factual Contradiction, Identity\nMisattribution, Contextual Mismatch, Quantitative Discrepancy, and\nTemporal/Spatial Incoherence. We evaluate six state-of-the-art MLLMs, showing\nthat models with dedicated multimodal reasoning capabilities, such as o1,\nsubstantially outperform their counterparts while open-source models remain\nparticularly vulnerable to inconsistency errors. Detailed error analyses\nfurther show that models excel in detecting pairwise inconsistencies but\nstruggle with inconsistencies confined to single elements in complex layouts.\nProbing experiments reveal that single-modality prompting, including\nChain-of-Thought (CoT) and Set-of-Mark (SoM) methods, yields marginal gains,\nrevealing a key bottleneck in cross-modal reasoning. Our findings highlight the\nneed for advanced multimodal reasoning and point to future research on\nmultimodal inconsistency."}
{"id": "2502.19830", "pdf": "https://arxiv.org/pdf/2502.19830.pdf", "abs": "https://arxiv.org/abs/2502.19830", "title": "Revisiting Self-Consistency from Dynamic Distributional Alignment Perspective on Answer Aggregation", "authors": ["Yiwei Li", "Ji Zhang", "Shaoxiong Feng", "Peiwen Yuan", "Xinglin Wang", "Jiayi Shi", "Yueqi Zhang", "Chuyi Tan", "Boyuan Pan", "Yao Hu", "Kan Li"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "Self-consistency improves reasoning by aggregating diverse stochastic\nsamples, yet the dynamics behind its efficacy remain underexplored. We reframe\nself-consistency as a dynamic distributional alignment problem, revealing that\ndecoding temperature not only governs sampling randomness but also actively\nshapes the latent answer distribution. Given that high temperatures require\nprohibitively large sample sizes to stabilize, while low temperatures risk\namplifying biases, we propose a confidence-driven mechanism that dynamically\ncalibrates temperature: sharpening the sampling distribution under uncertainty\nto align with high-probability modes, and promoting exploration when confidence\nis high. Experiments on mathematical reasoning tasks show this approach\noutperforms fixed-diversity baselines under limited samples, improving both\naverage and best-case performance across varying initial temperatures without\nadditional data or modules. This establishes self-consistency as a\nsynchronization challenge between sampling dynamics and evolving answer\ndistributions."}
{"id": "2503.01940", "pdf": "https://arxiv.org/pdf/2503.01940.pdf", "abs": "https://arxiv.org/abs/2503.01940", "title": "AskToAct: Enhancing LLMs Tool Use via Self-Correcting Clarification", "authors": ["Xuan Zhang", "Yongliang Shen", "Zhe Zheng", "Linjuan Wu", "Wenqi Zhang", "Yuchen Yan", "Qiuying Peng", "Jun Wang", "Weiming Lu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ntool learning. In real-world scenarios, user queries are often ambiguous and\nincomplete, requiring effective clarification. However, existing interactive\nclarification approaches face two critical limitations: reliance on manually\nconstructed datasets, which inherently constrains training data scale and\ndiversity, and lack of error correction mechanisms during multi-turn\nclarification, leading to error accumulation that compromises both accuracy and\nefficiency. We present AskToAct, which addresses these challenges by exploiting\nthe structural mapping between queries and their tool invocation solutions. Our\nkey insight is that tool parameters naturally represent explicit user intents.\nBy systematically removing key parameters from queries while retaining them as\nground truth, we enable automated construction of high-quality training data.\nWe further enhance model robustness through error-correction pairs and\nselective masking, enabling dynamic error detection during clarification\ninteractions. Comprehensive experiments demonstrate that AskToAct significantly\noutperforms existing approaches, achieving above 57% accuracy in recovering\ncritical unspecified intents and enhancing clarification efficiency by an\naverage of 10.46% while maintaining high accuracy in tool invocation. Our\nframework exhibits robust performance across different model architectures and\nsuccessfully generalizes to entirely unseen APIs without additional training,\nachieving performance comparable to GPT-4o with substantially fewer\ncomputational resources."}
{"id": "2503.02450", "pdf": "https://arxiv.org/pdf/2503.02450.pdf", "abs": "https://arxiv.org/abs/2503.02450", "title": "Measuring What Makes You Unique: Difference-Aware User Modeling for Enhancing LLM Personalization", "authors": ["Yilun Qiu", "Xiaoyan Zhao", "Yang Zhang", "Yimeng Bai", "Wenjie Wang", "Hong Cheng", "Fuli Feng", "Tat-Seng Chua"], "categories": ["cs.CL"], "comment": "2025 ACL Findings", "summary": "Personalizing Large Language Models (LLMs) has become a critical step in\nfacilitating their widespread application to enhance individual life\nexperiences. In pursuit of personalization, distilling key preference\ninformation from an individual's historical data as instructional preference\ncontext to customize LLM generation has emerged as a promising direction.\nHowever, these methods face a fundamental limitation by overlooking the\ninter-user comparative analysis, which is essential for identifying the\ninter-user differences that truly shape preferences. To address this\nlimitation, we propose Difference-aware Personalization Learning (DPL), a novel\napproach that emphasizes extracting inter-user differences to enhance LLM\npersonalization. DPL strategically selects representative users for comparison\nand establishes a structured standard to extract meaningful, task-relevant\ndifferences for customizing LLM generation. Extensive experiments on real-world\ndatasets demonstrate that DPL significantly enhances LLM personalization. We\nrelease our code at https://github.com/SnowCharmQ/DPL."}
{"id": "2503.04793", "pdf": "https://arxiv.org/pdf/2503.04793.pdf", "abs": "https://arxiv.org/abs/2503.04793", "title": "Sentence-level Reward Model can Generalize Better for Aligning LLM from Human Preference", "authors": ["Wenjie Qiu", "Yi-Chen Li", "Xuqin Zhang", "Tianyi Zhang", "Yihang Zhang", "Zongzhang Zhang", "Yang Yu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Learning reward models from human preference datasets and subsequently\noptimizing language models via reinforcement learning has emerged as a\nfundamental paradigm for aligning LLMs with human preferences. The performance\nof the reward model plays a crucial role in the effectiveness of alignment.\nPrevious reward models operate at a coarse-grained level, requiring the\ngeneration of a complete response to obtain a reward value. The sparse reward\nmay present challenges for downstream reinforcement learning. While recent\nefforts have attempted to learn token-level reward models, the lack of explicit\nsemantic information makes it difficult to model the credit of every individual\ntoken. In this paper, we propose assigning scores to every sentence,\nintroducing an intermediate-grained reward model. By segmenting the complete\nresponse into sentences and applying differential operations to reward output\nat the start and end positions of each sentence, we can effectively model the\nrewards of sentences. Moreover, a novel attention mechanism is introduced to\naggregate the scores of all sentences into a response-level score, which allows\nit to be trained using the Bradley-Terry model. On common benchmarks, our\nmethod outperforms the response-level reward model by 2.7% on RewardBench (for\nreward modeling evaluation) and surpasses all baselines on AlpacaEval (for\nalignment evaluation)."}
{"id": "2503.11314", "pdf": "https://arxiv.org/pdf/2503.11314.pdf", "abs": "https://arxiv.org/abs/2503.11314", "title": "Unlocking General Long Chain-of-Thought Reasoning Capabilities of Large Language Models via Representation Engineering", "authors": ["Xinyu Tang", "Xiaolei Wang", "Zhihao Lv", "Yingqian Min", "Wayne Xin Zhao", "Binbin Hu", "Ziqi Liu", "Zhiqiang Zhang"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Recent advancements in long chain-of-thoughts(long CoTs) have significantly\nimproved the reasoning capabilities of large language models(LLMs). Existing\nwork finds that the capability of long CoT reasoning can be efficiently\nelicited by tuning on only a few examples and can easily transfer to other\ntasks. This motivates us to investigate whether long CoT reasoning is a general\ncapability for LLMs. In this work, we conduct an empirical analysis for this\nquestion from the perspective of representation. We find that LLMs do encode\nlong CoT reasoning as a general capability, with a clear distinction from\nvanilla CoTs. Furthermore, domain-specific representations are also required\nfor the effective transfer of long CoT reasoning. Inspired by these findings,\nwe propose GLoRE, a novel representation engineering method to unleash the\ngeneral long CoT reasoning capabilities of LLMs. Extensive experiments\ndemonstrate the effectiveness and efficiency of GLoRE in both in-domain and\ncross-domain scenarios."}
{"id": "2503.18491", "pdf": "https://arxiv.org/pdf/2503.18491.pdf", "abs": "https://arxiv.org/abs/2503.18491", "title": "MAGIC-VQA: Multimodal And Grounded Inference with Commonsense Knowledge for Visual Question Answering", "authors": ["Shuo Yang", "Siwen Luo", "Soyeon Caren Han", "Eduard Hovy"], "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "Visual Question Answering (VQA) requires reasoning across visual and textual\nmodalities, yet Large Vision-Language Models (LVLMs) often lack integrated\ncommonsense knowledge, limiting their robustness in real-world scenarios. To\naddress this, we introduce MAGIC-VQA, a novel framework that enhances VQA by\nsystematically integrating commonsense knowledge with LVLMs. MAGIC-VQA employs\na three-stage process: (1) Explicit Knowledge Integration from external\nsources, (2) By-Type Post-Processing for contextual refinement, and (3)\nImplicit Knowledge Augmentation using a Graph Neural Network (GNN) for\nstructured reasoning. While GNNs bring greater depth to structured inference,\nthey enable superior relational inference beyond LVLMs. MAGIC-VQA bridges a key\ngap by unifying commonsensse knowledge with LVLM-driven reasoning, eliminating\nthe need for extensive pre-training or complex prompt tuning. Our framework\nachieves state-of-the-art performance on benchmark datasets, significantly\nimproving commonsense reasoning in VQA."}
{"id": "2504.01738", "pdf": "https://arxiv.org/pdf/2504.01738.pdf", "abs": "https://arxiv.org/abs/2504.01738", "title": "Style over Substance: Distilled Language Models Reason Via Stylistic Replication", "authors": ["Philip Lippmann", "Jie Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Specialized reasoning language models (RLMs) have demonstrated that scaling\ntest-time computation through detailed reasoning traces significantly enhances\nperformance. Although these traces effectively facilitate knowledge\ndistillation into smaller, instruction-tuned models, the precise nature of\ntransferred reasoning remains unclear. In this study, we investigate to what\nextent distilled models internalize replicated stylistic patterns during\nreasoning. To this end, we systematically analyze reasoning traces, identifying\nstructural and lexical patterns that characterize successful reasoning. We then\nintroduce two new datasets -- a dataset of emergent reasoning traces and a\nsynthetic dataset explicitly constructed to replicate these stylistic patterns\n-- to precisely examine their influence on distilled models' reasoning\ncapabilities. We find that models trained on the synthetic traces achieve\ncomparable performance, indicating that distilled reasoning abilities rely\nsignificantly on surface-level patterns. Surprisingly, we observe an increase\nin performance even when the synthetic traces are altered to lead to the wrong\nanswer. Our findings highlight how stylistic patterns can be leveraged to\nefficiently enhance LM reasoning across diverse model families."}
{"id": "2504.02132", "pdf": "https://arxiv.org/pdf/2504.02132.pdf", "abs": "https://arxiv.org/abs/2504.02132", "title": "One Pic is All it Takes: Poisoning Visual Document Retrieval Augmented Generation with a Single Image", "authors": ["Ezzeldin Shereen", "Dan Ristea", "Shae McFadden", "Burak Hasircioglu", "Vasilios Mavroudis", "Chris Hicks"], "categories": ["cs.CL", "cs.CR", "cs.CV", "cs.IR"], "comment": "19 pages, 7 figures", "summary": "Multi-modal retrieval augmented generation (M-RAG) is instrumental for\ninhibiting hallucinations in large multi-modal models (LMMs) through the use of\na factual knowledge base (KB). However, M-RAG introduces new attack vectors for\nadversaries that aim to disrupt the system by injecting malicious entries into\nthe KB. In this paper, we present the first poisoning attack against M-RAG\ntargeting visual document retrieval applications where the KB contains images\nof document pages. We propose two attacks, each of which require injecting only\na single adversarial image into the KB. Firstly, we propose a universal attack\nthat, for any potential user query, influences the response to cause a\ndenial-of-service (DoS) in the M-RAG system. Secondly, we present a targeted\nattack against one or a group of user queries, with the goal of spreading\ntargeted misinformation. For both attacks, we use a multi-objective\ngradient-based adversarial approach to craft the injected image while\noptimizing for both retrieval and generation. We evaluate our attacks against\nseveral visual document retrieval datasets, a diverse set of state-of-the-art\nretrievers (embedding models) and generators (LMMs), demonstrating the attack\neffectiveness in both the universal and targeted settings. We additionally\npresent results including commonly used defenses, various attack\nhyper-parameter settings, ablations, and attack transferability."}
{"id": "2504.04745", "pdf": "https://arxiv.org/pdf/2504.04745.pdf", "abs": "https://arxiv.org/abs/2504.04745", "title": "Can LLMs Interpret and Leverage Structured Linguistic Representations? A Case Study with AMRs", "authors": ["Ankush Raut", "Xiaofeng Zhu", "Maria Leonor Pacheco"], "categories": ["cs.CL"], "comment": "13 pages, 23 figures. Accepted at XLLM @ ACL 2025", "summary": "This paper evaluates the ability of Large Language Models (LLMs) to leverage\ncontextual information in the form of structured linguistic representations.\nSpecifically, we examine the impact of encoding both short and long contexts\nusing Abstract Meaning Representation (AMR) structures across a diverse set of\nlanguage tasks. We perform our analysis using 8-bit quantized and\ninstruction-tuned versions of Llama 3.1 (8B), Phi-3, and Mistral 7B. Our\nresults indicate that, for tasks involving short contexts, augmenting the\nprompt with the AMR of the original language context often degrades the\nperformance of the underlying LLM. However, for tasks that involve long\ncontexts, such as dialogue summarization in the SAMSum dataset, this\nenhancement improves LLM performance, for example, by increasing the zero-shot\ncosine similarity score of Llama 3.1 from 66% to 76%. This improvement is more\nevident in the newer and larger LLMs, but does not extend to the older or\nsmaller ones. In addition, we observe that LLMs can effectively reconstruct the\noriginal text from a linearized AMR, achieving a cosine similarity of 81% in\nthe best-case scenario."}
{"id": "2504.12347", "pdf": "https://arxiv.org/pdf/2504.12347.pdf", "abs": "https://arxiv.org/abs/2504.12347", "title": "Assessment of Evolving Large Language Models in Upper Secondary Mathematics", "authors": ["Mika Setälä", "Pieta Sikström", "Ville Heilala", "Tommi Kärkkäinen"], "categories": ["cs.CL", "cs.AI", "cs.CY", "K.3; I.2"], "comment": null, "summary": "Large language models (LLMs) have shown increasing promise in educational\nsettings, yet their mathematical reasoning has been considered evolving. This\nstudy evaluates the mathematical capabilities of various LLMs using the Finnish\nmatriculation examination, a high-stakes digital test for upper secondary\neducation. Initial tests yielded moderate performance corresponding to\nmid-range grades, but later evaluations demonstrated substantial improvements\nas the language models evolved. Remarkably, some models achieved near-perfect\nor perfect scores, matching top student performance and qualifying for\nuniversity admission. Our findings highlight the rapid advances in the\nmathematical proficiency of LLMs and illustrate their potential as underlying\ntools to support learning and teaching in a variety of ways."}
{"id": "2504.12663", "pdf": "https://arxiv.org/pdf/2504.12663.pdf", "abs": "https://arxiv.org/abs/2504.12663", "title": "Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment", "authors": ["Xiaotian Zhang", "Ruizhe Chen", "Yang Feng", "Zuozhu Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL Finding", "summary": "Aligning language models with human preferences presents significant\nchallenges, particularly in achieving personalization without incurring\nexcessive computational costs. Existing methods rely on reward signals and\nadditional annotated data, limiting their scalability and adaptability to\ndiverse human values. To address these challenges, we introduce Persona-judge,\na novel discriminative paradigm that enables training-free personalized\nalignment with unseen preferences. Instead of optimizing policy parameters\nthrough external reward feedback, Persona-judge leverages the intrinsic\npreference judgment capabilities of the model. Specifically, a draft model\ngenerates candidate tokens conditioned on a given preference, while a judge\nmodel, embodying another preference, cross-validates the predicted tokens\nwhether to be accepted. Experimental results demonstrate that Persona-judge,\nusing the inherent preference evaluation mechanisms of the model, offers a\nscalable and computationally efficient solution to personalized alignment,\npaving the way for more adaptive customized alignment. Our code is available\nhere."}
{"id": "2505.01015", "pdf": "https://arxiv.org/pdf/2505.01015.pdf", "abs": "https://arxiv.org/abs/2505.01015", "title": "Value Portrait: Assessing Language Models' Values through Psychometrically and Ecologically Valid Items", "authors": ["Jongwook Han", "Dongmin Choi", "Woojung Song", "Eun-Ju Lee", "Yohan Jo"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "This paper has been accepted for publication at ACL 2025", "summary": "The importance of benchmarks for assessing the values of language models has\nbeen pronounced due to the growing need of more authentic, human-aligned\nresponses. However, existing benchmarks rely on human or machine annotations\nthat are vulnerable to value-related biases. Furthermore, the tested scenarios\noften diverge from real-world contexts in which models are commonly used to\ngenerate text and express values. To address these issues, we propose the Value\nPortrait benchmark, a reliable framework for evaluating LLMs' value\norientations with two key characteristics. First, the benchmark consists of\nitems that capture real-life user-LLM interactions, enhancing the relevance of\nassessment results to real-world LLM usage. Second, each item is rated by human\nsubjects based on its similarity to their own thoughts, and correlations\nbetween these ratings and the subjects' actual value scores are derived. This\npsychometrically validated approach ensures that items strongly correlated with\nspecific values serve as reliable items for assessing those values. Through\nevaluating 44 LLMs with our benchmark, we find that these models prioritize\nBenevolence, Security, and Self-Direction values while placing less emphasis on\nTradition, Power, and Achievement values. Also, our analysis reveals biases in\nhow LLMs perceive various demographic groups, deviating from real human data."}
{"id": "2505.06987", "pdf": "https://arxiv.org/pdf/2505.06987.pdf", "abs": "https://arxiv.org/abs/2505.06987", "title": "Convert Language Model into a Value-based Strategic Planner", "authors": ["Xiaoyu Wang", "Yue Zhao", "Qingqing Gu", "Zhonglin Jiang", "Xiaokai Chen", "Yong Chen", "Luo Ji"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 6 figures, Accepted by ACL 2025 Industry Track", "summary": "Emotional support conversation (ESC) aims to alleviate the emotional distress\nof individuals through effective conversations. Although large language models\n(LLMs) have obtained remarkable progress on ESC, most of these studies might\nnot define the diagram from the state model perspective, therefore providing a\nsuboptimal solution for long-term satisfaction. To address such an issue, we\nleverage the Q-learning on LLMs, and propose a framework called straQ*. Our\nframework allows a plug-and-play LLM to bootstrap the planning during ESC,\ndetermine the optimal strategy based on long-term returns, and finally guide\nthe LLM to response. Substantial experiments on ESC datasets suggest that\nstraQ* outperforms many baselines, including direct inference, self-refine,\nchain of thought, finetuning, and finite state machines."}
{"id": "2505.07859", "pdf": "https://arxiv.org/pdf/2505.07859.pdf", "abs": "https://arxiv.org/abs/2505.07859", "title": "Product of Experts with LLMs: Boosting Performance on ARC Is a Matter of Perspective", "authors": ["Daniel Franzen", "Jan Disselhoff", "David Hartmann"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML 2025 camera-ready; 15 pages, 6 figures, 5 tables", "summary": "The Abstraction and Reasoning Corpus (ARC-AGI) poses a significant challenge\nfor large language models (LLMs), exposing limitations in their abstract\nreasoning abilities. In this work, we leverage task-specific data augmentations\nthroughout the training, generation, and scoring phases, and employ a\ndepth-first search algorithm to generate diverse, high-probability candidate\nsolutions. Furthermore, we utilize the LLM not only as a generator but also as\na scorer, using its output probabilities to select the most promising\nsolutions. Our method achieves a score of 71.6% (286.5/400 solved tasks) on the\npublic ARC-AGI evaluation set, demonstrating state-of-the-art performance among\npublicly available approaches. While concurrent closed-source work has reported\nhigher scores, our method distinguishes itself through its transparency,\nreproducibility, and remarkably low inference cost, averaging only around 2ct\nper task on readily available hardware (we assume a price of 36ct/hour for a\nNvidia 4090 GPU)."}
{"id": "2505.13990", "pdf": "https://arxiv.org/pdf/2505.13990.pdf", "abs": "https://arxiv.org/abs/2505.13990", "title": "DecIF: Improving Instruction-Following through Meta-Decomposition", "authors": ["Tingfeng Hui", "Pengyu Zhu", "Bowen Ping", "Ling Tang", "Guanting Dong", "Yaqi Zhang", "Sen Su"], "categories": ["cs.CL"], "comment": "We release the source code and SFT data in this version", "summary": "Instruction-following has emerged as a crucial capability for large language\nmodels (LLMs). However, existing approaches often rely on pre-existing\ndocuments or external resources to synthesize instruction-following data, which\nlimits their flexibility and generalizability. In this paper, we introduce\nDecIF, a fully autonomous, meta-decomposition guided framework that generates\ndiverse and high-quality instruction-following data using only LLMs. DecIF is\ngrounded in the principle of decomposition. For instruction generation, we\nguide LLMs to iteratively produce various types of meta-information, which are\nthen combined with response constraints to form well-structured and\nsemantically rich instructions. We further utilize LLMs to detect and resolve\npotential inconsistencies within the generated instructions. Regarding response\ngeneration, we decompose each instruction into atomic-level evaluation\ncriteria, enabling rigorous validation and the elimination of inaccurate\ninstruction-response pairs. Extensive experiments across a wide range of\nscenarios and settings demonstrate DecIF's superior performance on\ninstruction-following tasks. Further analysis highlights its strong\nflexibility, scalability, and generalizability in automatically synthesizing\nhigh-quality instruction data."}
{"id": "2505.16234", "pdf": "https://arxiv.org/pdf/2505.16234.pdf", "abs": "https://arxiv.org/abs/2505.16234", "title": "LIFEBench: Evaluating Length Instruction Following in Large Language Models", "authors": ["Wei Zhang", "Zhenhong Zhou", "Kun Wang", "Junfeng Fang", "Yuanhe Zhang", "Rui Wang", "Ge Zhang", "Xavier Li", "Li Sun", "Lingjuan Lyu", "Yang Liu", "Sen Su"], "categories": ["cs.CL", "cs.AI"], "comment": "81 pages, 22 tables, 32 figures. Homepage:\n  https://ydyjya.github.io/LIFEBench/", "summary": "While large language models (LLMs) can solve PhD-level reasoning problems\nover long context inputs, they still struggle with a seemingly simpler task:\nfollowing explicit length instructions-e.g., write a 10,000-word novel.\nAdditionally, models often generate far too short outputs, terminate\nprematurely, or even refuse the request. Existing benchmarks focus primarily on\nevaluating generations quality, but often overlook whether the generations meet\nlength constraints. To this end, we introduce Length Instruction Following\nEvaluation Benchmark (LIFEBench) to comprehensively evaluate LLMs' ability to\nfollow length instructions across diverse tasks and a wide range of specified\nlengths. LIFEBench consists of 10,800 instances across 4 task categories in\nboth English and Chinese, covering length constraints ranging from 16 to 8192\nwords. We evaluate 26 widely-used LLMs and find that most models reasonably\nfollow short-length instructions but deteriorate sharply beyond a certain\nthreshold. Surprisingly, almost all models fail to reach the vendor-claimed\nmaximum output lengths in practice, as further confirmed by our evaluations\nextending up to 32K words. Even long-context LLMs, despite their extended\ninput-output windows, counterintuitively fail to improve length-instructions\nfollowing. Notably, Reasoning LLMs outperform even specialized long-text\ngeneration models, achieving state-of-the-art length following. Overall,\nLIFEBench uncovers fundamental limitations in current LLMs' length instructions\nfollowing ability, offering critical insights for future progress."}
{"id": "2505.17441", "pdf": "https://arxiv.org/pdf/2505.17441.pdf", "abs": "https://arxiv.org/abs/2505.17441", "title": "Discovering Forbidden Topics in Language Models", "authors": ["Can Rager", "Chris Wendler", "Rohit Gandikota", "David Bau"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Refusal discovery is the task of identifying the full set of topics that a\nlanguage model refuses to discuss. We introduce this new problem setting and\ndevelop a refusal discovery method, Iterated Prefill Crawler (IPC), that uses\ntoken prefilling to find forbidden topics. We benchmark IPC on Tulu-3-8B, an\nopen-source model with public safety tuning data. Our crawler manages to\nretrieve 31 out of 36 topics within a budget of 1000 prompts. Next, we scale\nthe crawler to a frontier model using the prefilling option of Claude-Haiku.\nFinally, we crawl three widely used open-weight models: Llama-3.3-70B and two\nof its variants finetuned for reasoning: DeepSeek-R1-70B and\nPerplexity-R1-1776-70B. DeepSeek-R1-70B reveals patterns consistent with\ncensorship tuning: The model exhibits \"thought suppression\" behavior that\nindicates memorization of CCP-aligned responses. Although\nPerplexity-R1-1776-70B is robust to censorship, IPC elicits CCP-aligned\nrefusals answers in the quantized model. Our findings highlight the critical\nneed for refusal discovery methods to detect biases, boundaries, and alignment\nfailures of AI systems."}
{"id": "2505.20354", "pdf": "https://arxiv.org/pdf/2505.20354.pdf", "abs": "https://arxiv.org/abs/2505.20354", "title": "Rethinking Text-based Protein Understanding: Retrieval or LLM?", "authors": ["Juntong Wu", "Zijing Liu", "He Cao", "Hao Li", "Bin Feng", "Zishan Shu", "Ke Yu", "Li Yuan", "Yu Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In recent years, protein-text models have gained significant attention for\ntheir potential in protein generation and understanding. Current approaches\nfocus on integrating protein-related knowledge into large language models\nthrough continued pretraining and multi-modal alignment, enabling simultaneous\ncomprehension of textual descriptions and protein sequences. Through a thorough\nanalysis of existing model architectures and text-based protein understanding\nbenchmarks, we identify significant data leakage issues present in current\nbenchmarks. Moreover, conventional metrics derived from natural language\nprocessing fail to accurately assess the model's performance in this domain. To\naddress these limitations, we reorganize existing datasets and introduce a\nnovel evaluation framework based on biological entities. Motivated by our\nobservation, we propose a retrieval-enhanced method, which significantly\noutperforms fine-tuned LLMs for protein-to-text generation and shows accuracy\nand efficiency in training-free scenarios. Our code and data can be seen at\nhttps://github.com/IDEA-XL/RAPM."}
{"id": "2505.24593", "pdf": "https://arxiv.org/pdf/2505.24593.pdf", "abs": "https://arxiv.org/abs/2505.24593", "title": "Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis", "authors": ["Junzhuo Li", "Bo Wang", "Xiuze Zhou", "Peijie Jiang", "Jia Liu", "Xuming Hu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025", "summary": "The interpretability of Mixture-of-Experts (MoE) models, especially those\nwith heterogeneous designs, remains underexplored. Existing attribution methods\nfor dense models fail to capture dynamic routing-expert interactions in sparse\nMoE architectures. To address this issue, we propose a cross-level attribution\nalgorithm to analyze sparse MoE architectures (Qwen 1.5-MoE, OLMoE,\nMixtral-8x7B) against dense models (Qwen 1.5-7B, Llama-7B, Mistral-7B). Results\nshow MoE models achieve 37% higher per-layer efficiency via a \"mid-activation,\nlate-amplification\" pattern: early layers screen experts, while late layers\nrefine knowledge collaboratively. Ablation studies reveal a \"basic-refinement\"\nframework--shared experts handle general tasks (entity recognition), while\nrouted experts specialize in domain-specific processing (geographic\nattributes). Semantic-driven routing is evidenced by strong correlations\nbetween attention heads and experts (r=0.68), enabling task-aware coordination.\nNotably, architectural depth dictates robustness: deep Qwen 1.5-MoE mitigates\nexpert failures (e.g., 43% MRR drop in geographic tasks when blocking top-10\nexperts) through shared expert redundancy, whereas shallow OLMoE suffers severe\ndegradation (76% drop). Task sensitivity further guides design: core-sensitive\ntasks (geography) require concentrated expertise, while distributed-tolerant\ntasks (object attributes) leverage broader participation. These insights\nadvance MoE interpretability, offering principles to balance efficiency,\nspecialization, and robustness."}
{"id": "2506.00103", "pdf": "https://arxiv.org/pdf/2506.00103.pdf", "abs": "https://arxiv.org/abs/2506.00103", "title": "Writing-Zero: Bridge the Gap Between Non-verifiable Tasks and Verifiable Rewards", "authors": ["Ruipeng Jia", "Yunyi Yang", "Yongbo Gai", "Kai Luo", "Shihao Huang", "Jianhe Lin", "Xiaoxi Jiang", "Guanjun Jiang"], "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has enabled large\nlanguage models (LLMs) to achieve remarkable breakthroughs in reasoning tasks\nwith objective ground-truth answers, such as mathematics and code generation.\nHowever, a significant gap remains for non-verifiable tasks, like creative\nwriting and open-ended dialogue, where quality assessment is inherently\nsubjective and lacks definitive references. Existing approaches for these\ndomains often rely on scalar reward models trained with human preferences,\nwhich suffer from limited generalization and are prone to reward hacking, such\nas over-explanation and length bias. In this work, we propose a unified\nRLVR-based training paradigm that bridges the gap between non-verifiable tasks\nand verifiable rewards. We introduce a writing-principle-based pairwise\nGenerative Reward Model (GenRM) and a novel Bootstrapped Relative Policy\nOptimization (BRPO) algorithm. The pairwise writing GenRM leverages\nself-principled critique to transform subjective assessments into reliable,\nverifiable rewards, while BRPO enables dynamic, reference-free pairwise\ncomparison by leveraging a bootstrapped response as temporary reference from\nwithin group rollouts during RL training. Our approach empowers LLMs to develop\nrobust writing capabilities without supervised fine-tuning, as demonstrated by\nWriting-Zero, which shows consistent improvement and strong resistance to\nreward hacking compared to scalar reward baselines. Furthermore, our method\nachieves competitive results on both in-house and open-source writing\nbenchmarks. Our findings suggest the potential to unify rule-based,\nreference-based, and reference-free reward modeling under the RLVR framework,\nthus paving the way for a comprehensive and scalable RL training paradigm\napplicable across all language tasks."}
{"id": "2506.00267", "pdf": "https://arxiv.org/pdf/2506.00267.pdf", "abs": "https://arxiv.org/abs/2506.00267", "title": "CASPER: A Large Scale Spontaneous Speech Dataset", "authors": ["Cihan Xiao", "Ruixing Liang", "Xiangyu Zhang", "Mehmet Emre Tiryaki", "Veronica Bae", "Lavanya Shankar", "Rong Yang", "Ethan Poon", "Emmanuel Dupoux", "Sanjeev Khudanpur", "Leibny Paola Garcia Perera"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "The success of large language models has driven interest in developing\nsimilar speech processing capabilities. However, a key challenge is the\nscarcity of high-quality spontaneous speech data, as most existing datasets\ncontain scripted dialogues. To address this, we present a novel pipeline for\neliciting and recording natural dialogues and release our dataset with 100+\nhours of spontaneous speech. Our approach fosters fluid, natural conversations\nwhile encouraging a diverse range of topics and interactive exchanges. Unlike\ntraditional methods, it facilitates genuine interactions, providing a\nreproducible framework for future data collection. This paper introduces our\ndataset and methodology, laying the groundwork for addressing the shortage of\nspontaneous speech data. We plan to expand this dataset in future stages,\noffering a growing resource for the research community."}
{"id": "2506.00628", "pdf": "https://arxiv.org/pdf/2506.00628.pdf", "abs": "https://arxiv.org/abs/2506.00628", "title": "LID Models are Actually Accent Classifiers: Implications and Solutions for LID on Accented Speech", "authors": ["Niyati Bafna", "Matthew Wiesner"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Prior research indicates that LID model performance significantly declines on\naccented speech; however, the specific causes, extent, and characterization of\nthese errors remain under-explored. (i) We identify a common failure mode on\naccented speech whereby LID systems often misclassify L2 accented speech as the\nspeaker's native language or a related language. (ii) We present evidence\nsuggesting that state-of-the-art models are invariant to permutations of short\nspans of speech, implying they classify on the basis of short phonotactic\nfeatures indicative of accent rather than language. Our analysis reveals a\nsimple method to enhance model robustness to accents through input chunking.\n(iii) We present an approach that integrates sequence-level information into\nour model without relying on monolingual ASR systems; this reduces\naccent-language confusion and significantly enhances performance on accented\nspeech while maintaining comparable results on standard LID."}
{"id": "2506.00975", "pdf": "https://arxiv.org/pdf/2506.00975.pdf", "abs": "https://arxiv.org/abs/2506.00975", "title": "NTPP: Generative Speech Language Modeling for Dual-Channel Spoken Dialogue via Next-Token-Pair Prediction", "authors": ["Qichao Wang", "Ziqiao Meng", "Wenqian Cui", "Yifei Zhang", "Pengcheng Wu", "Bingzhe Wu", "Irwin King", "Liang Chen", "Peilin Zhao"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted by ICML 2025", "summary": "Inspired by the impressive capabilities of GPT-4o, there is growing interest\nin enabling speech language models (SLMs) to engage in natural, fluid spoken\ninteractions with humans. Recent advancements have led to the development of\nseveral SLMs that demonstrate promising results in this area. However, current\napproaches have yet to fully exploit dual-channel speech data, which inherently\ncaptures the structure and dynamics of human conversation. In this work, we\nsystematically explore the use of dual-channel speech data in the context of\nmodern large language models, and introduce a novel generative modeling\nparadigm, Next-Token-Pair Prediction (NTPP), to enable speaker-independent\ndual-channel spoken dialogue learning using decoder-only architectures for the\nfirst time. We evaluate our approach on standard benchmarks, and empirical\nresults show that our proposed method, NTPP, significantly improves the\nconversational abilities of SLMs in terms of turn-taking prediction, response\ncoherence, and naturalness. Moreover, compared to existing methods, NTPP\nachieves substantially lower inference latency, highlighting its practical\nefficiency for real-time applications."}
{"id": "2506.01687", "pdf": "https://arxiv.org/pdf/2506.01687.pdf", "abs": "https://arxiv.org/abs/2506.01687", "title": "StochasTok: Improving Fine-Grained Subword Understanding in LLMs", "authors": ["Anya Sims", "Thom Foster", "Klara Kaleb", "Tuan-Duy H. Nguyen", "Joseph Lee", "Jakob N. Foerster", "Yee Whye Teh", "Cong Lu"], "categories": ["cs.CL"], "comment": null, "summary": "Subword-level understanding is integral to numerous tasks, including\nunderstanding multi-digit numbers, spelling mistakes, abbreviations, rhyming,\nand wordplay. Despite this, current large language models (LLMs) still often\nstruggle with seemingly simple subword-level tasks like How many 'r's in\n'strawberry'?. A key factor behind these failures is tokenization which\nobscures the fine-grained structure of words. Current alternatives, such as\ncharacter-level and dropout tokenization methods, significantly increase\ncomputational costs and provide inconsistent improvements. In this paper we\nrevisit tokenization and introduce StochasTok, a simple, efficient stochastic\ntokenization scheme that randomly splits tokens during training, allowing LLMs\nto 'see' their internal structure. Our experiments show that pretraining with\nStochasTok substantially improves LLMs' downstream performance across multiple\nsubword-level language games, including character counting, substring\nidentification, and math tasks. Furthermore, StochasTok's simplicity allows\nseamless integration at any stage of the training pipeline; and we demonstrate\nthat post-training with StochasTok can instill improved subword understanding\ninto existing pretrained models, thus avoiding costly pretraining from scratch.\nThese dramatic improvements achieved with a minimal change suggest StochasTok\nholds exciting potential when applied to larger, more capable models. Code\nopen-sourced at: https://github.com/anyasims/stochastok."}
{"id": "2506.02404", "pdf": "https://arxiv.org/pdf/2506.02404.pdf", "abs": "https://arxiv.org/abs/2506.02404", "title": "GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation", "authors": ["Yilin Xiao", "Junnan Dong", "Chuang Zhou", "Su Dong", "Qian-wen Zhang", "Di Yin", "Xing Sun", "Xiao Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing\nrecognition for its potential to enhance large language models (LLMs) by\nstructurally organizing domain-specific corpora and facilitating complex\nreasoning. However, current evaluations of GraphRAG models predominantly rely\non traditional question-answering datasets. Their limited scope in questions\nand evaluation metrics fails to comprehensively assess the reasoning capacity\nimprovements enabled by GraphRAG models. To address this gap, we introduce\nGraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously\nevaluate GraphRAG models. Our benchmark offers three key superiorities: \\((i)\\)\nChallenging question design. Featuring college-level, domain-specific questions\nthat demand multi-hop reasoning, the benchmark ensures that simple content\nretrieval is insufficient for problem-solving. For example, some questions\nrequire mathematical reasoning or programming. \\((ii)\\) Diverse task coverage.\nThe dataset includes a broad spectrum of reasoning tasks, multiple-choice,\ntrue/false, multi-select, open-ended, and fill-in-the-blank. It spans 16\ndisciplines in twenty core textbooks. \\((iii)\\) Holistic evaluation framework.\nGraphRAG-Bench provides comprehensive assessment across the entire GraphRAG\npipeline, including graph construction, knowledge retrieval, and answer\ngeneration. Beyond final-answer correctness, it evaluates the logical coherence\nof the reasoning process. By applying nine contemporary GraphRAG methods to\nGraphRAG-Bench, we demonstrate its utility in quantifying how graph-based\nstructuring improves model reasoning capabilities. Our analysis reveals\ncritical insights about graph architectures, retrieval efficacy, and reasoning\ncapabilities, offering actionable guidance for the research community."}
{"id": "2506.03949", "pdf": "https://arxiv.org/pdf/2506.03949.pdf", "abs": "https://arxiv.org/abs/2506.03949", "title": "TableEval: A Real-World Benchmark for Complex, Multilingual, and Multi-Structured Table Question Answering", "authors": ["Junnan Zhu", "Jingyi Wang", "Bohan Yu", "Xiaoyu Wu", "Junbo Li", "Lei Wang", "Nan Xu"], "categories": ["cs.CL"], "comment": null, "summary": "LLMs have shown impressive progress in natural language processing. However,\nthey still face significant challenges in TableQA, where real-world\ncomplexities such as diverse table structures, multilingual data, and\ndomain-specific reasoning are crucial. Existing TableQA benchmarks are often\nlimited by their focus on simple flat tables and suffer from data leakage.\nFurthermore, most benchmarks are monolingual and fail to capture the\ncross-lingual and cross-domain variability in practical applications. To\naddress these limitations, we introduce TableEval, a new benchmark designed to\nevaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes\ntables with various structures (such as concise, hierarchical, and nested\ntables) collected from four domains (including government, finance, academia,\nand industry reports). Besides, TableEval features cross-lingual scenarios with\ntables in Simplified Chinese, Traditional Chinese, and English. To minimize the\nrisk of data leakage, we collect all data from recent real-world documents.\nConsidering that existing TableQA metrics fail to capture semantic accuracy, we\nfurther propose SEAT, a new evaluation framework that assesses the alignment\nbetween model responses and reference answers at the sub-question level.\nExperimental results have shown that SEAT achieves high agreement with human\njudgment. Extensive experiments on TableEval reveal critical gaps in the\nability of state-of-the-art LLMs to handle these complex, real-world TableQA\ntasks, offering insights for future improvements. We make our dataset available\nhere: https://github.com/wenge-research/TableEval."}
{"id": "2506.04907", "pdf": "https://arxiv.org/pdf/2506.04907.pdf", "abs": "https://arxiv.org/abs/2506.04907", "title": "Context Is Not Comprehension: Unmasking LLM reasoning blind spots with VLO", "authors": ["Alex Pan", "Mary-Anne Williams"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "24 pages, 2 figures, 4 tables; to appear in AAAI 2026", "summary": "The dominant evaluation of Large Language Models has centered on their\nability to surface explicit facts from increasingly vast contexts. While\ntoday's best models demonstrate near-perfect recall on these tasks, this\napparent success is overly simplistic and non-representative of the complexity\nof human reasoning which is often highly nested. We introduce Verbose ListOps\n(VLO), a novel benchmark designed to isolate this failure. VLO programmatically\nweaves deterministic, nested computations into coherent stories, forcing models\nto track and update internal state rather than simply locate explicit values.\nOur experiments show that leading LLMs, capable of solving the raw ListOps\nequations with near-perfect accuracy, collapse in performance on VLO at just\n10k tokens. The extensibility of VLO's generation framework to any verifiable\nreasoning pattern will be a critical tool, enabling model developers to move\nbeyond context windows and robustly test new reasoning architectures; a\nnecessary step to automating the world's knowledge work."}
{"id": "2506.05176", "pdf": "https://arxiv.org/pdf/2506.05176.pdf", "abs": "https://arxiv.org/abs/2506.05176", "title": "Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models", "authors": ["Yanzhao Zhang", "Mingxin Li", "Dingkun Long", "Xin Zhang", "Huan Lin", "Baosong Yang", "Pengjun Xie", "An Yang", "Dayiheng Liu", "Junyang Lin", "Fei Huang", "Jingren Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "In this work, we introduce the Qwen3 Embedding series, a significant\nadvancement over its predecessor, the GTE-Qwen series, in text embedding and\nreranking capabilities, built upon the Qwen3 foundation models. Leveraging the\nQwen3 LLMs' robust capabilities in multilingual text understanding and\ngeneration, our innovative multi-stage training pipeline combines large-scale\nunsupervised pre-training with supervised fine-tuning on high-quality datasets.\nEffective model merging strategies further ensure the robustness and\nadaptability of the Qwen3 Embedding series. During the training process, the\nQwen3 LLMs serve not only as backbone models but also play a crucial role in\nsynthesizing high-quality, rich, and diverse training data across multiple\ndomains and languages, thus enhancing the training pipeline. The Qwen3\nEmbedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both\nembedding and reranking tasks, addressing diverse deployment scenarios where\nusers can optimize for either efficiency or effectiveness. Empirical\nevaluations demonstrate that the Qwen3 Embedding series achieves\nstate-of-the-art results across diverse benchmarks. Notably, it excels on the\nmultilingual evaluation benchmark MTEB for text embedding, as well as in\nvarious retrieval tasks, including code retrieval, cross-lingual retrieval and\nmultilingual retrieval. To facilitate reproducibility and promote\ncommunity-driven research and development, the Qwen3 Embedding models are\npublicly available under the Apache 2.0 license."}
{"id": "2506.05387", "pdf": "https://arxiv.org/pdf/2506.05387.pdf", "abs": "https://arxiv.org/abs/2506.05387", "title": "Advancing Decoding Strategies: Enhancements in Locally Typical Sampling for LLMs", "authors": ["Jaydip Sen", "Saptarshi Sengupta", "Subhasis Dasgupta"], "categories": ["cs.CL", "cs.AI"], "comment": "This is the accepted but pre-reviewed version of the chapter that has\n  been accepted for publication in the Springer volume 'Decision-Making in\n  Computational Intelligence-Based Systems,' edited by Witold Pedrycz, Gilberto\n  Rivera, Rose Ma Rodriguez, and Salvador Ibarra Martinez. The chapter is 39\n  pages long, and it contains 2 figures and 6 tables. This is NOT the final\n  camera-ready version", "summary": "This chapter explores advancements in decoding strategies for large language\nmodels (LLMs), focusing on enhancing the Locally Typical Sampling (LTS)\nalgorithm. Traditional decoding methods, such as top-k and nucleus sampling,\noften struggle to balance fluency, diversity, and coherence in text generation.\nTo address these challenges, Adaptive Semantic-Aware Typicality Sampling (ASTS)\nis proposed as an improved version of LTS, incorporating dynamic entropy\nthresholding, multi-objective scoring, and reward-penalty adjustments. ASTS\nensures contextually coherent and diverse text generation while maintaining\ncomputational efficiency. Its performance is evaluated across multiple\nbenchmarks, including story generation and abstractive summarization, using\nmetrics such as perplexity, MAUVE, and diversity scores. Experimental results\ndemonstrate that ASTS outperforms existing sampling techniques by reducing\nrepetition, enhancing semantic alignment, and improving fluency."}
{"id": "2506.06395", "pdf": "https://arxiv.org/pdf/2506.06395.pdf", "abs": "https://arxiv.org/abs/2506.06395", "title": "Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models", "authors": ["Pengyi Li", "Matvey Skripkin", "Alexander Zubrey", "Andrey Kuznetsov", "Ivan Oseledets"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) excel at reasoning, yet post-training remains\ncritical for aligning their behavior with task goals. Existing reinforcement\nlearning (RL) methods often depend on costly human annotations or external\nreward models. We propose Reinforcement Learning via Self-Confidence (RLSC),\nwhich uses the model's own confidence as reward signals-eliminating the need\nfor labels, preference models, or reward engineering. Applied to\nQwen2.5-Math-7B with only 16 samples per question and 10 or 20 training steps,\nRLSC improves accuracy by +13.4% on AIME2024, +21.2% on MATH500, +21.7% on\nMinerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23. RLSC provides a\nsimple, scalable post-training method for inference models, requiring only a\nsmall number of samples and unlabelled supervision."}
{"id": "2506.06821", "pdf": "https://arxiv.org/pdf/2506.06821.pdf", "abs": "https://arxiv.org/abs/2506.06821", "title": "Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems", "authors": ["Yuhan Cao", "Zian Chen", "Kun Quan", "Ziliang Zhang", "Yu Wang", "Xiaoning Dong", "Yeqi Feng", "Guanzhong He", "Jingcheng Huang", "Jianhao Li", "Yixuan Tan", "Jiafu Tang", "Yilin Tang", "Junlei Wu", "Qianyu Xiao", "Can Zheng", "Shouchen Zhou", "Yuxiang Zhu", "Yiming Huang", "Tian Xie", "Tianxing He"], "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": "37 pages, 22 figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode generation, capable of tackling complex tasks during inference. However,\nthe extent to which LLMs can be utilized for code checking or debugging through\ntest case generation remains largely unexplored. We investigate this problem\nfrom the perspective of competition-level programming (CP) programs and propose\nTCGBench, a Benchmark for (LLM generation of) Test Case Generators. This\nbenchmark comprises two tasks, aimed at studying the capabilities of LLMs in\n(1) generating valid test case generators for a given CP problem, and further\n(2) generating targeted test case generators that expose bugs in human-written\ncode. Experimental results indicate that while state-of-the-art LLMs can\ngenerate valid test case generators in most cases, most LLMs struggle to\ngenerate targeted test cases that reveal flaws in human code effectively.\nEspecially, even advanced reasoning models (e.g., o3-mini) fall significantly\nshort of human performance in the task of generating targeted generators.\nFurthermore, we construct a high-quality, manually curated dataset of\ninstructions for generating targeted generators. Analysis demonstrates that the\nperformance of LLMs can be enhanced with the aid of this dataset, by both\nprompting and fine-tuning."}
{"id": "2506.07044", "pdf": "https://arxiv.org/pdf/2506.07044.pdf", "abs": "https://arxiv.org/abs/2506.07044", "title": "Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning", "authors": ["LASA Team", "Weiwen Xu", "Hou Pong Chan", "Long Li", "Mahani Aljunied", "Ruifeng Yuan", "Jianyu Wang", "Chenghao Xiao", "Guizhen Chen", "Chaoqun Liu", "Zhaodonghui Li", "Yu Sun", "Junao Shen", "Chaojun Wang", "Jie Tan", "Deli Zhao", "Tingyang Xu", "Hao Zhang", "Yu Rong"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Technical Report, 53 pages, 25 tables, and 16 figures", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities in understanding common visual elements, largely due to their\nlarge-scale datasets and advanced training strategies. However, their\neffectiveness in medical applications remains limited due to the inherent\ndiscrepancies between data and tasks in medical scenarios and those in the\ngeneral domain. Concretely, existing medical MLLMs face the following critical\nlimitations: (1) limited coverage of medical knowledge beyond imaging, (2)\nheightened susceptibility to hallucinations due to suboptimal data curation\nprocesses, (3) lack of reasoning capabilities tailored for complex medical\nscenarios. To address these challenges, we first propose a comprehensive data\ncuration procedure that (1) efficiently acquires rich medical knowledge data\nnot only from medical imaging but also from extensive medical texts and\ngeneral-domain data; and (2) synthesizes accurate medical captions, visual\nquestion answering (VQA), and reasoning samples. As a result, we build a\nmultimodal dataset enriched with extensive medical knowledge. Building on the\ncurated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu\nundergoes multi-stage training to embed medical expertise and enhance its\ntask-solving capabilities progressively. Besides, we preliminarily explore the\npotential of applying reinforcement learning with verifiable rewards paradigm\nto enhance Lingshu's medical reasoning ability. Additionally, we develop\nMedEvalKit, a unified evaluation framework that consolidates leading multimodal\nand textual medical benchmarks for standardized, fair, and efficient model\nassessment. We evaluate the performance of Lingshu on three fundamental medical\ntasks, multimodal QA, text-based QA, and medical report generation. The results\nshow that Lingshu consistently outperforms the existing open-source multimodal\nmodels on most tasks ..."}
{"id": "2506.07664", "pdf": "https://arxiv.org/pdf/2506.07664.pdf", "abs": "https://arxiv.org/abs/2506.07664", "title": "Synthesis by Design: Controlled Data Generation via Structural Guidance", "authors": ["Lei Xu", "Sirui Chen", "Yuxuan Huang", "Chaochao Lu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Mathematical reasoning remains challenging for LLMs due to complex logic and\nthe need for precise computation. Existing methods enhance LLM reasoning by\nsynthesizing datasets through problem rephrasing, but face issues with\ngeneration quality and problem complexity. To address this, we propose to\nextract structural information with generated problem-solving code from\nmathematical reasoning and guide data generation with structured solutions.\nApplied to MATH and GSM8K, our approach produces 39K problems with labeled\nintermediate steps and a 6.1K-problem benchmark of higher difficulty. Results\non our benchmark show that model performance declines as reasoning length\nincreases. Additionally, we conducted fine-tuning experiments using the\nproposed training data on a range of LLMs, and the results validate the\neffectiveness of our dataset. We hope the proposed method and dataset will\ncontribute to future research in enhancing LLM reasoning capabilities. Our code\nand data are available at https://github.com/OpenCausaLab/StructuralGeneration."}
{"id": "2506.07751", "pdf": "https://arxiv.org/pdf/2506.07751.pdf", "abs": "https://arxiv.org/abs/2506.07751", "title": "AbstRaL: Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking", "authors": ["Silin Gao", "Antoine Bosselut", "Samy Bengio", "Emmanuel Abbe"], "categories": ["cs.CL", "cs.AI", "cs.SC"], "comment": "Under review", "summary": "Recent studies have shown that large language models (LLMs), especially\nsmaller ones, often lack robustness in their reasoning. I.e., they tend to\nexperience performance drops when faced with distribution shifts, such as\nchanges to numerical or nominal variables, or insertions of distracting\nclauses. A possible strategy to address this involves generating synthetic data\nto further \"instantiate\" reasoning problems on potential variations. In\ncontrast, our approach focuses on \"abstracting\" reasoning problems. This not\nonly helps counteract distribution shifts but also facilitates the connection\nto symbolic tools for deriving solutions. We find that this abstraction process\nis better acquired through reinforcement learning (RL) than just supervised\nfine-tuning, which often fails to produce faithful abstractions. Our method,\nAbstRaL -- which promotes abstract reasoning in LLMs using RL on granular\nabstraction data -- significantly mitigates performance degradation on recent\nGSM perturbation benchmarks."}
{"id": "2506.08174", "pdf": "https://arxiv.org/pdf/2506.08174.pdf", "abs": "https://arxiv.org/abs/2506.08174", "title": "LLM-BT-Terms: Back-Translation as a Framework for Terminology Standardization and Dynamic Semantic Embedding", "authors": ["Li Weigang", "Pedro Carvalho Brom"], "categories": ["cs.CL"], "comment": "23 pages", "summary": "The rapid expansion of English technical terminology presents a significant\nchallenge to traditional expert-based standardization, particularly in rapidly\ndeveloping areas such as artificial intelligence and quantum computing. Manual\napproaches face difficulties in maintaining consistent multilingual\nterminology. To address this, we introduce LLM-BT, a back-translation framework\npowered by large language models (LLMs) designed to automate terminology\nverification and standardization through cross-lingual semantic alignment. Our\nkey contributions include: (1) term-level consistency validation: by performing\nEnglish -> intermediate language -> English back-translation, LLM-BT achieves\nhigh term consistency across different models (such as GPT-4, DeepSeek, and\nGrok). Case studies demonstrate over 90 percent of terms are preserved either\nexactly or semantically; (2) multi-path verification workflow: we develop a\nnovel pipeline described as Retrieve -> Generate -> Verify -> Optimize, which\nsupports both serial paths (e.g., English -> Simplified Chinese -> Traditional\nChinese -> English) and parallel paths (e.g., English -> Chinese / Portuguese\n-> English). BLEU scores and term-level accuracy indicate strong cross-lingual\nrobustness, with BLEU scores exceeding 0.45 and Portuguese term accuracy\nreaching 100 percent; (3) back-translation as semantic embedding: we\nreinterpret back-translation as a form of dynamic semantic embedding that\nuncovers latent trajectories of meaning. In contrast to static embeddings,\nLLM-BT offers transparent, path-based embeddings shaped by the evolution of the\nmodels. This reframing positions back-translation as an active mechanism for\nmultilingual terminology standardization, fostering collaboration between\nmachines and humans - machines preserve semantic integrity, while humans\nprovide cultural interpretation."}
{"id": "2506.08184", "pdf": "https://arxiv.org/pdf/2506.08184.pdf", "abs": "https://arxiv.org/abs/2506.08184", "title": "Unable to Forget: Proactive lnterference Reveals Working Memory Limits in LLMs Beyond Context Length", "authors": ["Chupei Wang", "Jiaqiu Vince Sun"], "categories": ["cs.CL", "cs.AI", "q-bio.NC"], "comment": null, "summary": "Information retrieval in Large Language Models (LLMs) is increasingly\nrecognized as intertwined with generation capabilities rather than mere lookup.\nWhile longer contexts are often assumed to improve retrieval, the effects of\nintra-context interference remain understudied. To address this, we adapt the\nproactive interference (PI) paradigm from cognitive science, where earlier\ninformation disrupts recall of newer updates. In humans, susceptibility to such\ninterference is inversely linked to working memory capacity. We introduce\nPI-LLM, an evaluation that sequentially streams semantically related key-value\nupdates and queries only the final values. Although these final values are\nclearly positioned just before the query, LLM retrieval accuracy declines\nlog-linearly toward zero as interference accumulates; errors arise from\nretrieving previously overwritten values. Attempts to mitigate interference via\nprompt engineering (e.g., instructing models to ignore earlier input) yield\nlimited success. These findings reveal a fundamental constraint on LLMs'\nability to disentangle interference and flexibly manipulate information,\nsuggesting a working memory bottleneck beyond mere context access. This calls\nfor approaches that strengthen models' ability to suppress irrelevant content\nduring retrieval."}
{"id": "2506.08364", "pdf": "https://arxiv.org/pdf/2506.08364.pdf", "abs": "https://arxiv.org/abs/2506.08364", "title": "CC-RAG: Structured Multi-Hop Reasoning via Theme-Based Causal Graphs", "authors": ["Jash Rajesh Parekh", "Pengcheng Jiang", "Jiawei Han"], "categories": ["cs.CL"], "comment": null, "summary": "Understanding cause and effect relationships remains a formidable challenge\nfor Large Language Models (LLMs), particularly in specialized domains where\nreasoning requires more than surface-level correlations. Retrieval-Augmented\nGeneration (RAG) improves factual accuracy, but standard RAG pipelines treat\nevidence as flat context, lacking the structure required to model true causal\ndependencies. We introduce Causal-Chain RAG (CC-RAG), a novel approach that\nintegrates zero-shot triple extraction and theme-aware graph chaining into the\nRAG pipeline, enabling structured multi-hop inference. Given a domain specific\ncorpus, CC-RAG constructs a Directed Acyclic Graph (DAG) of <cause, relation,\neffect> triples and uses forward/backward chaining to guide structured answer\ngeneration. Experiments on two real-world domains: Bitcoin price fluctuations\nand Gaucher disease, show that CC-RAG outperforms standard RAG and zero-shot\nLLMs in chain similarity, information density, and lexical diversity. Both\nLLM-as-a-Judge and human evaluations consistently favor CC-RAG. Our results\ndemonstrate that explicitly modeling causal structure enables LLMs to generate\nmore accurate and interpretable responses, especially in specialized domains\nwhere flat retrieval fails."}
{"id": "2506.08371", "pdf": "https://arxiv.org/pdf/2506.08371.pdf", "abs": "https://arxiv.org/abs/2506.08371", "title": "Mitigating Posterior Salience Attenuation in Long-Context LLMs with Positional Contrastive Decoding", "authors": ["Zikai Xiao", "Ziyang Wang", "Wen Ma", "Yan Zhang", "Wei Shen", "Yan Wang", "Luqi Gong", "Zuozhu Liu"], "categories": ["cs.CL"], "comment": null, "summary": "While Large Language Models (LLMs) support long contexts, they struggle with\nperformance degradation within the context window. Current solutions incur\nprohibitive training costs, leaving statistical behaviors and cost-effective\napproaches underexplored. From the decoding perspective, we identify the\nPosterior Salience Attenuation (PSA) phenomenon, where the salience ratio\ncorrelates with long-text performance degradation. Notably, despite the\nattenuation, gold tokens still occupy high-ranking positions in the decoding\nspace. Motivated by it, we propose the training-free Positional Contrastive\nDecoding (PCD) that contrasts the logits derived from long-aware attention with\nthose from designed local-aware attention, enabling the model to focus on the\ngains introduced by large-scale short-to-long training. Through the analysis of\nlong-term decay simulation, we demonstrate that PCD effectively alleviates\nattention score degradation. Experimental results show that PCD achieves\nstate-of-the-art performance on long-context benchmarks."}
{"id": "2506.08403", "pdf": "https://arxiv.org/pdf/2506.08403.pdf", "abs": "https://arxiv.org/abs/2506.08403", "title": "TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration", "authors": ["Weiya Li", "Junjie Chen", "Bei Li", "Boyang Liu", "Zichen Wen", "Nuanqiao Shan", "Xiaoqian Liu", "Anping Liu", "Huajie Liu", "Hu Song", "Linfeng Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 4 figures, Under review. Code:\n  https://github.com/weiyali126/TACTIC", "summary": "Machine translation has long been a central task in natural language\nprocessing. With the rapid advancement of large language models (LLMs), there\nhas been remarkable progress in translation quality. However, fully realizing\nthe translation potential of LLMs remains an open challenge. Recent studies\nhave explored multi-agent systems to decompose complex translation tasks into\ncollaborative subtasks, showing initial promise in enhancing translation\nquality through agent cooperation and specialization. Nevertheless, existing\nmulti-agent translation frameworks largely neglect foundational insights from\ncognitive translation studies. These insights emphasize how human translators\nemploy different cognitive strategies, such as balancing literal and free\ntranslation, refining expressions based on context, and iteratively evaluating\noutputs. To address this limitation, we propose a cognitively informed\nmulti-agent framework called TACTIC, which stands for T ranslation A gents with\nCognitive- T heoretic Interactive Collaboration. The framework comprises six\nfunctionally distinct agents that mirror key cognitive processes observed in\nhuman translation behavior. These include agents for drafting, refinement,\nevaluation, scoring, context reasoning, and external knowledge gathering. By\nsimulating an interactive and theory-grounded translation workflow, TACTIC\neffectively leverages the full capacity of LLMs for high-quality translation.\nExperimental results on diverse language pairs from the FLORES-200 and WMT24\nbenchmarks show that our method consistently achieves state-of-the-art\nperformance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by\nan average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it\nfurther improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at\nhttps://github.com/weiyali126/TACTIC."}
{"id": "2506.08433", "pdf": "https://arxiv.org/pdf/2506.08433.pdf", "abs": "https://arxiv.org/abs/2506.08433", "title": "Low-resource domain adaptation while minimizing energy and hardware resource consumption", "authors": ["Hernán Maina", "Nicolás Wolovick", "Luciana Benotti"], "categories": ["cs.CL", "cs.DC", "cs.LG"], "comment": "A shorter version of this work was accepted as a two-page abstract\n  for presentation at the Widening Natural Language Processing (WiNLP) 2023\n  Workshop. That version was not publicly released, and this is the first\n  public version of the work", "summary": "Training Large Language Models (LLMs) is costly in terms of energy, hardware,\nand annotated data, often resulting in a positionality rooted in predominant\ncultures and values (Santy et al., 2023). Domain adaptation has emerged as a\npromising strategy to better align models with diverse cultural and value\ncontexts (Hershcovich et al., 2022), but its computational cost remains a\nsignificant barrier, particularly for research groups lacking access to\nlarge-scale infrastructure. In this paper, we evaluate how the use of different\nnumerical precision formats and data parallelization strategies impacts both\ntraining speed (as a proxy to energy and hardware consumption) and model\naccuracy, with the goal of facilitating domain adaptation in low-resource\nenvironments. Our findings are relevant to any setting where energy efficiency,\naccessibility, or limited hardware availability are key concerns."}
{"id": "2506.08700", "pdf": "https://arxiv.org/pdf/2506.08700.pdf", "abs": "https://arxiv.org/abs/2506.08700", "title": "ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts", "authors": ["Ruiran Su", "Jiasheng Si", "Zhijiang Guo", "Janet B. Pierrehumbert"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Scientific fact-checking has mostly focused on text and tables, overlooking\nscientific charts, which are key for presenting quantitative evidence and\nstatistical reasoning. We introduce ClimateViz, the first large-scale benchmark\nfor scientific fact-checking using expert-curated scientific charts. ClimateViz\ncontains 49,862 claims linked to 2,896 visualizations, each labeled as support,\nrefute, or not enough information. To improve interpretability, each example\nincludes structured knowledge graph explanations covering trends, comparisons,\nand causal relations. We evaluate state-of-the-art multimodal language models,\nincluding both proprietary and open-source systems, in zero-shot and few-shot\nsettings. Results show that current models struggle with chart-based reasoning:\neven the best systems, such as Gemini 2.5 and InternVL 2.5, reach only 76.2 to\n77.8 percent accuracy in label-only settings, far below human performance (89.3\nand 92.7 percent). Explanation-augmented outputs improve performance in some\nmodels. We released our dataset and code alongside the paper."}
{"id": "2506.08738", "pdf": "https://arxiv.org/pdf/2506.08738.pdf", "abs": "https://arxiv.org/abs/2506.08738", "title": "Societal AI Research Has Become Less Interdisciplinary", "authors": ["Dror Kris Markus", "Fabrizio Gilardi", "Daria Stetsenko"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "As artificial intelligence (AI) systems become deeply embedded in everyday\nlife, calls to align AI development with ethical and societal values have\nintensified. Interdisciplinary collaboration is often championed as a key\npathway for fostering such engagement. Yet it remains unclear whether\ninterdisciplinary research teams are actually leading this shift in practice.\nThis study analyzes over 100,000 AI-related papers published on ArXiv between\n2014 and 2024 to examine how ethical values and societal concerns are\nintegrated into technical AI research. We develop a classifier to identify\nsocietal content and measure the extent to which research papers express these\nconsiderations. We find a striking shift: while interdisciplinary teams remain\nmore likely to produce societally-oriented research, computer science-only\nteams now account for a growing share of the field's overall societal output.\nThese teams are increasingly integrating societal concerns into their papers\nand tackling a wide range of domains - from fairness and safety to healthcare\nand misinformation. These findings challenge common assumptions about the\ndrivers of societal AI and raise important questions. First, what are the\nimplications for emerging understandings of AI safety and governance if most\nsocietally-oriented research is being undertaken by exclusively technical\nteams? Second, for scholars in the social sciences and humanities: in a\ntechnical field increasingly responsive to societal demands, what distinctive\nperspectives can we still offer to help shape the future of AI?"}
{"id": "2506.08768", "pdf": "https://arxiv.org/pdf/2506.08768.pdf", "abs": "https://arxiv.org/abs/2506.08768", "title": "AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP", "authors": ["Ahmed Hasanaath", "Aisha Alansari", "Ahmed Ashraf", "Chafik Salmane", "Hamzah Luqman", "Saad Ezzini"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable progress in reasoning\nabilities and general natural language processing (NLP) tasks, yet their\nperformance on Arabic data, characterized by rich morphology, diverse dialects,\nand complex script, remains underexplored. This paper presents a comprehensive\nbenchmarking study of multiple reasoning-focused LLMs, with a special emphasis\non the newly introduced DeepSeek models, across a suite of fifteen Arabic NLP\ntasks. We experiment with various strategies, including zero-shot, few-shot,\nand fine-tuning. This allows us to systematically evaluate performance on\ndatasets covering a range of applications to examine their capacity for\nlinguistic reasoning under different levels of complexity. Our experiments\nreveal several key findings. First, carefully selecting just three in-context\nexamples delivers an average uplift of over 13 F1 points on classification\ntasks-boosting sentiment analysis from 35.3% to 87.5% and paraphrase detection\nfrom 56.1% to 87.0%. Second, reasoning-focused DeepSeek architectures\noutperform a strong GPT o4-mini baseline by an average of 12 F1 points on\ncomplex inference tasks in the zero-shot setting. Third, LoRA-based fine-tuning\nyields up to an additional 8 points in F1 and BLEU compared to equivalent\nincreases in model scale. The code is available at\nhttps://anonymous.4open.science/r/AraReasoner41299"}
{"id": "2506.08885", "pdf": "https://arxiv.org/pdf/2506.08885.pdf", "abs": "https://arxiv.org/abs/2506.08885", "title": "AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)", "authors": ["Danush Khanna", "Krishna Kumar", "Basab Ghosh", "Vinija Jain", "Vasu Sharma", "Aman Chadha", "Amitava Das"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Adversarial threats against LLMs are escalating faster than current defenses\ncan adapt. We expose a critical geometric blind spot in alignment: adversarial\nprompts exploit latent camouflage, embedding perilously close to the safe\nrepresentation manifold while encoding unsafe intent thereby evading surface\nlevel defenses like Direct Preference Optimization (DPO), which remain blind to\nthe latent geometry. We introduce ALKALI, the first rigorously curated\nadversarial benchmark and the most comprehensive to date spanning 9,000 prompts\nacross three macro categories, six subtypes, and fifteen attack families.\nEvaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates\n(ASRs) across both open and closed source models, exposing an underlying\nvulnerability we term latent camouflage, a structural blind spot where\nadversarial completions mimic the latent geometry of safe ones. To mitigate\nthis vulnerability, we introduce GRACE - Geometric Representation Aware\nContrastive Enhancement, an alignment framework coupling preference learning\nwith latent space regularization. GRACE enforces two constraints: latent\nseparation between safe and adversarial completions, and adversarial cohesion\namong unsafe and jailbreak behaviors. These operate over layerwise pooled\nembeddings guided by a learned attention profile, reshaping internal geometry\nwithout modifying the base model, and achieve up to 39% ASR reduction.\nMoreover, we introduce AVQI, a geometry aware metric that quantifies latent\nalignment failure via cluster separation and compactness. AVQI reveals when\nunsafe completions mimic the geometry of safe ones, offering a principled lens\ninto how models internally encode safety. We make the code publicly available\nat https://anonymous.4open.science/r/alkali-B416/README.md."}
{"id": "2506.08952", "pdf": "https://arxiv.org/pdf/2506.08952.pdf", "abs": "https://arxiv.org/abs/2506.08952", "title": "Can LLMs Ground when they (Don't) Know: A Study on Direct and Loaded Political Questions", "authors": ["Clara Lachenmaier", "Judith Sieker", "Sina Zarrieß"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint accepted at ACL Main Conference 2025", "summary": "Communication among humans relies on conversational grounding, allowing\ninterlocutors to reach mutual understanding even when they do not have perfect\nknowledge and must resolve discrepancies in each other's beliefs. This paper\ninvestigates how large language models (LLMs) manage common ground in cases\nwhere they (don't) possess knowledge, focusing on facts in the political domain\nwhere the risk of misinformation and grounding failure is high. We examine the\nability of LLMs to answer direct knowledge questions and loaded questions that\npresuppose misinformation. We evaluate whether loaded questions lead LLMs to\nengage in active grounding and correct false user beliefs, in connection to\ntheir level of knowledge and their political bias. Our findings highlight\nsignificant challenges in LLMs' ability to engage in grounding and reject false\nuser beliefs, raising concerns about their role in mitigating misinformation in\npolitical discourse."}
{"id": "2506.09003", "pdf": "https://arxiv.org/pdf/2506.09003.pdf", "abs": "https://arxiv.org/abs/2506.09003", "title": "SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner", "authors": ["Lei Zhang", "Jiaxi Yang", "Min Yang", "Jian Yang", "Mouxiang Chen", "Jiajun Zhang", "Zeyu Cui", "Binyuan Hui", "Junyang Lin"], "categories": ["cs.CL"], "comment": "Accepted by ICML2025", "summary": "We introduce **SWE-Flow**, a novel data synthesis framework grounded in\nTest-Driven Development (TDD). Unlike existing software engineering data that\nrely on human-submitted issues, **SWE-Flow** automatically infers incremental\ndevelopment steps directly from unit tests, which inherently encapsulate\nhigh-level requirements. The core of **SWE-Flow** is the construction of a\nRuntime Dependency Graph (RDG), which precisely captures function interactions,\nenabling the generation of a structured, step-by-step *development schedule*.\nAt each step, **SWE-Flow** produces a partial codebase, the corresponding unit\ntests, and the necessary code modifications, resulting in fully verifiable TDD\ntasks. With this approach, we generated 16,061 training instances and 2,020\ntest instances from real-world GitHub projects, creating the **SWE-Flow-Eval**\nbenchmark. Our experiments show that fine-tuning open model on this dataset\nsignificantly improves performance in TDD-based coding. To facilitate further\nresearch, we release all code, datasets, models, and Docker images at\n[Github](https://github.com/Hambaobao/SWE-Flow)."}
{"id": "2506.09009", "pdf": "https://arxiv.org/pdf/2506.09009.pdf", "abs": "https://arxiv.org/abs/2506.09009", "title": "UD-KSL Treebank v1.3: A semi-automated framework for aligning XPOS-extracted units with UPOS tags", "authors": ["Hakyung Sung", "Gyu-Ho Shin", "Chanyoung Lee", "You Kyung Sung", "Boo Kyung Jung"], "categories": ["cs.CL"], "comment": null, "summary": "The present study extends recent work on Universal Dependencies annotations\nfor second-language (L2) Korean by introducing a semi-automated framework that\nidentifies morphosyntactic constructions from XPOS sequences and aligns those\nconstructions with corresponding UPOS categories. We also broaden the existing\nL2-Korean corpus by annotating 2,998 new sentences from argumentative essays.\nTo evaluate the impact of XPOS-UPOS alignments, we fine-tune L2-Korean\nmorphosyntactic analysis models on datasets both with and without these\nalignments, using two NLP toolkits. Our results indicate that the aligned\ndataset not only improves consistency across annotation layers but also\nenhances morphosyntactic tagging and dependency-parsing accuracy, particularly\nin cases of limited annotated data."}
{"id": "2506.09021", "pdf": "https://arxiv.org/pdf/2506.09021.pdf", "abs": "https://arxiv.org/abs/2506.09021", "title": "Comparing human and LLM proofreading in L2 writing: Impact on lexical and syntactic features", "authors": ["Hakyung Sung", "Karla Csuros", "Min-Chang Sung"], "categories": ["cs.CL"], "comment": null, "summary": "This study examines the lexical and syntactic interventions of human and LLM\nproofreading aimed at improving overall intelligibility in identical second\nlanguage writings, and evaluates the consistency of outcomes across three LLMs\n(ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b). Findings show that both human and\nLLM proofreading enhance bigram lexical features, which may contribute to\nbetter coherence and contextual connectedness between adjacent words. However,\nLLM proofreading exhibits a more generative approach, extensively reworking\nvocabulary and sentence structures, such as employing more diverse and\nsophisticated vocabulary and incorporating a greater number of adjective\nmodifiers in noun phrases. The proofreading outcomes are highly consistent in\nmajor lexical and syntactic features across the three models."}
{"id": "2506.09047", "pdf": "https://arxiv.org/pdf/2506.09047.pdf", "abs": "https://arxiv.org/abs/2506.09047", "title": "Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs", "authors": ["Yaniv Nikankin", "Dana Arad", "Yossi Gandelsman", "Yonatan Belinkov"], "categories": ["cs.CL", "68T5", "I.2.7"], "comment": null, "summary": "Vision-Language models (VLMs) show impressive abilities to answer questions\non visual inputs (e.g., counting objects in an image), yet demonstrate higher\naccuracies when performing an analogous task on text (e.g., counting words in a\ntext). We investigate this accuracy gap by identifying and comparing the\n\\textit{circuits} - the task-specific computational sub-graphs - in different\nmodalities. We show that while circuits are largely disjoint between\nmodalities, they implement relatively similar functionalities: the differences\nlie primarily in processing modality-specific data positions (an image or a\ntext sequence). Zooming in on the image data representations, we observe they\nbecome aligned with the higher-performing analogous textual representations\nonly towards later layers, too late in processing to effectively influence\nsubsequent positions. To overcome this, we patch the representations of visual\ndata tokens from later layers back into earlier layers. In experiments with\nmultiple tasks and models, this simple intervention closes a third of the\nperformance gap between the modalities, on average. Our analysis sheds light on\nthe multi-modal performance gap in VLMs and suggests a training-free approach\nfor reducing it."}
{"id": "2309.09652", "pdf": "https://arxiv.org/pdf/2309.09652.pdf", "abs": "https://arxiv.org/abs/2309.09652", "title": "Speech Synthesis By Unrolling Diffusion Process using Neural Network Layers", "authors": ["Peter Ochieng"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "10 pages", "summary": "This work introduces UDPNet, a novel architecture designed to accelerate the\nreverse diffusion process in speech synthesis. Unlike traditional diffusion\nmodels that rely on timestep embeddings and shared network parameters, UDPNet\nunrolls the reverse diffusion process directly into the network architecture,\nwith successive layers corresponding to equally spaced steps in the diffusion\nschedule. Each layer progressively refines the noisy input, culminating in a\nhigh-fidelity estimation of the original data, \\(x_0\\). Additionally, we\nredefine the learning target by predicting latent variables instead of the\nconventional \\(x_0\\) or noise \\(\\epsilon_0\\). This shift addresses the common\nissue of large prediction errors in early denoising stages, effectively\nreducing speech distortion. Extensive evaluations on single- and multi-speaker\ndatasets demonstrate that UDPNet consistently outperforms state-of-the-art\nmethods in both quality and efficiency, while generalizing effectively to\nunseen speech. These results position UDPNet as a robust solution for real-time\nspeech synthesis applications. Sample audio is available at\nhttps://onexpeters.github.io/UDPNet."}
{"id": "2402.03710", "pdf": "https://arxiv.org/pdf/2402.03710.pdf", "abs": "https://arxiv.org/abs/2402.03710", "title": "Listen, Chat, and Remix: Text-Guided Soundscape Remixing for Enhanced Auditory Experience", "authors": ["Xilin Jiang", "Cong Han", "Yinghao Aaron Li", "Nima Mesgarani"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted by IEEE Journal of Selected Topics in Signal Processing\n  (JSTSP)", "summary": "In daily life, we encounter a variety of sounds, both desirable and\nundesirable, with limited control over their presence and volume. Our work\nintroduces \"Listen, Chat, and Remix\" (LCR), a novel multimodal sound remixer\nthat controls each sound source in a mixture based on user-provided text\ninstructions. LCR distinguishes itself with a user-friendly text interface and\nits unique ability to remix multiple sound sources simultaneously within a\nmixture, without needing to separate them. Users input open-vocabulary text\nprompts, which are interpreted by a large language model to create a semantic\nfilter for remixing the sound mixture. The system then decomposes the mixture\ninto its components, applies the semantic filter, and reassembles filtered\ncomponents back to the desired output. We developed a 160-hour dataset with\nover 100k mixtures, including speech and various audio sources, along with text\nprompts for diverse remixing tasks including extraction, removal, and volume\ncontrol of single or multiple sources. Our experiments demonstrate significant\nimprovements in signal quality across all remixing tasks and robust performance\nin zero-shot scenarios with varying numbers and types of sound sources. An\naudio demo is available at: https://listenchatremix.github.io/demo."}
{"id": "2403.13106", "pdf": "https://arxiv.org/pdf/2403.13106.pdf", "abs": "https://arxiv.org/abs/2403.13106", "title": "Using Shapley interactions to understand how models use structure", "authors": ["Divyansh Singhvi", "Diganta Misra", "Andrej Erkelens", "Raghav Jain", "Isabel Papadimitriou", "Naomi Saphra"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "Published in ACL 2025", "summary": "Language is an intricately structured system, and a key goal of NLP\ninterpretability is to provide methodological insights for understanding how\nlanguage models represent this structure internally. In this paper, we use\nShapley Taylor interaction indices (STII) in order to examine how language and\nspeech models internally relate and structure their inputs. Pairwise Shapley\ninteractions measure how much two inputs work together to influence model\noutputs beyond if we linearly added their independent influences, providing a\nview into how models encode structural interactions between inputs. We relate\nthe interaction patterns in models to three underlying linguistic structures:\nsyntactic structure, non-compositional semantics, and phonetic coarticulation.\nWe find that autoregressive text models encode interactions that correlate with\nthe syntactic proximity of inputs, and that both autoregressive and masked\nmodels encode nonlinear interactions in idiomatic phrases with\nnon-compositional semantics. Our speech results show that inputs are more\nentangled for pairs where a neighboring consonant is likely to influence a\nvowel or approximant, showing that models encode the phonetic interaction\nneeded for extracting discrete phonemic representations."}
{"id": "2406.14917", "pdf": "https://arxiv.org/pdf/2406.14917.pdf", "abs": "https://arxiv.org/abs/2406.14917", "title": "LLM2TEA: Agentic AI Designer Finds Innovative Objects with Generative Evolutionary Multitasking", "authors": ["Melvin Wong", "Jiao Liu", "Thiago Rios", "Stefan Menzel", "Yew Soon Ong"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.NE"], "comment": "This work has been submitted to the IEEE for review", "summary": "In this paper, we introduce LLM-driven MultiTask Evolutionary Algorithm\n(LLM2TEA), the first agentic AI designer within a generative evolutionary\nmultitasking (GEM) framework that promotes the crossover and synergy of designs\nfrom multiple domains, leading to innovative solutions that transcend\nindividual disciplines. Of particular interest is the discovery of objects that\nare not only innovative but also conform to the physical specifications of the\nreal world in science and engineering. LLM2TEA comprises a large language model\nto initialize a population of genotypes (defined by text prompts) describing\nthe objects of interest, a text-to-3D generative model to produce phenotypes\nfrom these prompts, a classifier to interpret the semantic representations of\nthe objects, and a physics simulation model to assess their physical\nproperties. We propose several novel LLM-based multitask evolutionary operators\nto guide the search toward the discovery of high-performing practical objects.\nExperimental results in conceptual design optimization validate the\neffectiveness of LLM2TEA, revealing from 97\\% to 174\\% improvement in the\ndiversity of innovative objects compared to the present text-to-3D generative\nmodel baseline. In addition, more than 73\\% of the generated designs have\nbetter physical performance than the top 1\\% percentile of the designs\ngenerated in the baseline. Moreover, LLM2TEA generates designs that are not\nonly aesthetically creative but also functional in real-world applications.\nSeveral of these designs have been successfully 3D-printed, emphasizing the\nproposed approach's capacity to transform AI-generated outputs into tangible\nphysical objects. The designs produced by LLM2TEA meets practical requirements\nwhile showcasing creative and innovative features, underscoring its potential\napplications in complex design optimization and discovery."}
{"id": "2406.15481", "pdf": "https://arxiv.org/pdf/2406.15481.pdf", "abs": "https://arxiv.org/abs/2406.15481", "title": "Code-Switching Red-Teaming: LLM Evaluation for Safety and Multilingual Understanding", "authors": ["Haneul Yoo", "Yongjin Yang", "Hwaran Lee"], "categories": ["cs.AI", "cs.CL"], "comment": "To appear in ACL 2025", "summary": "As large language models (LLMs) have advanced rapidly, concerns regarding\ntheir safety have become prominent. In this paper, we discover that\ncode-switching in red-teaming queries can effectively elicit undesirable\nbehaviors of LLMs, which are common practices in natural language. We introduce\na simple yet effective framework, CSRT, to synthesize codeswitching red-teaming\nqueries and investigate the safety and multilingual understanding of LLMs\ncomprehensively. Through extensive experiments with ten state-of-the-art LLMs\nand code-switching queries combining up to 10 languages, we demonstrate that\nthe CSRT significantly outperforms existing multilingual red-teaming\ntechniques, achieving 46.7% more attacks than standard attacks in English and\nbeing effective in conventional safety domains. We also examine the\nmultilingual ability of those LLMs to generate and understand codeswitching\ntexts. Additionally, we validate the extensibility of the CSRT by generating\ncodeswitching attack prompts with monolingual data. We finally conduct detailed\nablation studies exploring code-switching and propound unintended correlation\nbetween resource availability of languages and safety alignment in existing\nmultilingual LLMs."}
{"id": "2406.19384", "pdf": "https://arxiv.org/pdf/2406.19384.pdf", "abs": "https://arxiv.org/abs/2406.19384", "title": "The Remarkable Robustness of LLMs: Stages of Inference?", "authors": ["Vedang Lad", "Wes Gurnee", "Max Tegmark"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "For Github code see\n  https://github.com/vdlad/Remarkable-Robustness-of-LLMs. Send all\n  correspondence to the first author", "summary": "We investigate the robustness of Large Language Models (LLMs) to structural\ninterventions by deleting and swapping adjacent layers during inference.\nSurprisingly, models retain 72-95% of their original top-1 prediction accuracy\nwithout any fine-tuning. We find that performance degradation is not uniform\nacross layers: interventions to the early and final layers cause the most\ndegradation, while the model is remarkably robust to dropping middle layers.\nThis pattern of localized sensitivity motivates our hypothesis of four stages\nof inference, observed across diverse model families and sizes: (1)\ndetokenization, where local context is integrated to lift raw token embeddings\ninto higher-level representations; (2) feature engineering, where task- and\nentity-specific features are iteratively refined; (3) prediction ensembling,\nwhere hidden states are aggregated into plausible next-token predictions; and\n(4) residual sharpening, where irrelevant features are suppressed to finalize\nthe output distribution. Synthesizing behavioral and mechanistic evidence, we\nprovide a framework for interpreting depth-dependent computations in LLMs."}
{"id": "2407.01067", "pdf": "https://arxiv.org/pdf/2407.01067.pdf", "abs": "https://arxiv.org/abs/2407.01067", "title": "Human-like object concept representations emerge naturally in multimodal large language models", "authors": ["Changde Du", "Kaicheng Fu", "Bincheng Wen", "Yi Sun", "Jie Peng", "Wei Wei", "Ying Gao", "Shengpei Wang", "Chuncheng Zhang", "Jinpeng Li", "Shuang Qiu", "Le Chang", "Huiguang He"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC", "cs.LG"], "comment": "Published on Nature Machine Intelligence", "summary": "Understanding how humans conceptualize and categorize natural objects offers\ncritical insights into perception and cognition. With the advent of Large\nLanguage Models (LLMs), a key question arises: can these models develop\nhuman-like object representations from linguistic and multimodal data? In this\nstudy, we combined behavioral and neuroimaging analyses to explore the\nrelationship between object concept representations in LLMs and human\ncognition. We collected 4.7 million triplet judgments from LLMs and Multimodal\nLLMs (MLLMs) to derive low-dimensional embeddings that capture the similarity\nstructure of 1,854 natural objects. The resulting 66-dimensional embeddings\nwere stable, predictive, and exhibited semantic clustering similar to human\nmental representations. Remarkably, the dimensions underlying these embeddings\nwere interpretable, suggesting that LLMs and MLLMs develop human-like\nconceptual representations of objects. Further analysis showed strong alignment\nbetween model embeddings and neural activity patterns in brain regions such as\nEBA, PPA, RSC, and FFA. This provides compelling evidence that the object\nrepresentations in LLMs, while not identical to human ones, share fundamental\nsimilarities that reflect key aspects of human conceptual knowledge. Our\nfindings advance the understanding of machine intelligence and inform the\ndevelopment of more human-like artificial cognitive systems."}
{"id": "2408.03573", "pdf": "https://arxiv.org/pdf/2408.03573.pdf", "abs": "https://arxiv.org/abs/2408.03573", "title": "AcTracer: Active Testing of Large Language Model via Multi-Stage Sampling", "authors": ["Yuheng Huang", "Jiayang Song", "Qiang Hu", "Felix Juefei-Xu", "Lei Ma"], "categories": ["cs.SE", "cs.AI", "cs.CL", "D.2.5; I.2.7"], "comment": "To appear in ACM Transactions on Software Engineering and Methodology\n  (2025)", "summary": "Performance evaluation plays a crucial role in the development life cycle of\nlarge language models (LLMs). It estimates the model's capability, elucidates\nbehavior characteristics, and facilitates the identification of potential\nissues and limitations, thereby guiding further improvement. Given that LLMs'\ndiverse task-handling abilities stem from large volumes of training data, a\ncomprehensive evaluation also necessitates abundant, well-annotated, and\nrepresentative test data to assess LLM performance across various downstream\ntasks. However, the demand for high-quality test data often entails substantial\ntime, computational resources, and manual efforts, sometimes causing the\nevaluation to be inefficient or impractical. To address these challenges,\nresearchers propose active testing, which estimates the overall performance by\nselecting a subset of test data. Nevertheless, the existing active testing\nmethods tend to be inefficient, even inapplicable, given the unique new\nchallenges of LLMs (e.g., diverse task types, increased model complexity, and\nunavailability of training data). To mitigate such limitations and expedite the\ndevelopment cycle of LLMs, in this work, we introduce AcTracer, an active\ntesting framework tailored for LLMs that strategically selects a small subset\nof test data to achieve a more accurate performance estimation for LLMs.\nAcTracer utilizes both internal and external information from LLMs to guide the\ntest sampling process, reducing variance through a multi-stage pool-based\nactive selection. Our experiment results demonstrate that AcTracer achieves\nstate-of-the-art performance compared to existing methods across various tasks."}
{"id": "2410.02080", "pdf": "https://arxiv.org/pdf/2410.02080.pdf", "abs": "https://arxiv.org/abs/2410.02080", "title": "EMMA: Efficient Visual Alignment in Multi-Modal LLMs", "authors": ["Sara Ghazanfari", "Alexandre Araujo", "Prashanth Krishnamurthy", "Siddharth Garg", "Farshad Khorrami"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Multi-modal Large Language Models (MLLMs) have recently exhibited impressive\ngeneral-purpose capabilities by leveraging vision foundation models to encode\nthe core concepts of images into representations. These are then combined with\ninstructions and processed by the language model to generate high-quality\nresponses. Despite significant progress in enhancing the language component,\nchallenges persist in optimally fusing visual encodings within the language\nmodel for task-specific adaptability. Recent research has focused on improving\nthis fusion through modality adaptation modules but at the cost of\nsignificantly increased model complexity and training data needs. In this\npaper, we propose EMMA (Efficient Multi-Modal Adaptation), a lightweight\ncross-modality module designed to efficiently fuse visual and textual\nencodings, generating instruction-aware visual representations for the language\nmodel. Our key contributions include: (1) an efficient early fusion mechanism\nthat integrates vision and language representations with minimal added\nparameters (less than 0.2% increase in model size), (2) an in-depth\ninterpretability analysis that sheds light on the internal mechanisms of the\nproposed method; (3) comprehensive experiments that demonstrate notable\nimprovements on both specialized and general benchmarks for MLLMs. Empirical\nresults show that EMMA boosts performance across multiple tasks by up to 9.3%\nwhile significantly improving robustness against hallucinations. Our code is\navailable at https://github.com/SaraGhazanfari/EMMA"}
{"id": "2410.02197", "pdf": "https://arxiv.org/pdf/2410.02197.pdf", "abs": "https://arxiv.org/abs/2410.02197", "title": "Beyond Bradley-Terry Models: A General Preference Model for Language Model Alignment", "authors": ["Yifan Zhang", "Ge Zhang", "Yue Wu", "Kangping Xu", "Quanquan Gu"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to the 42nd International Conference on Machine Learning\n  (ICML 2025)", "summary": "Modeling human preferences is crucial for aligning foundation models with\nhuman values. Traditional reward modeling methods, such as the Bradley-Terry\n(BT) reward model, fall short in expressiveness, particularly in addressing\nintransitive preferences. In this paper, we introduce preference embedding, an\napproach that embeds responses into a latent space to capture intricate\npreference structures efficiently, achieving linear query complexity.\nAdditionally, we propose preference score-based General Preference Optimization\n(GPO), which generalizes reward-based reinforcement learning from human\nfeedback (RLHF). Experimental results show that our General Preference\nembedding Model (GPM) consistently outperforms the BT reward model on the\nRewardBench benchmark and effectively models cyclic preferences where any BT\nreward model behaves like a random guess. Furthermore, evaluations on\ndownstream tasks such as AlpacaEval2.0, following the language model\npost-training with GPO and our general preference model, reveal performance\nimprovements over BT models. These findings indicate that our method may\nenhance the alignment of foundation models with nuanced human values. The code\nis available at https://github.com/general-preference/general-preference-model."}
{"id": "2410.23323", "pdf": "https://arxiv.org/pdf/2410.23323.pdf", "abs": "https://arxiv.org/abs/2410.23323", "title": "Phonology-Guided Speech-to-Speech Translation for African Languages", "authors": ["Peter Ochieng", "Dennis Kaburu"], "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": null, "summary": "We present a prosody-guided framework for speech-to-speech translation (S2ST)\nthat aligns and translates speech \\emph{without} transcripts by leveraging\ncross-linguistic pause synchrony. Analyzing a 6{,}000-hour East African news\ncorpus spanning five languages, we show that \\emph{within-phylum} language\npairs exhibit 30--40\\% lower pause variance and over 3$\\times$ higher\nonset/offset correlation compared to cross-phylum pairs. These findings\nmotivate \\textbf{SPaDA}, a dynamic-programming alignment algorithm that\nintegrates silence consistency, rate synchrony, and semantic similarity. SPaDA\nimproves alignment $F_1$ by +3--4 points and eliminates up to 38\\% of spurious\nmatches relative to greedy VAD baselines. Using SPaDA-aligned segments, we\ntrain \\textbf{SegUniDiff}, a diffusion-based S2ST model guided by\n\\emph{external gradients} from frozen semantic and speaker encoders. SegUniDiff\nmatches an enhanced cascade in BLEU (30.3 on CVSS-C vs.\\ 28.9 for UnitY),\nreduces speaker error rate (EER) from 12.5\\% to 5.3\\%, and runs at an RTF of\n1.02. To support evaluation in low-resource settings, we also release a\nthree-tier, transcript-free BLEU suite (M1--M3) that correlates strongly with\nhuman judgments. Together, our results show that prosodic cues in multilingual\nspeech provide a reliable scaffold for scalable, non-autoregressive S2ST."}
{"id": "2411.12977", "pdf": "https://arxiv.org/pdf/2411.12977.pdf", "abs": "https://arxiv.org/abs/2411.12977", "title": "MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong Cultural Learning", "authors": ["Mircea Lică", "Ojas Shirekar", "Baptiste Colle", "Chirag Raman"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Embodied agents powered by large language models (LLMs), such as Voyager,\npromise open-ended competence in worlds such as Minecraft. However, when\npowered by open-weight LLMs they still falter on elementary tasks after\ndomain-specific fine-tuning. We propose MindForge, a generative-agent framework\nfor cultural lifelong learning through explicit perspective taking. We\nintroduce three key innovations: (1) a structured theory of mind representation\nlinking percepts, beliefs, desires, and actions; (2) natural inter-agent\ncommunication; and (3) a multi-component memory system. Following the cultural\nlearning framework, we test MindForge in both instructive and collaborative\nsettings within Minecraft. In an instructive setting with GPT-4, MindForge\nagents powered by open-weight LLMs significantly outperform their Voyager\ncounterparts in basic tasks yielding $3\\times$ more tech-tree milestones and\ncollecting $2.3\\times$ more unique items than the Voyager baseline.\nFurthermore, in fully \\textit{collaborative} settings, we find that the\nperformance of two underachieving agents improves with more communication\nrounds, echoing the Condorcet Jury Theorem. MindForge agents demonstrate\nsophisticated behaviors, including expert-novice knowledge transfer,\ncollaborative problem solving, and adaptation to out-of-distribution tasks\nthrough accumulated cultural experiences."}
{"id": "2412.20367", "pdf": "https://arxiv.org/pdf/2412.20367.pdf", "abs": "https://arxiv.org/abs/2412.20367", "title": "Enhancing Code LLMs with Reinforcement Learning in Code Generation: A Survey", "authors": ["Junqiao Wang", "Zeng Zhang", "Yangfan He", "Zihao Zhang", "Yuyang Song", "Tianyu Shi", "Yuchen Li", "Hengyuan Xu", "Kunyu Wu", "Xin Yi", "Zhongwei Wan", "Xinhang Yuan", "Kuan Lu", "Menghao Huo", "Tang Jingqun", "Guangwu Qian", "Keqin Li", "Qiuwu Chen", "Lewei He"], "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing\nlarge language models (LLMs) in code generation and optimization. This survey\nsystematically reviews RL-driven techniques across the code development\nlifecycle, from compiler-level optimizations and resource allocation strategies\nto end-to-end code synthesis frameworks. We first examine classical and modern\nRL algorithms -- spanning policy gradients, actor-critic methods,\nhuman-feedback alignment, and preference-based optimization -- and their\nadaptations to the unique challenges of code generation, such as sparse and\ndelayed rewards. Next, we analyze key benchmarks, datasets, and evaluation\nmetrics that drive progress in RL-augmented Code LLMs. Finally, we identify\nopen problems, including the need for richer feedback sources, support for\nlow-level and domain-specific languages, and methods to reduce computational\noverhead. By consolidating current insights and outlining future directions,\nthis work aims to guide researchers and practitioners in leveraging RL to\nproduce more robust, efficient, and human-aligned code generation systems."}
{"id": "2501.00654", "pdf": "https://arxiv.org/pdf/2501.00654.pdf", "abs": "https://arxiv.org/abs/2501.00654", "title": "ICONS: Influence Consensus for Vision-Language Data Selection", "authors": ["Xindi Wu", "Mengzhou Xia", "Rulin Shao", "Zhiwei Deng", "Pang Wei Koh", "Olga Russakovsky"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "31 pages, 19 figures", "summary": "Training vision-language models via instruction tuning often relies on large\nmixtures of data spanning diverse tasks and domains. However, these mixtures\nfrequently include redundant information, increasing computational costs\nwithout proportional performance gains, necessitating more effective data\nselection strategies. Existing methods typically rely on task-agnostic\nheuristics to estimate data importance or focus on optimizing single tasks in\nisolation, limiting their effectiveness in multitask settings. In this work, we\nintroduce ICONS, a gradient-based Influence CONsensus approach for\nvision-language data Selection. Our method leverages first-order training\ndynamics to estimate the influence of individual training examples on\nvalidation performance and aggregates these estimates across tasks via majority\nvoting over task-specific influences. This cross-task consensus identifies data\npoints that are consistently valuable across tasks, enabling us to prioritize\nexamples that drive overall performance. The voting-based design further\nmitigates issues such as score calibration and outlier sensitivity, resulting\nin robust and scalable data selection for diverse multitask mixtures. With only\n20% of the data from LLaVA-665K and Cambrian-7M, our selected subsets retain\n98.6% and 98.8% of the performance achieved with full datasets, and can even\nsurpass full data training at a 60% selection ratio on LLaVA-665K. Our approach\nalso generalizes to unseen tasks and architectures, demonstrating strong\ntransfer. We release two compact, high-utility subsets, LLaVA-ICONS-133K and\nCambrian-ICONS-1.4M, preserving impactful training examples for efficient and\nscalable vision-language model development."}
{"id": "2501.11223", "pdf": "https://arxiv.org/pdf/2501.11223.pdf", "abs": "https://arxiv.org/abs/2501.11223", "title": "Reasoning Language Models: A Blueprint", "authors": ["Maciej Besta", "Julia Barth", "Eric Schreiber", "Ales Kubicek", "Afonso Catarino", "Robert Gerstenberger", "Piotr Nyczyk", "Patrick Iff", "Yueling Li", "Sam Houliston", "Tomasz Sternal", "Marcin Copik", "Grzegorz Kwaśniewski", "Jürgen Müller", "Łukasz Flis", "Hannes Eberhard", "Zixuan Chen", "Hubert Niewiadomski", "Torsten Hoefler"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning language models (RLMs), also known as Large Reasoning Models\n(LRMs), such as OpenAI's o1 and o3, DeepSeek-R1, and Alibaba's QwQ, have\nredefined AI's problem-solving capabilities by extending LLMs with advanced\nreasoning mechanisms. Yet, their high costs, proprietary nature, and complex\narchitectures - uniquely combining reinforcement learning (RL), search\nheuristics, and LLMs - present accessibility and scalability challenges. To\naddress these, we propose a comprehensive blueprint that organizes RLM\ncomponents into a modular framework, based on a survey and analysis of all RLM\nworks. This blueprint incorporates diverse reasoning structures (chains, trees,\ngraphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search,\nBeam Search), RL concepts (policy, value models and others), supervision\nschemes (Outcome-Based and Process-Based Supervision), and other related\nconcepts (e.g., Test-Time Compute, Retrieval-Augmented Generation, agent\ntools). We also provide detailed mathematical formulations and algorithmic\nspecifications to simplify RLM implementation. By showing how schemes like\nLLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases,\nwe demonstrate the blueprint's versatility and unifying potential. To\nillustrate its utility, we introduce x1, a modular implementation for rapid RLM\nprototyping and experimentation. Using x1 and a literature review, we provide\nkey insights, such as multi-phase training for policy and value models, and the\nimportance of familiar training distributions. Finally, we discuss scalable RLM\ncloud deployments and we outline how RLMs can integrate with a broader LLM\necosystem. Our work demystifies RLM construction, democratizes advanced\nreasoning capabilities, and fosters innovation, aiming to mitigate the gap\nbetween \"rich AI\" and \"poor AI\" by lowering barriers to RLM design and\nexperimentation."}
{"id": "2502.10450", "pdf": "https://arxiv.org/pdf/2502.10450.pdf", "abs": "https://arxiv.org/abs/2502.10450", "title": "Trustworthy AI: Safety, Bias, and Privacy -- A Survey", "authors": ["Xingli Fang", "Jianwei Li", "Varun Mulchandani", "Jung-Eun Kim"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "The capabilities of artificial intelligence systems have been advancing to a\ngreat extent, but these systems still struggle with failure modes,\nvulnerabilities, and biases. In this paper, we study the current state of the\nfield, and present promising insights and perspectives regarding concerns that\nchallenge the trustworthiness of AI models. In particular, this paper\ninvestigates the issues regarding three thrusts: safety, privacy, and bias,\nwhich hurt models' trustworthiness. For safety, we discuss safety alignment in\nthe context of large language models, preventing them from generating toxic or\nharmful content. For bias, we focus on spurious biases that can mislead a\nnetwork. Lastly, for privacy, we cover membership inference attacks in deep\nneural networks. The discussions addressed in this paper reflect our own\nexperiments and observations."}
{"id": "2502.13131", "pdf": "https://arxiv.org/pdf/2502.13131.pdf", "abs": "https://arxiv.org/abs/2502.13131", "title": "Rethinking Diverse Human Preference Learning through Principal Component Analysis", "authors": ["Feng Luo", "Rui Yang", "Hao Sun", "Chunyuan Deng", "Jiarui Yao", "Jingyan Shen", "Huan Zhang", "Hanjie Chen"], "categories": ["cs.AI", "cs.CL"], "comment": "14 pages", "summary": "Understanding human preferences is crucial for improving foundation models\nand building personalized AI systems. However, preferences are inherently\ndiverse and complex, making it difficult for traditional reward models to\ncapture their full range. While fine-grained preference data can help,\ncollecting it is expensive and hard to scale. In this paper, we introduce\nDecomposed Reward Models (DRMs), a novel approach that extracts diverse human\npreferences from binary comparisons without requiring fine-grained annotations.\nOur key insight is to represent human preferences as vectors and analyze them\nusing Principal Component Analysis (PCA). By constructing a dataset of\nembedding differences between preferred and rejected responses, DRMs identify\northogonal basis vectors that capture distinct aspects of preference. These\ndecomposed rewards can be flexibly combined to align with different user needs,\noffering an interpretable and scalable alternative to traditional reward\nmodels. We demonstrate that DRMs effectively extract meaningful preference\ndimensions (e.g., helpfulness, safety, humor) and adapt to new users without\nadditional training. Our results highlight DRMs as a powerful framework for\npersonalized and interpretable LLM alignment. Our code is available at\nhttps://github.com/amandaluof/DRMs."}
{"id": "2502.16794", "pdf": "https://arxiv.org/pdf/2502.16794.pdf", "abs": "https://arxiv.org/abs/2502.16794", "title": "AAD-LLM: Neural Attention-Driven Auditory Scene Understanding", "authors": ["Xilin Jiang", "Sukru Samet Dindar", "Vishal Choudhari", "Stephan Bickel", "Ashesh Mehta", "Guy M McKhann", "Daniel Friedman", "Adeen Flinker", "Nima Mesgarani"], "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.HC", "eess.AS"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Auditory foundation models, including auditory large language models (LLMs),\nprocess all sound inputs equally, independent of listener perception. However,\nhuman auditory perception is inherently selective: listeners focus on specific\nspeakers while ignoring others in complex auditory scenes. Existing models do\nnot incorporate this selectivity, limiting their ability to generate\nperception-aligned responses. To address this, we introduce Intention-Informed\nAuditory Scene Understanding (II-ASU) and present Auditory Attention-Driven LLM\n(AAD-LLM), a prototype system that integrates brain signals to infer listener\nattention. AAD-LLM extends an auditory LLM by incorporating intracranial\nelectroencephalography (iEEG) recordings to decode which speaker a listener is\nattending to and refine responses accordingly. The model first predicts the\nattended speaker from neural activity, then conditions response generation on\nthis inferred attentional state. We evaluate AAD-LLM on speaker description,\nspeech transcription and extraction, and question answering in multitalker\nscenarios, with both objective and subjective ratings showing improved\nalignment with listener intention. By taking a first step toward\nintention-aware auditory AI, this work explores a new paradigm where listener\nperception informs machine listening, paving the way for future\nlistener-centered auditory systems. Demo and code available:\nhttps://aad-llm.github.io."}
{"id": "2502.19409", "pdf": "https://arxiv.org/pdf/2502.19409.pdf", "abs": "https://arxiv.org/abs/2502.19409", "title": "ImageChain: Advancing Sequential Image-to-Text Reasoning in Multimodal Large Language Models", "authors": ["Danae Sánchez Villegas", "Ingo Ziegler", "Desmond Elliott"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Code, dataset, and checkpoints are publicly available at\n  https://github.com/danaesavi/ImageChain; v2: added human annotation study to\n  validate SimRate", "summary": "Reasoning over sequences of images remains a challenge for multimodal large\nlanguage models (MLLMs). While recent models incorporate multi-image data\nduring pre-training, they still struggle to recognize sequential structures,\noften treating images independently. This work introduces ImageChain, a\nframework that enhances MLLMs with sequential reasoning capabilities over image\ndata by modeling visual sequences as a multi-turn conversation. In ImageChain,\nimages are interleaved with corresponding textual descriptions to form a\ncontrolled dialogue that explicitly captures temporal dependencies and\nnarrative progression. Our method optimizes for the task of next-scene\ndescription, where the model generates a context-aware description of an\nupcoming scene based on preceding visual and textual cues. We demonstrate that\nour approach improves performance on the next-scene description task --\nachieving an average improvement from 3.7% to 19% in SimRate, a metric that\nquantifies semantic similarity to human-annotated ground truths. Moreover,\nImageChain achieves robust zero-shot out-of-domain performance in applications\nranging from comics to robotics. Extensive experiments validate that\ninstruction-tuning in a multimodal, multi-turn conversation design is key to\nbridging the gap between static image understanding and temporally-aware\nreasoning."}
{"id": "2502.20383", "pdf": "https://arxiv.org/pdf/2502.20383.pdf", "abs": "https://arxiv.org/abs/2502.20383", "title": "Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis", "authors": ["Jeffrey Yang Fan Chiang", "Seungjae Lee", "Jia-Bin Huang", "Furong Huang", "Yizheng Chen"], "categories": ["cs.LG", "cs.CL"], "comment": "Project website: http://vulnerable-ai-agents.github.io", "summary": "Recent advancements in Web AI agents have demonstrated remarkable\ncapabilities in addressing complex web navigation tasks. However, emerging\nresearch shows that these agents exhibit greater vulnerability compared to\nstandalone Large Language Models (LLMs), despite both being built upon the same\nsafety-aligned models. This discrepancy is particularly concerning given the\ngreater flexibility of Web AI Agent compared to standalone LLMs, which may\nexpose them to a wider range of adversarial user inputs. To build a scaffold\nthat addresses these concerns, this study investigates the underlying factors\nthat contribute to the increased vulnerability of Web AI agents. Notably, this\ndisparity stems from the multifaceted differences between Web AI agents and\nstandalone LLMs, as well as the complex signals - nuances that simple\nevaluation metrics, such as success rate, often fail to capture. To tackle\nthese challenges, we propose a component-level analysis and a more granular,\nsystematic evaluation framework. Through this fine-grained investigation, we\nidentify three critical factors that amplify the vulnerability of Web AI\nagents; (1) embedding user goals into the system prompt, (2) multi-step action\ngeneration, and (3) observational capabilities. Our findings highlights the\npressing need to enhance security and robustness in AI agent design and provide\nactionable insights for targeted defense strategies."}
{"id": "2503.16563", "pdf": "https://arxiv.org/pdf/2503.16563.pdf", "abs": "https://arxiv.org/abs/2503.16563", "title": "Chem42: a Family of chemical Language Models for Target-aware Ligand Generation", "authors": ["Aahan Singh", "Engin Tekin", "Maryam Nadeem", "Nancy A. ElNaker", "Mohammad Amaan Sayeed", "Natalia Vassilieva", "Boulbaba Ben Amor"], "categories": ["cs.LG", "cs.AI", "cs.CL", "q-bio.BM"], "comment": null, "summary": "Revolutionizing drug discovery demands more than just understanding molecular\ninteractions - it requires generative models that can design novel ligands\ntailored to specific biological targets. While chemical Language Models (cLMs)\nhave made strides in learning molecular properties, most fail to incorporate\ntarget-specific insights, restricting their ability to drive de-novo ligand\ngeneration. Chem42, a cutting-edge family of generative chemical Language\nModels, is designed to bridge this gap. By integrating atomic-level\ninteractions with multimodal inputs from Prot42, a complementary protein\nLanguage Model, Chem42 achieves a sophisticated cross-modal representation of\nmolecular structures, interactions, and binding patterns. This innovative\nframework enables the creation of structurally valid, synthetically accessible\nligands with enhanced target specificity. Evaluations across diverse protein\ntargets confirm that Chem42 surpasses existing approaches in chemical validity,\ntarget-aware design, and predicted binding affinity. By reducing the search\nspace of viable drug candidates, Chem42 could accelerate the drug discovery\npipeline, offering a powerful generative AI tool for precision medicine. Our\nChem42 models set a new benchmark in molecule property prediction, conditional\nmolecule generation, and target-aware ligand design. The models are publicly\navailable at huggingface.co/inceptionai."}
{"id": "2504.15629", "pdf": "https://arxiv.org/pdf/2504.15629.pdf", "abs": "https://arxiv.org/abs/2504.15629", "title": "CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation Correction", "authors": ["Harsh Maheshwari", "Srikanth Tenneti", "Alwarappan Nakkiran"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Retrieval Augmented Generation (RAG) has emerged as a powerful application of\nLarge Language Models (LLMs), revolutionizing information search and\nconsumption. RAG systems combine traditional search capabilities with LLMs to\ngenerate comprehensive answers to user queries, ideally with accurate\ncitations. However, in our experience of developing a RAG product, LLMs often\nstruggle with source attribution, aligning with other industry studies\nreporting citation accuracy rates of only about 74% for popular generative\nsearch engines. To address this, we present efficient post-processing\nalgorithms to improve citation accuracy in LLM-generated responses, with\nminimal impact on latency and cost. Our approaches cross-check generated\ncitations against retrieved articles using methods including keyword + semantic\nmatching, fine tuned model with BERTScore, and a lightweight LLM-based\ntechnique. Our experimental results demonstrate a relative improvement of\n15.46% in the overall accuracy metrics of our RAG system. This significant\nenhancement potentially enables a shift from our current larger language model\nto a relatively smaller model that is approximately 12x more cost-effective and\n3x faster in inference time, while maintaining comparable performance. This\nresearch contributes to enhancing the reliability and trustworthiness of\nAI-generated content in information retrieval and summarization tasks which is\ncritical to gain customer trust especially in commercial products."}
{"id": "2504.17834", "pdf": "https://arxiv.org/pdf/2504.17834.pdf", "abs": "https://arxiv.org/abs/2504.17834", "title": "Unveiling the Hidden: Movie Genre and User Bias in Spoiler Detection", "authors": ["Haokai Zhang", "Shengtao Zhang", "Zijian Cai", "Heng Wang", "Ruixuan Zhu", "Zinan Zeng", "Minnan Luo"], "categories": ["cs.IR", "cs.CL"], "comment": "ECML PKDD 2025", "summary": "Spoilers in movie reviews are important on platforms like IMDb and Rotten\nTomatoes, offering benefits and drawbacks. They can guide some viewers' choices\nbut also affect those who prefer no plot details in advance, making effective\nspoiler detection essential. Existing spoiler detection methods mainly analyze\nreview text, often overlooking the impact of movie genres and user bias,\nlimiting their effectiveness. To address this, we analyze movie review data,\nfinding genre-specific variations in spoiler rates and identifying that certain\nusers are more likely to post spoilers. Based on these findings, we introduce a\nnew spoiler detection framework called GUSD (The code is available at\nhttps://github.com/AI-explorer-123/GUSD) (Genre-aware and User-specific Spoiler\nDetection), which incorporates genre-specific data and user behavior bias. User\nbias is calculated through dynamic graph modeling of review history.\nAdditionally, the R2GFormer module combines RetGAT (Retentive Graph Attention\nNetwork) for graph information and GenreFormer for genre-specific aggregation.\nThe GMoE (Genre-Aware Mixture of Experts) model further assigns reviews to\nspecialized experts based on genre. Extensive testing on benchmark datasets\nshows that GUSD achieves state-of-the-art results. This approach advances\nspoiler detection by addressing genre and user-specific patterns, enhancing\nuser experience on movie review platforms."}
{"id": "2505.14479", "pdf": "https://arxiv.org/pdf/2505.14479.pdf", "abs": "https://arxiv.org/abs/2505.14479", "title": "Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach", "authors": ["Oren Sultan", "Eitan Stern", "Dafna Shahaf"], "categories": ["cs.AI", "cs.CL"], "comment": "long paper", "summary": "Large language models (LLMs) struggle with formal domains that require\nrigorous logical deduction and symbolic reasoning, such as mathematical proof\ngeneration. We propose a neuro-symbolic approach that combines LLMs' generative\nstrengths with structured components to overcome this challenge. As a\nproof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)\nwe retrieve analogous problems and use their proofs to guide the LLM, and (2) a\nformal verifier evaluates the generated proofs and provides feedback, helping\nthe model fix incorrect proofs. We demonstrate that our method significantly\nimproves proof accuracy for OpenAI's o1 model (58%-70% improvement); both\nanalogous problems and the verifier's feedback contribute to these gains. More\nbroadly, shifting to LLMs that generate provably correct conclusions could\ndramatically improve their reliability, accuracy and consistency, unlocking\ncomplex tasks and critical real-world applications that require\ntrustworthiness."}
{"id": "2505.23885", "pdf": "https://arxiv.org/pdf/2505.23885.pdf", "abs": "https://arxiv.org/abs/2505.23885", "title": "OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation", "authors": ["Mengkang Hu", "Yuhang Zhou", "Wendong Fan", "Yuzhou Nie", "Bowei Xia", "Tao Sun", "Ziyu Ye", "Zhaoxuan Jin", "Yingru Li", "Qiguang Chen", "Zeyu Zhang", "Yifeng Wang", "Qianshuo Ye", "Bernard Ghanem", "Ping Luo", "Guohao Li"], "categories": ["cs.AI", "cs.CL"], "comment": "Project Page: https://github.com/camel-ai/owl", "summary": "Large Language Model (LLM)-based multi-agent systems show promise for\nautomating real-world tasks but struggle to transfer across domains due to\ntheir domain-specific nature. Current approaches face two critical\nshortcomings: they require complete architectural redesign and full retraining\nof all components when applied to new domains. We introduce Workforce, a\nhierarchical multi-agent framework that decouples strategic planning from\nspecialized execution through a modular architecture comprising: (i) a\ndomain-agnostic Planner for task decomposition, (ii) a Coordinator for subtask\nmanagement, and (iii) specialized Workers with domain-specific tool-calling\ncapabilities. This decoupling enables cross-domain transferability during both\ninference and training phases: During inference, Workforce seamlessly adapts to\nnew domains by adding or modifying worker agents; For training, we introduce\nOptimized Workforce Learning (OWL), which improves generalization across\ndomains by optimizing a domain-agnostic planner with reinforcement learning\nfrom real-world feedback. To validate our approach, we evaluate Workforce on\nthe GAIA benchmark, covering various realistic, multi-domain agentic tasks.\nExperimental results demonstrate Workforce achieves open-source\nstate-of-the-art performance (69.70%), outperforming commercial systems like\nOpenAI's Deep Research by 2.34%. More notably, our OWL-trained 32B model\nachieves 52.73% accuracy (+16.37%) and demonstrates performance comparable to\nGPT-4o on challenging tasks. To summarize, by enabling scalable generalization\nand modular domain transfer, our work establishes a foundation for the next\ngeneration of general-purpose AI assistants."}
{"id": "2506.05765", "pdf": "https://arxiv.org/pdf/2506.05765.pdf", "abs": "https://arxiv.org/abs/2506.05765", "title": "Do Large Vision-Language Models Distinguish between the Actual and Apparent Features of Illusions?", "authors": ["Taiga Shinozaki", "Tomoki Doi", "Amane Watahiki", "Satoshi Nishida", "Hitomi Yanaka"], "categories": ["cs.CV", "cs.CL"], "comment": "To appear in the Proceedings of the 47th Annual Meeting of the\n  Cognitive Science Society (COGSCI 2025)", "summary": "Humans are susceptible to optical illusions, which serve as valuable tools\nfor investigating sensory and cognitive processes. Inspired by human vision\nstudies, research has begun exploring whether machines, such as large vision\nlanguage models (LVLMs), exhibit similar susceptibilities to visual illusions.\nHowever, studies often have used non-abstract images and have not distinguished\nactual and apparent features, leading to ambiguous assessments of machine\ncognition. To address these limitations, we introduce a visual question\nanswering (VQA) dataset, categorized into genuine and fake illusions, along\nwith corresponding control images. Genuine illusions present discrepancies\nbetween actual and apparent features, whereas fake illusions have the same\nactual and apparent features even though they look illusory due to the similar\ngeometric configuration. We evaluate the performance of LVLMs for genuine and\nfake illusion VQA tasks and investigate whether the models discern actual and\napparent features. Our findings indicate that although LVLMs may appear to\nrecognize illusions by correctly answering questions about both feature types,\nthey predict the same answers for both Genuine Illusion and Fake Illusion VQA\nquestions. This suggests that their responses might be based on prior knowledge\nof illusions rather than genuine visual understanding. The dataset is available\nat https://github.com/ynklab/FILM"}
{"id": "2506.06975", "pdf": "https://arxiv.org/pdf/2506.06975.pdf", "abs": "https://arxiv.org/abs/2506.06975", "title": "Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test", "authors": ["Xiaoyuan Zhu", "Yaowen Ye", "Tianyi Qiu", "Hanlin Zhu", "Sijun Tan", "Ajraf Mannan", "Jonathan Michala", "Raluca Ada Popa", "Willie Neiswanger"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "As API access becomes a primary interface to large language models (LLMs),\nusers often interact with black-box systems that offer little transparency into\nthe deployed model. To reduce costs or maliciously alter model behaviors, API\nproviders may discreetly serve quantized or fine-tuned variants, which can\ndegrade performance and compromise safety. Detecting such substitutions is\ndifficult, as users lack access to model weights and, in most cases, even\noutput logits. To tackle this problem, we propose a rank-based uniformity test\nthat can verify the behavioral equality of a black-box LLM to a locally\ndeployed authentic model. Our method is accurate, query-efficient, and avoids\ndetectable query patterns, making it robust to adversarial providers that\nreroute or mix responses upon the detection of testing attempts. We evaluate\nthe approach across diverse threat scenarios, including quantization, harmful\nfine-tuning, jailbreak prompts, and full model substitution, showing that it\nconsistently achieves superior statistical power over prior methods under\nconstrained query budgets."}
{"id": "2506.07564", "pdf": "https://arxiv.org/pdf/2506.07564.pdf", "abs": "https://arxiv.org/abs/2506.07564", "title": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems", "authors": ["Peiran Li", "Xinkai Zou", "Zhuohang Wu", "Ruifeng Li", "Shuo Xing", "Hanwen Zheng", "Zhikai Hu", "Yuping Wang", "Haoxi Li", "Qin Yuan", "Yingmo Zhang", "Zhengzhong Tu"], "categories": ["cs.AI", "cs.CL"], "comment": "Former versions either contain unrelated content or cannot be\n  properly converted to PDF", "summary": "Recent advances in large language models (LLMs) and vision-language models\n(VLMs) have enabled powerful autonomous agents capable of complex reasoning and\nmulti-modal tool use. Despite their growing capabilities, today's agent\nframeworks remain fragile, lacking principled mechanisms for secure information\nflow, reliability, and multi-agent coordination. In this work, we introduce\nSAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based\nagents. SAFEFLOW enforces fine-grained information flow control (IFC),\nprecisely tracking provenance, integrity, and confidentiality of all the data\nexchanged between agents, tools, users, and environments. By constraining LLM\nreasoning to respect these security labels, SAFEFLOW prevents untrusted or\nadversarial inputs from contaminating high-integrity decisions. To ensure\nrobustness in concurrent multi-agent settings, SAFEFLOW introduces\ntransactional execution, conflict resolution, and secure scheduling over shared\nstate, preserving global consistency across agents. We further introduce\nmechanisms, including write-ahead logging, rollback, and secure caches, that\nfurther enhance resilience against runtime errors and policy violations. To\nvalidate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark\nsuite designed to evaluate agent reliability under adversarial, noisy, and\nconcurrent operational conditions. Extensive experiments demonstrate that\nagents built with SAFEFLOW maintain impressive task performance and security\nguarantees even in hostile environments, substantially outperforming\nstate-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for\nprincipled, robust, and secure agent ecosystems, advancing the frontier of\nreliable autonomy."}
{"id": "2506.08022", "pdf": "https://arxiv.org/pdf/2506.08022.pdf", "abs": "https://arxiv.org/abs/2506.08022", "title": "Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining", "authors": ["Chenxi Liu", "Tianyi Xiong", "Ruibo Chen", "Yihan Wu", "Junfeng Guo", "Tianyi Zhou", "Heng Huang"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "The task adaptation and alignment of Large Multimodal Models (LMMs) have been\nsignificantly advanced by instruction tuning and further strengthened by recent\npreference optimization. Yet, most LMMs still suffer from severe modality\nimbalance during reasoning, i.e., outweighing language prior biases over visual\ninputs, which bottlenecks their generalization to downstream tasks and causes\nhallucinations. However, existing preference optimization approaches for LMMs\ndo not focus on restraining the internal biases of their Large Language Model\n(LLM) backbones when curating the training data. Moreover, they heavily rely on\noffline data and lack the capacity to explore diverse responses adaptive to\ndynamic distributional shifts during training. Meanwhile, Group Relative Policy\nOptimization (GRPO), a recent method using online-generated data and verified\nrewards to improve reasoning capabilities, remains largely underexplored in LMM\nalignment. In this paper, we propose a novel preference learning framework,\nModality-Balancing Preference Optimization (MBPO), to address the modality\nimbalance in LMMs. MBPO constructs a more effective offline preference dataset\nby generating hard negatives, i.e., rejected responses misled by LLM biases due\nto limited usage of visual information, through adversarial perturbation of\ninput images. Moreover, MBPO leverages the easy-to-verify nature of close-ended\ntasks to generate online responses with verified rewards. GRPO is then employed\nto train the model with offline-online hybrid data. Extensive experiments\ndemonstrate that MBPO can enhance LMM performance on challenging\nvision-language tasks and effectively reduce hallucinations."}
