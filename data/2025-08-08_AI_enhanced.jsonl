{"id": "2508.04713", "pdf": "https://arxiv.org/pdf/2508.04713.pdf", "abs": "https://arxiv.org/abs/2508.04713", "title": "AI Should Be More Human, Not More Complex", "authors": ["Carlo Esposito"], "categories": ["cs.HC", "cs.AI"], "comment": "2025 - Knowledge Commons - Eyed Research Collection", "summary": "Large Language Models (LLMs) in search applications increasingly prioritize\nverbose, lexically complex responses that paradoxically reduce user\nsatisfaction and engagement. Through a comprehensive study of 10.000 (est.)\nparticipants comparing responses from five major AI-powered search systems, we\ndemonstrate that users overwhelmingly prefer concise, source-attributed\nresponses over elaborate explanations. Our analysis reveals that current AI\ndevelopment trends toward \"artificial sophistication\" create an uncanny valley\neffect where systems sound knowledgeable but lack genuine critical thinking,\nleading to reduced trust and increased cognitive load. We present evidence that\noptimal AI communication mirrors effective human discourse: direct, properly\nsourced, and honest about limitations. Our findings challenge the prevailing\nassumption that more complex AI responses indicate better performance, instead\nsuggesting that human-like brevity and transparency are key to user engagement\nand system reliability.", "AI": {"tldr": "LLMs in search applications often produce verbose responses that users find unsatisfactory. Users prefer concise and sourced answers, revealing issues in current AI communication trends.", "motivation": "To investigate user preferences in AI-powered search responses and the impact of response complexity on user satisfaction.", "method": "Conducted a comprehensive study with approximately 10,000 participants comparing responses from five major AI-powered search systems.", "result": "Users overwhelmingly preferred concise, source-attributed responses to verbose explanations, indicating a misalignment between AI development and user needs.", "conclusion": "Optimal AI communication should prioritize brevity and transparency, as more complex responses do not equate to better performance.", "key_contributions": ["Demonstrated user preference for concise responses", "Identified the impact of verbose AI responses on user trust", "Challenged existing paradigms of AI response complexity"], "limitations": "", "keywords": ["Large Language Models", "User engagement", "AI communication"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.04787", "pdf": "https://arxiv.org/pdf/2508.04787.pdf", "abs": "https://arxiv.org/abs/2508.04787", "title": "Evaluating the Impact of LLM-guided Reflection on Learning Outcomes with Interactive AI-Generated Educational Podcasts", "authors": ["Vishnu Menon", "Andy Cherney", "Elizabeth B. Cloude", "Li Zhang", "Tiffany D. Do"], "categories": ["cs.HC", "cs.AI"], "comment": "Accepted to NCME Special Interest Group on AI in Measurement:\n  AIME-CON 2025 conference", "summary": "This study examined whether embedding LLM-guided reflection prompts in an\ninteractive AI-generated podcast improved learning and user experience compared\nto a version without prompts. Thirty-six undergraduates participated, and while\nlearning outcomes were similar across conditions, reflection prompts reduced\nperceived attractiveness, highlighting a call for more research on reflective\ninteractivity design.", "AI": {"tldr": "The study investigates the impact of LLM-guided reflection prompts in AI-generated podcasts on learning and user experience.", "motivation": "To explore how interactive AI elements can enhance educational outcomes and user engagement in podcast formats.", "method": "An experimental design with 36 undergraduates comparing a podcast with LLM-guided reflection prompts to one without.", "result": "Learning outcomes were similar, but reflection prompts negatively affected perceived attractiveness of the podcast.", "conclusion": "Further research is needed to improve reflective interactivity design to maintain user engagement.", "key_contributions": ["Examined the role of LLM-guided reflection prompts in education", "Provided insights on user experience in podacasts", "Indentified the need for refining reflective interactivity design"], "limitations": "The study has a small sample size and limited scope of application.", "keywords": ["LLM", "reflection prompts", "user experience", "interactive podcasts", "learning outcomes"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2508.04821", "pdf": "https://arxiv.org/pdf/2508.04821.pdf", "abs": "https://arxiv.org/abs/2508.04821", "title": "At a Glance to Your Fingertips: Enabling Direct Manipulation of Distant Objects Through SightWarp", "authors": ["Yang Liu", "Thorbj√∏rn Mikkelsen", "Zehai Liu", "Gengchen Tian", "Diako Mardanbegi", "Qiushi Zhou", "Hans Gellersen", "Ken Pfeuffer"], "categories": ["cs.HC"], "comment": "12 pages, 11 figures, The 38th Annual ACM Symposium on User Interface\n  Software and Technology (UIST '25), September 28-October 01, 2025, Busan,\n  Republic of Korea", "summary": "In 3D user interfaces, reaching out to grab and manipulate something works\ngreat until it is out of reach. Indirect techniques like gaze and pinch offer\nan alternative for distant interaction, but do not provide the same immediacy\nor proprioceptive feedback as direct gestures. To support direct gestures for\nfaraway objects, we introduce SightWarp, an interaction technique that exploits\neye-hand coordination to seamlessly summon object proxies to the user's\nfingertips. The idea is that after looking at a distant object, users either\nshift their gaze to the hand or move their hand into view-triggering the\ncreation of a scaled near-space proxy of the object and its surrounding\ncontext. The proxy remains active until the eye-hand pattern is released. The\nkey benefit is that users always have an option to immediately operate on the\ndistant object through a natural, direct hand gesture. Through a user study of\na 3D object docking task, we show that users can easily employ SightWarp, and\nthat subsequent direct manipulation improves performance over gaze and pinch.\nApplication examples illustrate its utility for 6DOF manipulation,\noverview-and-detail navigation, and world-in-miniature interaction. Our work\ncontributes to expressive and flexible object interactions across near and far\nspaces.", "AI": {"tldr": "Introducing SightWarp, an interaction technique enabling direct manipulation of distant objects in 3D interfaces by using eye-hand coordination to summon object proxies.", "motivation": "To provide a solution for manipulating distant objects in 3D UIs, maintaining the inherent benefits of direct gestures.", "method": "SightWarp creates a scaled near-space proxy of a distant object and its context when users shift their gaze or hand, allowing for immediate object interaction.", "result": "User studies indicate that SightWarp improves performance in 3D object docking tasks compared to traditional gaze and pinch techniques.", "conclusion": "SightWarp enhances usability in 3D environments by allowing natural interaction with distant objects, thus supporting more expressive object manipulation.", "key_contributions": ["Introduction of the SightWarp interaction technique", "Demonstration of improved performance in 3D tasks", "Application examples for diverse manipulation scenarios"], "limitations": "", "keywords": ["3D user interfaces", "interaction technique", "eye-hand coordination"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2508.04842", "pdf": "https://arxiv.org/pdf/2508.04842.pdf", "abs": "https://arxiv.org/abs/2508.04842", "title": "Charts-of-Thought: Enhancing LLM Visualization Literacy Through Structured Data Extraction", "authors": ["Amit Kumar Das", "Mohammad Tarun", "Klaus Mueller"], "categories": ["cs.HC"], "comment": "11 pages, 8 figures. Accepted at IEEE VIS: Visualization & Visual\n  Analytics 2025 conference, November 2-7, 2025, Vienna, Austria", "summary": "This paper evaluates the visualization literacy of modern Large Language\nModels (LLMs) and introduces a novel prompting technique called\nCharts-of-Thought. We tested three state-of-the-art LLMs (Claude-3.7-sonnet,\nGPT-4.5 preview, and Gemini-2.0-pro) on the Visualization Literacy Assessment\nTest (VLAT) using standard prompts and our structured approach. The\nCharts-of-Thought method guides LLMs through a systematic data extraction,\nverification, and analysis process before answering visualization questions.\nOur results show Claude-3.7-sonnet achieved a score of 50.17 using this method,\nfar exceeding the human baseline of 28.82. This approach improved performance\nacross all models, with score increases of 21.8% for GPT-4.5, 9.4% for\nGemini-2.0, and 13.5% for Claude-3.7 compared to standard prompting. The\nperformance gains were consistent across original and modified VLAT charts,\nwith Claude correctly answering 100% of questions for several chart types that\npreviously challenged LLMs. Our study reveals that modern multimodal LLMs can\nsurpass human performance on visualization literacy tasks when given the proper\nanalytical framework. These findings establish a new benchmark for LLM\nvisualization literacy and demonstrate the importance of structured prompting\nstrategies for complex visual interpretation tasks. Beyond improving LLM\nvisualization literacy, Charts-of-Thought could also enhance the accessibility\nof visualizations, potentially benefiting individuals with visual impairments\nor lower visualization literacy.", "AI": {"tldr": "The paper evaluates Large Language Models' (LLMs) visualization literacy using a novel prompting technique called Charts-of-Thought and establishes that these models can exceed human performance in visualization tasks with structured prompting.", "motivation": "To assess and improve the visualization literacy of modern LLMs, providing a benchmark and a method for better visual interpretation.", "method": "Three LLMs (Claude-3.7-sonnet, GPT-4.5 preview, and Gemini-2.0-pro) were tested using the Visualization Literacy Assessment Test (VLAT) with standard prompts and a structured method called Charts-of-Thought.", "result": "Claude-3.7-sonnet scored 50.17‚Äîwell above the human baseline of 28.82‚Äîwith significant score increases across models using the Charts-of-Thought method.", "conclusion": "Modern multimodal LLMs, when guided by a systematic analytical framework, can outperform humans in visualization literacy tasks, establishing a new standard for evaluating and enhancing LLM capabilities.", "key_contributions": ["Introduction of the Charts-of-Thought prompting technique", "Demonstration of LLMs surpassing human visualization literacy benchmarks", "Implications for improving accessibility of visualizations for individuals with lower literacy"], "limitations": "", "keywords": ["Large Language Models", "Visualization Literacy", "Charts-of-Thought", "Human-Computer Interaction", "Accessibility"], "importance_score": 9, "read_time_minutes": 11}}
{"id": "2508.04795", "pdf": "https://arxiv.org/pdf/2508.04795.pdf", "abs": "https://arxiv.org/abs/2508.04795", "title": "Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a Frozen LLM", "authors": ["Thomas Thebaud", "Yen-Ju Lu", "Matthew Wiesner", "Peter Viechnicki", "Najim Dehak"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted in the 2025 IEEE Automatic Speech Recognition and\n  Understanding Workshop", "summary": "In dialogue transcription pipelines, Large Language Models (LLMs) are\nfrequently employed in post-processing to improve grammar, punctuation, and\nreadability. We explore a complementary post-processing step: enriching\ntranscribed dialogues by adding metadata tags for speaker characteristics such\nas age, gender, and emotion. Some of the tags are global to the entire\ndialogue, while some are time-variant. Our approach couples frozen audio\nfoundation models, such as Whisper or WavLM, with a frozen LLAMA language model\nto infer these speaker attributes, without requiring task-specific fine-tuning\nof either model. Using lightweight, efficient connectors to bridge audio and\nlanguage representations, we achieve competitive performance on speaker\nprofiling tasks while preserving modularity and speed. Additionally, we\ndemonstrate that a frozen LLAMA model can compare x-vectors directly, achieving\nan Equal Error Rate of 8.8% in some scenarios.", "AI": {"tldr": "This paper introduces a novel post-processing step for dialogue transcription that incorporates metadata tagging for speaker characteristics using combined audio and language models without fine-tuning.", "motivation": "The aim is to enhance transcription outputs by enriching them with metadata related to speaker characteristics such as age, gender, and emotion, thereby improving the utility of the transcriptions in various applications.", "method": "The study utilizes frozen audio foundation models (like Whisper or WavLM) in conjunction with a frozen LLAMA language model for the task of speaker profiling, using efficient connectors to integrate audio and language representations without task-specific fine-tuning.", "result": "The approach delivers competitive performance on speaker profiling tasks and demonstrates the capability of the frozen LLAMA model to compare x-vectors with an Equal Error Rate of 8.8% in certain scenarios.", "conclusion": "The presented method shows promise in enhancing transcription quality and allowing for detailed speaker characterization while maintaining speed and modularity.", "key_contributions": ["Introduction of a new metadata tagging method for transcribed dialogues", "Combination of audio foundation models with language models without fine-tuning", "Achieved competitive performance in speaker profiling tasks with an EER of 8.8%."], "limitations": "", "keywords": ["dialogue transcription", "speaker profiling", "metadata tagging", "Large Language Models", "frozen models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.04859", "pdf": "https://arxiv.org/pdf/2508.04859.pdf", "abs": "https://arxiv.org/abs/2508.04859", "title": "An Implementation of a Visual Stepper in the GRASP Programming System", "authors": ["Panicz Maciej Godek"], "categories": ["cs.HC", "68", "H.5.2"], "comment": "Scheme Workshop 2024 (ICFP), 23 pages", "summary": "The direct purpose of this paper - as its title suggests - is to present how\nthe visual evaluator extension is implemented in the GRASP programming system.\nThe indirect purpose is to provide a tutorial around the design of GRASP, and\nin particular - around the architecture of its extension mechanism. Neither\nGRASP nor its extension mechanisms are, at the moment of writing this paper,\nfinal or complete, and we are certain that some details of the solutions\ndescribed in here will change even before the first release. What will not\nchange, though, is the set of problems that need to be solved in order to build\na system with capabilities similar to those of GRASP. We believe that these\nproblems might be of interest to the Scheme community.", "AI": {"tldr": "This paper discusses the implementation of the visual evaluator extension in the GRASP programming system and provides a design tutorial around GRASP's architecture and extension mechanisms.", "motivation": "To present the implementation of the visual evaluator in GRASP and to offer insights into the design and architecture of its extension mechanism, addressing unresolved challenges in system capabilities.", "method": "The paper outlines the design and architectural considerations of GRASP, detailing how the visual evaluator extension is integrated into the system.", "result": "The details of the visual evaluator's implementation and the design principles of GRASP are shared, highlighting the ongoing development and the type of problems faced in achieving desired system capabilities.", "conclusion": "While the implementation details are not final, the identified problems in building a system like GRASP are relevant to the Scheme community and may influence future developments.", "key_contributions": ["Implementation details of the visual evaluator extension in GRASP", "Tutorial on GRASP's architecture and extension mechanisms", "Identification of key problems for building similar systems"], "limitations": "Details may change before final release; the system is not complete as of the writing.", "keywords": ["GRASP", "visual evaluator", "Scheme", "programming system", "extension mechanisms"], "importance_score": 2, "read_time_minutes": 20}}
{"id": "2508.04796", "pdf": "https://arxiv.org/pdf/2508.04796.pdf", "abs": "https://arxiv.org/abs/2508.04796", "title": "Parity-Aware Byte-Pair Encoding: Improving Cross-lingual Fairness in Tokenization", "authors": ["Negar Foroutan", "Clara Meister", "Debjit Paul", "Joel Niklaus", "Sina Ahmadi", "Antoine Bosselut", "Rico Sennrich"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Tokenization is the first -- and often least scrutinized -- step of most NLP\npipelines. Standard algorithms for learning tokenizers rely on frequency-based\nobjectives, which favor languages dominant in the training data and\nconsequently leave lower-resource languages with tokenizations that are\ndisproportionately longer, morphologically implausible, or even riddled with\n<UNK> placeholders. This phenomenon ultimately amplifies computational and\nfinancial inequalities between users from different language backgrounds. To\nremedy this, we introduce Parity-aware Byte Pair Encoding (BPE), a variant of\nthe widely-used BPE algorithm. At every merge step, Parity-aware BPE maximizes\nthe compression gain of the currently worst-compressed language, trading a\nsmall amount of global compression for cross-lingual parity. We find\nempirically that Parity-aware BPE leads to more equitable token counts across\nlanguages, with negligible impact on global compression rate and no substantial\neffect on language-model performance in downstream tasks.", "AI": {"tldr": "Parity-aware BPE addresses the inequities in tokenization across languages in NLP by focusing on better token counts for lower-resource languages.", "motivation": "To mitigate the computational and financial inequalities faced by users from different language backgrounds due to suboptimal tokenization in lower-resource languages.", "method": "Introducing Parity-aware Byte Pair Encoding (BPE), which prioritizes token compression for the worst-compressed language at each merge step while maintaining overall compression rates.", "result": "Parity-aware BPE improves token counts for lower-resource languages with a minimal impact on global compression and language-model performance.", "conclusion": "The method enhances equity in tokenization without significantly compromising performance, thus promoting fairer NLP applications across diverse languages.", "key_contributions": ["Development of Parity-aware BPE for equitable tokenization", "Empirical evidence showing improved token counts for lower-resource languages", "Balancing compression gains with language-model performance"], "limitations": "", "keywords": ["Tokenization", "NLP", "Byte Pair Encoding", "Cross-lingual", "Equity"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.04902", "pdf": "https://arxiv.org/pdf/2508.04902.pdf", "abs": "https://arxiv.org/abs/2508.04902", "title": "Learning AI Auditing: A Case Study of Teenagers Auditing a Generative AI Model", "authors": ["Luis Morales-Navarro", "Michelle Gan", "Evelyn Yu", "Lauren Vogelstein", "Yasmin B. Kafai", "Dana√© Metaxa"], "categories": ["cs.HC", "cs.CY", "H.5.0; K.3.2"], "comment": null, "summary": "This study investigates how high school-aged youth engage in algorithm\nauditing to identify and understand biases in artificial intelligence and\nmachine learning (AI/ML) tools they encounter daily. With AI/ML technologies\nbeing increasingly integrated into young people's lives, there is an urgent\nneed to equip teenagers with AI literacies that build both technical knowledge\nand awareness of social impacts. Algorithm audits (also called AI audits) have\ntraditionally been employed by experts to assess potential harmful biases, but\nrecent research suggests that non-expert users can also participate\nproductively in auditing. We conducted a two-week participatory design workshop\nwith 14 teenagers (ages 14-15), where they audited the generative AI model\nbehind TikTok's Effect House, a tool for creating interactive TikTok filters.\nWe present a case study describing how teenagers approached the audit, from\ndeciding what to audit to analyzing data using diverse strategies and\ncommunicating their results. Our findings show that participants were engaged\nand creative throughout the activities, independently raising and exploring new\nconsiderations, such as age-related biases, that are uncommon in professional\naudits. We drew on our expertise in algorithm auditing to triangulate their\nfindings as a way to examine if the workshop supported participants to reach\ncoherent conclusions in their audit. Although the resulting number of changes\nin race, gender, and age representation uncovered by the teens were slightly\ndifferent from ours, we reached similar conclusions. This study highlights the\npotential for auditing to inspire learning activities to foster AI literacies,\nempower teenagers to critically examine AI systems, and contribute fresh\nperspectives to the study of algorithmic harms.", "AI": {"tldr": "This study explores how high school students engage in algorithm auditing to identify biases in AI tools, particularly through a workshop where they audited TikTok's generative AI model.", "motivation": "The study addresses the need for equipping youth with AI literacy skills and understanding the social impacts of AI/ML technologies they encounter in their daily lives.", "method": "Conducted a two-week participatory design workshop with 14 teenagers, guiding them to audit a generative AI tool used for creating TikTok filters.", "result": "Participants were engaged and creative, uncovering unique biases such as age-related issues, and their findings aligned with professional audits in some aspects.", "conclusion": "The research emphasizes the importance of algorithm auditing in fostering AI literacy among teenagers and encouraging them to critically analyze AI systems.", "key_contributions": ["Demonstrates non-expert participation in AI auditing", "Highlights unique perspectives on biases from youth", "Shows the potential of workshops to enhance AI literacy"], "limitations": "", "keywords": ["Algorithm Auditing", "AI Literacy", "Youth Engagement", "Biases in AI", "Participatory Design"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.04814", "pdf": "https://arxiv.org/pdf/2508.04814.pdf", "abs": "https://arxiv.org/abs/2508.04814", "title": "Pitch Accent Detection improves Pretrained Automatic Speech Recognition", "authors": ["David Sasu", "Natalie Schluter"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "We show the performance of Automatic Speech Recognition (ASR) systems that\nuse semi-supervised speech representations can be boosted by a complimentary\npitch accent detection module, by introducing a joint ASR and pitch accent\ndetection model. The pitch accent detection component of our model achieves a\nsignificant improvement on the state-of-the-art for the task, closing the gap\nin F1-score by 41%. Additionally, the ASR performance in joint training\ndecreases WER by 28.3% on LibriSpeech, under limited resource fine-tuning. With\nthese results, we show the importance of extending pretrained speech models to\nretain or re-learn important prosodic cues such as pitch accent.", "AI": {"tldr": "This paper presents a joint model for Automatic Speech Recognition (ASR) and pitch accent detection, showing significant improvements in both tasks.", "motivation": "The research aims to improve ASR systems by incorporating pitch accent detection, addressing limitations in current models that overlook prosodic features.", "method": "A joint ASR and pitch accent detection model is developed, leveraging semi-supervised speech representations alongside a new component for pitch accent detection.", "result": "The joint model achieves a 41% improvement in F1-score for pitch accent detection and reduces the Word Error Rate (WER) by 28.3% on the LibriSpeech dataset during fine-tuning.", "conclusion": "The findings suggest that integrating prosodic cues like pitch accent in training enhances the performance of ASR systems, emphasizing the value of extending pretrained models.", "key_contributions": ["Development of a joint ASR and pitch accent detection model", "Significant improvement in pitch accent detection accuracy", "Reduction in ASR WER on the LibriSpeech dataset"], "limitations": "", "keywords": ["Automatic Speech Recognition", "pitch accent detection", "semi-supervised learning", "prosodic cues", "LibriSpeech"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.04904", "pdf": "https://arxiv.org/pdf/2508.04904.pdf", "abs": "https://arxiv.org/abs/2508.04904", "title": "Root Cause Analysis Training for Healthcare Professionals With AI-Powered Virtual Simulation: A Proof-of-Concept", "authors": ["Yuqi Hu", "Qiwen Xiong", "Zhenzhen Qin", "Brandon Watanabe", "Yujing Wang", "Mirjana Prpa", "Ilmi Yoon"], "categories": ["cs.HC"], "comment": null, "summary": "Root Cause Analysis (RCA) is a critical tool for investigating adverse events\nin healthcare and improving patient safety. However, existing RCA training\nprograms are often limited by high resource demands, leading to insufficient\ntraining and inconsistent implementation. To address this challenge, we present\nan AI-powered 3D simulation game that helps healthcare professionals develop\nRCA skills through interactive, immersive simulations. This approach offers a\ncost-effective, scalable, and accessible alternative to traditional training.\nThe prototype simulates an RCA investigation following a death in the ICU,\nwhere learners interview five virtual avatars representing ICU team members to\ninvestigate the incident and complete a written report. The system enables\nnatural, life-like interactions with avatars via large language models (LLMs),\nemotional text-to-speech, and AI-powered animations. An additional LLM\ncomponent provides formative and summative feedback to support continual\nimprovement. We conclude by outlining plans to empirically evaluate the\nsystem's efficacy.", "AI": {"tldr": "An AI-based 3D simulation game is developed to enhance Root Cause Analysis (RCA) training for healthcare professionals, providing an interactive and immersive learning experience.", "motivation": "Existing RCA training programs have high resource demands, leading to inadequate training and inconsistent implementation in healthcare settings.", "method": "The paper presents a prototype of a 3D simulation game that allows users to conduct RCA investigations in a simulated ICU environment by interacting with virtual avatars using LLMs and other AI technologies.", "result": "The prototype facilitates life-like interactions and supports formative feedback, improving the learning process for healthcare professionals.", "conclusion": "Plans are discussed for empirical evaluation of the efficacy of the AI-powered simulation training system.", "key_contributions": ["Development of an AI-powered 3D simulation game for RCA training", "Integration of large language models for realistic interactions", "Cost-effective and scalable alternative to traditional RCA training"], "limitations": "", "keywords": ["Root Cause Analysis", "3D simulation", "healthcare training", "large language models", "immersive learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.04826", "pdf": "https://arxiv.org/pdf/2508.04826.pdf", "abs": "https://arxiv.org/abs/2508.04826", "title": "Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History", "authors": ["Tommaso Tosato", "Saskia Helbling", "Yorguin-Jose Mantilla-Ramos", "Mahmood Hegazy", "Alberto Tosato", "David John Lemay", "Irina Rish", "Guillaume Dumas"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models require consistent behavioral patterns for safe\ndeployment, yet their personality-like traits remain poorly understood. We\npresent PERSIST (PERsonality Stability in Synthetic Text), a comprehensive\nevaluation framework testing 25+ open-source models (1B-671B parameters) across\n500,000+ responses. Using traditional (BFI-44, SD3) and novel LLM-adapted\npersonality instruments, we systematically vary question order, paraphrasing,\npersonas, and reasoning modes. Our findings challenge fundamental deployment\nassumptions: (1) Even 400B+ models exhibit substantial response variability (SD\n> 0.4); (2) Minor prompt reordering alone shifts personality measurements by up\nto 20%; (3) Interventions expected to stabilize behavior, such as\nchain-of-thought reasoning, detailed personas instruction, inclusion of\nconversation history, can paradoxically increase variability; (4) LLM-adapted\ninstruments show equal instability to human-centric versions, confirming\narchitectural rather than translational limitations. This persistent\ninstability across scales and mitigation strategies suggests current LLMs lack\nthe foundations for genuine behavioral consistency. For safety-critical\napplications requiring predictable behavior, these findings indicate that\npersonality-based alignment strategies may be fundamentally inadequate.", "AI": {"tldr": "This paper introduces PERSIST, a framework for evaluating the behavioral consistency of large language models, revealing substantial response variability and challenges in personality alignment for safe deployment.", "motivation": "To address the poorly understood personality traits of large language models and their implications for safe and consistent deployment in critical applications.", "method": "Evaluates 25+ open-source models using traditional and LLM-adapted personality instruments, varying question order, paraphrasing, personas, and reasoning modes across over 500,000 responses.", "result": "Findings indicate that even large models show significant response variability, prompt reordering affects personality measurements substantially, and common stabilization strategies can increase variability.", "conclusion": "The instability in responses across model scales and requested stabilization strategies implies current LLMs are not equipped for consistent behavioral performance, questioning the efficacy of personality-alignment strategies for safety-critical applications.", "key_contributions": ["Introduction of the PERSIST evaluation framework", "Empirical analysis of personality stability across 25+ large language models", "Insights on the inadequacy of personality-based alignment strategies for safety-critical applications."], "limitations": "Focuses on open-source models; findings may not generalize to proprietary models; does not explore long-term stability in deployments.", "keywords": ["Personality Stability", "Large Language Models", "Evaluation Framework"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.04920", "pdf": "https://arxiv.org/pdf/2508.04920.pdf", "abs": "https://arxiv.org/abs/2508.04920", "title": "Toward Supporting Narrative-Driven Data Exploration: Barriers and Design Opportunities", "authors": ["Oliver Huang", "Carolina Nobre"], "categories": ["cs.HC"], "comment": "VIS 2025 Poster Summary", "summary": "Analysts increasingly explore data through evolving, narrative-driven\ninquiries, moving beyond static dashboards and predefined metrics as their\nquestions deepen and shift. As these explorations progress, insights often\nbecome dispersed across views, making it challenging to maintain context or\nclarify how conclusions arise. Through a formative study with 48 participants,\nwe identify key barriers that hinder narrative-driven exploration, including\ndifficulty maintaining context across views, tracing reasoning paths, and\nexternalizing evolving interpretations. Our findings surface design\nopportunities to support narrative-driven analysis better.", "AI": {"tldr": "This paper explores challenges in narrative-driven data exploration and proposes design opportunities to improve support for such analyses.", "motivation": "The motivation for this study stems from the need to enhance narrative-driven inquiries in data exploration, which are becoming increasingly common among analysts.", "method": "The research involved a formative study with 48 participants to identify barriers in narrative-driven exploration.", "result": "Key barriers identified include difficulties in maintaining context across views, tracing reasoning paths, and externalizing evolving interpretations.", "conclusion": "The findings highlight design opportunities to better support narrative-driven analysis.", "key_contributions": ["Identification of key barriers in narrative-driven data exploration", "Insights into maintaining context and tracing reasoning in data analysis", "Proposed design opportunities for improving narrative analysis tools"], "limitations": "", "keywords": ["Narrative-driven exploration", "Data analysis", "Design opportunities"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2508.04903", "pdf": "https://arxiv.org/pdf/2508.04903.pdf", "abs": "https://arxiv.org/abs/2508.04903", "title": "RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM Systems with Structured Memory", "authors": ["Jun Liu", "Zhenglun Kong", "Changdi Yang", "Fan Yang", "Tianqi Li", "Peiyan Dong", "Joannah Nanjekye", "Hao Tang", "Geng Yuan", "Wei Niu", "Wenbin Zhang", "Pu Zhao", "Xue Lin", "Dong Huang", "Yanzhi Wang"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "Multi-agent large language model (LLM) systems have shown strong potential in\ncomplex reasoning and collaborative decision-making tasks. However, most\nexisting coordination schemes rely on static or full-context routing\nstrategies, which lead to excessive token consumption, redundant memory\nexposure, and limited adaptability across interaction rounds. We introduce\nRCR-Router, a modular and role-aware context routing framework designed to\nenable efficient, adaptive collaboration in multi-agent LLMs. To our knowledge,\nthis is the first routing approach that dynamically selects semantically\nrelevant memory subsets for each agent based on its role and task stage, while\nadhering to a strict token budget. A lightweight scoring policy guides memory\nselection, and agent outputs are iteratively integrated into a shared memory\nstore to facilitate progressive context refinement. To better evaluate model\nbehavior, we further propose an Answer Quality Score metric that captures\nLLM-generated explanations beyond standard QA accuracy. Experiments on three\nmulti-hop QA benchmarks -- HotPotQA, MuSiQue, and 2WikiMultihop -- demonstrate\nthat RCR-Router reduces token usage (up to 30%) while improving or maintaining\nanswer quality. These results highlight the importance of structured memory\nrouting and output-aware evaluation in advancing scalable multi-agent LLM\nsystems.", "AI": {"tldr": "RCR-Router is a dynamic context routing framework for multi-agent LLMs that improves efficiency by intelligently selecting relevant memory subsets based on agent roles and tasks.", "motivation": "Existing coordination methods for multi-agent LLMs lead to high token consumption and poor adaptability. RCR-Router aims to address these issues.", "method": "The framework uses a scoring policy to select relevant memory subsets dynamically for each agent based on its role and task stage, integrating agent outputs into a shared memory for context refinement.", "result": "Experiments showed RCR-Router reduces token usage by up to 30% while maintaining or improving answer quality on multi-hop QA benchmarks.", "conclusion": "Structured memory routing and output-aware evaluation are critical for enhancing scalable performance in multi-agent LLM systems.", "key_contributions": ["Introduction of a novel role-aware context routing framework (RCR-Router) for multi-agent LLMs", "Dynamic memory selection based on agent roles and stages", "Development of the Answer Quality Score metric for evaluating LLM outputs"], "limitations": "", "keywords": ["multi-agent LLMs", "context routing", "memory selection", "collaboration", "QA benchmarks"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.04995", "pdf": "https://arxiv.org/pdf/2508.04995.pdf", "abs": "https://arxiv.org/abs/2508.04995", "title": "Situated Epistemic Infrastructures: A Diagnostic Framework for Post-Coherence Knowledge", "authors": ["Matthew Kelly"], "categories": ["cs.HC", "cs.AI", "cs.DL", "K.4.1; K.3; K.2"], "comment": "27 pages including references. Draft prepared for submission to\n  Science, Technology & Human Values", "summary": "Large Language Models (LLMs) such as ChatGPT have rendered visible the\nfragility of contemporary knowledge infrastructures by simulating coherence\nwhile bypassing traditional modes of citation, authority, and validation. This\npaper introduces the Situated Epistemic Infrastructures (SEI) framework as a\ndiagnostic tool for analyzing how knowledge becomes authoritative across hybrid\nhuman-machine systems under post-coherence conditions. Rather than relying on\nstable scholarly domains or bounded communities of practice, SEI traces how\ncredibility is mediated across institutional, computational, and temporal\narrangements. Integrating insights from infrastructure studies, platform\ntheory, and epistemology, the framework foregrounds coordination over\nclassification, emphasizing the need for anticipatory and adaptive models of\nepistemic stewardship. The paper contributes to debates on AI governance,\nknowledge production, and the ethical design of information systems by offering\na robust alternative to representationalist models of scholarly communication.", "AI": {"tldr": "The paper introduces the Situated Epistemic Infrastructures (SEI) framework to analyze how knowledge is validated in hybrid human-machine systems, focusing on coordination over classification in knowledge production.", "motivation": "To address the fragility of current knowledge infrastructures exposed by Large Language Models and to propose a framework for understanding how authority is established in hybrid systems.", "method": "The Situated Epistemic Infrastructures (SEI) framework is presented as a diagnostic tool that integrates insights from various fields to analyze the mediation of credibility in knowledge production.", "result": "The SEI framework emphasizes the importance of coordination in knowledge validation rather than strict classification, which can enhance our understanding of AI governance and the design of information systems.", "conclusion": "The framework offers a novel approach to understanding the complexities of epistemic stewardship in the age of AI, positioning itself as a robust alternative to traditional models of scholarly communication.", "key_contributions": ["Introduction of the SEI framework for hybrid human-machine systems.", "Focus on coordination over classification in knowledge validation.", "Contribution to AI governance and ethical design discussions."], "limitations": "", "keywords": ["Large Language Models", "knowledge production", "AI governance", "epistemology", "infrastructure studies"], "importance_score": 8, "read_time_minutes": 27}}
{"id": "2508.04939", "pdf": "https://arxiv.org/pdf/2508.04939.pdf", "abs": "https://arxiv.org/abs/2508.04939", "title": "I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating Linguistic Shibboleth Detection in LLM Hiring Evaluations", "authors": ["Julia Kharchenko", "Tanya Roosta", "Aman Chadha", "Chirag Shah"], "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces a comprehensive benchmark for evaluating how Large\nLanguage Models (LLMs) respond to linguistic shibboleths: subtle linguistic\nmarkers that can inadvertently reveal demographic attributes such as gender,\nsocial class, or regional background. Through carefully constructed interview\nsimulations using 100 validated question-response pairs, we demonstrate how\nLLMs systematically penalize certain linguistic patterns, particularly hedging\nlanguage, despite equivalent content quality. Our benchmark generates\ncontrolled linguistic variations that isolate specific phenomena while\nmaintaining semantic equivalence, which enables the precise measurement of\ndemographic bias in automated evaluation systems. We validate our approach\nalong multiple linguistic dimensions, showing that hedged responses receive\n25.6% lower ratings on average, and demonstrate the benchmark's effectiveness\nin identifying model-specific biases. This work establishes a foundational\nframework for detecting and measuring linguistic discrimination in AI systems,\nwith broad applications to fairness in automated decision-making contexts.", "AI": {"tldr": "This paper presents a benchmark for evaluating Large Language Models' responses to subtle linguistic markers that can indicate demographic attributes, revealing biases in automated evaluation systems.", "motivation": "To address and measure demographic bias present in AI systems, particularly in Large Language Models, by using linguistic shibboleths as markers.", "method": "The paper utilizes a benchmark created through interview simulations with 100 validated question-response pairs, allowing for controlled linguistic variations while maintaining content quality.", "result": "Hedged responses from LLMs received, on average, 25.6% lower ratings, indicating systematic penalties for certain linguistic patterns.", "conclusion": "The benchmark establishes a foundation for detecting linguistic discrimination in AI systems, highlighting its importance for fairness in automated decision-making.", "key_contributions": ["Introduces a benchmark for evaluating linguistic bias in LLMs.", "Demonstrates systematic penalties for hedging language in LLM responses.", "Validates the effectiveness of the benchmark in identifying model-specific biases."], "limitations": "", "keywords": ["Large Language Models", "linguistic bias", "demographic attributes", "fairness", "automated evaluation systems"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.05045", "pdf": "https://arxiv.org/pdf/2508.05045.pdf", "abs": "https://arxiv.org/abs/2508.05045", "title": "Human-AI Schema Discovery and Application for Creative Problem Solving", "authors": ["Sitong Wang"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Humans often rely on underlying structural patterns-schemas-to create,\nwhether by writing stories, designing software, or composing music. Schemas\nhelp organize ideas and guide exploration, but they are often difficult to\ndiscover and apply, especially in complex or unfamiliar domains. My Ph.D.\nresearch develops a framework for human-AI schema discovery and application to\nsupport creative problem solving. I design systems that support users in\nsensemaking over examples to abstract schemas, and in operationalizing schemas\ninto human-AI co-creative workflows for application. This research offers\ninsights into how schema-guided interaction can make implicit knowledge more\naccessible and actionable, advancing more transparent and collaborative\nhuman-AI systems.", "AI": {"tldr": "The research develops a framework for human-AI schema discovery to enhance creative problem solving and collaboration.", "motivation": "To assist users in recognizing and applying schemas for creative tasks, improving the accessibility of implicit knowledge in complex domains.", "method": "Designing systems that facilitate sensemaking of examples and integrating schemas into workflows for human-AI co-creation.", "result": "The developed framework supports users in discovering and operationalizing schemas, potentially leading to more effective human-AI collaborations.", "conclusion": "Schema-guided interaction can enhance creativity and collaboration between humans and AI by making implicit knowledge more accessible.", "key_contributions": ["Framework for human-AI schema discovery", "Support for sensemaking over examples", "Operationalization of schemas in co-creative workflows"], "limitations": "", "keywords": ["human-AI collaboration", "schema discovery", "creative problem solving"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.04945", "pdf": "https://arxiv.org/pdf/2508.04945.pdf", "abs": "https://arxiv.org/abs/2508.04945", "title": "Towards Robust Evaluation of Visual Activity Recognition: Resolving Verb Ambiguity with Sense Clustering", "authors": ["Louie Hong Yao", "Nicholas Jarvis", "Tianyu Jiang"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "18 pages, 5 figures", "summary": "Evaluating visual activity recognition systems is challenging due to inherent\nambiguities in verb semantics and image interpretation. When describing actions\nin images, synonymous verbs can refer to the same event (e.g., brushing vs.\ngrooming), while different perspectives can lead to equally valid but distinct\nverb choices (e.g., piloting vs. operating). Standard exact-match evaluation,\nwhich relies on a single gold answer, fails to capture these ambiguities,\nresulting in an incomplete assessment of model performance. To address this, we\npropose a vision-language clustering framework that constructs verb sense\nclusters, providing a more robust evaluation. Our analysis of the imSitu\ndataset shows that each image maps to an average of 2.8 sense clusters, with\neach cluster representing a distinct perspective of the image. We evaluate\nmultiple activity recognition models and compare our cluster-based evaluation\nwith standard evaluation methods. Additionally, our human alignment analysis\nsuggests that the cluster-based evaluation better aligns with human judgements,\noffering a more nuanced assessment of model performance.", "AI": {"tldr": "Proposes a vision-language clustering framework to improve evaluation of visual activity recognition systems by addressing verb semantic ambiguities.", "motivation": "Current evaluation methods fail to capture the nuances of verb semantics and image interpretations in activity recognition, leading to incomplete assessments.", "method": "Developing a clustering framework that groups synonymous verbs and different perspectives, allowing for a more robust evaluation of models.", "result": "Analysis of the imSitu dataset reveals each image corresponds to an average of 2.8 sense clusters, indicating multiple valid interpretations per image. The new cluster-based evaluation method aligns better with human judgments compared to standard methods.", "conclusion": "The proposed method provides a more nuanced and complete assessment of activity recognition model performance, reflecting human interpretation more accurately.", "key_contributions": ["Introduces a vision-language clustering framework for verb sense clustering", "Demonstrates improved evaluation metrics for activity recognition", "Shows that cluster-based evaluations align better with human judgments"], "limitations": "", "keywords": ["visual activity recognition", "evaluation methods", "verb semantics", "clustering framework", "human judgments"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2508.05056", "pdf": "https://arxiv.org/pdf/2508.05056.pdf", "abs": "https://arxiv.org/abs/2508.05056", "title": "Accessibility Beyond Accommodations: A Systematic Redesign of Introduction to Computer Science for Students with Visual Impairments", "authors": ["Vaanee Tripathi", "Aalok Thakkar"], "categories": ["cs.HC"], "comment": null, "summary": "Computer science education has evolved extensively; however, systemic\nbarriers still prevent students with visual impairments from fully\nparticipating. While existing research has developed specialized programming\ntools and assistive technologies, these solutions remain fragmented and often\nrequire complex technical infrastructure, which limits their classroom\nimplementation. Current approaches treat accessibility as individual\naccommodations rather than integral curriculum design, creating gaps in\nholistic educational support. This paper presents a comprehensive framework for\nredesigning introductory computer science curricula to provide equitable\nlearning experiences for students with visual impairments without requiring\nspecialized technical infrastructure. The framework outlines five key\ncomponents that together contribute a systematic approach to curriculum\naccessibility: accessible learning resources with pre-distributed materials and\ntactile diagrams, in-class learning kits with hands-on demonstrations,\nstructured support systems with dedicated teaching assistance, an online tool\nrepository, and psychosocial support for classroom participation. Unlike\nexisting tool-focused solutions, this framework addresses both technical and\npedagogical dimensions of inclusive education while emphasizing practical\nimplementation in standard university settings. The design is grounded in\nuniversal design principles and validated through expert consultation with\naccessibility specialists and disability services professionals, establishing\nfoundations for future empirical evaluation of learning outcomes and student\nengagement while serving as a template for broader institutional adoption.", "AI": {"tldr": "The paper presents a framework for redesigning introductory computer science curricula to enhance accessibility for students with visual impairments.", "motivation": "Address systemic barriers preventing students with visual impairments from fully participating in computer science education.", "method": "The framework comprises five components: accessible learning resources, in-class learning kits, structured support systems, an online tool repository, and psychosocial support.", "result": "The framework offers a systematic approach to accessibility in education that does not rely on complex technical infrastructure.", "conclusion": "This comprehensive framework aims to integrate accessibility into curriculum design, emphasizing practical implementation for equitable learning experiences.", "key_contributions": ["Comprehensive framework for curriculum redesign", "Integration of technical and pedagogical dimensions", "Grounded in universal design principles"], "limitations": "Limited to introductory computer science curricula; empirical evaluation of learning outcomes still required.", "keywords": ["accessibility", "computer science education", "visual impairments", "inclusive education", "universal design"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.05003", "pdf": "https://arxiv.org/pdf/2508.05003.pdf", "abs": "https://arxiv.org/abs/2508.05003", "title": "A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health", "authors": ["Song Wang", "Yishu Wei", "Haotian Ma", "Max Lovitt", "Kelly Deng", "Yuan Meng", "Zihan Xu", "Jingze Zhang", "Yunyu Xiao", "Ying Ding", "Xuhai Xu", "Joydeep Ghosh", "Yifan Peng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Background: Understanding social determinants of health (SDoH) factors\ncontributing to suicide incidents is crucial for early intervention and\nprevention. However, data-driven approaches to this goal face challenges such\nas long-tailed factor distributions, analyzing pivotal stressors preceding\nsuicide incidents, and limited model explainability. Methods: We present a\nmulti-stage large language model framework to enhance SDoH factor extraction\nfrom unstructured text. Our approach was compared to other state-of-the-art\nlanguage models (i.e., pre-trained BioBERT and GPT-3.5-turbo) and reasoning\nmodels (i.e., DeepSeek-R1). We also evaluated how the model's explanations help\npeople annotate SDoH factors more quickly and accurately. The analysis included\nboth automated comparisons and a pilot user study. Results: We show that our\nproposed framework demonstrated performance boosts in the overarching task of\nextracting SDoH factors and in the finer-grained tasks of retrieving relevant\ncontext. Additionally, we show that fine-tuning a smaller, task-specific model\nachieves comparable or better performance with reduced inference costs. The\nmulti-stage design not only enhances extraction but also provides intermediate\nexplanations, improving model explainability. Conclusions: Our approach\nimproves both the accuracy and transparency of extracting suicide-related SDoH\nfrom unstructured texts. These advancements have the potential to support early\nidentification of individuals at risk and inform more effective prevention\nstrategies.", "AI": {"tldr": "A multi-stage large language model framework enhances the extraction of social determinants of health related to suicide incidents from unstructured text, improving accuracy and explainability.", "motivation": "Understanding social determinants of health factors is crucial for early intervention and prevention of suicide incidents, but data-driven approaches face challenges.", "method": "A multi-stage large language model framework was developed, compared to state-of-the-art models like BioBERT and GPT-3.5-turbo, and evaluated through automated comparisons and a pilot user study.", "result": "The framework showed improved performance in extracting SDoH factors, with fine-tuning a smaller model achieving comparable results at lower costs. It also enhanced explainability by providing intermediate explanations.", "conclusion": "The approach not only improves accuracy but also transparency in extracting suicide-related SDoH, supporting early risk identification and effective prevention strategies.", "key_contributions": ["Multi-stage framework for SDoH extraction", "Comparison with state-of-the-art language models", "Improved explainability through intermediate models"], "limitations": "", "keywords": ["social determinants of health", "suicide prevention", "large language models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.05088", "pdf": "https://arxiv.org/pdf/2508.05088.pdf", "abs": "https://arxiv.org/abs/2508.05088", "title": "A Desktop-Centric Design Space for Direct Object Examination and Visualization in Mixed-Reality Environments", "authors": ["Sam Johnson-Lacoss", "Santiago V. Lombeyda", "S. George Djorgovski"], "categories": ["cs.HC"], "comment": null, "summary": "Mixed reality (MR) environments are bound to become ubiquitous as MR\ntechnology becomes lighter, higher resolution, more affordable, and overall\nbecomes a seamless extension of our current work and living spaces. For\nresearch scientists and clinicians focused on understanding 3D phenomena or\npatient pathologies within the context of the larger human anatomy, that means\na necessary evolution of their workstations currently only utilizing 2D\ninterfaces for everyday communication, logistics and data analysis. MR\ntechnologies bring forth immersive 3D representations coexisting in our natural\nspaces, while allowing for richer interconnected information displays, where 3D\nrepresentations greatly aid in the detailed understanding of physical\nstructures, spatial relationships, and 3D contextualization of 2D measurements,\nprojections, abstractions, and other data details. We present a breakdown of\nthe different interaction zones and modalities into a design space that best\naccommodates the creation of applications for users engaged through MR\ntechnologies in precise object-centric data analysis within the ergonomic\nconfines of their desktop physical spaces.", "AI": {"tldr": "The paper discusses the impact of mixed reality (MR) on data analysis and communication in research and clinical settings, proposing a design framework for MR applications.", "motivation": "To explore how MR technologies can enhance 3D data analysis and workspace ergonomics for clinicians and researchers currently limited by 2D interfaces.", "method": "The authors create a design space breakdown, categorizing different interaction zones and modalities for MR technology application.", "result": "Mixed reality applications can significantly improve the understanding of complex 3D phenomena by providing immersive 3D representations of data within natural spaces.", "conclusion": "The integration of MR technologies into research and clinical workflows can lead to better data analysis and communication through enhanced spatial contextualization.", "key_contributions": ["Development of a design framework for MR applications", "Identification of interaction zones and modalities", "Enhancement of object-centric data analysis capabilities"], "limitations": "Potential challenges in ergonomic integration and user adaptation to MR environments.", "keywords": ["mixed reality", "3D data analysis", "human-computer interaction", "design framework", "immersive technology"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.05023", "pdf": "https://arxiv.org/pdf/2508.05023.pdf", "abs": "https://arxiv.org/abs/2508.05023", "title": "Dialogues Aspect-based Sentiment Quadruple Extraction via Structural Entropy Minimization Partitioning", "authors": ["Kun Peng", "Cong Cao", "Hao Peng", "Zhifeng Hao", "Lei Jiang", "Kongjing Gu", "Yanbing Liu", "Philip S. Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by CIKM2025", "summary": "Dialogues Aspect-based Sentiment Quadruple Extraction (DiaASQ) aims to\nextract all target-aspect-opinion-sentiment quadruples from a given\nmulti-round, multi-participant dialogue. Existing methods typically learn word\nrelations across entire dialogues, assuming a uniform distribution of sentiment\nelements. However, we find that dialogues often contain multiple semantically\nindependent sub-dialogues without clear dependencies between them. Therefore,\nlearning word relationships across the entire dialogue inevitably introduces\nadditional noise into the extraction process. To address this, our method\nfocuses on partitioning dialogues into semantically independent sub-dialogues.\nAchieving completeness while minimizing these sub-dialogues presents a\nsignificant challenge. Simply partitioning based on reply relationships is\nineffective. Instead, we propose utilizing a structural entropy minimization\nalgorithm to partition the dialogues. This approach aims to preserve relevant\nutterances while distinguishing irrelevant ones as much as possible.\nFurthermore, we introduce a two-step framework for quadruple extraction: first\nextracting individual sentiment elements at the utterance level, then matching\nquadruples at the sub-dialogue level. Extensive experiments demonstrate that\nour approach achieves state-of-the-art performance in DiaASQ with much lower\ncomputational costs.", "AI": {"tldr": "This paper presents a method for extracting sentiment quadruples from dialogues by partitioning dialogues into semantically independent sub-dialogues, improving extraction accuracy and reducing noise.", "motivation": "Current methods for aspect-based sentiment extraction in dialogues are ineffective due to the uniform treatment of dialogue structures, which often contain independent sub-dialogues.", "method": "The proposed method utilizes a structural entropy minimization algorithm to partition dialogues into semantically independent sub-dialogues, followed by a two-step framework for quadruple extraction.", "result": "The proposed approach achieves state-of-the-art performance in Dialogue Aspect-based Sentiment Quadruple Extraction with significantly lower computational costs compared to existing methods.", "conclusion": "The research demonstrates that focused partitioning of dialogues significantly enhances the accuracy of sentiment extraction while minimizing computational issues.", "key_contributions": ["Introduction of a structural entropy minimization algorithm to dialogue partitioning", "A novel two-step framework for quadruple extraction", "Achieving state-of-the-art performance in DiaASQ with reduced computational costs"], "limitations": "", "keywords": ["aspect-based sentiment analysis", "dialogue processing", "machine learning", "natural language processing", "information extraction"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.05098", "pdf": "https://arxiv.org/pdf/2508.05098.pdf", "abs": "https://arxiv.org/abs/2508.05098", "title": "SparseEMG: Computational Design of Sparse EMG Layouts for Sensing Gestures", "authors": ["Anand Kumar", "Antony Albert Raj Irudayaraj", "Ishita Chandra", "Adwait Sharma", "Aditya Shekhar Nittala"], "categories": ["cs.HC"], "comment": "UIST'25: Proceedings of the 38th Annual ACM Symposium on User\n  Interface Software and Technology", "summary": "Gesture recognition with electromyography (EMG) is a complex problem\ninfluenced by gesture sets, electrode count and placement, and machine learning\nparameters (e.g., features, classifiers). Most existing toolkits focus on\nstreamlining model development but overlook the impact of electrode selection\non classification accuracy. In this work, we present the first data-driven\nanalysis of how electrode selection and classifier choice affect both accuracy\nand sparsity. Through a systematic evaluation of 28 combinations (4 selection\nschemes, 7 classifiers), across six datasets, we identify an approach that\nminimizes electrode count without compromising accuracy. The results show that\nPermutation Importance (selection scheme) with Random Forest (classifier)\nreduces the number of electrodes by 53.5\\%. Based on these findings, we\nintroduce SparseEMG, a design tool that generates sparse electrode layouts\nbased on user-selected gesture sets, electrode constraints, and ML parameters\nwhile also predicting classification performance. SparseEMG supports 50+ unique\ngestures and is validated in three real-world applications using different\nhardware setups. Results from our multi-dataset evaluation show that the\nlayouts generated from the SparseEMG design tool are transferable across users\nwith only minimal variation in gesture recognition performance.", "AI": {"tldr": "This paper analyzes the impact of electrode selection on gesture recognition accuracy in EMG systems and introduces SparseEMG, a tool for generating efficient electrode layouts.", "motivation": "To address the overlooked impact of electrode selection on the accuracy of gesture recognition systems using EMG.", "method": "The study evaluates 28 combinations of selection schemes and classifiers across six datasets to find an optimal balance between electrode count and classification accuracy.", "result": "The analysis reveals that using Permutation Importance with Random Forest can reduce the number of electrodes by 53.5% while maintaining accuracy, and SparseEMG generates effective layouts validated in real-world applications.", "conclusion": "SparseEMG effectively supports gesture recognition in various scenarios while minimizing the number of electrodes required for accurate performance.", "key_contributions": ["First data-driven analysis of electrode selection in EMG gesture recognition.", "Introduction of SparseEMG, a design tool for electrode layout generation.", "Demonstration of the transferability of generated layouts across users."], "limitations": "The study may have limitations regarding the generalizability of results to all possible hardware setups.", "keywords": ["EMG", "gesture recognition", "electrode selection", "machine learning", "SparseEMG"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.05028", "pdf": "https://arxiv.org/pdf/2508.05028.pdf", "abs": "https://arxiv.org/abs/2508.05028", "title": "Evaluation of LLMs in AMR Parsing", "authors": ["Shu Han Ho"], "categories": ["cs.CL", "cs.AI"], "comment": "27 pages, 32 figures", "summary": "Meaning Representation (AMR) is a semantic formalism that encodes sentence\nmeaning as rooted, directed, acyclic graphs, where nodes represent concepts and\nedges denote semantic relations. Finetuning decoder only Large Language Models\n(LLMs) represent a promising novel straightfoward direction for AMR parsing.\nThis paper presents a comprehensive evaluation of finetuning four distinct LLM\narchitectures, Phi 3.5, Gemma 2, LLaMA 3.2, and DeepSeek R1 LLaMA Distilled\nusing the LDC2020T02 Gold AMR3.0 test set. Our results have shown that\nstraightfoward finetuning of decoder only LLMs can achieve comparable\nperformance to complex State of the Art (SOTA) AMR parsers. Notably, LLaMA 3.2\ndemonstrates competitive performance against SOTA AMR parsers given a\nstraightforward finetuning approach. We achieved SMATCH F1: 0.804 on the full\nLDC2020T02 test split, on par with APT + Silver (IBM) at 0.804 and approaching\nGraphene Smatch (MBSE) at 0.854. Across our analysis, we also observed a\nconsistent pattern where LLaMA 3.2 leads in semantic performance while Phi 3.5\nexcels in structural validity.", "AI": {"tldr": "This paper evaluates the performance of four LLMs finetuned for AMR parsing, demonstrating that simpler finetuning methods can achieve competitive results compared to complex models.", "motivation": "To explore the effectiveness of finetuning decoder-only LLMs for AMR parsing and compare their performance with that of State of the Art parsers.", "method": "Four distinct LLM architectures (Phi 3.5, Gemma 2, LLaMA 3.2, DeepSeek R1 LLaMA Distilled) were finetuned and evaluated using the LDC2020T02 Gold AMR3.0 test set.", "result": "Finetuning achieved SMATCH F1: 0.804, comparable to advanced AMR parsers, with LLaMA 3.2 showing the best semantic performance and Phi 3.5 excelling in structural validity.", "conclusion": "Straightforward finetuning of decoder-only LLMs can rival complex SOTA AMR parsers, indicating potential for easier implementations in practical applications.", "key_contributions": ["Evaluation of four LLM architectures in AMR parsing", "Demonstration of comparable performance to SOTA parsers with simpler methods", "Identification of LLaMA 3.2 as a leading model in semantic performance"], "limitations": "", "keywords": ["semantic formalism", "AMR parsing", "Large Language Models", "finetuning", "machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.05112", "pdf": "https://arxiv.org/pdf/2508.05112.pdf", "abs": "https://arxiv.org/abs/2508.05112", "title": "Metacognition and self-regulated learning in manipulative robotic problem-solving task", "authors": ["Margarida Romero", "George Kalmpourtzis"], "categories": ["cs.HC"], "comment": null, "summary": "Metacognition is an important aspect in creative problem solving (CPS) and\nthrough this chapter we analyse the meta-reasoning aspects applied in the\ndifferent processes of monitoring the progress of learners' reasoning and CPS\nactivities. Meta-reasoning monitors the way that problem-solving processes\nadvance and regulate time and efforts towards a solution. In the context of an\nill-defined problem, exploration is required to develop a better-defined\nproblem space and advance towards the solution space. The way learners engage\nin exploration and exploitations is regulated by the meta-reasoning within the\nCPS activity. The objective of this chapter is to examine and identify the CPS\nprocess with educational robots through a metacognitive and interactionist\napproach. This chapter presents a case study, where, to solve a problem, a\nparticipant had to explore a set of robot cubes to develop the technological\nknowledge associated with each single component of the system, but also\nconceptualize a system-level behaviour of the cubes when they are assembled.\nThe chapter presents the emergence of knowledge through the metacognitive\nregulation of the process of exploration and exploitation of prior knowledge\nand emergent knowledge until finding a solution", "AI": {"tldr": "This chapter explores the role of metacognition in creative problem solving (CPS) with a focus on educational robots, highlighting a case study involving robot cubes and the regulation of exploration and exploitation of knowledge.", "motivation": "The analysis of meta-reasoning in CPS activities is crucial for understanding how learners monitor and regulate their problem-solving processes.", "method": "The chapter examines the CPS process through a case study where participants engage with robot cubes to develop both technological knowledge and conceptual understanding of system-level behavior.", "result": "Findings reveal how metacognitive regulation influences the exploration and exploitation of knowledge, guiding learners toward solutions in CPS tasks.", "conclusion": "Metacognitive regulation plays a vital role in enhancing learners' problem-solving capabilities, particularly in ill-defined problem contexts.", "key_contributions": ["Analysis of metacognitive processes in CPS with educational robots", "Case study illustrating interactions with robot cubes", "Identification of knowledge emergence through metacognitive regulation"], "limitations": "", "keywords": ["metacognition", "creative problem solving", "educational robots", "exploration", "knowledge regulation"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2508.05078", "pdf": "https://arxiv.org/pdf/2508.05078.pdf", "abs": "https://arxiv.org/abs/2508.05078", "title": "Align, Don't Divide: Revisiting the LoRA Architecture in Multi-Task Learning", "authors": ["Jinda Liu", "Bo Cheng", "Yi Chang", "Yuan Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Parameter-Efficient Fine-Tuning (PEFT) is essential for adapting Large\nLanguage Models (LLMs). In practice, LLMs are often required to handle a\ndiverse set of tasks from multiple domains, a scenario naturally addressed by\nmulti-task learning (MTL). Within this MTL context, a prevailing trend involves\nLoRA variants with multiple adapters or heads, which advocate for structural\ndiversity to capture task-specific knowledge. Our findings present a direct\nchallenge to this paradigm. We first show that a simplified multi-head\narchitecture with high inter-head similarity substantially outperforms complex\nmulti-adapter and multi-head systems. This leads us to question the\nmulti-component paradigm itself, and we further demonstrate that a standard\nsingle-adapter LoRA, with a sufficiently increased rank, also achieves highly\ncompetitive performance. These results lead us to a new hypothesis: effective\nMTL generalization hinges on learning robust shared representations, not\nisolating task-specific features. To validate this, we propose Align-LoRA,\nwhich incorporates an explicit loss to align task representations within the\nshared adapter space. Experiments confirm that Align-LoRA significantly\nsurpasses all baselines, establishing a simpler yet more effective paradigm for\nadapting LLMs to multiple tasks. The code is available at\nhttps://github.com/jinda-liu/Align-LoRA.", "AI": {"tldr": "The paper proposes Align-LoRA, a simplified method for adapting Large Language Models (LLMs) through multi-task learning (MTL) that outperforms complex models with multiple adapters.", "motivation": "To address the challenges of adapting LLMs for diverse tasks in multi-task learning (MTL) scenarios.", "method": "The authors investigate a simplified multi-head architecture and propose Align-LoRA, which uses a single-adapter LoRA with an explicit loss for aligning task representations.", "result": "The simplified approach with high inter-head similarity outperforms more complex multi-adapter systems, leading to competitive performance with a single-adapter when rank is increased.", "conclusion": "Learning robust shared representations is crucial for effective multi-task learning; Align-LoRA establishes a simpler yet more effective approach for LLM adaptation.", "key_contributions": ["Introduction of Align-LoRA, a simplified multi-task learning method for LLMs.", "Demonstration that high inter-head similarity with fewer components can yield better performance.", "Challenge to the conventional multi-component paradigm in adapting LLMs."], "limitations": "Focus on specific architectures may limit generalizability across all LLM designs.", "keywords": ["Parameter-Efficient Fine-Tuning", "Multi-Task Learning", "Large Language Models", "Align-LoRA", "Task Representations"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.05156", "pdf": "https://arxiv.org/pdf/2508.05156.pdf", "abs": "https://arxiv.org/abs/2508.05156", "title": "AI Conversational Tutors in Foreign Language Learning: A Mixed-Methods Evaluation Study", "authors": ["Nikolaos Avouris"], "categories": ["cs.HC"], "comment": "To be cited as: Avouris N., (2025). AI Conversational Tutors in\n  Foreign Language Learning: A Mixed-Methods Evaluation Study, in Proceedings\n  14th Panhellenic Conference ICT in Education HCICTE 2025, Rhodes, October\n  2025", "summary": "This paper focuses on AI tutors in foreign language learning, a field of\napplication of AI tutors with great development, especially during the last\nyears, when great advances in natural language understanding and processing in\nreal time, have been achieved. These tutors attempt to address needs for\nimproving language skills (speaking, or communicative competence,\nunderstanding). In this paper, a mixed-methos empirical study on the use of\ndifferent kinds of state-of-the-art AI tutors for language learning is\nreported. This study involves a user experience evaluation of typical such\ntools, with special focus in their conversation functionality and an evaluation\nof their quality, based on chat transcripts. This study can help establish\ncriteria for assessing the quality of such systems and inform the design of\nfuture tools, including concerns about data privacy and secure handling of\nlearner information.", "AI": {"tldr": "The paper evaluates state-of-the-art AI tutors for foreign language learning through a mixed-method empirical study, focusing on user experience and conversation functionality.", "motivation": "To improve language skills through AI tutors and establish quality assessment criteria for future tools.", "method": "Mixed-method empirical study evaluating state-of-the-art AI tutors with an emphasis on conversation functionality and analysis of chat transcripts.", "result": "Findings include user experience insights and quality assessment criteria for AI tutors based on their conversation capabilities.", "conclusion": "The study contributes to understanding the effectiveness of AI tutors in language learning and offers guidelines for future tool design, particularly regarding privacy.", "key_contributions": ["User experience evaluation of AI tutors for language learning", "Criteria for assessing the quality of AI tutoring systems", "Insights into conversation functionality and data privacy concerns"], "limitations": "", "keywords": ["AI tutors", "language learning", "user experience", "natural language understanding", "data privacy"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.05097", "pdf": "https://arxiv.org/pdf/2508.05097.pdf", "abs": "https://arxiv.org/abs/2508.05097", "title": "Multimodal Fact Checking with Unified Visual, Textual, and Contextual Representations", "authors": ["Aditya Kishore", "Gaurav Kumar", "Jasabanta Patro"], "categories": ["cs.CL"], "comment": null, "summary": "The growing rate of multimodal misinformation, where claims are supported by\nboth text and images, poses significant challenges to fact-checking systems\nthat rely primarily on textual evidence. In this work, we have proposed a\nunified framework for fine-grained multimodal fact verification called\n\"MultiCheck\", designed to reason over structured textual and visual signals.\nOur architecture combines dedicated encoders for text and images with a fusion\nmodule that captures cross-modal relationships using element-wise interactions.\nA classification head then predicts the veracity of a claim, supported by a\ncontrastive learning objective that encourages semantic alignment between\nclaim-evidence pairs in a shared latent space. We evaluate our approach on the\nFactify 2 dataset, achieving a weighted F1 score of 0.84, substantially\noutperforming the baseline. These results highlight the effectiveness of\nexplicit multimodal reasoning and demonstrate the potential of our approach for\nscalable and interpretable fact-checking in complex, real-world scenarios.", "AI": {"tldr": "Proposed MultiCheck framework for multimodal fact verification using text and images.", "motivation": "To address the challenges of fact-checking systems that primarily rely on textual evidence amidst increasing multimodal misinformation.", "method": "MultiCheck integrates dedicated encoders for text and images with a fusion module to capture cross-modal relationships, followed by a classification head that predicts claim veracity.", "result": "Achieved a weighted F1 score of 0.84 on the Factify 2 dataset, outperforming baseline methods.", "conclusion": "The effectiveness of explicit multimodal reasoning in fact-checking is underscored, demonstrating scalability and interpretability in real-world applications.", "key_contributions": ["Development of MultiCheck framework for fact verification", "Integration of text and image encoders for multimodal reasoning", "Successful application leading to improved performance on the Factify 2 dataset"], "limitations": "", "keywords": ["multimodal misinformation", "fact-checking", "machine learning", "human-computer interaction", "contrastive learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.05228", "pdf": "https://arxiv.org/pdf/2508.05228.pdf", "abs": "https://arxiv.org/abs/2508.05228", "title": "CWEFS: Brain volume conduction effects inspired channel-wise EEG feature selection for multi-dimensional emotion recognition", "authors": ["Xueyuan Xu", "Wenjia Dong", "Fulin Wei", "Li Zhuo"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Due to the intracranial volume conduction effects, high-dimensional\nmulti-channel electroencephalography (EEG) features often contain substantial\nredundant and irrelevant information. This issue not only hinders the\nextraction of discriminative emotional representations but also compromises the\nreal-time performance. Feature selection has been established as an effective\napproach to address the challenges while enhancing the transparency and\ninterpretability of emotion recognition models. However, existing EEG feature\nselection research overlooks the influence of latent EEG feature structures on\nemotional label correlations and assumes uniform importance across various\nchannels, directly limiting the precise construction of EEG feature selection\nmodels for multi-dimensional affective computing. To address these limitations,\na novel channel-wise EEG feature selection (CWEFS) method is proposed for\nmulti-dimensional emotion recognition. Specifically, inspired by brain volume\nconduction effects, CWEFS integrates EEG emotional feature selection into a\nshared latent structure model designed to construct a consensus latent space\nacross diverse EEG channels. To preserve the local geometric structure, this\nconsensus space is further integrated with the latent semantic analysis of\nmulti-dimensional emotional labels. Additionally, CWEFS incorporates adaptive\nchannel-weight learning to automatically determine the significance of\ndifferent EEG channels in the emotional feature selection task. The\neffectiveness of CWEFS was validated using three popular EEG datasets with\nmulti-dimensional emotional labels. Comprehensive experimental results,\ncompared against nineteen feature selection methods, demonstrate that the EEG\nfeature subsets chosen by CWEFS achieve optimal emotion recognition performance\nacross six evaluation metrics.", "AI": {"tldr": "The paper presents a novel channel-wise EEG feature selection method to improve emotion recognition from EEG signals by addressing redundancy and relevance issues in high-dimensional data.", "motivation": "Existing EEG feature selection techniques do not consider the influence of latent EEG feature structures on emotional correlations, leading to suboptimal models for emotion recognition.", "method": "The proposed CWEFS method utilizes a shared latent structure model that integrates EEG emotional feature selection and latent semantic analysis to determine the significance of various EEG channels.", "result": "Experimental validation on three EEG datasets showed that CWEFS outperforms nineteen existing feature selection methods, achieving optimal emotion recognition across six metrics.", "conclusion": "CWEFS enhances emotion recognition by effectively selecting relevant EEG features while considering the latent structures of emotional labels and channel importance.", "key_contributions": ["Introduction of CWEFS for emotion recognition in EEG data", "Integration of latent semantic analysis in feature selection", "Adaptive learning of channel significance"], "limitations": "", "keywords": ["EEG", "emotion recognition", "feature selection", "latent structure", "multi-dimensional"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.05100", "pdf": "https://arxiv.org/pdf/2508.05100.pdf", "abs": "https://arxiv.org/abs/2508.05100", "title": "BEE-RAG: Balanced Entropy Engineering for Retrieval-Augmented Generation", "authors": ["Yuhao Wang", "Ruiyang Ren", "Yucheng Wang", "Jing Liu", "Wayne Xin Zhao", "Hua Wu", "Haifeng Wang"], "categories": ["cs.CL"], "comment": null, "summary": "With the rapid advancement of large language models (LLMs),\nretrieval-augmented generation (RAG) has emerged as a critical approach to\nsupplement the inherent knowledge limitations of LLMs. However, due to the\ntypically large volume of retrieved information, RAG tends to operate with long\ncontext lengths. From the perspective of entropy engineering, we identify\nunconstrained entropy growth and attention dilution due to long retrieval\ncontext as significant factors affecting RAG performance. In this paper, we\npropose the balanced entropy-engineered RAG (BEE-RAG) framework, which improves\nthe adaptability of RAG systems to varying context lengths through the\nprinciple of entropy invariance. By leveraging balanced context entropy to\nreformulate attention dynamics, BEE-RAG separates attention sensitivity from\ncontext length, ensuring a stable entropy level. Building upon this, we\nintroduce a zero-shot inference strategy for multi-importance estimation and a\nparameter-efficient adaptive fine-tuning mechanism to obtain the optimal\nbalancing factor for different settings. Extensive experiments across multiple\nRAG tasks demonstrate the effectiveness of BEE-RAG.", "AI": {"tldr": "The BEE-RAG framework enhances retrieval-augmented generation by managing attention entropy, allowing better adaptability to varying context lengths and improving overall RAG performance.", "motivation": "To address performance issues in RAG systems caused by long context lengths and attention dilution from unconstrained entropy growth.", "method": "The BEE-RAG framework implements entropy invariance to reformulate attention dynamics, while introducing a zero-shot inference strategy for multi-importance estimation and a parameter-efficient adaptive fine-tuning mechanism.", "result": "Extensive experiments show that BEE-RAG significantly improves the performance of retrieval-augmented generation tasks compared to existing methods.", "conclusion": "BEE-RAG effectively balances entropy sensitivity with context length, leading to improved adaptability and performance in RAG applications.", "key_contributions": ["Proposes a framework that improves RAG by controlling entropy dynamics", "Introduces zero-shot inference for multi-importance estimation", "Develops a parameter-efficient adaptive fine-tuning mechanism"], "limitations": "", "keywords": ["large language models", "retrieval-augmented generation", "entropy engineering", "attention dynamics", "adaptive fine-tuning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.05229", "pdf": "https://arxiv.org/pdf/2508.05229.pdf", "abs": "https://arxiv.org/abs/2508.05229", "title": "ADSEL: Adaptive dual self-expression learning for EEG feature selection via incomplete multi-dimensional emotional tagging", "authors": ["Tianze Yu", "Junming Zhang", "Wenjia Dong", "Xueyuan Xu", "Li Zhuo"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "EEG based multi-dimension emotion recognition has attracted substantial\nresearch interest in human computer interfaces. However, the high\ndimensionality of EEG features, coupled with limited sample sizes, frequently\nleads to classifier overfitting and high computational complexity. Feature\nselection constitutes a critical strategy for mitigating these challenges. Most\nexisting EEG feature selection methods assume complete multi-dimensional\nemotion labels. In practice, open acquisition environment, and the inherent\nsubjectivity of emotion perception often result in incomplete label data, which\ncan compromise model generalization. Additionally, existing feature selection\nmethods for handling incomplete multi-dimensional labels primarily focus on\ncorrelations among various dimensions during label recovery, neglecting the\ncorrelation between samples in the label space and their interaction with\nvarious dimensions. To address these issues, we propose a novel incomplete\nmulti-dimensional feature selection algorithm for EEG-based emotion\nrecognition. The proposed method integrates an adaptive dual self-expression\nlearning (ADSEL) with least squares regression. ADSEL establishes a\nbidirectional pathway between sample-level and dimension-level self-expression\nlearning processes within the label space. It could facilitate the\ncross-sharing of learned information between these processes, enabling the\nsimultaneous exploitation of effective information across both samples and\ndimensions for label reconstruction. Consequently, ADSEL could enhances label\nrecovery accuracy and effectively identifies the optimal EEG feature subset for\nmulti-dimensional emotion recognition.", "AI": {"tldr": "A novel algorithm for EEG-based emotion recognition that addresses incomplete label data using adaptive dual self-expression learning for improved feature selection.", "motivation": "To overcome the challenges of high dimensionality in EEG features and limited sample sizes that lead to classifier overfitting and complexity in emotion recognition.", "method": "The proposed method integrates an adaptive dual self-expression learning (ADSEL) with least squares regression to improve feature selection under incomplete multi-dimensional emotion labels.", "result": "The ADSEL method enhances label recovery accuracy and effectively identifies the optimal EEG feature subset for emotion recognition tasks.", "conclusion": "The integration of sample-level and dimension-level self-expression learning improves the exploitation of information necessary for effective emotion label reconstruction.", "key_contributions": ["Proposes a novel incomplete multi-dimensional feature selection algorithm for EEG emotion recognition.", "Integrates ADSEL with least squares regression to improve label recovery and feature selection.", "Addresses the neglect of correlations between samples and dimensions in feature selection methods."], "limitations": "", "keywords": ["EEG", "emotion recognition", "feature selection", "machine learning", "label recovery"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2508.05128", "pdf": "https://arxiv.org/pdf/2508.05128.pdf", "abs": "https://arxiv.org/abs/2508.05128", "title": "Attention Basin: Why Contextual Position Matters in Large Language Models", "authors": ["Zihao Yi", "Delong Zeng", "Zhenqing Ling", "Haohao Luo", "Zhe Xu", "Wei Liu", "Jian Luan", "Wanxia Cao", "Ying Shen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The performance of Large Language Models (LLMs) is significantly sensitive to\nthe contextual position of information in the input. To investigate the\nmechanism behind this positional bias, our extensive experiments reveal a\nconsistent phenomenon we term the attention basin: when presented with a\nsequence of structured items (e.g., retrieved documents or few-shot examples),\nmodels systematically assign higher attention to the items at the beginning and\nend of the sequence, while neglecting those in the middle. Crucially, our\nanalysis further reveals that allocating higher attention to critical\ninformation is key to enhancing model performance. Based on these insights, we\nintroduce Attention-Driven Reranking (AttnRank), a two-stage framework that (i)\nestimates a model's intrinsic positional attention preferences using a small\ncalibration set, and (ii) reorders retrieved documents or few-shot examples to\nalign the most salient content with these high-attention positions. AttnRank is\na model-agnostic, training-free, and plug-and-play method with minimal\ncomputational overhead. Experiments on multi-hop QA and few-shot in-context\nlearning tasks demonstrate that AttnRank achieves substantial improvements\nacross 10 large language models of varying architectures and scales, without\nmodifying model parameters or training procedures.", "AI": {"tldr": "This paper investigates the positional bias in Large Language Models and proposes a framework called Attention-Driven Reranking (AttnRank) to enhance model performance by optimizing the arrangement of input information.", "motivation": "To understand the positional bias in LLMs and improve their performance by enhancing how they attend to contextually important information.", "method": "The authors conduct extensive experiments to observe how LLMs allocate attention and propose AttnRank, which ranks information based on intrinsic positional preferences without requiring additional training.", "result": "AttnRank led to significant performance improvements across 10 different large language models in tasks like multi-hop QA and few-shot learning, proving effective without modifying model parameters.", "conclusion": "Optimizing attention allocation based on the identified positional bias can substantially enhance the performance of LLMs without extensive retraining or adaptations.", "key_contributions": ["Identification of the attention basin phenomenon in LLMs", "Introduction of the Attention-Driven Reranking (AttnRank) framework", "Demonstration of AttnRank's effectiveness across multiple tasks and model architectures"], "limitations": "", "keywords": ["Large Language Models", "Attention Bias", "Reranking", "Machine Learning", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.05231", "pdf": "https://arxiv.org/pdf/2508.05231.pdf", "abs": "https://arxiv.org/abs/2508.05231", "title": "FDC-Net: Rethinking the association between EEG artifact removal and multi-dimensional affective computing", "authors": ["Wenjia Dong", "Xueyuan Xu", "Tianze Yu", "Junming Zhang", "Li Zhuo"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Electroencephalogram (EEG)-based emotion recognition holds significant value\nin affective computing and brain-computer interfaces. However, in practical\napplications, EEG recordings are susceptible to the effects of various\nphysiological artifacts. Current approaches typically treat denoising and\nemotion recognition as independent tasks using cascaded architectures, which\nnot only leads to error accumulation, but also fails to exploit potential\nsynergies between these tasks. Moreover, conventional EEG-based emotion\nrecognition models often rely on the idealized assumption of \"perfectly\ndenoised data\", lacking a systematic design for noise robustness. To address\nthese challenges, a novel framework that deeply couples denoising and emotion\nrecognition tasks is proposed for end-to-end noise-robust emotion recognition,\ntermed as Feedback-Driven Collaborative Network for Denoising-Classification\nNexus (FDC-Net). Our primary innovation lies in establishing a dynamic\ncollaborative mechanism between artifact removal and emotion recognition\nthrough: (1) bidirectional gradient propagation with joint optimization\nstrategies; (2) a gated attention mechanism integrated with frequency-adaptive\nTransformer using learnable band-position encoding. Two most popular EEG-based\nemotion datasets (DEAP and DREAMER) with multi-dimensional emotional labels\nwere employed to compare the artifact removal and emotion recognition\nperformance between ASLSL and nine state-of-the-art methods. In terms of the\ndenoising task, FDC-Net obtains a maximum correlation coefficient (CC) value of\n96.30% on DEAP and a maximum CC value of 90.31% on DREAMER. In terms of the\nemotion recognition task under physiological artifact interference, FDC-Net\nachieves emotion recognition accuracies of 82.3+7.1% on DEAP and 88.1+0.8% on\nDREAMER.", "AI": {"tldr": "A novel framework, FDC-Net, integrates denoising and emotion recognition for robust EEG-based emotion recognition.", "motivation": "Current EEG emotion recognition methods fail to integrate denoising and recognition tasks, leading to errors and not leveraging synergies between them.", "method": "FDC-Net employs bidirectional gradient propagation and a gated attention mechanism with a frequency-adaptive Transformer for joint optimization of denoising and emotion recognition.", "result": "FDC-Net achieved high performance with a maximum CC of 96.30% on DEAP and 90.31% on DREAMER for denoising, and emotion recognition accuracies of 82.3% and 88.1% respectively.", "conclusion": "The proposed FDC-Net enhances noise robustness in EEG-based emotion recognition by effectively coupling artifact removal and emotion detection.", "key_contributions": ["A framework that couples denoising and emotion recognition tasks in EEG data", "Dynamic collaborative mechanism using bidirectional gradient propagation", "Gated attention mechanism with frequency-adaptive Transformer"], "limitations": "", "keywords": ["EEG", "emotion recognition", "denoising", "deep learning", "artifact removal"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.05132", "pdf": "https://arxiv.org/pdf/2508.05132.pdf", "abs": "https://arxiv.org/abs/2508.05132", "title": "Towards Assessing Medical Ethics from Knowledge to Practice", "authors": ["Chang Hong", "Minghao Wu", "Qingying Xiao", "Yuchi Wang", "Xiang Wan", "Guangjun Yu", "Benyou Wang", "Yan Hu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The integration of large language models into healthcare necessitates a\nrigorous evaluation of their ethical reasoning, an area current benchmarks\noften overlook. We introduce PrinciplismQA, a comprehensive benchmark with\n3,648 questions designed to systematically assess LLMs' alignment with core\nmedical ethics. Grounded in Principlism, our benchmark features a high-quality\ndataset. This includes multiple-choice questions curated from authoritative\ntextbooks and open-ended questions sourced from authoritative medical ethics\ncase study literature, all validated by medical experts. Our experiments reveal\na significant gap between models' ethical knowledge and their practical\napplication, especially in dynamically applying ethical principles to\nreal-world scenarios. Most LLMs struggle with dilemmas concerning Beneficence,\noften over-emphasizing other principles. Frontier closed-source models, driven\nby strong general capabilities, currently lead the benchmark. Notably, medical\ndomain fine-tuning can enhance models' overall ethical competence, but further\nprogress requires better alignment with medical ethical knowledge.\nPrinciplismQA offers a scalable framework to diagnose these specific ethical\nweaknesses, paving the way for more balanced and responsible medical AI.", "AI": {"tldr": "This paper introduces PrinciplismQA, a benchmark for evaluating large language models' adherence to medical ethics, revealing gaps in ethical reasoning, particularly in applying principles like Beneficence.", "motivation": "To rigorously evaluate the ethical reasoning capabilities of large language models in healthcare, as current benchmarks lack this focus.", "method": "Developed PrinciplismQA, a benchmark comprising 3,648 questions that assess models' alignment with core medical ethics, using multiple-choice and open-ended questions validated by medical experts.", "result": "Found significant discrepancies between LLMs' ethical knowledge and their practical applications, particularly in handling ethical dilemmas involving Beneficence.", "conclusion": "PrinciplismQA serves as a scalable tool to identify ethical weaknesses in AI, highlighting the need for better alignment of LLMs with medical ethical standards.", "key_contributions": ["Introduction of a comprehensive benchmark for evaluating ethical reasoning in LLMs in healthcare.", "Demonstration of the gap between models' knowledge and their practical ethical applications.", "Showing the impact of medical domain fine-tuning on improving models' ethical competence."], "limitations": "", "keywords": ["Large Language Models", "Medical Ethics", "Benchmarking", "Artificial Intelligence", "Healthcare"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.05238", "pdf": "https://arxiv.org/pdf/2508.05238.pdf", "abs": "https://arxiv.org/abs/2508.05238", "title": "Driver Assistant: Persuading Drivers to Adjust Secondary Tasks Using Large Language Models", "authors": ["Wei Xiang", "Muchen Li", "Jie Yan", "Manling Zheng", "Hanfei Zhu", "Mengyun Jiang", "Lingyun Sun"], "categories": ["cs.HC", "cs.AI"], "comment": "6 pages, 4 figures, 2025 IEEE International Conference on Systems,\n  Man, and Cybernetics (SMC)", "summary": "Level 3 automated driving systems allows drivers to engage in secondary tasks\nwhile diminishing their perception of risk. In the event of an emergency\nnecessitating driver intervention, the system will alert the driver with a\nlimited window for reaction and imposing a substantial cognitive burden. To\naddress this challenge, this study employs a Large Language Model (LLM) to\nassist drivers in maintaining an appropriate attention on road conditions\nthrough a \"humanized\" persuasive advice. Our tool leverages the road conditions\nencountered by Level 3 systems as triggers, proactively steering driver\nbehavior via both visual and auditory routes. Empirical study indicates that\nour tool is effective in sustaining driver attention with reduced cognitive\nload and coordinating secondary tasks with takeover behavior. Our work provides\ninsights into the potential of using LLMs to support drivers during multi-task\nautomated driving.", "AI": {"tldr": "This study presents a tool utilizing a Large Language Model (LLM) to assist drivers in maintaining attention during Level 3 automated driving by providing humanized persuasive advice.", "motivation": "To reduce cognitive load and improve driver attention in Level 3 automated driving systems, where drivers engage in secondary tasks.", "method": "The research uses a Large Language Model to provide visual and auditory persuasive advice to drivers, based on real-time road conditions.", "result": "The tool was empirically shown to sustain driver attention while reducing cognitive load and improving the coordination of secondary tasks during takeover situations.", "conclusion": "The findings suggest that LLMs can effectively support drivers in automated environments through tailored, proactive engagement.", "key_contributions": ["Development of an LLM-based tool for driver assistance in automated driving", "Proactive steering of driver behavior through real-time condition triggers", "Empirical evidence demonstrating reduced cognitive burden and enhanced attention management"], "limitations": "", "keywords": ["Level 3 automation", "Large Language Model", "driver attention", "cognitive load", "persuasive advice"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2508.05179", "pdf": "https://arxiv.org/pdf/2508.05179.pdf", "abs": "https://arxiv.org/abs/2508.05179", "title": "ATLANTIS at SemEval-2025 Task 3: Detecting Hallucinated Text Spans in Question Answering", "authors": ["Catherine Kobus", "Fran√ßois Lancelot", "Marion-C√©cile Martin", "Nawal Ould Amer"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents the contributions of the ATLANTIS team to SemEval-2025\nTask 3, focusing on detecting hallucinated text spans in question answering\nsystems. Large Language Models (LLMs) have significantly advanced Natural\nLanguage Generation (NLG) but remain susceptible to hallucinations, generating\nincorrect or misleading content. To address this, we explored methods both with\nand without external context, utilizing few-shot prompting with a LLM,\ntoken-level classification or LLM fine-tuned on synthetic data. Notably, our\napproaches achieved top rankings in Spanish and competitive placements in\nEnglish and German. This work highlights the importance of integrating relevant\ncontext to mitigate hallucinations and demonstrate the potential of fine-tuned\nmodels and prompt engineering.", "AI": {"tldr": "The paper discusses methods for detecting hallucinated text spans in question answering systems using LLMs.", "motivation": "LLMs are prone to generating hallucinated content, which can mislead users, necessitating effective detection methods.", "method": "The study explores methods using few-shot prompting and token-level classification, with models fine-tuned on synthetic data, both with and without external context.", "result": "The approaches achieved top rankings in Spanish and competitive placements in English and German for the task of detecting hallucinations.", "conclusion": "Integrating relevant context is crucial for reducing hallucination instances, and fine-tuning and prompt engineering can enhance model performance.", "key_contributions": ["Developed effective detection methods for hallucinated text spans.", "Demonstrated the impact of context integration on model accuracy.", "Achieved competitive results across multiple languages."], "limitations": "", "keywords": ["hallucination detection", "question answering", "large language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.05281", "pdf": "https://arxiv.org/pdf/2508.05281.pdf", "abs": "https://arxiv.org/abs/2508.05281", "title": "A Methodological Framework and Questionnaire for Investigating Perceived Algorithmic Fairness", "authors": ["Ahmed Abdal Shafi Rasel", "Ahmed Mustafa Amlan", "Tasmim Shajahan Mim", "Tanvir Hasan"], "categories": ["cs.HC"], "comment": "34 pages, Submitted for review", "summary": "This study explores perceptions of fairness in algorithmic decision-making\namong users in Bangladesh through a comprehensive mixed-methods approach. By\nintegrating quantitative survey data with qualitative interview insights, we\nexamine how cultural, social, and contextual factors influence users'\nunderstanding of fairness, transparency, and accountability in AI systems. Our\nfindings reveal nuanced attitudes toward human oversight, explanation\nmechanisms, and contestability, highlighting the importance of culturally aware\ndesign principles for equitable and trustworthy algorithmic systems. These\ninsights contribute to ongoing discussions on algorithmic fairness by\nforegrounding perspectives from a non-Western context, thus broadening the\nglobal dialogue on ethical AI deployment.", "AI": {"tldr": "This study examines users' perceptions of fairness in algorithmic decision-making in Bangladesh, utilizing a mixed-methods approach combining surveys and interviews.", "motivation": "To understand how cultural, social, and contextual factors shape perceptions of fairness, transparency, and accountability in AI systems among users from Bangladesh.", "method": "A comprehensive mixed-methods approach integrating quantitative survey data with qualitative insights from interviews.", "result": "Users exhibit nuanced attitudes towards human oversight, explanation mechanisms, and contestability in AI systems, emphasizing the need for culturally aware design principles.", "conclusion": "Culturally informed design is crucial for creating equitable and trustworthy algorithmic systems, contributing to the global dialogue on ethical AI.", "key_contributions": ["Exploration of algorithmic fairness perceptions in a non-Western context.", "Integration of quantitative and qualitative research methods.", "Recommendations for culturally aware design principles in AI systems."], "limitations": "", "keywords": ["algorithmic fairness", "cultural perspectives", "AI transparency"], "importance_score": 7, "read_time_minutes": 34}}
{"id": "2508.05234", "pdf": "https://arxiv.org/pdf/2508.05234.pdf", "abs": "https://arxiv.org/abs/2508.05234", "title": "Resource-Limited Joint Multimodal Sentiment Reasoning and Classification via Chain-of-Thought Enhancement and Distillation", "authors": ["Haonan Shangguan", "Xiaocui Yang", "Shi Feng", "Daling Wang", "Yifei Zhang", "Ge Yu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The surge in rich multimodal content on social media platforms has greatly\nadvanced Multimodal Sentiment Analysis (MSA), with Large Language Models (LLMs)\nfurther accelerating progress in this field. Current approaches primarily\nleverage the knowledge and reasoning capabilities of parameter-heavy\n(Multimodal) LLMs for sentiment classification, overlooking autonomous\nmultimodal sentiment reasoning generation in resource-constrained environments.\nTherefore, we focus on the Resource-Limited Joint Multimodal Sentiment\nReasoning and Classification task, JMSRC, which simultaneously performs\nmultimodal sentiment reasoning chain generation and sentiment classification\nonly with a lightweight model. We propose a Multimodal Chain-of-Thought\nReasoning Distillation model, MulCoT-RD, designed for JMSRC that employs a\n\"Teacher-Assistant-Student\" distillation paradigm to address deployment\nconstraints in resource-limited environments. We first leverage a\nhigh-performance Multimodal Large Language Model (MLLM) to generate the initial\nreasoning dataset and train a medium-sized assistant model with a multi-task\nlearning mechanism. A lightweight student model is jointly trained to perform\nefficient multimodal sentiment reasoning generation and classification.\nExtensive experiments on four datasets demonstrate that MulCoT-RD with only 3B\nparameters achieves strong performance on JMSRC, while exhibiting robust\ngeneralization and enhanced interpretability.", "AI": {"tldr": "This paper introduces the MulCoT-RD model for joint multimodal sentiment reasoning and classification in resource-constrained environments.", "motivation": "To tackle multimodal sentiment analysis (MSA) challenges in resource-limited settings using lightweight models instead of heavy parameter ones.", "method": "The MulCoT-RD model adopts a 'Teacher-Assistant-Student' distillation approach, where a high-performance MLLM generates initial reasoning data, an assistant model is trained with multi-task learning, and a lightweight student model performs reasoning and classification.", "result": "MulCoT-RD, with only 3B parameters, shows strong performance on the JMSRC task across four datasets, enhancing generalization and interpretability.", "conclusion": "The proposed model efficiently manages to generate reasoning chains and classify sentiments, making it suitable for deployment in resource-constrained environments.", "key_contributions": ["Introduction of MulCoT-RD for joint reasoning and classification in MSA", "Use of a 'Teacher-Assistant-Student' distillation strategy", "Demonstrated strong performance with fewer parameters"], "limitations": "", "keywords": ["Multimodal Sentiment Analysis", "Resource-Limited", "Large Language Models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.05325", "pdf": "https://arxiv.org/pdf/2508.05325.pdf", "abs": "https://arxiv.org/abs/2508.05325", "title": "Critical Design Strategy: a Method for Heuristically Evaluating Visualisation Designs", "authors": ["Jonathan C. Roberts", "Hanan Alnjar", "Aron E. Owen", "Panagiotis D. Ritsos"], "categories": ["cs.HC", "H.5.2; K.3.0; D.0; I.3.8"], "comment": "11 pages, 6 pages supplemental material, 2 CDS versions", "summary": "We present the Critical Design Strategy (CDS) - a structured method designed\nto facilitate the examination of visualisation designs through reflection and\ncritical thought. The CDS helps designers think critically and make informed\nimprovements using heuristic evaluation. When developing a visual tool or\npioneering a novel visualisation approach, identifying areas for enhancement\ncan be challenging. Critical thinking is particularly crucial for visualisation\ndesigners and tool developers, especially those new to the field, such as\nstudying visualisation in higher education. The CDS consists of three stages\nacross six perspectives: Stage 1 captures the essence of the idea by assigning\nan indicative title and selecting five adjectives (from twenty options) to form\ninitial impressions of the design. Stage 2 involves an in-depth critique using\n30 heuristic questions spanning six key perspectives - user, environment,\ninterface, components, design, and visual marks. Stage 3 focuses on\nsynthesising insights, reflecting on design decisions, and determining the next\nsteps forward. We introduce the CDS and explore its use across three\nvisualisation modules in both undergraduate and postgraduate courses. Our\nlongstanding experience with the CDS has allowed us to refine and develop it\nover time: from its initial creation through workshops in 2017/18 to\nimprovements in wording and the development of two applications by 2020,\nfollowed by the expansion of support notes and refinement of heuristics through\n2023; while using it in our teaching each year. This sustained use allows us to\nreflect on its practical application and offer guidance on how others can\nincorporate it into their own work.", "AI": {"tldr": "The paper introduces the Critical Design Strategy (CDS), a method for enhancing visualisation designs through structured reflection and heuristic evaluation.", "motivation": "The CDS aims to support designers, particularly those new to visualisation, in critically evaluating and improving their designs.", "method": "The CDS involves three stages: Stage 1 defines the idea and captures initial impressions; Stage 2 conducts an in-depth critique with heuristic questions; Stage 3 synthesizes insights and reflects on future steps.", "result": "The CDS has been successfully implemented in visualisation modules for both undergraduate and postgraduate courses, leading to significant enhancements in design reflection and critique.", "conclusion": "The sustained application and refinement of the CDS demonstrate its effectiveness and provide a framework for others to follow in their visualisation practices.", "key_contributions": ["Introduction of a structured method for design evaluation", "Implementation in educational settings", "Refinement of heuristic evaluation techniques"], "limitations": "No specific limitations mentioned; focus on practical application and refinement over time.", "keywords": ["Critical Design Strategy", "visualization", "heuristic evaluation", "design thinking", "higher education"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.05239", "pdf": "https://arxiv.org/pdf/2508.05239.pdf", "abs": "https://arxiv.org/abs/2508.05239", "title": "Pruning Large Language Models by Identifying and Preserving Functional Networks", "authors": ["Yiheng Liu", "Junhao Ning", "Sichen Xia", "Xiaohui Gao", "Ning Qiang", "Bao Ge", "Junwei Han", "Xintao Hu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, 5 figures", "summary": "Structured pruning is one of the representative techniques for compressing\nlarge language models (LLMs) to reduce GPU memory consumption and accelerate\ninference speed. It offers significant practical value in improving the\nefficiency of LLMs in real-world applications. Current structured pruning\nmethods typically rely on assessment of the importance of the structure units\nand pruning the units with less importance. Most of them overlooks the\ninteraction and collaboration among artificial neurons that are crucial for the\nfunctionalities of LLMs, leading to a disruption in the macro functional\narchitecture of LLMs and consequently a pruning performance degradation.\nInspired by the inherent similarities between artificial neural networks and\nfunctional neural networks in the human brain, we alleviate this challenge and\npropose to prune LLMs by identifying and preserving functional networks within\nLLMs in this study. To achieve this, we treat an LLM as a digital brain and\ndecompose the LLM into functional networks, analogous to identifying functional\nbrain networks in neuroimaging data. Afterwards, an LLM is pruned by preserving\nthe key neurons within these functional networks. Experimental results\ndemonstrate that the proposed method can successfully identify and locate\nfunctional networks and key neurons in LLMs, enabling efficient model pruning.\nOur code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION.", "AI": {"tldr": "This paper proposes a novel structured pruning method for large language models that preserves functional networks to improve efficiency during model compression.", "motivation": "To address the performance degradation in structured pruning by recognizing the importance of interactions among neurons in large language models.", "method": "Models are decomposed into functional networks, akin to human brain networks, allowing for targeted preservation of key neurons during pruning.", "result": "The proposed pruning approach successfully identifies functional networks and key neurons in LLMs, leading to more efficient model compression with less performance degradation.", "conclusion": "Preserving functional networks enhances the structured pruning of LLMs, offering a promising direction for maintaining performance while reducing resource consumption.", "key_contributions": ["Introduction of a novel pruning method based on preserving functional networks in LLMs.", "Demonstration of improved pruning performance by maintaining key interactions among neurons.", "Code availability for practical application and further research."], "limitations": "", "keywords": ["structured pruning", "large language models", "functional networks", "neural networks", "model compression"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.05332", "pdf": "https://arxiv.org/pdf/2508.05332.pdf", "abs": "https://arxiv.org/abs/2508.05332", "title": "Implementation and Application of Multi-Format 3D Data Integration in a Cross-Device Commercial Metaverse Platform", "authors": ["Masanori Ibara", "Yuichi Hiroi", "Takushi Kamegai", "Takefumi Hiraki"], "categories": ["cs.HC"], "comment": "10 pages, to appear in IEEE International Symposium on Emerging\n  Metaverse (ISEMV)", "summary": "Traditionally, specialized 3D design data, such as BIM and CAD, have been\naccessible only to a select group of experts, creating significant barriers\nthat prevent general users from participating in decision-making processes.\nThis paper provides a systematic overview of practical insights for utilizing\n3D data in industrial and architectural domains by presenting implementation\ncases of the industrial metaverse on Cluster, a commercial cross-device\nmetaverse platform. This paper analyzes the characteristics and constraints of\nmajor data formats in the industrial and architectural fields and organizes\nintegration workflows for the metaverse. Through application cases utilizing 3D\ndata across multiple domains, we present practical examples of collaborative\ndecision-making support enabled by the fusion of metaverse and digital twin\ntechnologies. Specifically, we demonstrate that multi-device access and\nsimultaneous multi-user participation capabilities foster democratic\nenvironments in the industrial metaverse, which are challenging to achieve with\nconventional, expert-dependent systems.", "AI": {"tldr": "This paper explores how the integration of 3D design data with the metaverse can enhance collaborative decision-making in industrial and architectural settings.", "motivation": "To address barriers that limit access to specialized 3D design data for general users and to promote more inclusive decision-making processes in industry and architecture.", "method": "The paper presents a systematic overview and implementation cases on Cluster, a metaverse platform, analyzing major data formats and outlining integration workflows for the industrial metaverse.", "result": "The study demonstrates that multi-device access and simultaneous participation improve collaborative decision-making, fostering democratic environments in the industrial metaverse.", "conclusion": "The fusion of metaverse and digital twin technologies in industrial applications can democratize access to 3D design data, involving a wider range of participants in decision-making processes.", "key_contributions": ["Overview of practical insights for using 3D data in industrial metaverse applications", "Case studies demonstrating the integration of metaverse with digital twin technologies", "Analysis of data formats and workflows for improved collaboration in decision-making."], "limitations": "", "keywords": ["3D design", "metaverse", "collaborative decision-making", "digital twins", "industrial applications"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.05242", "pdf": "https://arxiv.org/pdf/2508.05242.pdf", "abs": "https://arxiv.org/abs/2508.05242", "title": "CodeBoost: Boosting Code LLMs by Squeezing Knowledge from Code Snippets with RL", "authors": ["Sijie Wang", "Quanjiang Guo", "Kai Zhao", "Yawei Zhang", "Xin Li", "Xiang Li", "Siqi Li", "Rui She", "Shangshu Yu", "Wee Peng Tay"], "categories": ["cs.CL"], "comment": "Technical report. Project page: https://github.com/sijieaaa/CodeBoost", "summary": "Code large language models (LLMs) have become indispensable tools for\nbuilding efficient and automated coding pipelines. Existing models are\ntypically post-trained using reinforcement learning (RL) from general-purpose\nLLMs using \"human instruction-final answer\" pairs, where the instructions are\nusually from manual annotations. However, collecting high-quality coding\ninstructions is both labor-intensive and difficult to scale. On the other hand,\ncode snippets are abundantly available from various sources. This imbalance\npresents a major bottleneck in instruction-based post-training. We propose\nCodeBoost, a post-training framework that enhances code LLMs purely from code\nsnippets, without relying on human-annotated instructions. CodeBoost introduces\nthe following key components: (1) maximum-clique curation, which selects a\nrepresentative and diverse training corpus from code; (2) bi-directional\nprediction, which enables the model to learn from both forward and backward\nprediction objectives; (3) error-aware prediction, which incorporates learning\nsignals from both correct and incorrect outputs; (4) heterogeneous\naugmentation, which diversifies the training distribution to enrich code\nsemantics; and (5) heterogeneous rewarding, which guides model learning through\nmultiple reward types including format correctness and execution feedback from\nboth successes and failures. Extensive experiments across several code LLMs and\nbenchmarks verify that CodeBoost consistently improves performance,\ndemonstrating its effectiveness as a scalable and effective training pipeline.", "AI": {"tldr": "CodeBoost enhances code LLMs without relying on human-annotated instructions by using code snippets.", "motivation": "Existing models rely on difficult to scale human-annotated instructions, creating a bottleneck in instruction-based post-training.", "method": "CodeBoost introduces maximum-clique curation, bi-directional prediction, error-aware prediction, heterogeneous augmentation, and heterogeneous rewarding.", "result": "Extensive experiments show that CodeBoost improves performance across several code LLMs and benchmarks.", "conclusion": "CodeBoost proves to be a scalable and effective training pipeline for code LLMs.", "key_contributions": ["Maximum-clique curation of a diverse training corpus from code snippets.", "Bi-directional and error-aware prediction learning objectives.", "Heterogeneous rewarding incorporating multiple types of feedback."], "limitations": "", "keywords": ["Code LLMs", "Post-training", "Reinforcement learning", "Heterogeneous augmentation", "Error-aware prediction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.05497", "pdf": "https://arxiv.org/pdf/2508.05497.pdf", "abs": "https://arxiv.org/abs/2508.05497", "title": "Towards Human-Centric Evaluation of Interaction-Aware Automated Vehicle Controllers: A Framework and Case Study", "authors": ["Federico Scar√¨", "Olger Siebinga", "Arkady Zgonnikov"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "As automated vehicles (AVs) increasingly integrate into mixed-traffic\nenvironments, evaluating their interaction with human-driven vehicles (HDVs)\nbecomes critical. In most research focused on developing new AV control\nalgorithms (controllers), the performance of these algorithms is assessed\nsolely based on performance metrics such as collision avoidance or lane-keeping\nefficiency, while largely overlooking the human-centred dimensions of\ninteraction with HDVs. This paper proposes a structured evaluation framework\nthat addresses this gap by incorporating metrics grounded in the human-robot\ninteraction literature. The framework spans four key domains: a) interaction\neffect, b) interaction perception, c) interaction effort, and d) interaction\nability. These domains capture both the performance of the AV and its impact on\nhuman drivers around it. To demonstrate the utility of the framework, we apply\nit to a case study evaluating how a state-of-the-art AV controller interacts\nwith human drivers in a merging scenario in a driving simulator. Measuring\nHDV-HDV interactions as a baseline, this study included one representative\nmetric per domain: a) perceived safety, b) subjective ratings, specifically how\nparticipants perceived the other vehicle's driving behaviour (e.g.,\naggressiveness or predictability) , c) driver workload, and d) merging success.\nThe results showed that incorporating metrics covering all four domains in the\nevaluation of AV controllers can illuminate critical differences in driver\nexperience when interacting with AVs. This highlights the need for a more\ncomprehensive evaluation approach. Our framework offers researchers,\ndevelopers, and policymakers a systematic method for assessing AV behaviour\nbeyond technical performance, fostering the development of AVs that are not\nonly functionally capable but also understandable, acceptable, and safe from a\nhuman perspective.", "AI": {"tldr": "This paper proposes a framework for evaluating automated vehicle (AV) interactions with human-driven vehicles (HDVs) that includes human-centered metrics. A case study demonstrates its application in assessing AV behavior during merging scenarios to improve driver experience.", "motivation": "To address the oversight of human-centered dimensions in evaluating automated vehicle (AV) control algorithms by incorporating metrics from human-robot interaction research.", "method": "The paper introduces a structured evaluation framework that spans four domains: interaction effect, perception, effort, and ability, and applies it in a driving simulator case study of AV and HDV interactions during merging.", "result": "The study found critical differences in driver experience when interacting with AVs, emphasizing the importance of including human-centered metrics in AV evaluations.", "conclusion": "A comprehensive evaluation approach is necessary for understanding AV behavior, promoting the development of AVs that are safe and acceptable from a human perspective.", "key_contributions": ["Proposed an evaluation framework incorporating human-centered metrics for AVs.", "Demonstrated the framework's application in a simulated merging scenario.", "Highlight critical differences in human driver experience with AVs based on the proposed metrics."], "limitations": "", "keywords": ["automated vehicles", "human-robot interaction", "evaluation framework", "drivingSimulator", "human-centered design"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.05282", "pdf": "https://arxiv.org/pdf/2508.05282.pdf", "abs": "https://arxiv.org/abs/2508.05282", "title": "ASCoT: An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs", "authors": ["Dongxu Zhang", "Ning Yang", "Jihua Zhu", "Jinnan Yang", "Miao Xin", "Baoliang Tian"], "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-Thought (CoT) prompting has significantly advanced the reasoning\ncapabilities of Large Language Models (LLMs), yet the reliability of these\nreasoning chains remains a critical challenge. A widely held \"cascading\nfailure\" hypothesis suggests that errors are most detrimental when they occur\nearly in the reasoning process. This paper challenges that assumption through\nsystematic error-injection experiments, revealing a counter-intuitive\nphenomenon we term \"Late-Stage Fragility\": errors introduced in the later\nstages of a CoT chain are significantly more likely to corrupt the final answer\nthan identical errors made at the beginning. To address this specific\nvulnerability, we introduce the Adaptive Self-Correction Chain-of-Thought\n(ASCoT) method. ASCoT employs a modular pipeline in which an Adaptive\nVerification Manager (AVM) operates first, followed by the Multi-Perspective\nSelf-Correction Engine (MSCE). The AVM leverages a Positional Impact Score\nfunction I(k) that assigns different weights based on the position within the\nreasoning chains, addressing the Late-Stage Fragility issue by identifying and\nprioritizing high-risk, late-stage steps. Once these critical steps are\nidentified, the MSCE applies robust, dual-path correction specifically to the\nfailure parts. Extensive experiments on benchmarks such as GSM8K and MATH\ndemonstrate that ASCoT achieves outstanding accuracy, outperforming strong\nbaselines, including standard CoT. Our work underscores the importance of\ndiagnosing specific failure modes in LLM reasoning and advocates for a shift\nfrom uniform verification strategies to adaptive, vulnerability-aware\ncorrection mechanisms.", "AI": {"tldr": "This paper challenges the assumption that early errors in Chain-of-Thought prompting are most detrimental, introducing the concept of 'Late-Stage Fragility' where late errors are more harmful. It proposes the ASCoT method, which includes an Adaptive Verification Manager and a Multi-Perspective Self-Correction Engine to improve LLM reasoning accuracy.", "motivation": "To address the reliability challenge of reasoning chains in Large Language Models, which is critical for their effective application, especially in scenarios requiring high accuracy.", "method": "The paper conducts error-injection experiments to investigate the impact of errors at different stages of Chain-of-Thought reasoning, subsequently proposing the Adaptive Self-Correction Chain-of-Thought (ASCoT) method to enhance error correction.", "result": "ASCoT significantly outperforms standard Chain-of-Thought prompting methods on benchmarks like GSM8K and MATH, demonstrating improved reasoning accuracy.", "conclusion": "The findings highlight the importance of identifying specific failure modes in LLM reasoning and favor a shift towards adaptive correction strategies rather than uniform strategies.", "key_contributions": ["Challenges the 'cascading failure' hypothesis in LLM reasoning", "Introduces the 'Late-Stage Fragility' concept in reasoning chains", "Proposes the ASCoT method for adaptive correction of errors"], "limitations": "", "keywords": ["Chain-of-Thought", "Large Language Models", "Adaptive Self-Correction", "Machine Learning", "Error Analysis"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.05572", "pdf": "https://arxiv.org/pdf/2508.05572.pdf", "abs": "https://arxiv.org/abs/2508.05572", "title": "Discrepancy-Aware Contrastive Adaptation in Medical Time Series Analysis", "authors": ["Yifan Wang", "Hongfeng Ai", "Ruiqi Li", "Maowei Jiang", "Ruiyuan Kang", "Jiahua Dong", "Cheng Jiang", "Chenzhong Li"], "categories": ["cs.HC"], "comment": "10 pages", "summary": "In medical time series disease diagnosis, two key challenges are identified.\nFirst, the high annotation cost of medical data leads to overfitting in models\ntrained on label-limited, single-center datasets. To address this, we propose\nincorporating external data from related tasks and leveraging AE-GAN to extract\nprior knowledge, providing valuable references for downstream tasks. Second,\nmany existing studies employ contrastive learning to derive more generalized\nmedical sequence representations for diagnostic tasks, usually relying on\nmanually designed diverse positive and negative sample pairs. However, these\napproaches are complex, lack generalizability, and fail to adaptively capture\ndisease-specific features across different conditions. To overcome this, we\nintroduce LMCF (Learnable Multi-views Contrastive Framework), a framework that\nintegrates a multi-head attention mechanism and adaptively learns\nrepresentations from different views through inter-view and intra-view\ncontrastive learning strategies. Additionally, the pre-trained AE-GAN is used\nto reconstruct discrepancies in the target data as disease probabilities, which\nare then integrated into the contrastive learning process. Experiments on three\ntarget datasets demonstrate that our method consistently outperforms other\nseven baselines, highlighting its significant impact on healthcare applications\nsuch as the diagnosis of myocardial infarction, Alzheimer's disease, and\nParkinson's disease. We release the source code at xxxxx.", "AI": {"tldr": "The paper introduces the LMCF framework to enhance disease diagnosis in medical time series, addressing annotation costs and representation challenges by integrating contrastive learning and an AE-GAN.", "motivation": "The paper aims to improve medical time series disease diagnosis by addressing high annotation costs and limitations of existing contrastive learning approaches.", "method": "The proposed Learnable Multi-views Contrastive Framework (LMCF) incorporates a multi-head attention mechanism and contrastive learning strategies to adaptively learn representations from diverse data views, leveraging pre-trained AE-GAN for enhancing data quality.", "result": "Experiments show that LMCF outperforms seven baseline methods across three medical datasets, demonstrating its effectiveness in diagnosing conditions like myocardial infarction, Alzheimer's, and Parkinson's disease.", "conclusion": "LMCF significantly enhances the process of disease diagnosis in medical time series, addressing key challenges related to data complexity and representation.", "key_contributions": ["Introduction of LMCF for disease-specific feature learning in medical time series", "Integration of AE-GAN for data reconstruction and enhancing contrastive learning", "Empirical validation on multiple datasets showing superior performance"], "limitations": "", "keywords": ["medical time series", "disease diagnosis", "contrastive learning", "AE-GAN", "healthcare applications"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.05283", "pdf": "https://arxiv.org/pdf/2508.05283.pdf", "abs": "https://arxiv.org/abs/2508.05283", "title": "Decision-Making with Deliberation: Meta-reviewing as a Document-grounded Dialogue", "authors": ["Sukannya Purkayastha", "Nils Dycke", "Anne Lauscher", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": "36 pages, 16 tables, 13 figures", "summary": "Meta-reviewing is a pivotal stage in the peer-review process, serving as the\nfinal step in determining whether a paper is recommended for acceptance. Prior\nresearch on meta-reviewing has treated this as a summarization problem over\nreview reports. However, complementary to this perspective, meta-reviewing is a\ndecision-making process that requires weighing reviewer arguments and placing\nthem within a broader context. Prior research has demonstrated that\ndecision-makers can be effectively assisted in such scenarios via dialogue\nagents. In line with this framing, we explore the practical challenges for\nrealizing dialog agents that can effectively assist meta-reviewers. Concretely,\nwe first address the issue of data scarcity for training dialogue agents by\ngenerating synthetic data using Large Language Models (LLMs) based on a\nself-refinement strategy to improve the relevance of these dialogues to expert\ndomains. Our experiments demonstrate that this method produces higher-quality\nsynthetic data and can serve as a valuable resource towards training\nmeta-reviewing assistants. Subsequently, we utilize this data to train dialogue\nagents tailored for meta-reviewing and find that these agents outperform\n\\emph{off-the-shelf} LLM-based assistants for this task. Finally, we apply our\nagents in real-world meta-reviewing scenarios and confirm their effectiveness\nin enhancing the efficiency of meta-reviewing.\\footnote{Code and Data:\nhttps://github.com/UKPLab/arxiv2025-meta-review-as-dialog", "AI": {"tldr": "This paper explores the development of dialogue agents to assist in the meta-reviewing process of academic papers, leveraging synthetic data generated by LLMs.", "motivation": "Meta-reviewing plays a critical role in the peer-review process, yet there is a lack of effective tools to assist reviewers in decision-making.", "method": "The authors generate synthetic data using Large Language Models to address data scarcity in training dialogue agents, and then train these agents specifically for the meta-reviewing task.", "result": "The study finds that the dialogue agents trained with synthetic data outperform typical LLM-based assistants and enhance efficiency in real-world meta-reviewing scenarios.", "conclusion": "The research indicates the efficacy of leveraging dialogue agents and synthetic data in improving the meta-reviewing process, suggesting a new approach to assist decision-making in peer-review.", "key_contributions": ["Development of dialogue agents specifically for meta-reviewing using synthetic data.", "Improvement in the quality of synthetic data through a self-refinement strategy.", "Demonstration of the effectiveness of these agents in practical scenarios."], "limitations": "The study may be limited by the quality of synthetic data and the generalizability of the results across different domains.", "keywords": ["meta-reviewing", "dialogue agents", "synthetic data", "peer review", "large language models"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2508.05305", "pdf": "https://arxiv.org/pdf/2508.05305.pdf", "abs": "https://arxiv.org/abs/2508.05305", "title": "SONAR-LLM: Autoregressive Transformer that Thinks in Sentence Embeddings and Speaks in Tokens", "authors": ["Nikita Dragunov", "Temurbek Rahmatullaev", "Elizaveta Goncharova", "Andrey Kuznetsov", "Anton Razzhigaev"], "categories": ["cs.CL"], "comment": null, "summary": "The recently proposed Large Concept Model (LCM) generates text by predicting\na sequence of sentence-level embeddings and training with either mean-squared\nerror or diffusion objectives. We present SONAR-LLM, a decoder-only transformer\nthat \"thinks\" in the same continuous SONAR embedding space, yet is supervised\nthrough token-level cross-entropy propagated via the frozen SONAR decoder. This\nhybrid objective retains the semantic abstraction of LCM while eliminating its\ndiffusion sampler and restoring a likelihood-based training signal. Across\nmodel sizes from 39M to 1.3B parameters, SONAR-LLM attains competitive\ngeneration quality. We report scaling trends, ablations, benchmark results, and\nrelease the complete training code and all pretrained checkpoints to foster\nreproducibility and future research.", "AI": {"tldr": "SONAR-LLM is a decoder-only transformer that generates text using token-level cross-entropy while retaining the semantic abstractness of the Large Concept Model (LCM).", "motivation": "To improve text generation quality by retaining semantic abstraction while eliminating diffusion sampling from the LCM architecture.", "method": "SONAR-LLM uses a decoder-only transformer architecture, employing a hybrid objective that combines token-level cross-entropy with a frozen SONAR decoder.", "result": "SONAR-LLM achieves competitive text generation quality across various model sizes (39M to 1.3B parameters).", "conclusion": "The proposed SONAR-LLM architecture not only improves generation quality but also supports reproducibility with released training code and pretrained checkpoints.", "key_contributions": ["Introduces the SONAR-LLM, a novel text generation model leveraging token-level cross-entropy.", "Maintains semantic abstraction while avoiding the limitations of diffusion sampling in LCM.", "Provides open-source resources to aid reproducibility in future research."], "limitations": "", "keywords": ["Large Concept Model", "SONAR-LLM", "text generation", "transformer architecture", "token-level cross-entropy"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.05337", "pdf": "https://arxiv.org/pdf/2508.05337.pdf", "abs": "https://arxiv.org/abs/2508.05337", "title": "Efficient Reasoning for Large Reasoning Language Models via Certainty-Guided Reflection Suppression", "authors": ["Jiameng Huang", "Baijiong Lin", "Guhao Feng", "Jierun Chen", "Di He", "Lu Hou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Technical Report", "summary": "Recent Large Reasoning Language Models (LRLMs) employ long chain-of-thought\nreasoning with complex reflection behaviors, typically signaled by specific\ntrigger words (e.g., \"Wait\" and \"Alternatively\") to enhance performance.\nHowever, these reflection behaviors can lead to the overthinking problem where\nthe generation of redundant reasoning steps that unnecessarily increase token\nusage, raise inference costs, and reduce practical utility. In this paper, we\npropose Certainty-Guided Reflection Suppression (CGRS), a novel method that\nmitigates overthinking in LRLMs while maintaining reasoning accuracy. CGRS\noperates by dynamically suppressing the model's generation of reflection\ntriggers when it exhibits high confidence in its current response, thereby\npreventing redundant reflection cycles without compromising output quality. Our\napproach is model-agnostic, requires no retraining or architectural\nmodifications, and can be integrated seamlessly with existing autoregressive\ngeneration pipelines. Extensive experiments across four reasoning benchmarks\n(i.e., AIME24, AMC23, MATH500, and GPQA-D) demonstrate CGRS's effectiveness: it\nreduces token usage by an average of 18.5% to 41.9% while preserving accuracy.\nIt also achieves the optimal balance between length reduction and performance\ncompared to state-of-the-art baselines. These results hold consistently across\nmodel architectures (e.g., DeepSeek-R1-Distill series, QwQ-32B, and Qwen3\nfamily) and scales (4B to 32B parameters), highlighting CGRS's practical value\nfor efficient reasoning.", "AI": {"tldr": "This paper introduces Certainty-Guided Reflection Suppression (CGRS), a method to reduce overthinking in Large Reasoning Language Models (LRLMs) while maintaining reasoning accuracy.", "motivation": "LRLMs often face the overthinking problem due to redundant reasoning prompted by specific trigger words, leading to increased token usage and reduced practical utility.", "method": "CGRS suppresses the generation of reflection triggers when the model shows high confidence, preventing unnecessary reflection cycles without degrading output quality. It is model-agnostic and does not require retraining or architectural changes.", "result": "CGRS reduces token usage by 18.5% to 41.9% while preserving accuracy across multiple reasoning benchmarks.", "conclusion": "The method effectively balances length reduction and performance, validated across various model architectures and scales, thereby providing a practical solution for efficient reasoning in LRLMs.", "key_contributions": ["Introduction of Certainty-Guided Reflection Suppression (CGRS)", "Demonstrated effectiveness in reducing token usage", "Maintained accuracy without architectural changes"], "limitations": "", "keywords": ["Large Reasoning Language Models", "Reflection Suppression", "Efficient Reasoning", "Token Usage", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.05358", "pdf": "https://arxiv.org/pdf/2508.05358.pdf", "abs": "https://arxiv.org/abs/2508.05358", "title": "Evaluation of a Sign Language Avatar on Comprehensibility, User Experience \\& Acceptability", "authors": ["Fenya Wasserroth", "Eleftherios Avramidis", "Vera Czehmann", "Tanja Kojic", "Fabrizio Nunnari", "Sebastian M√∂ller"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "This paper presents an investigation into the impact of adding adjustment\nfeatures to an existing sign language (SL) avatar on a Microsoft Hololens 2\ndevice. Through a detailed analysis of interactions of expert German Sign\nLanguage (DGS) users with both adjustable and non-adjustable avatars in a\nspecific use case, this study identifies the key factors influencing the\ncomprehensibility, the user experience (UX), and the acceptability of such a\nsystem. Despite user preference for adjustable settings, no significant\nimprovements in UX or comprehensibility were observed, which remained at low\nlevels, amid missing SL elements (mouthings and facial expressions) and\nimplementation issues (indistinct hand shapes, lack of feedback and menu\npositioning). Hedonic quality was rated higher than pragmatic quality,\nindicating that users found the system more emotionally or aesthetically\npleasing than functionally useful. Stress levels were higher for the adjustable\navatar, reflecting lower performance, greater effort and more frustration.\nAdditionally, concerns were raised about whether the Hololens adjustment\ngestures are intuitive and easy to familiarise oneself with. While\nacceptability of the concept of adjustability was generally positive, it was\nstrongly dependent on usability and animation quality. This study highlights\nthat personalisation alone is insufficient, and that SL avatars must be\ncomprehensible by default. Key recommendations include enhancing mouthing and\nfacial animation, improving interaction interfaces, and applying participatory\ndesign.", "AI": {"tldr": "Investigation of sign language avatar adjustments on Hololens 2 shows no significant UX improvement and highlights the need for better comprehensibility and animation quality.", "motivation": "To analyze the impact of adjustable features on sign language avatars and their effect on user experience and comprehensibility.", "method": "Detailed analysis of interactions of expert DGS users with adjustable vs non-adjustable avatars in specific use cases.", "result": "Users preferred adjustable settings, but no significant improvements in UX or comprehensibility were observed, with noted implementation issues and missing SL elements.", "conclusion": "Personalization is not enough; SL avatars must be inherently comprehensible, and design improvements are necessary.", "key_contributions": ["Identification of factors affecting SL avatar usability", "Empirical findings on user preferences and interaction issues", "Recommendations for enhancing SL avatar design"], "limitations": "Low UX and comprehensibility ratings due to implementation issues and missing features.", "keywords": ["sign language avatar", "user experience", "Hololens 2", "adjustable features", "comprehensibility"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2508.05366", "pdf": "https://arxiv.org/pdf/2508.05366.pdf", "abs": "https://arxiv.org/abs/2508.05366", "title": "Can Language Models Critique Themselves? Investigating Self-Feedback for Retrieval Augmented Generation at BioASQ 2025", "authors": ["Samy Ateia", "Udo Kruschwitz"], "categories": ["cs.CL"], "comment": "Version as accepted at the BioASQ Lab at CLEF 2025", "summary": "Agentic Retrieval Augmented Generation (RAG) and 'deep research' systems aim\nto enable autonomous search processes where Large Language Models (LLMs)\niteratively refine outputs. However, applying these systems to domain-specific\nprofessional search, such as biomedical research, presents challenges, as\nautomated systems may reduce user involvement and misalign with expert\ninformation needs. Professional search tasks often demand high levels of user\nexpertise and transparency. The BioASQ CLEF 2025 challenge, using\nexpert-formulated questions, can serve as a platform to study these issues. We\nexplored the performance of current reasoning and nonreasoning LLMs like\nGemini-Flash 2.0, o3-mini, o4-mini and DeepSeek-R1. A key aspect of our\nmethodology was a self-feedback mechanism where LLMs generated, evaluated, and\nthen refined their outputs for query expansion and for multiple answer types\n(yes/no, factoid, list, ideal). We investigated whether this iterative\nself-correction improves performance and if reasoning models are more capable\nof generating useful feedback. Preliminary results indicate varied performance\nfor the self-feedback strategy across models and tasks. This work offers\ninsights into LLM self-correction and informs future work on comparing the\neffectiveness of LLM-generated feedback with direct human expert input in these\nsearch systems.", "AI": {"tldr": "This paper explores the performance of LLMs in autonomous search processes for biomedical research, focusing on a self-feedback mechanism for iterative output refinement.", "motivation": "To address challenges of applying RAG systems in domain-specific professional search, particularly in biomedical research, where high user expertise and transparency are essential.", "method": "The study involved testing reasoning and nonreasoning LLMs (Gemini-Flash 2.0, o3-mini, o4-mini, DeepSeek-R1) using a self-feedback mechanism for query expansion and answer generation across multiple types.", "result": "Preliminary results show varied performance of the self-feedback strategy across different models and tasks, with indications that reasoning models may provide better feedback.", "conclusion": "This research provides insights into LLM self-correction mechanisms and sets the stage for future comparisons between LLM-generated feedback and expert input.", "key_contributions": ["Investigation of LLM performance in biomedical professional search", "Implementation of a self-feedback mechanism in output generation", "Analysis of reasoning vs. non-reasoning LLM capabilities."], "limitations": "", "keywords": ["Agentic Retrieval", "Large Language Models", "Biomedical Research", "Self-feedback Mechanism", "Professional Search"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.05358", "pdf": "https://arxiv.org/pdf/2508.05358.pdf", "abs": "https://arxiv.org/abs/2508.05358", "title": "Evaluation of a Sign Language Avatar on Comprehensibility, User Experience \\& Acceptability", "authors": ["Fenya Wasserroth", "Eleftherios Avramidis", "Vera Czehmann", "Tanja Kojic", "Fabrizio Nunnari", "Sebastian M√∂ller"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "This paper presents an investigation into the impact of adding adjustment\nfeatures to an existing sign language (SL) avatar on a Microsoft Hololens 2\ndevice. Through a detailed analysis of interactions of expert German Sign\nLanguage (DGS) users with both adjustable and non-adjustable avatars in a\nspecific use case, this study identifies the key factors influencing the\ncomprehensibility, the user experience (UX), and the acceptability of such a\nsystem. Despite user preference for adjustable settings, no significant\nimprovements in UX or comprehensibility were observed, which remained at low\nlevels, amid missing SL elements (mouthings and facial expressions) and\nimplementation issues (indistinct hand shapes, lack of feedback and menu\npositioning). Hedonic quality was rated higher than pragmatic quality,\nindicating that users found the system more emotionally or aesthetically\npleasing than functionally useful. Stress levels were higher for the adjustable\navatar, reflecting lower performance, greater effort and more frustration.\nAdditionally, concerns were raised about whether the Hololens adjustment\ngestures are intuitive and easy to familiarise oneself with. While\nacceptability of the concept of adjustability was generally positive, it was\nstrongly dependent on usability and animation quality. This study highlights\nthat personalisation alone is insufficient, and that SL avatars must be\ncomprehensible by default. Key recommendations include enhancing mouthing and\nfacial animation, improving interaction interfaces, and applying participatory\ndesign.", "AI": {"tldr": "Investigation of adjustable sign language avatars on Microsoft Hololens 2 shows no significant UX improvements despite user preference for adjustability.", "motivation": "To analyze the impact of adjustable features on sign language avatars for enhancing user experience and comprehensibility.", "method": "Detailed analysis of interactions of expert German Sign Language users with adjustable and non-adjustable avatars in a specific context.", "result": "Users preferred adjustable avatars but reported low levels of UX and comprehensibility. High stress levels associated with adjustable avatars indicated lower performance and increased frustration.", "conclusion": "Personalization alone is insufficient; comprehensibility of SL avatars must be improved by enhancing mouthing and facial animations and better designing interaction interfaces.", "key_contributions": ["Investigation of SL avatar adjustability in AR settings", "User experience analysis between adjustable and non-adjustable avatars", "Recommendations for improving sign language avatar design"], "limitations": "Issues with missing SL elements and indistinct hand shapes, along with usability concerns regarding adjustment gestures.", "keywords": ["Sign Language Avatars", "User Experience", "Augmented Reality", "Adjustability", "Human-Computer Interaction"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.05374", "pdf": "https://arxiv.org/pdf/2508.05374.pdf", "abs": "https://arxiv.org/abs/2508.05374", "title": "The TUB Sign Language Corpus Collection", "authors": ["Eleftherios Avramidis", "Vera Czehmann", "Fabian Deckert", "Lorenz Hufe", "Aljoscha Lipski", "Yuni Amaloa Quintero Villalobos", "Tae Kwon Rhee", "Mengqian Shi", "Lennart St√∂lting", "Fabrizio Nunnari", "Sebastian M√∂ller"], "categories": ["cs.CL"], "comment": null, "summary": "We present a collection of parallel corpora of 12 sign languages in video\nformat, together with subtitles in the dominant spoken languages of the\ncorresponding countries. The entire collection includes more than 1,300 hours\nin 4,381 video files, accompanied by 1,3~M subtitles containing 14~M tokens.\nMost notably, it includes the first consistent parallel corpora for 8 Latin\nAmerican sign languages, whereas the size of the German Sign Language corpora\nis ten times the size of the previously available corpora. The collection was\ncreated by collecting and processing videos of multiple sign languages from\nvarious online sources, mainly broadcast material of news shows, governmental\nbodies and educational channels. The preparation involved several stages,\nincluding data collection, informing the content creators and seeking usage\napprovals, scraping, and cropping. The paper provides statistics on the\ncollection and an overview of the methods used to collect the data.", "AI": {"tldr": "This paper presents a large collection of parallel corpora for 12 sign languages, featuring over 1,300 hours of video and extensive subtitles, significantly advancing resources in this field.", "motivation": "To enhance the availability and consistency of parallel corpora for sign languages, particularly for research and applications in sign language processing and interpretation.", "method": "The collection was created through systematic data collection from various online sources, involving video scraping, cropping, and subtitle generation with a multi-stage preparation process.", "result": "The final collection consists of 1,300 hours of video across 4,381 files, paired with 1.3 million subtitles containing 14 million tokens, including significant contributions to Latin American sign languages and a substantial German Sign Language corpus.", "conclusion": "This resource is a crucial step towards improving sign language data availability for research and practical applications.", "key_contributions": ["First consistent parallel corpora for 8 Latin American sign languages", "German Sign Language corpus is significantly larger than previous datasets", "Comprehensive statistics and methods for data collection presented"], "limitations": "", "keywords": ["sign languages", "parallel corpora", "video data", "language processing", "data collection"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2508.05429", "pdf": "https://arxiv.org/pdf/2508.05429.pdf", "abs": "https://arxiv.org/abs/2508.05429", "title": "MyCulture: Exploring Malaysia's Diverse Culture under Low-Resource Language Constraints", "authors": ["Zhong Ken Hew", "Jia Xin Low", "Sze Jue Yang", "Chee Seng chan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often exhibit cultural biases due to training\ndata dominated by high-resource languages like English and Chinese. This poses\nchallenges for accurately representing and evaluating diverse cultural\ncontexts, particularly in low-resource language settings. To address this, we\nintroduce MyCulture, a benchmark designed to comprehensively evaluate LLMs on\nMalaysian culture across six pillars: arts, attire, customs, entertainment,\nfood, and religion presented in Bahasa Melayu. Unlike conventional benchmarks,\nMyCulture employs a novel open-ended multiple-choice question format without\npredefined options, thereby reducing guessing and mitigating format bias. We\nprovide a theoretical justification for the effectiveness of this open-ended\nstructure in improving both fairness and discriminative power. Furthermore, we\nanalyze structural bias by comparing model performance on structured versus\nfree-form outputs, and assess language bias through multilingual prompt\nvariations. Our evaluation across a range of regional and international LLMs\nreveals significant disparities in cultural comprehension, highlighting the\nurgent need for culturally grounded and linguistically inclusive benchmarks in\nthe development and assessment of LLMs.", "AI": {"tldr": "Introducing MyCulture, a new benchmark for evaluating LLMs on Malaysian cultural understanding.", "motivation": "To address cultural biases in LLMs due to the prevalence of training data from high-resource languages.", "method": "MyCulture uses open-ended multiple-choice questions without predefined options to assess LLMs on six cultural pillars in Bahasa Melayu.", "result": "Significant disparities in cultural comprehension were found across LLMs when evaluated with MyCulture, exposing the limitations of conventional benchmarks.", "conclusion": "There is an urgent need for culturally grounded benchmarks to improve fairness and inclusivity in LLM evaluations.", "key_contributions": ["MyCulture benchmark assesses cultural understanding of LLMs in low-resource settings.", "Innovative open-ended question format reduces guessing and bias.", "Highlights disparities in LLM cultural comprehension across different languages."], "limitations": "", "keywords": ["Large Language Models", "Cultural Bias", "Benchmarking", "Malaysian Culture", "Open-ended Questions"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.05452", "pdf": "https://arxiv.org/pdf/2508.05452.pdf", "abs": "https://arxiv.org/abs/2508.05452", "title": "LLMEval-3: A Large-Scale Longitudinal Study on Robust and Fair Evaluation of Large Language Models", "authors": ["Ming Zhang", "Yujiong Shen", "Jingyi Deng", "Yuhui Wang", "Yue Zhang", "Junzhe Wang", "Shichun Liu", "Shihan Dou", "Huayu Sha", "Qiyuan Peng", "Changhao Jiang", "Jingqi Tong", "Yilong Wu", "Zhihao Zhang", "Mingqi Wu", "Zhiheng Xi", "Mingxu Chai", "Tao Liang", "Zhihui Fei", "Zhen Wang", "Mingyang Wan", "Guojun Ma", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Existing evaluation of Large Language Models (LLMs) on static benchmarks is\nvulnerable to data contamination and leaderboard overfitting, critical issues\nthat obscure true model capabilities. To address this, we introduce LLMEval-3,\na framework for dynamic evaluation of LLMs. LLMEval-3 is built on a proprietary\nbank of 220k graduate-level questions, from which it dynamically samples unseen\ntest sets for each evaluation run. Its automated pipeline ensures integrity via\ncontamination-resistant data curation, a novel anti-cheating architecture, and\na calibrated LLM-as-a-judge process achieving 90% agreement with human experts,\ncomplemented by a relative ranking system for fair comparison. An 20-month\nlongitudinal study of nearly 50 leading models reveals a performance ceiling on\nknowledge memorization and exposes data contamination vulnerabilities\nundetectable by static benchmarks. The framework demonstrates exceptional\nrobustness in ranking stability and consistency, providing strong empirical\nvalidation for the dynamic evaluation paradigm. LLMEval-3 offers a robust and\ncredible methodology for assessing the true capabilities of LLMs beyond\nleaderboard scores, promoting the development of more trustworthy evaluation\nstandards.", "AI": {"tldr": "LLMEval-3 introduces a dynamic evaluation framework for Large Language Models (LLMs) that addresses issues like data contamination and leaderboard overfitting, using a proprietary bank of questions and automated methodologies to ensure integrity and robust ranking.", "motivation": "To mitigate the vulnerabilities in current static evaluations of LLMs that lead to misrepresented model capabilities and leaderboards.", "method": "LLMEval-3 utilizes a dynamic sampling method from a proprietary bank of 220k graduate-level questions, employing automated data curation and evaluation processes to maintain integrity.", "result": "A longitudinal study of nearly 50 leading models identified a performance ceiling in knowledge memorization and highlighted data contamination issues, proving that static benchmarks can miss critical vulnerabilities.", "conclusion": "LLMEval-3 establishes a more credible methodology for LLM evaluation, advancing the standards for assessing AI model performance beyond traditional leaderboard metrics.", "key_contributions": ["Dynamic evaluation framework for LLMs", "Robust anti-cheating architecture", "Automated contamination-resistant data curation"], "limitations": "", "keywords": ["Large Language Models", "evaluation", "dynamic assessment", "data contamination", "robustness"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2310.05853", "pdf": "https://arxiv.org/pdf/2310.05853.pdf", "abs": "https://arxiv.org/abs/2310.05853", "title": "\"Mango Mango, How to Let The Lettuce Dry Without A Spinner?\": Exploring User Perceptions of Using An LLM-Based Conversational Assistant Toward Cooking Partner", "authors": ["Szeyi Chan", "Jiachen Li", "Bingsheng Yao", "Amama Mahmood", "Chien-Ming Huang", "Holly Jimison", "Elizabeth D Mynatt", "Dakuo Wang"], "categories": ["cs.HC"], "comment": "To appear at CSCW 2025", "summary": "The rapid advancement of Large Language Models (LLMs) has created numerous\npotentials for integration with conversational assistants (CAs) assisting\npeople in their daily tasks, particularly due to their extensive flexibility.\nHowever, users' real-world experiences interacting with these assistants remain\nunexplored. In this research, we chose cooking, a complex daily task, as a\nscenario to explore people's successful and unsatisfactory experiences while\nreceiving assistance from an LLM-based CA, Mango Mango. We discovered that\nparticipants value the system's ability to offer customized instructions based\non context, provide extensive information beyond the recipe, and assist them in\ndynamic task planning. However, users expect the system to be more adaptive to\noral conversation and provide more suggestive responses to keep them actively\ninvolved. Recognizing that users began treating our LLM-CA as a personal\nassistant or even a partner rather than just a recipe-reading tool, we propose\nfive design considerations for future development.", "AI": {"tldr": "This research explores user experiences with a Large Language Model-based conversational assistant in cooking scenarios, identifying key expectations and preferences for future development.", "motivation": "To investigate the real-world interactions of users with Large Language Model-based conversational assistants, focusing on cooking tasks as a complex and everyday scenario.", "method": "The study involved participants interacting with the LLM-based conversational assistant, Mango Mango, while cooking, allowing for the collection of both successful and unsatisfactory experiences.", "result": "Participants appreciated the system's ability to provide customized instructions, extensive information beyond recipes, and support for dynamic task planning, but desired better conversational adaptability and more suggestive responses.", "conclusion": "Users began to see the LLM-CA as a partner rather than just a tool, leading to the proposal of five design considerations for enhancing conversational assistants.", "key_contributions": ["Identification of user preferences in LLM interaction during cooking tasks", "Introduction of design considerations for improving LLM-CA responsiveness", "Insights into user perception of LLMs as personal assistants rather than just tools"], "limitations": "", "keywords": ["Large Language Models", "Conversational Assistants", "User Experience", "Human-Computer Interaction", "Cooking"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.05468", "pdf": "https://arxiv.org/pdf/2508.05468.pdf", "abs": "https://arxiv.org/abs/2508.05468", "title": "TASE: Token Awareness and Structured Evaluation for Multilingual Language Models", "authors": ["Chenzhuo Zhao", "Xinda Wang", "Yue Huang", "Junting Lu", "Ziqian Liu"], "categories": ["cs.CL"], "comment": null, "summary": "While large language models (LLMs) have demonstrated remarkable performance\non high-level semantic tasks, they often struggle with fine-grained,\ntoken-level understanding and structural reasoning--capabilities that are\nessential for applications requiring precision and control. We introduce TASE,\na comprehensive benchmark designed to evaluate LLMs' ability to perceive and\nreason about token-level information across languages. TASE covers 10 tasks\nunder two core categories: token awareness and structural understanding,\nspanning Chinese, English, and Korean, with a 35,927-instance evaluation set\nand a scalable synthetic data generation pipeline for training. Tasks include\ncharacter counting, token alignment, syntactic structure parsing, and length\nconstraint satisfaction. We evaluate over 30 leading commercial and open-source\nLLMs, including O3, Claude 4, Gemini 2.5 Pro, and DeepSeek-R1, and train a\ncustom Qwen2.5-14B model using the GRPO training method. Results show that\nhuman performance significantly outpaces current LLMs, revealing persistent\nweaknesses in token-level reasoning. TASE sheds light on these limitations and\nprovides a new diagnostic lens for future improvements in low-level language\nunderstanding and cross-lingual generalization. Our code and dataset are\npublicly available at https://github.com/cyzcz/Tase .", "AI": {"tldr": "TASE is a benchmark to evaluate large language models' token-level understanding and structural reasoning across multiple languages.", "motivation": "To address the limitations of large language models in token-level understanding and structural reasoning which are crucial for applications requiring precision.", "method": "Introduction of TASE, covering 10 tasks in token awareness and structural understanding across Chinese, English, and Korean with a scalable synthetic data generation pipeline.", "result": "Evaluation of over 30 leading LLMs showed that human performance significantly surpasses that of current models, indicating persistent weaknesses in token-level reasoning.", "conclusion": "TASE highlights weaknesses in LLMs and provides a framework for future improvements in language understanding and cross-lingual generalization.", "key_contributions": ["Introduction of the TASE benchmark for evaluating LLMs' token-level abilities.", "In-depth analysis of LLMs' performance on structural reasoning tasks.", "Public availability of the benchmark dataset and code."], "limitations": "Current models still show significant weaknesses compared to human performance in token-level reasoning tasks.", "keywords": ["large language models", "token-level understanding", "benchmark", "structural reasoning", "cross-lingual generalization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.01325", "pdf": "https://arxiv.org/pdf/2502.01325.pdf", "abs": "https://arxiv.org/abs/2502.01325", "title": "The Homework Wars: Exploring Emotions, Behaviours, and Conflicts in Parent-Child Homework Interactions", "authors": ["Nan Gao", "Yibin Liu", "Xin Tang", "Yanyan Liu", "Chun Yu", "Yun Huang", "Yuntao Wang", "Flora D. Salim", "Xuhai Orson Xu", "Jun Wei", "Yuanchun Shi"], "categories": ["cs.HC"], "comment": "This paper was accepted by Proceedings of the ACM on Interactive,\n  Mobile, Wearable and Ubiquitous Technologies (IMWUT)", "summary": "Parental involvement in homework is a crucial aspect of family education, but\nit often triggers emotional strain and conflicts. Despite growing concern over\nits impact on family well-being, prior research has lacked access to\nfine-grained, real-time dynamics of these interactions. To bridge this gap, we\npresent a framework that leverages naturalistic parent-child interaction data\nand large language models (LLMs) to analyse homework conversations at scale. In\na four-week in situ study with 78 Chinese families, we collected 475 hours of\naudio recordings and accompanying daily surveys, capturing 602 homework\nsessions in everyday home settings. Our LLM-based pipeline reliably extracted\nand categorised parental behaviours and conflict patterns from transcribed\nconversations, achieving high agreement with expert annotations. The analysis\nrevealed significant emotional shifts in parents before and after homework, 18\nrecurring parental behaviours and seven common conflict types, with Knowledge\nConflict being the most frequent. Notably, even well-intentioned behaviours\nwere significantly positively correlated with specific conflicts. This work\nadvances ubiquitous computing methods for studying complex family dynamics and\noffers empirical insights to enrich family education theory and inform more\neffective parenting strategies and interventions in the future.", "AI": {"tldr": "The paper presents a framework utilizing naturalistic parent-child interaction data and large language models to analyze homework conversations, revealing emotional dynamics and conflict patterns.", "motivation": "To better understand the emotional and conflict dynamics of parental involvement in homework and its effects on family well-being.", "method": "A four-week in situ study with 78 Chinese families, collecting 475 hours of audio recordings and daily surveys, which were analyzed using a large language model-based pipeline to extract parental behaviors and conflict types from homework sessions.", "result": "The study identified significant emotional shifts in parents, 18 recurring behaviors, and seven common conflict types, with a particular focus on the frequent occurrence of Knowledge Conflict.", "conclusion": "This research enhances understanding of family dynamics in educational contexts and suggests avenues for more effective parenting strategies based on empirical insights.", "key_contributions": ["Development of a framework for analyzing parent-child interactions using LLMs", "Identification of common conflict types in homework situations", "Empirical insights into emotional dynamics linked to parental behaviors"], "limitations": "", "keywords": ["parental involvement", "homework", "large language models", "family dynamics", "conflict patterns"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.05470", "pdf": "https://arxiv.org/pdf/2508.05470.pdf", "abs": "https://arxiv.org/abs/2508.05470", "title": "Rethinking Creativity Evaluation: A Critical Analysis of Existing Creativity Evaluations", "authors": ["Li-Chun Lu", "Miri Liu", "Pin-Chun Lu", "Yufei Tian", "Shao-Hua Sun", "Nanyun Peng"], "categories": ["cs.CL"], "comment": "15 pages, 6 figures", "summary": "We systematically examine, analyze, and compare representative creativity\nmeasures--creativity index, perplexity, syntactic templates, and\nLLM-as-a-Judge--across diverse creative domains, including creative writing,\nunconventional problem-solving, and research ideation. Our analyses reveal that\nthese metrics exhibit limited consistency, capturing different dimensions of\ncreativity. We highlight key limitations, including the creativity index's\nfocus on lexical diversity, perplexity's sensitivity to model confidence, and\nsyntactic templates' inability to capture conceptual creativity. Additionally,\nLLM-as-a-Judge shows instability and bias. Our findings underscore the need for\nmore robust, generalizable evaluation frameworks that better align with human\njudgments of creativity.", "AI": {"tldr": "This paper analyzes and compares various creativity measures across different domains, revealing their inconsistencies and limitations, and calls for improved evaluation frameworks.", "motivation": "To understand the effectiveness of different creativity measures in capturing diverse aspects of creativity and to identify their limitations.", "method": "A systematic examination and comparison of creativity measures: creativity index, perplexity, syntactic templates, and LLM-as-a-Judge across diverse creative domains.", "result": "The analysis shows that these creativity measures demonstrate limited consistency and capture different dimensions of creativity, highlighting specific weaknesses in each approach.", "conclusion": "There is a pressing need for the development of more robust and generalizable evaluation frameworks that align better with human judgments of creativity.", "key_contributions": ["Comparison of various creativity metrics across domains", "Identification of specific limitations of existing metrics", "Call for improved evaluation frameworks for creativity"], "limitations": "Measures exhibit instability, bias, and focus on narrow aspects of creativity, such as lexical diversity.", "keywords": ["creativity measures", "creativity index", "LLM-as-a-Judge", "evaluation frameworks", "human creativity"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.05509", "pdf": "https://arxiv.org/pdf/2508.05509.pdf", "abs": "https://arxiv.org/abs/2508.05509", "title": "LAG: Logic-Augmented Generation from a Cartesian Perspective", "authors": ["Yilin Xiao", "Chuang Zhou", "Qinggang Zhang", "Su Dong", "Shengyuan Chen", "Xiao Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet exhibit critical limitations in knowledge-intensive\ntasks, often generating hallucinations when faced with questions requiring\nspecialized expertise. While retrieval-augmented generation (RAG) mitigates\nthis by integrating external knowledge, it struggles with complex reasoning\nscenarios due to its reliance on direct semantic retrieval and lack of\nstructured logical organization. Inspired by Cartesian principles from\n\\textit{Discours de la m\\'ethode}, this paper introduces Logic-Augmented\nGeneration (LAG), a novel paradigm that reframes knowledge augmentation through\nsystematic question decomposition and dependency-aware reasoning. Specifically,\nLAG first decomposes complex questions into atomic sub-questions ordered by\nlogical dependencies. It then resolves these sequentially, using prior answers\nto guide context retrieval for subsequent sub-questions, ensuring stepwise\ngrounding in logical chain. To prevent error propagation, LAG incorporates a\nlogical termination mechanism that halts inference upon encountering\nunanswerable sub-questions and reduces wasted computation on excessive\nreasoning. Finally, it synthesizes all sub-resolutions to generate verified\nresponses. Experiments on four benchmark datasets demonstrate that LAG\nsignificantly enhances reasoning robustness, reduces hallucination, and aligns\nLLM problem-solving with human cognition, offering a principled alternative to\nexisting RAG systems.", "AI": {"tldr": "The paper introduces Logic-Augmented Generation (LAG), a new approach that improves knowledge augmentation in LLMs by systematically decomposing complex questions and enhancing reasoning capabilities, addressing limitations in retrieval-augmented generation methods.", "motivation": "Address the limitations of large language models in knowledge-intensive tasks and reduce hallucinations during complex reasoning.", "method": "LAG decomposes complex questions into atomic sub-questions based on logical dependencies, resolves them sequentially while guiding context retrieval, and incorporates a logical termination mechanism to prevent error propagation.", "result": "LAG significantly enhances reasoning robustness and reduces hallucinations, aligning LLM problem-solving with human cognition based on experiments on four benchmark datasets.", "conclusion": "LAG offers a principled alternative to existing retrieval-augmented generation methods, improving the reliability of responses from LLMs.", "key_contributions": ["Introduction of the Logic-Augmented Generation paradigm", "Systematic question decomposition and dependency-aware reasoning", "Logical termination mechanism to prevent error propagation"], "limitations": "", "keywords": ["Logic-Augmented Generation", "Large language models", "Knowledge augmentation", "Reasoning robustness", "Hallucination reduction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.05525", "pdf": "https://arxiv.org/pdf/2508.05525.pdf", "abs": "https://arxiv.org/abs/2508.05525", "title": "The World According to LLMs: How Geographic Origin Influences LLMs' Entity Deduction Capabilities", "authors": ["Harsh Nishant Lalai", "Raj Sanjay Shah", "Jiaxin Pei", "Sashank Varma", "Yi-Chia Wang", "Ali Emami"], "categories": ["cs.CL", "cs.AI"], "comment": "Conference on Language Modeling 2025", "summary": "Large Language Models (LLMs) have been extensively tuned to mitigate explicit\nbiases, yet they often exhibit subtle implicit biases rooted in their\npre-training data. Rather than directly probing LLMs with human-crafted\nquestions that may trigger guardrails, we propose studying how models behave\nwhen they proactively ask questions themselves. The 20 Questions game, a\nmulti-turn deduction task, serves as an ideal testbed for this purpose. We\nsystematically evaluate geographic performance disparities in entity deduction\nusing a new dataset, Geo20Q+, consisting of both notable people and culturally\nsignificant objects (e.g., foods, landmarks, animals) from diverse regions. We\ntest popular LLMs across two gameplay configurations (canonical 20-question and\nunlimited turns) and in seven languages (English, Hindi, Mandarin, Japanese,\nFrench, Spanish, and Turkish). Our results reveal geographic disparities: LLMs\nare substantially more successful at deducing entities from the Global North\nthan the Global South, and the Global West than the Global East. While\nWikipedia pageviews and pre-training corpus frequency correlate mildly with\nperformance, they fail to fully explain these disparities. Notably, the\nlanguage in which the game is played has minimal impact on performance gaps.\nThese findings demonstrate the value of creative, free-form evaluation\nframeworks for uncovering subtle biases in LLMs that remain hidden in standard\nprompting setups. By analyzing how models initiate and pursue reasoning goals\nover multiple turns, we find geographic and cultural disparities embedded in\ntheir reasoning processes. We release the dataset (Geo20Q+) and code at\nhttps://sites.google.com/view/llmbias20q/home.", "AI": {"tldr": "This paper investigates subtle implicit biases in Large Language Models (LLMs) by analyzing their performance in a multi-turn game called 20 Questions, revealing geographic disparities in entity deduction.", "motivation": "The study aims to explore the implicit biases in LLMs that are often overlooked in conventional evaluations, particularly focusing on geographic performance in a multi-turn context.", "method": "The authors propose using the 20 Questions game as a creative evaluation framework, testing various LLMs on a new dataset, Geo20Q+, which includes entities from diverse regions and evaluates performance in multiple languages.", "result": "The evaluation reveals that LLMs perform significantly better in recognizing entities from the Global North compared to the Global South, with mild correlations observed with Wikipedia pageviews and corpus frequency but insufficient to explain the disparities.", "conclusion": "This study highlights the importance of nuanced evaluation methods to uncover biases in LLMs, providing insights into their reasoning processes and the geographic and cultural biases that may be present.", "key_contributions": ["Introduction of Geo20Q+, a dataset to evaluate LLMs in a multi-turn game context.", "Demonstration of geographic disparities in LLM performance that are not evident in standard evaluations.", "Release of tools and dataset for further research into LLM biases."], "limitations": "The paper primarily addresses geographic disparities; other forms of biases may not be explored and the findings may not generalize across all LLMs.", "keywords": ["Large Language Models", "implicit bias", "20 Questions game", "Geo20Q+", "geographic disparities"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.05534", "pdf": "https://arxiv.org/pdf/2508.05534.pdf", "abs": "https://arxiv.org/abs/2508.05534", "title": "CoCoLex: Confidence-guided Copy-based Decoding for Grounded Legal Text Generation", "authors": ["Santosh T. Y. S. S", "Youssef Tarek Elkhayat", "Oana Ichim", "Pranav Shetty", "Dongsheng Wang", "Zhiqiang Ma", "Armineh Nourbakhsh", "Xiaomo Liu"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025-Main Conference", "summary": "Due to their ability to process long and complex contexts, LLMs can offer key\nbenefits to the Legal domain, but their adoption has been hindered by their\ntendency to generate unfaithful, ungrounded, or hallucinatory outputs. While\nRetrieval-Augmented Generation offers a promising solution by grounding\ngenerations in external knowledge, it offers no guarantee that the provided\ncontext will be effectively integrated. To address this, context-aware decoding\nstrategies have been proposed to amplify the influence of relevant context, but\nthey usually do not explicitly enforce faithfulness to the context. In this\nwork, we introduce Confidence-guided Copy-based Decoding for Legal Text\nGeneration (CoCoLex)-a decoding strategy that dynamically interpolates the\nmodel produced vocabulary distribution with a distribution derived based on\ncopying from the context. CoCoLex encourages direct copying based on the\nmodel's confidence, ensuring greater fidelity to the source. Experimental\nresults on five legal benchmarks demonstrate that CoCoLex outperforms existing\ncontext-aware decoding methods, particularly in long-form generation tasks.", "AI": {"tldr": "CoCoLex is a decoding strategy for legal text generation that improves fidelity to context by combining model confidence with context-aware copying.", "motivation": "LLMs in the Legal domain face challenges such as generating unfaithful outputs, hindering their adoption.", "method": "CoCoLex uses a confidence-guided mechanism that interpolates model vocabulary distribution with context-derived distributions to enhance fidelity.", "result": "Experimental results show that CoCoLex outperforms existing decoding methods, especially in long-form generation tasks on five legal benchmarks.", "conclusion": "The proposed method ensures greater fidelity to the context, making it a viable solution for legal text generation challenges.", "key_contributions": ["Introduction of Confidence-guided Copy-based Decoding (CoCoLex) for legal text generation", "Demonstration of improved performance over existing context-aware methods", "Focus on ensuring context fidelity in LLM outputs"], "limitations": "", "keywords": ["Legal Text Generation", "Confidence-guided Decoding", "Retrieval-Augmented Generation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.05544", "pdf": "https://arxiv.org/pdf/2508.05544.pdf", "abs": "https://arxiv.org/abs/2508.05544", "title": "Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees", "authors": ["Guang Yang", "Xinyang Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "under review", "summary": "Large Language Models (LLMs) have shown remarkable progress in\nmultiple-choice question answering (MCQA), but their inherent unreliability,\nsuch as hallucination and overconfidence, limits their application in high-risk\ndomains. To address this, we propose a frequency-based uncertainty\nquantification method under black-box settings, leveraging conformal prediction\n(CP) to ensure provable coverage guarantees. Our approach involves multiple\nindependent samplings of the model's output distribution for each input, with\nthe most frequent sample serving as a reference to calculate predictive entropy\n(PE). Experimental evaluations across six LLMs and four datasets (MedMCQA,\nMedQA, MMLU, MMLU-Pro) demonstrate that frequency-based PE outperforms\nlogit-based PE in distinguishing between correct and incorrect predictions, as\nmeasured by AUROC. Furthermore, the method effectively controls the empirical\nmiscoverage rate under user-specified risk levels, validating that sampling\nfrequency can serve as a viable substitute for logit-based probabilities in\nblack-box scenarios. This work provides a distribution-free model-agnostic\nframework for reliable uncertainty quantification in MCQA with guaranteed\ncoverage, enhancing the trustworthiness of LLMs in practical applications.", "AI": {"tldr": "This paper presents a frequency-based uncertainty quantification method for large language models in multiple-choice question answering, addressing inherent unreliability and enhancing trustworthiness through a novel framework.", "motivation": "Address the unreliability of large language models in high-risk domains, particularly in multiple-choice question answering where hallucination and overconfidence pose significant challenges.", "method": "The proposed method uses multiple independent samplings of the model's output distribution to calculate predictive entropy (PE), leveraging conformal prediction to ensure provable coverage guarantees.", "result": "Experimental evaluations show that frequency-based predictive entropy outperforms logit-based predictive entropy in distinguishing between correct and incorrect predictions, achieving better AUROC scores.", "conclusion": "The study establishes a distribution-free, model-agnostic framework for reliable uncertainty quantification in multiple-choice question answering, enhancing the trustworthiness of LLMs in various applications.", "key_contributions": ["Frequency-based uncertainty quantification method", "Model-agnostic framework for reliable uncertainty quantification", "Demonstrated effectiveness in black-box settings with empirical miscoverage control"], "limitations": "", "keywords": ["Large Language Models", "Uncertainty Quantification", "Multiple-Choice Question Answering", "Conformal Prediction", "Predictive Entropy"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2508.05553", "pdf": "https://arxiv.org/pdf/2508.05553.pdf", "abs": "https://arxiv.org/abs/2508.05553", "title": "Do Political Opinions Transfer Between Western Languages? An Analysis of Unaligned and Aligned Multilingual LLMs", "authors": ["Franziska Weeber", "Tanise Ceron", "Sebastian Pad√≥"], "categories": ["cs.CL", "cs.CY", "I.2.7; J.4"], "comment": null, "summary": "Public opinion surveys show cross-cultural differences in political opinions\nbetween socio-cultural contexts. However, there is no clear evidence whether\nthese differences translate to cross-lingual differences in multilingual large\nlanguage models (MLLMs). We analyze whether opinions transfer between languages\nor whether there are separate opinions for each language in MLLMs of various\nsizes across five Western languages. We evaluate MLLMs' opinions by prompting\nthem to report their (dis)agreement with political statements from voting\nadvice applications. To better understand the interaction between languages in\nthe models, we evaluate them both before and after aligning them with more left\nor right views using direct preference optimization and English alignment data\nonly. Our findings reveal that unaligned models show only very few significant\ncross-lingual differences in the political opinions they reflect. The political\nalignment shifts opinions almost uniformly across all five languages. We\nconclude that in Western language contexts, political opinions transfer between\nlanguages, demonstrating the challenges in achieving explicit socio-linguistic,\ncultural, and political alignment of MLLMs.", "AI": {"tldr": "This paper investigates whether political opinions in multilingual large language models (MLLMs) differ across languages and if opinions transfer between them.", "motivation": "To examine cross-lingual differences in political opinions in MLLMs and understand the effects of language alignment.", "method": "The authors prompt various MLLMs in five Western languages with political statements from voting advice applications to assess their (dis)agreement.", "result": "Unaligned MLLMs show minimal significant cross-lingual differences; however, political alignment influences opinions uniformly across all languages.", "conclusion": "Political opinions in MLLMs tend to transfer across languages, highlighting the difficulties in aligning MLLMs with socio-linguistic, cultural, and political contexts.", "key_contributions": ["Analysis of cross-lingual opinion transfer in MLLMs", "Demonstrated effects of political alignment on language models", "Study of multilingual political sentiment representation"], "limitations": "Focus on Western languages may not generalize to non-Western contexts and diverse socio-political settings.", "keywords": ["multilingual large language models", "political opinions", "cross-lingual transfer", "alignment", "socio-linguistic dynamics"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2508.05592", "pdf": "https://arxiv.org/pdf/2508.05592.pdf", "abs": "https://arxiv.org/abs/2508.05592", "title": "MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging Synthetic Problems with a Reinforced Policy", "authors": ["Shaoxiong Zhan", "Yanlin Lai", "Ziyu Lu", "Dahua Lin", "Ziqing Yang", "Fei Tang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models have achieved substantial progress in mathematical\nreasoning, yet their advancement is limited by the scarcity of high-quality,\nhigh-difficulty training data. Existing synthesis methods largely rely on\ntransforming human-written templates, limiting both diversity and scalability.\nWe propose MathSmith, a novel framework for synthesizing challenging\nmathematical problems to enhance LLM reasoning. Rather than modifying existing\nproblems, MathSmith constructs new ones from scratch by randomly sampling\nconcept-explanation pairs from PlanetMath, ensuring data independence and\navoiding contamination. To increase difficulty, we design nine predefined\nstrategies as soft constraints during rationales. We further adopts\nreinforcement learning to jointly optimize structural validity, reasoning\ncomplexity, and answer consistency. The length of the reasoning trace generated\nunder autoregressive prompting is used to reflect cognitive complexity,\nencouraging the creation of more demanding problems aligned with\nlong-chain-of-thought reasoning. Experiments across five benchmarks,\ncategorized as easy & medium (GSM8K, MATH-500) and hard (AIME2024, AIME2025,\nOlympiadBench), show that MathSmith consistently outperforms existing baselines\nunder both short and long CoT settings. Additionally, a weakness-focused\nvariant generation module enables targeted improvement on specific concepts.\nOverall, MathSmith exhibits strong scalability, generalization, and\ntransferability, highlighting the promise of high-difficulty synthetic data in\nadvancing LLM reasoning capabilities.", "AI": {"tldr": "MathSmith is a framework for synthesizing challenging mathematical problems, aimed at improving the reasoning capabilities of large language models (LLMs).", "motivation": "To address the limitations in quality and diversity of training data for mathematical reasoning in LLMs.", "method": "MathSmith constructs new mathematical problems from scratch using concept-explanation pairs, applies nine predefined strategies to increase difficulty, and utilizes reinforcement learning to optimize for structural validity and reasoning complexity.", "result": "MathSmith consistently outperforms existing baselines in generating mathematical problems across various difficulty benchmarks and shows strong scalability.", "conclusion": "High-difficulty synthetic data can significantly enhance LLM reasoning capabilities.", "key_contributions": ["Development of MathSmith framework for problem synthesis", "Implementation of reinforcement learning for optimization", "Demonstrated scalability and effectiveness across multiple benchmarks"], "limitations": "", "keywords": ["large language models", "mathematical reasoning", "synthetic data", "reinforcement learning", "problem generation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.05613", "pdf": "https://arxiv.org/pdf/2508.05613.pdf", "abs": "https://arxiv.org/abs/2508.05613", "title": "Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models", "authors": ["Haitao Hong", "Yuchen Yan", "Xingyu Wu", "Guiyang Hou", "Wenqi Zhang", "Weiming Lu", "Yongliang Shen", "Jun Xiao"], "categories": ["cs.CL", "cs.AI"], "comment": "Project Page: https://zju-real.github.io/cooper Code:\n  https://github.com/zju-real/cooper", "summary": "Large language models (LLMs) have demonstrated remarkable performance in\nreasoning tasks, where reinforcement learning (RL) serves as a key algorithm\nfor enhancing their reasoning capabilities. Currently, there are two mainstream\nreward paradigms: model-based rewards and rule-based rewards. However, both\napproaches suffer from limitations: rule-based rewards lack robustness, while\nmodel-based rewards are vulnerable to reward hacking. To address these issues,\nwe propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework\nthat jointly optimizes both the policy model and the reward model. Cooper\nleverages the high precision of rule-based rewards when identifying correct\nresponses, and dynamically constructs and selects positive-negative sample\npairs for continued training the reward model. This design enhances robustness\nand mitigates the risk of reward hacking. To further support Cooper, we\nintroduce a hybrid annotation strategy that efficiently and accurately\ngenerates training data for the reward model. We also propose a reference-based\nreward modeling paradigm, where the reward model takes a reference answer as\ninput. Based on this design, we train a reward model named VerifyRM, which\nachieves higher accuracy on VerifyBench compared to other models of the same\nsize. We conduct reinforcement learning using both VerifyRM and Cooper. Our\nexperiments show that Cooper not only alleviates reward hacking but also\nimproves end-to-end RL performance, for instance, achieving a 0.54% gain in\naverage accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that\ndynamically updating reward model is an effective way to combat reward hacking,\nproviding a reference for better integrating reward models into RL.", "AI": {"tldr": "Cooper is a reinforcement learning framework that optimizes both the policy and reward models to enhance reasoning in large language models while addressing limitations of existing reward paradigms.", "motivation": "To improve the robustness of reward frameworks in reinforcement learning for large language models and mitigate issues like reward hacking.", "method": "Cooper optimizes policy and reward models simultaneously, utilizing rule-based rewards for precision and dynamically generates sample pairs for continued training of the reward model.", "result": "Cooper enhances robustness against reward hacking and shows a 0.54% gain in average accuracy on Qwen2.5-1.5B-Instruct compared to previous models.", "conclusion": "The approach demonstrates that dynamically updating the reward model is effective in improving reinforcement learning performance and combating reward hacking.", "key_contributions": ["Introduction of Cooper, a novel RL framework", "Development of VerifyRM, a reward model outperforming others", "Hybrid annotation strategy for efficient training data generation"], "limitations": "", "keywords": ["Reinforcement Learning", "Reward Models", "Large Language Models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2405.00708", "pdf": "https://arxiv.org/pdf/2405.00708.pdf", "abs": "https://arxiv.org/abs/2405.00708", "title": "Understanding Large Language Model Behaviors through Interactive Counterfactual Generation and Analysis", "authors": ["Furui Cheng", "Vil√©m Zouhar", "Robin Shing Moon Chan", "Daniel F√ºrst", "Hendrik Strobelt", "Mennatallah El-Assady"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG", "I.2.7; H.5.2"], "comment": null, "summary": "Understanding the behavior of large language models (LLMs) is crucial for\nensuring their safe and reliable use. However, existing explainable AI (XAI)\nmethods for LLMs primarily rely on word-level explanations, which are often\ncomputationally inefficient and misaligned with human reasoning processes.\nMoreover, these methods often treat explanation as a one-time output,\noverlooking its inherently interactive and iterative nature. In this paper, we\npresent LLM Analyzer, an interactive visualization system that addresses these\nlimitations by enabling intuitive and efficient exploration of LLM behaviors\nthrough counterfactual analysis. Our system features a novel algorithm that\ngenerates fluent and semantically meaningful counterfactuals via targeted\nremoval and replacement operations at user-defined levels of granularity. These\ncounterfactuals are used to compute feature attribution scores, which are then\nintegrated with concrete examples in a table-based visualization, supporting\ndynamic analysis of model behavior. A user study with LLM practitioners and\ninterviews with experts demonstrate the system's usability and effectiveness,\nemphasizing the importance of involving humans in the explanation process as\nactive participants rather than passive recipients.", "AI": {"tldr": "LLM Analyzer is an interactive visualization system for exploring LLM behaviors through counterfactual analysis, addressing limitations of current XAI methods.", "motivation": "To improve the understanding of LLM behaviors and provide more effective means of explanation beyond traditional methods.", "method": "An interactive visualization system that uses counterfactual analysis, featuring an algorithm for generating fluent counterfactuals and integration with a table-based visualization for dynamic analysis.", "result": "User studies indicate that LLM Analyzer enhances understanding and usability for LLM practitioners by enabling active participation in the explanation process.", "conclusion": "Involving users actively in the explanation process improves the understanding of LLM behaviors and the effectiveness of XAI methods.", "key_contributions": ["Introduction of LLM Analyzer for interactive exploration of LLMs", "Use of counterfactuals for generating feature attribution scores", "Demonstrated usability and effectiveness through user studies"], "limitations": "The scope of the system may be limited by the choice of counterfactual generation operations and user-defined granularity levels.", "keywords": ["Large Language Models", "Explainable AI", "Counterfactual Analysis", "User Interaction", "Visualization"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.05614", "pdf": "https://arxiv.org/pdf/2508.05614.pdf", "abs": "https://arxiv.org/abs/2508.05614", "title": "OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks", "authors": ["Zixuan Wang", "Dingming Li", "Hongxing Li", "Shuo Chen", "Yuchen Yan", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "categories": ["cs.CL", "cs.AI"], "comment": "Project Page: https://zju-real.github.io/OmniEmbodied Code:\n  https://github.com/ZJU-REAL/OmniEmbodied", "summary": "Large language models excel at abstract reasoning but their capacity for\nembodied agent reasoning remains largely unexplored. We present OmniEAR, a\ncomprehensive framework for evaluating how language models reason about\nphysical interactions, tool usage, and multi-agent coordination in embodied\ntasks. Unlike existing benchmarks that provide predefined tool sets or explicit\ncollaboration directives, OmniEAR requires agents to dynamically acquire\ncapabilities and autonomously determine coordination strategies based on task\ndemands. Through text-based environment representation, we model continuous\nphysical properties and complex spatial relationships across 1,500 scenarios\nspanning household and industrial domains. Our systematic evaluation reveals\nsevere performance degradation when models must reason from constraints: while\nachieving 85-96% success with explicit instructions, performance drops to\n56-85% for tool reasoning and 63-85% for implicit collaboration, with compound\ntasks showing over 50% failure rates. Surprisingly, complete environmental\ninformation degrades coordination performance, indicating models cannot filter\ntask-relevant constraints. Fine-tuning improves single-agent tasks dramatically\n(0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing\nfundamental architectural limitations. These findings demonstrate that embodied\nreasoning poses fundamentally different challenges than current models can\naddress, establishing OmniEAR as a rigorous benchmark for evaluating and\nadvancing embodied AI systems. Our code and data are included in the\nsupplementary materials and will be open-sourced upon acceptance.", "AI": {"tldr": "OmniEAR framework evaluates language models' reasoning in embodied tasks, revealing performance challenges in tool usage and multi-agent coordination.", "motivation": "To assess how language models reason about physical interactions and coordination in embodied tasks, which is largely unexplored.", "method": "The framework involves text-based environment representation in 1,500 scenarios, requiring agents to dynamically acquire capabilities and establish coordination strategies based on task demands.", "result": "Models perform well (85-96%) with explicit instructions but struggle significantly (56-85% for tool reasoning, 63-85% for implicit collaboration), highlighting architectural limitations in reasoning under constraints.", "conclusion": "OmniEAR establishes a new benchmark for embodied AI, demonstrating that current models have fundamental limitations in addressing embodied reasoning tasks.", "key_contributions": ["Introduction of OmniEAR as a comprehensive evaluation framework for embodied AI", "Identification of performance discrepancies between explicit instruction and dynamic reasoning", "Demonstration of architectural limitations in multi-agent coordination tasks"], "limitations": "Current models cannot filter task-relevant constraints despite having complete environmental information, leading to degraded performance in coordination.", "keywords": ["embodied AI", "language models", "reasoning", "evaluation framework", "multi-agent coordination"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.05618", "pdf": "https://arxiv.org/pdf/2508.05618.pdf", "abs": "https://arxiv.org/abs/2508.05618", "title": "Learning to Reason for Factuality", "authors": ["Xilun Chen", "Ilia Kulikov", "Vincent-Pierre Berges", "Barlas Oƒüuz", "Rulin Shao", "Gargi Ghosh", "Jason Weston", "Wen-tau Yih"], "categories": ["cs.CL"], "comment": null, "summary": "Reasoning Large Language Models (R-LLMs) have significantly advanced complex\nreasoning tasks but often struggle with factuality, generating substantially\nmore hallucinations than their non-reasoning counterparts on long-form\nfactuality benchmarks. However, extending online Reinforcement Learning (RL), a\nkey component in recent R-LLM advancements, to the long-form factuality setting\nposes several unique challenges due to the lack of reliable verification\nmethods. Previous work has utilized automatic factuality evaluation frameworks\nsuch as FActScore to curate preference data in the offline RL setting, yet we\nfind that directly leveraging such methods as the reward in online RL leads to\nreward hacking in multiple ways, such as producing less detailed or relevant\nresponses. We propose a novel reward function that simultaneously considers the\nfactual precision, response detail level, and answer relevance, and applies\nonline RL to learn high quality factual reasoning. Evaluated on six long-form\nfactuality benchmarks, our factual reasoning model achieves an average\nreduction of 23.1 percentage points in hallucination rate, a 23% increase in\nanswer detail level, and no degradation in the overall response helpfulness.", "AI": {"tldr": "The paper introduces a novel reward function for online reinforcement learning aimed at improving factual reasoning in large language models, reducing hallucinations and enhancing response detail without sacrificing helpfulness.", "motivation": "Although reasoning large language models have made strides in complex reasoning tasks, they produce more hallucinations compared to non-reasoning models, particularly on long-form factual benchmarks. This paper addresses the challenge of reliable verification methods in the context of online reinforcement learning to improve factual accuracy.", "method": "The authors propose a new reward function that evaluates factual precision, response detail, and answer relevance while applying online reinforcement learning (RL) techniques.", "result": "The new factual reasoning model outperformed previous attempts, achieving an average reduction of 23.1 percentage points in hallucination rates and a 23% increase in answer detail level, with no decline in helpfulness on six long-form factuality benchmarks.", "conclusion": "The proposed model enhances the quality of factual reasoning in large language models by effectively minimizing hallucinations and improving detail in responses through a well-defined reward function.", "key_contributions": ["Introduction of a new reward function for online RL in long-form factuality settings", "Demonstration of significant reductions in hallucination rates", "Increase in response detail without degrading helpfulness"], "limitations": "", "keywords": ["Large Language Models", "Reinforcement Learning", "Factuality", "Natural Language Processing", "Online Learning"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.05625", "pdf": "https://arxiv.org/pdf/2508.05625.pdf", "abs": "https://arxiv.org/abs/2508.05625", "title": "How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations", "authors": ["Brandon Jaipersaud", "David Krueger", "Ekdeep Singh Lubana"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have started to demonstrate the ability to\npersuade humans, yet our understanding of how this dynamic transpires is\nlimited. Recent work has used linear probes, lightweight tools for analyzing\nmodel representations, to study various LLM skills such as the ability to model\nuser sentiment and political perspective. Motivated by this, we apply probes to\nstudy persuasion dynamics in natural, multi-turn conversations. We leverage\ninsights from cognitive science to train probes on distinct aspects of\npersuasion: persuasion success, persuadee personality, and persuasion strategy.\nDespite their simplicity, we show that they capture various aspects of\npersuasion at both the sample and dataset levels. For instance, probes can\nidentify the point in a conversation where the persuadee was persuaded or where\npersuasive success generally occurs across the entire dataset. We also show\nthat in addition to being faster than expensive prompting-based approaches,\nprobes can do just as well and even outperform prompting in some settings, such\nas when uncovering persuasion strategy. This suggests probes as a plausible\navenue for studying other complex behaviours such as deception and\nmanipulation, especially in multi-turn settings and large-scale dataset\nanalysis where prompting-based methods would be computationally inefficient.", "AI": {"tldr": "This paper explores the use of linear probes to study persuasion dynamics in large language models during multi-turn conversations, revealing insights into persuasion success, personality, and strategy.", "motivation": "To understand the dynamics of persuasion in conversations mediated by large language models, an area that is currently underexplored in AI research.", "method": "The authors employ linear probes to analyze model representations focusing on aspects of persuasion including success, personality traits of the persuadee, and the strategies employed during persuasion. The effectiveness of probes is tested against traditional prompting methods.", "result": "Probes successfully capture various aspects of persuasion, identifying key moments of persuasion and outperforming traditional prompting in certain scenarios, demonstrating their efficacy in analyzing complex behaviors in conversation.", "conclusion": "Linear probes present a practical and computationally efficient alternative to prompting-based methods for studying persuasion in natural language processing, with potential applications in understanding other complex behaviors.", "key_contributions": ["Introduction of linear probes for studying persuasion in LLMs", "Demonstration of probes identifying persuasion moments in conversations", "Comparison showing probes' efficiency and efficacy over traditional methods"], "limitations": "", "keywords": ["Large Language Models", "Persuasion", "Linear Probes", "Natural Language Processing", "Cognitive Science"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.05628", "pdf": "https://arxiv.org/pdf/2508.05628.pdf", "abs": "https://arxiv.org/abs/2508.05628", "title": "H-Net++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling in Morphologically-Rich Languages", "authors": ["Mehrdad Zakershahrak", "Samira Ghodratnama"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Byte-level language models eliminate fragile tokenizers but face\ncomputational challenges in morphologically-rich languages (MRLs), where words\nspan many bytes. We propose H-NET++, a hierarchical dynamic-chunking model that\nlearns linguistically-informed segmentation through end-to-end training. Key\ninnovations include: (1) a lightweight Transformer context-mixer (1.9M\nparameters) for cross-chunk attention, (2) a two-level latent hyper-prior for\ndocument-level consistency, (3) specialized handling of orthographic artifacts\n(e.g. Persian ZWNJ), and (4) curriculum-based training with staged sequence\nlengths. On a 1.4B-token Persian corpus, H-NET++ achieves state-of-the-art\nresults: 0.159 BPB reduction versus BPE-based GPT-2-fa (12% better\ncompression), 5.4pp gain on ParsGLUE, 53% improved robustness to ZWNJ\ncorruption, and 73.8% F1 on gold morphological boundaries. Our learned chunks\nalign with Persian morphology without explicit supervision, demonstrating that\nhierarchical dynamic chunking provides an effective tokenizer-free solution for\nMRLs while maintaining computational efficiency.", "AI": {"tldr": "H-NET++ is a hierarchical language model that efficiently learns segmentation for morphologically-rich languages, achieving state-of-the-art results.", "motivation": "To address the challenges of processing morphologically-rich languages with byte-level language models that currently face computational issues.", "method": "The model utilizes a lightweight Transformer context-mixer for cross-chunk attention, coupled with a two-level latent hyper-prior for document consistency and specialized handling of orthographic artifacts, trained through curriculum-based approaches.", "result": "H-NET++ achieved a 0.159 BPB reduction compared to BPE-based GPT-2-fa, a 5.4pp improvement on ParsGLUE, enhanced robustness to ZWNJ corruption, and an F1 score of 73.8 on morphological boundaries.", "conclusion": "H-NET++ demonstrates that hierarchical dynamic chunking is an effective solution for tokenizer-free processing of morphologically-rich languages, offering enhanced computational efficiency.", "key_contributions": ["Hierarchical dynamic chunking for effective tokenization", "Lightweight Transformer for context-mixing", "Curriculum-based training with staged sequence lengths"], "limitations": "", "keywords": ["hierarchical modeling", "dynamic chunking", "morphologically-rich languages", "byte-level language models", "natural language processing"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2201.08214", "pdf": "https://arxiv.org/pdf/2201.08214.pdf", "abs": "https://arxiv.org/abs/2201.08214", "title": "A Latent-Variable Model for Intrinsic Probing", "authors": ["Karolina Sta≈Ñczak", "Lucas Torroba Hennigen", "Adina Williams", "Ryan Cotterell", "Isabelle Augenstein"], "categories": ["cs.CL"], "comment": null, "summary": "The success of pre-trained contextualized representations has prompted\nresearchers to analyze them for the presence of linguistic information. Indeed,\nit is natural to assume that these pre-trained representations do encode some\nlevel of linguistic knowledge as they have brought about large empirical\nimprovements on a wide variety of NLP tasks, which suggests they are learning\ntrue linguistic generalization. In this work, we focus on intrinsic probing, an\nanalysis technique where the goal is not only to identify whether a\nrepresentation encodes a linguistic attribute but also to pinpoint where this\nattribute is encoded. We propose a novel latent-variable formulation for\nconstructing intrinsic probes and derive a tractable variational approximation\nto the log-likelihood. Our results show that our model is versatile and yields\ntighter mutual information estimates than two intrinsic probes previously\nproposed in the literature. Finally, we find empirical evidence that\npre-trained representations develop a cross-lingually entangled notion of\nmorphosyntax.", "AI": {"tldr": "This paper presents a novel approach to intrinsic probing of pre-trained contextualized representations in NLP, revealing insights into their linguistic encoding.", "motivation": "To analyze pre-trained contextualized representations for their linguistic information and understanding where this information is encoded.", "method": "A latent-variable formulation for constructing intrinsic probes is proposed, along with a tractable variational approximation to the log-likelihood.", "result": "The proposed model outperforms previous intrinsic probes in mutual information estimation, showing that pre-trained representations exhibit cross-lingually entangled morphosyntax.", "conclusion": "The findings indicate that pre-trained models not only encode linguistic attributes but do so in a way that reflects cross-linguistic similarities in morphosyntax.", "key_contributions": ["Novel latent-variable approach for intrinsic probing", "Derivation of variational approximation for log-likelihood", "Empirical evidence of cross-lingual morphosyntactic entanglement."], "limitations": "", "keywords": ["intrinsic probing", "linguistic representation", "pre-trained models", "morphosyntax", "variational approximation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2402.13213", "pdf": "https://arxiv.org/pdf/2402.13213.pdf", "abs": "https://arxiv.org/abs/2402.13213", "title": "Probabilities of Chat LLMs Are Miscalibrated but Still Predict Correctness on Multiple-Choice Q&A", "authors": ["Benjamin Plaut", "Nguyen X. Khanh", "Tu Trinh"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published in Transactions on Machine Learning Research (TMLR)", "summary": "We study 15 large language models (LLMs) fine-tuned for chat and find that\ntheir maximum softmax probabilities (MSPs) are consistently miscalibrated on\nmultiple-choice Q&A. However, those MSPs might still encode useful uncertainty\ninformation. Specifically, we hypothesized that wrong answers would be\nassociated with smaller MSPs compared to correct answers. Via rigorous\nstatistical testing, we show that this hypothesis holds for models which\nperform well on the underlying Q&A task. We also find a strong direction\ncorrelation between Q&A accuracy and MSP correctness prediction, while finding\nno correlation between Q&A accuracy and calibration error. This suggests that\nwithin the current fine-tuning paradigm, we can expect correctness prediction\nbut not calibration to improve as LLM capabilities progress. To demonstrate the\nutility of correctness prediction, we show that when models have the option to\nabstain, performance can be improved by selectively abstaining based on the MSP\nof the initial model response, using only a small amount of labeled data to\nchoose the MSP threshold.", "AI": {"tldr": "Study of LLMs finding miscalibration in maximum softmax probabilities (MSPs) on Q&A tasks, suggesting utility in correctness prediction despite calibration errors.", "motivation": "To explore the calibration of maximum softmax probabilities (MSPs) in chat-tuned large language models and how they relate to correctness in multiple-choice Q&A.", "method": "Statistical testing on LLMs to analyze the relationship between MSPs and Q&A answer correctness, examining the impact of MSPs on performance when models can abstain from answering.", "result": "Demonstrated that lower MSPs correlate with wrong answers and that accuracy in Q&A is directionally related to correct predictions based on MSPs, irrespective of calibration error.", "conclusion": "As fine-tuning improves LLMs, correctness prediction will enhance even as calibration remains an issue, suggesting strategies to improve model performance by judiciously abstaining based on initial MSPs.", "key_contributions": ["Identified miscalibration of MSPs in LLMs on multi-choice Q&A tasks.", "Established correlation between MSPs and accuracy in predicting correctness.", "Proposed abstention strategies for improved performance leveraging MSPs."], "limitations": "", "keywords": ["language models", "machine learning", "calibration", "uncertainty", "Q&A"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2405.00708", "pdf": "https://arxiv.org/pdf/2405.00708.pdf", "abs": "https://arxiv.org/abs/2405.00708", "title": "Understanding Large Language Model Behaviors through Interactive Counterfactual Generation and Analysis", "authors": ["Furui Cheng", "Vil√©m Zouhar", "Robin Shing Moon Chan", "Daniel F√ºrst", "Hendrik Strobelt", "Mennatallah El-Assady"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG", "I.2.7; H.5.2"], "comment": null, "summary": "Understanding the behavior of large language models (LLMs) is crucial for\nensuring their safe and reliable use. However, existing explainable AI (XAI)\nmethods for LLMs primarily rely on word-level explanations, which are often\ncomputationally inefficient and misaligned with human reasoning processes.\nMoreover, these methods often treat explanation as a one-time output,\noverlooking its inherently interactive and iterative nature. In this paper, we\npresent LLM Analyzer, an interactive visualization system that addresses these\nlimitations by enabling intuitive and efficient exploration of LLM behaviors\nthrough counterfactual analysis. Our system features a novel algorithm that\ngenerates fluent and semantically meaningful counterfactuals via targeted\nremoval and replacement operations at user-defined levels of granularity. These\ncounterfactuals are used to compute feature attribution scores, which are then\nintegrated with concrete examples in a table-based visualization, supporting\ndynamic analysis of model behavior. A user study with LLM practitioners and\ninterviews with experts demonstrate the system's usability and effectiveness,\nemphasizing the importance of involving humans in the explanation process as\nactive participants rather than passive recipients.", "AI": {"tldr": "The paper introduces LLM Analyzer, an interactive visualization system for exploring LLM behaviors through counterfactual analysis, addressing limitations of existing XAI methods.", "motivation": "To improve the understanding and usability of large language models (LLMs) by providing more interactive and intuitive explanations.", "method": "The system employs a novel algorithm for generating counterfactuals by targeted removal and replacement operations to analyze LLM behavior dynamically.", "result": "User studies indicate that LLM Analyzer enhances usability and effectiveness in understanding model behaviors and feature attributions.", "conclusion": "Engaging users as active participants in the explanation process significantly improves their understanding of LLM behaviors.", "key_contributions": ["Interactive visualization for LLM behavior analysis", "Novel counterfactual generation algorithm", "User-centered approach in explanation processes"], "limitations": "The study may need more extensive scalability testing for various LLMs and user scenarios.", "keywords": ["large language models", "explainable AI", "counterfactual analysis", "interactive visualization", "human-in-the-loop"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2406.15477", "pdf": "https://arxiv.org/pdf/2406.15477.pdf", "abs": "https://arxiv.org/abs/2406.15477", "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics", "authors": ["Kai Yin", "Bo Li", "Chengkai Liu", "Ali Mostafavi", "Xia Hu"], "categories": ["cs.CL", "cs.AI"], "comment": "Relevant source code and data is available:\n  https://github.com/KaiYin97/CrsisLLM", "summary": "In the field of crisis/disaster informatics, social media is increasingly\nbeing used for improving situational awareness to inform response and relief\nefforts. Efficient and accurate text classification tools have been a focal\narea of investigation in crisis informatics. However, current methods mostly\nrely on single-label text classification models, which fails to capture\ndifferent insights embedded in dynamic and multifaceted disaster-related social\nmedia data. This study introduces a novel approach to disaster text\nclassification by enhancing a pre-trained Large Language Model (LLM) through\ninstruction fine-tuning targeted for multi-label classification of\ndisaster-related tweets. Our methodology involves creating a comprehensive\ninstruction dataset from disaster-related tweets, which is then used to\nfine-tune an open-source LLM, thereby embedding it with disaster-specific\nknowledge. This fine-tuned model can classify multiple aspects of\ndisaster-related information simultaneously, such as the type of event,\ninformativeness, and involvement of human aid, significantly improving the\nutility of social media data for situational awareness in disasters. The\nresults demonstrate that this approach enhances the categorization of critical\ninformation from social media posts, thereby facilitating a more effective\ndeployment for situational awareness during emergencies. This research paves\nthe way for more advanced, adaptable, and robust disaster management tools,\nleveraging the capabilities of LLMs to improve real-time situational awareness\nand response strategies in disaster scenarios.", "AI": {"tldr": "This paper presents a novel approach for multi-label classification of disaster-related tweets using a fine-tuned Large Language Model to enhance situational awareness in crisis informatics.", "motivation": "To improve situational awareness in disaster response by developing more effective text classification tools for social media data, particularly in multi-label contexts.", "method": "The study fine-tunes a pre-trained Large Language Model with an instruction dataset compiled from disaster-related tweets, targeting multi-label classification.", "result": "The fine-tuned model successfully classifies multiple aspects of disaster-related information, improving the categorization of critical insights from social media.", "conclusion": "This research enhances the deployment of social media data for situational awareness during emergencies, paving the way for advanced disaster management tools leveraging LLM capabilities.", "key_contributions": ["Introduces a novel multi-label classification approach using fine-tuned LLMs for disaster tweets.", "Enhances the ability to capture diverse insights from social media in crisis scenarios.", "Demonstrates improved utility of social media data for real-time situational awareness."], "limitations": "", "keywords": ["disaster informatics", "text classification", "Large Language Model", "multi-label classification", "situational awareness"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2407.06323", "pdf": "https://arxiv.org/pdf/2407.06323.pdf", "abs": "https://arxiv.org/abs/2407.06323", "title": "When in Doubt, Cascade: Towards Building Efficient and Capable Guardrails", "authors": ["Manish Nagireddy", "Inkit Padhi", "Soumya Ghosh", "Prasanna Sattigeri"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have convincing performance in a variety of\ndownstream tasks. However, these systems are prone to generating undesirable\noutputs such as harmful and biased text. In order to remedy such generations,\nthe development of guardrail (or detector) models has gained traction.\nMotivated by findings from developing a detector for social bias, we adopt the\nnotion of a use-mention distinction - which we identified as the primary source\nof under-performance in the preliminary versions of our social bias detector.\nArmed with this information, we describe a fully extensible and reproducible\nsynthetic data generation pipeline which leverages taxonomy-driven instructions\nto create targeted and labeled data. Using this pipeline, we generate over 300K\nunique contrastive samples and provide extensive experiments to systematically\nevaluate performance on a suite of open source datasets. We show that our\nmethod achieves competitive performance with a fraction of the cost in compute\nand offers insight into iteratively developing efficient and capable guardrail\nmodels.\n  Warning: This paper contains examples of text which are toxic, biased, and\npotentially harmful.", "AI": {"tldr": "This paper presents a synthetic data generation pipeline for developing guardrail models to detect bias in large language models (LLMs), demonstrating competitive performance with reduced compute costs.", "motivation": "To address the generation of harmful and biased text by large language models, the paper seeks to enhance the performance of detectors for social bias.", "method": "The authors developed a synthetic data generation pipeline that uses taxonomy-driven instructions to create targeted and labeled data, generating over 300K unique contrastive samples for systematic evaluation.", "result": "The method achieves competitive performance comparable to existing models at a fraction of the computational cost.", "conclusion": "The study provides a framework for developing more efficient and capable guardrail models, highlighting the importance of synthetic data in this process.", "key_contributions": ["Introduction of a synthetic data generation pipeline for bias detection", "Generation of over 300K contrastive samples", "Cost-effective method for improving guardrail model performance"], "limitations": "The paper contains potentially harmful examples of toxic and biased text.", "keywords": ["Large Language Models", "Synthetic Data", "Guardrail Models", "Bias Detection", "Text Generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2409.02098", "pdf": "https://arxiv.org/pdf/2409.02098.pdf", "abs": "https://arxiv.org/abs/2409.02098", "title": "CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation", "authors": ["Ingo Ziegler", "Abdullatif K√∂ksal", "Desmond Elliott", "Hinrich Sch√ºtze"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at TACL; Pre-MIT Press publication version. Code and dataset\n  available at: https://github.com/ziegler-ingo/CRAFT", "summary": "Building high-quality datasets for specialized tasks is a time-consuming and\nresource-intensive process that often requires specialized domain knowledge. We\npropose Corpus Retrieval and Augmentation for Fine-Tuning (CRAFT), a method for\ngenerating synthetic datasets, given a small number of user-written few-shots\nthat demonstrate the task to be performed. Given these examples, CRAFT uses\nlarge-scale public web-crawled corpora and similarity-based document retrieval\nto find other relevant human-written documents. Lastly, instruction-tuned large\nlanguage models (LLMs) augment the retrieved documents into custom-formatted\ntask samples, which then can be used for fine-tuning. We demonstrate that CRAFT\ncan efficiently generate large-scale task-specific training datasets for four\ndiverse tasks: biology, medicine, and commonsense question-answering (QA), as\nwell as summarization. Our experiments show that CRAFT-based models outperform\nor match general LLMs on QA tasks, while exceeding models trained on\nhuman-curated summarization data by 46 preference points. CRAFT outperforms\nother synthetic dataset generation methods such as Self- and Evol-Instruct, and\nremains robust even when the quality of the initial few-shots varies.", "AI": {"tldr": "CRAFT is a method for generating synthetic datasets from a few user-written examples using document retrieval and instruction-tuned LLMs, demonstrated on multiple tasks.", "motivation": "The need for high-quality datasets for specialized tasks in fields like biology, medicine, and QA, which are often challenging to create due to resource intensity and domain knowledge requirements.", "method": "CRAFT employs document retrieval from large web-crawled corpora based on similarity to user-written few-shots, followed by augmentation using LLMs to create task-specific samples.", "result": "CRAFT demonstrates superior performance on QA tasks and summarization when compared to existing methods, including models trained on curated data.", "conclusion": "CRAFT efficiently generates large-scale, task-specific datasets and remains effective even with varying initial few-shot quality.", "key_contributions": ["Introduces a novel method for synthetic dataset generation.", "Outperforms existing methods in various relevant tasks.", "Shows robustness in performance despite the quality of input samples."], "limitations": "", "keywords": ["synthetic datasets", "task-specific training", "large language models"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2409.06518", "pdf": "https://arxiv.org/pdf/2409.06518.pdf", "abs": "https://arxiv.org/abs/2409.06518", "title": "Medal Matters: Probing LLMs' Failure Cases Through Olympic Rankings", "authors": ["Juhwan Choi", "Seunguk Yu", "JungMin Yun", "YoungBin Kim"], "categories": ["cs.CL", "cs.AI"], "comment": "COLM 2025 ORIGen Workshop", "summary": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage processing tasks, yet their internal knowledge structures remain\npoorly understood. This study examines these structures through the lens of\nhistorical Olympic medal tallies, evaluating LLMs on two tasks: (1) retrieving\nmedal counts for specific teams and (2) identifying rankings of each team.\nWhile state-of-the-art LLMs excel in recalling medal counts, they struggle with\nproviding rankings, highlighting a key difference between their knowledge\norganization and human reasoning. These findings shed light on the limitations\nof LLMs' internal knowledge integration and suggest directions for improvement.\nTo facilitate further research, we release our code, dataset, and model\noutputs.", "AI": {"tldr": "This study investigates the internal knowledge structures of large language models (LLMs) using Olympic medal tallies, revealing strengths in recalling medal counts but weaknesses in ranking teams.", "motivation": "To better understand the internal knowledge structures of LLMs and their limitations in knowledge integration.", "method": "The study evaluates LLM performance on tasks related to retrieving Olympic medal counts and identifying team rankings based on historical data.", "result": "LLMs perform well at recalling medal counts but struggle with ranking teams, indicating a divergence from human reasoning.", "conclusion": "The findings highlight the limitations of LLMs in knowledge organization and suggest potential research directions for improvement in LLM capabilities.", "key_contributions": ["Reveals weaknesses of LLMs in ranking tasks compared to human reasoning", "Provides a dataset and code for further research", "Offers insights into the internal knowledge organization of LLMs"], "limitations": "", "keywords": ["large language models", "knowledge structures", "Olympic medal counts", "ranking tasks", "natural language processing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2409.08107", "pdf": "https://arxiv.org/pdf/2409.08107.pdf", "abs": "https://arxiv.org/abs/2409.08107", "title": "WhisperNER: Unified Open Named Entity and Speech Recognition", "authors": ["Gil Ayache", "Menachem Pirchi", "Aviv Navon", "Aviv Shamsian", "Gill Hetz", "Joseph Keshet"], "categories": ["cs.CL", "cs.LG"], "comment": "ASRU 2025, IEEE", "summary": "Integrating named entity recognition (NER) with automatic speech recognition\n(ASR) can significantly enhance transcription accuracy and informativeness. In\nthis paper, we introduce WhisperNER, a novel model that allows joint speech\ntranscription and entity recognition. WhisperNER supports open-type NER,\nenabling recognition of diverse and evolving entities at inference. Building on\nrecent advancements in open NER research, we augment a large synthetic dataset\nwith synthetic speech samples. This allows us to train WhisperNER on a large\nnumber of examples with diverse NER tags. During training, the model is\nprompted with NER labels and optimized to output the transcribed utterance\nalong with the corresponding tagged entities. To evaluate WhisperNER, we\ngenerate synthetic speech for commonly used NER benchmarks and annotate\nexisting ASR datasets with open NER tags. Our experiments demonstrate that\nWhisperNER outperforms natural baselines on both out-of-domain open type NER\nand supervised finetuning.", "AI": {"tldr": "WhisperNER integrates named entity recognition with automatic speech recognition to improve transcription accuracy and informativeness, allowing open-type NER.", "motivation": "To enhance the accuracy and informativeness of transcriptions by integrating NER with ASR in a single model.", "method": "We introduce WhisperNER, a model that jointly performs speech transcription and entity recognition by training on a large synthetic dataset augmented with synthetic speech samples.", "result": "WhisperNER shows improved performance compared to natural baselines on various benchmarks for out-of-domain open-type NER and in supervised finetuning tasks.", "conclusion": "The results indicate that integrating NER and ASR in WhisperNER provides significant advantages in transcription quality.", "key_contributions": ["Introduction of WhisperNER for joint transcription and entity recognition", "Support for open-type named entity recognition", "Training on a large synthetic dataset with diverse NER tags"], "limitations": "", "keywords": ["Named Entity Recognition", "Automatic Speech Recognition", "WhisperNER"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2409.19492", "pdf": "https://arxiv.org/pdf/2409.19492.pdf", "abs": "https://arxiv.org/abs/2409.19492", "title": "MedHalu: Hallucinations in Responses to Healthcare Queries by Large Language Models", "authors": ["Vibhor Agarwal", "Yiqiao Jin", "Mohit Chandra", "Munmun De Choudhury", "Srijan Kumar", "Nishanth Sastry"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ICWSM2026", "summary": "Large language models (LLMs) are starting to complement traditional\ninformation seeking mechanisms such as web search. LLM-powered chatbots like\nChatGPT are gaining prominence among the general public. AI chatbots are also\nincreasingly producing content on social media platforms. However, LLMs are\nalso prone to hallucinations, generating plausible yet factually incorrect or\nfabricated information. This becomes a critical problem when laypeople start\nseeking information about sensitive issues such as healthcare. Existing works\nin LLM hallucinations in the medical domain mainly focus on testing the medical\nknowledge of LLMs through standardized medical exam questions which are often\nwell-defined and clear-cut with definitive answers. However, these approaches\nmay not fully capture how these LLMs perform during real-world interactions\nwith patients. This work conducts a pioneering study on hallucinations in\nLLM-generated responses to real-world healthcare queries from patients.We\nintroduce MedHalu, a novel medical hallucination benchmark featuring diverse\nhealth-related topics and hallucinated responses from LLMs, with detailed\nannotation of the hallucination types and text spans. We also propose\nMedHaluDetect, a comprehensive framework for evaluating LLMs' abilities to\ndetect hallucinations. Furthermore, we study the vulnerability to medical\nhallucinations among three groups -- medical experts, LLMs, and laypeople.\nNotably, LLMs significantly underperform human experts and, in some cases, even\nlaypeople in detecting medical hallucinations. To improve hallucination\ndetection, we propose an expert-in-the-loop approach that integrates expert\nreasoning into LLM inputs, significantly improving hallucination detection for\nall LLMs, including a 6.3% macro-F1 improvement for GPT-4.", "AI": {"tldr": "This paper studies hallucinations in large language models (LLMs) within healthcare contexts and introduces MedHalu, a benchmark for evaluating these hallucinations, along with a detection framework, MedHaluDetect.", "motivation": "The increasing use of LLMs in healthcare raises concerns about their accuracy due to hallucinations, making it essential to understand their performance in real-world patient interactions.", "method": "The study introduces MedHalu, a benchmark capturing various health-related responses generated by LLMs, annotated for types of hallucinations. It also proposes MedHaluDetect for evaluating LLMs' hallucination detection capabilities and assesses vulnerability to hallucinations across medical experts, LLMs, and laypeople.", "result": "LLMs showed significantly poorer performance in detecting medical hallucinations compared to human experts and sometimes even underperformed compared to laypeople. The expert-in-the-loop approach improved detection efficacy, achieving a 6.3% macro-F1 score improvement for GPT-4.", "conclusion": "Integrating expert reasoning into LLM inputs enhances hallucination detection capabilities, emphasizing the need for robust evaluation benchmarks like MedHalu to assess LLMs in critical fields like healthcare.", "key_contributions": ["Introduction of MedHalu, a medical hallucination benchmark", "Development of MedHaluDetect for evaluating LLM hallucination detection", "Demonstration of expert-in-the-loop method to improve LLM performance in detecting medical hallucinations"], "limitations": "The study focuses on a specific set of health-related queries and may not generalize across all healthcare scenarios or to all LLMs.", "keywords": ["large language models", "hallucinations", "healthcare", "MedHalu", "MedHaluDetect"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.01215", "pdf": "https://arxiv.org/pdf/2410.01215.pdf", "abs": "https://arxiv.org/abs/2410.01215", "title": "From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging", "authors": ["Yuling Shi", "Songsong Wang", "Chengcheng Wan", "Min Wang", "Xiaodong Gu"], "categories": ["cs.CL", "cs.AI", "cs.PL", "cs.SE"], "comment": "Code and data available at https://github.com/YerbaPage/MGDebugger", "summary": "While large language models have made significant strides in code generation,\nthe pass rate of the generated code is bottlenecked on subtle errors, often\nrequiring human intervention to pass tests, especially for complex problems.\nExisting LLM-based debugging systems treat generated programs as monolithic\nunits, failing to address bugs at multiple levels of granularity, from\nlow-level syntax errors to high-level algorithmic flaws. In this paper, we\nintroduce Multi-Granularity Debugger (MGDebugger), a hierarchical code debugger\nby isolating, identifying, and resolving bugs at various levels of granularity.\nMGDebugger decomposes problematic code into a hierarchical tree structure of\nsubfunctions, with each level representing a particular granularity of error.\nDuring debugging, it analyzes each subfunction and iteratively resolves bugs in\na bottom-up manner. To effectively test each subfunction, we propose an\nLLM-simulated Python executor, which traces code execution and tracks important\nvariable states to pinpoint errors accurately. Extensive experiments\ndemonstrate that MGDebugger outperforms existing debugging systems, achieving\nan 18.9% improvement in accuracy over seed generations in HumanEval and a 97.6%\nrepair success rate in HumanEvalFix. Furthermore, MGDebugger effectively fixes\nbugs across different categories and difficulty levels, demonstrating its\nrobustness and effectiveness.", "AI": {"tldr": "MGDebugger is a hierarchical code debugger that improves the pass rate of LLM-generated code by identifying and resolving bugs at multiple levels of granularity, demonstrating significant performance improvements over existing debugging systems.", "motivation": "To address the limitations of current LLM-based debugging systems that treat generated code as monolithic entities, leading to inefficiencies in debugging complex programs.", "method": "MGDebugger uses a hierarchical tree structure to decompose code into subfunctions, analyzing and debugging them iteratively from the bottom up, aided by an LLM-simulated Python executor that tracks execution and variable states.", "result": "MGDebugger achieves an 18.9% improvement in accuracy on HumanEval and a 97.6% repair success rate on HumanEvalFix, surpassing traditional debugging systems.", "conclusion": "The robustness and effectiveness of MGDebugger in fixing bugs across various categories highlight its potential to improve code generation from LLMs.", "key_contributions": ["Introduction of a novel hierarchical debugging approach", "Use of LLM-simulated execution for accurate error tracing", "Demonstrated substantial accuracy improvements over existing systems"], "limitations": "", "keywords": ["Large language models", "Code generation", "Debugging systems", "Human-Computer Interaction", "Machine Learning"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2410.03751", "pdf": "https://arxiv.org/pdf/2410.03751.pdf", "abs": "https://arxiv.org/abs/2410.03751", "title": "Recent Advances in Speech Language Models: A Survey", "authors": ["Wenqian Cui", "Dianzhi Yu", "Xiaoqi Jiao", "Ziqiao Meng", "Guangyan Zhang", "Qichao Wang", "Yiwen Guo", "Irwin King"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "The reduced version of this paper has been accepted at ACL 2025", "summary": "Large Language Models (LLMs) have recently garnered significant attention,\nprimarily for their capabilities in text-based interactions. However, natural\nhuman interaction often relies on speech, necessitating a shift towards\nvoice-based models. A straightforward approach to achieve this involves a\npipeline of ``Automatic Speech Recognition (ASR) + LLM + Text-to-Speech (TTS)\",\nwhere input speech is transcribed to text, processed by an LLM, and then\nconverted back to speech. Despite being straightforward, this method suffers\nfrom inherent limitations, such as information loss during modality conversion,\nsignificant latency due to the complex pipeline, and error accumulation across\nthe three stages. To address these issues, Speech Language Models (SpeechLMs)\n-- end-to-end models that generate speech without converting from text -- have\nemerged as a promising alternative. This survey paper provides the first\ncomprehensive overview of recent methodologies for constructing SpeechLMs,\ndetailing the key components of their architecture and the various training\nrecipes integral to their development. Additionally, we systematically survey\nthe various capabilities of SpeechLMs, categorize their evaluation metrics, and\ndiscuss the challenges and future research directions in this rapidly evolving\nfield. The GitHub repository is available at\nhttps://github.com/dreamtheater123/Awesome-SpeechLM-Survey", "AI": {"tldr": "This paper surveys recent methodologies for constructing Speech Language Models (SpeechLMs) that generate speech directly, bypassing text conversion, to address issues in traditional models using ASR + LLM + TTS pipelines.", "motivation": "The shift towards voice-based models is necessary for more natural human-computer interactions, as traditional text-based methods have limitations like information loss and latency.", "method": "This survey reviews architectures and training methods for SpeechLMs and categorizes their capabilities, evaluation metrics, and research challenges.", "result": "SpeechLMs present a more efficient alternative to traditional speech processing pipelines by eliminating the conversion from speech to text and back.", "conclusion": "The field of SpeechLMs is rapidly evolving, with numerous challenges and future research opportunities identified.", "key_contributions": ["First comprehensive overview of methodologies for SpeechLMs.", "Detailed analysis of architecture and training recipes for SpeechLMs.", "Categorization of capabilities and evaluation metrics in SpeechLMs."], "limitations": "The paper focuses primarily on methodologies without extensive experimental results from various models.", "keywords": ["Speech Language Models", "Automatic Speech Recognition", "Text-to-Speech", "Human-Computer Interaction", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2410.04094", "pdf": "https://arxiv.org/pdf/2410.04094.pdf", "abs": "https://arxiv.org/abs/2410.04094", "title": "BloomWise: Enhancing Problem-Solving capabilities of Large Language Models using Bloom's-Taxonomy-Inspired Prompts", "authors": ["Maria-Eleni Zoumpoulidi", "Georgios Paraskevopoulos", "Alexandros Potamianos"], "categories": ["cs.CL"], "comment": "16 pages, 2 figures", "summary": "Despite the remarkable capabilities of large language models (LLMs) across a\nrange of tasks, mathematical reasoning remains a challenging frontier.\nMotivated by the observation that humans learn more effectively when prompted\nnot what to think but how to think, we introduce BloomWise, a\ncognitively-inspired prompting technique designed to enhance LLMs' performance\non mathematical problem solving while making their solutions more explainable.\nBloomWise encourages LLMs to generate solutions - in the form of explanations -\nby progressing through a sequence of cognitive operations-from basic (e.g.,\nremembering) to more advanced reasoning skills (e.g., evaluating) - mirroring\nhow humans build understanding. The process iterates through these levels,\nhalting early if a convergence criterion is met: specifically, if two or more\nconsecutive levels yield the same answer, the solution from the earliest such\nlevel is output; otherwise, the process continues until all levels are\ncompleted. Through extensive experiments across five popular math reasoning\ndatasets, we demonstrate the effectiveness of BloomWise. We also present\ncomprehensive ablation studies to analyze the strengths of each component\nwithin our system.", "AI": {"tldr": "BloomWise enhances LLMs' mathematical problem-solving through cognitively-inspired prompting techniques, making solutions more explainable.", "motivation": "To improve mathematical reasoning in LLMs by mimicking human thought processes.", "method": "BloomWise employs a sequence of cognitive operations, allowing LLMs to generate solutions with explanations by progressing through levels of reasoning.", "result": "Extensive experiments show BloomWise's effectiveness on five popular math reasoning datasets; ablation studies highlight strengths in the system's components.", "conclusion": "BloomWise can significantly enhance LLM performance in mathematical reasoning, making it more interpretable.", "key_contributions": ["Introduction of BloomWise for LLMs in mathematical reasoning", "Cognitive operation progression mirroring human learning", "Demonstration of effectiveness across multiple datasets"], "limitations": "", "keywords": ["large language models", "mathematical reasoning", "cognitive prompting"], "importance_score": 8, "read_time_minutes": 16}}
{"id": "2410.17519", "pdf": "https://arxiv.org/pdf/2410.17519.pdf", "abs": "https://arxiv.org/abs/2410.17519", "title": "Large Language Models Still Exhibit Bias in Long Text", "authors": ["Wonje Jeung", "Dongjae Jeon", "Ashkan Yousefpour", "Jonghyun Choi"], "categories": ["cs.CL"], "comment": "Accepted by ACL, code and models are available at\n  https://github.com/WonjeJeung/LTF-TEST", "summary": "Existing fairness benchmarks for large language models (LLMs) primarily focus\non simple tasks, such as multiple-choice questions, overlooking biases that may\narise in more complex scenarios like long-text generation. To address this gap,\nwe introduce the Long Text Fairness Test (LTF-TEST), a framework that evaluates\nbiases in LLMs through essay-style prompts. LTF-TEST covers 14 topics and 10\ndemographic axes, including gender and race, resulting in 11,948 samples. By\nassessing both model responses and the reasoning behind them, LTF-TEST uncovers\nsubtle biases that are difficult to detect in simple responses. In our\nevaluation of five recent LLMs, including GPT-4o and LLaMa3, we identify two\nkey patterns of bias. First, these models frequently favor certain demographic\ngroups in their responses. Second, they show excessive sensitivity toward\ntraditionally disadvantaged groups, often providing overly protective responses\nwhile neglecting others. To mitigate these biases, we propose FT-REGARD, a\nfinetuning approach that pairs biased prompts with neutral responses. FT-REGARD\nreduces gender bias by 34.6% and improves performance by 1.4 percentage points\non the BBQ benchmark, offering a promising approach to addressing biases in\nlong-text generation tasks.", "AI": {"tldr": "Introduction of the Long Text Fairness Test (LTF-TEST) framework that assesses biases in large language models (LLMs) through essay-style prompts.", "motivation": "Existing fairness benchmarks for LLMs focus mainly on simple tasks, neglecting biases in complex scenarios like long-text generation.", "method": "The LTF-TEST framework evaluates biases in LLMs using 11,948 samples across 14 topics and 10 demographic axes, analyzing both model responses and their reasoning.", "result": "Evaluation of five recent LLMs revealed biases favoring certain demographic groups and excessive sensitivity towards disadvantaged groups. FT-REGARD finetuning reduced gender bias by 34.6% and improved BBQ benchmark performance by 1.4 percentage points.", "conclusion": "The LTF-TEST framework provides a comprehensive evaluation of LLM biases, and FT-REGARD presents a viable approach for mitigating these biases in long-text generation tasks.", "key_contributions": ["Introduction of LTF-TEST for evaluating long-text biases in LLMs", "Identification of bias patterns in LLM model responses", "Proposal of FT-REGARD as a finetuning method to mitigate biases"], "limitations": "", "keywords": ["fairness", "large language models", "long text generation", "bias", "finetuning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2412.03930", "pdf": "https://arxiv.org/pdf/2412.03930.pdf", "abs": "https://arxiv.org/abs/2412.03930", "title": "GuARD: Effective Anomaly Detection through a Text-Rich and Graph-Informed Language Model", "authors": ["Yunhe Pang", "Bo Chen", "Fanjin Zhang", "Yanghui Rao", "Evgeny Kharlamov", "Jie Tang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at KDD 2025", "summary": "Anomaly detection on text-rich graphs is widely prevalent in real life, such\nas detecting incorrectly assigned academic papers to authors and detecting bots\nin social networks. The remarkable capabilities of large language models (LLMs)\npave a new revenue by utilizing rich-text information for effective anomaly\ndetection. However, simply introducing rich texts into LLMs can obscure\nessential detection cues and introduce high fine-tuning costs. Moreover, LLMs\noften overlook the intrinsic structural bias of graphs which is vital for\ndistinguishing normal from abnormal node patterns. To this end, this paper\nintroduces GuARD, a text-rich and graph-informed language model that combines\nkey structural features from graph-based methods with fine-grained semantic\nattributes extracted via small language models for effective anomaly detection\non text-rich graphs. GuARD is optimized with the progressive multi-modal\nmulti-turn instruction tuning framework in the task-guided instruction tuning\nregime tailed to incorporate both rich-text and structural modalities.\nExtensive experiments on four datasets reveal that GuARD outperforms\ngraph-based and LLM-based anomaly detection methods, while offering up to\n5$\\times$ times speedup in training and 5$\\times$ times speedup in inference\nover vanilla long-context LLMs on the large-scale WhoIsWho dataset.", "AI": {"tldr": "Introduction of GuARD, a model combining aspects of graphs and language models for anomaly detection on text-rich graphs.", "motivation": "To leverage LLMs and structural graph features for effective anomaly detection while overcoming high fine-tuning costs and structural biases typically neglected by LLMs.", "method": "GuARD combines structural features from graph methods with semantic attributes from small language models using a multi-modal multi-turn instruction tuning framework tailored for task-guided instruction tuning.", "result": "GuARD outperforms existing methods in anomaly detection on four datasets and provides significant speedups in both training and inference on large-scale datasets.", "conclusion": "GuARD effectively incorporates both rich-text and structural modalities, achieving state-of-the-art performance for anomaly detection.", "key_contributions": ["Introduction of GuARD model for anomaly detection", "Combines graph structural features with semantic analysis from LLMs", "Achieves significant speed improvements over traditional methods"], "limitations": "", "keywords": ["anomaly detection", "large language models", "graph-based methods", "text-rich graphs", "multi-modal tuning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2412.14964", "pdf": "https://arxiv.org/pdf/2412.14964.pdf", "abs": "https://arxiv.org/abs/2412.14964", "title": "Efficient Knowledge Injection in LLMs via Self-Distillation", "authors": ["Kalle Kujanp√§√§", "Pekka Marttinen", "Harri Valpola", "Alexander Ilin"], "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "In many practical applications, large language models (LLMs) need to acquire\nnew knowledge not present in their pre-training data. Efficiently leveraging\nthis knowledge usually relies on supervised fine-tuning or retrieval-augmented\ngeneration (RAG). Although RAG has emerged as the industry standard for\nknowledge injection, fine-tuning has not yet achieved comparable success. This\npaper proposes utilizing prompt distillation, a self-distillation-based method\npreviously explored primarily for style alignment and instruction tuning, to\ninternalize new factual knowledge from free-form documents. Unlike prior\nmethods, our approach requires neither larger teacher models nor structured\nknowledge formats. Across multiple LLM sizes and model families, we show that\nprompt distillation outperforms standard supervised fine-tuning and can even\nsurpass RAG. We analyze the key factors contributing to prompt distillation's\neffectiveness and examine how it scales.", "AI": {"tldr": "This paper introduces prompt distillation, a method for efficiently internalizing new knowledge in large language models (LLMs) without needing larger teacher models or structured formats, outperforming fine-tuning and RAG in several cases.", "motivation": "To improve the process of knowledge acquisition in LLMs, which often rely on outdated pre-training data, by exploring a new method that can surpass existing approaches.", "method": "The proposed method, prompt distillation, employs a self-distillation technique to internalize new knowledge from free-form text, compared against supervised fine-tuning and retrieval-augmented generation (RAG).", "result": "Prompt distillation demonstrates superior performance compared to standard fine-tuning and can exceed RAG outcomes across various LLM sizes and families.", "conclusion": "The effectiveness of prompt distillation is attributed to its unique approach, allowing for successful knowledge acquisition in LLMs without the constraints of prior methods.", "key_contributions": ["Introduction of prompt distillation for knowledge internalization in LLMs.", "Demonstration of improved performance over fine-tuning and RAG methods.", "Analysis of scaling factors for prompt distillation's effectiveness."], "limitations": "", "keywords": ["large language models", "knowledge acquisition", "prompt distillation", "fine-tuning", "retrieval-augmented generation"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2412.18351", "pdf": "https://arxiv.org/pdf/2412.18351.pdf", "abs": "https://arxiv.org/abs/2412.18351", "title": "Multi-Agents Based on Large Language Models for Knowledge-based Visual Question Answering", "authors": ["Zhongjian Hu", "Peng Yang", "Bing Li", "Zhenqi Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "We would like to withdraw this submission due to ongoing internal\n  review and coordination among the author team. Upon the supervisor's\n  recommendation, we have decided to delay public dissemination until the\n  manuscript undergoes further refinement and aligns with our intended academic\n  trajectory", "summary": "Large Language Models (LLMs) have achieved impressive results in\nknowledge-based Visual Question Answering (VQA). However existing methods still\nhave challenges: the inability to use external tools autonomously, and the\ninability to work in teams. Humans tend to know whether they need to use\nexternal tools when they encounter a new question, e.g., they tend to be able\nto give a direct answer to a familiar question, whereas they tend to use tools\nsuch as search engines when they encounter an unfamiliar question. In addition,\nhumans also tend to collaborate and discuss with others to get better answers.\nInspired by this, we propose the multi-agent voting framework. We design three\nLLM-based agents that simulate different levels of staff in a team, and assign\nthe available tools according to the levels. Each agent provides the\ncorresponding answer, and finally all the answers provided by the agents are\nvoted to get the final answer. Experiments on OK-VQA and A-OKVQA show that our\napproach outperforms other baselines by 2.2 and 1.0, respectively.", "AI": {"tldr": "Proposal of a multi-agent voting framework using LLMs for improved Visual Question Answering (VQA) in teams with external tool utilization.", "motivation": "Address challenges in current VQA methods related to autonomous use of external tools and collaboration among agents.", "method": "Developed a multi-agent voting framework with three levels of LLM-based agents that utilize available tools and vote on final answers.", "result": "Achieved improvements over baseline methods with increases of 2.2 and 1.0 in performance on OK-VQA and A-OKVQA datasets, respectively.", "conclusion": "The proposed method enhances VQA capabilities by simulating collaborative team responses and tool usage.", "key_contributions": ["Introduction of a multi-agent voting framework for VQA", "Demonstration of LLM-based agents with varying levels of expertise", "Empirical results showing improved performance on benchmark datasets"], "limitations": "None stated due to the paper's withdrawal for review.", "keywords": ["Large Language Models", "Visual Question Answering", "multi-agent systems", "collaboration", "tool utilization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.13417", "pdf": "https://arxiv.org/pdf/2502.13417.pdf", "abs": "https://arxiv.org/abs/2502.13417", "title": "RLTHF: Targeted Human Feedback for LLM Alignment", "authors": ["Yifei Xu", "Tusher Chakraborty", "Emre Kƒ±cƒ±man", "Bibek Aryal", "Eduardo Rodrigues", "Srinagesh Sharma", "Roberto Estevao", "Maria Angels de Luis Balaguer", "Jessica Wolk", "Rafael Padilha", "Leonardo Nunes", "Shobana Balakrishnan", "Songwu Lu", "Ranveer Chandra"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Presented at ICML 2025", "summary": "Fine-tuning large language models (LLMs) to align with user preferences is\nchallenging due to the high cost of quality human annotations in Reinforcement\nLearning from Human Feedback (RLHF) and the generalizability limitations of AI\nFeedback. To address these challenges, we propose RLTHF, a human-AI hybrid\nframework that combines LLM-based initial alignment with selective human\nannotations to achieve full-human annotation alignment with minimal effort.\nRLTHF identifies hard-to-annotate samples mislabeled by LLMs using a reward\nmodel's reward distribution and iteratively enhances alignment by integrating\nstrategic human corrections while leveraging LLM's correctly labeled samples.\nEvaluations on HH-RLHF and TL;DR datasets show that RLTHF reaches full-human\nannotation-level alignment with only 6-7% of the human annotation effort.\nFurthermore, models trained on RLTHF's curated datasets for downstream tasks\noutperform those trained on fully human-annotated datasets, underscoring the\neffectiveness of RLTHF.", "AI": {"tldr": "This paper introduces RLTHF, a human-AI hybrid framework designed to fine-tune large language models with minimal human annotation effort by combining LLM initial alignment and selective human corrections.", "motivation": "Aligning large language models with user preferences is hampered by the cost and limitations of human annotations; RLTHF aims to reduce the required human effort while improving model alignment.", "method": "The RLTHF framework utilizes an LLM-based initial alignment and selectively incorporates human annotations on hard-to-annotate samples identified through a reward model's distribution, iteratively refining alignment using both LLM-labeled and human-corrected samples.", "result": "Experiments demonstrate that RLTHF achieves alignment comparable to full human annotation using only 6-7% of the typical human effort, and models trained on RLTHF datasets outperform those trained on fully annotated datasets.", "conclusion": "RLTHF highlights a more efficient approach to model alignment, achieving significant performance with reduced human resource investment.", "key_contributions": ["Introduction of RLTHF framework", "Demonstrated effectiveness in reducing human annotation effort", "Outperformed traditional human-annotated dataset models in downstream tasks"], "limitations": "", "keywords": ["Reinforcement Learning", "Human Feedback", "Large Language Models", "Model Alignment", "AI Annotation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.17383", "pdf": "https://arxiv.org/pdf/2502.17383.pdf", "abs": "https://arxiv.org/abs/2502.17383", "title": "Which Questions Improve Learning the Most? Utility Estimation of Questions with LM-based Simulations", "authors": ["Dong-Ho Lee", "Hyundong Cho", "Jonathan May", "Jay Pujara"], "categories": ["cs.CL"], "comment": "17 pages, 5 figures, 6 tables", "summary": "Asking good questions is critical for comprehension and learning, yet\nevaluating and generating such questions remains a challenging problem. Prior\nwork on inquisitive questions focuses on learner-generated, curiosity-driven\nqueries and evaluates them using indirect metrics, such as salience or\ninformation gain, that do not directly capture a question's impact on actual\nlearning outcomes. We introduce QUEST (Question Utility Estimation with\nSimulated Tests), a framework that uses language models to simulate learners\nand directly quantify the utility of a question - its contribution to exam\nperformance. QUEST simulates a learner who asks questions and receives answers\nwhile studying a textbook chapter, then uses them to take an end-of-chapter\nexam. Through this simulation, the utility of each question is estimated by its\ndirect effect on exam performance, rather than inferred indirectly based on the\nunderlying content. To support this evaluation, we curate TEXTBOOK-EXAM, a\nbenchmark that aligns textbook sections with end-of-section exam questions\nacross five academic disciplines. Using QUEST, we filter for high-utility\nquestions and fine-tune question generators via rejection sampling. Experiments\nshow that questions generated by QUEST-trained models improve simulated test\nscores by over 20% compared to strong baselines that are fine-tuned using\nindirect metrics or leverage prompting methods. Furthermore, utility is only\nweakly correlated with salience and similarity to exam questions, suggesting\nthat it captures unique signal that benefits downstream performance. QUEST\noffers a new outcome-driven paradigm for question evaluation and generation -\none that moves beyond question-answer content toward measurable improvements in\nlearning outcomes.", "AI": {"tldr": "This paper introduces QUEST, a framework for evaluating and generating learner-driven questions based on their direct impact on examination performance, using language models.", "motivation": "Evaluating and generating good questions that enhance learning is challenging, as prior metrics do not measure actual learning outcomes effectively.", "method": "The QUEST framework simulates learners asking questions while studying and measures the direct contribution of these questions to exam performance, using a curated benchmark called TEXTBOOK-EXAM.", "result": "Experiments show that questions generated by QUEST improve simulated test scores by over 20% compared to models using indirect evaluation metrics.", "conclusion": "QUEST provides a new paradigm for question evaluation and generation, focusing on measurable learning improvements rather than indirect metrics.", "key_contributions": ["Introduction of QUEST framework for question evaluation", "Curated TEXTBOOK-EXAM benchmark", "Demonstrated significant improvement in test scores using QUEST-generated questions"], "limitations": "The framework relies on simulations, and its applicability in real-world settings may vary.", "keywords": ["question generation", "learning outcomes", "language models", "human-computer interaction", "educational technology"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2503.19168", "pdf": "https://arxiv.org/pdf/2503.19168.pdf", "abs": "https://arxiv.org/abs/2503.19168", "title": "Language Model Uncertainty Quantification with Attention Chain", "authors": ["Yinghao Li", "Rushi Qiang", "Lama Moukheiber", "Chao Zhang"], "categories": ["cs.CL"], "comment": "36 pages, 7 figures, 36 tables", "summary": "Accurately quantifying a large language model's (LLM) predictive uncertainty\nis crucial for judging the reliability of its answers. While most existing\nresearch focuses on short, directly answerable questions with closed-form\noutputs (e.g., multiple-choice), involving intermediate reasoning steps in LLM\nresponses is increasingly important. This added complexity complicates\nuncertainty quantification (UQ) because the probabilities assigned to answer\ntokens are conditioned on a vast space of preceding reasoning tokens. Direct\nmarginalization is infeasible, and the dependency inflates probability\nestimates, causing overconfidence in UQ. To address this, we propose UQAC, an\nefficient method that narrows the reasoning space to a tractable size for\nmarginalization. UQAC iteratively constructs an \"attention chain\" of tokens\ndeemed \"semantically crucial\" to the final answer via a backtracking procedure.\nStarting from the answer tokens, it uses attention weights to identify the most\ninfluential predecessors, then iterates this process until reaching the input\ntokens. The resulting chain is further refined with similarity filtering and\nprobability thresholding, which reduce the reasoning space, facilitating the\napproximation of the marginal answer token probabilities. We validate UQAC on\nmultiple reasoning benchmarks with advanced open-source LLMs, demonstrating\nthat it consistently delivers reliable UQ estimates with high computational\nefficiency.", "AI": {"tldr": "Proposes UQAC, an efficient method for quantifying predictive uncertainty in large language models (LLMs) that involves intermediate reasoning steps.", "motivation": "Accurately quantifying predictive uncertainty in LLMs is essential for assessing the reliability of their answers, especially for complex questions requiring intermediate reasoning.", "method": "UQAC narrows the reasoning space by constructing an 'attention chain' of semantically crucial tokens for the final answer through a backtracking procedure, using attention weights to identify influential predecessors.", "result": "UQAC is validated on multiple reasoning benchmarks with advanced open-source LLMs, consistently delivering reliable uncertainty estimates while maintaining high computational efficiency.", "conclusion": "The approach enhances the ability to quantify uncertainty for LLMs in complex reasoning scenarios, addressing challenges posed by traditional methods of marginalization.", "key_contributions": ["Introduction of UQAC for efficient uncertainty quantification in LLMs", "Construction of an attention chain to narrow the reasoning space", "Empirical validation demonstrating improved reliability and efficiency in UQ estimation"], "limitations": "Limited to the frameworks and benchmarks used in validation; further exploration needed across diverse domains and models.", "keywords": ["predictive uncertainty", "large language models", "attention chains"], "importance_score": 9, "read_time_minutes": 36}}
{"id": "2503.24013", "pdf": "https://arxiv.org/pdf/2503.24013.pdf", "abs": "https://arxiv.org/abs/2503.24013", "title": "You Cannot Feed Two Birds with One Score: the Accuracy-Naturalness Tradeoff in Translation", "authors": ["Gergely Flamich", "David Vilar", "Jan-Thorsten Peter", "Markus Freitag"], "categories": ["cs.CL"], "comment": "Accepted to COLM 2025. Camera-ready version", "summary": "The goal of translation, be it by human or by machine, is, given some text in\na source language, to produce text in a target language that simultaneously 1)\npreserves the meaning of the source text and 2) achieves natural expression in\nthe target language. However, researchers in the machine translation community\nusually assess translations using a single score intended to capture semantic\naccuracy and the naturalness of the output simultaneously. In this paper, we\nbuild on recent advances in information theory to mathematically prove and\nempirically demonstrate that such single-score summaries do not and cannot give\nthe complete picture of a system's true performance. Concretely, we prove that\na tradeoff exists between accuracy and naturalness and demonstrate it by\nevaluating the submissions to the WMT24 shared task. Our findings help explain\nwell-known empirical phenomena, such as the observation that optimizing\ntranslation systems for a specific accuracy metric (like BLEU) initially\nimproves the system's naturalness, while ``overfitting'' the system to the\nmetric can significantly degrade its naturalness. Thus, we advocate for a\nchange in how translations are evaluated: rather than comparing systems using a\nsingle number, they should be compared on an accuracy-naturalness plane.", "AI": {"tldr": "This paper critiques the use of single-score evaluations in machine translation, demonstrating the tradeoff between accuracy and naturalness.", "motivation": "To address the inadequacy of single-score evaluations in machine translation and to propose a more nuanced evaluation framework.", "method": "The authors use information theory to mathematically prove the existence of a tradeoff between accuracy and naturalness in machine translation, supported by empirical data from WMT24 submissions.", "result": "The study shows that while optimizing for accuracy can initially improve naturalness, excessive optimization leads to a decrease in naturalness.", "conclusion": "The authors advocate for evaluating translations using a dual-axis approach that considers both accuracy and naturalness rather than a single score.", "key_contributions": ["Mathematical proof of the accuracy-naturalness tradeoff in translations.", "Empirical demonstration using WMT24 dataset.", "Proposal for a dual-axis evaluation framework for machine translation."], "limitations": "", "keywords": ["machine translation", "accuracy", "naturalness", "evaluation metrics", "information theory"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2504.00255", "pdf": "https://arxiv.org/pdf/2504.00255.pdf", "abs": "https://arxiv.org/abs/2504.00255", "title": "SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers", "authors": ["Yanzheng Xiang", "Hanqi Yan", "Shuyin Ouyang", "Lin Gui", "Yulan He"], "categories": ["cs.CL", "cs.AI", "cs.MA", "cs.SE"], "comment": null, "summary": "This study evaluates large language models (LLMs) in generating code from\nalgorithm descriptions in recent NLP papers. The task requires two key\ncompetencies: (1) algorithm comprehension: synthesizing information from papers\nand academic literature to understand implementation logic, and (2) coding\nexpertise: identifying dependencies and correctly implementing necessary APIs.\nTo facilitate rigorous evaluation, we introduce SciReplicate-Bench, a benchmark\nof 100 tasks from 36 NLP papers published in 2024, featuring detailed\nannotations and comprehensive test cases. Building on SciReplicate-Bench, we\npropose Sci-Reproducer, a dual-agent framework consisting of a Paper Agent that\ninterprets algorithmic concepts from literature and a Code Agent that retrieves\ndependencies from repositories and implements solutions. To assess algorithm\nunderstanding, we introduce reasoning graph accuracy, which quantifies\nsimilarity between generated and reference reasoning graphs derived from code\ncomments and structure. For evaluating implementation quality, we employ\nexecution accuracy, CodeBLEU, and repository dependency/API recall metrics. In\nour experiments, we evaluate various powerful non-reasoning and reasoning LLMs\nas foundational models. The best-performing LLM using \\ModelName~achieves only\n39% execution accuracy, highlighting the benchmark's difficulty. Our analysis\nidentifies missing or inconsistent algorithm descriptions as key barriers to\nsuccessful reproduction. We make available our benchmark and code at\nhttps://github.com/xyzCS/SciReplicate-Bench and project homepage at\nhttps://xyzcs.github.io/scireplicate.github.io/.", "AI": {"tldr": "This research evaluates LLMs in generating code from algorithms in recent NLP papers, introducing a benchmark and a dual-agent framework for improved understanding and implementation of algorithms.", "motivation": "To assess the capabilities of large language models in generating code from algorithm descriptions in academic literature and tackle the challenge of algorithm comprehension and coding expertise.", "method": "We introduce the SciReplicate-Bench benchmark with 100 tasks from 36 NLP papers and propose the Sci-Reproducer framework, which consists of a Paper Agent for interpreting algorithm concepts and a Code Agent for retrieving dependencies and implementing solutions.", "result": "The evaluation reveals the best-performing LLM achieving only 39% execution accuracy, indicating the complexity of the benchmark and the importance of clear algorithm descriptions.", "conclusion": "The study emphasizes the challenges in reproduction due to inconsistent algorithm descriptions and provides a resource for further research.", "key_contributions": ["Introduction of SciReplicate-Bench, a benchmark for task evaluation.", "Development of Sci-Reproducer, a dual-agent framework for code generation.", "Introduction of metrics for assessing algorithm understanding and implementation quality."], "limitations": "Challenges in successful reproduction due to missing or inconsistent algorithm descriptions.", "keywords": ["Large Language Models", "Algorithm Comprehension", "Code Generation", "NLP", "Benchmark"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.04377", "pdf": "https://arxiv.org/pdf/2504.04377.pdf", "abs": "https://arxiv.org/abs/2504.04377", "title": "PolyGuard: A Multilingual Safety Moderation Tool for 17 Languages", "authors": ["Priyanshu Kumar", "Devansh Jain", "Akhila Yerukola", "Liwei Jiang", "Himanshu Beniwal", "Thomas Hartvigsen", "Maarten Sap"], "categories": ["cs.CL"], "comment": "Accepted to COLM 2025 Main Conference", "summary": "Truly multilingual safety moderation efforts for Large Language Models (LLMs)\nhave been hindered by a narrow focus on a small set of languages (e.g.,\nEnglish, Chinese) as well as a limited scope of safety definition, resulting in\nsignificant gaps in moderation capabilities. To bridge these gaps, we release\nPOLYGUARD, a new state-of-the-art multilingual safety model for safeguarding\nLLM generations, and the corresponding training and evaluation datasets.\nPOLYGUARD is trained on POLYGUARDMIX, the largest multilingual safety training\ncorpus to date containing 1.91M samples across 17 languages (e.g., Chinese,\nCzech, English, Hindi). We also introduce POLYGUARDPROMPTS, a high quality\nmultilingual benchmark with 29K samples for the evaluation of safety\nguardrails. Created by combining naturally occurring multilingual human-LLM\ninteractions and human-verified machine translations of an English-only safety\ndataset (WildGuardMix; Han et al., 2024), our datasets contain prompt-output\npairs with labels of prompt harmfulness, response harmfulness, and response\nrefusal. Through extensive evaluations across multiple safety and toxicity\nbenchmarks, we demonstrate that POLYGUARD outperforms existing state-of-the-art\nopen-weight and commercial safety classifiers by 5.5%. Our contributions\nadvance efforts toward safer multilingual LLMs for all global users.", "AI": {"tldr": "POLYGUARD is a new multilingual safety model for Large Language Models, addressing gaps in moderation by providing comprehensive safety definitions and a large training corpus.", "motivation": "Current multilingual safety moderation for LLMs is limited to a few languages and lacks comprehensive safety definitions, creating significant gaps in moderation functionality.", "method": "POLYGUARD is trained on POLYGUARDMIX, a multilingual corpus with 1.91M samples in 17 languages, and evaluated with POLYGUARDPROMPTS, a benchmark of 29K samples for safety evaluations.", "result": "POLYGUARD outperforms existing state-of-the-art safety classifiers by 5.5%, showing improved moderation capabilities across multiple safety and toxicity benchmarks.", "conclusion": "The advancements provided by POLYGUARD help achieve safer multilingual LLMs, enhancing moderation for global users.", "key_contributions": ["Introduction of POLYGUARD and practical multilingual safety model for LLMs", "Release of POLYGUARDMIX and POLYGUARDPROMPTS datasets", "Demonstrated performance improvement over existing safety classifiers"], "limitations": "", "keywords": ["multilingual LLMs", "safety moderation", "POLYGUARD", "safety benchmarks", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.05598", "pdf": "https://arxiv.org/pdf/2504.05598.pdf", "abs": "https://arxiv.org/abs/2504.05598", "title": "DEL: Context-Aware Dynamic Exit Layer for Efficient Self-Speculative Decoding", "authors": ["Hossein Entezari Zarch", "Lei Gao", "Chaoyi Jiang", "Murali Annavaram"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Speculative Decoding (SD) is a widely used approach to accelerate the\ninference of large language models (LLMs) without reducing generation quality.\nIt operates by first using a compact model to draft multiple tokens\nefficiently, followed by parallel verification using the target LLM. This\napproach leads to faster inference compared to auto-regressive decoding. While\nthere are multiple approaches to create a draft model, one promising approach\nis to use early-exit methods. These methods draft candidate tokens by using a\nsubset of layers of the primary model and applying the remaining layers for\nverification, allowing a single model to handle both drafting and verification.\nWhile this technique reduces memory usage and computational cost, its\nperformance relies on the choice of the exit layer for drafting and the number\nof tokens drafted (speculation length) in each SD round. Prior works use\nhyperparameter exploration to statically select these values. However, our\nevaluations show that these hyperparameter values are task-specific, and even\nwithin a task they are dependent on the current sequence context. We introduce\nDEL (Dynamic Exit Layer), a plug-and-play method that adaptively selects the\nexit layer and speculation length during inference. DEL dynamically tracks the\ntoken acceptance rate if the tokens are drafted at each layer of an LLM and\nuses that knowledge to heuristically select the optimal exit layer and\nspeculation length. Our experiments across a broad range of models and\ndownstream tasks show that DEL achieves overall speedups of\n$2.16\\times$$\\sim$$2.62\\times$ over vanilla auto-regressive decoding and\nimproves upon state-of-the-art SD methods, which peak at $2.43\\times$, by up to\n$0.19\\times$. The code is available at https://github.com/hoenza/DEL.", "AI": {"tldr": "Introduction of DEL, a method to improve inference speed in LLMs via dynamic exit layer selection.", "motivation": "To enhance the inference speed of large language models while maintaining generation quality by addressing static hyperparameter selection in Speculative Decoding.", "method": "DEL employs dynamic tracking of token acceptance rates during inference to adaptively choose the exit layer and speculation length for improved performance.", "result": "DEL achieves speedups of 2.16x to 2.62x over standard auto-regressive decoding and enhances existing Speculative Decoding methods.", "conclusion": "The adaptive method shows significant improvements in inference efficiency across various models and tasks, offering a more effective approach to Speculative Decoding.", "key_contributions": ["Introduction of DEL for dynamic exit layer selection in LLMs", "Demonstrated significant speedup over conventional decoding methods", "Provided empirical evidence of performance improvements across multiple tasks"], "limitations": "", "keywords": ["Speculative Decoding", "Large Language Models", "Dynamic Exit Layer"], "importance_score": 9, "read_time_minutes": 15}}
