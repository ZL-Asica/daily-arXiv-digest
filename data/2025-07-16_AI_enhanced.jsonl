{"id": "2507.10773", "pdf": "https://arxiv.org/pdf/2507.10773.pdf", "abs": "https://arxiv.org/abs/2507.10773", "title": "Theory of Mind and Self-Disclosure to CUIs", "authors": ["Samuel Rhys Cox"], "categories": ["cs.HC", "cs.CL"], "comment": "Workshop paper presented at ToMinHAI at CUI'2025: Theory of Mind in\n  Human-CUI Interaction, held in conjunction with the 2025 ACM conference on\n  Conversational User Interfaces, July 8th, 2025. 4 pages. 3 figures", "summary": "Self-disclosure is important to help us feel better, yet is often difficult.\nThis difficulty can arise from how we think people are going to react to our\nself-disclosure. In this workshop paper, we briefly discuss self-disclosure to\nconversational user interfaces (CUIs) in relation to various social cues. We\nthen, discuss how expressions of uncertainty or representation of a CUI's\nreasoning could help encourage self-disclosure, by making a CUI's intended\n\"theory of mind\" more transparent to users.", "AI": {"tldr": "This workshop paper explores the role of self-disclosure in conversational user interfaces (CUIs) and suggests that making CUIs' reasoning more transparent may encourage users to share more.", "motivation": "The paper aims to address the challenges of self-disclosure in interactions with conversational user interfaces and how social cues influence these interactions.", "method": "The authors discuss various social cues related to self-disclosure in CUIs and propose that expressing uncertainty or clarifying a CUI's reasoning can facilitate user self-disclosure.", "result": "The findings suggest that enhancing transparency regarding a CUI's reasoning can lead to greater user comfort in self-disclosure.", "conclusion": "By improving the transparency of CUIs' 'theory of mind', it may be possible to foster deeper user engagement and communication.", "key_contributions": ["Analyzes the impact of social cues on self-disclosure in CUIs.", "Proposes methods for exposing CUI reasoning to users.", "Highlights the relationship between user self-disclosure and conversational transparency."], "limitations": "", "keywords": ["self-disclosure", "conversational user interfaces", "theory of mind", "social cues", "user engagement"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.10812", "pdf": "https://arxiv.org/pdf/2507.10812.pdf", "abs": "https://arxiv.org/abs/2507.10812", "title": "React to This (RTT): A Nonverbal Turing Test for Embodied AI", "authors": ["Chuxuan Zhang", "Yasaman Etesam", "Angelica Lim"], "categories": ["cs.HC", "cs.AI"], "comment": "5 pages, 3 figures", "summary": "We propose an approach to test embodied AI agents for interaction awareness\nand believability, particularly in scenarios where humans push them to their\nlimits. Turing introduced the Imitation Game as a way to explore the question:\n\"Can machines think?\" The Total Turing Test later expanded this concept beyond\npurely verbal communication, incorporating perceptual and physical interaction.\nBuilding on this, we propose a new guiding question: \"Can machines react?\" and\nintroduce the React to This (RTT) test for nonverbal behaviors, presenting\nresults from an initial experiment.", "AI": {"tldr": "This paper introduces the React to This (RTT) test to evaluate embodied AI agents' interaction awareness and believability through nonverbal behavior.", "motivation": "To investigate how embodied AI agents can react in interactive scenarios, particularly under challenging conditions posed by human users.", "method": "The paper proposes the RTT test, which assesses nonverbal behaviors of AI agents during interactions, building on the principles of Turing's tests.", "result": "Initial experiments demonstrate the RTT test's potential to capture the believability and interaction awareness of embodied AI agents.", "conclusion": "The RTT test is a promising framework for evaluating AI agents in contexts where reactive nonverbal communication is crucial for user interaction.", "key_contributions": ["Introduction of the React to This (RTT) test for assessing nonverbal behaviors in AI", "Expansion of Turing's concepts to focus on interactive reactions of AI agents", "Initial experimental results showcasing the viability of the RTT test"], "limitations": "The study presents preliminary findings; further research is required to validate results across diverse scenarios.", "keywords": ["Embodied AI", "Interaction Awareness", "Believability", "Nonverbal Behavior", "React to This (RTT) Test"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2507.10813", "pdf": "https://arxiv.org/pdf/2507.10813.pdf", "abs": "https://arxiv.org/abs/2507.10813", "title": "Static or Temporal? Semantic Scene Simplification to Aid Wayfinding in Immersive Simulations of Bionic Vision", "authors": ["Justin M. Kasowski", "Apurv Varshney", "Michael Beyeler"], "categories": ["cs.HC"], "comment": null, "summary": "Visual neuroprostheses (bionic eye) aim to restore a rudimentary form of\nvision by translating camera input into patterns of electrical stimulation. To\nimprove scene understanding under extreme resolution and bandwidth constraints,\nprior work has explored computer vision techniques such as semantic\nsegmentation and depth estimation. However, presenting all task-relevant\ninformation simultaneously can overwhelm users in cluttered environments. We\ncompare two complementary approaches to semantic preprocessing in immersive\nvirtual reality: SemanticEdges, which highlights all relevant objects at once,\nand SemanticRaster, which staggers object categories over time to reduce visual\nclutter. Using a biologically grounded simulation of prosthetic vision, 18\nsighted participants performed a wayfinding task in a dynamic urban environment\nacross three conditions: edge-based baseline (Control), SemanticEdges, and\nSemanticRaster. Both semantic strategies improved performance and user\nexperience relative to the baseline, with each offering distinct trade-offs:\nSemanticEdges increased the odds of success, while SemanticRaster boosted the\nlikelihood of collision-free completions. These findings underscore the value\nof adaptive semantic preprocessing for prosthetic vision and, more broadly, may\ninform the design of low-bandwidth visual interfaces in XR that must balance\ninformation density, task relevance, and perceptual clarity.", "AI": {"tldr": "This paper investigates the use of two semantic preprocessing techniques in virtual reality for visual neuroprostheses, aimed at improving user experience and task performance under bandwidth constraints.", "motivation": "To improve scene understanding for users of visual neuroprostheses under significant resolution and bandwidth constraints while ensuring that the information presented does not overwhelm users in complex environments.", "method": "The study compares two approaches, SemanticEdges and SemanticRaster, using a biologically grounded simulation of prosthetic vision in a wayfinding task across three conditions: Control, SemanticEdges, and SemanticRaster.", "result": "Both semantic strategies improved performance and user experience compared to the baseline, with SemanticEdges enhancing success odds and SemanticRaster increasing the likelihood of collision-free completions.", "conclusion": "The findings highlight the importance of adaptive semantic preprocessing in prosthetic vision and can guide the design of low-bandwidth visual interfaces that manage information density and perceptual clarity.", "key_contributions": ["Comparison of SemanticEdges and SemanticRaster in virtual reality for enhancing prosthetic vision.", "Demonstration of distinct trade-offs in improving user performance and experience in cluttered environments.", "Insights into designing low-bandwidth interfaces in XR contexts."], "limitations": "The study is constrained to a simulated environment; real-world applicability remains to be tested.", "keywords": ["visual neuroprostheses", "semantic preprocessing", "immersive virtual reality", "user experience", "wayfinding"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.10963", "pdf": "https://arxiv.org/pdf/2507.10963.pdf", "abs": "https://arxiv.org/abs/2507.10963", "title": "AROMA: Mixed-Initiative AI Assistance for Non-Visual Cooking by Grounding Multi-modal Information Between Reality and Videos", "authors": ["Zheng Ning", "Leyang Li", "Daniel Killough", "JooYoung Seo", "Patrick Carrington", "Yapeng Tian", "Yuhang Zhao", "Franklin Mingzhe Li", "Toby Jia-Jun Li"], "categories": ["cs.HC"], "comment": null, "summary": "Videos offer rich audiovisual information that can support people in\nperforming activities of daily living (ADLs), but they remain largely\ninaccessible to blind or low-vision (BLV) individuals. In cooking, BLV people\noften rely on non-visual cues, such as touch, taste, and smell, to navigate\ntheir environment, making it difficult to follow the predominantly audiovisual\ninstructions found in video recipes. To address this problem, we introduce\nAROMA, an AI system that provides timely responses to the user based on\nreal-time, context-aware assistance by integrating non-visual cues perceived by\nthe user, a wearable camera feed, and video recipe content. AROMA uses a\nmixed-initiative approach: it responds to user requests while also proactively\nmonitoring the video stream to offer timely alerts and guidance. This\ncollaborative design leverages the complementary strengths of the user and AI\nsystem to align the physical environment with the video recipe, helping the\nuser interpret their current cooking state and make sense of the steps. We\nevaluated AROMA through a study with eight BLV participants and offered\ninsights for designing interactive AI systems to support BLV individuals in\nperforming ADLs.", "AI": {"tldr": "AROMA is an AI system designed to assist blind or low-vision individuals in cooking by integrating non-visual cues with video recipe content.", "motivation": "To make cooking videos more accessible to blind or low-vision individuals, who face challenges in following audiovisual instructions.", "method": "AROMA employs a mixed-initiative approach, using a wearable camera and context-aware responses to integrate non-visual cues into the cooking process.", "result": "The system was evaluated with eight participants, providing insights into its effectiveness in assisting BLV individuals in performing daily cooking tasks.", "conclusion": "The collaborative design of AROMA helps BLV users interpret their cooking environment and enhances their ability to follow video recipes.", "key_contributions": ["Introduction of AROMA, an AI system for blind and low-vision users", "Mixed-initiative approach combining user requests and proactive monitoring", "Insights from user evaluations that inform the design of interactive AI systems for BLV individuals"], "limitations": "Limited to a specific user group (BLV participants) and focused on cooking tasks.", "keywords": ["AROMA", "Blind and Low-Vision", "Cooking assistance", "AI systems", "Mixed-initiative"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.10577", "pdf": "https://arxiv.org/pdf/2507.10577.pdf", "abs": "https://arxiv.org/abs/2507.10577", "title": "Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos and influence opinions", "authors": ["Logé Cécile", "Ghori Rehan"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Misinformation poses a significant threat in today's digital world, often\nspreading rapidly through platforms like YouTube. This paper introduces a novel\napproach to combating misinformation by developing an AI-powered system that\nnot only fact-checks claims made in YouTube videos but also actively engages\nusers in the comment section and challenge misleading narratives. Our system\ncomprises two main agents: Truth Sleuth and Trend Bender.\n  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented\nGeneration (RAG) approach - drawing on sources like Wikipedia, Google Search,\nGoogle FactCheck - to accurately assess their veracity and generates a nuanced\nand comprehensive report. Through rigorous prompt engineering, Trend Bender\nleverages this report along with a curated corpus of relevant articles to\ngenerate insightful and persuasive comments designed to stimulate a productive\ndebate. With a carefully set up self-evaluation loop, this agent is able to\niteratively improve its style and refine its output.\n  We demonstrate the system's capabilities through experiments on established\nbenchmark datasets and a real-world deployment on YouTube, showcasing its\npotential to engage users and potentially influence perspectives. Our findings\nhighlight the high accuracy of our fact-checking agent, and confirm the\npotential of AI-driven interventions in combating misinformation and fostering\na more informed online space.", "AI": {"tldr": "This paper presents an AI system for fact-checking claims in YouTube videos and engaging users in discussions to counter misinformation.", "motivation": "To address the growing issue of misinformation on digital platforms, specifically YouTube, by utilizing AI to fact-check and engage users.", "method": "The system comprises two agents: Truth Sleuth for extracting and verifying claims using a RAG approach, and Trend Bender for generating engaging comments to challenge misinformation.", "result": "The system showed high accuracy in its fact-checking abilities and demonstrated potential for user engagement and influencing perspectives in real-world YouTube deployments.", "conclusion": "AI-driven interventions can effectively combat misinformation and promote a more informed online community.", "key_contributions": ["Introduction of an AI-powered misinformation combat system for YouTube.", "Development of Truth Sleuth and Trend Bender agents for fact-checking and user engagement.", "Demonstration of high accuracy and engagement in real-world settings."], "limitations": "", "keywords": ["misinformation", "YouTube", "fact-checking", "AI", "user engagement"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.10967", "pdf": "https://arxiv.org/pdf/2507.10967.pdf", "abs": "https://arxiv.org/abs/2507.10967", "title": "Self++: Merging Human and AI for Co-Determined XR Living in the Metaverse", "authors": ["Thammathip Piumsomboon"], "categories": ["cs.HC"], "comment": null, "summary": "This position paper introduces Self++, a novel nine-level framework for\nco-determined living in the Metaverse, grounded in Self-Determination Theory.\nSelf++ prioritises human flourishing by progressively cultivating competence,\nautonomy, and relatedness through dynamic human-AI collaboration in extended\nreality (XR). Unlike technologically deterministic approaches, Self++\nemphasises user empowerment by enhancing competency, mitigating cognitive\nbiases and leveraging XR's immersive capabilities. Key research directions\nproposed include exploring the boundaries of user-defined AI autonomy,\ndesigning for meaningful social connection in XR, and establishing proactive\nethical safeguards. Ultimately, Self++ offers a roadmap for creating a\nhuman-centred, AI-enhanced Metaverse where technology amplifies, rather than\ndiminishes, human potential.", "AI": {"tldr": "This paper presents Self++, a framework for co-determined living in the Metaverse that focuses on human flourishing through dynamic human-AI collaboration in XR.", "motivation": "To create a human-centred, AI-enhanced Metaverse that fosters human potential rather than diminishing it.", "method": "The framework is grounded in Self-Determination Theory and emphasizes user empowerment and meaningful social connections in XR.", "result": "Self++ proposes research directions for AI autonomy, social connection in XR, and ethical safeguards.", "conclusion": "The roadmap provided by Self++ aims to enhance human competency and address cognitive biases in technology use.", "key_contributions": ["Introduction of the Self++ framework.", "Emphasis on human flourishing through AI collaboration.", "Proposals for ethical safeguards in XR."], "limitations": "", "keywords": ["Self-Determination Theory", "Metaverse", "Human-AI Collaboration", "Extended Reality", "Ethical Safeguards"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.10580", "pdf": "https://arxiv.org/pdf/2507.10580.pdf", "abs": "https://arxiv.org/abs/2507.10580", "title": "An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation", "authors": ["Vimaleswar A", "Prabhu Nandan Sahu", "Nilesh Kumar Sahu", "Haroon R Lone"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Mental health plays a crucial role in the overall well-being of an\nindividual. In recent years, digital platforms have been increasingly used to\nexpand mental health and emotional support. However, there are persistent\nchallenges related to limited user accessibility, internet connectivity, and\ndata privacy, which highlight the need for an offline, smartphone-based\nsolution. To address these challenges, we propose EmoSApp (Emotional Support\nApp): an entirely offline, smartphone-based conversational app designed for\nmental health and emotional support. The system leverages Large Language Models\n(LLMs), specifically fine-tuned, quantized and deployed using Torchtune and\nExecutorch for resource-constrained devices, allowing all inferences to occur\non the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned\nthe LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of\n14,582 mental-health QA pairs, along with the multi-turn conversational data.\n  Through qualitative human evaluation with the student population, we\ndemonstrate that EmoSApp has the ability to respond coherently, empathetically,\nmaintain interactive dialogue, and provide relevant suggestions to user's\nmental health problems. Additionally, quantitative evaluations on nine standard\ncommonsense and reasoning benchmarks demonstrate the efficacy of our\nfine-tuned, quantized model in low-resource settings. By prioritizing on-device\ndeployment and specialized domain adaptation, EmoSApp serves as a blueprint for\nfuture innovations in portable, secure, and highly tailored AI-driven mental\nhealth solutions.", "AI": {"tldr": "EmoSApp is an offline smartphone app leveraging LLMs for mental health support, addressing accessibility and privacy issues.", "motivation": "To address challenges related to limited accessibility, internet connectivity, and data privacy in mental health support applications.", "method": "EmoSApp uses fine-tuned LLaMA-3.2-1B-Instruct model on a custom dataset of mental-health Q&A pairs and multi-turn dialogues, allowing inferences to happen locally on smartphones.", "result": "Qualitative evaluations show EmoSApp's capabilities in coherent, empathetic responses and interactive dialogue, while quantitative assessments confirm its efficacy in low-resource settings.", "conclusion": "EmoSApp demonstrates potential for portable, secure AI-driven mental health solutions, serving as a blueprint for future innovations.", "key_contributions": ["Development of an offline, smartphone-based mental health app", "Utilization of fine-tuned LLMs for domain-specific knowledge", "Demonstrated efficacy in delivering mental health support under resource constraints"], "limitations": "", "keywords": ["Mental Health", "Emotional Support", "Large Language Models", "Offline Applications", "Conversational AI"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.10970", "pdf": "https://arxiv.org/pdf/2507.10970.pdf", "abs": "https://arxiv.org/abs/2507.10970", "title": "Terms and Conditions (Do Not) Apply: Understanding Exploitation Disparities in Design of Mobile-Based Financial Services", "authors": ["Lindah Kotut"], "categories": ["cs.HC"], "comment": "Accepted for Publication at The 5th Biennial African Human Computer\n  Interaction Conference (AfriCHI 2025). 10 pages (excluding references), 3\n  figures", "summary": "Mobile-based financial services have made it possible for the traditionally\nunbanked to access infrastructure that have been routinely unattainable.\nResearchers have explored how these systems have made for safer environments to\nsend and receive money and have expanded financial opportunities such as\nincreased borrowing. With this expansion, challenges such as detrimental\ninterest rates, lack of access to policy documents, and inadequate user\nprotective guardrails emerge, amplifying the risks due to technology-aided\nunethical financial practices that are aided by design patterns. Supported by\nuser interviews, we detail user experiences of mobile-based financial\ntransactions and explore the foundations and guidelines that undergird the\nfinancial service provisions: highlighting both affordances and harms enabled\nin the design of such systems. We discuss the findings by highlighting\nfinancial exploitation disparities, deliberating strategies for mitigation of\nrisks and enabling recovery from harms caused by the technology use. We then\nrecommend guidelines for empowering design approaches that support users'\nmechanisms of trust, their understanding of technological processes, and\ndetermination of risks.", "AI": {"tldr": "This paper explores user experiences with mobile-based financial services, analyzing both their benefits and risks, and proposes guidelines for design improvements that empower users.", "motivation": "The study investigates the impact of mobile financial services on traditionally unbanked populations, emphasizing the inherent challenges and risks associated with their design and usage.", "method": "User interviews were conducted to gather qualitative insights on users' experiences and the design patterns of mobile-based financial transaction systems.", "result": "Findings reveal significant user exploitation and risks due to unethical financial practices perpetuated by design patterns, alongside suggested strategies for mitigation and recovery from harms.", "conclusion": "The paper concludes with recommendations for design guidelines that enhance user trust and understanding, ultimately aiming to protect users from the financial risks associated with mobile technologies.", "key_contributions": ["Detailed user experiences of mobile financial services", "Identification of risks and unethical practices due to design patterns", "Recommendations for user-centered design improvements"], "limitations": "", "keywords": ["mobile financial services", "user experience", "HCI", "financial exploitation", "design guidelines"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.10582", "pdf": "https://arxiv.org/pdf/2507.10582.pdf", "abs": "https://arxiv.org/abs/2507.10582", "title": "Transforming Sensitive Documents into Quantitative Data: An AI-Based Preprocessing Toolchain for Structured and Privacy-Conscious Analysis", "authors": ["Anders Ledberg", "Anna Thalén"], "categories": ["cs.CL", "stat.ME"], "comment": null, "summary": "Unstructured text from legal, medical, and administrative sources offers a\nrich but underutilized resource for research in public health and the social\nsciences. However, large-scale analysis is hampered by two key challenges: the\npresence of sensitive, personally identifiable information, and significant\nheterogeneity in structure and language. We present a modular toolchain that\nprepares such text data for embedding-based analysis, relying entirely on\nopen-weight models that run on local hardware, requiring only a\nworkstation-level GPU and supporting privacy-sensitive research.\n  The toolchain employs large language model (LLM) prompting to standardize,\nsummarize, and, when needed, translate texts to English for greater\ncomparability. Anonymization is achieved via LLM-based redaction, supplemented\nwith named entity recognition and rule-based methods to minimize the risk of\ndisclosure. We demonstrate the toolchain on a corpus of 10,842 Swedish court\ndecisions under the Care of Abusers Act (LVM), comprising over 56,000 pages.\nEach document is processed into an anonymized, standardized summary and\ntransformed into a document-level embedding. Validation, including manual\nreview, automated scanning, and predictive evaluation shows the toolchain\neffectively removes identifying information while retaining semantic content.\nAs an illustrative application, we train a predictive model using embedding\nvectors derived from a small set of manually labeled summaries, demonstrating\nthe toolchain's capacity for semi-automated content analysis at scale.\n  By enabling structured, privacy-conscious analysis of sensitive documents,\nour toolchain opens new possibilities for large-scale research in domains where\ntextual data was previously inaccessible due to privacy and heterogeneity\nconstraints.", "AI": {"tldr": "A modular toolchain for analyzing unstructured text data while ensuring privacy and standardization using LLMs.", "motivation": "To tackle the challenges of sensitive information and text heterogeneity in public health and social sciences research using unstructured text.", "method": "The toolchain uses LLM prompting for summarization, standardization, translation, and LLM-based redaction, combined with entity recognition and rule-based methods for anonymization.", "result": "Demonstrated effective anonymization and standardization on a corpus of Swedish court decisions, maintaining semantic content while processing into document-level embeddings.", "conclusion": "The toolchain facilitates large-scale, privacy-sensitive analysis of previously inaccessible textual data in research.", "key_contributions": ["Development of a modular toolchain for text analysis", "Use of LLMs for summarization and anonymization", "Successful application to a large dataset of legal documents"], "limitations": "", "keywords": ["unstructured text", "privacy-sensitive research", "large language models", "anonymization", "text analysis"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.10981", "pdf": "https://arxiv.org/pdf/2507.10981.pdf", "abs": "https://arxiv.org/abs/2507.10981", "title": "An Exploratory Study on AI-driven Visualisation Techniques on Decision Making in Extended Reality", "authors": ["Ze Dong", "Binyang Han", "Jingjing Zhang", "Ruoyu Wen", "Barrett Ens", "Adrian Clark", "Tham Piumsomboon"], "categories": ["cs.HC"], "comment": null, "summary": "The integration of extended reality (XR) with artificial intelligence (AI)\nintroduces a new paradigm for user interaction, enabling AI to perceive user\nintent, stimulate the senses, and influence decision-making. We explored the\nimpact of four AI-driven visualisation techniques -- `Inform,' `Nudge,'\n`Recommend,' and `Instruct' -- on user decision-making in XR using the Meta\nQuest Pro. To test these techniques, we used a pre-recorded 360-degree video of\na supermarket, overlaying each technique through a virtual interface. We aimed\nto investigate how these different visualisation techniques with different\nlevels of user autonomy impact preferences and decision-making. An exploratory\nstudy with semi-structured interviews provided feedback and design\nrecommendations. Our findings emphasise the importance of maintaining user\nautonomy, enhancing AI transparency to build trust, and considering context in\nvisualisation design.", "AI": {"tldr": "The paper explores how AI-driven visualization techniques in XR affect user decision-making and preferences, highlighting the importance of user autonomy and AI transparency.", "motivation": "To investigate the impact of AI-driven visualisation techniques on user decision-making in XR environments.", "method": "An exploratory study using a pre-recorded 360-degree video of a supermarket, overlaid with four AI-driven visualisation techniques during semi-structured interviews.", "result": "The study found that different visualisation techniques affect user preferences and decision-making, emphasizing user autonomy and the need for AI transparency.", "conclusion": "Maintaining user autonomy and enhancing AI transparency are crucial for effective decision-making in XR environments.", "key_contributions": ["Analysis of four AI-driven visualisation techniques: Inform, Nudge, Recommend, Instruct", "The role of user autonomy and trust in AI-driven decision-making", "Design recommendations for effective visualisation in XR applications"], "limitations": "Limited to a single XR application and context; further studies needed to generalize findings.", "keywords": ["Extended Reality", "Artificial Intelligence", "User Decision-Making", "Visualisation Techniques", "User Autonomy"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.10585", "pdf": "https://arxiv.org/pdf/2507.10585.pdf", "abs": "https://arxiv.org/abs/2507.10585", "title": "A Taxonomy for Design and Evaluation of Prompt-Based Natural Language Explanations", "authors": ["Isar Nejadgholi", "Mona Omidyeganeh", "Marc-Antoine Drouin", "Jonathan Boisvert"], "categories": ["cs.CL", "cs.AI"], "comment": "Presented at the Workshop of Technical AI Governance, 5 pages 2\n  figures", "summary": "Effective AI governance requires structured approaches for stakeholders to\naccess and verify AI system behavior. With the rise of large language models,\nNatural Language Explanations (NLEs) are now key to articulating model\nbehavior, which necessitates a focused examination of their characteristics and\ngovernance implications. We draw on Explainable AI (XAI) literature to create\nan updated XAI taxonomy, adapted to prompt-based NLEs, across three dimensions:\n(1) Context, including task, data, audience, and goals; (2) Generation and\nPresentation, covering generation methods, inputs, interactivity, outputs, and\nforms; and (3) Evaluation, focusing on content, presentation, and user-centered\nproperties, as well as the setting of the evaluation. This taxonomy provides a\nframework for researchers, auditors, and policymakers to characterize, design,\nand enhance NLEs for transparent AI systems.", "AI": {"tldr": "This paper presents a taxonomy for Natural Language Explanations (NLEs) based on three dimensions: context, generation and presentation, and evaluation, to enhance AI governance.", "motivation": "To improve AI governance by providing structured methods for stakeholders to access and verify AI system behaviors, particularly through natural language explanations.", "method": "The authors develop an updated taxonomy for Explainable AI (XAI) based on existing literature, tailored to the prompt-based nature of NLEs across three dimensions.", "result": "The taxonomy categorizes NLEs into context, generation and presentation, and evaluation, offering a systematic approach to enhance the transparency and understanding of AI systems.", "conclusion": "The taxonomy serves as a valuable framework for researchers, auditors, and policymakers, aiding in the characterization and design of NLEs to foster transparent AI systems.", "key_contributions": ["An updated XAI taxonomy adapted to prompt-based NLEs", "A structured approach across three informative dimensions", "Framework for stakeholders to improve AI transparency"], "limitations": "", "keywords": ["AI governance", "Natural Language Explanations", "Explainable AI", "NLE taxonomy", "user-centered evaluation"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2507.11210", "pdf": "https://arxiv.org/pdf/2507.11210.pdf", "abs": "https://arxiv.org/abs/2507.11210", "title": "Role-Playing LLM-Based Multi-Agent Support Framework for Detecting and Addressing Family Communication Bias", "authors": ["Rushia Harada", "Yuken Kimura", "Keito Inoshita"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Well-being in family settings involves subtle psychological dynamics that\nconventional metrics often overlook. In particular, unconscious parental\nexpectations, termed ideal parent bias, can suppress children's emotional\nexpression and autonomy. This suppression, referred to as suppressed emotion,\noften stems from well-meaning but value-driven communication, which is\ndifficult to detect or address from outside the family. Focusing on these\nlatent dynamics, this study explores Large Language Model (LLM)-based support\nfor psychologically safe family communication. We constructed a Japanese\nparent-child dialogue corpus of 30 scenarios, each annotated with metadata on\nideal parent bias and suppressed emotion. Based on this corpus, we developed a\nRole-Playing LLM-based multi-agent dialogue support framework that analyzes\ndialogue and generates feedback. Specialized agents detect suppressed emotion,\ndescribe implicit ideal parent bias in parental speech, and infer contextual\nattributes such as the child's age and background. A meta-agent compiles these\noutputs into a structured report, which is then passed to five selected expert\nagents. These agents collaboratively generate empathetic and actionable\nfeedback through a structured four-step discussion process. Experiments show\nthat the system can detect categories of suppressed emotion with moderate\naccuracy and produce feedback rated highly in empathy and practicality.\nMoreover, simulated follow-up dialogues incorporating this feedback exhibited\nsigns of improved emotional expression and mutual understanding, suggesting the\nframework's potential in supporting positive transformation in family\ninteractions.", "AI": {"tldr": "The study explores an LLM-based framework designed to support family communication by detecting suppressed emotions and ideal parent bias, aiming to enhance emotional expression and autonomy in children.", "motivation": "Conventional metrics often overlook the subtle psychological dynamics that affect well-being in family settings, particularly the influence of ideal parent bias on children's emotional expression.", "method": "A Role-Playing LLM-based multi-agent dialogue support framework was constructed using a Japanese parent-child dialogue corpus, which analyzes dialogue, detects suppressed emotion, and generates comprehensive feedback.", "result": "The system shows moderate accuracy in detecting suppressed emotions and produces empathetic and practical feedback, leading to improved emotional expression and mutual understanding in simulated follow-up dialogues.", "conclusion": "The framework demonstrates potential for enhancing family communication and fostering emotional safety, with implications for positive transformation in interpersonal relationships.", "key_contributions": ["Developed a multi-agent framework for analyzing family dialogues", "Established a corpus for studying ideal parent bias and suppressed emotion", "Demonstrated the ability to generate actionable feedback that improves emotional communication"], "limitations": "", "keywords": ["family dynamics", "ideal parent bias", "suppressed emotion", "LLM", "dialogue support"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.10586", "pdf": "https://arxiv.org/pdf/2507.10586.pdf", "abs": "https://arxiv.org/abs/2507.10586", "title": "AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight Adapters", "authors": ["Kaushik Dwivedi", "Padmanabh Patanjali Mishra"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable fluency across a\nrange of natural language tasks, yet remain vulnerable to hallucinations -\nfactual inaccuracies that undermine trust in real world deployment. We present\nAutoRAG-LoRA, a modular framework for Retrieval-Augmented Generation (RAG) that\ntackles hallucination in large language models through lightweight LoRA-based\nadapters and KL-regularized training. Our pipeline integrates automated prompt\nrewriting, hybrid retrieval, and low-rank adapter tuning to ground responses in\nretrieved evidence. A hallucination detection module, using both\nclassifier-based and self-evaluation techniques, assigns confidence scores to\ngenerated outputs, triggering an optional feedback correction loop. This loop\nenforces factual alignment via contrastive KL loss and adapter fine tuning. We\ndemonstrate that AutoRAG-LoRA significantly reduces the factual drift while\npreserving the efficiency and modularity of the model.", "AI": {"tldr": "AutoRAG-LoRA is a framework designed to reduce hallucinations in Large Language Models by using lightweight adapters and KL-regularized training.", "motivation": "LLMs show fluency in language tasks but are prone to hallucinations, which affect their reliability in real-world applications.", "method": "The AutoRAG-LoRA framework incorporates automated prompt rewriting, hybrid retrieval, and low-rank adapter tuning to ground responses in retrieved data. A hallucination detection module evaluates generated outputs and enables a feedback correction loop.", "result": "AutoRAG-LoRA markedly decreases factual drift while maintaining model efficiency and modularity.", "conclusion": "The integration of the detection module and the feedback correction loop enhances the factual accuracy of outputs from LLMs.", "key_contributions": ["Introduction of AutoRAG-LoRA framework for LLMs", "Implementation of hallucination detection using classifiers and self-evaluation", "Effectiveness in reducing factual inaccuracies while preserving model efficiency."], "limitations": "", "keywords": ["Large Language Models", "Retrieval-Augmented Generation", "hallucinations", "LoRA", "factual accuracy"], "importance_score": 10, "read_time_minutes": 5}}
{"id": "2507.11470", "pdf": "https://arxiv.org/pdf/2507.11470.pdf", "abs": "https://arxiv.org/abs/2507.11470", "title": "REVA: Supporting LLM-Generated Programming Feedback Validation at Scale Through User Attention-based Adaptation", "authors": ["Xiaohang Tang", "Sam Wong", "Zicheng He", "Yalong Yang", "Yan Chen"], "categories": ["cs.HC"], "comment": null, "summary": "This paper introduces REVA, a human-AI system that expedites instructor\nreview of voluminous AI-generated programming feedback by sequencing\nsubmissions to minimize cognitive context shifts and propagating\ninstructor-driven revisions across semantically similar instances. REVA\nintroduces a novel approach to human-AI collaboration in educational feedback\nby adaptively learning from instructors' attention in the review and revision\nprocess to continuously improve the feedback validation process. REVA's\nusefulness and effectiveness in improving feedback quality and the overall\nfeedback review process were evaluated through a within-subjects lab study with\n12 participants.", "AI": {"tldr": "REVA is a human-AI system that streamlines the instructor review of AI-generated programming feedback by reducing cognitive shifts and refining feedback based on instructor input.", "motivation": "To enhance the quality of AI-generated feedback in programming education and make the review process more efficient for instructors.", "method": "A within-subjects lab study was conducted with 12 participants to evaluate REVA's effectiveness in improving the feedback review process.", "result": "The study showed that REVA effectively improves feedback quality and reduces cognitive context shifts during the review process for instructors.", "conclusion": "REVA demonstrates significant potential for enhancing human-AI collaboration in educational contexts by adaptively learning from instructor feedback.", "key_contributions": ["Novel methodology for sequencing submissions to reduce cognitive context shifts.", "Adaptive learning from instructor attention during feedback review.", "Evaluation of effectiveness through empirical study."], "limitations": "Limited sample size of 12 participants may affect the generalizability of the results.", "keywords": ["Human-AI collaboration", "Educational feedback", "Cognitive context shifts"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.10587", "pdf": "https://arxiv.org/pdf/2507.10587.pdf", "abs": "https://arxiv.org/abs/2507.10587", "title": "Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing", "authors": ["Dennis Ulmer", "Alexandra Lorson", "Ivan Titov", "Christian Hardmeier"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Human users increasingly rely on natural language interactions with large\nlanguage models (LLMs) in order to receive help on a large variety of tasks and\nproblems. However, the trustworthiness and perceived legitimacy of LLMs is\nundermined by the fact that their output is frequently stated in very confident\nterms, even when its accuracy is questionable. Therefore, there is a need to\nsignal the confidence of the language model to a user in order to reap the\nbenefits of human-machine collaboration and mitigate potential harms.\nVerbalized uncertainty is the expression of confidence with linguistic means,\nan approach that integrates perfectly into language-based interfaces.\nNevertheless, most recent research in natural language processing (NLP)\noverlooks the nuances surrounding human uncertainty communication and the data\nbiases that influence machine uncertainty communication. We argue for\nanthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty\ncommunication requires a degree of linguistic authenticity and personalization\nto the user, which could be achieved by emulating human communication. We\npresent a thorough overview over the research in human uncertainty\ncommunication, survey ongoing research, and perform additional analyses to\ndemonstrate so-far overlooked biases in verbalized uncertainty. We conclude by\npointing out unique factors in human-machine communication of uncertainty and\ndeconstruct anthropomimetic uncertainty into future research directions for\nNLP.", "AI": {"tldr": "The paper addresses the need for reliable uncertainty communication in interactions between humans and large language models (LLMs) to improve trust and utility. It discusses the concept of anthropomimetic uncertainty, emphasizing the importance of authentic and personalized communication.", "motivation": "To improve the trustworthiness and perceived legitimacy of LLMs by effectively communicating uncertainty and mitigating potential harms.", "method": "The paper provides a thorough overview of existing research on human uncertainty communication, surveys ongoing research efforts, and includes analyses of biases in verbalized uncertainty.", "result": "It identifies overlooked biases in how uncertainty is verbally communicated between humans and LLMs, arguing for the need to emulate human linguistic practices in uncertainty communication.", "conclusion": "The paper concludes with future research directions focusing on enhancing human-machine communication by deconstructing anthropomimetic uncertainty.", "key_contributions": ["Introduction of the concept of anthropomimetic uncertainty.", "Analysis of biases in verbalized uncertainty communication.", "Recommendations for improving LLMs' uncertainty communication strategies."], "limitations": "", "keywords": ["human-computer interaction", "large language models", "uncertainty communication", "natural language processing", "bias"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.11490", "pdf": "https://arxiv.org/pdf/2507.11490.pdf", "abs": "https://arxiv.org/abs/2507.11490", "title": "Towards Creating Infrastructures for Values and Ethics Work in the Production of Software Technologies", "authors": ["Richmond Y. Wong"], "categories": ["cs.HC"], "comment": "In The sixth decennial Aarhus conference: Computing X Crisis (AAR\n  2025)", "summary": "Recognizing how technical systems can embody social values or cause harms,\nhuman-computer interaction (HCI) research often approaches addressing values\nand ethics in design by creating tools to help tech workers integrate social\nvalues into the design of products. While useful, these approaches usually do\nnot consider the politics embedded in the broader processes, organizations,\nsocial systems, and governance structures that affect the types of actions that\ntech workers can take to address values and ethics. This paper argues that\ncreating infrastructures to support values and ethics work, rather than tools,\nis an approach that takes these broader processes into account and opens them\nup for (re)design. Drawing on prior research conceptualizing infrastructures\nfrom science \\& technology studies and media studies, this paper outlines\nconceptual insights from infrastructures studies that open up new tactics for\nHCI researchers and designers seeking to support values and ethics in design.", "AI": {"tldr": "The paper discusses the need to create infrastructures supporting values and ethics in HCI design rather than just tools, considering the broader political and social contexts.", "motivation": "The paper addresses the limitations of current HCI approaches that primarily focus on tools for integrating social values in design, highlighting the need for a broader approach that includes infrastructures.", "method": "The authors draw on science and technology studies as well as media studies to conceptualize infrastructures and their relevance to HCI values and ethics work.", "result": "The paper outlines new insights and tactics for HCI researchers and designers, advocating for the redesign of systems that incorporate values and ethics in tech development.", "conclusion": "By shifting from tools to infrastructures, HCI can better integrate social values and ethics in the design process, potentially transforming design practices.", "key_contributions": ["Proposes an infrastructure-focused approach to values and ethics in HCI design", "Highlights the political and social contexts influencing design actions", "Offers new tactics for HCI researchers and practitioners"], "limitations": "", "keywords": ["human-computer interaction", "values and ethics", "infrastructures", "design processes", "social systems"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.10596", "pdf": "https://arxiv.org/pdf/2507.10596.pdf", "abs": "https://arxiv.org/abs/2507.10596", "title": "PLEX: Perturbation-free Local Explanations for LLM-Based Text Classification", "authors": ["Yogachandran Rahulamathavan", "Misbah Farooq", "Varuna De Silva"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) excel in text classification, but their\ncomplexity hinders interpretability, making it difficult to understand the\nreasoning behind their predictions. Explainable AI (XAI) methods like LIME and\nSHAP offer local explanations by identifying influential words, but they rely\non computationally expensive perturbations. These methods typically generate\nthousands of perturbed sentences and perform inferences on each, incurring a\nsubstantial computational burden, especially with LLMs. To address this, we\npropose \\underline{P}erturbation-free \\underline{L}ocal \\underline{Ex}planation\n(PLEX), a novel method that leverages the contextual embeddings extracted from\nthe LLM and a ``Siamese network\" style neural network trained to align with\nfeature importance scores. This one-off training eliminates the need for\nsubsequent perturbations, enabling efficient explanations for any new sentence.\nWe demonstrate PLEX's effectiveness on four different classification tasks\n(sentiment, fake news, fake COVID-19 news and depression), showing more than\n92\\% agreement with LIME and SHAP. Our evaluation using a ``stress test\"\nreveals that PLEX accurately identifies influential words, leading to a similar\ndecline in classification accuracy as observed with LIME and SHAP when these\nwords are removed. Notably, in some cases, PLEX demonstrates superior\nperformance in capturing the impact of key features. PLEX dramatically\naccelerates explanation, reducing time and computational overhead by two and\nfour orders of magnitude, respectively. This work offers a promising solution\nfor explainable LLM-based text classification.", "AI": {"tldr": "Introducing PLEX, a novel perturbation-free method for generating local explanations in LLM-based text classification.", "motivation": "Current explainable AI methods for LLMs are computationally expensive and hinder interpretability, making it essential to find efficient approaches for providing explanations.", "method": "PLEX utilizes contextual embeddings from LLMs and a Siamese network to align with feature importance scores, eliminating the need for perturbation-based methods.", "result": "PLEX shows over 92% agreement with traditional XAI methods (LIME, SHAP) while dramatically reducing computational time and overhead.", "conclusion": "PLEX provides a promising, efficient solution for local explanations in LLM text classification, maintaining accuracy while being significantly faster than existing methods.", "key_contributions": ["Introduction of a perturbation-free method for local explanations in LLMs.", "Demonstration of high agreement with existing XAI methods while offering greater efficiency.", "Improvement in capturing the impact of key features in classification tasks."], "limitations": "", "keywords": ["Large Language Models", "Explainable AI", "Local Explanations", "Siamese Network", "Text Classification"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.10580", "pdf": "https://arxiv.org/pdf/2507.10580.pdf", "abs": "https://arxiv.org/abs/2507.10580", "title": "An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation", "authors": ["Vimaleswar A", "Prabhu Nandan Sahu", "Nilesh Kumar Sahu", "Haroon R Lone"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Mental health plays a crucial role in the overall well-being of an\nindividual. In recent years, digital platforms have been increasingly used to\nexpand mental health and emotional support. However, there are persistent\nchallenges related to limited user accessibility, internet connectivity, and\ndata privacy, which highlight the need for an offline, smartphone-based\nsolution. To address these challenges, we propose EmoSApp (Emotional Support\nApp): an entirely offline, smartphone-based conversational app designed for\nmental health and emotional support. The system leverages Large Language Models\n(LLMs), specifically fine-tuned, quantized and deployed using Torchtune and\nExecutorch for resource-constrained devices, allowing all inferences to occur\non the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned\nthe LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of\n14,582 mental-health QA pairs, along with the multi-turn conversational data.\n  Through qualitative human evaluation with the student population, we\ndemonstrate that EmoSApp has the ability to respond coherently, empathetically,\nmaintain interactive dialogue, and provide relevant suggestions to user's\nmental health problems. Additionally, quantitative evaluations on nine standard\ncommonsense and reasoning benchmarks demonstrate the efficacy of our\nfine-tuned, quantized model in low-resource settings. By prioritizing on-device\ndeployment and specialized domain adaptation, EmoSApp serves as a blueprint for\nfuture innovations in portable, secure, and highly tailored AI-driven mental\nhealth solutions.", "AI": {"tldr": "EmoSApp is an offline conversational app for mental health support, utilizing fine-tuned LLMs for on-device inference.", "motivation": "To address the challenges of accessibility, connectivity, and privacy in digital mental health solutions, an offline approach is proposed.", "method": "EmoSApp employs a fine-tuned, quantized version of the LLaMA-3.2-1B-Instruct model, trained on a custom dataset of mental health Q&A pairs, enabling inferences on smartphones.", "result": "User evaluations show EmoSApp's effectiveness in providing empathetic, coherent responses and maintaining engaging dialogues, alongside quantitative benchmarks validating the model's performance.", "conclusion": "EmoSApp exemplifies a practical framework for deploying AI-driven mental health solutions that prioritize user privacy and accessibility.", "key_contributions": ["Introduction of an offline mental health support app", "Utilization of fine-tuned LLMs for conversational AI", "Demonstration of effective user engagement and response quality"], "limitations": "", "keywords": ["mental health", "offline app", "large language models", "conversational AI", "data privacy"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2507.10599", "pdf": "https://arxiv.org/pdf/2507.10599.pdf", "abs": "https://arxiv.org/abs/2507.10599", "title": "Emergence of Hierarchical Emotion Organization in Large Language Models", "authors": ["Bo Zhao", "Maya Okawa", "Eric J. Bigelow", "Rose Yu", "Tomer Ullman", "Ekdeep Singh Lubana", "Hidenori Tanaka"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "As large language models (LLMs) increasingly power conversational agents,\nunderstanding how they model users' emotional states is critical for ethical\ndeployment. Inspired by emotion wheels -- a psychological framework that argues\nemotions organize hierarchically -- we analyze probabilistic dependencies\nbetween emotional states in model outputs. We find that LLMs naturally form\nhierarchical emotion trees that align with human psychological models, and\nlarger models develop more complex hierarchies. We also uncover systematic\nbiases in emotion recognition across socioeconomic personas, with compounding\nmisclassifications for intersectional, underrepresented groups. Human studies\nreveal striking parallels, suggesting that LLMs internalize aspects of social\nperception. Beyond highlighting emergent emotional reasoning in LLMs, our\nresults hint at the potential of using cognitively-grounded theories for\ndeveloping better model evaluations.", "AI": {"tldr": "The paper analyzes how large language models (LLMs) represent users' emotional states, revealing the formation of hierarchical emotion trees and biases in emotion recognition across different socioeconomic groups.", "motivation": "Understanding the emotional state modeling in LLMs for ethical deployment in conversational agents is crucial.", "method": "The authors analyze probabilistic dependencies between emotional states in LLM outputs, inspired by psychological emotion frameworks, and conduct human studies for comparison.", "result": "LLMs exhibit hierarchical emotion trees aligned with human psychological models, with larger models displaying more complex hierarchies and evident biases across socioeconomic personas.", "conclusion": "The findings suggest the potential of utilizing cognitively-grounded theories to improve model evaluations and highlight emergent emotional reasoning in LLMs.", "key_contributions": ["Demonstrated LLMs create hierarchical representations of emotional states similar to human psychology.", "Identified systematic biases in emotion recognition related to socioeconomic factors.", "Proposed the application of cognitive theories for better evaluation of emotional reasoning in LLMs."], "limitations": "The study may be limited by the specific models analyzed and the contexts of human studies conducted.", "keywords": ["large language models", "emotional states", "human-computer interaction", "socioeconomic biases", "cognitive theories"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.10743", "pdf": "https://arxiv.org/pdf/2507.10743.pdf", "abs": "https://arxiv.org/abs/2507.10743", "title": "Language Models for Adult Service Website Text Analysis", "authors": ["Nickolas Freeman", "Thanh Nguyen", "Gregory Bott", "Jason Parton", "Collin Francel"], "categories": ["cs.CL", "cs.LG"], "comment": "32 pages, 12 figures, 1 table", "summary": "Sex trafficking refers to the use of force, fraud, or coercion to compel an\nindividual to perform in commercial sex acts against their will. Adult service\nwebsites (ASWs) have and continue to be linked to sex trafficking, offering a\nplatform for traffickers to advertise their victims. Thus, organizations\ninvolved in the fight against sex trafficking often use ASW data when\nattempting to identify potential sex trafficking victims. A critical challenge\nin transforming ASW data into actionable insight is text analysis. Previous\nresearch using ASW data has shown that ASW ad text is important for linking\nads. However, working with this text is challenging due to its extensive use of\nemojis, poor grammar, and deliberate obfuscation to evade law enforcement\nscrutiny. We conduct a comprehensive study of language modeling approaches for\nthis application area, including simple information retrieval methods,\npre-trained transformers, and custom transformer models. We demonstrate that\ncharacteristics of ASW text data allow efficient custom transformer models to\nbe trained with relatively small GPU resources and used efficiently for\ninference on consumer hardware. Our custom models outperform fine-tuned\nvariants of well-known encoder-only transformer models, including BERT-base,\nRoBERTa, and ModernBERT, on accuracy, recall, F1 score, and ROC AUC. We\ndemonstrate the use of our best-performing custom configuration on three tasks\nrelated to ASW data analysis: (i) decomposing the giant component in a graph\nrepresentation of ASW data, (ii) clustering ASW ad text, and (iii) using the\nlearned token embeddings to understand the use of emojis in the illicit context\nwe study. The models we develop represent a significant advancement in ASW text\nanalysis, which can be leveraged in a variety of downstream applications and\nresearch.", "AI": {"tldr": "The study develops custom transformer models to analyze adult service website data for identifying potential sex trafficking victims, outperforming conventional models in various metrics.", "motivation": "To enhance identification of sex trafficking victims by analyzing text from adult service websites, which poses unique challenges due to its format.", "method": "The research employs language modeling approaches including pre-trained and custom transformer models trained specifically for adult service website ad data.", "result": "Custom models exceeded performance of BERT-base, RoBERTa, and ModernBERT on metrics like accuracy and F1 score, successfully aiding in ASW ad text analysis.", "conclusion": "The developed models represent a significant advancement in understanding ASW data, with potential applicability across various research domains.", "key_contributions": ["Introduction of efficient custom transformer models tailored for ASW text analysis", "Demonstration of superior performance metrics compared to established transformer variants", "Application of models to diverse tasks like clustering and understanding emoji use in ASW data"], "limitations": "", "keywords": ["sex trafficking", "adult service websites", "text analysis", "transformer models", "language modeling"], "importance_score": 6, "read_time_minutes": 32}}
{"id": "2507.10772", "pdf": "https://arxiv.org/pdf/2507.10772.pdf", "abs": "https://arxiv.org/abs/2507.10772", "title": "Applying Text Embedding Models for Efficient Analysis in Labeled Property Graphs", "authors": ["Michal Podstawski"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Labeled property graphs often contain rich textual attributes that can\nenhance analytical tasks when properly leveraged. This work explores the use of\npretrained text embedding models to enable efficient semantic analysis in such\ngraphs. By embedding textual node and edge properties, we support downstream\ntasks including node classification and relation prediction with improved\ncontextual understanding. Our approach integrates language model embeddings\ninto the graph pipeline without altering its structure, demonstrating that\ntextual semantics can significantly enhance the accuracy and interpretability\nof property graph analysis.", "AI": {"tldr": "This paper explores integrating pretrained text embedding models into labeled property graphs to enhance semantic analysis and improve accuracy in tasks like node classification and relation prediction.", "motivation": "To leverage rich textual attributes in property graphs for better analytical results.", "method": "Embedding textual node and edge properties using pretrained text embedding models within the graph pipeline without structural modifications.", "result": "Demonstrated improved contextual understanding and enhanced accuracy and interpretability in property graph analysis tasks.", "conclusion": "Textual semantics significantly boost the performance of property graph analytical tasks.", "key_contributions": ["Integration of language model embeddings into graph analysis", "Enhanced accuracy in node classification and relation prediction", "Improved interpretability of property graphs"], "limitations": "", "keywords": ["property graphs", "text embedding", "semantic analysis", "node classification", "relation prediction"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.10787", "pdf": "https://arxiv.org/pdf/2507.10787.pdf", "abs": "https://arxiv.org/abs/2507.10787", "title": "Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers", "authors": ["Yilun Zhao", "Chengye Wang", "Chuhan Li", "Arman Cohan"], "categories": ["cs.CL", "cs.CV"], "comment": "ACL 2025 Findings", "summary": "This paper introduces MISS-QA, the first benchmark specifically designed to\nevaluate the ability of models to interpret schematic diagrams within\nscientific literature. MISS-QA comprises 1,500 expert-annotated examples over\n465 scientific papers. In this benchmark, models are tasked with interpreting\nschematic diagrams that illustrate research overviews and answering\ncorresponding information-seeking questions based on the broader context of the\npaper. We assess the performance of 18 frontier multimodal foundation models,\nincluding o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant\nperformance gap between these models and human experts on MISS-QA. Our analysis\nof model performance on unanswerable questions and our detailed error analysis\nfurther highlight the strengths and limitations of current models, offering key\ninsights to enhance models in comprehending multimodal scientific literature.", "AI": {"tldr": "This paper presents MISS-QA, a benchmark to assess models' ability to interpret schematic diagrams in scientific literature, revealing significant performance gaps compared to human experts.", "motivation": "To evaluate how well models can interpret schematic diagrams in scientific literature, which is a crucial skill for understanding research content.", "method": "The benchmark consists of 1,500 expert-annotated examples across 465 papers. The performance of 18 multimodal foundation models was assessed through their ability to answer questions related to schematic diagrams.", "result": "The study found a significant performance gap between multimodal models and human experts, especially in handling unanswerable questions, indicating models' current limitations in comprehension.", "conclusion": "Insights from the performance analysis can guide improvements in multimodal models to better understand scientific multimodal literature.", "key_contributions": ["Introduction of the MISS-QA benchmark for model evaluation", "Performance comparison of 18 multimodal models against human experts", "Analysis of model limitations and error patterns in interpreting diagrams"], "limitations": "The models show a performance gap compared to humans, particularly with unanswerable questions, highlighting areas needing improvement.", "keywords": ["MISS-QA", "benchmark", "multimodal models", "scientific literature", "diagram interpretation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.10810", "pdf": "https://arxiv.org/pdf/2507.10810.pdf", "abs": "https://arxiv.org/abs/2507.10810", "title": "Testing Hypotheses from the Social Approval Theory of Online Hate: An Analysis of 110 Million Posts from Parler", "authors": ["David M. Markowitz", "Samuel Hardman Taylor"], "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "In this paper, we explored how online hate is motivated by receiving social\napproval from others. We specifically examined two central tenets of Walther's\n(2024) social approval theory of online hate: (H1a) more signals of social\napproval on hate messages predicts more subsequent hate messages, and (H1b) as\nsocial approval increases, hate speech messages become more extreme. Using over\n110 million posts from Parler (2018-2021), we observed that the number of\nupvotes a person received on a hate speech post was unassociated with the\namount of hate speech in their next post and posts during the next week, month,\nthree months, and six months. Between-person effects revealed an average\nnegative relationship between social approval and hate speech production at the\npost level, but this relationship was mixed at other time intervals. Social\napproval reinforcement mechanisms of online hate may operate differently on\nniche social media platforms.", "AI": {"tldr": "The paper investigates the relationship between social approval and online hate speech, finding mixed results counter to existing theories.", "motivation": "To explore the motivations behind online hate speech, particularly the role of social approval in encouraging or mitigating such behavior.", "method": "Analysis of over 110 million posts from the social media platform Parler, focusing on the correlation between social approval (upvotes) and subsequent hate speech production.", "result": "An average negative relationship was observed between social approval and hate speech production at the post level, with mixed results at longer time intervals.", "conclusion": "Social approval reinforcement mechanisms for online hate may vary significantly across different social media platforms.", "key_contributions": ["Investigated social approval theory in the context of online hate.", "Utilized a large dataset from Parler to analyze hate speech behavior.", "Demonstrated that social approval may not consistently lead to increased hate speech."], "limitations": "Findings may not generalize beyond the specific niche of Parler or other similar platforms.", "keywords": ["online hate", "social approval", "hate speech", "Parler", "social media"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2507.10852", "pdf": "https://arxiv.org/pdf/2507.10852.pdf", "abs": "https://arxiv.org/abs/2507.10852", "title": "LLMs on Trial: Evaluating Judicial Fairness for Large Language Models", "authors": ["Yiran Hu", "Zongyue Xue", "Haitao Li", "Siyuan Zheng", "Qingjing Chen", "Shaochun Wang", "Xihan Zhang", "Ning Zheng", "Yun Liu", "Qingyao Ai", "Yiqun Liu", "Charles L. A. Clarke", "Weixing Shen"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used in high-stakes fields\nwhere their decisions impact rights and equity. However, LLMs' judicial\nfairness and implications for social justice remain underexplored. When LLMs\nact as judges, the ability to fairly resolve judicial issues is a prerequisite\nto ensure their trustworthiness. Based on theories of judicial fairness, we\nconstruct a comprehensive framework to measure LLM fairness, leading to a\nselection of 65 labels and 161 corresponding values. Applying this framework to\nthe judicial system, we compile an extensive dataset, JudiFair, comprising\n177,100 unique case facts. To achieve robust statistical inference, we develop\nthree evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and\nintroduce a method to assess the overall fairness of multiple LLMs across\nvarious labels. Through experiments with 16 LLMs, we uncover pervasive\ninconsistency, bias, and imbalanced inaccuracy across models, underscoring\nsevere LLM judicial unfairness. Particularly, LLMs display notably more\npronounced biases on demographic labels, with slightly less bias on substance\nlabels compared to procedure ones. Interestingly, increased inconsistency\ncorrelates with reduced biases, but more accurate predictions exacerbate\nbiases. While we find that adjusting the temperature parameter can influence\nLLM fairness, model size, release date, and country of origin do not exhibit\nsignificant effects on judicial fairness. Accordingly, we introduce a publicly\navailable toolkit containing all datasets and code, designed to support future\nresearch in evaluating and improving LLM fairness.", "AI": {"tldr": "This paper explores the fairness of Large Language Models (LLMs) in judicial contexts, introducing a framework for measurement and a comprehensive dataset, along with evaluation metrics that reveal significant biases and inconsistencies across LLMs.", "motivation": "The increasing use of LLMs in high-stakes domains necessitates an assessment of their fairness, particularly in judicial applications where their decisions affect rights and equity.", "method": "A framework for measuring LLM fairness is developed, comprising 65 labels and 161 values. An extensive dataset, JudiFair, with 177,100 unique case facts is compiled. Three evaluation metrics are introduced to measure inconsistency, bias, and imbalanced inaccuracy across 16 LLMs.", "result": "The study reveals significant judicial unfairness in LLMs, with pronounced biases on demographic labels and a complex relationship between consistency, accuracy, and bias levels across different models.", "conclusion": "Adjusting model parameters can influence fairness, while other factors like model size and origin showed no significant effect. A toolkit with datasets and code is released for further research.", "key_contributions": ["Development of a comprehensive fairness measurement framework for LLMs", "Creation of the JudiFair dataset", "Introduction of new evaluation metrics for assessing LLM fairness"], "limitations": "", "keywords": ["Large Language Models", "judicial fairness", "social justice", "bias", "dataset"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.10918", "pdf": "https://arxiv.org/pdf/2507.10918.pdf", "abs": "https://arxiv.org/abs/2507.10918", "title": "How Stylistic Similarity Shapes Preferences in Dialogue Dataset with User and Third Party Evaluations", "authors": ["Ikumi Numaya", "Shoji Moriya", "Shiki Sato", "Reina Akama", "Jun Suzuki"], "categories": ["cs.CL"], "comment": "Accepted to SIGDIAL 2025 (long)", "summary": "Recent advancements in dialogue generation have broadened the scope of\nhuman-bot interactions, enabling not only contextually appropriate responses\nbut also the analysis of human affect and sensitivity. While prior work has\nsuggested that stylistic similarity between user and system may enhance user\nimpressions, the distinction between subjective and objective similarity is\noften overlooked. To investigate this issue, we introduce a novel dataset that\nincludes users' preferences, subjective stylistic similarity based on users'\nown perceptions, and objective stylistic similarity annotated by third party\nevaluators in open-domain dialogue settings. Analysis using the constructed\ndataset reveals a strong positive correlation between subjective stylistic\nsimilarity and user preference. Furthermore, our analysis suggests an important\nfinding: users' subjective stylistic similarity differs from third party\nobjective similarity. This underscores the importance of distinguishing between\nsubjective and objective evaluations and understanding the distinct aspects\neach captures when analyzing the relationship between stylistic similarity and\nuser preferences. The dataset presented in this paper is available online.", "AI": {"tldr": "This paper introduces a novel dataset to analyze stylistic similarity in human-bot interactions, revealing important distinctions between user perceptions and third-party evaluations of stylistic similarity.", "motivation": "To examine the differences between subjective and objective stylistic similarity in user-bot interactions and its effect on user preferences.", "method": "A novel dataset was created that includes users' preferences, subjective stylistic similarity based on users' perceptions, and objective stylistic similarity annotated by third-party evaluators, followed by analysis of these components.", "result": "The analysis shows a strong positive correlation between subjective stylistic similarity and user preference, highlighting that users' perceptions of stylistic similarity differ from objective evaluations.", "conclusion": "Recognizing the differences between subjective and objective stylistic similarity is crucial for understanding user preferences in human-bot interactions.", "key_contributions": ["Introduction of a novel dataset for analyzing stylistic similarity in dialogue generation.", "Establishment of the relationship between subjective stylistic similarity and user preference.", "Highlighting the distinction between subjective and objective evaluations in user experience."], "limitations": "The study's analysis is limited to open-domain dialogue settings and may not generalize to other contexts.", "keywords": ["dialogue generation", "stylistic similarity", "user preferences", "human-bot interaction", "natural language processing"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2507.11330", "pdf": "https://arxiv.org/pdf/2507.11330.pdf", "abs": "https://arxiv.org/abs/2507.11330", "title": "Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge", "authors": ["Wenqing Wu", "Chengzhi Zhang", "Yi Zhao"], "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.HC"], "comment": "Journal of the Association for Information Science and Technology,\n  2025", "summary": "Novelty is a crucial criterion in the peer review process for evaluating\nacademic papers. Traditionally, it's judged by experts or measure by unique\nreference combinations. Both methods have limitations: experts have limited\nknowledge, and the effectiveness of the combination method is uncertain.\nMoreover, it's unclear if unique citations truly measure novelty. The large\nlanguage model (LLM) possesses a wealth of knowledge, while human experts\npossess judgment abilities that the LLM does not possess. Therefore, our\nresearch integrates the knowledge and abilities of LLM and human experts to\naddress the limitations of novelty assessment. The most common novelty in\nacademic papers is the introduction of new methods. In this paper, we propose\nleveraging human knowledge and LLM to assist pretrained language models (PLMs,\ne.g. BERT etc.) in predicting the method novelty of papers. Specifically, we\nextract sentences related to the novelty of the academic paper from peer review\nreports and use LLM to summarize the methodology section of the academic paper,\nwhich are then used to fine-tune PLMs. In addition, we have designed a\ntext-guided fusion module with novel Sparse-Attention to better integrate human\nand LLM knowledge. We compared the method we proposed with a large number of\nbaselines. Extensive experiments demonstrate that our method achieves superior\nperformance.", "AI": {"tldr": "This paper proposes a novel method for assessing the novelty of academic papers by integrating human expertise with large language models (LLMs), specifically focusing on method novelty assessment.", "motivation": "To address the limitations of traditional novelty evaluation in academic peer review, which relies on expert judgment and citation analysis, both of which have significant shortcomings.", "method": "The proposed method involves extracting sentences related to novelty from peer review reports and utilizing an LLM to summarize the methodology section of academic papers, which are then used to fine-tune pretrained language models (PLMs). A Sparse-Attention text-guided fusion module is designed to better integrate human and LLM knowledge.", "result": "The proposed approach exhibits superior performance over numerous baseline methods in predicting the method novelty of academic papers.", "conclusion": "Integrating LLMs with human expert knowledge can effectively enhance the assessment of novelty in academic research, providing a more reliable evaluation framework.", "key_contributions": ["Integration of LLMs and human knowledge for novelty assessment in academic papers", "Development of a Sparse-Attention text-guided fusion module", "Demonstration of superior performance compared to baseline methods"], "limitations": "", "keywords": ["novelty assessment", "large language models", "human-computer interaction", "method prediction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.10920", "pdf": "https://arxiv.org/pdf/2507.10920.pdf", "abs": "https://arxiv.org/abs/2507.10920", "title": "HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented Pre-Training", "authors": ["Seungho Choi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) often show poor performance in low-resource\nlanguages like Korean, partly due to unique linguistic challenges such as\nhomophonous Sino-Korean words that are indistinguishable in Hangul script. To\naddress this semantic ambiguity, we propose HanjaBridge, a novel\nmeaning-injection technique integrated into a continual pre-training (CPT)\nframework. Instead of deterministically mapping a word to a single Hanja\n(Chinese character), HanjaBridge presents the model with all possible Hanja\ncandidates for a given homograph, encouraging the model to learn contextual\ndisambiguation. This process is paired with token-level knowledge distillation\nto prevent catastrophic forgetting. Experimental results show that HanjaBridge\nsignificantly improves Korean language understanding, achieving a 21\\% relative\nimprovement on the KoBALT benchmark. Notably, by reinforcing semantic alignment\nbetween Korean and Chinese through shared Hanja, we observe a strong positive\ncross-lingual transfer. Furthermore, these gains persist even when Hanja\naugmentation is omitted at inference time, ensuring practical efficiency with\nno additional run-time cost.", "AI": {"tldr": "HanjaBridge is a novel technique that enhances language understanding in Korean by integrating all possible Hanja characters for homographs into a continual pre-training framework.", "motivation": "To improve the performance of large language models on low-resource languages like Korean, addressing the unique challenges presented by homophonous Sino-Korean words.", "method": "HanjaBridge provides all possible Hanja candidates for homographs, encouraging better contextual disambiguation, along with token-level knowledge distillation to prevent forgetting.", "result": "HanjaBridge leads to a 21% relative improvement in Korean language understanding on the KoBALT benchmark and enables strong cross-lingual transfer between Korean and Chinese.", "conclusion": "The improvements persist even without Hanja augmentation during inference, showing efficiency without extra runtime costs.", "key_contributions": ["Introduction of HanjaBridge for semantic ambiguity resolution in Korean", "Integration of continual pre-training with meaning injection", "Demonstrated significant cross-lingual transfer benefits"], "limitations": "", "keywords": ["Large Language Models", "HanjaBridge", "Korean Language Processing", "Cross-lingual Transfer", "Continual Pre-training"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.10957", "pdf": "https://arxiv.org/pdf/2507.10957.pdf", "abs": "https://arxiv.org/abs/2507.10957", "title": "Modeling Understanding of Story-Based Analogies Using Large Language Models", "authors": ["Kalit Inani", "Keshav Kabra", "Vijay Marupudi", "Sashank Varma"], "categories": ["cs.CL", "cs.AI"], "comment": "To appear at CogSci 2025", "summary": "Recent advancements in Large Language Models (LLMs) have brought them closer\nto matching human cognition across a variety of tasks. How well do these models\nalign with human performance in detecting and mapping analogies? Prior research\nhas shown that LLMs can extract similarities from analogy problems but lack\nrobust human-like reasoning. Building on Webb, Holyoak, and Lu (2023), the\ncurrent study focused on a story-based analogical mapping task and conducted a\nfine-grained evaluation of LLM reasoning abilities compared to human\nperformance. First, it explored the semantic representation of analogies in\nLLMs, using sentence embeddings to assess whether they capture the similarity\nbetween the source and target texts of an analogy, and the dissimilarity\nbetween the source and distractor texts. Second, it investigated the\neffectiveness of explicitly prompting LLMs to explain analogies. Throughout, we\nexamine whether LLMs exhibit similar performance profiles to those observed in\nhumans by evaluating their reasoning at the level of individual analogies, and\nnot just at the level of overall accuracy (as prior studies have done). Our\nexperiments include evaluating the impact of model size (8B vs. 70B parameters)\nand performance variation across state-of-the-art model architectures such as\nGPT-4 and LLaMA3. This work advances our understanding of the analogical\nreasoning abilities of LLMs and their potential as models of human reasoning.", "AI": {"tldr": "This paper evaluates the reasoning abilities of Large Language Models (LLMs) in analogical mapping tasks, comparing their performance to human reasoning and examining the impact of model size and architecture.", "motivation": "The study aims to determine how well LLMs align with human cognition in detecting and mapping analogies, addressing gaps in prior research regarding LLM reasoning capabilities.", "method": "The research involves a fine-grained evaluation of LLMs' analogical reasoning through semantic representation assessments using sentence embeddings, and by explicitly prompting LLMs to explain analogies, while analyzing the impact of model size and architecture.", "result": "The experiments reveal insights into the semantic representation of analogies in LLMs and indicate performance profiles that may or may not align with human reasoning, particularly when evaluating individual analogies instead of just overall accuracy.", "conclusion": "The findings of this research enhance the understanding of LLMs' analogical reasoning abilities and their potential as models for human reasoning, emphasizing the necessity for more nuanced evaluation techniques in this field.", "key_contributions": ["Fine-grained evaluation of LLM reasoning in analogical tasks.", "Investigation of sentence embeddings for semantic representation of analogies.", "Analysis of model size and architecture effects on reasoning performance."], "limitations": "", "keywords": ["Large Language Models", "Analogy Detection", "Human Cognition", "Reasoning Evaluation", "Machine Learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.10958", "pdf": "https://arxiv.org/pdf/2507.10958.pdf", "abs": "https://arxiv.org/abs/2507.10958", "title": "DS@GT at eRisk 2025: From prompts to predictions, benchmarking early depression detection with conversational agent based assessments and temporal attention models", "authors": ["Anthony Miyaguchi", "David Guecha", "Yuwen Chiu", "Sidharth Gaur"], "categories": ["cs.CL"], "comment": null, "summary": "This Working Note summarizes the participation of the DS@GT team in two eRisk\n2025 challenges. For the Pilot Task on conversational depression detection with\nlarge language-models (LLMs), we adopted a prompt-engineering strategy in which\ndiverse LLMs conducted BDI-II-based assessments and produced structured JSON\noutputs. Because ground-truth labels were unavailable, we evaluated cross-model\nagreement and internal consistency. Our prompt design methodology aligned model\noutputs with BDI-II criteria and enabled the analysis of conversational cues\nthat influenced the prediction of symptoms. Our best submission, second on the\nofficial leaderboard, achieved DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27.", "AI": {"tldr": "The DS@GT team participated in eRisk 2025 challenges focusing on conversational depression detection using LLMs, achieving notable leaderboard results with a prompt-engineering strategy.", "motivation": "To improve conversational depression detection through the use of LLMs in the absence of ground-truth labels.", "method": "Employing a prompt-engineering strategy across various LLMs for BDI-II-based evaluations and analyzing outputs structured in JSON format.", "result": "Achieved second place on the leaderboard with DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27 through evaluation of model agreement and internal consistency.", "conclusion": "The prompt design methodology was effective in aligning outputs with BDI-II criteria and provided insights into conversational cues influencing symptom prediction.", "key_contributions": ["Novel prompt-engineering strategy for LLMs in mental health assessment", "Evaluation of cross-model agreement and internal consistency", "Structured JSON outputs for systematic analysis of conversational cues"], "limitations": "", "keywords": ["conversational depression detection", "large language models", "prompt engineering", "BDI-II assessment", "mental health"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.10972", "pdf": "https://arxiv.org/pdf/2507.10972.pdf", "abs": "https://arxiv.org/abs/2507.10972", "title": "Teach Me Sign: Stepwise Prompting LLM for Sign Language Production", "authors": ["Zhaoyi An", "Rei Kawakami"], "categories": ["cs.CL", "cs.CV", "cs.MM"], "comment": "Accepted by IEEE ICIP 2025", "summary": "Large language models, with their strong reasoning ability and rich\nknowledge, have brought revolution to many tasks of AI, but their impact on\nsign language generation remains limited due to its complexity and unique\nrules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign\nlanguage as another natural language. By fine-tuning an LLM, we enable it to\nlearn the correspondence between text and sign language, and facilitate\ngeneration. Considering the differences between sign and spoken language, we\nemploy a stepwise prompting strategy to extract the inherent sign language\nknowledge within the LLM, thereby supporting the learning and generation\nprocess. Experimental results on How2Sign and Phoenix14T datasets demonstrate\nthat our approach effectively leverages both the sign language knowledge and\nreasoning capabilities of LLM to align the different distribution and\ngrammatical rules between sign and spoken language.", "AI": {"tldr": "This paper presents TEAch Me Sign (TEAM-Sign), a method utilizing large language models to enhance sign language generation by treating it like a natural language, exploiting LLMs' reasoning and knowledge capabilities to improve the mapping of text to sign language.", "motivation": "Sign language generation has seen limited advancements using large language models (LLMs) due to unique complexities involved. This research aims to bridge that gap by effectively leveraging LLMs for better sign language communication.", "method": "The proposed method, TEAM-Sign, fine-tunes a large language model to learn the correspondence between text and sign language, using a stepwise prompting strategy to align inherent sign language knowledge with the model's reasoning abilities.", "result": "Experimental results show that TEAM-Sign significantly improves the generation of sign language from text, effectively addressing the grammatical discrepancies and distribution differences between sign and spoken languages, evidenced by performance on the How2Sign and Phoenix14T datasets.", "conclusion": "The study concludes that fine-tuning LLMs with a structured approach can yield effective sign language generation tools, thereby enhancing communication accessibility.", "key_contributions": ["Proposed TEAM-Sign to facilitate sign language generation using LLMs.", "Introduced a stepwise prompting strategy tailored for sign language.", "Demonstrated improved performance in sign language generation on benchmark datasets."], "limitations": "", "keywords": ["sign language generation", "large language models", "machine learning", "natural language processing", "HCI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.10996", "pdf": "https://arxiv.org/pdf/2507.10996.pdf", "abs": "https://arxiv.org/abs/2507.10996", "title": "Mario at EXIST 2025: A Simple Gateway to Effective Multilingual Sexism Detection", "authors": ["Lin Tian", "Johanne R. Trippas", "Marian-Andrei Rizoiu"], "categories": ["cs.CL"], "comment": "12 pages, 5 tables, CLEF 2025", "summary": "This paper presents our approach to EXIST 2025 Task 1, addressing text-based\nsexism detection in English and Spanish tweets through hierarchical Low-Rank\nAdaptation (LoRA) of Llama 3.1 8B. Our method introduces conditional adapter\nrouting that explicitly models label dependencies across three hierarchically\nstructured subtasks: binary sexism identification, source intention detection,\nand multilabel sexism categorization. Unlike conventional LoRA applications\nthat target only attention layers, we apply adaptation to all linear\ntransformations, enhancing the model's capacity to capture task-specific\npatterns. In contrast to complex data processing and ensemble approaches, we\nshow that straightforward parameter-efficient fine-tuning achieves strong\nperformance. We train separate LoRA adapters (rank=16, QLoRA 4-bit) for each\nsubtask using unified multilingual training that leverages Llama 3.1's native\nbilingual capabilities. The method requires minimal preprocessing and uses\nstandard supervised learning. Our multilingual training strategy eliminates the\nneed for separate language-specific models, achieving 1.7-2.4\\% F1 improvements\nthrough cross-lingual transfer. With only 1.67\\% trainable parameters compared\nto full fine-tuning, our approach reduces training time by 75\\% and model\nstorage by 98\\%, while achieving competitive performance across all subtasks\n(ICM-Hard: 0.6774 for binary classification, 0.4991 for intention detection,\n0.6519 for multilabel categorization).", "AI": {"tldr": "This paper proposes a novel approach to detect text-based sexism in English and Spanish tweets using a parameter-efficient method based on hierarchical Low-Rank Adaptation of Llama 3.1, achieving significant performance improvements with minimal resources.", "motivation": "To address the growing need for effective sexism detection in social media, especially across multiple languages, utilizing advanced machine learning techniques.", "method": "The approach utilizes hierarchical Low-Rank Adaptation (LoRA) on the Llama 3.1 model, implementing conditional adapter routing for three subtasks: binary sexism identification, source intention detection, and multilabel categorization, with minimal data preprocessing.", "result": "The proposed method demonstrates significant F1 score improvements through cross-lingual transfer with only 1.67% of trainable parameters, reducing training time by 75% and model storage by 98%.", "conclusion": "The findings indicate that a straightforward parameter-efficient fine-tuning approach can effectively enhance sexism detection performance, proving to be a viable alternative to more complex methods.", "key_contributions": ["Introduction of conditional adapter routing for hierarchical subtasks", "Significant reductions in training time and model storage with competitive performance", "Achievement of cross-lingual transfer improvements without the need for separate models"], "limitations": "", "keywords": ["sexism detection", "Low-Rank Adaptation", "multilingual training", "Llama 3.1", "social media"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.11004", "pdf": "https://arxiv.org/pdf/2507.11004.pdf", "abs": "https://arxiv.org/abs/2507.11004", "title": "Team HUMANE at AVeriTeC 2025: HerO 2 for Efficient Fact Verification", "authors": ["Yejun Yoon", "Jaeyoon Jung", "Seunghyun Yoon", "Kunwoo Park"], "categories": ["cs.CL"], "comment": "ACL 2025 Workshop (FEVER)", "summary": "This paper presents HerO 2, Team HUMANE's system for the AVeriTeC shared task\nat the FEVER-25 workshop. HerO 2 is an enhanced version of HerO, the\nbest-performing open-source model from the previous year's challenge. It\nimproves evidence quality through document summarization and answer\nreformulation, optimizes veracity prediction via post-training quantization\nunder computational constraints, and enhances overall system performance by\nintegrating updated language model (LM) backbones. HerO 2 ranked second on the\nleaderboard while achieving the shortest runtime among the top three systems,\ndemonstrating both high efficiency and strong potential for real-world fact\nverification. The code is available at https://github.com/ssu-humane/HerO2.", "AI": {"tldr": "HerO 2 is an enhanced model for fact verification that improves evidence quality and system performance with optimized veracity prediction and updated language models.", "motivation": "To enhance the performance and efficiency of fact verification systems.", "method": "The model incorporates document summarization, answer reformulation, and post-training quantization while using updated language model backbones.", "result": "HerO 2 ranked second on the FEVER-25 leaderboard and had the shortest runtime among the top systems.", "conclusion": "HerO 2 shows high efficiency and strong potential for real-world applications in fact verification.", "key_contributions": ["Improved evidence quality through document summarization and answer reformulation.", "Optimized veracity prediction via post-training quantization.", "Integration of updated language model backbones for better performance."], "limitations": "", "keywords": ["fact verification", "language models", "evidence quality"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.04025", "pdf": "https://arxiv.org/pdf/2410.04025.pdf", "abs": "https://arxiv.org/abs/2410.04025", "title": "IdeaSynth: Iterative Research Idea Development Through Evolving and Composing Idea Facets with Literature-Grounded Feedback", "authors": ["Kevin Pu", "K. J. Kevin Feng", "Tovi Grossman", "Tom Hope", "Bhavana Dalvi Mishra", "Matt Latzke", "Jonathan Bragg", "Joseph Chee Chang", "Pao Siangliulue"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Research ideation involves broad exploring and deep refining ideas. Both\nrequire deep engagement with literature. Existing tools focus primarily on idea\nbroad generation, yet offer little support for iterative specification,\nrefinement, and evaluation needed to further develop initial ideas. To bridge\nthis gap, we introduce IdeaSynth, a research idea development system that uses\nLLMs to provide literature-grounded feedback for articulating research\nproblems, solutions, evaluations, and contributions. IdeaSynth represents these\nidea facets as nodes on a canvas, and allow researchers to iteratively refine\nthem by creating and exploring variations and composing them. Our lab study\n(N=20) showed that participants, while using IdeaSynth, explored more\nalternative ideas and expanded initial ideas with more details compared to a\nstrong LLM-based baseline. Our deployment study (N=7) demonstrated that\nparticipants effectively used IdeaSynth for real-world research projects at\nvarious ideation stages from developing initial ideas to revising framings of\nmature manuscripts, highlighting the possibilities to adopt IdeaSynth in\nresearcher's workflows.", "AI": {"tldr": "IdeaSynth is a system using LLMs to assist in the iterative refinement of research ideas, enhancing idea generation and specification.", "motivation": "There is a lack of tools that support the iterative specification, refinement, and evaluation needed for developing initial research ideas, as current tools focus mainly on broad idea generation.", "method": "IdeaSynth uses large language models (LLMs) to provide feedback and facilitate exploration of research ideas, representing various facets of ideas as nodes on a canvas for iterative refinement.", "result": "Participants using IdeaSynth explored more alternative ideas and added more details to initial concepts compared to a baseline; it was effectively used for real-world research projects across different ideation stages.", "conclusion": "IdeaSynth shows potential to be incorporated into researchers' workflows for ideation and refinement of research projects.", "key_contributions": ["Introduction of IdeaSynth for literature-grounded feedback in research ideation.", "Utilization of LLMs for real-time assistance in idea refinement.", "Demonstrated effectiveness in enhancing research idea exploration and development."], "limitations": "", "keywords": ["Human-Computer Interaction", "Machine Learning", "Research Ideation", "LLM", "Idea Development"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.11049", "pdf": "https://arxiv.org/pdf/2507.11049.pdf", "abs": "https://arxiv.org/abs/2507.11049", "title": "Journalism-Guided Agentic In-Context Learning for News Stance Detection", "authors": ["Dahyun Lee", "Jonghyeon Choi", "Jiyoung Han", "Kunwoo Park"], "categories": ["cs.CL"], "comment": "Preprint. 24 pages", "summary": "As online news consumption grows, personalized recommendation systems have\nbecome integral to digital journalism. However, these systems risk reinforcing\nfilter bubbles and political polarization by failing to incorporate diverse\nperspectives. Stance detection -- identifying a text's position on a target --\ncan help mitigate this by enabling viewpoint-aware recommendations and\ndata-driven analyses of media bias. Yet, existing stance detection research\nremains largely limited to short texts and high-resource languages. To address\nthese gaps, we introduce \\textsc{K-News-Stance}, the first Korean dataset for\narticle-level stance detection, comprising 2,000 news articles with\narticle-level and 19,650 segment-level stance annotations across 47 societal\nissues. We also propose \\textsc{JoA-ICL}, a \\textbf{Jo}urnalism-guided\n\\textbf{A}gentic \\textbf{I}n-\\textbf{C}ontext \\textbf{L}earning framework that\nemploys a language model agent to predict the stances of key structural\nsegments (e.g., leads, quotes), which are then aggregated to infer the overall\narticle stance. Experiments show that \\textsc{JoA-ICL} outperforms existing\nstance detection methods, highlighting the benefits of segment-level agency in\ncapturing the overall position of long-form news articles. Two case studies\nfurther demonstrate its broader utility in promoting viewpoint diversity in\nnews recommendations and uncovering patterns of media bias.", "AI": {"tldr": "This paper introduces K-News-Stance, a dataset for article-level stance detection in Korean, and proposes JoA-ICL, a framework that improves stance detection in long-form news articles by predicting and aggregating segment-level stances.", "motivation": "To address the limitations of existing stance detection methods which are primarily focused on short texts and high-resource languages, especially in the context of personalized news recommendations.", "method": "K-News-Stance dataset with 2,000 articles and 19,650 segment-level annotations; JoA-ICL framework utilizing a language model agent for segment-level stance prediction and aggregation.", "result": "JoA-ICL outperforms existing stance detection methods, demonstrating effectiveness in capturing the stance of long-form articles.", "conclusion": "The study highlights the importance of segment-level agency in stance detection to promote diversity in news recommendations and analyze media bias.", "key_contributions": ["Introduction of K-News-Stance, the first Korean dataset for article-level stance detection.", "Development of JoA-ICL, a new framework for stance prediction that improves upon existing methods.", "Demonstration of the utility of stance detection in promoting viewpoint diversity and revealing media bias."], "limitations": "Limited to the Korean language and may not generalize to other languages or cultures.", "keywords": ["stance detection", "news recommendation", "media bias", "Korean dataset", "ML frameworks"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2502.18658", "pdf": "https://arxiv.org/pdf/2502.18658.pdf", "abs": "https://arxiv.org/abs/2502.18658", "title": "Assistance or Disruption? Exploring and Evaluating the Design and Trade-offs of Proactive AI Programming Support", "authors": ["Kevin Pu", "Daniel Lazaro", "Ian Arawjo", "Haijun Xia", "Ziang Xiao", "Tovi Grossman", "Yan Chen"], "categories": ["cs.HC", "cs.AI", "cs.SE"], "comment": null, "summary": "AI programming tools enable powerful code generation, and recent prototypes\nattempt to reduce user effort with proactive AI agents, but their impact on\nprogramming workflows remains unexplored. We introduce and evaluate\nCodellaborator, a design probe LLM agent that initiates programming assistance\nbased on editor activities and task context. We explored three interface\nvariants to assess trade-offs between increasingly salient AI support:\nprompt-only, proactive agent, and proactive agent with presence and context\n(Codellaborator). In a within-subject study (N=18), we find that proactive\nagents increase efficiency compared to prompt-only paradigm, but also incur\nworkflow disruptions. However, presence indicators and interaction context\nsupport alleviated disruptions and improved users' awareness of AI processes.\nWe underscore trade-offs of Codellaborator on user control, ownership, and code\nunderstanding, emphasizing the need to adapt proactivity to programming\nprocesses. Our research contributes to the design exploration and evaluation of\nproactive AI systems, presenting design implications on AI-integrated\nprogramming workflow.", "AI": {"tldr": "This paper evaluates Codellaborator, a proactive AI programming assistant that enhances coding efficiency while raising concerns about workflow disruptions.", "motivation": "Investigate the impact of proactive AI agents on programming workflows, particularly focusing on user-effort reduction and design implications in AI-assisted coding.", "method": "Conducted a within-subject study with 18 participants to compare three interface variants: prompt-only, proactive agent, and proactive agent with presence and context.", "result": "Proactive agents improved coding efficiency over a prompt-only approach but also introduced workflow disruptions; however, appropriate presence indicators mitigated these disruptions and enhanced awareness.", "conclusion": "The study emphasizes the importance of adapting AI proactivity to users' programming processes, highlighting trade-offs in user control and code understanding.", "key_contributions": ["Introduction of Codellaborator as a design probe for LLM agents in coding assistance", "Empirical findings on the trade-offs between efficiency and workflow disruption", "Design implications for integrating AI into programming workflows"], "limitations": "Study limited to 18 participants; findings may not generalize across larger, diverse coding populations.", "keywords": ["AI programming tools", "proactive agent", "workflow disruption", "programming assistance", "human-computer interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.11052", "pdf": "https://arxiv.org/pdf/2507.11052.pdf", "abs": "https://arxiv.org/abs/2507.11052", "title": "LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk Prediction: A Clinical NLP", "authors": ["Haowei Yang", "Ziyu Shen", "Junli Shao", "Luyao Men", "Xinyue Han", "Jing Dong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Timely identification and accurate risk stratification of cardiovascular\ndisease (CVD) remain essential for reducing global mortality. While existing\nprediction models primarily leverage structured data, unstructured clinical\nnotes contain valuable early indicators. This study introduces a novel\nLLM-augmented clinical NLP pipeline that employs domain-adapted large language\nmodels for symptom extraction, contextual reasoning, and correlation from\nfree-text reports. Our approach integrates cardiovascular-specific fine-tuning,\nprompt-based inference, and entity-aware reasoning. Evaluations on MIMIC-III\nand CARDIO-NLP datasets demonstrate improved performance in precision, recall,\nF1-score, and AUROC, with high clinical relevance (kappa = 0.82) assessed by\ncardiologists. Challenges such as contextual hallucination, which occurs when\nplausible information contracts with provided source, and temporal ambiguity,\nwhich is related with models struggling with chronological ordering of events\nare addressed using prompt engineering and hybrid rule-based verification. This\nwork underscores the potential of LLMs in clinical decision support systems\n(CDSS), advancing early warning systems and enhancing the translation of\npatient narratives into actionable risk assessments.", "AI": {"tldr": "This study presents an LLM-augmented NLP pipeline for improving cardiovascular disease risk stratification using unstructured clinical notes.", "motivation": "To enhance the timely identification and risk stratification of cardiovascular disease (CVD) using valuable data from unstructured clinical notes.", "method": "The study employs domain-adapted large language models for symptom extraction, contextual reasoning, and correlation from free-text reports, integrating fine-tuning, prompt-based inference, and entity-aware reasoning.", "result": "The proposed approach outperforms traditional models on MIMIC-III and CARDIO-NLP datasets, showing improved precision, recall, F1-score, and AUROC, with high clinical relevance validated by cardiologists.", "conclusion": "The findings highlight the potential of LLMs in clinical decision support systems to improve early warning systems and translate patient narratives into actionable assessments.", "key_contributions": ["Introduction of an LLM-augmented clinical NLP pipeline for cardiovascular disease risk assessment.", "Integration of fine-tuning and prompt engineering for better symptom extraction.", "Demonstration of high clinical relevance and performance improvements over existing models."], "limitations": "Challenges such as contextual hallucination and temporal ambiguity were identified and addressed.", "keywords": ["Cardiovascular disease", "NLP pipeline", "Large language models", "Clinical decision support", "Risk stratification"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.08836", "pdf": "https://arxiv.org/pdf/2503.08836.pdf", "abs": "https://arxiv.org/abs/2503.08836", "title": "A Critical Analysis of the Usage of Dimensionality Reduction in Four Domains", "authors": ["Dylan Cashman", "Mark Keller", "Hyeon Jeon", "Bum Chul Kwon", "Qianwen Wang"], "categories": ["cs.HC"], "comment": "Accepted at IEEE Transactions on Visualization and Computer Graphics,\n  to be presented at IEEE Visualization conference", "summary": "Dimensionality reduction is used as an important tool for unraveling the\ncomplexities of high-dimensional datasets in many fields of science, such as\ncell biology, chemical informatics, and physics. Visualizations of the\ndimensionally reduced data enable scientists to delve into the intrinsic\nstructures of their datasets and align them with established hypotheses.\nVisualization researchers have thus proposed many dimensionality reduction\nmethods and interactive systems designed to uncover latent structures. At the\nsame time, different scientific domains have formulated guidelines or common\nworkflows for using dimensionality reduction techniques and visualizations for\ntheir respective fields. In this work, we present a critical analysis of the\nusage of dimensionality reduction in scientific domains outside of computer\nscience. First, we conduct a bibliometric analysis of 21,249 academic\npublications that use dimensionality reduction to observe differences in the\nfrequency of techniques across fields. Next, we conduct a survey of a 71-paper\nsample from four fields: biology, chemistry, physics, and business. Through\nthis survey, we uncover common workflows, processes, and usage patterns,\nincluding the mixed use of confirmatory data analysis to validate a dataset and\nprojection method and exploratory data analysis to then generate more\nhypotheses. We also find that misinterpretations and inappropriate usage is\ncommon, particularly in the visual interpretation of the resulting\ndimensionally reduced view. Lastly, we compare our observations with recent\nworks in the visualization community in order to match work within our\ncommunity to potential areas of impact outside our community.", "AI": {"tldr": "The paper analyzes the usage of dimensionality reduction in scientific fields beyond computer science, highlighting workflows, common practices, and issues in interpretation.", "motivation": "To explore how dimensionality reduction techniques are applied in various scientific domains and identify common practices and pitfalls.", "method": "A bibliometric analysis of 21,249 publications followed by a survey of 71 papers in biology, chemistry, physics, and business to observe usage patterns.", "result": "Identified differences in the frequency of techniques across fields, common workflows, and prevalent misinterpretations in visual data representation.", "conclusion": "The study highlights areas for improvement in the use of dimensionality reduction and suggests potential impacts on visualization research.", "key_contributions": ["Critical analysis of dimensionality reduction in fields outside computer science", "Identification of workflows and common practices in multiple disciplines", "Highlighting common misinterpretations in data visualization"], "limitations": "May not cover all scientific fields or applications of dimensionality reduction", "keywords": ["dimensionality reduction", "data visualization", "scientific workflows", "interdisciplinary analysis", "bibliometric study"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2507.11084", "pdf": "https://arxiv.org/pdf/2507.11084.pdf", "abs": "https://arxiv.org/abs/2507.11084", "title": "Social Media Sentiments Analysis on the July Revolution in Bangladesh: A Hybrid Transformer Based Machine Learning Approach", "authors": ["Md. Sabbir Hossen", "Md. Saiduzzaman", "Pabon Shaha"], "categories": ["cs.CL"], "comment": "This paper has been accepted and presented at the IEEE ECAI 2025. The\n  final version will be available in the IEEE Xplore Digital Library", "summary": "The July Revolution in Bangladesh marked a significant student-led mass\nuprising, uniting people across the nation to demand justice, accountability,\nand systemic reform. Social media platforms played a pivotal role in amplifying\npublic sentiment and shaping discourse during this historic mass uprising. In\nthis study, we present a hybrid transformer-based sentiment analysis framework\nto decode public opinion expressed in social media comments during and after\nthe revolution. We used a brand new dataset of 4,200 Bangla comments collected\nfrom social media. The framework employs advanced transformer-based feature\nextraction techniques, including BanglaBERT, mBERT, XLM-RoBERTa, and the\nproposed hybrid XMB-BERT, to capture nuanced patterns in textual data.\nPrinciple Component Analysis (PCA) were utilized for dimensionality reduction\nto enhance computational efficiency. We explored eleven traditional and\nadvanced machine learning classifiers for identifying sentiments. The proposed\nhybrid XMB-BERT with the voting classifier achieved an exceptional accuracy of\n83.7% and outperform other model classifier combinations. This study\nunderscores the potential of machine learning techniques to analyze social\nsentiment in low-resource languages like Bangla.", "AI": {"tldr": "This study presents a hybrid transformer-based sentiment analysis framework utilizing Bangla social media comments during the July Revolution in Bangladesh, achieving 83.7% accuracy.", "motivation": "To analyze public sentiment expressed on social media during the July Revolution in Bangladesh, and to demonstrate the effectiveness of machine learning in low-resource languages.", "method": "A hybrid transformer-based sentiment analysis framework was developed using advanced features from BanglaBERT, mBERT, XLM-RoBERTa, and the new XMB-BERT model, with PCA for dimensionality reduction and eleven machine learning classifiers for sentiment classification.", "result": "The hybrid XMB-BERT model, combined with a voting classifier, achieved an accuracy of 83.7%, outperforming other model combinations.", "conclusion": "The study highlights the potential of machine learning techniques for sentiment analysis in low-resource languages, showcasing the importance of social media during political movements.", "key_contributions": ["Introduction of a hybrid transformer-based sentiment analysis framework", "Utilization of a novel dataset of 4,200 Bangla social media comments", "Demonstration of high accuracy (83.7%) using advanced ML techniques"], "limitations": "", "keywords": ["sentiment analysis", "transformer models", "social media", "Bangla language", "machine learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2503.15511", "pdf": "https://arxiv.org/pdf/2503.15511.pdf", "abs": "https://arxiv.org/abs/2503.15511", "title": "The Trust Calibration Maturity Model for Characterizing and Communicating Trustworthiness of AI Systems", "authors": ["Scott T Steinmetz", "Asmeret Naugle", "Paul Schutte", "Matt Sweitzer", "Alex Washburne", "Lisa Linville", "Daniel Krofcheck", "Michal Kucer", "Samuel Myren"], "categories": ["cs.HC", "cs.CY", "cs.LG"], "comment": "19 pages, 4 figures, 2 tables", "summary": "Recent proliferation of powerful AI systems has created a strong need for\ncapabilities that help users to calibrate trust in those systems. As AI systems\ngrow in scale, information required to evaluate their trustworthiness becomes\nless accessible, presenting a growing risk of using these systems\ninappropriately. We propose the Trust Calibration Maturity Model (TCMM) to\ncharacterize and communicate information about AI system trustworthiness. The\nTCMM incorporates five dimensions of analytic maturity: Performance\nCharacterization, Bias & Robustness Quantification, Transparency, Safety &\nSecurity, and Usability. The TCMM can be presented along with system\nperformance information to (1) help a user to appropriately calibrate trust,\n(2) establish requirements and track progress, and (3) identify research needs.\nHere, we discuss the TCMM and demonstrate it on two target tasks: using ChatGPT\nfor high consequence nuclear science determinations, and using PhaseNet (an\nensemble of seismic models) for categorizing sources of seismic events.", "AI": {"tldr": "The paper introduces the Trust Calibration Maturity Model (TCMM) to help users evaluate AI system trustworthiness across five dimensions.", "motivation": "To address the increasing difficulty of evaluating the trustworthiness of AI systems as they become more complex and pervasive.", "method": "Introduction of the Trust Calibration Maturity Model (TCMM), which incorporates five dimensions: Performance Characterization, Bias & Robustness Quantification, Transparency, Safety & Security, and Usability.", "result": "The TCMM aids users in calibrating trust appropriately, tracking system performance, and identifying research needs, illustrated through examples in nuclear science and seismic event categorization.", "conclusion": "The TCMM serves as a framework for communicating and improving trust in AI systems, which is essential as their impact on society grows.", "key_contributions": ["Introduction of the Trust Calibration Maturity Model (TCMM)", "Incorporation of five dimensions to evaluate AI trustworthiness", "Practical application of TCMM in critical domains like nuclear science and seismic analysis."], "limitations": "", "keywords": ["Trust Calibration", "AI Trustworthiness", "Human-Computer Interaction", "Usability", "Machine Learning"], "importance_score": 9, "read_time_minutes": 19}}
{"id": "2507.11086", "pdf": "https://arxiv.org/pdf/2507.11086.pdf", "abs": "https://arxiv.org/abs/2507.11086", "title": "Beyond Traditional Algorithms: Leveraging LLMs for Accurate Cross-Border Entity Identification", "authors": ["Andres Azqueta-Gavaldón", "Joaquin Ramos Cosgrove"], "categories": ["cs.CL"], "comment": null, "summary": "The growing prevalence of cross-border financial activities in global markets\nhas underscored the necessity of accurately identifying and classifying foreign\nentities. This practice is essential within the Spanish financial system for\nensuring robust risk management, regulatory adherence, and the prevention of\nfinancial misconduct. This process involves a labor-intensive entity-matching\ntask, where entities need to be validated against available reference sources.\nChallenges arise from linguistic variations, special characters, outdated\nnames, and changes in legal forms, complicating traditional matching algorithms\nlike Jaccard, cosine, and Levenshtein distances. These methods struggle with\ncontextual nuances and semantic relationships, leading to mismatches. To\naddress these limitations, we explore Large Language Models (LLMs) as a\nflexible alternative. LLMs leverage extensive training to interpret context,\nhandle abbreviations, and adapt to legal transitions. We evaluate traditional\nmethods, Hugging Face-based LLMs, and interface-based LLMs (e.g., Microsoft\nCopilot, Alibaba's Qwen 2.5) using a dataset of 65 Portuguese company cases.\nResults show traditional methods achieve accuracies over 92% but suffer high\nfalse positive rates (20-40%). Interface-based LLMs outperform, achieving\naccuracies above 93%, F1 scores exceeding 96%, and lower false positives\n(40-80%).", "AI": {"tldr": "This paper discusses the identification and classification of foreign entities in the Spanish financial system using Large Language Models (LLMs) to improve accuracy and reduce false positives in entity-matching tasks compared to traditional methods.", "motivation": "The increase in cross-border financial activities demands accurate identification of foreign entities for risk management and compliance, which is challenged by traditional matching algorithms.", "method": "The study compares traditional entity-matching algorithms (Jaccard, cosine, Levenshtein distances) with LLMs based on Hugging Face and interface-based systems like Microsoft Copilot and Alibaba's Qwen 2.5, utilizing a dataset of 65 Portuguese company cases for evaluation.", "result": "Traditional methods achieved over 92% accuracy but had high false positive rates of 20-40%. Interface-based LLMs showed better performance with accuracies above 93%, F1 scores exceeding 96%, and significantly reduced false positive rates (40-80%).", "conclusion": "LLMs offer a more effective solution for entity matching in financial systems, addressing limitations of traditional algorithms and improving overall accuracy and reliability.", "key_contributions": ["Comparison of traditional entity-matching algorithms with LLM approaches", "Demonstrated improvements in accuracy and false positive rates using LLMs", "Validated findings using a real-world dataset of Portuguese companies."], "limitations": "", "keywords": ["entity matching", "Large Language Models", "financial systems", "risk management", "accuracy"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.11097", "pdf": "https://arxiv.org/pdf/2507.11097.pdf", "abs": "https://arxiv.org/abs/2507.11097", "title": "The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs", "authors": ["Zichen Wen", "Jiashu Qu", "Dongrui Liu", "Zhiyuan Liu", "Ruixi Wu", "Yicun Yang", "Xiangqi Jin", "Haoyun Xu", "Xuyang Liu", "Weijia Li", "Chaochao Lu", "Jing Shao", "Conghui He", "Linfeng Zhang"], "categories": ["cs.CL"], "comment": "21 pages, 9 figures, work in progress", "summary": "Diffusion-based large language models (dLLMs) have recently emerged as a\npowerful alternative to autoregressive LLMs, offering faster inference and\ngreater interactivity via parallel decoding and bidirectional modeling.\nHowever, despite strong performance in code generation and text infilling, we\nidentify a fundamental safety concern: existing alignment mechanisms fail to\nsafeguard dLLMs against context-aware, masked-input adversarial prompts,\nexposing novel vulnerabilities. To this end, we present DIJA, the first\nsystematic study and jailbreak attack framework that exploits unique safety\nweaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial\ninterleaved mask-text prompts that exploit the text generation mechanisms of\ndLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional\nmodeling drives the model to produce contextually consistent outputs for masked\nspans, even when harmful, while parallel decoding limits model dynamic\nfiltering and rejection sampling of unsafe content. This causes standard\nalignment mechanisms to fail, enabling harmful completions in alignment-tuned\ndLLMs, even when harmful behaviors or unsafe instructions are directly exposed\nin the prompt. Through comprehensive experiments, we demonstrate that DIJA\nsignificantly outperforms existing jailbreak methods, exposing a previously\noverlooked threat surface in dLLM architectures. Notably, our method achieves\nup to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior\nbaseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and\nby 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of\nharmful content in the jailbreak prompt. Our findings underscore the urgent\nneed for rethinking safety alignment in this emerging class of language models.\nCode is available at https://github.com/ZichenWen1/DIJA.", "AI": {"tldr": "This paper introduces DIJA, a framework that systematically studies and exploits vulnerabilities in diffusion-based large language models (dLLMs) using adversarial prompts, revealing shortcomings in current safety mechanisms.", "motivation": "To address safety concerns in diffusion-based large language models that are not sufficiently guarded against adversarial prompts, demonstrating the need for improved alignment mechanisms.", "method": "The authors developed DIJA, a systematic framework that creates adversarial interleaved mask-text prompts to exploit dLLMs' safety weaknesses due to bidirectional modeling and parallel decoding.", "result": "DIJA significantly outperforms previous jailbreak methods, achieving up to 100% keyword-based ASR and surpassing prior benchmarks in the effectiveness of jailbreak prompts without content rewriting.", "conclusion": "The study highlights critical vulnerabilities in dLLMs, emphasizing an urgent need to rethink safety alignment in these models to ensure their safe deployment in applications.", "key_contributions": ["Introduction of DIJA as a novel jailbreak attack framework for dLLMs", "Comprehensive demonstrations of the framework outperforming existing methods", "Identification of specific safety weaknesses in dLLMs related to alignment mechanisms"], "limitations": "", "keywords": ["diffusion-based language models", "jailbreak attack", "adversarial prompts", "safety mechanisms"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2507.11112", "pdf": "https://arxiv.org/pdf/2507.11112.pdf", "abs": "https://arxiv.org/abs/2507.11112", "title": "Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs", "authors": ["Sanhanat Sivapiromrat", "Caiqi Zhang", "Marco Basaldella", "Nigel Collier"], "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "Recent studies have shown that Large Language Models (LLMs) are vulnerable to\ndata poisoning attacks, where malicious training examples embed hidden\nbehaviours triggered by specific input patterns. However, most existing works\nassume a phrase and focus on the attack's effectiveness, offering limited\nunderstanding of trigger mechanisms and how multiple triggers interact within\nthe model. In this paper, we present a framework for studying poisoning in\nLLMs. We show that multiple distinct backdoor triggers can coexist within a\nsingle model without interfering with each other, enabling adversaries to embed\nseveral triggers concurrently. Using multiple triggers with high embedding\nsimilarity, we demonstrate that poisoned triggers can achieve robust activation\neven when tokens are substituted or separated by long token spans. Our findings\nexpose a broader and more persistent vulnerability surface in LLMs. To mitigate\nthis threat, we propose a post hoc recovery method that selectively retrains\nspecific model components based on a layer-wise weight difference analysis. Our\nmethod effectively removes the trigger behaviour with minimal parameter\nupdates, presenting a practical and efficient defence against multi-trigger\npoisoning.", "AI": {"tldr": "This paper presents a framework to study data poisoning attacks in Large Language Models (LLMs), revealing that multiple backdoor triggers can coexist without interference and introducing a mitigation method through selective retraining.", "motivation": "To understand the vulnerabilities of LLMs to data poisoning attacks, specifically the interactions of multiple triggers within a model and the mechanisms of activation.", "method": "A framework analyzing how distinct backdoor triggers can operate concurrently within LLMs, along with a post hoc recovery method based on layer-wise weight difference analysis for mitigation.", "result": "Demonstrated that multiple triggers can coexist without interference, achieving activation even with altered token arrangements, exposing a broader vulnerability in LLMs.", "conclusion": "The proposed recovery method effectively mitigates the threat of multi-trigger poisoning with minimal parameter updates, enhancing model security.", "key_contributions": ["Framework for studying multiple trigger interactions in LLMs", "Demonstration of robust activation of poisoned triggers", "Post hoc recovery method for defense against multi-trigger poisoning"], "limitations": "Focuses primarily on multi-trigger scenarios and may not cover all types of attacks.", "keywords": ["Large Language Models", "Data poisoning", "Backdoor triggers", "Mitigation method", "Model security"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.11114", "pdf": "https://arxiv.org/pdf/2507.11114.pdf", "abs": "https://arxiv.org/abs/2507.11114", "title": "MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models", "authors": ["Seif Ahmed", "Mohamed T. Younes", "Abdelrahman Moustafa", "Abdelrahman Allam", "Hamza Moustafa"], "categories": ["cs.CL"], "comment": null, "summary": "We present a robust ensemble-based system for multilingual multimodal\nreasoning, designed for the ImageCLEF 2025 EXAMS V challenge. Our approach\nintegrates Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption\nrefinement and consistency checks, and Gemini 2.5 Pro as a reasoner which\nhandles final answer selection, all coordinated through carefully engineered\nfew-shot and zero-shot prompts. We conducted an extensive ablation study,\ntraining several large language models (Gemini 2.5 Flash, Phi 4, Gemma 3,\nMistral) on an English dataset and its multilingual augmented version.\nAdditionally, we evaluated Gemini 2.5 Flash in a zero-shot setting for\ncomparison and found it to substantially outperform the trained models. Prompt\ndesign also proved critical: enforcing concise, language-normalized formats and\nprohibiting explanatory text boosted model accuracy on the English validation\nset from 55.9% to 61.7%. On the official leaderboard, our system (Team MSA)\nachieved first place overall in the multilingual track with 81.4% accuracy, and\nled 11 out of 13 individual language tracks, with top results such as 95.07%\nfor Croatian and 92.12% for Italian. These findings highlight that lightweight\nOCR-VLM ensembles, when paired with precise prompt strategies and cross-lingual\naugmentation, can outperform heavier end-to-end models in high-stakes,\nmultilingual educational settings.", "AI": {"tldr": "An ensemble system for multilingual multimodal reasoning achieves top results in the ImageCLEF 2025 EXAMS V challenge, demonstrating effective prompt strategies and cross-lingual training.", "motivation": "To enhance multilingual reasoning capabilities in educational settings using a robust ensemble-based system.", "method": "The approach integrates various models (Gemini 2.5 Flash, Gemini 1.5 Pro, Gemini 2.5 Pro) and utilizes few-shot and zero-shot prompts in an ablation study to evaluate effectiveness.", "result": "Outperformed trained models with a first-place achievement on the leaderboard, scoring 81.4% accuracy overall and leading in 11 out of 13 individual language tracks.", "conclusion": "Lightweight OCR-VLM ensembles with targeted prompting strategies can outperform heavier models in multilingual tasks.", "key_contributions": ["Introduction of an ensemble-based multilingual reasoning system.", "Demonstration of effective prompt design leading to improved accuracy.", "Achievement of top performance in a competitive multilingual reasoning challenge."], "limitations": "", "keywords": ["multilingual reasoning", "ensemble methods", "prompt engineering"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2507.11128", "pdf": "https://arxiv.org/pdf/2507.11128.pdf", "abs": "https://arxiv.org/abs/2507.11128", "title": "What Should LLMs Forget? Quantifying Personal Data in LLMs for Right-to-Be-Forgotten Requests", "authors": ["Dimitri Staufer"], "categories": ["cs.CL", "cs.CY", "cs.LG", "I.2.6; H.2.8"], "comment": "16 pages, 3 figures. Accepted at the 7th Workshop on eXplainable\n  Knowledge Discovery in Data Mining (XKDD 2025), ECML PKDD 2025, Porto,\n  Portugal", "summary": "Large Language Models (LLMs) can memorize and reveal personal information,\nraising concerns regarding compliance with the EU's GDPR, particularly the\nRight to Be Forgotten (RTBF). Existing machine unlearning methods assume the\ndata to forget is already known but do not address how to identify which\nindividual-fact associations are stored in the model. Privacy auditing\ntechniques typically operate at the population level or target a small set of\nidentifiers, limiting applicability to individual-level data inquiries. We\nintroduce WikiMem, a dataset of over 5,000 natural language canaries covering\n243 human-related properties from Wikidata, and a model-agnostic metric to\nquantify human-fact associations in LLMs. Our approach ranks ground-truth\nvalues against counterfactuals using calibrated negative log-likelihood across\nparaphrased prompts. We evaluate 200 individuals across 15 LLMs (410M-70B\nparameters), showing that memorization correlates with subject web presence and\nmodel scale. We provide a foundation for identifying memorized personal data in\nLLMs at the individual level, enabling the dynamic construction of forget sets\nfor machine unlearning and RTBF requests.", "AI": {"tldr": "This paper introduces WikiMem, a novel dataset and a model-agnostic metric to identify memorized personal data in large language models (LLMs) for compliance with GDPR's Right to Be Forgotten.", "motivation": "Concerns about LLMs revealing personal information and the need for compliance with GDPR's RTBF.", "method": "Development of WikiMem, a dataset of 5,000 natural language canaries and a metric to evaluate individual-fact associations in LLMs using negative log-likelihood for ground-truth and counterfactual comparisons.", "result": "Evaluation of 200 individuals across 15 LLMs demonstrates a correlation between memorization and both subject web presence and model scale.", "conclusion": "This work lays the groundwork for identifying and managing memorized personal data in LLMs, facilitating machine unlearning and RTBF processes.", "key_contributions": ["Introduction of a new dataset (WikiMem) comprising natural language canaries.", "A model-agnostic metric for quantifying human-fact associations in LLMs.", "Empirical evaluation linking memorization in LLMs to web presence and model size."], "limitations": "The focus on known models and may not generalize to all LLM architectures.", "keywords": ["Large Language Models", "Right to Be Forgotten", "data privacy", "machine unlearning", "WikiMem"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.11198", "pdf": "https://arxiv.org/pdf/2507.11198.pdf", "abs": "https://arxiv.org/abs/2507.11198", "title": "Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding", "authors": ["Conrad Borchers", "Bahar Shahrokhian", "Francesco Balzan", "Elham Tajik", "Sreecharan Sankaranarayanan", "Sebastian Simon"], "categories": ["cs.CL", "cs.AI"], "comment": "Manuscript submitted for review", "summary": "Large Language Models (LLMs) enable new possibilities for qualitative\nresearch at scale, including coding and data annotation. While multi-agent\nsystems (MAS) can emulate human coding workflows, their benefits over\nsingle-agent coding remain poorly understood. We conducted an experimental\nstudy of how agent persona and temperature shape consensus-building and coding\naccuracy of dialog segments based on a codebook with 8 codes. Our open-source\nMAS mirrors deductive human coding through structured agent discussion and\nconsensus arbitration. Using six open-source LLMs (with 3 to 32 billion\nparameters) and 18 experimental configurations, we analyze over 77,000 coding\ndecisions against a gold-standard dataset of human-annotated transcripts from\nonline math tutoring sessions. Temperature significantly impacted whether and\nwhen consensus was reached across all six LLMs. MAS with multiple personas\n(including neutral, assertive, or empathetic), significantly delayed consensus\nin four out of six LLMs compared to uniform personas. In three of those LLMs,\nhigher temperatures significantly diminished the effects of multiple personas\non consensus. However, neither temperature nor persona pairing lead to robust\nimprovements in coding accuracy. Single agents matched or outperformed MAS\nconsensus in most conditions. Only one model (OpenHermesV2:7B) and code\ncategory showed above-chance gains from MAS deliberation when temperature was\n0.5 or lower and especially when the agents included at least one assertive\npersona. Qualitative analysis of MAS collaboration for these configurations\nsuggests that MAS may nonetheless aid in narrowing ambiguous code applications\nthat could improve codebooks and human-AI coding. We contribute new insight\ninto the limits of LLM-based qualitative methods, challenging the notion that\ndiverse MAS personas lead to better outcomes. We open-source our MAS and\nexperimentation code.", "AI": {"tldr": "This study examines the effectiveness of multi-agent systems (MAS) using large language models (LLMs) for qualitative research coding, revealing that single-agent coding often outperforms MAS in accuracy.", "motivation": "To understand the benefits and limitations of using multi-agent systems in qualitative research coding, particularly in relation to coding accuracy and consensus-building among different agent personas and temperatures.", "method": "An experimental study using an open-source MAS with six LLMs of varying sizes, investigating the impacts of agent persona and temperature on the consensus-building process and coding accuracy from a dataset of over 77,000 decisions.", "result": "While temperature influenced consensus-building, it did not improve coding accuracy; single-agent systems frequently outperformed MAS, with only one LLM showing minor improvements under specific conditions.", "conclusion": "MAS does not demonstrably improve coding accuracy over single-agent systems; however, it may help clarify ambiguous coding scenarios and improve human-AI interactions.", "key_contributions": ["Experimental insights on the effectiveness of MAS vs. single-agent systems for coding research", "Challenging the assumption that diverse agent personas enhance coding outcomes", "Open-sourcing of MAS and experimentation code for further research"], "limitations": "Limited to the specific configurations and datasets used in the study; findings may not generalize across all qualitative coding tasks.", "keywords": ["large language models", "multi-agent systems", "coding accuracy", "qualitative research", "open-source"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.11216", "pdf": "https://arxiv.org/pdf/2507.11216.pdf", "abs": "https://arxiv.org/abs/2507.11216", "title": "EsBBQ and CaBBQ: The Spanish and Catalan Bias Benchmarks for Question Answering", "authors": ["Valle Ruiz-Fernández", "Mario Mina", "Júlia Falcão", "Luis Vasquez-Reina", "Anna Sallés", "Aitor Gonzalez-Agirre", "Olatz Perez-de-Viñaspre"], "categories": ["cs.CL"], "comment": null, "summary": "Previous literature has largely shown that Large Language Models (LLMs)\nperpetuate social biases learnt from their pre-training data. Given the notable\nlack of resources for social bias evaluation in languages other than English,\nand for social contexts outside of the United States, this paper introduces the\nSpanish and the Catalan Bias Benchmarks for Question Answering (EsBBQ and\nCaBBQ). Based on the original BBQ, these two parallel datasets are designed to\nassess social bias across 10 categories using a multiple-choice QA setting, now\nadapted to the Spanish and Catalan languages and to the social context of\nSpain. We report evaluation results on different LLMs, factoring in model\nfamily, size and variant. Our results show that models tend to fail to choose\nthe correct answer in ambiguous scenarios, and that high QA accuracy often\ncorrelates with greater reliance on social biases.", "AI": {"tldr": "This paper introduces the Spanish and Catalan Bias Benchmarks for Question Answering (EsBBQ and CaBBQ) to evaluate social biases in LLMs in Spain.", "motivation": "The need for resources to evaluate social biases in non-English languages and different social contexts.", "method": "The paper presents two datasets designed for question answering, assessing social bias across 10 categories using a multiple-choice format.", "result": "LLMs show a tendency to choose incorrect answers in ambiguous scenarios, with high QA accuracy linked to reliance on social biases.", "conclusion": "Social biases in LLMs persist across different languages and contexts, necessitating robust evaluation frameworks.", "key_contributions": ["Introduction of EsBBQ and CaBBQ datasets for assessing social bias in LLMs in Spanish and Catalan contexts", "Evaluation results highlighting the link between QA accuracy and social bias reliance", "Expansion of bias evaluation resources beyond English and U.S. contexts"], "limitations": "The study is limited to Spanish and Catalan languages within the context of Spain, which may not generalize globally.", "keywords": ["Large Language Models", "Bias Evaluation", "Spanish", "Catalan", "Question Answering"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.11222", "pdf": "https://arxiv.org/pdf/2507.11222.pdf", "abs": "https://arxiv.org/abs/2507.11222", "title": "An Agentic Flow for Finite State Machine Extraction using Prompt Chaining", "authors": ["Fares Wael", "Youssef Maklad", "Ali Hamdi", "Wael Elsersy"], "categories": ["cs.CL", "cs.AI", "cs.NI"], "comment": null, "summary": "Finite-State Machines (FSMs) are critical for modeling the operational logic\nof network protocols, enabling verification, analysis, and vulnerability\ndiscovery. However, existing FSM extraction techniques face limitations such as\nscalability, incomplete coverage, and ambiguity in natural language\nspecifications. In this paper, we propose FlowFSM, a novel agentic framework\nthat leverages Large Language Models (LLMs) combined with prompt chaining and\nchain-of-thought reasoning to extract accurate FSMs from raw RFC documents.\nFlowFSM systematically processes protocol specifications, identifies state\ntransitions, and constructs structured rule-books by chaining agent outputs.\nExperimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM\nachieves high extraction precision while minimizing hallucinated transitions,\nshowing promising results. Our findings highlight the potential of agent-based\nLLM systems in the advancement of protocol analysis and FSM inference for\ncybersecurity and reverse engineering applications.", "AI": {"tldr": "This paper presents FlowFSM, an agent-based framework using Large Language Models (LLMs) for extracting finite-state machines (FSMs) from protocol specifications, improving accuracy and minimizing hallucinations.", "motivation": "Existing FSM extraction techniques are limited in scalability and coverage, leading to issues in modeling network protocols effectively.", "method": "FlowFSM leverages LLMs in conjunction with prompt chaining and chain-of-thought reasoning to extract FSMs from raw RFC documents, systematically processing protocol specifications to identify state transitions.", "result": "FlowFSM demonstrates high extraction precision and significantly reduces hallucinated transitions across FTP and RTSP protocols during experimental evaluation.", "conclusion": "The study reveals the potential of using agent-based LLM systems for enhancing protocol analysis and FSM inference, particularly for applications in cybersecurity and reverse engineering.", "key_contributions": ["Introduction of FlowFSM framework for FSM extraction using LLMs", "Demonstration of high precision in FSM extraction", "Innovative use of prompt chaining and reasoning with LLMs"], "limitations": "", "keywords": ["Finite-State Machines", "Large Language Models", "Protocol Analysis", "Cybersecurity", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.11230", "pdf": "https://arxiv.org/pdf/2507.11230.pdf", "abs": "https://arxiv.org/abs/2507.11230", "title": "Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages", "authors": ["Lyzander Marciano Andrylie", "Inaya Rahmanisa", "Mahardika Krisna Ihsani", "Alfan Farizki Wicaksono", "Haryo Akbarianto Wibowo", "Alham Fikri Aji"], "categories": ["cs.CL", "68T50"], "comment": null, "summary": "Understanding the multilingual mechanisms of large language models (LLMs)\nprovides insight into how they process different languages, yet this remains\nchallenging. Existing studies often focus on individual neurons, but their\npolysemantic nature makes it difficult to isolate language-specific units from\ncross-lingual representations. To address this, we explore sparse autoencoders\n(SAEs) for their ability to learn monosemantic features that represent concrete\nand abstract concepts across languages in LLMs. While some of these features\nare language-independent, the presence of language-specific features remains\nunderexplored. In this work, we introduce SAE-LAPE, a method based on feature\nactivation probability, to identify language-specific features within the\nfeed-forward network. We find that many such features predominantly appear in\nthe middle to final layers of the model and are interpretable. These features\ninfluence the model's multilingual performance and language output and can be\nused for language identification with performance comparable to fastText along\nwith more interpretability. Our code is available at\nhttps://github.com/LyzanderAndrylie/language-specific-features .", "AI": {"tldr": "This paper presents SAE-LAPE, a method for identifying language-specific features in large language models (LLMs) using sparse autoencoders.", "motivation": "To provide a better understanding of how large language models process multilingual data by isolating language-specific features from cross-lingual representations.", "method": "The authors utilize sparse autoencoders (SAEs) with a focus on feature activation probability to identify and analyze language-specific features in LLMs.", "result": "The proposed method, SAE-LAPE, reveals that many language-specific features appear in the model's middle to final layers, are interpretable, and can effectively identify languages with performance similar to fastText.", "conclusion": "The findings suggest that recognizing language-specific features can enhance our understanding of LLMs and contribute to improved multilingual performance and language identification tasks.", "key_contributions": ["Introduction of SAE-LAPE for identifying language-specific features in LLMs", "Demonstration of interpretable language-specific features found in intermediate model layers", "Achievement of competitive language identification performance compared to fastText."], "limitations": "", "keywords": ["large language models", "sparse autoencoders", "language-specific features", "multilingual processing", "NLP"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.11273", "pdf": "https://arxiv.org/pdf/2507.11273.pdf", "abs": "https://arxiv.org/abs/2507.11273", "title": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware Rotary Positional Embedding", "authors": ["Luohe Shi", "Zuchao Li", "Lefei Zhang", "Guoming Liu", "Baoyuan Qi", "Hai Zhao"], "categories": ["cs.CL"], "comment": "To be published in The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent.", "AI": {"tldr": "This paper introduces a new paradigm called KV-Latent to enhance the efficiency of Transformer Decoders in conversational AI by down-sampling Key-Value caches, leading to reduced memory usage and faster inference.", "motivation": "To tackle the efficiency bottlenecks related to memory consumption and data transfer bandwidth in Transformer Decoder architectures used for conversational generative AI.", "method": "The proposed KV-Latent paradigm downsamples Key-Value vector dimensions into a latent space, complemented by modifications to the Rotary Positional Embedding frequency sampling mechanism.", "result": "The experiments demonstrated that the KV-Latent approach reduces KV Cache size and enhances inference speed, with a minor increase in training cost.", "conclusion": "The findings suggest that KV-Latent enables the creation of more efficient language models, paving the way for improved KV Cache management in LLMs.", "key_contributions": ["Introduction of the KV-Latent paradigm for Key-Value cache optimization.", "Enhancement of Rotary Positional Embedding stability in lower dimensions.", "Empirical evidence showing improved model performance with reduced Key-Value components."], "limitations": "The additional training required is less than 1% of pre-training, but the specific scenarios in which the improvements apply need further study.", "keywords": ["Large Language Models", "Key-Value Cache", "Inference Efficiency", "Transformer Decoders", "Rotary Positional Embedding"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.11275", "pdf": "https://arxiv.org/pdf/2507.11275.pdf", "abs": "https://arxiv.org/abs/2507.11275", "title": "FMC: Formalization of Natural Language Mathematical Competition Problems", "authors": ["Jiaxuan Xie", "Chengwu Liu", "Ye Yuan", "Siqi Li", "Zhiping Xiao", "Ming Zhang"], "categories": ["cs.CL"], "comment": "Accepted in ICML 2025 AI4MATH Workshop", "summary": "Efficient and accurate autoformalization methods, which leverage large-scale\ndatasets of extensive natural language mathematical problems to construct\nformal language datasets, are key to advancing formal mathematical reasoning.\nIn this paper, we propose an autoformalization pipeline based on large language\nmodels with error feedback, achieving a fully automatic and training-free\nformalization approach. Using this pipeline, we curate an Olympiad-level\ndataset aligning natural language problems with Lean formalizations. The\ndataset comprises $3,922$ mathematical problems in natural language and $9,787$\nin Lean, of which $64.46\\%$ were assessed as at least above-average quality,\nmaking it suitable as a benchmark for automated theorem provers. Additionally,\nwe investigate the formalization and reasoning capabilities of various LLMs and\nempirically demonstrate that few-shot learning, error feedback, and increasing\nsampling numbers enhance the autoformalization process. Experiments of three\nautomated theorem provers on the \\dataset\\ dataset also highlight its\nchallenging nature and its value as a benchmark for formal reasoning tasks.", "AI": {"tldr": "This paper presents an autoformalization pipeline using large language models to create a dataset of natural language mathematical problems formalized in Lean, achieving a fully automated and training-free approach to formal mathematical reasoning.", "motivation": "The research aims to enhance formal mathematical reasoning through efficient autoformalization methods that utilize large datasets of natural language mathematical problems.", "method": "The paper introduces an autoformalization pipeline based on large language models that incorporates error feedback to formalize natural language problems into a formal language without the need for training.", "result": "A curated dataset consisting of 3,922 natural language mathematical problems and 9,787 corresponding formalizations in Lean was created, with 64.46% rated above-average quality, proving valuable for benchmarking automated theorem provers.", "conclusion": "The findings demonstrate the effectiveness of LLMs and techniques such as few-shot learning and error feedback in improving the autoformalization process, making the dataset a valuable benchmark for future work in formal reasoning tasks.", "key_contributions": ["Development of an automated and training-free autoformalization pipeline", "Creation of a high-quality benchmark dataset for formal reasoning", "Empirical investigation of LLM capabilities in formalization tasks"], "limitations": "", "keywords": ["autoformalization", "large language models", "formal reasoning", "dataset creation", "benchmarking"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.11292", "pdf": "https://arxiv.org/pdf/2507.11292.pdf", "abs": "https://arxiv.org/abs/2507.11292", "title": "Fine-Grained Chinese Hate Speech Understanding: Span-Level Resources, Coded Term Lexicon, and Enhanced Detection Frameworks", "authors": ["Zewen Bai", "Liang Yang", "Shengdi Yin", "Yuanyuan Sun", "Hongfei Lin"], "categories": ["cs.CL"], "comment": null, "summary": "The proliferation of hate speech has inflicted significant societal harm,\nwith its intensity and directionality closely tied to specific targets and\narguments. In recent years, numerous machine learning-based methods have been\ndeveloped to detect hateful comments on online platforms automatically.\nHowever, research on Chinese hate speech detection lags behind, and\ninterpretability studies face two major challenges: first, the scarcity of\nspan-level fine-grained annotated datasets limits models' deep semantic\nunderstanding of hate speech; second, insufficient research on identifying and\ninterpreting coded hate speech restricts model explainability in complex\nreal-world scenarios. To address these, we make the following contributions:\n(1) We introduce the Span-level Target-Aware Toxicity Extraction dataset (STATE\nToxiCN), the first span-level Chinese hate speech dataset, and evaluate the\nhate semantic understanding of existing models using it. (2) We conduct the\nfirst comprehensive study on Chinese coded hate terms, LLMs' ability to\ninterpret hate semantics. (3) We propose a method to integrate an annotated\nlexicon into models, significantly enhancing hate speech detection performance.\nOur work provides valuable resources and insights to advance the\ninterpretability of Chinese hate speech detection research.", "AI": {"tldr": "The paper addresses the challenges of Chinese hate speech detection by introducing a new annotated dataset and proposing methods to improve model interpretability and performance.", "motivation": "The need for effective Chinese hate speech detection is critical due to its societal harm, yet current research is limited by the lack of fine-grained datasets and interpretability issues.", "method": "The study introduces the STATE ToxiCN dataset and evaluates existing models on this dataset, alongside a comprehensive exploration of coded hate speech and a method to integrate an annotated lexicon into detection models.", "result": "The introduction of the STATE ToxiCN dataset allows for better understanding and evaluation of hate speech detection models, while the proposed methods enhance model performance and interpretability.", "conclusion": "This work offers important resources that can significantly improve both the detection and understanding of Chinese hate speech in online contexts.", "key_contributions": ["Introduction of the first span-level Chinese hate speech dataset (STATE ToxiCN).", "Comprehensive study on the interpretation of coded hate speech in Chinese.", "Proposal of a method to integrate an annotated lexicon to improve detection performance."], "limitations": "", "keywords": ["hate speech detection", "Chinese language", "interpretability", "machine learning", "toxic comment identification"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.11299", "pdf": "https://arxiv.org/pdf/2507.11299.pdf", "abs": "https://arxiv.org/abs/2507.11299", "title": "Dr.Copilot: A Multi-Agent Prompt Optimized Assistant for Improving Patient-Doctor Communication in Romanian", "authors": ["Andrei Niculae", "Adrian Cosma", "Cosmin Dumitrache", "Emilian Rǎdoi"], "categories": ["cs.CL"], "comment": "10 figures, 2 tables, 2 listings", "summary": "Text-based telemedicine has become increasingly common, yet the quality of\nmedical advice in doctor-patient interactions is often judged more on how\nadvice is communicated rather than its clinical accuracy. To address this, we\nintroduce Dr.Copilot , a multi-agent large language model (LLM) system that\nsupports Romanian-speaking doctors by evaluating and enhancing the presentation\nquality of their written responses. Rather than assessing medical correctness,\nDr.Copilot provides feedback along 17 interpretable axes. The system comprises\nof three LLM agents with prompts automatically optimized via DSPy. Designed\nwith low-resource Romanian data and deployed using open-weight models, it\ndelivers real-time specific feedback to doctors within a telemedicine platform.\nEmpirical evaluations and live deployment with 41 doctors show measurable\nimprovements in user reviews and response quality, marking one of the first\nreal-world deployments of LLMs in Romanian medical settings.", "AI": {"tldr": "Dr.Copilot is a multi-agent LLM system designed to enhance the communication quality of Romanian-speaking doctors' telemedicine responses, improving user reviews and response quality through interpretable feedback.", "motivation": "The need for improved communication quality in doctor-patient interactions, especially in telemedicine, where advice quality is often judged by presentation rather than clinical accuracy.", "method": "Dr.Copilot employs three LLM agents providing feedback along 17 axes, optimized with DSPy, using low-resource Romanian data. It delivers real-time feedback within a telemedicine platform.", "result": "Empirical evaluations showed measurable improvements in user reviews and response quality from 41 doctors using the system.", "conclusion": "Dr.Copilot marks a significant advancement in real-world applications of LLMs in Romanian healthcare, emphasizing communication quality in medical advice.", "key_contributions": ["Introduction of a multi-agent LLM system for telemedicine communication", "Use of interpretable axes for feedback on response quality", "Empirical validation through live deployment with healthcare professionals"], "limitations": "", "keywords": ["telemedicine", "large language models", "communication quality", "healthcare technology", "Romanian language"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.11316", "pdf": "https://arxiv.org/pdf/2507.11316.pdf", "abs": "https://arxiv.org/abs/2507.11316", "title": "Internal Value Alignment in Large Language Models through Controlled Value Vector Activation", "authors": ["Haoran Jin", "Meng Li", "Xiting Wang", "Zhihao Xu", "Minlie Huang", "Yantao Jia", "Defu Lian"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "25 pages, 14 figures. Accepted by ACL 2025 (main conference)", "summary": "Aligning Large Language Models (LLMs) with human values has attracted\nincreasing attention since it provides clarity, transparency, and the ability\nto adapt to evolving scenarios. In this paper, we introduce a Controlled Value\nVector Activation (ConVA) method that directly aligns the internal values of\nLLMs by interpreting how a value is encoded in their latent representations and\nmodifies relevant activations to ensure consistent values in LLMs. To ensure an\naccurate and unbiased interpretation, we propose a context-controlled value\nvector identification method. To consistently control values without\nsacrificing model performance, we introduce a gated value vector activation\nmethod for effective and minimum degree of value control. Experiments show that\nour method achieves the highest control success rate across 10 basic values\nwithout hurting LLM performance and fluency, and ensures target values even\nwith opposite and potentially malicious input prompts. Source code and data are\navailable at~ https://github.com/hr-jin/ConVA.", "AI": {"tldr": "This paper introduces the Controlled Value Vector Activation (ConVA) method for aligning Large Language Models (LLMs) with human values, ensuring model performance while providing clarity and adaptability. It includes an innovative context-controlled value vector identification process and a gated activation method.", "motivation": "The motivation behind this work is to align LLMs with human values to enhance clarity, transparency, and adaptability in various scenarios.", "method": "The paper presents a Controlled Value Vector Activation (ConVA) method that interprets how values are encoded in LLMs' latent representations and modifies activations to ensure value consistency. It includes a context-controlled value vector identification method and a gated value vector activation approach.", "result": "Experiments demonstrate that the ConVA method achieves the highest control success rate across 10 basic values while maintaining LLM performance and fluency, even with adversarial prompts.", "conclusion": "The study concludes that the proposed ConVA method effectively controls LLM values without compromising performance, paving the way for more aligned and responsible AI systems.", "key_contributions": ["Introduction of Controlled Value Vector Activation (ConVA) method", "Context-controlled value vector identification method", "Gated value vector activation for efficient value control"], "limitations": "", "keywords": ["Large Language Models", "Value Alignment", "Human Values", "Machine Learning", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 25}}
{"id": "2507.11330", "pdf": "https://arxiv.org/pdf/2507.11330.pdf", "abs": "https://arxiv.org/abs/2507.11330", "title": "Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge", "authors": ["Wenqing Wu", "Chengzhi Zhang", "Yi Zhao"], "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.HC"], "comment": "Journal of the Association for Information Science and Technology,\n  2025", "summary": "Novelty is a crucial criterion in the peer review process for evaluating\nacademic papers. Traditionally, it's judged by experts or measure by unique\nreference combinations. Both methods have limitations: experts have limited\nknowledge, and the effectiveness of the combination method is uncertain.\nMoreover, it's unclear if unique citations truly measure novelty. The large\nlanguage model (LLM) possesses a wealth of knowledge, while human experts\npossess judgment abilities that the LLM does not possess. Therefore, our\nresearch integrates the knowledge and abilities of LLM and human experts to\naddress the limitations of novelty assessment. The most common novelty in\nacademic papers is the introduction of new methods. In this paper, we propose\nleveraging human knowledge and LLM to assist pretrained language models (PLMs,\ne.g. BERT etc.) in predicting the method novelty of papers. Specifically, we\nextract sentences related to the novelty of the academic paper from peer review\nreports and use LLM to summarize the methodology section of the academic paper,\nwhich are then used to fine-tune PLMs. In addition, we have designed a\ntext-guided fusion module with novel Sparse-Attention to better integrate human\nand LLM knowledge. We compared the method we proposed with a large number of\nbaselines. Extensive experiments demonstrate that our method achieves superior\nperformance.", "AI": {"tldr": "The paper addresses the limitations in assessing novelty in academic papers by integrating the expertise of human reviewers with large language models (LLMs) to evaluate method novelty more effectively.", "motivation": "Novelty is a key criterion in peer review, but both expert judgment and existing citation methods have limitations.", "method": "The study proposes combining human knowledge and LLMs to assist pretrained language models (PLMs) in predicting the novelty of academic methods. This involves extracting sentences from peer review reports and summarizing methodology sections for fine-tuning PLMs, utilizing a text-guided fusion module with Sparse-Attention.", "result": "The proposed method outperforms numerous baselines in predicting method novelty in academic papers.", "conclusion": "Integrating human and LLM insights significantly enhances the assessment of novelty in academic publishing.", "key_contributions": ["Integration of LLM and human expertise for novelty assessment", "Development of a text-guided fusion module with Sparse-Attention", "Improvement over baseline methods in method novelty prediction"], "limitations": "", "keywords": ["Novelty", "Human-Computer Interaction", "Large Language Models", "Method Prediction", "Sparse-Attention"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.11356", "pdf": "https://arxiv.org/pdf/2507.11356.pdf", "abs": "https://arxiv.org/abs/2507.11356", "title": "What is the Best Process Model Representation? A Comparative Analysis for Process Modeling with Large Language Models", "authors": ["Alexis Brissard", "Frédéric Cuppens", "Amal Zouaq"], "categories": ["cs.CL"], "comment": "12 pages, 7 figures, to be published in AI4BPM 2025 Proceedings", "summary": "Large Language Models (LLMs) are increasingly applied for Process Modeling\n(PMo) tasks such as Process Model Generation (PMG). To support these tasks,\nresearchers have introduced a variety of Process Model Representations (PMRs)\nthat serve as model abstractions or generation targets. However, these PMRs\ndiffer widely in structure, complexity, and usability, and have never been\nsystematically compared. Moreover, recent PMG approaches rely on distinct\nevaluation strategies and generation techniques, making comparison difficult.\nThis paper presents the first empirical study that evaluates multiple PMRs in\nthe context of PMo with LLMs. We introduce the PMo Dataset, a new dataset\ncontaining 55 process descriptions paired with models in nine different PMRs.\nWe evaluate PMRs along two dimensions: suitability for LLM-based PMo and\nperformance on PMG. \\textit{Mermaid} achieves the highest overall score across\nsix PMo criteria, whereas \\textit{BPMN text} delivers the best PMG results in\nterms of process element similarity.", "AI": {"tldr": "This paper presents an empirical study evaluating various Process Model Representations (PMRs) for Process Modeling tasks using Large Language Models (LLMs), introducing a new dataset and comparing PMR suitability and performance.", "motivation": "To systematically compare diverse Process Model Representations (PMRs) used in conjunction with Large Language Models for Process Model Generation, as existing comparisons are lacking.", "method": "An empirical study utilizing the PMo Dataset containing 55 process descriptions and models in nine PMRs, evaluating PMRs on their suitability for LLM-based Process Modeling and their performance in Process Model Generation.", "result": "Mermaid shows the highest score across six criteria of Process Modeling, while BPMN text achieves the best results for process element similarity in Process Model Generation.", "conclusion": "The findings highlight the varying effectiveness of different PMRs for LLM-based Process Modeling, underscoring the importance of systematic evaluation.", "key_contributions": ["Introduction of the PMo Dataset with 55 process descriptions and nine PMRs.", "First empirical evaluation of multiple PMRs in the context of LLMs for Process Modeling.", "Identification of the top-performing PMRs based on suitability and generation performance."], "limitations": "The study is limited to only nine PMRs and specific process descriptions, which may not encompass all potential model representations.", "keywords": ["Process Modeling", "Large Language Models", "Process Model Representations", "Empirical Study", "AI4BPM"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2507.11384", "pdf": "https://arxiv.org/pdf/2507.11384.pdf", "abs": "https://arxiv.org/abs/2507.11384", "title": "Addressing Data Imbalance in Transformer-Based Multi-Label Emotion Detection with Weighted Loss", "authors": ["Xia Cui"], "categories": ["cs.CL"], "comment": "10 pages, 1 figure, SemEval 2025", "summary": "This paper explores the application of a simple weighted loss function to\nTransformer-based models for multi-label emotion detection in SemEval-2025\nShared Task 11. Our approach addresses data imbalance by dynamically adjusting\nclass weights, thereby enhancing performance on minority emotion classes\nwithout the computational burden of traditional resampling methods. We evaluate\nBERT, RoBERTa, and BART on the BRIGHTER dataset, using evaluation metrics such\nas Micro F1, Macro F1, ROC-AUC, Accuracy, and Jaccard similarity coefficients.\nThe results demonstrate that the weighted loss function improves performance on\nhigh-frequency emotion classes but shows limited impact on minority classes.\nThese findings underscore both the effectiveness and the challenges of applying\nthis approach to imbalanced multi-label emotion detection.", "AI": {"tldr": "This paper investigates using a weighted loss function to improve emotion detection in multi-label settings for Transformer models, specifically targeting class imbalance in data.", "motivation": "To enhance performance in multi-label emotion detection tasks by addressing class imbalance in datasets without the need for traditional resampling methods.", "method": "A weighted loss function is applied to Transformer-based models (BERT, RoBERTa, BART) for analyzing performance on the BRIGHTER dataset, focusing on multiple evaluation metrics.", "result": "The introduction of a weighted loss function improved performance on high-frequency emotion classes but had limited effects on the minority classes.", "conclusion": "While the weighted loss function shows promise in handling data imbalance, it also reveals ongoing challenges in effectively improving predictions for underrepresented classes.", "key_contributions": ["Proposed a simple weighted loss function for Transformer-based models in emotion detection", "Demonstrated the application on the BRIGHTER dataset for SemEval-2025", "Evaluated results using multiple metrics focusing on both high and low-frequency emotion classes."], "limitations": "The weighted loss function shows limited impact on minority emotion classes despite improvements in overall performance for high-frequency classes.", "keywords": ["Emotion Detection", "Weighted Loss Function", "Transformer Models", "Multilabel Classification", "Data Imbalance"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.11405", "pdf": "https://arxiv.org/pdf/2507.11405.pdf", "abs": "https://arxiv.org/abs/2507.11405", "title": "DCR: Quantifying Data Contamination in LLMs Evaluation", "authors": ["Cheng Xu", "Nan Yan", "Shuhao Guan", "Changhong Jin", "Yuke Mei", "Yibing Guo", "M-Tahar Kechadi"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has heightened concerns\nabout benchmark data contamination (BDC), where models inadvertently memorize\nevaluation data, inflating performance metrics and undermining genuine\ngeneralization assessment. This paper introduces the Data Contamination Risk\n(DCR) framework, a lightweight, interpretable pipeline designed to detect and\nquantify BDC across four granular levels: semantic, informational, data, and\nlabel. By synthesizing contamination scores via a fuzzy inference system, DCR\nproduces a unified DCR Factor that adjusts raw accuracy to reflect\ncontamination-aware performance. Validated on 9 LLMs (0.5B-72B) across\nsentiment analysis, fake news detection, and arithmetic reasoning tasks, the\nDCR framework reliably diagnoses contamination severity and with accuracy\nadjusted using the DCR Factor to within 4% average error across the three\nbenchmarks compared to the uncontaminated baseline. Emphasizing computational\nefficiency and transparency, DCR provides a practical tool for integrating\ncontamination assessment into routine evaluations, fostering fairer comparisons\nand enhancing the credibility of LLM benchmarking practices.", "AI": {"tldr": "This paper presents the Data Contamination Risk (DCR) framework for detecting and quantifying benchmark data contamination in large language models (LLMs).", "motivation": "The paper addresses concerns regarding benchmark data contamination (BDC) in LLMs, which can inflate performance metrics and affect genuine generalization assessments.", "method": "The DCR framework detects BDC at four levels: semantic, informational, data, and label, utilizing a fuzzy inference system to synthesize contamination scores and produce a unified DCR Factor.", "result": "Validated on 9 LLMs, the DCR framework reliably measures contamination severity and adjusts accuracy to within 4% error compared to uncontaminated baselines across various tasks.", "conclusion": "DCR enhances the credibility of LLM benchmarking by providing a computationally efficient and transparent tool for routine contamination assessments.", "key_contributions": ["Introduction of the DCR framework for BDC detection and quantification", "Fuzzy inference system for synthesizing contamination scores", "Practical implementation for fairer LLM benchmarking"], "limitations": "", "keywords": ["benchmark data contamination", "large language models", "data contamination risk", "performance metrics", "fuzzy inference system"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.11407", "pdf": "https://arxiv.org/pdf/2507.11407.pdf", "abs": "https://arxiv.org/abs/2507.11407", "title": "EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes", "authors": ["LG AI Research", ":", "Kyunghoon Bae", "Eunbi Choi", "Kibong Choi", "Stanley Jungkyu Choi", "Yemuk Choi", "Kyubeen Han", "Seokhee Hong", "Junwon Hwang", "Taewan Hwang", "Joonwon Jang", "Hyojin Jeon", "Kijeong Jeon", "Gerrard Jeongwon Jo", "Hyunjik Jo", "Jiyeon Jung", "Euisoon Kim", "Hyosang Kim", "Jihoon Kim", "Joonkee Kim", "Seonghwan Kim", "Soyeon Kim", "Sunkyoung Kim", "Yireun Kim", "Yongil Kim", "Youchul Kim", "Edward Hwayoung Lee", "Gwangho Lee", "Haeju Lee", "Honglak Lee", "Jinsik Lee", "Kyungmin Lee", "Sangha Park", "Young Min Paik", "Yongmin Park", "Youngyong Park", "Sanghyun Seo", "Sihoon Yang", "Heuiyeen Yeen", "Sihyuk Yi", "Hyeongu Yun"], "categories": ["cs.CL", "cs.AI"], "comment": "Technical Report, 30 Pages", "summary": "This technical report introduces EXAONE 4.0, which integrates a Non-reasoning\nmode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5\nand the advanced reasoning abilities of EXAONE Deep. To pave the way for the\nagentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool\nuse, and its multilingual capabilities are extended to support Spanish in\naddition to English and Korean. The EXAONE 4.0 model series consists of two\nsizes: a mid-size 32B model optimized for high performance, and a small-size\n1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates\nsuperior performance compared to open-weight models in its class and remains\ncompetitive even against frontier-class models. The models are publicly\navailable for research purposes and can be easily downloaded via\nhttps://huggingface.co/LGAI-EXAONE.", "AI": {"tldr": "EXAONE 4.0 is a new AI model offering both high usability and advanced reasoning capabilities, with enhanced multilingual support and two model sizes.", "motivation": "To advance the usability and reasoning capabilities in AI models, paving the way for the agentic AI era.", "method": "Introduces two modes (Non-reasoning and Reasoning) and provides models in two sizes (32B and 1.2B) for different applications.", "result": "EXAONE 4.0 shows improved performance over open-weight models and competes well with frontier-class models.", "conclusion": "EXAONE 4.0 models are publicly available for research and demonstrate exceptional performance in various applications.", "key_contributions": ["Integration of Non-reasoning and Reasoning modes", "Multilingual support including Spanish", "Two model sizes for diverse applications"], "limitations": "", "keywords": ["EXAONE 4.0", "AI models", "agentic AI", "multilingual capabilities", "machine learning"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2507.11408", "pdf": "https://arxiv.org/pdf/2507.11408.pdf", "abs": "https://arxiv.org/abs/2507.11408", "title": "KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical Reasoning?", "authors": ["Soumadeep Saha", "Akshay Chaturvedi", "Saptarshi Saha", "Utpal Garain", "Nicholas Asher"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "15 pages, 9 figures", "summary": "Chain-of-thought traces have been shown to improve performance of large\nlanguage models in a plethora of reasoning tasks, yet there is no consensus on\nthe mechanism through which this performance boost is achieved. To shed more\nlight on this, we introduce Causal CoT Graphs (CCGs), which are directed\nacyclic graphs automatically extracted from reasoning traces that model\nfine-grained causal dependencies in the language model output. A collection of\n$1671$ mathematical reasoning problems from MATH500, GSM8K and AIME, and their\nassociated CCGs are compiled into our dataset -- \\textbf{KisMATH}. Our detailed\nempirical analysis with 15 open-weight LLMs shows that (i) reasoning nodes in\nthe CCG are mediators for the final answer, a condition necessary for\nreasoning; and (ii) LLMs emphasise reasoning paths given by the CCG, indicating\nthat models internally realise structures akin to our graphs. KisMATH enables\ncontrolled, graph-aligned interventions and opens up avenues for further\ninvestigation into the role of chain-of-thought in LLM reasoning.", "AI": {"tldr": "The paper introduces Causal CoT Graphs (CCGs) to elucidate how chain-of-thought reasoning enhances the performance of large language models in problem-solving tasks.", "motivation": "To understand the mechanism behind the performance boosts in large language models (LLMs) when using chain-of-thought reasoning.", "method": "Causal CoT Graphs (CCGs) are constructed as directed acyclic graphs from reasoning traces, capturing fine-grained causal dependencies. The dataset KisMATH, containing 1671 mathematical reasoning problems and associated CCGs, is analyzed using 15 open-weight LLMs.", "result": "The analysis shows that reasoning nodes in CCGs act as mediators for achieving final answers, indicating that LLMs utilize reasoning paths represented by CCGs.", "conclusion": "KisMATH facilitates controlled interventions for investigating the influence of chain-of-thought in LLM reasoning, suggesting that LLMs may internally recognize structures similar to CCGs.", "key_contributions": ["Introduction of Causal CoT Graphs (CCGs) for modeling reasoning processes in LLMs.", "Compilation of a new dataset, KisMATH, for evaluating reasoning in mathematics.", "Empirical evidence that LLMs leverage structured reasoning paths for enhanced performance."], "limitations": "", "keywords": ["Causal CoT Graphs", "Large Language Models", "Mathematical Reasoning", "Chain-of-Thought Reasoning", "Dataset KisMATH"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.11412", "pdf": "https://arxiv.org/pdf/2507.11412.pdf", "abs": "https://arxiv.org/abs/2507.11412", "title": "Seq vs Seq: An Open Suite of Paired Encoders and Decoders", "authors": ["Orion Weller", "Kathryn Ricci", "Marc Marone", "Antoine Chaffin", "Dawn Lawrie", "Benjamin Van Durme"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "The large language model (LLM) community focuses almost exclusively on\ndecoder-only language models, since they are easier to use for text generation.\nHowever, a large subset of the community still uses encoder-only models for\ntasks such as classification or retrieval. Previous work has attempted to\ncompare these architectures, but is forced to make comparisons with models that\nhave different numbers of parameters, training techniques, and datasets. We\nintroduce the SOTA open-data Ettin suite of models: paired encoder-only and\ndecoder-only models ranging from 17 million parameters to 1 billion, trained on\nup to 2 trillion tokens. Using the same recipe for both encoder-only and\ndecoder-only models produces SOTA recipes in both categories for their\nrespective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as\ndecoders. Like previous work, we find that encoder-only models excel at\nclassification and retrieval tasks while decoders excel at generative tasks.\nHowever, we show that adapting a decoder model to encoder tasks (and vice\nversa) through continued training is subpar compared to using only the reverse\nobjective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa\nfor generative tasks). We open-source all artifacts of this study including\ntraining data, training order segmented by checkpoint, and 200+ checkpoints to\nallow future work to analyze or extend all aspects of training.", "AI": {"tldr": "This paper introduces the Ettin suite of models, comparing encoder-only and decoder-only architectures in NLP. It establishes that encoder models outperform decoders for classification and retrieval, while decoders excel in generative tasks. The study provides comprehensive open-source resources for further research.", "motivation": "To compare encoder-only and decoder-only models effectively, addressing the variability in parameters, training techniques, and datasets used in past research.", "method": "The study introduces the Ettin suite of paired encoder-only and decoder-only models, trained using a consistent methodology across varying sizes (17M to 1B parameters) on up to 2 trillion tokens.", "result": "The results indicate that encoder-only models are superior for classification and retrieval tasks, whereas decoder-only models excel at generative tasks. Continued training a decoder for encoder tasks (and vice versa) yielded subpar results.", "conclusion": "Using models specifically designed for their tasks (encoder for classification, decoder for generation) is more effective than adapting one type for the tasks of the other.", "key_contributions": ["Introduction of the SOTA open-data Ettin suite of paired models", "Demonstration of superior performance of encoder models for classification and retrieval tasks", "Provision of open-source artifacts including training data and model checkpoints for future research"], "limitations": "", "keywords": ["language models", "encoder-only", "decoder-only", "natural language processing", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.11423", "pdf": "https://arxiv.org/pdf/2507.11423.pdf", "abs": "https://arxiv.org/abs/2507.11423", "title": "Reasoning Strategies in Large Language Models: Can They Follow, Prefer, and Optimize?", "authors": ["Yanjian Zhang", "Guillaume Wisniewski", "Nadi Tomeh", "Thierry Charnois"], "categories": ["cs.CL"], "comment": null, "summary": "Human reasoning involves different strategies, each suited to specific\nproblems. Prior work shows that large language model (LLMs) tend to favor a\nsingle reasoning strategy, potentially limiting their effectiveness in diverse\nreasoning challenges. In this work, we investigate whether prompting can\ncontrol LLMs reasoning strategies and assess its impact on logical\nproblem-solving. While our experiments show that no single strategy\nconsistently improves accuracy, performance could be enhanced if models could\nadaptively choose the optimal strategy. We propose methods to guide LLMs in\nstrategy selection, highlighting new ways to refine their reasoning abilities.", "AI": {"tldr": "This paper investigates whether prompting can control the reasoning strategies of large language models (LLMs) to enhance logical problem-solving.", "motivation": "To understand if LLMs can be guided to use more effective reasoning strategies for diverse problem-solving tasks.", "method": "The study evaluates the impact of different prompting techniques on LLMs' reasoning strategies and their accuracy in logical problem-solving tasks.", "result": "The results indicate that no single reasoning strategy consistently enhances accuracy, but performance improves when models can adaptively select optimal strategies.", "conclusion": "Adaptive strategy selection in LLMs could refine their reasoning abilities, suggesting potential methods for guiding their reasoning processes.", "key_contributions": ["Investigates control over LLMs' reasoning strategies using prompting", "Highlights the importance of adaptive strategy selection for improved performance", "Proposes methods for refining LLMs' reasoning capabilities"], "limitations": "The study acknowledges that results may vary across different LLM architectures and tasks, limiting generalizability.", "keywords": ["Large Language Models", "Reasoning Strategies", "Logical Problem-Solving", "Prompting Techniques", "Adaptive Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.11502", "pdf": "https://arxiv.org/pdf/2507.11502.pdf", "abs": "https://arxiv.org/abs/2507.11502", "title": "HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong", "authors": ["Sirui Han", "Junqi Zhu", "Ruiyuan Zhang", "Yike Guo"], "categories": ["cs.CL", "cs.CE", "cs.LG"], "comment": null, "summary": "This paper presents the development of HKGAI-V1, a foundational sovereign\nlarge language model (LLM), developed as part of an initiative to establish\nvalue-aligned AI infrastructure specifically tailored for Hong Kong. Addressing\nthe region's unique multilingual environment (Cantonese, Mandarin, and\nEnglish), its distinct socio-legal context under the \"one country, two systems\"\nframework, and specific local cultural and value considerations, the model is\nbuilt upon the DeepSeek architecture and systematically aligned with regional\nnorms through a multifaceted full parameter fine-tuning process. It is further\nintegrated with a retrieval-augmented generation (RAG) system to ensure timely\nand factually grounded information access. The core contribution lies in the\ndesign and implementation of a comprehensive, region-specific AI alignment and\nsafety framework, demonstrated through two key achievements: 1) The successful\ndevelopment of HKGAI-V1 itself - which outper-forms general-purpose models in\nhandling Hong Kong-specific culturally sensitive queries, and embodies a\n\"governance-embedded\" approach to digital sovereignty - empowers Hong Kong to\nexercise control over AI applications in critical sectors including public\nservices, legal systems, and edu-cation. 2) The development of the proprietary\nAdversarial HK Value Benchmark, a rigorous tool for evaluating model alignment\nwith local ethical and legal stand-ards under challenging conditions. By\ndocumenting these achievements, the paper provides not only a technological\nartifact but also a replicable blueprint for developing advanced, regionally\nfocused AI systems deeply rooted in their local identities.", "AI": {"tldr": "Development of HKGAI-V1, a large language model tailored for Hong Kong's multilingual and cultural context, integrated with a RAG system and an alignment framework.", "motivation": "To establish a value-aligned AI infrastructure specifically for Hong Kong that respects its unique socio-legal context and culture.", "method": "Developed using the DeepSeek architecture and a comprehensive fine-tuning process, integrated with a retrieval-augmented generation (RAG) system for accurate information access.", "result": "HKGAI-V1 outperforms general-purpose models in handling culturally sensitive queries related to Hong Kong, while a proprietary benchmark aids in evaluating model alignment with local standards.", "conclusion": "The paper outlines a replicable framework for creating AI systems that are deeply rooted in regional identities, enhancing governance in critical sectors.", "key_contributions": ["Development of HKGAI-V1 for handling Hong Kong-specific queries.", "Creation of the Adversarial HK Value Benchmark for evaluating AI alignment with local standards.", "Presentation of a governance-embedded approach to AI sovereignty."], "limitations": "", "keywords": ["large language model", "AI alignment", "cultural sensitivity", "retrieval-augmented generation", "regional AI"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.11508", "pdf": "https://arxiv.org/pdf/2507.11508.pdf", "abs": "https://arxiv.org/abs/2507.11508", "title": "Real-World Summarization: When Evaluation Reaches Its Limits", "authors": ["Patrícia Schmidtová", "Ondřej Dušek", "Saad Mahamood"], "categories": ["cs.CL"], "comment": null, "summary": "We examine evaluation of faithfulness to input data in the context of hotel\nhighlights: brief LLM-generated summaries that capture unique features of\naccommodations. Through human evaluation campaigns involving categorical error\nassessment and span-level annotation, we compare traditional metrics, trainable\nmethods, and LLM-as-a-judge approaches. Our findings reveal that simpler\nmetrics like word overlap correlate surprisingly well with human judgments\n(Spearman correlation rank of 0.63), often outperforming more complex methods\nwhen applied to out-of-domain data. We further demonstrate that while LLMs can\ngenerate high-quality highlights, they prove unreliable for evaluation as they\ntend to severely under- or over-annotate. Our analysis of real-world business\nimpacts shows incorrect and non-checkable information pose the greatest risks.\nWe also highlight challenges in crowdsourced evaluations.", "AI": {"tldr": "This paper evaluates the effectiveness of various methods in assessing the faithfulness of LLM-generated hotel summaries, revealing that simpler metrics perform well despite LLMs generating high-quality content.", "motivation": "To assess the reliability of LLM-generated summaries in faithfully capturing the unique features of hotel accommodations and to compare evaluation methodologies.", "method": "Utilized human evaluation campaigns with categorical error assessment and span-level annotation, comparing traditional metrics against trainable methods and LLM-driven evaluations.", "result": "Simpler metrics like word overlap showed a strong correlation with human judgments (Spearman correlation of 0.63), outperforming complex methods on out-of-domain data.", "conclusion": "LLMs are effective in generating highlights but unreliable in evaluation, posing risks with incorrect information; challenges in crowdsourced evaluations were noted.", "key_contributions": ["Demonstrated the efficacy of simpler metrics in evaluating LLM outputs.", "Revealed the limitations of LLMs in evaluation roles.", "Analyzed real-world business impacts of incorrect summaries."], "limitations": "LLMs tend to severely under- or over-annotate, compromising evaluation reliability.", "keywords": ["LLM", "human evaluation", "hotel highlights", "evaluation metrics", "faithfulness"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.10773", "pdf": "https://arxiv.org/pdf/2507.10773.pdf", "abs": "https://arxiv.org/abs/2507.10773", "title": "Theory of Mind and Self-Disclosure to CUIs", "authors": ["Samuel Rhys Cox"], "categories": ["cs.HC", "cs.CL"], "comment": "Workshop paper presented at ToMinHAI at CUI'2025: Theory of Mind in\n  Human-CUI Interaction, held in conjunction with the 2025 ACM conference on\n  Conversational User Interfaces, July 8th, 2025. 4 pages. 3 figures", "summary": "Self-disclosure is important to help us feel better, yet is often difficult.\nThis difficulty can arise from how we think people are going to react to our\nself-disclosure. In this workshop paper, we briefly discuss self-disclosure to\nconversational user interfaces (CUIs) in relation to various social cues. We\nthen, discuss how expressions of uncertainty or representation of a CUI's\nreasoning could help encourage self-disclosure, by making a CUI's intended\n\"theory of mind\" more transparent to users.", "AI": {"tldr": "This paper discusses self-disclosure to conversational user interfaces (CUIs) and explores how social cues and displays of uncertainty can facilitate this process.", "motivation": "To understand how self-disclosure can be encouraged in interactions with CUIs, particularly addressing user concerns about conversational responses.", "method": "The paper discusses the relationship between social cues, user reactions, and self-disclosure in CUIs.", "result": "Expressions of uncertainty and clearer reasoning from CUIs may enhance user comfort in self-disclosing personal information.", "conclusion": "Enhancing transparency in CUI reasoning could promote better user interaction and self-disclosure.", "key_contributions": ["Exploration of social cues in CUIs", "Discussion on CUI reasoning and transparency", "Insights on user self-disclosure dynamics"], "limitations": "The workshop paper format limits depth and empirical validation of concepts discussed.", "keywords": ["self-disclosure", "conversational user interfaces", "social cues", "theory of mind", "user interaction"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2401.13444", "pdf": "https://arxiv.org/pdf/2401.13444.pdf", "abs": "https://arxiv.org/abs/2401.13444", "title": "Fine-grained Stateful Knowledge Exploration: Effective and Efficient Graph Retrieval with Large Language Models", "authors": ["Dehao Tao", "Congqi Wang", "Feng Huang", "Junhao Chen", "Yongfeng Huang", "Minghu Jiang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown impressive capabilities, yet updating\ntheir knowledge remains a significant challenge, often leading to outdated or\ninaccurate responses. A proposed solution is the integration of external\nknowledge bases, such as knowledge graphs, with LLMs. Most existing methods use\na paradigm that treats the whole question as the objective, with relevant\nknowledge being incrementally retrieved from the knowledge graph. However, this\nparadigm often leads to a granularity mismatch between the target question and\nthe retrieved entities and relations. As a result, the information in the\nquestion cannot precisely correspond to the retrieved knowledge. This may cause\nredundant exploration or omission of vital knowledge, thereby leading to\nenhanced computational consumption and reduced retrieval accuracy. To address\nthe limitations of coarse-grained knowledge exploration, we propose FiSKE, a\nnovel paradigm for Fine-grained Stateful Knowledge Exploration. FiSKE first\ndecomposes questions into fine-grained clues, then employs an adaptive mapping\nstrategy during knowledge exploration process to resolve ambiguity in\nclue-to-graph mappings. This strategy dynamically infers contextual\ncorrespondences while maintaining a stateful record of the mappings. A\nclue-driven termination mechanism ensures rigorous augmentation--leveraging\nfully mapped paths for LLMs while reverting to chain-of-thought reasoning when\nnecessary. Our approach balances precision and efficiency. Experiments on\nmultiple datasets revealed that our paradigm surpasses current advanced methods\nin knowledge retrieval while significantly reducing the average number of LLM\ninvocations.", "AI": {"tldr": "FiSKE is a novel paradigm for Fine-grained Stateful Knowledge Exploration, enhancing the integration of LLMs with external knowledge bases like knowledge graphs.", "motivation": "Updating knowledge in LLMs poses challenges, often resulting in outdated responses. Integrating external knowledge bases can help, but existing methods suffer from granularity mismatches leading to inefficiencies.", "method": "FiSKE decomposes questions into fine-grained clues and utilizes an adaptive mapping strategy to enhance knowledge exploration and retrieval accuracy.", "result": "FiSKE significantly outperforms existing methods in knowledge retrieval and reduces the number of LLM invocations across multiple datasets.", "conclusion": "The proposed approach balances the precision of knowledge retrieval with efficiency, addressing critical limitations of current methodologies.", "key_contributions": ["Introduction of a fine-grained clue decomposition approach", "Development of an adaptive mapping strategy for knowledge exploration", "Implementation of a clue-driven termination mechanism for LLM augmentation"], "limitations": "", "keywords": ["Large Language Models", "Knowledge Graphs", "Fine-grained Knowledge Retrieval", "Human-Computer Interaction", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2412.06136", "pdf": "https://arxiv.org/pdf/2412.06136.pdf", "abs": "https://arxiv.org/abs/2412.06136", "title": "AIDE: Attribute-Guided MultI-Hop Data Expansion for Data Scarcity in Task-Specific Fine-tuning", "authors": ["Jiayu Li", "Xuan Zhu", "Fang Liu", "Yanjun Qi"], "categories": ["cs.CL"], "comment": "Accepted for publication in ACL 2025. The official version will be\n  available in the ACL Anthology", "summary": "Fine-tuning large language models (LLMs) for specific tasks requires diverse,\nhigh-quality training data. However, obtaining sufficient relevant data remains\na significant challenge. Existing data synthesis methods either depend on\nextensive seed datasets or struggle to balance task relevance and data\ndiversity. To address these challenges, we propose Attribute-guided multI-hop\nData Expansion (AIDE), a novel data synthesis framework that uses a multi-hop\nprocess to expand very few seed data points while ensuring data diversity and\ntask relevance. AIDE extracts the main topic and key knowledge attributes from\nthe seeds to guide the synthesis steps. The process repeats for K hops, using\nthe generated data as seeds. To prevent irrelevant data generation as the hop\ndepth increases, AIDE incorporates a residual connection mechanism. Our\nempirical results show that AIDE enables fine-tuning of Mistral-7B,\nLlama-3.1-8B and Llama-3.2-3B from 10 seeds, surpassing the models fine-tuned\non human curated data. Furthermore, AIDE outperforms state-of-the-art data\nsynthesis methods, such as Evol-Instruct, by over 30% in task-specific\nfine-tuning. Code is available at https://github.com/Code4Graph/AIDE.", "AI": {"tldr": "AIDE is a novel framework for synthesizing diverse, relevant training data for fine-tuning large language models using a multi-hop process.", "motivation": "The need for diverse, high-quality training data in fine-tuning large language models is crucial, yet obtaining such data remains challenging.", "method": "AIDE employs a multi-hop approach to expand a small number of seed data points while ensuring task relevance and data diversity, incorporating a residual connection mechanism to avoid irrelevant data generation.", "result": "AIDE allows fine-tuning of Mistral-7B, Llama-3.1-8B, and Llama-3.2-3B with as few as 10 seeds, outperforming models trained on human-curated data by over 30% compared to state-of-the-art methods like Evol-Instruct.", "conclusion": "AIDE demonstrates significant improvement in data synthesis for fine-tuning, proving effective in generating relevant training data from minimal seed inputs.", "key_contributions": ["Proposes a novel framework for data synthesis for fine-tuning LLMs", "Introduces a multi-hop approach with a residual connection mechanism to enhance data relevance", "Shows substantial performance improvements over existing data synthesis methods"], "limitations": "", "keywords": ["data synthesis", "large language models", "fine-tuning", "multi-hop", "machine learning"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2412.14959", "pdf": "https://arxiv.org/pdf/2412.14959.pdf", "abs": "https://arxiv.org/abs/2412.14959", "title": "Understanding the Dark Side of LLMs' Intrinsic Self-Correction", "authors": ["Qingjie Zhang", "Di Wang", "Haoting Qian", "Yiming Li", "Tianwei Zhang", "Minlie Huang", "Ke Xu", "Hewu Li", "Yan Liu", "Han Qiu"], "categories": ["cs.CL"], "comment": null, "summary": "Intrinsic self-correction was proposed to improve LLMs' responses via\nfeedback prompts solely based on their inherent capability. However, recent\nworks show that LLMs' intrinsic self-correction fails without oracle labels as\nfeedback prompts. In this paper, we aim to interpret LLMs' intrinsic\nself-correction for different tasks, especially for those failure cases. By\nincluding one simple task and three complex tasks with state-of-the-art (SOTA)\nLLMs like ChatGPT families (o1, 4o, 3.5-turbo) and Llama families (2-7B, 3-8B,\nand 3.1-8B), we design three interpretation methods to reveal the dark side of\nLLMs' intrinsic self-correction. We identify intrinsic self-correction can (1)\ncause LLMs to waver both intermedia and final answers and lead to prompt bias\non simple factual questions; (2) introduce human-like cognitive bias on complex\ntasks. In light of our findings, we also provide two simple yet effective\nstrategies for alleviation: question repeating and supervised fine-tuning with\na few samples. We open-source our work at https://x-isc.info/.", "AI": {"tldr": "This paper investigates the limitations of intrinsic self-correction in LLMs and proposes strategies to mitigate its negative effects.", "motivation": "To interpret the failures of LLMs' intrinsic self-correction across various tasks, as it relies heavily on feedback prompts that are often not available.", "method": "The study involves conducting experiments with SOTA LLMs on one simple and three complex tasks, employing three interpretation methods to analyze the effects of intrinsic self-correction.", "result": "The findings reveal that intrinsic self-correction can cause instability in LLM responses, leading to prompt bias in simple questions and introducing cognitive bias in complex tasks.", "conclusion": "The paper suggests two strategies to alleviate the issues identified: repeating questions and using supervised fine-tuning with a limited number of samples, along with an open-sourced implementation.", "key_contributions": ["Analysis of intrinsic self-correction failure in LLMs", "Identification of biases introduced by LLM responses", "Proposition of mitigation strategies for LLM response issues"], "limitations": "The study may depend on the specific tasks and LLMs analyzed, limiting the generalizability of the findings.", "keywords": ["LLM", "intrinsic self-correction", "prompt bias", "cognitive bias", "fine-tuning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2412.21033", "pdf": "https://arxiv.org/pdf/2412.21033.pdf", "abs": "https://arxiv.org/abs/2412.21033", "title": "Plancraft: an evaluation dataset for planning with LLM agents", "authors": ["Gautier Dagan", "Frank Keller", "Alex Lascarides"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present Plancraft, a multi-modal evaluation dataset for LLM agents.\nPlancraft has both a text-only and multi-modal interface, based on the\nMinecraft crafting GUI. We include the Minecraft Wiki to evaluate tool use and\nRetrieval Augmented Generation (RAG), as well as a handcrafted planner and\nOracle Retriever, to ablate the different components of a modern agent\narchitecture. To evaluate decision-making, Plancraft also includes a subset of\nexamples that are intentionally unsolvable, providing a realistic challenge\nthat requires the agent not only to complete tasks but also to decide whether\nthey are solvable at all. We benchmark both open-source and closed-source LLMs\nand compare their performance and efficiency to a handcrafted planner. Overall,\nwe find that LLMs and VLMs struggle with the planning problems that Plancraft\nintroduces, and offer suggestions on how to improve their capabilities.", "AI": {"tldr": "Plancraft is a dataset for evaluating LLM agents with a focus on decision-making and tool use in a Minecraft crafting environment.", "motivation": "The motivation behind Plancraft is to create a dataset that evaluates the capabilities of LLM agents in decision-making and tool use, particularly in a multi-modal interface.", "method": "Plancraft includes both text-only and multi-modal interfaces and leverages tools like the Minecraft Wiki, a handcrafted planner, and an Oracle Retriever for benchmarking different agent architectures.", "result": "Benchmarking of both open-source and closed-source LLMs shows that they struggle with the challenges posed by Plancraft, particularly in planning and decision-making scenarios.", "conclusion": "The study offers insights and suggestions for improving the planning capabilities of LLMs and VLMs, highlighting their current limitations.", "key_contributions": ["Introduction of the Plancraft dataset for evaluating multi-modal LLM agents", "Benchmarking performance of various LLMs against decision-making challenges", "Identification of challenges in planning problems faced by current LLM architectures"], "limitations": "Plancraft's scenarios are intentionally unsolvable in some cases, which may limit the applicability of findings to purely solvable environments.", "keywords": ["LLM agents", "Plancraft", "decision-making", "tool use", "RAG"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.01706", "pdf": "https://arxiv.org/pdf/2502.01706.pdf", "abs": "https://arxiv.org/abs/2502.01706", "title": "Comply: Learning Sentences with Complex Weights inspired by Fruit Fly Olfaction", "authors": ["Alexei Figueroa", "Justus Westerhoff", "Golzar Atefi", "Dennis Fast", "Benjamin Winter", "Felix Alexander Gers", "Alexander Löser", "Wolfgang Nejdl"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "comment": "Accepted at NICE2025", "summary": "Biologically inspired neural networks offer alternative avenues to model data\ndistributions. FlyVec is a recent example that draws inspiration from the fruit\nfly's olfactory circuit to tackle the task of learning word embeddings.\nSurprisingly, this model performs competitively even against deep learning\napproaches specifically designed to encode text, and it does so with the\nhighest degree of computational efficiency. We pose the question of whether\nthis performance can be improved further. For this, we introduce Comply. By\nincorporating positional information through complex weights, we enable a\nsingle-layer neural network to learn sequence representations. Our experiments\nshow that Comply not only supersedes FlyVec but also performs on par with\nsignificantly larger state-of-the-art models. We achieve this without\nadditional parameters. Comply yields sparse contextual representations of\nsentences that can be interpreted explicitly from the neuron weights.", "AI": {"tldr": "Comply is a neural network model that incorporates positional information to learn sequence representations, outperforming FlyVec while maintaining computational efficiency.", "motivation": "To improve the performance of biologically inspired neural networks like FlyVec, which efficiently learns word embeddings.", "method": "Introduced Comply, incorporating complex weights for positional information in a single-layer neural network to learn sequence representations.", "result": "Comply surpasses the performance of FlyVec and matches larger state-of-the-art models without additional parameters.", "conclusion": "Comply produces sparse contextual sentence representations that are interpretable through the neuron weights, showing potential for further advancements in modeling data distributions.", "key_contributions": ["Introduction of Comply model for sequence representation learning", "Demonstrated improvements over FlyVec", "Achieved competitive performance with larger models without extra parameters"], "limitations": "", "keywords": ["neural networks", "word embeddings", "sequence representation", "biologically inspired models", "contextual representations"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2502.16366", "pdf": "https://arxiv.org/pdf/2502.16366.pdf", "abs": "https://arxiv.org/abs/2502.16366", "title": "A Generative Approach to LLM Harmfulness Detection with Special Red Flag Tokens", "authors": ["Sophie Xhonneux", "David Dobre", "Mehrnaz Mofakhami", "Leo Schwinn", "Gauthier Gidel"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "14 pages, 6 figures", "summary": "Most safety training methods for large language models (LLMs) are based on\nfine-tuning that forces models to shift from an unsafe answer to refusal when\nfaced with harmful requests. Unfortunately, these drastic distribution shifts\ngenerally compromise model capabilities. To avoid that, we propose to expand\nthe model's vocabulary with a special token we call red flag token (<rf>) and\npropose to train the model to insert this token into its response at any time\nwhen harmful content is generated or about to be generated. Our approach offers\nseveral advantages: it enables the model to explicitly learn the concept of\nharmfulness while marginally affecting the generated distribution, thus\nmaintaining the model's utility. It also evaluates each generated answer and\nprovides robustness as good as adversarial training without the need to run\nattacks during training. Moreover, by encapsulating our safety tuning in a LoRA\nmodule, we provide additional defenses against fine-tuning API attacks.", "AI": {"tldr": "This paper introduces a novel safety training method for LLMs via the insertion of a red flag token to identify and manage harmful content without sacrificing model performance.", "motivation": "Existing safety training methods compromise model capabilities by inducing drastic distribution shifts. This approach aims to maintain model utility while ensuring safety.", "method": "The authors propose a red flag token (<rf>) that the model learns to insert when harmful content is generated or imminent, allowing it to evaluate responses while preserving performance.", "result": "The approach enhances model robustness against harmful content generation while maintaining its utility, showing performance comparable to adversarial training without requiring attack methods.", "conclusion": "By encapsulating safety tuning in a LoRA module, the method provides additional defenses against fine-tuning API attacks and preserves model capabilities.", "key_contributions": ["Introduction of the red flag token (<rf>) for harm identification", "Maintaining model utility during safety training", "Use of LoRA modules for enhanced safety against API attacks"], "limitations": "The approach may require extensive evaluation on various datasets to ensure generalization across different contexts.", "keywords": ["language models", "safety training", "red flag token", "harmful content", "LoRA"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.05294", "pdf": "https://arxiv.org/pdf/2504.05294.pdf", "abs": "https://arxiv.org/abs/2504.05294", "title": "Truthful or Fabricated? Using Causal Attribution to Mitigate Reward Hacking in Explanations", "authors": ["Pedro Ferreira", "Wilker Aziz", "Ivan Titov"], "categories": ["cs.CL"], "comment": "20 pages, 10 figures, 6 tables", "summary": "Chain-of-thought explanations are widely used to inspect the decision process\nof large language models (LLMs) and to evaluate the trustworthiness of model\noutputs, making them important for effective collaboration between LLMs and\nhumans. We demonstrate that preference optimization - a key step in the\nalignment phase - can inadvertently reduce the faithfulness of these\nexplanations. This occurs because the reward model (RM), which guides\nalignment, is tasked with optimizing both the expected quality of the response\nand the appropriateness of the explanations (e.g., minimizing bias or adhering\nto safety standards), creating potential conflicts. The RM lacks a mechanism to\nassess the consistency between the model's internal decision process and the\ngenerated explanation. Consequently, the LLM may engage in \"reward hacking\" by\nproducing a final response that scores highly while giving an explanation\ntailored to maximize reward rather than accurately reflecting its reasoning. To\naddress this issue, we propose enriching the RM's input with a causal\nattribution of the prediction, allowing the RM to detect discrepancies between\nthe generated self-explanation and the model's decision process. In controlled\nsettings, we show that this approach reduces the tendency of the LLM to\ngenerate misleading explanations.", "AI": {"tldr": "The paper investigates how preference optimization in large language models can compromise the trustworthiness of chain-of-thought explanations, leading to misleading outputs. It proposes a method to improve the reward model by enriching input with causal attribution to rectify this issue.", "motivation": "To enhance the collaboration between large language models and humans, it's crucial to ensure that explanations of model processes remain faithful and trustworthy, particularly during the alignment phase of model training.", "method": "The authors propose modifying the reward model (RM) input by integrating a causal attribution of the prediction, enabling the RM to detect inconsistencies between the model's reasoning and its self-generated explanations.", "result": "The proposed method effectively reduces the tendency of large language models to produce misleading chain-of-thought explanations in controlled tests.", "conclusion": "Enriching the reward model's input can significantly improve the fidelity of explanations generated by LLMs, making them more aligned with their internal decision processes.", "key_contributions": ["Identifying the conflict between reward optimization and explanation fidelity in LLMs.", "Proposing a method to enhance RM input with causal attributions for improved explanation accuracy.", "Demonstrating the effectiveness of the approach in controlled experimental settings."], "limitations": "", "keywords": ["large language models", "chain-of-thought explanations", "reward optimization", "explanation fidelity", "causal attribution"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.10157", "pdf": "https://arxiv.org/pdf/2504.10157.pdf", "abs": "https://arxiv.org/abs/2504.10157", "title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users", "authors": ["Xinnong Zhang", "Jiayu Lin", "Xinyi Mou", "Shiyue Yang", "Xiawei Liu", "Libo Sun", "Hanjia Lyu", "Yihang Yang", "Weihong Qi", "Yue Chen", "Guanying Li", "Ling Yan", "Yao Hu", "Siming Chen", "Yu Wang", "Xuanjing Huang", "Jiebo Luo", "Shiping Tang", "Libo Wu", "Baohua Zhou", "Zhongyu Wei"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Social simulation is transforming traditional social science research by\nmodeling human behavior through interactions between virtual individuals and\ntheir environments. With recent advances in large language models (LLMs), this\napproach has shown growing potential in capturing individual differences and\npredicting group behaviors. However, existing methods face alignment challenges\nrelated to the environment, target users, interaction mechanisms, and\nbehavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven\nworld model for social simulation. Our framework features four powerful\nalignment components and a user pool of 10 million real individuals. To\nvalidate its effectiveness, we conducted large-scale simulation experiments\nacross three distinct domains: politics, news, and economics. Results\ndemonstrate that SocioVerse can reflect large-scale population dynamics while\nensuring diversity, credibility, and representativeness through standardized\nprocedures and minimal manual adjustments.", "AI": {"tldr": "SocioVerse is an LLM-agent-driven world model designed for social simulation, capable of reflecting large-scale population dynamics with validated effectiveness across politics, news, and economics.", "motivation": "To address alignment challenges in social simulation involving human behavior modeling and improve the predictability of group behaviors using large language models.", "method": "Introduction of SocioVerse, which features four alignment components and utilizes a user pool of 10 million real individuals for large-scale simulation experiments in diverse domains.", "result": "SocioVerse successfully simulates population dynamics while ensuring diversity, credibility, and representativeness with minimal manual adjustments.", "conclusion": "The framework demonstrates the potential of LLMs in social simulation and can significantly enhance the understanding of human behavior in various environments.", "key_contributions": ["Development of the SocioVerse framework for social simulation using LLMs", "Implementation of four alignment components for improved accuracy", "Large-scale validation across three domains (politics, news, and economics)"], "limitations": "", "keywords": ["social simulation", "LLM", "human behavior", "population dynamics", "alignment components"], "importance_score": 8, "read_time_minutes": 10}}
