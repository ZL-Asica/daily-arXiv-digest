{"id": "2505.00821", "pdf": "https://arxiv.org/pdf/2505.00821.pdf", "abs": "https://arxiv.org/abs/2505.00821", "title": "Should AI Mimic People? Understanding AI-Supported Writing Technology Among Black Users", "authors": ["Jeffrey Basoah", "Jay L. Cunningham", "Erica Adams", "Alisha Bose", "Aditi Jain", "Kaustubh Yadav", "Zhengyang Yang", "Katharina Reinecke", "Daniela Rosner"], "categories": ["cs.HC"], "comment": null, "summary": "AI-supported writing technologies (AISWT) that provide grammatical\nsuggestions, autocomplete sentences, or generate and rewrite text are now a\nregular feature integrated into many people's workflows. However, little is\nknown about how people perceive the suggestions these tools provide. In this\npaper, we investigate how Black American users perceive AISWT, motivated by\nprior findings in natural language processing that highlight how the underlying\nlarge language models can contain racial biases. Using interviews and\nobservational user studies with 13 Black American users of AISWT, we found a\nstrong tradeoff between the perceived benefits of using AISWT to enhance their\nwriting style and feeling like \"it wasn't built for us\". Specifically,\nparticipants reported AISWT's failure to recognize commonly used names and\nexpressions in African American Vernacular English, experiencing its\ncorrections as hurtful and alienating and fearing it might further minoritize\ntheir culture. We end with a reflection on the tension between AISWT that fail\nto include Black American culture and language, and AISWT that attempt to mimic\nit, with attention to accuracy, authenticity, and the production of social\ndifference.", "AI": {"tldr": "This paper investigates how Black American users perceive AI-supported writing technologies, highlighting issues of racial bias and cultural misalignment.", "motivation": "To understand perceptions of AI-supported writing technologies among Black American users in light of racial biases present in language models.", "method": "Interviews and observational user studies conducted with 13 Black American users of AISWT.", "result": "Participants experienced a tradeoff between the benefits of AISWT for writing enhancement and feeling that these tools did not cater to their cultural language use.", "conclusion": "The findings reflect the tension between the failures of AISWT to recognize Black American culture and language and the attempts of these tools to authentically represent it.", "key_contributions": ["Explores user perceptions of AISWT among Black American users.", "Identifies cultural and linguistic biases in AISWT.", "Discusses the social implications of linguistic corrections in AISWT."], "limitations": "Study limited to 13 participants, which may not represent the wider Black American community.", "keywords": ["AI-supported writing technologies", "racial biases", "African American Vernacular English"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.00855", "pdf": "https://arxiv.org/pdf/2505.00855.pdf", "abs": "https://arxiv.org/abs/2505.00855", "title": "Beyond the Mirror: Personal Analytics through Visual Juxtaposition with Other People's Data", "authors": ["Sungbok Shin", "Sunghyo Chung", "Hyeon Jeon", "Hyunwook Lee", "Minje Choi", "Taehun Kim", "Jaehoon Choi", "Sungahn Ko", "Jaegul Choo"], "categories": ["cs.HC"], "comment": "Submitted to IEEE VIS2025 Short Paper", "summary": "An individual's data can reveal facets of behavior and identity, but its\ninterpretation is context dependent. We can easily identify various\nself-tracking applications that help people reflect on their lives. However,\nself-tracking confined to one person's data source may fall short in terms of\nobjectiveness, and insights coming from various perspectives. To address this,\nwe examine how those interpretations about a person's data can be augmented\nwhen the data are juxtaposed with that of others using anonymized online\ncalendar logs from a schedule management app. We develop CALTREND, a visual\nanalytics system that compares an individuals anonymized online schedule logs\nwith using those from other people. Using CALTREND as a probe, we conduct a\nstudy with two domain experts, one in information technology and one in Korean\nherbal medicine. We report our observations on how comparative views help\nenrich the characterization of an individual based on the experts' comments. We\nfind that juxtaposing personal data with others' can potentially lead to\ndiverse interpretations of one dataset shaped by domain-specific mental models.", "AI": {"tldr": "The paper presents CALTREND, a visual analytics system for comparing individual anonymized online schedule logs with those of others to enhance behavioral insights through comparative views.", "motivation": "To improve insights from self-tracking applications by juxtaposing individual data with that of others for broader interpretation.", "method": "Developed a visual analytics system named CALTREND that compares anonymized online schedule logs among individuals and conducted a study with domain experts.", "result": "The study revealed that comparative views allowed for enriched characterization of individuals based on domain-specific interpretations from the experts.", "conclusion": "Juxtaposing personal data with others can lead to varied interpretations influenced by domain-specific mental models, enhancing the understanding of individual behavior.", "key_contributions": ["Introduction of CALTREND, a visual analytics tool for comparative analysis of schedule logs.", "Demonstrated the value of comparative perspectives in self-tracking data interpretation.", "Provided insights from domain experts on individual behavior characterization."], "limitations": "Study based on a small sample of experts; findings may not generalize across all domains or populations.", "keywords": ["visual analytics", "self-tracking", "behavioral insights", "comparative analysis", "domain-specific interpretation"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2505.00879", "pdf": "https://arxiv.org/pdf/2505.00879.pdf", "abs": "https://arxiv.org/abs/2505.00879", "title": "Inattentional Blindness with Augmented Reality HUDS: An On-road Study", "authors": ["Nayara de Oliveira Faria", "Joseph L. Gabbard"], "categories": ["cs.HC"], "comment": null, "summary": "As the integration of augmented reality (AR) technology in head-up displays\n(HUDs) becomes more prevalent in vehicles, it is crucial to understand how to\ndesign and evaluate AR interfaces to ensure safety. With new AR displays\ncapable of rendering images with larger field of views and at varying depths,\nthe visual and cognitive separation between graphical and real-world visual\nstimuli will be increasingly more difficult to quantify as will drivers'\nability to efficiently allocate visual attention between the two sets of\nstimuli. In this study, we present a user study that serves as a crucial first\nstep in gaining insight into inattentional blindness while using AR in surface\ntransportation, where understanding is currently limited. Our primary goal is\nto investigate how the visual demand of AR tasks influences drivers' ability to\ndetect stimuli, and whether the nature of the stimuli itself plays a role in\nthis effect. To address these questions, we designed an on-road user study\naimed at producing a more realistic and ecologically valid understanding of the\nphenomenon.\n  Our results show that drivers' ability to timely detect stimuli in the\nenvironment decreased as the AR task visual demand increased demonstrated by\nboth detection performance and inattentional blindness metrics. Further,\ninattentional blindness caused by AR displays appears to be more prevalent\nwithin drivers' central field of view. We conclude by discussing implications\ntowards a safety-centric evaluation framework for AR HUDs.", "AI": {"tldr": "This study investigates the impact of augmented reality (AR) tasks on driver attention and stimulus detection in vehicles, highlighting the phenomenon of inattentional blindness induced by AR displays.", "motivation": "To understand the design and evaluation of AR interfaces in head-up displays (HUDs) for vehicle safety, particularly in relation to inattentional blindness during AR tasks.", "method": "Conducted an on-road user study to analyze how the visual demand of AR tasks affects drivers' detection of environmental stimuli.", "result": "Drivers' ability to detect stimuli decreased as the visual demand of AR tasks increased, with inattentional blindness being more pronounced in the central field of view.", "conclusion": "The study suggests a need for a safety-centric evaluation framework for AR HUDs, given the risks posed by inattentional blindness.", "key_contributions": ["Investigation of inattentional blindness in AR HUDs", "Empirical data on how visual demand affects driver attention", "Framework proposal for AR HUD safety evaluation"], "limitations": "", "keywords": ["Augmented Reality", "Inattentional Blindness", "Driver Attention", "Head-Up Displays", "User Study"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.00907", "pdf": "https://arxiv.org/pdf/2505.00907.pdf", "abs": "https://arxiv.org/abs/2505.00907", "title": "Co-Designing a Knowledge Graph Navigation Interface: A Participatory Approach", "authors": ["Stanislava Gardasevic", "Manika Lamba", "Jasmine S. Malone"], "categories": ["cs.HC", "cs.DL"], "comment": null, "summary": "Navigating and visualizing multilayered knowledge graphs remains a\nchallenging, unresolved problem in information systems design. Building on our\nearlier study, which engaged end users in both the design and population of a\ndomain-specific knowledge graph, we now focus on translating their insights\ninto actionable interface guidelines. In this paper, we synthesize\nrecommendations drawn from a participatory workshop with doctoral students. We\nthen demonstrate how these recommendations inform the design of a prototype\ninterface. Finally, we found that a participatory iterative design approach can\nhelp designers in decision making, leading to interfaces that are both\ninnovative and user-centric. By combining user-driven requirements with proven\nvisualization techniques, this paper presents a coherent framework for guiding\nfuture development of knowledge-graph navigation tools.", "AI": {"tldr": "This paper discusses the design of user-centric interfaces for multilayered knowledge graphs based on user insights from workshops.", "motivation": "The need for effective navigation and visualization of multilayered knowledge graphs in information systems design.", "method": "The authors conducted a participatory workshop with doctoral students to gather insights and developed interface guidelines from these findings.", "result": "The findings indicate that a participatory iterative design approach aids designers in creating innovative, user-centric interfaces for knowledge graphs.", "conclusion": "By merging user-driven requirements with visualization techniques, the paper provides a framework for enhancing the development of knowledge-graph navigation tools.", "key_contributions": ["User-driven interface guidelines for knowledge graph navigation.", "Demonstration of participatory iterative design's effectiveness in interface development.", "Framework for combining user insights with visualization techniques."], "limitations": "", "keywords": ["Knowledge Graphs", "User-Centric Design", "Participatory Design", "Visualization Techniques", "Interface Guidelines"], "importance_score": 6, "read_time_minutes": 20}}
{"id": "2505.00725", "pdf": "https://arxiv.org/pdf/2505.00725.pdf", "abs": "https://arxiv.org/abs/2505.00725", "title": "FinBERT-QA: Financial Question Answering with pre-trained BERT Language Models", "authors": ["Bithiah Yuan"], "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.7; I.5.1; H.3.3"], "comment": "Submitted in partial fulfillment of the requirements for the Master\n  of Science degree in Computer Science at the University of Freiburg, July 31,\n  2020", "summary": "Motivated by the emerging demand in the financial industry for the automatic\nanalysis of unstructured and structured data at scale, Question Answering (QA)\nsystems can provide lucrative and competitive advantages to companies by\nfacilitating the decision making of financial advisers. Consequently, we\npropose a novel financial QA system using the transformer-based pre-trained\nBERT language model to address the limitations of data scarcity and language\nspecificity in the financial domain. Our system focuses on financial\nnon-factoid answer selection, which retrieves a set of passage-level texts and\nselects the most relevant as the answer. To increase efficiency, we formulate\nthe answer selection task as a re-ranking problem, in which our system consists\nof an Answer Retriever using BM25, a simple information retrieval approach, to\nfirst return a list of candidate answers, and an Answer Re-ranker built with\nvariants of pre-trained BERT language models to re-rank and select the most\nrelevant answers. We investigate various learning, further pre-training, and\nfine-tuning approaches for BERT. Our experiments suggest that FinBERT-QA, a\nmodel built from applying the Transfer and Adapt further fine-tuning and\npointwise learning approach, is the most effective, improving the\nstate-of-the-art results of task 2 of the FiQA dataset by 16% on MRR, 17% on\nNDCG, and 21% on Precision@1.", "AI": {"tldr": "Proposal of a novel financial QA system using BERT to enhance decision-making for financial advisers by addressing data scarcity and language specificity.", "motivation": "Emerging demand in the financial industry for automatic analysis of unstructured and structured data at scale to facilitate decision making for financial advisers.", "method": "The QA system utilizes a transformer-based BERT model to perform financial non-factoid answer selection, framing this as a re-ranking problem involving an initial answer retrieval using BM25 followed by a BERT-based re-ranking for relevance.", "result": "FinBERT-QA, developed through further training and fine-tuning, achieved significant improvements on the FiQA dataset, with increases of 16% in MRR, 17% in NDCG, and 21% in Precision@1 compared to state-of-the-art results.", "conclusion": "The proposed model effectively enhances financial question answering by utilizing advancements in language modeling with BERT and tailored training strategies for better performance.", "key_contributions": ["Introduction of FinBERT-QA for financial QA tasks", "Application of transfer learning and fine-tuning approaches on BERT for improved results", "Demonstrated significant performance improvement over the existing state-of-the-art QA systems in finance."], "limitations": "", "keywords": ["financial QA", "BERT", "information retrieval", "machine learning", "question answering"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.00945", "pdf": "https://arxiv.org/pdf/2505.00945.pdf", "abs": "https://arxiv.org/abs/2505.00945", "title": "SSRLBot: Designing and Developing an LLM-based Agent using Socially Shared Regulated Learning", "authors": ["Xiaoshan Huang", "Jie Gao", "Haolun Wu"], "categories": ["cs.HC"], "comment": "8 pages, 2 figures", "summary": "Large language model (LLM)-based agents are increasingly used to support\nhuman experts by streamlining complex tasks and offering actionable insights.\nHowever, their application in multi-professional decision-making, particularly\nin teamwork contexts, remains underexplored. This design-based study addresses\nthat gap by developing LLM functions to enhance collaboration, grounded in the\nSocially Shared Regulation of Learning (SSRL) framework and applied to medical\ndiagnostic teamwork. SSRL emphasizes metacognitive, cognitive, motivational,\nand emotional processes in shared learning, focusing on how teams manage these\nprocesses to improve decision-making. This paper introduces SSRLBot, a\nprototype chatbot designed to help team members reflect on both their\ndiagnostic performance and key SSRL skills. Its core functions include\nsummarizing dialogues, analyzing SSRL behaviors, evaluating diagnostic\noutcomes, annotating SSRL markers in conversation, assessing their impact on\nperformance, and identifying interpersonal regulatory dynamics. We compare\nSSRLBot's capabilities with those of Gemini-1.5, GPT-3.5, and Deepseek-R1 in a\ncase study. SSRLBot demonstrates stronger alignment with SSRL theory, offering\ndetailed evaluations that link behaviors to regulatory dimensions and\nsuggesting improvements for collaboration. By integrating SSRL theory with LLM\ncapabilities, SSRLBot contributes a novel tool for enhancing team-based\ndecision-making and collaborative learning in high-stakes environments, such as\nmedical education.", "AI": {"tldr": "This paper presents SSRLBot, an LLM-based chatbot aimed at enhancing collaborative decision-making in medical diagnostic teamwork through the application of the Socially Shared Regulation of Learning (SSRL) framework.", "motivation": "To address the gap in utilizing LLMs for multi-professional decision-making, particularly in teamwork contexts, and to improve collaborative learning outcomes in medicine.", "method": "The study develops SSRLBot, a chatbot that analyzes team conversations using SSRL principles to enhance collaborative learning and decision-making. It includes functions for summarizing dialogues, assessing SSRL skills, and evaluating diagnostic outcomes.", "result": "SSRLBot was compared with established models like Gemini-1.5 and GPT-3.5, demonstrating superior alignment with SSRL theory and providing detailed evaluations that improve team collaboration.", "conclusion": "SSRLBot represents a significant advancement in enhancing team-based decision-making in high-stakes environments, such as medical education, by integrating SSRL principles with LLM capabilities.", "key_contributions": ["Development of SSRLBot as a novel tool for teamwork", "Integration of SSRL framework with LLM functionalities", "Enhanced understanding of team dynamics in medical diagnostics"], "limitations": "", "keywords": ["Large Language Models", "Human-Computer Interaction", "Collaborative Learning", "Medical Education", "Decision-Making"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.00753", "pdf": "https://arxiv.org/pdf/2505.00753.pdf", "abs": "https://arxiv.org/abs/2505.00753", "title": "A Survey on Large Language Model based Human-Agent Systems", "authors": ["Henry Peng Zou", "Wei-Chieh Huang", "Yaozu Wu", "Yankai Chen", "Chunyu Miao", "Hoang Nguyen", "Yue Zhou", "Weizhi Zhang", "Liancheng Fang", "Langzhou He", "Yangning Li", "Yuwei Cao", "Dongyuan Li", "Renhe Jiang", "Philip S. Yu"], "categories": ["cs.CL", "cs.LG"], "comment": "Paper lists and resources are available at\n  \\url{https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers}", "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin building fully autonomous agents. However, fully autonomous LLM-based agents\nstill face significant challenges, including limited reliability due to\nhallucinations, difficulty in handling complex tasks, and substantial safety\nand ethical risks, all of which limit their feasibility and trustworthiness in\nreal-world applications. To overcome these limitations, LLM-based human-agent\nsystems (LLM-HAS) incorporate human-provided information, feedback, or control\ninto the agent system to enhance system performance, reliability and safety.\nThis paper provides the first comprehensive and structured survey of LLM-HAS.\nIt clarifies fundamental concepts, systematically presents core components\nshaping these systems, including environment & profiling, human feedback,\ninteraction types, orchestration and communication, explores emerging\napplications, and discusses unique challenges and opportunities. By\nconsolidating current knowledge and offering a structured overview, we aim to\nfoster further research and innovation in this rapidly evolving\ninterdisciplinary field. Paper lists and resources are available at\nhttps://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers.", "AI": {"tldr": "This paper surveys LLM-based human-agent systems (LLM-HAS) to address challenges in fully autonomous LLM-based agents by incorporating human feedback for improved reliability and safety.", "motivation": "The motivation for this work is to enhance the performance, reliability, and safety of LLM-based agents, which face significant challenges in real-world applications due to issues like hallucinations and task complexity.", "method": "The paper presents a structured survey of LLM-HAS, clarifying fundamental concepts and systematically outlining core components such as environment profiling, human feedback, and communication methods.", "result": "The paper consolidates existing knowledge on LLM-HAS and reveals emerging applications, along with the unique challenges and opportunities these systems present.", "conclusion": "By providing a comprehensive overview of LLM-HAS, the paper aims to drive further research and innovation in the field.", "key_contributions": ["First comprehensive survey of LLM-based human-agent systems", "Clarifies core components shaping LLM-HAS", "Explores unique challenges and opportunities in the field"], "limitations": "", "keywords": ["LLM-based agents", "human-agent systems", "survey", "machine learning", "autonomous agents"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.00948", "pdf": "https://arxiv.org/pdf/2505.00948.pdf", "abs": "https://arxiv.org/abs/2505.00948", "title": "What Makes Teamwork Work? A Multimodal Case Study on Emotions and Diagnostic Expertise in an Intelligent Tutoring System", "authors": ["Xiaoshan Huang", "Haolun Wu", "Xue Liu", "Susanne P. Lajoie"], "categories": ["cs.HC"], "comment": "8 pages, 1 figure", "summary": "Teamwork is pivotal in medical teamwork when professionals with diverse\nskills and emotional states collaborate to make critical decisions. This case\nstudy examines the interplay between emotions and professional skills in group\ndecision-making during collaborative medical diagnosis within an Intelligent\nTutoring System (ITS). By comparing verbal and physiological data between\nhigh-performing and low-performing teams of medical professionals working on a\npatient case within the ITS, alongside individuals' retrospective collaboration\nexperiences, we employ multimodal data analysis to identify patterns in team\nemotional climate and their impact on diagnostic efficiency. Specifically, we\ninvestigate how emotion-driven dialogue and professional expertise influence\nboth the information-seeking process and the final diagnostic decisions.\nGrounded in the socially shared regulation of learning framework and utilizing\nsentiment analysis, we found that social-motivational interactions are key\ndrivers of a positive team emotional climate. Furthermore, through content\nanalysis of dialogue and physiological signals to pinpoint emotional\nfluctuations, we identify episodes where knowledge exchange and skill\nacquisition are most likely to occur. Our findings offer valuable insights into\noptimizing group collaboration in medical contexts by harmonizing emotional\ndynamics with adaptive strategies for effective decision-making, ultimately\nenhancing diagnostic accuracy and teamwork effectiveness.", "AI": {"tldr": "This study analyzes the role of emotions and professional skills in medical teamwork through case studies within an Intelligent Tutoring System, focusing on their impact on diagnostic efficiency and team dynamics.", "motivation": "Understanding the interplay between emotions and skills in medical decision-making is critical for improving group collaboration and diagnostic accuracy.", "method": "The study employs multimodal data analysis comparing verbal and physiological data from high-performing and low-performing medical teams, along with retrospective experiences.", "result": "Key findings reveal that social-motivational interactions significantly enhance the team emotional climate, which in turn positively affects diagnostic accuracy and team effectiveness.", "conclusion": "Optimizing group collaboration in medical contexts can be achieved by focusing on emotional dynamics and adaptive strategies, improving decision-making and diagnostic outcomes.", "key_contributions": ["Identification of emotional climate patterns in medical teams.", "Analysis of how emotion-driven dialogue impacts diagnostic efficiency.", "Insights into harmonizing emotional dynamics with decision-making strategies."], "limitations": "", "keywords": ["medical teamwork", "decision-making", "emotional dynamics", "Intelligent Tutoring System", "multimodal data analysis"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.00776", "pdf": "https://arxiv.org/pdf/2505.00776.pdf", "abs": "https://arxiv.org/abs/2505.00776", "title": "Reasoning Capabilities and Invariability of Large Language Models", "authors": ["Alessandro Raganato", "Rafael Peñaloza", "Marco Viviani", "Gabriella Pasi"], "categories": ["cs.CL"], "comment": "Accepted for publication in the Proceedings of the 23rd IEEE/WIC\n  International Conference on Web Intelligence and Intelligent Agent Technology\n  (WI-IAT 2024)", "summary": "Large Language Models (LLMs) have shown remarkable capabilities in\nmanipulating natural language across multiple applications, but their ability\nto handle simple reasoning tasks is often questioned. In this work, we aim to\nprovide a comprehensive analysis of LLMs' reasoning competence, specifically\nfocusing on their prompt dependency. In particular, we introduce a new\nbenchmark dataset with a series of simple reasoning questions demanding shallow\nlogical reasoning. Aligned with cognitive psychology standards, the questions\nare confined to a basic domain revolving around geometric figures, ensuring\nthat responses are independent of any pre-existing intuition about the world\nand rely solely on deduction. An empirical analysis involving zero-shot and\nfew-shot prompting across 24 LLMs of different sizes reveals that, while LLMs\nwith over 70 billion parameters perform better in the zero-shot setting, there\nis still a large room for improvement. An additional test with chain-of-thought\nprompting over 22 LLMs shows that this additional prompt can aid or damage the\nperformance of models, depending on whether the rationale is required before or\nafter the answer.", "AI": {"tldr": "This paper analyzes the reasoning capabilities of Large Language Models (LLMs) using a new benchmark of simple geometric reasoning tasks to assess prompt dependency and performance variations across sizes of models.", "motivation": "To evaluate the reasoning competence of LLMs and investigate their prompt dependency through a structured benchmark.", "method": "Introduces a new benchmark dataset with simple reasoning questions based on geometric figures; conducts empirical tests across 24 LLMs using zero-shot and few-shot prompting.", "result": "LLMs over 70 billion parameters show better performance in zero-shot settings, yet significant improvement potential exists; chain-of-thought prompting affects model performance variably.", "conclusion": "Further research and model improvement are necessary to enhance reasoning capabilities, particularly in the context of appropriate prompting strategies.", "key_contributions": ["Development of a benchmark dataset for LLM reasoning analysis", "Empirical analysis of reasoning across varying LLM sizes", "Insights into the effects of chain-of-thought prompting on model performance"], "limitations": "The reasoning tasks are limited to a specific domain (geometric figures) which may not fully represent broader reasoning capabilities.", "keywords": ["Large Language Models", "Reasoning", "Prompt Dependency", "Benchmark Dataset", "Geometric Figures"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.00956", "pdf": "https://arxiv.org/pdf/2505.00956.pdf", "abs": "https://arxiv.org/abs/2505.00956", "title": "Audio Personas: Augmenting Social Perception via Body-Anchored Audio Cues", "authors": ["Yujie Tao", "Libby Ye", "Jeremy N. Bailenson", "Sean Follmer"], "categories": ["cs.HC"], "comment": null, "summary": "We introduce Audio Personas, enabling users to \"decorate\" themselves with\nbody-anchored sounds in audio augmented reality. Like outfits, makeup, and\nfragrances, audio personas offer an alternative yet dynamic channel to augment\nface-to-face interactions. For instance, one can set their audio persona as\nrain sounds to reflect a bad mood, bee sounds to establish personal boundaries,\nor a playful \"woosh\" sound to mimic passing by someone like a breeze. To\ninstantiate the concept, we implemented a headphone-based prototype with\nmulti-user tracking and audio streaming. Our formative study with designers\nrevealed that audio personas were preferred in public and semi-public-private\nspaces for managing social impressions (e.g., personality) and signaling\ncurrent states (e.g., emotions). Our preregistered in-lab study with 64\nparticipants showed that audio personas influenced how participants formed\nimpressions. Individuals with positive audio personas were rated as more\nsocially attractive, more likable, and less threatening than those with\nnegative audio personas.", "AI": {"tldr": "This paper introduces Audio Personas, a method for users to express themselves through sound in augmented reality, impacting social perceptions in face-to-face interactions.", "motivation": "The motivation behind Audio Personas is to enhance interpersonal communication by allowing individuals to use sound to reflect their emotions and personality, similar to how they use clothing and other personal items for self-expression.", "method": "The authors implemented a headphone-based prototype that allows for multi-user tracking and audio streaming, followed by formative studies and a preregistered in-lab study with 64 participants.", "result": "The study found that audio personas significantly influenced social impressions, with positive audio personas leading to higher ratings of social attractiveness, likability, and lower perceptions of threat compared to negative personas.", "conclusion": "The findings highlight the potential of Audio Personas in audio augmented reality to enrich social interactions by allowing users to communicate their emotions and personality through sound.", "key_contributions": ["Introduction of the concept of Audio Personas for self-expression in social settings.", "Development of a headphone-based prototype facilitating multi-user sound interactions.", "Demonstration through studies that audio personas can significantly affect social impression formation."], "limitations": "Potential limitations include the specificity of the study sample and environmental factors that may affect audio perception in real-world settings.", "keywords": ["Audio Personas", "Augmented Reality", "Social Impressions", "Human-Computer Interaction", "Emotion Expression"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.00814", "pdf": "https://arxiv.org/pdf/2505.00814.pdf", "abs": "https://arxiv.org/abs/2505.00814", "title": "Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction", "authors": ["Mario Sänger", "Ulf Leser"], "categories": ["cs.CL"], "comment": null, "summary": "Automatic relationship extraction (RE) from biomedical literature is critical\nfor managing the vast amount of scientific knowledge produced each year. In\nrecent years, utilizing pre-trained language models (PLMs) has become the\nprevalent approach in RE. Several studies report improved performance when\nincorporating additional context information while fine-tuning PLMs for RE.\nHowever, variations in the PLMs applied, the databases used for augmentation,\nhyper-parameter optimization, and evaluation methods complicate direct\ncomparisons between studies and raise questions about the generalizability of\nthese findings. Our study addresses this research gap by evaluating PLMs\nenhanced with contextual information on five datasets spanning four relation\nscenarios within a consistent evaluation framework. We evaluate three baseline\nPLMs and first conduct extensive hyperparameter optimization. After selecting\nthe top-performing model, we enhance it with additional data, including textual\nentity descriptions, relational information from knowledge graphs, and\nmolecular structure encodings. Our findings illustrate the importance of i) the\nchoice of the underlying language model and ii) a comprehensive hyperparameter\noptimization for achieving strong extraction performance. Although inclusion of\ncontext information yield only minor overall improvements, an ablation study\nreveals substantial benefits for smaller PLMs when such external data was\nincluded during fine-tuning.", "AI": {"tldr": "The paper evaluates the effectiveness of pre-trained language models (PLMs) enhanced with contextual information for automatic relationship extraction (RE) in biomedical literature.", "motivation": "Automatic relationship extraction from biomedical literature is essential for managing scientific knowledge, but variations in models and methodologies complicate direct comparisons of prior studies.", "method": "The study evaluates three baseline PLMs across five datasets while applying hyperparameter optimization and augmenting the models with additional contextual data such as textual entity descriptions and relational information.", "result": "The results demonstrate that while the choice of language model and hyperparameter optimization is crucial, adding context information provides only minor improvements overall but significant benefits for smaller PLMs.", "conclusion": "The study concludes that effective model selection and hyperparameter tuning are key to improving RE performance, particularly for smaller PLMs when enhanced with contextual data.", "key_contributions": ["Evaluation of PLMs with a uniform framework across diverse datasets", "Insights on the impact of hyperparameter tuning on model performance", "Demonstration of context data benefits for smaller PLMs in RE tasks"], "limitations": "The improvement from context information was minor in general and may not translate to all contexts or models.", "keywords": ["relationship extraction", "biomedical literature", "pre-trained language models", "hyperparameter optimization", "contextual information"], "importance_score": 6, "read_time_minutes": 12}}
{"id": "2505.00987", "pdf": "https://arxiv.org/pdf/2505.00987.pdf", "abs": "https://arxiv.org/abs/2505.00987", "title": "Destructive Interference: Encoding Loss in the Overlap", "authors": ["Nik Aberle"], "categories": ["cs.HC"], "comment": null, "summary": "Destructive Interference is a data visualization installation that\nrepresenting the deaths and injuries caused by mass shootings in 2024 in the\nUnited States. I parametrically designed and fabricated an interlocking ring\nsculpture for each month of 2024; where the overall height corresponds to the\nlevel of violence in that month. Taller forms mark the deadliest months, while\nshorter ones reflect fewer casualties. Each inner ring encodes the number of\npeople killed or injured, and each outer ring encodes the number of shootings\nand the number of days without them. The interlocking cylinders are powered via\na motor to rotate, and lit from within. As the cylinders rotate, they cast\noverlapping shadows that represent those killed or injured by mass shootings.\nThe goal of this work is to visualize otherwise overwhelming and disparate\nstatistics in a way that is both physically present and emotionally resonant.\nBy inviting viewers to step into and engage with these shadows, the piece\ncreates space for reflection, conversation, and confrontation with the scale of\nthis ongoing crisis.", "AI": {"tldr": "Destructive Interference is a data visualization installation that represents mass shooting casualties in 2024 through an interactive sculpture involving interlocking rings that encode violence statistics.", "motivation": "The work aims to visualize the overwhelming statistics of mass shootings in a manner that is physically engaging and emotionally impactful.", "method": "The installation features a series of interlocking ring sculptures, one for each month of 2024, designed to reflect the level of violence through varying heights and encoded data on casualties and shootings.", "result": "The resulting sculpture physically portrays the complexity of mass shooting data, with rotating cylinders that cast overlapping shadows symbolizing the deaths and injuries.", "conclusion": "This installation invites reflection and discussion about the ongoing crisis of gun violence by making the statistics tangible and visually compelling.", "key_contributions": ["Parametric design of interactive sculptures for data visualization.", "Physical representation of complex statistics related to mass shootings.", "Engagement of viewers through immersive visual and physical experience."], "limitations": "", "keywords": ["data visualization", "mass shootings", "interactive installation", "sculpture", "statistics"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2505.00931", "pdf": "https://arxiv.org/pdf/2505.00931.pdf", "abs": "https://arxiv.org/abs/2505.00931", "title": "Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy in English Language Learner Writing", "authors": ["Timur Jaganov", "John Blake", "Julián Villegas", "Nicholas Carr"], "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 8 Figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "This study investigates the potential for Large Language Models (LLMs) to\nscale-up Dynamic Assessment (DA). To facilitate such an investigation, we first\ndeveloped DynaWrite-a modular, microservices-based grammatical tutoring\napplication which supports multiple LLMs to generate dynamic feedback to\nlearners of English. Initial testing of 21 LLMs, revealed GPT-4o and neural\nchat to have the most potential to scale-up DA in the language learning\nclassroom. Further testing of these two candidates found both models performed\nsimilarly in their ability to accurately identify grammatical errors in user\nsentences. However, GPT-4o consistently outperformed neural chat in the quality\nof its DA by generating clear, consistent, and progressively explicit hints.\nReal-time responsiveness and system stability were also confirmed through\ndetailed performance testing, with GPT-4o exhibiting sufficient speed and\nstability. This study shows that LLMs can be used to scale-up dynamic\nassessment and thus enable dynamic assessment to be delivered to larger groups\nthan possible in traditional teacher-learner settings.", "AI": {"tldr": "Study explores LLMs for scaling Dynamic Assessment in language learning using a modular app.", "motivation": "To investigate how LLMs can improve Dynamic Assessment (DA) in language education.", "method": "Developed DynaWrite, a microservices-based app, testing 21 LLMs with focus on GPT-4o and neural chat.", "result": "GPT-4o and neural chat showed similar error identification; however, GPT-4o provided higher quality dynamic assessment.", "conclusion": "LLMs have the potential to scale up dynamic assessment, allowing it to be applied to larger groups beyond traditional classroom settings.", "key_contributions": ["Development of DynaWrite for grammatical tutoring", "Performance comparison of 21 LLMs for DA", "Demonstration of LLMs enhancing scalability in education"], "limitations": "", "keywords": ["Large Language Models", "Dynamic Assessment", "Grammatical Tutoring", "DynaWrite", "Language Learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.01000", "pdf": "https://arxiv.org/pdf/2505.01000.pdf", "abs": "https://arxiv.org/abs/2505.01000", "title": "Togedule: Scheduling Meetings with Large Language Models and Adaptive Representations of Group Availability", "authors": ["Jaeyoon Song", "Zahra Ashktorab", "Thomas W. Malone"], "categories": ["cs.HC"], "comment": "This paper has been accepted at CSCW 2025", "summary": "Scheduling is a perennial-and often challenging-problem for many groups.\nExisting tools are mostly static, showing an identical set of choices to\neveryone, regardless of the current status of attendees' inputs and\npreferences. In this paper, we propose Togedule, an adaptive scheduling tool\nthat uses large language models to dynamically adjust the pool of choices and\ntheir presentation format. With the initial prototype, we conducted a formative\nstudy (N=10) and identified the potential benefits and risks of such an\nadaptive scheduling tool. Then, after enhancing the system, we conducted two\ncontrolled experiments, one each for attendees and organizers (total N=66). For\neach experiment, we compared scheduling with verbal messages, shared calendars,\nor Togedule. Results show that Togedule significantly reduces the cognitive\nload of attendees indicating their availability and improves the speed and\nquality of the decisions made by organizers.", "AI": {"tldr": "Togedule is an adaptive scheduling tool that leverages large language models to tailor scheduling options based on user inputs, enhancing decision-making for both attendees and organizers.", "motivation": "Existing scheduling tools are static and do not account for attendees' preferences and inputs, leading to inefficient decision-making.", "method": "We developed Togedule, an adaptive scheduling tool, and conducted a formative study followed by two controlled experiments to assess its impact on scheduling efficiency.", "result": "Togedule significantly reduces cognitive load for attendees and enhances the quality and speed of decisions made by organizers in comparison to traditional scheduling methods.", "conclusion": "Togedule provides a responsive alternative to static scheduling tools, improving the overall scheduling experience for users.", "key_contributions": ["Introduction of an adaptive scheduling tool using large language models", "Empirical validation through controlled experiments", "Reduced cognitive load and improved decision-making efficiency"], "limitations": "The study involved a limited sample size, which may impact the generalizability of the findings.", "keywords": ["adaptive scheduling", "large language models", "user preferences", "cognitive load", "decision making"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.00949", "pdf": "https://arxiv.org/pdf/2505.00949.pdf", "abs": "https://arxiv.org/abs/2505.00949", "title": "Llama-Nemotron: Efficient Reasoning Models", "authors": ["Akhiad Bercovich", "Itay Levy", "Izik Golan", "Mohammad Dabbah", "Ran El-Yaniv", "Omri Puny", "Ido Galil", "Zach Moshe", "Tomer Ronen", "Najeeb Nabwani", "Ido Shahaf", "Oren Tropp", "Ehud Karpas", "Ran Zilberstein", "Jiaqi Zeng", "Soumye Singhal", "Alexander Bukharin", "Yian Zhang", "Tugrul Konuk", "Gerald Shen", "Ameya Sunil Mahabaleshwarkar", "Bilal Kartal", "Yoshi Suhara", "Olivier Delalleau", "Zijia Chen", "Zhilin Wang", "David Mosallanezhad", "Adi Renduchintala", "Haifeng Qian", "Dima Rekesh", "Fei Jia", "Somshubra Majumdar", "Vahid Noroozi", "Wasi Uddin Ahmad", "Sean Narenthiran", "Aleksander Ficek", "Mehrzad Samadi", "Jocelyn Huang", "Siddhartha Jain", "Igor Gitman", "Ivan Moshkov", "Wei Du", "Shubham Toshniwal", "George Armstrong", "Branislav Kisacanin", "Matvei Novikov", "Daria Gitman", "Evelina Bakhturina", "Jane Polak Scowcroft", "John Kamalu", "Dan Su", "Kezhi Kong", "Markus Kliegl", "Rabeeh Karimi", "Ying Lin", "Sanjeev Satheesh", "Jupinder Parmar", "Pritam Gundecha", "Brandon Norick", "Joseph Jennings", "Shrimai Prabhumoye", "Syeda Nahida Akter", "Mostofa Patwary", "Abhinav Khattar", "Deepak Narayanan", "Roger Waleffe", "Jimmy Zhang", "Bor-Yiing Su", "Guyue Huang", "Terry Kong", "Parth Chadha", "Sahil Jain", "Christine Harvey", "Elad Segal", "Jining Huang", "Sergey Kashirsky", "Robert McQueen", "Izzy Putterman", "George Lam", "Arun Venkatesan", "Sherry Wu", "Vinh Nguyen", "Manoj Kilaru", "Andrew Wang", "Anna Warno", "Abhilash Somasamudramath", "Sandip Bhaskar", "Maka Dong", "Nave Assaf", "Shahar Mor", "Omer Ullman Argov", "Scot Junkin", "Oleksandr Romanenko", "Pedro Larroy", "Monika Katariya", "Marco Rovinelli", "Viji Balas", "Nicholas Edelman", "Anahita Bhiwandiwalla", "Muthu Subramaniam", "Smita Ithape", "Karthik Ramamoorthy", "Yuting Wu", "Suguna Varshini Velury", "Omri Almog", "Joyjit Daw", "Denys Fridman", "Erick Galinkin", "Michael Evans", "Katherine Luna", "Leon Derczynski", "Nikki Pope", "Eileen Long", "Seth Schneider", "Guillermo Siman", "Tomasz Grzegorzek", "Pablo Ribalta", "Monika Katariya", "Joey Conway", "Trisha Saar", "Ann Guan", "Krzysztof Pawelec", "Shyamala Prayaga", "Oleksii Kuchaiev", "Boris Ginsburg", "Oluwatobi Olabiyi", "Kari Briski", "Jonathan Cohen", "Bryan Catanzaro", "Jonah Alben", "Yonatan Geifman", "Eric Chung"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce the Llama-Nemotron series of models, an open family of\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\ninference efficiency, and an open license for enterprise use. The family comes\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\noffering superior inference throughput and memory efficiency. In this report,\nwe discuss the training procedure for these models, which entails using neural\narchitecture search from Llama 3 models for accelerated inference, knowledge\ndistillation, and continued pretraining, followed by a reasoning-focused\npost-training stage consisting of two main parts: supervised fine-tuning and\nlarge scale reinforcement learning. Llama-Nemotron models are the first\nopen-source models to support a dynamic reasoning toggle, allowing users to\nswitch between standard chat and reasoning modes during inference. To further\nsupport open research and facilitate model development, we provide the\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\nOpen Model License Agreement. 2. We release the complete post-training dataset:\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM.", "AI": {"tldr": "Introduction of the Llama-Nemotron series of models enhancing reasoning capabilities with an open license.", "motivation": "To provide an open-source family of models that excel in reasoning while maintaining high inference efficiency and memory usage for enterprise applications.", "method": "Employs neural architecture search for accelerated inference, knowledge distillation, and reinforcement learning in a post-training phase focused on reasoning.", "result": "Llama-Nemotron models outperform state-of-the-art reasoning models with superior throughput and memory efficiency while allowing dynamic mode switching during inference.", "conclusion": "The introduction of Llama-Nemotron models fosters open research and development in reasoning-focused AI applications.", "key_contributions": ["Dynamic reasoning toggle for user flexibility between chat and reasoning modes", "Release of Llama-Nemotron models under NVIDIA Open Model License", "Provision of comprehensive post-training dataset and training codebases"], "limitations": "", "keywords": ["Llama-Nemotron", "Reasoning models", "Open-source AI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.01030", "pdf": "https://arxiv.org/pdf/2505.01030.pdf", "abs": "https://arxiv.org/abs/2505.01030", "title": "Barriers to Employment: The Deaf Multimedia Authoring Tax", "authors": ["C. Vogler", "A. Glasser", "R. Kushalnagar", "M. Seita", "M. Arroyo Chavez", "K. Delk", "P. DeVries", "M. Feanny", "B. Thompson", "J. Waller"], "categories": ["cs.HC"], "comment": "5 pages", "summary": "This paper describes the challenges that deaf and hard of hearing people face\nwith creating accessible multimedia content, such as portfolios, instructional\nvideos and video presentations. Unlike content consumption, the process of\ncontent creation itself remains highly inaccessible, creating barriers to\nemployment in all stages of recruiting, hiring, and carrying out assigned job\nduties. Overcoming these barriers incurs a \"deaf content creation tax\" that\ntranslates into requiring significant additional time and resources to produce\ncontent equivalent to what a non-disabled person would produce. We highlight\nthis process and associated challenges through real-world examples experienced\nby the authors, and provide guidance and recommendations for addressing them.", "AI": {"tldr": "The paper discusses the challenges faced by deaf and hard of hearing individuals in creating accessible multimedia content, highlighting the barriers in the content creation process and offering recommendations for improvement.", "motivation": "To address the accessibility challenges that deaf and hard of hearing individuals encounter when creating multimedia content, which affects their employment opportunities.", "method": "The authors provide real-world examples of the challenges faced by deaf creators and propose guidance and recommendations for making the content creation process more accessible.", "result": "The paper highlights the concept of a 'deaf content creation tax,' demonstrating the additional time and resources required to produce content that is equivalent to that of non-disabled individuals.", "conclusion": "Improving accessibility in content creation for the deaf and hard of hearing can reduce barriers to employment and improve their ability to create multimedia content.", "key_contributions": ["Identification of barriers in the process of multimedia content creation for deaf individuals.", "Introduction of the 'deaf content creation tax' concept.", "Recommendations for making content creation more accessible."], "limitations": "", "keywords": ["accessibility", "deaf", "hard of hearing", "multimedia content creation", "employment"], "importance_score": 3, "read_time_minutes": 5}}
{"id": "2505.00977", "pdf": "https://arxiv.org/pdf/2505.00977.pdf", "abs": "https://arxiv.org/abs/2505.00977", "title": "A Character-based Diffusion Embedding Algorithm for Enhancing the Generation Quality of Generative Linguistic Steganographic Texts", "authors": ["Yingquan Chen", "Qianmu Li", "Xiaocong Wu", "Huifeng Li", "Qing Chang"], "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Generating high-quality steganographic text is a fundamental challenge in the\nfield of generative linguistic steganography. This challenge arises primarily\nfrom two aspects: firstly, the capabilities of existing models in text\ngeneration are limited; secondly, embedding algorithms fail to effectively\nmitigate the negative impacts of sensitive information's properties, such as\nsemantic content or randomness. Specifically, to ensure that the recipient can\naccurately extract hidden information, embedding algorithms often have to\nconsider selecting candidate words with relatively low probabilities. This\nphenomenon leads to a decrease in the number of high-probability candidate\nwords and an increase in low-probability candidate words, thereby compromising\nthe semantic coherence and logical fluency of the steganographic text and\ndiminishing the overall quality of the generated steganographic material. To\naddress this issue, this paper proposes a novel embedding algorithm,\ncharacter-based diffusion embedding algorithm (CDEA). Unlike existing embedding\nalgorithms that strive to eliminate the impact of sensitive information's\nproperties on the generation process, CDEA leverages sensitive information's\nproperties. It enhances the selection frequency of high-probability candidate\nwords in the candidate pool based on general statistical properties at the\ncharacter level and grouping methods based on power-law distributions, while\nreducing the selection frequency of low-probability candidate words in the\ncandidate pool. Furthermore, to ensure the effective transformation of\nsensitive information in long sequences, we also introduce the XLNet model.\nExperimental results demonstrate that the combination of CDEA and XLNet\nsignificantly improves the quality of generated steganographic text,\nparticularly in terms of perceptual-imperceptibility.", "AI": {"tldr": "This paper introduces a novel embedding algorithm called the character-based diffusion embedding algorithm (CDEA) to improve the quality of generated steganographic text by effectively leveraging sensitive information properties and utilizing XLNet for better processing of long sequences.", "motivation": "The challenge of generating high-quality steganographic text is due to the limitations of existing models and embedding algorithms in effectively handling sensitive information's properties.", "method": "The paper proposes CDEA, which enhances the selection frequency of high-probability candidate words while reducing low-probability candidates using statistical properties at the character level and power-law distribution methods. The XLNet model is also utilized for better transformation of sensitive information.", "result": "Experimental results show that the CDEA combined with XLNet significantly enhances the quality of steganographic text generation, especially in perceptual-imperceptibility aspects.", "conclusion": "CDEA provides a more effective approach to generating high-quality steganographic text by smartly leveraging the properties of sensitive information.", "key_contributions": ["Introduction of the character-based diffusion embedding algorithm (CDEA) for steganographic text generation.", "Demonstration of improved quality in generated text through the combination of CDEA and XLNet.", "Utilization of power-law distribution methods in candidate word selection."], "limitations": "", "keywords": ["steganography", "text generation", "embedding algorithms", "XLNet", "character-based diffusion"], "importance_score": 4, "read_time_minutes": 20}}
{"id": "2505.01351", "pdf": "https://arxiv.org/pdf/2505.01351.pdf", "abs": "https://arxiv.org/abs/2505.01351", "title": "Closing the Loop: A Systematic Review of Experience-Driven Game Adaptation", "authors": ["Phil Lopes", "Nuno Fachada", "Maria Fonseca"], "categories": ["cs.HC"], "comment": null, "summary": "Adaptive game systems aim to enrich player experiences by dynamically\nadjusting game content in response to user data. While extensive research has\naddressed content personalization and player experience modeling, the\nintegration of these components into fully operational adaptive gameplay\nsystems remains limited. This systematic review, conducted in accordance with\nPRISMA guidelines, analyzes 17 empirical studies published between January 2015\nand May 2024, identifying and analyzing approaches that implement the complete\nexperience-driven loop -- including player sensing, modeling, and content\nadaptation. Game telemetry remains the most prevalent sensing modality,\nalthough other non-invasive methods suitable for affective modeling -- such as\nfacial expression analysis (FEA) and peripheral interaction data -- remain\nunderutilized despite their potential for real-time emotional inference.\nKnowledge-based methods, such as rule-based systems and heuristics, dominate\nmodeling and adaptation due to their interpretability and low resource demands,\nwhereas machine learning approaches face challenges related to data\navailability and transparency. Despite their relevance to immersive and\ntherapeutic experiences, affective states such as stress and anxiety remain\nlargely ignored, as systems continue to favor performance over\nemotion-sensitive adaptation. These findings highlight a crucial research\ndirection: advancing emotionally responsive game systems that move beyond\nperformance optimization by incorporating underutilized sensing modalities --\nsuch as FEA and peripheral interaction -- to enable real-time affect-driven\npersonalization. Advancing in this direction holds strong potential to increase\nimmersion, personalize gameplay, and support affect regulation across\nentertainment and therapeutic contexts.", "AI": {"tldr": "This review analyzes adaptive game systems, focusing on integrating player sensing, modeling, and content adaptation for enhanced player experiences.", "motivation": "To address the limitations in integrating content personalization and player experience modeling into operational adaptive gameplay systems.", "method": "A systematic review of 17 empirical studies following PRISMA guidelines, examining the player sensing modalities, modeling methods, and their application in adaptive gameplay.", "result": "Identified the prevalence of game telemetry for sensing, the dominance of knowledge-based methods for modeling, and noted the neglect of emotionally adaptive systems despite their potential.", "conclusion": "Advancing emotionally responsive game systems through underutilized sensing modalities can increase immersion and support emotional regulation in gameplay.", "key_contributions": ["Systematic review of adaptive game systems", "Highlighting underutilized sensing modalities for emotional inference", "Identifying knowledge-based methods as dominant in modeling and adaptation"], "limitations": "Focus primarily on performance over emotional adaptation; affective states like stress and anxiety largely ignored.", "keywords": ["adaptive systems", "game experience", "player sensing", "emotional inference", "machine learning"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2505.00979", "pdf": "https://arxiv.org/pdf/2505.00979.pdf", "abs": "https://arxiv.org/abs/2505.00979", "title": "Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models", "authors": ["Xuhui Jiang", "Shengjie Ma", "Chengjin Xu", "Cehao Yang", "Liyu Zhang", "Jian Guo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success but remain\ndata-inefficient, especially when learning from small, specialized corpora with\nlimited and proprietary data. Existing synthetic data generation methods for\ncontinue pre-training focus on intra-document content and overlook\ncross-document knowledge associations, limiting content diversity and depth. We\npropose Synthetic-on-Graph (SoG), a synthetic data generation framework that\nincorporates cross-document knowledge associations for efficient corpus\nexpansion. SoG constructs a context graph by extracting entities and concepts\nfrom the original corpus, representing cross-document associations, and\nemploying a graph walk strategy for knowledge-associated sampling. This\nenhances synthetic data diversity and coherence, enabling models to learn\ncomplex knowledge structures and handle rare knowledge. To further improve\nsynthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive\nClarifying (CC) synthetic, enhancing reasoning processes and discriminative\npower. Experiments show that SoG outperforms the state-of-the-art (SOTA) method\nin a multi-hop document Q&A dataset while performing comparably to the SOTA\nmethod on the reading comprehension task datasets, which also underscores the\nbetter generalization capability of SoG. Our work advances synthetic data\ngeneration and provides practical solutions for efficient knowledge acquisition\nin LLMs, especially in domains with limited data availability.", "AI": {"tldr": "The paper presents Synthetic-on-Graph (SoG), a framework for generating synthetic data by leveraging cross-document knowledge associations, improving the efficiency and diversity of learning in Large Language Models (LLMs).", "motivation": "To address the data inefficiency in LLMs, particularly when working with small, specialized corpora containing limited data.", "method": "SoG constructs a context graph from the original corpus, capturing cross-document associations and utilizing a graph walk strategy for knowledge-associated sampling. It also integrates Chain-of-Thought (CoT) and Contrastive Clarifying (CC) techniques to enhance reasoning and clarity.", "result": "SoG demonstrates superior performance over state-of-the-art methods in a multi-hop document Q&A dataset and comparable results on reading comprehension tasks, indicating improved generalization capabilities.", "conclusion": "The proposed SoG framework enhances synthetic data generation and provides effective solutions for knowledge acquisition in LLMs, particularly in data-scarce domains.", "key_contributions": ["Introduction of Synthetic-on-Graph framework for synthetic data generation.", "Use of cross-document knowledge associations for improved data efficiency.", "Demonstrated superior performance in specific LLM tasks compared to SOTA methods."], "limitations": "", "keywords": ["Large Language Models", "synthetic data generation", "cross-document associations", "knowledge acquisition", "context graph"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.01413", "pdf": "https://arxiv.org/pdf/2505.01413.pdf", "abs": "https://arxiv.org/abs/2505.01413", "title": "Group Gaze-Sharing with Projection Displays", "authors": ["Maurice Koch", "Tobias Rau", "Vladimir Mikheev", "Seyda Öney", "Michael Becher", "Xiangyu Wang", "Nelusa Pathmanathan", "Patrick Gralka", "Daniel Weiskopf", "Kuno Kurzhals"], "categories": ["cs.HC"], "comment": "2025 Symposium on Eye Tracking Research and Applications (ETRA '25)", "summary": "The eyes play an important role in human collaboration. Mutual and shared\ngaze help communicate visual attention to each other or to a specific object of\ninterest. Shared gaze was typically investigated for pair collaborations in\nremote settings and with people in virtual and augmented reality. With our\nwork, we expand this line of research by a new technique to communicate gaze\nbetween groups in tabletop workshop scenarios. To achieve this communication,\nwe use an approach based on projection mapping to unify gaze data from multiple\nparticipants into a common visualization space on a tabletop. We showcase our\napproach with a collaborative puzzle-solving task that displays shared visual\nattention on individual pieces and provides hints to solve the problem at hand.", "AI": {"tldr": "This paper presents a technique for communicating gaze between groups in tabletop collaborative settings using projection mapping.", "motivation": "To improve understanding and interaction in group collaborations by enhancing the communication of visual attention through shared gaze.", "method": "The authors use projection mapping to unify gaze data from multiple participants and display it in a common visualization space on a tabletop.", "result": "The technique was showcased through a collaborative puzzle-solving task, which visualized shared attention on puzzle pieces and provided hints for solving the task.", "conclusion": "The approach successfully lets groups communicate their visual attention more effectively, enhancing collaboration dynamics.", "key_contributions": ["Development of a projection mapping technique for gaze communication in group settings", "Application of the technique in a tabletop collaborative puzzle-solving task", "Improved visualization of shared gaze to enhance group interaction"], "limitations": "The technique may be limited by the accuracy of gaze detection and the physical arrangement of participants around the table.", "keywords": ["gaze communication", "group collaboration", "projection mapping", "tabletop interaction", "eye tracking"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.00985", "pdf": "https://arxiv.org/pdf/2505.00985.pdf", "abs": "https://arxiv.org/abs/2505.00985", "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling", "authors": ["Ayan Sengupta", "Yash Goel", "Tanmoy Chakraborty"], "categories": ["cs.CL"], "comment": null, "summary": "We challenge the dominant focus on neural scaling laws and advocate for a\nparadigm shift toward downscaling in the development of large language models\n(LLMs). While scaling laws have provided critical insights into performance\nimprovements through increasing model and dataset size, we emphasize the\nsignificant limitations of this approach, particularly in terms of\ncomputational inefficiency, environmental impact, and deployment constraints.\nTo address these challenges, we propose a holistic framework for downscaling\nLLMs that seeks to maintain performance while drastically reducing resource\ndemands. This paper outlines practical strategies for transitioning away from\ntraditional scaling paradigms, advocating for a more sustainable, efficient,\nand accessible approach to LLM development.", "AI": {"tldr": "This paper advocates for downscaling large language models (LLMs) instead of focusing solely on scaling laws, proposing a framework to maintain performance while reducing resource demands.", "motivation": "The paper challenges the prevalent reliance on neural scaling laws, highlighting the computational inefficiency and environmental impact of traditional scaling methods in LLM development.", "method": "A holistic framework for downscaling LLMs is proposed, incorporating practical strategies to enhance efficiency and sustainability.", "result": "Downscaling strategies can maintain LLM performance while significantly reducing resource usage and making deployment more accessible.", "conclusion": "The shift from scaling to downscaling can lead to a more sustainable approach in LLM development, addressing crucial environmental and efficiency concerns.", "key_contributions": ["Proposes a paradigm shift from scaling to downscaling in LLM development.", "Outlines practical strategies for efficient LLM deployment.", "Highlights the limitations of traditional scaling laws and their impacts."], "limitations": "May not comprehensively address all performance aspects of LLMs compared to scaling.", "keywords": ["downscaling", "large language models", "sustainability", "computational efficiency", "deployment"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.00989", "pdf": "https://arxiv.org/pdf/2505.00989.pdf", "abs": "https://arxiv.org/abs/2505.00989", "title": "VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel Traffic Services through Natural Language", "authors": ["Sijin Sun", "Liangbin Zhao", "Ming Deng", "Xiuju Fu"], "categories": ["cs.CL"], "comment": "8 pages, 5 figures, 7 tablels, submitted to ITSC2025", "summary": "Vessel Traffic Services (VTS) are essential for maritime safety and\nregulatory compliance through real-time traffic management. However, with\nincreasing traffic complexity and the prevalence of heterogeneous, multimodal\ndata, existing VTS systems face limitations in spatiotemporal reasoning and\nintuitive human interaction. In this work, we propose VTS-LLM Agent, the first\ndomain-adaptive large LLM agent tailored for interactive decision support in\nVTS operations. We formalize risk-prone vessel identification as a\nknowledge-augmented Text-to-SQL task, combining structured vessel databases\nwith external maritime knowledge. To support this, we construct a curated\nbenchmark dataset consisting of a custom schema, domain-specific corpus, and a\nquery-SQL test set in multiple linguistic styles. Our framework incorporates\nNER-based relational reasoning, agent-based domain knowledge injection,\nsemantic algebra intermediate representation, and query rethink mechanisms to\nenhance domain grounding and context-aware understanding. Experimental results\nshow that VTS-LLM outperforms both general-purpose and SQL-focused baselines\nunder command-style, operational-style, and formal natural language queries,\nrespectively. Moreover, our analysis provides the first empirical evidence that\nlinguistic style variation introduces systematic performance challenges in\nText-to-SQL modeling. This work lays the foundation for natural language\ninterfaces in vessel traffic services and opens new opportunities for\nproactive, LLM-driven maritime real-time traffic management.", "AI": {"tldr": "This paper introduces VTS-LLM Agent, a domain-adaptive large LLM designed to improve decision support in Vessel Traffic Services (VTS) by integrating multimodal data and enhancing human interaction through advanced natural language processing.", "motivation": "The paper addresses the challenges of spatiotemporal reasoning and human interaction in existing VTS systems, prompted by increasing traffic complexity and varied data sources.", "method": "The authors propose a knowledge-augmented Text-to-SQL approach for risk-prone vessel identification, utilizing a curated benchmark dataset that includes a custom schema and diverse query styles. The framework enhances understanding through NER-based reasoning and agent knowledge injection.", "result": "VTS-LLM outperforms general-purpose and SQL-specific models in various query styles, demonstrating improved decision support capabilities.", "conclusion": "The study provides foundational insights for developing natural language interfaces in VTS, highlighting the impact of linguistic styles on performance in Text-to-SQL tasks and exploiting LLMs for enhanced maritime traffic management.", "key_contributions": ["Introduction of the VTS-LLM Agent specifically for maritime traffic management.", "Development of a benchmark dataset tailored for the VTS domain.", "First empirical evidence that linguistic style variation affects Text-to-SQL performance."], "limitations": "", "keywords": ["Vessel Traffic Services", "large LLM", "Text-to-SQL", "natural language processing", "maritime safety"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.01006", "pdf": "https://arxiv.org/pdf/2505.01006.pdf", "abs": "https://arxiv.org/abs/2505.01006", "title": "Token-free Models for Sarcasm Detection", "authors": ["Sumit Mamtani", "Maitreya Sonawane", "Kanika Agarwal", "Nishanth Sanjeev"], "categories": ["cs.CL"], "comment": null, "summary": "Tokenization is a foundational step in most natural language processing (NLP)\npipelines, yet it introduces challenges such as vocabulary mismatch and\nout-of-vocabulary issues. Recent work has shown that models operating directly\non raw text at the byte or character level can mitigate these limitations. In\nthis paper, we evaluate two token-free models, ByT5 and CANINE, on the task of\nsarcasm detection in both social media (Twitter) and non-social media (news\nheadlines) domains. We fine-tune and benchmark these models against token-based\nbaselines and state-of-the-art approaches. Our results show that ByT5-small and\nCANINE outperform token-based counterparts and achieve new state-of-the-art\nperformance, improving accuracy by 0.77% and 0.49% on the News Headlines and\nTwitter Sarcasm datasets, respectively. These findings underscore the potential\nof token-free models for robust NLP in noisy and informal domains such as\nsocial media.", "AI": {"tldr": "This paper evaluates token-free models, ByT5 and CANINE, for sarcasm detection, demonstrating their superiority over token-based models.", "motivation": "Tokenization in NLP introduces challenges like vocabulary mismatch, leading to the exploration of token-free modeling alternatives.", "method": "Two token-free models, ByT5 and CANINE, are fine-tuned and benchmarked on sarcasm detection tasks across social media and news headlines, compared against token-based baselines.", "result": "ByT5-small and CANINE achieve state-of-the-art performance, improving accuracy by 0.77% and 0.49% on the News Headlines and Twitter Sarcasm datasets, respectively.", "conclusion": "The study highlights the effectiveness of token-free models in handling tasks in noisy and informal contexts like social media.", "key_contributions": ["Evaluation of token-free models for sarcasm detection", "Demonstration of improved performance over token-based models", "Insights into the effectiveness of models on noisy data"], "limitations": "", "keywords": ["Tokenization", "Natural Language Processing", "Sarcasm Detection", "Token-Free Models", "ByT5", "CANINE"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.01015", "pdf": "https://arxiv.org/pdf/2505.01015.pdf", "abs": "https://arxiv.org/abs/2505.01015", "title": "Value Portrait: Understanding Values of LLMs with Human-aligned Benchmark", "authors": ["Jongwook Han", "Dongmin Choi", "Woojung Song", "Eun-Ju Lee", "Yohan Jo"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "32 pages, 7 figures", "summary": "The importance of benchmarks for assessing the values of language models has\nbeen pronounced due to the growing need of more authentic, human-aligned\nresponses. However, existing benchmarks rely on human or machine annotations\nthat are vulnerable to value-related biases. Furthermore, the tested scenarios\noften diverge from real-world contexts in which models are commonly used to\ngenerate text and express values. To address these issues, we propose the Value\nPortrait benchmark, a reliable framework for evaluating LLMs' value\norientations with two key characteristics. First, the benchmark consists of\nitems that capture real-life user-LLM interactions, enhancing the relevance of\nassessment results to real-world LLM usage and thus ecological validity.\nSecond, each item is rated by human subjects based on its similarity to their\nown thoughts, and correlations between these ratings and the subjects' actual\nvalue scores are derived. This psychometrically validated approach ensures that\nitems strongly correlated with specific values serve as reliable items for\nassessing those values. Through evaluating 27 LLMs with our benchmark, we find\nthat these models prioritize Benevolence, Security, and Self-Direction values\nwhile placing less emphasis on Tradition, Power, and Achievement values. Also,\nour analysis reveals biases in how LLMs perceive various demographic groups,\ndeviating from real human data.", "AI": {"tldr": "Introduction of the Value Portrait benchmark for assessing LLMs' value orientations based on real-life user interactions.", "motivation": "To create a benchmark that addresses biases in current evaluations of language models and enhances ecological validity by reflecting real-world use cases.", "method": "The benchmark comprises items rated by human subjects based on similarity to their values, allowing for correlations between user ratings and actual value scores.", "result": "Evaluation of 27 LLMs revealed a priority for Benevolence, Security, and Self-Direction values, with less focus on Tradition, Power, and Achievement, highlighting biases in LLM perceptions of demographic groups.", "conclusion": "The Value Portrait benchmark provides a psychometrically validated framework for assessing LLMs, revealing their value orientations and biases in perspective.", "key_contributions": ["Introduction of a new benchmark for evaluating LLMs' values", "Focus on real-life user interactions to improve ecological validity", "Psychometric validation of assessment items based on human ratings"], "limitations": "Limited to the tested LLMs and may not generalize across all language models or contexts.", "keywords": ["Language Models", "Benchmark", "Human-Computer Interaction", "Value Orientation", "Bias"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2505.01035", "pdf": "https://arxiv.org/pdf/2505.01035.pdf", "abs": "https://arxiv.org/abs/2505.01035", "title": "Do We Need a Detailed Rubric for Automated Essay Scoring using Large Language Models?", "authors": ["Lui Yoshida"], "categories": ["cs.CL"], "comment": "Accepted in AIED 2025. This preprint has not undergone any\n  post-submission improvements or corrections", "summary": "This study investigates the necessity and impact of a detailed rubric in\nautomated essay scoring (AES) using large language models (LLMs). While using\nrubrics are standard in LLM-based AES, creating detailed rubrics requires\nsubstantial ef-fort and increases token usage. We examined how different levels\nof rubric detail affect scoring accuracy across multiple LLMs using the TOEFL11\ndataset. Our experiments compared three conditions: a full rubric, a simplified\nrubric, and no rubric, using four different LLMs (Claude 3.5 Haiku, Gemini 1.5\nFlash, GPT-4o-mini, and Llama 3 70B Instruct). Results showed that three out of\nfour models maintained similar scoring accuracy with the simplified rubric\ncompared to the detailed one, while significantly reducing token usage.\nHowever, one model (Gemini 1.5 Flash) showed decreased performance with more\ndetailed rubrics. The findings suggest that simplified rubrics may be\nsufficient for most LLM-based AES applications, offering a more efficient\nalternative without compromis-ing scoring accuracy. However, model-specific\nevaluation remains crucial as per-formance patterns vary across different LLMs.", "AI": {"tldr": "This study evaluates the impact of rubric detail on the accuracy of automated essay scoring using large language models.", "motivation": "To investigate the necessity of detailed rubrics in automated essay scoring (AES) and their effect on scoring accuracy.", "method": "Examined different levels of rubric detail (full, simplified, none) across four large language models using the TOEFL11 dataset.", "result": "Three out of four models showed similar scoring accuracy with a simplified rubric, reducing token usage; one model performed worse with detailed rubrics.", "conclusion": "Simplified rubrics may suffice for most LLM-based AES applications, but model-specific evaluations are crucial due to varying performance.", "key_contributions": ["Demonstrated that simplified rubrics can maintain scoring accuracy in LLM-based AES", "Identified the token usage efficiency of simplified rubrics", "Highlighted the performance variability across different LLMs with rubric detail levels"], "limitations": "Results could vary with other datasets or different LLMs not tested in this study.", "keywords": ["automated essay scoring", "large language models", "rubric detail", "scoring accuracy", "TOEFL11 dataset"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2402.08978", "pdf": "https://arxiv.org/pdf/2402.08978.pdf", "abs": "https://arxiv.org/abs/2402.08978", "title": "Prismatic: Interactive Multi-View Cluster Analysis of Concept Stocks", "authors": ["Wong Kam-Kwai", "Yan Luo", "Xuanwu Yue", "Wei Chen", "Huamin Qu"], "categories": ["cs.HC", "cs.CE", "cs.LG"], "comment": "14 pages. A preprint version accepted to IEEE Transactions on\n  Visualization and Computer Graphics (TVCG), 2025", "summary": "Financial cluster analysis allows investors to discover investment\nalternatives and avoid undertaking excessive risks. However, this analytical\ntask faces substantial challenges arising from many pairwise comparisons, the\ndynamic correlations across time spans, and the ambiguity in deriving\nimplications from business relational knowledge. We propose Prismatic, a visual\nanalytics system that integrates quantitative analysis of historical\nperformance and qualitative analysis of business relational knowledge to\ncluster correlated businesses interactively. Prismatic features three\nclustering processes: dynamic cluster generation, knowledge-based cluster\nexploration, and correlation-based cluster validation. Utilizing a multi-view\nclustering approach, it enriches data-driven clusters with knowledge-driven\nsimilarity, providing a nuanced understanding of business correlations. Through\nwell-coordinated visual views, Prismatic facilitates a comprehensive\ninterpretation of intertwined quantitative and qualitative features,\ndemonstrating its usefulness and effectiveness via case studies on formulating\nconcept stocks and extensive interviews with domain experts.", "AI": {"tldr": "Prismatic is a visual analytics system designed for financial cluster analysis, combining quantitative performance and qualitative business knowledge.", "motivation": "The need for better tools to simplify the analytical task of financial cluster analysis due to challenges like pairwise comparisons and dynamic correlations.", "method": "Prismatic integrates quantitative historical data analysis with qualitative insights from business relationships, employing three processes: dynamic cluster generation, knowledge-based cluster exploration, and correlation-based cluster validation.", "result": "The system demonstrates effective clustering of correlated businesses, supported by case studies on concept stocks and expert interviews.", "conclusion": "Prismatic enhances the understanding of business correlations through a multi-view approach, combining data-driven and knowledge-driven insights.", "key_contributions": ["Introduces a novel visual analytics system for financial clustering.", "Integrates quantitative and qualitative analyses seamlessly.", "Provides a multi-view approach for enhanced interpretation of business correlations."], "limitations": "", "keywords": ["financial cluster analysis", "visual analytics", "business correlations"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2505.01068", "pdf": "https://arxiv.org/pdf/2505.01068.pdf", "abs": "https://arxiv.org/abs/2505.01068", "title": "Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs", "authors": ["Yijie Jin", "Junjie Peng", "Xuanchao Lin", "Haochen Yuan", "Lan Wang", "Cangzhi Zheng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal Sentiment Analysis (MSA) is a rapidly developing field that\nintegrates multimodal information to recognize sentiments, and existing models\nhave made significant progress in this area. The central challenge in MSA is\nmultimodal fusion, which is predominantly addressed by Multimodal Transformers\n(MulTs). Although act as the paradigm, MulTs suffer from efficiency concerns.\nIn this work, from the perspective of efficiency optimization, we propose and\nprove that MulTs are hierarchical modal-wise heterogeneous graphs (HMHGs), and\nwe introduce the graph-structured representation pattern of MulTs. Based on\nthis pattern, we propose an Interlaced Mask (IM) mechanism to design the\nGraph-Structured and Interlaced-Masked Multimodal Transformer (GsiT). It is\nformally equivalent to MulTs which achieves an efficient weight-sharing\nmechanism without information disorder through IM, enabling All-Modal-In-One\nfusion with only 1/3 of the parameters of pure MulTs. A Triton kernel called\nDecomposition is implemented to ensure avoiding additional computational\noverhead. Moreover, it achieves significantly higher performance than\ntraditional MulTs. To further validate the effectiveness of GsiT itself and the\nHMHG concept, we integrate them into multiple state-of-the-art models and\ndemonstrate notable performance improvements and parameter reduction on widely\nused MSA datasets.", "AI": {"tldr": "This paper introduces an efficient multimodal transformer model, GsiT, utilizing a novel Interlaced Mask mechanism to optimize multimodal sentiment analysis by reducing parameters while improving performance over traditional models.", "motivation": "To address efficiency concerns in existing multimodal transformers used for sentiment analysis.", "method": "The authors propose the Graph-Structured and Interlaced-Masked Multimodal Transformer (GsiT), leveraging a hierarchical modal-wise heterogeneous graph representation and an Interlaced Mask mechanism for optimized parameter sharing.", "result": "GsiT achieves significant performance improvements over traditional multimodal transformers with only one-third of the parameters and has been validated across multiple MSA datasets.", "conclusion": "The proposed GsiT model demonstrates that hierarchical modal-wise graphs can enhance efficiency in multimodal sentiment analysis without compromising performance.", "key_contributions": ["Introduction of the Interlaced Mask mechanism for multimodal transformers.", "Proposition of GsiT as a significant improvement over traditional MulTs in terms of efficiency and performance.", "Integration of GsiT into existing state-of-the-art models showing notable enhancements."], "limitations": "", "keywords": ["Multimodal Sentiment Analysis", "Multimodal Transformers", "Efficiency Optimization"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2411.11382", "pdf": "https://arxiv.org/pdf/2411.11382.pdf", "abs": "https://arxiv.org/abs/2411.11382", "title": "Quantifying Haptic Affection of Car Door through Data-Driven Analysis of Force Profile", "authors": ["Mudassir Ibrahim Awan", "Ahsan Raza", "Waseem Hassan", "Ki-Uk Kyung", "Seokhee Jeon"], "categories": ["cs.HC"], "comment": "12 pages, 9 figures, 3 tables. Mudassir Ibrahim Awan and Ahsan Raza\n  are equally contributing authors", "summary": "Haptic affection plays a crucial role in user experience, particularly in the\nautomotive industry where the tactile quality of components can influence\ncustomer satisfaction. This study aims to accurately predict the affective\nproperty of a car door by only watching the force or torque profile of it when\nopening. To this end, a deep learning model is designed to capture the\nunderlying relationships between force profiles and user-defined adjective\nratings, providing insights into the door-opening experience. The dataset\nemployed in this research includes force profiles and user adjective ratings\ncollected from six distinct car models, reflecting a diverse set of\ndoor-opening characteristics and tactile feedback. The model's performance is\nassessed using Leave-One-Out Cross-Validation, a method that measures its\ngeneralization capability on unseen data. The results demonstrate that the\nproposed model achieves a high level of prediction accuracy, indicating its\npotential in various applications related to haptic affection and design\noptimization in the automotive industry.", "AI": {"tldr": "This study explores how to predict the affective qualities of car doors using deep learning models based on force profiles during opening actions.", "motivation": "The research addresses the significance of haptic affection in enhancing user experience and customer satisfaction in the automotive sector.", "method": "A deep learning model analyzes force and torque profiles of car doors, correlating them with user-defined adjective ratings to predict tactile feedback.", "result": "The model demonstrates high accuracy in predicting affective properties from the force profiles, suggesting its efficiency in application for haptic affection and design optimization.", "conclusion": "The findings indicate that the model can significantly contribute to design processes in automotive development by enhancing user experience related to tactile interaction.", "key_contributions": ["Development of a deep learning model for predicting haptic affection", "Use of force profile data correlated with user adjective ratings", "Demonstration of high prediction accuracy in a diverse dataset"], "limitations": "", "keywords": ["Haptic Affection", "Deep Learning", "Automotive Industry", "User Experience", "Force Profiles"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2505.01110", "pdf": "https://arxiv.org/pdf/2505.01110.pdf", "abs": "https://arxiv.org/abs/2505.01110", "title": "MateICL: Mitigating Attention Dispersion in Large-Scale In-Context Learning", "authors": ["Murtadha Ahmed", "Wenbo", "Liu yunfeng"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nIn-Context Learning (ICL). However, the fixed position length constraints in\npre-trained models limit the number of demonstration examples. Recent efforts\nto extend context suffer from attention dispersion as the number of\ndemonstrations increases. In this paper, we introduce Mitigating Attention\nDispersion in large-scale ICL (MateICL) that enables LLMs to maintain effective\nself-attention as the context size grows. We first split the context into\nmultiple windows, each filled to the model's context capacity, which are\nprocessed separately. Then, we introduce an additional layer to recalibrate the\nattention weights, prioritizing the query tokens as the number of\ndemonstrations increases. Our empirical results show that MateICL can\neffectively leverage larger contexts to improve ICL performance. Compared to\nretrieval-based baselines, MateICL consistently achieves better performance\nwithout requiring an externally trained retrieval model. Despite recent\nadvances in inference strategies (e.g., 32k token contexts), our results\ndemonstrate that MateICL remains beneficial in computationally\nresource-constrained settings. The code is publicly available at\nhttps://github.com/amurtadha/MateICL.", "AI": {"tldr": "Introducing MateICL, a method to improve in-context learning by managing attention dispersion in large language models.", "motivation": "Improving the effectiveness of in-context learning in large language models by addressing the limitations of fixed position length constraints and attention dispersion.", "method": "The proposed method splits the context into multiple windows processed separately and adds a layer to recalibrate attention weights, focusing on query tokens as the number of demonstrations increases.", "result": "MateICL demonstrates improved in-context learning performance compared to retrieval-based models without requiring an external retrieval system, even in resource-constrained settings.", "conclusion": "MateICL effectively supports larger contexts while maintaining attention efficiency, enhancing in-context learning capabilities of LLMs.", "key_contributions": ["Introduction of MateICL to mitigate attention dispersion in LLMs", "Ability to effectively utilize larger contexts for ICL performance", "Performance improvement without reliance on external retrieval models"], "limitations": "", "keywords": ["Large Language Models", "In-Context Learning", "Attention Mechanism"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.19877", "pdf": "https://arxiv.org/pdf/2502.19877.pdf", "abs": "https://arxiv.org/abs/2502.19877", "title": "Towards Multimodal Large-Language Models for Parent-Child Interaction: A Focus on Joint Attention", "authors": ["Weiyan Shi", "Viet Hai Le", "Kenny Tsu Wei Choo"], "categories": ["cs.HC"], "comment": "Accepted at CHI 2025 Late Breaking Work", "summary": "Joint attention is a critical component of early speech-language development\nand a key indicator of effective parent-child interaction. However, research on\ndetecting and analysing joint attention remains limited, particularly for\nMultimodal Large Language Models (MLLMs). This study evaluates MLLMs' ability\nto comprehend joint attention by analysing 26 parent-child interaction videos\nannotated by two speech-language pathologists. These annotations identify\nstrong and poor joint attention segments, serving as benchmarks for evaluating\nthe models' interpretive capabilities. Our findings reveal that current MLLMs\nstruggle to accurately interpret joint attention due to a lack of nuanced\nunderstanding of child-initiated eye contact, a crucial component of joint\nattention dynamics. This study highlights the importance of incorporating\ndetailed eye contact to enhance MLLMs' multimodal reasoning. Addressing these\ngaps is essential for future research to advance the use of MLLMs in analysing\nand supporting parent-child interactions.", "AI": {"tldr": "This study assesses the performance of Multimodal Large Language Models (MLLMs) in understanding joint attention during parent-child interactions.", "motivation": "To evaluate the capabilities of MLLMs in analyzing joint attention, which is vital for early speech-language development and effective parent-child communication.", "method": "Analyzed 26 annotated parent-child interaction videos using insights from speech-language pathologists, focusing on segments with strong and poor joint attention.", "result": "Current MLLMs showed difficulty in accurately interpreting joint attention, particularly regarding child-initiated eye contact.", "conclusion": "Improving MLLMs' understanding of nuanced eye contact is crucial for advancing their application in parent-child interaction analysis.", "key_contributions": ["First comprehensive evaluation of MLLMs on joint attention detection.", "Identification of specific shortcomings in MLLM comprehension related to eye contact.", "Recommendations for enhancing MLLM capabilities in multimodal reasoning relevant to caregiver interactions."], "limitations": "Limited sample size of annotated videos; Findings may not generalize across broader contexts.", "keywords": ["joint attention", "Multimodal Large Language Models", "parent-child interaction", "speech-language development", "multimodal reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.01162", "pdf": "https://arxiv.org/pdf/2505.01162.pdf", "abs": "https://arxiv.org/abs/2505.01162", "title": "On the Limitations of Steering in Language Model Alignment", "authors": ["Chebrolu Niranjan", "Kokil Jaidka", "Gerard Christopher Yeo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Steering vectors are a promising approach to aligning language model behavior\nat inference time. In this paper, we propose a framework to assess the\nlimitations of steering vectors as alignment mechanisms. Using a framework of\ntransformer hook interventions and antonym-based function vectors, we evaluate\nthe role of prompt structure and context complexity in steering effectiveness.\nOur findings indicate that steering vectors are promising for specific\nalignment tasks, such as value alignment, but may not provide a robust\nfoundation for general-purpose alignment in LLMs, particularly in complex\nscenarios. We establish a methodological foundation for future investigations\ninto steering capabilities of reasoning models.", "AI": {"tldr": "This paper evaluates the limitations of steering vectors in aligning language model behavior at inference, highlighting their effectiveness for specific tasks but questioning their general-purpose applicability.", "motivation": "To assess the limitations of steering vectors as methods for aligning language model behavior during inference.", "method": "The study utilizes transformer hook interventions and antonym-based function vectors to evaluate how prompt structure and context complexity affect steering effectiveness.", "result": "The findings suggest that while steering vectors are effective for certain alignment tasks like value alignment, they lack robustness for general-purpose alignment in complex scenarios.", "conclusion": "The paper establishes a methodological framework aimed at guiding future research into the steering capabilities of reasoning models.", "key_contributions": ["Evaluation of steering vectors for alignment in language models", "Methodological framework for assessing alignment mechanisms", "Insights into the limitations of steering vectors in complex scenarios"], "limitations": "Limited applicability of steering vectors for general-purpose alignment in complex scenarios.", "keywords": ["steering vectors", "alignment mechanisms", "language models", "transformer interventions", "context complexity"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.20331", "pdf": "https://arxiv.org/pdf/2503.20331.pdf", "abs": "https://arxiv.org/abs/2503.20331", "title": "WiCross: Indoor Human Zone-Crossing Detection Using Commodity WiFi Devices", "authors": ["Weiyan Shi", "Xuanzhi Wang", "Kai Niu", "Leye Wang", "Daqing Zhang"], "categories": ["cs.HC"], "comment": "Accepted at UbiComp/ISWC 2023 Poster", "summary": "Detecting whether a target crosses the given zone (e.g., a door) can enable\nvarious practical applications in smart homes, including intelligent security\nand people counting. The traditional infrared-based approach only covers a line\nand can be easily cracked. In contrast, reusing the ubiquitous WiFi devices\ndeployed in homes has the potential to cover a larger area of interest as WiFi\nsignals are scattered throughout the entire space. By detecting the walking\ndirection (i.e., approaching and moving away) with WiFi signal strength change,\nexisting work can identify the behavior of crossing between WiFi transceiver\npair. However, this method mistakenly classifies the turn-back behavior as\ncrossing behavior, resulting in a high false alarm rate. In this paper, we\npropose WiCross, which can accurately distinguish the turn-back behavior with\nthe phase statistics pattern of WiFi signals and thus robustly identify whether\nthe target crosses the area between the WiFi transceiver pair. We implement\nWiCross with commercial WiFi devices and extensive experiments demonstrate that\nWiCross can achieve an accuracy higher than 95\\% with a false alarm rate of\nless than 5%.", "AI": {"tldr": "WiCross is a novel approach using WiFi signals to accurately detect zone crossings by distinguishing between crossing and turn-back behaviors, achieving over 95% accuracy.", "motivation": "The need for accurate zone crossing detection in smart homes for applications such as security and people counting, addressing limitations of current infrared methods.", "method": "WiCross analyzes the phase statistics pattern of WiFi signals to differentiate between crossing and turn-back behaviors, utilizing commercial WiFi devices.", "result": "WiCross achieved over 95% accuracy in detecting zone crossings with a false alarm rate of less than 5% through extensive experimentation.", "conclusion": "WiCross offers a robust solution for zone crossing detection in smart home environments by leveraging existing WiFi infrastructure.", "key_contributions": ["Proposes a new method for detecting zone crossings using WiFi signals.", "Successfully distinguishes between crossing and turn-back behaviors, reducing false alarms.", "Demonstrates high accuracy and low false alarm rates in real-world experiments."], "limitations": "", "keywords": ["WiFi", "zone crossing detection", "smart homes", "turn-back behavior", "phase statistics"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2505.01198", "pdf": "https://arxiv.org/pdf/2505.01198.pdf", "abs": "https://arxiv.org/abs/2505.01198", "title": "Gender Bias in Explainability: Investigating Performance Disparity in Post-hoc Methods", "authors": ["Mahdi Dhaini", "Ege Erdogan", "Nils Feldhus", "Gjergji Kasneci"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACM Conference on Fairness, Accountability, and\n  Transparency (FAccT) 2025", "summary": "While research on applications and evaluations of explanation methods\ncontinues to expand, fairness of the explanation methods concerning disparities\nin their performance across subgroups remains an often overlooked aspect. In\nthis paper, we address this gap by showing that, across three tasks and five\nlanguage models, widely used post-hoc feature attribution methods exhibit\nsignificant gender disparity with respect to their faithfulness, robustness,\nand complexity. These disparities persist even when the models are pre-trained\nor fine-tuned on particularly unbiased datasets, indicating that the\ndisparities we observe are not merely consequences of biased training data. Our\nresults highlight the importance of addressing disparities in explanations when\ndeveloping and applying explainability methods, as these can lead to biased\noutcomes against certain subgroups, with particularly critical implications in\nhigh-stakes contexts. Furthermore, our findings underscore the importance of\nincorporating the fairness of explanations, alongside overall model fairness\nand explainability, as a requirement in regulatory frameworks.", "AI": {"tldr": "This paper addresses the gender disparity in the performance of explanation methods used in machine learning, highlighting issues in faithfulness, robustness, and complexity across multiple tasks and language models.", "motivation": "To address the overlooked aspect of fairness in explanation methods, especially concerning performance discrepancies across different gender subgroups.", "method": "The authors evaluate widely used post-hoc feature attribution methods across three tasks and five language models, assessing their faithfulness, robustness, and complexity.", "result": "The study finds significant gender disparities in the explanation methods, which persist regardless of the models being trained on unbiased datasets.", "conclusion": "The findings suggest that fairness in explanations is critical to avoid biased outcomes in high-stakes contexts and should be part of regulatory frameworks alongside overall model fairness.", "key_contributions": ["Identification of gender disparity in post-hoc feature attribution methods", "Establishing importance of addressing explanation disparities in machine learning models", "Recommendations for incorporating fairness of explanations into regulatory frameworks."], "limitations": "The study focuses only on gender disparities without exploring other demographic factors.", "keywords": ["explainability", "fairness", "machine learning", "gender disparities", "feature attribution"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.01238", "pdf": "https://arxiv.org/pdf/2505.01238.pdf", "abs": "https://arxiv.org/abs/2505.01238", "title": "EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods on NLP Models", "authors": ["Mahdi Dhaini", "Kafaite Zahra Hussain", "Efstratios Zaradoukas", "Gjergji Kasneci"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to the xAI World Conference (2025) - System Demonstration", "summary": "As Natural Language Processing (NLP) models continue to evolve and become\nintegral to high-stakes applications, ensuring their interpretability remains a\ncritical challenge. Given the growing variety of explainability methods and\ndiverse stakeholder requirements, frameworks that help stakeholders select\nappropriate explanations tailored to their specific use cases are increasingly\nimportant. To address this need, we introduce EvalxNLP, a Python framework for\nbenchmarking state-of-the-art feature attribution methods for transformer-based\nNLP models. EvalxNLP integrates eight widely recognized explainability\ntechniques from the Explainable AI (XAI) literature, enabling users to generate\nand evaluate explanations based on key properties such as faithfulness,\nplausibility, and complexity. Our framework also provides interactive,\nLLM-based textual explanations, facilitating user understanding of the\ngenerated explanations and evaluation outcomes. Human evaluation results\nindicate high user satisfaction with EvalxNLP, suggesting it is a promising\nframework for benchmarking explanation methods across diverse user groups. By\noffering a user-friendly and extensible platform, EvalxNLP aims at\ndemocratizing explainability tools and supporting the systematic comparison and\nadvancement of XAI techniques in NLP.", "AI": {"tldr": "EvalxNLP is a Python framework for benchmarking explainability methods for NLP models, integrating multiple techniques to enhance interpretability.", "motivation": "To address the need for frameworks that help stakeholders select appropriate explanations tailored to their specific use cases in NLP.", "method": "EvalxNLP integrates eight widely recognized explainability techniques, allowing users to generate and evaluate explanations based on properties like faithfulness, plausibility, and complexity. It also provides interactive LLM-based textual explanations.", "result": "Human evaluations show high user satisfaction with EvalxNLP, indicating its effectiveness as a benchmarking framework for explanation methods across user groups.", "conclusion": "EvalxNLP is positioned to democratize explainability tools and support the advancement of XAI techniques in NLP.", "key_contributions": ["Development of EvalxNLP as a benchmarking tool for explainability methods in NLP", "Integration of interactive LLM-based explanations", "High user satisfaction demonstrated through human evaluation"], "limitations": "", "keywords": ["explainability", "NLP", "feature attribution", "XAI", "EvalxNLP"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.01255", "pdf": "https://arxiv.org/pdf/2505.01255.pdf", "abs": "https://arxiv.org/abs/2505.01255", "title": "PREMISE: Matching-based Prediction for Accurate Review Recommendation", "authors": ["Wei Han", "Hui Chen", "Soujanya Poria"], "categories": ["cs.CL", "cs.IR", "cs.MM"], "comment": "19 pages, 16 figures", "summary": "We present PREMISE (PREdict with Matching ScorEs), a new architecture for the\nmatching-based learning in the multimodal fields for the multimodal review\nhelpfulness (MRHP) task. Distinct to previous fusion-based methods which\nobtains multimodal representations via cross-modal attention for downstream\ntasks, PREMISE computes the multi-scale and multi-field representations,\nfilters duplicated semantics, and then obtained a set of matching scores as\nfeature vectors for the downstream recommendation task. This new architecture\nsignificantly boosts the performance for such multimodal tasks whose context\nmatching content are highly correlated to the targets of that task, compared to\nthe state-of-the-art fusion-based methods. Experimental results on two publicly\navailable datasets show that PREMISE achieves promising performance with less\ncomputational cost.", "AI": {"tldr": "PREMISE is a new architecture for matching-based learning in multimodal fields, focusing on improving performance for the multimodal review helpfulness task.", "motivation": "To enhance the performance of multimodal learning tasks while reducing computational costs, particularly for review helpfulness recommendations.", "method": "The PREMISE architecture computes multi-scale and multi-field representations, filters out duplicated semantics, and generates matching scores as feature vectors for downstream recommendation tasks.", "result": "PREMISE significantly outperforms state-of-the-art fusion-based methods in multimodal tasks with high context matching to target queries, while also requiring less computational resources.", "conclusion": "The PREMISE framework provides an effective alternative for multimodal representation learning, demonstrating improved efficiency and performance in real-world applications.", "key_contributions": ["Introduction of the PREMISE architecture for multimodal learning", "Improved performance on the multimodal review helpfulness task", "Reduced computational cost compared to traditional approaches"], "limitations": "", "keywords": ["multimodal learning", "matching scores", "recommendation systems", "review helpfulness", "computational efficiency"], "importance_score": 6, "read_time_minutes": 20}}
{"id": "2505.01273", "pdf": "https://arxiv.org/pdf/2505.01273.pdf", "abs": "https://arxiv.org/abs/2505.01273", "title": "Anti-adversarial Learning: Desensitizing Prompts for Large Language Models", "authors": ["Xuan Li", "Zhe Yin", "Xiaodong Gu", "Beijun Shen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the widespread use of LLMs, preserving privacy in user prompts has\nbecome crucial, as prompts risk exposing privacy and sensitive data to the\ncloud LLMs. Traditional techniques like homomorphic encryption, secure\nmulti-party computation, and federated learning face challenges due to heavy\ncomputational costs and user participation requirements, limiting their\napplicability in LLM scenarios. In this paper, we propose PromptObfus, a novel\nmethod for desensitizing LLM prompts. The core idea of PromptObfus is\n\"anti-adversarial\" learning, which perturbs privacy words in the prompt to\nobscure sensitive information while retaining the stability of model\npredictions. Specifically, PromptObfus frames prompt desensitization as a\nmasked language modeling task, replacing privacy-sensitive terms with a [MASK]\ntoken. A desensitization model is trained to generate candidate replacements\nfor each masked position. These candidates are subsequently selected based on\ngradient feedback from a surrogate model, ensuring minimal disruption to the\ntask output. We demonstrate the effectiveness of our approach on three NLP\ntasks. Results show that PromptObfus effectively prevents privacy inference\nfrom remote LLMs while preserving task performance.", "AI": {"tldr": "The paper introduces PromptObfus, a method for desensitizing prompts to protect user privacy when using LLMs by perturbing sensitive words while maintaining task performance.", "motivation": "The increasing use of LLMs raises privacy concerns as prompts may expose sensitive information, motivating the need for effective desensitization methods.", "method": "PromptObfus employs 'anti-adversarial' learning, treating prompt desensitization as a masked language modeling task where privacy-sensitive terms in prompts are replaced by a [MASK] token. A desensitization model is trained to propose replacements for these tokens, guided by gradient feedback from a surrogate model.", "result": "The approach was tested on three NLP tasks, demonstrating that PromptObfus effectively prevents privacy inference while maintaining performance.", "conclusion": "PromptObfus is a practical solution for preserving user privacy in LLM interactions, balancing desensitization with task performance.", "key_contributions": ["Introduction of PromptObfus for LLM prompt desensitization", "Application of anti-adversarial learning to protect privacy", "Demonstration of effectiveness through empirical results on NLP tasks"], "limitations": "The method may still require further validation across diverse NLP tasks and scenarios.", "keywords": ["privacy", "LLMs", "desensitization", "anti-adversarial learning", "NLP"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.01311", "pdf": "https://arxiv.org/pdf/2505.01311.pdf", "abs": "https://arxiv.org/abs/2505.01311", "title": "A Factorized Probabilistic Model of the Semantics of Vague Temporal Adverbials Relative to Different Event Types", "authors": ["Svenja Kenneweg", "Jörg Deigmöller", "Julian Eggert", "Philipp Cimiano"], "categories": ["cs.CL"], "comment": "7 pages, 1 figure, to be published in CogSci Proceedings 2025", "summary": "Vague temporal adverbials, such as recently, just, and a long time ago,\ndescribe the temporal distance between a past event and the utterance time but\nleave the exact duration underspecified. In this paper, we introduce a\nfactorized model that captures the semantics of these adverbials as\nprobabilistic distributions. These distributions are composed with\nevent-specific distributions to yield a contextualized meaning for an adverbial\napplied to a specific event. We fit the model's parameters using existing data\ncapturing judgments of native speakers regarding the applicability of these\nvague temporal adverbials to events that took place a given time ago. Comparing\nour approach to a non-factorized model based on a single Gaussian distribution\nfor each pair of event and temporal adverbial, we find that while both models\nhave similar predictive power, our model is preferable in terms of Occam's\nrazor, as it is simpler and has better extendability.", "AI": {"tldr": "This paper presents a factorized model for vague temporal adverbials that captures their semantics as probabilistic distributions, offering improved simplicity and extendability compared to traditional models.", "motivation": "To capture the nuanced semantics of vague temporal adverbials in a context-aware manner.", "method": "The authors introduce a factorized model that composes vague adverbials with event-specific distributions, fitting parameters using native speaker judgments on the applicability of these adverbials to past events.", "result": "The model aligns closely in predictive power with a non-factorized Gaussian model but is simpler and allows for better extendability.", "conclusion": "The factorized model is advantageous due to its simplicity and Occam's razor principles, despite similar predictive performance.", "key_contributions": ["Introduction of a factorized model for vague temporal adverbials", "Demonstration of improved simplicity and extendability compared to traditional models", "Parameter fitting using native speaker judgments for contextual relevance."], "limitations": "", "keywords": ["temporal adverbials", "probabilistic distributions", "factorized model"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2505.01314", "pdf": "https://arxiv.org/pdf/2505.01314.pdf", "abs": "https://arxiv.org/abs/2505.01314", "title": "A Transformer-based Neural Architecture Search Method", "authors": ["Shang Wang", "Huanrong Tang", "Jianquan Ouyang"], "categories": ["cs.CL", "cs.NE"], "comment": "GECCO 2023", "summary": "This paper presents a neural architecture search method based on Transformer\narchitecture, searching cross multihead attention computation ways for\ndifferent number of encoder and decoder combinations. In order to search for\nneural network structures with better translation results, we considered\nperplexity as an auxiliary evaluation metric for the algorithm in addition to\nBLEU scores and iteratively improved each individual neural network within the\npopulation by a multi-objective genetic algorithm. Experimental results show\nthat the neural network structures searched by the algorithm outperform all the\nbaseline models, and that the introduction of the auxiliary evaluation metric\ncan find better models than considering only the BLEU score as an evaluation\nmetric.", "AI": {"tldr": "The paper introduces a neural architecture search method using Transformers to improve translation results through multi-objective genetic algorithms, incorporating perplexity as an evaluation metric.", "motivation": "To find better neural network structures for translation tasks beyond traditional BLEU score evaluations.", "method": "The authors developed a method that searches cross multihead attention computation ways for varying encoder and decoder configurations and utilized a multi-objective genetic algorithm to iteratively enhance neural networks.", "result": "The neural network structures identified by the proposed algorithm outperformed all baseline models significantly.", "conclusion": "Incorporating perplexity as an auxiliary evaluation metric led to better model discovery compared to relying solely on BLEU scores.", "key_contributions": ["Introduction of a novel neural architecture search method for Transformers", "Inclusion of perplexity as an auxiliary metric in model evaluation", "Demonstrated superiority of searched models over baseline translation models"], "limitations": "", "keywords": ["neural architecture search", "Transformer", "multi-objective genetic algorithm"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.01315", "pdf": "https://arxiv.org/pdf/2505.01315.pdf", "abs": "https://arxiv.org/abs/2505.01315", "title": "Helping Big Language Models Protect Themselves: An Enhanced Filtering and Summarization System", "authors": ["Sheikh Samit Muhaimin", "Spyridon Mastorakis"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The recent growth in the use of Large Language Models has made them\nvulnerable to sophisticated adversarial assaults, manipulative prompts, and\nencoded malicious inputs. Existing countermeasures frequently necessitate\nretraining models, which is computationally costly and impracticable for\ndeployment. Without the need for retraining or fine-tuning, this study presents\na unique defense paradigm that allows LLMs to recognize, filter, and defend\nagainst adversarial or malicious inputs on their own. There are two main parts\nto the suggested framework: (1) A prompt filtering module that uses\nsophisticated Natural Language Processing (NLP) techniques, including zero-shot\nclassification, keyword analysis, and encoded content detection (e.g. base64,\nhexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and\n(2) A summarization module that processes and summarizes adversarial research\nliterature to give the LLM context-aware defense knowledge. This approach\nstrengthens LLMs' resistance to adversarial exploitation by fusing text\nextraction, summarization, and harmful prompt analysis. According to\nexperimental results, this integrated technique has a 98.71% success rate in\nidentifying harmful patterns, manipulative language structures, and encoded\nprompts. By employing a modest amount of adversarial research literature as\ncontext, the methodology also allows the model to react correctly to harmful\ninputs with a larger percentage of jailbreak resistance and refusal rate. While\nmaintaining the quality of LLM responses, the framework dramatically increases\nLLM's resistance to hostile misuse, demonstrating its efficacy as a quick and\neasy substitute for time-consuming, retraining-based defenses.", "AI": {"tldr": "This study presents a defense framework for Large Language Models that enables them to independently recognize and filter adversarial inputs without the need for retraining. It uses prompt filtering and summarization of adversarial research literature to enhance LLMs' resistance against malicious inputs.", "motivation": "To address the vulnerability of Large Language Models to adversarial assaults and manipulative prompts, providing a defense mechanism that does not require computationally expensive retraining.", "method": "The proposed framework consists of a prompt filtering module using NLP techniques like zero-shot classification, keyword analysis, and encoded content detection, along with a summarization module that provides context-aware knowledge about adversarial literature.", "result": "The integrated technique achieved a 98.71% success rate in identifying harmful patterns and manipulative language structures, significantly increasing the LLM's resistance to adversarial misuse without compromising response quality.", "conclusion": "This framework serves as an effective, low-cost alternative to traditional retraining-based defenses, showcasing how LLMs can self-defend against adversarial attacks.", "key_contributions": ["Development of a framework that does not require retraining for LLM defense", "Integration of NLP techniques for prompt filtering", "Enhanced jailbreak resistance and refusal rate through contextual knowledge"], "limitations": "", "keywords": ["Large Language Models", "adversarial attacks", "deployable defenses", "natural language processing", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.01325", "pdf": "https://arxiv.org/pdf/2505.01325.pdf", "abs": "https://arxiv.org/abs/2505.01325", "title": "TRAVELER: A Benchmark for Evaluating Temporal Reasoning across Vague, Implicit and Explicit References", "authors": ["Svenja Kenneweg", "Jörg Deigmöller", "Philipp Cimiano", "Julian Eggert"], "categories": ["cs.CL"], "comment": "24 pages, 6 figures, submitted to Springer Nature Computer Science", "summary": "Understanding and resolving temporal references is essential in Natural\nLanguage Understanding as we often refer to the past or future in daily\ncommunication. Although existing benchmarks address a system's ability to\nreason about and resolve temporal references, systematic evaluation of specific\ntemporal references remains limited. Towards closing this gap, we introduce\nTRAVELER, a novel synthetic benchmark dataset that follows a Question Answering\nparadigm and consists of questions involving temporal references with the\ncorresponding correct answers. TRAVELER assesses models' abilities to resolve\nexplicit, implicit relative to speech time, and vague temporal references.\nBeyond investigating the performance of state-of-the-art LLMs depending on the\ntype of temporal reference, our benchmark also allows evaluation of performance\nin relation to the length of the set of events. For the category of vague\ntemporal references, ground-truth answers were established via human surveys on\nProlific, following a procedure similar to the one from Kenneweg et al. To\ndemonstrate the benchmark's applicability, we evaluate four state-of-the-art\nLLMs using a question-answering task encompassing 3,300 questions. Our findings\nshow that while the benchmarked LLMs can answer questions over event sets with\na handful of events and explicit temporal references successfully, performance\nclearly deteriorates with larger event set length and when temporal references\nget less explicit. Notably, the vague question category exhibits the lowest\nperformance across all models.\n  The benchmark is publicly available at:\nhttps://gitlab.ub.uni-bielefeld.de/s.kenneweg/TRAVELER", "AI": {"tldr": "Introduction of TRAVELER, a synthetic benchmark dataset for evaluating models on temporal reference resolution in question answering.", "motivation": "To address the limited systematic evaluation of models' ability to resolve different types of temporal references in Natural Language Understanding.", "method": "TRAVELER consists of a variety of questions with explicit, implicit, and vague temporal references evaluated through a Question Answering paradigm.", "result": "Assessment of four state-of-the-art LLMs on 3,300 questions showed that performance deteriorates with larger event sets and less explicit temporal references, particularly for vague questions.", "conclusion": "TRAVELER provides a valuable tool for evaluating temporal reference resolution in models, highlighting significant performance gaps that need to be addressed.", "key_contributions": ["Introduction of a novel synthetic benchmark dataset for temporal reference evaluation.", "Evaluation of LLM performance across different types of temporal references.", "Public availability of the benchmark for further research."], "limitations": "Performance of models is notably low for vague temporal references and with increased event set sizes.", "keywords": ["Temporal References", "Natural Language Understanding", "Question Answering", "Large Language Models", "Benchmark Dataset"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2402.14359", "pdf": "https://arxiv.org/pdf/2402.14359.pdf", "abs": "https://arxiv.org/abs/2402.14359", "title": "Rethinking Scientific Summarization Evaluation: Grounding Explainable Metrics on Facet-aware Benchmark", "authors": ["Xiuying Chen", "Tairan Wang", "Qingqing Zhu", "Taicheng Guo", "Shen Gao", "Zhiyong Lu", "Xin Gao", "Xiangliang Zhang"], "categories": ["cs.CL"], "comment": "14pages", "summary": "The summarization capabilities of pretrained and large language models (LLMs)\nhave been widely validated in general areas, but their use in scientific\ncorpus, which involves complex sentences and specialized knowledge, has been\nless assessed. This paper presents conceptual and experimental analyses of\nscientific summarization, highlighting the inadequacies of traditional\nevaluation methods, such as $n$-gram, embedding comparison, and QA,\nparticularly in providing explanations, grasping scientific concepts, or\nidentifying key content. Subsequently, we introduce the Facet-aware Metric\n(FM), employing LLMs for advanced semantic matching to evaluate summaries based\non different aspects. This facet-aware approach offers a thorough evaluation of\nabstracts by decomposing the evaluation task into simpler subtasks.Recognizing\nthe absence of an evaluation benchmark in this domain, we curate a Facet-based\nscientific summarization Dataset (FD) with facet-level annotations. Our\nfindings confirm that FM offers a more logical approach to evaluating\nscientific summaries. In addition, fine-tuned smaller models can compete with\nLLMs in scientific contexts, while LLMs have limitations in learning from\nin-context information in scientific domains. This suggests an area for future\nenhancement of LLMs.", "AI": {"tldr": "This paper assesses the use of pretrained LLMs for scientific summarization, introduces the Facet-aware Metric for evaluation, and curates a new dataset with facet-level annotations.", "motivation": "The inadequacy of traditional evaluation methods for scientific summarization and the need for effective metrics driven by LLMs.", "method": "Introducing the Facet-aware Metric (FM) for evaluation of scientific summaries using LLMs and curating a Facet-based scientific summarization Dataset (FD).", "result": "FM provides a more logical evaluation method for scientific summaries; fine-tuned smaller models can match the performance of LLMs in scientific contexts.", "conclusion": "While LLMs show limitations in in-context learning for scientific domains, the FM evaluation approach is promising for accurate summarization assessments.", "key_contributions": ["Introduction of the Facet-aware Metric (FM) for summarization evaluation", "Creation of the Facet-based summarization Dataset (FD)", "Demonstration that smaller models can effectively compete with LLMs in scientific contexts"], "limitations": "Identifies limitations of LLMs in scientific domain in-context information learning.", "keywords": ["scientific summarization", "large language models", "evaluation metrics"], "importance_score": 9, "read_time_minutes": 14}}
{"id": "2404.18624", "pdf": "https://arxiv.org/pdf/2404.18624.pdf", "abs": "https://arxiv.org/abs/2404.18624", "title": "Do Vision & Language Decoders use Images and Text equally? How Self-consistent are their Explanations?", "authors": ["Letitia Parcalabescu", "Anette Frank"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "68Txx", "I.2.7; I.2.10"], "comment": "30 pages, 8 figures, 11 tables", "summary": "Vision and language model (VLM) decoders are currently the best-performing\narchitectures on multimodal tasks. Next to answers, they are able to produce\nnatural language explanations, either in post-hoc or CoT settings. However, it\nis not clear to what extent they are using the input vision and text modalities\nwhen generating answers or explanations. In this work, we investigate if VLMs\nrely on their input modalities differently when they produce explanations as\nopposed to answers. We also evaluate the self-consistency of VLM decoders in\nboth post-hoc and CoT explanation settings, by extending existing unimodal\ntests and measures to VLM decoders. We find that most tested VLMs are less\nself-consistent than LLMs. Text contributions in all tested VL decoders are\nmore important than image contributions in all examined tasks. However, when\ncomparing explanation generation to answer generation, the contributions of\nimages are significantly stronger for generating explanations compared to\nanswers. This difference is even larger in CoT compared to post-hoc\nexplanations. Lastly, we provide an up-to-date benchmarking of state-of-the-art\nVL decoders on the VALSE benchmark, which before was restricted to VL encoders.\nWe find that the tested VL decoders still struggle with most phenomena tested\nby VALSE.", "AI": {"tldr": "This paper investigates the reliance of vision and language model decoders on input modalities when generating answers and explanations, finding a notable difference in their usage.", "motivation": "To understand how VLM decoders utilize vision and text inputs differently in generating answers versus explanations.", "method": "The study extends existing unimodal tests to evaluate VLM decoders, measuring their self-consistency in post-hoc and CoT explanation settings.", "result": "Most VLMs exhibit lower self-consistency than LLMs, with text contributions being more significant than image contributions across tasks; however, image contributions are more critical for explanation generation than for answer generation.", "conclusion": "VLM decoders continue to face challenges with various phenomena as per the VALSE benchmark, indicating the need for further improvement in multimodal integration.", "key_contributions": ["Investigating the modality reliance in VLM decoders for explanation vs. answer generation", "Benchmarking VLM decoders on the VALSE benchmark", "Highlighting the differences in self-consistency between VLMs and LLMs"], "limitations": "The tested models still struggle with many phenomena as per the VALSE benchmark.", "keywords": ["vision and language models", "self-consistency", "multimodal tasks", "explanation generation", "benchmarking"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2409.00292", "pdf": "https://arxiv.org/pdf/2409.00292.pdf", "abs": "https://arxiv.org/abs/2409.00292", "title": "REFFLY: Melody-Constrained Lyrics Editing Model", "authors": ["Songyan Zhao", "Bingxuan Li", "Yufei Tian", "Nanyun Peng"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Automatic melody-to-lyric (M2L) generation aims to create lyrics that align\nwith a given melody. While most previous approaches generate lyrics from\nscratch, revision, editing plain text draft to fit it into the melody, offers a\nmuch more flexible and practical alternative. This enables broad applications,\nsuch as generating lyrics from flexible inputs (keywords, themes, or full text\nthat needs refining to be singable), song translation (preserving meaning\nacross languages while keeping the melody intact), or style transfer (adapting\nlyrics to different genres). This paper introduces REFFLY (REvision Framework\nFor LYrics), the first revision framework for editing and generating\nmelody-aligned lyrics. We train the lyric revision module using our curated\nsynthesized melody-aligned lyrics dataset, enabling it to transform plain text\ninto lyrics that align with a given melody. To further enhance the revision\nability, we propose training-free heuristics aimed at preserving both semantic\nmeaning and musical consistency throughout the editing process. Experimental\nresults demonstrate the effectiveness of REFFLY across various tasks (e.g.\nlyrics generation, song translation), showing that our model outperforms strong\nbaselines, including Lyra (Tian et al., 2023) and GPT-4, by 25% in both\nmusicality and text quality.", "AI": {"tldr": "REFFLY is a novel framework for editing and generating melody-aligned lyrics, allowing flexible inputs and outperforming existing models in various music-related tasks.", "motivation": "Existing melody-to-lyric generation methods primarily create lyrics from scratch; revision offers a more flexible and practical alternative.", "method": "REFFLY uses a curated dataset of melody-aligned lyrics to train a revision module that transforms plain text into lyrics, implementing training-free heuristics for semantic and musical consistency.", "result": "REFFLY outperforms strong baselines (e.g., Lyra and GPT-4) by 25% in both musicality and text quality across various tasks like lyrics generation and song translation.", "conclusion": "The results demonstrate the effectiveness of REFFLY in generating and editing lyrics that align with given melodies and hold semantic and musical integrity.", "key_contributions": ["Introduction of REFFLY, the first revision framework for melody-aligned lyrics", "Utilization of a curated dataset for training", "Implementation of training-free heuristics for better revision quality"], "limitations": "", "keywords": ["melody-to-lyric generation", "lyric revision", "music technology", "natural language generation", "AI in music"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2412.00359", "pdf": "https://arxiv.org/pdf/2412.00359.pdf", "abs": "https://arxiv.org/abs/2412.00359", "title": "Does Self-Attention Need Separate Weights in Transformers?", "authors": ["Md Kowsher", "Nusrat Jahan Prottasha", "Chun-Nam Yu", "Ozlem Ozmen Garibay", "Niloofar Yousefi"], "categories": ["cs.CL"], "comment": "Preprint paper", "summary": "The success of self-attention lies in its ability to capture long-range\ndependencies and enhance context understanding, but it is limited by its\ncomputational complexity and challenges in handling sequential data with\ninherent directionality. This work introduces a shared weight\nself-attention-based BERT model that only learns one weight matrix for (Key,\nValue, and Query) representations instead of three individual matrices for each\nof them. Our shared weight attention reduces the training parameter size by\nmore than half and training time by around one-tenth. Furthermore, we\ndemonstrate higher prediction accuracy on small tasks of GLUE over the BERT\nbaseline and in particular a generalization power on noisy and out-of-domain\ndata. Experimental results indicate that our shared self-attention method\nachieves a parameter size reduction of 66.53% in the attention block. In the\nGLUE dataset, the shared weight self-attention-based BERT model demonstrates\naccuracy improvements of 0.38%, 5.81%, and 1.06% over the standard, symmetric,\nand pairwise attention-based BERT models, respectively. The model and source\ncode are available at Anonymous.", "AI": {"tldr": "This paper presents a new shared weight self-attention-based BERT model that reduces parameter size and training time while improving prediction accuracy on small GLUE tasks.", "motivation": "To address the computational complexity of self-attention and improve its application in sequential data processing.", "method": "The proposed model uses a single weight matrix for Key, Value, and Query representations, significantly reducing parameter size and training time.", "result": "Achieves a 66.53% reduction in parameter size within the attention block and improved accuracy on the GLUE dataset compared to standard models.", "conclusion": "The shared weight self-attention method enhances efficiency and prediction accuracy, offering a promising alternative to traditional self-attention mechanisms.", "key_contributions": ["Introduction of a shared weight self-attention mechanism for BERT", "Reduction of training parameters and time by over half", "Improved generalization on noisy and out-of-domain data"], "limitations": "", "keywords": ["self-attention", "BERT", "parameter reduction", "GLUE", "machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2412.06926", "pdf": "https://arxiv.org/pdf/2412.06926.pdf", "abs": "https://arxiv.org/abs/2412.06926", "title": "When Every Token Counts: Optimal Segmentation for Low-Resource Language Models", "authors": ["Bharath Raj", "Garvit Suri", "Vikrant Dewangan", "Raghav Sonavane"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "LoResLM @ COLING 2025. Project page at\n  https://vikr-182.github.io/loreslm/", "summary": "Traditional greedy tokenization methods have been a critical step in Natural\nLanguage Processing (NLP), influencing how text is converted into tokens and\ndirectly impacting model performance. While subword tokenizers like Byte-Pair\nEncoding (BPE) are widely used, questions remain about their optimality across\nmodel scales and languages. In this work, we demonstrate through extensive\nexperiments that an optimal BPE configuration significantly reduces token count\ncompared to greedy segmentation, yielding improvements in token-saving\npercentages and performance benefits, particularly for smaller models. We\nevaluate tokenization performance across various intrinsic and extrinsic tasks,\nincluding generation and classification. Our findings suggest that\ncompression-optimized tokenization strategies could provide substantial\nadvantages for multilingual and low-resource language applications,\nhighlighting a promising direction for further research and inclusive NLP.", "AI": {"tldr": "The paper investigates the effectiveness of optimal Byte-Pair Encoding (BPE) configurations in tokenization for NLP, showcasing improvements in token count and model performance, especially for smaller models.", "motivation": "To explore the optimality of subword tokenization methods and their impact on model performance across scales and languages.", "method": "Extensive experiments evaluating tokenization performance using various intrinsic and extrinsic tasks, including generation and classification.", "result": "An optimal BPE configuration significantly reduces token count compared to greedy segmentation, leading to performance improvements, particularly in smaller models.", "conclusion": "Compression-optimized tokenization strategies show promise for enhancing NLP applications in multilingual and low-resource contexts, suggesting a new research direction.", "key_contributions": ["Demonstrated significant reductions in token count with optimal BPE configurations", "Provided insights into tokenization performance across multiple tasks", "Highlighted the value of compression-optimized methods for diverse language applications"], "limitations": "", "keywords": ["tokenization", "Byte-Pair Encoding", "Natural Language Processing", "multilingual applications", "low-resource languages"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2412.10422", "pdf": "https://arxiv.org/pdf/2412.10422.pdf", "abs": "https://arxiv.org/abs/2412.10422", "title": "AutoPrep: Natural Language Question-Aware Data Preparation with a Multi-Agent Framework", "authors": ["Meihao Fan", "Ju Fan", "Nan Tang", "Lei Cao", "Guoliang Li", "Xiaoyong Du"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Answering natural language (NL) questions about tables, known as Tabular\nQuestion Answering (TQA), is crucial because it allows users to quickly and\nefficiently extract meaningful insights from structured data, effectively\nbridging the gap between human language and machine-readable formats. Many of\nthese tables are derived from web sources or real-world scenarios, which\nrequire meticulous data preparation (or data prep) to ensure accurate\nresponses. However, preparing such tables for NL questions introduces new\nrequirements that extend beyond traditional data preparation. This\nquestion-aware data preparation involves specific tasks such as column\nderivation and filtering tailored to particular questions, as well as\nquestion-aware value normalization or conversion, highlighting the need for a\nmore nuanced approach in this context. Because each of the above tasks is\nunique, a single model (or agent) may not perform effectively across all\nscenarios. In this paper, we propose AutoPrep, a large language model\n(LLM)-based multi-agent framework that leverages the strengths of multiple\nagents, each specialized in a certain type of data prep, ensuring more accurate\nand contextually relevant responses. Given an NL question over a table,\nAutoPrep performs data prep through three key components. Planner: Determines a\nlogical plan, outlining a sequence of high-level operations. Programmer:\nTranslates this logical plan into a physical plan by generating the\ncorresponding low-level code. Executor: Executes the generated code to process\nthe table. To support this multi-agent framework, we design a novel\nChain-of-Clauses reasoning mechanism for high-level operation suggestion, and a\ntool-augmented method for low-level code generation...", "AI": {"tldr": "AutoPrep is an LLM-based multi-agent framework designed for question-aware data preparation in tabular question answering, improving accuracy and relevance in responses.", "motivation": "The motivation is to efficiently extract meaningful insights from structured data via natural language questions, addressing the challenges of existing data preparation methods.", "method": "The framework uses three components: Planner for logical planning, Programmer for translating plans into code, and Executor for executing table processing tasks, all enhanced by a novel reasoning mechanism.", "result": "AutoPrep demonstrates improved performance in question-aware data preparation tasks, leading to more accurate answers to natural language questions about tables.", "conclusion": "The study concludes that a multi-agent approach tailored to specific data prep tasks allows for better handling of question-aware scenarios.", "key_contributions": ["Introduction of AutoPrep as a multi-agent framework for data preparation", "Development of a Chain-of-Clauses reasoning mechanism", "Innovative tool-augmented code generation method"], "limitations": "", "keywords": ["Tabular Question Answering", "Data Preparation", "Multi-Agent Framework", "Natural Language Processing", "Large Language Models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2501.00070", "pdf": "https://arxiv.org/pdf/2501.00070.pdf", "abs": "https://arxiv.org/abs/2501.00070", "title": "ICLR: In-Context Learning of Representations", "authors": ["Core Francisco Park", "Andrew Lee", "Ekdeep Singh Lubana", "Yongyi Yang", "Maya Okawa", "Kento Nishi", "Martin Wattenberg", "Hidenori Tanaka"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICLR 2025", "summary": "Recent work has demonstrated that semantics specified by pretraining data\ninfluence how representations of different concepts are organized in a large\nlanguage model (LLM). However, given the open-ended nature of LLMs, e.g., their\nability to in-context learn, we can ask whether models alter these pretraining\nsemantics to adopt alternative, context-specified ones. Specifically, if we\nprovide in-context exemplars wherein a concept plays a different role than what\nthe pretraining data suggests, do models reorganize their representations in\naccordance with these novel semantics? To answer this question, we take\ninspiration from the theory of conceptual role semantics and define a toy\n\"graph tracing\" task wherein the nodes of the graph are referenced via concepts\nseen during training (e.g., apple, bird, etc.) and the connectivity of the\ngraph is defined via some predefined structure (e.g., a square grid). Given\nexemplars that indicate traces of random walks on the graph, we analyze\nintermediate representations of the model and find that as the amount of\ncontext is scaled, there is a sudden re-organization from pretrained semantic\nrepresentations to in-context representations aligned with the graph structure.\nFurther, we find that when reference concepts have correlations in their\nsemantics (e.g., Monday, Tuesday, etc.), the context-specified graph structure\nis still present in the representations, but is unable to dominate the\npretrained structure. To explain these results, we analogize our task to energy\nminimization for a predefined graph topology, providing evidence towards an\nimplicit optimization process to infer context-specified semantics. Overall,\nour findings indicate scaling context-size can flexibly re-organize model\nrepresentations, possibly unlocking novel capabilities.", "AI": {"tldr": "This paper explores how large language models (LLMs) adapt their semantic representations when provided with context-specific examples, particularly in a graph tracing task, revealing a shift from pretraining semantics to context-adopted semantics as context size increases.", "motivation": "The influence of pretraining data on LLM representations and the potential for these models to reorganize semantics based on in-context learning.", "method": "A graph tracing task is introduced where nodes represent concepts learned during training, and connectivity is based on a defined structure. The study analyzes model representations as the context size varies and uses energy minimization analogies to explain the findings.", "result": "As context size scales, models demonstrate a significant re-organization of semantic representations from pretrained to context-specified structures. However, in cases with correlated reference concepts, the pretrained semantics still prevail.", "conclusion": "Scaling context size can lead to flexible re-organization of representations in LLMs, which may enable the discovery of new capabilities in how these models interpret and respond to context.", "key_contributions": ["Demonstrates the reorganization of LLM representations with increased context size.", "Establishes a link between semantic representations and graph topology through energy minimization.", "Provides insights into in-context learning and its effects on pretrained semantics."], "limitations": "", "keywords": ["large language models", "contextual semantics", "graph tracing", "semantic reorganization", "in-context learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2501.06382", "pdf": "https://arxiv.org/pdf/2501.06382.pdf", "abs": "https://arxiv.org/abs/2501.06382", "title": "Dynamics of Spontaneous Topic Changes in Next Token Prediction with Self-Attention", "authors": ["Mumin Jia", "Jairo Diaz-Rodriguez"], "categories": ["cs.CL", "cs.AI", "stat.ML"], "comment": null, "summary": "Human cognition is punctuated by abrupt, spontaneous shifts between\ntopics-driven by emotional, contextual, or associative cues-a phenomenon known\nas spontaneous thought in neuroscience. In contrast, self-attention based\nmodels depend on structured patterns over their inputs to predict each next\ntoken, lacking spontaneity. Motivated by this distinction, we characterize\nspontaneous topic changes in self-attention architectures, revealing both their\nsimilarities and their divergences from spontaneous human thought. First, we\nestablish theoretical results under a simplified, single-layer self-attention\nmodel with suitable conditions by defining the topic as a set of Token Priority\nGraphs (TPGs). Specifically, we demonstrate that (1) the model maintains the\npriority order of tokens related to the input topic, (2) a spontaneous topic\nchange can occur only if lower-priority tokens outnumber all higher-priority\ntokens of the input topic, and (3) unlike human cognition, the longer context\nlength or the more ambiguous input topic reduces the likelihood of spontaneous\nchange. Second, we empirically validate that these dynamics persist in modern,\nstate-of-the-art LLMs, underscoring a fundamental disparity between human\ncognition and AI behaviour in the context of spontaneous topic changes. To the\nbest of our knowledge, no prior work has explored these questions with a focus\nas closely aligned to human thought.", "AI": {"tldr": "The paper explores spontaneous topic changes in self-attention models, contrasting them with human cognition.", "motivation": "To investigate how self-attention models handle spontaneous topic changes in comparison to human thought, highlighting the lack of spontaneity in AI.", "method": "The paper analyzes a simplified, single-layer self-attention model theoretically, defining topics via Token Priority Graphs (TPGs) and empirically validating the findings in modern LLMs.", "result": "The model maintains token priority orders, indicates that spontaneous changes occur under specific conditions, and shows that longer contexts reduce the likelihood of these changes, unlike human cognition.", "conclusion": "There is a fundamental disparity between human cognition and AI behavior in terms of spontaneous topic changes, with implications for understanding AI limitations.", "key_contributions": ["Characterization of spontaneous topic changes in self-attention architectures", "Empirical validation in modern LLMs", "Identification of conditions under which AI can exhibit topic shifts."], "limitations": "", "keywords": ["self-attention", "spontaneous thought", "human cognition", "LLM", "Token Priority Graphs"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2501.19378", "pdf": "https://arxiv.org/pdf/2501.19378.pdf", "abs": "https://arxiv.org/abs/2501.19378", "title": "TableMaster: A Recipe to Advance Table Understanding with Language Models", "authors": ["Lang Cao", "Hanbing Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Tables serve as a fundamental format for representing structured relational\ndata. While current language models (LMs) excel at many text-based tasks, they\nstill face challenges in table understanding due to the complex characteristics\nof tabular data, such as their structured nature. In this paper, we aim to\nenhance LMs for improved table understanding. We identify four key challenges:\n1) difficulty in locating target data, 2) deficiency in table semantics, 3)\nnumerical inaccuracies in textual reasoning, and 4) semantic inflexibility in\nsymbolic reasoning. To address these issues, we propose TableMaster, a recipe\nand comprehensive framework that integrates multiple solutions to overcome\nthese obstacles. TableMaster first extracts relevant table content and\nverbalizes it with enriched semantic context. Additionally, we introduce\nadaptive reasoning, a flexible approach that dynamically adjusts between\ntextual and symbolic reasoning, tailoring the reasoning process to each query.\nExtensive analyses and experiments demonstrate our findings and the\neffectiveness of TableMaster. On the WikiTQ dataset, TableMaster achieves an\naccuracy of 78.13% using GPT-4o-mini, surpassing existing baselines.", "AI": {"tldr": "This paper introduces TableMaster, a framework designed to enhance language models' understanding of tabular data by addressing key challenges in table semantics, reasoning, and numerical accuracy.", "motivation": "To improve language models' (LMs) performance on table understanding, tackling inherent challenges in structured relational data.", "method": "The authors propose TableMaster, which extracts relevant table content and uses adaptive reasoning to adjust between textual and symbolic reasoning based on the query.", "result": "TableMaster achieves an accuracy of 78.13% on the WikiTQ dataset using GPT-4o-mini, outperforming existing methods.", "conclusion": "The framework effectively addresses the identified challenges in table understanding, proving beneficial for language models.", "key_contributions": ["Introduction of TableMaster framework for table understanding.", "Adaptive reasoning approach for flexible processing of queries.", "Enhanced extraction and semantic context for tabular data."], "limitations": "", "keywords": ["table understanding", "language models", "adaptive reasoning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.01976", "pdf": "https://arxiv.org/pdf/2502.01976.pdf", "abs": "https://arxiv.org/abs/2502.01976", "title": "CITER: Collaborative Inference for Efficient Large Language Model Decoding with Token-Level Routing", "authors": ["Wenhao Zheng", "Yixiao Chen", "Weitong Zhang", "Souvik Kundu", "Yun Li", "Zhengzhong Liu", "Eric P. Xing", "Hongyi Wang", "Huaxiu Yao"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF"], "comment": null, "summary": "Large language models have achieved remarkable success in various tasks but\nsuffer from high computational costs during inference, limiting their\ndeployment in resource-constrained applications. To address this issue, we\npropose a novel Collaborative Inference with Token-lEvel Routing (CITER)\nframework that enables efficient collaboration between small and large language\nmodels (SLMs \\& LLMs) through a token-level routing strategy. Specifically,\nCITER routes non-critical tokens to an SLM for efficiency and routes critical\ntokens to an LLM for generalization quality. We formulate router training as a\npolicy optimization, where the router receives rewards based on both the\nquality of predictions and the inference costs of generation. This allows the\nrouter to learn to predict token-level routing scores and make routing\ndecisions based on both the current token and the future impact of its\ndecisions. To further accelerate the reward evaluation process, we introduce a\nshortcut which significantly reduces the costs of the reward estimation and\nimproving the practicality of our approach. Extensive experiments on five\nbenchmark datasets demonstrate that CITER reduces the inference costs while\npreserving high-quality generation, offering a promising solution for real-time\nand resource-constrained applications. Our data and code are available at\nhttps://github.com/aiming-lab/CITER.", "AI": {"tldr": "The CITER framework enhances inference efficiency in large language models by using a token-level routing strategy between small and large models.", "motivation": "To reduce the high computational costs during inference of large language models, which limits their usability in resource-constrained settings.", "method": "A novel collaborative inference framework called CITER, which implements token-level routing between small and large language models, routing non-critical tokens to small models and critical tokens to large models. Router training is formulated as policy optimization with rewards based on prediction quality and inference costs.", "result": "CITER significantly lessens inference costs while maintaining high-quality generation across five benchmark datasets.", "conclusion": "CITER presents a viable solution for efficient language model deployment in real-time and resource-constrained applications.", "key_contributions": ["Introduced a token-level routing strategy for efficient collaboration between small and large language models.", "Formulated router training as a policy optimization problem to balance prediction quality and cost efficiency.", "Developed a shortcut for reward evaluation that reduces the computational overhead of the routing process."], "limitations": "", "keywords": ["large language models", "inference efficiency", "token-level routing", "resource-constrained applications", "collaborative inference"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.13068", "pdf": "https://arxiv.org/pdf/2504.13068.pdf", "abs": "https://arxiv.org/abs/2504.13068", "title": "Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models", "authors": ["Sudesh Ramesh Bhagat", "Ibne Farabi Shihab", "Anuj Sharma"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study investigates the relationship between deep learning (DL) model\naccuracy and expert agreement in classifying crash narratives. We evaluate five\nDL models -- including BERT variants, USE, and a zero-shot classifier --\nagainst expert labels and narratives, and extend the analysis to four large\nlanguage models (LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our findings reveal\nan inverse relationship: models with higher technical accuracy often show lower\nagreement with human experts, while LLMs demonstrate stronger expert alignment\ndespite lower accuracy. We use Cohen's Kappa and Principal Component Analysis\n(PCA) to quantify and visualize model-expert agreement, and employ SHAP\nanalysis to explain misclassifications. Results show that expert-aligned models\nrely more on contextual and temporal cues than location-specific keywords.\nThese findings suggest that accuracy alone is insufficient for safety-critical\nNLP tasks. We argue for incorporating expert agreement into model evaluation\nframeworks and highlight the potential of LLMs as interpretable tools in crash\nanalysis pipelines.", "AI": {"tldr": "This study examines the relationship between the accuracy of deep learning models and expert agreement in classifying crash narratives, finding that models with higher accuracy often have lower agreement with experts, while LLMs align more closely with expert judgments.", "motivation": "To investigate the effectiveness of various deep learning models and LLMs in accurately classifying crash narratives and their alignment with expert assessments.", "method": "Evaluated five deep learning models (including BERT variants and a zero-shot classifier) and extended the analysis to four large language models (GPT-4, LLaMA 3, Qwen, Claude) using Cohen's Kappa, PCA, and SHAP analysis to explore misclassifications and model-expert agreement.", "result": "The study found an inverse relationship between model accuracy and expert agreement; LLMs showed greater alignment with expert classifications despite being less accurate than some DL models.", "conclusion": "Accuracy alone is not a sufficient metric for safety-critical NLP tasks, suggesting the incorporation of expert agreement into evaluation frameworks and highlighting the utility of LLMs as interpretable tools in crash analysis.", "key_contributions": ["Demonstrated an inverse relationship between DL model accuracy and expert agreement.", "Highlighted the stronger alignment of LLMs with expert assessments.", "Proposed the need for incorporating expert agreement into model evaluation metrics."], "limitations": "The study primarily focuses on crash narratives and may not be generalizable to other safety-critical contexts.", "keywords": ["deep learning", "expert agreement", "LLM", "crash analysis", "NLP"], "importance_score": 9, "read_time_minutes": 15}}
